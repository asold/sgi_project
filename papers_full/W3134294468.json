{
  "title": "Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers",
  "url": "https://openalex.org/W3134294468",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2396430639",
      "name": "Shijie Geng",
      "affiliations": [
        "Rutgers Sexual and Reproductive Health and Rights"
      ]
    },
    {
      "id": "https://openalex.org/A1892031112",
      "name": "Peng Gao",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2097992455",
      "name": "Moitreya Chatterjee",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A1993283691",
      "name": "Chiori Hori",
      "affiliations": [
        "Mitsubishi Electric (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2150216086",
      "name": "Jonathan Le Roux",
      "affiliations": [
        "Mitsubishi Electric (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2112127489",
      "name": "Yongfeng Zhang",
      "affiliations": [
        "Rutgers Sexual and Reproductive Health and Rights"
      ]
    },
    {
      "id": "https://openalex.org/A2114915690",
      "name": "Hongsheng Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2155554263",
      "name": "Anoop Cherian",
      "affiliations": [
        "Mitsubishi Electric (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2396430639",
      "name": "Shijie Geng",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A1892031112",
      "name": "Peng Gao",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2097992455",
      "name": "Moitreya Chatterjee",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A1993283691",
      "name": "Chiori Hori",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2150216086",
      "name": "Jonathan Le Roux",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112127489",
      "name": "Yongfeng Zhang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2114915690",
      "name": "Hongsheng Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2155554263",
      "name": "Anoop Cherian",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2745461083",
    "https://openalex.org/W6640773114",
    "https://openalex.org/W2904742344",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W6745764446",
    "https://openalex.org/W2412400526",
    "https://openalex.org/W2914190582",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W2968388725",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W6757010476",
    "https://openalex.org/W6768852837",
    "https://openalex.org/W2584992898",
    "https://openalex.org/W2176353499",
    "https://openalex.org/W2606982687",
    "https://openalex.org/W2996278493",
    "https://openalex.org/W6669605930",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3035314827",
    "https://openalex.org/W3099388488",
    "https://openalex.org/W6761665040",
    "https://openalex.org/W2913618459",
    "https://openalex.org/W6721769334",
    "https://openalex.org/W2808877322",
    "https://openalex.org/W3014258398",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2937473825",
    "https://openalex.org/W6761417287",
    "https://openalex.org/W3000075142",
    "https://openalex.org/W3100232116",
    "https://openalex.org/W2924996095",
    "https://openalex.org/W6680375555",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W6751936687",
    "https://openalex.org/W6747904511",
    "https://openalex.org/W2769430803",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W7043946624",
    "https://openalex.org/W6756147243",
    "https://openalex.org/W2922136209",
    "https://openalex.org/W2724359148",
    "https://openalex.org/W2999894541",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963287297",
    "https://openalex.org/W2141080726",
    "https://openalex.org/W2963383024",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2902469875",
    "https://openalex.org/W2963623904",
    "https://openalex.org/W3034679267",
    "https://openalex.org/W2914694625",
    "https://openalex.org/W3015591594",
    "https://openalex.org/W3000279895",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2981182029",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2980339970",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W3104529101",
    "https://openalex.org/W4299522971",
    "https://openalex.org/W2982515679",
    "https://openalex.org/W2963165299",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3025269113",
    "https://openalex.org/W2967927722",
    "https://openalex.org/W2479423890",
    "https://openalex.org/W3044511083",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W2964072591",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2939208918",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3002557610",
    "https://openalex.org/W2806331055",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3154807520",
    "https://openalex.org/W3035392611",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2810643877",
    "https://openalex.org/W2963175980",
    "https://openalex.org/W3015246548",
    "https://openalex.org/W2964218959",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2768661419",
    "https://openalex.org/W2963532541",
    "https://openalex.org/W2077069816",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W4288325606",
    "https://openalex.org/W4297783004",
    "https://openalex.org/W2139501017",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2956354060",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2964213933",
    "https://openalex.org/W2911400095"
  ],
  "abstract": "Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a semantics-controlled multi-modal shuffled Transformer reasoning framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an intra-frame reasoning layer producing spatio-semantic graph representations for every frame, and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics.",
  "full_text": "Dynamic Graph Representation Learning for Video Dialog\nvia Multi-Modal Shufﬂed Transformers\nShijie Geng1, Peng Gao2, Moitreya Chatterjee3, Chiori Hori4,\nJonathan Le Roux4, Yongfeng Zhang1, Hongsheng Li2, Anoop Cherian4\n1Rutgers University, Piscataway, NJ, USA\n2The Chinese University of Hong Kong, Hong Kong, China\n3University of Illinois at Urbana Champaign, Urbana, IL, USA\n4Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA\nAbstract\nGiven an input video, its associated audio, and a brief cap-\ntion, the audio-visual scene aware dialog (A VSD) task re-\nquires an agent to indulge in a question-answer dialog with a\nhuman about the audio-visual content. This task thus poses a\nchallenging multi-modal representation learning and reason-\ning scenario, advancements into which could inﬂuence sev-\neral human-machine interaction applications. To solve this\ntask, we introduce a semantics-controlled multi-modal shuf-\nﬂed Transformer reasoning framework, consisting of a se-\nquence of Transformer modules, each taking a modality as\ninput and producing representations conditioned on the in-\nput question. Our proposed Transformer variant uses a shuf-\nﬂing scheme on their multi-head outputs, demonstrating bet-\nter regularization. To encode ﬁne-grained visual informa-\ntion, we present a novel dynamic scene graph representation\nlearning pipeline that consists of an intra-frame reasoning\nlayer producing spatio-semantic graph representations for ev-\nery frame, and an inter-frame aggregation module capturing\ntemporal cues. Our entire pipeline is trained end-to-end. We\npresent experiments on the benchmark A VSD dataset, both\non answer generation and selection tasks. Our results demon-\nstrate state-of-the-art performances on all evaluation metrics.\nIntroduction\nThe success of deep learning in producing effective so-\nlutions to several fundamental problems in computer vi-\nsion, natural language processing, and speech/audio under-\nstanding has provided an impetus to explore more com-\nplex multi-modal problems at the intersections of these do-\nmains, attracting wide interest recently (Zhu et al. 2020).\nA few notable ones include: (i) visual question answering\n(VQA) (Antol et al. 2015; Yang et al. 2003), the goal of\nwhich is to build an agent that can generate correct an-\nswers to free-form questions about visual content, (ii) au-\ndio/visual captioning (Hori et al. 2017; Venugopalan et al.\n2015; Xu et al. 2015; Drossos, Lipping, and Virtanen 2019),\nin which the agent needs to generate a sentence in natural\nlanguage describing the audio/visual content, (iii) visual di-\nalog (Das et al. 2017), in which the agent needs to engage\nin a natural conversation with a human about a static image,\nand (iv) audio-visual scene-aware dialog (A VSD) (Alamri\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n… …\nCaption: “A man walks into the room carrying a folder, that he throws on a pile of clothes. He then picks up a vacuum, turns it on and vacuums. Then, shuts it off, and sneezes four times.”\nQ1: “Is the machine vacuum cleaner?” A1: “Yes, the machine on the floor is a vacuum.”\n“It looks like a laundry room”\nQuestion\nOur generated answer\nInput Video and its Audio\nQ2: “What room do you think it is? “A2: __UNDISCLOSED__\nDialog History\nFigure 1: A result from our proposed model for the A VSD\ntask. Given a video clip, its caption, dialog history, and a\nquestion, the A VSD generation task aims to generate the an-\nswer in natural language form.\net al. 2019; Hori et al. 2019) – that generalizes (i), (ii), and\n(iii) – in which the agent needs to produce a natural an-\nswer to a question about a given audio-visual clip, in a con-\nversation setting or select the correct answer from a set of\ncandidates. The A VSD task1 emulates a real-world human-\nmachine conversation setting that is potentially useful in a\nvariety of practical applications, such as building virtual as-\nsistants (Deruyttere et al. 2019) or in human-robot interac-\ntions (Thomason et al. 2019). See Figure 1 for an illustration\nof this task.\nThe generality of the A VSD task, however, poses a chal-\nlenging multi-modal representation learning and reasoning\nproblem. Speciﬁcally, some of the input modalities to this\ntask may offer complementary information (such as video\nand audio), while a few others may be independent (au-\ndio and captions), or even conﬂict with each other, e.g.,\nthe provided text (captions/dialogs) may include details\nfrom human experience that are absent in the video (e.g.,\n“I think...”), or may include abstract responses (“happy”,\n“bored”, etc.) that may be subjective. Thus, the main quest\nin this task is to represent these modalities such that in-\nference on them is efﬁcient and effective. Previous ap-\nproaches to this problem used holistic video features pro-\nduced by a generic 3D convolutional neural network (Car-\nreira and Zisserman 2017), and either focused on extend-\n1https://video-dialog.com/\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAA I-21)\n1415\ning attention models on these features to include additional\nmodalities (Alamri et al. 2019; Hori et al. 2019; Schwartz,\nSchwing, and Hazan 2019), or use vanilla Transformer net-\nworks (Vaswani et al. 2017) to produce effective multi-\nmodal embeddings (Le et al. 2019). These off-the-shelf vi-\nsual representations or Transformer architectures are not at-\ntuned to the task, potentially leading to sub-optimal perfor-\nmance.\nIn this paper, we present a neural inference algo-\nrithm that hierarchically reduces the complexity of the\nA VSD task using the machinery of graph neural net-\nworks and sequential multi-head Transformers. Speciﬁ-\ncally, we ﬁrst present a spatio-temoral scene graph rep-\nresentation (STSGR) for encoding the video compactly\nwhile capturing its semantics. Speciﬁcally, our scheme\nbuilds on visual scene graphs (Johnson et al. 2015) to-\nwards video representation learning by introducing two\nnovel modules: (i) an intra-frame reasoning module that\ncombines graph-attention (Veliˇckovi´c et al. 2018) and edge-\nconvolutions (Wang et al. 2019) to produce a semantic\nvisual representation for every frame, (ii) subsequently,\nan inter-frame aggregation module uses these representa-\ntions and updates them using information from temporal-\nneighborhoods, thereby producing compact spatio-temporal\nvisual memories. We then couple these memories with tem-\nporally aligned audio features. Next, multi-head Transform-\ners (Vaswani et al. 2017), encodes each of the other data\nmodalities (dialog history, captions, and the pertinent ques-\ntion) separately alongside these audio-visual memories and\nfuses them sequentially using Transformer decoders. These\nfused features are then used to select or generate theanswers\nauto-regressively. We also present a novel extension of the\nstandard multi-head Transformer network in which the out-\nputs of the heads are mixed. We call this variant a shufﬂed\nTransformer. Such random shufﬂing avoids overﬁtting of the\nheads to its inputs, thereby regularizing them, leading to bet-\nter generalization.\nTo empirically evaluate our architecture, we present ex-\nperiments on two variants of the A VSD dataset available\nas part of the 7th and 8th Dialog System Technology Chal-\nlenges (DSTC). We provide experiments on both the answer\ngeneration and the answer selection tasks – the former re-\nquiring the algorithm to produce free-form sentences as an-\nswers, while the latter selects an answer from 100 choices\nfor each question. Our results reveal that using the proposed\nSTSGR and our shufﬂed Transformer lead to signiﬁcant im-\nprovements on both tasks against state-of-the-art methods on\nall metrics. The key contributions of this paper are:\n• We propose to represent videos as spatio-temporal scene\ngraphs capturing key audio-visual cues and semantic\nstructure. To the best of our knowledge, the combination\nof our intra/inter-frame reasoning modules is novel.\n• We introduce a sequential Transformer architecture that\nuses shufﬂed multi-head attention, yielding question-\naware representations of each modality while generating\nanswers (or their embeddings) auto-regressively.\n• Extensive experiments on the A VSD answer generation\nand selection tasks demonstrate the superiority of our ap-\nproach over several challenging recent baselines.\nRelated Work\nOur proposed framework has similarities with prior works\nalong three different axes, viz. (i) graph-based reasoning, (ii)\nmulti-modal attention, and (iii) visual dialog methods.\nScene Graphs: (Johnson et al. 2015) combine objects de-\ntected in static images, their attributes, and object-object re-\nlationships (Lu et al. 2016) to form a directed graph that\nnot only provides an explicit and interpretable represen-\ntation of the image, but is also seen to be beneﬁcial for\nhigher-order reasoning tasks such as image captioning (Li\nand Jiang 2019; Yang et al. 2019), and visual question an-\nswering (Ghosh et al. 2019; Norcliffe-Brown, Vafeias, and\nParisot 2018; Geng et al. 2019, 2020). There have been ef-\nforts (Wang et al. 2018; Girdhar et al. 2019; Jain et al. 2016;\nHerzig et al. 2019; Jang et al. 2017; Tsai et al. 2019) at\ncapturing spatio-temporal evolution of localized visual cues.\nIn (Wang and Gupta 2018), a space-time graph reasoning\nframework is proposed for action recognition. Similarly, the\nefﬁcacy of manually-labeled video scene graphs is explored\nin (Ji et al. 2020). Similar to ours, they use object detec-\ntions per video frame, and construct a spatio-temporal graph\nbased on the afﬁnities of the features from the detected ob-\njects. Spatio-temporal graphs using knowledge distillation\nis explored for video captioning in (Pan et al. 2020). In con-\ntrast, our task involves several diverse modalities, demand-\ning richer architectural choices. Speciﬁcally, we present a\nhierarchically organized intra/inter-frame reasoning pipeline\nfor generating visual memories, trained via neural message\npassing, offering a powerful inference engine. Our ablation\nstudies demonstrate the usefulness of these modules.\nMulti-modal Fusion/Attention: has been explored in sev-\neral prior works (Hori et al. 2017, 2018, 2019; Shi et al.\n2020a), however does not use the power of Transformers.\nSelf-attention and feature embeddings using Transformers\nis attempted in multi-modal settings (Gao et al. 2019a,b; Shi\net al. 2020b), however only on static images. Bilinear fusion\nmethods (Ben-Younes et al. 2019; Fukui et al. 2016) have\nbeen explored towards inter-modality semantic alignment,\nhowever they often result in high-dimensional interaction\ntensors that are computationally expensive during inference.\nIn contrast, our pipeline is the ﬁrst to leverage the power\nof Transformers in a hierarchical graph reasoning setup for\nvideo dialogs and is cheap to compute.\nMulti-modal Dialogs: have been explored in various ways\nbefore. Free-form human-like answers were ﬁrst consid-\nered in (Das et al. 2017), which also proposed the VisDial\ndataset, however on static images. A difﬁculty in designing\nalgorithms on multi-modal data is in deriving effective at-\ntention mechanisms that can divulge information from dis-\nparate cues. To tackle this challenge, (Wu et al. 2018) pro-\nposed a sequential co-attention scheme in which the neural\nembeddings of various modalities are co-attended with vi-\nsual embeddings in a speciﬁc order. (Schwartz et al. 2019)\ngeneralized the co-attention problem by treating the modal-\nities as nodes of a graph, aggregating them as factors, using\nneural message passing. We use a combination of these two\n1416\nFigure 2: A schematic illustration of our overall pipeline for dialog response generation/retrieval.\napproaches; speciﬁcally we use Transformer encoders for\nembedding each modality, and attend on these multi-modal\nembeddings sequentially to generate the answer. Further, in\ncontrast to (Schwartz et al. 2019; Wu et al. 2018), that tackle\nsolely the answer generation problem, we consider the an-\nswer selection task on A VSD as well. (Yeh et al. 2019) also\nproposed using Transformers (Vaswani et al. 2017) for fus-\ning audio-visual features on the A VSD task. A multi-step\nreasoning scheme is proposed in (Gan et al. 2019) using\njoint attention via an RNN for generating a multi-modal rep-\nresentation. The Simple baseline (Schwartz, Schwing, and\nHazan 2019) extends factor graphs (Schwartz et al. 2019)\nfor the A VSD problem demonstrating promising results. A\nmulti-modal Transformer for embedding various modalities\nand a query-aware attention is introduced in (Le et al. 2019).\n(Le and Hoi 2020) ﬁne-tunes pretrained GPT-2 to obtain im-\nproved performance. However, these works neither consider\nricher visual representations using scene graphs, nor varia-\ntions of the standard Transformers, like the shufﬂing scheme\nwe present.\nProposed Method\nIn this section, we will ﬁrst present our spatio-temporal\nscene graph representation (STSGR) for encoding the video\nsequences, following which we elaborate on our multi-\nmodal shufﬂed Transformer architecture.\nOverview of Spatio-Temporal Scene Graphs\nGiven a video sequence V, let C denote the associated\nhuman-generated video caption, and let (Qi;Ai) repre-\nsent the tuple of the text-based i-th question and its an-\nswer in the given human dialog about V (see Fig. 1).\nWe will succinctly represent the dialog history by H =\nh(Q1;A1);:::; (Ql\u00001;Al\u00001)i. Further, let Ql represent the\nquestion under consideration. The audio-visual scene-aware\ndialog (A VSD) task requires the generation (or selection) of\nthe answer denoted byAl, corresponding to the questionQl.\nOur proposed pipeline to solve this task is schemati-\ncally illustrated in Fig. 2. It consists of four components:\n(1) a scene graph construction module, which extracts ob-\njects and relation proposals from the video using pretrained\nneural network models, building a scene graph for every\n(temporally-sampled) video frame, (2) an intra-frame rea-\nsoning module, which conducts node-level and edge-level\ngraph reasoning, producing compact feature representations\nfor each scene graph, (3) an inter-frame information aggre-\ngation module, that aggregates these features within a tem-\nporal sliding window to produce a visual memory for each\nframe’s scene graph (at the center frame in that window),\nand (4) a semantics-controlled Transformer reasoning mod-\nule, which performs multi-modal reasoning and language\nmodelling based on a semantic controller. In this module, we\nalso use a newly-proposed shufﬂe-augmented co-attention to\nenable head interactions in order to boost performance. Be-\nlow, we describe in detail each of these modules.\nScene Graph Representation of Video\nOur approach to generate scene graphs for the video frames\nis loosely similar to the ones adopted in recent works such\nas (Pan et al. 2020; Herzig et al. 2019; Wang and Gupta\n2018), and has three components: (a) object detection, (b)\nvisual-relation detection, and (c) region of interest (ROI) re-\ncrop on union bounding boxes. For (a), we train a Faster\nR-CNN model (Ren et al. 2015) on the Visual Genome\ndataset (Krishna et al. 2017) using the MMDetection repos-\nitory (Chen et al. 2019). For a video frame I, this Faster-\nRCNN model produces: FI;BI;SI = RCNN( I), where\nFI 2RNo\u0002do denotes the do-dimensional object features,\nBI 2RNo\u00024 are the object bounding boxes, and SI is a list\nof semantic labels associated with each bounding box. The\npair (FI;SI) forms the nodes of our scene graph. Next, to\nﬁnd the graph edges, we train a relation model on the VG200\ndataset (Krishna et al. 2017), which contains 150 objects and\n50 predicates, and apply this learned model on the frames\nfrom the given video. The output of this model is a set of\n1417\nhS;P;O itriplets per frame, where S, P, and O represent\nthe subject, predicate, and object, respectively. We keep the\nhS;Oipairs as relation proposals and discard the original\npredicate semantics, as the relation predicates of the model\ntrained on VG200 are limited and ﬁxed. Instead, we let our\nreasoning model learn implicit relation semantics during our\nend-to-end training. For the detectedhS;Oipairs, we regard\nthe union box of the bounding boxes for S and O as the\npredicate region of interest. Next, we apply the ROI-align\noperator (Ren et al. 2015) on the last layer of the backbone\nnetwork using this union box and make the resultant feature\nan extra node in the scene graph.\nIntra-Frame Reasoning\nRepresenting videos directly as sequences of scene graphs\nleads to a complex graph reasoning problem that can be\ncomputationally challenging. To avoid this issue, we pro-\npose to hierarchically reduce this complexity by embedding\nthese graphs into learned representation spaces. Speciﬁcally,\nwe propose an intra-frame reasoning scheme that bifurcates\na scene graph into two streams: (i) a visual scene graph that\ngenerates a representation summarizing the visual cues cap-\ntured in the graph nodes, and (ii) a semantic scene graph\nthat summarizes the graph edges. Formally, let us deﬁne a\nscene graph as G= f(xi;eij;xj) jxi;xj 2V ;eij 2Eg ,\nwhere Vdenotes the set of nodes consisting of single ob-\njects and Eis the set of edges consisting of relations linking\ntwo objects. The triplet (xi;eij;xj) indicates that the sub-\nject node xi and the object node xj are connected by the di-\nrected relation edge eij. We denote by Gv and Gs the visual\nscene graph and the semantic scene graph respectively: the\nformer is a graph attention network (Veliˇckovi´c et al. 2018)\nwhich computes an attention coefﬁcient for each edge and\nupdates node features based on these coefﬁcients; the latter\nis based on EdgeConv (Wang et al. 2019), which computes\nextra edge features based on node features and then updates\nthe node features by aggregating the edge features linked to\na given node. Both networks are explained in detail next. We\ncombine these two complementary graph neural networks in\na cascade to conduct intra-frame reasoning.\nVisual Scene Graph Reasoning: For M node features\nX = fx1;x2;:::; xMgin a scene graph, multi-head self-\nattention (Vaswani et al. 2017) is ﬁrst performed for each\npair of linked nodes. In each head k, for two linked nodes\nxi and xj, the attention coefﬁcient \u000bk\nij indicating the impor-\ntance of node jto node iis computed by\n\u000bk\nij = exp\n\u0000\n\u001b\n\u0000\n\u0002>\nk[Wk\n1 xi kWk\n1 xj]\n\u0001\u0001\nP\nk2Ni\nexp\n\u0000\n\u001b\n\u0000\n\u0002>\nk[Wk\n1 xi kWk\n1 xk]\n\u0001\u0001; (1)\nwhere kdenotes feature concatenation, \u001b is a nonlinearity\n(Leaky ReLU), Ni indicates the neighboring graph nodes of\nobject i(including i), Wk\n1 2Rdh\u0002din is a (learned) weight\nmatrix transforming the original features to a shared latent\nspace, and \u0002k 2 R2dh is the (learned) attention weight\nvector. Using the attention weights \u000bk and a set of learned\nweight matrices Wk\n2 2Rdh=K\u0002din , we update the node fea-\ntures as:\nx0\ni =\n\r\rK\nk=1\u001b\n\u0010X\nj2Ni\n\u000bk\nijWk\n2 xj\n\u0011\n: (2)\nOutputs of the K heads are concatenated to produce x0\ni 2\nRdh , which is used as input to the semantic graph network.\nSemantic Scene Graph Reasoning: This sub-module cap-\ntures higher-order semantics between nodes in the scene\ngraph. To this end, EdgeConv (Wang et al. 2019), which is\na multi-layer fully-connected network h\u0003, is employed to\ngenerate edge features eij from its two connected node fea-\ntures (x0\ni;x0\nj): eij = h\u0003(x0\ni;x0\nj), where h\u0003 : Rdh \u0002Rdh !\nRdh is a nonlinear transformation with learnable parameters\n\u0003. We then obtain the output node features x?\ni by aggregat-\ning features from the edges that are directed to the object\nnode i, i.e.,\nx?\ni = max\nj:(j;i)2Ei\neji; (3)\nwhere Ei denotes the set of edges directed to node i. All ob-\nject features inside the scene graph are updated by the above\nintra-frame feature aggregation.\nMemory Generation with Graph Pooling: After conduct-\ning intra-frame reasoning to obtain higher-level features for\neach node, we pool the updated graph into a memory for\nfurther temporal aggregation. Since different frame-level\nscene graphs have different numbers of nodes and edges, we\nadopt graph average pooling (GAP) and graph max pooling\n(GMP) (Lee, Lee, and Kang 2019) to generate two graph\nmemories and concatenate them to produce V?:\nV? = GAP(X?;E) kGMP(X?;E); (4)\nwhere Edenotes the scene graph connection structure, and\nX? the M node features fx?\n1;x?\n2;:::; x?\nMgfrom (3).\nInter-Frame Information Aggregation\nApart from the spatial graph representations described\nabove, there is a temporal continuity of visual cues in the\nvideo frames that needs to be captured as well. To this end,\nwe propose an inter-frame aggregation scheme that operates\non the spatial graph embeddings. Speciﬁcally, for a sequence\nof scene graph memories hv?\n1 ;v?\n2 ;:::;v ?\nLiof length Lpro-\nduced using (4) on a sequence of Lframes, we use temporal\nsliding windows of size\u001c to update the graph memory of the\ncenter frame in each window by aggregating the graph mem-\nories of its neighboring frames in that window, both in the\npast and in the future. Let F 2R2dh\u0002\u001c denotes a matrix of\ngraph embeddings within this window of length \u001c, then we\nperform window-level summarization over all frame mem-\nories within F as: \f = softmax(\u0000 >tanh(W\u001cF)), where\nW\u001c 2R2dh\u00022dh is a learned weight matrix, \u0000 2R2dh is a\nweight vector, and \fdenotes the attention weights. We then\nuse \f to update the memory vc of the center frame (in this\nwindow) by aggregating information across this window, as:\nv0\nc = F\f>. Repeating this step for all sliding windows, we\nget the ﬁnal visual graph memory V0 = hv0\n1;v0\n2;:::;v 0\nLi\naggregating both spatial and temporal information. We also\naugment these visual features with their temporally-aligned\naudio embeddings hs1;s2;\u0001\u0001\u0001 ;sLiproduced using an Au-\ndioSet VGGish network (Hershey et al. 2017).\nSemantics-Controlled Transformer Reasoning\nThe above modules encode a video into a sequence of\ngraph memories via reasoning on visual and semantic scene\n1418\nFigure 3: depicts the sequential attention ﬂow in our\nsemantics-controlled Transformer. MHA stands for multi-\nhead attention. FFN is short for feed-forward networks. The\nacronyms A, V , H, and Q stand for the answers, visual mem-\nory, caption/dialog history, and the question, respectively.\ngraphs. Besides encoding audio-visual information, we also\nneed to encode the text information available in the A VSD\ntask. For the sentence generation task, we propose to gen-\nerate the answer autoregressively (Anderson et al. 2018;\nHori et al. 2018), i.e., predict the next word in the answer\nfrom the vocabulary based on source sequences including\nthe visual memory, query Ql, caption C, the dialog history\nH = h(Q1;A1);:::; (Ql\u00001;Al\u00001)i, and the partially gener-\nated answer so far, denoted Ain\nl (see Fig. 2 and Fig. 3). This\nsub-answer Ain\nl forms the semantics that control the atten-\ntion on the various modalities to generate the next word. As\nshown in Fig. 3, our semantics-controlled Transformer mod-\nule consists of a graph encoder, a text encoder, and a multi-\nmodal decoder. It takes in source sequences and outputs the\nprobability distribution of the next token for all tokens in the\nvocabulary. We detail the steps in this module next.\nEncoder: We ﬁrst use Transformer to embed all text sources\n(H, C, Ql, Ain\nl ) using token and positional embeddings, gen-\nerating feature matrices eh;ec;eq, and ea, each of the same\nfeature dimensionality dh. We also use a single-layer fully-\nconnected network to transfer the audio-augmented visual\nmemories in V0 to dh-dimensional features ev that match\nthe dimension of the text sources. Next, for the answer gen-\neration task, the input sub-answer (generated so far) ea is\nencoded with a Transformer consisting of multi-head self-\nattention to get hidden representations ha\nenc:\nha\nenc = FFNa(Attention(Wa\nQea;Wa\nKea;Wa\nVea)); (5)\nwhere Wa\nQ, Wa\nK, Wa\nV are weight matrices for query, key,\nand value respectively (Vaswani et al. 2017), FFNa is a\nfeed-forward module consisting of two fully-connected lay-\ners with ReLU in between. The Attention function is de-\nﬁned as in (Vaswani et al. 2017):\nAttention(Q; K; V) = softmax(QK>\npdh\n)V; (6)\nwith a scaling factor pdh that maintains the order of magni-\ntude in features, and Q;K;V represent the query, key, and\nvalue triplets as described in (Vaswani et al. 2017). After\nencoding the input sub-answer, we conduct co-attention in\nturn for each of the other text and visual embeddings ej,\nFigure 4: An illustration of our multi-head shufﬂed Trans-\nformer, where we shufﬂe the output of each head before\npassing it on to the FFN module.\nwhere j 2fv;c;h;q g, with a similar Transformer architec-\nture. That is, the encoding hj\nenc for a given embedding type\nej is obtained by using the encoding hj0\nenc for the previous\nembedding type ej0 as query (Fig. 3):\nhj\nenc = FFNj(Attention(Wj\nQhj0\nenc;Wj\nKej;Wj\nVej)): (7)\nIn our implementation, the embeddings for history and cap-\ntion are concatenated as ec+h = ecjjeh. Processing occurs\nin the following order: starting from ha\nenc, we compute hv\nenc,\nthen hc+h\nenc , and laterhq\nenc. Finally, we get a feature vectorh?\nenc\nthat fuses all the information from the text and audio-visual\nsources by concatenating these multi-modal features.\nMulti-head Shufﬂed Transformer: In this paper, we also\npropose to utilize head shufﬂing to further improve the per-\nformance of the Transformer structure as shown in Fig. 4. In\nthe original Transformer (Vaswani et al. 2017), the feature\nvectors of all heads are directly concatenated before being\nfed into the last fully-connected layer. Thus, there is no in-\nteraction between those heads from the start to the end. To\nenable the interactions across heads, we propose to divide\neach head and shufﬂe all head vectors before passing them\non to separate fully-connected layers. The outputs are ﬁnally\nconcatenated in a late fusion style. This scheme is similar to\nShufﬂeNet (Zhang et al. 2018), with the key difference that\nhere we conduct shufﬂing between different heads within\nthe multi-head attention, while in ShufﬂeNet the shufﬂing is\nacross channels. Our empirical results show that our shuf-\nﬂing operation results in better generalization of the model.\nDecoder: For the generation setting, with the ﬁnal encoded\nfeature h?\nenc, we use a feed-forward network with softmax\nto predict the next token probability distribution P over all\ntokens in the vocabulary V; i.e., P = softmax(FFN(h?\nenc)).\nIn the testing stage, we conduct beam search with bbeams\nto generate an answer sentence.\nLoss Function: Let Pdenote the collection of all next-token\nprobability distributions Pj 2RjVj, j = 1;:::;N for batch\nsize N, and let Gbe the collection of respective distribu-\ntions Gj for the ground truth answer tokens. For the gener-\nation setting, we apply label smoothing (M ¨uller, Kornblith,\nand Hinton 2019) to account for the sparsity of the token\ndistributions, leading to ~G. We use the cross-entropy ( CE)\nloss between the predicted and the smoothed ground truth\n1419\ndistributions to train our model end-to-end:\nL= CE(Pj~G) = \u00001\nN\nNX\nj=1\nX\nu2V\n~Gj(u) logPj(u): (8)\nFor the retrieval setting, we ﬁrst concatenate the feature\nembeddings of the query and the various input modali-\nties obtained from the Encoder module of our network\n(eh;ec;eq;ev). Next, the candidate answers are embedded\ninto this joint space using LSTMs, and a dot product is taken\nbetween the concatenated inputs and embeddings of each of\nthe answer candidates. We then train this model with the bi-\nnary cross-entropy loss.\nExperiments\nIn this section, we detail our experimental setup, datasets,\nand evaluation protocols, before furnishing our results.\nDataset and Evaluation: We use the audio-visual scene-\naware dialog (A VSD) dataset (Alamri et al. 2019) for our ex-\nperiments, which is the benchmark dataset for this task. This\ndataset emulates a real-world human-human natural conver-\nsation scenario about an audio-visual clip. See (Alamri et al.\n2019) for details of this task and the dataset. We evaluate\non two variants of this dataset corresponding to annotations\navailable for the DSTC-7 and DSTC-8 challenges,2 consist-\ning of 7,659, 1,787, 1,710, and 1,710 dialogs for training,\nvalidation, DSTC-7 testing, and DSTC-8 testing, respec-\ntively for the answer generation task. The quality of the gen-\nerated answers is evaluated using the standard MS COCO\nevaluation metrics (Chen et al. 2015), such as BLEU, ME-\nTEOR, ROUGE-L, and CIDEr. Apart from the answer gen-\neration task (Hori et al. 2018), we also report experiments on\nthe answer selection task, described in (Alamri et al. 2019)\nusing their annotations and ground truth answers. This task\nrequires selecting the answer to a question from a set of 100\nanswers. Speciﬁcally, in this task, an algorithm is to present\na ranking over a set of 100 provided answers, with ideally\nthe correct answer ranked as the ﬁrst. The evaluation is then\nbased on the mean retrieval rank over the test set.\nData Processing: We follow (Le et al. 2019) to perform text\npreprocessing which include lowercasing, tokenization, and\nbuilding a vocabulary by only selecting tokens that occur at\nleast ﬁve times. Thus, we use a vocabulary with 3,254 words,\nboth for the generation and retrieval tasks.\nFeature Extraction: Motivated by (Anderson et al. 2018),\nwe train a detector on Visual Genome with 1601 classes\nand 401 attributes, which incorporates a “background” la-\nbel and a “no-attribute” label. We use ResNext-101 as the\nneural backbone with a multiscale feature pyramid network.\nWe further use ﬁne-grained ROI-alignment instead of ROI-\npooling for better feature representation. We extract the\n1024-D features for the 36 highest scoring regions, their\nclass labels, and attributes. After extracting the region fea-\ntures, we apply a pretrained relationship detector (Zhang\net al. 2019) to ﬁnd visually-related regions. We calculate the\nminimal bounding box which covers two visually-related re-\ngions and perform ROI-alignment to get compact represen-\ntations for relationship regions. In order to incorporate audio\n2https://sites.google.com/dstc.community/dstc8/home\nMethod B4 MET ROUGE CIDEr\nSTSGR full model 0.133 0.165 0.361 1.265\nw/o head shufﬂing 0.127 0.161 0.354 1.208\nw/o GAT 0.118 0.160 0.347 1.125\nw/o EdgeConv 0.131 0.162 0.356 1.244\nw/o union box 0.124 0.163 0.352 1.175\nw/o visual features 0.127 0.160 0.356 1.203\nw/o temporal 0.125 0.164 0.357 1.212\nSTSGR + audio 0.133 0.165 0.362 1.272\nTable 1: Ablation study using A VSD@DSTC7 dataset.\ninto the STSGR framework, we extract AudioSet VGG-ish\nfeatures (Hershey et al. 2017) from the audio stream for ev-\nery video. These are 128-D features obtained from the Au-\ndioSet VGG-ish CNN, pretrained on 0.96s Mel Spectrogram\npatches on the AudioSet data (Gemmeke et al. 2017).\nModel Training: We set our Transformer hyperparameters\nfollowing (Vaswani et al. 2017). The feature dimension is\n512, while the inner-layer dimension of the feed-forward\nnetwork is set to 2048. For multi-head attention, we maintain\nh = 8 parallel attention heads and apply shufﬂing to boost\nperformance. For the semantic labels, we build a 300-D em-\nbedding layer for the 1651 words in the vocabulary (which\nis available with the dataset), and initialize the embeddings\nusing GloVe word vectors (Pennington, Socher, and Man-\nning 2014). For semantic labels consisting of more than one\nword, we use the average word embedding as the label em-\nbedding. Our model is trained on one Nvidia Titan XP GPU\nwith Adam optimizer (Kingma and Ba 2015) with\f1 = 0:9,\n\f2 = 0:98. The batch size is set to 16 and we adopt the\nwarm-up strategy as suggested in (Vaswani et al. 2017) for\nlearning rate adjustment with about 10,000 steps.\nBaselines: We consider the following four baselines on the\ngeneration task: (i) Baseline (Hori et al. 2019), (ii) Multi-\nmodal Attention (Hori et al. 2019), that uses attention over\nconcatenated features, (iii) Simple (Schwartz et al. 2019)\nthat uses factor-graph attention on the modalities, and (iv)\nMTN (Le et al. 2019) that applies self-attention and co-\nattention to aggregate multi-modal information. For the re-\ntrieval task, we compare our method against the state-of-the-\nart method of (Alamri et al. 2019) on the DSTC-7 split.\nAblation Study: To understand the importance of each com-\nponent in our model, Table 1 details an ablation study. We\nanalyze several key components: (i) shufﬂing in the Trans-\nformer structure, (ii) visual and semantic graph, (iii) ROI\nRecrop on the union bounding boxes, and (iv) temporal\naggregation. From the table, we see that Graph Attention\nNetwork (GAT), which is used to produce the visual scene\ngraph, is important to aggregate information from neigh-\nboring nodes (e.g., improving CIDEr from 1.125 to 1.265),\nwhile EdgeConv, used in the semantic graph, offers some\nimprovement (e.g., CIDEr from 1.244 to 1.265). Moreover,\nthe use of shufﬂing in the multi-head Transformer archi-\ntecture boosts the performance signiﬁcantly (from 1.208 to\n1.265 for CIDEr). We can also conclude that union bound-\ning boxes, semantic labels, and inter-frame aggregation con-\ntribute to stabilize the generation performance. Overall, by\nadopting all these key components, the full model outper-\n1420\ncaption:\"a girl is standing and cooking at the stovetop . she is sipping on a coffee cup at she stirs whats in the pan .\"\nquestion 1:\"is there just one person in the video ?\"\nanswer 1:\"yes there is just one person\"\nquestion 2:\"is she in the room the whole time ?\"\nanswer 2:\"__UNDISCLOSED__\"\n[Generation Task]\nQS: is she in the room the whole time ?\nGT: yes she is in that room the whole time\nHYPO-1: yes she is in the room the whole time  (Score: 6.79)\nHYPO-2: yes she is in the room the whole time the whole time  (Score: 6.63)\nHYPO-3: yes she is in the same room the whole time  (Score: 6.36)\n[Retrieval Task]\nQS: is she in the room the whole time ?\nGT: yes she is in that room the whole time\nRANK-1: yes she is in that room the whole time  (Score: 0.52)\nRANK-2: yes she is in the kitchen  (Score: 0.23)\nRANK-3: yes she stands there all time  (Score: 0.13)\nRANK-4: yes she is by herself the whole time  (Score: 0.08)\nRANK-5: no the women does not have her shoes on  (Score: 0.01)\ncaption:\"a woman is sitting on a chair inside a pantry room . \"\nquestion 1:\"is there only one person in the whole video ?\"\nanswer 1:\"yes there is only the one female .\"\nquestion 2:\"where does she pick up the can of coffee from ?\"\nanswer 2:\"__UNDISCLOSED__\"\n[Generation Task]\nQS: where does she pick up the can of coffee from ?\nGT: she picks it up from one of the shelves in the pantry .\nHYPO-1: she picks it up from the pantry .  (Score: 4.03)\nHYPO-2: she picks it up from the counter .  (Score: 2.84)\nHYPO-3: she picks it up from the shelf .  (Score: 2.83)\n[Retrieval Task]\nQS: where does she pick up the can of coffee from ?\nGT: she picks it up from one of the shelves in the pantry .\nRANK-1: it was mug , actually , already sitting on the table . (Score: 0.38)\nRANK-2: she picks it up from one of the shelves in the pantry .  (Score: 0.26)\nRANK-3: she pours water into a cup .  (Score: 0.19)\nRANK-4: she is holding it at the end of the video as well  (Score: 0.17)\nRANK-5: she is drinking from a white mug .  (Score: 0.06)\nFigure 5: Qualitative results from our model on both generation and retrieval tasks of A VSD. Left: input video frames, Top-\nright: caption and dialog history, Bottom-middle: top-3 generated answers with conﬁdence scores. Bottom-right: top-5 ranked\ncandidate answers with conﬁdence scores.\nA VSD@DSTC7\nMethod B4 MET ROUGE CIDEr\nBaseline 0.075 0.110 0.275 0.701\nMulti-modal Attention 0.078 0.113 0.277 0.727\nSimple 0.091 0.125 0.307 0.883\nMTN 0.128 0.162 0.355 1.249\nSTSGR (Ours) 0.133 0.165 0.362 1.272\nA VSD@DSTC8\nBaseline 0.289 0.210 0.480 0.651\nMulti-modal Attention 0.293 0.212 0.483 0.679\nSimple 0.311 0.224 0.502 0.766\nMTN 0.352 0.263 0.547 0.978\nSTSGR (Ours) 0.357 0.267 0.553 1.004\nTable 2: Comparisons of our method against the state of the\nart on the A VSD test splits for DSTC7 and DSTC8.\nMethod Full model w/o C w/o C, H\nAlamri et al. (2019) 5.88 N/A 7.41\nHori et al. (2019) 5.60 N/A 7.23\nMTN w/o audio 4.51 4.90 6.85\nMTN w/ audio 4.29 4.78 6.46\nSTSGR 4.33 4.67 6.54\nSTSGR w/ audio 4.08 4.55 5.91\nTable 3: State-of-the-art comparisons on answer selection as\nmeasured by Mean Retrieval Rank (lower the better).\nforms all the ablations. From Tables 1 and 3, we notice that\nincorporation of audio helps improve the performance of our\nmodel. For instance, on the retrieval setting we observe that\nincorporating audio lowers the Mean-Retrieval Rank notice-\nably down to 4.08 from 4.33 for the full model and to 5.91\nfrom 6.54 when no language context is available.\nComparisons to the State of the Art: In Table 2, we com-\npare STSGR against baseline methods on various quality\nmetrics based on ground-truth answers. As is clear, our ap-\nproach achieves better performance against all the baselines.\nThe performance on the answer selection task (mean re-\ntrieval rank) is provided in Table 3, demonstrating clearly\nstate-of-the-art results against the baseline in (Alamri et al.\n2019). We also show that including audio into the STSGR\nrepresentation helps improve the mean retrieval rank.\nQualitative Results and Discussion: In Fig. 5, we pro-\nvide two qualitative results from our STSGR model. For\nthe ﬁrst case, our model consistently detects the woman in\nthe frames and ﬁnds that she maintains many connections\nwith other objects inside the scene throughout the whole\nvideo, thus our model makes/selects the correct answer with\nhigh conﬁdence. For the second case, the clutter background\nposes a challenge to our model. However, STSGR can still\ngenerate/rank the correct answer in top-2. In general, we\nﬁnd that STSGR can answer spatial and temporal questions\nvery well. This is quantitatively evidenced by observing that\nwhile both STSGR and MTN (Le et al. 2019) use similar\nbackends, they differ in the input representations (I3D in (Le\net al. 2019) vs. scene graphs in ours), and our model outper-\nforms MTN noticeably (1.272 vs 1.249 on CIDEr, Table 2),\nsubstantiating the importance of our STSGR representation.\nConclusions\nWe presented a novel hierarchical graph representation\nlearning and Transformer reasoning framework for the prob-\nlem of audio-visual scene-aware dialog. Speciﬁcally, our\nmodel generates object, frame, and video-level representa-\ntions that are systematically integrated to produce visual\nmemories, which are sequentially fused to the encodings of\nother modalities (dialog history, etc.) conditioned on the in-\nput question using a multi-head shufﬂed Transformer. Ex-\nperiments demonstrate the beneﬁts of our framework for\nboth generation/selection tasks on the A VSD benchmark.\nAcknowledgments\nShijie Geng, Peng Gao, and Moitreya Chatterjee worked on\nthis project during MERL internships.\n1421\nReferences\nAlamri, H.; Cartillier, V .; Das, A.; Wang, J.; Cherian, A.;\nEssa, I.; Batra, D.; Marks, T. K.; Hori, C.; Anderson, P.; et al.\n2019. Audio Visual Scene-Aware Dialog. In CVPR.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn CVPR.\nAntol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.;\nLawrence Zitnick, C.; and Parikh, D. 2015. Vqa: Visual\nquestion answering. In ICCV.\nBen-Younes, H.; Cadene, R.; Thome, N.; and Cord, M.\n2019. Block: Bilinear superdiagonal fusion for visual ques-\ntion answering and visual relationship detection. In AAAI.\nCarreira, J.; and Zisserman, A. 2017. Quo vadis, action\nrecognition? A new model and the kinetics dataset. In\nCVPR.\nChen, K.; Wang, J.; Pang, J.; Cao, Y .; Xiong, Y .; Li, X.; Sun,\nS.; Feng, W.; Liu, Z.; Xu, J.; Zhang, Z.; Cheng, D.; Zhu, C.;\nCheng, T.; Zhao, Q.; Li, B.; Lu, X.; Zhu, R.; Wu, Y .; Dai,\nJ.; Wang, J.; Shi, J.; Ouyang, W.; Loy, C. C.; and Lin, D.\n2019. MMDetection: Open MMLab Detection Toolbox and\nBenchmark. arXiv:1906.07155 .\nChen, X.; Fang, H.; Lin, T.-Y .; Vedantam, R.; Gupta,\nS.; Doll ´ar, P.; and Zitnick, C. L. 2015. Microsoft\nCOCO captions: Data collection and evaluation server.\narXiv:1504.00325 .\nDas, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura,\nJ. M.; Parikh, D.; and Batra, D. 2017. Visual dialog. In\nCVPR.\nDeruyttere, T.; Vandenhende, S.; Grujicic, D.; Van Gool, L.;\nand Moens, M.-F. 2019. Talk2Car: Taking Control of Your\nSelf-Driving Car. In EMNLP-IJCNLP.\nDrossos, K.; Lipping, S.; and Virtanen, T. 2019. Clotho: An\nAudio Captioning Dataset. arXiv:1910.09387 .\nFukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.;\nand Rohrbach, M. 2016. Multimodal compact bilinear pool-\ning for visual question answering and visual grounding. In\nEMNLP.\nGan, Z.; Cheng, Y .; Kholy, A. E.; Li, L.; Liu, J.; and Gao, J.\n2019. Multi-step reasoning via recurrent dual attention for\nvisual dialog. In ACL.\nGao, P.; Jiang, Z.; You, H.; Lu, P.; Hoi, S. C. H.; Wang, X.;\nand Li, H. 2019a. Dynamic Fusion With Intra- and Inter-\nModality Attention Flow for Visual Question Answering. In\nCVPR.\nGao, P.; You, H.; Zhang, Z.; Wang, X.; and Li, H. 2019b.\nMulti-Modality Latent Interaction Network for Visual Ques-\ntion Answering. In ICCV.\nGemmeke, J. F.; Ellis, D. P.; Freedman, D.; Jansen, A.;\nLawrence, W.; Moore, R. C.; Plakal, M.; and Ritter, M.\n2017. Audio set: An ontology and human-labeled dataset\nfor audio events. In ICASSP.\nGeng, S.; Zhang, J.; Fu, Z.; Gao, P.; Zhang, H.; and de Melo,\nG. 2020. Character Matters: Video Story Understanding\nwith Character-Aware Relations. arXiv:2005.08646 .\nGeng, S.; Zhang, J.; Zhang, H.; Elgammal, A.; and Metaxas,\nD. N. 2019. 2nd Place Solution to the GQA Challenge 2019.\narXiv:1907.06794 .\nGhosh, S.; Burachas, G.; Ray, A.; and Ziskind, A. 2019.\nGenerating natural language explanations for visual ques-\ntion answering using scene graphs and visual attention.\narXiv:1902.05715 .\nGirdhar, R.; Carreira, J.; Doersch, C.; and Zisserman, A.\n2019. Video action transformer network. In CVPR.\nHershey, S.; Chaudhuri, S.; Ellis, D. P.; Gemmeke, J. F.;\nJansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous,\nR. A.; Seybold, B.; et al. 2017. CNN architectures for large-\nscale audio classiﬁcation. In ICASSP.\nHerzig, R.; Levi, E.; Xu, H.; Gao, H.; Brosh, E.; Wang, X.;\nGloberson, A.; and Darrell, T. 2019. Spatio-temporal action\ngraph networks. In ICCV Workshops.\nHori, C.; Alamri, H.; Wang, J.; Wichern, G.; Hori, T.;\nCherian, A.; Marks, T. K.; Cartillier, V .; Lopes, R. G.; Das,\nA.; et al. 2019. End-to-end audio visual scene-aware di-\nalog using multimodal attention-based video features. In\nICASSP.\nHori, C.; Hori, T.; Lee, T.-Y .; Zhang, Z.; Harsham, B.; Her-\nshey, J. R.; Marks, T. K.; and Sumi, K. 2017. Attention-\nbased multimodal fusion for video description. In ICCV.\nHori, C.; Hori, T.; Wichern, G.; Wang, J.; Lee, T.-y.;\nCherian, A.; and Marks, T. K. 2018. Multimodal Attention\nfor Fusion of Audio and Spatiotemporal Features for Video\nDescription. In CVPR Workshops.\nJain, A.; Zamir, A. R.; Savarese, S.; and Saxena, A. 2016.\nStructural-rnn: Deep learning on spatio-temporal graphs. In\nCVPR.\nJang, Y .; Song, Y .; Yu, Y .; Kim, Y .; and Kim, G. 2017. TGIF-\nQA: Toward spatio-temporal reasoning in visual question\nanswering. In CVPR.\nJi, J.; Krishna, R.; Fei-Fei, L.; and Niebles, J. C. 2020. Ac-\ntion Genome: Actions as Composition of Spatio-temporal\nScene Graphs. In CVPR.\nJohnson, J.; Krishna, R.; Stark, M.; Li, L.-J.; Shamma, D.;\nBernstein, M.; and Fei-Fei, L. 2015. Image retrieval using\nscene graphs. In CVPR.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. In ICLR.\nKrishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.;\nKravitz, J.; Chen, S.; Kalantidis, Y .; Li, L.-J.; Shamma,\nD. A.; et al. 2017. Visual Genome: Connecting language and\nvision using crowdsourced dense image annotations. IJCV .\nLe, H.; and Hoi, S. C. 2020. Video-Grounded Dialogues\nwith Pretrained Generation Language Models. In ACL.\nLe, H.; Sahoo, D.; Chen, N. F.; and Hoi, S. C. 2019.\nMultimodal Transformer Networks for End-to-End Video-\nGrounded Dialogue Systems. In ACL.\n1422\nLee, J.; Lee, I.; and Kang, J. 2019. Self-Attention Graph\nPooling. In ICML.\nLi, X.; and Jiang, S. 2019. Know more say less: Image cap-\ntioning based on scene graphs. IEEE Transactions on Mul-\ntimedia .\nLu, C.; Krishna, R.; Bernstein, M.; and Fei-Fei, L. 2016. Vi-\nsual Relationship Detection with Language Priors. InECCV.\nM¨uller, R.; Kornblith, S.; and Hinton, G. E. 2019. When\ndoes label smoothing help? In NeurIPS.\nNorcliffe-Brown, W.; Vafeias, S.; and Parisot, S. 2018.\nLearning conditioned graph structures for interpretable vi-\nsual question answering. In NeurIPS.\nPan, B.; Cai, H.; Huang, D.-A.; Lee, K.-H.; Gaidon, A.;\nAdeli, E.; and Niebles, J. C. 2020. Spatio-Temporal Graph\nfor Video Captioning with Knowledge Distillation. In\nCVPR.\nPennington, J.; Socher, R.; and Manning, C. 2014. GloVe:\nGlobal vectors for word representation. In EMNLP.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster R-\nCNN: Towards real-time object detection with region pro-\nposal networks. In NIPS.\nSchwartz, I.; Schwing, A. G.; and Hazan, T. 2019. A Simple\nBaseline for Audio-Visual Scene-Aware Dialog. In CVPR.\nSchwartz, I.; Yu, S.; Hazan, T.; and Schwing, A. G. 2019.\nFactor Graph Attention. In CVPR.\nShi, L.; Geng, S.; Shuang, K.; Hori, C.; Liu, S.; Gao, P.;\nand Su, S. 2020a. Multi-Layer Content Interaction Through\nQuaternion Product For Visual Question Answering. In\nICASSP.\nShi, L.; Shuang, K.; Geng, S.; Su, P.; Jiang, Z.; Gao, P.;\nFu, Z.; de Melo, G.; and Su, S. 2020b. Contrastive Visual-\nLinguistic Pretraining. arXiv:2007.13135 .\nThomason, J.; Padmakumar, A.; Sinapov, J.; Walker, N.;\nJiang, Y .; Yedidsion, H.; Hart, J.; Stone, P.; and Mooney,\nR. J. 2019. Improving grounded natural language under-\nstanding through human-robot dialog. In ICRA.\nTsai, Y .-H. H.; Divvala, S.; Morency, L.-P.; Salakhutdinov,\nR.; and Farhadi, A. 2019. Video relationship reasoning using\ngated spatio-temporal energy graph. In CVPR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2018. Graph attention networks. InICLR.\nVenugopalan, S.; Rohrbach, M.; Donahue, J.; Mooney, R.;\nDarrell, T.; and Saenko, K. 2015. Sequence to sequence-\nvideo to text. In ICCV.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In CVPR.\nWang, X.; and Gupta, A. 2018. Videos as Space-Time Re-\ngion Graphs. In ECCV.\nWang, Y .; Sun, Y .; Liu, Z.; Sarma, S. E.; Bronstein, M. M.;\nand Solomon, J. M. 2019. Dynamic graph cnn for learning\non point clouds. ACM Transactions on Graphics (TOG).\nWu, Q.; Wang, P.; Shen, C.; Reid, I.; and Van Den Hengel,\nA. 2018. Are you talking to me? reasoned visual dialog gen-\neration through adversarial learning. In CVPR.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y . 2015. Show, attend and\ntell: Neural image caption generation with visual attention.\nIn ICML.\nYang, H.; Chaisorn, L.; Zhao, Y .; Neo, S.-Y .; and Chua, T.-\nS. 2003. VideoQA: question answering on news video. In\nACM Multimedia.\nYang, X.; Tang, K.; Zhang, H.; and Cai, J. 2019. Auto-\nencoding scene graphs for image captioning. In CVPR.\nYeh, Y .-T.; Lin, T.-C.; Cheng, H.-H.; Deng, Y .-H.; Su, S.-Y .;\nand Chen, Y .-N. 2019. Reactive multi-stage feature fusion\nfor multimodal dialogue modeling. arXiv:1908.05067 .\nZhang, J.; Shih, K. J.; Elgammal, A.; Tao, A.; and Catanzaro,\nB. 2019. Graphical Contrastive Losses for Scene Graph\nGeneration. In CVPR.\nZhang, X.; Zhou, X.; Lin, M.; and Sun, J. 2018. Shufﬂenet:\nAn extremely efﬁcient convolutional neural network for mo-\nbile devices. In CVPR.\nZhu, H.; Luo, M.; Wang, R.; Zheng, A.; and He, R. 2020.\nDeep Audio-Visual Learning: A Survey. arXiv:2001.04758\n.\n1423",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8030511736869812
    },
    {
      "name": "Transformer",
      "score": 0.7003755569458008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.594507098197937
    },
    {
      "name": "Dialog box",
      "score": 0.5094175934791565
    },
    {
      "name": "ENCODE",
      "score": 0.47675901651382446
    },
    {
      "name": "Graph",
      "score": 0.4696934223175049
    },
    {
      "name": "Natural language processing",
      "score": 0.45823317766189575
    },
    {
      "name": "Feature learning",
      "score": 0.4578690528869629
    },
    {
      "name": "Modal",
      "score": 0.4119233787059784
    },
    {
      "name": "Speech recognition",
      "score": 0.38983458280563354
    },
    {
      "name": "Machine learning",
      "score": 0.36930108070373535
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1968439817428589
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I4210096112",
      "name": "Rutgers Sexual and Reproductive Health and Rights",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210133125",
      "name": "Mitsubishi Electric (Japan)",
      "country": "JP"
    }
  ]
}