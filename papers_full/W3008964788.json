{
  "title": "Sketchformer: Transformer-Based Representation for Sketched Structure",
  "url": "https://openalex.org/W3008964788",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3035568328",
      "name": "Leo Sampaio Ferraz Ribeiro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2291776233",
      "name": "Tu Bui",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A1978803655",
      "name": "John Collomosse",
      "affiliations": [
        "University of Surrey",
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2290301730",
      "name": "Moacir Ponti",
      "affiliations": [
        "Universidade de São Paulo",
        "Universidade Federal de São Carlos"
      ]
    },
    {
      "id": "https://openalex.org/A3035568328",
      "name": "Leo Sampaio Ferraz Ribeiro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2291776233",
      "name": "Tu Bui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978803655",
      "name": "John Collomosse",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2290301730",
      "name": "Moacir Ponti",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1981934656",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2962721615",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2912775113",
    "https://openalex.org/W2962747232",
    "https://openalex.org/W2964266708",
    "https://openalex.org/W2890889139",
    "https://openalex.org/W2463931728",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6638318767",
    "https://openalex.org/W2089995859",
    "https://openalex.org/W2083118758",
    "https://openalex.org/W1983556963",
    "https://openalex.org/W1972420097",
    "https://openalex.org/W2128543433",
    "https://openalex.org/W2148953314",
    "https://openalex.org/W2237261560",
    "https://openalex.org/W1976664910",
    "https://openalex.org/W2781749799",
    "https://openalex.org/W2685669664",
    "https://openalex.org/W2619098136",
    "https://openalex.org/W2938500406",
    "https://openalex.org/W2781305742",
    "https://openalex.org/W2948085278",
    "https://openalex.org/W2511925527",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2963307918",
    "https://openalex.org/W6736562241",
    "https://openalex.org/W2471581439",
    "https://openalex.org/W2336302573",
    "https://openalex.org/W1975517671",
    "https://openalex.org/W2467281799",
    "https://openalex.org/W2965128575",
    "https://openalex.org/W6704508018",
    "https://openalex.org/W2962837952",
    "https://openalex.org/W6948100495"
  ],
  "abstract": "Sketchformer is a novel transformer-based representation for encoding free-hand sketches input in a vector form, i.e. as a sequence of strokes. Sketchformer effectively addresses multiple tasks: sketch classification, sketch based image retrieval (SBIR), and the reconstruction and interpolation of sketches. We report several variants exploring continuous and tokenized input representations, and contrast their performance. Our learned embedding, driven by a dictionary learning tokenization scheme, yields state of the art performance in classification and image retrieval tasks, when compared against baseline representations driven by LSTM sequence to sequence architectures: SketchRNN and derivatives. We show that sketch reconstruction and interpolation are improved significantly by the Sketchformer embedding for complex sketches with longer stroke sequences.",
  "full_text": "Sketchformer: Transformer-based Representation for Sketched Structure\nLeo Sampaio Ferraz Ribeiro*1, Tu Bui*2, John Collomosse2, 3, and Moacir Ponti1\n1ICMC, Universidade de S˜ao Paulo – S˜ao Carlos/SP, Brazil\nleo.sampaio.ferraz.ribeiro@gmail.com, ponti@usp.br\n2CVSSP, University of Surrey – Guildford, Surrey, UK\n{t.bui,j.collomosse}@surrey.ac.uk\n3Adobe Research, Creative Intelligence Lab — San Jose, CA, USA\nAbstract\nSketchformer is a novel transformer-based representa-\ntion for encoding free-hand sketches input in a vector form,\ni.e. as a sequence of strokes. Sketchformer effectively ad-\ndresses multiple tasks: sketch classiﬁcation, sketch based\nimage retrieval (SBIR), and the reconstruction and interpo-\nlation of sketches. We report several variants exploring con-\ntinuous and tokenized input representations, and contrast\ntheir performance. Our learned embedding, driven by a\ndictionary learning tokenization scheme, yields state of the\nart performance in classiﬁcation and image retrieval tasks,\nwhen compared against baseline representations driven by\nLSTM sequence to sequence architectures: SketchRNN and\nderivatives. We show that sketch reconstruction and inter-\npolation are improved signiﬁcantly by the Sketchformer em-\nbedding for complex sketches with longer stroke sequences.\n1. Introduction\nSketch representation and interpretation remains an open\nchallenge, particularly for complex and casually con-\nstructed drawings. Yet, the ability to classify, search,\nand manipulate sketched content remains attractive as ges-\nture and touch interfaces reach ubiquity. Advances in re-\ncurrent network architectures within language processing\nhave recently inspired sequence modeling approaches to\nsketch (e.g. SketchRNN [12]) that encode sketch as a vari-\nable length sequence of strokes, rather than in a raster-\nized or ‘pixel’ form. In particular, long-short term mem-\nory (LSTM) networks have shown signiﬁcant promise in\nlearning search embeddings [32, 5] due to their ability to\nmodel higher-level structure and temporal order versus con-\nvolutional neural networks (CNNs) on rasterized sketches\n[3, 18, 6, 22]. Yet, the limited temporal extent of LSTM\nrestricts the structural complexity of sketches that may be\naccommodated in sequence embeddings. In language mod-\n*These authors contributed equally to this work\neling domain, this shortcoming has been addressed through\nthe emergence of Transformer networks [8, 7, 28] in which\nslot masking enhances the ability to learn longer term tem-\nporal structure in the stroke sequence.\nThis paper proposes Sketchformer, the ﬁrst Transformer\nbased network for learning a deep representation for free-\nhand sketches. We build on the language modeling Trans-\nformer architecture of Vaaswani et al. [28] to develop sev-\neral variants of Sketchformer that process sketch sequences\nin continuous and tokenized forms. We evaluate the efﬁcacy\nof each learned sketch embedding for common sketch inter-\npretation tasks. We make three core technical contributions:\n1) Sketch Classiﬁcation. We show that Sketchformer\ndriven by a dictionary learning tokenization scheme outper-\nforms state of the art sequence embeddings for sketched ob-\nject recognition over QuickDraw! [19]; the largest and most\ndiverse public corpus of sketched objects.\n2) Generative Sketch Model. We show that for more\ncomplex, detailed sketches comprising lengthy stroke se-\nquences, Sketchformer improves generative modeling of\nsketch – demonstrated by higher ﬁdelity reconstruction of\nsketches from the learned embedding. We also show that\nfor sketches of all complexities, interpolation in the Sketch-\nformer embedding is stable, generating more plausible in-\ntermediate sketches for both inter- and intra-class blends.\n3) Sketch based Image Retrieval (SBIR) We show that\nSketchformer can be uniﬁed with raster embedding to pro-\nduce a search embedding for SBIR (after [5] for LSTM)\nto deliver improved prevision over a large photo corpus\n(Stock10M).\nThese enhancements to sketched object understanding,\ngenerative modeling and matching demonstrated for a di-\nverse and complex sketch dataset suggest Transformer as a\npromising direction for stroke sequence modeling.\n2. Related Work\nRepresentation learning for sketch has received exten-\nsive attention within the domain of visual search. Classi-\n1\narXiv:2002.10381v1  [cs.CV]  24 Feb 2020\ncal sketch based image retrieval (SBIR) techniques explored\nspectral, edge-let based, and sparse gradient features the lat-\nter building upon the success of dictionary learning based\nmodels (e.g. bag of words) [26, 1, 23]. With the advent of\ndeep learning, convolutional neural networks (CNNs) were\nrapidly adopted to learn search embedding [35]. Triplet loss\nmodels are commonly used for visual search in the pho-\ntographic domain [29, 20, 11], and have been extended to\nSBIR. Sangkloy et al. [22] used a three-branch CNN with\ntriplet loss to learn a general cross-domain embedding for\nSBIR. Fine-grained (within-class) SBIR was similarly ex-\nplored by Yu et al . [34]. Qi et al . [18] instead use con-\ntrastive loss to learn correspondence between sketches and\npre-extracted edge maps. Bui et al. [2, 3] perform cross-\ncategory retrieval using a triplet model and combined their\ntechnique with a learned model of visual aesthetics [31] to\nconstrain SBIR using aesthetic cues in [6]. A quadruplet\nloss was proposed by [25] for ﬁne-grained SBIR. The gener-\nalization of sketch embeddings beyond training classes have\nalso been studied [4, 15], and parameterized for zero-shot\nlearning [9]. Such concepts were later applied in sketch-\nbased shape retrieval tasks [33]. Variants of CycleGAN [36]\nhave also shown to be useful as generative models for sketch\n[13]. Sketch-A-Net was a seminal work for sketch clas-\nsiﬁcation that employed a CNN with large convolutional\nkernels to accommodate the sparsity of stroke pixels [34].\nRecognition of partial sketches has also been explored by\n[24]. Wang et al . [30] proposed sketch classiﬁcation by\nsampling unordered points of a sketch image to learning a\ncanonical order.\nAll the above works operate over rasterized sketchese.g.\nconverting the captured vector representation of sketch (as\na sequence of strokes) to pixel form, discarding tempo-\nral order of strokes, and requiring the network to recover\nhigher level spatial structure. Recent SBIR work has begun\nto directly input a vector (stroke sequence) representations\nfor sketches [21], notably SketchRNN; an LSTM based se-\nquence to sequence (seq2seq) variational auto-proposed by\nEck et al. [12], trained on the largest public sketch corpus\n‘QuickDraw! [19]. SketchRNN embedding was incorpo-\nrated in a triplet network by Xu et al . [32] to search for\nsketches using sketches. A variation using cascaded at-\ntention networks was proposed by [14] to improve vector\nsketch classiﬁcation over Sketch-A-Net. Later, LiveSketch\n[5] extended SketchRNN to a triplet network to perform\nSBIR over tens of millions of images, harnessing the sketch\nembedding to suggest query improvements and guide the\nuser via relevance feedback. The limited temporal scope of\nLSTM based seq2seq models can prevent such representa-\ntions modeling long, complex sketches, a problem mitigated\nby our Transformer based model which builds upon the suc-\ncess shown by such architectures for language modeling\n[28, 7, 8]. Transformers encode long term temporal depen-\ndencies by modeling direct connections between data units.\nThe temporal range of such dependencies was increased via\nthe Transformer-XL [7] and BERT [8], which recently set\nnew state-of-the-art performance on sentence classiﬁcation\nand sentence-pair regression tasks using a cross-encoder.\nRecent work explores transformer beyond sequence mod-\neling to 2D images [17]. Our work is ﬁrst to apply these\ninsights to the problem of sketch modeling, incorporating\nthe Transformer architecture of Vaswani et al. [28] to de-\nliver a multi-purpose embedding that exceeds the state of\nthe art for several common sketch representation tasks.\n3. Sketch Representation\nWe propose Sketchformer; a multi-purpose sketch rep-\nresentation from stroke sequence input. In this section we\ndiscuss the pre-processing steps, the adaptions made to the\ncore architecture proposed by Vaswani et al. [28] and the\nthree application tasks.\n3.1. Pre-processing and Tokenization\nFollowing Eck et al . [12] we simplify all sketches us-\ning the RDP algorithm [10] and normalize stroke length.\nSketches for all our experiments are drawn from Quick-\nDraw50M [19] (see Sec. 4; for dataset partitions).\n1) Continuous. Quickdraw50M sketches are released in\nthe ‘stroke-3’ format where each point(δx,δy,p ) stores its\nrelative position to the previous point together with its bi-\nnary pen state. To also include the ‘end of sketch’ state,\nthe stroke-5 format is often employed: (δx,δy,p 1,p2,p3),\nwhere the the pen states p1 - draw, p2 - lift and p3 - end are\nmutually exclusive [12]. Our experiments with continuous\nsketch modeling use the ‘stroke-5’ format.\n2) Dictionary learning. We build a dictionary of K code\nwords (K = 1000) to model the relative pen motion i.e.\n(δx,δy). We randomly sample 100k sketched pen move-\nments in the training set for clustering via K-means. We\nallocate 20% of sketch points for sampling inter-stroke tran-\nsition, i.e. relative transition when the pen is lifted, to bal-\nance with the more common within-stroke transitions. Each\ntransition point (δx,δy) is then assigned to the nearest code\nword, resulting in a sequence of discrete tokens. We also in-\nclude 4 special tokens; a Start of Sketch (SOS) token at the\nbeginning of every sketch, an End of Sketch (EOS) token\nat the end, a Stroke End Point (SEP) token to be inserted\nbetween strokes (indicate pen lifting) and a padding (PAD)\ntoken to pad the sketch to a ﬁxed length.\n3) Spatial Grid. The sketch canvas is ﬁrst quantized into\nn×n(n = 100) square cells, each cell is represented by\na token in our dictionary. Given the absolute (x,y) sketch\npoints, we determine which cell contains this point and as-\nsign the cell’s token to the point. The same four special\ntokens above are used to complete the sketch sequence.\nFig. 2 visualizes sketch reconstruction under the tok-\nenized methods to explore sensitivity to the quantization pa-\nrameters. Compared with the stroke-5 format (continuous)\nthe tokenization methods are more compact. Dictionary\nlearned tokenization (Tok-Dict) can have a small dictionary\nsize and is invariant to translation since it is derived from\nstroke-3. On the other hand quantization error could ac-\ncumulate over longer sketches if dictionary size is too low,\nFigure 1. Schematic of the Transformer architecture used for Sketchformer, which utilizes the original architecture of Vaswani et al. [28]\nbut modiﬁes it with an alternate mechanism for formulating the bottleneck (sketch embedding) layer using a self-attention block, as well\nas conﬁguration changes e.g. MHA head count (see Sec. 3.2).\nshifting the position of strokes closer to the sequence’s end.\nThe spatial grid based tokenization method (Tok-Grid), on\nthe other hand, does not accumulate error but is sensitive to\ntranslation and yields a larger vocabulary (n2).\n3.2. Transformer Architecture for Sketch\nSketchformer uses the Transformer network of Vaswani\net al. [28]. We add stages ( e.g. self-attention and modiﬁed\nbottleneck) and adapt parameters in their design to learn\na multi-purpose representation for stroke sequences, rather\nthan language. A transformer network consists of an en-\ncoder and decoder blocks, each comprising several layers\nof multihead attention followed by a feed forward network.\nFig. 1 illustrates the architecture with dotted lines indicating\nre-use of architecture stages from [28]. In Fig. 4 we show\nhow our learned embedding is used across multiple appli-\ncations. Compared to [28] we use 4 MHA blocks versus 6\nand a feed-forward dimension of 512 instead of 2048. Un-\nlike traditional sequence modeling methods (RNN/LSTM)\nwhich learns the temporal order of current time steps from\nprevious steps (or future steps in bidirectional encoding),\nthe attention mechanism in transformers allows the network\nto decide which time steps to focus on to improve the task at\nhand. Each multihead attention (MHA) layer is formulated\nas such:\nSHA(k,q,v ) =softmax(αqkT )v (1)\nMHA(k,q,v ) = [SHA0(kWk\n0 ,qW q\n0 ,vW v\n0 ),... (2)\nSHAm(kWk\nm,qW q\nm,vW v\nm)]W0 (3)\nwhere k, q and v are respective Key, Query and Value in-\nputs to the single head attention (SHA) module. This mod-\nule computes the similarity between pairs ofQuery and Key\nfeatures, normalizes those scores and ﬁnally uses them as a\nprojection matrix for the Value features. The multihead at-\ntention (MHA) module concatenates the output of multiple\nsingle heads and projects the result to a lower dimension. α\nis a scaling constant andW(.)\n(.) are learnable weight matrices.\nThe MHA output is fed to a positional feed forward net-\nwork (FFN), which consists of two fully connected layers\nwith ReLU activation. The MHA-FFN ( F(.)) blocks are\nthe basis of the encoder side of our network (E(.)):\nFFN (x) =max(0,xWf\n1 + bf\n1 )Wf\n2 + bf\n2 (4)\nF(x) =FFN (MHA(x,x,x )) (5)\nE(x) =FN (..(F1(x))) (6)\nwhere X indicates layer normalization over X and N is\nnumber of the MHA-FFN units F(.).\nThe decoder takes as inputs the encoder output and target\nsequence in an auto-regressive fashion. In our case we are\nlearning an transformer autoencoder so the target sequence\nis also the input sequence shifted forward by 1:\nG(h,x) =FFN (MHA2(h,h, MHA1(x,x,x ))) (7)\nD(h,x∗) =GN (h,GN−1(h,...G1(h,x∗))) (8)\nFigure 2. Visualizing the impact of quantization on the recon-\nstruction of short, median and long sequence length sketches.\nGrid sizes of n = [10,100] (Tok-Grid) and dictionary sizes of\nK = [500,1000] (Tok-Dict). Sketches are generated from the\ntokenized sketch representations, independent of transformer.\nFigure 3. t-SNE visualization of the learned embedding space from Sketchformer’s three variants, compared to LiveSketch (left) and\ncomputed over the QD-862k test set; 10 categories and 1000 samples were randomly selected.\nwhere h is the encoder output, x∗ is the shifted auto-\nregressive version of input sequence x.\nThe conventional transformer is designed for language\ntranslation and thus does not provide a feature embedding\nas required in Sketchformer (output of Eis also a sequence\nof vectors of the same length as x). To learn a compact\nrepresentation for sketch we propose to apply self-attention\non the encoder output, inspired by [27]:\ns= softmax(tanh(hKT + b)v) (9)\nz=\n∑\ni\nsihi (10)\nwhich is similar to SHA however the Key matrix K, Value\nvector vand bias bare now trainable parameters. This self-\nattention layer learns a weight vector sdescribing the im-\nportance of each time step in sequence h, which is then\naccumulated to derive the compact embedding z. On the\ndecoder side, zis passed through a FFN to resume the orig-\ninal shape of h. These are the key novel modiﬁcations to the\noriginal Transformer architecture of Vaswaniet al. (beyond\nabove-mentioned parameter changes).\nWe also had to change how masking worked on the de-\ncoder. The Transformer uses a padding mask to stop atten-\ntion blocks from giving weight to out-of-sequence points.\nSince we want a meaningful embedding for reconstruction\nand interpolation, we removed this mask from the decoder,\nforcing our transformer to learn reconstruction without pre-\nviously knowing the sequence length and using only the em-\nbedding representation.\n3.2.1 Training Losses\nWe employ two losses in training Sketchformer. A classiﬁ-\ncation (softmax) loss is connected to the sketch embedding\nz to preserve semantic information while a reconstruction\nloss ensures the decoder can reconstruct the input sequence\nfrom its embedding. If the input sequence is continuous (i.e.\nstroke-5) the reconstruction loss consists of a L2 loss term\nmodeling relative transitions (δx,δy) and a 3-way classi-\nﬁcation term modeling the pen states. Otherwise the re-\nconstruction loss uses softmax to regularize a dictionary of\nsketch tokens as per a language model. We found these\nlosses simple yet effective in learning a robust sketch em-\nbedding. Fig. 3 visualizes the learned embedding for each\nof the three pre-processing variants, alongside that of a state\nof the art sketch encoding model using stroke sequences [5].\n3.3. Cross-modal Search Embedding\nTo use our learned embedding for SBIR, we follow the\njoint embedding approach ﬁrst presented in [5] and train an\nauxiliary network that uniﬁes the vector (sketch) and raster\n(image corpus) representations into a common subspace.\nThis auxiliary network is composed of four fully con-\nnected layers (see Fig. 4) with ReLU activations. These are\ntrained within a triplet framework and have input from three\npre-trained branches: an anchor branch that models vector\nrepresentations (our Sketchformer), plus positive and nega-\ntive branches extracting representations from raster space.\nThe ﬁrst two fully connected layers are domain-speciﬁc\nand we call each set FV (.),FR(.), referring to vector-\nspeciﬁc and raster-speciﬁc. The ﬁnal two layers are shared\nbetween domains; we refer to this set as FS(.). Thus\nthe end-to-end mapping from vector sketch and raster\nsketch/image to the joint embedding is:\nuv = FS(FV (E(xv))) (11)\nur = FS(FR(P(xr))) (12)\nwhere xv and xr are the input vector sketches and raster\nimages respectively, anduv and ur their corresponding rep-\nresentations in the common embedding. E(.) is the net-\nwork that models vector representations and P(.) is the one\nfor raster images. In the original LiveSketch [5], E(.) is a\nSketchRNN [12]-based model, while we employ our multi-\ntask Sketchformer encoder instead. For P(.) we use the\nsame off-the-shelf GoogLeNet-based network, pre-trained\non a joint embedding task (from [4]).\nThe training is performed using triplet loss regularized\nwith the help of a classiﬁcation task. Training requires an\naligned sketch and image dataset i.e. a sketch set and im-\nage set that share the same category list. This is not the\ncase for Quickdraw, which is a sketch-only dataset without\na corresponding image set. Again following [5], we use\nthe raster sketch as a medium to bridge vector sketch with\nraster image. The off-the-shelf P(.) (from [4]) was trained\nFigure 4. Schematic showing how the learned sketch embedding\nis leveraged for sketch synthesis (reconstruction/interpolation),\nclassiﬁcation and cross-modal retrieval experiments (see en-\ncoder/embedding inset, refer to Fig. 1 for detail). Classiﬁcation\nappends fully-connected (fc) and softmax layers to the embedding\nspace. Retrieval tasks require uniﬁcation with a raster (CNN) em-\nbedding for images [4] via several fc layers trained via triplet loss.\nto produce a joint embedding model unifying raster sketch\nand raster image; This allowed the authors train theFR, FV\nand FS sets using vector and raster versions of sketch only.\nBy following the same procedure, we eliminate the need of\nhaving an aligned image set for Quickdraw as our network\nnever sees an image feature during training.\nThe training is implemented in two phases. At phase\none, the anchor and positive samples are vector and raster\nforms of random sketches in the same category while raster\ninput of the negative branch is sampled from a different cat-\negory. At phase two, we sample hard negatives from the\nsame category with the anchor vector sketch and choose the\nraster form of the exact instance of the anchor sketch for the\npositive branch. The triplet loss maintains a margin between\nthe anchor-positive and anchor-negative distances:\nLT (x,x+,x−) =max(0,|FS(FV (E(x))) −FS(FR(P(x+)))|\n−||FS(FV (E(x))) −FS(FR(P(x−)))||+ m)\n(13)\nand margin m= 0.2 in phase one, m= 0.05 in phase two.\n4. Experiments and Discussion\nWe evaluate the performance of the proposed trans-\nformer embeddings for three common tasks; sketch classi-\nﬁcation, sketch reconstruction and interpolation, and sketch\nbased image retrieval (SBIR). We compare against two\nbaseline sketch embeddings for encoding stroke sequences;\nSketchRNN [12] (also used for search in [32]) and LiveS-\nketch [5]. We evaluate using sketches from QuickDraw50M\n[19], and a large corpus of photos (Stock10M).\nQuickDraw50M [19] comprises over 50M sketches of\n345 object categories, crowd-sourced within a gamiﬁed\ncontext that encouraged casual sketches drawn at speed.\nSketches are often messy and complex in their structure,\nconsistent with tasks such as SBIR. Quickdraw50M cap-\ntures sketches as stroke sequences, in contrast to earlier\nraster-based and less category-diverse datasets such as TU-\nBerlin/Sketchy. We sample 2.5M sketches randomly with\neven class distribution from the public Quickdraw50M\ntraining partition to create training set (QD-2.5M) and use\nthe public test partition of QuickDraw50M (QD-862k) com-\nprising 2.5k ×345 = 862ksketches to evaluate our trained\nmodels. For SBIR and interpolation experiments we sort\nQD-862k by sequence length, and sample three datasets\n(QD345-S, QD345-M, QD345-L) at centiles 10, 50 and 90\nrespectively to create a set of short, medium and long stroke\nsequences. Each of these three datasets samples one sketch\nper class at random from the centile yielding three evalua-\ntion sets of 345 sketches. We sampled an additional query\nset QD345-Q for use in sketch search experiments, using\nthe same 345 sketches as LiveSketch [5]. The median stroke\nlengths of QD345-S, QD345-M, QD345-L are 30, 47 and\n75 strokes respectively (after simpliﬁcation via RDP [10]).\nStock67M is a diverse, unannotated corpus of photos\nused in prior SBIR work [5] to evaluate large-scale SBIR\nretrieval performance. We sample 10M of these images at\nrandom for our search corpus (Stock10M).\n4.1. Evaluating Sketch Classiﬁcation\nWe evaluate the class discrimination of the proposed\nsketch embedding via attaching dense and softmax lay-\ners to the transformer encoder stage, and training a 345-\nway classiﬁer on QD2.5M. Table 1 reports the classiﬁca-\ntion performance over QD-862k for each of the three pro-\nposed transformer embeddings, alongside two LSTM base-\nlines – the SketchRNN [12] and LiveSketch [5] variational\nautoencoder networks. Whilst all transformers outperform\nthe baseline, the tokenized variant of the transformer based\non dictionary learning (TForm-Tok-Dict) yields highest ac-\ncuracy. We explore this further by shufﬂing the order of\nthe sketch strokes retraining the transformer models from\nMethod mAP%\nBaseline LiveSketch [5] 72.93\nSketchRNN [12] 67.69\nShufﬂed\nTForm-Cont 76.95\nTForm-Tok-Grid 76.22\nTForm-Tok-Dict 76.66\nProposed\nTForm-Cont 77.68\nTForm-Tok-Grid 77.36\nTForm-Tok-Dict 78.34\nTable 1. Sketch classiﬁcation results over QuickDraw! [19] for\nthree variants of the proposed transformer embedding, contrast-\ning each to models learned from randomly permuted stroke order.\nComparing to two recent LSTM based approaches for sketch se-\nquence encoding [5, 12].\nFigure 5. Visualization of sketches reconstructed from mean em-\nbedding for 3 object categories. We add Gaussian noise with stan-\ndard deviation σ = 0.5 and σ = 1.0 to the mean embedding of\nthree example categories on the Quickdraw test set. The recon-\nstructed sketches of Tform-Tok-Dict retain salient features even\nwith high noise perturbation.\nMethod Short Mid Long\nBaseline LiveSketch [5] 62.1 59.2 27.8\nSketchRNN [12] 4.05 3.70 1.38\nProposed\nTForm-Cont 0.00 0.00 0.00\nTForm-Tok-Grid 6.75 6.10 5.56\nTForm-Tok-Dict 24.3 28.4 51.4\nUncertain 2.70 2.47 13.9\nTable 2. User study quantifying accuracy of sketch reconstruction.\nPreference is expressed by 5 independent workers, and results with\n> 50% agreement are included. Experiment repeated for short,\nmedium and longer stroke sequences. For longer sketches, the\nproposed transformer method TForm-Tok-Dict is preferred.\nscratch. We were surprised to see comparable performance,\nsuggesting this gain is due to spatial continuity rather than\ntemporal information.\n4.2. Reconstruction and Interpolation\nWe explore the generative power of the proposed em-\nbedding by measuring the degree of ﬁdelity with which: 1)\nencoded sketches can be reconstructed via the decoder to re-\nsemble the input; 2) a pair of sketches may be interpolated\nwithin, and synthesized from, the embedding. The experi-\nments are repeated for short (QD345-S), medium (QD345-\nM) and long (QD345-L) sketch complexities. We assess\nthe ﬁdelity of sketch reconstruction and the visual plausibil-\nity of interpolations via Amazon Mechanical Turk (MTurk).\nMTurk workers are presented with a set of reconstructions\nor interpolations and asked to make a 6-way preference\nchoice; 5 methods and a ’cannot determine’ option. Each\ntask is presented to ﬁve unique workers, and we only in-\nclude results for which there is > 50% (i.e. > 2 worker)\nconsensus on the choice.\nReconstruction results are shown in Table 2 and fa-\nvor the LiveSketch [5] embedding for short or medium\nFigure 6. Representative sketch reconstructions from each of\nthe ﬁve embeddings evaluated in Table 2. (a) Original, (b)\nSketchRNN, (c) LiveSketch, (d) TForm-Cont, (e) TForm-Tok-\nGrid and (f) TForm-Tok-Dict. The last row represents a hard-to-\nreconstruct sketch.\nlength strokes, with the proposed tokenized transformer\n(TForm-Tok-Dict) producing better results for more com-\nplex sketches aided by the improved representational power\nof transformer for longer stroke sequences. Fig 6 provides\nrepresentative visual examples for each sketch complexity.\nWe explore interpolation in Table 3 blending between\npairs of sketches within (intra-) class and between (inter-)\nclass. In all cases we encode sketches separately to the em-\nbedding, interpolate via slerp (after [12, 5] in which slerp\nwas shown to offer best performance), and decode the in-\nterpolated point to generate the output sketch. Fig. 7 pro-\nvides visual examples of inter- and intra- class interpolation\nfor each method evaluated. In all cases the proposed to-\nkenized transformer (TForm-Tok-Dict) outperforms other\ntransformer variants and baselines, although the perfor-\nmance separation is narrower for shorter strokes echoing\nresults of the reconstruction experiment. The stability of\nour representation is further demonstrated via local sam-\npling within the embedding in Fig. 5.\n4.3. Cross-modal Matching\nWe evaluate the performance of Sketchformer for sketch\nbased retrieval of sketches (S-S) and images (S-I).\nSketch2Sketch (S-S) Matching. We quantify the\naccuracy of retrieving sketches in one modality (raster)\ngiven a sketched query in another (vector, i.e. stroke\nsequence) – and vice-versa. This evaluates the performance\nof Sketchformer in discriminating between sketched visual\nstructures invariant to their input modality. Sketchformer\nis trained on QD-2.5M and we query the test corpus\nQD-826k using QD-345Q as the query set. We measure\noverall mean average precision (mAP) for both coarse\ngrain (i.e. class-speciﬁc) and ﬁne-grain ( i.e. instance-level)\nsimilarity, as mean average of mAP for each query. As\nper [5] for the former we consider a retrieved record a\nmatch if it matches the sketched object class. For the\nlatter, exactly the same single sketch must match (in its\ndifferent modality). To run raster variants, a rasterized\nversion of QD-862k (for V-R) and of QD345-Q (for R-V)\nis produced by rendering strokes to a 256 ×256 pixel\ncanvas using the CairoSVG Python library. Table 4 show\nFigure 7. Representative sketch interpolations from each of the ﬁve embeddings evaluated in Table 3. For each embedding: (ﬁrst row)\ninter-class interpolation from ’birthday cake’ to ‘ice-cream’ and (second row) intra-class interpolation between two ‘birthday cakes’.\nFigure 8. Representative visual search results over Stock10M indexed by our proposed embedding (TForm-Tok-Dict) for a vector sketch\nquery. The two bottom rows (‘animal migration’ and ‘tree’) are failure cases.\nthat for both class and instance level retrieval, the R-V con-\nﬁguration outperforms V-R indicating a performance gain\ndue to encoding this large search index using the vector\nrepresentation. In contrast to other experiments reported,\nthe continuous variant of Sketchformer appears slightly\npreferred, matching higher for early ranked results for the\nS-S case – see Fig. 9a for category-level precision-recall\ncurve. Although Transformer outperforms RNN baselines\nby 1-3% in the V-R case the gain is more limited and\nindeed the performance over baselines is equivocal in the\nS-S where the search index is formed of rasterized sketches.\nSketch2Image (S-I) Matching. We evaluate sketch\nbased image retrieval (SBIR) over Stock10M dataset of di-\nverse photos and artworks, as such data is commonly in-\ndexed for large-scale SBIR evaluation [6, 5]. We compare\nagainst the state of the art SBIR algorithms accepting vector\n(LiveSketch [5]) and raster (Buiet al. [4]) sketched queries.\nSince no ground-truth annotation is possible for this size of\ncorpus, we crowd-source per-query annotation via Mechan-\nical Turk (MTurk) for the top-k(k=15) results and compute\nboth mAP% and precision@ k curve averaged across all\nQD345-Q query sketches. Table 5 compares performance\nof our tokenized variants to these baselines, alongside as-\nsociated Precision@k curves in Fig. 9b. The proposed dic-\ntionary learned transformer embedding (TForm-Tok-Dict)\ndelivers the best performance (visual results in Fig. 8).\nPartition Embedding Intra- Inter-\nShort (10th cent.)\nSketchRNN [12] 0.00 2.06\nLiveSketch [5] 25.8 30.9\nTForm-Cont 14.0 6.18\nTForm-Tok-Grid 19.4 17.5\nTForm-Tok-Dict 31.2 33.0\nUncertain 8.60 10.3\nMean (50th cent.)\nSketchRNN [12] 0.00 0.00\nLiveSketch [5] 25.2 20.2\nTForm-Cont 15.6 16.0\nTForm-Tok-Grid 15.6 19.1\nTForm-Tok-Dict 35.8 35.1\nUncertain 7.36 9.57\nLong (90th cent.)\nSketchRNN [12] 0.00 0.00\nLiveSketch [5] 25.0 21.1\nTForm-Cont 12.5 8.42\nTForm-Tok-Grid 16.7 10.5\nTForm-Tok-Dict 40.6 50.5\nUncertain 5.21 9.47\nTable 3. User study quantifying interpolation quality for a pair of\nsketches of same (intra-) or between (inter-) classes. Preference\nis expressed by 5 independent workers, and results with > 50%\nagreement are included. Experiment repeated for short, medium\nand longer stroke sequences.\nFigure 9. Quantifying search accuracy. a) Sketch2Sketch via\nprecision-recall (P-R) curves for Vector-2-Raster and Raster-2-\nVector category-level retrieval. b) Sketch2Image (SBIR) accuracy\nvia precision @ k=[1,15] curve over Stock10M.\nMethod Instance Category\nLivesketch [5] V-R 6.71 20.49\nR-V 7.15 20.93\nTForm-Cont V-R 5.29 22.22\nR-V 6.21 23.48\nTForm-Tok-Grid V-R 6.42 21.26\nR-V 7.38 22.10\nTForm-Tok-Dict V-R 6.07 21.56\nR-V 7.08 22.51\nTable 4. Quantifying the performance of Sketch2Sketch retrieval\nunder two RNN baselines and three proposed variants. We report\ncategory- and instance-level retrieval (mAP%).\nMethod mAP%\nBaseline LiveSketch [5] 54.80\nCAG [3] 51.97\nProposed\nTForm-Tok-Grid 53.75\nTForm-Tok-Dict 56.96\nTable 5. Quantifying accuracy of Sketchformer for Sketch2Image\nsearch (SBIR). Mean average precision (mAP) computed to rank\n15 over Stock10M for the QD345-Q query set.\n5. Conclusion\nWe presented Sketchformer; a learned representation for\nsketches based on the Transformer architecture [28]. Sev-\neral variants were explored using continuous and tokenized\ninput; a dictionary learning based tokenization scheme de-\nlivers performance gains of 6% on previous LSTM autoen-\ncoder models (SketchRNN and derivatives). We showed\ninterpolation within the embedding yields plausible blend-\ning of sketches within and between classes, and that re-\nconstruction (auto-encoding) of sketches is also improved\nfor complex sketches. Sketchformer was also shown effec-\ntive as a basis for indexing sketch and image collections\nfor sketch based visual search. Future work could further\nexplore our continuous representation variant, or other vari-\nants with more symmetric encoder-decoder structure. We\nhave demonstrated the potential for Transformer networks\nto learn a multi-purpose representation for sketch, but be-\nlieve many further applications of Sketchformer exist be-\nyond the three tasks studied here. For example, fusion with\nadditional modalities might enable sketch driven photo gen-\neration [16] using complex sketches, or with a language em-\nbedding for novel sketch synthesis applications.\nReferences\n[1] Tu Bui and John Collomosse. Scalable sketch-based image\nretrieval using color gradient features. In Proc. ICCV Work-\nshops, pages 1–8, 2015. 2\n[2] T. Bui, L. Ribeiro, M. Ponti, and J. Collomosse. Generali-\nsation and sharing in triplet convnets for sketch based visual\nsearch. CoRR Abs, arXiv:1611.05301, 2016. 2\n[3] T. Bui, L. Ribeiro, M. Ponti, and J. Collomosse. Compact de-\nscriptors for sketch-based image retrieval using a triplet loss\nconvolutional neural network. Computer Vision and Image\nUnderstanding (CVIU), 2017. 1, 2, 8\n[4] T. Bui, L. Ribeiro, M. Ponti, and J. Collomosse. Sketching\nout the details: Sketch-based image retrieval using convolu-\ntional neural networks with multi-stage regression. Elsevier\nComputers & Graphics, 2018. 2, 4, 5, 7\n[5] J. Collomosse, T. Bui, and H. Jin. Livesketch: Query per-\nturbations for guided sketch-based visual search. In Proc.\nCVPR, pages 1–9, 2019. 1, 2, 4, 5, 6, 7, 8\n[6] J. Collomosse, T. Bui, M. Wilber, C. Fang, and H. Jin.\nSketching with style: Visual search with sketches and aes-\nthetic context. In Proc. ICCV, 2017. 1, 2, 7\n[7] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen,\nJaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-\nlength context. arXiv preprint arXiv:1901.02860, 2019. 1,\n2\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, v.1, pages 4171–4186. Association for Com-\nputational Linguistics, 2019. 1, 2\n[9] S. Dey, P. Riba, A. Dutta, J. Llados, and Y . Song. Doodle to\nsearch: Practical zero-shot sketch-based image retrieval. In\nProc. CVPR, 2019. 2\n[10] David H Douglas and Thomas K Peucker. Algorithms for\nthe reduction of the number of points required to represent a\ndigitized line or its caricature. Cartographica: the interna-\ntional journal for geographic information and geovisualiza-\ntion, 10(2):112–122, 1973. 2, 5\n[11] Albert Gordo, Jon Almaz ´an, Jerome Revaud, and Diane Lar-\nlus. Deep image retrieval: Learning global representations\nfor image search. In Proc. ECCV, pages 241–257, 2016. 2\n[12] D. Ha and D. Eck. A neural representation of sketch draw-\nings. In Proc. ICLR. IEEE, 2018. 1, 2, 4, 5, 6, 8\n[13] Y . Song T. Xiang T. Hospedales J. Song, K. Pang. Learning\nto sketch with shortcut cycle consistency. In Proc. CVPR,\n2018. 2\n[14] Lei Li, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo\nFu, and Chiew-Lan Tai. Sketch-r2cnn: An attentive\nnetwork for vector sketch recognition. arXiv preprint\narXiv:1811.08170, 2018. 2\n[15] K. Pang, K. Li, Y . Yang, H. Zhang, T. Hospedales, T. Xiang,\nand Y . Song. Generalising ﬁne-grained sketch-based image\nretrieval. In Proc. CVPR, 2019. 2\n[16] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Gaugan: semantic image synthesis with spatially adap-\ntive normalization. In ACM SIGGRAPH 2019 Real-Time\nLive!, page 2. ACM, 2019. 8\n[17] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,\nA. Ku, and D. Tran. Image transformer. In Proc. NeurIPS,\n2019. 2\n[18] Yonggang Qi, Yi-Zhe Song, Honggang Zhang, and Jun Liu.\nSketch-based image retrieval via siamese convolutional neu-\nral network. In Proc. ICIP, pages 2460–2464. IEEE, 2016.\n1, 2\n[19] The Quick, Draw! Dataset. https://github.com/\ngooglecreativelab/quickdraw-dataset. Ac-\ncessed: 2018-10-11. 1, 2, 5\n[20] Filip Radenovi ´c, Giorgos Tolias, and Ond ˇrej Chum. CNN\nimage retrieval learns from BoW: Unsupervised ﬁne-tuning\nwith hard examples. In Proc. ECCV, pages 3–20, 2016. 2\n[21] Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao\nXiang, and Timothy M Hospedales. Learning deep sketch\nabstraction. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 8014–8023,\n2018. 2\n[22] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James\nHays. The sketchy database: Learning to retrieve badly\ndrawn bunnies. In Proc. ACM SIGGRAPH, 2016. 1, 2\n[23] Ros ´alia G Schneider and Tinne Tuytelaars. Sketch classiﬁ-\ncation and classiﬁcation-driven analysis using ﬁsher vectors.\nACM Transactions on Graphics (TOG), 33(6):174, 2014. 2\n[24] Omar Seddati, Stephane Dupont, and Sa ¨ıd Mahmoudi.\nDeepsketch 2: Deep convolutional neural networks for par-\ntial sketch recognition. In 2016 14th International Workshop\non Content-Based Multimedia Indexing (CBMI), pages 1–6.\nIEEE, 2016. 2\n[25] O. Seddati, S. Dupont, and S. Mahoudi. Quadruplet net-\nworks for sketch-based image retrieval. In Proc. ICMR ,\n2017. 2\n[26] J. Sivic and A. Zisserman. Video google: A text retrieval\napproach to object matching in videos. In Proc. ICCV, 2003.\n2\n[27] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-\nto-end memory networks. In Advances in neural information\nprocessing systems, pages 2440–2448, 2015. 4\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you\nneed. In Proc. NeurIPS. IEEE, 2017. 1, 2, 3, 8\n[29] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg,\nJingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learn-\ning ﬁne-grained image similarity with deep ranking. InProc.\nCVPR, pages 1386–1393, 2014. 2\n[30] Xiangxiang Wang, Xuejin Chen, and Zhengjun Zha. Sketch-\npointnet: A compact network for robust sketch recognition.\nIn 2018 25th IEEE International Conference on Image Pro-\ncessing (ICIP), pages 2994–2998. IEEE, 2018. 2\n[31] M. Wilber, C. Fang, H. Jin, A. Hertzmann, J. Collomosse,\nand S. Belongie. Bam! the behance artistic media dataset for\nrecognition beyond photography. In Proc. ICCV, 2017. 2\n[32] P. Xu, Y . Huang, T. Yuan, K. Pang, Y-Z. Song, T. Xiang, and\nT. Hospedales. Sketchmate: Deep hashing for million-scale\nhuman sketch retrieval. In Proc. CVPR, 2018. 1, 2, 5\n[33] Yongzhe Xu, Jiangchuan Hu, Kun Zeng, and Yongyi Gong.\nSketch-based shape retrieval via multi-view attention and\ngeneralized similarity. In 2018 7th International Conference\non Digital Home (ICDH), pages 311–317. IEEE, 2018. 2\n[34] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M\nHospedales, and Chen-Change Loy. Sketch me that shoe. In\nProc. CVPR, pages 799–807, 2016. 2\n[35] Hua Zhang, Si Liu, Changqing Zhang, Wenqi Ren, Rui\nWang, and Xiaochun Cao. Sketchnet: Sketch classiﬁcation\nwith web images. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 1105–\n1113, 2016. 2\n[36] J. Zhu, T. Park, P. Isola, and A. Efros. Unpaired image-\nto-image translation using cycle-consistent adversarial net-\nworks. In Proc. ICCV, 2017. 2",
  "topic": "Sketch",
  "concepts": [
    {
      "name": "Sketch",
      "score": 0.770681619644165
    },
    {
      "name": "Embedding",
      "score": 0.7203018069267273
    },
    {
      "name": "Computer science",
      "score": 0.6911259293556213
    },
    {
      "name": "Transformer",
      "score": 0.6451954245567322
    },
    {
      "name": "Artificial intelligence",
      "score": 0.542492151260376
    },
    {
      "name": "Interpolation (computer graphics)",
      "score": 0.5039069056510925
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4914776086807251
    },
    {
      "name": "Encoding (memory)",
      "score": 0.449418842792511
    },
    {
      "name": "Representation (politics)",
      "score": 0.4412224590778351
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4031106233596802
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33596140146255493
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24097290635108948
    },
    {
      "name": "Algorithm",
      "score": 0.23306667804718018
    },
    {
      "name": "Engineering",
      "score": 0.08588400483131409
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1306409833",
      "name": "Adobe Systems (United States)",
      "country": "US"
    }
  ],
  "cited_by": 30
}