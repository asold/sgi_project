{
  "title": "ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer",
  "url": "https://openalex.org/W4224495123",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1968724871",
      "name": "Shen Xu",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A4224733125",
      "name": "Lacayo, Matthew",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A4224733126",
      "name": "Guggilla, Nidhir",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2744122294",
      "name": "Borrelli, Francesco",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962921175",
    "https://openalex.org/W3209988186",
    "https://openalex.org/W3160050461",
    "https://openalex.org/W2892119467",
    "https://openalex.org/W2424778531",
    "https://openalex.org/W3132535424",
    "https://openalex.org/W4200247997",
    "https://openalex.org/W2097545165",
    "https://openalex.org/W2509788514",
    "https://openalex.org/W3080564557",
    "https://openalex.org/W2900001365",
    "https://openalex.org/W3119824613"
  ],
  "abstract": "The problem of multimodal intent and trajectory prediction for human-driven\\nvehicles in parking lots is addressed in this paper. Using models designed with\\nCNN and Transformer networks, we extract temporal-spatial and contextual\\ninformation from trajectory history and local bird's eye view (BEV) semantic\\nimages, and generate predictions about intent distribution and future\\ntrajectory sequences. Our methods outperform existing models in accuracy, while\\nallowing an arbitrary number of modes, encoding complex multi-agent scenarios,\\nand adapting to different parking maps. To train and evaluate our method, we\\npresent the first public 4K video dataset of human driving in parking lots with\\naccurate annotation, high frame rate, and rich traffic scenarios.\\n",
  "full_text": "ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in\nParking Lots with CNN and Transformer\nXu Shen, Matthew Lacayo, Nidhir Guggilla, and Francesco Borrelli\nAbstract— The problem of multimodal intent and trajec-\ntory prediction for human-driven vehicles in parking lots is\naddressed in this paper. Using models designed with CNN\nand Transformer networks, we extract temporal-spatial and\ncontextual information from trajectory history and local bird’s\neye view (BEV) semantic images, and generate predictions\nabout intent distribution and future trajectory sequences. Our\nmethods outperform existing models in accuracy, while allowing\nan arbitrary number of modes, encoding complex multi-agent\nscenarios, and adapting to different parking maps. To train\nand evaluate our method, we present the ﬁrst public 4K\nvideo dataset of human driving in parking lots with accurate\nannotation, high frame rate, and rich trafﬁc scenarios.\nI. I NTRODUCTION\nWhile the rapid advancement of self-driving technology\nin the past decade has brought people much closer to an\nera with automated mobility, autonomous vehicles (A Vs)\nstill face great challenges in interacting with other road\nusers safely and efﬁciently. In addition to having a robust\nperception system, the ability to make predictions and infer\nthe potential future intents and trajectories of other vehicles\nwill be essential for A Vs to make optimal decisions.\nResearchers have made great strides in the ﬁeld of motion\nprediction. Physics model-based methods [1] such as the\nKalman Filter and its variants leverage the dynamics of the\nvehicle to propagate the state of the vehicle forward for\nintuitive short-term predictions. Reachability study [2] also\nprovides a formal way to measure the uncertainties of vehicle\nbehavior for the control design.\nWhen vehicles operate in complex environments, model-\nbased approaches tend to suffer from the difﬁculty of accu-\nrate modeling, along with the burden of heavy computation.\nIn contrast, deep learning methods have demonstrated great\npotential to incorporate various forms of information and\ngeneralize to new environments. Recurrent Neural Networks\n(RNNs) and Long Short-Term Memory (LSTM) networks [3]\nare widely known for learning sequential data, and various\nresearch papers have also focused on adapting the networks\nto account for the multi-agent interactions [4]. There is\nalso interest in using Convolutional Neural Networks (CNN)\nto make predictions [5] where vehicle trajectories and local\nenvironments can embedded in images efﬁciently.\nIn recent years, Transformer networks [6] have achieved\ngreat success in Natural Language Processing tasks. Their\nattention mechanism helps to keep track of global depen-\ndencies in input and output sequences regardless of the\nrelative position, which overcomes the limitations of RNNs\nUniversity of California, Berkeley, CA, USA ( {xu shen, mattlacayo,\nnidhir.guggilla, fborrelli}@berkeley.edu).\nin learning from long temporal sequences [7]. For trajectory\nprediction tasks [8], Transformer networks have been shown\nto outperform the LSTMs in many aspects, including accu-\nracy [9] and interaction modeling [7].\nMost of the existing work mentioned above focuses on\npedestrians or vehicles driving in a road network. These\nenvironments feature simple dynamics or clear lane markings\nand trafﬁc rules. However, for vehicles driving in a parking\nlot, we are faced with the following challenges:\n1) There is no strict enforcement of trafﬁc rules regarding\nlane directions and boundaries.\n2) The vehicles need to perform complex parking maneu-\nvers to drive in and out of the parking spots.\n3) There are few public datasets for the motion of ve-\nhicles in parking lots. The existing ones such as\nCNRPark+EXT [10] and CARPK [11] are only for\ncar detection in images and do not provide continuous\ntrajectories in ground coordinates.\nThe ParkPredict [12] work addresses the vehicle behavior\nprediction problem in parking lots by using LSTM and CNN\nnetworks. However, in [12] the driving data was collected\nusing a video game simulator to model just a single vehicle.\nAlso, the problem formulation was restricted to a speciﬁc\nglobal map and could not be generalized. In this work, we\nare presenting ParkPredict+, an extensible approach that gen-\nerates multimodal intent and trajectory prediction with CNN\nand Transformer. We also present the Dragon Lake Parking\n(DLP) dataset, the ﬁrst human driving dataset in a parking\nlot environment. We offer the following contributions:\n1) We propose a CNN-based model to predict the proba-\nbilities of vehicle intents in parking lots. The model is\nagnostic to the global map and the number of intents.\n2) We propose a Transformer and CNN-based model to\npredict the future vehicle trajectory based on intent, im-\nage, and trajectory history. Multimodality is achieved\nby coupling the model with the top-k intent prediction.\n3) We release the DLP dataset and its Python toolkit for\nautonomous driving research in parking lots.\nThe paper is organized as follows: Section II formulates\nthe multimodal intent and trajectory prediction problem,\nSection III elaborates on the model design of ParkPredict++,\nSection IV discusses the dataset, experiment setting, and\nresults, and ﬁnally Section V concludes the paper.\nII. P ROBLEM FORMULATION\nWe aim to generate multimodal intent and trajectory\npredictions for vehicles driving in parking lots.\narXiv:2204.10777v2  [cs.CV]  10 Jan 2023\n(a) Ntail = 0\n (b) Ntail = 10.\nFig. 1: Rasterized BEV images with different fading tails.\nA. Inputs\nWe make predictions based on two types of input:\n1) Trajectory history: Denote the current time step as\n0, the trajectory history Zhist = {z(t)}0\nt=−(Nhist−1) ∈\nRNhist×3 is the sequence of target vehicle states z(t) =\n(x(t),y(t),ψ(t)) ∈R3 sampled backward in time from 0\nwith horizon Nhist and step interval ∆t. For convenience of\nnotation, we denote the index set of history time steps by\nThist = {−(Nhist −1),..., 0}.\nTo obtain better numerical properties and generalize the\nmodel to different maps, all states mentioned in this paper\nare local with respect to the vehicle body frame at t = 0,\ntherefore indicating that z(0) ≡(0,0,0).\n2) Local Contextual Information: Similarly to [5], we\ngenerate a rasterized bird’s eye view (BEV) image of size\nn×nand resolution rthat encodes the local environment and\nneighboring agents of the target vehicle. This image reﬂects\nthe farthest sensing limit L of the target vehicle in four\ndirections to make decisions. Each color represents a distinct\ntype of objects. As shown in Fig. 1a, the driving lanes are\nplotted in gray, static obstacles in blue, open spaces in green,\nthe target vehicle in red, and other agents in yellow.\nSince we are interested in representing the local context\nof the target vehicle, we always position the target vehicle\nat the center pixel (n/2,n/2) and rotate the image so that\nthe vehicle always faces east. Denoting the image at time t\nas I(t), along the same horizon Thist, we obtain the image\nhistories as I= {I(t)|t∈Thist}∈ RNhist×n×n.\nThe agents’ motion histories can also be encoded in a\nsingle image by plotting polygons with reduced level of\nbrightness, resulting in a “fading tail“ behind each agent. By\nsetting the length of the fading tail Ntail, a longer history up\nto t= −(Nhist + Ntail −1) can be encoded implicitly in I.\nFig. 1b shows a BEV image with Ntail = 10.\nNote that both the trajectory and image inputs can be\nconstructed by on-board sensors such as LiDAR. Therefore\nthe model is adaptable to different environments without the\nglobal map or Vehicle to Everything (V2X) access.\nB. Outputs\nThe outputs are the probability distributions over intents\nand future trajectory sequence:\n1) Intent: We deﬁne the intent to be a location η= (x,y)\nin the local context that the vehicle is “aiming at“ in the long\nterm. The vehicle does not need to reach η at the end of the\nFig. 2: The orange stars indicate the detected possible intents,\nincluding the center of 4 empty spots and 3 lanes to drive away.\nprediction horizon, but the effort to approach it would be\nimplicitly contained along the trajectory.\nIn this work, we are interested in two types of intents\naround the target vehicle: empty parking spots and lanes.\nWe assume the existence of certain algorithms to detect\nall possible intents through BEV or other on-board sensor\noutputs, as shown in Fig. 2. Given Ms empty spots and\nMd lanes, the intent prediction will output a probability\ndistribution for the M = Ms + Md detected intents\nˆp=\n[\np[1]\ns ,...,p [Ms]\ns ,p[1]\nd ,...,p [Md]\nd\n]\n∈∆M−1 (1)\nwhere p[i]\ns ,i ∈Ns = {1,...,M s}represents the probability\nthat the vehicle will choose the i-th parking spot centered\nat η[i]\ns = ( x[i]\ns ,y[i]\ns ), and p[j]\nd ,j ∈ Nd = {1,...,M d}\nrepresents the probability that the vehicle will bypass all\nspots and continue driving along the j-th lane towards η[j]\nd =\n(x[j]\nd ,y[j]\nd ), which is at the boundary of its sensing limit.\n2) Future trajectories: The trajectory output Zpred =\n{ˆz(t)}Npred\nt=1 ∈RNpred×3 is the sequence of future vehicle\nstates along the prediction horizon Npred.\nAs to be discussed in Section III-B, the trajectory predic-\ntion model takes the intent as input, so given a probability\np[m] of choosing intent η[m],m ∈{1,...,M }, the predicted\ntrajectory Z[m]\npred is also distributed with probability p[m].\nIII. P ARK PREDICT +\nOur model has two main components: 1) a Convolutional\nNeural Network (CNN) model to predict vehicle intent based\non local contextual information, and 2) a CNN-Transformer\nmodel to predict future trajectory based on the trajectory\nhistory, image history, and the predicted intent.\nA. Intent Prediction\nWe ﬁrst consider a distribution\n˜p=\n[\np[1]\ns ,...,p [Ms]\ns ,p−s\n]\n∈∆Ms\nwhere there are only the probabilities of choosing the spots\n1,...,M s and bypassing all of them (denoted p−s). Com-\npared to Eq. (1), it is intuitive to have p−s = ∑Md\nj=1 p[j]\nd .\nGiven the i-th spot intent η[i]\ns = (x[i]\ns ,y[i]\ns ), we can gener-\nate a spot-speciﬁc image I[i](0) by painting the correspond-\ning spot a different color in the BEV image I(0), as shown\nin Fig. 3. We could also generate supplementary features for\nIntent \nPredict\nCNN\nBEV Image with \ni-th spot painted\nDistance to the i-th spot\nAngle to the i-th spot\nFlatten\nFeed forward\nScore\nFig. 3: Mapping f constructed by CNN and feed forward layers.\nThe i-th spot is painted in purple on image I[i](0).\nthis intent, such as the distance to it ∥η[i]\ns ∥, and the difference\nof heading angle |∆ψ[i]|= |arctan2\n(\ny[i]\ns ,x[i]\ns\n)\n|.\nDenoting by f : Rn×n ×R ×R →[0,1] a mapping that\nassigns a “score“ to an intent, we can compute the scores\nfor all Ms spot intents as\nˆs[i] = f\n(\nI[i](0),∥η[i]\ns ∥,|∆ψ[i]|\n)\n,i ∈Ns. (2)\nWithout painting any particular spot on I(0) and setting the\ndistance and angle to 0, the score for bypassing all spots is\nˆs[0] = f(I(0),0,0) . (3)\nGetting a higher score means that the corresponding intent\nis more likely.\nWe train a CNN based model as the mappingf. The model\narchitecture is demonstrated in Fig. 3. The loss function is\nbinary cross-entropy (BCE) between the predicted score ˆs[i]\nand the ground truth label s[i]\ngt\nℓ= −\n[\ns[i]\ngt log ˆs[i] +\n(\n1 −s[i]\ngt\n)\nlog\n(\n1 −ˆs[i]\n)]\n, (4)\nwhere s[i]\ngt = 1 if the vehicle chooses the i-th spot, and s[i]\ngt =\n0 if not.\nThe probability outputs are normalized scores\np[i]\ns = ˆs[i]\n∑Ms\nj=0 ˆs[j]\n,i ∈Ns, (5a)\np−s = ˆs[0]\n∑Ms\nj=0 ˆs[j]\n. (5b)\nTo split p−s into the probabilities of continuing to\ndrive through different lanes, we generate a set of weights{\nw[j] ∈[0,1]|j ∈Nd\n}\nas an arithmetic sequence and reorder\nthem based on a simple heuristic: an intent η[j]\nd which\nrequires more steering and longer driving distance will have\nlower weight w[j]. Then, the probabilities are split as\np[j]\nd = w[j]\n∑Md\nk=1 w[k]\np−s,j ∈Nd. (6)\nWe would like to highlight here that since f is only a\nmapping from a single intent to its score, the intent prediction\nmodel proposed above is invariant to the number of detected\nintents Ms and Md in the local context.\nEncoder\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head \nAttention\nFeed Forward\nConcatenation\nBase CNN\nAdd & Norm\nFeed Forward\nAdd & Norm\nMasked \nMulti-Head \nAttention\nEncoder \nMulti-Head \nAttention\nFeed Forward\n(Shifted Right)\nAdd & Norm\nIntent\nMulti-Head \nAttention\nAdd & Norm\nFeed Forward\nIntent\nLinear\nTrajectory \nPrediction\nor\nDecoder\nFig. 4: Trajectory prediction model based on Transformer.\nB. Trajectory Prediction\nWe leverage the Multi-Head Attention mechanism and\nTransformer architecture [6] to construct the trajectory pre-\ndiction model F: RNhist×3 ×RNhist×n×n ×R2 →RNpred×3,\nwhich predicts future trajectory Zpred with the trajectory\nhistory Zhist, image history Ihist, and intent η. The model\nstructure is illustrated in Fig. 4 and will be elaborated below.\n1) Positional Encoding: As pointed out in [6], the Atten-\ntion mechanism does not contain recurrence or convolution,\nso we need to inject unique position encoding to inform the\nmodel about the relative position of data along the horizon.\nThe positional encoding mask PE is calculated as sinusoidal\nwaves [13]:\nPEt,i =\n\n\n\nsin\n( t\n10000i/D\n)\n,i = 2k\ncos\n( t\n10000(i−1)/D\n)\n,i = 2k+ 1\n(7)\nwhere t denotes the time step along the input horizon, D\ndenotes the model dimension, i∈{1,...,D }.\n2) Transformer Encoder: The image history I is ﬁrst\nprocessed by a base CNN network g : Rn×n →Rdimg to\nencode contextual information: Xin\nimg(t) = g(I(t)),t ∈Thist.\nSubsequently the processed image history is concatenated\nwith the trajectory history, projected to the model dimension\nD with a linear layer Φen : Rdimg+3 →RD, and summed\nwith the positional encoding as in Eq. (7):\nXin\nen(t) = Φen\n[\nConcat\n(\nXin\nimg(t), z(t)\n)]\n⊕PEt, t∈Thist (8)\nWe then apply a classical Transformer Encoder Fen :\nRNhist×D →RNhist×D that consists of nhead self-attention\nlayers and a fully connected layer: Xout\nen = Fen(Xin\nen).\nResidual connections are employed around each layer.\n3) Intent Embedding: Given a certain intent η ∈R2, we\napply a fully connected layer Φit : R2 →RD to embed it as\nlatent state for the Transformer Decoder: Xout\nit = Φit(η).\nWe use the ground truth intent to train the model, and\nat run time we obtain multimodal trajectory predictions by\npicking intents with high probabilities.\n4) Transformer Decoder: The decoder predicts the future\ntrajectory in an autoregressive fashion. The input of the\ndecoder Xin\nde ∈RNpred×D is the trajectory prediction shifted\none step to the right, together with a linear projection\nand positional encoding as the encoder in Sect. III-B.2.\nThe Masked Multi-Head Attention block also prevents the\ndecoder from looking ahead to future time steps.\nThe Encoder Multi-Head Attention block uses the Trans-\nformer Encoder output Xout\nen ∈RNhist×D as the key ( K)\nand value ( V), and the output of the Masked Multi-Head\nAttention block Xout\nmm ∈ RNpred×D as the query ( Q) to\ncompute cross-attention\nattnen = softmax\n(Xout\nmmXout,⊤\nen√\nD\n)\nXout\nen . (9)\nWe add the third block, Intent Multi-Head Attention, to\ncompute the attention weights using intent so that the ﬁnal\ntrajectory prediction will be affected by intent. Here, we use\nintent embedding Xout\nit ∈RD as key ( K) and value ( V),\nand the output of the previous Multi-Head Attention block\nXout\nma ∈RNpred×D\nattnit = softmax\n(\nXout\nma Xout,⊤\nit√\nD\n)\nXout\nit . (10)\nFinally, we apply fully connected layers at the end to gen-\nerate the trajectory output Zpred = {ˆz(t)}Npred\nt=1 ∈RNpred×3.\nResidual connections are also employed in the decoder.\nThe loss function is L1 loss since 1) the gradient does not\ndecrease as the prediction gets closer to the ground truth,\nand 2) L1 is more resistant to outliers in the dataset:\nIV. E XPERIMENTS\nA. Dataset\nWe collected 3.5 hours of video data by ﬂying a drone\nabove a huge parking lot and named it as Dragon Lake\nParking (DLP) dataset 1. The videos were taken in 4K\nresolution, covering a parking area of 140 m ×80 m with\nabout 400 parking spots (Fig. 5a). With precise annotation,\nwe obtain the dimension, position, heading, velocity, and\nacceleration of all vehicles at 25 fps. Abundant vehicle\n1https://sites.google.com/berkeley.edu/dlp-dataset.\n(a) Annotated video data.\n (b) Rasterized semantic view.\nFig. 5: DLP dataset.\nFig. 6: Interaction-intense scenarios in DLP dataset.\nparking maneuvers and interactions are recorded, as shown\nin Fig. 6. To the best of our knowledge, this is the ﬁrst and\nlargest public dataset designated for the parking scenario,\nfeaturing high data accuracy and a rich variety of realistic\nhuman driving behavior.\nWe are also releasing a Python toolkit which provides\nconvenient APIs to query and visualize data (Fig. 5b and all\nBEV images included in this paper). The data is organized\nin a graph structure with the following components:\n1) Agent: An agent is an object that has moved in this\nscene. It contains the object’s type, dimension, and\ntrajectory as a list of instances.\n2) Instance: An instance is the state of an agent at a time\nstep, which includes position, orientation, velocity,\nand acceleration. It also points to the preceding /\nsubsequent instance along the agent’s trajectory.\n3) Frame: A frame is a discrete sample from the record-\ning. It contains a list of visible instances at this time\nstep, and points to the preceding / subsequent frame.\n4) Obstacle: Obstacles are vehicles that never move in\nthis recording.\n5) Scene: A scene represents a consecutive video record-\ning with certain length. It points to all frames, agents,\nand obstacles in this recording.\nThe entire DLP dataset contains 30 scenes, 317,873 frames,\n5,188 agents, and 15,383,737 instances.\nIn this work, we are using the sensing limit L= 10 m and\nresolution r= 0.1 m/pixel, so that the semantic BEV image\nI is of size 200×200. The sampling time interval ∆t= 0.4s,\nNhist = Ntail = Npred = 10. In other words, there are total\nof 8s’ information history encoded in the inputs and we are\npredicting the vehicle trajectory over the next 4s.\nAfter data cleaning, ﬁltering, and random shufﬂing, we\nobtain a training set of size 51750 and a validation set of\nsize 5750. The models are trained 2 on an Alienware Area\n2 https://github.com/XuShenLZ/ParkSim/ .\nTABLE I: Hyperparameters of the Intent Prediction Model\nTypes Parameters\nConv2d →BatchNorm 8 ×(7 ×7) →8\nDropout →LeakyReLU →MaxPool 0.2 →0.01 →2\nConv2d →BatchNorm 8 ×(5 ×5) →8\nDropout →LeakyReLU →MaxPool 0.2 →0.01 →2\nConv2d →BatchNorm 3 ×(3 ×3) →3\nDropout →LeakyReLU →MaxPool 0.2 →0.01 →2\nLinear 6629 ×100\nLinear & Sigmoid 100 ×1\nFig. 7: Top-k accuracy of intent prediction\n51m PC with 9th Gen Intel Core i9-9900K, 32GB RAM,\nand NVIDIA GeForce RTX 2080 GPU.\nSince most well-known motion prediction benchmarks\nare not for the parking scenario, we choose the physics-\nbased EKF model presented in the ParkPredict [12] paper as\nour baseline. The CNN-LSTM approach requires a different\nglobal map encoding thus cannot establish a fair comparison.\nB. Intent Prediction\n1) Hyperparameters: The hyperparameters of the intent\nprediction model in Fig. 3 are outlined in Table. I. We use\nthe Adam optimizer with learning rate 0.001 and stop early\nat convergence.\n2) Evaluation Metrics: We use the top- k accuracy to\nevaluate our model: Let the set N(i)\nk ⊆ {1,...,M (i)}\ninclude the k most likely intent indices in the i-th predicted\ndistribution ˆp(i) and the ground truth intent be at index l(i),\nthen the top- k accuracy Ak is computed as\nAk = 1\nMD\nMD∑\ni=1\nI\n(\nl(i) ∈N(i)\nk\n)\n.\nThe variable MD here corresponds to the cardinality of the\nvalidation set and I(·) is the indicator function.\n3) Results: Fig. 7 shows the Top-k prediction accuracy up\nto k = 5. The proposed method outperforms EKF methods\nfor all values of k and achieves almost 100% accuracy in\ntop-3 results, which means we can reliably cover the ground\ntruth intent when making multimodal predictions.\nC. Trajectory Prediction\n1) Hyperparameters: We use the same Convolutional\nlayers as Table. I for the Base CNN in trajectory prediction\nmodel. The subsequent feed forward layers reshape the\n(a) Positional displacement.\n (b) Difference of heading angle.\nFig. 8: Trajectory prediction error vs time step. Blue line represents\nthe proposed (complete) Transformer model, red and yellow lines\nrepresent removing the intent and image inputs respectively, and\ngreen line represents the EKF model. The 95% conﬁdence intervals\nare plotted as the shaded regions.\ninputs Xin\nen and Xin\nde to the model dimension D = 52. We\nconstruct 16 encoder layers, 8 decoder layers, and 4 heads\nfor all Multi-Head Attention blocks. The dropout rates are\n0.14 for all blocks. We use the SGD optimizer with learning\nrate 0.0025 and stop early at convergence.\n2) Evaluation Metrics: Given the i-th unimodal prediction\nZ(i)\npred =\n{\nˆz(i)(t) =\n(\nˆx(i)(t),ˆy(i)(t), ˆψ(i)(t)\n)}Npred\nt=1\ncorre-\nsponding to the ground truth intent, we compute the mean po-\nsition error ep(t) and mean heading error ea(t) to the ground\ntruth label\n{\nz(i)\ngt (t) =\n(\nx(i)\ngt (t),y(i)\ngt (t),ψ(i)\ngt (t)\n)}Npred\nt=1\nas a\nfunction of the time step t:\nep(t) = 1\nMD\nMD∑\ni=1\n\n[\nˆx(i)(t) −x(i)\ngt (t),ˆy(i)(t) −y(i)\ngt (t)\n]⊤\n2\n,\nea(t) = 1\nMD\nMD∑\ni=1\n|ˆψ(i)(t) −ψ(i)\ngt (t)|.\n3) Results: We report the performance of our trajectory\nprediction model and the results of an ablation study in\nFig. 8. For positional displacement error in Fig. 8a, we can\nsee that our model outperform the EKF approach [12] in\nboth short- and long-term. However, by removing the intent\ninformation, the model performs almost the same as the EKF,\nand by removing the image, the performance is severely\nworsened. The error of heading angle in Fig. 8b has less\ndifference among models but reﬂects the same trend. The\ncomparison results demonstrate that the model learns a lot\nof information from image and intent inputs to generate\npredictions with high accuracy, especially in the long-term.\nD. Case Study\nFig. 9 presents our prediction for four representative sce-\nnarios in the parking lot. The model generates accurate tra-\njectory predictions when compared to the ground truth, while\nproviding other possible behaviors according to the local\ncontext. Video link: https://youtu.be/rjWdeuvRXp8.\nIn Fig. 9a, the vehicles have already slowed down and\nstarted steering towards parking spots. In the left example, it\nis still hard to tell the actual intention of the vehicle, therefore\n(a) Multi-modal prediction in the middle of parking maneuver.\n(b) Multi-modal prediction while cruising with high speed.\nFig. 9: Examples of multimodal prediction. The top-3 modes are\nvisualized by orange, green, and purple. The stars and the text close\nby indicate the predicted intents and their probabilities. The white\ntrajectories are the ground truth future trajectories.\nthe top-3 intents have similar probabilities. Among them, the\nmode marked with green matches the ground truth to steer\ninto the corresponding spot. At the same time, the mode\nin purple describes that the vehicle might continue driving\ntowards the intersection. The mode in orange represents the\ncase that the vehicle might prepare for a reverse parking\nmaneuver. The trajectory prediction in orange is so short\nthat it is mostly occluded by the other two modes, indicating\nthe need for the car to slow down before backing up. The\nscenario is simpler in the right example, where the model\nbelieves with high probability that the vehicle is backing up\ninto the spot marked by purple intent.\nFig. 9b shows two other scenarios in which the vehicles\nare still driving with relatively high speeds (indicated by the\nlength of the fading tail). In the left example, the model\nﬁrstly predicts the most likely spot marked in orange and\nthe resulting trajectory prediction matches the ground truth.\nSince there is an intersection in front, it is possible that the\nvehicle will continue driving across it. The model predicts\nthis behavior with the modes in green and purple. The\nexample on the right shows a situation where the vehicle\nneeds to turn right at the intersection. The mode in purple\nsuccessfully predicts it according to the local context. Since\nno empty spot is visible, the model assign some probabilities\nto the other two intents along the lanes, but with lower\nvalue. The orange and green trajectories also try to reach\nthe corresponding intents by slowing the vehicle down.\nV. C ONCLUSION\nIn this work, we investigate the problem of multimodal\nintent and trajectory prediction for vehicles in parking lots.\nCNN and Transformer networks are leveraged to build the\nprediction model, which take the trajectory history and\nvehicle-centered BEV images as input. The proposed model\ncan be ﬂexibly applied to various types of parking maps\nwith an arbitrary number of intents. The result shows that the\nmodel learns to predict the intent with almost 100% accuracy\nin top-3 candidates, and generates multimodal trajectory\nrollouts. Furthermore, we release the DLP dataset with its\nsoftware kit. As the ﬁrst human driving dataset in parking lot,\nit can be applied to a wide range of prospective applications.\nACKNOWLEDGMENT\nWe would like to thank Michelle Pan and Dr. Vijay\nGovindarajan for their contribution in building DLP dataset.\nREFERENCES\n[1] S. Lef `evre, D. Vasquez, and C. Laugier, “A survey on motion pre-\ndiction and risk assessment for intelligent vehicles,” ROBOMECH\nJournal, vol. 1, no. 1, p. 1, Dec. 2014.\n[2] K. Leung, E. Schmerling, M. Zhang, M. Chen, J. Talbot, J. C. Gerdes,\nand M. Pavone, “On infusing reachability-based safety assurance\nwithin planning frameworks for human–robot vehicle interactions,”\nInternational Journal of Robotics Research , vol. 39, no. 10-11, pp.\n1326–1345, 2020.\n[3] Y . Ma, X. Zhu, S. Zhang, R. Yang, W. Wang, and D. Manocha, “Traf-\nﬁcPredict: Trajectory Prediction for Heterogeneous Trafﬁc-Agents,”\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33,\npp. 6120–6127, July 2019.\n[4] A. Alahi, K. Goel, V . Ramanathan, A. Robicquet, L. Fei-Fei, and\nS. Savarese, “Social LSTM: Human Trajectory Prediction in Crowded\nSpaces,” in 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), vol. 81. IEEE, June 2016, pp. 961–971.\n[5] N. Djuric, V . Radosavljevic, H. Cui, T. Nguyen, F.-C. Chou, T.-H.\nLin, and J. Schneider, “Short-term Motion Prediction of Trafﬁc Actors\nfor Autonomous Driving using Deep Convolutional Networks,” Aug.\n2018.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nProceedings of the 31st International Conference on Neural Informa-\ntion Processing Systems, ser. NIPS’17. Red Hook, NY , USA: Curran\nAssociates Inc., Dec. 2017, pp. 6000–6010.\n[7] L. L. Li, B. Yang, M. Liang, W. Zeng, M. Ren, S. Segal, and\nR. Urtasun, “End-to-end contextual perception and prediction with\ninteraction transformer,” IEEE International Conference on Intelligent\nRobots and Systems , pp. 5784–5791, 2020.\n[8] A. Quintanar, D. Fernandez-Llorca, I. Parra, R. Izquierdo, and M. A.\nSotelo, “Predicting vehicles trajectories in urban scenarios with trans-\nformer networks and augmented information,” IEEE Intelligent Vehi-\ncles Symposium, Proceedings , vol. 2021-July, no. Iv, pp. 1051–1056,\n2021.\n[9] F. Giuliari, I. Hasan, M. Cristani, and F. Galasso, “Transformer\nnetworks for trajectory forecasting,” Proceedings - International Con-\nference on Pattern Recognition , pp. 10 335–10 342, 2020.\n[10] G. Amato, F. Carrara, F. Falchi, C. Gennaro, and C. Vairo, “Car\nparking occupancy detection using smart camera networks and deep\nlearning,” in Computers and Communication (ISCC), 2016 IEEE\nSymposium on. IEEE, 2016, pp. 1212–1217.\n[11] M.-R. Hsieh, Y .-L. Lin, and W. H. Hsu, “Drone-based object counting\nby spatially regularized regional proposal network,” in Proceedings\nof the IEEE international conference on computer vision , 2017, pp.\n4145–4153.\n[12] X. Shen, I. Batkovic, V . Govindarajan, P. Falcone, T. Darrell, and\nF. Borrelli, “ParkPredict: Motion and Intent Prediction of Vehicles\nin Parking Lots,” in 2020 IEEE Intelligent Vehicles Symposium (IV) .\nIEEE, Oct. 2020, pp. 1170–1175.\n[13] Z. Sui, Y . Zhou, X. Zhao, A. Chen, and Y . Ni, “Joint Intention and\nTrajectory Prediction Based on Transformer,” pp. 7082–7088, 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7751456499099731
    },
    {
      "name": "Trajectory",
      "score": 0.6814806461334229
    },
    {
      "name": "Transformer",
      "score": 0.6209862232208252
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5075958967208862
    },
    {
      "name": "Encoding (memory)",
      "score": 0.46981340646743774
    },
    {
      "name": "Real-time computing",
      "score": 0.40335339307785034
    },
    {
      "name": "Computer vision",
      "score": 0.38934290409088135
    },
    {
      "name": "Machine learning",
      "score": 0.36694857478141785
    },
    {
      "name": "Engineering",
      "score": 0.11728796362876892
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}