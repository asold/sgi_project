{
  "title": "End-to-End Trainable Multi-Instance Pose Estimation with Transformers",
  "url": "https://openalex.org/W3139517252",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287475869",
      "name": "Stoffl, Lucas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287475870",
      "name": "Vidal, Maxime",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222629804",
      "name": "Mathis, Alexander",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3184564979",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2948527806",
    "https://openalex.org/W2578797046",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3081605501",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2032481801",
    "https://openalex.org/W3083835029",
    "https://openalex.org/W2559085405",
    "https://openalex.org/W3112160422",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W3117707723",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W2509865052",
    "https://openalex.org/W2962773068",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W3034750257",
    "https://openalex.org/W3135454126",
    "https://openalex.org/W2962954622",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2993728126",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2620105270",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3117784098",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3034399482",
    "https://openalex.org/W2949962589",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2382036597",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2555751471",
    "https://openalex.org/W3000322757",
    "https://openalex.org/W2962820842",
    "https://openalex.org/W2080873731",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2746314669"
  ],
  "abstract": "We propose an end-to-end trainable approach for multi-instance pose estimation, called POET (POse Estimation Transformer). Combining a convolutional neural network with a transformer encoder-decoder architecture, we formulate multiinstance pose estimation from images as a direct set prediction problem. Our model is able to directly regress the pose of all individuals, utilizing a bipartite matching scheme. POET is trained using a novel set-based global loss that consists of a keypoint loss, a visibility loss and a class loss. POET reasons about the relations between multiple detected individuals and the full image context to directly predict their poses in parallel. We show that POET achieves high accuracy on the COCO keypoint detection task while having less parameters and higher inference speed than other bottom-up and top-down approaches. Moreover, we show successful transfer learning when applying POET to animal pose estimation. To the best of our knowledge, this model is the first end-to-end trainable multi-instance pose estimation method and we hope it will serve as a simple and promising alternative.",
  "full_text": "End-to-End Trainable Multi-Instance Pose\nEstimation with Transformers\nLucas Stofﬂ Maxime Vidal Alexander Mathis\nEcole Polytechnique Fédérale de Lausanne (EPFL); CH-1015 Lausanne, Switzerland\nalexander.mathis@epfl.ch\nWe propose an end-to-end trainable approach for multi-\ninstance pose estimation, called POET (POse Estimation Trans-\nformer). Combining a convolutional neural network with a\ntransformer encoder-decoder architecture, we formulate multi-\ninstance pose estimation from images as a direct set prediction\nproblem. Our model is able to directly regress the pose of all\nindividuals, utilizing a bipartite matching scheme. POET is\ntrained using a novel set-based global loss that consists of a\nkeypoint loss, a visibility loss and a class loss. POET reasons\nabout the relations between multiple detected individuals and\nthe full image context to directly predict their poses in parallel.\nWe show that POET achieves high accuracy on the challeng-\ning COCO keypoint detection task while having less parame-\nters and higher inference speed than other bottom-up and top-\ndown approaches. Moreover, we show successful transfer learn-\ning when applying POET to animal pose estimation. To the best\nof our knowledge, this model is the ﬁrst end-to-end trainable\nmulti-instance pose estimation method and we hope it will serve\nas a simple and promising alternative.\nIntroduction\nMulti-instance pose estimation from a single image, the task\nof predicting the body part locations for each individual, is\nan important computer vision problem. It has wide ranging\napplications from measuring behavior in health care and biol-\nogy to virtual reality and human-computer interactions (1–5).\nMulti-human pose estimation can be thought of as a hi-\nerarchical set prediction task. An algorithm needs to pre-\ndict the bodyparts of all individuals and group them correctly\ninto humans, i.e., it needs to predict a set of (bodypart) sets\n(per individual). Due to the complexity of this process, cur-\nrent methods consist of multiple steps and are not end-to-end\ntrainable. Fundamentally, top-down and bottom-up methods\nare the major approaches. Top-down methods ﬁrst predict\nthe location (bounding boxes) of all individuals based on an\nobject detection algorithm and then predict the location of\nall the bodyparts per cropped individual with a separate net-\nwork (6–9) – thus one needs n+ 1 network evaluations per\nimage with n individual (detections). Bottom-up methods\nﬁrst predict all the bodyparts, and then group them into indi-\nviduals with various techniques (10–17). Hence, both ap-\nproaches require either two different networks or complex\npost-processing. This motivates the search for end-to-end so-\nlutions.\nInspired by DETR (18), a recent transformer-based ar-\nchitecture for object detection, we propose a novel end-\n1 2 3 4 5 6 7 8 9 10 11 12 13\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nNumber of people per image\nFrames per second (FPS)\nAE+R50\nAE+HRNet\nOpenPose\nTop-Down\nTop-Down w/o bb det\nPOET\nFig. 1. Inference speed comparison between AE+R50 (Bottom-Up), AE+HRNet\n(strong Bottom-Up), OpenPose, Mask R-CNN + R50 Simple Baseline (Top-Down),\nthe same Top-Down baseline with provided bounding boxes, and POET. POET’s\ninference speed is invariant to the number of people in an image and outperforms\nprevious multi-instance pose estimation methods. Tests were performed on the\nCOCO-val set with a batch size of one on a single Nvidia TITAN RTX.\nto-end trainable method for multi-instance pose estimation.\nPOse Estimation Transformer (POET) is the ﬁrst model\nthat is trained end-to-end for multi-instance pose estima-\ntion, without the need for any post-processing or two net-\nworks as typically employed in top-down approaches. POET\nis trained with a novel, yet simple loss function, which al-\nlows bipartite matching between predicted and ground-truth\nhuman poses. Our approach achieves strong results on the\nCOCO keypoint challenge (19), especially for large humans,\nand performs better than a baseline model even with higher\nspatial resolution. Analyses of the transformer attentions and\nof the decoder depth give insight into how POET tackles the\nproblem of multi-instance pose estimation, while extensive\nloss ablations show the importance of computing an adapt-\nable center that is successfully learned by POET. Moreover,\nwe show that POET also achieves strong performance on a\nmuch smaller dataset for animal pose estimation (Macaque-\nPose (20)) for which the same COCO-deﬁned keypoints have\nbeen annotated. We show that the transformer weights can be\ntransferred from COCO to MacaquePose, yielding even bet-\nter performance. At last, POET shows excellent inference\nspeed, being faster than various light-weight bottom-up and\ntop-down methods (Figure 1).\nStofﬂ et al. | December 22, 2021 | 1–11\narXiv:2103.12115v2  [cs.CV]  21 Dec 2021\nFig. 2. Overview of our model. a) POET combines a CNN backbone and a transformer to directly predict the pose of multiple humans. b) Each pose is represented as\na vector comprising the center (xc,yc), the relative offset (∆ xi,∆ yi) of each bodypart iand its visibility vi. c) POET is trained end-to-end by bipartite matching of the\nclosest predictions to the ground truth pose, and then backpropagating the loss.\nRelated Work\nTransformers in Vision and Beyond: Transformers were\nﬁrst introduced for machine translation (21), and have vastly\nimproved the performance of deep learning models on lan-\nguage tasks (21–23). Their architecture inherently allows\nmodeling and discovering long-range interactions in data\nand their use has recently been extended to speech recogni-\ntion (24), automated theorem proving (25), and many other\ntasks (26). In computer vision, transformers have been used\nwith great effect either in combination or as an alternative to\nconvolutional neural networks (CNNs) (26). Notably, Visual\nTransformer (ViT) (27) demonstrated state-of-the-art perfor-\nmance on image recognition tasks with pure transformer\nmodels. In other visual tasks, such as text-to-image, excel-\nlent results have been shown, e.g., by DALL-E (28).\nRecently Carion et al. (18) developed a new end-to-\nend paradigm for visual object detection with transformers,\na task which previously required either two-stage approaches\nor post-processing. This approach, DETR, formulated ob-\nject detection as a set prediction problem combined with a\nbipartite matching loss. DETR is an elegant solution, how-\never, the model requires long training times and shows com-\nparatively low performance on small objects (18). These\nproblems were mitigated through further works; Deformable\nDETR (29) presents a multi-scale deformable attention mod-\nule which only attends to a set number of points within the\nfeature map, for different scales, and in this way reducing the\ntraining time and improving small object detection perfor-\nmance. Sun et al. removed the transformer decoder and fed\nthe features coming out of the CNN backbone to a Feature\nPyramid Network (30).\nImportantly, end-to-end approaches were successfully\napplied in many complex prediction tasks such as speech\nrecognition or machine translation (22, 23), but are still lack-\ning in multi-instance pose estimation.\nPose Estimation is a classical computer vision problem\nwith wide ranging applications (1, 3, 4, 31). Pose estimation\nmethods are evaluated on several benchmarks for 2D multi-\nhuman pose estimation, incl. COCO (3, 19, 32, 33)\nMulti-instance pose estimation methods can be classi-\nﬁed as top-down and bottom-up (3, 4, 31). Top-down meth-\nods predict the location of each bodypart of each individ-\nual, based on bounding boxes localizing individuals with\na separate network (6–9, 35, 39, 40). Bottom-up methods\nﬁrst predict all the bodyparts, and then group them into in-\ndividuals by using part afﬁnity ﬁelds (10), pairwise predic-\ntions (11, 12, 15, 41, 42), composite ﬁelds (14, 17), or asso-\nciative embeddings (13, 16). Both top-down and bottom-up\napproaches, require either post-processing steps (for assem-\nbly) or two different neural networks (for localization and\nthen pose estimation). Thus, these multi-stage approaches\nare prone to errors and typically show poor inference speed.\nMost recent (state-of-the-art) approaches are fully con-\nvolutional and predict keypoint heatmaps (e.g., (7, 8, 10, 13,\n16)). Recently, Yang et al. proposed TransPose, a top-down\nmethod, which predicts heatmaps as well, but by using atten-\ntion after a CNN encoder (9). Mao et al. applies a top-down\ntransformer architecture with the difference that it is directly\n2 Stofﬂ et al. | POET\n0 50 1001502002500\n0.2\n0.4\n0.6 class loss\n0 50 1001502002500\n1\n2\n3\n4\n5 (abs) keypoint loss\n0 50 1001502002500.2\n0.3\n0.4\n0.5\n0.6\n0.7 visibility loss\n0 50 100150200250\n1\n2\n3\n4\n5\n6·10−3\ncenter loss\n0 50 100150200250\n5·10−2\n0.1\n0.15\n0.2\n0.25 delta loss\nFig. 3. Evolution of the different loss parts over training epochs (Equation 4) for POET -R50. Solid lines correspond to training losses, and dashed lines to validation losses\n(on COCO).\nFig. 4. Example predictions on COCO-eval with POET -R50 model listed in Table 1. Top row: examples with good performance. Bottom row: examples with errors.\nTable 1. Comparison to state-of-the-art models on COCO test-dev. Note that most models use an overall stride of 4 (or smaller), when extracting features, while POET uses\nstride 32 and therefore much smaller feature maps which harms performance on APM . However, with regard to APL it can compete with state-of-the-art models.\nMethod Stride #Params AP AP 50 AP75 APM APL AR AR 50 AR75 ARM ARL\nTop-Down\nMask-RCNN (34) 4 44.4M 63.1 87.3 68.7 57.8 71.4 - - - - -\nCPN (35) - - 72.1 91.4 80.0 68.7 77.2 78.5 95.1 85.3 74.2 84.3\nHRNet-W48 (7, 36) 4 63.6M 75.5 92.5 83.3 71.9 81.5 80.5 - - - -\nTansPose-H-A6 (9) 4 17.5M 75.0 92.2 82.3 71.3 81.1 - - - - -\nBottom-Up\nOpenPose (10) 4 - 61.8 84.9 67.5 57.1 68.2 - - - - -\nHourglass (13) 4 277.8M 56.6 81.8 61.8 49.8 67.0 - - - - -\nPersonLab (37) 8 68.7M 66.5 88.0 72.6 62.4 72.3 71.0 90.3 76.6 66.1 77.7\nAE + R50 (38) 4 31.9M 46.6 74.2 47.9 44.6 49.3 55.2 79.7 57.5 48.1 65.1\nPifPaf (14) 4 - 66.7 - - 62.4 72.9 - - - - -\nHigherHRNet (16) 2 63.8M 68.4 88.2 75.1 64.4 74.2 - - - - -\nPOET-R50 (Ours) 32 41.3M 53.6 82.2 57.6 42.5 68.1 61.4 87.8 65.5 50.8 75.7\nregresses the poses (for single persons) (43). A regression-\nbased method is also proposed by the authors of (44) building\na two-stage network consisting of a person-detection trans-\nformer (pre-trained DETR (18)) and a keypoint-detection\ntransformer. Transformers were also used for 3D pose and\nmesh reconstruction on (single) humans, which achieved\nstate-of-the-art on Human3.6M (45, 46). Extending this\nwork, we build on DETR (18), to propose an end-to-end\ntrainable pose estimation method for multiple instances that\ndirectly outputs poses as vectors (without relying on any\nheatmaps). To cast pose estimation as a hierarchical set pre-\ndiction problem, we adapt the pose representations of Center-\nNet (42) and Single-Stage Multi-Person Pose Machines (15).\nThe POET Model\nOur work is closely related to DETR (18) and fundamentally\nextends this object detection framework to multi-instance\npose estimation by introducing a novel loss function and a\npose representation that, combined, can tackle the difﬁculties\nfaced in multi-instance pose estimation, such as non-visible\nbodyparts. POse Estimation Transformer (POET) consists of\ntwo major ingredients: (1) a transformer-based architecture\nthat predicts a set of human poses in parallel (Figure 2a) and\n(2) a set prediction loss that is a linear combination of sim-\nple sub-losses for classes, keypoint coordinates and visibili-\nties. To cast multi-instance pose estimation as a set predic-\nStofﬂ et al. | POET\ntion problem, we represent the pose of each individual as the\ncenter (of mass) together with the relative offsets per body-\npart. Each bodypart can be occluded or visible. POET is\ntrained to directly output a vector comprising the center, rel-\native bodyparts as well as (binary) bodypart visibility indica-\ntors (Figure 2b). The POET architecture contains three main\nelements: a CNN backbone that extracts features of the input\nimages, an encoder-decoder transformer and a pose predic-\ntion head that outputs the set of estimated poses.\nCNN backbone.The convolutional backbone is given a batch\nof images, I ∈RB×3×H×W, as input with batch size B, 3\ncolor channels and image dimensions (H,W). Through sev-\neral computing and downsampling steps, the CNN generates\nlower-resolution feature maps, F ∈RB×C×H/S×W/S with\nstride S. Speciﬁcally, we choose different ResNets (47) with\nvarious strides S, as detailed in the experiments section.\nEncoder-Decoder transformer.The encoder-decoder trans-\nformer model follows the standard design (18, 21). Both en-\ncoder and decoder consist of 6 layers with 8 attention heads\neach. The encoder takes the output features of the CNN back-\nbone, reduces their channel dimensions by a 1 ×1 convo-\nlution and collapses them along the spatial dimension into\none dimension as the multi-head mechanism expects sequen-\ntial input. It contains a ﬁxed positional encoding to the en-\ncoder input, as the transformer architecture is (otherwise)\npermutation-invariant and would disregard the spatial image\nstructure. In contrast, the input embeddings for the decoder\nare learned positional encodings, which we refer to as ob-\nject queries. They are added to the encoder output to form\nthe decoder input. The decoder transforms the queries into\noutput embeddings, which are then taken by the pose pre-\ndiction head and independently decoded into the ﬁnal set of\nposes and class labels. Thereby, every query can search for\none instance and predicts its pose and class. With the aid\nof the self-attention in both the encoder and the decoder, the\nnetwork is able to globally reason about all objects together\nusing pairwise relations between them, and at the same time\nusing the whole image as context information.\nPose prediction head.The N query embeddings at the end\nof the transformer decoder are independently processed by a\nfeedforward network. This pose estimation head consists of\n(1) a 3-layer perceptron with ReLU activation, which trans-\nforms the embeddings into normalized (with respect to the\nimage size) center coordinates, the displacements to all body-\nparts relative to the center and the visibility scores for every\nbody part in a single vector (Figure 2b); and (2) a linear pro-\njection layer predicting the class label using a softmax func-\ntion.\nTraining Loss: In order to predict all poses in parallel, the\nnetwork is trained with the loss after ﬁnding an optimal\nmatching between predictions and ground-truth and sum-\nming over the individuals. Therefore, our loss has to score\npredictions accordingly, with respect to the class, the key-\npoint coordinates and their visibilities.\nFrom the absolute joint coordinates Ai =\n[(x1,y1),(x2,y2),...,(xn,yn)] for every instance\ni in the ground truth we compute the center\nas the center of mass of all visible keypoints.\nThe ground truth vector for individual i is then\n[xc,yc,∆ x1,∆ y1,v1,∆ x2,∆ y2,v2 ...,∆ xn,∆ yn,vn],\nfor center (xc,yc), relative offset (∆ xi,∆ yi) of each body-\npart iand its visibility vi. In order to make the loss functions\nmore legible, we split this vector into yi = (ci,Ci,Zi,Vi),\nwhich consists of the target class label (individuals/non-\nobject) ci, the center Ci = ( xc,yc), the relative pose:\nZi = [∆ x1,∆ y1,∆ x2,∆ y2,...,∆ xn,∆ yn] (relative joint\ndisplacements from the center Ci) and a binary visibility\nvector Vi = [v1,v1,v2,v2,...,vn,vn] encoding for every\njoint in the image, whether it is visible or not.\nThe prediction of the network for instance iis then de-\nﬁned as ˆyi = (ˆp(ci),ˆCi,ˆZi,ˆVi), where ˆp(ci) is the predicted\nprobability for class ci, ˆCi the predicted center, ˆZi the pre-\ndicted pose, and ˆVi the predicted visibility. Note that the\nnetwork does not predict the visibility for the center. By\nadding the predicted offsets to the predicted centers we ob-\ntain the predicted, absolute keypoint positions: ˆAi = [( ˆxc+\nˆ∆ x1,ˆyc+ ˆ∆ y1),( ˆxc+ ˆ∆ x2,ˆyc+ ˆ∆ y2),...,( ˆxc+ ˆ∆ xn,ˆyc+\nˆ∆ yn)].\nIn the following, we denote by ythe ground truth set of\nposes, and ˆy= {ˆyi}N\ni=1 the set of N predictions. Here, y is\nthe set of individuals in the image padded with non-objects.\nWe deﬁne our pair-wise matching cost between ground truth\nyi and a prediction with index σ(i) as:\nLmatch(yi,ˆyσ(i))) = −1{ci̸=∅}ˆpσ(i)(ci)\n+1{ci̸=∅}Lpose(Ci,Zi,Vi,ˆCσ(i),ˆZσ(i),ˆVσ(i)) (1)\nHere, Lpose is the pose-speciﬁc cost that we will deﬁne below\nand involves costs for the centers, the bodyparts and their\nvisibilities.\nThe optimal assignment is then found as a bipartite\nmatching with the lowest matching cost based on the Hun-\ngarian algorithm (18, 48). This assignment is the follow-\ning permutation of N elements σ∈GN for symmetric group\nGN (49):\nˆσ= argmin\nσ∈GN\nN∑\ni\nLmatch(yi,ˆyσ(i)) (2)\nOnce the optimal matching is obtained, we can compute\nthe Hungarian loss for all matched pairs. Like the matching\ncost, it contains a loss part scoring the poses, which is a linear\ncombination of a L1 loss to compute the differences between\nabsolute keypoint coordinates, a L2 loss for the visibilities\nand two regularization terms, namely a L2 loss for the center\ncoordinates and a L1 loss for the keypoint offsets. Thus we\n4 Stofﬂ et al. | POET\nhave coefﬁcients λabs, λvis, λctr and λdeltas:\nLpose(Ci,Zi,Vi,ˆCσ(i),ˆZσ(i),ˆVσ(i)) =\nλabs∥Vi◦Ai−Vi◦ˆAσ(i)∥1\n+λvis∥Vi−ˆVσ(i)∥2\n2\n+λctr∥(xc,yc)i−( ˆxc,ˆyc)σ(i)∥2\n2\n+λdeltas∥Vi◦Zi−Vi◦ˆZσ(i)∥1 (3)\nThereby, ◦denotes point-wise multiplication. These four\nlosses are normalized by the number of ground truth indi-\nviduals per batch.\nThe ﬁnal loss, the Hungarian loss, then is a linear com-\nbination of a negative log-likelihood for class prediction and\nthe keypoint-speciﬁc loss deﬁned above, for all pairs from the\noptimal assignment ˆσ:\nLHungarian(y,ˆy) =\nN∑\ni=1\n[\n−log ˆpˆσ(i)(ci) (4)\n+1{ci̸=∅}Lpose(Ci,Zi,Vi,ˆCˆσ(i),ˆZˆσ(i),ˆVˆσ(i))\n]\nExperiments\nWe evaluated POET on the COCO keypoint estimation chal-\nlenge (19) and MacaquePose (20). We illustrate qualitative\nresults and show that it reaches good performance (espe-\ncially for large individuals). Then we show that it outper-\nforms baseline methods that we trained based on an estab-\nlished bottom-up method using associative embedding with\nthe same backbone (13, 50). Then, we analyze different as-\npects of the architecture and the loss. Finally, we discuss\nchallenges and future work.\nCOCO Keypoint Detection Challenge: The COCO\ndataset (19) comprises more than 200,000 images with more\nthan 150,000 people for which up to 17 keypoints are anno-\ntated. The dataset is split into train/val/test-dev sets with57k,\n5kand 20kimages, respectively. We trained on the training\nimages (that contain humans) and report results on the val-\nidation set for our comparison study and on the test set for\ncomparing to state-of-the-art models. Most COCO images\ncontain only a few annotated humans. To account for this\nclass imbalance, we down-weight the log-probability term in\nEquation 4 by a factor of 10 for all non-objects (we used the\nsame re-weighing for Macaque Pose).\nMacaquePose: The MacaquePose dataset (20) comprises\naround 13,000 images with more than 16,000 macaques for\nwhich the same 17 keypoints as in COCO (19) are annotated.\nWe randomly split the dataset into 80% train and 20% val\nsets. MacaquePose does not provide any bounding box an-\nnotations, that are needed to deﬁne the scaling parameter s\nin the OKS evaluation metric, and therefore we computed\nbounding boxes by ﬁtting boxes around the provided segmen-\ntation masks.\nInference speed analysis: To analyze the inference run-\ntime performance of the methods, we resize all images of the\nCOCO-validation dataset to 512 ×512 and run inference on\nthem. The runtime analysis is carried out on one Nvidia TI-\nTAN RTX with a batch size of 1. We calculate the average\nruntime for all images that contain the same amount of people\nand show the inference speed in terms of frames per second\nand dependent of the number of persons.\nEvaluation Metrics: We assess the performance with the\nstandard evaluation metric based on Object Keypoint Simi-\nlarity (OKS):\nOKS =\n∑\niexp(−d2\ni/2s2k2\ni)δ(vi>0)∑\niδ(vi>0) (5)\nThereby, for each keypoint i∈{1,2,...,17}, di is the Eu-\nclidean distance between the detected keypoint and its cor-\nresponding ground truth, vi is the (boolean) visibility of the\nground truth, s is the object scale, ki is the labeling uncer-\ntainty (a COCO constant) and δis 1 for positive visibilities\nand zero otherwise. We calculated the average precision and\nrecall scores: AP50 (AP at OKS =0.50), AP75, AP (the mean\nof AP scores at OKS = 0.50, 0.55, . . . ,0.90, 0.95), APM for\nmedium objects, AP L for large objects, and AR (the mean\nof recalls at OKS = 0.50,0.55,...,0.90,0.95), as well as\nAR50, AR75, ARM and ARL.\nInference speed analysis.To analyze the inference runtime,\nwe resize all images of the COCO-validation dataset to\n512x512 and run inference on them. The analysis was car-\nried out on one Nvidia TITAN RTX with a batch size of 1. We\ncalculated the runtimes averaged over all images that contain\nthe same amount of people.\nImplementation Details: We trained all (POET) models\nwith the following coefﬁcients in the keypoint loss:λabs= 4,\nλvis= 0.2, λctr= 0.5 and λdeltas= 0.5 (but see Table 2 and\n3).\nWe set the transformer’s initial learning rate to 0.5 ·\n10−4, the backbone’s to 0.5 ·10−5, and weight decay to\n10−4 (18) and train POET with AdamW (51). A dropout\nrate of 0.1 is applied on the transformer’s weights, which are\ninitialized with Xavier initialization (52). For the encoder,\nwe choose ResNet50 (47) with different strides S. Accord-\ningly, models are called POET-R50 as well as POET-DC5-\nR50, when using a dilated C5 stage (which decreases the\nstride from 32 to 16). The replacement of a stride by a di-\nlation in the last stage of the backbone increases the feature\nresolution by a factor of two, but comes with an increase in\ncomputational cost by the same factor.\nDuring training we augmented by applying rotation uni-\nformly drawn from (−25,+25) degrees, random cropping,\nhorizontal ﬂipping and coarse dropout (53) with a probabil-\nity of 0.5 each. Additionally, we resize the images such that\nthe shortest side falls in the range [400,800] and the longest\nside is at most 1,333. We set the number of prediction slots\nStofﬂ et al. | POET\nN to 25, as the maximum number of keypoint annotated hu-\nmans in COCO images is 13 (we used the same number for\nMacaquePose).\nWe carried out two different sets of experiments: (1)\ntraining POET initialized from ImageNet (54) weights to\ncompare with current state-of-the-art models and (2) train-\ning multiple models as well as baseline models, whereby\nwe started with COCO keypoint challenge pretrained weights\nfrom MMPose (50).\nFor the comparison with state-of-the-art models, we\ntrain POET-R50 with a batch size of 6 on two NVIDIA V100\nGPUs (hence a total batch size of 12) for 250 epochs, with\na learning rate drop by a factor of 10 after 200 epochs. One\nepoch takes approximately one hour in this setting.\nFor the comparison to baseline models, we utilized\nthe associative embedding (13, 16) implementation in MM-\nPose (38, 50). To account for the high memory footprint and\nthe long training times, we restrict the maximum image size\nto 512 during training and train POET models (POET-R50,\nand POET-DC5-R50) with a total batch size of 64 (50 for\nDC5 models; we also multiply the learning rates by a fac-\ntor of 2 here) for 250 epochs, for the comparison with the\nbaseline models. The baseline models were trained for 100\nepochs with the default learning schedule for AE + ResNet\nmodels in MMPose (38), with similar augmentation meth-\nods (without coarse dropout, but with afﬁne transformation\naugmentation). We reduced the stride by removing upsam-\npling layers. We trained the baseline models only for 100\nepochs, as we initialized the AE + ResNet models from the\none trained with stride 4 on COCO and the performance sat-\nurated already.\n0 50 100 150 200 250\n0\n10\n20\n30\n40\n50\n60\nTraining epochs\nCOCO mAP performance (%)\nFig. 5. Evolution of mAP on COCO validation set for POET -R50 trained for 250\nepochs. Learning rate was dropped after 200. Loss parameters:λabs = 4, λvis =\n0.2, λctr = 0.5 and λdeltas = 0.5.\nResults\nQualitative Results: When we trained POET-R50 on\nCOCO with the proposed loss (Eq. 4) as well as the cross-\nvalidated hyperparameters, we found that class, keypoint,\nvisibility, center and delta loss decreased (Figure 3). When\nvisualizing predictions of this POET-R50 model for COCO-\nval, we saw that POET can successfully tackle the problem\n4 8 12 16 20 24 28 32\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nCNN stride\nCOCO mAP performance (%) Ours\nBaseline\nFig. 6. POET vs. the fully convolutional associative embedding method perfor-\nmance as a function of the stride of the encoder on the COCO keypoint estimation\ntask. Our method achieves strong performance despite lower resolution features.\nof multi-instance pose estimation (Figure 4). Next we quan-\ntiﬁed the performance.\nQuantitative Evaluation: Firstly, to quantify the perfor-\nmance, we computed mAP over the learning period and\nfound that it reaches high performance (Figure 5). Next,\nwe compare our results to state-of-the-art methods on COCO\ntest-dev (Table 1). We split the methods into top-down and\nbottom-up approaches and report numbers without multi-\nscale testing or extra training data in order to have a fair\ncomparison. We ﬁnd that POET-R50 performs competitively\nwith other bottom-up methods for large humans ( APL), but\nhas lower performance for small/medium humans.\nWe reasoned that this is due to larger stride for the\nencoders in all strong methods (e.g. ≥4, see Table1),\nwhich contributes to inferior spatial resolution at the in-\nput to the transformer. Transformers scale quadratically\nO((H·W/S2)2) in the input dimensions, and thus increasing\nthe stride is costly. In order to demonstrate the powerful po-\ntential of our method, in comparison to previous methods, we\nnext compare POET to baseline models with ResNet back-\nbones (and varying strides). We chose associative embedding\n(AE) (13, 50) as the model to compare to, as this method\nwas proven to be a strong bottom-up method and currently is\nstate-of-the-art when applied with the high resolution back-\nbone HigherHRNet (16). We created baseline models that\nwere trained with the same pretrained ResNet backbones, in-\nput image sizes and similar data augmentation. We varied the\noverall stride of the ResNet backbones for AE from4 to up to\n32, to assure that both methods receive the same feature di-\nmensions as input. POET outperforms the baseline methods\n(with the same stride) by a large margin and (for stride 16) is\nbetter than the baseline models even with stride4, which pro-\nvides evidence that the transformer head is suitable to learn\nmultiple poses in an image, even from low-resolution feature\nmaps (Figure 6). Future work could focus on more efﬁcient\nmodels to leverage smaller strides.\n6 Stofﬂ et al. | POET\nFig. 7. a) The encoder self-attention maps for six reference points, highlighted in yellow, are shown. Pixels belonging to individuals locally attend to other pixels belonging to\nthe same individual. Thus, the encoder learnt well to distinguish between different instances. b) The decoder cross-attention scores for predicted individuals are highlighted\nfor six object queries. Each focuses on a different person; for larger persons the attention is higher on extremities (such as the knee or shoulder) and especially the face while\nfor smaller persons it is more distributed over the whole body. The image is from the COCO validation set.\nAnalysis:\nTransformer Attentions.In order to better understand the\nroles of the encoder and decoder in the transformer archi-\ntecture for tackling multi-instance pose estimation, we visu-\nalized the attention maps. We found that the transformer’s\nencoder attends locally to each individual while the decoder\nseems to attend to salient parts of each individual (Fig-\nure 7a,b).\nDecoder Analysis.Next, we analyzed the role of the trans-\nformer decoder’s depth. We found that the average perfor-\nmance stabilizes after 3-5 decoder layers (Figure 8).\n1 2 3 4 5 6\n30\n40\n50\n60\n70\nTransformer decoder layer\nCOCO mAP performance (%)\nall\nmedium\nlarge\nFig. 8. Evolution of the mAP accuracy on COCO-val through decoder layers of\nPOET -R50 illustrating that the performance saturates after four decoding layers.\nLoss Ablations.Given that our loss (Eq. 3) depends on four\nparts, we sought to identify which terms are most rele-\nvant for the performance. In the following ablation experi-\nments we refer to λabs∥Vi◦Ai−Vi◦ˆAσ(i)∥1 as Labs and\nto λdeltas∥Vi◦Zi−Vi◦ˆZσ(i)∥1 as Ldeltas. We interro-\ngated the relationship between the loss on absolute coordi-\nnates (Labs) and the center (Lctrs) and deltas loss (Ldeltas).\nAs their weight is already small compared to Labs for the\nbest parameters we found (Figure 3), we ﬁrst set both the\ndeltas loss part and the center loss part to 0 and thereby giv-\ning the model freedom for predicting centers and offsets. We\nfound that the performance stays the same. Secondly, we\nincreased λctrs and λdeltas so that the loss magnitudes are\ncomparable to Labs. Performance drops when center and\ndelta loss parts are weighted higher (Table 2). Next, we tested\nif using losses on centers and deltas alone are sufﬁcient to\nreach high performance. To run more experiments, we only\ntrained for 50 epochs. We ﬁrst deﬁned an extended version\nof Ldeltas in which, additionally to the distances between\noffsets, the L1 distance between the predicted center and the\nground truth center is included ( Ldeltas&ctr). Including this\nterm improved the performance when the absolute error loss\npart was ablated (Table 3). To show that the same results\nhold also when training for the full 250 epochs, we took the\nbest model trained without the absolute error loss part and\ncompared its performance after 250 epochs to POET trained\nwith the full loss. We conclude that the usage of the error on\nabsolute coordinates Labs is crucial for the performance of\nPOET and that using separate loss terms for centers and off-\nsets, respectively, does not work for various parameter com-\nbinations. Thus, including Labs in the pose loss is crucial.\nLearned Centers.As shown in the last section, POET models\nwith weaker constraints on centers and deltas show the best\nperformance. This raises the questions, where are the pre-\ndicted centers? By making the body part locations dependent\non a center, every joint position carries the information about\nthe person instance it belongs to (Figure 2b). Transformers\nare particularly well suited at modeling interactions and rela-\ntions between data points and representations. By encoding\nthe human pose into centers and offsets the architecture can\nbeneﬁt from reasoning about person instances and their poses\nat the same time.\nHowever, thecenter is rather complex, as it strongly de-\npends on the visible keypoints, and the posture. Therefore,\nthe model needs not only to understand the geometry of a hu-\nman body, but also its appearance in a given image, as often\nonly parts of the body can be seen (Figure 9a). In the top\nimage all persons can be fully visible, and POET predicts the\ncenter in the middle of the body. In the lower images, per-\nsons of different poses, scales and occlusions require a very\nﬂexible interpretation of the center. This suggests that POET\npredicts the centers well. Indeed, the learned centers are very\nclose to the GT centers (Figure 9b).\nHowever, this pose representation is very ﬂexible. One\nStofﬂ et al. | POET\nFig. 9. Analysis of learned centers. (a) Predicted centers (red) and relative displacements per bodypart as blue vectors for multiple individuals on example images from\nCOCO val. (b) The scatter plot shows the relative offset of predicted center vs. ground truth center (normalized by bounding box width/height). (c) Predictions of two models,\ndifferently constrained on predicting the deﬁned ground truth center (see Table 2). The model without constraints ( λctrs = λdeltas = 0) learns centers that are far apart\nfrom the center of mass of visible keypoints and achieves also higher performance in terms of OKS.\ncan vary the center and deltas loss weight strongly with rel-\natively little change on quantitative (Table 2) and qualita-\ntive results while strongly impacting the center location (Fig-\nure 9c).\nVisibility. In applications, it is often important not only to es-\ntimate the location of body parts, but also to ascertain if a\nparticular bodypart is occluded (2). However, the metrics\nused for the COCO keypoint detection challenge do not take\nvisibility into account (i.e. false-alarm predictions are not\npenalized in Eq. 5). Our loss formulation (Eq. 2) allows the\nmodel to learn the visibility of each keypoint together with its\nlocation. We found that POET accurately predicts the corre-\nsponding visibility for each predicted body part (Figure 10).\nInference speed is another important characteristic of pose\nestimation algorithms for applications (55). We compared\nthe inference speed of POET with four well-known multi-\nperson pose estimation models: the bottom-up methods\nOpenPose (10) and Associative Embedding (13) in combi-\nnation with R50 (47) and HRNet backbones (7, 36) as well\nas a top-down method consisting of a Mask R-CNN (34) for\nbounding box detection and a R50-SimpleBaseline (6) net-\nwork for pose estimation.\nAs expected for top-down approaches, the inference\ntimes of Mask R-CNN and R50-SimpleBaseline is propor-\ntional to the number of proposals that the person detector\n(Mask R-CNN) extracts (Figure 1). A similar trend can be\nobserved for Associative Embedding with R50/HRNet back-\nFig. 10. Violin plot of POET -R50 predicted visibility vs. COCO ground truth visibil-\nity for all images and annotated humans in COCO-val. On COCO-val, POET -R50\npredicts 94.84% of all 123,359 non-visible keypoints with a visibility score smaller\nthan 0.5, and 91.09% of all the 59,850 visible keypoints with a visibility conﬁdence\nlarger than 0.5.\nbone. In contrast, the widely-used OpenPose method is in-\nvariant to the number of people in the image. The same holds\nfor POET that also shows slightly higher inference speeds\nthan OpenPose (Figure 1).\nPOET for animal pose estimation: Datasets for animal\npose estimation are typically much smaller than human\nbenchmarks (2, 4). We thus sought to test POET on\nthe MacaquePose dataset (20) to evaluate if a transformer\nbased architecture gives competitive performance on small\ndatasets. We found that POET qualitatively performs well on\nMacaquePose (Figure 11). We quantiﬁed the performance of\ndifferent POET instances (Table 4). As expected, the version\n8 Stofﬂ et al. | POET\nFig. 11. Example predictions on MacaquePose validation set. Top row: examples with good performance. Bottom row: examples with errors.\nwith a overall stride 16 (POET-DC5-R50) outperformed the\nstride 32 model (POET-R50). POET outperforms associative\nembedding with the same backbone but a stride of four.\nAs MacaquePose has the same keypoints as COCO,\nwe experimented also with transfer learning by initializing\nPOET with the weights of the POET-R50 model that was ﬁrst\npre-trained on the COCO keypoint challenge (see POET-R50\nin Figure 6). We found that using this pre-trained model fur-\nther boosts the performance on MacaquePose. Perhaps as ex-\npected, this model also learned much quicker, while quickly\nunlearning COCO (Figure 12).\n0 50 100 150 200 250\n0\n10\n20\n30\n40\n50\n60\n70\n80\nTraining epochs\nmAP performance (%)\nPOET-R50-pre MP\nPOET-R50-pre CO\nPOET-DC5-R50-pre MP\nPOET-DC5-R50-pre CO\nPOET-R50 MP\nPOET-DC5-R50 MP\nFig. 12. Evolution of mAP for different POET models. Performance on the\nMacaquePose (MP) validation set and on COCO (CO) val are shown for the model\ninitialized with the weights of a POET model trained on COCO (POET -pre). It learns\nfaster and achieves higher performance than the two models trained from ImageNet\ninitialization (POET -R50, POET -DC5-R50), while the performance on COCO decays\nwithin one epoch and plateaus (dashed lines).\nConclusions\nWe presented POET, a pose estimation method based on a\nconvolutional encoder, transformers and bipartite matching\nloss for direct set prediction. Our approach achieves strong\nresults on COCO keypoint challenge and is the ﬁrst that is\nend-to-end trainable. POET builds on DETR (18), which\ntackled object recognition and panoptic segmentation, with\ntransformers, and extends this framework by introducing a\nnovel loss function that, combined with a suitable pose rep-\nresentation, can successfully tackle the problem of multi-\ninstance pose estimation. At the current stage, POET does\nnot achieve state-of-the-art performance, but we hope that it\nwill inspire future research.\nAcknowledgements\nWe are grateful to Steffen Schneider, Shaokai Ye, Mu Zhou,\nand Mackenzie Mathis for discussions as well as Axel\nBisi, Alberto Chiappa, Alessandro Marin Vargas, Nicholas\nRobertson, Lazar Stojkovic, Liza Kozlova, Haozhe Qi, and\nAristotelis Economides for comments on this manuscript.\nStofﬂ et al. | POET\nTable 2. Effect of center loss and deltas loss components on AP . We trained a model turning off the loss on the centers and the deltas (ﬁrst row). The second row shows our\nbaseline model with all loss terms. The maximum input image size is kept at 512, the number of parameters is 41.3M and class loss as well as visibility loss are used by all\nmodels.\nMethod λabs λctrs λdeltas AP AP M APL AR\nPOET-R50 4.0 0.0 0.0 48.5 33.4 68.1 53.8\nPOET-R50 4.0 0.5 0.5 48.5 33.4 68.3 53.7\nPOET-R50 4.0 100.0 0.5 48.2 33.1 67.9 53.0\nPOET-R50 4.0 0.5 5.0 47.9 32.7 67.6 52.9\nPOET-R50 4.0 100.0 5.0 47.0 31.9 66.6 52.0\nTable 3. Effect of pose loss components on AP . We trained several models turning off the loss on the absolute coordinates and solely training with the center and offset loss\nparts. Additionally, we included the center into the L1 delta loss for some models as we saw strong performance boosts. We observe that after 50 epochs no model can reach\nthe performance of POET trained with the L1 loss on absolute coordinates (last row). Additionally, the best performing model after 50 epochs remains worse also after 250\nepochs. The maximum input image size is kept at 512, the number of parameters is 41.3M and class loss as well as visibility loss are used by all models.\nMethod λabs λdeltas Ldeltas&ctrs AP (after 50 epochs) AP (after 250 epochs)\nPOET-R50 - 5.0 - 8.4 -\nPOET-R50 - 5.0 ✓ 18.0 -\nPOET-R50 - 20.0 - 11.1 -\nPOET-R50 - 20.0 ✓ 20.0 35.5\nPOET-R50 - 50.0 - 9.3 -\nPOET-R50 - 50.0 ✓ 19.0 -\nPOET-R50 - 500.0 - 2.1 -\nPOET-R50 - 500.0 ✓ 12.0 -\nPOET-R50 - 0.5 - 2.2 -\nPOET-R50 - 0.5 ✓ 18.7 -\nPOET-R50 4.0 0.5 - 32.2 48.5\nTable 4. Performance on MacaquePose (20) for various POET models. Using a CNN with a smaller stride improves performance, and initializing weights from a pre-trained\nmodel on the COCO keypoint challenge gives a strong boost. We compare POET to a ResNet-50 + associative embedding (AE) (13, 38) baseline trained for 300 epochs.\nMethod Input Size / Stride AP AP 50 AP75 AR AR 50 AR75\nAE + R50 512 / 4 52.5 81.2 50.8 68.6 88.6 68.6\nAE + R50 (COCO pre-trained) 512 / 4 75.4 95.4 81.4 83.8 97.2 88.7\nPOET-R50 512 / 32 65.5 92.8 75.1 77.9 97.0 87.0\nPOET-DC5-R50 512 / 16 68.2 94.5 78.1 78.2 97.1 87.0\nPOET-R50 (COCO pre-trained) 512 / 32 75.5 96.0 85.2 82.0 97.8 90.1\nPOET-DC5-R50 (COCO pre-trained) 512 / 16 77.1 96.1 86.7 83.6 97.9 91.3\nReferences\n1. Ronald Poppe. Vision-based human motion analysis: An overview. Computer Vision and\nImage Understanding, 108(1):4 – 18, 2007. ISSN 1077-3142. doi: https://doi.org/10.1016/j.\ncviu.2006.10.016. Special Issue on Vision for Human-Computer Interaction.\n2. Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga Abe, Venkatesh N Murthy,\nMackenzie Weygandt Mathis, and Matthias Bethge. Deeplabcut: markerless pose estima-\ntion of user-deﬁned body parts with deep learning.Nature Neuroscience, 21(9):1281–1289,\n2018.\n3. Yucheng Chen, Yingli Tian, and Mingyi He. Monocular human pose estimation: A survey\nof deep learning-based methods. Computer Vision and Image Understanding, 192:102897,\n2020.\n4. Alexander Mathis, Steffen Schneider, Jessy Lauer, and Mackenzie Weygandt Mathis. A\nprimer on motion capture with deep learning: principles, pitfalls, and perspectives. Neuron,\n108(1):44–65, 2020.\n5. Thomas K Uchida and Scott L Delp. Biomechanics of Movement: The Science of Sports,\nRobotics, and Rehabilitation. MIT Press, 2021.\n6. Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and\ntracking. In Proceedings of the European conference on computer vision (ECCV) , pages\n466–481, 2018.\n7. Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation\nlearning for human pose estimation. arXiv preprint arXiv:1902.09212, 2019.\n8. Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Y e, and Ce Zhu. Distribution-aware coordinate\nrepresentation for human pose estimation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 7093–7102, 2020.\n9. Sen Y ang, Zhibin Quan, Mu Nie, and Wankou Y ang. Transpose: Towards explainable human\npose estimation by transformer. arXiv preprint arXiv:2012.14214, 2020.\n10. Zhe Cao, Tomas Simon, Shih-En Wei, and Y aser Sheikh. Realtime multi-person 2d pose\nestimation using part afﬁnity ﬁelds. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7291–7299, 2017.\n11. Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov,\nBjoern Andres, and Bernt Schiele. Arttrack: Articulated multi-person tracking in the wild.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n6457–6465, 2017.\n12. George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson,\nChris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the\nwild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 4903–4911, 2017.\n13. Alejandro Newell, Zhiao Huang, and Jia Deng. Associative embedding: End-to-end learning\nfor joint detection and grouping. In NIPS, 2017.\n14. Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Pifpaf: Composite ﬁelds for human\npose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 11977–11986, 2019.\n15. Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng Y an. Single-stage multi-person\npose machines. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 6951–6960, 2019.\n16. Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang, and Lei Zhang.\nHigherhrnet: Scale-aware representation learning for bottom-up human pose estimation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 5386–5395, 2020.\n17. Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Openpifpaf: Composite ﬁelds for se-\nmantic keypoint detection and spatio-temporal association.arXiv preprint arXiv:2103.02440,\n2021.\n18. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with transformers. In European Con-\nference on Computer Vision, pages 213–229. Springer, 2020.\n19. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Euro-\npean conference on computer vision, pages 740–755. Springer, 2014.\n20. Rollyn Labuguen, Jumpei Matsumoto, Salvador Negrete, Hiroshi Nishimaru, Hisao Nishijo,\nMasahiko Takada, Y asuhiro Go, Ken-ichi Inoue, and Tomohiro Shibata. Macaquepose: A\nnovel ‘in the wild’macaque monkey pose dataset for markerless motion capture. bioRxiv,\n10 Stofﬂ et al. | POET\n2020.\n21. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n22. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. Albert: A lite bert for self-supervised learning of language representations.\narXiv preprint arXiv:1909.11942, 2019.\n23. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n24. Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-\nsequence model for speech recognition. In 2018 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages 5884–5888. IEEE, 2018.\n25. Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem\nproving. arXiv preprint arXiv:2009.03393, 2020.\n26. Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shah-\nbaz Khan, and Mubarak Shah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021.\n27. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n28. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint\narXiv:2102.12092, 2021.\n29. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020.\n30. Zhiqing Sun, Shengcao Cao, Yiming Y ang, and Kris Kitani. Rethinking transformer-based\nset prediction for object detection. arXiv preprint arXiv:2011.10881, 2020.\n31. Ce Zheng, Wenhan Wu, Taojiannan Y ang, Sijie Zhu, Chen Chen, Ruixu Liu, Ju Shen,\nNasser Kehtarnavaz, and Mubarak Shah. Deep learning-based human pose estimation:\nA survey. arXiv preprint arXiv:2012.13392, 2020.\n32. Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose\nestimation: New benchmark and state of the art analysis. In Proceedings of the IEEE\nConference on computer Vision and Pattern Recognition, pages 3686–3693, 2014.\n33. Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose:\nEfﬁcient crowded scenes pose estimation and a new benchmark. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 10863–10872, 2019.\n34. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings\nof the IEEE international conference on computer vision, pages 2961–2969, 2017.\n35. Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cas-\ncaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages 7103–7112, 2018.\n36. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Y ang Zhao, Dong\nLiu, Y adong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation\nlearning for visual recognition. IEEE transactions on pattern analysis and machine intelli-\ngence, 2020.\n37. George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson,\nand Kevin Murphy. Personlab: Person pose estimation and instance segmentation with\na bottom-up, part-based, geometric embedding model. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 269–286, 2018.\n38. MMPose Contributors. Openmmlab pose estimation toolbox and benchmark - ae-\nresnet. https://mmpose.readthedocs.io/en/latest/bottom_up_models.\nhtml#associative-embedding-ae-resnet , 2020.\n39. Alejandro Newell, Kaiyu Y ang, and Jia Deng. Stacked hourglass networks for human pose\nestimation. In European conference on computer vision, pages 483–499. Springer, 2016.\n40. Umar Iqbal and Juergen Gall. Multi-person pose estimation with local joint-to-person asso-\nciations. In European Conference on Computer Vision, pages 627–642. Springer, 2016.\n41. Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and Bernt\nSchiele. DeeperCut: A deeper, stronger, and faster multi-person pose estimation model.\nIn European Conference on Computer Vision, pages 34–50. Springer, 2016.\n42. Qi Dang, Jianqin Yin, Bin Wang, and Wenqing Zheng. Deep learning based 2d human pose\nestimation: A survey. Tsinghua Science and Technology, 24(6):663–676, Dec 2019. ISSN\n1007-0214. doi: 10.26599/TST.2018.9010100.\n43. Weian Mao, Y ongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, and Zhibin Wang. Tf-\npose: Direct human pose estimation with transformers. arXiv preprint arXiv:2103.15320 ,\n2021.\n44. Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and Zhuowen Tu. Pose recognition\nwith cascade transformers. arXiv preprint arXiv:2104.06976, 2021.\n45. Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Y ang, Chen Chen, and Zhengming\nDing. 3d human pose estimation with spatial and temporal transformers. arXiv preprint\narXiv:2103.10455, 2021.\n46. Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction\nwith transformers. arXiv preprint arXiv:2012.09760, 2020.\n47. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n48. Harold W Kuhn. The hungarian method for the assignment problem. Naval research logis-\ntics quarterly, 2(1-2):83–97, 1955.\n49. Bartel Leendert Van der Waerden. Algebra, volume 1. Springer Science & Business Media,\n2003.\n50. MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https:\n//github.com/open-mmlab/mmpose, 2020.\n51. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n52. Xavier Glorot and Y oshua Bengio. Understanding the difﬁculty of training deep feedforward\nneural networks. In Proceedings of the thirteenth international conference on artiﬁcial in-\ntelligence and statistics , pages 249–256. JMLR Workshop and Conference Proceedings,\n2010.\n53. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural\nnetworks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n54. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International Journal of Computer Vision , 115(3):211–\n252, 2015.\n55. Gary A Kane, Gonçalo Lopes, Jonny L Saunders, Alexander Mathis, and Mackenzie W\nMathis. Real-time, low-latency closed-loop feedback using markerless posture tracking.\nElife, 9:e61909, 2020.\nStofﬂ et al. | POET",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7978790998458862
    },
    {
      "name": "Pose",
      "score": 0.7718920111656189
    },
    {
      "name": "Computer science",
      "score": 0.7421621084213257
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6771961450576782
    },
    {
      "name": "Encoder",
      "score": 0.5423868298530579
    },
    {
      "name": "End-to-end principle",
      "score": 0.5339184403419495
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5030786395072937
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44357830286026
    },
    {
      "name": "Artificial neural network",
      "score": 0.42457228899002075
    },
    {
      "name": "Computer vision",
      "score": 0.41719892621040344
    },
    {
      "name": "Object detection",
      "score": 0.4143100082874298
    },
    {
      "name": "Engineering",
      "score": 0.11402073502540588
    },
    {
      "name": "Voltage",
      "score": 0.10274645686149597
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "topic": "Transformer"
}