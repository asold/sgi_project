{
  "title": "Entity Alignment with Reliable Path Reasoning and Relation-aware Heterogeneous Graph Transformer",
  "url": "https://openalex.org/W4285600445",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2603705813",
      "name": "wei-shan Cai",
      "affiliations": [
        "South China Normal University",
        "Hanshan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2102893201",
      "name": "Wenjun Ma",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2275808795",
      "name": "Jieyu Zhan",
      "affiliations": [
        "South China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2484001433",
      "name": "Yuncheng Jiang",
      "affiliations": [
        "South China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2808284704",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2906413107",
    "https://openalex.org/W3212664589",
    "https://openalex.org/W3098038527",
    "https://openalex.org/W2903963001",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2942259124",
    "https://openalex.org/W4288092459",
    "https://openalex.org/W2964263523",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W3012715399",
    "https://openalex.org/W3098583774",
    "https://openalex.org/W2551361256",
    "https://openalex.org/W2962916648",
    "https://openalex.org/W3089874281",
    "https://openalex.org/W3034906292",
    "https://openalex.org/W2741750617",
    "https://openalex.org/W3012000912",
    "https://openalex.org/W3211203754",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2964855489",
    "https://openalex.org/W2997062749",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W3161248896",
    "https://openalex.org/W3175988714",
    "https://openalex.org/W2996993961"
  ],
  "abstract": "Entity Alignment (EA) has attracted widespread attention in both academia and industry, which aims to seek entities with same meanings from different Knowledge Graphs (KGs). There are substantial multi-step relation paths between entities in KGs, indicating the semantic relations of entities. However, existing methods rarely consider path information because not all natural paths facilitate for EA judgment. In this paper, we propose a more effective entity alignment framework, RPR-RHGT, which integrates relation and path structure information, as well as the heterogeneous information in KGs. Impressively, an initial reliable path reasoning algorithm is developed to generate the paths favorable for EA task from the relation structures of KGs. This is the first algorithm in the literature to successfully use unrestricted path information. In addition, to efficiently capture heterogeneous features in entity neighborhoods, a relation-aware heterogeneous graph transformer is designed to model the relation and path structures of KGs. Extensive experiments on three well-known datasets show RPR-RHGT significantly outperforms 10 state-of-the-art methods, exceeding the best performing baseline up to 8.62% on Hits@1. We also show its better performance than the baselines on different ratios of training set, and harder datasets.",
  "full_text": "Entity Alignment with Reliable Path Reasoning and Relation-Aware\nHeterogeneous Graph Transformer\nWeishan Cai1;2 , Wenjun Ma1 , Jieyu Zhan1 , Yuncheng Jiang1;3\u0003\n1School of Computer Science, South China Normal University, China\n2School of Computer and Information Engineering, Hanshan Normal University, China\n3School of Artiﬁcial Intelligence, South China Normal University, China\ncaiws@m.scnu.edu.cn, phoenixsam@sina.com, zhanjieyu,ycjiang@scnu.edu.cn\nAbstract\nEntity Alignment (EA) has attracted widespread at-\ntention in both academia and industry, which aims\nto seek entities with same meanings from different\nKnowledge Graphs (KGs). There are substantial\nmulti-step relation paths between entities in KGs,\nindicating the semantic relations of entities. How-\never, existing methods rarely consider path infor-\nmation because not all natural paths facilitate for\nEA judgment. In this paper, we propose a more\neffective entity alignment framework, RPR-RHGT,\nwhich integrates relation and path structure infor-\nmation, as well as the heterogeneous information\nin KGs. Impressively, an initial reliable path rea-\nsoning algorithm is developed to generate the paths\nfavorable for EA task from the relation structures of\nKGs. This is the ﬁrst algorithm in the literature to\nsuccessfully use unrestricted path information. In\naddition, to efﬁciently capture heterogeneous fea-\ntures in entity neighborhoods, a relation-aware het-\nerogeneous graph transformer is designed to model\nthe relation and path structures of KGs. Extensive\nexperiments on three well-known datasets show\nRPR-RHGT signiﬁcantly outperforms 10 state-of-\nthe-art methods, exceeding the best performing\nbaseline up to 8.62% on Hits@1. We also show its\nbetter performance than the baselines on different\nratios of training set, and harder datasets.\n1 Introduction\nMost Knowledge Graphs (KGs) are often disconnected from\neach other because they are constructed with different tech-\nnologies and languages, which poses challenges for merging\nand integrating different KGs. Entity Alignment (EA) is a\ntask to connect entities with the same meaning in different\nKGs, which plays a fundamental role in the knowledge fu-\nsion of KGs. Recently, EA methods based on the Graph Neu-\nral Networks (GNNs) are more favored by researchers than\nthe translation-based methods. GNNs not only exhibit excel-\nlent performance in aggregating the neighborhood features of\nnodes, but also can design corresponding feature acquisition\n∗Corresponding Author\nmethods for EA tasks, while translation-based methods are\ndesigned for link prediction.\nAlthough current GNNs-based methods have achieved\npromising results, they still suffer from the following three\nlimitations. First, many methods [Wu et al., 2019; Sunet al.,\n2020a] treat KGs as homogeneous graphs without consider-\ning the heterogeneous features of sides between entities. Ac-\ntually, the heterogeneous information helps to improve the ac-\ncuracy and robustness of alignment judgments. Second, some\nsemantic information other than relation structures is consid-\nered by many works, such as entity attributes[Liu et al., 2020;\nCai et al., 2022], text descriptions [Yang et al., 2019], and\nmulti-modal information [Liu et al., 2021]. However, the\nmore semantic information a method integrates, the more\ndata its application requires, which cannot be satisﬁed in\nsome scenarios. Third, some other works [Wu et al., 2020;\nZhu et al., 2020] only rely on the relation structures, and ob-\ntain inter-graph information based on Graph Matching Net-\nworks (GMN) [Li et al., 2019] to mine more similar features\nbetween aligned entities. Nonetheless, the matching mod-\nule they introduced for learning inter-graph information runs\nthrough the entire training process with high temporal and\nspace complexity.\nTherefore, for the ﬁrst limitation above, we design a\nRelation-Aware Heterogeneous Graph Transformer (RHGT )\nto effectively extract the similarity features of aligned entities\nin their heterogeneous structures. For the latter two limita-\ntions above, we develop a Reliable Path Reasoning algorithm\n(RPR) that can directly extract the path structures favorable\nfor EA tasks from the original relation structures. Existing\nmethods rarely consider the path information of KGs (i.e., the\nindirect neighborhood of aligned entities), despite their suc-\ncess in modeling of direct relationship facts. It is known that\nsubstantial multi-step relational paths exist between entities,\nindicating their semantic relationships. But not all natural\npaths facilitate EA judgment, and some even backﬁre. Al-\nthough IPTransE [Zhu et al., 2017] considers the reliability of\npaths, it assumes all relations between KGs are pre-aligned.\nEssentially, our idea is to make full leverage of the richness\nof KGs by simultaneously comparing the similarities of re-\nlation and path structures of aligned entities. We believe the\npaths that frequently appear near pre-aligned entities can be\nregarded as reliable and used to align other entities. The fu-\nsion of relation and path structure information complements\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1930\neach other, alleviating the inconsistency between each type\ninformation of aligned entities.\nAfter all, we combine above two methods into a entity\nalignment framework calledRPR-RHGT, which not only con-\nsiders the heterogeneous information of sides in KGs, but also\nmines the path information within the relation structures of\nKGs. Extensive experiments on three well-known benchmark\ndatasets show RPR-RHGT not only outperforms 10 state-of-\nthe-art models signiﬁcantly, but also has impressive scalabil-\nity and robustness.\n2 Related Work\nTranslation-based Entity Alignment. Such methods are\nmainly based on TransE [Bordes et al., 2013] and its vari-\nants. MTransE [Chen et al., 2017] is the pioneering work,\nwhich uses TransE to model the entities and relations and\nevaluates the transform between two vector spaces based\non pre-aligned entities. Other works utilize additional in-\nformation or external knowledge of KGs, such as attribute\nstructures [Sun et al., 2017; Zhang et al., 2019b; Trisedya\net al., 2019 ], entity descriptions [Chen et al., 2018 ], en-\ntity names [Zhang et al., 2019b ], ontology schemata [Xi-\nang et al., 2021], to ﬁnd more similar features of aligned\nentities. There are also some works [Sun et al., 2018;\nZhu et al., 2017] that try to discover more new aligned en-\ntities by iterative strategies.\nGNNs-based Entity Alignment. GNNs-based methods\nmainly utilize Graph Convolutional Networks (GCNs) and\nGraph Attention Networks (GATs) to aggregate the neighbor-\nhood feature of each entity, thereby obtaining the neighbor-\nhood similarity between aligned entities. Most of them di-\nrectly compare the neighborhood similarity between aligned\nentities in relation structures [Wu et al., 2019]. There are\nseveral attemptsto simultaneously consider the similarities\nin the attribute and relation structures [Liu et al., 2020 ].\nSome other works smartly model both intra-graph and cross-\ngraph information, and learn similarities by building cross-\ngraph attention mechanism using GMN [Wu et al., 2020;\nZhu et al., 2020]. Besides, some researchers believe that the\nheterogeneity of edges in KGs should be considered when\naggregate the neighborhood features, because KGs are het-\nerogeneous graphs. They propose or apply some heteroge-\nneous graph embedding methods to learn better representa-\ntions for entities [Cai et al., 2022]. All aforementioned works\nonly consider the similarity of direct neighborhoods between\naligned entities. However, aligned entities have some simi-\nlarity in their indirect neighborhoods. Hence, we attempt to\nobtain the similarity between aligned entities in the relation\nstructures and multi-hop path structures of KGs simultane-\nously in the paper.\nHeterogeneous GNNs. Recently, many works have tried\nto extend GNNs to the modeling of heterogeneous graphs.\nRGCNs [Schlichtkrull et al., 2018] and RGATs[Busbridge et\nal., 2019] model heterogeneous graphs by using a weight ma-\ntrix for each relation. HAN [Wang et al., 2019] proposes a hi-\nerarchical attention mechanism to learn the weights of nodes\nand meta-paths from node-level and semantic-level attention,\nrespectively. HetGNN [Zhang et al., 2019a] adopts differ-\nent Recurrent Neural Networks (RNNs) for different types\nof nodes to integrate multi-modal features. Howerer, due to\nthe large number of relations in KGs, the training complexity\nis high when applying them to model KGs. More recently,\nHGT [Hu et al., 2020] and RHGT [Mei et al., 2022] try to\nmodel the heterogeneity by heterogeneous graph transform-\ners. But they are not designed to capture neighborhood simi-\nlarity, so it is difﬁcult to directly apply to EA tasks. Therefore,\nan improved heterogeneous graph transformer is designed to\nconsider the heterogeneity of KGs, thereby obtaining high-\nquality entity embeddings for EA tasks.\n3 Proposed Framework\n3.1 Problem Deﬁnition\nTo increase the neighborhood semantics of entities, we in-\ntroduce a meta path-based similarity framework for EA. The\nclassic meta-path paradigm is deﬁned as a sequence of rela-\ntions between objects, so we deﬁne the new compound re-\nlation between two entities as a relation path in this paper.\nFor example, suppose (e1\nh;e2\nk) is an aligned entity, where\nsuperscripts denote different KGs. There is a path rela-\ntion (r1\nf;r1\ng) near e1\nh, because the following relation exists:\ne1\nh\nr1\nf\n!e1\ni\nr1\ng\n!e1\nt, but there may not be a similar path near e2\nk.\nTherefore, not all paths in the neighborhood of entities are re-\nliable for EA learning. In other words, we should only keep\npartially reliable paths to learn the neighborhood similarity of\naligned entities.\nDeﬁnition 1 (Reliable Path Set). In this paper, we use\nP = fp1;:::;p i;:::; pNgrepresents reliable path set, where\neach pi = (rf;rg) is effective for EA learning. “Reliable\npath” here refer to path that facilitate EA learning, rather than\nthe meaningful path. We believe the paths that frequently oc-\ncur in the neighborhoods of pre-aligned entities can be con-\nsidered reliable. In this paper, we only consider paths based\non two-hop relations, and the study of a wider range of path\nstructures will be left to future work.\nDeﬁnition 2 (KG with Reliable Path Set). We deﬁne KG as\nG=(E;R;T rel;P;T path), where Eis entity set,Ris relation\nset, Trel\u0012E\u0002R\u0002E is relation triple set, P is reliable path\nset, and Tpath=fheh;pk;etijpk=(rf;rg)2P;heh;rf;eai2\nTrel;hea;rg;eti2Trelgis path triple set.\nDeﬁnition 3 (Entity Alignment). G1 = (E1;R1;T1\nrel;P1;\nT1\npath) and G2 = (E2;R2;T2\nrel;P2;T2\npath) are two KGs to\nbe aligned. Let L = f(e1\ni;e2\nj)je1\ni 2E1;e2\nj 2E2;e1\ni \u0011e2\njgbe\nthe pre-aligned entity set, where \u0011refers to the same real-\nworld object. Entity Alignment tasks aim to ﬁnd the remain-\ning aligned entities between two KGs.\nFormally, we use bold letter for embedding vector. For ex-\nample, E1 represents the embedding matrix of entities in G1,\nand e1\ni represents the i-th row of E1. In addition, the entity\nname is the most common text used to identify a entity, which\ncan be used to effectively capture the semantic similarity of\naligned entities. Therefore, we apply pre-trained word em-\nbeddings to generate initial representations of entities, En,\nand use them as the input of our framework.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1931\nAlignment Learning\nRelation \nTriples\nInput\nPre-aligned\nKG 1\nKG 2\nReliable Path Triples\nPath-based \nEmbeddings\nBack Propagation \nOutput\nEntity Alignment\n...\n...\nRelation-based  \nEmbeddings Margin-based Loss\nRelation \nTriples\nRPR  Module\nr 5\nr 7\nKG 2\nr 6\nKG 1\nr 8\npre-aligned\nr 1\nr 3\nr 2\nr 5\nmatching path\nmatching path\nRHGT  Module\nName\nembedding\nRelation\nembedding\nAttention\nRelation-aware\nembedding\nMessage\nAggregate\nSimilatrity Matrix\n0.91 ... 0.56\n...\n0.87 0.79\n...\n...\n0.72 0.99\n...\n......\n...\n... ...\n... ...\n... ...\nFigure 1: The overall architecture of RPR-RHGT.\n3.2 Overview Framework of RPR-RHGT\nIn this section, we introduce our proposed framework RPR-\nRHGT, a novel robust EA framework based on a reliable\npath reasoning algorithm and a relation-aware heterogeneous\ngraph transformer. Speciﬁcally, RPR-RHGT is mainly com-\nposed of three modules, as shown in Figure 1: (1) Reli-\nable Path Reasoning (RPR). A reliable path reasoning al-\ngorithm is developed to infer the reliable relation paths and\nform path structures of two KGs. (2) Relation-Aware Het-\nerogeneous Graph Transformer (RHGT ). We design the\nRHGT to capture the features of speciﬁc patterns of relations\nand paths with fewer parameters, which contain the hetero-\ngeneous neighborhood features of aligned entities in relation\nand path structures. (3) Alignment Learning. This module\ncomputes the loss function and similarity matrices of path-\nbased and relation-based entity embeddings, and evaluates\nthe probabilities of EA.\n3.3 Reliable Path Reasoning (RPR)\nAs discussed in Section 1, not all relation paths are reliable\nfor EA learning. It is known that each KG is constructed\naccording to relatively stable data sources and construction\nrules. Our key insight is the path with a high number of\nmatches between the neighborhoods of pre-aligned entities\n(small range) can be regarded as reliable, which can be used\nto match judgments of other entities (large range). We ﬁrst\nestablish the path neighborhood matching between each pre-\naligned pair (see Figure 2(a)), derive the matching paths (see\nFigure 2(b)), ﬁnally select those paths with high numbers of\nmatches to form a reliable path set P.\nSpeciﬁcally, for a given pair (e1\na;e2\na) 2L, the similarity\nmatrix Sdenotes the similarities between path neighborhoods\nPN(e1\na) and PN(e2\na), where PN(\u0001) indicates path neighbor-\nhood of a entity. Firstly, the entities with maximum similar-\nities in each row of S are selected as the matching neigh-\nbors. As shown in Figure 2(a), the matching result of e1\n1\n(one neighbor of e1\na) is e2\nn−1, because their similarity is the\nlargest in ﬁrst row. However, there may be multiple neigh-\nbors of e1\na that match the same neighbor of e2\na, such as e1\n1 and\n0.1 0.2 ... 0.9 0.0\n0.6 0.2 ... 0.7 0.0\n... ... ... ... ...\n0.7 1.0 ... 0.1 0.0\n0.8 0.4 ... 0.3 0.0\n...\n...\n(a) Path neighborbood matching\n...\n...\n(b) Reliable path matching\nFigure 2: Illustration of the path neighborhood matching between\nthe pre-aligned pair (e1\na;e2\na) ∈L.\ne1\n2 match with e2\nn−1 simultaneously. Therefore, the neigh-\nbor matching requires some one-to-one constraints: 1) the\nsimilarity of matching neighbor must reach a certain thresh-\nold: MN(S) = fe2\nsmax\nk\njsmax\nk > \u001csimg; 2) sort the simi-\nlarity values that satisfy the threshold from high to low, and\nthen perform one-to-one matching: Match1:1(MN(S)) =\n[(e1\n1;e2\ni);(e1\n2;e2\nj);:::]. So as a result, e1\n1 is chosen to match\ne2\nn−1, because 0:9 >0:7. Obviously, only some neighbors of\ne1\na may end up ﬁnding matching neighbors.\nSecondly, for each(e1\ni;e2\nj) 2Match1:1(MN(S)), we can\ndeduce the path matching pair (p1\nk;p2\nt) according to the fol-\nlowing reasoning relationship, as shown in Figure 2(b):\ne1\ni $e2\nj ) (e1\na;p1\nk;e1\ni) $(e2\na;p2\nt;e2\nj) ) p1\nk $p2\nt; (1)\nwhere $indicates the matching relationship;(e1\na;p1\nk;e1\ni) and\n(e2\na;p2\nt;e2\nj) are the path triples.\nThe last step is to count the matching number of each\nmatching path, counter(p1\nk $ p2\nt), and select those paths\nwith high numbers of matches to form reliable path set P:\nP = {(p1\nk;p2\nt)|counter(p1\nk ↔ p2\nt) >\u001c path}; (2)\nwhere \u001cpath is set according to the speciﬁc dataset. Algo-\nrithm 1 gives the procedure of our RPR algorithm.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1932\nAlgorithm 1 Procedure of RPR Algorithm.\nInput: (1) G = (E;R;Trel); (2) pre-aligned entities L; (3)\nentity name embeddings En.\nOutput: reliable path set P, path triple set Tpath.\n1: Set Pall  ;;\n2: for (e1\na;e2\na) 2L do\n3: Compute matching neighbors of path structures\nMatch1:1(MN(S)) between PN(e1\na) and PN(e2\na);\n4: for (e1\ni;e2\nj) 2Match1:1(MN(S)) do\n5: Deduce the path matching pair (p1\nk;p2\nt) using\nEq.(1);\n6: Pall  Pall [(p1\nk;p2\nt);\n7: end for\n8: end for\n9: Generate the reliable path set P using Eq. (2);\n10: Generate the path triple set Tpath using Deﬁnition 2;\n11: Return P and Tpath;\n3.4 Relation-Aware Heterogeneous Graph\nTransformer (RHGT)\nThe process of Graph Transformer [Yun et al., 2019] aggre-\ngating all neighborhood features of node hcan be brieﬂy ex-\npressed as:\ne(l)\nh ← Aggregate\n∀t∈N(h)\n(Attention(h;t) ·Message(h;t)); (3)\nwhere Attention is to estimate the importance of each neigh-\nborhood node; Message is to extract the feature of each\nneighborhood node; and Aggregate aggregates the neighbor-\nhood message through attention weights.\nAs shown in Eq.(3), Graph Transformer does not consider\nthe edge features. Inspired by [Hu et al., 2020], we design\na relation-aware heterogeneous graph transformer (RHGT ),\nwhich enables our model to distinguish the heterogeneity fea-\ntures of relations and paths, to better obtain the neighborhood\nsimilarity of aligned entities. Let E(l) denote the output of\n(l)-th layer of RHGT, which is also the input of the (l+ 1)-th\nlayer. Initially, E(0) = En. When the input of RHGT is the\nrelation triples, the output is relation-based embeddings, and\nwhen the input is the path triples, the output is path-based em-\nbeddings. As shown in Figure 3, RHGT is mainly composed\nof four layers.\n(a) Relation Embedding. Considering that the head en-\ntities and tail entities associated with aligned relations or\naligned paths have certain similarities, we generate relation\nfeatures by aggregating the features of associated entities.\nSpeciﬁcally, the embedding of ris approximated by averag-\ning the embeddings of its associated head entities Hr and as-\nsociated tail entities Tr as:\nRl(r) =\u001b\n2\n4\nP\nei∈Hr bhe(l−1)\ni\n|Hr| ∥\nP\nej∈Tr bte(l−1)\nj\n|Tr|\n3\n5; (4)\nwhere j\u0001jindicates the size of collection;bh;bt are the atten-\ntion vectors; kdenotes concatenation and\u001bis ReLU function.\n(b) Heterogeneous Attention. Inspired by [Hu et al.,\n2020], we map the entity h into a key vector Ki(h) and its\nRelation-Aware Heterogeneous Graph Transformer\nMulit-Heads\n(c) Heterogeneous Message(a) Relation\nEmbedding\n(d) Heterogeneous\nAggregate\n(b) Heterogeneous Attention\nK_Linear i (·)\nV_Linear i (·)\nV_Linear i (·)\nQ_Linear i (·)\nQ_Linear i (·)\nSoftmax(·)\nN_Linear (·)\nA_Linear (·)\n: Concat\n: Product\n: Add\nFigure 3: The overall architecture of RHGT.\nneighborhood entity tinto a query vector Qi(t). The key dif-\nference from other methods is that instead of directly using\nthe dot product of key and query vector as attention, we use\nthe dot product between their concatenated result and Rl(r).\nRl(r) comes from the feature aggregation of the associated\nhead and tail entities (see Eq.(4)), so it will not deviate too\nfar from the embeddings of its associated entities. Moreover,\nRl(r) denotes heterogeneous features of edges, so neighbors\nassociated with different edges contribute differently to the\nentity h. Speciﬁcally, we compute the multi-head attention\nfor each neighborhood relation (h;r;t), as follows:\nHAttention(h;r;t)= ||\ni∈[1;hn]\nSoftmax\n∀(r;t)∈RN(h)\n(HATTheadi (h;r;t));\nHATTheadi (h;r;t)= aT ([Ki(h)||Qi(t)]R(l)(r))=\np\nd=hn;\n(5)\nwhere Ki(h) = K Lineari(e(l−1)\nh ); Qi(t) = Q Lineari\n(e(l−1)\nt ); RN(h) denotes the neighborhood of entity h; a 2\nRd=hn×1 is the attention parameter;hnis the number of atten-\ntion heads and d=hn is the vector dimension per head. Note\nthat the Softmax process is to make the sum of attention vec-\ntors of all neighborhood entities equal to 1.\n(c) Heterogeneous Message. Similarly, we hope to incor-\nporate relations into the message passing process to distin-\nguish the differences of different types of edges. For any\n(h;r;t) 2T, its multi-head message is computed as follows:\nHMessage(h;r;t) = ||\ni∈[1;hn]\nHMSGHeadi (h;r;t);\nHMSGHeadi (h;r;t) = [VLineari(e(l−1)\nt )||R(l)(r)]:\n(6)\nTo get the i-th head message HMSGHeadi (h;r;t), we ﬁrst\napply a linear projection V Lineari to project the features of\ntail entity t, and then concatenate the features of tand rela-\ntion r. The ﬁnal heterogeneous message can be obtained by\nconcatenating all hn message headers.\n(d) Heterogeneous Aggregate. The ﬁnal step is to aggre-\ngate heterogeneous multi-head attentions and messages of en-\ntities (see Figure 3 (c)), thereby aggregating the information\nfrom neighbors with different feature to entity h. The up-\ndate vector ~e(l)\nh of hcan be obtained simply by averaging the\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1933\ncorresponding messages from neighborhood entities with the\nattention coefﬁcients as weights:\n~e(l)\nh = ⊕\n∀(r;t)∈RN(h)\nHAttention(h;r;t)·HMessage(h;r;t); (7)\nwhere \bdenotes the overlay operation. To incorporate the\nname features and the features obtained by the multilayer\nneural network, the residual connection is used to generate\nthe ﬁnal updated embeddings as following:\ne(l)\nh = w\fA Linear(~e(l)\nh ) + (1−w\f)N\nLinear(e(l−1)\nh ); (8)\nwhere w\f is trainable weights, A\nLinear(\u0001);N Linear(\u0001)\nare linear projections. Finally, we can generate relation-based\nembeddings Erel and path-based embeddingsEpathbased on\nentire relation structure Trel and path structure Tpath respec-\ntively, and use them for end-to-end EA tasks.\n3.5 Alignment Learning\nAfter obtaining the ﬁnal entity representations, we use Man-\nhattan distance to measure the similarity of candidate entity\npair. A smaller distance means a higher probability of en-\ntity alignment. The following function is used to compute the\nsimilarity of candidate entity pair based on Erel and Epath:\ndf(e1\ni;e1\nj) =∥e1\nf;i −e2\nf;j∥L1 ; (9)\nwhere f=frel;pathg; L1 indicates the Manhattan distance.\nTo capture various aspects of the entities, previous meth-\nods usually concatenate the multi-source embeddings of enti-\nties and directly use them for the loss function. However, we\nargue that the contribution of relation-based and path-based\nembeddings to EA should be different, since these two struc-\ntures of a entity may be quite diverse. Therefore, instead of\ndirectly using concatenated embeddings, we assign different\nweights to the loss functions of the two embeddings, thereby\ndistinguishing their different contributions during training. In\nview of this, the following margin-based ranking loss func-\ntion is used in model training, the goal of which is to keep the\nembedding distance of positive pair as small as possible and\nthe embedding distance of negative pair as large as possible:\nL=\nX\n(p;q)∈L;(p0;q0)∈L0\nrel\n[drel(p;q) −drel(p′;q′) +\r1]+\n+\u0012(\nX\n(p;q)∈L;(p0;q0)∈L0\npath\n[dpath(p;q) −dpath(p′;q′) +\r2]+ ;\n(10)\nwhere [\u0001]+ = maxf0;\u0001g; L′\nrel and L′\npath represent the neg-\native pair of relation-based and path-based embeddings, re-\nspectively; \r1;\r2 > 0 are the margin hyper-parameters for\nseparating positive and negative pairs, respectively.\n4 Experiments\nIn this section, we evaluate the performance of RPR-RHGT\non three widely used benchmark datasets. The code is now\navailable at https://github.com/cwswork/RPR-RHGT.\n4.1 Experiment Settings\nDatasets. Three experimental datasets contain cross-\nlingual datasets and mono-lingual dataset: WK31-15K [Sun\nDatasets KGs Entities Relation Path\nRels Triples P\naths Triples\nJA-EN(DBP) JA 65,744 2,043 164,373 139 283,311\nEN 95,680 2,096 233,319 266,759\nFR-EN(DBP) FR 66,858 1,379 192,191 172 559,984\nEN 105,889 2,209 278,590 505,443\nZH-EN(DBP) ZH 66,469 2,830 153,929 140 166,991\nEN 98,125 2,317 237,674 436,418\nEN-DE(V1) EN 15,000 215 47,676 13 12,393\nDE 15,000 131 50,419 18,153\nEN-DE(V2) EN 15,000 169 84,867 38 58,517\nDE 15,000 96 92,632 77,243\nEN-FR(V1) EN 15,000 267 47,334 46 51,349\nFR 15,000 210 40,864 50,504\nEN-FR(V2) EN 15,000 193 96,318 80 379,112\nFR 15,000 166 80,112 294,751\nDBP-WD DBP 100,000 330 463,294 460 1,834,831\nWD 100,000 220 448,774 2,709,929\nDBP-YG YG 100,000 302 428,952 115 1,148,939\nDBP 100,000 21 502,563 2,893,006\nTable 1: Statistics of datasets.\net al., 2020b] is from multi-lingual DBpedia and used to eval-\nuate model performance on sparse and dense datasets, where\neach subset contains two versions: V1 is sparse set obtained\nby using IDS algorithm, and V2 is twice as dense as V1.\nDBP-15K [Sun et al., 2017] is the most used dataset in the\nliterature, and is also from DBpedia. DWY-100K [Sun et al.,\n2018] contains two mono-lingual KGs, which serve as large-\nscale datasets to better evaluate the scalability of experimen-\ntal models. Table 1 outlines the statistics of above datasets\nwhich also contains the numbers of paths and path triples gen-\nerated by Algorithm 1, to demonstrate the effect of the RPR\nmodule. Due to ﬁve-fold cross-validation used onWK31-15K\nand DBP-15K, the ”Path.Paths” and ”Path.Triplets” of these\ntwo datasets are the average statistic for the ﬁve training sets.\nMetrics. Hits@k is the proportion of correctly alignment\nranked at the top-k candidates; MRR (Mean Meciprocal\nRank) is the average of the reciprocal ranks. Higher Hits@k\nand MRR scores indicate better performance of EA.\nBaselines. For WK31-15K and DBP-15K, we compare\nRPR-RHGT with eight previous state-of-the-art alignment\nmodels (mentioned in Section 2): MTransE [Chen et al.,\n2017], IPTransE [Zhu et al., 2017], JAPE [Sun et al., 2017],\nBootEA [Sun et al., 2018 ], AttrE [Trisedya et al., 2019 ],\nRDGCN [Wu et al., 2019], NMN [Wu et al., 2020], RAGA\n[Zhu et al., 2021]. Since only a few models are evaluated\non DWY-100K, we compare with the following models: Mul-\ntiKE [Zhang et al., 2019b], RDGCN [Wu et al., 2019], NMN\n[Wu et al., 2020], COTSAE [Yang et al., 2020].\nImplementation Settings. For WK31-15K and DBP-15K,\nthe proportion of train, validation and test is 2:1:7, the same\nas [Sun et al., 2020b]. For DWY-100K, we adopt the same\ntrain (30%) / test (70%) split as baselines. We use fast-\nText 1 to generate entity name embeddings that are uniformly\napplied to baseline recurrence, including RDGCN, NMN,\nRAGA, MultiKE and COTSAE. The embedding dimensions\nof 15K and 100K datasets are 300 and 200, respectively.\n1https://fasttext.cc/docs/en/crawl-vectors.html\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1934\nDatasets EN-DE(V1) EN-DE(V2) EN-FR(V1) EN-FR(V2)\nModels Hits@1 Hits@5\nMRR Hits@1 Hits@5 MRR Hits@1 Hits@5 MRR Hits@1 Hits@5 MRR\n*MTransE 30.7 51.8\n.407 19.3 35.2 .274 24.7 46.7 .351 24.0 43.6 .336\n*IPTransE 35.0 51.5 .430 47.6 67.8 .571 16.9 32.0 .243 23.6 44.9 .339\n*JAPE 28.8 51.2\n.394 16.7 32.9 .250 26.2 49.7 .372 29.2 52.4 .402\n*BootEA 67.5 82.0 .740 83.3 91.2 .869 50.7 71.8 .603 66.0 85.0 .745\n*AttrE 51.7 68.7 .597\n65.0 81.6 .726 48.1 67.1 .569 53.5 74.6 .631\nRDGCN 81.98 87.65 .846 81.61 86.98 .841 80.53 87.66 .837 87.12 92.88 .898\nNMN 85.57 90.45 .877\n85.18 89.57 .871 85.12 90.74 .876 89.29 94.28 .915\nRAGA 87.90 94.28 .908 81.34 89.15 .849 82.71 91.55 .867 88.95 95.36 .919\nw/o.RPR 90.26 95.58 .927 92.08\n96.39 .940 88.31 95.07 .913 93.60 97.57 .954\nRPR-RHGT 92.18 96.32 .940 93.80 97.20 .953 90.92 95.54 .930 94.95 98.00 .963\nImprov.\nbest 4.28 2.04 .032 8.62 7.63 .082 5.80 3.93 .054 5.66 2.64 .044\nDatasets JA-EN(DBP) FR-EN(DBP) ZH-EN(DBP)\nModels Hits@1\nHits@5 MRR Hits@1 Hits@5 MRR Hits@1 Hits@5 MRR\nMTransE 20.41 40.52\n.303 19.74 40.37 .297 20.89 42.09 .311\nIPTransE 27.92 52.70 .396 31.22 57.42 .434 17.34 37.05 .268\nJAPE 23.86 44.50\n.340 22.98 45.22 .336 26.46 50.30 .378\nBootEA 52.71 71.89 .616 57.61 77.27 .666 55.45 73.72 .639\nAttrE 35.96 60.31 .475\n40.21 66.09 .522 16.02 33.29 .250\nRDGCN 81.22 87.98 .844 80.88 88.08 .842 62.11 73.88 .676\nNMN 84.29 90.47 .870 83.46 90.10 .864 65.16\n76.64 .702\nRAGA 79.29 89.12 .838 85.27 93.17 .889 68.72 82.55 .750\nw/o.RPR 87.43 94.30 .905 87.69\n95.13 .910 66.85 81.60 .736\nRPR-RHGT 88.64 94.30 .912 88.92 95.59 .919 69.30 82.66 .754\nImprov.\nbest 4.35 3.83 .042 3.65 2.42 .030 0.58 0.11 .004\nTable 2: Overall performances of all models on WK31-15K and DBP-15K. “” marks the results obtained from OpenEA [Sun et al., 2020b].\nOther results are produced using their source code.\nFor all datasets, we use the same weight hyper-parameters:\n\u001csim = 0:5;\u001cpath = 20;hn=4;\r1 =\r2 =10, \u0012= 0:3.\n4.2 Main Results\nTables 2 and 3 report all performances on three datasets. The\nHits@k is in percentage (%), while number in bold denotes\nthe best results of all models and number in underlinedenotes\nthe best result of baselines.\nResults on WK31-15K. As shown in Table 2, RPR-RHGT\nachieves the best performance on WK31-15K, exceeding by\n4.28%\u00188.62% on Hits@1. By reducing the numbers of re-\nlations and triples, WK31-15K challenges the ability of EA\nmodels to model sparse KGs. RPR-RHGT achieves signif-\nicant improvements over the baselines on both sparse KGs\nand dense KGs. Besides, it is noteworthy that the improve-\nments of RPR-RHGT on Hits@1 are much higher than that\non Hits@5, indicating that RPR-RHGT can more accurately\nidentify true entity among the top-5 indistinguishable align-\nment candidates. This experiment shows RPR-RHGT can\ncompensate the neighborhood sparsity problem of some enti-\nties to a certain extent.\nResults on DBP-15K. From observing Table 2, the Hits@1\nof RPR-RHGT on DBP-15K is higher than the best baselines\nby 4.35%\u00180.58%, which indicates that our model performs\nbest on all DBP-15K. It is noteworthy that the performance\nof RAGA on ZH-EN(DBP) is comparable to that of RPR-\nRHGT. We believe that ZH-EN(DBP) has more mismatched\npaths as one of the reasons. As shown in Table 1, ZH-\nEN(DBP) has more relations than other datasets, but no more\nreliable paths obtained by RPR algorithm. Besides, NMN\nis one of the best performing baselines and effectively cap-\ntures the cross-graph information and relation information of\nKG, while RPR-RHGT still achieves good performance. Al-\nthough the gap between RPR-RHGT and RAGA is smaller,\nRPR-RHGT has an advantage on DBP-15K.\nDatasets DBP-WD DBP-YG\nModels Hits@1 Hits@10\nMRR Hits@1 Hits@10 MRR\nMultiKE 91.86 96.26 .935\n88.03 95.32 .906\nRDGCN 97.90 99.10 - 94.70 97.30 -\nNMN 98.10 99.20 - 96.00 98.20 -\nCOTSAE 92.68 97.86 .945 94.39 98.74 .961\nw/o.RPR 99.11 99.84 .994 96.30\n98.78 .972\nRPR-RHGT 99.26 99.86 .995 96.58 98.86 .974\nImprov.\nbest 1.16 0.66 .050 0.58 0.12 .013\nTable 3: Overall performance of all models onDWY-100K. All base-\nline performances are taken from their papers.\nResults on DWY-100K. As the largest dataset, DWY-100K\nraises challenges to the time and space complexity of EA\nmodels. As show in Table 3, although RPR-RHGT does not\nrely on attribute structures, it still outperforms all baselines\non DWY-100K. Since DWY-100K is several times larger than\nother datasets, this experiment demonstrates that RPR-RHGT\nhas good scalability and superiority in larger real-world and\nmonolingual KGs.\n4.3 Ablation Experiments\nw/o.RPR is the RPR-RHGT without RPR module, the results\nof which are shown at the bottom of Tables 2 and 3. It\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1935\n5% 10% 15% 20% 25% 30%\n(a)EN-DE(V1)\n75\n80\n85\n90\n95Hits@1…(%)\n78.64\n83.1482.77\n87.07\n81.51\n89.59\n87.05\n91.49\n87.67\n93.33\n5% 10% 15% 20% 25% 30%\n(b)EN-DE(V2)\n65\n70\n75\n80\n85\n90\n95\n79.08\n82.7182.57\n86.0\n66.09\n85.91\n87.42\n93.63\n88.24\n95.1\nRDGCN\nNMN\nRAGA\nw/o.RPR\nRPR-RHGT\nFigure 4: Performances with different ratios of pre-aligned entities.\nRDGCNNMN RAGAw/o.RPRRPR-RHGT\n(a)EN-DE(V1)\n50\n60\n70\n80\n90Hits@1…(%)\nRegular(name) Regular(model)\n+6.22\n+15.92\n+9.81\n+21.5\n+12.14\n+28.53\n+14.5\n+29.81\n+16.42\n+32.48\nRDGCNNMN RAGAw/o.RPRRPR-RHGT\n(b)EN-DE(V2)\n50\n60\n70\n80\n90\nHarder(name) Harder(model)\n+7.67\n+19.28\n+11.24\n+25.19\n+7.4\n+19.43\n+18.14\n+36.94\n+19.86\n+41.1\nFigure 5: Hits@1 performances under regular and harder settings.\ncan be observed that w/o.RPR performs better than all base-\nline models on all datasets, except for ZH-EN(DBP), which\nconﬁrms the effectiveness of RHGT design. Besides, RPR-\nRHGT achieves better performance than w/o.RPR across all\nmetrics and datasets. This experiment conﬁrms the assump-\ntion that the relational and path structure information of KGs\ncan mutually reinforce each other.\n4.4 Further Analysis\nSensitivity to Ratios of Pre-Aligned Entities. To explore\nthe impact of pre-aligned entities on EA model training, we\nimplement a further evaluation based on different ratios of\ntraining set. We take EN-DE(V1) and EN-DE(V2) as exam-\nples, and vary the ratio from 5% to 30%, while the valida-\ntion dataset remains at 10%. RDGCN, NMN and RAGA are\nchosen as comparison models, all of which use name embed-\ndings and perform best among baselines. As shown in Fig-\nure 4, our two models maintain consistent performance, sig-\nniﬁcantly outperforming the baselines on training sets for all\nscales. This indicates that RPR-RHGT can achieves satisfac-\ntory results based on fewer pre-aligned entities.\nAnalysis on Harder Datasets. For a more objective eval-\nuation of EA models, we take EN-DE(V1) and EN-DE(V2)\nas examples (called regular datasets), to construct two ex-\nperimental datasets with relatively low similarities of entity\nnames (called harder datasets). Speciﬁcally, we ﬁrst com-\npute the name embedding similarities of aligned entity pairs\nand rank them (low to high), then pick the highest-ranked\n50% as the harder datasets, which are divided in the same\nway as above. To compare the effects of name embeddings\non the performances of regular and harder datasets, we also\ncompute the alignment accuracy of entity embeddings based\nonly on their name embeddings without training, i.e., Regu-\nModel w/o.RPR RPR-RHGT RA GA\nNMN\nTake time(s) CPU GPU CPU GPU CPU CPU\nTrain of each\nepoch 10.27 0.27 15.48 0.40 8.71 29.74\nAlignment of test set 2.56 1.92 3.49 2.04 33.89 101.49\nTable 4: Comparison of training time and alignment time.\nlar(name) and Harder(name). As shown in Figure 5, the per-\nformances of all models based on name embeddings drop on\nharder datasets. However, comparing the performance on reg-\nular datasets, the performance of all models on harder dataset\nshows a more signiﬁcant improvement over the performance\nof name embeddings. In particular, RPR-RHGT achieves up\nto 32.48% and 41.1% improvement over the name embed-\ndings in Hits@1 on two harder datasets. This result demon-\nstrates the robustness of RPR-RHGT, which can still promote\neffective EA on the datasets with less similar entity names.\nAnalysis on Training Time and Alignment Time. To\nevaluate the training and alignment efﬁciency ofRPR-RHGT,\nwe compare the training time and alignment time of the fol-\nlowing four models on EN-DE(V1). The results running on\na workstation with CPU (EPYC 3975WX +256G RAM) and\nGPU (RTX A4000 with 16G) are shown in Table 4, which\nshows large differences between different methods. Although\nthe training time of RPR-RHGT is not optimal, its time com-\nplexity is competitive. Overall, our model balances well be-\ntween effectiveness and efﬁciency.\n5 Conclusions\nTraditional GNNs either donot consider the heterogeneous\ninformation of KGs, or cannot effectively extract heteroge-\nneous information that is effective for EA tasks. This paper\nproposes a new EA framework, RPR-RHGT, which focuses\non mining reliable path information and heterogeneous infor-\nmation, thereby making full use of KGs’ own relation struc-\ntures to improve alignment accuracy. First, we develop aRPR\nalgorithm, which infers reliable paths from relation structures\nand only needs to be executed once. This algorithm is the\nﬁrst in the literature to successfully use unrestricted path in-\nformation. Second, we improve a RHGT model for modeling\nthe heterogeneity of KGs, to better capture the heterogeneous\nneighborhood similarity of aligned entities. Experimental re-\nsults show RPR-RHGT not only outperforms state-of-the-art\nmodels, but also achieves better performance in multiple ab-\nlation studies and analysis experiments. In the future, we will\ncontinue to explore better ways to mine the heterogeneous\ninformation and path information of KGs for EA tasks.\nAcknowledgments\nThe works described in this paper are supported by The Na-\ntional Natural Science Foundation of China under Grant Nos.\n61772210 and U1911201; Guangdong Province Universities\nPearl River Scholar Funded Scheme (2018); The Project of\nScience and Technology in Guangzhou in China under Grant\nNo. 202007040006.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1936\nReferences\n[Bordes et al., 2013] Antoine Bordes, Nicolas Usunier,\nAlberto Garc ´ıa-Dur´an, Jason Weston, and Oksana\nYakhnenko. Translating Embeddings for Modeling\nMulti-relational Data. In Proceedings of NeurIPS, 2013.\n[Busbridge et al., 2019] Dan Busbridge, Dane Sherburn,\nPietro Cavallo, and Nils Y . Hammerla. Relational Graph\nAttention Networks. In Proceedings of ICLR, 2019.\n[Cai et al., 2022] Weishan Cai, Yizhao Wang, Shun Mao,\nJieyu Zhan, and Yuncheng Jiang. Multi-heterogeneous\nNeighborhood-aware for Knowledge Graphs Alignment.\nInformation Processing & Management, (1), 2022.\n[Chen et al., 2017] Muhao Chen, Yingtao Tian, Mohan\nYang, and Carlo Zaniolo. Multilingual Knowledge Graph\nEmbeddings for Cross-lingual Knowledge Alignment. In\nProceedings of IJCAI, 2017.\n[Chen et al., 2018] Muhao Chen, Yingtao Tian, Kai-Wei\nChang, Steven Skiena, and Carlo Zaniolo. Co-training\nEmbeddings of Knowledge Graphs and Entity Descrip-\ntions for Cross-lingual Entity Alignment. In Proceedings\nof IJCAI, 2018.\n[Hu et al., 2020] Ziniu Hu, Yuxiao Dong, Kuansan Wang,\nand Yizhou Sun. Heterogeneous Graph Transformer. In\nProceedings of WWW, 2020.\n[Li et al., 2019] Yujia Li, Chenjie Gu, Thomas Dullien,\nOriol Vinyals, and Pushmeet Kohli. Graph Matching Net-\nworks for Learning the Similarity of Graph Structured Ob-\njects. In Proceedings of ICML, 2019.\n[Liu et al., 2020] Zhiyuan Liu, Yixin Cao, Liangming Pan,\nJuanzi Li, Zhiyuan Liu, and Tat-Seng Chua. Exploring\nand Evaluating Attributes, Values, and Structures for En-\ntity Alignment. In Proceedings of EMNLP, 2020.\n[Liu et al., 2021] Fangyu Liu, Muhao Chen, Dan Roth, and\nNigel Collier. Visual Pivoting for (Unsupervised) Entity\nAlignment. In Proceedings of AAAI, 2021.\n[Mei et al., 2022] Xin Mei, Xiaoyan Cai, Libin Yang, and\nNanxin Wang. Relation-aware Heterogeneous Graph\nTransformer based drug repurposing. Expert Systems with\nApplications, 2022.\n[Schlichtkrull et al., 2018] Michael Schlichtkrull, Thomas N\nKipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and\nMax Welling. Modeling relational data with graph convo-\nlutional networks. In Proceedings of ESWC, 2018.\n[Sun et al., 2017] Zequn Sun, Wei Hu, and Chengkai Li.\nCross-Lingual Entity Alignment via Joint Attribute-\nPreserving Embedding. In Proceedings of ISWC, 2017.\n[Sun et al., 2018] Zequn Sun, Wei Hu, Qingheng Zhang,\nand Yuzhong Qu. Bootstrapping Entity Alignment with\nKnowledge Graph Embedding. In Proceedings of IJCAI,\n2018.\n[Sun et al., 2020a] Zequn Sun, Chengming Wang, Wei Hu,\nMuhao Chen, Jian Dai, Wei Zhang, and Yuzhong Qu.\nKnowledge Graph Alignment Network with Gated Multi-\nHop Neighborhood Aggregation. In Proceedings of AAAI,\n2020.\n[Sun et al., 2020b] Zequn Sun, Qingheng Zhang, Wei Hu,\nChengming Wang, Muhao Chen, Farahnaz Akrami, and\nChengkai Li. A Benchmarking Study of Embedding-based\nEntity Alignment for Knowledge Graphs. In Proceedings\nof PVLDB, 2020.\n[Trisedya et al., 2019] Bayu Distiawan Trisedya, Jianzhong\nQi, and Rui Zhang. Entity Alignment between Knowledge\nGraphs Using Attribute Embeddings. In Proceedings of\nAAAI, 2019.\n[Wang et al., 2019] Xiao Wang, Houye Ji, Chuan Shi, Bai\nWang, Yanfang Ye, Peng Cui, and Philip S Yu. Heteroge-\nneous Graph Attention Network. InProceedings of WWW,\n2019.\n[Wu et al., 2019] Yuting Wu, Xiao Liu, Yansong Feng,\nZheng Wang, Rui Yan, and Dongyan Zhao. Relation-\nAware Entity Alignment for Heterogeneous Knowledge\nGraphs. In Proceedings of IJCAI, 2019.\n[Wu et al., 2020] Yuting Wu, Xiao Liu, Yansong Feng,\nZheng Wang, and Dongyan Zhao. Neighborhood Match-\ning Network for Entity Alignment. InProceedings of ACL,\n2020.\n[Xiang et al., 2021] Yuejia Xiang, Ziheng Zhang, Jiaoyan\nChen, Xi Chen, Zhenxi Lin, and Yefeng Zheng. OntoEA:\nOntology-guided Entity Alignment via Joint Knowledge\nGraph Embedding. In Proceedings of ACL, 2021.\n[Yang et al., 2019] Hsiu-Wei Yang, Yanyan Zou, Peng Shi,\nWei Lu, Jimmy Lin, and Xu Sun. Aligning Cross-Lingual\nEntities with Multi-Aspect Information. In Proceedings of\nEMNLP-IJCNLP, 2019.\n[Yang et al., 2020] Kai Yang, Shaoqin Liu, Junfeng Zhao,\nYasha Wang, and Bing Xie. COTSAE: CO-Training of\nStructure and Attribute Embeddings for Entity Alignment.\nIn Proceedings of AAAI, 2020.\n[Yun et al., 2019] Seongjun Yun, Minbyul Jeong, Raehyun\nKim, Jaewoo Kang, and Hyunwoo J Kim. Graph Trans-\nformer Networks. In Proceedings of NeurIPS, 2019.\n[Zhang et al., 2019a] Chuxu Zhang, Dongjin Song, Chao\nHuang, Ananthram Swami, and Nitesh V . Chawla. Het-\nerogeneous Graph Neural Network. In Proceedings of\nSIGKDD, 2019.\n[Zhang et al., 2019b] Qingheng Zhang, Zequn Sun, Wei Hu,\nMuhao Chen, Lingbing Guo, and Yuzhong Qu. Multi-view\nKnowledge Graph Embedding for Entity Alignment. In\nProceedings of IJCAI, 2019.\n[Zhu et al., 2017] Hao Zhu, Ruobing Xie, Zhiyuan Liu, and\nMaosong Sun. Iterative Entity Alignment via Joint Knowl-\nedge Embeddings. In Proceedings of IJCAI, 2017.\n[Zhu et al., 2020] Qi Zhu, Hao Wei, Bunyamin Sisman,\nDa Zheng, Christos Faloutsos, Xin Luna Dong, and Ji-\nawei Han. Collective Multi-type Entity Alignment Be-\ntween Knowledge Graphs. In Proceedings of WWW, 2020.\n[Zhu et al., 2021] Renbo Zhu, Meng Ma, and Ping Wang.\nRAGA: Relation-Aware Graph Attention Networks for\nGlobal Entity Alignment. In Proceedings of PAKDD,\n2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1937",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7716427445411682
    },
    {
      "name": "Relation (database)",
      "score": 0.6459781527519226
    },
    {
      "name": "Path (computing)",
      "score": 0.6207408308982849
    },
    {
      "name": "Knowledge graph",
      "score": 0.5914050340652466
    },
    {
      "name": "Transformer",
      "score": 0.5695971250534058
    },
    {
      "name": "Graph",
      "score": 0.5363110303878784
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4675741493701935
    },
    {
      "name": "Data mining",
      "score": 0.41288474202156067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38934698700904846
    },
    {
      "name": "Information retrieval",
      "score": 0.3305009603500366
    },
    {
      "name": "Programming language",
      "score": 0.11439642310142517
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165736",
      "name": "Hanshan Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I187400657",
      "name": "South China Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 20
}