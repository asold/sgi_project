{
  "title": "Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models",
  "url": "https://openalex.org/W3170488153",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3152562840",
      "name": "James Y. Huang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2246354230",
      "name": "Kuan-Hao Huang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2250473257",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3155393281",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2963090765",
    "https://openalex.org/W4300822525",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2951535825",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W2962917899",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2931212643",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W3034531294",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2915240437",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W3103680885",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2963506530",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W3040809437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W2468484304"
  ],
  "abstract": "Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-the-art sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1372–1379\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n1372\nDisentangling Semantics and Syntax in Sentence Embeddings\nwith Pre-trained Language Models\nJames Y. Huangand Kuan-Hao Huang and Kai-Wei Chang\nUniversity of California, Los Angeles\n{jyhuang, khhuang, kwchang}@cs.ucla.edu\nAbstract\nPre-trained language models have achieved\nhuge success on a wide range of NLP tasks.\nHowever, contextual representations from pre-\ntrained models contain entangled semantic and\nsyntactic information, and therefore cannot be\ndirectly used to derive useful semantic sen-\ntence embeddings for some tasks. Paraphrase\npairs offer an effective way of learning the\ndistinction between semantics and syntax, as\nthey naturally share semantics and often vary\nin syntax. In this work, we present ParaBART,\na semantic sentence embedding model that\nlearns to disentangle semantics and syntax in\nsentence embeddings obtained by pre-trained\nlanguage models. ParaBART is trained to per-\nform syntax-guided paraphrasing, based on a\nsource sentence that shares semantics with the\ntarget paraphrase, and a parse tree that speci-\nﬁes the target syntax. In this way, ParaBART\nlearns disentangled semantic and syntactic rep-\nresentations from their respective inputs with\nseparate encoders. Experiments in English\nshow that ParaBART outperforms state-of-the-\nart sentence embedding models on unsuper-\nvised semantic similarity tasks. Additionally,\nwe show that our approach can effectively re-\nmove syntactic information from semantic sen-\ntence embeddings, leading to better robustness\nagainst syntactic variation on downstream se-\nmantic tasks.\n1 Introduction\nSemantic sentence embedding models encode sen-\ntences into ﬁxed-length vectors based on their se-\nmantic relatedness with each other. If two sen-\ntences are more semantically related, their corre-\nsponding sentence embeddings are closer. As sen-\ntence embeddings can be used to measures seman-\ntic relatedness without requiring supervised data,\nthey have been used in many applications, such as\nsemantic textual similarity (Agirre et al., 2016a),\nquestion answering (Nakov et al., 2017), and nat-\nural language inference (Artetxe and Schwenk,\n2019a).\nRecent years have seen huge success of pre-\ntrained language models across a wide range of\nNLP tasks (Devlin et al., 2019; Lewis et al., 2020).\nHowever, several studies (Reimers and Gurevych,\n2019; Li et al., 2020) have found that sentence\nembeddings from pre-trained language models per-\nform poorly on semantic similarity tasks when the\nmodels are not ﬁne-tuned on task-speciﬁc data.\nMeanwhile, Goldberg (2019) shows that BERT\nwithout ﬁne-tuning performs surprisingly well on\nsyntactic tasks. Hence, we posit that these con-\ntextual representations from pre-trained language\nmodels without ﬁne-tuning capture entangled se-\nmantic and syntactic information, and therefore are\nnot suitable for sentence-level semantic tasks.\nIdeally, the semantic embedding of a sentence\nshould not encode its syntax, and two semantically\nsimilar sentences should have close semantic em-\nbeddings regardless of their syntactic differences.\nWhile various models (Conneau et al., 2017; Cer\net al., 2018; Reimers and Gurevych, 2019) have\nbeen proposed to improve the performance of sen-\ntence embeddings on downstream semantic tasks,\nmost of these approaches do not attempt to separate\nsyntactic information from sentence embeddings.\nTo this end, we propose ParaBART, a semantic\nsentence embedding model that learns to disen-\ntangle semantics and syntax in sentence embed-\ndings. Our model is built upon BART (Lewis\net al., 2020), a sequence-to-sequence Transformer\n(Vaswani et al., 2017) model pre-trained with self-\ndenoising objectives. Parallel paraphrase data is\na good source of learning the distinction between\nsemantics and syntax, as paraphrase pairs naturally\nshare the same meaning but often differ in syntax.\nTaking advantage of this fact, ParaBART is trained\nto perform syntax-guided paraphrasing, where a\nsource sentence containing the desired semantics\nand a parse tree specifying the desired syntax are\ngiven as inputs. In order to generate a paraphrase\n1373\nthat follows the given syntax, ParaBART uses sep-\narate encoders to learn disentangled semantic and\nsyntactic representations from their respective in-\nputs. In this way, the disentangled representations\ncapture sufﬁcient semantic and syntactic informa-\ntion needed for paraphrase generation. The seman-\ntic encoder is also encouraged to ignore the syntax\nof the source sentence, as the desired syntax is\nalready provided by the syntax input.\nParaBART achieves strong performance across\nunsupervised semantic textual similarity tasks.\nFurthermore, semantic embeddings learned by\nParaBART contain signiﬁcantly less syntactic infor-\nmation as suggested by probing results, and yield\nrobust performance on datasets with syntactic vari-\nation.\nOur source code is available at https://\ngithub.com/uclanlp/ParaBART.\n2 Related Work\nVarious sentence embedding models have been pro-\nposed in recent years. Most of these models uti-\nlize supervision from parallel data (Wieting and\nGimpel, 2018; Artetxe and Schwenk, 2019b; Wi-\neting et al., 2019, 2020), natural language infer-\nence data (Conneau et al., 2017; Cer et al., 2018;\nReimers and Gurevych, 2019), or a combination of\nboth (Subramanian et al., 2018).\nMany efforts towards controlled text generation\nhave been focused on learning disentangled sen-\ntence representations (Hu et al., 2017; Fu et al.,\n2018; John et al., 2019). In the context of disen-\ntangling semantics and syntax, Bao et al. (2019)\nand Chen et al. (2019) utilize variational autoen-\ncoders to learn two latent variables for semantics\nand syntax. In contrast, we use the outputs of a\nconstituency parser to learn purely syntactic rep-\nresentations, and facilitate the usage of powerful\npre-trained language models as semantic encoders.\nOur approach is also related to prior work\non syntax-controlled paraphrase generation (Iyyer\net al., 2018; Kumar et al., 2020; Goyal and Dur-\nrett, 2020; Huang and Chang, 2021). While these\napproaches focus on generating high-quality para-\nphrases that conform to the desired syntax, we are\ninterested in how semantic and syntactic informa-\ntion can be disentangled and how to obtain good\nsemantic sentence embeddings.\nFigure 1: An overview of ParaBART. The model ex-\ntracts semantic and syntactic representations from a\nsource sentence and a target parse respectively, and\nuses both the semantic sentence embedding and the tar-\nget syntactic representations to generate the target para-\nphrase. ParaBART is trained in an adversarial setting,\nwith the syntax discriminator (red) trying to decode\nthe source syntax from the semantic embedding, and\nthe paraphrasing model (blue) trying to fool the syntax\ndiscriminator and generate the target paraphrase at the\nsame time.\n3 Proposed Model – ParaBART\nOur goal is to build a semantic sentence embedding\nmodel that learns to separate syntax from seman-\ntic embeddings. ParaBART is trained to generate\nsyntax-guided paraphrases, where the model at-\ntempts to only extract the semantic part from the\ninput sentence, and combine it with a different syn-\ntax speciﬁed by the additional syntax input in the\nform of a constituency parse tree.\nFigure 1 outlines the proposed model, which\nconsists of a semantic encoder that learns the se-\nmantics of a source sentence, a syntactic encoder\nthat encodes the desired syntax of a paraphrase,\nand a decoder that generates a corresponding para-\nphrase. Additionally, we add a syntax discriminator\nto adversarially remove syntactic information from\nthe semantic embeddings.\nGiven a source sentence S1 and a target con-\nstituency parse tree P2, ParaBART is trained to\ngenerate a paraphrase S2 that shares the semantics\nof S1 and conforms to the syntax speciﬁed by P2.\nSemantics and syntax are two key aspects that de-\ntermine how a sentence is generated. Our model\nlearns purely syntactic representations from the out-\nput trees generated by a constituency parser, and\nextracts the semantic embedding directly from the\nsource sentence. The syntax discriminator and the\n1374\nsyntactic encoder are designed to remove source\nsyntax and provide target syntax, thus encourag-\ning the semantic encoder to only capture source\nsemantics.\nSemantic Encoder The semantic encoder Esem\nis a Transformer encoder that embeds a sentence\nS = (s(1),...,s (m)) into contextual semantic repre-\nsentations:\nU = (u(1),..., u(m)) =Esem\n(\n(s(1),...,s (m))\n)\n.\nThen, we take the mean of these contextual repre-\nsentations u(i) to get a ﬁxed-length semantic sen-\ntence embedding\n¯ u= 1\nm\nm∑\ni=1\nu(i).\nSyntactic Encoder The syntactic encoder Esyn\nis a Transformer encoder that takes a linearized\nconstituency parse tree P = (p(1),...,p (n)) and\nconverts it into contextual syntactic representations\nV = (v(1),..., v(n)) =Esyn\n(\n(p(1),...,p (n))\n)\n.\nFor example, the linearized parse tree of the sen-\ntence “This book is good.” is “(S (NP (DT) (NN))\n(VP (VBZ) (ADJP)) (.))”. Such input sequence\npreserves the tree structure, allowing the syntac-\ntic encoder to capture the exact syntax needed for\ndecoding.\nDecoder The decoder Ddec uses the semantic\nsentence embedding ¯ uand the contextual syntac-\ntic representations V to generate a paraphrase that\nshares semantics with the source sentence while\nfollowing the syntax of the given parse tree. In\nother words,\n(y(1),...,y (l)) =Ddec (Concat(¯ u,V )) .\nDuring training, given a source sentence S1, a tar-\nget parse tree P2 and a target paraphrase S2 =\n(s1\n2,...,s l\n2), we minimize the following paraphrase\ngeneration loss:\nLpara = −\nl∑\ni=1\nlog P(y(i) = s(i)\n2 |S1,P2).\nSince the syntactic representations do not contain\nsemantics, the semantic encoder needs to accu-\nrately capture the semantics of the source sentence\nfor a paraphrase to be generated. Meanwhile, the\nfull syntactic structure of the target is provided by\nthe syntactic encoder, thus encouraging the seman-\ntic encoder to ignore the source syntax.\nSyntax Discriminator To further encourage the\ndisentanglement of semantics and syntax, we em-\nploy a syntax discriminator to adversarially remove\nsyntactic information from semantic embeddings.\nWe ﬁrst train the syntax discriminator to predict\nthe syntax from its semantic embedding, and then\ntrain the semantic encoder to “fool” the syntax dis-\ncriminator such that the source syntax cannot be\npredicted from the semantic embedding.\nMore speciﬁcally, we adopt a simpliﬁed ap-\nproach similar to John et al. (2019) by encoding\nsource syntax as a Bag-of-Words vector h of its\nconstituency parse tree. For any given source parse\ntree, this vector contains the count of occurrences\nof every constituent tag, divided by the total num-\nber of constituents in the parse tree. Given the\nsemantic sentence embedding ¯ u, our linear syntax\ndiscriminator Ddis predicts h by\nyh = Ddis(¯ u) =softmax(W¯ u+ b)\nwith the following adversarial loss:\nLadv = −\n∑\nt∈T\nh(t) log(yh(t)),\nwhere T denotes the set of all constituent tags.\nTraining We adversarially train Esem, Esyn,\nDdec, and Ddis with the following objective:\nmin\nEsem,Esyn,Ddec\n(\nmax\nDdis\n(Lpara −λadvLadv)\n)\n,\nwhere λadv is a hyperparameter to balance loss\nterms. In each iteration, we update the Ddis by\nconsidering the inner optimization, and then up-\ndate Esem, Esyn and Ddec by considering the outer\noptimization.\n4 Experiments\nIn this section, we demonstrate that ParaBART is\ncapable of learning semantic sentence embeddings\nthat capture semantic similarity, contain less syn-\ntactic information, and yield robust performance\nagainst syntactic variation on semantic tasks.\n4.1 Setup\nWe sample 1 million English paraphrase pairs from\nParaNMT-50M (Wieting and Gimpel, 2018), and\nsplit this dataset into 5,000 pairs as the validation\nset and the rest as our training set. The constituency\nparse trees of all sentences are obtained from Stan-\nford CoreNLP (Manning et al., 2014). We ﬁne-\ntune a 6-layer BARTbase encoder as the semantic\n1375\nModel STS12 STS13 STS14 STS15 STS16 STS-B Avg.\nAvg. BERT embeddings (Devlin et al., 2019) 46.9 52.8 57.2 63.5 64.5 47.9 55.5\nAvg. BART embeddings (Lewis et al., 2020) 50.8 42.8 56.1 63.9 59.5 52.0 54.2\nInferSent (Conneau et al., 2017) 59.3 59.0 70.0 71.5 71.5 70.0 66.9\nVGV AE (Chen et al., 2019) 61.8 62.2 69.2 72.5 67.8 74.2 68.0\nUSE (Cer et al., 2018) 61.4 63.5 70.6 74.3 73.9 74.2 69.7\nSentence-BERT (Reimers and Gurevych, 2019) 64.6 67.5 73.2 74.3 70.1 74.1 70.6\nBGT (Wieting et al., 2020) 68.9 62.2* 75.9 79.4 79.3 - -\nParaBART 68.4 71.1 76.4 80.7 80.1 78.5 75.9\n- w/o adversarial loss 67.5 70.0 75.8 80.9 80.0 78.7 75.5\n- w/o adversarial loss and syntactic guidance 66.4 65.3 73.6 80.0 78.6 75.4 73.2\nTable 1: Pearson’s r (in percentage) between cosine similarity of sentence embeddings and gold labels on STS\ntasks from 2012 to 2016 and STS Benchmark test set. BGT results are taken from Wieting et al. (2020). *BGT is\nevaluated on an additional dataset from STS13, which is not included in the standard SentEval toolkit.\nencoder and the ﬁrst BARTbase decoder layer as the\ndecoder for our model.\nWe train ParaBART on a GTX 1080Ti GPU us-\ning AdamW (Loshchilov and Hutter, 2019) opti-\nmizer with a learning rate of 2 ×10−5 for the en-\ncoder and syntax discriminator, and 1 ×10−4 for\nthe rest of the model. The batch size is set to 64.\nAll models are trained for 10 epochs, which takes\nabout 2 days to complete. The maximum length of\ninput sentences and linearized parse trees are set\nto 40 and 160 respectively. We set the weight of\nadversarial loss to 0.1. Appendix A shows more\nimplementation details.\nBaselines We compare our model with other\nsentence embeddings models, including InferSent\n(Conneau et al., 2017), Universal Sentence En-\ncoder (USE) (Cer et al., 2018), Sentence-BERTbase\n(Reimers and Gurevych, 2019), VGV AE (Chen\net al., 2019), and BGT (Wieting et al., 2020). We\nalso include mean-pooled BERTbase and BARTbase\nembeddings. In addition to ParaBART, we consider\ntwo model ablations: ParaBART without adversar-\nial loss, and ParaBART without syntactic guidance\nand adversarial loss.\n4.2 Semantic Textual Similarity\nWe evaluate our semantic sentence embeddings\non the unsupervised Semantic Textual Similarity\n(STS) tasks from SemEval 2012 to 2016 (Agirre\net al., 2012; 2013; 2014; 2015; 2016b) and STS\nBenchmark test set (Cer et al., 2017), where the\ngoal is to predict a continuous-valued score be-\ntween 0 and 5 indicating how similar the meanings\nof a sentence pair are. For all models, we compute\nthe cosine similarity of embedding vectors as the\nsemantic similarity measure. We use the standard\nSentEval toolkit (Conneau and Kiela, 2018) for\nevaluation and report average Pearson correlation\nover all domains.\nModel BShift TreeDepth TopConst\nAvg. BART embed. 90.5 47.8 80.1\nParaBART 72.4 33.9 67.2\n- w/o AL 75.4 36.6 71.7\n- w/o AL and SG 83.3 46.5 83.1\nTable 2: Results on syntactic probing tasks. Semantic\nembeddings with lower accuracy on downstream syn-\ntactic tasks contain less syntactic information, suggest-\ning better disentanglement of semantics and syntax. AL\nand SG denote adversarial loss and syntactic guidance,\nrespectively.\nAs shown in Table 1, both average BERT em-\nbeddings and average BART embeddings perform\npoorly on STS tasks, as the entanglement of seman-\ntic and syntactic information leads to low correla-\ntion with semantic similarity. Training ParaBART\non paraphrase data substantially improves the cor-\nrelation. With the addition of syntactic guidance\nand adversarial loss, ParaBART achieves the best\noverall performance across STS tasks, showing the\neffectiveness of our approach.\n4.3 Syntactic Probing\nTo better understand how well our model learns\nto disentangle syntactic information from seman-\ntic embeddings, we probe our semantic sentence\nembeddings with downstream syntactic tasks. Fol-\nlowing Conneau et al. (2018), we investigate to\nwhat degree our semantic sentence embeddings\ncan be used to identify bigram word reordering\n(BShift), estimate parse tree depth (TreeDepth),\nand predict parse tree top-level constituents (Top-\nConst). Top-level constituents are deﬁned as the\ngroup of constituency parse tree nodes immediately\nbelow the sentence (S) node. We use the datasets\nprovided by SentEval (Conneau and Kiela, 2018)\nto train a Multi-Layer Perceptron classiﬁer with a\nsingle 50-neuron hidden layer on top of semantic\nsentence embeddings, and report accuracy on all\n1376\nQQP-Easy\nWhat are the essential skills of the project management?\nWhat are the essential skills of a project manager?\nQQP-Hard\nIs there a reason why we should travel alone?\nWhat are some reasons to travel alone?\nTable 3: Examples of paraphrase pairs fromQQP-Easy\nand QQP-Hard.\ntasks.\nAs shown in Table 2, sentence embeddings\npooled from pre-trained BART model contain rich\nsyntactic information that can be used to accurately\npredict syntactic properties including word order\nand top-level constituents. The disentanglement\ninduced by ParaBART is evident, lowering the ac-\ncuracy of downstream syntactic tasks by more than\n10 points compared to pre-trained BART embed-\ndings and ParaBART without adversarial loss and\nsyntactic guidance. The results suggest that the se-\nmantic sentence embeddings learned by ParaBART\nindeed contain less syntactic information.\n4.4 Robustness Against Syntactic Variation\nIntuitively, semantic sentence embedding models\nthat learn to disentangle semantics and syntax are\nexpected to yield more robust performance on\ndatasets with high syntactic variation. We consider\nthe task of paraphrase detection on Quora Ques-\ntion Pairs (Iyer et al., 2017) dev set as a testbed for\nevaluating model robustness. We categorize para-\nphrase pairs based on whether they share the same\ntop-level constituents. We randomly sample 1,000\nparaphrase pairs from each of the two classes, com-\nbined with a common set of 1,000 randomly sam-\npled non-paraphrase pairs, to create two datasets\nQQP-Easy and QQP-Hard. Paraphrase pairs from\nQQP-Hard are generally harder to identify as they\nare much more syntactically different compared\nto those from QQP-Easy. Table 3 shows some\nexamples from these two datasets. We evaluate\nsemantic sentence embeddings on these datasets in\nan unsupervised manner by computing the cosine\nsimilarity as the semantic similarity measure. We\nsearch for the best threshold between -1 and 1 with\na step size of 0.01 on each dataset, and report the\nhighest accuracy. The results are shown in Table 4.\nWhile Universal Sentence Encoder scores much\nhigher than other models on QQP-Easy, its perfor-\nmance degrades signiﬁcantly on QQP-Hard. In\ncomparison, ParaBART demonstrates better robust-\nness against syntactic variation, and surpasses USE\nto become the best model on the more syntactically\nModel QQP-Easy QQP-Hard\nAvg. BART embed. 72.3 64.1\nInferSent 72.1 67.5\nVGV AE 71.5 67.1\nUSE 80.7 72.4\nSentence-BERT 74.3 70.7\nParaBART 76.5 72.7\n- w/o AL 76.8 72.1\n- w/o AL and SG 76.1 69.9\nTable 4: Results on QQP-Easy and QQP-Hard. For ev-\nery model we report the highest accuracy after ﬁnding\nthe best threshold. AL and SG denote adversarial loss\nand syntactic guidance, respectively.\ndiverse QQP-Hard. It is worth mentioning that\neven pre-trained BART embeddings give decent\nresults on QQP-Easy, suggesting large overlaps\nbetween paraphrase pairs from QQP-Easy. On the\nother hand, the poor performance of pre-trained\nBART embeddings on a more syntactically diverse\ndataset like QQP-Hard clearly shows its incompe-\ntence as semantic sentence embeddings.\n5 Conclusion\nIn this paper, we present ParaBART, a semantic\nsentence embedding model that learns to disentan-\ngle semantics and syntax in sentence embeddings\nfrom pre-trained language models. Experiments\nshow that our semantic sentence embeddings yield\nstrong performance on unsupervised semantic sim-\nilarity tasks. Further investigation demonstrates\nthe effectiveness of disentanglement, and robust-\nness of our semantic sentence embeddings against\nsyntactic variation on downstream semantic tasks.\nAcknowledgments\nWe thank anonymous reviewers for their helpful\nfeedback. We thank UCLA-NLP group for the\nvaluable discussions and comments. This work is\nsupported in part by Amazon Research Award.\nEthics Considerations\nOur sentence embeddings can potentially capture\nbias reﬂective of the training data we use, which\nis a common problem for models trained on large\nannotated datasets. While the focus of our work\nis to disentangle semantics and syntax, our model\ncan potentially generate offensive or biased content\nlearned from training data if it is used for para-\nphrase generation. We suggest carefully examining\nthe potential bias exhibited in our models before\ndeploying them in any real-world applications.\n1377\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, German Rigau, Larraitz Uria, and Janyce\nWiebe. 2015. SemEval-2015 task 2: Semantic tex-\ntual similarity, English, Spanish and pilot on inter-\npretability. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015) ,\npages 252–263.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. SemEval-2014 task 10: Multilingual\nsemantic textual similarity. In Proceedings of the\n8th International Workshop on Semantic Evaluation\n(SemEval 2014), pages 81–91.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016a. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evalua-\ntion (SemEval-2016), pages 497–511.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016b. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evalua-\ntion (SemEval-2016), pages 497–511.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In *SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics – Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385–\n393.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\ntask: Semantic textual similarity. In Second Joint\nConference on Lexical and Computational Seman-\ntics (*SEM), Volume 1: Proceedings of the Main\nConference and the Shared Task: Semantic Textual\nSimilarity, pages 32–43.\nMikel Artetxe and Holger Schwenk. 2019a. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nMikel Artetxe and Holger Schwenk. 2019b. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7(0).\nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou,\nOlga Vechtomova, Xin-yu Dai, and Jiajun Chen.\n2019. Generating sentences from disentangled syn-\ntactic and semantic spaces. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6008–6019.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Evalu-\nation (SemEval-2017), pages 1–14.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019. A multi-task approach for dis-\nentangling syntax and semantics in sentence repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2453–2464.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 670–680.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Ex-\nploration and evaluation. In Proceedings of The\n1378\nThirty-Second Association for the Advancement of\nArtiﬁcial Intelligence Conference on Artiﬁcial Intel-\nligence (AAAI).\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. CoRR, abs/1901.05287.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic\npreordering for controlled paraphrase generation. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 238–\n252.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Toward con-\ntrolled generation of text. Proceedings of Machine\nLearning Research.\nKuan-Hao Huang and Kai-Wei Chang. 2021. Generat-\ning syntactically controlled paraphrases without us-\ning annotated parallel pairs. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics.\nShankar Iyer, Nikhil Dandekar, and Kornél Csernai.\n2017. First quora dataset release: Question pairs.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daumé III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classiﬁcation.\nIn Association for Computational Linguistics, pages\n1681–1691.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1875–1885.\nVineet John, Lili Mou, Hareesh Bahuleyan, and Olga\nVechtomova. 2019. Disentangled representation\nlearning for non-parallel text style transfer. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 424–434.\nAshutosh Kumar, Kabir Ahuja, Raghuram Vadapalli,\nand Partha Talukdar. 2020. Syntax-guided con-\ntrolled generation of paraphrases. Transactions\nof the Association for Computational Linguistics ,\npages 329–345.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, pages 55–60.\nPreslav Nakov, Doris Hoogeveen, Lluís Màrquez,\nAlessandro Moschitti, Hamdy Mubarak, Timothy\nBaldwin, and Karin Verspoor. 2017. SemEval-2017\ntask 3: Community question answering. In Proceed-\nings of the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 27–48.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992.\nSandeep Subramanian, Adam Trischler, Yoshua Ben-\ngio, and Christopher J Pal. 2018. Learning gen-\neral purpose distributed sentence representations via\nlarge scale multi-task learning. In International\nConference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-\n50M: Pushing the limits of paraphrastic sentence em-\nbeddings with millions of machine translations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 451–462.\nJohn Wieting, Kevin Gimpel, Graham Neubig, and Tay-\nlor Berg-Kirkpatrick. 2019. Simple and effective\nparaphrastic similarity from parallel translations. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4602–\n4608.\nJohn Wieting, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2020. A bilingual generative trans-\nformer for semantic sentence embedding. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 1581–1594.\n1379\nA Implementation Details\nDatasets We use the ParaNMT-50M dataset re-\nleased by Wieting and Gimpel (2018), which\ncan be obtained from https://github.com/\njwieting/para-nmt-50m. We sample 1 mil-\nlion English paraphrase pairs from ParaNMT-\n50M, and split this dataset into 5000 pairs as\nthe validation set and the rest as our train-\ning set. STS and syntactic probing datasets\nare directly taken from SentEval, which can\nbe accessed from https://github.com/\nfacebookresearch/SentEval. Quora\nQuestion Pairs are downloaded from the of-\nﬁcial GLUE Benchmark website ( https://\ngluebenchmark.com/).\nWord Dropout We observe that some para-\nphrase pairs in our training set contain many over-\nlapping words, which means our model can learn\nto generate the target paraphrase by just copying\nwords from a source sentence without fully under-\nstanding the semantics of the sentence. To alleviate\nthis issue, we apply word dropout (Iyyer et al.,\n2015) that randomly masks a portion of the input\ntokens. We don’t apply word dropout to syntactic\ninputs, as these inputs are designed to provide the\nexact syntactic structure of the paraphrase and en-\ncourage disentanglement of syntactic and semantic\nrepresentations. We set the word dropout probabil-\nity to 0.2 for all our models.\nHyperparameter Search Hyperparameters of\nParaBART are tuned manually based on the para-\nphrase generation loss on the validation set. Specif-\nically, the weight of adversarial loss is tuned within\n{0.1, 0.2, 0.5, 1.0}. Word dropout is selected from\n{0.0, 0.1, 0.2, 0.4}. Learning rate is tuned within\n{1,2,5,10}×10−5.\nNone of the previous models we compare\nin this work involves any hyperparameter\nsearch. The results for BGT are taken from\nWieting et al. (2020). For all other sentence\nembedding models, we use the trained model\nprovided by their respective authors. These\nmodels include InferSent ( https://github.\ncom/facebookresearch/InferSent,\nUSE ( https://tfhub.dev/google/\nuniversal-sentence-encoder-large/\n2), Sentence-BERT base (https:\n//github.com/UKPLab/\nsentence-transformers) and VGV AE\n(https://github.com/mingdachen/\nsyntactic-template-generation).\nPerformance on STS and QQP are evaluated un-\nder unsupervised settings. For syntactic probing\ntasks that involve training classiﬁers, we report the\naccuracy on the validation set provided by SentEval\nin Table 5.\nModel BShift TreeDepth TopConst\nAvg. BART embed. 90.4 47.5 80.2\nParaBART 73.0 34.8 67.6\n- w/o AL 75.4 36.7 72.1\n- w/o AL and SG 84.0 46.7 82.7\nTable 5: Validation accuracy on syntactic probing\ntasks.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8396533727645874
    },
    {
      "name": "Natural language processing",
      "score": 0.79682457447052
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7019425630569458
    },
    {
      "name": "Syntax",
      "score": 0.6953905820846558
    },
    {
      "name": "Sentence",
      "score": 0.6424460411071777
    },
    {
      "name": "Paraphrase",
      "score": 0.6293911337852478
    },
    {
      "name": "Parsing",
      "score": 0.535523533821106
    },
    {
      "name": "Semantic similarity",
      "score": 0.4780902564525604
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.46817535161972046
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.45112600922584534
    },
    {
      "name": "Semantic role labeling",
      "score": 0.4230797290802002
    },
    {
      "name": "Embedding",
      "score": 0.4199884831905365
    },
    {
      "name": "Programming language",
      "score": 0.1690903604030609
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ],
  "cited_by": 13
}