{
  "title": "Can large language models fully automate or partially assist paper selection in systematic reviews?",
  "url": "https://openalex.org/W4406419660",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2110616381",
      "name": "Haichao Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2734553920",
      "name": "Zehua Jiang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2115338485",
      "name": "Xinyu Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A3136028528",
      "name": "Can Can Xue",
      "affiliations": [
        "Singapore National Eye Center",
        "Singapore Eye Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5092683731",
      "name": "Samantha Min Er Yew",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2131212651",
      "name": "Bin Sheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2303207505",
      "name": "Ying Feng Zheng",
      "affiliations": [
        "Singapore National Eye Center",
        "Singapore Eye Research Institute",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2108704591",
      "name": "Xiaofei Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2104860046",
      "name": "You Wu",
      "affiliations": [
        "Johns Hopkins University",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2779757878",
      "name": "Sobha Sivaprasad",
      "affiliations": [
        "University College London",
        "Moorfields Eye Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2394853576",
      "name": "Tien Yin Wong",
      "affiliations": [
        "Tsinghua University",
        "Singapore National Eye Center",
        "Beijing Tsinghua Chang Gung Hospital",
        "Singapore Eye Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2145927365",
      "name": "Varun Chaudhary",
      "affiliations": [
        "McMaster University"
      ]
    },
    {
      "id": "https://openalex.org/A2006203058",
      "name": "Yih‐Chung Tham",
      "affiliations": [
        "National University of Singapore",
        "Duke-NUS Medical School",
        "Singapore National Eye Center",
        "Singapore Eye Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4221141536",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W4367051110",
    "https://openalex.org/W4400734098",
    "https://openalex.org/W2885869279",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W3111278950",
    "https://openalex.org/W4385356790",
    "https://openalex.org/W4387144848",
    "https://openalex.org/W3158522809",
    "https://openalex.org/W4220931790",
    "https://openalex.org/W4327676757",
    "https://openalex.org/W3020712187"
  ],
  "abstract": "Background/aims Large language models (LLMs) have substantial potential to enhance the efficiency of academic research. The accuracy and performance of LLMs in a systematic review, a core part of evidence building, has yet to be studied in detail. Methods We introduced two LLM-based approaches of systematic review: an LLM-enabled fully automated approach (LLM-FA) utilising three different GPT-4 plugins (Consensus GPT, Scholar GPT and GPT web browsing modes) and an LLM-facilitated semi-automated approach (LLM-SA) using GPT4’s Application Programming Interface (API). We benchmarked these approaches using three published systematic reviews that reported the prevalence of diabetic retinopathy across different populations (general population, pregnant women and children). Results The three published reviews consisted of 98 papers in total. Across these three reviews, in the LLM-FA approach, Consensus GPT correctly identified 32.7% (32 out of 98) of papers, while Scholar GPT and GPT4’s web browsing modes only identified 19.4% (19 out of 98) and 6.1% (6 out of 98), respectively. On the other hand, the LLM-SA approach not only successfully included 82.7% (81 out of 98) of these papers but also correctly excluded 92.2% of 4497 irrelevant papers. Conclusions Our findings suggest LLMs are not yet capable of autonomously identifying and selecting relevant papers in systematic reviews. However, they hold promise as an assistive tool to improve the efficiency of the paper selection process in systematic reviews.",
  "full_text": "1\nChen H, et al. Br J Ophthalmol 2025;0:1–6. doi:10.1136/bjo-2024-326254\nEpidemiology\nClinical science\nCan large language models fully automate or \npartially assist paper selection in systematic reviews?\nHaichao Chen,1 Zehua Jiang,1 Xinyu Liu,2 Can Can Xue,3 Samantha Min Er Yew,4,5 \nBin Sheng,6,7 Ying- Feng Zheng,3,8 Xiaofei Wang    ,9 You Wu,10,11 Sobha Sivaprasad,12 \nTien Yin Wong,1,3,13 Varun Chaudhary,14 Yih Chung Tham    3,4,5,15\nTo cite: Chen H, Jiang Z, \nLiu X, et al. Br J Ophthalmol \nEpub ahead of print: [please \ninclude Day Month Year]. \ndoi:10.1136/bjo-2024-\n326254\n ► Additional supplemental \nmaterial is published online \nonly. To view, please visit the \njournal online (https://\n \ndoi.\n \norg/\n \n10.\n \n1136/\n \nbjo-\n \n2024-\n \n326254).\nFor numbered affiliations see \nend of article.\nCorrespondence to\nDr Yih Chung Tham;  \n \nthamyc@\n \nnus.\n \nedu.\n \nsg\nHC and ZJ contributed equally.\nHC and ZJ are joint first authors.\nVC and YCT are joint senior \nauthors.\nReceived 31 July 2024\nAccepted 30 November 2024\n© Author(s) (or their \nemployer(s)) 2025. Re-\n use \npermitted under CC BY\n-\n NC\n. No \ncommercial re-\n use\n. See rights \nand permissions. Published by \nBMJ Group.\nABSTRACT\nBackground/aims\n Large language models (LLMs) \nhave substantial potential to enhance the efficiency of \nacademic research.\n The accuracy and performance of \nLLMs in a systematic review, a core part of evidence \nbuilding, has yet to be studied in detail.\nMethods\n W\ne introduced two LLM-\n based approaches \nof systematic review:\n an LLM-\n enabled fully automated \napproach (LLM-\n F\nA) utilising three different GPT-\n 4 plugins \n(Consensus GPT\n, Scholar GPT and GPT web browsing \nmodes) and an LLM-\n facilitated semi-\n automated approach \n(LLM-\n SA) using GPT4’\ns Application Programming \nInterface (API). We benchmarked these approaches \nusing three published systematic reviews that reported \nthe prevalence of diabetic retinopathy across different \npopulations (general population, pregnant women and \nchildren).\nResults\n T\nhe three published reviews consisted of 98 \npapers in total. Across these three reviews, in the LLM-\n F\nA \napproach, Consensus GPT correctly identified 32.7% (32 \nout of 98) of papers, while Scholar GPT and GPT4’s web \nbrowsing modes only identified 19.4% (19 out of 98) \nand 6.1% (6 out of 98), respectively. On the other hand, \nthe LLM-\n SA approach not only successfully included \n82.7% (81 out of 98) of these papers but also correctly \nexcluded 92.2% of 4497 irrelev\nant papers.\nConclusions\n Our findings suggest LLMs are not yet \ncapable of autonomously identifying and selecting \nrelev\nant papers in systematic reviews. However, they hold \npromise as an assistive tool to improve the efficiency of \nthe paper selection process in systematic reviews.\nINTRODUCTION\nLarge language models (LLMs) have demonstrated \nsubstantial potential in natural language processing \ntasks in various cross-\n disciplinary domains and can \nbe \napplied in diverse areas such as basic science, \nclinical medicine settings and computer program-\nming.\n1–3 LLMs are poised to be a transformative aid \nin academic research and can facilitate basic litera-\nture searches, generate abstracts, and even provide \nreviewer-\n \nlike feedback.4–6\nSystematic review is a key element of evidence-  \nbased \nmedicine, which searches relevant papers \ncomprehensively on a specific topic. 7 However, \nsystematic reviews require significant manual time \nand effort, especially in paper selection. T ypically, \na systematic review process requires two indepen-\ndent reviewers and one senior expert as adjudicator \nand often takes months to complete. The exten-\nsive need for repetitive reading and considerable \ntime commitment highlight gaps and opportuni-\nties for improvement. T raditional natural language \nprocessing approaches have previously been \nemployed to improve the efficiency of systemic \nreviews. T ools like Rayyan and AsReview help \nextract relevant keywords and offer recommenda-\ntion scores to assist reviewers.\n8 9 While these tools \nhave enhanced efficiency, they stop short of making \nindependent decisions. With the rise of LLMs \nand their advanced reasoning capabilities, there is \ngrowing interest in whether LLMs can take on a \nmore autonomous role, providing independent \ndecisions rather than merely offering recommenda-\ntions. T o that end, several recent studies explored \nthe capacity of LLMs in the title and abstract \nscreening of the systematic review process and \nfound that they could achieve a satisfactory accu -\nracy of 0.9.\n10–12 However, no evaluation has been \nWHAT IS ALREADY KNOWN ON THIS TOPIC\n ⇒ Large language models (LLMs) have shown \nsignificant promise in academic research \nand scientific writing. Previous studies have \ndemonstrated that LLMs can help with abstract \nscreening in systematic reviews, but their \npotential in the entire selection process and \ntheir capability in screening ophthalmology-\n \nrelated literature have not yet been studied.\nWHA\nT THIS STUDY ADDS\n ⇒ Our study developed two LLM- based pipelines \nin paper selection,\n ranging from abstract \nscreening to full-\n text selection:\n an LLM-\n enabled \nfully automated approach (LLM-\n F\nA) and an \nLLM-\n \nfacilitated semi-\n \nautomated approach \n(LLM-\n SA).\n Though the performance of LLM-\n \nF\nA was suboptimal, LLM-\n SA demonstrated \nsignificant potential\n, accurately identifying over \n80% of the papers in the original reviews.\nHOW THIS STUDY MIGHT AFFECT RESEARCH, \nPRACTICE OR POLICY\n ⇒ Our study highlighted the substantial potential \nof LLMs as a time-\n efficient and labour\n-\n saving \nalternative in systemic review.\n Our detailed \nerror analysis also points out the future \ndirection for improvement.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://bjo.bmj.com/Downloaded from 15 January 2025. 10.1136/bjo-2024-326254 on Br J Ophthalmol: first published as \n2\nChen H, et al. Br J Ophthalmol 2025;0:1–6. doi:10.1136/bjo-2024-326254\nEpidemiology\nperformed for the entire selection process of a systematic review \n(ie, from abstract screening to full-\n text selections).\nIn this study\n, we aimed to explore the performance of LLMs, in \naccurately selecting papers when conducting systematic review. \nWe conducted a retrospective comparative analysis, introducing \ntwo newly derived approaches: LLM-\n enabled \nfully automated \napproach (LLM-\n F\nA) and LLM-\n assisted semi-\n automated approach \n(LLM-\n SA). W\ne benchmarked these approaches using three \npublished systematic reviews that reported the prevalence of \ndiabetic retinopathy across different populations (general popu-\nlation, pregnant women and children). Our evaluation focused \non the two most labour-\n intensive and time-\n intensive stages of \nsystematic reviews: abstract screening and full-\n text selection.\nMATERIALS AND METHODS\nThree original systematic reviews published in the past 3 years \non the prevalence of DR\n13–15 were selected as the gold standards \nto evaluate LLMs’ performance in the paper selection of system-\natic reviews. All three studies were published in high-\n quality \njournals and received high citations. These studies focused either \non the general \ndiabetic population or subpopulations, including \npregnant women or type 2 diabetic (T2D) children (table\n 1\n). \nPapers included in the original studies were labelled as ‘true \npositive’, while those not included were considered ‘true nega-\ntive’. The study objectives, search query and inclusion/exclusion \ncriteria were extracted from the original studies. The instruction \nprompts for LLM-\n F\nA and LLM-\n SA approaches were designed \naccording to the extracted study \npurpose and inclusion/exclu-\nsion criteria (details in online supplemental appendices A and B).\nLLM-enabled fully automated systematic review (LLM-FA)\nWe used three popular GPT research plugins, Scholar GPT, \nConsensus GPT and ChatGPT4’s browsing mode in the LLM-\n F\nA \napproach (figure\n 1\n). We input the predesigned prompts into the \nthree plugins, which can automatically generate search queries \nand identify the relevant papers. In our pilot experiments, a \nsingle prompt only yielded a limited number of results. T o \nensure comprehensive coverage and reduce the risk of missing \nrelevant studies, we used additional prompts ‘Could you find \nme more papers?’ following the initial prompt, until no more \nnew papers were identified (online supplemental appendix A). \nWe repeated this process up to 10 times and evaluated the cumu-\nlative percentage of correctly identified papers after removing \nduplicates.\nLLM-assisted semi-automated systematic review (LLM-SA)\nThe LLM-  SA encompassed four major steps, as illustrated in \nfigure\n \n1. The first step involved conducting manual searches \nusing the same search query used in the three original studies. \nA publication date limit was added to mimic the original search \nprocess. Second, the titles and abstracts of searched papers were \nscreened by GPT-\n 4 \nAPI (model, gpt-\n 4; \ntemperature, 0.7) using \nthe predesigned prompts (online supplemental appendix B). \nTable 1 T hree published systematic reviews on diabetic retinopathy prevalence\nPaper Journal Publish date Citation Study objective\nTeo et al 202113 Ophthalmology April 2021 1048 To study the global prevalence of diabetic retinopathy\nWidyaputri et al 202214 JAMA Ophthalmology March 2022 23 To investigate the prevalence and progression of diabetic retinopathy in pregnant women \nwith preexisting diabetes\nCioana et al 202315 JAMA Network Open March 2023 15 To investigate the global prevalence of diabetic retinopathy among paediatric patients \nwith type 2 diabetes\nFigure 1 T he workflow of two different large language models (LLM) approaches. Upper panel, the workflow of the LLM-  enabled fully automated \napproach (LLM-\n F\nA). Lower panel, the workflow of the LLM-\n assisted semi-\n automated approach (LLM-\n SA).\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://bjo.bmj.com/Downloaded from 15 January 2025. 10.1136/bjo-2024-326254 on Br J Ophthalmol: first published as \n3\nChen H, et al. Br J Ophthalmol 2025;0:1–6. doi:10.1136/bjo-2024-326254\nEpidemiology\nT o minimise inappropriate exclusion at this stage, the prompts \nasked GPT-\n 4 \nto be conservative at this stage by emphasising that \nthe abstracts may not provide adequate information for decisive \nexclusions. GPT-\n 4 then \nevaluated each paper and categorised \nthem as ‘to exclude’, ‘not sure’ or ‘to include’ with reasoning \nprovided. Papers marked ‘to include’ or ‘not sure’ advanced to \nthe next step. In the third step, we automatically extracted the \nfull texts from the papers selected in Step 2 with several API \ntools, including PubMed API, Elsevier API, Wiley library API, \nthe Scidownl Python package and EndNote. Any full texts not \naccessible through these automated methods were manually \ndownloaded. Lastly, the extracted full-\n text \narticles were then \nautomatically screened using GPT4-\n API (model, gpt-\n 4; tempera\n-\nture, 0.7). GPT-\n 4 was prompted \nto make ‘to include’ or ‘to \nexclude’ decisions with reasoning provided. Papers subsequently \nmarked as ‘to include’ were then considered as the final selected \npapers based on the LLM-\n SA approach. T\no assess the impact \nof different prompting strategies on LLM-\n SA performance, \nwe \nrandomly selected 200 papers from each systematic review \n(with a 1:9 ratio between truly included and excluded papers). \nThe corresponding prompts are shown in online supplemental \nappendix C.\nData analysis\nAll data were recorded and analysed using Microsoft Excel \n(V .16.83) and Prism 10 (V .10.1.1).\nRESULTS\nA total of 98 papers were included in the original studies (54 in \nT eo et al\n13; 18 in Widyaputri et al14; 26 in Cioana et al)15.\nIn the LLM-\n F\nA approach, the performance increased as the \ntrial number increased, reaching a plateau of around seven trials \n(online supplemental figure S1). Consequently, the performance \nafter seven trials was selected as the final measure. The consensus \nGPT was the most accurate, identifying 32.7% (32 out of 98) of \ncorrect papers across the three original papers (16 out of 54 for \nT eo et al\n13; 11 out of 18 for Widyaputri et al 14; 5 out of 26 \nfor Cioana et al.15 In contrast, Scholar GPT and GPT4’s web \nbrowsing modes showed lower accuracy. Scholar GPT identified \n19.4% (19 out of 98) of correct papers across three papers (12 \nout of 54, for T eo et al\n13; 1 out of 18, for Widyaputri et al14; 6 \nout of 26, for Cioana et al)15 and GPT’s web browsing modes \ncorrectly included 6.1% (0 out of 54, for T eo et al13; 3 out of 18, \nfor Widyaputri et al14; 3 out of 26, for Cioana et al15 (figure 2).\nIn the \nLLM-\n SA approach, \nusing the same search query used in \nthe original studies, 4595 papers were identified (2838 in T eo \net al\n13; 688 in Widyaputri et al14; 1069 in Cioana et al15 (online \nsupplemental figures S2–4). Among the 4497 papers excluded in \nthe three original studies, the LLM-\n SA \ncorrectly excluded 92.2% \nof papers (4148 out of 4497). Respectively, it excluded 90.7% \n(2526 out of 2784) of irrelevant papers in T eo et al\n13, 95.1% (637 \nout of 670) in Widyaputri et al14, and 94.4% (985 out of 1043) \nin Cioana et al 15 Among the 98 papers included in the original \nstudies, the LLM-\n SA approach correctly identified 88.9% (48 \nout of 54) in \nT eo et al13 for the general population, 94.4% (17 \nout of 18) in Widyaputri et al14 for pregnant women and 61.5% \n(16 out of 26) in Cioana et al15 for T2D children (figure 2 ). The \noverall accuracy was 0.91 (sensitivity, 0.89; specificity, 0.91) for \nT eo et al\n13, 0.95 (sensitivity, 0.94; specificity, 0.95) for Widy -\naputri et al14, and 0.94 (sensitivity, 0.62; specificity, 0.94) for \nCioana et al 15. T o explore how different prompting strategies \nmay affect LLM-\n SA performance, we \nevaluated its accuracy \nwhen each inclusion/exclusion criterion was provided separately \n(online supplemental table S1). The LLM-\n SA \nwith separate \ncriterion assessment performed worse, correctly including only \n15.0% in T eo et al13, 11.1% in Widyapytri et al14, and 47.4% in \nCioana et al15.\nConsidering that incorrectly excluding papers may lead to \nirreparable biases in conclusions, we further analysed those \nerrors committed by the LLM-\n SA approach. It incorrectly \nex\ncluded 17 of the 98 selected papers across the three studies: 6 \nwere incorrectly excluded during abstract screening, 10 during \nfull-\n text selection and 1 due to incomplete text extraction. The \ntype of erroneous judgments by \nGPT varied among different \narticles (online supplemental table S2). For the two studies on \nDR prevalence in the general population and pregnant women, \namong the incorrectly excluded papers, GPT-\n 4 cited \nthe reasons \nas lacking specific information, such as certain sampling methods \nand response rates, indicating a limited capacity of GPT4 to \nidentify relevant information from the long texts. For the study \nby Cioana et al on type 2 diabetic children, GPT-\n 4 \nwrongly \nexcluded 5 abstracts for not reporting outcomes, possibly over -\nlooking the fact that the abstract may not encompass all perti-\nnent information. During the full-\n text \nreview, GPT-\n 4 \nincorrectly \nexcluded 2 papers for not reporting prevalence and 3 papers \nfor not meeting the study population criteria. It was observed \nthat, in addition to failing to identify relevant information and \nbeing overly aggressive in abstract exclusions, GPT-\n 4’s logical \nreasoning was \nsometimes flawed. For example, it incorrectly \njustified the exclusion of a paper16 by erroneously asserting that \npatients diagnosed under the age of 20 did not meet the criterion \nof being 21 years or younger.\nDISCUSSION\nOur proof-  of -  concept study compared the performance of  \nthe LLM-\n F\nA and LLM-\n SA, benchmarking them against \nthe \nconventional manual approach. We found that the LLM-\n F\nA \napproach retrieved less than 30% of papers in the original \nstudies, while the LLM-\n SA approach retrieved over 80%. In\n \naddition, LLM-\n SA \ncorrectly excluded 92.2% of irrelevant \npapers. These findings highlight that although LLMs are not \nyet capable of autonomously and independently selecting \npapers in systematic reviews, the LLM-\n SA \napproach shows \nconsiderable promise in enhancing and streamlining the \nsystematic review process.\nOur study provided three key insights for LLMs’ imple-\nmentation in a systematic review. First, we demonstrated that \ndespite numerous GPT plugins tailored for research purposes \nwith an advanced capacity to traditional search engines, their \napplication in a scientific systematic review context remains \nconstrained. They may help to retrieve relevant literature, but \nsuch retrieval still lacks comprehensiveness, which may lead \nto an unacceptable biased conclusion for clinical decisions. \nSecond, the LLM-\n SA approach has great potential to facilitate \nthe systematic review process, significantly reducing the volume \nof literature to be reviewed. \nIn our study, the LLM-\n SA correctly \nex\ncluded 92.2% of over 4498 irrelevant papers across three \nstudies, significantly reducing the workload. Third, LLM-\n SA \nstill \nneeds further optimisation. Through our analysis of erroneous \ndecisions, we found that the most common reason is LLM-\n SA\n’s \ninability to locate relevant information. This may be related to \nthe relatively limited ability of LLMs to handle long texts.\n17 18 \nCombining search algorithms that are not limited by text length \nor applying an alternative attention algorithm (attention approx-\nimation or sliding window attention) may further enhance the \napproach’s performance.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://bjo.bmj.com/Downloaded from 15 January 2025. 10.1136/bjo-2024-326254 on Br J Ophthalmol: first published as \n4\nChen H, et al. Br J Ophthalmol 2025;0:1–6. doi:10.1136/bjo-2024-326254\nEpidemiology\nFigure 2 T he percentage of correct inclusions and incorrect exclusions by LLM-  F A and LLM-  SA approaches benchmark ed against included papers \nreported in the published systematic reviews. The percentage of correct paper inclusions and incorrect paper exclusions by LLM-\n F\nA and LLM-\n SA \napproaches across three published reviews\n. (B-\n D) \nThe percentage of correct paper inclusions and incorrect paper exclusions by LLM-\n F\nA and LLM-\n SA \napproaches in each review (B:\n Teo et al 2021; C: Widyaputri et al 2022; D: Cioana et al 2023). LLM, large language models; LLM-\n F\nA, large language \nmodels fully automated approach; LLM-\n SA,\n LLM-\n assisted semi-\n automated approach\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://bjo.bmj.com/Downloaded from 15 January 2025. 10.1136/bjo-2024-326254 on Br J Ophthalmol: first published as \n5\nChen H, et al. Br J Ophthalmol 2025;0:1–6. doi:10.1136/bjo-2024-326254\nEpidemiology\nDespite the significant potential of LLM-  SA in systematic \nreviews, further research is needed to facilitate \nits integration \ninto systematic reviews. First, the paper selection task lacks an \nopen-\n source benchmark, \nwhich hinders the LLMs’ evaluation \nand development in this task. Although numerous systematic \nreviews have been published, it is difficult to find one where all \nparticipant papers are clearly labelled as ‘exclude’ or ‘include’. \nAs a result, most prior research replicated systematic reviews \nfrom the same group, which significantly compromised compre-\nhensiveness. Publishing paper selection results alongside system-\natic reviews could be valuable for building such benchmarking \ndatasets. Second, a standardised way to customise prompts for \nsystematic review should be developed. This is because each \nreview requires prompts tailored to its specific criteria and the \nprompt designed for one review may not be suitable for others. \nEstablishing a standardised prompting framework and evalu-\nating its efficacy across different systematic reviews is therefore \ncrucial. Third, the cost of using LLMs may also limit their large-\n \nscale \napplication. Screening thousands of papers via API can be \nexpensive, highlighting the need for fine-\n tuned, open-\n source \nLLMs to help reduce these costs.\nThere are \nseveral limitations of our study. First, our meth-\nodology evaluates only the overall performance of LLMs, but \nnot their performance at individual stages such as abstract \nscreening. This is because the majority of published papers do \nnot list the literature filtered out during abstract screening. \nAlthough selecting reviews previously conducted by our group \ncould provide ground truth for each stage, choosing high-\n quality \nsystematic reviews centred around a specific topic could better \nevaluate \nthe LLMs’ performance on that topic. Our research \nparadigm can also be easily extended to other fields. Moreover, \nalthough we did not assess every step, assessing overall perfor -\nmance provides a direct measure of LLMs’ potential. While \nprevious studies primarily focused on abstract screening\n10- 12, the \nperformance of this stage alone does not fully reflect the capabil-\nities of LLMs, as errors made during abstract screening can often \nbe corrected during full-\n text selection.\nSecond, the LL\nM-\n F\nA approach did not fully mimic the \nconventional systematic review process. Rather than screening \nall results generated by the search query, it screened only the \nmost relevant subset by the search algorithm. It may explain the \npoor performance of LLM-\n F\nA. Given its increasing popularity, \nwe believe it is still essential to evaluate its performance and \nprevent potential bias in the field.\nThirdly, although our study endeavoured to align the LLM-\n SA\n \napproach closely with the manual approach in terms of the \nsearch strategy and query, some discrepancies were inevitable. \nOne such discrepancy stemmed from varying access to literature \ndatabases. In the full-\n text extraction phase, some articles could\n \nnot be fully accessed due to database restrictions, preventing \ntheir evaluation by GPT. Another discrepancy arose from the \ndifference in papers retrieved by the manual search between \nthe LLM-\n SA and the manual approach. These discrepancies\n \nmay introduce some biases in our evaluation of the LLM-\n SA\n \napproach.\nFourthly, our study has limited generalisability. We only focused \non the topic of diabetic retinopathy. Additionally, the type of \nresearch we focused on is relatively simple, only concerning \nthe prevalence of the disease without involving interventions. \nThus, the generalisability of our findings to other domains ought \nto be evaluated in future work. Moreover, our study primarily \nevaluated the performance of GPT4 and its plugins. For a more \ncomprehensive understanding, future research should explore \nthe capabilities of other LLMs, such as Google’s Gemini and \nAnthropic’s Claude, as well as other AI tools, in the context of \nsystematic reviews.\nOur study offers valuable insights into the application of LLMs \nin systematic reviews. We found that the LLM-\n F\nA approach \nidentified 36.7% of the total papers across three original studies. \nIn comparison, the LLM-\n SA \napproach successfully identified \n82.7% of the selected papers and accurately excluded 92.2% of \nirrelevant papers. These results highlight the current utility of \nLLMs in enhancing systematic review processes. While our find-\nings reveal limitations in fully autonomous LLM applications, \nintegrating LLMs to assist in manual review workflows can \nachieve better and considerable accuracy. With further refine-\nment, LLM has the potential to assist or transform systematic \nreview approaches.\nAuthor affiliations\n1Tsinghua Medicine, Tsinghua University, Beijing, China\n2Institute of Medical Technology, Peking University Health Science Center, Beijing, \nChina\n3Singapore Eye Research Institute, Singapore National Eye Centre, Singapore\n4Centre for Innovation and Precision Eye Health, Yong Loo Lin School of Medicine, \nNational University of Singapore, Singapore\n5Department of Ophthalmology, Yong Loo Lin School of Medicine, National University \nof Singapore, Singapore\n6MOE Key Laboratory of AI, School of Electronic, Information, and Electrical \nEngineering, Shanghai Jiao Tong University, Shanghai, China\n7Department of Computer Science and Engineering, School of Electronic, \nInformation, and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, \nChina\n8State Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun Yat- sen \nUniversity,\n Guangzhou, China\n9Beijing Advanced Innovation Center for Biomedical Engineering, Key Laboratory for \nBiomechanics and Mechanobiology of Ministry of Education, School of Biological \nScience and Medical Engineering, Beihang University, Beijing, China\n10Tsinghua University School of Medicine, Beijing, China\n11Department of Health Policy and Management, Johns Hopkins University, \nBaltimore, Maryland, USA\n12Moorfields Eye Hospital City Road Campus, London, UK\n13School of Clinical Medicine, Beijing Tsinghua Changgung Hospital, Tsinghua \nUniversity, Beijing, China\n14Department of Surgery, Division of Ophthalmology, McMaster University, Hamilton, \nOntario, Canada\n15Ophthalmology and Visual Science Academic Clinical Program, Duke- NUS Medical \nSchool\n, Singapore\nX Yih Chung Tham @Yihtham\nContributors\n YCT had full access to all of the data in the study and took \nresponsibility for the integrity of the data and the accur\nacy of the data analysis. YCT \nwas the guarantor. YCT, TYW and VC were responsible for the concept and design \nof the study. HC, ZJ and XL contributed in the acquisition, analysis or interpretation \nof data. Drafting of the manuscript was done by HC and ZJ. Critical review of the \nmanuscript for important intellectual content was done by XL, CCX, SMEY , BS, Y-\n FZ,\n \nXW, YW and SS. Statistical analysis was performed by HC. Obtained funding was \ndone by YCT and TYW. We evaluated the performance of large language models in \nthe paper selection of systematic reviews. No AI was used to create manuscript texts \nand figures.\nFunding\n T\nhe study was supported by National Key R & D Program of China \n(2022YFC2502802), National Natural Science Fund of China (8238810007) and \nBeijing Natural Science Foundation (IS23096).\nCompeting interests\n TYW declares consulting fees from \nAldropika Therapeutics, \nBayer, Boehringer Ingelheim, Genentech, Iveric Bio, Novartis, Plano, Oxurion, Roche, \nSanofi and Shanghai Henlius; funding from the National Key R&D Program, China \n(grant number 2022YFC2502802); and being an inventor, patent holder and \nco-\n founder of the start-\n up companies EyRiS and \nVisre. All other authors declare no \ncompeting interests.\nPatient consent for publication\n Not applicable\n.\nEthics approval\n Not applicable\n.\nProvenance and peer review\n Not commissioned;\n externally peer reviewed.\nData availability statement\n Data are av\nailable upon reasonable request. All \ndata and code are available upon request by emailing \n thamyc@\n nus\n.\n edu.\n sg.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://bjo.bmj.com/Downloaded from 15 January 2025. 10.1136/bjo-2024-326254 on Br J Ophthalmol: first published as \n6\nChen H, et al. Br J Ophthalmol 2025;0:1–6. doi:10.1136/bjo-2024-326254\nEpidemiology\nSupplemental material T his content has been supplied by the author(s). It \nhas not been vetted by BMJ Publishing Group Limited (BMJ) and may not have \nbeen peer-\n reviewed.\n Any opinions or recommendations discussed are solely those \nof the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and \nresponsibility arising from any reliance placed on the content. Where the content \nincludes any translated material, BMJ does not warrant the accuracy and reliability \nof the translations (including but not limited to local regulations, clinical guidelines, \nterminology, drug names and drug dosages), and is not responsible for any error \nand/or omissions arising from translation and adaptation or otherwise.\nOpen access\n T\nhis is an open access article distributed in accordance with the \nCreative Commons Attribution Non Commercial (CC BY-\n NC 4.0) license\n, which \npermits others to distribute, remix, adapt, build upon this work non-\n commercially,\n \nand license their derivative works on different terms, provided the original work is \nproperly cited, appropriate credit is given, any changes made indicated, and the use \nis non-\n commercial\n. See: http://creativecommons.org/licenses/by-nc/4.0/.\nORCID iDs\nXiaofei Wang http://orcid.org/0000-0002-3175-0344\nYih Chung Tham http://orcid.org/0000-0002-6752-797X\nREFERENCES\n 1  Boik o DA, MacKnight R, Kline B, et al. Autonomous chemical research with large \nlanguage models. Nature New Biol 2023;624:570–8. \n 2\n A systematic ev\naluation of large language models of code. Proceedings of the 6th \nACM SIGPLAN International Symposium on Machine Programming; 2022\n 3\n Singhal \nK, Azizi S, Tu T, et al. Large language models encode clinical knowledge. \nNature New Biol 2023;620:172–80. \n 4\n Noy S\n, Zhang W. Experimental Evidence on the Productivity Effects of Generative \nArtificial Intelligence. Science 2023;381:187–92.\n 5\n Gao CA,\n Howard FM, Markov NS, et al. Comparing scientific abstracts generated by \nChatGPT to real abstracts with detectors and blinded human reviewers. NPJ Digit Med \n2023;6:75. \n 6\n Liang \nW, Zhang Y , Cao H, et al. Can Large Language Models Provide Useful Feedback \non Research Papers? A Large-\n Scale Empirical \nAnalysis. NEJM AI 2024;1:231001783. \n 7\n Siddaw\nay AP , Wood AM, Hedges LV . How to Do a Systematic Review: A Best Practice \nGuide for Conducting and Reporting Narrative Reviews, Meta-\n Analyses\n, and Meta-\n \nSyntheses\n. Annu Rev Psychol 2019;70:747–70. \n 8\n Ouzzani M,\n Hammady H, Fedorowicz Z, et al. Rayyan-\n a web and mobile app for \nsystematic reviews\n. Syst Rev 2016;5:210. \n 9\n v\nan de Schoot R, de Bruin J, Schram R, et al. An open source machine learning \nframework for efficient and transparent systematic reviews. Nat Mach Intell \n2021;3:125–33. \n 10\n Matsui \nK, Utsumi T, Aoki Y , et al. Large language model demonstrates human-\n \ncompar\nable sensitivity in initial screening of systematic reviews: a semi-\n automated \nstr\nategy using gpt-\n 3.5.\n SSRN [Preprint]. \n 11\n Guo E,\n Gupta M, Deng J, et al. Automated Paper Screening for Clinical Reviews Using \nLarge Language Models: Data Analysis Study. J Med Internet Res 2024;26:e48996. \n 12\n Robinson A,\n Thorne W, Wu BP , et al. n.d. Bio-\n SIEVE:\n Exploring Instruction Tuning Large \nLanguage Models for Systematic Review Automation. arXiv Preprint arXiv.\n 13\n T\neo ZL, Tham YC, Yu M, et al. Global Prevalence of Diabetic Retinopathy and \nProjection of Burden through 2045: Systematic Review and Meta-\n analysis\n. \nOphthalmology 2021;128:1580–91. \n 14\n Widy\naputri F , Rogers SL, Kandasamy R, et al. Global Estimates of Diabetic Retinopathy \nPrevalence and Progression in Pregnant Women With Preexisting Diabetes: A \nSystematic Review and Meta-\n  analysis . JAMA Ophthalmol 2022;140:486–94. \n 15\n Cioana M,\n Deng J, Nadarajah A, et al. Global Prevalence of Diabetic Retinopathy in \nPediatric Type 2 Diabetes: A Systematic Review and Meta-\n analysis\n. JAMA Netw Open \n2023;6:e231887. \n 16\n Amutha \nA, Ranjit U, Anjana RM, et al. Clinical profile and incidence of microvascular \ncomplications of childhood and adolescent onset type 1 and type 2 diabetes seen at a \ntertiary diabetes center in India. Pediatr Diabetes 2021;22:67–74. \n 17\n Bai \nY , Lv X, Zhang J, et al. n.d. LongBench: a bilingual, multitask benchmark for long \ncontext understanding. arXiv Preprint arXiv.\n 18\n Zhang C\n, Liu F , Basaldella M, et al. n.d. LUQ: long-\n text uncertainty quantification for \nLLMS\n. arXiv Preprint arXiv.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies. \n. by guest on November 5, 2025 http://bjo.bmj.com/Downloaded from 15 January 2025. 10.1136/bjo-2024-326254 on Br J Ophthalmol: first published as ",
  "topic": "Selection (genetic algorithm)",
  "concepts": [
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.7241909503936768
    },
    {
      "name": "Medicine",
      "score": 0.6908557415008545
    },
    {
      "name": "Systematic review",
      "score": 0.47184088826179504
    },
    {
      "name": "Data science",
      "score": 0.4020816385746002
    },
    {
      "name": "Natural language processing",
      "score": 0.39300981163978577
    },
    {
      "name": "MEDLINE",
      "score": 0.37225356698036194
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31680363416671753
    },
    {
      "name": "Computer science",
      "score": 0.3050702214241028
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116917",
      "name": "Singapore Eye Research Institute",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2799299286",
      "name": "Singapore National Eye Center",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210150574",
      "name": "Moorfields Eye Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210153550",
      "name": "Beijing Tsinghua Chang Gung Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I98251732",
      "name": "McMaster University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210126319",
      "name": "Duke-NUS Medical School",
      "country": "SG"
    }
  ]
}