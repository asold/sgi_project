{
  "title": "Natural Language Processing and User Modeling: Synergies and Limitations",
  "url": "https://openalex.org/W73056518",
  "year": 2001,
  "authors": [
    {
      "id": "https://openalex.org/A5035648097",
      "name": "Ingrid Zukerman",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A5048682514",
      "name": "Diane Litman",
      "affiliations": [
        "AT&T (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1587737494",
    "https://openalex.org/W2132271306",
    "https://openalex.org/W2120589136",
    "https://openalex.org/W1781978087",
    "https://openalex.org/W2141284149",
    "https://openalex.org/W2098442941",
    "https://openalex.org/W1591653509",
    "https://openalex.org/W2030721126",
    "https://openalex.org/W1600273718",
    "https://openalex.org/W2011375074",
    "https://openalex.org/W1502428651",
    "https://openalex.org/W2154976144",
    "https://openalex.org/W2033842033",
    "https://openalex.org/W1776191765",
    "https://openalex.org/W1516008966",
    "https://openalex.org/W2137826469",
    "https://openalex.org/W1591806884",
    "https://openalex.org/W1557666342",
    "https://openalex.org/W1549942482",
    "https://openalex.org/W242806660",
    "https://openalex.org/W176475720",
    "https://openalex.org/W68015109",
    "https://openalex.org/W1491011092",
    "https://openalex.org/W1526775548",
    "https://openalex.org/W2005111406",
    "https://openalex.org/W2009559332",
    "https://openalex.org/W2007393257",
    "https://openalex.org/W2089699663",
    "https://openalex.org/W2911664941",
    "https://openalex.org/W4206625585",
    "https://openalex.org/W2162597969",
    "https://openalex.org/W1943086199",
    "https://openalex.org/W1845416965",
    "https://openalex.org/W4234134528",
    "https://openalex.org/W1653632886",
    "https://openalex.org/W2136028118",
    "https://openalex.org/W1511194710",
    "https://openalex.org/W1486344352",
    "https://openalex.org/W2111309679",
    "https://openalex.org/W1545815294",
    "https://openalex.org/W20672442",
    "https://openalex.org/W2051079680",
    "https://openalex.org/W2187794424",
    "https://openalex.org/W2018196704",
    "https://openalex.org/W23010510",
    "https://openalex.org/W1572783005",
    "https://openalex.org/W1974395842",
    "https://openalex.org/W1993099716",
    "https://openalex.org/W2152316548",
    "https://openalex.org/W2018268564",
    "https://openalex.org/W2066662917",
    "https://openalex.org/W2063157598",
    "https://openalex.org/W1562973825",
    "https://openalex.org/W1576867306",
    "https://openalex.org/W1982660426",
    "https://openalex.org/W2040205007",
    "https://openalex.org/W2165699164",
    "https://openalex.org/W2124756613",
    "https://openalex.org/W2142123839",
    "https://openalex.org/W1999960578",
    "https://openalex.org/W2171804848",
    "https://openalex.org/W1504264472",
    "https://openalex.org/W2090026396",
    "https://openalex.org/W2001414994",
    "https://openalex.org/W4232711488",
    "https://openalex.org/W4246309165",
    "https://openalex.org/W2952751702",
    "https://openalex.org/W2012852193",
    "https://openalex.org/W1521743653",
    "https://openalex.org/W18412640",
    "https://openalex.org/W3212193146",
    "https://openalex.org/W4233233143",
    "https://openalex.org/W2099143991",
    "https://openalex.org/W5642659",
    "https://openalex.org/W103920111",
    "https://openalex.org/W1577945499",
    "https://openalex.org/W76665929",
    "https://openalex.org/W2094484179",
    "https://openalex.org/W1482911759",
    "https://openalex.org/W1549872659",
    "https://openalex.org/W1990334545",
    "https://openalex.org/W1545977227",
    "https://openalex.org/W1489539247",
    "https://openalex.org/W4234922297",
    "https://openalex.org/W166962601",
    "https://openalex.org/W4249152926",
    "https://openalex.org/W3127032894",
    "https://openalex.org/W1593902738",
    "https://openalex.org/W4230088309",
    "https://openalex.org/W2151910972",
    "https://openalex.org/W1489625368",
    "https://openalex.org/W1595980070",
    "https://openalex.org/W1594046968",
    "https://openalex.org/W1607009024",
    "https://openalex.org/W2153166272",
    "https://openalex.org/W2034742893",
    "https://openalex.org/W2124099005",
    "https://openalex.org/W1555547974",
    "https://openalex.org/W54118280",
    "https://openalex.org/W1519515095",
    "https://openalex.org/W140876290",
    "https://openalex.org/W2159080219",
    "https://openalex.org/W1503736908",
    "https://openalex.org/W1572108163",
    "https://openalex.org/W1503622209",
    "https://openalex.org/W101555383",
    "https://openalex.org/W4250375266",
    "https://openalex.org/W2161868670",
    "https://openalex.org/W1577418821",
    "https://openalex.org/W4236904821",
    "https://openalex.org/W92119760",
    "https://openalex.org/W4242982351",
    "https://openalex.org/W219140104",
    "https://openalex.org/W3022363216",
    "https://openalex.org/W2116794885",
    "https://openalex.org/W1565690548",
    "https://openalex.org/W4250143236",
    "https://openalex.org/W129930415"
  ],
  "abstract": null,
  "full_text": "Natural Language Processing and User Modeling:\nSynergies and Limitations\nINGRID ZUKERMAN1 and DIANE LITMAN2\n1School of Computer Science and Software Engineering, Monash University, Clayton,\nVictoria 3800, Australia, E-mail: ingrid@csse.monash.edu.au\n2AT&T Labs ^ Research, Florham Park, New Jersey 07932, U.S.A.,\nE-mail: diane@research.att.com\n(Received: 28 March 2000; in ¢nal form 1 June, 2000)\nAbstract. The ¢elds of user modeling and natural language processing have been closely linked\nsince the early days of user modeling. Natural language systems consult user models in order\nto improve their understanding of users' requirements and to generate appropriate and relevant\nresponses. At the same time, the information natural language systems obtain from their users\nis expected to increase the accuracyoftheir user models. In this paper, we review natural language\nsystems for generation, understanding and dialogue, focusingon the requirements and limitations\nthese systems and user models place on each other.We then propose avenues for future research.\nKey words: natural language generation, natural language understanding, plan recognition,\nsurface features, dialogue systems\n1. Introduction\nOne of the main goals of the ¢eld of natural language processing is to endow a com-\nputer with the ability to interact with people the way people interact with each other.\nIt is both intuitively appealing and widely accepted by the research community that\npeople use some model of their interlocutors when they interact with each other.This\nmodel assists them in all aspects of their interaction. For example, it helps them\nadjust the style and level of generated and accepted language to the style and capa-\nbilities of the interlocutor, understand the interlocutor's intentions even if they\nare not articulated precisely, and generate appropriate responses. In addition, people\noften update their models of their interlocutors during an interaction or as a result of\nan interaction.\nThis use of user models inspired several hopes regarding the advantages user\nmodels would bring to natural language systems. User models were expected to\nimprove the ability of natural language systems to understand a user; help achieve\nadaptivity in natural language interactions; and increase the robustness of natural\nlanguage systems, so that they could be used by anyone under various circumstances.\nHowever, these hopes have been achieved only partially. Research in plan recog-\nnition has produced Natural Language Understanding (NLU) systems that can infer\na user's intentions even when they have not been articulated precisely; the incorpor-\nUser Modeling and User-Adapted Interaction11: 129^158, 2001. 129\n# 2001 Kluwer Academic Publishers. Printed in the Netherlands.\nation of user models into Natural Language Generation (NLG) systems has yielded\nsystems that adapt their output to users' beliefs and capabilities; and models of users'\nlanguage usage have improved the robustness of natural language interfaces.\nHowever, most of these systems are research prototypes that were developed to test\nspeci¢c ideas, and are applicable only in restricted domains. In addition, few of these\nsystems support fully interactive behaviour, and hence cannot demonstrate the con-\ntribution of user models to the entire interaction cycle: understanding a user's\nrequirements, followed by a possible adjustment of the user model, the generation\nof a response, the understanding of new requirements, and so on. However, the\ndevelopment of such systems is now within our reach, both due to advances in natu-\nral language and user modeling, and due to the current emphasis on developing\ncomplete practical systems, albeit of limited scope.\nThe dream of the natural language community is a grand one. However, the reality\nis that advances are required in many sub-¢elds of natural language in order to\nachieve this dream. Parsing techniques must be robust enough to handle ill-formed\nand incomplete sentences and multimedia input; semantic components must be able\nto produce an internal representation from a user's input; discourse handling\nmechanisms must be able to put together the meaning of a piece of discourse; com-\nponents that handle pragmatics must be able to make inferences that go beyond\nliteral meaning; dialogue systems must be able to handle interruptions, false starts\nand changes in topic, and recover from mis-communication; and generation systems\nmust be able to produce just the right discourse using appropriate media. These\ndiverse requirements have prompted the separate investigation of different sub-¢elds\nof natural language. As a result, the research in user modeling for natural language\nhas also been fragmented, since different aspects of a user model are required\nfor each sub-¢eld.\nInterestingly, the ¢elds of NLG and NLU have experienced similar trends\nregarding their use of user models, in the sense that user models have been considered\nmainly in relation to pragmatics. Most NLG systems that consult user models do so\nduring content planning, i.e. when deciding what to say (Section 2), while most NLU\nsystems are concerned with building user models that represent a user's plans and\ngoals (Section 3). Only a few natural language systems consult user models in\nrelation to surface features of language (Section 4). Finally, dialogue systems consult\nuser models for a variety of high-level tasks (e.g. providing tailored and cooperative\nresponses to users, or switching the control of the interaction), and also use the\ndialogue to dynamically update their user models (Section 5). The insights obtained\nfrom these natural language systems motivate the challenges and research avenues\nproposed in Section 6.\n2. Natural Language Generation ^ Content Planning\nOne of the ¢rst NLG systems that consults a user model is described in (Wallis and\nShortliffe, 1985). Wallis and Shortliffe recognized the need to model two aspects of a\n130 INGRID ZUKERMAN AND DIANE LITMAN\nuser in order to generate appropriate explanations: his/her expertise in the subject\nmatter and his/her preferences regarding the level of detail of an explanation.\nHowever, their model was rather coarse, consisting of a single number to represent\neach of these aspects. Their system tailored explanations to the user's level of\nexpertise by omitting simple reasoning steps from explanations generated for an\nexpert user and omitting detail from explanations generated for a novice.\nFrom this start, it became clear that more sophisticated user models were required\nin order to enable NLG systems to generate appropriate and relevant discourse. Such\ndiscourse presents different types of information depending on the audience's per-\nceived expertise or preferences, addresses a user's likely misconceptions and\ninferences, and takes into account contextual information. We now consider\nNLG systems that generate discourse that incorporates these features, and discuss\nthe user models that support the requirements of these systems.\n2.1. C\nONSIDERING A USER'S ATTRIBUTES ^ ONE-DIMENSIONAL USER MODELS\nMany NLG systems consult user models which represent a single aspect of a user,\ne.g. expertise or preferences, where this aspect is in£uenced by the type of discourse\nbeing generated. For instance, systems that generated concept descriptions consulted\na model of a user's expertise or interests (e.g. Paris, 1989; Tattersall, 1992; Stock et\nal., 1993), systems that produced evaluative discourse used a model of a user's prefer-\nences (e.g. Jameson, 1989; Carenini and Moore, 1999), and systems that generated\nhealth advice consulted a user model which consists of the user's medical record\n(e.g. Carenini et al., 1994; Binsted et al., 1995).\nParis (1989) observed that a user's level of expertise not only affects the amount of\ndetail provided in an explanation, but also the kind of information given: process\ntraces are normally presented to naive users, while descriptions based on the com-\nponents of an object are given to expert users. This observation inspired the devel-\nopment of a system that tailors the type of information included in a\ndescription to a user's level of expertise. To support such explanations, Paris' user\nmodel distinguished between two types of information items in the domain, basic\nconcepts and speci¢c artifacts, recording the items that were known to the user.\nAlthough this model was rather coarse, it was suf¢cient to support the distinction\nbetween the two types of explanations under consideration. Tattersall's user model\nwas slightly ¢ner-grained, distinguishing between three levels of expertise a user\ncould have with respect to a concept (Tattersall, 1992). As for Paris' system, this\ndistinction was motivated by the three main explanation strategies considered by\nTattersall: initial , which describes unknown concepts;consolidating , which describes\npreviously introduced concepts; and reminding , which describes known concepts.\nThese strategies also determined whether comparisons were optional or mandatory,\nand whether known or unknown target concepts should be used in comparisons.\nThe system described in (Stock et al., 1993) consulted a model of a user's interests\nin order to select information items to be included in multimedia presentations about\nNATURAL LANGUAGE AND USER MODELING 131\nFourteenth Century Italian frescoes. This model represented the relationship\nbetween different areas of interest by means of an activation/inhibition network.\nEach node in the network represented an area of interest, e.g. a painting school\nor a period of time, and was associated with a set of concepts. The activation of\na node, e.g. by the user asking about a particular concept or area of interest, resulted\nin the activation of nodes in related areas (connected to this node by activation links)\nand the inhibition of nodes in incompatible areas (connected to this node by inhibi-\ntory links).\nJameson (1989) and Carenini and Moore (1999) developed mechanisms which\ngenerate discourse that supports an evaluative process. Carenini and Moore's sys-\ntem generated discourse that evaluates an object by consulting a model of the user's\npreferences represented by means of a multi-attribute value function. In contrast,\nthe discourse generated by Jameson's system elicited the hearer's evaluative judg-\nment implicitly, i.e. without expressing any value judgments in the discourse. This\nwas done by consulting a model of the hearer's ``evaluation standards'' (i.e.\npreferences) in order to determine whether to include or exclude a piece of infor-\nmation or whether to be ambiguous or precise. For example, if an interviewer asked\nabout the family situation of a job applicant, and the applicant thought her inter-\nviewer had a negative bias against people who are in a relationship, the applicant\nmay choose to say ``I live alone'' instead of the more accurate ``I have a\nboyfriend''. *\nThe observation that conveying appropriate information to patients reduces\nthe overall cost of health care has created a demand for health documents tailored\nto patients' needs and abilities. Several researchers developed systems that gen-\nerate medical advice (Carenini et al., 1994; Binsted et al., 1995; Hirst et al., 1997;\nde Rosis et al., 1999).** Carenini et al. (1994) developed a system that advised\npatients about migraines and their treatment. A user model for each patient\nwas obtained from an electronic questionnaire, which collected information\nabout the patient's symptoms, habits, family history and medications taken. This\nsupported the tailoring of the discourse to the patient's particular circumstances.\nIn addition, the system modeled class concerns of migraine sufferers, e.g. the fear\nthat migraines are a life threatening condition. However, the consideration of\nthese concerns was hard-coded into the discourse planning system. As for\nCarenini et al.'s system, the explanations generated by Binsted et al.'s system\nmixed general medical information with speci¢c information about a patient.\nBinsted et al. evaluated the quality of the generated text and the contribution\nof the personalization aspect by showing their system's output to health\nprofessionals. The results of this evaluation were encouraging. However, in order\nto improve system adaptivity, class concerns should be explicitly incorporated in\nuser models.\n* For a discussion on acquiring and addressing preferences in a dialogue, see Section 5.4.\n** The research of Hirst et al. and de Rosis et al. is described in Section 2.3.\n132 INGRID ZUKERMAN AND DIANE LITMAN\n2.2. CONSIDERING INFERENTIAL PATTERNS ^ ENHANCING USER MODELS\nMany NLG systems (e.g. Paris, 1989; Tattersall, 1992) laboured under the implicit\nassumption that a user's beliefs are a subset of the system's beliefs. In addition,\nas indicated above, users' beliefs were often modeled at a rather coarse level of detail\n(which was adequate for the generation phenomena considered by these systems).\nHowever, this level of detail does not support the generation of discourse that\naddresses a user's misconceptions or inferences.\nMcCoy (1989) and Milosavljevic (1997) considered the generation of explanations\nthat take into account a user's perceptions regarding similarities between objects.\nMilosavljevic's system generated different types of comparisons, e.g. to distinguish\nbetween a target concept and a potential confusor, to convey a concept in terms\nof a similar concept, or to convey an attribute of a concept in terms of the same\nattribute of another concept. McCoy's system generated discourse that addresses\ntwo types of object-related misconceptions: misconceptions due to the\nmisclassi¢cation of an object, and misconceptions due to the assignment of attribute\nvalues to an object which differ from those in the system's domain model. Both\nsystems required a more detailed user model than that required by the systems\nmentioned above, as a user's beliefs regarding object attributes had to be explicitly\nrepresented. Further, in McCoy's system, the object taxonomy in the user model,\nthe attributes associated with the objects in the user model and the values of these\nattributes were allowed to differ from those in the system's model of the world (this\nsystem is discussed further in Section 2.3).\nSeveral researchers developed discourse planning systems which take into con-\nsideration inferences from the discourse, (e.g. Joshi et al., 1984; Lascarides and\nOberlander, 1992; Mehl, 1994). However, the research described in (Zukerman\nand McConachy, 1993, 1995, 2001; Kashihara et al., 1995; Kashihara et al., 1996;\nZukerman et al.,1996; Horacek, 1997) is of particular interest, since these researchers\ndistinguish between the inferences drawn by different types of users.\nZukerman and McConachy (1993, 2001), Zukerman et al. (1996) and Horacek\n(1997) use inference rules to model a user's inferences in systems that generate\nexplanations. Zukerman and McConachy's system generates concept descriptions\nthat address anticipated erroneous inferences and omit easily inferred information,\nand both Zukerman et al.'s system and Horacek's generated arguments. However,\nZukerman et al. focused on tailoring an argument to the user's capabilities, while\nHoracek considered the omission of easily inferred information. These researchers\nmodeled different types of inferences and used different discourse planning\nmechanisms. Zukerman et al. (1996) modeled users' domain-speci¢c inferences\nby attributing different degrees of belief in domain-speci¢c inference rules to dif-\nferent types of users. These rules were used to build candidate arguments, and\npreference rules were then applied to select the argument which best suited a par-\nticular type of user. Horacek (1997) modeled domain-speci¢c and problem-solving\ninference rules (attributed to different user stereotypes), and contextual inference\nNATURAL LANGUAGE AND USER MODELING 133\nrules (attributed to all users). He used these inference rules to annotate the output of\na goal-based discourse planner, and applied preference rules to determine which\ninformation could be omitted on the basis of these inferences. Finally, Zukerman\nand McConachy (1993, 2001) maintain one set of inference rules applicable to a\nconcept hierarchy, whose effect is moderated by the type of the user (as done by\nZukerman et al.). They use an optimization-based discourse planner which deter-\nmines the most concise combination of propositions to be presented, while consider-\ning the inferential effect of these propositions. Zukerman and McConachy (1995)\ndescribed an extension of this discourse planner which took into account the system's\nuncertainty regarding the stereotypical model to which a user belongs, and used a\nconstraint-based mechanism to generate discourse that avoids boredom and\ncognitive overload while achieving as much of the communicative goal as possible.\nKashihara et al. (1995) considered a student's inferences for the generation of\nexplanations which impose a cognitive load that is optimal for knowledge\nacquisition. * According to Kashihara et al., such a cognitive load is achieved by\nomitting information from explanations. This may be information a student must\nrecall to understand an explanation, or information the student requires to build\na mental structure from an explanation. During discourse planning, their system\nactivated inferences to determine the level and type of cognitive load imposed\nby a candidate explanation, and adjusted the explanation when this level was deemed\nexcessive. These inferences are similar to those described by Zukerman and\nMcConachy (1993), in the sense that they are applicable to a concept hierarchy\nand their effect is moderated by factors that re£ect a student's capabilities. However,\nunlike Zukerman and McConachy's system, Kashihara et al.'s system learned the\nvalues of these factors from interactions with each student. Kashihara et al.'s system\nwas later extended so that it can select follow-up conversational actions, e.g.\nclari¢cation questions or directives that restrict the topic of discussion, in order\nto ease a student's perceived cognitive load (Kashihara et al., 1996).\n2.3. I\nMPROVING RELEV ANCE AND APPROPRIATENESS ^ MUL TI-DIMENSIONAL USER\nMODELS\nThe discourse planning systems described in Section 2.1 consulted user models which\nrepresented one aspect of a user, e.g. beliefs, preferences or medical record. The\nrelevance and appropriateness of the generated discourse can be improved by\nintegrating in a user model several factors, such as beliefs, contextual information\nand preferences.\nIn McCoy's system (McCoy, 1989), contextual information took the form of an\nobject perspective, which highlighted the attributes in the model of a user's beliefs\nthat were most relevant to the user's topic of discussion. This perspective assisted\nin the identi¢cation of the likely source of a user's misconception. Zukerman et\n* See Kay’s article in this issue (Kay, 2001) for a discussion of student modeling.\n134 INGRID ZUKERMAN AND DIANE LITMAN\nal. (1998) modeled a similar aspect of discourse context ^ focus of attention ^ in a\nsystem that generated arguments. This was done by spreading activation from\nrecently mentioned propositions through a semantic network that was part of\nthe user model (such a network was also part of the ``system model''). The pro-\npositions activated in this manner constituted focal points around which the search\nfor an argument was conducted (this system is discussed further in Section 2.5).\nThe systems described in (van Beek, 1987; Sarner and Carberry, 1992) consulted a\nmulti-dimensional user model to generate extended responses to a user's plan-based\nqueries. The responses generated by van Beek's system pointed out incompatibilities\nbetween a user's plan and his/her goals or preferences, and provided alternatives to\nthe user's sub-optimal plans. These responses were generated by consulting a user\nmodel that represented contextual information (in the form of a user's immediate\nplans, goals and preferences); the user's background (e.g. the degree in which a\nstudent is enrolled), and default goals and plans (e.g. avoiding failure). In addition\nto contextual information in the form of a user's plans and goals, Sarner and\nCarberry's multifaceted user model represented the user's beliefs and the user's\nstylistic preferences for different types of rhetorical predicates. The combination\nof the user's beliefs with the contextual information enabled Sarner and Carberry's\nsystem to evaluate the potential usefulness of different candidate propositions to\nthe user; the most useful propositions were then included in the system's reply.\nThe system described in Hovy (1988) used a multi-dimensional user model that\nrepresented a hearer's level of expertise, emotional state, interest in the topic of\ndiscussion, and opinion regarding this topic to generate descriptions of events. This\nmodel affected decisions regarding different content planning aspects, such as\nthe level of detail of a description, which depended on the hearer's expertise and\ninterest; and the partiality of the description, which was in£uenced by the hearer's\nopinion about the topic.\nTwo systems that generate health advice also consulted multi-dimensional user\nmodels (Hirst et al., 1997; de Rosis et al., 1999). Hirst et al. (1997) developed\nan authoring tool for use by technical writers, which produced a £uent rendering\nof the information selected by these writers. In addition to basic information from\na patient's medical record, their user model contained information about a patient's\nattitude to health care, e.g. locus of control and desire to read technical detail, which\nnormally is not part of a medical record. This model afforded a degree of explanation\ncustomization that could not be achieved with user models that consisted of a\npatient's medical record only (Carenini et al., 1994; Binsted et al., 1995).\nde Rosis et al.'s system, which generated explanations about drug prescriptions,\nmaintained two user models, direct and indirect (de Rosis et al., 1999). The direct\nuser model represented doctors, who interacted with the system and had the `¢nal\nword' regarding which information was presented. The indirect model represented\nnurses or patients, who were the recipients of the ¢nal explanations and did not\ninteract with the system. The patient models contained stereotypical information\nas well as information extracted from the medical record of individual patients, while\nNATURAL LANGUAGE AND USER MODELING 135\nthe models of doctors and nurses contained only stereotypical information.The\nstereotypical model of direct users represented their propensity to discuss certain\ntopics and their stylistic preference for conciseness or verboseness, while the\nstereotypical model of indirect users represented their interest in certain topics plus\nattributes which in£uence their discourse understanding ability. This information\naffected both content planning (i.e. deciding what to say) and surface generation\n(i.e. deciding how to present the information). During content planning, an oper-\nation mode given as input to the system determined the extent to which the discourse\nshould re£ect the propensities of direct users and the interests of indirect users.\nDuring surface generation, the text was tailored to the stylistic preferences of direct\nusers and the comprehension capabilities of indirect users (Section 4).\nFinally, multi-dimensional user models play an important role in multimedia\ninterfaces. The input and output modalities of these interfaces may include linguistic\nmodalities, such as text and speech, and non-linguistic modalities, such as graphics,\nanimations and pointing. Thus, in addition to making decisions about the content\nof a presentation, multimedia interfaces must determine its modality, which is affec-\nted by users' preferences and interests. The systems described in (Bonarini, 1993;\nChin et al., 1994) consulted a multi-dimensional user model when planning\nmultimedia presentations. Bonarini's arti¢cial co-pilot consulted a model of a\ndriver's beliefs and goals to decide which advice to give to the driver, and a model\nof the driver's psychological states (attention, agitation, irritation and tiredness),\nattitudes and preferences to determine the modality of this advice (speech, text,\nmap or icon). An interesting feature of Bonarini's system is that the model of\nthe psychological states was inferred mainly from sensor input, e.g. pressing pedals\nor beeping the horn. Chin et al.'s MC (Maintenance Consultant) system used a\nmulti-dimensional user model to perform different tasks in a multimedia interface.\nTheir model represented static information, such as a user's job type, skill level\nand security access level; dynamic information regarding the user's programming\nexpertise and display preferences (inferred from the interaction with the user); con-\ntextual information pertaining to the current task; and the conversational and visual\ncontext shared by the user and the system. Skill level and security access level helped\nthe system interpret the user's requests, e.g. ``Can I do X?'' could be a direct speech\nact that asks whether the user has permission to do X, or an indirect speech act\nthat requests the system to perform X; contextual task information enabled the sys-\ntem to guide inexperienced users; programming expertise was used to decide whether\nto volunteer explanations about the system's tools; the modality of the presentation\nwas determined according to the user's display preferences; and the conversational\ncontext was used to parse the user's utterances.\n2.4. R\nEACTING TO THE USER'S FEEDBACK ^ TOWARDS INTERACTIVE SYSTEMS\nAll the systems described so far carried the burden of generating appropriate\ndiscourse, and most of these systems (except the system developed by Kashihara\n136 INGRID ZUKERMAN AND DIANE LITMAN\net al., 1996) generated `one-shot' explanations, and assumed that the user model is\nobtained from another component. In fact, Kashihara et al. foreshadowed questions\nthat must be answered as systems move towards more extended interactions with\nusers: (1) which user modeling information can we realistically expect to obtain from\ninteractions with a user? and (2) how appropriate is the discourse that can be gen-\nerated on the basis of this information?\nIn this section, we consider ¢ve NLG systems which adopt different approaches to\nanswering these questions. These systems used users' responses to gather user\nmodeling information required to plan their presentation.* Two of these systems\nfocused on planning the content of their explanations (Moore and Paris, 1992; Peter\nand Ro« sner, 1994), while the remaining systems planned both the content and the\ntype of their contribution (Cawsey, 1990, 1993; Shifroni and Shanon, 1992; Asami\net al., 1996).\nMoore and Paris (1992) adopted the opposite approach to that of the systems\ndescribed above, rejecting the reliance of NLG systems on complete and accurate\nuser models, and arguing that a system's ability to react to a user's feedback com-\npensates for the lack of reliability of user models. This approach effectively placed\non the user the majority of the burden of obtaining an acceptable explanation.\nHowever, this burden was eased by the ability of Moore and Paris' system to identify\na user's requirements from vaguely articulated follow-up questions. Such an ability\nwas afforded by a goal-based discourse planner, which yielded suf¢cient contextual\ninformation to support the interpretation of such queries. It is important to note\nthat Moore and Paris' system still consulted a user model during discourse planning\n(this model was obtained from stereotypes, interactions with the user, and\nobservable artifacts such as the user's program). However, little emphasis was placed\non the accuracy of this model, as the system made only limited attempts to update it\nas the dialogue progressed.\nIn contrast, both Cawsey (1990, 1993) and Peter and Ro« sner (1994) stressed the\nneed to maintain user models that are as complete and accurate as possible, while\ntaking into account that these models may be £awed. Both systems useddouble\nstereotypes (Chin, 1989) to determine the content of the discourse (double\nstereotypes represent the relation between a user's level of expertise and the dif¢culty\nof the concepts in the system's knowledge base). In Cawsey's system, the user model\nin£uenced the content of the explanation and the selection of rhetorical devices, as\nwell as the style of the interaction (by asking the user questions when the user model\ndid not provide suf¢cient information for content planning). In contrast, in Peter and\nRo« sner's system, the user model in£uenced mainly the content of the explanation.\nBoth systems used explicit and implicit user model acquisition. However, they\ndiffered in their approach to perform these tasks. Cawsey's system initially consulted\na stereotypical user model, and updated this model implicitly from a user's\n* Dialogue systems that focused on analyzing users’ responses to build up a user model are\ndescribed in Section 5.3.\nNATURAL LANGUAGE AND USER MODELING 137\nclari¢cation questions and acknowledgments, and explicitly from the user's answers\nto the system's questions. Implicit user model acquisition was also performed\nthrough indirect inferences drawn from information already present in the user\nmodel. However, these inferences were dynamically activated only when a piece\nof information about the user was needed for discourse planning, and their result\nwas not recorded in the user model. This obviated the need for procedures that\nhandled inconsistencies in the user model. Peter and Ro« sner's system performed\nexplicit user model acquisition brie£y at the beginning of an interaction; during\nthe remainder of the interaction, it performed only implicit acquisition from the\nuser's clari¢cation questions. In addition, Peter and Ro« sner's rules of inference\nmodi¢ed the user model, which in turn mandated a mechanism for resolving\ninconsistencies. Shifroni and Shanon (1992) re¢ned the distinction between implicit\nand explicit user-model acquisition in that they postulated a spectrum of\nmodel-acquisition actions which may be part of a generated explanation.\nSpeci¢cally, their system determined which instruction/acquisition strategy\n(explanation, question, explicit assumptionor implicit assumption) should be chosen\nin light of the information in the user model. The system then made inferences from\nthe user's response to the presented text. These inferences updated the (stereotypical)\nmodel to which the user was assigned, which in turn affected subsequent parts of the\nexplanation.\nAsami et al. (1996) adopted a similar view to that of Shifroni and Shanon, in the\nsense that they considered both questions and explanations as strategies which help\nconvey information to a student. Speci¢cally, Asami et al.'s tutoring system con-\nsulted a student model representing causal understanding of physical systems in\norder to determine the following aspects of the system's dialogue contribution:\nthe topic to be discussed, the focus of the discussion (identifying the cause of an\nobservation or predicting the effect of an action), the level of granularity of the\ncontribution (detailed or outline), and the style of the system's utterance (question,\nexplanation or alteration of the problem). The student model was in turn updated\non the basis of the student's answers.\nFinally, the two multimedia systems described in Section 2.3 also acquired some\nuser modeling information from their interaction with a user. Bonarini's\nsystem (Bonarini, 1993) inferred a user's psychological states from the user's actions,\nand updated its model of a user's beliefs and goals. Chin et al.'s system (Chin et al.,\n1994) inferred the user's expertise, display preferences and task (an initial conjecture\nregarding the user's expertise was made using double stereotypes, Chin, 1989).\nThe above discussion enables us to propose some answers to the questions posed at\nthe beginning of this section. Regarding Question (1), we see that most of the systems\ndiscussed in this section (except the systems described in Bonarini, 1993; Chin et al.,\n1994) made inferences regarding one user modeling dimension only: his/her beliefs.\nThe reply to Question (2) follows from this answer. The discourse generated by these\nsystems is similar to that generated by early content planning systems (e.g. Paris,\n1989; Tattersall, 1992), in the sense that this discourse did not take into account\n138\nINGRID ZUKERMAN AND DIANE LITMAN\ncontextual information, a user's interests and preferences, or his/her inferences,\nsince these aspects of a user model were not automatically obtained from the inter-\naction with the user.* As indicated above, Bonarini and Chin et al. point the\nway towards systems that integrate the acquisition and use of multi-dimensional\nuser models. Such systems would acquire a model of a user's beliefs using the acqui-\nsition techniques described above and in Section 5.3, and infer contextual infor-\nmation and preferences using plan recognition techniques (Section 3 and Section\n5.4). However, the acquisition of information regarding a user's inferential patterns\nis a topic for future research.\n2.5. U\nSING BAYESIAN NETWORKS TO MAKE ENHANCED USER MODELS FEASIBLE IN\nINTERACTIVE SYSTEMS\nBayesian networks (BNs) (Pearl, 1988) have been used for a variety of user modeling\ntasks (Jameson, 1996). They offer a potential solution to the problem of maintaining\nenhanced user models, as they represent both beliefs and inferences. BNs are directed\nacyclic graphs, where each node represents a belief in the value of a variable. Each\nnode is associated with aconditional probability table which represents the effect\nof other nodes on the probability of each possible value of this node. BNs can\nbe used both to make predictions and to analyze outcomes. In the context of user\nmodels for discourse planning systems, this means that a BN can be used to antici-\npate a user's beliefs from planned discourse, and that its assumptions about the user\ncan be updated according to the user's response. This avoids problems of inconsist-\nency in user models.\nThe systems described in (Zukerman et al., 1998; Jitnah et al., 2000) used BNs to\ngenerate arguments and rebuttals. Zukerman et al. (1998) used one BN to represent\nnormative beliefs and inferences (i.e. the system's beliefs) and another BN to rep-\nresent the user's beliefs and inferences in a system that generated arguments.\nBayesian propagation was used during content planning in order to determine\nthe effect of a planned argument on the system's and the user's beliefs. This distinc-\ntion between normative beliefs and the user's beliefs supported the generation of\narguments that balance normative correctness and persuasiveness. In follow-on\nwork, the system described in (Jitnah et al., 2000) generated rebuttals to rejoinders\nposed by a user to the system's arguments. This was done by consulting the user\nmodel BN and the normative model BN, as well as contextual information in\nthe form of a line of reasoning inferred from the user's rejoinder (Zukerman et al.,\n2000).\nThese systems (and others described in Jameson, 1996) point the way towards\ninteractive systems that have two important features: (1) they take advantage of\nthe ability of BNs to represent beliefs and inferences in order to maintain enhanced\n* Cawsey’s system identified and corrected erroneous beliefs, but it lacked the user modeling\ninformation to determine the source of these beliefs, as done in McCoy’s system (McCoy,\n1989).\nNATURAL LANGUAGE AND USER MODELING 139\nuser models; and (2) they use the predictive and analytical capabilities of BNs to\nautomatically maintain consistent and accurate user models. Although this solves\nsome of the problems pointed out earlier, there are questions that must be answered\nin order to fully support enhanced user models for interactive systems: (1) how\ndo we determine the structure of a BN and the conditional probability tables?\n(2) can we adapt the structure of a network and the conditional probability tables\nto accommodate individual users and changes in users? At present, the structure\nof BNs is mainly hand-crafted, but the conditional probability tables can be auto-\nmatically obtained from the observed behaviour of users. However, owing to the\nlarge amounts of data required to learn these tables, they are currently obtained\nfrom populations of users, rather than from individual users (Zukerman and\nAlbrecht, 2001). The second question and the related question of learning the struc-\nture of BNs are currently being considered by the machine learning and uncertainty\nin Arti¢cial Intelligence communities (Friedman and Goldszmidt, 1999).\n3. Natural Language Understanding ^ Plan Recognition\nAs shown in the previous section, if a natural language system is to tailor its\nresponses to a particular user, the system needs to have some sort of model of\nthe user. One type of information that has often been included in such user models\nis knowledge about a user's plans and goals. Plan recognition in natural language\nis concerned with constructing a model of a user's plans and goals during the course\nof understanding the user's utterances. The system can then use this model to both\nbetter understand subsequent utterances, and to generate more cooperative and\nhelpful responses.\nMore generally, plan recognition is an active research area in Arti¢cial\nIntelligence, as well as a promising approach for handling many problems that arise\nin the area of natural language pragmatics. In fact, the complex problems faced by\nnatural language processing systems have led to many signi¢cant advances in\nthe ¢eld of plan recognition. Underlying the plan-based approach to natural\nlanguage is a view of human communication as rational action. Speakers are\nassumed to have goals, which they are attempting to achieve via plans containing\ncommunicative actions (e.g. utterances) and possibly other types of actions. Plan\nrecognition, in turn, is the process of inferring such goals and plans from a speaker's\nutterances. Here, we only brie£y note some of the natural language understanding\ntasks that have been handled using plan recognition. Details regarding both plan\nrecognition techniques and motivating applications (both within and outside the\narea of natural language processing) can be found in Carberry (2001) in this issue.\nThe ability to reason about plans has been used in diverse areas of natural\nlanguage processing, such as understanding stories, understanding the intentions\nbehind a user's utterances, providing new methods for explaining surface linguistic\nphenomena, and handling issues arising in extended dialogue. In the area of story\nunderstanding, Wilensky (1983) used plan-based inferences to explain the actions\n140\nINGRID ZUKERMAN AND DIANE LITMAN\nin a story, while Charniak and Goldman (1993) assembled BNs from a plan-based\nrepresentation in order to handle the probabilistic aspects of story understanding.\nIn question-answering and dialogue systems, recognition of the intentions behind\na user's utterances has enabled systems to generate a wide range of cooperative\nresponses. For example, by inferring an underlying plan motivating a user's\nutterances, a system can anticipate obstacles that might prevent the user from suc-\ncessfully executing his/her plan, and provide augmented responses to remove these\nobstacles (Allen and Perrault, 1980). In addition to using plan recognition to supply\nmore information than explicitly requested (Allen and Perrault, 1980), plan recog-\nnition has formed the basis of systems that understand indirect speech\nacts (Perrault and Allen, 1980), respond to ill-formed queries (Carberry, 1988),\ndetect and correct misconceptions (Quilici, 1989), handle queries based on invalid\nplans (Pollack, 1990), and recognize complex discourse acts such as expressions\nof doubt (Carberry and Lambert, 1999). Plan-based reasoning has also provided\nnew methodologies for handling traditional linguistic phenomena, such as resolving\nreferring expressions (Grosz, 1977) and inter-sentential ellipsis (Carberry, 1985;\nLitman, 1986). Furthermore, extending plan recognition to track context and to\nrecognize plans at multiple levels has proved to be effective for reasoning about\nissues arising in extended dialogue: understanding sub-dialogues entered into to\nclarify or correct plans (Litman and Allen, 1987), obtain information required\nto execute a plan (Lochbaum, 1995), or negotiate con£icting beliefs (Carberry\nand Lambert, 1999); inferring an agent's plan incrementally as a dialogue\nprogresses (Carberry, 1990); and inferring communicative goals that are conveyed\nincrementally rather than in a single utterance (Lambert and Carberry, 1991).\nFinally, plan recognition systems that explicitly handle uncertainty are capable\nof tracking and evaluating multiple alternatives in order to recognize intentions that\nare revised over multiple utterances (Carberry, 1990; Raskutti and Zukerman, 1991;\nCharniak and Goldman, 1993).\n4. Considering Surface Features of Language\nNLU systems that consult user models in relation to surface features have focused on\ntwo main tasks: (1) assistance with second language acquisition (Schuster, 1985;\nMcCoy et al., 1996), and (2) adaptation of natural language interfaces (Fain Lehman\nand Carbonell, 1989; Allen and Bryant, 1996). Both types of systems consulted user\nmodels which encoded deviations of the user's language from the system's language.\nSchuster's system, which covered only verbs and prepositions, represented these\ndeviations implicitly in a grammar of Spanish ^ the native language of the users\nbeing modeled (Schuster, 1985). In contrast, Fain Lehman and Carbonell (1989),\nAllen and Bryant (1996) and McCoy et al. (1996), whose systems covered a variety\nof linguistic phenomena, represented these deviations by means of rules that\nperformed dynamic modi¢cations to correct grammar productions. McCoy et al.\nused a large set of mal-rules (Sleeman, 1984) which represented common errors\nNATURAL LANGUAGE AND USER MODELING 141\nperformed by deaf students of written English whose native language was American\nSign Language. Fain Lehman and Carbonell used a set of four devices (insertion,\ndeletion, substitution and transposition of terms in correct grammar productions)\nto model users' possible departures from standard English when interacting with\na natural language interface. Following in Fain Lehman and Carbonell's footsteps,\nAllen and Bryant investigated the use of a uni¢cation-based formalism to represent\nthe linguistic knowledge that supports an adaptive parser, instead of Fain Lehman\nand Carbonell's case-based formalism. This led to the identi¢cation of additional\nopportunities for language adaptation.\nBoth McCoy et al. (1996) and Fain Lehman and Carbonell (1989) enhanced their\nuser models with additional information that moderated the applicability of differ-\nent deviations from a normative grammar. These enhancements were in£uenced\nby the task performed by their system and by the system's operating conditions.\nMcCoy et al.'s system corrected a student's grammar mistakes, which were expected\nto diminish over time. Their system maintained a feature-based model of English,\nwhere the ordering of the features was tailored to a user's characteristics (e.g.\nthe user's native language). This model together with the mal-rules (each of which\nwas applicable to only a few features) supported the identi¢cation of language fea-\ntures where the user was likely to make mistakes. In contrast, Fain Lehman and\nCarbonell's system dynamically adapted the grammar accepted by a natural\nlanguage interface in order to facilitate a user's interaction with the interface. This\nwas done by learning `ungrammatical' language patterns that re£ected the user's\ngrammatical style, and using these patterns when parsing the user's subsequent\ninput.\nThe NLG systems described in Bateman and Paris (1989) and de Rosis et al. (1999)\nconsulted a user model during surface generation. Bateman and Paris de¢ned\nregisters (Halliday, 1978) which speci¢ed language features employed by different\ntypes of users (e.g. system developers and end users). By using these registers, their\nsystem generated different phrasings for the same propositional content according\nto the type of the target audience. In de Rosis et al.'s system, the verbalization\nof individual sentences was tailored to the comprehension capabilities of a user,\nwhich were determined from the user's age and level of instruction. A future avenue\nof research which parallels the developments in NLU systems involves the automatic\nadaptation of the vocabulary presented by a reading tutor as a student's reading\nability improves (a non-adaptive reading tutor is described in Mostow and Aist,\n1997). In addition, as indicated in Marcu (1996), a promising area of investigation,\nin particular in the area of health education, pertains to tailoring the verbalization\nof discourse according to its persuasiveness for different types of users.\n5. Dialogue Systems\nWhile the majority of the earliest user modeling research was in the area of dialogue\nsystems (e.g. Kass and Finin, 1988; Kobsa and Wahlster, 1989), research in user\n142 INGRID ZUKERMAN AND DIANE LITMAN\nmodeling and dialogue has both dramatically dropped off over the years, and\nchanged greatly in character. The initial dialogue systems were typically text-based,\nwhere a user was expected to ask a question or enter some input. The system would\nthen use sophisticated user modeling capabilities to tailor its responses to individual\nusers, to provide more cooperative responses, to correct or prevent user\nmisconceptions, etc. The dimensions modeled by early user models included\ngoals/plans, * capabilities, attitudes, and beliefs regarding the domain, the world\nand other agents (although many systems focused on one dimension). Often, the\ndialogue itself was used to incrementally acquire or update the user model, both\nvia system inferences on user utterances and via system queries to the user.\nUnfortunately, most of these early systems typically worked on only a few carefully\nhand-crafted examples. Furthermore, while the adaptations performed by these sys-\ntems were intuitive, the utility of these adaptations was never seriously evaluated.\nThis type of system has largely disappeared in recent years. Recent research has\ninstead emphasized the development of often shallower but more robust systems,\nwhere the user models are more empirically motivated and sometimes even acquired\nautomatically, the logic-based knowledge representations are less expressive but\nmore tractable, and where new representations that are robust to uncertainty\nare used. In addition, speech is increasingly supported, as are experimental and\nquantitative evaluations of system performance.\n5.1. K\nNOWLEDGE REPRESENTATION AND REASONING ^ PROVIDING INFERENTIAL\nSUPPORT FOR DIALOGUE\nThere has been a long tradition of modeling the participants in a dialogue as rational\nagents who perform plan recognition (see Carberry, 2001 in this issue) and other\ntypes of rational inferences. As a result, much of the work in the ¢eld of user\nmodeling for dialogue has been concerned with developing representational and\nreasoning mechanisms to support such inferential approaches.\nInitially, the ¢eld was dominated by knowledge-based formalisms such as plans\n(Section 3) and various types of logics. Hustadt (1994) argued that modal logic,\nwhich is a very expressive representation, is needed to deal with the important\nnotions of belief and desire; he then presented a modal logic for supporting\nstereotype-based user modeling in mixed-initiative dialogue. His work was\nmotivated by a previous proposal to use modal logic, again with the goal of\nrepresenting and reasoning about agents using notions such as belief, intention\nand argument (Allgayer et al., 1992). Dols and van der Sloot (1992) presented a\nbelief-based formalization of communicative conventions to support the acquisition\nof new beliefs as a dialogue progressed. Models of an agent's beliefs (both about\nhim/herself and the conversational partner) have also often been represented in\nlanguages derived from epistemic logic (Moore, 1980; Konolige, 1986) (see\n* See Section 3 for a discussion of plan recognition applications to natural language.\nNATURAL LANGUAGE AND USER MODELING 143\nHintikka, 1962 for a discussion on epistemic logic). However, Taylor, Carletta and\nMellish (1996) argued that such complex belief models supporting unlimited nesting\nof beliefs are in fact unnecessary if dialogue participants are cooperative. They pro-\nposed a simpler belief representation for such cases, and argued that besides being\nmore computationally tractable, their belief model yields more human-like referring\nand repair strategies.\nIncreasingly, probabilistic representation systems have been used to support\ninference in dialogue systems. Jameson et al. (1995) showed how multi-attribute\nutility theory and BNs can provide a unifying framework for dialogue (and other)\nsystems that perform adaptive evaluation-oriented information provision. The users\nof such systems had the goal of making evaluative judgments, while the system pro-\nvided the users with the information needed to make these judgments. Such an\napproach integrates dialogue tasks such as move planning within a general frame-\nwork of quantitative user modeling. More recently, Horvitz and Paek (1999) illus-\ntrated the use of Bayesian user models for a variety of conversational tasks. In\nparticular, they described a Bayesian representation, inference strategies and control\nprocedures that can be used to infer speaker goals from linguistic and other inputs, to\ncontrol question asking, and to control dialogue £ow. As discussed below, Berthold\nand Jameson (1999) used a BN to assess the cognitive load of a user, and Chu-Carroll\nand Brown (1998) used Dempster^Shafer theory for modeling mixed initiative.\n5.2. M\nIXED-INITIATIVE DIALOGUE ^ ALLOWING FOR FLEXIBLE CONTROL OF AN\nINTERACTION\nIn collaborative situations, expertise is often distributed among multiple agents. As a\nresult, the agent in control (i.e. the agent with the initiative) often switches back and\nforth to other agents. Similarly, a system that can participate in mixed-initiative\ndialogue must be able to recognize when to take control of the interaction, and when\nto relinquish control to the user. Thus, from a user modeling perspective, one way a\nsystem can tailor a dialogue is to adapt the level of initiative that the system takes\nin response to the user's needs and preferences. However, as will become evident\nfrom the discussion below, there have been many different uses of the term ``mixed\ninitiative'' in the literature. Cohen et al. (1998) presented a useful survey and syn-\nthesis of existing approaches.\nChu-Carroll and Brown (1998) argued that in a collaborative problem-solving\nenvironment, it is necessary to distinguish between two types of initiative ^ task\nand dialogue. An agent has the task initiative when s/he controls how a task plan\nshould be accomplished (e.g. by proposing a domain action). In contrast, an agent\nhas the dialogue initiative when s/he controls the conversation (e.g. by establishing\nmutual beliefs between the agents). Chu-Carroll and Brown used an evidential\napproach based on Dempster^Shafer theory to learn a model for predicting when\nboth types of initiative shifted from one conversational participant to another, given\nthe current initiative holders and a set of user cues. In particular, who held each type\n144\nINGRID ZUKERMAN AND DIANE LITMAN\nof initiative and various cues for shifting initiative (e.g. silence at the end of an\nutterance) were hand-labeled in a training set of utterances. An evaluation showed\nthat user cues improved the accuracy of predictions about who held both types\nof initiative.\nIn follow-on work, Chu-Carroll used this framework to build an adaptive\nmixed-initiative spoken dialogue system for providing information about\n¢lms (Chu-Carroll, 2000). The system predicted whether it or the user had each type\nof initiative, and generated different responses depending on the initiative holder. An\nempirical evaluation with human subjects demonstrated that the use of mixed\ninitiative increased user satisfaction and dialogue ef¢ciency (Chu-Carroll and\nNickerson, 2000). Smith and Hipp (1994) also collected human^computer dialogues\n(using their circuit trouble-shooting spoken dialogue system), in order to empirically\nevaluate whether the use of adirective or declarative initiative mode would yield\nmore ef¢cient dialogues. In the directive mode their system always tried to achieve\nits own goals, while in the declarative mode the system tried to ¢nd and adopt com-\nmon goals with the user. Their experimental results demonstrated that the\ndeclarative mode allowed the system to achieve a goal using fewer utterances.*\nOther research has used computer^computer dialogue simulations to explore the\nutility of allowing mixed-initiative dialogue behaviors. Guinn (1998) presented a\nprescriptive model for automating mixed initiative, and used experimental\ncomputer^computer dialogue simulations to assess the ef¢ciency of various schemes.\nSimilarly, Ishizaki et al. (1999) used computer^computer simulations to determine\nwhen a mixed-initiative dialogue strategy (using an initiative model based on\nWhittaker and Stenton, 1988) improves dialogue ef¢ciency in a route ¢nding\ndomain. Their experimental results showed that for easy problems (where dif¢culty\nwas estimated as the ratio of the length of the shortest route and the length of\nthe found route), mixed-initiative dialogues are sometimes a little more ef¢cient\nthan non-mixed-initiative dialogues. However, for dif¢cult problems, non-mixed-\ninitiative dialogues are in fact more ef¢cient.\nAlthough not in the area of natural language systemsper se, a recent research\ntrend has been to apply ideas from mixed-initiative dialogue systems to other types\nof interactive systems.** Stein et al. (1999) focused on providing mixed-initiative\ncapabilities in the context of information retrieval interactions, where users often\nopportunistically change their goals and strategies. Cesta and D'Aloisi (1999) con-\nsidered issues of mixed-initiative interaction in the context of delegation-based agent\nsystems (e.g. a meeting scheduling assistant), while Lester et al. (1999) focused on the\narea of life-like agents for learning environments. Rich and Sidner (1998) developed\na collaboration manager for software interface agents, based on collaborative\ntheories from the area of natural language discourse.\n* Other work on initiative and spoken dialogue (Allen et al., 1996; Hagen, 1999; Litman and\nPan, 1999) is described below in Section 5.5.\n** Several of these systems hope to incorporate full-blown natural language understanding\ncomponents in the future.\nNATURAL LANGUAGE AND USER MODELING 145\nFinally, the research described in Chu-Carroll and Carberry (1995) and Green and\nCarberry (1999) is in the intersection of the areas of mixed initiative and NLG\n(Section 2). Chu-Carroll and Carberry developed a computational strategy for\ndeciding when to initiate an information-sharing sub-dialogue and determining\nthe focus of this sub-dialogue in the context of a collaborative activity. Green\nand Carberry focused on the use of mixed initiative for reply generation. They pro-\nposed that a system should maintain and evaluate a set of stimulus conditions\nas supplements to discourse plan operators, in order to know when and how to incor-\nporate extra information into replies to yes^no questions. Both systems relied on a\nmodel of the user's beliefs and the user's plan. Such a model can be explicitly or\nimplicitly conveyed (sometimes incrementally) from previous dialogue, or inferred\nusing stereotypes, by applying standard techniques for inferring user models (e.g.\nKass, 1991).\n5.3. U\nSING DIALOGUE TO INCREMENTALL Y BUILD USER MODELS\nUser modeling systems have often relied on hand-crafted user models provided in\nadvance by a system designer. In the context of a dialogue system (and as discussed\nin Section 2.4), there is also the opportunity for a system to dynamically and\nincrementally update its user model as a dialogue progresses. For example, each\ntime the user makes an utterance, the system can use the contents of the utterance\n(explicitly or via inference) to update the user model. A system can also take a more\nactive role, by initiating a sub-dialogue whenever it feels that it would be useful\nto obtain some missing user modeling information (e.g. Cawsey, 1990, 1993, dis-\ncussed in Section 2; Wu, 1991 and van Beek, 1991, discussed in Carberry, 2001\nin this issue). The intuition behind the active approach is that the length added\nby the extra utterances in a knowledge-acquisition sub-dialogue might in fact reduce\nthe length of subsequent dialogue, thus reducing dialogue length overall (Shifroni\nand Shanon, 1992).\nChin (1989) and Kass (1991) made a ¢rst step towards developing a domain inde-\npendent module for acquiring a user model, where information about a user was\ninferred during the user's dialogue with an advisory system. This was done by using\ninformation obtained during the interaction to activate hand-crafted\ndomain-independent heuristics for acquiring a user model (e.g. infer that the user\nknows the generalization of a concept if the user talks about three speci¢c\ninstantiations of the concept, infer that the user understands some terminology\nif the user does not ask for a clari¢cation). Interestingly, the domain-independent\nrules separately developed by Kass and by Chin had very little overlap.\nMore recently, machine learning techniques have been used to automatically\nderive user model acquisition rules. However, while the automatically learned rules\nare similar in form to those developed by hand by Chin and Kass, their content\nand focus differ. For example, rules have been automatically learned to predict\nspeech misrecognitions (Litman and Pan, 2000), as discussed below in Section 5.6.\n146\nINGRID ZUKERMAN AND DIANE LITMAN\nQuilici (1994) showed how to use speci¢c types of negative user-feedback during\nplan-oriented dialogues with an advisory system in order to infer a set of\nplan-oriented user beliefs that are likely contributors to misconceptions. This work\ndiffers from work discussed earlier in this article (e.g. `one-shot' approaches to hand-\nling misconceptions described in McCoy, 1989; Pollack 1990) in that the user model\nwas inferred gradually from user feedback as the dialogue progressed. This allowed\nthe system to (1) provide an initial ``standard'' answer which is tuned as the dialogue\nprogresses, (2) avoid inference by waiting for the user to provide missing details, and\n(3) control inference by trying to relate feedback only to previous dialogue responses.\nIn addition, Quilici's work focused primarily on understanding a user's feedback,\nwhile previous work on combining explanation planning and user feedback focused\non generating a response (e.g. Moore and Paris, 1992 and Cawsey, 1990, 1993\nin Section 2.4). As Quilici himself noted, work in explanation and feedback could\nbene¢t by integrating these approaches.\n5.4. M\nODELING PREFERENCES IN CONSUL TATION DIALOGUES\nMorik (1989) and Elzer et al. (1994) argued that to better support consultation\ndialogues, users' preferences should be recognized during a dialogue. Both Morik\nand Elzer et al. considered the acquisition of a model of a user's preferences.\nHowever, Morik used this model to generate replies to the user's questions, while\nElzer et al. used it to evaluate and improve the user's plan.\nMorik's system (Morik, 1989), used both explicit and implicit user model acqui-\nsition to determine the user's preferences (which she called ``evaluation criteria''\nor ``evaluation standards''). Explicit user model acquisition was performed to obtain\nbasic facts about a user from which a set of stereotypes was inferred. These\nstereotypes, as well as the user's follow-up questions, were used to infer the user's\nevaluation criteria. The match between these criteria and the domain properties\nknown to the system enabled the system to moderate the strength of its recommen-\ndations and to determine whether and how to provide additional information when\nanswering the user's questions. For instance, when asking whether a proposition\nP is true, the user may wantP or :P. Depending on the user's evaluation criteria,\nthe system may offer additional domain information related to the features ofP\nor to those of:P.*\nElzer et al. (1994) presented a recognition strategy that utilizes characteristics of\nboth a user's utterances and the dialogue to model attribute-value preferences.\nFor example, a user of a course-advising system might have a preference for after-\nnoon courses. This preference could be explicitly conveyed by the user (e.g. ``I like\nafternoon courses''), or deduced by the system based on an analysis of rejected\nsuggestions. In addition to recognizing a user's preferences, the system used the\nway in which the preferences were conveyed to generate a strength for these prefer-\n* This idea is similar to Jameson’s (1989), whereby the inclusion or exclusion of information in\na reply depends on the perceived requirements of the interlocutor (Section 2.1).\nNATURAL LANGUAGE AND USER MODELING 147\nences (i.e. how important are these preferences to the user). Finally, the system main-\ntained endorsements to represent its con¢dence in its beliefs (i.e. how strongly the\nsystem believes that the preferences in its model re£ect the user's). By exploiting\nthis model of user preferences, Elzer et al. showed how a dialogue system can detect\nthat a user has proposed a sub-optimal solution, and then suggest a better\nalternative.\n5.5. T\nOWARDS SPOKEN DIALOGUE SYSTEMS\nDue to recent technological advances, it is now possible to build real-time, interactive\nspoken dialogue systems, where the user's input is passed through an automatic\nspeech recognizer, and the system's output is sent to a text-to-speech synthesizer.\nThe use of speech in dialogue systems presents new opportunities for user modeling,\nincluding the development of new types of user models, and the acquisition of\ntraditional types of user models using new methodologies and triggers.\nFor example, recent research in the area of mixed-initiative dialogues has focused\non new issues that are particularly relevant for spoken dialogue systems.\nHagen (1999) developed an approach to mixed-initiative dialogue that was tailored\nto the types of restrictions often found in telephone-based, spoken language inter-\nfaces to databases. Such systems must operate in real time, and often have access\nto only simple linguistic representations (in their case, speech recognition was based\non phrase spotting techniques). Thus, the user modeling techniques used in these\nsystems need to respect these constraints. Hagen showed how a computationally\ntractable dialogue grammar could be used to parse dialogues into a dialogue history\nof speech acts. By modeling the dialogue history and the data needs of the underlying\ninformation retrieval system, the dialogue manager of the system was able to adapt\nto a user's attempts to change initiative. Similarly, the TRAINS mixed-initiative\nspoken dialogue system (Allen et al., 1996) focused on adapting ideas underlying\nthe plan-based approach in order to handle the demands of robustness to recognition\nerrors and real-time performance inherent in speech-based systems. Finally, the\nVERBMOBIL speech-to-speech translation system (Alexandersson et al., 1997)\nfocused on achieving robustness by combining knowledge-based and statistical\napproaches to dialogue processing (this system is discussed further in Section 5.6).\nLitman et al. (1999) proposed a new type of user model for managing initiative in\ndialogue systems with speech input. They argued that a spoken dialogue system\nshould dynamically estimate the performance of the speech recognition component\nfor every user in every dialogue. They applied a machine learning approach to learn\nclassi¢cation rules that predict poor speech recognition performance from features\navailable in system logs, using previously labeled human^computer dialogue corpora\nas training data. In follow-on work, Litman and Pan (2000) implemented a spoken\ndialogue system which used these predictions to automatically change dialogue\ninitiative, e.g. the system started asking focused questions when a sustained high\nlevel of misrecognition was inferred.\n148\nINGRID ZUKERMAN AND DIANE LITMAN\nBecause the use of speech recognition often leads to errorful input, the issue of\nsystem veri¢cation of user input takes on extra importance in spoken dialogue\nsystems. Smith (1998) designed and evaluated strategies for dynamically deciding\nwhether to con¢rm each user utterance during a task-oriented dialogue. Simulation\nresults suggested that context-dependent adaptation strategies (e.g. using dialogue\nexpectations based on task knowledge, in addition to using local information from\nthe parser) can improve performance, especially when the system has greater\ninitiative.\nBerthold and Jameson (1999) also proposed a new component for incorporation\ninto a user model for a dialogue system, namely a representation of the cognitive\nload of a user. Since high cognitive load results from situational distractions,\nthe authors speculated that it would be useful for a system to adapt to inferred\ncognitive load by using simpler input or output strategies. As a ¢rst step toward\nthis goal, the authors used an empirically motivated BN to assess a user's cognitive\nload from characteristics of his/her speech (e.g. sentence fragments and articulation\nrate).\n5.6. T\nOWARDS EMPIRICAL METHODS ^ STRENGTHENING THE EMPIRICAL BASIS OF\nUSER MODELS, AND EXPERIMENTALL Y EV ALUATING THEIR UTILITY\nMost of the early work in user modeling for dialogue was motivated and justi¢ed by\nintuition, or at best by fairly informal analyses of human^human example dialogues.\nFurthermore, traditional prototype dialogue systems typically relied on\nhand-crafted plan libraries or detailed models of users' beliefs. As in many other\nareas of user modeling (see the papers in this issue by Webb et al., 2001; Zukerman\nand Albrecht, 2001; and Chin, 2001), the area of user modeling for dialogue has\nrecently seen a welcome and increasing use of empirical methods. Machine learning\nand reasoning under uncertainty techniques have been used to automatically acquire\nuser models from dialogue corpora, where the accuracy of the learned models can be\nquantitatively evaluated. In addition, experiments with human participants as well\nas with computer^computer simulations have been used to evaluate the impact that\na user modeling component has on the rest of a dialogue system.\nIn the context of the work on cognitive load discussed above, Berthold and\nJameson (1999) presented a method for synthesizing previous experimental results,\nwhich they used to derive qualitative and quantitative constraints for a BN, thus\nstrengthening its empirical basis. They also showed how to use such analyses to\ngenerate arti¢cial user data that can be used for evaluation purposes. Recall that\nother uses of arti¢cial data were discussed above, namely the use of\ncomputer^computer dialogue simulations in the context of evaluating\nmixed-initiative dialogue systems (Guinn, 1998; Ishizaki et al., 1999; Smith, 1998).\nAdvances in machine learning and reasoning under uncertainty approaches have\nalso strengthened the empirical basis of user models in dialogue systems. As dis-\ncussed above, the systems described in (Chu-Carroll and Brown, 1998; Litman\nNATURAL LANGUAGE AND USER MODELING 149\net al., 1999) automatically learned user models which were used to determine when\ninitiative should change during the course of a dialogue. To this effect, both systems\nused observations regarding the progress of the dialogue as training data. Then, the\naccuracy of the acquired user models was quantitatively evaluated on different test\ndata. In the dialogue module of the VERBMOBIL speech-to-speech translation\nsystem (Alexandersson et al., 1997), models that made plan-based inferences and\nmodels that predicted dialogue acts were acquired automatically (at least partially).\nDeclarative plan operators for plan recognition were both hand-coded and auto-\nmatically derived from a corpus using a Bayesian learning technique transferred\nfrom the ¢eld of grammar extraction. The corpus was also used to generate\npredictive models which calculate probabilities of follow-up dialogue acts given\na previous dialogue act sequence.\nFinally, as recent systems have become robust enough to try out with people,\ncontrolled experiments involving human^computer interactions have become\npossible. Litman and Pan (1999) empirically demonstrated the utility of initiative\nadaptation in a train-timetable spoken dialogue system where the user (as opposed\nto the system) triggered the adaptation process. The experimental evaluation of\nthe follow-on system described in Litman and Pan (2000), where the adaptation\nprocess was automated, showed that the adaptive system outperformed a\nnon-adaptive version. Other experimental evaluations of spoken dialogue systems\nwith human users were discussed above (Smith and Hipp, 1994; Chu-Carroll\nand Nickerson, 2000).\n6. Thoughts for the Future\nAs stated in Section 1, user models were expected to improve the ability of natural\nlanguage systems to understand a user, help achieve adaptivity in natural language\ninteractions, and increase the robustness of natural language systems. These hopes\nhave been achieved only partially, and represent a continuing challenge. In this\nsection, we consider additional challenges for user models in natural language sys-\ntems in the context of insights obtained from the preceding discussion.\nAs seen in the previous sections, a substantial proportion of natural language\nsystems which consult user models focused on achieving speci¢c natural language\ncapabilities. Researchers working on these systems stated the demands these capa-\nbilities imposed on user models, but often gave little consideration to how these\ndemands were to be satis¢ed. At ¢rst glance, such research may appear de¢cient\nin its coverage of the relevant factors. However, it is crucial for such research to\ncontinue, as it highlights the desired capabilities of a user model, without being\nrestricted by what is currently possible. This gives researchers who investigate\nthe acquisition of user models clear targets to aim for.\nAt the same time, we should build systems that integrate several natural language\ncomponents. The development of such systems offers several advantages from the\nuser modeling perspective: (1) they will require the development of comprehensive\n15 0\nINGRID ZUKERMAN AND DIANE LITMAN\nuser models that can contribute to several aspects of natural language processing,\ne.g. interpreting users' utterances, addressing possible misconceptions, and adapting\nthe content of the discourse and the style of the interaction to the users'\nrequirements; (2) these systems will afford the opportunity to evaluate the contri-\nbution of different parts of user models to natural language interactions as a whole;\nand (3) they will provide additional opportunities for the deployment of user\nmodeling systems (as well as natural language systems).\nNatural language systems that consult user models should be able to obtain suf-\n¢cient information from their interactions with users so that they can maintain con-\nsistent and accurate user models. In addition, they should be able to recover\nfrom £aws in these models and adapt to the changing requirements of users.\nMulti-dimensional user models and enhanced user models make these problems\nmore dif¢cult. Advances in machine learning and predictive statistical models offer\npromising techniques for acquiring and updating user models, thereby increasing\nthe applicability of the systems which consult these models (Webb et al., 2001;\nZukerman and Albrecht, 2001). However, the application of these techniques to\nlearn features of user models that affect natural language systems (or natural\nlanguage features that affect user models) is a relatively recent occurrence. To date,\nmachine learning techniques have been used to learn mainly coarse features of natu-\nral language systems, e.g. when to change initiative in the course of a dialogue\n(Chu-Carroll and Brown, 1998; Litman et al., 1999), and how to derive plan\noperators and predict dialogue acts (Alexandersson et al., 1997). Bayesian networks\nhave been applied to a variety of user modeling tasks related to natural language,\nsuch as plan recognition (Charniak and Goldman, 1993), argument generation\n(Zukerman et al., 1998) and dialogue modeling (Jameson et al., 1995; Horvitz\nand Paek, 1999), but these networks were hand-crafted. In Section 2 we have\nprojected future advances that will enable Bayesian networks to better support\nthe requirements of adaptive natural language systems. However, more general\nquestions must be answered in the context of using techniques from machine learning\nand reasoning under uncertainty to support the user modeling requirements of natu-\nral language systems: (1) which natural language and user modeling capabilities can\nbe supported by the currently available techniques? and (2) which additional\nadvances are required to support capabilities that are not catered for at present?\nThe evaluation of the contribution of user models to natural language systems is\nan important and dif¢cult issue. Such an evaluation must assess the accuracy of\nthe user model consulted by a natural language system, and demonstrate a\nmeasurable improvement in the performance of this system as a result of consulting\nthis model (which assesses indirectly the accuracy of the user model). Although\nthe evaluation of the performance of natural language systems is the topic of much\ndebate, and some researchers have conducted both types of evaluations with respect\nto the speci¢c capabilities of their systems (e.g. Litman and Pan, 1999; Zukerman\nand McConachy, 2001), generally accepted guidelines for the evaluation of user\nmodels and natural language systems are still forthcoming.\nNATURAL LANGUAGE AND USER MODELING 151\nFinally, the deployment of natural language systems that consult user models will\nincrease the immediacy of the demand for robust natural language systems. Such\nsystems are expected to operate in realistic application domains and to be useful\nto a wide variety of people. The construction of such systems will also require\nthe extension of currently available user modeling techniques. Here too we look\nto advances in machine learning and predictive statistical models to provide insights\nregarding such extensions.\nAcknowledgments\nThe authors would like to thank Sandra Carberry, David Chin and Constantinos\nStephanidis for their thoughtful comments.\nReferences\nAlexandersson, J., Reithinger, N. and Maier, E.: 1997, Insights into the Dialogue Processing\nof VERBMOBIL. In: Proceedings of the Fifth Conference on Applied Natural Language\nProcessing. Washington, D.C., pp. 33^40.\nAllen, C. S. and Bryant, B. R.: 1996, Learning a User's Linguistic Style: Using an Adaptive\nParser to Automatically Customize a Uni¢cation-Based Natural Language Grammar.\nIn: UM96 ^ Proceedings of the Fifth International Conference on User Modeling.Kona,\nHawaii, pp. 35^42.\nAllen, J. and Perrault, C.: 1980, Analyzing Intention in Utterances.Arti¢cial Intelligence 15,\n143^178.\nAllen, J. F., Miller, B. W., Ringger, E. K. and Sikorski, T.: 1996, A Robust System For Natu-\nral Spoken Dialogue. In:Proceedings of the Thirty-Fourth Annual Meeting of the Associ-\nation of Computational Linguistics (ACL). Santa Cruz, California, pp. 62^70.\nAllgayer, J., Ohlback, H. J. and Reddig, C.: 1992, Modelling Agents with Logic. In:UM92 ^\nProceedings of the Third International Workshop on User Modeling. Dagstuhl, Germany,\npp. 22^34.\nAsami, K., Takeuchi, A. and Otsuki, S.: 1996, Methods for Modeling and Assisting Causal\nUnderstanding in Physical Systems. In:UM96 ^ Proceedings of the Fifth International Con-\nference on User Modeling.Kona, Hawaii, pp. 145^152.\nBateman, J. and Paris, C.: 1989, Phrasing a Text in Terms a User can Understand. In:IJCAI89\n^ Proceedings of the Eleventh International Joint Conference on Arti¢cial Intelligence.\nDetroit, Michigan, pp. 1511^1517.\nBerthold, A. and Jameson, A.: 1999, Interpreting Symptoms of Cognitive Load in Speech\nInput. In:UM99 ^ Proceedings of the Seventh International Conference on User Modeling.\nBanff, Canada, pp. 235^144.\nBinsted, K., Cawsey, A. and Jones, R.: 1995, Generating Personalized Patient Information\nUsing the Medical Record. In:Proceedings of the Fifth Conference on Arti¢cial Intelligence\nin Medicine ^ Europe, Lecture Notes in Computer Science,934, 29^41.\nBonarini, A.: 1993, Modeling Issues in Multimedia Car-Driver Interaction. In: M. T.\nMaybury (ed.): Intelligent Multimedia Interfaces. Menlo Park, California: AAAI\nPress/The MIT Press, pp. 353^371.\nCarberry, S.: 1985, A Pragmatics-based Approach to Understanding Intersentential Ellipsis.\nIn: Proceedings of the Twenty-Third Annual Meeting of the Association for Computational\nLinguistics. Chicago, Illinois, pp. 188^197.\n152 INGRID ZUKERMAN AND DIANE LITMAN\nCarberry, S.: 1988, Modeling the User's Plans and Goals.Computational Linguistics 14(3),\n23^37.\nCarberry, S.: 1990, Incorporating Default Inferences into Plan Recognition. In:AAAI90 ^\nProceedings of the Eight National Conference on Arti¢cial Intelligence. Boston,\nMassachusetts, pp. 471^478.\nCarberry, S.: 2001, Techniques for Plan Recognition.User Modeling and User-Adapted Inter-\naction 11(1^2), 31^48 (this issue).\nCarberry, S. and Lambert, L.: 1999, A Process Model for Recognizing Communicative Acts\nand Modeling Negotiation Subdialogues. Computational Linguistics 25(1), 1^53.\nCarenini, G., Mittal, V. and Moore, J. D.: 1994, Generating Patient Speci¢c Interactive Natu-\nral Language Explanations. In: Proceedings of the Eighteenth Symposium on Computer\nApplications in Medical Care. Banff, Canada.\nCarenini, G. and Moore, J. D.: 1999, Tailoring Evaluative Arguments to a User's Preferences.\nIn: UM99 ^ Proceedings of the Seventh International Conference on User Modeling.Banff,\nCanada, pp. 299^301.\nCawsey, A.: 1990, Generating Explanatory Discourse. In: R. Dale, C. Mellish, and M. Zock\n(eds.): Current Research in Natural Language Generation. Academic Press, pp. 75^102.\nCawsey, A.: 1993, User Modeling in Interactive Explanations. User Modeling and\nUser-Adapted Interaction 3(3), 221^248.\nCesta, A. and D'Aloisi, D.: 1999, Mixed-Initiative Issues in an Agent-Based Meeting\nScheduler. User Modeling and User-Adapted Interaction 9(1-2), 45^78.\nCharniak, E. and Goldman, R. P.: 1993, A Bayesian Model of Plan Recognition.Arti¢cial\nIntelligence 64(1), 50^56.\nChin, D.: 1989, KNOME: Modeling What the User Knows. In: A. Kobsa and W. Wahlster\n(eds.): User Models in Dialog Systems.Springer-Verlag, pp. 74^107.\nChin, D.: 2001, Empirical Evaluation of User Models.User Modeling and User Adapted Inter-\naction 11(1^2), 181^194 (this issue).\nChin, D. N., Inaba, M., Pareek, H., Nemoto, K., Wasson, M. and Miyamoto, I.: 1994,\nMulti-Dimensional User Models for Multi-media I/O in the Maintenance Consultant.\nIn: UM94 ^ Proceedings of the Fourth International Conference on User Modeling.Hyannis,\nMassachusetts, pp. 139^144.\nChu-Carroll, J.: 2000, MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for\nInformation Queries. In: Proceedings of the Sixth ACL Conference on Applied Natural\nLanguage Processing (ANLP). Seattle, Washington, pp. 97^104.\nChu-Carroll, J. and Brown, M. K.: 1998, An Evidential Model for Tracking Initiative in\nCollaborative Dialogue Interactions.User Modeling and User-Adapted Interaction8(3-4),\n215^253.\nChu-Carroll, J. and Carberry, S.: 1995, Generating Information-Sharing Subdialogues in\nExpert-User Consultation. In:IJCAI95 ^ Proceedings of the Fourteenth International Joint\nConference on Arti¢cial Intelligence. Montreal, Canada, pp. 1243^1250.\nChu-Carroll, J. and Nickerson, J. S.: 2000, Evaluating Automatic Dialogue Strategy Adap-\ntation for a Spoken Dialogue System. In:Proceedings of the First Conference of the North\nAmerican Chapter of the Association for Computational Linguistics (NAACL). Seattle,\nWashington, pp. 202^209.\nCohen, R., Allaby, C., Cumbaa, C., Fitzgerald, M., Ho, K., Hui, B., Latulipe, C., Lu, F.,\nMoussa, N., Pooley, D., Quian, A. and Siddiqi, S.: 1998, What is Initiative?User Modeling\nand User-Adapted Interaction 8(3-4), 171^214.\nde Rosis, F., Grasso, F. and Berry, D.: 1999, Re¢ning Instructional Text Generation after\nEvaluation. Arti¢cial Intelligence in Medicine 17, 1^36.\nNATURAL LANGUAGE AND USER MODELING 153\nDols, F. J. and van der Sloot, K.: 1992, Modelling Mutual Effects in Belief-based Interactive\nSystems. In: UM92 ^ Proceedings of the Third International Workshop on User Modeling.\nDagstuhl, Germany, pp. 3^19.\nElzer, S., Chu-Carroll, J. and Carberry, S.: 1994, Recognizing and Utilizing User Preferences\nin Collaborative Consultation Dialogues. In: UM94 ^ Proceedings of the Fourth Inter-\nnational Conference on User Modeling. Hyannis, Massachusetts, pp. 19^24.\nFain Lehman, J. and Carbonell, J.: 1989, Learning the User's Language: A Step Towards\nAutomated Creation of User Models. In: A. Kobsa and W. Wahlster (eds.):User Models\nin Dialog Systems. Springer-Verlag, pp. 163^194.\nFriedman, N. and Goldszmidt, M.: 1999, Learning Bayesian Networks from Data.Tutorial\nD3 ^ The Sixteenth International Joint Conference on Arti¢cial Intelligence.Stockholm,\nSweden.\nGreen, N. and Carberry, S.: 1999, A Computational Mechanism for Initiative in Answer\nGeneration. User Modeling and User-Adapted Interaction 9(1^2), 93^132.\nGrosz, B. J.: 1977, The Representation and Use of Focus in Dialogue Understanding. Tech-\nnical Report 151, SRI International, Menlo Park, California.\nGuinn, C. I.: 1998, An Analysis of Initiative Selection in Collaborative Task-Oriented Dis-\ncourse. User Modeling and User-Adapted Interaction 8(3-4), 255^314.\nHagen, E.: 1999, An Approach to Mixed Initiative Spoken Information Retrieval Dialogue.\nUser Modeling and User-Adapted Interaction 9(1-2), 45^78.\nHalliday, M. A.: 1978,Language as Social Semiotic. London: Edward Arnold.\nHintikka, J.: 1962, Knowledge and Belief. New York: Cornell University Press.\nHirst, G., DiMarco, C., Hovy, E. and Parsons, K.: 1997, Authoring and Generating\nHealth-Education Documents that are Tailored to the Needs of the Individual Patient.\nIn: UM97 ^ Proceedings of the Sixth International Conference on User Modeling.Sardinia,\nItaly, pp. 107^118.\nHoracek, H.: 1997, A Model for Adapting Explanations to the User's Likely Inferences.User\nModeling and User-Adapted Interaction 7(1), 1^55.\nHorvitz, E. and Paek, T.: 1999, A Computational Architecture for Conversation. In:UM99 ^\nProceedings of the Seventh International Conference on User Modeling.Banff, Canada, pp.\n201^210.\nHovy, E. H.: 1988,Generating Natural Language under Pragmatic Constraints. Hillsdale,\nNew Jersey: Lawrence Erlbaum Associates.\nHustadt, U.: 1994, A Multi-Modal Logic for Stereotyping. In:UM94 ^ Proceedings of the\nFourth International Conference on User Modeling. Hyannis, Massachusetts, pp. 87^92.\nIshizaki, M., Crocker, M. and Mellish, C.: 1999, Exploring Mixed-Initiative Dialogue Using\nComputer Dialogue Simulation. User Modeling and User-Adapted Interaction 9(1^2),\n45^78.\nJameson, A.: 1989, But What Will the Listener Think? Belief Ascription and Image Main-\ntenance in Dialog. In: A. Kobsa and W. Wahlster (eds.):User Models in Dialog Systems.\nSpringer-Verlag, pp. 255^312.\nJameson, A.: 1996, Numerical Uncertainty Management in User and Student Modeling: An\nOverview of Systems and Issues.User Modeling and User-Adapted Interaction5, 193^251.\nJameson, A., Schafer, R., Simons, J. and Weis, T.: 1995, Adaptive Provision of\nEvaluation-Oriented Information: Tasks and Techniques. In: IJCAI95 ^ Proceedings of\nthe Fourteenth International Joint Conference on Arti¢cial Intelligence.Montreal, Canada,\npp. 1886^1893.\nJitnah, N., Zukerman, I., McConachy, R. and George, S.: 2000, Towards the Generation of\nRebuttals in a Bayesian Argumentation System. In:Proceedings of the International Natu-\nral Language Generation Conference. Mitzpe Ramon, Israel, pp. 39^46.\n15 4 INGRID ZUKERMAN AND DIANE LITMAN\nJoshi, A., Webber, B. L. and Weischedel, R. M.: 1984, Living Up to Expectations: Computing\nExpert Responses. In:AAAI84 ^ Proceedings of the Fourth National Conference on Arti-\n¢cial Intelligence. Austin, Texas, pp. 169^175.\nKashihara, A., Hirashima, T. and Toyoda, J.: 1995, A Cognitive Load Application in\nTutoring. User Modeling and User-Adapted Interaction 4(4), 279^303.\nKashihara, A., Nomura, K. Hirashima, T. and Toyoda, J.: 1996, Collaboration and Student\nModeling in Instructional Explanation. In:UM96 ^ Proceedings of the Fifth International\nConference on User Modeling. Kona, Hawaii, pp. 161^168.\nKass, R.: 1991, Building a User Model Implicitly from a Cooperative Advisory Dialog.User\nModeling and User-Adapted Interaction 1(3), 203^258.\nKass, R. and Finin, T.: 1988, Modeling the User in Natural Language Systems. Com-\nputational Linguistics 14(3), 5^22.\nKay, J.: 2001, Learner Control. User Modeling and User Adapted Interaction 11(1^2),\n111^127 (this issue).\nKobsa, A. and Wahlster, W. (eds.): 1989,User Models in Dialog Systems.Springer-Verlag.\nKonolige, K.: 1986, A Deduction Model of Belief.Morgan Kaufman.\nLambert, L. and Carberry, S.: 1991, A Tripartite Plan-based Model of Dialogue. In:Pro-\nceedings of the Twenty-Ninth Annual Meeting of the Association for Computational\nLinguistics. Berkeley, California, pp. 47^54.\nLascarides, A. and Oberlander, J.: 1992, Abducing Temporal Discourse. In: R. Dale, E. H.\nHovy, D. Ro« sner, and O. Stock (eds.): Aspects of Automated Language Generation.\nSpringer-Verlag, Berlin, pp. 167^182.\nLester, J. C., Stone, B. A. and Stelling, G. D.: 1999, Lifelike Pedagogical Agents for\nMixed-Initiative Problem Solving in Constructivist Learning Environments.User Modeling\nand User-Adapted Interaction 9(1-2), 1^44.\nLitman, D. J.: 1986, Understanding Plan Ellipsis. In:AAAI86 ^ Proceedings of the Fifth\nNational Conference on Arti¢cial Intelligence. Philadelphia, Pennsylvania, pp. 619^624.\nLitman, D. J. and Allen, J. F.: 1987, A Plan Recognition Model for Subdialogues in Con-\nversation. Cognitive Science 11, 163^200.\nLitman, D. J. and Pan, S.: 1999, Empirically Evaluating an Adaptable Spoken Dialogue\nSystem. In:UM99 ^ Proceedings of the Seventh International Conference on User Modeling.\nBanff, Canada, pp. 55^64.\nLitman, D. J. and Pan, S.: 2000, Predicting and Adapting to Poor Speech Recognition in a\nSpoken Dialogue System. In:AAAI00 ^ Proceedings of the Seventeenth National Confer-\nence on Arti¢cial Intelligence. San Antonio, Texas, pp. 722^728.\nLitman, D. J., Walker, M. A. and Kearns, M. J.: 1999, Automatic Detection of Poor Speech\nRecognition at the Dialogue Level. In:Proceedings of the Thirty-Seventh Annual Meeting\nof the Association for Computational Linguistics (ACL). College Park, Maryland, pp.\n309^316.\nLochbaum, K.: 1995, The Use of Knowledge Preconditions in Language Processing. In:\nIJCAI95 ^ Proceedings of the Fourteenth International Joint Conference on Arti¢cial\nIntelligence. Montreal, Canada, pp. 1260^1266.\nMarcu, D.: 1996, The Conceptual and Linguistic Facets of Persuasive Arguments. In:Pro-\nceedings of ECAI-96 Workshop ^ Gaps and Bridges: New Directions in Planning and NLG.\nBudapest, Hungary, pp. 43^46.\nMcCoy, K. F.: 1989, Highlighting a User Model to Respond to Misconceptions. In: A. Kobsa\nand W. Wahlster (eds.):User Models in Dialog Systems.Springer-Verlag, pp. 233^254.\nMcCoy, K. F., Pennington, C. and Suri, L. Z.: 1996, English Error Correction: A Syntactic\nUser Model Based on Principled ``Mal-Rule'' Scoring. In:UM96 ^ Proceedings of the Fifth\nInternational Conference on User Modeling. Kona, Hawaii, pp. 59^66.\nNATURAL LANGUAGE AND USER MODELING 155\nMehl, S.: 1994, Forward Inferences in Text Generation. In:ECAI94 ^ Proceedings of the\nEleventh European Conference on Arti¢cial Intelligence. Amsterdam, The Netherlands,\npp. 525^529.\nMilosavljevic, M.: 1997, Augmenting the User's Knowledge via Comparison. In:UM97 ^\nProceedings of the Sixth International Conference on User Modeling. Sardinia, Italy,\npp. 119^130.\nMoore, J. D. and Paris, C. L.: 1992, Exploiting User Feedback to Compensate for the\nUnreliability of User Models.User Modeling and User-Adapted Interaction2(4), 287^330.\nMoore, R. C.: 1980, Reasoning about Knowledge and Action. Ph. D. thesis, Department of\nElectrical Engineering and Computer Science.\nMorik, K.: 1989, User Models and Conversational Settings: Modeling the User's Wants. In:\nA. Kobsa and W. Wahlster (eds.): User Models in Dialog Systems. Springer-Verlag,\npp. 364^385.\nMostow, J. and Aist, G.: 1997, The Sounds of Silence: Towards Automated Evaluation of\nStudent Learning in a Reading Tutor that Listens. In: AAAI97 ^ Proceedings of the\nFourteenth National Conference on Arti¢cial Intelligence. Providence, Rhode Island,\npp. 355^361.\nParis, C. L.: 1989, The Use of Explicit User Models in a Generation System for Tailoring\nAnswers to the User's Level of Expertise. In: A. Kobsa and W. Wahlster (eds.):User Models\nin Dialog Systems. Springer-Verlag, pp. 200^232.\nPearl, J.: 1988,Probabilistic Reasoning in Intelligent Systems.San Mateo, California: Morgan\nKaufmann Publishers.\nPerrault, C. and Allen, J.: 1980, A Plan-Based Analysis of Indirect Speech Acts.American\nJournal of Computational Linguistics 6, 167^182.\nPeter, G. and Ro« sner, D.: 1994, User-Model-Driven Generation of Instructions. User\nModeling and User-Adapted Interaction 3(4), 289^320.\nPollack, M.: 1990, Plans as Complex Mental Attitudes. In: P. Cohen, J. Morgan, and M.\nPollack (eds.): Intentions in Communication. MIT Press, pp. 77^103.\nQuilici, A.: 1989, Detecting and Responding to Plan-Oriented Misconceptions. In: A. Kobsa\nand W. Wahlster (eds.):User Models in Dialog Systems.Springer-Verlag, pp. 108^132.\nQuilici, A.: 1994, Forming User Models by Understanding User Feedback.User Modeling and\nUser-Adapted Interaction 3(4), 321^358.\nRaskutti, B. and Zukerman, I.: 1991, Generation and Selection of Likely Interpretations\nduring Plan Recognition. User Modeling and User-Adapted Interaction 1(4), 323^353.\nRich, C. and Sidner, C. L.: 1998, COLLAGEN: A Collaboration Manager for Software Inter-\nface Agents. User Modeling and User-Adapted Interaction 8(3^4), 315^350.\nSarner, M. H. and Carberry, S.: 1992, Generating Tailored De¢nitions Using a Multifaceted\nUser Model. User Modeling and User-Adapted Interaction 2(3), 181^210.\nSchuster, E.: 1985, Grammars as User Models. In:IJCAI85 ^ Proceedings of the Ninth In-\nternational Joint Conference on Arti¢cial Intelligence.Los Angeles, California, pp. 20^22.\nShifroni, E. and Shanon, B.: 1992, Interactive User Modeling: An Integrative Explicit-Implicit\nApproach. User Modeling and User-Adapted Interaction 2(4), 331^366.\nSleeman, D.: 1984, Mis-Generalization: An Explanation of Observed Mal-rules. In:Proceed-\nings of the Sixth Annual Conference of the Cognitive Science Society.Boulder, Colorado,\npp. 51^56.\nSmith, R. W.: 1998, An Evaluation of Strategies for Selectively Verifying Utterance Meanings\nin Spoken Natural Language Dialog.International Journal of Human-Computer Studies48,\n627^647.\nSmith, R. W. and Hipp, D. R.: 1994,Spoken Natural Language Dialog Systems: A Practical\nApproach. Oxford University Press.\n15 6 INGRID ZUKERMAN AND DIANE LITMAN\nStein, A., Gulla, J. A. and Thiel, U.: 1999, User-Tailored Planning of Mixed Initiative\nInformation-Seeking Dialogues. User Modeling and User-Adapted Interaction 9(1^2),\n133^166.\nStock, O. and the ALFRESCO Project Team: 1993, ALFRESCO: Enjoying the Combination\nof Natural Language Processing and Hypermedia for Information Exploration. In: M. T.\nMaybury (ed.): Intelligent Multimedia Interfaces. Menlo Park, California: AAAI\nPress/The MIT Press, pp. 197^2244.\nTattersall, C.: 1992, Generating Help for Users of Application Software.User Modeling and\nUser-Adapted Interaction 2(3), 211^248.\nTaylor, J. A., Carletta, J. and Mellish, C.: 1996, Requirements for Belief Models in\nCooperative Dialogue. User Modeling and User-Adapted Interaction 6(1), 23^68.\nvan Beek, P.: 1987, A Model for Generating Better Explanations. In:Proceedings of the\nTwenty-Fifth Annual Meeting of the Association for Computational Linguistics.Stanford,\nCalifornia, pp. 215^220.\nvan Beek, P. and Cohen, R.: 1991, Resolving Plan Ambiguity for Cooperative Response Gen-\neration. In: IJCAI91 ^ Proceedings of the Twelfth International Joint Conference on Arti-\n¢cial Intelligence. Sydney, Australia, pp. 938^944.\nWallis, J. W. and Shortliffe, E. H.: 1985, Customized Explanations Using Causal Knowledge.\nIn: B. C. Buchanan and E. H. Shortliffe (eds.):Rule-based Expert Systems: The MYCIN\nExperiments of the Stanford Heuristic Programming Project.Addison-Wesley Publishing\nCompany, pp. 371^388.\nWebb, G. I., Pazzani, M. J. and Billsus D.: 2001, Machine Learning for User Modeling.User\nModeling and User Adapted Interaction 11(1^2), 19^29 (this issue).\nWhittaker, S. and Stenton, P.: 1988, Cues and Control in Expert-Client Dialogues. In:Pro-\nceedings of the Twenty-Sixth Annual Meeting of the Association of Computational Linguis-\ntics (ACL). Buffalo, New York, pp. 123^130.\nWilensky, R.: 1983,Planning and Understanding. Reading, Massachusetts: Addison-Wesley.\nWu, D.: 1991, Active Acquisition of User Models: Implications for Decision-Theoretic Dialog\nPlanning and Plan Recognition. User Modeling and User-Adapted Interaction 1(2),\n149^172.\nZukerman, I. and Albrecht, D.: 2001, Predictive Statistical Models for User Modeling.User\nModeling and User Adapted Interaction 11(1^2), 5^18 (this issue).\nZukerman, I., Jitnah, N. McConachy, R. and George, S.: 2000, Recognizing Intentions from\nRejoinders in a Bayesian Interactive Argumentation System. In:PRICAI2000 ^ Proceed-\nings of the Sixth Paci¢c Rim International Conference on Arti¢cial Intelligence.Melbourne,\nAustralia, pp. 252^263.\nZukerman, I. and McConachy, R.: 1993, Consulting a User Model to Address a User's\nInferences during Content Planning. User Modeling and User Adapted Interaction 3(2),\n155^185.\nZukerman, I. and McConachy, R.: 1995, Generating Discourse across Several User Models:\nMaximizing Belief while Avoiding Boredom and Overload. In: IJCAI95 ^ Proceedings\nof the Fourteenth International Joint Conference on Arti¢cial Intelligence. Montreal,\nCanada, pp. 1251^1257.\nZukerman, I. and McConachy, R.: 2001, WISHFUL: A Discourse Planning System that Con-\nsiders a User's Inferences.Computational Intelligence 17(1).\nZukerman, I., McConachy, R. and Korb, K. B.: 1996, Consulting a User Model while Gen-\nerating Arguments. In:UM96 ^ Proceedings of the Fifth International Conference on User\nModeling. Kona, Hawaii, pp. 153^160.\nNATURAL LANGUAGE AND USER MODELING 157\nZukerman, I., McConachy, R. and Korb, K. B.: 1998, Bayesian Reasoning in an Abductive\nMechanism for Argument Generation and Analysis. In: AAAI98 ^ Proceedings of the\nFifteenth National Conference on Arti¢cial Intelligence.Madison, Wisconsin, pp. 833^838.\nAuthors' Vitae\nIngrid Zukerman is an Associate Professor in Computer Science at Monash\nUniversity. She received her B.Sc. degree in Industrial Engineering and Management\nand her M.Sc. degree in Operations Research from the Technion ^ Israel Institute of\nTechnology. She received her Ph.D. degree in Computer Science from UCLA in\n1986. Since then, she has been working in the School of Computer Science and Soft-\nware Engineering at Monash University. Her areas of interest are discourse\nplanning, plan recognition and agent modeling.\nDiane Litman is a Principal Technical Staff Member at AT&T Labs ^ Research. She\nreceived her A.B. degree in Mathematics and Computer Science from the College of\nWilliam and Mary in Virginia in 1980, and her M.S. and Ph.D. degrees in Computer\nScience from the University of Rochester in 1982 and 1986, respectively. Since then,\nshe has been working in the Arti¢cial Intelligence Principles Research Department,\nAT&T Labs ^ Research (formerly Bell Laboratories); from 1990^1992, she was also\nan Assistant Professor of Computer Science at Columbia University. Her research\ninterests include computational linguistics, knowledge representation and reasoning,\nnatural language learning, plan recognition, and spoken language processing.\n158\nINGRID ZUKERMAN AND DIANE LITMAN",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.793756365776062
    },
    {
      "name": "Natural language generation",
      "score": 0.6732596158981323
    },
    {
      "name": "Natural language",
      "score": 0.6105586886405945
    },
    {
      "name": "Modeling language",
      "score": 0.5665830373764038
    },
    {
      "name": "Natural language user interface",
      "score": 0.546840488910675
    },
    {
      "name": "User modeling",
      "score": 0.5369176268577576
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5043176412582397
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.5029954314231873
    },
    {
      "name": "User requirements document",
      "score": 0.48873114585876465
    },
    {
      "name": "User interface",
      "score": 0.3857506215572357
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2963714003562927
    },
    {
      "name": "Software engineering",
      "score": 0.1702762246131897
    },
    {
      "name": "Programming language",
      "score": 0.1037190854549408
    },
    {
      "name": "Software",
      "score": 0.06373879313468933
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}