{
  "title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
  "url": "https://openalex.org/W4392669898",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2614489282",
      "name": "JoongHoon Kim",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2102859279",
      "name": "Sang-Min Lee",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2230276977",
      "name": "Seung Hun Han",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Saeran Park",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2110071566",
      "name": "Jiyoon Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5111072209",
      "name": "Kiyoon Jeong",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2127074906",
      "name": "Pilsung Kang",
      "affiliations": [
        "Korea University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392669714",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4319165821",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3042103108",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W3174489133",
    "https://openalex.org/W3194252721",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4389518754",
    "https://openalex.org/W3047988459",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4377864715",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4323697341",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4389523995",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4385849424",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4365211542"
  ],
  "abstract": "This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.",
  "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 164–183\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nWhich is better? Exploring Prompting Strategy For LLM-based Metrics\nJoonghoon Kim Saeran Park Kiyoon Jeong\nSangmin Lee Seung Hun Han Jiyoon Lee Pilsung Kang*\nKorea University, Seoul, Republic of Korea\n{joonghoon_kim,saeran_park,kiyoon_jeong,sangmin_lee,andrewhan,jiyoon_lee,pilsung_kang}\n@korea.ac.kr\nAbstract\nThis paper describes the DSBA submissions\nto the Prompting Large Language Models as\nExplainable Metrics shared task, where sys-\ntems were submitted to two tracks: small and\nlarge summarization tracks. With advanced\nLarge Language Models (LLMs) such as GPT-\n4, evaluating the quality of Natural Language\nGeneration (NLG) has become increasingly\nparamount. Traditional similarity-based met-\nrics such as BLEU and ROUGE have shown\nto misalign with human evaluation and are ill-\nsuited for open-ended generation tasks. To ad-\ndress this issue, we explore the potential capa-\nbility of LLM-based metrics, especially lever-\naging open-source LLMs. In this study, wide\nrange of prompts and prompting techniques are\nsystematically analyzed with three approaches:\nprompting strategy, score aggregation, and ex-\nplainability. Our research focuses on formu-\nlating effective prompt templates, determin-\ning the granularity of NLG quality scores and\nassessing the impact of in-context examples\non LLM-based evaluation. Furthermore, three\naggregation strategies are compared to iden-\ntify the most reliable method for aggregating\nNLG quality scores. To examine explainabil-\nity, we devise a strategy that generates ratio-\nnales for the scores and analyzes the charac-\nteristics of the explanation produced by the\nopen-source LLMs. Extensive experiments pro-\nvide insights regarding evaluation capabilities\nof open-source LLMs and suggest effective\nprompting strategies.1\n1 Introduction\nAs Large Language Models (LLMs) like GPT-4\ncontinue to advance rapidly, the Natural Language\nGeneration (NLG) capability is approaching a level\nof expertise comparable to that of a human. As\na result, the precise evaluation of NLG has be-\ncome increasingly paramount. However, traditional\n1Code for this paper is available athttps://github.com/\nkjhoon7686/Prompt4LLM-Eval.\nsimilarity-based metrics like BLEU (Papineni et al.,\n2002) and ROUGE (Lin, 2004), which are widely\nused in NLG evaluations, tend to show a discrep-\nancy from human assessments (Liu et al., 2023).\nAdditionally, the reliance on reference texts for\nthese metrics can hinder an accurate assessment of\nNLG quality, particularly for open-ended genera-\ntion tasks.\nRecent research has introduced methodologies\nthat leverage LLMs as NLG evaluators, showcas-\ning the potential of LLM-based metrics. These ap-\nproaches are motivated from findings in recent re-\nsearch which revealed that LLM can directly evalu-\nate NLG capabiltiy harnessing knowledge retained\nduring the pre-train (Xu et al., 2023). These metrics\nhave demonstrated notable correlation (Fu et al.,\n2023; Liu et al., 2023; Kocmi and Federmann,\n2023; Fernandes et al., 2023) with human eval-\nuations to learned evaluators (Chiang and yi Lee,\n2023; Svikhnushina and Pu, 2023).\nConcurrently, recent advancement of LLMs such\nas LLaMA (Touvron et al., 2023), Vicuna (Zheng\net al., 2023), and Orca (Mukherjee et al., 2023), has\npaved a way for research on NLG evaluations utiliz-\ning open-source LLMs (Xu et al., 2023). However,\nthere are few comprehensive studies that systemati-\ncally evaluate the vast amount of possible prompts\nand prompting techniques for LLM-based metrics.\nEspecially, research assessing the capabilities of\nopen-source LLMs in the context of LLM-based\nmetrics is even more scarce. Given the importance\nof enhancing the reproducibility of LLM-based\nmetrics in metric research, there is a clear need for\nstudies that explore effective prompts and prompt-\ning techniques specifically for open-source LLMs\n(Chiang and yi Lee, 2023).\nIn this work, we conduct a thorough exploration\nof various prompts and prompting techniques for\neffective deployment of open-source LLMs as met-\nrics: analyze them in terms of prompting strategy,\nscore aggregation, and explainability.\n164\nWithin the scope of prompting strategies, we\ncompare the effectiveness of human and model in-\nstruction templates for NLG evaluation. In addition,\nwe explore granularity in score assignment to ac-\ncurately evaluate NLG quality. Additionally, we\ngauge the influence of the open-source LLM’s In-\nContext Learning (ICL) capability (Brown et al.,\n2020) in NLG evaluation by employing various\ntypes of demonstrated examples. For score aggre-\ngation, we compare three methodologies to dis-\ncern the optimal strategy for aggregating NLG\nquality scores. To infer the explainability of open-\nsource LLMs, we generate rationale when com-\nputing scores. These comprehensive experiments\non prompting techniques for LLM-based metrics\nprovide insights into the evaluation capabilities\nof open-source LLMs and guidelines for effective\nprompting strategies.\nFurthermore, we provide insights derived from\nanalysis of the features embedded in prompts and\nbehaviors of open-source LLMs as LLM-based\nmetrics. Additionally, we report our strategies and\noutcomes applied to the test set of summarization\ntrack in Eval4NLP 2023 shared task.\n2 Related Work\nSimilarity-based Metrics Similarity-based\nmetrics evaluate the quality of NLG outputs\nby comparing reference and candidate text.\nThey can be categorized into lexical-based and\nsemantic-based metrics. Lexical-based metrics,\nsuch as BLEU (Papineni et al., 2002) and ROUGE\n(Lin, 2004), utilize N-grams to measure lexical\noverlap between a reference and a candidate\ntext. However, research has highlighted their\ninadequacy in accurately assessing the quality of\ngenerated outputs and identifying both syntactical\nand semantic discrepancies (Liu et al., 2023;\nPolišenská et al., 2021; Wu et al., 2021). On the\nother hand, semantic-based metrics, including\nBERTScore (Zhang et al., 2019) and MoverScore\n(Zhao et al., 2019), measure semantic similarity by\ncomparing the embeddings of both reference and\ncandidate texts. However, similar to lexical-based\nmetrics, they face challenges when evaluating\nopen-ended generation tasks due to their inherent\ndependence on reference text (Chiang and yi Lee,\n2023; Guan et al., 2021; Gu et al., 2021).\nLLM-based Metrics The recent substantial ad-\nvancement in the NLG capabilities of LLMs has\nmotivated research interests related to LLM-based\nmetrics. Consequently, the latest studies, primarily\nexploring various prompting approaches that do not\nrequire additional training of an LLM, has shown a\ncorrelation with human evaluation comparable to\nthat of learned evaluators (Chiang and yi Lee, 2023;\nSvikhnushina and Pu, 2023). Also, building upon\nthe foundational work of LLaMA (Touvron et al.,\n2023), research on the fine-tuning approach which\nconstructs an evaluator by fine-tuning an LLM with\nsuitable supervised data for the evaluation task, is\nbeing actively pursued (Bosselut et al., 2019; Xu\net al., 2023).\n3 Summarization Track\nThe summarization track of Eval4NLP 2023 shared\ntask (Leiter et al., 2023) aims to propose a\nreference-free metric for summarization. Specif-\nically, reference-free metric evaluates a given sum-\nmary using only the provided source sentence or\nparagraph without additional human-written refer-\nences. The objective of shared task is to develop\nLLM-based metrics by exploring effective prompt-\ning strategies for open-source LLMs.\n3.1 Dataset\n3.1.1 Train and Development Set\nIn this study, we utilize the SummEval bench-\nmark dataset provided by Fabbri et al. (2020) as\nboth train and development sets. While the origi-\nnal benchmark provides human annotation scores\nfor each of four aspects, including relevance,\nconsistency, coherence, and fluency, the sum-\nmarization track adopts the average of these aspect\nscores as golden human annotation scores. The\nperformance of the evaluation task is measured\nthrough sentence-level correlation with the golden\nhuman annotation scores.\n3.1.2 Test Set\nDataset provided in the shared task (Leiter et al.,\n2023), consisting of sentences and fragments of\nparagraphs from English Wikipedia documents\nwritten after July 15, 2023, is used as the test set.\nSummaries in the test dataset were generated by\na summary generation model that are annotated\nwith reference to Multidimensional Quality Metrics\n(MQM) annotation for aspects like factuality,\nrelevance, and readability.\n2\n165\nFigure 1: Examples of Human Guideline (HG) prompt and Model Guideline (MG) prompt. HG prompt and MG\nprompt consists of task description, evaluation criteria, and evaluation steps. The HG prompt is used as the annotation\nguideline for summarization evaluation, serving as the basis for human annotators assessments. In contrast, the MG\nprompt was used as the instruction for the model.\n3.2 Models\nWe use four out of six open-source LLMs provided\nin the Eval4NLP 2023 shared task.\n• Hermes-13B - LLaMA-13B model trained on\nover 300,000 instructions.\n• Orca-7B - LLaMA2-7B model trained on\nOrca Style dataset.\n• Orca-13B - LLaMA2-13B model trained on\nOpen-Platypus dataset and OpenOrca dataset.\n• Platypus-70B - LLaMA2-70B model trained\nby Lee et al. (2023).\n4 Method\nIn this section, we address the prompting strate-\ngies and score aggregation methods, as well as ap-\nproaches to assess the explainability of open-source\nLLMs.\n4.1 Prompting Strategy\nPrompting strategies consist of prompt template,\ngranularity of score, and demonstration.\n4.1.1 Prompt Template\nWe propose Human Guideline (HG) prompt and\nModel Guideline (MG) prompt for summary eval-\nuation as illustrated in Figure 1. The HG prompt,\nadapted from the human evaluation guideline of\nSummEval (Fabbri et al., 2020), provides clear\nevaluation instructions and criteria for human an-\nnotators.\nConversely, the MG prompt, implemented from\na guideline given to LLM such as GPT-4 for sum-\nmary evaluation in G-EV AL (Liu et al., 2023), in-\nstructs LLM to assess summaries, offering detailed,\ndirective instructions and criteria.\nBoth HG prompt and MG prompt consist of\nelements such as task description, evaluation\ncriteria, and evaluation steps. To assess the impact\nof each element, we create variants by modifying\neach one.\nTask Description The task description provides\ninstructions for the specified task. To explore\nthe influence of its length, we craft short and\nlong descriptions by varying sentence lengths,\nmaintaining the original context. Additionally, we\ncreate an expert-role task description to study the\neffect of providing an expert role in the evaluation\n(e.g. “you’re an expert at summarizing news\narticles.\"). Each variant is developed for both HG\nand MG prompts, with details in Appendix D.\nEvaluation Criteria The evaluation criteria out-\nlines the scoring standards for the given summary\nper aspect. It is categorized into three components,\n1) Aspect Definition (AD) 2) Human-Targeted cri-\nteria (HT) 3) Model-Targeted criteria (MT).\nAD, adopted from GPTScore (Fu et al., 2023),\nconcisely describes the evaluation aspect defini-\ntions. HT and MT, used in HG and MG Prompts\nrespectively, include scoring considerations and as-\n3\n166\npect descriptions.\nTo investigate the effects of each components,\nwe generate modified version of AD, HT, and MT\nfor each aspect using GPT-4. We instruct GPT-4 to\nmaintain a consistent format with the existing ones.\nExamples are provided in Appendix D.\nEvaluation Steps The evaluation steps, which\ncould be considered as a Chain-of-Thought (CoT)\n(Zhang et al., 2023), provide step-by-step instruc-\ntions for the evaluation task, enhancing the reason-\ning capabilities of LLM. To explore the impact of\nvaried evaluation steps descriptions, we construct\ndetailed complex evaluation steps for both HG and\nMG prompts. Examples are provided in Appendix\nD.\n4.1.2 Granularity of Score\nFor assigning a score, we consider the following\ntwo scoring approaches: coarse-grained scoring\nand fine-grained scoring. Coarse-grained scoring\nyields a singular and holistic score that considers\nall evaluation aspects collectively, but does not pro-\nvide scores for individual aspects. Conversely, fine-\ngrained scoring assigns the score for each aspect,\nderiving individual scores and then averaging them\nto yield the final singular score. This approach en-\nables the LLMs to furnish both the overall score\nand specific aspect scores, granting a more nuanced\nunderstanding of for score derivation compared to\nthe coarse-grained method. Given that NLG evalu-\nations commonly score by jointly taking multiple\naspects into account, adpoting fined-grained scor-\ning when constructing variants of the prompt is\nnaturally apt approach.\n4.1.3 Demonstration\nTo examine the ICL capability of open-source\nLLMs in evaluation tasks, we craft two distinct\ntypes of demonstrated examples.\nOne set of examples includes raw source text, a\nsummary, and a human annotation score. On the\nother hand, another set of examples incorporates a\nrationale derived from the assigned human annota-\ntion score, which has been distilled from GPT-42, in\naddition to the components found in the former set\nof examples. Examples are provided in Appendix\nD.\nFurthermore, we construct examples for each in-\ndividual aspect and subsequently group them into\n2https://openai.com/research/gpt-4\n’worst’ and ’best’ categories based on human an-\nnotation scores. In our study, ’worst’ examples are\nassigned a score of 1, while ’best’ examples receiv-\ning a score of 5. Categorization is undertaken to\ninvestigate potential biases in the quality and the\nscore of the provided examples. Due to the maxi-\nmum input length constraint of the LLMs, we use\nonly one example as demonstration per summary.\n4.2 Score Aggregation\nTo derive scores for individual aspects, we propose\nthe following three score aggregation methods:\nDirect, Logprob, and Approximation (see Figure\n2).\nDirect This method is the most general scoring\nmethod. It leverages the score generated by the\nLLM directly.\nLogprob This method calculates the score by\nsumming the product of a pre-defined discrete score\nrange (e.g. 1 to 5) and the generation probability\nof the corresponding tokens. This method is con-\nsidered as a weighted summation approach, using\neach score’s token probability as its weight. By\nincorporating the model’s token generation proba-\nbilities, this method distinctively produces a more\ncontinuous score.\nFor a given set of pre-defined discrete scores\nS = {s1, ..., sK}, Logprob multiplies each discrete\nscore si by its token probability p(si). K in (1) is\nthe number of pre-defined discrete scores.\nscore =\nK∑\ni=1\np(si) ·si (1)\nApproximation This method calculates the\nscore by averaging N sampled scores generated by\nLLM. Intending to approximate the token probabil-\nity distribution, we design Approximation method\nto distinguish it from the Logprob method, which\ndirectly uses the actual token probabilities. This\naggregation is inspired by techniques explored in\n(Liu et al., 2023; Fu et al., 2023).\nFor a given set of pre-defined discrete scores\nS = {s1, ..., sK}, Approximation multiplies each\ndiscrete score si by its approximated token proba-\nbility g(si). In (2), count(si) denotes the number\nof count discrete score si appears in N samples.\n4\n167\nFigure 2: (a) Left - Score AggregationAn example of how the Score Aggregation is calculated. ‘Direct’ uses\nscores directly generated by the model, ‘Logprob’ uses a weighted summation based on generation probabilities of\npre-defined scores (e.g. 1 to 5), and ‘Approximation’ uses an average from N sampled scores. (b) Right - Rational\nGeneration promptAn example of Rationale Generation (RG) prompt and the corresponding outputs. Using the\nRG prompt as input, the model provides a score for the quality of the summary and the corresponding rationale.\ng(si) = count(si)\nN (2)\nscore =\nK∑\ni=1\ng(si) ·si (3)\n4.3 Explainability\nEvaluations that employ the previously described\nmethods yield only a sole scalar score with no ad-\nditional explanation for the assigned score at all.\nThus, we manually craft the Rationale Generation\n(RG) prompt to derive rationales for the scores.\nUsing this prompt, we aim to explore the explain-\nability of open-source LLMs (see Figure 2).\nFurthermore, similar to the approach used in the\ndemonstration section 4.1.3, we use examples to\nanalyze the influence of demonstrated examples on\nrationale generation. Each example is divided into\n‘worst’ and ‘best’ example to examine potential\nbiases in the outputs.\n4.4 Test phase\nFor the test set, we incorporate two supplementary\napproaches alongside the previously described\nprompting strategy, tailored to the attributes of the\ntest set.\nFiltering Although many summaries in the test\nset exhibit appropriate sentence structures, certain\nsamples retain repetitive words or phrases (e.g. “A\nfamily of four members, including a first member,\na second member, a third member, and a fourth\nmember.\"). We deem such instance as a failure to\ngenerate an appropriate summary and uniformly\nassigned them lowest score. To account such\ninstances, we design a Filtering prompt that filters\nfailed samples. For given summaries, when model\ngenerates a ‘Yes’ response, they are assigned the\nminimum score. Example of the Filtering prompt\nis provided in Appendix D.\nBinning After analyzing the scores assigned by\nthe model for the test data, we observe that open-\nsource LLMs are generally adept at evaluating sum-\nmaries. Nevertheless, we note the model’s tendency\nof assigning excessively fine-grained scores among\nsamples of equivalent quality (e.g. scores of 1 and\n1.01). In light of these observations, we implement\nBinning to simplify the score distribution and mit-\nigate noise, thereby integrating proximate scores\ninto same categories. Detailed explanations can be\nfound in the Appendix B.\n5 Experiments\n5.1 Experimental Setup\nExperiments are conducted using the development\nset of the summarization track provided in the\nshared task. We use the provided prompt template\nfor the summarization track as the baseline prompt.\nThe baseline prompt contains a brief task descrip-\ntion and score guide. Additionally, the HG and MG\nprompt in 5.2 are adapted from SummEval (Fabbri\n5\n168\nTemplate Fine-grained Demonstration AggregationOrca-7B Orca-13B\nPrompting\nBase x x Direct 0.2500 0.3040\nHuman x x Direct 0.3094 0.4343\nModel x x Direct 0.2651 0.3583\nBase o x Direct 0.2746 0.3891\nHuman o x Direct 0.3472 0.4468\nModel o x Direct 0.2864 0.3844\nDemonstration\nHuman o Base-worst Direct 0.1758 0.3690\nHuman o Base-best Direct 0.2854 0.4092\nHuman o Reason-worst Direct 0.2309 0.3899\nHuman o Reason-best Direct 0.2733 0.4133\nAggregation\nHuman o x Approximation 0.3239 0.4002\nHuman o x Logprob 0.3296 0.4210\nHuman o x Direct 0.3472 0.4468\nModel o x Approximation0.2687 0.3530\nModel o x Logprob 0.2926 0.3851\nModel o x Direct 0.2864 0.3844\nExplainability\nRationale o x Direct 0.3506 0.4220\nRationale o Reason-worst Direct 0.2915 0.3876\nRationale o Reason-best Direct 0.3262 0.4330\nTable 1: Main result. Experimental results of combination sets for each Prompting Strategy, Score Aggregation, and\nExplainability. ‘Human’ and ‘Model’ mean Human Guideline prompt and Model Guideline prompt respectively.\nAlso, ‘Base-worst/best’ and ‘Reason-worst/best’ are abbreviations of two types of demonstration that are distin-\nguished, including rationale. Best results for each set of variants are in bold.\net al., 2020) and G-EV AL (Liu et al., 2023) with\nminimal modification. Examples of prompts are\nprovided in Appendix D. For scoring, we averaged\nthe scores derived from the aspects of relevance,\nconsistency, coherence, and fluency for fine-\ngrained scoring. For the demonstration experi-\nments, we sample examples from the train set based\non human annotation scores for each aspect. Ratio-\nnales for the scores in the examples are generated\nusing GPT-4. Throughout the entire score genera-\ntion process, we set top_p to 0.1. For Direct and\nLogprob aggregation, the temperature is set to 0.\nLastly, we set the temperature to 1 and n_samples\nto 20, respectively, for Approximation aggregation.\nMoreover, we report the leaderboard results for\nthe test set using Orca-13B and Platypus-70B for\nthe small and large track, respectively. Test set ex-\nperiments share the almost the same setting with de-\nvelopment set experiments: same HG prompt, fine-\ngrained scoring, hyperparameters for Direct aggre-\ngation are implemented. For factuality evalua-\ntion criteria, not originally provided in SummEval\n(Fabbri et al., 2020), we use GPT-4 to generate it.\nSpecifically, scores for relevance, factuality,\nand fluency, obtained from Direct aggregation,\nare averaged to compute the final score. Through-\nout our all experiments, segment-level Kendall’s\nTau correlation is used as the performance metric.\nFor optimized inference with open-source LLMs,\nwe employ Guidance3 and vLLM4 libraries. Details\nof experimental setup are provided in Appendix A.\n5.2 Main Results\n5.2.1 Prompting Strategy\nWe compare the performance with different types\nof the prompt templates. As shown in Prompting\nsection of Table 1, regardless of the granularity of\nthe score, we observe that HG and MG prompts,\nespecially HG prompt, consistently outperform the\nbaseline prompt. We hypothesize that a more de-\ntailed description of task provided in the HG and\nMG prompt allows LLM to understand and follow\nthe instructions more clearly. Moreover, among\nall the prompts, the HG prompt achieves the best\nperformance, indicating that succinct and clear in-\nstructions are better than complex ones.\nAs for granularity of the scoring, fine-grained\nscoring consistently outperforms coarse-grained\nscoring across various model sizes and prompt tem-\nplates. The coarse-grained scoring may introduce\nambiguity in the evaluation criteria by requiring the\nLLM to consider aspect-specific considerations in\nan integrated manner. Conversely, the fine-grained\nscoring removes such ambiguity by providing eval-\nuation criteria of each aspect independently.\n3https://github.com/guidance-ai/guidance\n4https://github.com/vllm-project/vllm\n6\n169\nAs shown in Demonstration section of Table 1,\nwe observe that the use of demonstration leads to\ndecrease in performance, likely due to the inher-\nent bias introduced by the demonstrated example.\nNotably, the smaller model exhibits a significant\ndecline in performance, which could be attributed\nto their limited ICL capabilities (Dong et al., 2022;\nHan et al., 2023; Wei et al., 2023), resulting in\ninaccurate understanding of in-context examples,\nand vice versa. The performance differs among\nmodels based on whether they are provided with\nexamples containing only the score or examples\nwith additional rationales. This discrepancy can\nbe attributed to the superior ability of larger mod-\nels in comprehending in-context examples, which\nleads to better understanding when explanations\nfor scores are added. In contrast, the smaller model\nexhibits the opposite behavior. Furthermore, pro-\nviding the ‘best’ examples consistently yields su-\nperior performance across all model sizes when\ncompared to the ‘worst’ examples. After conduct-\ning an analysis of the model’s score distribution, we\nobserve a bias wherein the model tends to assign\nhigher scores when provided with the ‘best’ ex-\nample. We hypothesize that observed bias may be\ndriven by the skewed distribution of human anno-\ntation scores in the development set, where human\nannotation scores are predominantly distributed to-\nwards higher values, mainly falling between 3 and\n5.\n5.2.2 Score Aggregation\nWe assess the performance based on the different\nscore aggregation methods. Aggregation section\nof Table 1 illustrates that, across various model\nsizes and prompt templates, Direct and Logprob\naggregation consistently demonstrates superior per-\nformance when compared to the Approximation\naggregation. In both Direct and Logprob aggre-\ngation, the decoding temperature is set to 0. This\nlikely leads the model to assign scores in a more\ndeterministic manner compared to the Approxima-\ntion, potentially resulting in superior performance.\nSpecifically, since Approximation estimates the dis-\ntribution of score token probability through sam-\npling, sampling noise could account for its lower\nperformance. Unlike other aggregation methods,\nDirect aggregation generates integer values rang-\ning from 1 to 5, thereby offering a much fewer\nscore range. On the other hand, Xu et al. (2023)\nsuggest that Kendall Tau might favor tie pairs. Such\ntendency could explain the notably high correlation\nobserved with Direct aggregation.\n5.2.3 Explainability\nWe assess the LLM’s ability to provide appropriate\nexplanations for the scores. Examining Explain-\nability section of Table 1, we observe that the RG\nprompt results in performance similar to or slightly\nlower than the HG prompt and better than the MG\nprompt. This suggests that generating rationales\nfor scores can also aid the evaluation process it-\nself. Furthermore, it is noteworthy that Orca-7B\nexhibits a slight performance decline when pro-\nvided with a demonstrated example, in contrast to\nthe performance of Orca-13B. The RG prompt is\nmeticulously designed to facilitate the generation\nof rationales, possibly benefiting from the exam-\nples. Therefore, Orca-13B, with superior ICL ca-\npabilities as mentioned in 4.1, has outperformed\nthe other smaller model. Analysis of the rationales\ngenerated by Orca-13B is discussed in 5.3.3.\n5.2.4 Test Phase\nOrca-13B Platypus-70B\nHuman 0.4699 0.4764\nFiltering 0.4815 -\nBinning 0.5016 0.4916\nTable 2: Kendall’s Tau correlation on test set where\nHuman denotes test result obtained with HG prompt.\nIn Table 2, we report the performance of the\nHG prompt on the test set. Details of HG prompt\napplied for the test set are provided in Appendix D.\nAs evident from the results of our development set\nexperiments, the performance of the HG prompt\non the test set is consistently satisfactory across\nall models. Furthermore, we observe a discernible\nimprovement in performance when the Filtering is\napplied. This observation suggests that uniformly\nassigning lowest scores to inadequately generated\nsummaries can enhance performance. Similarly,\nBinning enhances performance by reducing noise\nin the scores on the test set. This improvement\nis achieved by integrating closely related scores\ninto same categories. While the Orca-13B model\nexhibits a slightly lower performance compared\nto the Platypus-70B with the base HG prompt, it\nshows superior performance after the application\nof Filtering and Binning. Details of test phase are\nprovided in Appendix B.\n7\n170\n5.3 Analysis\n5.3.1 The Effect of Different Model Sizes\nWe compare the performance depending on dif-\nferent model sizes: Orca-7B, Hermes-13B, Orca-\n13B, and Platypus-70B. As shown in Appendix\nTable 4 and Table 5, despite the same size with\nOrca-13B, the performance of Hermes-13B is sig-\nnificantly lower, even lower than Orca-7B. Except\nfor Hermes-13B, generally positive correlation be-\ntween model size and performance is observed. We\nspeculate such outcome may be due to the differ-\nences in the backbone model’s performance (e.g.\nLLaMA, LLaMA 2) and the type of datasets and ap-\nproaches used for fine-tuning (Freitag et al., 2022).\nInsignificant performance gap between Platypus-\n70B and Orca-13B proves that Orca-13B is as ef-\nfective as Platypus-70B for the evaluation task.\n5.3.2 Comparisons of each Component\nTask Description Types We investigate the\nimpact of varying the length of task descrip-\ntions within the HG prompt and MG prompt\non performance. Additionally, we compare\nperformance when an expert role is assigned\nin the task description versus when it is not.\nAs shown in Appendix Table 6, for Orca-7B,\nthere is no significant performance difference\nbased on length of task descriptions. However,\nfor Orca-13B, we observe higher performance\nwhen a longer task description is employed. Such\ntendency suggests that, Orca-13B benefits from\nlonger length of task descriptions in facilitating\nthe execution of instructions, even when the\ncontent remains the same. Furthermore, when\nthe expert role is assigned, there is a discernible\nperformance improvement with Orca-7B. However,\nfor Orca-13B, the performance difference between\ncases with and without the expert role is not\nsubstantial, indicating that this approach can be\nmore effective for smaller models.\nEvaluation Criteria Variants We analyze\nthe influence of various evaluation criteria, AD,\nHT, and MT. As shown in Appendix Table 7,\nutilizing aspect definitions consistently improves\nperformance, regardless of the prompt template\nor model size. Furthermore, similar results are\nobtained even when evaluation criteria gen-\nerated by GPT-4 are used. This suggests that\nproviding a simple definition of each aspect is an ef-\nfective approach when evaluating summary quality.\nComplexity of Evaluation Steps As shown in\nTable 8, there is no significant trend in performance\nbetween standard and complex evaluation steps\nboth for the HG prompt and the MG prompt. This\nobservation implies that while the evaluation steps\nare effective in offering step-by-step instructions\nto the model, the precise description or complexity\nlevel of the evaluation steps does not exert a signif-\nicant influence on the evaluation of summaries.\n5.3.3 Error Analysis\nTo investigate whether the model generates well-\nfounded rationales for the assigned scores, we\nperform an error analysis on the rationales gener-\nated using the RG prompt described in section 4.3.\nSpecifically, we conduct such comparative analysis\non 36 sampled instances for two different rationale\ngeneration method: one generated with Orca-13B\nand RG prompt, and another with RG prompt in-\ncluding demonstrated examples.\nOur analysis reveals that, in general, the model\nexhibits the capability to provide rationales cor-\nrectly. However, we identify several types of errors:\n(Error type 1)provided rationale is inconsistent\nwith the assigned evaluation scores, (Error type\n2) provided rationale shows hallucination where\nthe rationale includes information not present in\nthe source text or summary, (Error type 3)pro-\nvided rationale describes explanation about aspect\ndifferent from the designated one. Detailed descrip-\ntions and examples for each error type can be found\nin Appendix C. Addressing and mitigating these\nerrors through further research efforts could signif-\nicantly enhance the explainability and reliability of\nLLM-based metrics.\n6 Conclusion\nIn this work, we conduct a systematic analysis of\neffective prompting techniques and strategies for\nLLM-based metrics in NLG evaluation. Our com-\nprehensive experiments reveal that providing clear\nand straightforward instructions, akin to those ex-\nplained to humans, proves to be more effective.\nFurthermore, we examine various score aggrega-\ntion methods to achieve effective score assignments\nand show the potential for enhancing explainability\nwithin open-source LLMs. Additionally, we ex-\nplore performance change relative to model size\nand scrutinize the influence of various elements\nwithin the prompt template. We hope that our re-\nsearch findings will furnish valuable insights for\n8\n171\nfuture studies focused on LLM-based metrics, es-\npecially those leveraging open-source LLMs.\nReferences\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: commonsense transformers for\nautomatic knowledge graph construction. CoRR,\nabs/1906.05317.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nCheng-Han Chiang and Hung yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions?\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi-\nfang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nAlexander R. Fabbri, Wojciech Kryscinski, Bryan\nMcCann, Caiming Xiong, Richard Socher, and\nDragomir R. Radev. 2020. Summeval: Re-evaluating\nsummarization evaluation. CoRR, abs/2007.12626.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein,\nParker Riley, André F. T. Martins, Graham Neubig,\nAnkush Garg, Jonathan H. Clark, Markus Freitag,\nand Orhan Firat. 2023. The devil is in the errors:\nLeveraging large language models for fine-grained\nmachine translation evaluation.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nJing Gu, Qingyang Wu, and Zhou Yu. 2021. Perception\nscore: A learned metric for open-ended text gener-\nation evaluation. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, pages\n12902–12910.\nJian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-\nbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie\nHuang. 2021. Openmeva: A benchmark for evalu-\nating open-ended story generation metrics. arXiv\npreprint arXiv:2105.08920.\nChi Han, Ziqi Wang, Han Zhao, and Heng Ji. 2023.\nIn-context learning of large language models ex-\nplained as kernel regression. arXiv preprint\narXiv:2305.12766.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality.\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023.\nPlatypus: Quick, cheap, and powerful refinement of\nllms.\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023. The eval4nlp\n2023 shared task on prompting large language models\nas explainable metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nKamila Polišenská, Shula Chiat, Jakub Szewczyk, and\nKatherine E Twomey. 2021. Effects of semantic plau-\nsibility, syntactic complexity and n-gram frequency\non children’s sentence repetition. Journal of Child\nLanguage, 48(2):261–284.\nEkaterina Svikhnushina and Pearl Pu. 2023. Approx-\nimating human evaluation of social chatbots with\nprompting. arXiv preprint arXiv:2304.05253.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert\nWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, et al. 2023. Larger language\nmodels do in-context learning differently. arXiv\npreprint arXiv:2303.03846.\n9\n172\nHaiyan Wu, Zhiqiang Zhang, and Qingfeng Wu. 2021.\nExploring syntactic and semantic features for au-\nthorship attribution. Applied Soft Computing ,\n111:107815.\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao\nSong, Markus Freitag, William Yang Wang, and Lei\nLi. 2023. Instructscore: Towards explainable text\ngeneration evaluation with automatic feedback.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with BERT. CoRR,\nabs/1904.09675.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023. Multimodal\nchain-of-thought reasoning in language models.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. Mover-\nscore: Text generation evaluating with contextual-\nized embeddings and earth mover distance. CoRR,\nabs/1909.02622.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\n10\n173\nA Experimental Setup\nLibrary Version\nguidance 0.0.64\nvllm 0.1.7\ntorch 2.0.1\nTable 3: Version of libraries used for the experiments.\nFor optimized inference with open-source LLMs, we employ Guidance and vLLM libraries. The\nlibraries and their respective versions used for the experiments can be found in Table 3.\nB Test Phase\nWe submit the final results for the test set after equally applying Filtering and Binning to the HG prompt\non both Orca-13B and Platypus-70B (for the small and large track, respectively). We use HT as the\nevaluation criteria of the factuality, generated using GPT-4. Scores for relevance, factuality, and\nfluency, obtained from Direct aggregation, are averaged to compute the final score. The hyperparameters\nfor Direct aggregation is set identical to the development set, with top_p to 0.1 and temperature to 0,\nrespectively. The prompts used for the test set can be found in Table 22, 23, and 24.\nFiltering is applied using the Filtering prompt on both Orca-13B and Platypus-70B models. Example of\nthe Filtering prompt is provided in Table 18. After applying Binning, the number of unique scores has\nbeen diminished from 36 to 10 and 46 to 13 for Orca-13B and Platypus-70B, respectively.\nC Analysis\nC.1 The Effect of Different Model Sizes\nWe conduct experiments to analyze the performance differences depending on model sizes using Orca-7B,\nHermes-13B, Orca-13B, and Platypus-70B. The experiments for Orca-7B, Hermes-13B, and Orca-13B are\nconducted using vLLM, while the Platypus-70B experiments are conducted using Guidance. In Table 4,\nwe conduct experiments comparing performance across model sizes for different prompt templates and\ngranularity of score. In Table 5, we carry out experiments to compare performance across model sizes for\ndifferent prompt templates and score aggregations.\nTemplate Fine-grained Demonstration Aggregation Orca-7B Orca-13B Hermes-13B Platypus-70B\nBase x x Direct 0.2500 0.3040 0.1554 0.3956\nHuman x x Direct 0.3094 0.4343 0.2041 0.4260\nModel x x Direct 0.2651 0.3583 0.1915 0.4383\nBase o x Direct 0.2746 0.3891 0.1402 0.4082\nHuman o x Direct 0.3472 0.4468 0.2063 0.4354\nModel o x Direct 0.2864 0.3744 0.2170 0.4039\nTable 4: Comparison of Kendall’s Tau correlation across various Prompt Templates and Models. Fine-grained\ndenotes whether the fine-grained scoring is used or not. Aggregation denotes the type of Score Aggregation method\nused.\nC.2 Comparisons of each Component\nTask description, evaluation criteria and evaluation steps of the prompt templates are slightly modified to\nensure the suitability for each experiment. Examples are provided in Appendix D.\nC.2.1 Task Description type\nWe investigate the impact of varying the length of task descriptions within the HG prompt and MG\nprompt on performance. Additionally, we compare performance when an expert role is assigned in the\ntask description versus when it is not. Various task descriptions are manually crafted for each prompt\n11\n174\nTemplate Fine-grained Demonstration Aggregation Orca-7B Orca-13B Hermes-13B Platypus-70B\nHuman o x Approximation 0.3239 0.4002 0.2127 0.4041\nHuman o x Logprob 0.3296 0.4210 0.2060 0.4305\nHuman o x Direct 0.3472 0.4468 0.2063 0.4354\nModel o x Approximation 0.2687 0.3530 0.2152 0.4058\nModel o x Logprob 0.2926 0.3851 0.2250 0.4316\nModel o x Direct 0.2864 0.3844 0.2170 0.4039\nTable 5: Comparison of Kendall’s Tau correlation across various Score Aggregation and Models. Fine-grained\ndenotes whether the fine-grained scoring is used or not. Aggregation denotes the type of Score Aggregation method\nused.\ntemplate, and examples can be found in Appendix D. The experimental results for the task description\ntypes can be found in Table 6.\nTemplate Task Description Orca-7B Orca-13B\nHuman\nBase 0.3472 0.4468\nExpert 0.3544 0.4383\nShort 0.3339 0.4239\nLong 0.3383 0.4501\nModel\nBase 0.2864 0.3744\nExpert 0.3302 0.3881\nShort 0.2721 0.3508\nLong 0.2767 0.3891\nTable 6: Comparison of Kendall’s Tau correlation of cases using various types of task description on development\nset. Direct aggregation and fine-grained scoring are used for the experiment. Any demonstration is not provided.\nC.2.2 Evaluation Criteria variants\nAD-GPT, HT-GPT, and MT-GPT are generated using GPT-4, tailored respectively to the AD, HT, and MT\nstyles. The experimental results based on the types of the evaluation criteria can be found in Table 7.\nTemplate Evaluation Criteria Orca-7B Orca-13B\nAD 0.3343 0.4279\nAD-GPT 0.3345 0.4336\nHT 0.3256 0.4192\nHT-GPT 0.3293 0.4192\nMT 0.3303 0.4314\nHuman\nMT-GPT 0.3344 0.4297\nAD 0.3116 0.4001\nAD-GPT 0.3115 0.4066\nHT 0.3013 0.3904\nHT-GPT 0.2987 0.3894\nMT 0.3141 0.4102\nModel\nMT-GPT 0.3037 0.3949\nTable 7: Comparison of Kendall’s Tau correlation of cases using various types of evaluation criteria on development\nset. AD-GPT, HT-GPT, and MT-GPT denote AD, HT, and MT generated by GPT-4. Direct aggregation and fine-\ngrained scoring are used for the experiment. Any demonstrated example is not provided.\nC.2.3 Complexity of evaluation steps\nComplex evaluation steps are crafted using GPT-4 for both HG and MG prompt. Examples are provided\nin Appendix D. The experimental results for the evaluation steps can be found in Table 8.\nC.3 Error Analysis\n12\n175\nTemplate Evaluation Steps Orca-7B Orca-13B\nHuman Base 0.3317 0.4135\nComplex 0.2969 0.4027\nModel Base 0.2866 0.3767\nComplex 0.2840 0.3751\nTable 8: Comparison of Kendall’s Tau correlation of base and complex evaluation steps on development set. Direct\naggregation and fine-grained scoring are used for the experiment. No demonstrated example is provided to either\nmethod.\nError Type Base Reason-best\n0 Good 50% 69%\n1 Inconsistent 11% 17%\n2 Hallucination 36% 6%\n3 Different Aspect 6% 8%\nTable 9: Error Occurrence Ratio when RG prompt with and without ‘Reason-best’ demonstration are used. In this\nanalysis, we use Orca-13B to generate a score and rationale for each aspect. Error Type 1 means that the rationale is\ninconsistent with the score. Error Type 2 means that the rationale includes hallucinated information not mentioned\nin the source text and/or summary. Error Type 3 means that the rationale is about different aspect rather than the\ndesignated aspect.\nExample\nSource\nEsteban Cambiasso has won all the major European competitions a player can during his illustrious career\nbut revealed that keeping Leicester City in the Premier League would be up there with the best.\nThe Foxes are currently seven points adrift at the bottom of the table, with only eight games remaining,\nknowing that time is running out to save themselves. Cambiasso refuses to give up and admits that keeping\nLeicester up will feel like winning a trophy. Esteban Cambiasso says that helping keep Leicester in the\nPremier League will feel like winning a trophy ‘For me, it’s like another cup,’ he told BBC East Midlands Today.\n‘When you start another season you have an objective, and this is the objective for us. ‘For me, winning a cup\nor winning the league with another team is the same now as having the possibility to save Leicester in the\nPremier League.’ The Argentinian midfielder poses with the trophy after his team won the 2010 FIFA Club\nWorld Cup Cambiasso had an illustrious career at Inter Milan, winning an impressive 15 trophies during his\nstint River Plate (2001-2002) Argentine Primera Division Real Madrid (2002-2004) La Liga Super Cup\nSupercopa de Espana Inter Milan (2004-2014) Champions League Serie A (5) Coppa Italia (4) Supercoppa\n(4) FIFA Club World Cup Having not won a game since January, Nigel Pearson’s men face West Ham United\non Saturday and Cambiasso is still convinced they can avoid the drop. ‘I understood when I signed for Leicester\nit’s not an easy job to stay in the Premier League,’ he said. ‘It’s a difficult situation but I think we have our\nchances to win matches. There’s a quarter of the Premier League left to finish. ‘I think some people think for\nLeicester all is finished. But I’m sure, because I watch my team-mates every day, we can save Leicester and stay\nin the Premier League.’ The former Inter Milan star signed for the Foxes in the summer, leaving Italy after ten\nyears and embarking on a new challenge in England. After agreeing to a one-year-deal, Cambiasso has quickly\nestablished himself as a key player but it remains to be seen if he’ll still be in the East Midlands at the start\nof next season. The former Real Madrid man was also successful during his short spell in Spain for Real Madrid\nCambiasso played during Real’s ‘Galatico’ era, with Luis Figo, Zinedine Zidane, Ronaldo and David Beckham\n‘Leicester always wanted me,’ he added. ‘After these nine months or eight months, I’m very happy because\nmy family is OK, and I’m good. ‘I want a few more points, but all the rest is perfect.’ Cambiasso is happy in\nthe East Midlands and could stay beyond his current one-year-deal\nSummaryEsteban Cambiasso is confident Leicester can win the Premier League and keep themselves in the top flight.\nScore 1\nRationale\nThe summary is coherent with the source text, as it captures the main points about Esteban Cambiasso’s confidence\nin Leicester’s chances of staying in the Premier League. However, it could be improved by including more details\nand quotes from the source text, making it more comprehensive and engaging.\nTable 10: Example of Error Type 1. The rationale is inconsistent with the score for thecoherence.\n13\n176\nExample\nSource\n\"(CNN)Two passengers found dead on a cruise ship in Puerto Rico appear to have died in a murder-suicide, the cruise line said.\nHolland America Line said two guests were found dead inside their stateroom on the ms Ryndam at 11:30 a.m. Thursday. \"\n\"The cabin was immediately secured, and the authorities were notified, including the FBI,\"\" Holland America said.\n\"\"We are cooperating fully with the investigation, and the authorities will make the official determination on what occurred.\"\"\nFBI spokesman Moises Quiñones said authorities were on scene investigating. The ship left Tampa, Florida, on March 29\non a 14-day Southern Caribbean cruise. It’s currently in San Juan, Puerto Rico. Puerto Rico Port Authority spokesman\nEfraín Santiago told El Nuevo Dia newspaper that the cleaning staff on the ship had discovered the deceased passengers\nafter knocking on the cabin’s door.\nSummaryholland america line said two guests were found dead inside their stateroom on the ship .\nthe ship left tampa , florida , on march 29 on a 14-day southern caribbean cruise . it’s currently in san juan , puerto rico\nScore 2\nRationale\nThe summary is not coherent with the source text. It omits crucial information, such as the cause of death,\nthe identities of the passengers, and the cruise ship’s itinerary. The summary also does not convey the urgency and\nseriousness of the situation, as indicated by the FBI’s involvement andthe ship’s immediate return to port. The coherence\nof the summary is poor, as it does not represent the main events and context of the source text\nTable 11: Example of Error Type 2. Rationale includes hallucination forcoherence.\nExample\nSource\nFirst blood to United after their Under 18s saw off City 1-0 in the ‘mini-derby’. Kits aside, this was probably as far removed\nfrom Sunday’s big match as you could imagine. For a start, no less than 13 out of the 22 players on show at kick-off were English.\nOf those, 10 were Mancunian. Callum Gribbin was the matchwinner for Manchester United with a delightful free-kick Ticket prices?\nEntry was free and close to 1,000 gathered on the seats and terraces of Moss Lane for a match that kicked off at 3pm on Saturday\nwith half-and-half scarves nowhere to be seen. Altrincham’s compact 6,000-capacity home may not be Old Trafford, but it does\nhave a proud history of its own. It was certainly a grander stage than a windswept Saturday morning on an outfield at United’s\nCarrington complex, where Paul McGuinness’s Under 18s usually ply their trade. The young Reds coach wanted to make the\nexperience closer to what his tyros could expect should they make the step up to the seniors. And his side lined up with three\nat the back and supporting wingbacks in a formation seen more than once this season in the first team. In an even first-half\nthe impressive Marcus Wood, from just down the road in Sale, came closest for City with an audacious chip. United manager\nPaul McGuinness saw his side claim victory in the ‘mini derby’ For the home side towering centre-forward Marcus Rashford,\nanother local lad from whom big things are expected, wasted two decent opportunities when put through. Just before the\nhour mark England Under 17 star Callum Gribbin made and scored the first. In March, the nifty-footed attacking midfielder\nwent viral when United tweeted a clip of him showing outrageous skills to trick four Aston Villa players. He was at it again\nhere, showing nifty footwork on the edge of the box and drawing a foul. After dusting himself down Gribbin stepped up to curl\na delightful free-kick around the wall and beyond blonde Joe Hart-a-like Charlie Albinson in the City goal. Moments later it was\nalmost two when full-back Cameron Borthwick-Jackson’s rasping 25-yard drive struck the angle of the crossbar and post. The same\nman looked to have gotten away with one when he appeared to clip Buckley in the box before sub Aaron Nemane hit the side-netting\nas the visitors looked for an equaliser. Dutch winger Javairo Dilrosun curled a free kick onto the roof of the net in stoppage time for\nCity but that was all she wrote. The result leaves United, who have won two of this year’s cross-town clashes, two points behind l\neaders Middlesbrough with a game in hand. Injury-ravaged City however, have now lost both matches in the Barclays Under 18s\nPremier League end-of-season play-off series. FA chief Greg Dyke recently bemoaned the lack of first-team opportunities for homegrown\ntalent between the ages of 18 and 21. On the evidence of this entertaining 90 minutes high on skill and commitment, there are talents\non both sides that may well deserve their chance in the On the evidence of this entertaining 90 minutes high on skill and commitment,\nthere are talents on both sides that may well deserve their chance in the coming years.\nSummarycallum gribbin was the matchwinner for manchester united with a free-kick . paul mcguinness’s side claim victory in the\n‘mini derby’ for a match. paul mcguinness and marcus wood were in the first team .\nScore 3\nRationale\nThe summary is not entirely consistent with the source document. It does mention Callum Gribbin as the matchwinner and the victory\nin the ‘mini derby’, but it omits important details such as the final score, the formation used by Manchester United, and the\nperformance of Marcus Wood.\nTable 12: Example of Error Type 3. The rationale does not discuss forconsistency.\nD Example Prompts\n14\n177\nTask DescriptionTemplate Prompt\nExpert Human\nYou read and summarize a lot of news articles, and you’re an expert at summarizing news articles.\nIn this task you will evaluate the quality of a summary written for a news article.\nTo correctly solve this task, follow these steps:\nExpert Model\nYou read and summarize a lot of news articles, and you’re an expert at summarizing news articles.\nYou will be given one summary written for a news article. Your task is to evaluate the summary\nbased on a specific metric, rating it on a scale from 1 (worst) to 5 (best).\nPlease make sure you read and understand these instructions carefully.\nPlease keep this document open while reviewing, and refer to it as needed.\nLong Human\nIn this task, you will evaluate the quality of a summary written for a news article.\nPlease take your time to carefully evaluate the provided summary, and don’t hesitate to refer back\nto this instruction document if you need clarification or guidance at any point during your evaluation.\nTo correctly solve this task, follow these steps:\nLong Model\nYou will be given one summary written for a news article.\nYour task is to evaluate the summary based on a specific metric, rating it on a scale from 1 (worst)\nto 5 (best). Please make sure you read and understand these instructions carefully.\nPlease keep this document open while reviewing, and refer to it as needed.Please take your time\nto carefully evaluate the provided summary, and don’t hesitate to refer back to this instruction document\nif you need clarification or guidance at any point during your evaluation.\nShort Human Evaluate the news article summary quality.\nShort Model\nEvaluate a news article summary using a specific metric, rating it from 1 (worst) to 5 (best).\nPlease read and understand these instructions carefully. Keep this document open for reference\nwhile reviewing.\nTable 13: Examples of different variants of Task Description\nEvaluation CriteriaTemplate Prompt\nHT-GPT Human\nRelevance:This rating assesses the extent to which the summary highlights the central themes\nof the original article. Evaluate if the summary encompasses the crucial elements while omitting\nany non-essential details.\nMT-GPT Model\nRelevance - gauges the summary’s alignment with the article’s primary ideas. Check if the\nsummary includes essential points and omits unrelated details. It may help to list the article’s\nmain points and verify their presence in the summary.\nAD Human,ModelRelevance - How well is the generated text relevant to its source text?\nAD-GPT Human,ModelRelevance - To what extent does the generated summary capture and reflect the core details of its source text?\nTable 14: Examples of different variants of Evaluation Criteria\nEvaluation StepsTemplate Prompt\nHuman\nIn this task, your primary aim is to conduct a thorough assessment of the summary provided for a news article.\nTo effectively accomplish this task, please adhere to the following comprehensive steps:\n1. Initiate the evaluation process by engaging in an in-depth examination of the news article.\nYour aim here is to establish a profound understanding of the article’s entire spectrum of content,\nensuring you grasp its core message, nuances, and key elements.\n2. Proceed to scrutinize the proposed summary provided alongside the article.\nIn this phase, your task is to meticulously evaluate the summary for its aspect.\n3. Assign a rating to each summary based on its aspect,\nutilizing a scale ranging from 1 (indicating the lowest quality) to 5 (signifying the highest quality).Complex\nModel\n1. Thoroughly examine the provided summary and the source document with meticulous attention to detail.\n2. Conduct a comprehensive comparative analysis, scrutinizing the summary in relation to the source document\nto discern and delineate the primary focal points and pivotal elements elucidated within the article.\n3. Engage in a judicious evaluation to gauge the summary’s efficacy\nin addressing and encompassing the central facets of the source document,\nconcurrently assessing the presence of any extraneous or duplicative information that might detract from its relevance.\n4. Utilize a relevance rating scale, ranging from 1 (indicating minimal relevance) to 5 (indicating maximal relevance),\nfor the purpose of assigning a numerical score.\nThis score serves as a quantitative reflection of the extent to which the summary aligns with\nand encapsulates the core substance of the source document.\nTable 15: Examples of Complex Evaluation Steps\n15\n178\nTemplate Prompt\nHuman, Model, Rationale\nPlease refer to following example below.\nSource text: Twice French Open champion Serena Williams said her struggle to beat Sara Errani i\nn the Fed Cup on Sunday had been a real ‘eye-opener ’ as the claycourt season gets into full swing .\nWorld No 1 Williams eventually prevailed 4-6 7-6 ( 3 ) 6-3 against the dogged Italian to take her career\nrecord over her to 8-0 but the American was not impressed . The US were beaten 3-2 as Williams\nand Alison Riske were thrashed 6-0 6-3 in the doubles rubber by Errani and Flavia Pennetta ,\nmeaning they were relegated to World Group II . American tennis star Serena Williams fought back\nto beat Italian Sara Errani in the Fed Cup play-off on Sunday Tough weather conditions made it\ndifficult for both players who had to keep on re-tossing their serves Errani gave Williams a real scare\nbut in the end the world No 1 ’s power proved to be too much ‘Today has been a big eye opener ,\n’ Williams said afterwards . ‘ I ’m totally not as ready for the claycourt season as I thought I was .\nNow I ’m in the mindset of , “ You know what , I ’m not on hard court . “ I ’m playing like I ’m on hard\ncourt and I ’m not . ‘So I have to play and be ready to hit a thousand shots if necessary . ’ Williams , 33 ,\nwon her 19th singles grand slam at the Australian Open and her dominance has raised talk of her\nclaiming all the majors this year . The French Open has been her least successful of the four though\ndespite claiming the title in Paris in 2002 and 2013 . Her doubles defeat on Sunday blotted an otherwise\nflawless Fed Cup record and left the US facing a battle to get back amongst the elite nations next year .\n‘We have to work harder , ’ US captain Mary Joe Fernandez said . ‘We came close today and need to\njust keep plugging away . ’The good news is that we have a lot of players in the top 100 and , hopefully ,\nwe can get two wins next year and get back into the World Group . ‘ Williams congratulates Italy captain\nCorrado Barazzutti after competing in America ’s doubles defeat.\nSummary: Serena Williams beat Sara Errani 4-6 7-6 ( 3 ) 6-3 in the Fed Cup play-off .\nThe US were beaten 3-2 as Williams and Alison Riske were thrashed in the doubles rubber .\nThe doubles defeat saw the US relegated to World Group II .\\u2019\n—-\nExample Score: 5\nExplanation: The summary effectively captures the key points from the article. It mentions Serena\nWilliams’ challenging match against Sara Errani and her eventual victory. The summary also highlights\nthe US team’s overall defeat and its consequence \\u2013 relegation to World Group II. These details\nare central to the main storyline of the source text, making the summary highly relevant. Thus, a score\nof 5 (best) is appropriate for the summary’s relevance.\nTable 16: Example of Demonstration with rationale\nTemplate Prompt\nRationale\nYour task is to evaluate the relevance of a provided summary based on its source document.\nFollow these steps:\n1. Read the source document\n2. Review the summary\n3. Analyze for relevance\n4. Assign a Score: Rate the summary on a scale of 1 to 5, where:\n- 1 means the summary is not relevant with the source.\n- 5 means the summary is entirely relevant with the source.\n5. Provide a Rationale: After assigning a score, explain your reasons based on your analysis.\n# Definition:\nRelevance:\nThe rating measures how well the summary captures the key points of the article.\nConsider whether all and only the important aspects are contained in the summary.\"\n—–\nSource text:\nSummary:\nTable 17: Example of Rationale Generation(RG) prompt\n16\n179\nTemplate Prompt\nFiltering\nIn this task you will evaluate the quality of a summary written for a document.\nProvided summary may include direct or rephrased repetitions of the same word or phrase.\nWith that in mind do the following:\n1. Answer whether the summary is redundant or not.\n- Your answer must be in \"Yes\" or \"No\" format, where \"Yes\" means that the summary is redundant and\n\"No\" means that the summary is not redundant.\n2. Please provide brief explanation for your answer.\n- Your explanation should only discuss the redundancy of the summary, not the quality of the summary\nin general.\n—-\nsummary:\nTable 18: Example of Filtering prompt\nTemplate Prompt\nBaseline\nScore the summarization with respect to the summarized document on a continuous scale from 0 to 100,\nwhere a score of zero means irrelevant, factually incorrect and not readable and score of one hundred means,\nrelevant, factually correct, good readability\n—-\nSource text:\nSummary:\nTable 19: Example of Baseline prompt\n17\n180\nTemplate Prompt\nModel\nYou will be given one summary written for a news article.\nYour task is to rate the summary on one metric.\nPlease make sure you read and understand these instructions carefully.\nPlease keep this document open while reviewing, and refer to it as needed.\nEvaluation Criteria:\nRelevance - selection of important content from the source. The summary should\ninclude only important information from the source document. Annotators were\ninstructed to penalize summaries which contained redundancies and excess information.\nEvaluation Steps:\n1. Read the summary and the source document carefully.\n2. Compare the summary to the source document and identify the main points of the article.\n3. Assess how well the summary covers the main points of the article, and how much irrelevant\nor redundant information it contains.\n4. Assign a relevance score from 1 to 5.\nExample:\nSource Text:\nSummary:\nEvaluation Form (scores ONLY):\n- Relevance:\nTable 20: Example of Model Guideline(MG) prompt\nTemplate Prompt\nHuman\nIn this task you will evaluate the quality of a summary written for a document.\nTo correctly solve this task, follow these steps:\n1. Carefully read the document, be aware of the information it contains.\n2. Read the proposed summary.\n3. Rate each summary on a scale from 1 (worst) to 5 (best) by its relevance.\n# Definition:\nRelevance: The rating measures how well the summary captures the key points of the article.\nConsider whether all and only the important aspects are contained in the summary.\nSource text:\nSummary:\nScore:\nTable 21: Example of Human Guideline(HG) prompt\n18\n181\nTemplate Prompt\nHuman\nInstruction:\nIn this task you will evaluate the quality of a summary written for a document.\nTo correctly solve this task, follow these steps:\n1. Carefully read the document, be aware of the information it contains.\n2. Read the proposed summary.\n3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Relevance.\n# Definition:\nRelevance: The rating measures how well the summary captures the key points of the article.\nConsider whether all and only the important aspects are contained in the summary.\nSource text:\nSummary:\nScore:\nTable 22: Example of Human Guideline(HG) prompt of relevance used in test phase\nTemplate Prompt\nHuman\nInstruction:\nIn this task you will evaluate the quality of a summary written for a document.\nTo correctly solve this task, follow these steps:\n1. Carefully read the document, be aware of the information it contains.\n2. Read the proposed summary.\n3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Factuality.\n# Definition:\nFactuality: This rating gauges the accuracy and truthfulness of the information presented\nin the summary compared to the original article.\nScrutinize the summary to ensure it presents facts without distortion or misrepresentation,\nstaying true to the source content’s details and intent.\nSource text:\nSummary:\nScore:\nTable 23: Example of Human Guideline(HG) prompt of factuality used in test phase\n19\n182\nTemplate Prompt\nHuman\nInstruction:\nIn this task you will evaluate the quality of a summary written for a document.\nTo correctly solve this task, follow these steps:\n1. Carefully read the document, be aware of the information it contains.\n2. Read the proposed summary.\n3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Fluency.\n# Definition:\nFluency: This rating evaluates the clarity and grammatical integrity of each sentence in the summary.\nExamine each sentence for its structural soundness and linguistic clarity.\nSource text:\nSummary:\nScore:\nTable 24: Example of Human Guideline (HG) prompt of fluency used in test phase\n20\n183",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7992035150527954
    },
    {
      "name": "Automatic summarization",
      "score": 0.7050648331642151
    },
    {
      "name": "Natural language generation",
      "score": 0.6131042242050171
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5932846069335938
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5810716152191162
    },
    {
      "name": "Granularity",
      "score": 0.5407661199569702
    },
    {
      "name": "Task (project management)",
      "score": 0.5037993788719177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46449947357177734
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.44893842935562134
    },
    {
      "name": "Machine learning",
      "score": 0.3852694034576416
    },
    {
      "name": "Data science",
      "score": 0.3279721736907959
    },
    {
      "name": "Natural language",
      "score": 0.28243136405944824
    },
    {
      "name": "Systems engineering",
      "score": 0.09831503033638
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}