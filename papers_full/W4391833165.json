{
  "title": "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
  "url": "https://openalex.org/W4391833165",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2164610742",
      "name": "Anjali Khurana",
      "affiliations": [
        "Simon Fraser University"
      ]
    },
    {
      "id": "https://openalex.org/A2225895094",
      "name": "Hariharan Subramonyam",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2001101459",
      "name": "Parmit K Chilana",
      "affiliations": [
        "Simon Fraser University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4230031335",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W2139374478",
    "https://openalex.org/W2916797267",
    "https://openalex.org/W2111009050",
    "https://openalex.org/W2149074597",
    "https://openalex.org/W3094588931",
    "https://openalex.org/W1984565341",
    "https://openalex.org/W2141613588",
    "https://openalex.org/W2138350998",
    "https://openalex.org/W2161052636",
    "https://openalex.org/W2941270448",
    "https://openalex.org/W2035818959",
    "https://openalex.org/W2272645513",
    "https://openalex.org/W2142389575",
    "https://openalex.org/W3094211608",
    "https://openalex.org/W2898125335",
    "https://openalex.org/W2155005209",
    "https://openalex.org/W2026566771",
    "https://openalex.org/W2107621192",
    "https://openalex.org/W2022797053",
    "https://openalex.org/W4220962633",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4366003124",
    "https://openalex.org/W3123221944",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W2516223638"
  ],
  "abstract": "Large Language Model (LLM) assistants, such as ChatGPT, have emerged as\\npotential alternatives to search methods for helping users navigate complex,\\nfeature-rich software. LLMs use vast training data from domain-specific texts,\\nsoftware manuals, and code repositories to mimic human-like interactions,\\noffering tailored assistance, including step-by-step instructions. In this\\nwork, we investigated LLM-generated software guidance through a within-subject\\nexperiment with 16 participants and follow-up interviews. We compared a\\nbaseline LLM assistant with an LLM optimized for particular software contexts,\\nSoftAIBot, which also offered guidelines for constructing appropriate prompts.\\nWe assessed task completion, perceived accuracy, relevance, and trust.\\nSurprisingly, although SoftAIBot outperformed the baseline LLM, our results\\nrevealed no significant difference in LLM usage and user perceptions with or\\nwithout prompt guidelines and the integration of domain context. Most users\\nstruggled to understand how the prompt's text related to the LLM's responses\\nand often followed the LLM's suggestions verbatim, even if they were incorrect.\\nThis resulted in difficulties when using the LLM's advice for software tasks,\\nleading to low task completion rates. Our detailed analysis also revealed that\\nusers remained unaware of inaccuracies in the LLM's responses, indicating a gap\\nbetween their lack of software expertise and their ability to evaluate the\\nLLM's assistance. With the growing push for designing domain-specific LLM\\nassistants, we emphasize the importance of incorporating explainable,\\ncontext-aware cues into LLMs to help users understand prompt-based\\ninteractions, identify biases, and maximize the utility of LLM assistants.\\n",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6801971197128296
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6680587530136108
    },
    {
      "name": "Task (project management)",
      "score": 0.6165212392807007
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5853386521339417
    },
    {
      "name": "Software",
      "score": 0.5504546761512756
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5228793621063232
    },
    {
      "name": "Relevance (law)",
      "score": 0.451787531375885
    },
    {
      "name": "Perception",
      "score": 0.4169352650642395
    },
    {
      "name": "Software engineering",
      "score": 0.3706682324409485
    },
    {
      "name": "Psychology",
      "score": 0.27055782079696655
    },
    {
      "name": "Engineering",
      "score": 0.12224143743515015
    },
    {
      "name": "Programming language",
      "score": 0.07633417844772339
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}