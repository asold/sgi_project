{
  "title": "AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy",
  "url": "https://openalex.org/W3208996281",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100690428",
      "name": "Yunxiang Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082199637",
      "name": "Guodong Zeng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032184244",
      "name": "Yifan Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100384655",
      "name": "Jun Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067368513",
      "name": "Qianni Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061363755",
      "name": "Qun Jin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031821249",
      "name": "Lingling Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027426657",
      "name": "Qisi Lian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082007895",
      "name": "Neng Xia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015009174",
      "name": "Ruizi Peng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067743569",
      "name": "Kai Tang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100352467",
      "name": "Yaqi Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100328354",
      "name": "Shuai Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3157262272",
    "https://openalex.org/W3038093367",
    "https://openalex.org/W3120723148",
    "https://openalex.org/W1967631878",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1976468890",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3019648618",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W2986413572",
    "https://openalex.org/W3099319035",
    "https://openalex.org/W2979394680",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3047026765",
    "https://openalex.org/W3014795415",
    "https://openalex.org/W1964940342",
    "https://openalex.org/W3092573066",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2475515783",
    "https://openalex.org/W2273953083",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2958187724",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2021478309",
    "https://openalex.org/W2990732233",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W2075026964",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3004358802",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2014418634",
    "https://openalex.org/W2158135241",
    "https://openalex.org/W2301541953",
    "https://openalex.org/W2478394825",
    "https://openalex.org/W3107410755",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2110754128",
    "https://openalex.org/W2592949994",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2614217642",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3111325699",
    "https://openalex.org/W3132065658",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W1985716027",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2533800772",
    "https://openalex.org/W2956655325"
  ],
  "abstract": "Accurate evaluation of the treatment result on X-ray images is a significant and challenging step in root canal therapy since the incorrect interpretation of the therapy results will hamper timely follow-up which is crucial to the patients' treatment outcome. Nowadays, the evaluation is performed in a manual manner, which is time-consuming, subjective, and error-prone. In this paper, we aim to automate this process by leveraging the advances in computer vision and artificial intelligence, to provide an objective and accurate method for root canal therapy result assessment. A novel anatomy-guided multi-branch Transformer (AGMB-Transformer) network is proposed, which first extracts a set of anatomy features and then uses them to guide a multi-branch Transformer network for evaluation. Specifically, we design a polynomial curve fitting segmentation strategy with the help of landmark detection to extract the anatomy features. Moreover, a branch fusion module and a multi-branch structure including our progressive Transformer and Group Multi-Head Self-Attention (GMHSA) are designed to focus on both global and local features for an accurate diagnosis. To facilitate the research, we have collected a large-scale root canal therapy evaluation dataset with 245 root canal therapy X-ray images, and the experiment results show that our AGMB-Transformer can improve the diagnosis accuracy from 57.96% to 90.20% compared with the baseline network. The proposed AGMB-Transformer can achieve a highly accurate evaluation of root canal therapy. To our best knowledge, our work is the first to perform automatic root canal therapy evaluation and has important clinical value to reduce the workload of endodontists.",
  "full_text": "1\nAGMB-Transformer: Anatomy-Guided\nMulti-Branch Transformer Network for Automated\nEvaluation of Root Canal Therapy\nYunxiang Li*, Guodong Zeng*, Yifan Zhang, Jun Wang, Qun Jin, Senior Member, IEEE, Lingling Sun, Qianni\nZhang, Qisi Lian, Guiping Qian, Neng Xia, Ruizi Peng, Kai Tang, Shuai Wang †, Yaqi Wang†\nAbstract—Accurate evaluation of the treatment result on X-\nray images is a signiﬁcant and challenging step in root canal\ntherapy since the incorrect interpretation of the therapy results\nwill hamper timely follow-up which is crucial to the patients’\ntreatment outcome. Nowadays, the evaluation is performed in\na manual manner, which is time-consuming, subjective, and\nerror-prone. In this paper, we aim to automate this process\nby leveraging the advances in computer vision and artiﬁcial\nintelligence, to provide an objective and accurate method for root\ncanal therapy result assessment. A novel anatomy-guided multi-\nbranch Transformer (AGMB-Transformer) network is proposed,\nwhich ﬁrst extracts a set of anatomy features and then uses them\nto guide a multi-branch Transformer network for evaluation.\nSpeciﬁcally, we design a polynomial curve ﬁtting segmentation\nstrategy with the help of landmark detection to extract the\nanatomy features. Moreover, a branch fusion module and a\nmulti-branch structure including our progressive Transformer\nand Group Multi-Head Self-Attention (GMHSA) are designed to\nfocus on both global and local features for an accurate diagnosis.\nTo facilitate the research, we have collected a large-scale root\ncanal therapy evaluation dataset with 245 root canal therapy\nX-ray images, and the experiment results show that our AGMB-\nTransformer can improve the diagnosis accuracy from 57.96%\nto 90.20% compared with the baseline network. The proposed\nAGMB-Transformer can achieve a highly accurate evaluation\nThis research is supported in part by the National Natural Science Foun-\ndation of China (No. 61827806), National Key Research and Development\nProgram of China (No. 2019YFC0118404) and Public Projects of Zhejiang\nProvince (No. LGG20F020001).\n* Co-ﬁrst author.\n†Corresponding author.\nYunxiang Li, Lingling Sun, Neng Xia, Ruizi Peng, Kai Tang are with\nMicroelectronics CAD Center, Hangzhou Dianzi University, Hangzhou, China\n(e-mail:li1124325213@hdu.edu.cn, sunll@hdu.edu.cn, amoreyo@hdu.edu.cn,\nprince75@hdu.edu.cn, 19061129@hdu.edu.cn)\nGuodong Zeng is with sitem Center for Translational Medicine and\nBiomedical Entrepreneurship, University of Bern, Bern, Switzerland (e-mail:\nguodong.zeng@sitem.unibe.ch).\nYifan Zhang, Qisi Lian are with National Clinical Research Center for Oral\nDiseases, West China Hospital of Stomatology, Sichuan University, Chengdu,\nChina (e-mail: zhangyifan@hzyk.com.cn, qisiscu@163.com).\nJun Wang is with School of Biomedical Engineering, Shanghai Jiao Tong\nUniversity, Shanghai, China (e-mail: wjcy19870122@sjtu.edu.cn).\nQun Jin is with the Department of Human Informatics and Cognitive\nSciences, Faculty of Human Sciences, Waseda University, Tokyo, Japan (e-\nmail: jin@waseda.jp).\nQianni Zhang is with School of Electronic Engineering and Com-\nputer Science, Queen Mary University of London, London, UK (e-mail:\nqianni.zhang@qmul.ac.uk).\nGuiping Qian, Yaqi Wang is with the College of Media Engineering,\nCommunication University of Zhejiang, Hangzhou, China (e-mail: qianguip-\ning@163.com, wangyaqi@cuz.edu.cn).\nShuai Wang is with School of Mechanical, Electrical and Informa-\ntion Engineering, Shandong University, Weihai, China (e-mail: shuai-\nwang@sdu.edu.cn).\nCorrect-filling\n Under-filling \nOver-filling\nStep I Step II Step III Step IV \nTreatment \nEvaluation\nOral X-ray \nMachine\nDental Drill\nCleaning File\nGutta-percha\nRoot\nRoot CanalPulp\nRoot\nOther\nTissues\nTop of\nGutta-percha\n(a)\n(a)\n(b) (c)\n(b) (c)\nFig. 1. The ﬁrst part shows the conventional steps of root canal therapy. The\nsecond part gives examples of three types of root canal therapy results, and the\nﬁrst row shows original images and the second row shows the corresponding\nimages overlaid with red, blue, and cyan to represent some anatomy features.\nof root canal therapy. To our best knowledge, our work is the\nﬁrst to perform automatic root canal therapy evaluation and has\nimportant clinical value to reduce the workload of endodontists.\nIndex Terms—Root Canal Therapy, Progressive Transformer,\nClassiﬁcation, Segmentation, X-ray Image\nI. I NTRODUCTION\nSevere periodontitis is the sixth-most prevalent health con-\ndition, affecting approximately 10% of people worldwide [1].\nRoot canal therapy is a common method in periodontal disease\ntreatment, but it is easily prone to errors [2]–[5], which leads\narXiv:2105.00381v2  [cs.CV]  28 Oct 2021\n2\nto a signiﬁcant negative impact on patient outcomes [6]–[9].\nMore precisely, under-ﬁlling will lead to the bacterial residue,\nwhich can form acute or chronic inﬂammation, and patients\nwith over-ﬁlling may experience complications such as pain\nor tissue necrosis, even more severe, resulting in neurologic\ncomplications such as hyperaesthesia or dysaesthesia [10].\nTherefore, correct evaluation in post-therapy is very signiﬁcant\nsince under-ﬁlling and over-ﬁlling need to be retreated and\nremedied in time. Nowadays, the evaluation of a root canal\ntherapy result is relied on the personal empirical assessment\nof endodontists, to decide it as a case of under-ﬁlling, over-\nﬁlling, or correct-ﬁlling [11]. Unfortunately, there are many\nbottlenecks in this manual process. Firstly, the manual evalua-\ntion of multiple X-ray images in each case is time-consuming\nand tedious work. Secondly, the manual evaluation results\nare easily prone to errors due to the requirement of high-\nlevel expertise and are highly subjective due to unavoidable\ninter-observer variability. Therefore, an automatic and accurate\nevaluation method for the root canal therapy results is highly\ndesired to improve the diagnosis accuracy and efﬁciency, and\nat the same time, reduce the cost associated with human effort\nand ﬁnance.\nHowever, the evaluation of root canal therapy is a chal-\nlenging task to computers due to many intrinsic complexities:\n1) the apices area boundaries of tooth roots, which are the\nmost important anatomy cues for the evaluation, are difﬁcult\nto extract using conventional edge detection methods since\nthe boundaries of the teeth in X-ray images are unclear\nand irregular. An example is shown in Fig.1 (a); 2) the\nimaging quality of some X-ray images is poor (overexposed or\nunderexposed) that even most experienced endodontists ﬁnd\nit hard to make an accurate visual assessment, as shown in\nFig.1 (b); and 3) the tooth root may be obscured by other\nbones and tissues of the head and cannot depict adequate visual\ninformation for the evaluation, as shown in Fig.1 (c).\nTo address these challenges, we propose an anatomy-guided\nmulti-branch Transformer network that jointly models the\nglobal and local visual cues with the assistance of anatomy\nfeatures to make an accurate evaluation of root canal therapy.\nThe detailed architecture of the proposed network framework\nis illustrated in Fig. 2. Speciﬁcally, in the anatomy feature\nextractor module, the apical boundary of the tooth root and\nthe top of the gutta-percha in the root canal are encoded\nas anatomy features and are extracted based on a landmark\ndetection strategy and a polynomial curve ﬁtting approach.\nThen, the anatomy features, together with the original images,\nare fed as the input to the proposed multi-branch Trans-\nformer network, which considers both global and local features\nthrough two branches - group convolution and progressive\nTransformer. To avoid the curve blocking the information\nfrom the original image, the curve covers the original image\nwith the width of one pixel. Finally, a branch fusion module\nis developed to effectively fuse the features from the two\nbranches and make a joint prediction.\nThe main contributions of this paper are listed as follows:\n• To our best knowledge, this is the ﬁrst research that\nrealizes an automatic and accurate method for root canal\ntherapy evaluation, which can potentially bring signiﬁcant\nbeneﬁts for root canal therapy in clinical practice.\n• To effectively extract the anatomy cues, we propose\na novel anatomy feature extractor that is suitable for\ndetecting the fuzzy boundaries in oral X-ray images.\nIt achieves an accurate tooth apices area segmentation\nthrough a polynomial curve ﬁtted to detected landmarks.\n• We design a multi-branch Transformer network, combin-\ning the advantages of group convolution and progressive\nTransformer, where our progressive Transformer achieves\nmulti-scale self-attention and reduces the amount of com-\nputation through Group Multi-Head Self-Attention. Our\nnovel network structure solves the intrinsic locality issue\nof convolution and accomplishes multi-branch structure\nfeatures fusion.\nII. R ELATED WORK\nIn this paper, we propose a deep learning-based method for\nthe evaluation of the root canal therapy results. Our method\nentails two main components: an anatomy feature extractor\nthat relies on a segmentation approach, and a root canal\ntherapy evaluation method following a classiﬁcation approach.\nThis research mainly involves the two ﬁelds: medical image\nclassiﬁcation and segmentation [12]–[16]. Thus, the most\nrelated works in these two ﬁelds are reviewed in this section.\nA. Medical Image Segmentation\nIn many biomedical image modalities, the important region\nboundaries are fuzzy despite the best possible image quality,\nincluding the root boundaries in oral X-ray images. It is a\nchallenging task to segment the apical area boundaries of\ntooth roots accurately, while the apical boundaries are the\nkey anatomy features for root canal therapy evaluation. To\nsolve the problem of tooth root segmentation, Zhao et al.\n[17] proposed a two-stage attention segmentation network for\nthe tooth segmentation task, following a similar approach to\nAttention U-Net [18]. Lee et al. [19] proposed a deep-learning\nmethod using a ﬁne-tuned mask R-CNN [20] algorithm. Koch\net al. [21] improved U-Net by using patches as inputs replacing\nfull images. However, these methods do not solve the seg-\nmentation problem of fuzzy boundaries, and the performance\nimprovement is mostly incremental. Cheng et al. [22] proposed\nU-Net+DFM to learn a direction ﬁeld. It characterizes the\ndirectional relationship between pixels and implicitly restricts\nthe shape of the segmentation result. Their method can obtain\na signiﬁcant improvement on the fuzzy boundary segmentation\ntask, but it is still limited by the accuracy of U-Net [23].\nMoreover, an efﬁcient anatomy feature for the evaluation of\nroot canal therapy is the apical area boundary of the tooth, but\ntraditional dental segmentation methods all aim to segment the\nwhole tooth, occupying unnecessary computational resources\nin the less relevant parts of the tooth. Many current methods\nare based on the variations of U-Net, and their improvements\ncompared to U-Net are incremental. However, the boundary of\nthe apical region is extremely vague, and there is no model that\ncan accurately segment it to meet the precision requirements\nof root canal therapy evaluation in the existing segmentation\nmethods.\n3\nAnatomy Feature Extractor\nApical \nBoundary\nLandmarks\nFitting\nResults\nGutta-percha \nLandmark\nMulti-Branch Transformer Network\nGlobal Feature \nBranch\nOriginal Image\nAnatomy\n  Feature\nHRNet\nAnatomy Feature + Image\nUnder-filling\nCorrect-filling\nOver-filling\nPost-\nprocessing\nLocal Feature\nBranch\nBranch \nFusion\n Module\nCurve\n Fitting\nFC\nFirst 4 stages\nof ResNeXt\nEvaluation Result\nFig. 2. An illustration of the proposed AGMB-Transformer. First, anatomy features are generated by ﬁtting segmentation and landmark detection. Then,\nanatomy features are combined with the X-ray images as the input for the AGMB-Transformer. In order to better visualize the process, we mark the segmentation\nresults with red lines and the landmarks with blue points.\nB. Medical Image Classiﬁcation\nCorrect classiﬁcation of root canal therapy results is the ul-\ntimate goal of this research. Similarly, in many medical image\nanalysis tasks, classiﬁcation is an important solution to disease\ndiagnosis or grading. A considerable amount of effort has been\ndedicated to the automatic classiﬁcation of medical images.\nTraditional classiﬁers mostly rely on handcrafted features such\nas Random Forest (RF) [24], Support Vector Machine (SVM)\n[25], and Multi-Layer Perceptron (MLP) [26]. However, it\nis recognized that hand-crafted features are not descriptive\nenough for representing the comprehensive structural diversity\nin most medical images like X-ray, and more effective methods\nneed to be explored.\n1) Convolutional Neural Networks: Recent advances in\ndeep learning and computer vision have indicated neural\nnetworks are more suitable for automatic feature learning\n[27]. In particular, Convolutional Neural Networks (CNNs)\nhave been widely applied to achieve state-of-the-art results in\nvarious computer vision and biomedical image analyses. The\nmass application of CNNs [28]–[30] reveals their advantage\nin solving image classiﬁcation problems and illuminates a\npromising direction for medical evaluation tasks by using\nCNNs models to explore inconspicuous local features. The\nmost widely used CNN is ResNet proposed by He et al\n[31]. Based on it, Xie et al. developed ResNeXt [32], and\nHu et al. designed SENet [33]. ResNeXt and Xception [34]\nintroduces group convolution and SEResNeXt is a commonly\nused ﬁne-grained classiﬁcation baseline. Besides, Huang et al.\n[35] proposed DenseNet, which adopts dense connections to\navoid the vanishing gradient problem. Unfortunately, the great\nsuccess of CNNs relies heavily on the amount of training data\nand on obvious features for classiﬁcation. Learning high-level\nsemantic information for ﬁne-grained image classiﬁcation re-\nmains a challenging task for classiﬁcation networks. Moreover,\nCNNs suffer from an intrinsic limitation that they pay more\nattention to local information, and thus are not as capable of\nprocessing global features.\n2) Transformers: To address the limitations of CNNs,\na new architecture is necessary to extract global features.\nTransformer-based architectures employ position encoding,\nwhich has recently been proven to be more suitable for many\nvision applications [36]. It was ﬁrst proposed by Vaswani et\nal. [37] for machine translation and has become the state-of-\nthe-art method in most Natural Language Processing (NLP)\ntasks, where multi-head self-attention (MHSA) is employed as\nthe base structure. In addition, in the process of self-attention\nexploration, much progress is being made by researchers.\nWang et al. [38] presented a novel type of neural networks\nthat capture long-range dependencies via non-local operations.\nWithin this general framework, Yue et al. [39] designed a\nbetter instantiation named global context network (GCNet),\nwhich is lightweight and able to effectively model the global\nfeatures. Although these network structures can achieve long-\nrange dependencies, Transformer gives a modeling pipeline\nthat does not rely on CNNs to achieve that. Due to the\ndifference between NLP and computer vision, it is difﬁcult to\napply transformers directly to image processing tasks. In order\nto design a transformer structure suitable for image processing,\nseveral approaches have been proposed in recent years. Parmar\net al. [40] applied self-attention for each query pixel only in\nlocal neighborhoods rather than globally. Nicolas et al. [41]\npresented DETR, a new design for object recognition based on\nTransformer that signiﬁcantly improved the performance over\nFaster R-CNN for large objects. Alexey et al. [42] explored the\ndirect application of Transformer to image recognition. This\napplication is called ViT and interprets images as a series of\npatches and then processes them using an off-the-shelf Trans-\nformer encoder. Due to the lack of translation equivariance\nand locality, Transformer does not perform well on insufﬁcient\ndatasets. The amount of training data required for ViT is\n14-300 million images. The combination of Transformer and\nconvolution is considered an effective scheme to reduce the\nscale of training data, and thus Aravind et al. [43] presented\nBoTNet, a simple but powerful instance segmentation and\nobject detection backbone, where a Bottleneck Transformer\nis used as a base structure by simply replacing a part of\nconvolutions with MHSA in bottleneck blocks.\n4\nIII. M ETHOD\nThe proposed anatomy-guided multi-branch Transformer\nnetwork comprises two stages. The ﬁrst stage is anatomy\nfeature extraction that works by extracting the apical boundary\nof the tooth root and the landmark of the gutta-percha in\nthe root canal. In the second stage, the extracted anatomy\nfeatures together with the X-ray images are fed into a multi-\nbranch Transformer network for deciding the ﬁnal evaluation\noutcome.\nU-Net Segmentation Fitting Segmentation\nGround \nTruth U-Net Curve Fitting Landmarks\nFig. 3. The comparison of traditional segmentation network U-Net and our\nproposed ﬁtting segmentation method in the segmentation of extremely fuzzy\nboundaries.\nA. Anatomy Feature Extractor\nIn medical imaging, remarkable progress has been made\nin high-performance classiﬁcation models based on CNNs.\nDespite the performance peaks, new advanced neural networks\nstill require large, representative, and high-quality annotated\ndatasets. Unfortunately, both data and annotations are expen-\nsive to obtain [44], [45]. One method that can reduce the\ndependency on large-scale annotated data for training without\nsacriﬁcing classiﬁcation performance is to leverage anatomy\nfeatures as prior knowledge to facilitate the classiﬁcation\nmodel [46]. The root canal therapy evaluation is a very\nsubjective process, which requires comprehensive consider-\nation of cues such as the shape and location of the tooth\napical boundary, the position of gutta-percha ﬁlling in the\nroot canal, the X-ray projection angle, etc, with a particular\nemphasis on the relative position of the apical boundary and\nthe top of the gutta-percha. Consequently, the performance\nof the classiﬁcation network could be improved if the apical\nboundary of the tooth root and the top of the ﬁlled gutta-percha\nwere to be considered as anatomy features. Our anatomy\nfeature extractor is to detect the landmarks, and then the top of\ngutta-percha landmark and the curve ﬁtted by the landmarks\nof tooth apical boundary are used as anatomy features.\n1) Landmark Detection: The landmarks uniformly dis-\ntributed on the apical region of the tooth and the top of\nthe gutta-percha landmark are ﬁrst detected by HRNet [47],\n[48], in which the high-resolution representation is maintained\nby combining parallel different-resolution representations and\nrepeated multiscale fusion. Among the detected landmarks, the\nlandmark of gutta-percha ﬁlled in the root canal is directly\nused as one of the anatomy features, while the landmarks\nof tooth apical boundary are ﬁtting to a polynomial curve as\nthe segmentation result, that is, the other signiﬁcant anatomy\nfeature.\n2) Fitting Segmentation of Tooth Apical Boundary: As the\nmost signiﬁcant anatomy feature, the segmentation of the tooth\napical boundary requires very high accuracy. In the traditional\nsegmentation methods, there are many bottlenecks in the\njudging of the category of each pixel to segment an actual\nboundary. When segmenting with extremely fuzzy bound-\naries, not the category for each pixel all can be accurately\ndetermined. Our ﬁtting segmentation can be summarized as\nderiving the actual segmentation boundary according to prior\nknowledge and determining characteristics.\nOur ﬁtting segmentation method is named High-resolution\nSegmentation based on Polynomial Curve Fitting with Land-\nmark Detection (HS-PCL). In the segmentation of the tooth\napical fuzzy boundary, the landmarks in the tooth apical\nboundary are extracted, and the image is rotated and corrected\naccording to the angle between the root canal and the vertical\ndirection, and we ﬁt the detected landmarks to polynomials of\ndegree δ. We found that the shape characteristics of the apical\nboundary of the tooth root and the quadratic polynomial curve\nshown a high degree of similarity. Taking advantage of this, we\napplied the quadratic polynomial curve ﬁtting of the ordinary\nleast squares method [49] to the boundary segmentation to\nbring the segmentation result to be as close as possible to the\nactual boundary. The quadratic polynomial curve is deﬁned in\nEq. (1).\ny= ax2 + bx+ c (a̸= 0) (1)\nThe matrix equation for the quadratic curve is given in Eq.\n(2). According to this, the values of parameters a, b, and c\ncan be calculated.\n\n\n∑x4\ni\n∑x3\ni\n∑x2\ni∑x3\ni\n∑x2\ni\n∑xi∑x2\ni\n∑xi n\n\n\n\n\na\nb\nc\n\n =\n\n\n∑x2\niyi∑xiyi∑yi\n\n (2)\nwhere ( xi,yi) is the coordinate position of the landmark.\nBesides, i and n represent the serial number and the total\nnumber of the landmarks, respectively.\nAlthough the landmarks detected by HRNet are not dis-\ntributed on the boundary with complete accuracy, they can\nmaintain a uniform distribution around the boundary. There-\nfore, as a segmentation result, the ﬁtting curve can achieve\nrelatively accurate results in the case of uncertain and fuzzy\nroot boundaries. This is shown in Fig. 3. Our method not\nonly increases the accuracy of the segmentation but also\nsigniﬁcantly improves the efﬁciency of the calculation.\nB. Multi-Branch Transformer Network for Automatic Evalua-\ntion\nWhile the convolution operation can effectively capture\nlocal information, this task may also require global features,\nsuch as the shape of the tooth root. In order to globally\n5\nInput\nFirst 4 Stages \nof  ResNeXt\n1×1 \nConv\n1×1 \nConv\nProgressive Transformer\n1×1 \nConv\n1×1 \nConv\nGMHSA 1×1 \nConv\n1×1 \nConv\nGMHSA\n1×1 \nConv\n3×3 \nConv\nStride=2\n1×1 \nConv\n1×1 \nConv\n3×3 \nConv\n1×1 \nConv\n1×1 \nConv\n3×3 \nConv\n1×1 \nConv\n1024-d\n32-d\n32-d\n...\n...\ntotal \n32 paths\n. . . Group Convolution\nGlobal \nPooling\n1×1 \nConv ReLU\n1×1 \nConv Sigmoid\nFilter \nBranch Fusion Module\nFC\n. . .\n. . .\n. . .\nLocal Feature Branch\nGlobal Feature Branch\n64-d\n....\ntotal \n64 paths\n1×1 \nConvStages C1-C4\nStage C5\n2×2 \nAvgPool\nStride=2\nGrouping \nModule\nMerge \nModule\nGrouping \nModule\nGrouping \nModule\nMerge \nModule\nMerge \nModule\nGMHSA\n1 × 1\nMatrix \nMultiplication\nElement\nWise Sum\nPointwise\nConvolution\n\u0000\u0000\n\u0000\u0000\n\u0000 \u0000\nSelf-Attention \nLayer\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000: 1 × 1\n\u0000\u0000: 1 × 1\n\u0000\u0000: 1 × 1\n\u0000 × \u0000 × \u0000\n\u0000 × \u0000 × \u0000\n\u0000 × \u0000 × \u0000\n\u0000 × 1 × \u0000\n1 × \u0000 × \u0000\n\u0000 × \u0000 × \u0000\ncontent-\nposition\ncontent-\ncontent\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0000 ∗ \u0000 × \u0000 ∗ \u0000\n\u0000 ∗ \u0000 × \u0000 ∗ \u0000\n\u0000 × \u0000 × \u0000\n\u0000\n\u0000\n\u0000\n\u0000\nGMHSA\nUnit\nFig. 4. The architecture of our multi-branch Transformer network. The ReLU follows all convolution layers and the skip connection is used between the base\nblock to avoid gradient vanishing. The upper branch is the local feature branch, which is processed by group convolution, and the lower one is the global\nfeature branch, which extracted the global features through progressive Transformer. At the end of the network structure, it is our feature fusion module.\nGroup Multi-Head Self-Attention is embedded in our progressive Transformer. Self-Attention layer is the base structure of GMHSA where our GMHSA has\n4 heads, and we do not illustrate them on the ﬁgure for simplicity.\naggregate the locally acquired features, it is imperative to\nachieve this through Transformer-based architectures. There-\nfore, an explicit mechanism for modeling global dependencies\ncould be a more powerful and scalable solution than stacking\nmultiple layers. To combine the advantages of convolution and\nTransformer, we propose a multi-branch Transformer network,\nwhere one is the local feature branch and the other is the global\nfeature branch. An overview of our multi-branch Transformer\nnetwork is shown in Fig. 4.\n1) Overall Network Structure: First of all, we will give a\nbrief introduction to our network structure. As shown in Fig.\n4, the stages C1-C4 of our network adopt the ﬁrst four stages\nstructure of ResNeXt. In stage C5, our network is divided\ninto a local feature branch and a global feature branch. In the\nlocal feature branch, we employ group convolution as the base\nstructure. In the global feature branch, we deploy it based on\nour progressive Transformer. The ﬁnal classiﬁcation result can\nbe obtained through the branch fusion module and the fully\nconnected layer. In this part, we introduced our network as a\nwhole, and in the following paragraphs, we will introduce our\nnetwork structure in modules.\n2) Local Feature Branch: Due to the existence of the multi-\nbranch structure, the local feature branch only needs to focus\non better obtaining the local features. As is known to all,\nconvolution extracts the features of different location areas,\nand the upper layer convolution combines the features of the\nlower layer convolution and extracts the deeper information.\nThe locality is one of the most important characteristics of\nconvolution. However, ordinary convolution can only realize\nthe locality in space. The proposition of group convolution\nachieved locality between channels, which further develops\nthe local convolution. In order to obtain local features better,\nwe take the group convolution as the base structure of the local\nfeature branch.\n3) Global Feature Branch: In the global feature branch,\nthe ultimate goal is to extract global features such as root\nshape and X-ray projection angle, etc. To our disappoint-\nment, the locality is an inherent characteristic of CNNs,\nand thus it is necessary to design a new architecture for\nglobal feature extraction. Transformer-based architectures use\n6\nposition coding, which can consider the information in differ-\nent locations simultaneously. Therefore, they can effectively\nextract global features. Nevertheless, Transformer lacks some\nof the inductive biases inherent in CNNs, such as locality and\ntranslational equivariance, so a hybrid architecture is a suit-\nable solution where CNNs are constructed to extract feature\nrepresentations and Transformer is constructed to model the\nremote dependency of the extracted feature maps. Speciﬁcally,\nour progressive Transformer is composed of skip connection,\nconvolutions, average-pooling, and Group Multi-Head Self-\nAttention.\nWhen a picture is being observed, the same attention cannot\nbe paid everywhere. Hence, different features may be given\ndifferent importance. A widely applied attention function is\nscaled dot-product attention, which is calculated in Eq. (3):\nAttention(Q,K,V ) = softmax(QKT/\n√\ndk)V (3)\nwhere dk refers to the dimension of the key. It is known as self-\nattention when Q(queries), K(keys), and V(values) are equal.\nMulti-Head Self-Attention (MHSA) [37] as the base structure\nof Transformer is a type of widely used attention mechanism\nwhich is more focused on the internal structure. Transformer\nwas ﬁrst proposed for NLP tasks, and so it is difﬁcult to apply\nTransformer directly to medical image tasks on account of the\ndiscrepancy between natural language and image. The number\nof words in natural language is limited, but the number of\npixels increases quadratic with the increase of image size.\nBecause of this, we designed Group Multi-Head Self-Attention\nin our progressive Transformer to solve the problem of too\nmuch computation due to the image characteristics in the\nmedical image. We deploy our progressive Transformer in\nstage C5 of the network, where our progressive Transformer\nachieves multi-scale self-attention and reduces the amount\nof computation. The detailed architecture of our GMHSA is\nillustrated in Fig. 4, where the position encoding method is\nrelative-distance-aware position encoding [50]–[52] with Rh\nand Rw for height and width. The attention logit is qkT + qrT,\nand q,k,v,r denote query, key, value, and position encodings,\nrespectively. Besides, X represents the feature block obtained\nafter grouping module, and Z represents the output of self\nattention layer. The grouping module divides the original\nfeature blocks of C×H×W into small blocks of N×C×h×w,\nand performs self-attention operation on the N small feature\nblocks respectively. The merging module merges and restores\nthe feature blocks after grouping operation to the original\nfeature block arrangement.\nAssuming that the original feature block size is H×W×C,\nand the size of each GMHSA unit is set to h ×w, the\ncomputation will be greatly reduced through the grouping\nstructure and bottleneck structure. The calculation amount of\nMHSA before improvement [53] and that of our GMHSA are\ngiven in Eq. (4) and (5), respectively:\nΩ(MHSA) = 4HWC2 + 2(HW)2C (4)\nΩ(GMHSA) = 4hw(C\nϕ)2 + 2(hw)2 C\nϕ\n= 4hw\nϕ2HW HWC2 + 2h2w2\nϕH2W2 (HW)2C\n(5)\nwhere ϕ is the channel scaling factor of the bottleneck\nstructure, and the size of each GMHSA unit is determined\nby the input image size. Since GMHSA has not the ability\nto downsampling, average-pooling with the stride of 2 is\nimplemented for the spatial downsampling following the ﬁrst\nGMHSA block. While the feature block size is reduced, the\nsize of GMHSA is gradually increased to achieve multi-scale\nself-attention and eventually global self-attention.\nBranch Fusion\n Module\n4096×h×w\nGlobal \nPooling\n4096×1×1\n1×1 \nConv\n512×1×1\nReLU\n1×1 \nConv\n64×1×1\nSigmoid\n64×h×w\n64×h×w\nFilter \ntotal \n64 paths\n....\ntotal \n32 paths\n1×1 \nConv\n2048×h×w\n...\n...\n...\n...\nFig. 5. The proposed branch fusion module for two branch feature fusion.\nThe basic structure is to model the channel block interdependencies, remove\nhalf of channel block features with lower scores through the channel ﬁlter,\nand fuse the remaining channel features as the output.\n4) Branch Fusion Module: The multi-branch structure\nis worth exploring, but it is equally important to better\nfuse different branch features. The two branches of AGMB-\nTransformer ﬁnally output 2048-channels feature blocks, re-\nspectively, and the total number of feature block channels of\nthe two branches is 4096. Too many channels may lead to\noverﬁtting and extract more noise features. In order to better\nfuse the feature blocks of the two branches, we design our\nbranch fusion module following the two branches, improving\nthe quality of branch fusion by explicitly modeling the inter-\ndependencies between the channels of two branches’ features\nand ﬁlter out half of the channel features for reconnection.\nTherefore, our branch fusion module can automatically em-\nphasize more informative branch features and suppress less\nvaluable ones. A diagram illustrating the structure of branch\nfusion module is shown in Fig. 5.\nOur branch fusion module explicitly models channel inter-\ndependencies of two branchs’ features to ﬁlter the useless\nfeatures so that the network has the ability to enhance its\nsensitivity to channel features which can be exploited by the\nchannel ﬁlter. In order to generate channel-wise statistics,\nwe squeeze the channel features into a channel descriptor U\nthrough global average pooling. Then, two 1×1 convolutions,\na ReLU activation, and a sigmoid activation are adopted to\nobtain the channel block score information for the channel\n7\nﬁlter. The score S of each channel block is obtained in Eq.\n(6):\nS = σ(W2γ(W1U)) (6)\nwhere σ denotes sigmoid activation and γ is ReLU activa-\ntion. W1 and W2 belong to the ﬁrst convolution C4096→512\nand second convolution C512→64, respectively.\nAfter the connection of the two branch features, the 4096\nchannel features are divided into θ blocks, and there are\n4096/θ channels for each block. According to the channel\nblock scores calculated previously, we ﬁlter out the θ/2 blocks\nwith lower scores and reconnect the θ/2 ones with higher\nscores into a feature block with 2048 channels, and then they\nare fused by a 1×1 convolution as the ﬁnal feature. Moreover,\nθ is 64 in this paper.\nIV. E XPERIMENTS AND DISCUSSIONS\nA. Datasets and Experimental Settings\nTo evaluate segmentation performance and classiﬁcation\nability, the proposed model was trained and tested on a multi-\nmodality dataset, which includes three kinds of modality\nannotation: classiﬁcation, segmentation, and landmarks. In\nmarking the landmarks of the datasets, we marked a total of\n19 landmarks from left to right at the tooth root boundary\nand one landmark of the top of the ﬁlled gutta-percha in\nthe root canal. The ﬁnal dataset consisted of 245 images\nfrom different patients, including 90 under-ﬁlling images, 96\ncorrect-ﬁlling images and 59 over-ﬁlling images. These images\nwere collected from the National Clinical Research Center for\nOral Diseases, West China Hospital of Stomatology. All the\ndata described in this paper are acquired the ethics approvals\n(No. 2020484) from Sichuan Academy of Medical Sciences. A\nstomatologist experienced in oral radiography and root canal\ntherapy classiﬁed the images of the tooth to be treated and\nlabeled both the segmentation and the landmarks. Besides, the\nlabeling results were reviewed by two stomatologists to ensure\nthe accuracy of the labeling. When testing the performance\nboth of the segmentation and the segmentation, we adopted\n3-fold cross-validation.\nFor implementation, the models were based on the open-\nsource deep learning framework Pytorch [54]. For the com-\npared models, we directly followed the default settings and\nﬁne-tuned the baseline model by our best using the ofﬁcial\ncode of the original papers. For optimization technique, we\nuse cosine annealing, which helps the neural network model\nto converge much faster than using a static learning rate,\nand it has proved to beats other scheduling techniques in\nmany cases. We use the early stop mechanism in training,\nwhich is a widely used method to avoid over ﬁtting, and\nwe set it to trigger early stop when 70 consecutive epoch\nresults are not improved. For each training, 300 training\nepochs were deployed, with 0.0005 weight decay, a Adam\n[55] with (0.9, 0.999) betas, 32 cases per minibatch, 0.01\nlearning rate at the beginning, and the channel scaling factor\nϕ is set to 4. Images were resized to 128 ×128 for input.\nOur data augmentation was based on the fast and ﬂexible\nimage library Albumentations [56]. During the training of all\nthe models, contrast limited adaptive histogram equalization\n(CLAHE) [57] ( clip limit = 4 ,p = 0 .5) was used to\nrestrain noise signals and enhance the contrast between tissues.\nData augmentations including random brightness ( limit =\n0.1,p = 1 ), random contrast ( limit = 0 .1,p = 1 ), motion\nblur ( blur limit= 3 ,p = 0 .5), median blur ( blur limit=\n3,p = 0 .5), gaussian blur ( blur limit = 3 ,p = 0 .5),\nvertical ﬂip ( p = 0.5) and shift scale rotate ( shift limit=\n0.2,scale limit= 0.2,rotate limit= 20,p = 1) were also\nused to improve robustness and limit the impact of overﬁtting,\nwhere p is the probability value. Core code is available at\nhttps://github.com/Kent0n-Li/AGMB-Transformer.\nB. Evaluation Metrics\nWe used several metrics for the evaluation of our experi-\nments. Accuracy (ACC), Area Under Curve (AUC), Sensitivity\n(SEN), Speciﬁcity (SPC), and F1 score were employed.\n• Accuracy (ACC):\nAccuracy = TP + TN\nTP + TN + FP + FN (7)\n• Sensitivity (SEN):\nSensitivity = TP\nTP + FN (8)\n• Speciﬁcity (SPC):\nSpecificity = TN\nTN + FP (9)\n• F1 score:\nF1 = 2TP\n2TP + FP + FN (10)\n• Area Under Curve (AUC): AUC of the receiver operating\ncharacter (ROC) is used to compare model functionality.\nwhere TP, TN, FP, and FN stand for the number of true\npositive, true negative, false positive, and false negative pre-\ndictions. Mean values between classes were calculated to rep-\nresent the ﬁnal performance score for multi-class datasets of\neach model. Speciﬁcally, the evaluation of N-classiﬁcation was\ndivided into N two-classiﬁcation evaluations, and the metrics\nof each two classiﬁcations were calculated. The average of N\nmetrics is the ﬁnal result.\nWe choose the following commonly used metrics in the seg-\nmentation of medical images as evaluation criteria to evaluate\nthe segmentation performance of different methods:\nAverage Symmetric Surface Distance (ASD):\n(∑infp∈Sseg d(p,Sgt) + ∑infp′∈Sgt d(p\n′\n,Sseg))\n∥Sgt ∥+ ∥Sseg ∥\n(11)\n95th-percentile Bidirectional Hausdorff Distance (HD95):\nThe Hausdorff distance measures the surface distance between\nthe segmented and ground truth objects. dH(Sseg,Sgt) is\ndeﬁned in Eq. (12):\nmax{supainfbd(a,b),supbinfad(b,a)},a ∈Sseg,b ∈Sgt\n(12)\nIn this paper, we use the 95th-percentile bidirectional Haus-\ndorff distance as a metric for the measure. Where Sgt and\nSseg are the surface voxel sets of manually labeled ground\n8\nTABLE I\nCOMPARISON OF MODELS\nModel ACC(%) AUC(%) SEN(%) SPC(%) F1(%)\nResNet50 57.96±1.12 69.76 ±1.17 53.36 ±4.32 77.52 ±1.33 53.33 ±5.17\nResNeXt50 56.73±1.83 67.70 ±4.07 51.80 ±4.47 76.67 ±1.29 50.98 ±6.07\nGCNet50 55.93±2.19 63.49 ±5.01 50.95 ±5.87 76.21 ±2.31 50.06 ±7.03\nBoTNet50 56.33±3.68 66.03 ±7.50 51.32 ±6.52 76.57 ±2.69 50.29 ±7.88\nAGMB-Transformer 90.20±1.29 95.63 ±1.09 91.39 ±1.46 95.09 ±0.77 90.48 ±1.13\ntruth and automatically segmented results, respectively, and\nd is the Euclidean Distance, where sup indicates the upper\nbound and inf indicates the lower bound of the Euclidean\ndistance between a and b.\nC. Performance Comparison\nThe classiﬁcation accuracy of different networks is pre-\nsented in Table I. Our AGMB-Transformer ﬁrst automatically\nextracts the anatomy features and then classiﬁes the oral X-\nray images with the assistance of the anatomy features, while\nother networks classify the X-ray images directly. One of\nthe top-performing baseline networks, ResNet50, achieved an\naccuracy of 57.96% on the test dataset, but its performance\nis still far from the goal of the automatic evaluation of root\ncanal therapy, which also means that this task is strongly\narduous. Traditional classiﬁcation networks are not suitable for\nroot canal therapy classiﬁcation, but our AGMB-Transformer,\nwhich extracts anatomy features as the prior knowledge,\nis an effective method. Considering that the gap between\nvarious types is very small, it can be considered as a ﬁne-\ngrained classiﬁcation task, and the amount of data is very\nsmall. Extracting anatomy features as the guide may be an\nappropriate solution. In addition, AGMB-Transformer pays\nattention to global features and local features at the same time\nthrough the multi-branch structure. This is why our AGMB-\nTransformer achieves 90.20% accuracy and has shown such\nstrong improvement.\nD. Effectiveness of Each Stage\nOur method composition could be concluded into ﬁtting\nsegmentation, anatomy features, multi-branch Transformer\nnetwork, and branch fusion module. Parameters were main-\ntained unchanged as far as possible for condition control in\nall the experiments.\n1) Effectiveness of Fitting Segmentation: The effective po-\nsition of the segmentation result is the apical area of the tooth\nroot. Therefore, we only marked the landmarks of the apical\nboundary of the tooth root as the datasets. We rotated the\npredicted results of baselines as well as the ground truth to the\nsame rotation angle as our model. Additionally, we cropped the\nsame height on the left and right edge to ensure the comparison\nof the predicted results of each method at the same tooth root\nposition.\nTable II makes quantitative comparison of mean ASD and\nHD95 with a standard deviation with our HS-PCL. Further,\nwe performed a one-tail paired t-test between our method and\neach comparison method. All the comparing methods whose\ndifference with our method is statistically signiﬁcant ( p<0.05)\nare marked by the symbol * in Table II. The boxplot of ASD\nand HD95 values of the segmentation with our method and\nbaseline methods is shown in Fig. 6. It can be seen that\nour method signiﬁcantly leads all the baseline methods in\nboth HD95 and ASD metrics. We drew the results predicted\nby the model with a blue solid line and ground truth with\norange dashed line in the visual results as shown in Fig. 7,\nwhich presents a comparison of segmentation results with\nfuzzy boundaries. We can see from this ﬁgure that U-Net\nand its improved versions performed unsatisfactorily for fuzzy\nedge segmentation. To verify the effectiveness of our ﬁtting\nsegmentation, we also compared it with HRNet-OCR, which\nis directly segmented by HRNet. U-Net+DFM, which was\nproposed for fuzzy boundary segmentation, achieves much\nbetter performance than the other networks. Nevertheless, it\nis still outperformed by our method.\nIt is challenging to obtain accurate anatomy features due\nto the fuzzy boundary of the tooth root. Most traditional\ndeep-learning segmentation methods work by predicting the\nsegmentation map at the pixel level - they assess the cate-\ngory of each pixel, but the boundaries are not always well\ndeﬁned, especially for oral radiographs. In recent years, many\nresearchers have striven to improve U-Net to solve the prob-\nlem of fuzzy boundary segmentation and have made some\nprogress. Nevertheless, little effort has been made to get rid of\nthe basic architecture based on pixel-level segmentation maps\nand to perform fuzzy boundary segmentation in a completely\ndifferent way. Sharp image segmentation can be deﬁned as\nthe partitioning of the pixel set into a family of contiguous\nsubsets or regions. In the segmentation method, there are\nmany bottlenecks in judging the category of each pixel. When\nsegmenting with extremely fuzzy boundaries, not all pixels\ncan have their category accurately determined, and sometimes\nit is not clear even to experienced dentists which category\neach pixel should belong to. When dentists segment the fuzzy\nboundary of the tooth root, they often combine it with the\nanatomical context and some determinable features. This can\nbe summarized as deriving the actual segmentation boundary\naccording to prior knowledge, which has proven to be very\neffective for the segmentation of extremely fuzzy boundaries.\n2) Effectiveness of Anatomy Feature: Normally, root shape\nand X-ray projection angle are also necessary for the eval-\nuation of root canal therapy. Models trained by X-ray im-\nages with anatomy features and classifying by threshold by\nmeasuring the distance directly are compared in Table III. In\nthis circumstance, it is not accurate to measure the distance\nbetween the apical area boundary of the tooth root and the top\n9\nTABLE II\nEVALUATION OF SEGMENTATION\nU-Net Attention\nU-Net R2U-Net Attention\nR2U-Net\nU-Net\n+DFM HRNet-OCR HS-PCL\nASD(mm) 0.547±0.079 0.581±0.060 0.598±0.193 0.538±0.088 0.492±0.069 0.498±0.090 0.260±0.119\np-value 3.2E-7* 2.6E-6* 8.0E-3* 1.3E-5* 6.9E-7* 1.6E-10* N/A\nHD95(mm) 1.298±0.062 1.343±0.078 1.178±0.197 1.188±0.112 1.085±0.102 1.095±0.161 0.763±0.296\np-value 1.7E-10* 1.2E-12* 7.0E-6* 8.1E-9* 1.1E-6* 6.6E-7* N/A\nFig. 6. Boxplot of HD95 and ASD values of the tooth apical segmentation with respect to different Methods.\nU-Net\nAttention \nU-Net R2U-Net\nAttention \nR2U-Net\nHS-PCL\n(Ours) U-Net\nAttention \nU-Net R2U-Net\nAttention \nR2U-Net\nHS-PCL\n(Ours)\nI\nII\nComparison with Ground Trueth Model Prediction Results\nU-Net\n+DFM\nHRNet-\nOCR\nU-Net\n+DFM\nHRNet-\nOCR\nIII\nFig. 7. Three examples of the tooth apical area segmentation results of each class. The orange dashed line is the exact position of the tooth apical boundary,\nand the blue solid line is the model prediction result.\nof the ﬁlled gutta-percha.\nIn our implementation, we designed an anatomy-guided\nnetwork to extract the tooth apical boundary and the landmark\nof gutta-percha in the root canal as prior knowledge and then\nguided the classiﬁcation network. To evaluate its importance\nand effectiveness, we deﬁned two more inputs for comparison.\nThe ﬁrst input is the 3-channel X-ray image alone. The\nother has the anatomy feature as the input with only one\nchannel. Table IV reports the classiﬁcation performance based\non three different inputs on ResNet50. From these results, we\ncan see that anatomy features greatly enhance classiﬁcation\nperformance, which demonstrates the guidance capability of\nboundary and landmark information for diagnosis. When com-\npared with putting them together, having the anatomy feature\nas the only input is not very effective, which proves that tooth\nroot shape and X-ray projection angle in the original image\nis also necessary. Additionally, ResNet50 with the anatomy\nfeature and X-ray images as input achieves the best diagnosis\nperformance, revealing that classiﬁcation network, anatomy\nfeatures, and original images are all indispensable.\n10\nTABLE III\nCOMPARISON OF DIFFERENT CLASSIFICATION METHODS THROUGH ANATOMY FEATURES\nClassiﬁcation Method ACC(%) SEN(%) SPC(%) F1(%)\nFixed Threshold 73.33 74.69 86.93 71.19\nClassiﬁcation Network (ResNet50) 85.30±1.32 86.32 ±0.80 92.54 ±0.41 85.74 ±1.89\nTABLE IV\nCOMPARISON OF DIFFERENT INPUTS ON RESNET50\nInput ACC(%) AUC(%) SEN(%) SPC(%) F1(%)\nImage 57.96±1.12 69.76 ±1.17 53.36 ±4.32 77.52 ±1.33 53.33 ±5.17\nAnatomy Feature 85.30±1.32 93.82 ±1.76 86.32 ±0.80 92.54 ±0.41 85.74 ±1.89\nImage+Anatomy Feature 88.16±1.50 94.90 ±0.49 89.27 ±2.62 94.00 ±1.01 88.56 ±1.73\nTABLE V\nEVALUATION OF CLASSIFICATION MODELS WITH ANATOMY FEATURES\nModel ACC(%) AUC(%) SEN(%) SPC(%) F1(%) t-test p-value\nU C O\nResNet50 88.16±1.50 94.90 ±0.49 89.27 ±2.62 94.00 ±1.01 88.56 ±1.73 1.6E-5* 1.4E-14* 1.6E-12*\nResNeXt50 86.94±1.37 95.02 ±0.87 87.67 ±1.14 93.20 ±0.31 87.64 ±1.77 7.9E-4* 1.5E-21* 6.9E-9*\nXception 88.97±2.16 95.04 ±0.69 89.83 ±1.62 94.37 ±1.26 89.41 ±1.56 1.1E-6* 4.1E-6* 8.0E-7*\nDenseNet169 87.75±1.30 94.80 ±0.51 89.13 ±1.50 93.85 ±0.77 88.12 ±1.25 3.0E-1 3.7E-4* 4.1E-6*\nSEResNet50 87.74±2.53 94.38 ±1.20 88.96 ±2.98 93.76 ±1.24 88.13 ±2.77 2.1E-1 1.7E-8* 2.3E-8*\nSEResNeXt50 88.98±1.22 94.65 ±2.20 90.44 ±1.06 94.35 ±0.73 89.56 ±0.98 4.1E-2* 1.9E-1 8.6E-18*\nGCNet50 88.56±1.95 94.65 ±0.64 90.17 ±1.97 94.34 ±0.87 88.91 ±2.09 1.1E-5* 2.4E-3* 8.6E-5*\nBoTNet 89.38±1.93 94.81 ±0.42 89.97 ±2.25 94.62 ±1.11 89.51 ±1.77 2.1E-2* 1.5E-2* 1.9E-17*\nViT 60.44±6.49 72.31 ±4.85 59.67 ±8.10 78.89 ±4.15 59.70 ±7.48 4.0E-2* 3.6E-2* 7.1E-10*\nAGMB-Transformer 90.20±1.29 95.63 ±1.09 91.39 ±1.46 95.09 ±0.77 90.48 ±1.13 N/A N/A N/A\nResNet50 SEResNet50ResNeXt50\nSEResNeXt50\nXception DenseNet169\nBotNet50GCNet50 ViT AGMB-Transformer\nU\nC\nO\nU\nC\nO\nU C O U C O U C O U C O U C O\nFig. 8. Ten charts of confusion matrices demonstrate the distribution of predictions. The color of confusion matrices provides better visualization, and the depth\nof the color depends on the normalized values of predictions. Symbols used in the ﬁgure are denoted as U: Under-ﬁlling, C: Correct-ﬁlling, O: Over-ﬁlling.\n3) Effectiveness of Multi-Branch Transformer Network:\nThe advantages of architecture design have been deeply\nexplored. This has been done by evaluating classic deep\nneural structures (i.e, ResNet50, ResNeXt50, Xception and\nDenseNet169), widely used ﬁne-grained image classiﬁcation\nnetworks (i.e, SEResNet, SEResNeXt), high performance non-\nlocal networks (i.e, GCNet50), and models with Transformer\n(i.e, BoTNet, ViT). To evaluate the importance and effective-\nness of AGMB-Transformer, we did a quantitative comparison\nbetween our method and the baseline networks on the same\ndataset using the mean ACC, AUC, SEN, SPC, and F1 with\nstandard deviations. This is shown in Table V. To evaluate\nthe response level of the model to the proposed improved\nstructures, we perform a paired t-test to test the statistical\nsigniﬁcance of the model’s output. Set t-test be calculated\nas p-value = t-test (N1,N2), where t = x−µ0\ns√n\n, where µ0\n= the test value, x = sample mean, n = sample size, s\n= sample standard deviation, and N1 and N2 are the two\ndistribution to be evaluated. The p-value can be found from\nthe t-Distribution Table or be automatically calculated by\n11\nstatistical program (R, SPSS, etc.). We collected the output\ndistributions of all comparison models and our model accord-\ning to under-ﬁlling, correct-ﬁlling, and over-ﬁlling, respec-\ntively: (U1,U2,...),(C1,C2,...),(O1,O2,...). T-test uses t-\ndistribution theory to infer the probability of difference, which\nis the most common signiﬁcance test. For each category, we\nperformed a signiﬁcance test with t-test to assess the extent of\nthe effect on the improved model by every two models’ output\nresponse, and splitting the statistical analysis for different\ntypes is more helpful to explain that the difference of output\nresults of different networks is signiﬁcant. All comparing\nmethods, which showed a difference from our method that\nwas statistically signiﬁcant ( p<0.05) are marked by symbol\n* in Table V ( U: Under-ﬁlling, C: Correct-ﬁlling, O: Over-\nﬁlling).\nAll the networks were trained and tested with the assistance\nof anatomy features. From the experimental results, we can see\nthat, all the networks have improved a lot with the assistance\nof anatomy features. When compared with other networks,\nour AGMB-Transformer achieves the best results. Due to\nthe structure of multiple branches, our model achieves self-\nattention through the global feature branch and retains the\nadvantage of locality group convolution. This is why our\nmodel achieves a better result than all others. As we know,\nthe amount of training data required for ViT is 14-300 million\nimages, and so we loaded the pre-trained weight obtained from\nthe training on imagenet1k for ViT. However, the performance\nof ViT in this task is still very poor. Compared with other\nmodels, we can observe that our method has at least two\ncategories with a statistically signiﬁcant improvement.\nTo further evaluate the performance of the algorithm, it was\nnecessary to report the distribution of our prediction result,\nwhich is shown by the confusion matrix in Fig. 8. We can\ninfer that our model is better than all others for discriminating\nunder-ﬁlling, as our AGMB-Transformer achieved the highest\naccuracy of all comparison methods in correctly identifying\nunder-ﬁlling. One possible explanation is that the global fea-\nture branch models long-range dependencies. The convolution\nunit in CNNs only pays attention to the range of kernel size in\nthe neighborhood. Even if the receptive ﬁeld becomes larger\nand larger in the later stage, it is still a local-area operation that\nignores the contribution of other global areas, such as distant\npixels, to the current area. This is why classiﬁcation networks\nbased on CNNs cannot perform the task effectively, but the\nprogressive Transformer shows great improvement.\n4) Effectiveness of Global Feature Branch and Branch\nFusion Module: To further verify the respective effects of\nthe global feature branch and the branch fusion module in a\nmulti-branch Transformer network, we test the performance of\nremoving the global feature branch and removing the branch\nfusion module. A comparison with the performance of remov-\ning them shows that our global feature branch and our branch\nfusion module can signiﬁcantly improve the performance of\nthe network. As the result shown in Table VI, all components\nof multi-branch Transformer network are indispensable.\nE. Model Size, Computational Cost and GPU Memory Usage\nDue to the difference between NLP task and computer\nvision task, direct application of Transformer in computer\nvision task has the problem of too much computational cost\nand GPU memory usage. We will compare the effectiveness\nof our method from three aspects: model size, computational\ncost and GPU memory usage.\nAs we know, computational cost and GPU memory usage of\nMHSA will increase sharply with the increase of image size.\nTherefore, we tested the computational cost and GPU memory\nusage of our GMHSA and MHSA when inputting feature\nblocks of different sizes, and we set the size of GMHSA unit\nto 8 ×8 during the experiment.\nAs shown in Table VII, our model size has not increased\nvery much and is far lower than ViT. Moreover, the per-\nformance of our GMHSA is much better than MHSA in\nmore important computational cost and GPU memory usage.\nConsidering that a C ×H ×W feature block will produce\nthe matrix of H×W ×H×W size in the operation of the\nself-attention layer, which can be seen from Fig. 4, and so our\nGMHSA can better deal with large feature blocks. We can see\nfrom Table VIII that the 16 ×144 ×144 size feature block\nneeds about 41G GPU memory and 14G FLOPs with MHSA.\nWhile with GMHSA, it only needs 1.6G GPU memory and\n0.068G FLOPs.\nIn order to test the effect of our bottleneck structure, we\ntested the computational cost and GPU memory usage of our\nGMHSA under the input of feature blocks with different chan-\nnel amounts. The channel scaling factor ϕ of our bottleneck\nstructure is set to 4. As shown in Tabel IX, FLOPs have been\ndecreased from 102.54G to 10.53G when the original channel\nis 4096.\nThrough the above results, we can determine that our pro-\ngressive Transformer, whether GMHSA or bottleneck struc-\nture, can well reduce the computational cost and GPU memory\nusage, which is of great signiﬁcance for the application of\nTransformer in computer vision tasks.\nF . Impact of Parameters θ and Degree of Polynomial δ\nThere are two other parameters that impact the network\nperformance, namely, θ in branch fusion module and degree\nof polynomial δ. To evaluate the sensitivity of performance to\nthem, we change θ and δ separately and report the classiﬁca-\ntion performance in Table X and Table XI. From Table X, we\ncan see that the smaller or larger θ values will lead to too big\nor too small feature blocks, which cannot ﬁlter feature blocks\nwell. When θ = 64, the 4096 channel features are divided into\n64 blocks, and there are 64 channels for each block, and the\nperformance is the best. When the degree of polynomial δ\ngradually increases from 2 to 5 in Table XI, the performance\nof the model will drop. Therefore, we set θ = 64 and δ = 2\nin our implementation.\nV. C ONCLUSION\nIn this paper, we have proposed an AGMB-Transformer\nthat is state-of-the-art in automatic X-ray image diagnosis of\nroot canal therapy. The proposed novel method ﬁrst extracts\n12\nTABLE VI\nCOMPARISON OF DIFFERENT NETWORK COMPONENTS\nComponent ACC(%) AUC(%) SEN(%) SPC(%) F1(%)\nLocal Feature Branch 86.94±1.37 95.02 ±0.87 87.67 ±1.14 93.20 ±0.31 87.64 ±1.77\nGlobal Feature Branch 88.98±0.08 95.38 ±0.70 90.55 ±0.23 94.57 ±0.18 89.34 ±0.02\nGlobal Feature Branch + Local Feature Branch 89.38±1.93 95.00 ±0.55 90.57 ±2.09 94.64 ±1.05 89.72 ±1.89\nLocal Feature Branch+Global Feature Branch+Branch Fusion Module 90.20±1.29 95.63 ±1.09 91.39 ±1.46 95.09 ±0.77 90.48 ±1.13\nTABLE VII\nCOMPARISON ON MODEL SIZE\nModel ResNet50 ResNeXt50 GCNet50 BoTNet50 ViT Ours\nSize(MB) 188 176 286 151 689 358\nTABLE VIII\nCOMPARISON OF MHSA AND GMHSA\nMHSA GMHSA\nFeature Size Memory(MB) FLOPs(G) Memory(MB) FLOPs(G)\n16 ×40 ×40 1699 0.087 1461 0.005\n16 ×56 ×56 2373 0.328 1465 0.010\n16 ×72 ×72 3951 0.888 1487 0.017\n16 ×88 ×88 7007 1.968 1511 0.026\n16 ×104 ×104 12273 3.824 1535 0.036\n16 ×120×120 20611 6.757 1563 0.047\n16 ×136 ×136 33025 11.125 1579 0.061\n16 ×144 ×144 41107 13.966 1611 0.068\nthe anatomy features and then uses them to guide a multi-\nbranch Transformer network for evaluation, where progressive\nTransformer is an explicit mechanism to model global de-\npendencies. Besides, the ﬁtting segmentation in our anatomy\nfeature extractor gets rid of the basic architecture based on\npixel-level segmentation maps and performs fuzzy boundary\nsegmentation in a completely different way. Combining the\nmultiple network structures in parallel is worth exploring\nsince the features obtained by different network structures\nhave their own advantages. Nevertheless, it is a challenge to\nbetter fuse multiple network features. We designed a multi-\nbranch structure where our branch fusion module can be\nan effective solution. Moreover, it is convenient to combine\nour multi-branch structure and branch fusion module with\nother networks. In theory, this can be extended to almost all\nexisting backbone networks, which indicates a strong potential\nfor its application to most existing medical imaging pattern\nrecognition models.\nTo the best of our knowledge, this is the ﬁrst time automatic\nroot canal therapy evaluation has been realized utilizing the\nanatomy feature as prior knowledge. However, there are still\nsome limitations in the current work. In our future work, we\nwill further improve diagnosis performance from the following\ntwo aspects: 1) to simplify the operation, we can investigate in\na more effective, end-to-end fashion, instead of in the current\nmulti-step one; 2) we can further explore the performance\nand combined effect of the multi-branch structure and various\nstate-of-the-art networks.\nTABLE IX\nCOMPARISON ON DIFFERENT FEATURE CHANNELS FOR GMHSA\nFeature Size Memory(MB) FLOPs(G)\n128×40 ×40 1713 0.77\n256 ×40 ×40 1739 1.69\n512 ×40 ×40 1765 4.01\n1024 ×40 ×40 1819 10.53\n2048 ×40 ×40 1983 31.14\n4096 ×40 ×40 2475 102.54\nTABLE X\nIMPACT OF PARAMETER θ\nθ ACC(%) AUC(%) SEN(%) SPC(%) F1(%)\n32 88.97±1.30 95.27 ±0.62 89.88 ±0.60 94.50 ±0.56 88.85 ±0.86\n64 90.20±1.29 95.63 ±1.09 91.39 ±1.46 95.09 ±0.77 90.48 ±1.13\n128 88.16±1.94 94.73 ±1.61 88.95 ±2.27 94.02 ±1.26 88.23 ±1.56\n256 88.97±1.30 95.44 ±0.57 90.04 ±0.99 94.47 ±0.64 89.29 ±1.12\nAPPENDIX\nThe performance test of classiﬁcation network often needs a\nlot of data, while the number of root canal therapy images used\nin this paper is too small to exactly compare the performance\nof the network. To further evaluate the performance of our\nAGMB-Transformer, we applied it on the widely used public\nPlant Pathology 2020 challenge dataset [58]. We divided 1821\nlabeled images into 60% training set, 20% validation set and\n20% test set to test our network. The dataset consists of 4\ncategories leaves: healthy, apple rust, apple scab, and more\nthan one disease. We resize the images to 256 ×256, and\nother parameters are exactly the same as those in the root\ncanal therapy dataset. The comparison results are reported in\nTable XII. It can be observed that our AGMB-Transformer\noutperforms other methods on ACC, AUC, SPC and F1.\nREFERENCES\n[1] M. A. Peres et al. , “Oral diseases: a global public health challenge,”\nThe Lancet, vol. 394, no. 10194, pp. 249–260, 2019.\n[2] J. Meirinhos et al., “Prevalence of apical periodontitis and its association\nwith previous root canal treatment, root canal ﬁlling length and type\nof coronal restoration–a cross-sectional study,” International endodontic\njournal, vol. 53, no. 4, pp. 573–584, 2020.\n[3] Y . Boucher et al. , “Radiographic evaluation of the prevalence and\ntechnical quality of root canal treatment in a french subpopulation,”\nInternational endodontic journal , vol. 35, no. 3, pp. 229–238, 2002.\n[4] M. P. Lazarski et al. , “Epidemiological evaluation of the outcomes of\nnonsurgical root canal treatment in a large cohort of insured dental\npatients,” Journal of endodontics , vol. 27, no. 12, pp. 791–796, 2001.\n[5] T. Kaplan et al., “Dental students’ perception of difﬁculties concerning\nroot canal therapy: A survey study,” pp. 33–38, 2019.\n[6] L. M. Lin et al. , “Do procedural errors cause endodontic treatment\nfailure?,” The Journal of the American Dental Association , vol. 136,\nno. 2, pp. 187–193, 2005.\n13\nTABLE XI\nIMPACT OF PARAMETER δ\nDegree δ ASD (mm) HD95 (mm)\n2 0.260±0.119 0.763 ±0.296\n3 0.308±0.114 0.885 ±0.313\n4 0.336±0.032 1.029 ±0.104\n5 0.288±0.100 0.884 ±0.276\nTABLE XII\nCOMPARISON ON PUBLIC DATASETS\nModel ACC(%) AUC(%) SEN(%) SPC(%) F1(%)\nResNet50 93.13 89.36 75.07 97.51 74.05\nResNeXt50 95.05 89.21 81.57 98.14 84.19\nGCNet50 93.96 90.50 83.96 97.91 85.44\nBoTNet50 93.68 89.17 74.36 97.64 72.27\nAGMB-Transformer 96.15 90.56 81.29 99.13 85.96\n[7] C. Estrela et al. , “Characterization of successful root canal treatment,”\nBrazilian dental journal , vol. 25, no. 1, pp. 3–11, 2014.\n[8] W. Saunders et al. , “Technical standard of root canal treatment in an\nadult scottish sub-population,” British dental journal , vol. 182, no. 10,\npp. 382–386, 1997.\n[9] P. E. Petersen et al., “The global burden of oral diseases and risks to oral\nhealth,”Bulletin of the World Health Organization, vol. 83, pp. 661–669,\n2005.\n[10] J.-E. Kim et al., “Accidental overextension of endodontic ﬁlling material\nin patients with neurologic complications: a retrospective case series,”\nDentomaxillofacial Radiology, vol. 45, no. 5, p. 20150394, 2016.\n[11] J. Field et al. , “A clinical radiographic retrospective assessment of\nthe success rate of single-visit root canal treatment,” International\nEndodontic Journal, vol. 37, no. 1, pp. 70–82, 2004.\n[12] B. Huang, J. Tian, H. Zhang, Z. Luo, J. Qin, C. Huang, X. He,\nY . Luo, Y . Zhou, G. Dan, H. Chen, S. Feng, and C. Yuan, “Deep\nsemantic segmentation feature-based radiomics for the classiﬁcation\ntasks in medical image analysis,” IEEE Journal of Biomedical and\nHealth Informatics, pp. 1–1, 2020.\n[13] H. Li, J. Fang, S. Liu, X. Liang, X. Yang, Z. Mai, M. T. Van, T. Wang,\nZ. Chen, and D. Ni, “Cr-unet: A composite network for ovary and follicle\nsegmentation in ultrasound images,” IEEE Journal of Biomedical and\nHealth Informatics, vol. 24, no. 4, pp. 974–983, 2020.\n[14] M. Zhang, X. Li, M. Xu, and Q. Li, “Automated semantic segmentation\nof red blood cells for sickle cell disease,” IEEE Journal of Biomedical\nand Health Informatics , vol. 24, no. 11, pp. 3095–3102, 2020.\n[15] Y . Song, L. He, F. Zhou, S. Chen, D. Ni, B. Lei, and T. Wang,\n“Segmentation, splitting, and classiﬁcation of overlapping bacteria in\nmicroscope images for automatic bacterial vaginosis diagnosis,” IEEE\nJournal of Biomedical and Health Informatics , vol. 21, no. 4, pp. 1095–\n1104, 2017.\n[16] R. Feng, X. Liu, J. Chen, D. Z. Chen, H. Gao, and J. Wu, “A deep\nlearning approach for colonoscopy pathology wsi analysis: Accurate seg-\nmentation and classiﬁcation,” IEEE Journal of Biomedical and Health\nInformatics, pp. 1–1, 2020.\n[17] Y . Zhao et al., “Tsasnet: Tooth segmentation on dental panoramic x-ray\nimages by two-stage attention segmentation network,”Knowledge-Based\nSystems, vol. 206, p. 106338, 2020.\n[18] O. Oktay et al. , “Attention u-net: Learning where to look for the\npancreas,” in Conference on Medical Imaging with Deep Learning ,\npp. 1–10, 2018.\n[19] J.-H. Lee et al. , “Application of a fully deep convolutional neural\nnetwork to the automation of tooth segmentation on panoramic radio-\ngraphs,” Oral surgery, oral medicine, oral pathology and oral radiology,\nvol. 129, no. 6, pp. 635–642, 2020.\n[20] K. He et al. , “Mask r-cnn,” in Proceedings of the IEEE international\nconference on computer vision , pp. 2961–2969, 2017.\n[21] T. L. Koch et al. , “Accurate segmentation of dental panoramic radio-\ngraphs with u-nets,” in 2019 IEEE 16th International Symposium on\nBiomedical Imaging (ISBI 2019) , pp. 15–19, IEEE, 2019.\n[22] F. Cheng et al., “Learning directional feature maps for cardiac mri seg-\nmentation,” in International Conference on Medical Image Computing\nand Computer-Assisted Intervention , pp. 108–117, Springer, 2020.\n[23] O. Ronneberger et al. , “U-net: Convolutional networks for biomedical\nimage segmentation,” in International Conference on Medical image\ncomputing and computer-assisted intervention , pp. 234–241, Springer,\n2015.\n[24] K. R. Gray et al. , “Random forest-based similarity measures for\nmulti-modal classiﬁcation of alzheimer’s disease,” NeuroImage, vol. 65,\npp. 167–175, 2013.\n[25] W. S. Noble, “What is a support vector machine?,” Nature biotechnology,\nvol. 24, no. 12, pp. 1565–1567, 2006.\n[26] J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for\nmultilayer perceptron,” IEEE transactions on neural networks and\nlearning systems, vol. 27, no. 4, pp. 809–821, 2015.\n[27] D. Shen et al., “Deep learning in medical image analysis,”Annual review\nof biomedical engineering , vol. 19, pp. 221–248, 2017.\n[28] Z. Huang, X. Liu, R. Wang, Z. Chen, Y . Yang, X. Liu, H. Zheng,\nD. Liang, and Z. Hu, “Learning a deep cnn denoising approach using\nanatomical prior information implemented with an attention mechanism\nfor low-dose ct imaging on clinical patient data from multiple anatomical\nsites,” IEEE Journal of Biomedical and Health Informatics , pp. 1–1,\n2021.\n[29] Z. Yu, E.-L. Tan, D. Ni, J. Qin, S. Chen, S. Li, B. Lei, and T. Wang,\n“A deep convolutional neural network-based framework for automatic\nfetal facial standard plane recognition,” IEEE Journal of Biomedical and\nHealth Informatics, vol. 22, no. 3, pp. 874–885, 2018.\n[30] F. Vaquerizo-Villar, D. Alvarez, L. Kheirandish-Gozal, G. C. Gutierrez-\nTobal, V . Barroso-Garcia, E. Santamaria-Vazquez, F. Del Campo,\nD. Gozal, and R. Hornero, “A convolutional neural network architecture\nto enhance oximetry ability to diagnose pediatric obstructive sleep\napnea,” IEEE Journal of Biomedical and Health Informatics , pp. 1–1,\n2021.\n[31] K. He et al., “Deep residual learning for image recognition,” in Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\npp. 770–778, 2016.\n[32] S. Xie et al. , “Aggregated residual transformations for deep neural\nnetworks,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , pp. 1492–1500, 2017.\n[33] J. Hu et al. , “Squeeze-and-excitation networks,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , pp. 7132–\n7141, 2018.\n[34] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\ntions,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1251–1258, 2017.\n[35] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , pp. 4700–4708, 2017.\n[36] K. Han et al., “A survey on visual transformer,”arXiv:2012.12556, 2020.\n[37] A. Vaswani et al., “Attention is all you need,” in NIPS, pp. 6000–6010,\n2017.\n[38] X. Wang et al., “Non-local neural networks,” inProceedings of the IEEE\nconference on computer vision and pattern recognition , pp. 7794–7803,\n2018.\n[39] Y . Cao et al. , “Gcnet: Non-local networks meet squeeze-excitation\nnetworks and beyond,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshops , 2019.\n[40] N. Parmar et al. , “Image transformer,” in International Conference on\nMachine Learning, pp. 4055–4064, 2018.\n[41] N. Carion et al. , “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision, pp. 213–229, Springer, 2020.\n[42] A. Dosovitskiy et al. , “An image is worth 16x16 words: Transformers\nfor image recognition at scale,” arXiv:2010.11929, 2020.\n[43] A. Srinivas et al. , “Bottleneck transformers for visual recognition,”\narXiv:2101.11605, 2021.\n[44] N. Tajbakhsh et al. , “Embracing imperfect datasets: A review of deep\nlearning solutions for medical image segmentation,” Medical Image\nAnalysis, vol. 63, p. 101693, 2020.\n[45] K. Pasupa and W. Sunhem, “A comparison between shallow and deep\narchitecture classiﬁers on small dataset,” in 2016 8th International\nConference on Information Technology and Electrical Engineering (ICI-\nTEE), pp. 1–6, IEEE, 2016.\n[46] Q.-D. Tran et al., “Segmentation-guided network for automatic thoracic\npathology classiﬁcation,” Research on Biomedical Engineering , pp. 1–\n14, 2021.\n[47] J. Wang et al., “Deep high-resolution representation learning for visual\nrecognition,” IEEE transactions on pattern analysis and machine intel-\nligence, 2020. doi:10.1109/TPAMI.2020.2983686.\n14\n[48] K. Sun et al., “Deep high-resolution representation learning for human\npose estimation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pp. 5693–5703, 2019.\n[49] G. D. Hutcheson, “Ordinary least-squares regression,” The SAGE dic-\ntionary of quantitative management research , pp. 224–228, 2011.\n[50] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\nposition representations,” in Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 2 (Short Papers) ,\npp. 464–468, 2018.\n[51] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\nJ. Shlens, “Stand-alone self-attention in vision models,” arXiv preprint\narXiv:1906.05909, 2019.\n[52] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2019.\n[53] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030 , 2021.\n[54] A. Paszke et al., “Pytorch: An imperative style, high-performance deep\nlearning library.,” in NeurIPS, 2019.\n[55] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin International Conference on Learning Representations (ICLR) , 2015.\n[56] A. Buslaev et al., “Albumentations: Fast and ﬂexible image augmenta-\ntions,” Information, vol. 11, no. 2, p. 125, 2020.\n[57] A. M. Reza, “Realization of the contrast limited adaptive histogram\nequalization (clahe) for real-time image enhancement,” Journal of VLSI\nsignal processing systems for signal, image and video technology ,\nvol. 38, no. 1, pp. 35–44, 2004.\n[58] R. Thapa, N. Snavely, S. Belongie, and A. Khan, “The plant pathology\n2020 challenge dataset to classify foliar disease of apples,” arXiv\npreprint arXiv:2004.11958, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6204147934913635
    },
    {
      "name": "Transformer",
      "score": 0.5922988653182983
    },
    {
      "name": "Root canal",
      "score": 0.5343078374862671
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4841381013393402
    },
    {
      "name": "Segmentation",
      "score": 0.48375988006591797
    },
    {
      "name": "Engineering",
      "score": 0.20556116104125977
    },
    {
      "name": "Medicine",
      "score": 0.16455814242362976
    },
    {
      "name": "Orthodontics",
      "score": 0.09522399306297302
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}