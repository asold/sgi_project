{
  "title": "ASFormer: Transformer for Action Segmentation",
  "url": "https://openalex.org/W3207346664",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4323931328",
      "name": "Yi, Fangqiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2218817930",
      "name": "Wen Hongyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222727177",
      "name": "Jiang Tingting",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2963247196",
    "https://openalex.org/W2550143307",
    "https://openalex.org/W2964311439",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2292288263",
    "https://openalex.org/W2990202385",
    "https://openalex.org/W2099614498",
    "https://openalex.org/W2963853051",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2461621749",
    "https://openalex.org/W3174889475",
    "https://openalex.org/W1923404803",
    "https://openalex.org/W2530494944",
    "https://openalex.org/W2314362175",
    "https://openalex.org/W2142258645",
    "https://openalex.org/W2014914041",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2794481829",
    "https://openalex.org/W2463824207",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W593558681",
    "https://openalex.org/W3165867039",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2031688197",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2019660985",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W2491875666",
    "https://openalex.org/W3100481960",
    "https://openalex.org/W2962876901",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W3092820804",
    "https://openalex.org/W2799262584",
    "https://openalex.org/W2109698606",
    "https://openalex.org/W3119038403",
    "https://openalex.org/W3166363426",
    "https://openalex.org/W3010531964",
    "https://openalex.org/W1988650444",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3034802267",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3108772932",
    "https://openalex.org/W2963722382",
    "https://openalex.org/W2508429489",
    "https://openalex.org/W3121523901"
  ],
  "abstract": "Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at \\url{https://github.com/ChinaYi/ASFormer}.",
  "full_text": "ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 1\nASFormer: Transformer for Action\nSegmentation\nFangqiu Yi\nchinayi@pku.edu.cn\nHongyu Wen\n1800013069@pku.edu.cn\nTingting Jiang\nttjiang@pku.edu.cn\nNELVT\nDepartment of Computer Science\nPeking University, China\nAbstract\nAlgorithms for the action segmentation task typically use temporal models to pre-\ndict what action is occurring at each frame for a minute-long daily activity. Recent\nstudies have shown the potential of Transformer in modeling the relations among el-\nements in sequential data. However, there are several major concerns when directly\napplying the Transformer to the action segmentation task, such as the lack of induc-\ntive biases with small training sets, the deﬁcit in processing long input sequence, and\nthe limitation of the decoder architecture to utilize temporal relations among multiple\naction segments to reﬁne the initial predictions. To address these concerns, we design\nan efﬁcient Transformer-based model for action segmentation task, named ASFormer,\nwith three distinctive characteristics: (i) We explicitly bring in the local connectivity in-\nductive priors because of the high locality of features. It constrains the hypothesis space\nwithin a reliable scope, and is beneﬁcial for the action segmentation task to learn a proper\ntarget function with small training sets. (ii) We apply a pre-deﬁned hierarchical repre-\nsentation pattern that efﬁciently handles long input sequences. (iii) We carefully design\nthe decoder to reﬁne the initial predictions from the encoder. Extensive experiments on\nthree public datasets demonstrate that effectiveness of our methods. Code is available at\nhttps://github.com/ChinaYi/ASFormer.\n1 Introduction\nAlgorithms for automatic detection and segmentation of human activities are crucial for\napplications such as home security, healthcare, and robot automatic. Different from the ac-\ntion classiﬁcation task that tries to classify a short trimmed video into a single action label,\nthe goal of the action segmentation task is to assign an action label for each frame for a\nminutes-long untrimmed video. Instead of using raw RGB video sequences as the input,\naction segmentation methods operate on pre-extracted frame-wise feature sequences and fo-\ncus on modeling the temporal relations among frames. Transformers, originally designed\nfor the machine translation task [41], have achieved great performance for almost all natural\nlanguage processing(NLP) tasks over the past years. Very recently, many researchers also\nshow the potential of pure or hybrid Transformer models for many vision tasks, including\nimage classiﬁcation [4, 10, 40, 45, 50], action classiﬁcation [1], segmentation [44, 46, 52],\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:2110.08568v1  [cs.CV]  16 Oct 2021\n2 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\net al. Action segmentation task is similar to NLP tasks, since both of them are sequence-\nto-sequence prediction tasks. With the success of Transformer-based models in modeling\nthe relations among elements in sequential data, one would expect the Transformer-based\nmodels to be highly effective for the action segmentation task as well.\nHowever, there are three major concerns when solving action segmentation task by the\nvanilla Transformer [41] :\n1. Due to the small size of training sets, the lack of inductive biases of the vanilla\nTransformer becomes the bottleneck of applying it to action segmentation problem.\nThe lack of inductive biases broadens the family of functions they can represent [6],\nhowever, it requires a large amount of training data. Compared to the NLP task and\nother vision tasks, the training set of action segmentation task is relatively small, mak-\ning it difﬁcult to learn a target function from a large hypothesis space.\n2. Due to the deﬁcit of self-attention for the long input video, the Transformer is\nhard to form an effective representation . At initialization, the self-attention layer\ncast nearly uniform attention weights to all the elements in the sequence. However,\nthe input video for the action segmentation task usually lasts for thousands of frames,\nmuch longer than the image-patch sequences in other vision tasks. Due to the length\nof videos, it is challenging for the self-attention layer to learn appropriate weights that\nfocus on meaningful locations [53]. The deﬁcit of each self-attention layer further\nraise a serious issue: it is hard for those self-attention layers in one Transformer model\nto cooperate with each other to form an effective representation for the input.\n3. The original encoder-decoder architecture of the Transformer does not meet the\nreﬁnement demand of action segmentation task. Temporal relations among mul-\ntiple action segments play an important role in the action segmentation task, e.g. the\naction after take bottle and pour water usually to be drink water. Given an initial\nprediction, previous works usually apply additional TCNs [11, 47] or GCNs [15, 43]\nover the initial prediction to perform a reﬁnement process to boost the performance.\nHowever, the decoder in the vanilla encoder-decoder architecture is not designed for\nsuch usage.\nIn this paper, we will address the above three concerns in our proposed ASFormer, as\nshown in Fig. 1. For the ﬁrst concern, we observe that one property of the action segmenta-\ntion task is the high locality of features because every action occupies continued timestamps.\nThus, the local connectivity inductive bias is important to the action segmentation task.\nIt constrains the hypothesis space within a reliable scope, and is beneﬁcial to learn a proper\ntarget function with small training sets. We bring in such strong inductive priors by applying\nadditional temporal convolutions in each layer. For the second concern that the Transformer\nwith serials of self-attention layers is hard to form an effective representation over the long\ninput sequence, we constraint each self-attention layer with apre-deﬁned hierarchical rep-\nresentation pattern, which forces the low-level self-attention layers to focus on the local\nrelations at ﬁrst and then gradually enlarges their footprints to capture longer dependencies\nin high-level layers. The local-to-global process assigns speciﬁc responsibilities to each\nself-attention layer, so that they can cooperate better to achieve faster convergence speed and\nhigher performance. Such hierarchical representation pattern also reduces the total space\nand time complexity to make our model scalable. Finally, we propose a new design of the\ndecoder to acquire the reﬁned predictions. The cross-attention mechanism in the decoder\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 3\nEncoder Block\nEncoder Block\nEncoder Block\nEncoder Block\n…\nDilated Conv\nReLU\nInstance Norm\nQ VK\n⊗\n⊗\n1×1 Conv\nSelf-Attention\nDilated Conv\nReLU\nInstance Norm\nQ VK\n⊗\n⊗\n1×1 Conv\nCross-Attention\nConcat\nInput Sequence\n(a) (b) (c)\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\nEncoder Decoder #1 Decoder #2 Decoder #3\nFigure 1: (b). An overall graph of the ASFormer model which consists of an encoder and\nseveral decoders to perform an iterative reﬁnement. For the encoder, it receives video se-\nquences and outputs initial predictions. Encoder consists of serials of encoder blocks with\npre-deﬁned hierarchical representation patterns. For the decoder, it receives predictions as\nthe input and has similar architecture with encoder. (a). In each encoder block, it consists of\na feed-forward layer (dilated temporal convolution) and a self-attention layer with residual\nconnections. (c). The decoder block uses cross-attention mechanism to bring in information\nfrom the encoder.\nallows every position in the encoder to attend over all positions in the reﬁnement process,\nsimultaneously avoiding the disturbance of the encoder to the learned feature space in the\nreﬁnement phase.\nExperiments are conducted on three common public datasets, including the 50Salads [37],\nBreakfast [19] and GTEA [12]. The experimental results demonstrate the proposed solution\nis capable of dealing with small training dataset and long videos with thousands of frames.\nThe design of decoders also takes advantage of temporal relations among multiple action\nsegments to help get smoother and accurate predictions. To summarize, the main contribu-\ntions of this work include: 1) An exploration for Transformer on action segmentation task\nwith three distinctive characteristics: the explicitly introduced local connectivity inductive\nbias, pre-deﬁned hierarchical representation pattern, and new design of the decoder; 2) state-\nof-the-art action segmentation results on three public datasets.\n2 Related Work\nAction Segmentation. Earlier approaches detect action segments using a sliding window\nand ﬁlter out redundant hypotheses with non-maximum suppression [18, 33]. Other ap-\nproaches model the temporal action sequence with Conditional Random Fields [30, 34, 39],\nMarkov model [20, 38] and RNN [9, 29, 35, 42, 49] to classify framewise actions. In recent\nyears, motivated by the success of temporal convolution in speech synthesis, most of the\nworks try to apply various temporal convolution networks for action segmentation task, such\nas the encoder-decoder temporal convolution [8, 23], deformable temporal convolution [24],\nor dilated temporal convolution [11, 16]. Other works build on top of these TCNs. Wang et\nal. [47] reuse the structure of [11] as cascade stages. Huang et al. [15] and Wang et al. [43]\n4 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\npropose a graph-based temporal reasoning module, which can be added to the top of action\nsegmentation models. Chen et al. [5] propose to exploit auxiliary unlabeled videos by shap-\ning this problem as a domain adaptation (DA) problem. Very recently, Ishikawa1 et al. [17]\nalleviate over-segmentation errors by detecting action boundaries. Singhania et al. [36] fol-\nlow a coarse-to-ﬁne structure with an implicit ensemble of multiple temporal resolutions.\nHowever, a single convolutional layer does not connect all pairs of input and output posi-\ntions, which remains room for improvement.\nTransformer.Transformer [41] is originally designed for natural language processing tasks,\nwhich relies on the self-attention mechanism to build dependencies among elements. Due\nto the strong capability of capturing global information, hybrid Transformer models have\nalso been successfully applied to a variety of vision problems, including image classiﬁca-\ntion [4, 10, 40, 45, 50], action classiﬁcation [1], segmentation [44, 46, 52], et al. More\nrecently, some researches study the efﬁcient version of Transformer models, which explore\nattention restrictions to local windows, such as Swin [28], BigBird [51]. In this paper, we\nexplore the application of Transformer based models on the action segmentation task.\nAction Detection. Different from action segmentation task, the action detection task aims\nat localizing the start/end of the action and recognizing it in a untrimmed video. The One-\nstage methods [25, 26] draw on the SSD [27] method in object detection and design end-\nto-end action detection networks with the similar feature pyramid structures. Two-stage\nmethods [3, 48] adopt the Faster-RCNN [31] architecture, including proposal generation and\nproposal classiﬁcation.\n3 Methods\nIn this work, we propose ASFormer to tackle the action segmentation task, as shown in\nFig. 1. Our ASFormer adapts an encoder-decoder structured Transformer. Given the pre-\nextracted frame-wise video feature sequence, the encoder will ﬁrst predict the initial action\nprobability for each frame. Then the initial predictions will be passed to multiple successive\ndecoders to perform an incremental reﬁnement. In Sec. 3.1, we ﬁrst illustrate the structure of\nencoder, showing how we deal with small training dataset and long videos with thousands\nof frames. After that, we introduce the design of decoders and our way to take advantage\nof temporal relations among multiple action segments for reﬁnement in Sec. 3.2. Finally, in\nSec. 3.3, we introduce the implementation and training details of our ASFormer.\n3.1 Encoder\nThe input for the encoder is the pre-extracted feature sequences of sizeT ×D, where T is the\nvideo length andD is the feature dimension. The ﬁrst layer of the encoder is a fully connected\nlayer that adjusts the dimension of the input feature. Then, this layer is followed by serials\nof encoder blocks. After that, a fully connected layer will output predictionsye ∈RT ×C from\nthe last encoder block, where C denotes the number of action classes.\nEach encoder block contains two sub-layers. The ﬁrst is a feed-forward layer, and the\nsecond is a single-head self-attention layer. We employ a residual connection around each\nof the two sub-layers, followed by instance normalization and ReLU activation, as shown in\nFig. 1(a). Different from the vanilla Transformer, we use a dilated temporal convolution as\nthe feed-forward layer instead of point-wise fully connected layer. This design is inspired\nby the properties of the action segmentation task, which are a) lack of the large training\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 5\ndataset for completely free inductive bias. b) high locality of features since every action\noccupies continued timestamps in the input video. In contrast to the fully connected layer,\nthe temporal convolution layer can bring beneﬁcial local inductive bias to our model.\nThe self-attention layer is hard to be learned to focus on meaningful locations over thou-\nsands of frames. For an input video, those self-attention layers are difﬁcult to cooperate with\neach other to form an effective representation. To mitigate this issue, we pre-deﬁne a hier-\narchical representation pattern. Such a hierarchical pattern is inspired from modern neural\nnetwork design: one would ﬁrst focus on the local feature and then gradually enlarge the re-\nceptive ﬁeld to capture the global information. For example, CNNs achieve such pattern with\nserials of pooling layers to enlarge the receptive ﬁeld in higher layers; or use dilated convolu-\ntions with gradually increasing dilation rate. Motivated by the success of such a hierarchical\npattern, we constraint the receptive ﬁelds of each self-attention layer within a local window\nwith size w (e.g. for a frame t, we only calculate the attention weights with the frames that\nwithin its local window). The size of the local window is then doubled at each layer i (i.e.,\nw = 2i,i = 1,2...). Meanwhile, we also double the dilation rate of the temporal convolution\nlayer with the encoder depth increasing, keeping consistent with the self-attention layer.\nFor an encoder with J blocks, the whole approximate memory usage for a vanilla Trans-\nformer is (J ·T ·T ), where T is the video length. With the hierarchical representation pattern,\nWe reduce the total space complexity to ((2 −ε) ·2J ·T ), where ε is a small number. In our\nsettings, we use J = 9, where 2J = 512 is almost 10 times smaller than T . Compared to the\nvanilla Transformer, our ASFormer is applicable to receiving long input sequence.\n3.2 Decoders\nTemporal relations among multiple action segments play an important role in the action\nsegmentation task. There are some prior relationship between actions segments, e.g. the\naction after take bottle and pour water usually to be drink water. As previous works showed,\napplying additional TCNs [11, 47] or GCNs [15, 43] over the initial prediction to perform\na reﬁnement process can boost the performance. In this section, we illustrate how the new\ndesigned decoder carries out the reﬁnement task for the initial predictions output by the\nencoder in one-forward-pass. For better explanation, we ﬁrst introduce a single decoder, and\nnaturally extend it to a multiple version to perform an iterative reﬁnement.\nA Single Decoder. The input for the decoder is the initial predictions output by the encoder.\nThe ﬁrst layer of the decoder is a fully connected layer to adjust the dimension, and then\nfollowed by a serial of decoder blocks. The architecture of each decoder block is shown in\nFig. 1(c). Similar to encoder, we use temporal convolution as the feed-forward layer and the\nhierarchical pattern is also applied for the cross-attention layer.\nCompared to the self-attention layer, the cross-attention has the following difference:\nThe query Q and key K are obtained from the concatenation of the output from the encoder\nand previous layer, while the value V is only obtained from the output of the previous layer.\nThe cross-attention mechanism allows every position in the encoder to attend over all posi-\ntions in the reﬁnement process by generating the attention weights. The feature space V is\ncompletely transformed from the input predictions, and will not be disturbed with the par-\nticipant of the encoder, because the generated attention weights are only used to perform a\nlinear combination within V . Such design is inspired by the previous work [11], where they\nshow the reﬁnement process is very sensitive to the disturbance of the learned feature space\nfrom the predictions.\n6 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\nMultiple Decoders. One would naturally expand the single decoder to a multiple version\nto perform iterative reﬁnement. In the multiple version, the input of each decoder are from\nthe previous one, as shown in Fig. 1(b).\nThe cross-attention mechanism allows to bring in external information to guide the re-\nﬁnement process. We hope to gradually reduce the weight of external information to avoid\nthe problem of error accumulation. For the input x in each decoder block, we use a weighted\nresidual connection for the output of the feed-forward layer and the cross-attention layer:\nout = f eed_ f orward(x)\nout = α ∗cross_att(out) +out (1)\nWe set theα = 1 for the ﬁrst decoder and then exponentially decreaseα for rest decoders.\n3.3 Loss Function & Implementation details\nThe loss function is a combination of classiﬁcation loss Lcls for each frame and smooth loss\nLsmo [11]. The classiﬁcation loss is a cross-entropy loss, while the smooth loss calculates\nthe mean squared error over the frame-wise probabilities. The ﬁnal loss function Lis,\nL= Lcls + λLsmo = 1\nT ∑\nt\n−log(yt, ˆc) +λ 1\nTC ∑\nt\n∑\nc\n(yt−1,c −yt,c)2\nwhere yt, ˆc is the the predicted probability for the ground truth label ˆc at time t. λ is a balance\nweight that is set to 0.25 in our experiments. Finally to train the complete model, the sum of\nthe losses over the encoder and all decoders is minimized.\nThe ﬁnal ASFormer consists of one encoder and three decoders, while each encoder and\nthe decoder contains nine blocks. The dimension of the ﬁrst fully connected layer in the en-\ncoder and decoder is set to 64, as well as the feature dimension in each encoder and decoder\nblock. Moreover, a special dropout layer is applied to the input feature of the encoder, which\nrandomly drops the entire feature channel with a dropout rate of 0.3. In all experiments, we\ntrain the model for 120 epochs through Adam optimizer with a learning rate 0.0005.\n4 Dataset\n50Salads [37] dataset contains 50 videos with 17 action classes of users making a salad and\nhas been used for both action segmentation and detection. On average, each video contains\n20 action instances and is about 6.4 minutes long. These activities were performed by 25\nactors where each actor prepared two different salads. For evaluation, we perform 5-fold\ncross-validation and report the average results.\nGTEA [12] dataset contains 28 videos of 11 action classes of daily activities in a kitchen,\nlike take or pour, performed by four subjects. On average, each video has 20 action instances\nand is about half-minute long on average. We use the standard four different train-test splits\nas previous works by leaving one subject out. Similar to 50Salads, the average results of four\nsplits are reported.\nBreakfast [19] dataset is the largest and the most challenging dataset among the three\ndatasets with 1712 videos. The videos were recorded in 18 different kitchens, showing\nbreakfast preparation-related activities. Overall, there are 48 different actions, and each\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 7\nvideo contains six action instances on average. We use the standard 4-fold cross-validation\nfor evaluation and report the average results.\nFor all the three datasets, we use the I3D [2] model, which is trained on kinetics [2] dataset,\nto pre-extract feature sequences as in previous works [7, 11, 15, 23, 43, 47]. The dimension\nof the I3D feature for each frame is 2048-d. The following three evaluation metrics are used\nto evaluate the performance: frame-wise accuracy(Acc.), segmental edit score (Edit), and\nsegmental overlap F1 score with threshold k/100, denoted as F1@k.\n5 Experiments\n5.1 Impact of position encoding and multi-head self-attention\nIn terms of implementation details, our ASFormer has two differences compared to the\nvanilla Transformer. First, the position encoding is not applied for the input of both encoder\nand decoders. Second, vanilla Transformer usually adapts multi-head attention that concate-\nnates the output from multiple single-head attention to enhance the feature transformation\nability. Instead, we only use a single-head attention in each encoder/decoder block. We con-\nduct the following two experiments to demonstrate the introduction of temporal convolution\nmakes our model free from position encoding and multi-head version of self-attention.\nThe ﬁrst experiment studies whether position encoding is still needed for ASFormer.\nThe results on 50Salads dataset are shown in Table 1. For the temporal convolution, the\nposition encoding is even counterproductive. When the position encoding is only applied\nfor the encoder, the performance already drops a lot. Further, when the position encoding\nis applied for both encoder and decoders, the performance is even worse. The possible\nreason is that the temporal convolution already has the ability to model the relative positional\nrelationships. The redundant absolute position encoding might be harmful to the temporal\nconvolutions to learn the feature embedding. Thus, we drop the position encoding from\nour model. The second experiment studies the impact of multi-head self-attention. Table 2\nshows the comparison of multi-head self-attention with the different numbers of heads on\n50Salads. For each head in the multi-head attention, the feature dimension is the same as the\nsingle-head attention. We can observe that the single head self-attention achieves comparable\nperformance with the multiple ones. This is because the convolution operation has a similar\nform with the multi-head self-attention operation (e.g. a 64-ﬁlters convolution operator also\nconcatenates the output from 64 1-ﬁlter convolutions). Considering the extra computation\nand memory budgets brought by the multi-head self-attention, we use the single-head self-\nattention by default.\nTable 1: Comparison of whether us-\ning position encoding on 50Salads.\nF1@{10, 25, 50} Edit Acc.\nw/o pe (ours) 85.1 83.4 76.0 79.6 85.6\n+ pe encoder 80.2 78.0 70.0 73.0 83.6\n+ pe decoder 78.1 76.7 69.4 72.0 84.1\nTable 2: Comparison of multi-head self-\nattention with different number of heads\non 50Salads.\nF1@{10, 25, 50} Edit Acc.\nsingle head (ours) 85.1 83.4 76.0 79.6 85.6\ntwo heads 85.0 83.0 75.9 78.2 85.9\nthree heads 85.2 83.2 76.8 78.1 85.3\n5.2 Effect of the local connective inductive bias\nIn this section, we study the effect of bringing in local connective inductive bias with the\ntemporal convolution. For comparison, we use an MLP as the feed-forward layer as the same\n8 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\nas the vanilla Transformer. It worth noting that when removing the temporal convolution\nfrom our model, the position encoding is needed to bring in position information as the\nvanilla transformer does. In order to directly compare the the two solutions, we only use the\nencoder part for the experiments to avoid the impact of the reﬁnement process. Results on the\n50Salads dataset are shown in Table 3. We can observe that when MLP is used as the feed-\nforward layer, the performance drops greatly, especially on F1 scores and Edit score, which\ndenotes that the model fails to model the temporal relationship among frames to produce\nsmoother and consistent predictions. This also demonstrates that the fusion of neighboring\nlocal information plays an important role in the performance.\nTable 3: Comparison of MLP with position encoding and temporal convolution as the feed-\nforward layer on 50Salads. Results on the the encoder are reported.\nF1@{10, 25, 50} Edit Acc.\nMLP+PE 27.6 25.3 19.9 20.0 74.2\nConv (ours) 53.1 51.4 47.0 43.3 85.7\n5.3 Effect of the hierarchical representation pattern\nThe hierarchical representation pattern plays an important role in our ASFormer. To demon-\nstrate its effectiveness, we conduct a non-hierarchical version by setting the size of the lo-\ncal window in all attention layers to 512, which is the largest window size in the last en-\ncoder/decoder block (out of memory if we directly set the size of the local window in all\nattention layers to the video length). The non-hierarchical version allows each self-attention\nlayer to span their attention weights ‘freely’. Experiments on 50Salads dataset are shown in\nTable 4. The performance of the non-hierarchical version drops signiﬁcantly.\nTo better understand why there is such a huge performance gap, we show some visual-\nization results in Fig. 2. For an anchor query frame, we plot its attention weights in each\nself-attention layer in the encoder (The attention weights are normalized with min-max 1.).\nFig. 2(a) shows the non-hierarchical version, while Fig. 2(b) shows the case that with the\npre-deﬁned hierarchical pattern. We have two observations. First, for the high-level blocks\n(e.g.block #9) that has the same window size, there are much more activation (blue ribbon) in\nthe non-hierarchical version after the min-max normalization. This means that the attention\nweights in many locations have close values and more similar to a trivial uniform distribu-\ntion. In contrast, self-attention trained with the hierarchical pattern tend to focus on several\nmeaningful locations. Secondly, the ‘freely’ attention cannot automatically learn a hierar-\nchical pattern from data. As shown in Fig. 2(a), the low-level blocks do not cast attention on\nits neighbors when the self-attention layer is not constrained.\n5.4 Effect of the multiple decoders\nTo demonstrate that our decoder takes advantage of temporal relations among multiple action\nsegments for reﬁnement, we conduct an ablation study about stacking the different numbers\nof decoders. Results on 50Salads dataset are shown in Table. 5. The decoders largely boost\nthe performance compared to the encoder, and we achieve the best results when stacking\n1x = (x −min)/(max −min)\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 9\nTable 4: Comparison of hierarchical representation pattern and the non-hierarchical pattern\n(by setting the size of local window in all attention layers to 512) on 50Salads.\nF1@{10, 25, 50} Edit Acc.\nnon-hierarchical 64.2 61.5 55.1 59.5 76.8\nhierarchical (ours) 85.1 83.4 76.0 79.6 85.6\n+\nBlock #1\nBlock #9\n(a)  non-hierarchical\nBlock #1\nBlock #9\n+\n(b) hierarchical\n512 512\nFigure 2: The visualization of attention weights for an anchor frame (red +) in each en-\ncoder block, more visualization can be found in the supplementary material. (a) the non-\nhierarchical (by setting the window size to 512 in all blocks). (b) With the hierarchical\npattern.\nthree decoders. To show the iterative reﬁnement process, we further plot the predictions in\nthe encoder and all decoders, as shown in the supplementary material.\n5.5 Ablations of the number of blocks\nThe number of blocksJ in the encoder/decoder is an important hyper-parameter, where more\nblocks bring larger receptive ﬁelds but also lead to higher memory costs as introduced in\nSec. 3.1. We conduct an ablation study about the different number of blocks in the en-\ncoder/decoder on 50salads dataset and report the peak value of the GPU memory cost for\nbatch size 1 during training. Results are shown in Table 6. We achieve higher performance\nwith the increasing number of blocks untilJ = 9. Using more than 9 blocks (J = 10) slightly\nimproves the frame-wise accuracy but does not increases the F1 scores. Meanwhile, the\nGPU memory cost increases dramatically with the largeJ. Thus, we choose J = 9 for all our\nexperiments.\nTable 5: Comparison of stacking different\nnumber of decoders on 50Salads.\nF1@{10, 25, 50} Edit Acc.\nEncoder Only 53.1 51.4 47.0 43.3 85.7\nOne Decoder 79.5 77.4 , 71.6 71.5 86.8\nTwo Decoders 83.9 82.8 76.8 76.7 86.8\nThree Decoders (ours) 85.1 83.4 76.0 79.6 85.6\nFour Decoders 84.0 82.2 75.8 76.5 84.3\nTable 6: Comparison of different number of\nblocks J on 50Salads.\nJ F1@{10, 25, 50} Edit Acc. GPU Mem.\n7 82.9 81.5 74.0 76.0 84.5 ∼2.1G\n8 84.4 82.8 75.4 78.2 85.4 ∼2.5G\n9 85.1 83.4 76.0 79.6 85.6 ∼3.5G\n10 84.7 83.6 76.5 79.5 86.4 ∼6.1G\n5.6 Comparison with SOTA\nThis section compares the proposed ASFormer to the state-of-the-art methods on three datasets:\n50Salads, GTEA, and Breakfast datasets. The results are presented in Table 7. Our model\n10 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\nachieves the state-of-the-art methods on the three datasets compared to previous work. The\nresults on three metrics highlight the ability of our ASFormer to obtain accurate and smooth\npredictions.\nTable 7: Comparison with the state-of-the-art on 50Salads, GTEA and the Breakfast dataset.\nBold and underlined denote the highest and the second value in each column.\n50Salads GTEA Breakfast\nF1@{10,25,50} Edit Acc. F1@{10,25,50} Edit Acc. F1@{10,25,50} Edit Acc.\nIDT+LM [32] 44.4 38.9 27.8 45.8 48.7 - - - - - -\nST-CNN [22] 55.9 49.6 37.1 45.9 59.4 58.7 54.4 41.9 - 60.6 - - -\nBi-LSTM [35] 62.6 58.3 47.0 55.6 55.7 66.5 59.0 43.6 - 55.5 - -\nED-TCN [23] 68.0 63.9 52.6 59.8 64.7 72.2 69.3 56.0 - 64.0 - - 43.3\nHTK [21] - - - - - - - - 50.7\nTCFPN [8] - - - - - - - - 52.0\nSA-TCN [7] - - - - - - - - 50.0\nHTK(64) [20] - - - - - - - - 56.3\nTDRN [24] 72.9 68.5 57.2 66.0 68.1 79.2 74.4 62.7 74.1 70.1 - - -\nSSA-GAN [13] 74.9 71.7 67.0 69.8 73.3 80.6 79.1 74.2 76.0 74.4 - - -\nMS-TCN [11] 76.3 74.0 64.5 67.9 80.7 85.8 83.4 69.8 79.0 76.3 52.6 48.1 37.9 61.7 66.3\nDTGRM [43] 79.1 75.9 66.1 72.0 80.0 87.8 86.6 72.9 83.0 77.6 68.7 61.9 46.6 68.9 68.3\nBCN [47] 82.3 81.3 74.0 74.3 84.4 88.5 87.1 77.3 84.4 79.8 68.7 65.5 55.0 66.2 70.4\nGao et al. [14] 80.3 78.0 69.8 73.4 82.2 89.9 87.3 75.8 84.6 78.5 74.9 69.0 55.2 73.3 70.7\nASRF [17] 84.9 83.5 77.3 79.3 84.5 89.4 87.8 79.8 83.7 77.3 74.3 68.9 56.1 72.4 67.6\nASFormer 85.1 83.4 76.0 79.6 85.6 90.1 88.8 79.2 84.6 79.7 76.0 70.6 57.4 75.0 73.5\nTable 8: Comparison of ASRF(build upon\nMS-TCN) and ASRF*(build upon our AS-\nFormer) on 50salads dataset.\nF1@{10, 25, 50} Edit Acc.\nASRF(MS-TCN) 84.9 83.5 77.3 79.3 84.5\nASRF*(ASFormer) 86.8 85.4 79.3 81.9 85.9\nTable 9: Comparison of ASFormer and MS-\nTCN on number of parameters, FLOPs and\nGPU memory cost on 50salads dataset.\n#params (M) FLOPs( G) GPU Mem.\nMS-TCN 0.799 4.79 ∼1.7G\nASFormer 1.134 6.80 ∼3.5G\nIt is worth noting that, our ASFormer can serve as a strong backbone model and thus\ncan be easily adapted by other works. To demonstrate that, we replace the original TCN-\nbased backbone model MS-TCN [11] in ASRF [17] with our ASFormer. The new model,\ndenoted as ASRF*, achieves even higher results on the 50salads dataset than the original\nASRF [17], as shown in Table 8. Besides the performance, we further compare with the\npopular TCN-based backbone MS-TCN [11] in terms of the computation cost, as shown in\nTable 9. The additional computational burden of our ASFormer is acceptable with the mod-\nern chips. Thus, our ASFormer provides an alternative backbone choice for the community\nto build their models.\n6 Conclusion\nIn this paper, we explore the application of Transformer-based models on action segmenta-\ntion task. We propose three major concerns and respective solutions in our ASFormer. The\nsuperior results on three public dataset demonstrate the effectiveness of our method, and\nprovide a new solution for community to address the action segmentation task.\nAcknowledgement. This work was partially supported by the Natural Science Foundation\nof China under contracts 62088102. We also acknowledge High-Performance Computing\nPlatform of Peking University for providing computational resources.\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 11\nReferences\n[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all\nyou need for video understanding? CoRR, abs/2102.05095, 2021. URL https:\n//arxiv.org/abs/2102.05095.\n[2] João Carreira and Andrew Zisserman. Quo Vadis, action recognition? a new model and\nthe kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4724–4733, 2017.\n[3] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia\nDeng, and Rahul Sukthankar. Rethinking the faster r-cnn architecture for temporal\naction localization. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1130–1139, 2018.\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-\nscale vision transformer for image classiﬁcation. CoRR, abs/2103.14899, 2021. URL\nhttps://arxiv.org/abs/2103.14899.\n[5] Min-Hung Chen, Baopu Li, Yingze Bao, and Ghassan AlRegib. Action segmentation\nwith mixed temporal domain adaptation. In IEEE Winter Conference on Applications\nof Computer Vision (WACV), pages 594–603, 2020.\n[6] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship be-\ntween self-attention and convolutional layers. InInternational Conference on Learning\nRepresentations (ICLR), 2020.\n[7] Rui Dai, Luca Minciullo, Lorenzo Garattoni, Gianpiero Francesca, and François Bre-\nmond. Self-attention temporal convolutional network for long-term daily living activ-\nity detection. In IEEE International Conference on Advanced Video and Signal Based\nSurveillance (AVSS), pages 1–7, 2019.\n[8] Li Ding, Chenliang Xu, and Juergen Gall. Weakly-supervised action segmentation\nwith iterative soft boundary assignment. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), page 6508–6516, 2018.\n[9] Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Ser-\ngio Guadarrama, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional\nnetworks for visual recognition and description.IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 39(4):677–691, 2017.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. CoRR, abs/2010.11929, 2020. URL\nhttps://arxiv.org/abs/2010.11929.\n[11] Yazan Abu Farha and Jurgen Gall. MS-TCN: multi-stage temporal convolutional net-\nwork for action segmentation. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3575–3584, 2019.\n12 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\n[12] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to recognize objects in\negocentric activities. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 3281–3288, 2011.\n[13] Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes. Fine-\ngrained action segmentation using the semi-supervised action GAN. Pattern Recogni-\ntion, 98:107039, 2020.\n[14] Shang-Hua Gao1, Qi Han, Zhong-Yu Li, Pai Peng, Liang Wang, and Ming-Ming\nCheng. Global2local: Efﬁcient structure search for video action segmentation. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n[15] Yifei Huang, Yusuke Sugano, and Yoichi Sato. Improving action segmentation via\ngraph-based temporal reasoning. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 14021–14031, 2020. doi: 10.1109/CVPR42600.2020.\n01404.\n[16] Noureldien Hussein, Efstratios Gavves, and Arnold W.M. Smeulders. Timeception\nfor complex action recognition. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 254–263, 2019.\n[17] Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, and Hirokatsu Kataoka. Alleviating\nover-segmentation errors by detecting action boundaries. In IEEE Winter Conference\non Applications of Computer Vision (WACV), 2020.\n[18] Svebor Karaman, Lorenzo Seidenari, and Alberto Del Bimbo. Fast saliency based\npooling of ﬁsher encoded dense trajectories. In European Conference on Computer\nVision (ECCV), THUMOS Workshop, 2014.\n[19] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering\nthe syntax and semantics of goal-directed human activities. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 780–787, 2014.\n[20] Hilde Kuehne, Juergen Gall, and Thomas Serre. An end-to-end generative framework\nfor video segmentation and recognition. In IEEE Winter Conference on Applications\nof Computer Vision (WACV), pages 1–8, 2016.\n[21] Hilde Kuehne, Alexander Richard, and Juergen Gall. Weakly supervised learning of\nactions from transcripts. Computer Vision and Image Understanding, 163:78–89, 2017.\n[22] Colin Lea, Austin Reiter, René Vidal, and Gregory D. Hager. Segmental spatiotempo-\nral cnns for ﬁne-grained action segmentation. In European Conference on Computer\nVision (ECCV), page 36–52, 2016.\n[23] Colin Lea, Michael D. Flynn, René Vidal, Austin Reiter, and Gregory D. Hager. Tem-\nporal convolutional networks for action segmentation and detection. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pages 1003–1012, 2017.\n[24] Peng Lei and Sinisa Todorovic. Temporal deformable residual networks for action seg-\nmentation in videos. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 6742–6751, 2018.\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 13\n[25] Xin Li, Tianwei Lin, Xiao Liu, Wangmeng Zuo, Chao Li, Xiang Long, Dongliang\nHe, Fu Li, Shilei Wen, and Chuang Gan. Deep concept-wise temporal convolutional\nnetworks for action localization. In Proceedings of the ACM International Conference\non Multimedia (MM), page 4004–4012, 2020.\n[26] Tianwei Lin, Xu Zhao, and Zheng Shou. Single shot temporal action detection. In Pro-\nceedings of the ACM International Conference on Multimedia (MM) , page 988–996,\n2017.\n[27] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-\nYang Fu, and Alexander C. Berg. SSD: Single shot multibox detector. In European\nConference on Computer Vision (ECCV), pages 21–37, 2016.\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. Swin transformer: Hierarchical vision transformer using shifted win-\ndows. CoRR, abs/2103.14030, 2021. URL https://arxiv.org/abs/2103.\n14030.\n[29] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals,\nRajat Monga, and George Toderici. Beyond short snippets: Deep networks for video\nclassiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4694–4702, 2015.\n[30] Hamed Pirsiavash and Deva Ramanan. Parsing videos of actions with segmental gram-\nmars. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page\n612–619, 2014.\n[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-\ntime object detection with region proposal networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 39:1137–1149, 2017.\n[32] Alexander Richard and Juergen Gall. Temporal action detection using a statistical\nlanguage model. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 3131–3140, 2016.\n[33] Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka, and Bernt Schiele. A\ndatabase for ﬁne grained activity detection of cooking activities. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages 1194–1201, 2012.\n[34] Qinfeng Shi, Li Cheng, Li Wang, and Alex Smola. Human action segmentation and\nrecognition using discriminative semi-Markov models. International Journal of Com-\nputer Vision (IJCV), 93:22–32, 2011.\n[35] Bharat Singh, Tim K. Marks, Michael Jones, Oncel Tuzel, and Ming Shao. A multi-\nstream bi-directional recurrent neural network for ﬁne-grained action detection. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1961–\n1970, 2016.\n[36] Dipika Singhania, Rahul Rahaman, and Angela Yao. Coarse to ﬁne multi-resolution\ntemporal convolutional network. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021.\n14 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\n[37] Sebastian Stein and Stephen J. McKenna. Combining embedded accelerometers with\ncomputer vision for recognizing food preparation activities. InProceedings of ACM In-\nternational Joint Conference on Pervasive and Ubiquitous Computing, page 729–738,\n2013.\n[38] Kevin Tang, Li Fei-Fei, and Daphne Koller. Learning latent temporal structure for com-\nplex event detection. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1250–1257, 2012.\n[39] Lingling Tao, Luca Zappella, Gregory Hager, and René Vidal. Surgical gesture seg-\nmentation and recognition. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), volume 16, pages 339–46, 09 2013.\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation\nthrough attention. CoRR, abs/2012.12877, 2020. URL https://arxiv.org/\nabs/2012.12877.\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, undeﬁnedukasz Kaiser, and Illia Polosukhin. Attention is all you need. InPro-\nceedings of the International Conference on Neural Information Processing Systems\n(NIPS), page 6000–6010, 2017. ISBN 9781510860964.\n[42] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A\nneural image caption generator. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3156–3164, 2015.\n[43] Dong Wang, Di Hu, Xingjian Li, and Dejing Dou. Temporal relational modeling with\nself-supervision for action segmentation. InAAAI Conference on Artiﬁcial Intelligence,\n2021.\n[44] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-\ndeeplab: End-to-end panoptic segmentation with mask transformers. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), 2021.\n[45] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. CoRR, abs/2102.12122, 2021. URL https:\n//arxiv.org/abs/2102.12122.\n[46] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao\nShen, and Huaxia Xia. End-to-end video instance segmentation with transformers.\nCoRR, abs/2011.14503, 2020. URL https://arxiv.org/abs/2011.14503.\n[47] Zhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li, and Gangshan Wu. Boundary-\naware cascade networks for temporal action segmentation. In European Conference on\nComputer Vision (ECCV), 2020.\n[48] Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: Region convolutional 3d network\nfor temporal activity detection. In IEEE International Conference on Computer Vision\n(ICCV), pages 5794–5803, 2017.\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 15\n[49] Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, and\nLi Fei-Fei. Every moment counts: Dense detailed labeling of actions in complex\nvideos. International Journal of Computer Vision (IJCV), 126(2):375–389, 2018.\n[50] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis E. H. Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. CoRR, abs/2101.11986, 2021.\n[51] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Al-\nberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.\nBig bird: Transformers for longer sequences. Advances in Neural Information Pro-\ncessing Systems, 33, 2020.\n[52] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,\nYanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking\nsemantic segmentation from a sequence-to-sequence perspective with transformers. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n[53] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. De-\nformable DETR: deformable transformers for end-to-end object detection. CoRR,\nabs/2010.04159, 2020. URL https://arxiv.org/abs/2010.04159.\nReferences\n[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all\nyou need for video understanding? CoRR, abs/2102.05095, 2021. URL https:\n//arxiv.org/abs/2102.05095.\n[2] João Carreira and Andrew Zisserman. Quo Vadis, action recognition? a new model and\nthe kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4724–4733, 2017.\n[3] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia\nDeng, and Rahul Sukthankar. Rethinking the faster r-cnn architecture for temporal\naction localization. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1130–1139, 2018.\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-\nscale vision transformer for image classiﬁcation. CoRR, abs/2103.14899, 2021. URL\nhttps://arxiv.org/abs/2103.14899.\n[5] Min-Hung Chen, Baopu Li, Yingze Bao, and Ghassan AlRegib. Action segmentation\nwith mixed temporal domain adaptation. In IEEE Winter Conference on Applications\nof Computer Vision (WACV), pages 594–603, 2020.\n[6] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship be-\ntween self-attention and convolutional layers. InInternational Conference on Learning\nRepresentations (ICLR), 2020.\n16 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\n[7] Rui Dai, Luca Minciullo, Lorenzo Garattoni, Gianpiero Francesca, and François Bre-\nmond. Self-attention temporal convolutional network for long-term daily living activ-\nity detection. In IEEE International Conference on Advanced Video and Signal Based\nSurveillance (AVSS), pages 1–7, 2019.\n[8] Li Ding, Chenliang Xu, and Juergen Gall. Weakly-supervised action segmentation\nwith iterative soft boundary assignment. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), page 6508–6516, 2018.\n[9] Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Ser-\ngio Guadarrama, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional\nnetworks for visual recognition and description.IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 39(4):677–691, 2017.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. CoRR, abs/2010.11929, 2020. URL\nhttps://arxiv.org/abs/2010.11929.\n[11] Yazan Abu Farha and Jurgen Gall. MS-TCN: multi-stage temporal convolutional net-\nwork for action segmentation. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3575–3584, 2019.\n[12] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to recognize objects in\negocentric activities. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 3281–3288, 2011.\n[13] Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes. Fine-\ngrained action segmentation using the semi-supervised action GAN. Pattern Recogni-\ntion, 98:107039, 2020.\n[14] Shang-Hua Gao1, Qi Han, Zhong-Yu Li, Pai Peng, Liang Wang, and Ming-Ming\nCheng. Global2local: Efﬁcient structure search for video action segmentation. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n[15] Yifei Huang, Yusuke Sugano, and Yoichi Sato. Improving action segmentation via\ngraph-based temporal reasoning. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 14021–14031, 2020. doi: 10.1109/CVPR42600.2020.\n01404.\n[16] Noureldien Hussein, Efstratios Gavves, and Arnold W.M. Smeulders. Timeception\nfor complex action recognition. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 254–263, 2019.\n[17] Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, and Hirokatsu Kataoka. Alleviating\nover-segmentation errors by detecting action boundaries. In IEEE Winter Conference\non Applications of Computer Vision (WACV), 2020.\n[18] Svebor Karaman, Lorenzo Seidenari, and Alberto Del Bimbo. Fast saliency based\npooling of ﬁsher encoded dense trajectories. In European Conference on Computer\nVision (ECCV), THUMOS Workshop, 2014.\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 17\n[19] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering\nthe syntax and semantics of goal-directed human activities. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 780–787, 2014.\n[20] Hilde Kuehne, Juergen Gall, and Thomas Serre. An end-to-end generative framework\nfor video segmentation and recognition. In IEEE Winter Conference on Applications\nof Computer Vision (WACV), pages 1–8, 2016.\n[21] Hilde Kuehne, Alexander Richard, and Juergen Gall. Weakly supervised learning of\nactions from transcripts. Computer Vision and Image Understanding, 163:78–89, 2017.\n[22] Colin Lea, Austin Reiter, René Vidal, and Gregory D. Hager. Segmental spatiotempo-\nral cnns for ﬁne-grained action segmentation. In European Conference on Computer\nVision (ECCV), page 36–52, 2016.\n[23] Colin Lea, Michael D. Flynn, René Vidal, Austin Reiter, and Gregory D. Hager. Tem-\nporal convolutional networks for action segmentation and detection. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pages 1003–1012, 2017.\n[24] Peng Lei and Sinisa Todorovic. Temporal deformable residual networks for action seg-\nmentation in videos. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 6742–6751, 2018.\n[25] Xin Li, Tianwei Lin, Xiao Liu, Wangmeng Zuo, Chao Li, Xiang Long, Dongliang\nHe, Fu Li, Shilei Wen, and Chuang Gan. Deep concept-wise temporal convolutional\nnetworks for action localization. In Proceedings of the ACM International Conference\non Multimedia (MM), page 4004–4012, 2020.\n[26] Tianwei Lin, Xu Zhao, and Zheng Shou. Single shot temporal action detection. In Pro-\nceedings of the ACM International Conference on Multimedia (MM) , page 988–996,\n2017.\n[27] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-\nYang Fu, and Alexander C. Berg. SSD: Single shot multibox detector. In European\nConference on Computer Vision (ECCV), pages 21–37, 2016.\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. Swin transformer: Hierarchical vision transformer using shifted win-\ndows. CoRR, abs/2103.14030, 2021. URL https://arxiv.org/abs/2103.\n14030.\n[29] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals,\nRajat Monga, and George Toderici. Beyond short snippets: Deep networks for video\nclassiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4694–4702, 2015.\n[30] Hamed Pirsiavash and Deva Ramanan. Parsing videos of actions with segmental gram-\nmars. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page\n612–619, 2014.\n[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-\ntime object detection with region proposal networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 39:1137–1149, 2017.\n18 ASFORMER: TRANSFORMER FOR ACTION SEGMENTATION\n[32] Alexander Richard and Juergen Gall. Temporal action detection using a statistical\nlanguage model. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 3131–3140, 2016.\n[33] Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka, and Bernt Schiele. A\ndatabase for ﬁne grained activity detection of cooking activities. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages 1194–1201, 2012.\n[34] Qinfeng Shi, Li Cheng, Li Wang, and Alex Smola. Human action segmentation and\nrecognition using discriminative semi-Markov models. International Journal of Com-\nputer Vision (IJCV), 93:22–32, 2011.\n[35] Bharat Singh, Tim K. Marks, Michael Jones, Oncel Tuzel, and Ming Shao. A multi-\nstream bi-directional recurrent neural network for ﬁne-grained action detection. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1961–\n1970, 2016.\n[36] Dipika Singhania, Rahul Rahaman, and Angela Yao. Coarse to ﬁne multi-resolution\ntemporal convolutional network. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021.\n[37] Sebastian Stein and Stephen J. McKenna. Combining embedded accelerometers with\ncomputer vision for recognizing food preparation activities. InProceedings of ACM In-\nternational Joint Conference on Pervasive and Ubiquitous Computing, page 729–738,\n2013.\n[38] Kevin Tang, Li Fei-Fei, and Daphne Koller. Learning latent temporal structure for com-\nplex event detection. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1250–1257, 2012.\n[39] Lingling Tao, Luca Zappella, Gregory Hager, and René Vidal. Surgical gesture seg-\nmentation and recognition. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), volume 16, pages 339–46, 09 2013.\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation\nthrough attention. CoRR, abs/2012.12877, 2020. URL https://arxiv.org/\nabs/2012.12877.\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, undeﬁnedukasz Kaiser, and Illia Polosukhin. Attention is all you need. InPro-\nceedings of the International Conference on Neural Information Processing Systems\n(NIPS), page 6000–6010, 2017. ISBN 9781510860964.\n[42] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A\nneural image caption generator. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3156–3164, 2015.\n[43] Dong Wang, Di Hu, Xingjian Li, and Dejing Dou. Temporal relational modeling with\nself-supervision for action segmentation. InAAAI Conference on Artiﬁcial Intelligence,\n2021.\nASFORMER: TRANSFORMER FOR ACTION SEGMENTATION 19\n[44] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-\ndeeplab: End-to-end panoptic segmentation with mask transformers. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), 2021.\n[45] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. CoRR, abs/2102.12122, 2021. URL https:\n//arxiv.org/abs/2102.12122.\n[46] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao\nShen, and Huaxia Xia. End-to-end video instance segmentation with transformers.\nCoRR, abs/2011.14503, 2020. URL https://arxiv.org/abs/2011.14503.\n[47] Zhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li, and Gangshan Wu. Boundary-\naware cascade networks for temporal action segmentation. In European Conference on\nComputer Vision (ECCV), 2020.\n[48] Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: Region convolutional 3d network\nfor temporal activity detection. In IEEE International Conference on Computer Vision\n(ICCV), pages 5794–5803, 2017.\n[49] Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, and\nLi Fei-Fei. Every moment counts: Dense detailed labeling of actions in complex\nvideos. International Journal of Computer Vision (IJCV), 126(2):375–389, 2018.\n[50] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis E. H. Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. CoRR, abs/2101.11986, 2021.\n[51] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Al-\nberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.\nBig bird: Transformers for longer sequences. Advances in Neural Information Pro-\ncessing Systems, 33, 2020.\n[52] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,\nYanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking\nsemantic segmentation from a sequence-to-sequence perspective with transformers. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n[53] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. De-\nformable DETR: deformable transformers for end-to-end object detection. CoRR,\nabs/2010.04159, 2020. URL https://arxiv.org/abs/2010.04159.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7791746854782104
    },
    {
      "name": "Computer science",
      "score": 0.7272963523864746
    },
    {
      "name": "Transformer",
      "score": 0.6421688795089722
    },
    {
      "name": "Encoder",
      "score": 0.5703813433647156
    },
    {
      "name": "Artificial intelligence",
      "score": 0.533051609992981
    },
    {
      "name": "Locality",
      "score": 0.4856177866458893
    },
    {
      "name": "Prior probability",
      "score": 0.458638995885849
    },
    {
      "name": "Task (project management)",
      "score": 0.4270114302635193
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4205387234687805
    },
    {
      "name": "Machine learning",
      "score": 0.38080012798309326
    },
    {
      "name": "Bayesian probability",
      "score": 0.10314229130744934
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}