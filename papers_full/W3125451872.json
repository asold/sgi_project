{
  "title": "Where do Clinical Language Models Break Down? A Critical Behavioural Exploration of the ClinicalBERT Deep Transformer Model",
  "url": "https://openalex.org/W3125451872",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5007303087",
      "name": "Alexander MacLean",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A5034161060",
      "name": "Alexander Wong",
      "affiliations": [
        "University of Waterloo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W3013681994",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6884859834",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3092190524",
    "https://openalex.org/W2156235098",
    "https://openalex.org/W3098467034"
  ],
  "abstract": "&#x0D; The introduction of Bidirectional Encoder Representations from Transformers (BERT) was a major breakthrough for transfer learning in natural language processing, enabling state-of-the-art performance across a large variety of complex language understanding tasks. In the realm of clinical language modeling, the advent of BERT led to the creation of ClinicalBERT, a state-of-the-art deep transformer model pretrained on a wealth of patient clinical notes to facilitate for downstream predictive tasks in the clinical domain. While ClinicalBERT has been widely leveraged by the research community as the foundation for building clinical domain-specific predictive models given its overall improved performance in the Medical Natural Language inference (MedNLI) challenge compared to the seminal BERT model, the fine-grained behaviour and intricacies of this popular clinical language model has not been well-studied. Without this deeper understanding, it is very challenging to understand where ClinicalBERT does well given its additional exposure to clinical knowledge, where it doesn't, and where it can be improved in a meaningful manner. Motivated to garner a deeper understanding, this study presents a critical behaviour exploration of the ClinicalBERT deep transformer model using MedNLI challenge dataset to better understanding the following intricacies: 1) decision-making similarities between ClinicalBERT and BERT (leverage a new metric we introduce called Model Alignment), 2) where ClinicalBERT holds advantages over BERT given its clinical knowledge exposure, and 3) where ClinicalBERT struggles when compared to BERT. The insights gained about the behaviour of ClinicalBERT will help guide towards new directions for designing and training clinical language models in a way that not only addresses the remaining gaps and facilitates for further improvements in clinical language understanding performance, but also highlights the limitation and boundaries of use for such models.&#x0D;",
  "full_text": "Where do Clinical Language Models Break Down? A Critical Behavioural Exploration of the\nClinicalBERT Deep Transformer Model\nAlexander MacLean Vision and Image Processing Group, University of Waterloo\nAlexander Wong Vision and Image Processing Group, University of Waterloo\nEmail: {alex.maclean, a28wong}@uwaterloo.ca\nAbstract\nThe introduction of Bidirectional Encoder Representations from\nTransformers (BERT) was a major breakthrough for transfer learn-\ning in natural language processing (NLP), enabling state-of-the-art\nperformance across a large variety of complex language under-\nstanding tasks. In the realm of clinical language modeling, the ad-\nvent of BERT led to the creation of ClinicalBERT, a state-of-the-art\ndeep transformer model pretrained on a wealth of patient clinical\nnotes to facilitate for downstream predictive tasks in the clinical do-\nmain. While ClinicalBERT has been widely leveraged by the re-\nsearch community as the foundation for building clinical domain-\nspeciﬁc predictive models given its overall improved performance in\nthe Medical Natural Language inference (MedNLI) challenge com-\npared to the seminal BERT model, the ﬁne-grained behaviour and\nintricacies of this popular clinical language model has not been\nwell-studied. Without this deeper understanding, it is very chal-\nlenging to understand where ClinicalBERT does well given its addi-\ntional exposure to clinical knowledge, where it doesn’t, and where\nit can be improved in a meaningful manner. Motivated to garner\na deeper understanding, this study presents a critical behaviour ex-\nploration of the ClinicalBERT deep transformer model using MedNLI\nchallenge dataset to better understanding the following intricacies:\n1) decision-making similarities between ClinicalBERT and BERT\n(leverage a new metric we introduce called Model Alignment), 2)\nwhere ClinicalBERT holds advantages over BERT given its clinical\nknowledge exposure, and 3) where ClinicalBERT struggles when\ncompared to BERT. The hope is the insights gained about the be-\nhaviour of ClinicalBERT will help guide towards new directions for\ndesigning and training clinical language models in a way that not\nonly addresses the remaining gaps and facilitates for further im-\nprovements in clinical language understanding performance, but\nalso highlights the limitation and boundaries of use for such mod-\nels.\n1 Introduction\nThe introduction of Bidirectional Encoder Representations from\nTransformers (BERT) [1] has been seen as a recent watershed\nmoment in transfer learning in natural language processing (NLP),\nand has facilitated state-of-the-art performance across a wide va-\nriety of complex natural language understanding tasks in recent\nyears. By demonstrating the ability to learn powerful general pur-\npose language models using large-scale unlabeled text corpus,\nBERT enables greatly improved supervised learning of downstream\nlanguage understanding tasks with much smaller task-speciﬁc text\ncorpus by taking advantage of the wealth of contextual relationships\ngarnered from large general purpose language corpus. In areas\nwhere domain speciﬁc terms and language are common, however,\nthe general-purpose BERT language models can have problems\nshifting to the new distribution of tokens and the effectiveness of\ntransferring learning is reduced [2]. Clinical language modelling,\nsuch as modelling clinical notes or predicting hospital readmission,\nis one such area, since there is a signiﬁcant amount of unique ter-\nminology, notably medical coding, which would not have been seen\nin a general pre-training corpus [3].\nTo combat this challenge, Alsentzer et al. [2] proposed a new\ndeep transformer language model, ClinicalBERT, which was trained\nin the same manner of BERT but on a large corpus of clinical data,\nnamely the MIMIC-III dataset [4, 5]. [2] shows that ClinicalBERT\ngenerally outperforms the seminal BERT model [1] on down-stream\ntasks using medical terminology such as the Medical Natural Lan-\nguage inference (MedNLI) challenge. As a result, ClinicalBERT\nhas become a popular foundation used by the reserach commu-\nnity for building clinical domain-speciﬁc predictive models. However,\nwhile the authors of ClinicalBERT focused analysis solely on over-\nall quantitative performance, there was little exploration into under-\nstanding the ﬁne-grained behaviour and intricacies of ClinicalBERT\nto derive deeper contextual insights. Conducting such ﬁne-grained\nbehavioural explorations on clinical language models is useful in\nmany ways. First of all, clinicians are often skeptical of releasing\ncontrol in clinical decision making, so being able to explain the be-\nhaviour of models is an important step in instilling trust in relevant\nstakeholders [6]. Additionally, such behavioural explorations can re-\nsult in better understanding of the context of success and failure of\nmodels, especially in comparison to related models. This knowl-\nedge can then provide not only direction for future development and\nimprovements in clinical language understanding performance, but\nalso highlight the limitation and boundaries of use for such models.\nMotivated to garner a deeper understanding, this study presents a\ncritical behaviour exploration of the ClinicalBERT deep transformer\nmodel using MedNLI challenge dataset to better understanding the\nfollowing intricacies: 1) decision-making similarities between Clini-\ncalBERT and BERT, 2) where ClinicalBERT holds advantages over\nBERT given its clinical knowledge exposure, and 3) where Clini-\ncalBERT struggles when compared to BERT. To the best of the\nauthors’ knowledge, such an exploration has not been previously\nexplored in research literature and can provide a much better un-\nderstanding into where such clinical language models succeed and\nwhere they break down.\n1.1 Related Work\nAlsentzer et al. [2] were inﬂuenced by the BioBERT model, which\nattempted to solve a similar problem in the context of biomedi-\ncal science research rather than clinical medicine [7]. BioBERT\nwas initialized using weights from BERT -base, the BERT model\nwith 12 attention heads, 12 attention layers, and 110 million pa-\nrameters in total [7]. [2] experimented with ClinicalBERT initial-\nized both from BioBERT and BERT, thus each model in the ex-\nperiments has the exact same architecture, and found that the\nBioBERT -ClinicalBERT generally outperformed BERT and Clinical-\nBERT without the biomedical text corpus training, likely due to the\nat least partial similarities of biomedical text to clinical text. It is that\nformer model which was selected to be examined in this study, and\nto which ClinicalBERT will refer in the rest of this analysis.\n2 Methodology\n2.1 Task\nThe Medical Natural Language Inference (MedNLI) challenge was\nselected [5, 8] in this study as the basis for critical behavioural ex-\nploration in order to evaluate the performance of ClinicalBERT by\nﬁne-tuning the model on a speciﬁc task. The MedNLI challenge\nconsists of two statements, with the goal for the model to determine\nhow the second statement relates to the ﬁrst. There are three possi-\nbilities: Entailment, meaning that the second statement can be log-\nically deduced from the ﬁrst; Contradiction, the second statement\ncannot logically be true based on the ﬁrst; and Neutral, where there\nis no relation between the veracity of the ﬁrst and second state-\nments. Examples of some of these statement pairs can be found\nin Tables 2 and 3 along with further analysis. The MedNLI dataset\nis split into 11,232 training samples, 1,395 validation samples, and\n1,422 testing samples.\n2.2 Research Questions\nMotivated by the desire for a better qualitative understanding of Clin-\nicalBERT, three research questions were explored in this critical be-\nhavioural exploration: 1) What are the decision making similarities\nbetween ClinicalBERT and BERT? 2) In which contexts is Clinical-\nBERT improving upon on BERT given its clinical knowledge expo-\nsure?, and 3) Where does ClinicalBERT struggle in comparison to\nBERT?\n2.2.1 Decision Making Similarities\nIt has been shown, in many contexts including with clinical text, that\npre-training on relevant corpora improved absolute quantitative per-\nformance on ﬁne-tuned downstream tasks when looking at accu-\nracy and related metrics [2]. However, these values do not differen-\ntiate between cases where improved models build on the successes\nof the ones to which they are compared, or if they are just simply\ncorrect on a different subset of \"difﬁcult\" samples. To this end, in\naddition to studying confusion matrices, we introduce a new metric\ncalled Model Agreement to better evaluate decision making sim-\nilarities. Model Agreement is calculated in the same manner as\nmodel accuracy, but using the predictions of the two models on the\ntest set rather than comparing the predictions to the baseline labels.\nThis is shown in Eq. 1:\nModel Agreement = Samples classiﬁed the same by both models\nTotal number of test samples\n(1)\nIn this way, a value higher than the accuracy of an individual\nmodel and close to 1 suggests that the two models are consistent\nin their decision making, with the variation coming from a few mis-\nclassiﬁed samples by one model being corrected in the output of\nthe other models. As the value decreases, more and more of the er-\nrors are different across the two models, meaning that even though\none model has higher overall performance, there are a signiﬁcant\namount of its errors which are correct in the output of the other\nmodel. Investigating this behaviour will lead to insights into the ef-\nfectiveness of the models.\n2.2.2 Sample-level Model Disagreement Analysis\nTo obtain a much ﬁner-grain understanding into the behavioural in-\ntricacies of ClinicalBERT, we further conduct a sample-level analy-\nsis of speciﬁc model disagreement scenarios where ClinicalBERT\nexhibits differing predictive behaviour when compared to the semi-\nnal BERT model. By studying the areas of model disagreement, one\ncan gain much deeper behavioural insights into: 1) the strengths\nof ClinicalBERT when dealing with clinical language understand-\ning tasks (where ClinicalBERT leads to a correct prediction while\nBERT does not), and 2) the limitations of ClinicalBERT (where Clin-\nicalBERT leads to an incorrect prediction while BERT provides a\ncorrect one).\n3 Results and Discussion\nThe BERT -Base and ClinicalBERT models were trained on the\nMedNLI challenge dataset for 50 epochs using the Adam optimizer\nwithin the Keras deep learning environment. Confusion matrices for\nthe two models for the testing set are shown in Figure 1 and Figure\n2 respectively. Additionally, both standard performance statistics\n(e.g., accuracy, precision, recall, and F1-score) along with the pro-\nposed Model Agreement scores are shown in Table 1.\nFig. 1: BERT Confusion Matrix.\nFig. 2: ClinicalBERT Confusion Matrix.\nModel BERT ClinicalBERT\nClass Entailment\nPrecision 0.7391 0.7515\nRecall 0.7532 0.7975\nF1-score 0.7461 0.7738\nClass Contradiction\nPrecision 0.8305 0.8574\nRecall 0.8376 0.8755\nF1-score 0.8340 0.8663\nClass Neutral\nPrecision 0.7462 0.7977\nRecall 0.7258 0.7321\nF1-score 0.7358 0.7635\nOverall Accuracy 0.7721 0.8017\nModel Agreement 0.8143\nTable 1: Results on Test Dataset.\n3.1 Decision Making Similarities\nAs seen in both the ﬁgures and Table 1, much of ClinicalBERT’s\nimprovements over BERT come in the form of fewer Entailment and\nContradiction samples being classiﬁed as Neutral. The largest de-\ncreases in off-diagonal elements in ClinicalBERT’s confusion matrix\nare seen where those classes were predicted as Neutral. Similarly,\nthe individual class statistic which increased the most was precision\nfor the Neutral case. Precision is deﬁned as TP\nTP+FP , so Clinical-\nBERT led to fewer false Neutral predictions more than it improved\non any other case.\nAdditionally, Table 1 contains the Model Agreement score. This\nvalue of 0.8143 is slightly higher than the accuracy of either model,\nbut still far from 1, suggesting that a signiﬁcant amount of cases\nwhere ClinicalBERT is incorrect are cases where BERT was suc-\ncessful, and vice versa. If that were not the case, and every\nsample that BERT predicted accurately were successfully classi-\nﬁed as well by ClinicalBERT, then Model Agreement would instead\nbe 1 −(accClinicalBERT −accBERT ) = 1 −(0.8017 −0.7721) = 0.9704,\nwith the only differences coming from cases where ClinicalBERT\nimproved over BERT. At the other extreme, where every error in\nClinicalBERT was predicted successfully by BERT, Model Agree-\nment would then be 1 −((1 −accClinicalBERT ) + (1 −accBERT )) =1 −\n((1 −0.8017) + (1 −0.7721)) = 0.5738. In this case, ClinicalBERT\nﬁxed every error made by BERT, but for almost every one made a\nnew error; the times where a new error was not made account for\nthe increased overall accuracy. 0.8143 falls partway between these\ntwo extremes, showing that some errors were consistent across the\ntwo models, while others differed. This result suggests that the\nthird research question, in which cases did ClinicalBERT struggle\nin comparison to BERT, is in fact relevant and should be studied\nclosely.\nThe following tables show selected test samples to illustrate model\nbehaviour. Table 2 displays samples whose label is Entailment, but\nwhich were misclassiﬁed by one or both of ClinicalBERT and BERT.\nTable 3 does the same, but for test samples whose label is Contra-\ndiction.\nSample Sentence1 Sentence2 BERT ClinicalBERT\nE1 The patient stopped intravenous\nﬂuids and got 10 mg of intra-\nvenous Lasix times three and put\nout 500 cc of urine output.\nThe patient received too much\nﬂuid.\nCONTRADICTION NEUTRAL\nE2 DM x 20yrs 4. Patient has elevated blood glu-\ncose\nNEUTRAL NEUTRAL\nE3 Labs notable for WBC 10.5 with-\nout bands, Hct 32.2 (prior base-\nline mid to upper 20s), Cr 0.9, CE\nneg X 1, and lactate 1.5.\nThe patient does not have an in-\nfection.\nCONTRADICTION ENTAILMENT\nE4 She was found to have new on-\nset a.ﬁb w/ rate in the 120’s to\n130’s and lateral ST depressions\nc/w demand ischemia.\nThe patient has coronary artery\ndisease.\nNEUTRAL ENTAILMENT\nE5 En route to the Emergency De-\npartment, she developed worsen-\ning substernal chest pain without\nany radiation.\npatient has an acute MI ENTAILMENT NEUTRAL\nE6 She was evaluated by neuro-\nsurgery, deemed to be intact neu-\nrologically.\nNo history of cerebrovascular ac-\ncidents\nENTAILMENT CONTRADICTION\nTable 2: Examples of failed test samples where the label is Entailment, with the predicted labels from each of the models also provided.\nSample Sentence1 Sentence2 BERT ClinicalBERT\nC1 Diastolic CHF , LVEF >70% 2/06\n10.\nPatient has angina ENTAILMENT ENTAILMENT\nC2 (Lactate only 1.3 and pt afebrile). Elevated temperature NEUTRAL ENTAILMENT\nC3 Pt saw PCP next day and atenolol\nwas stopped but no further w/u\ndone (ie scans/xray) for fall on [**\nLocation **]us day.\nThe patient is kept on a beta\nblocker.\nNEUTRAL CONTRADICTION\nC4 Because neonatology was not\npresent at delivery, resuscitation\nwas initiated by the labor and de-\nlivery nurses.\nThe patient did not need any addi-\ntional help after birth.\nENTAILMENT CONTRADICTION\nC5 She was discharged on bed rest\nand treated with terbutaline.\nPatient is no longer taking medi-\ncations\nCONTRADICTION NEUTRAL\nC6 He returned to [**Hospital 8682**]\nclinic three weeks later and was\nprescribed antibiotics\nthe patient is not infected CONTRADICTION ENTAILMENT\nTable 3: Examples of failed test samples where the label is Contradiction, with the predicted labels from each of the models also provided.\n3.2 Sample-level Model Disagreement Analysis\nWhile it is not possible to capture all possible patterns with only a\nfew samples, some patterns do seem to be emerging. The ﬁrst\ncase to be considered is when both models have incorrect predic-\ntions. In each case shown, samples E1, E2, C1, and C2, there\nis not only unique clinical terminology, but also clinical diagnostic\nknowledge that is relevant to the logical connection between the\ntwo statements. Thus, even if the model is able to interpret correctly\nwhat \"Lasix\" and \"cc\" (E1), \"DM\" (E2), \"CHF\" and \"LVEF\" (C1), and\n\"pt afebrile\" (C2) all mean, the meaning of the following sentences\nare additionally dependent on the quantities contained in the initial\nones. There could have been training samples with the same ter-\nminology, but with different quantities, and the model would need\nto be able to interpret those relationships. In a sense, the model\nwould need to see enough samples with differing quantities to build\nan internal \"classiﬁer\" to know that \"LVEF >70%\" does not indicate\nthat the patient has angina (C1) or that \"500 cc of urine output\" is\nrelated to too much input of ﬂuid (E1).\nRegarding cases where ClinicalBERT is correct while BERT is not,\nwhich are samples E3, E4, C3, and C4, they tended to be long,\ncomplex statements with signiﬁcant amounts of clinical terminol-\nogy, with the extra pre-training for ClinicalBERT perhaps allowing it\nto better follow the connections between the various terms within a\nsentence as well as between sentences. Sample E3 contains plenty\nof terms (WBC, Hct, Cr, CE) which are unique to clinical text, and\nMedNLI training alone may not have allowed BERT to recognize the\nmeaning of those terms in the context of infection. In sample C3,\nBERT may not have connected the term \"atenolol\" to beta blockers,\nand defaulted to the sentences being neutral, while ClinicalBERT\nwas able to identify that relationship properly. Similar discussion\ncan be had for the terms \"ischemia\" and \"coronary artery disease\"\n(E4). In sample C4, BERT’s error in labelling the sample as Entail-\nment shows that it understood the birth-related terms (\"neonatol-\nogy\", \"delivery\", \"labor\") perhaps could not determine that the focus\nof the logic was in fact on \"resuscitation\" and \"additional help\" which\nClinicalBERT successfully identiﬁed.\nIn examining these cases, a ﬁtting analogy can be found. When\nBERT undergoes pre-training, it is essentially a student moving\nthrough education and life, learning grammatical structures and\nmeanings of words, especially how they vary based on context.\nOnce graduated high school, an individual can make sense of most\nsources of text in their native language, at least when the distribu-\ntion of terms is familiar to them. However, when they are thrust\ninto a situation where terms are new to them, or used in new ways,\nthey have a much harder time understanding what they are read-\ning or hearing. If a recent high school graduate were shown the\nMedNLI dataset, they would likely have a hard time succeeding -\nnot because they cannot read English, but instead because they of-\nten cannot make the proper connections between the unique terms\nthat are important to the meanings of the various statements, even\nif they had taken a high school biology course or watched Grey’s\nAnatomy. On the other hand, ClinicalBERT has essentially been\n\"sent\" to medical school - by training it on the MIMIC-III data, it\nwas introduced to domain speciﬁc terminology that help it to per-\nform its future function. It has not lost the knowledge gained during\n\"high school\", which is the general understanding of the English\nlanguage, and has learned how clinical terminology ﬁts into its un-\nderstanding of language. Since [2] used BioBERT as the initial-\nization for ClinicalBERT, we can go one step further and say that\nBioBERT was analogous to a high school student who went to uni-\nversity and completed an undergraduate degree in biomedical sci-\nence. It learned terminology required for understanding relevant\ntexts, in addition to its previous general education. By initializing\nwith BioBERT, [2] sent a biomedical science graduate to medical\nschool, and this contextualizes why Bio-ClinicalBERT provided bet-\nter results for those authors; there is a reason why so many medical\nstudents come from related educational backgrounds, as the rel-\nevant background can make it easier to acquire medicine speciﬁc\nknowledge.\nIn the ﬁnal cases, E5, E6, C5, and C6, the statements tended to be\nsimpler and had fewer terms speciﬁc to clinical terminology. This\nmay show why BERT was able to be successful, but it is less clear\nwhy ClinicalBERT had difﬁculties. Of note, most of these failures\nwhere the true label was Entailment were misclassiﬁed as Neutral,\nwhile most of the Contradictions were wrongly labelled Entailment.\nIn the former cases, ClinicalBERT made errors by believing that\nthe statements were not related, possibly because the model did\nnot ﬁnd a strong enough connection between them. In any case,\nit rarely falsely believed the statements to be contradictory, which\nwould have been a greater error. In the latter case, ClinicalBERT\ninstead understood that the statements were related, but failed in\nunderstanding the manner of the connection by saying that the sec-\nond was true based on the ﬁrst. Many of these examples contained\nnegation in one of the statements (\"not\", \"no symptoms\", \"no re-\ncent history\", etc.) suggesting that failure was due to the model\nincorrectly understanding the purpose of that speciﬁc negation. Un-\nfortunately, these insights do not provide an explanation as to why\nBERT was successful in these cases despite the same challenges\nbeing present.\n3.3 Recommendations\nThe results of this study emphasize the importance of understand-\ning the context of what a model is being asked to do. As shown in\nexamples E1, E2, C1, and C2, ClinicalBERT’s improvements come\nfrom an improved understanding of clinical language; in essence,\nits time at medical school focused on the meanings of clinical ter-\nminology and while some decision making processes could be em-\nbedded in that understanding, there likely are not enough training\nexamples in MIMIC-III or the MedNLI datasets to trust its output in\nthose situations. Recognizing these limitations, what may be more\neffective is integrating such a language model into larger systems,\nwhereby ClinicalBERT can interpret incoming information and for-\nward it to other subsystems which are designed for such a task,\neither via clinician-deﬁned rulesets or possibly other machine learn-\ning models.\nAnother area of future study would be to investigate the MedNLI\nerror cases with clinicians to understand if there are any further\npatterns emerging. Recent work has gone into examining the be-\nhaviour of BERT -type models on inference tasks, namely the Stan-\nford NLI task, and noticed that some of the samples in the data set\nare somewhat ambiguous [9, 10]. In their experiments, the authors\nsurveyed 100 individuals to acquire a distribution of human opin-\nions for a subset of test samples, and noticed that samples which\nhave higher disagreement across human participants generally are\npredicted less accurately by their BERT model [10]. In the case of\nMedNLI, it would be much harder to ﬁnd 100 individuals with the re-\nquired medical knowledge to reliably provide annotations for a sub-\nset of test samples, but performing similarly designed experiments\nwould provide an interesting comparison to this recent work. Dis-\ncussing said results with clinicians may lead to more actionable in-\nsights which can either validate ClinicalBERT’s performance despite\nerrors according to the structure of MedNLI, or guide development\nof architecture and data curation to further improve performance.\n4 Conclusion\nIn conclusion, this study examined the qualitative performance of\na publicly available ClinicalBERT model trained on the MIMIC-III\ndataset applied to the MedNLI challenge. During investigation, its\nadvantages over a model trained from BERT -Base were identiﬁed\nto occur when samples contain higher amounts of clincal language-\nspeciﬁc terminology, as expected. Additionally, by analyzing its fail-\nures the limitations of the model were explored, which is especially\ncritical for any technology used in clinical contexts. Applying BERT\nto new language domains has consistently shown to improve upon\nstate-of-the-art results, but blind application runs the risk of over-\nlooking unacceptable errors or routes to improvement. Hopefully,\nstudies such as this one will lead to improvements in design and\nbetter trust in application to the clinical domain, both of which will\nhave positive impacts on health outcomes.\nAcknowledgments\nThe authors would like to acknowledge Dr. Helen Chen from the\nWaterloo Health Information Systems and Technology Lab at the\nUniversity of Waterloo for providing initial motivation for the explo-\nration of this problem area.\nReferences\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” inProceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers). Minneapolis, Minnesota: Association for\nComputational Linguistics, Jun. 2019, pp. 4171–4186. [On-\nline]. Available: https://www.aclweb.org/anthology/N19-1423\n[2] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin,\nT. Naumann, and M. B. A. McDermott, “Publicly available clini-\ncal bert embeddings,” 2019.\n[3] R. Leaman, R. Khare, and Z. Lu, “Challenges in clinical natural\nlanguage processing for automated disorder normalization,”\nJournal of Biomedical Informatics , vol. 57, pp. 28–37, Oct.\n2015. [Online]. Available: https://doi.org/10.1016/j.jbi.2015.07.\n010\n[4] A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng,\nM. Ghassemi, B. Moody, P . Szolovits, L. A. Celi, and R. G.\nMark, “Mimic-iii, a freely accessible critical care database,”Sci-\nentiﬁc data, vol. 3, p. 160035, 2016.\n[5] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff,\nP . C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-\nK. Peng, and H. E. Stanley, “PhysioBank, PhysioToolkit,\nand PhysioNet: Components of a new research re-\nsource for complex physiologic signals,” Circulation, vol.\n101, no. 23, pp. e215–e220, 2000, circulation Electronic\nPages: http://circ.ahajournals.org/content/101/23/e215.full\nPMID:1085218; doi: 10.1161/01.CIR.101.23.e215.\n[6] M. Nagendran, Y . Chen, C. A. Lovejoy, A. C. Gordon,\nM. Komorowski, H. Harvey, E. J. Topol, J. P . A. Ioannidis, G. S.\nCollins, and M. Maruthappu, “Artiﬁcial intelligence versus\nclinicians: systematic review of design, reporting standards,\nand claims of deep learning studies,” BMJ, p. m689, Mar.\n2020. [Online]. Available: https://doi.org/10.1136/bmj.m689\n[7] J. Lee, W. Y oon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\n“Biobert: a pre-trained biomedical language representation\nmodel for biomedical text mining,” 2019.\n[8] C. Shivade, “Mednli - a natural language inference dataset\nfor the clinical domain,” Oct 2019. [Online]. Available:\nhttps://physionet.org/content/mednli/1.0.0/\n[9] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning,\n“A large annotated corpus for learning natural language\ninference,” in Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing. Lisbon,\nPortugal: Association for Computational Linguistics, Sep.\n2015, pp. 632–642. [Online]. Available: https://www.aclweb.\norg/anthology/D15-1075\n[10] Y . Nie, X. Zhou, and M. Bansal, “What can we learn from\ncollective human opinions on natural language inference\ndata?” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nOnline: Association for Computational Linguistics, Nov. 2020,\npp. 9131–9143. [Online]. Available: https://www.aclweb.org/\nanthology/2020.emnlp-main.734",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7150117754936218
    },
    {
      "name": "Transformer",
      "score": 0.638587474822998
    },
    {
      "name": "Inference",
      "score": 0.5862441062927246
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5699012875556946
    },
    {
      "name": "Language model",
      "score": 0.5506246089935303
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5486298203468323
    },
    {
      "name": "Realm",
      "score": 0.5111525654792786
    },
    {
      "name": "Deep learning",
      "score": 0.506456732749939
    },
    {
      "name": "Encoder",
      "score": 0.42805805802345276
    },
    {
      "name": "Language understanding",
      "score": 0.41179999709129333
    },
    {
      "name": "Machine learning",
      "score": 0.3873261511325836
    },
    {
      "name": "Data science",
      "score": 0.37448662519454956
    },
    {
      "name": "Natural language processing",
      "score": 0.3301208019256592
    },
    {
      "name": "Engineering",
      "score": 0.12528154253959656
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "cited_by": 1
}