{
  "title": "ProgPrompt: program generation for situated robot task planning using large language models",
  "url": "https://openalex.org/W4386215566",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2792274784",
      "name": "Ishika Singh",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2774829741",
      "name": "Valts Blukis",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2051658587",
      "name": "Arsalan Mousavian",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2117115203",
      "name": "Ankit Goyal",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2470867827",
      "name": "Danfei Xu",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2158164010",
      "name": "Jonathan Tremblay",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2231782831",
      "name": "Dieter Fox",
      "affiliations": [
        "Nvidia (United States)",
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2504342659",
      "name": "Jesse Thomason",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A1975861307",
      "name": "Animesh Garg",
      "affiliations": [
        "Nvidia (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2792274784",
      "name": "Ishika Singh",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2774829741",
      "name": "Valts Blukis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051658587",
      "name": "Arsalan Mousavian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117115203",
      "name": "Ankit Goyal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2470867827",
      "name": "Danfei Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158164010",
      "name": "Jonathan Tremblay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231782831",
      "name": "Dieter Fox",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2504342659",
      "name": "Jesse Thomason",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A1975861307",
      "name": "Animesh Garg",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3122114180",
    "https://openalex.org/W1978892650",
    "https://openalex.org/W2737470465",
    "https://openalex.org/W3207460231",
    "https://openalex.org/W2950885698",
    "https://openalex.org/W2337392266",
    "https://openalex.org/W3001865277",
    "https://openalex.org/W3206072662",
    "https://openalex.org/W4386065691",
    "https://openalex.org/W2161414194",
    "https://openalex.org/W6674383371",
    "https://openalex.org/W6761551260",
    "https://openalex.org/W3099130708",
    "https://openalex.org/W2951725892",
    "https://openalex.org/W2912694244",
    "https://openalex.org/W2883471708",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3134840027",
    "https://openalex.org/W2973223114",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W3214188292",
    "https://openalex.org/W3202187802",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W4221152231",
    "https://openalex.org/W2804010078",
    "https://openalex.org/W3207187156",
    "https://openalex.org/W4383046944",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W4320559489",
    "https://openalex.org/W6767372934",
    "https://openalex.org/W2964055695",
    "https://openalex.org/W3207057769"
  ],
  "abstract": "Abstract Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at progprompt.github.io",
  "full_text": "Autonomous Robots (2023) 47:999–1012\nhttps://doi.org/10.1007/s10514-023-10135-3\nPROG PROMPT : program generation for situated robot task planning\nusing large language models\nIshika Singh 1 · Valts Blukis 2 · Arsalan Mousavian 2 · Ankit Goyal 2 · Danfei Xu 2 · Jonathan Tremblay 2 ·\nDieter Fox 2,3 · Jesse Thomason 1 · Animesh Garg 2,4\nReceived: 1 May 2023 / Accepted: 3 August 2023 / Published online: 28 August 2023\n© The Author(s) 2023\nAbstract\nTask planning can require deﬁning myriad domain knowledge about the world in which a robot needs to act. To ameliorate\nthat effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate\naction sequences directly, given an instruction in natural language with no additional domain information. However, such\nmethods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions\nnot possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with\nprogram-like speciﬁcations of the available actions and objects in an environment, as well as with example programs that\ncan be executed. We make concrete recommendations about prompt structure and generation constraints through ablation\nexperiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical\nrobot arm for tabletop tasks. Website and code at progprompt.github.io\nKeywords Robot task planning · LLM code generation · Planning domain generalization · Symbolic planning\nB Ishika Singh\nishikasi@usc.edu\nV alts Blukis\nvblukis@nvidia.com\nArsalan Mousavian\namousavian@nvidia.com\nAnkit Goyal\nangoyal@nvidia.com\nDanfei Xu\ndanfeix@nvidia.com\nJonathan Tremblay\njtremblay@nvidia.com\nDieter Fox\ndieterf@nvidia.com\nJesse Thomason\njessetho@usc.edu\nAnimesh Garg\nanimeshg@nvidia.com\n1 Computer Science, University of Southern California, Los\nAngeles, CA 90089, USA\n2 Seattle Robotics Lab, NVIDIA, Seattle, W A 98105, USA\n1 Introduction\nEveryday household tasks require both commonsense under-\nstanding of the world and situated knowledge about the\ncurrent environment. To create a task plan for “Make din-\nner,” an agent needs common sense: object affordances, such\nas that the stove and microwave can be used for heating;\nlogical sequences of actions , such as an oven must be pre-\nheated before food is added; and task relevance of objects\nand actions, such as heating and food are actions related to\n“dinner” in the ﬁrst place. However, this reasoning is infea-\nsible without state feedback. The agent needs to know what\nfood is available in the current environment, such as whether\nthe freezer contains ﬁsh or the fridge contains chicken.\nAutoregressive large language models (LLMs) trained\non large corpora to generate text sequences conditioned on\ninput prompts have remarkable multi-task generalization.\nThis ability has recently been leveraged to generate plau-\n3 Computer Science and Engineering, University of\nWashington, Seattle, W A 98195, USA\n4 School of Interactive Computing, Georgia Institute of\nTechnology, Atlanta, GA 30308, USA\n123\n1000 Autonomous Robots (2023) 47:999–1012\nFig. 1 ProgPrompt leverages LLMs’ strengths in both world knowl-\nedge and programming language understanding to generate situated\ntask plans that can be directly executed\nsible action plans in context of robotic task planning (Ahn et\nal., 2022; Huang et al., 2022b, a; Zeng et al., 2022) by either\nscoring next steps or generating new steps directly. In scoring\nmode, the LLM evaluates an enumeration of actions and their\narguments from the space of what’s possible. For instance,\ngiven a goal to “Make dinner” with ﬁrst action being “open\nthe fridge”, the LLM could score a list of possible actions:\n“pick up the chicken”, “pick up the soda”, “close the fridge”,\n... , “turn on the lightswitch.” In text-generation mode, the\nLLM can produce the next few words, which then need to be\nmapped to actions and world objects available to the agent.\nFor example, if the LLM produced “reach in and pick up\nthe jar of pickles,” that string would have to neatly map to\nan executable action like “pick up jar.” A key component\nmissing in LLM-based task planning is state feedback from\nthe environment. The fridge in the house might not contain\nchicken, soda, or pickles, but a high-level instruction “Make\ndinner” doesn’t give us that world state information. Our\nwork introduces situated-awareness in LLM-based robot\ntask planning.\nWe introduce ProgPrompt, a prompting scheme that\ngoes beyond conditioning LLMs in natural language. Prog-\nPrompt utilizes programming language structures, leverag-\ning the fact that LLMs are trained on vast web corpora that\nincludes many programming tutorials and code documen-\ntation (Fig. 1). ProgPrompt provides an LLM a Pythonic\nprogram header with an import statement for available\nactions and their expected parameters, a list of environ-\nment objects, and function deﬁnitions like make_dinner\nwhose bodies are sequences of actions operating on objects.\nWe incorporate situated state feedback from the environment\nby asserting preconditions of our plan, such as being close\nto the fridge before attempting to open it, and responding\nto failed assertions with recovery actions. What’s more, we\nshow that including natural language comments in Prog-\nPrompt programs to explain the goal of the upcoming action\nimproves task success of generated plan programs.\n2 Background and related work\n2.1 Task planning\nFor high-level planning, most works in robotics use search\nin a pre-deﬁned domain (Fikes and Nilsson, 1971; Jiang et\nal., 2018; Garrett et al., 2020). Unconditional search can be\nhard to scale in environments with many feasible actions\nand objects (Puig et al., 2018; Shridhar et al., 2020) due to\nlarge branching factors. Heuristics are often used to guide\nthe search (Baier et al., 2007; Hoffmann, 2001;H e l m e r t ,\n2006; Bryce and Kambhampati, 2007). Recent works have\nexplored learning-based task & motion planning, using meth-\nods such as representation learning, hierarchical learning,\nlanguage as planning space, learning compositional skills\nand more (Akakzia et al., 2021; Eysenbach et al., 2019; Jiang\net al., 2019; Kurutach et al., 2018; Mirchandani et al., 2021;\nNair and Finn, 2020; Shah et al., 2022; Sharma et al., 2022;\nSilver et al., 2022; Srinivas et al., 2018; Xu et al., 2018, 2019;\nZhu et al., 2020). Our method sidesteps search to directly\ngenerate a plan that includes conditional reasoning and error-\ncorrection.\nWe formulate task planning as the tuple ⟨O,P,A,T ,I,G,\nt⟩. O is a set of all the objects available in the environment, P\nis a set of properties of the objects which also informs object\naffordances, A is a set of executable actions that changes\ndepending on the current environment state deﬁned as s ∈ S.\nA state s is a speciﬁc assignment of all object properties, and\nS is a set of all possible assignments. T represents the tran-\nsition model T : S ×\nA → S, I and G are the initial and\ngoal states. The agent does not have access to the goal state\ng ∈ G, but only a high-level task description t.\nConsider the task t = “microwave salmon”. Task rele-\nvant objects microwave, salmon ∈ O will have properties\nmodiﬁed during action execution. For example, action a =\nopen(microwave) will change the state from closed\n(microwave) ∈ s to ¬closed(microwave) ∈ s\n′ if a is\nadmissible, i.e., ∃(a,s,s′) s.t. a ∈ A ∧s,s′ ∈ S ∧T (s,a) =\ns′. In this example a goal state g ∈ G could contain the con-\nditions heated(salmon) ∈ g, ¬closed(microwave) ∈ g\nand ¬switchedOn(microwave) ∈ g.\n2.2 Planning with LLMs\nA Large Language Model (LLM) is a neural network with\nmany parameters—currently hundreds of billions (Brown\net al., 2020; Chen et al., 2021)—trained on unsupervised\nlearning objectives such as next-token prediction or masked-\n123\nAutonomous Robots (2023) 47:999–1012 1001\nlanguage modelling. An autoregressive LLM is trained with\na maximum likelihood loss to model the probability of a\nsequence of tokens y conditioned on an input sequence x,\ni.e. θ = arg max\nθ P(y|x;θ), where θ are model param-\neters. The trained LLM is then used for prediction ˆy =\narg maxy∈S P(y|x;θ), where S is the set of all text sequences.\nSince search space S is huge, approximate decoding strate-\ngies are used for tractability (Holtzman et al., 2020; Luong\net al., 2015; Wiseman et al., 2017).\nLLMs are trained on large text corpora, and exhibit multi-\ntask generalization when provided with a relevant prompt\ninput x. Prompting LLMs to generate text useful for robot\ntask planning is a nascent topic (Ahn et al., 2022; Jansen,\n2020; Huang et al., 2022a, b;L ie ta l . ,2022; Patel and Pavlick,\n2022). Prompt design is challenging given the lack of paired\nnatural language instruction text with executable plans or\nrobot action sequences (Liu et al., 2021). Devising a prompt\nfor task plan prediction can be broken down into a prompting\nfunction and an answer search strategy (Liu et al., 2021).\nA prompting function, f\nprompt(.) transforms the input state\nobservation s into a textual prompt. Answer search is the\ngeneration step, in which the LLM outputs from the entire\nLLM vocabulary or scores a predeﬁned set of options.\nClosest to our work, Huang et al. ( 2022a) generates open-\ndomain plans using LLMs. In that work, planning proceeds\nby: 1) selecting a similar task in the prompt example ( f\nprompt);\n2) open-ended task plan generation (answer search); and\n3) 1:1 prediction to action matching. The entire plan is\ngenerated open-loop without any environment interaction ,\nand later tested for executability of matched actions. How-\never, action matching based on generated text doesn’t ensure\nthe action is admissible in the current situation. Inner-\nMonologue (Huang et al., 2022b) introduces environment\nfeedback and state monitoring, but still found that LLM\nplanners proposed actions involving objects not present in\nthe scene. Our work shows that a programming language-\ninspired prompt generator can inform the LLM of both\nsituated environment state and available robot actions, ensur-\ning output compatibility to robot actions.\nThe related SayCan (Ahn et al., 2022) uses natural lan-\nguage prompting with LLMs to generate a set of feasible\nplanning steps, re-scoring matched admissible actions using\na learned value function. SayCan constructs a set of all admis-\nsible actions expressed in natural language and scores them\nusing an LLM. This is challenging to do in environments\nwith combinatorial action spaces. Concurrent with our work\nare Socratic models (Zeng et al., 2022), which also use code-\ncompletion to generate robot plans. We go beyond (Zeng et\nal., 2022) by leveraging additional, familiar features of pro-\ngramming languages in our prompts. We deﬁne an f\nprompt\nthat includes import statements to model robot capabilities,\nnatural language comments to elicit common sense reason-\ning, and assertions to track execution state. Our answer search\nis performed by allowing the LLM to generate an entire, exe-\ncutable plan program directly.\n2.3 Recent developments following PROG PROMPT\nV emprala et al. ( 2023) further explores API-based planning\nwith ChatGPT1 in domains such as aerial robotics, manipula-\ntion and visual navigation. They discuss the design principles\nfor constructing interaction APIs, for action and perception,\nand prompts that can be used to generate code for robotic\napplications. Huang et al. ( 2023) builds on SayCan (Ahn et\nal., 2022) and generates planning steps token-by-token while\nscoring the tokens using both the LLM and the grounded\npretrained value function. Cao and Lee ( 2023) explores gen-\nerating behavior trees to study hierarchical task planning\nusing LLMs. Skreta et al. ( 2023) proposes iterative error\ncorrection via a syntax veriﬁer that repeatedly prompts the\nLLM with previous query appended with a list of errors.\nMai et al. ( 2023), similar in approach as Zeng et al. ( 2022),\nHuang et al. ( 2022b), integrates pretrained models for percep-\ntion, planning, control, memory, and dialogue zero-shot, for\nactive exploration and embodied question answering tasks.\nGupta and Kembhavi ( 2022) extends the LLM code gen-\neration and API-based perceptual interaction approach for a\nvariety of vision-langauge tasks. Some recent works Xie et al.\n(2023a), Capitanelli and Mastrogiovanni ( 2023) use PDDL\nas the translation language instead of code, and use the LLM\nto generate either a PDDL plan or the goal. A classical plan-\nner then plans for the PDDL goal or executes the generated\nplan. This approach ablated the need to generate precondi-\ntions using the LLM, however, needs the domain rules to be\nspeciﬁed for the planner.\n3 Our method: PROG PROMPT\nWe represent robot plans as pythonic programs. Following\nthe paradigm of LLM prompting, we create a prompt struc-\ntured as pythonic code and use an LLM to complete the code\n(Fig. 2). We use features available in Python to construct\nprompts that elicit an LLM to generate situated robot task\nplans, conditioned on a natural language instruction.\n3.1 Representing robot plans as pythonic functions\nPlan functions consist of API calls to action primitives,\ncomments to summarize actions, and assertions for tracking\nexecution (Fig. 3). Primitive actions use objects as arguments.\nFor example, the “ put salmon in the microwave” task includes\nAPI calls like find(salmon).\n1 https://openai.com/blog/chatgpt/\n123\n1002 Autonomous Robots (2023) 47:999–1012\nFig. 2 Our ProgPrompts include import statement, object list, and\nexample tasks ( PROMPT for Planning ). The Generated Plan is for\nmicrowave salmon . We highlight prompt comments, actions as\nimported function calls with objects as arguments, and assertions with\nrecovery steps. PROMPT for State Feedbackrepresents example asser-\ntion checks. We further illustrate the execution of the program via a\nscenario where an assertion succeeds or fails, and how the generated\nplan corrects the error before executing the next step. Full Execution is\nshown in bottom-right. ‘...’ used for brevity\nFig. 3 Pythonic ProgPrompt plan for “ put salmon in the microwave”\nWe utilize comments in the code to provide natural\nlanguage summaries for subsequent sequences of actions.\nComments help break down the high-level task into logi-\ncal sub-tasks. For example, in Fig. 3,t h e“ put salmon in\nmicrowave” task is broken down into sub-tasks using com-\nments “# grab salmon” and “# put salmon in microwave”.\nThis partitioning could help the LLM to express its knowl-\nedge about tasks and sub-tasks in natural language and aid\nplanning. Comments also inform the LLM about immedi-\nate goals, reducing the possibility of incoherent, divergent,\nor repetitive outputs. Prior work Wei et al. ( 2022) has also\nshown the efﬁcacy of similar intermediate summaries called\n‘chain of thought’ for improving performance of LLMs on a\nrange of arithmetic, commonsense, and symbolic reasoning\ntasks. We empirically verify the utility of comments (Table 1;\ncolumn Comments).\nAssertions provide an environment feedback mechanism\nthat encourages preconditions to be met, and allow error\nrecovery possibility when they are not. For example, in\nFig. 3, before the grab(salmon) action, the plan asserts\nthe agent is close to salmon. If not, the agent ﬁrst exe-\ncutes find(salmon). In Table 1, we show that such assert\nstatements (column Feedback) beneﬁt plan generation, and\nimprove success rates.\n3.2 Constructing programming language prompts\nWe provide information about the environment and primitive\nactions to the LLM through prompt construction. As done in\nfew-shot LLM prompting, we also provide the LLM with\nexamples of sample tasks and plans. Figure 2 illustrates our\nprompt function f\nprompt which takes in all the information\n(observations, action primitives, examples) and produces a\nPythonic prompt for the LLM to complete. The LLM then\npredicts the <next_task>(.) as an executable function\n(microwave_salmon in Fig. 2).\nIn the task microwave_salmon, a reasonable ﬁrst step\nthat an LLM could generate is take_out(salmon, grocery\n123\nAutonomous Robots (2023) 47:999–1012 1003\nbag). However, the agent responsible for executing the plan\nmight not have a primitive action to take_out. To inform\nthe LLM about the agent’s action primitives, we provide them\nas Pythonic import statements. These encourage the LLM\nto restrict its output to only functions that are available in the\ncurrent context. To change agents, ProgPrompt just needs a\nnew list of imported functions representing agent actions. A\ngrocery bag object might also not exist in the environment.\nWe provide the available objects in the environment as a list\nof strings. Since our prompting scheme explicitly lists out\nthe set of functions and objects available to the model, the\ngenerated plans typically contain actions an agent can take\nand objects available in the environment.\nProgPrompt also includes a few example tasks—fully\nexecutable program plans. Each example task demonstrates\nhow to complete a given task using available actions and\nobjects in the given environment. These examples demon-\nstrate the relationship between task name, given as the\nfunction handle, and actions to take, as well as the restrictions\non actions and objects to involve.\n3.3 Task plan generation and execution\nThe given task is fully inferred by the LLM based on the\nProgPrompt prompt. Generated plans are executed on a\nvirtual agent or a physical robot system using an interpreter\nthat executes each action command against the environment.\nAssertion checking is done in a closed-loop manner during\nexecution, providing current environment state feedback.\n4 Experiments\nWe evaluate our method with experiments in a virtual house-\nhold environment and on a physical robot manipulator.\n4.1 Simulation experiments\nWe evaluate our method in the Virtual Home (VH) Environ-\nment (Puig et al., 2018), a deterministic simulation platform\nfor typical household activities. A VH state s is a set of objects\nO and properties P. P encodes information like in(salmon,\nmicrowave) and agent_close_to(salmon). The action\nspace is A =\n{grab, putin, putback, walk, find,\nopen, close, switchon, switchoff, sit, standup} .\nWe experiment with 3 VH environments. Each environ-\nment contains 115 unique object instances (Fig. 2), including\nclass-level duplicates. Each object has properties correspond-\ning to its action affordances. Some objects also have a seman-\ntic state like heated, washed,o r used. For example, an\nobject in the Food category can become heated whenever\nin(object,microwave) ∧ switched_on(microwave).\nWe create a dataset of 70 household tasks. Tasks are posed\nwith high-level instructions like “ microwave salmon”. We\ncollect a ground-truth sequence of actions that completes the\ntask from an initial state, and record the ﬁnal state g that\ndeﬁnes a set of symbolic goal conditions, g ∈ P.\nWhen executing generated programs, we incorporate\nenvironment state feedback in response to assertions. VH\nprovides observations in the form of state graph with object\nproperties and relations. To check assertions in this environ-\nment, we extract information about the relevant object from\nthe state graph and prompt the LLM to return whether the\nassertion holds or not given the state graph and assertion as a\ntext prompt (Fig. 2 Prompt for State Feedback). We choose\nthis design over a rule-based checking since it’s more general.\n4.2 Real-robot experiments\nWe use a Franka-Emika Panda robot with a parallel-jaw grip-\nper. We assume access to a pick-and-place policy. The policy\ntakes as input two pointclouds of a target object and a tar-\nget container, and performs a pick-and-place operation to\nplace the object on or inside the container. We use the sys-\ntem of Danielczuk et al. ( 2021) to implement the policy, and\nuse MPPI for motion generation, SceneCollisionNet (Daniel-\nczuk et al., 2021) to avoid collisions, and generate grasp poses\nwith Contact-GraspNet (Sundermeyer et al., 2021).\nWe specify a single import statement for the action\ngrab_and_putin(obj1, obj2) for ProgPrompt.W e\nuse ViLD (Gu et al., 2022), an open-vocabulary object detec-\ntion model, to identify and segment objects in the scene and\nconstruct the available object list for the prompt. Unlike in the\nvirtual environment, where object list was a global variable in\ncommon for all tasks, here the object list is a local variable for\neach plan function, which allows greater ﬂexibility to adapt\nto new objects. The LLM outputs a plan containing function\ncalls of form grab_and_putin(obj1, obj2) . Here,\nobjects obj1 and obj2 are text strings that we map to point-\nclouds using ViLD segmentation masks and the depth image.\nDue to real world uncertainty, we do not implement assertion-\nbased closed loop options on the tabletop plans.\n4.3 Evaluation metrics\nWe use three metrics to evaluate system performance: suc-\ncess rate ( SR), executability ( Exec), and goal conditions\nrecall ( GCR). The task-relevant goal-conditions are the set\nof goal-conditions that changed between the initial and ﬁnal\nstate in the demonstration. SR is the fraction of executions\nthat achieved all task-relevant goal-conditions. Exec is the\nfraction of actions in the plan that are executable in the envi-\nronment, even if they are not relevant for the task. GCR is\nmeasured using the set difference between ground truth ﬁnal\nstate conditions g and the ﬁnal state achieved g\n′ with the\n123\n1004 Autonomous Robots (2023) 47:999–1012\nTable 1 Evaluation of generated programs on Virtual Home\n# — Prompt Format and Parameters —\nFormat Comments Feedback LLM Backbone SR Exec GCR\n* ProgPrompt ✓✓ GPT4 0.37 ± 0.06 0 .87 ± 0.01 0 .64 ± 0.02\n* ProgPrompt ✓✓ Davinci- 003 0.470.470.47 ± 0.15 0 .85 ± 0.02 0 .740.740.74±0.07\n1 ProgPrompt ✓✓ Codex 0.40 ± 0.11 0 .900.900.90 ± 0.05 0 .72 ± 0.09\n2 ProgPrompt ✓✓ Davinci 0.22 ± 0.04 0 .60 ± 0.04 0 .46 ± 0.04\n3 ProgPrompt ✓✓ GPT3 0.34 ± 0.08 0.84 ± 0.01 0.65 ± 0.05\n4 ProgPrompt ✓ ✗ GPT3 0.28 ± 0.04 0 .82 ± 0.01 0 .56 ± 0.02\n5 ProgPrompt ✗ ✓ GPT3 0.30 ± 0.00 0 .65 ± 0.01 0 .58 ± 0.02\n6 ProgPrompt ✗✗ GPT3 0.18 ± 0.04 0 .68 ± 0.01 0 .42 ± 0.02\n7 LangPrompt –– GPT3 0.00 ± 0.00 0 .36 ± 0.00 0 .42 ± 0.02\n8 Baseline from Huang et al. GPT3 0.00 ± 0.00 0 .45 ± 0.03 0 .21 ± 0.03\nProgPrompt uses 3 ﬁxed example programs, except the Davinci backbone which can ﬁt only 2 in the available API. Huang et al. ( 2022a)u s e\n1 dynamically selected example, as described in their paper. LangPrompt uses 3 natural language text examples. Best performing model with\na GPT3 backbone is shown in italic (used for our ablation studies); best performing model overall shown in bold. ProgPrompt signiﬁcantly\noutperforms the baseline Huang et al. ( 2022a)a n d LangPrompt. We also showcase how each ProgPrompt feature adds to the performance of\nthe method\nTable 2 ProgPrompt\nperformance on the VH\ntest-time tasks and their ground\ntruth actions sequence lengths\n|A|\nTask desc |A| SR Exec GCR\nwatch tv 30 .20 ± 0.40 0 .42 ± 0.13 0 .63 ± 0.28\nturn off light 30 .40 ± 0.49 1 .00 ± 0.00 0 .65 ± 0.30\nbrush teeth 80 .80 ± 0.40 0 .74 ± 0.09 0 .87 ± 0.26\nthrow away apple 81 .00 ± 0.00 1 .00 ± 0.00 1 .00 ± 0.00\nmake toast 80 .00 ± 0.00 1 .00 ± 0.00 0 .54 ± 0.33\neat chips on the sofa 50 .00 ± 0.00 0 .40 ± 0.00 0 .53 ± 0.09\nput salmon in the fridge 81 .00 ± 0.00 1 .00 ± 0.00 1 .00 ± 0.00\nwash the plate 18 0 .00 ± 0.00 0 .97 ± 0.04 0 .48 ± 0.11\nbring coffeepot and cupcake to the coffee table 80 .00 ± 0.00 1 .00 ± 0.00 0 .52 ± 0.14\nmicrowave salmon 11 0 .00 ± 0.00 0 .76 ± 0.13 0 .24 ± 0.09\nAvg: 0 ≤| A|≤ 50 .20 ± 0.40 0 .61 ± 0.29 0 .60 ± 0.25\nAvg: 6 ≤| A|≤ 10 0 .60 ± 0.50 0 .95 ± 0.11 0 .79 ± 0.29\nAvg: 11 ≤| A|≤ 18 0 .00 ± 0.00 0 .87 ± 0.14 0 .36 ± 0.16\ngenerated plan, divided by the number of task-speciﬁc goal-\nconditions; SR= 1 only if GCR= 1.\n5 Results\nWe show that ProgPrompt is an effective method for\nprompting LLMs to generate task plans for both virtual and\nphysical agents.\n5.1 Virtual experiment results\nTable 1 summarizes the performance of our task plan gen-\neration and execution system in the seen environment of\nVirtualHome. We utilize a GPT3 as a language model back-\nbone to receive ProgPrompt prompts and generate plans.\nEach result is averaged over 5 runs in a single VH envi-\nronment across 10 tasks. The variability in performance\nacross runs arises from sampling LLM output. We include\n3 Pythonic task plan examples per prompt after evaluating\nperformance on VH for between 1 prompt and 7 prompts\nand ﬁnding that 2 or more prompts result in roughly equal\nperformance for GPT3. The plan examples are ﬁxed to be:\n“put the wine glass in the kitchen cabinet”, “ throw away the\nlime”, and “ wash mug”.\nWe also include results on the recent GPT4 backbone.\nUnlike the GPT3 language model, GPT4 is a chat-bot model\ntrained with reinforcement learning with human feedback\n(RLHF) to act as a helpful digital assistant OpenAI ( 2023).\nGPT4 takes as input a system prompt followed by one or more\nuser prompts. Instead of simply auto-completing the code\nin the prompt, GPT4 interprets user prompts as questions\n123\nAutonomous Robots (2023) 47:999–1012 1005\nand generates answers as an assistant. To make GPT4 auto-\ncomplete our prompt, we used the following system prompt:\nYou are a helpful assistant. . The user prompt is the same\nf\nprompt as shown in Fig. 2.\nWe can draw several conclusions from Table 1. First,\nProgPrompt (rows 3–6) outperforms prior work (Huang\net al., 2022a) (row 8) by a substantial margin on all metrics\nusing the same large language model backbone. Second, we\nobserve that the Codex (Chen et al., 2021) and Davinci\nmodels (Brown et al., 2020)—themselves GPT3 variants—\nshow mixed success at the task. In particular, Davinci,t h e\noriginal GPT3 version, does not match base GPT3 perfor-\nmance (row 2 versus row 3), possibly because its prompt\nlength constraints limit it to 2 task examples versus the 3\navailable to other rows. Additionally, Codex exceeds GPT3\nperformance on every metric (row 1 versus row 3), likely\nbecause Codex is explicitly trained on programming lan-\nguage data. However, Codex has limited access in terms of\nnumber of queries per minute, so we continue to use GPT3\nas our main LLM backbone in the following ablation experi-\nments. Our recommendation to the community is to utilize a\nprogram-like prompt for LLM-based task planning and exe-\ncution, for which base GPT3 works well, and we note that an\nLLM ﬁne-tuned further on programming language data, such\nas Codex, can do even better. We additionally report results\non Davinci- 003 and GPT4 (row *), which is the latest GPT3\nvariant and the latest GPT variant in the series respectively\nat the time of this submission. Davinci- 003 has a better SR\nand GCR, indicating it might have an improved common-\nsense understanding, but lower Exec compared to Codex.\nThe newest model, GPT- 4 does not seem to be better than\nlatest GPT3 variant, on our tasks. Most of our results use\nthe Davinci- 002 variant (that we refer to as GPT3 in this\npaper), which was the latest model available when this study\nwas conducted.\nWe explore several ablations of ProgPrompt.F i r s t ,w e\nﬁnd that Feedback mechanisms in the example programs,\nnamely the assertions and recovery actions, improve perfor-\nmance (rows 3 versus 4 and 5 versus 6) across metrics, the\nsole exception being that Exec improves a bit without Feed-\nback when there are no Comments in the prompt example\ncode. Second, we observe that removing Comments from\nthe prompt code substantially reduces performance on all\nmetrics (rows 3 versus 5 and 4 versus 6), highlighting the\nusefulness of the natural language guidance within the pro-\ngramming language structure.\nWe also evaluate LangPrompt\n,a na l t e r n a t i v et oProg-\nPrompt that builds prompts from natural language text\ndescription of objects available and example task plans\n(row 7). LangPrompt is similar to the prompts built\nby Huang et al. ( 2022a). The outputs of LangPrompt\nare generated action sequences, rather than our proposed,\nprogram-like structures. Thus, we ﬁnetune GPT2 to learn a\nTable 3 ProgPrompt results on Virtual Home in additional scenes. We\nevaluate on 10 tasks each in two additional VH scenes beyond scene\nEnv- 0 where other reported results take place\nVH scene SR Exec GCR\nEnv- 0 0.34 ± 0.08 0 .84 ± 0.01 0 .65 ± 0.05\nEnv- 1 0.56 ± 0.08 0 .85 ± 0.02 0 .81 ± 0.07\nEnv- 2 0.56 ± 0.05 0 .85 ± 0.03 0 .72 ± 0.09\nAverage 0 .48 ± 0.13 0 .85 ± 0.02 0 .73 ± 0.10\npolicy P(at |st ,GPT3 step ,a1:t−1) to map those generated\nsequences to executable actions in the simulation environ-\nment. We use the 35 tasks in the training set, and annotate\nthe text steps and the corresponding action sequence to get\n400 data points for training and validation of this policy. We\nﬁnd that while this method achieves reasonable partial suc-\ncess through GCR, it does not match (Huang et al., 2022a)\nfor program executability Exec and does not generate any\nfully successful task executions.\nT ask-by-T ask PerformanceProgPrompt performance for\neach task in the test set is shown in Table 2. We observe\nthat tasks that are similar to prompt examples, such as throw\naway apple versus wash the plate have higher GCR since\nthe ground truth prompt examples hint about good stop-\nping points. Even with high Exec, some task GCR are low,\nbecause some tasks have multiple appropriate goal states, but\nwe only evaluate against a single “true” goal. For example,\nafter microwaving and plating salmon, the agent may put the\nsalmon on a table or a countertop.\nOther Environments We evaluate ProgPrompt in two\nadditional VH environments (Table 3). For each, we append\na new object list representing the new environment after the\nexample tasks in the prompt, followed by the task to be com-\npleted in the new scene. The action primitives and other\nProgPrompt settings remain unchanged. We evaluate on 10\ntasks with 5 runs each. For new tasks like wash the cutlery in\ndishwasher, ProgPrompt is able to infer that cutlery refers\nto spoons and forks in the new scenes, despite that cutlery\nalways refers to knives in example prompts.\n5.2 Qualitative analysis and limitations\nWe manually inspect generated programs and their execution\ntraces from ProgPrompt and characterize common fail-\nure modes. Many failures stem from the decision to make\nProgPrompt agnostic to the deployed environment and its\npeculiarities, which may be resolved through explicitly com-\nmunicating, for example, object affordances of the target\nenvironment as part of the ProgPrompt prompt.\n• Environment artifacts: the VH agent cannot ﬁnd or inter-\nact with objects nearby when sitting, and some\n123\n1006 Autonomous Robots (2023) 47:999–1012\nFig. 4 Robot plan execution rollout example on the sorting task show-\ning relevant objects banana, strawberry, bottle, plate and box,a n da\ndistractor object drill. The LLM recognizes that banana and straw-\nberry are fruits, and generates plan steps to place them on the plate,\nwhile placing the bottle in the box. The LLM ignores the distractor\nobject drill.S e eF i g .1 for the prompt structure used\ncommon sense actions for objects, such as open tvs-\ntand’s cabinets, are not available in VH.\n• Environment complexities: when an object is not acces-\nsible, the generated assertions might not be enough. For\nexample, if the agent ﬁnds an object in a cabinet,i tm a y\nnot plan to open the cabinet to grab the object.\n• Action success feedback is not provided to the agent,\nwhich may lead to failure of the subsequent actions.\nAssertion recovery modules in the plan can help, but\naren’t generated to cover all possibilities.\n• Incomplete generation:Some plans are cut short by LLM\nAPI caps. One possibility is to query the LLM again with\nthe prompt and partially generated plan.\nIn addition to these failure modes, our strict ﬁnal state\nchecking means if the agent completes the task and some,\nwe may infer failure, because the environment goal state will\nnot match our precomputed ground truth ﬁnal goal state. For\nexample, after making coffee, the agent may take the cof-\nfeepot to another table. Similarly, some task descriptions are\nambiguous and have multiple plausible correct programs.\nFor example, “ make dinner ” can have multiple possible\nsolutions. ProgPrompt generates plans that cooks salmon\nusing the fryingpan and stove, and sometimes the agent adds\nbellpepper or lime, or sometimes with a side of fruit,o rs e r v e d\nin a plate with cutlery. When run in a different VH environ-\nment, the agent cooks chicken instead. ProgPrompt is able\nto generate plans for such complex tasks as well while using\nthe objects available in the scene and not explicitly men-\ntioned in the task. However, automated evaluation of such\ntasks requires enumerating all valid and invalid possibilities\nor introducing human veriﬁcation.\nFurthermore, we note that while the reasoning capabilities\nof current state LLMs are impressive, our proposed method\ndoes not make any claims of providing guarantees. However,\nthe evaluations reported in Table 1 offer insights into the\ncapabilities of different LLMs within our task settings. While\nour method effectively prevents the LLM from generating\nunavailable actions or objects, it is worth acknowledging that\ndepending on the LLM’s generation quality and reasoning\ncapabilities, there is still a possibility of hallucination.\nTable 4 Results on the physical robot by task type\nTask description Distractors SR Plan SR GCR\nput the banana in the bowl 01 1 1 / 1\n41 1 1 / 1\nput the pear on the plate 01 1 1 / 1\n41 1 1 / 1\nput the banana on the plate 01 1 2 / 2\nand the pear in the bowl 31 1 2 / 2\nsort the fruits on the plate 00 1 2 / 3\nand the bottles in the box 11 1 3 / 3\n20 0 2 / 3\n5.3 Physical robot results\nThe physical robot results are shown in Table 4. We evaluate\non 4 tasks of increasing difﬁculty listed in Table 4. For each\ntask we perform two experiments: one in a scene that only\ncontains the necessary objects, and with one to four distractor\nobjects added.\nAll results shown use ProgPrompt with comments, but\nnot feedback. Our physical robot setup did not allow reliably\ntracking system state and checking assertions, and is prone\nto random failures due to things like grasps slipping. The\nreal world introduces randomness that complicates a quan-\ntitative comparison between systems. Therefore, we intend\nthe physical results to serve as a qualitative demonstration\nof the ease with which our prompting approach allows con-\nstraining and grounding LLM-generated plans to a physical\nrobot system. We report an additional metric Plan SR, which\nrefers to whether the plan would have likely succeeded,p r o -\nvided successful pick-and-place execution without gripper\nfailures.\nAcross tasks, with and without distractor objects, the sys-\ntem almost always succeeds, failing only on the sort task.\nThe run without distractors failed due to a random gripper\nfailure. The run with 2 distractors failed because the model\nmistakenly considered a soup can to be a bottle. The exe-\n123\nAutonomous Robots (2023) 47:999–1012 1007\ncutability for the generated plans was always Exec=1. An\nexecution rollout example is illustrated in Fig. 4.\nAfter this study was conducted, we re-attempted plan gen-\neration of the failed plan with GPT- 4 , using the same system\nprompt as in Sect. 5.1. GPT- 4 was able to successfully pre-\ndict the correct plan and not confuse the soup can for a bottle.\n6 Conclusions and future work\nWe present an LLM prompting scheme for robot task\nplanning that brings together the two strengths of LLMs:\ncommonsense reasoning and code understanding. We con-\nstruct prompts that include situated understanding of the\nworld and robot capabilities, enabling LLMs to directly gen-\nerate executable plans as programs. Our experiments show\nthat ProgPrompt programming language features improve\ntask performance across a range of metrics. Our method is\nintuitive and ﬂexible, and generalizes widely to new scenes,\nagents and tasks, including a real-robot deployment.\nAs a community, we are only scratching the surface of task\nplanning as robot plan generation and completion. We hope to\nstudy broader use of programming language features, includ-\ning real-valued numbers to represent measurements, nested\ndictionaries to represent scene graphs, and more complex\ncontrol ﬂow. Several works from the NLP community show\nthat LLMs can do arithmetic and understand numbers, yet\ntheir capabilities for complex robot behavior generation are\nstill relatively under-explored.\n7 FAQs and discussion\nQuestion 1 How does this approach compare with end-\nto-end robot learning models, and what are the current\nlimitations?\nProgPrompt is a hierarchical solution to task planning\nwhere the abstract task descriptions leverage LLM’s reason-\ning and maps the task plan to the grounded environment\nlabels. On the other hand, in end-to-end approaches, gen-\nerally the model implicitly learns reasoning, planning, and\ngrounding, while mapping the abstract task description to the\naction space directly.\nPros:\n• LLMs can do long-horizon planning from an abstract task\ndescription.\n• Decoupling the LLM planner from the environment\nmakes generalization to new tasks and environments fea-\nsible.\n• ProgPrompt enables LLMs to intelligently combine the\nrobot capabilities with the environment and their own\nreasoning ability to generate an executable and valid task\nplan.\n• The precondition checking helps recover from some fail-\nure modes that can happen if actions are generated in the\nwrong order or are missed by the base plan.\nCons:\n• Requires action space discretization, formalization of\nenvironments and objects.\n• Plan generation is open-loop, with commonsense precon-\ndition checking-based environment interaction.\n• Plan generation doesn’t consider low-level continuous\naspects of the environment state, and only reasons with\nthe semantic state for planning as well as precondition\nchecking.\n• The amount of information exchange between language\nmodels and other modules such as the robot’s perceptual\nor proprioceptive state encoders is limited, since API-\nbased access to these recent LLMs only allows textual\nqueries. However, this is still promising as it indicates\nthe need for a multimodal encoder that can work with\ninput such as vision, touch, force, temperature, etc.\nQuestion 2 How does it compare with the concurrent work:\nCode-as-Policies (CaP) (Liang et al., 2023)?\n• We believe that the general approach is quite similar to\nours. CaP deﬁnes Hints and Examples which may corre-\nspond to Imports/Object lists and Task Plan examples in\nProgPrompt .\n• CaP uses actions as API calls with certain parameters for\nthe calls such as robot arm pose, velocity, etc. We use\nactions as API calls with objects as parameters.\n• CaP uses APIs to obtain environment information as\nwell, like object pose or segmentation, for the pur-\npose of plan generation. However, ProgPrompt extracts\nenvironment information via precondition checking on\ncurrent environment state, to ensure plan executability.\nProgPrompt also generates the prompt conditioned on\ninformation from perception models.\nQuestion 3 During “PROMPT for State Feedback”, it seems\nthat the prompt already includes all information about the\nenvironment state. Is it necessary to prompt the LLM again for\nthe assertion (compared to a simple rule-based algorithm)?\n• The environment state input to the model is not the full\nstate for brevity. Thus, checking pre-conditions with the\nfull state separately helps, as shown in Table 1.\n• The environment state could change during execution.\n123\n1008 Autonomous Robots (2023) 47:999–1012\n• Using LLM as opposed to a rule-based algorithm is a\ndesign choice made to keep the approach more general,\ninstead of using a hand-coded rule-based algorithm. The\nassertion checking may also be replaced with a visual\nstate conditioned module, when a semantic state is not\navailable, such as in the real-world scenario. However,\nwe leave these aspects to be addressed in future research.\nQuestion 4 Is it possible that the generated code might lead\nthe robot to be stuck in an inﬁnite loop?\nLLM code generation could lead to loops by predicting\nthe same actions repeatedly as a generation artifact. LLMs\nused to suffer from such degeneration, but with latest LLMs\n(i.e. GPT-3) we have not encountered it at all.\nQuestion 5 Why are real-robot experiments simpler than vir-\ntual experiments?\nThe real-robot experiments were done as a demonstration\nof the approach on a real-robot, while studying the method\nin depth in a virtual simulator, for the sake of simplicity and\nefﬁciency.\nQuestion 6 What’s the difference between various GPT3\nmodel versions used in this project?\nWe name GPT3, which is the latest available version of\nGPT3 model on OpenAI at the time the paper was written:\ntext- davinci- 002 . We name davinci as the original ver-\nsion of GPT3 released: text- davinci .\n2\nQuestion 7 Why not a planning language like PDDL (or\nother planning languages) be used to construct Prog-\nPrompt? Any advantages of using a pythonic structure?\n• GPT-3 has been trained on data from the internet. There\nis a lot of python code on the internet, while PDDL is a\nlanguage of much more narrow interest. Thus, we expect\nthe LLM to better understand python syntax.\n• Python is a general purpose language, so it has more\nfeatures than PDDL. Furthermore, we want to avoid spec-\nifying the full planning domain, instead relying on the\nknowledge learned by the LLM to make common-sense\ninferences. A recent work Xie et al. ( 2023b) uses LLMs\nto generate PDDL goals, however, it requires full domain\nspeciﬁcation for a given environment.\n• Python is an accessible language that a larger community\nis familiar with.\nQuestion 8 How to handle multiple instances of the same\nobject type in the scene?\n2 More info on GPT3 models variations and naming can be found here:\nhttps://platform.openai.com/docs/models/overview\nProgPrompt doesn’t tackle the issue, however, Xie et al.\n(2023b) shows that multiple instances of the same objects can\nbe handled by using labels with object IDs such as “book_1,\nbook_2”.\nQuestion 9 Why doesn’t the paper compare the performance\nof the proposed method to InnerMonologue, SAYCAN, or\nSocratic models?\nAt the time of writing, the dataset or model from the\nabove papers were not public. However, we do compare with\na proxy approach, similar in underlying idea to the above\napproaches, in the VirtualHome environment. LangPlan in\nour baselines, uses GPT3 to get textual plan steps, which are\nthen executed using a GPT-2 based trained policy.\nQuestion 10 So the next step in this direction of research is\nto create highly structured inputs and outputs that could be\ncompiled, since eventually we want something that compiles\non robotic machines?\nThe disconnect and information bottleneck between LLM\nplanning module and skill execution module might make it\nless concrete on “how much” and “what” information should\nbe passed through the LLM during planning. That said, we\nthink that this would be an interesting direction to pursue and\ntest the limits of LLM’s highly structured input understanding\nand generation.\nQuestion 11 How does it compare to a classical planner?\n• Classical planners require concrete goal condition speci-\nﬁcation. An LLM planner reasons out a feasible goal state\nfrom a high level task description, such as “microwave\nsalmon”. From a user’s perspective, it is desirable to not\nhave to specify a concrete semantic goal state of the envi-\nronment and just be able to give an instruction to act on.\n• The search space would also be huge without common\nsense priors that an LLM planner leverages as opposed\nto a classical planner. Moreover, we also bypass the need\nto specify the domain knowledge needed for the search\nto roll out.\n• Moreover, the domain speciﬁcation and search space will\ngrow non-linearly with the complexity of the environ-\nment.\nQuestion 12 Is it possible to decouple high-level language\nplanning from low-level perceptual planning?\nIt may be feasible to an extent, however we believe that a\nclean decoupling might not be “all we need”. For instance,\nimagine an agent being stuck at an action that needs to be\nresolved at semantic level of reasoning, and probably very\nhard for the visual module to ﬁgure out. For instance, while\nplacing a dish on an oven tray, the robot may need to pull the\ndish rack out of the oven to be successful in the task.\n123\nAutonomous Robots (2023) 47:999–1012 1009\nQuestion 13 What are the kinds of failures that can happen\nwith ProgPrompt-like 2 stage decoupled pipeline?\nA few broad failure categories could be:\n• Generation of a semantically wrong action.\n• Robot might fail to execute the action at perception/action\n/skill level.\n• Robot needs to recover from a failure by taking a different\nhigh-level action, i.e., a precondition needs to be satisﬁed.\nThe challenge is to identify that precondition from the\ncurrent state of the environment and the agent.\nQuestion 14 What are the assumptions made about the\nactions used for ProgPrompt?\nWe assume a set of available action APIs that are imple-\nmented on the robot, without assuming the implementation\nmethod (e.g. motion planning or reinforcement learning).\nProgPrompt abstracts over and complements other research\non developing ﬂexible robot skills. This assumption is sim-\nilar to those made in classical TAMP planners, where the\nplanning space is restricted by the available robot skills.\nQuestion 15 Can the ProgPrompt planner handle more\nexpressive situations when “the embodied agent has to grasp\nan object in a speciﬁc way in order to complete an aspect of\nthe task”?\nThis is possible, provided the deployed robot is capable of\nhandling the requested action. For example, one can specify\n‘how’ along with ‘what’ parameters for an action as function\narguments, which may be discrete semantic grounded labels\naffecting the low-level skill execution, e.g. to select between\ndifferent modes of grasping intended for different task pur-\nposes. However, it is an open question as to what the right\nlevel of abstraction is between high-level task speciﬁcation\nand continuous control space actions, and the answer might\ndepend on the application domain.\nAuthor Contributions IS lead the research, conducted experiments, and\ndrafted the manuscript; VB provided feedback, conducted experiments,\ndrafted and reviewed the manuscript; JT and AG provided feedback,\ndrafted and reviewed the manuscript; AM, AG, DX, JT, and DF provided\nfeedback and reviewed the manuscript.\nFunding Open access funding provided by SCELC, Statewide Califor-\nnia Electronic Library Consortium. This project was conducted at and\nfunded by NVIDIA.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O., David, B.,\n& Yan, M. (2022). Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv.\nAkakzia, A., Colas, C., Oudeyer, P . Y ., Chetouani, M., & Sigaud, O.\n(2021). Grounding language to autonomously-acquired skills via\ngoal generation. In International conference on learning represen-\ntations.\nBaier, J. A., Bacchus, F., & McIlraith, S. A. (2007). A heuristic search\napproach to planning with temporally extended preferences. In\nProceedings of the 20th international joint conference on artiﬁcal\nintelligence (pp. 1808–1815). Morgan Kaufmann Publishers Inc.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,\nP ., & Amodei, D. (2020). Language models are few-shot learners.\narXiv.\nBryce, D., & Kambhampati, S. (2007). A tutorial on planning graph\nbased reachability heuristics. AI Magazine, 28(1), 47.\nCao, Y ., & Lee, C. (2023). Robot behavior-tree-based task generation\nwith large language models. arXiv preprint arXiv:2302.12927\nCapitanelli, A., & Mastrogiovanni, F. (2023). A framework to gen-\nerate neurosymbolic pddl-compliant planners. arXiv preprint\narXiv:2303.00438\nC h e n ,M . ,T w o r e k ,J . ,J u n ,H . ,Y u a n ,Q . ,P i n t o ,H .P .D .O . ,K a p l a n ,J . ,\n& Zaremba, W. (2021). Evaluating large language models trained\non code. arXiv.\nDanielczuk, M., Mousavian, A., Eppner, C.,& Fox, D. (2021). Object\nrearrangement using learned implicit collision functions. In IEEE\ninternational conference on robotics and automation (ICRA).\nEysenbach, B., Salakhutdinov, R. R., & Levine, S. (2019). Search on the\nreplay buffer: Bridging planning and reinforcement learning. In H.\nWallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox,\n& R. Garnett (Eds.), Advances in neural information processing\nsystems (vol. 32). Curran Associates, Inc.\nFikes, R. E., & Nilsson, N. J. (1971). Strips: A new approach to the\napplication of theorem proving to problem solving. In Proceedings\nof the 2nd international joint conference on artiﬁcial intelligence\n(pp. 608–620). Morgan Kaufmann Publishers Inc.\nGarrett, C. R., Lozano-Pérez, T., & Kaelbling, L. P . (2020). Pddl-\nstream: Integrating symbolic planners and blackbox samplers via\noptimistic adaptive planning. Proceedings of the International\nConference on Automated Planning and Scheduling, 30(1), 440–\n448.\nGu, X., Lin, T. Y ., Kuo, W., & Cui, Y . (2022). Open-vocabulary object\ndetection via vision and language knowledge distillation. In Inter-\nnational conference on learning representations.\nGupta, T., & Kembhavi, A. (2022). Visual programming: Com-\npositional visual reasoning without training. arXiv preprint\narXiv:2211.11559\nHelmert, M. (2006). The fast downward planning system. Journal of\nArtiﬁcial Intelligence Research, 26(1), 191–246.\nHoffmann, J. (2001). Ff: The fast-forward planning system. AI Maga-\nzine, 22(3), 57.\nHoltzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y . (2020). The curi-\nous case of neural text degeneration. In International conference\non learning representations.\n123\n1010 Autonomous Robots (2023) 47:999–1012\nHuang, W., Abbeel, P ., Pathak, D., & Mordatch, I. (2022). Language\nmodels as zero-shot planners: Extracting actionable knowledge for\nembodied agents. arXiv preprint arXiv:2201.07207\nHuang, W., Xia, F., Shah, D., Driess, D., Zeng, A., Lu, Y ., others (2023).\nGrounded decoding: Guiding text generation with grounded mod-\nels for robot control. arXiv preprint arXiv:2303.00855\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P ., & Ichter,\nB. (2022). Inner monologue: Embodied reasoning through plan-\nning with language models. arxiv preprint arxiv:2207.05608.\nJansen, P . (2020). Visually-grounded planning without vision: Lan-\nguage models infer detailed plans from high-level instructions. In\nFindings of the association for computational linguistics: Emnlp\n2020 (pp. 4412–4417). Online: Association for Computational\nLinguistics.\nJiang, Y ., Gu, S. S., Murphy, K. P ., & Finn, C. (2019). Language as\nan abstraction for hierarchical deep reinforcement learning. In H.\nWallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox,\n& R. Garnett (Eds.), Advances in neural information processing\nsystems. (vol. 32). Curran Associates, Inc.\nJiang, Y ., Zhang, S., Khandelwal, P ., & Stone, P . (2018). Task planning\nin robotics: An empirical comparison of pddl-based and asp-based\nsystems. arXiv.\nKurutach, T., Tamar, A., Yang, G., Russell, S. J., & Abbeel, P . (2018).\nLearning plannable representations with causal infogan. In S. Ben-\ngio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, &\nR. Garnett (Eds.), Advances in neural information processing sys-\ntems (vol. 31). Curran Associates, Inc.\nLi, S., Puig, X., Paxton, C., Du, Y ., Wang, C., Fan, L., & Zhu,\nY . (2022). Pre-trained language models for interactive decision-\nmaking. arXiv.\nLiang, J., Huang, W., Xia, F., Xu, P ., Hausman, K., Ichter, B., & Zeng, A.\n(2023). Code as policies: Language model programs for embodied\ncontrol.\nLiu, P ., Y uan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2021).\nPre-train, prompt, and predict: A systematic survey of prompting\nmethods in natural language processing. arXiv.\nLuong, T., Pham, H., & Manning, C. D. (2015). Effective approaches\nto attention-based neural machine translation. In Proceedings of\nthe 2015 conference on empirical methods in natural language\nprocessing (pp. 1412–1421). Association for Computational Lin-\nguistics.\nMai, J., Chen, J., Li, B., Qian, G., Elhoseiny, M., & Ghanem, B. (2023).\nLlm as a robotic brain: Unifying egocentric memory and control.\narXiv preprint arXiv:2304.09349\nMirchandani, S., Karamcheti, S., & Sadigh, D. (2021). Ella: Exploration\nthrough learned language abstraction. In M. Ranzato, A. Beygelz-\nimer, Y . Dauphin, P . Liang, J. W. V aughan (Eds.), Advances in\nneural information processing systems(vol. 34, pp. 29529–29540).\nCurran Associates, Inc.\nNair, S., & Finn, C. (2020). Hierarchical foresight: Self-supervised\nlearning of long-horizon tasks via visual subgoal generation. In\nInternational conference on learning representations.\nOpenAI (2023). Gpt-4 technical report. arXiv.\nPatel, R., & Pavlick, E. (2022). Mapping language models to grounded\nconceptual spaces. In International conference on learning repre-\nsentations.\nPuig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., & Tor-\nralba, A. (2018). Virtualhome: Simulating household activities via\nprograms. In 2018 IEEE/cvf conference on computer vision and\npattern recognition (pp. 8494–8502).\nShah, D., Toshev, A. T., Levine, S., & brian ichter. (2022). V alue function\nspaces: Skill-centric state abstractions for long-horizon reasoning.\nIn International conference on learning representations.\nSharma, P ., Torralba, A., & Andreas, J. (2022). Skill induction and\nplanning with latent language. In Proceedings of the 60th annual\nmeeting of the association for computational linguistics (volume\n1: Long papers) (pp. 1713–1726). Association for Computational\nLinguistics.\nShridhar, M., Thomason, J., Gordon, D., Bisk, Y ., Han, W., Mottaghi,\nR., & Fox, D. (2020). ALFRED: A Benchmark for Interpreting\nGrounded Instructions for Everyday Tasks. In The IEEE confer-\nence on computer vision and pattern recognition (cvpr).\nSilver, T., Chitnis, R., Kumar, N., McClinton, W., Lozano-Perez, T.,\nKaelbling, L. P ., & Tenenbaum, J. (2022). Inventing relational state\nand action abstractions for effective and efﬁcient bilevel planning.\nIn The multi-disciplinary conference on reinforcement learning\nand decision making (rldm).\nSkreta, M., Y oshikawa, N., Arellano-Rubach, S., Ji, Z., Kristensen, L.\nB., Darvish, K., & Garg, A. (2023). Errors are useful prompts:\nInstruction guided task programming with veriﬁer-assisted itera-\ntive prompting. arXiv preprint arXiv:2303.14100\nSrinivas, A., Jabri, A., Abbeel, P ., Levine, S., & Finn, C. (2018). Uni-\nversal planning networks: Learning generalizable representations\nfor visuomotor control. In J. Dy, & A. Krause (Eds.), Proceedings\nof the 35th international conference on machine learning(vol. 80,\npp. 4732–4741). PMLR.\nSundermeyer, M., Mousavian, A., Triebel, R., & Fox, D. (2021).\nContact-graspnet: Efﬁcient 6-dof grasp generation in cluttered\nscenes. In 2021 IEEE international conference on robotics and\nautomation (icra) (pp. 13438–13444).\nV emprala, S., Bonatti, R., Bucker, A., & Kapoor, A. (2023). Chatgpt\nfor robotics: Design principles and model abilities. 2023\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., &\nZhou, D. (2022). Chain of thought prompting elicits reasoning in\nlarge language models. arXiv.\nWiseman, S., Shieber, S., & Rush, A. (2017). Challenges in data-\nto-document generation. In Proceedings of the 2017 conference\non empirical methods in natural language processing (pp. 2253–\n2263). Association for Computational Linguistics.\nXie, Y ., Y u, C., Zhu, T., Bai, J., Gong, Z., & Soh, H. (2023a). Translating\nnatural language to planning goals with large-language models.\narXiv preprint arXiv:2302.05128\nXie, Y ., Y u, C., Zhu, T., Bai, J., Gong, Z., & Soh, H. (2023b). Translating\nnatural language to planning goals with large-language models.\nXu, D., Martín-Martín, R., Huang, D. A., Zhu, Y ., Savarese, S., & Fei-\nFei, L. F. (2019). Regression planning networks. In H. Wallach, H.\nLarochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, & R. Garnett\n(Eds.), Advances in neural information processing systems (vol.\n32). Curran Associates, Inc.\nXu, D., Nair, S., Zhu, Y ., Gao, J., Garg, A., Fei-Fei, L., & Savarese, S.\n(2018). Neural task programming: Learning to generalize across\nhierarchical tasks. In 2018 IEEE international conference on\nrobotics and automation (icra) (pp. 3795–3802).\nZeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker,\nS., & Florence, P . (2022). Socratic models: Composing zero-shot\nmultimodal reasoning with language. arXiv\nZhu, Y ., Tremblay, J., Birchﬁeld, S., & Zhu, Y . (2020). Hierarchical\nplanning for long-horizon manipulation with geometric and sym-\nbolic scene graphs. arXiv.\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123\nAutonomous Robots (2023) 47:999–1012 1011\nIshika Singh is a 3rd year PhD\nstudent advised by Professor Jesse\nThomason in the Computer Sci-\nence department at the Univer-\nsity of Southern California. Her\nresearch focuses on problems in\nlanguage-conditioned robot learn-\ning such as vision-language nav-\nigation, manipulation and task plan-\nning. Previously, she was an under-\ngrad at IIT Kanpur.\nValts Blukis is a research scien-\ntist at NVIDIA. His research goal\nis creating scalable and generaliz-\nable machine learning algorithms\nand models that enable robots to\ninteract with people through nat-\nural language while observing the\nunstructured world through ﬁrst-\nperson sensor observations. He received\nhis PhD from Cornell University\nand Cornell Tech.\nArsalan Mousavian s a senior\nresearch scientist at NVIDIA Seat-\ntle Robotics Lab. He is interested\nin using computer vision and 3D\nvision for robotics tasks such as\nobject manipulation. Prior to NVIDIA,\nhe ﬁnished his PhD in the Com-\nputer Science department at George\nMason University.\nAnkit Goyal is a Research Scientist\nin Robotics at NVIDIA. He did\nhis Ph.D. in Computer Science\nat Princeton University. I com-\npleted Masters from University of\nMichigan and Bachelors from IIT\nKanpur. He is interested in under-\nstanding various aspects of intel-\nligence, especially reasoning and\ncommon sense. In particular, he\nwants to develop computation mod-\nels for various reasoning skills\nthat humans possess.\nDanfei Xu is an Assistant Pro-\nfessor at the School of Interac-\ntive Computing at Georgia Tech\nand a (part-time) Research Sci-\nentist at NVIDIA AI. His cur-\nrent research focuses on visuo-\nmotor skill learning, long-horizon\nmanipulation planning, and data-\ndriven approaches to human-robot\ncollaboration. He received his Ph.D.\nin CS from Stanford University.\nJonathan Tremblay is a research\nscientist at NVIDIA. His research\ninterests are in computer vision,\nsynthetic data, and reinforcement\nlearning for robotics applications.\nAt NVIDIA, Jonathan has focused\non using synthetic data to train\nobject detectors, object pose esti-\nmation, few shot learning, etc. Jonathan’s\ngoal is to create robust and acces-\nsible computer vision systems for\nroboticists to use on their sys-\ntem. Prior to joining NVIDIA,\nJonathan received Ph.D. in com-\nputer science from McGill Uni-\nversity.\nDieter Fox is Senior Director of\nRobotics Research at Nvidia. His\nresearch is in robotics, with strong\nconnections to artiﬁcial intelligence,\ncomputer vision, and machine learn-\ning. He is currently on partial\nleave from the University of Wash-\nington, where he is a Professor\nin the Paul G. Allen School of\nComputer Science & Engineer-\ning. At UW, he also heads the\nUW Robotics and State Estima-\ntion Lab. From 2009 to 2011, he\nwas Director of the Intel Research\nLabs Seattle. Dieter obtained his\nPh.D. from the University of Bonn, Germany.\nJesse Thomason is an Assis-\ntant Professor at USC leading the\nGrounding Language in Multimodal\nObservations, Actions, and Robots\n(GLAMOR) lab. GLAMOR brings\ntogether natural language process-\ning and robotics (RoboNLP). Jesse\njoined USC in 2021 and received\nhis PhD from the University of\nTexas at Austin in 2018.\n123\n1012 Autonomous Robots (2023) 47:999–1012\nAnimesh Garg is an Stephen\nFleming Early Career Professor\nin Computer Science at Georgia\nTech. Previously, he was an Assis-\ntant Professor of Computer Sci-\nence at University of Toronto and\na Faculty Member at the V ector\nInstitute. He is also a Sr. Research\nScientist at Nvidia. He earned his\nPh.D. in Operations Research from\nUC Berkeley and postdoc at Stan-\nford. His group focuses on multi-\nmodal object-centric and spatiotem-\nporal event representations, self-\nsupervised pre-training for rein-\nforcement learning & control, principle of efﬁcient dexterous skill\nlearning.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8900922536849976
    },
    {
      "name": "Situated",
      "score": 0.7557584643363953
    },
    {
      "name": "Task (project management)",
      "score": 0.699960470199585
    },
    {
      "name": "Robot",
      "score": 0.6595640778541565
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6323493123054504
    },
    {
      "name": "Human–computer interaction",
      "score": 0.6101409196853638
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5781700015068054
    },
    {
      "name": "Action (physics)",
      "score": 0.5220087170600891
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.5201454758644104
    },
    {
      "name": "Code (set theory)",
      "score": 0.5046528577804565
    },
    {
      "name": "Natural language",
      "score": 0.47478124499320984
    },
    {
      "name": "Key (lock)",
      "score": 0.4431624114513397
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4194110631942749
    },
    {
      "name": "Programming language",
      "score": 0.21729063987731934
    },
    {
      "name": "Computer security",
      "score": 0.10098633170127869
    },
    {
      "name": "Systems engineering",
      "score": 0.06628313660621643
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}