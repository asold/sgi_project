{
  "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
  "url": "https://openalex.org/W4387947338",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2679805124",
      "name": "Li Minzhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360439434",
      "name": "Shi Tai-wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224869892",
      "name": "Ziems, Caleb",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224685732",
      "name": "Kan, Min-Yen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222454473",
      "name": "Chen, Nancy F.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2302081268",
      "name": "Liu, Zhengyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744134118",
      "name": "Yang Diyi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W4287815528",
    "https://openalex.org/W2952865802",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W4321855256",
    "https://openalex.org/W4320169959",
    "https://openalex.org/W2130903752",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W4286908383",
    "https://openalex.org/W2251960799",
    "https://openalex.org/W4323697401",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2132679783",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4235216760",
    "https://openalex.org/W4226250826",
    "https://openalex.org/W2950446064",
    "https://openalex.org/W3212689639",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W2122052811",
    "https://openalex.org/W2963955897",
    "https://openalex.org/W3204470331",
    "https://openalex.org/W2151401338",
    "https://openalex.org/W2890931111",
    "https://openalex.org/W3095830519",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4323570543",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W4385571545",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W4362700315",
    "https://openalex.org/W3027933721",
    "https://openalex.org/W3034403876",
    "https://openalex.org/W4327525855",
    "https://openalex.org/W4287887244",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W1983599491",
    "https://openalex.org/W3176259480",
    "https://openalex.org/W4389520124",
    "https://openalex.org/W3035097673",
    "https://openalex.org/W4297847342",
    "https://openalex.org/W4385571411",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2140721185"
  ],
  "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
  "full_text": "CoAnnotating: Uncertainty-Guided Work Allocation between\nHuman and Large Language Models for Data Annotation\nMinzhi Li †§ Taiwei Shi ‡ Caleb Ziems ¶\nMin-Yen Kan † Nancy F. Chen § Zhengyuan Liu § Diyi Yang ¶\n†National University of Singapore §Institute for Infocomm Research (I2R), A*STAR\n‡University of Southern California ¶Stanford University\nli.minzhi@u.nus.edu taiweish@usc.edu cziems@stanford.edu\nnfychen@i2r.a-star.edu.sg liu_zhengyuan@i2r.a-star.edu.sg\nkanmy@comp.nus.edu.sg diyiy@cs.stanford.edu\nAbstract\nAnnotated data plays a critical role in Natural\nLanguage Processing (NLP) in training models\nand evaluating their performance. Given re-\ncent developments in Large Language Models\n(LLMs), models such as ChatGPT demonstrate\nzero-shot capability on many text-annotation\ntasks, comparable with or even exceeding hu-\nman annotators. Such LLMs can serve as al-\nternatives for manual annotation, due to lower\ncosts and higher scalability. However, limited\nwork has leveraged LLMs as complementary\nannotators, nor explored how annotation work\nis best allocated among humans and LLMs\nto achieve both quality and cost objectives.\nWe propose CoAnnotating, a novel paradigm\nfor Human-LLM co-annotation of unstructured\ntexts at scale. Under this framework, we uti-\nlize uncertainty to estimate LLMs’ annotation\ncapability. Our empirical study shows CoAnno-\ntating to be an effective means to allocate work\nfrom results on different datasets, with up to\n21% performance improvement over random\nbaseline. For code implementation, see https:\n//github.com/SALT-NLP/CoAnnotating.\n1 Introduction\nLabeled data plays a critical role in establishing\nbenchmarks and developing models for Natural\nLanguage Processing (NLP). Although Large Lan-\nguage Models (LLMs) like ChatGPT have demon-\nstrated their strong zero-shot performance in var-\nious tasks such as question answering, reasoning,\nnatural language inference, sentiment analysis, and\nnamed entity recognition, results obtained by fine-\ntuned language models still outperform LLMs on\nmost of these tasks (Qin et al., 2023; Zhong et al.,\n2023; Ziems et al., 2023). Therefore, collecting\nlabeled data for model training and fine-tuning is\nstill valuable. Instead of deploying LLMs directly\nfor downstream uses, it is worthwhile to investi-\ngate how researchers can leverage LLMs’ zero-shot\ncapability in labeling text data to construct high-\nFigure 1: CoAnnotating framework. It differs from previous\nwork by considering how to allocate data within the same\ndataset to humans and ChatGPT by obtaining responses from\nChatGPT using different variations of prompts and estimating\nChatGPT’s annotation expertise with the use of uncertainty\nmetrics such as entropy.\nquality datasets and improve the performance of\nfine-tuned models.\nTypically, researchers recruit human annotators\nsuch as experts or crowd workers to perform data\nannotation (Kittur et al., 2008; Snow et al., 2008).\nSome challenges in manual annotation includes\nhigh costs of recruiting and training annotators,\nannotation inconsistency and human subjectivity\n(Lingren et al., 2014; Grosman et al., 2020). Re-\ncent work explored how LLMs perform relative\nto crowd workers (Ding et al., 2022) and results\nshowed that it is possible for LLMs like ChatGPT\nto replace large-scale manual annotation (Huang\net al., 2023; Kuzman et al., 2023). In some cases,\nLLMs’ annotation quality even outperforms human\nannotators on certain tasks (Gilardi et al., 2023).\nGiven the much lower annotation cost than crowd\nworkers, LLMs are considered to have great po-\narXiv:2310.15638v1  [cs.CL]  24 Oct 2023\ntential to increase the cost efficiency of the data\nannotation process. However, some studies also\nshow that, relative to human performance, LLMs’\nzero-shot performance falls short on more diffi-\ncult and pragmatic tasks (Wang et al., 2021; Ko-\nco´n et al., 2023). They suggest that practitioners\nshould use caution when using LLMs to annotate\ndata (Reiss, 2023; Huang et al., 2023). Such prior\nworks view humans and LLMs as competitors,\nmeasuring the accuracy of LLM labels as a replace-\nment for human annotation, rather than considering\nhow humans and LLMs might collaborate in an\nefficient manner. It is Human-LLM collaboration\nthat motivates this work. We propose the CoAn-\nnotating framework, which aims to balance the\ncomplementary profiles of humans and LLMs in\nterms of their respective annotation quality and cost.\nOur work tackles the problem of Human-LLM co-\nannotation from a resource allocation perspective.\nFollowing Gentile et al. (2022), Diao et al. (2023)\nand Wang et al. (2021), we consider model confi-\ndence as a reliable signal for the model’s expected\nperformance. As we consider allocating a given\ndatapoint for an LLM to annotate, we can use the\ninverse of the model’s uncertainty to estimate our\nconfidence in that allocation. Under CoAnnotat-\ning, we quantify LLMs’ annotating expertise on\nthe instance-level (estimating how well LLMs can\nannotate the given data point) beyond task-level\n(evaluating how LLMs performs on overall for each\ndataset). As such, a more informed allocation de-\ncision can be made with this fine-grained and con-\ntextualized instance-level perspective, rather than\nbroad and coarse dataset-level expertise.\nWe show that our proposed method using the un-\ncertainty of responses can achieve a more efficient\nand more accurate work allocation than the random\nallocation baseline. Our results also show that con-\nfidence scores generated by LLMs are generally\nwell-calibrated but not always reliable. It is possi-\nble to outsource some annotation work to achieve\nhuman-level performance for more straightforward\ntasks like topic understanding. On the other hand,\na tradeoff between annotation quality and anno-\ntation cost is inevitable for more nuanced tasks.\nOur framework establishes a guide to effectively\nallocate AI and human efforts in collaborative anno-\ntation, and in doing so, it provides key insights into\nthe capacities of LLMs, as well as the nature of the\ntasks and data that remain outside these capacities.\n2 Related Work\n2.1 Weak Supervision\nIn a traditional supervised learning setting, every\ntraining data point is labeled by human annota-\ntors. However, acquiring manually annotated la-\nbels for training data can be prohibitively costly\nand time-consuming. Weak supervision helps to\naddress the challenge using partially and imper-\nfectly labeled data for training (Zhang et al., 2022).\nWeak supervision techniques obtain these noisy la-\nbels by tapping into heuristics (Ratner et al., 2017;\nMeng et al., 2018; Awasthi et al., 2020), feature\nannotation (Mann and McCallum, 2010), external\nknowledge bases (Hoffmann et al., 2011; Min et al.,\n2013), pretrained models (Bach et al., 2019; Zhang\net al., 2021) and third-party tools (Lison et al.,\n2020). Moreover, weak supervision can be com-\nbined with the active learning framework (Gonsior\net al., 2020) to select the most informative data to\nbe annotated by humans and utilize weak supervi-\nsion to decide noisy labels. Given LLMs’ stunning\nzero-shot capabilities, our work explores the pos-\nsibility of using them as a more efficient labeling\nsource, thus freeing up resources to be reinvested\nin the research pipeline.\n2.2 LLMs for Annotation\nMost prior works frame the decision for human or\nLLM annotation as one of competition rather than\ncollaboration between these modes. These show\nthat LLMs like GPT-3 davinci-003 have strong\nzero-shot sentiment analysis performance (Ding\net al., 2022). ChatGPT (gpt-3.5-turbo) performs\nsurprisingly well on automatic genre detection in\nunder-resourced languages like Slovenian (Kuz-\nman et al., 2023). ChatGPT can even achieve high\naccuracy on some of the most nuanced tasks like\nimplicit hate speech detection (Huang et al., 2023).\nSimilarly, GPT-4 is able to annotate texts that re-\nquire reasoning and contextual knowledge and pro-\nvide explanations that could facilitate interpretive\nresearch (Törnberg, 2023). These results show the\ngreat potential of LLMs as data annotation tools\nwith just simple prompt design and without much\nmanual labeling efforts (Kuzman et al., 2023).\nHowever, there is still room to close significant\nperformance gaps between LLMs’ performance\nand existing fine-tuned baselines on some chal-\nlenging tasks. LLMs struggle with named entity\nrecognition (Ding et al., 2022; Qin et al., 2023),\nrelational reasoning (Bang et al., 2023), affective\ntasks (Koco´n et al., 2023; Amin et al., 2023) and\nsemantic similarity tasks (Kocmi and Federmann,\n2023; Wang et al., 2023). Moreover, it does not out-\nperform fine-tuned baselines for generation tasks\nlike question answering and text summarization\n(Tan et al., 2023; Wang et al., 2023). These works\nall take the perspective that LLMs and humans\nare competitors, making task-level comparisons\nbetween LLMs and humans/fine-tuned models for\neach dataset. Our work views LLMs and humans\nas potential collaborators, with the possibility to\nwork with each other to annotate the same dataset.\n2.3 Human-Machine Collaboration for\nDataset Creation\nThe quality of the dataset and the cost of creating a\ndataset are two important but sometimes conflict-\ning objectives in dataset creation. Previous work\nsuggests a human-AI collaborative framework that\nutilizes language models’ generation capability and\nhuman revision and evaluation skills (Tekiroglu\net al., 2020; Yuan et al., 2021; Bartolo et al., 2021;\nLiu et al., 2022) to create valuable datasets of high\nquality. For cost efficiency, some have proposed av-\neraging or majority vote over human and machine\noutputs (Chaganty et al., 2018; Ziems et al., 2023)\nand some initial empirical explorations such as an-\nalyzing the random combination of distillation of\nLLM and manual annotation (Kang et al., 2023)\nas well as active labeling assignments via the logit\noutputs (Wang et al., 2021). Our framework takes\nboth quality and cost into consideration by using\nuncertainty metrics to make informed human-AI\nwork-allocation decisions to ensure cost efficiency\nwithout compromising quality.\n3 CoAnnotating Framework\nOur CoAnnotating framework sets up a guide for\nannotating text data collaboratively (Figure 2). For\na given unlabeled train dataset Dt = {t1, t2, ...tm}\nwhere ti is the i-th instance in the dataset, our\nframework automatically decides whether each\ndata instance should be annotated by human or\nby the LLMs (Section 3.3) by computing the un-\ncertainty level of the LLMs’s annotations for each\ninstance (Section 3.2), with the goal of achieving\na higher annotation quality and a lower annotation\ncost for a given dataset (Section 3.4).\nText = Sentence1: {sentence1}\nSentence2: {sentence2}\nPrompt Type\nPlease label if the following two sen-\ntences are paraphrases of each other.\nPlease give your answer as “paraphrase”\nor “not paraphrase”.\n{Text}\nInstruction\n{Text}\nPlease label if the two sentences above\nare paraphrases of each other. Please\ngive your answer as “paraphrase” or\n“not paraphrase”.\nSequence\nSwapping\nGiven the following two sentences,\nplease classify the relationship of the\nfollowing two sentences as “paraphrase”\nor “not paraphrase”.\n{Text}\nParaphrase\nIs it true that the following two sen-\ntences are/are not paraphrases of each\nother? Give your answer as “true” or\n“false”.\n{Text}\nTrue/False\nWhat relationship do the following two\nsentences have? Is it “paraphrase” or\n“not paraphrase”?\n{Text}\nQuestion\nAnswering\nPlease choose one option that best de-\nscribes the relationship between the fol-\nlowing two sentences.\n{Text}\n(A) Paraphrase\n(B) Not paraphrase\nMultiple Choice\nQuestion\nI think the following two sentences\nare/are not paraphrases of each other.\nDo you agree?\n{Text}\nQuestion with\nConfirmation\nBias\nTable 1: Examples of our 7 designed prompt types ask-\ning ChatGPT to annotate each instance for the concrete\ntask of paraphrase detection.\n3.1 Prompt Construction\nPrevious work shows that LLMs’ performance\ncan be highly sensitive to perturbations in input\n(Jang and Lukasiewicz, 2023). Therefore, we in-\ntroduce a set of diverse types of prompts Pi =\n{pi1, pi2, ..., pik} for each instance ti. Besides the\n(1) basic instruction format, we vary the prompts by\nswapping its sequence of sentences (2; symmetric\nperturbation), paraphrasing the instruction (3; se-\nmantic perturbation), enquiring in various question\nformats (4; True/False, 5; Textual Short Responses\n6; Multiple Choice Question) and asking with con-\nfirmation bias (7; negation perturbation).\n3.2 Uncertainty Computation\nIn a real-world setting, there is no gold data on\nwhich to gauge the model’s expected accuracy and\nthus decide on the optimal annotation strategy.=\nFigure 2: Workflow of CoAnnotating. The framework consists of uncertainty-guided expertise estimation, work\nallocation, and cost performance Pareto analysis. With insights gained from Pareto analysis on the pilot dataset,\nuncertainty-guided work allocation can be applied on the original unlabeled dataset to achieve greater cost efficiency.\nHowever, model confidence can serve as a reli-\nable signal for model performance (Gentile et al.,\n2022; Diao et al., 2023; Wang et al., 2021). There-\nfore we compute the LLM uncertainty ui to guide\nthe work-allocation process. We compute ui in\ntwo ways which are easy to implement and have\nproven effectiveness in previous literature (Diao\net al., 2023):= (1) self-evaluation and (2) entropy.\nIn each case, for ti by prompting LLMs k times\nwith different prompts in Pi we get k annotations\nAi = {ai1, ai2, ..., aik} for each instance. As an\nablation study (5.4), we also prompt LLMs k times\nwith the same prompt to get k annotations to study\nthe effect of prompt perturbations.\nSelf-Evaluation. Previous work shows that\nLLMs are well calibrated and can provide informa-\ntion about their uncertainty themselves (Wang et al.,\n2021; Kadavath et al., 2022; Diao et al., 2023). We\nask the model to directly output its confidence score\n(Lin et al., 2022) by postpending the phrase \"and\nplease give a confidence score on a scale of 0 to\n1 for your prediction\" . The uncertainty for ti is\ncalculated by:\nui = 1− 1\nk\nkX\nj=1\nPθ(aij|pij)\nwhere Pθ(aij|pij) is the probability of a class label\nbeing annotated by ChatGPT given the prompt pij.\nWe obtain its value by extracting the confidence\nscore provided by LLMs directly.\nEntropy. Entropy is a measure of the impurity\nin a set of data and can be used to quantify the\nuncertainty associated with the class labels. The\nlarger the entropy value, the more uncertain the\nresponses are. We can use this metric to estimate\nthe uncertainty level:\nui = −\nkX\nj=1\nPθ(aij|pij) lnPθ(aij|pij)\nwhere Pθ(aij|pij) is calculated as the frequency of\na certain prediction among all predictions.\n3.3 Work Allocation Strategies\nBuilding upon the aforementioned uncertainty level\nestimation, we can then use the uncertainty level\nui to guide the work allocation.\nRandom Allocation. Random allocation is cho-\nsen as a baseline strategy for comparison. This\nis the strategy that randomly samples n instances\n(0 ≤ n ≤ m) in Dt to be annotated by LLMs while\nthe remaining m − n data is annotated by humans.\nSelf-Evaluation Guided Allocation. Wang et al.\n(2021) introduces an active label assignment ap-\nproach that ranks outputs by their logits. Not all\nLLM APIs support this computation, so we mod-\nify this baseline with our self-evaluation approach,\nsorting instances by the self-reported confidence\nscores in decreasing order. We then select the top\nn instances (0 ≤ n ≤ m) in Dt with the lowest\nlevel of uncertainty as the best candidates for LLM\nannotation. The remaining m − n data points are\nallocated to human annotators.\nEntropy Guided Allocation. It is not possible to\nentirely ensure the reliability of black box LLMs\nself-reported confidence. Therefore, we also pro-\npose the use of entropy across LLMs’ responses\nto gauge their certainty and reliability. We sort\nthe instances by their respective entropy values\nin increasing order and select the top n instances\n(0 ≤ n ≤ m) in Dt with the lowest level of un-\ncertainty to be annotated by LLMs. Again, the\nremaining m − n data points with inconsistent re-\nsponses will be allocated for human annotation.\n3.4 Strategy Selection\nWe frame the co-annotation process as a multi-\nobjective optimization problem with two main ob-\njectives, maximizing annotation quality and min-\nimizing annotation cost. We can determine anno-\ntation quality by the classification performance of\na model fine-tuned using a certain co-annotation\nstrategy. The total annotation cost is the sum of\nmanual annotation costs and those incurred by the\nLLM. Inspired by Kang et al. (2023), we apply\nthe Pareto efficiency concept in strategy selection.\nHere, the Pareto efficient scenario refers to the situ-\nation where it is impossible to increase the classifi-\ncation performance of the fine-tuned model with-\nout incurring a higher annotation cost. By adopting\ndifferent allocation strategies and setting different\nproportions of data allocated to LLMs, we get var-\nious allocation patterns with different annotation\nqualities and costs. We can then plot the perfor-\nmances of each quality-cost combination and ap-\nproximate the Pareto frontier by interpolating the\ndiscrete data points (Abdolrashidi et al., 2021; Tre-\nviso et al., 2022). Practitioners can plot annotation\nquality against the cost for pilot data to gain a bet-\nter understanding of this tradeoff, and they can use\nthe Pareto efficient points to decide which ratio of\ndata they should outsource to LLMs at their desired\nbudget level.\n4 Experiments\n4.1 Datasets\nWe use six classification datasets for different types\nof tasks. Since LLM inference costs much less than\na human salary, we know the simple allocation de-\ncision is to choose LLMs over humans whenever an\nLLM achieves a utility greater than or equal to that\nof human annotators. For a more challenging set-\nting, we identify tasks in which LLMs are known\nto struggle with discriminating the underlying con-\nstructs (Pikuliak, 2023; Wang et al., 2021). In such\ncases, there is a tradeoff between annotation quality\nand annotation cost and CoAnnotating facilitates\nbetter decision-making in such contexts. If the size\nof the train data is too large, we will take a stratified\nrandom sampling for approximately 1000 samples.\nTopic Classification is a challenging task for\nlarge pretrained language models like GPT-3\n(Wang et al., 2021). We choose two representa-\ntive datasets: TREC (Li and Roth, 2002) and AG\nNews (Zhang et al., 2015). AG News contains news\ntitles and their descriptions, which were gathered\nby an academic news search engine, and which\nspan four topics: world, sports, business, and sci-\nence/technology. TREC contains of English ques-\ntions with six manually labeled class labels: ab-\nbreviation; entity; description and abstract concept;\nhuman being; location;and numeric value.\nSemantic Similarity is known to challenge Chat-\nGPT (Jang and Lukasiewicz, 2023). We select\nMRPC (Dolan and Brockett, 2005) and Tem-\npoWiC (Loureiro et al., 2022) as two representa-\ntive datasets for semantic similarity understanding.\nMRPC is a corpus of sentence pairs extracted from\nonline news and annotated by humans for whether\nthe sentences are semantically equivalent. Tem-\npoWiC contains annotated tweet pairs for whether\nthere is a meaning shift of the target word.\nNuanced Comprehension We also experiment\nwith Tweet Stance Detection (Mohammad et al.,\n2016a) and Conversation Gone Awry (Zhang\net al., 2018) to explore the collaboration paradigm\non tasks requiring more nuanced comprehension.\nTweet Stance Detection in SemEval-2016 (Moham-\nmad et al., 2016b) is a dataset of tweets annotated\nwith the author’s stance (favorable, neutral, and\nnegative) toward a certain topic and we select the\ntopic of abortion.\n4.2 LLM Annotation\nWe obtain responses from ChatGPT\n(gpt-3.5-turbo) due to its high-quality an-\nnotations and low inference cost (Kuzman et al.,\n2023) using different prompts carefully crafted in\nTable 1. If the response is an ambiguous answer\nsuch as \"I cannot determine the class of the text\",\nwe encode it as a new class label which can result\nin higher uncertainty metrics. The uncertainty\ncomputation decides whether annotation will\nbe finally allocated to ChatGPT, and if so, we\ndecide the final label with a majority vote across\nChatGPT’s generations (Wang et al., 2022).\nFigure 3: Distribution of entropy and confidence values.\n4.3 Evaluation\nTo evaluate the quality of datasets annotated\nwith different strategies, we fine-tune the same\nRoBERTa base classifier and calculate macro F1\nscores on test data for a fair comparison. We re-\nport macro F1 as a more accurate representation of\nthe performance due to the unbalanced nature of\nLLMs’ annotations for some datasets.\nIn terms of cost, we only consider monetary cost\nin this work. We calculate human annotation costs\nbased on what was reported in the dataset paper.\nIf the information is not applicable, we assume\neach instance is annotated by 5 independent an-\nnotators with a wage of $15/hour. We calculate\nChatGPT annotation cost using the product of the\ntoken length of the input prompt and the price of\ncalling API for ( gpt-3.5-turbo) ($0.002/1k to-\nkens) at the time of experimentation.\n5 Results\n5.1 Strategy Comparison\nWe plot the histograms for distribution of uncer-\ntainty metrics (entropy with different prompts and\nsame prompt as well as confidence score). From\nFigure 3, we can observe that the model tends to\nbe confident with its predictions with a skewed dis-\ntribution towards high confidence value although\nwe ask ChatGPT to normalize its answer.\nWe hypothesize that a lower level of uncertainty\nin ChatGPT’s response indicates a higher degree\nof reliability in the label. Therefore, we set differ-\nent thresholds for entropy (lower than an entropy\nthreshold) and self-confidence score (higher than\na confidence threshold) to select data that Chat-\nGPT is more certain about. For those instances\nselected, we evaluate ChatGPT’s annotation qual-\nity by calculating its alignment with the gold label\n(human annotation). Figure 4’s decreasing trends\nfor entropy-guided allocation (green and blue dots)\non all datasets validate our hypothesis of an inverse\nrelationship between uncertainty and annotation\nquality. It justifies the helpfulness of using the en-\ntropy of ChatGPT’s annotations as an estimate for\nits annotating expertise. Importantly, we observe\nthat ChatGPT’s self-reported confidence scores (or-\nange dots) are not consistently a good estimate for\nits annotation quality. For some datasets such as\nAG News (top left), most of the data (94.3% with\ncalculation) has high self-reported confidence rang-\ning from 0.8 to 1, which leads to a weak separation\nof data in terms of annotation quality. For MRPC\n(top middle), there is a decreasing trend where data\ninstances with higher confidence scores in fact have\na poorer alignment with gold labels. This shows\nthat the reliability of using self-reported confidence\nfrom LLMs is not guaranteed.\nThe purpose of achieving a higher quality for\ntrain data is to ensure that it can teach the classi-\nfier accurate information through fine-tuning. In\nTable 2, we carry out comparisons of different allo-\ncation strategies in terms of test performance after\nfine-tuning with such data labeled. We see that\nholding the proportion of data allocated to Chat-\nFigure 4: Scatter plots of the average alignment of ChatGPT’s annotation with human annotation for train data against the\nthreshold. We vary the threshold for different metrics during work allocation to investigate the effectiveness of different metrics\nin quantifying ChatGPT’s annotation capability.\nAGNews TREC Stance Detection\n% ChatGPT 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100\nStrategies Macro F1\nRandom 88.2 87.9 85.8 79.8 81.8 82.692.1 88.1 86.1 81.6 76.4 75.860.2 53.9 53.6 55.0 50.4 53.6\nSelf−Evaluation 88.2 86.0 84.9 84.1∗ 82.1 82.192.1 91.5 87.2 86.5∗ 76.4 74.360.2 56.9 54.8 54.4 52.8 52.9\nEntropy(Diff.Prompts) 88.2 88.4 88.2 87.4∗ 84.0 82.692.1 91.9 87.480.8 79.2 75.860.2 58.2 55.1 56.8 54.7 53.6\nEntropy(SamePrompt) 88.2 85.1 85.5 85.4∗ 84.7 81.492.1 90.8 87.1 83.7 76.2 74.060.2 54.2 53.3 54.0 52.1 47.8\nTempoWIC MRPC Conversation\n% ChatGPT 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100\nStrategies Macro F1\nRandom 57.5 55.9 53.2 46.2 50.3 42.083.4 78.6 74.4 70.3 65.8 65.971.3 63.1 54.5 57.1 50.0 54.1\nSelf−Evaluation 57.5 57.8 55.9∗ 51.8∗ 52.9∗ 43.083.4 79.7 77.8 71.5 63.9 58.671.3 70.1∗ 62.6 64.2∗ 50.4 50.7\nEntropy(Diff.Prompts) 57.5 58.4∗ 56.9∗ 55.9∗ 53.8∗ 42.083.4 80.0 79.8∗ 76.6∗ 73.1∗ 65.971.3 66.5 64.2∗ 62.6 56.2∗ 54.1\nEntropy(SamePrompt) 57.5 56.3 53.5 52.9∗ 43.8 42.083.4 79.2 72.7 67.7 68.6 65.771.3 55.4 55.4 54.1 54.8 54.6\nTable 2: Test performance of fine-tuned RoBERTa under different allocation strategies. We vary the percentage\nof data allocated to ChatGPT for annotation and carry out finetuning using train data annotated under different\nstrategies for all six datasets. Figure with superscript * means the result under that strategy is significantly better\nthan baseline strategy at 10% significance level.\nGPT fixed (e.g., taking the setup of 40% for Tem-\npoWIC as an example), our proposed uncertainty-\nguided allocation using self-evaluation and entropy\nresults in a better-annotated dataset, reflected by\nits higher test F1 (56.9) than the random alloca-\ntion baseline (53.2). More often than not, entropy-\nguided allocation is better than confidence-guided\nallocation. This is probably due to the skewed dis-\ntribution of self-reported confidence, resulting in a\npoorer distinguishability between instances LLMs\nare better or worse at annotating.\n5.2 Pareto Efficient Allocation\nBy plotting test performance against annotation\ncost (Figure 5), practitioners can visualize the trade-\noff in annotation quality achievable at different\nbudgets with collaboration between human and an\nLLM like ChatGPT by studying the Pareto frontier.\nPoints along the highlighted Pareto frontier mean it\nis theoretically impossible to achieve a higher test\nFigure 5: Pareto curves under different allocation strategies (random, entropy guided, self-evaluation guided). The\nPareto frontier is highlighted, illustrating the optimal choices that are Pareto efficient.\nDataset Text Groundtruth ChatGPT\nAG News Title: Sprint Set to Debut Video-Streaming Cell Phone Description: OVERLAND PARK, Kan. (AP) –\nChannel surfing is moving off the couch as Sprint Corp... Sci/Tech Business\nTREC What does A&W of root beer fame stand for? Abbreviation Entity\nStance Detection@user As a former fetus I oppose #ProlifeYouth #SemST Negative Neutral\nConversation\nrjoccolenty: Shouldn’t her name be Zainab Yusef and not Zainab Khan?Bluebolt94: Does the credits at\nthe end of the episode say ”Zainab Yusef”? No they say ”Zainab Khan” and Yusef called her ”Mrs. Khan”\nduring the episode. So no, her name is ”Zainab Khan”. –AnemoneProjectors: The Khans are clearly not\nas traditional as the Masoods, or Afia would have been called Afia Yusef. We already know this! And\nwhat GS said. Watch the show properly P ––\nTrue False\nMRPC Sentence1:At 5 p.m. EDT , Henri had maximum sustained winds near 50 mph , with some gusts reaching\n60 mph.Sentence2:At 8 p.m. Friday , Henri was becoming disorganized , but still had maximum\nsustained winds near 50 mph , with stronger gusts. Not paraphrase Paraphrase\nTempoWiC tweet 1: If you need some to watch on Netflix, containment is so good.tweet 2: I have a lot of questions\nabout the containment series.target word: containment Same Different\nTable 3: Specific instances with high entropy values for ChatGPT annotations.\naccuracy without increasing the budget, and it is\nalso impossible to reduce the cost but achieve the\nsame level of annotation quality. Furthermore, it\nprovides information on the approximate propor-\ntion that can be outsourced to ChatGPT to achieve\nhuman-level performance. For more straightfor-\nward tasks like topic classification, part of the an-\nnotation work could be potentially outsourced to\nChatGPT and lead to a cost reduction (e.g., AG\nNews: 33%) by ensuring human-level annotation\nperformance. For datasets requiring nuanced com-\nprehensions like Stance Detection and Conversa-\ntion Gone Awry, any level of outsourcing to the\ncurrent version of ChatGPT compromises annota-\ntion quality. Practitioners can choose among the\nPareto efficient points based on their budgets.\n5.3 Qualitative Analysis\nWe select some instances with entropy values\nhigher than 0.8 from each dataset (Table 3) to un-\nderstand the current challenges faced by ChatGPT\nin annotating data. We find that ChatGPT has high\nuncertainty for instances containing sarcasm and\nincomplete sentences that require more inference\nduring opinion mining. For example, in deciding\nthe stance towards abortion for the tweet “as a for-\nmer fetus I oppose”, the incomplete nature of this\nsentence causes confusion to ChatGPT. Also, it\nstruggles with numerical reasoning as seen from its\ninability to compare wind speed during paraphrase\ndetection and may be misled by some keywords\n(“Corp”) related to other incorrect classes (“busi-\nness”) in topic classification.\n5.4 Ablation Study\nWe carry out inferences with the same instruction-\nformatted prompt for the same number of times\nand compute the entropy for ChatGPT’s responses.\nFrom Figure 4, we observe some extent of ef-\nfectiveness of computing entropy using the same\nprompt in quantifying ChatGPT’s capability, as re-\nflected by a decreasing pattern of alignment with\nthe increased threshold. However, it serves as a\nmuch weaker method to quantify expertise com-\npared with our method with different prompt de-\nsigns since the majority of the data has zero entropy\n(see Figure 3). This suggests that ChatGPT’s re-\nsponses are generally consistent within multiple\napplications of the same prompt. In Table 2, the\ntest performance of entropy-guided allocation un-\nder different prompts is consistently higher than\nwhen based on a single prompt. The performance\ngap gives strong evidence of the utility of applying\ndifferent prompt types in Table 1.\n6 Conclusion\nThis work introduces CoAnnotating, a framework\nwhich takes a collaborative angle to view the re-\nlationship between humans and LLMs when an-\nnotating each dataset. Under this framework, we\nuse uncertainty metrics to estimate LLMs’ anno-\ntating capability and guide effective work alloca-\ntion. Moreover, we apply the Pareto efficiency\nconcept for practitioners to compare strategies and\nunderstand cost-performance tradeoffs. The em-\npirical results demonstrate the effectiveness of our\nproposed framework in achieving greater cost effi-\nciency. Overall, our framework provides important\ninsights around the reliability of self-reported confi-\ndence score by LLMs, the sensitivity of ChatGPT’s\nresponses to prompt variations as well as the extent\nto which human resources can be freed by LLMs\nto be put on more meaningful areas.\n7 Limitations\nSince LLMs has been trained on a large num-\nber of datasets, there may be data leakage issue\nwhere LLMs has seen some datasets in our experi-\nment, making entropy values obtained for LLMs’\nresponses lower. As an initial exploration of the co-\nannotating concept, this work aims for human-level\nperformance in annotating datasets. It does not con-\nsider the scope of superhuman-level performance\nwhere we treat human annotation in each dataset\nas gold labels. Future work can further investigate\nthe instances where LLMs actually annotates better\nthan humans. We consider annotating profiles of\nhuman and LLMs as two groups but this frame-\nwork can be further enriched by taking variations\nwithin each group (expert, crowd workers, differ-\nent LLMs) into considerations. More exploration\ncan also be carried out to investigate how to design\nprompts in a way that can increase LLMs’s anno-\ntating expertise so that more annotation work can\nbe outsourced to LLMs for greater cost efficiency.\nMoreover, this work only did experiments for clas-\nsification tasks and English datasets. However, the\nidea of CoAnnotating is generalizable to generation\ntasks and datasets in other languages as well, which\nare meaningful to study in future work.\nEthical Statement\nWe are aware of the potential ethical concerns of\nusing LLMs as potential labelers in the data annota-\ntion process in terms of the perpetuation of existing\nbiases in LLMs. Since LLMs are trained on vast\namounts of texts on the Internet, they can unavoid-\nably incorporate the biases present in these data\nsources. Such biases could be under-representation\nof certain demographic groups, cultural stereotypes\nas well as linguistic biases. However, we believe\nthat the benefit of proposing a collaborative co-\nannotation framework outweighs the potential risks\nrelated to the framework.\nAcknowledgement\nWe are thankful to the members of SALT Lab and\nWING Lab as well as anonymous EMNLP review-\ners for their helpful feedback. Minzhi Li is sup-\nported by the A*STAR Computing and Information\nScience (ACIS) Scholarship. Caleb Ziems is sup-\nported by the NSF Graduate Research Fellowship\nunder Grant No. DGE-2039655. We would like\nto acknowledge a grant from the Office of Naval\nResearch to DY , and National Research Founda-\ntion, Singapore under its AI Singapore Programme\n(AISG Award No: AISG2-GC-2022-005).\nReferences\nAmirAli Abdolrashidi, Lisa Wang, Shivani Agrawal,\nJonathan Malmaud, Oleg Rybakov, Chas Leich-\nner, and Lukasz Lew. 2021. Pareto-optimal quan-\ntized resnet is mostly 4-bit. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) Workshops, pages 3091–\n3099.\nMostafa M Amin, Erik Cambria, and Björn W Schuller.\n2023. Will affective computing emerge from foun-\ndation models and general ai? a first evaluation on\nchatgpt. arXiv preprint arXiv:2303.03186.\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6:1817–1853.\nGalen Andrew and Jianfeng Gao. 2007. Scalable train-\ning of L1-regularized log-linear models. In Proceed-\nings of the 24th International Conference on Machine\nLearning, pages 33–40.\nAbhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal,\nand Sunita Sarawagi. 2020. Learning from rules\ngeneralizing labeled exemplars. arXiv preprint\narXiv:2004.06025.\nStephen H Bach, Daniel Rodriguez, Yintao Liu, Chong\nLuo, Haidong Shao, Cassandra Xia, Souvik Sen,\nAlex Ratner, Braden Hancock, Houman Alborzi, et al.\n2019. Snorkel drybell: A case study in deploying\nweak supervision at industrial scale. In Proceedings\nof the 2019 International Conference on Management\nof Data, pages 362–375.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nMax Bartolo, Tristan Thrush, Sebastian Riedel, Pon-\ntus Stenetorp, Robin Jia, and Douwe Kiela. 2021.\nModels in the loop: Aiding crowdworkers with\ngenerative annotation assistants. arXiv preprint\narXiv:2112.09062.\nArun Tejasvi Chaganty, Stephen Mussman, and Percy\nLiang. 2018. The price of debiasing automatic met-\nrics in natural language evaluation. arXiv preprint\narXiv:1807.02202.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models. arXiv preprint\narXiv:2302.12246.\nBosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,\nShafiq Joty, and Boyang Li. 2022. Is gpt-3 a good\ndata annotator? arXiv preprint arXiv:2212.10450.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nClaudio Gentile, Zhilei Wang, and Tong Zhang. 2022.\nFast rates in pool-based batch active learning. arXiv\npreprint arXiv:2202.05448.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nJulius Gonsior, Maik Thiele, and Wolfgang Lehner.\n2020. Weakal: Combining active learning and weak\nsupervision. In Discovery Science: 23rd Interna-\ntional Conference, DS 2020, Thessaloniki, Greece,\nOctober 19–21, 2020, Proceedings 23, pages 34–49.\nSpringer.\nJonatas S Grosman, Pedro HT Furtado, Ariane MB\nRodrigues, Guilherme G Schardong, Simone DJ Bar-\nbosa, and Hélio CV Lopes. 2020. Eras: Improving\nthe quality control in the annotation process for natu-\nral language processing tasks. Information Systems,\n93:101553.\nRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke\nZettlemoyer, and Daniel S Weld. 2011. Knowledge-\nbased weak supervision for information extraction\nof overlapping relations. In Proceedings of the 49th\nannual meeting of the association for computational\nlinguistics: human language technologies, pages 541–\n550.\nFan Huang, Haewoon Kwak, and Jisun An. 2023. Is\nchatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate\nspeech. arXiv preprint arXiv:2302.07736.\nMyeongjun Jang and Thomas Lukasiewicz. 2023.\nConsistency analysis of chatgpt. arXiv preprint\narXiv:2303.06273.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nJunmo Kang, Wei Xu, and Alan Ritter. 2023. Distill\nor annotate? cost-efficient fine-tuning of compact\nmodels. arXiv preprint arXiv:2305.01645.\nAniket Kittur, Ed H Chi, and Bongwon Suh. 2008.\nCrowdsourcing user studies with mechanical turk.\nIn Proceedings of the SIGCHI conference on human\nfactors in computing systems, pages 453–456.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, et al. 2023. Chatgpt: Jack of all trades,\nmaster of none. arXiv preprint arXiv:2302.10724.\nTaja Kuzman, Nikola Ljubeši´c, and Igor Mozetiˇc. 2023.\nChatgpt: Beginning of an end of manual annotation?\nuse case of automatic genre identification. arXiv\npreprint arXiv:2303.03953.\nXin Li and Dan Roth. 2002. Learning question clas-\nsifiers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTeaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334.\nTodd Lingren, Louise Deleger, Katalin Molnar, Hai-\njun Zhai, Jareen Meinzen-Derr, Megan Kaiser, Laura\nStoutenborough, Qi Li, and Imre Solti. 2014. Eval-\nuating the impact of pre-annotation on annotation\nspeed and potential bias: natural language processing\ngold standard development for clinical named entity\nrecognition in clinical trial announcements. Journal\nof the American Medical Informatics Association ,\n21(3):406–413.\nPierre Lison, Aliaksandr Hubin, Jeremy Barnes, and\nSamia Touileb. 2020. Named entity recognition\nwithout labelled data: A weak supervision approach.\narXiv preprint arXiv:2004.14723.\nAlisa Liu, Swabha Swayamdipta, Noah A Smith, and\nYejin Choi. 2022. Wanli: Worker and ai collaboration\nfor natural language inference dataset creation. arXiv\npreprint arXiv:2201.05955.\nDaniel Loureiro, Aminette D’Souza, Areej Nasser\nMuhajab, Isabella A White, Gabriel Wong, Luis Es-\npinosa Anke, Leonardo Neves, Francesco Barbieri,\nand Jose Camacho-Collados. 2022. Tempowic: An\nevaluation benchmark for detecting meaning shift in\nsocial media. arXiv preprint arXiv:2209.07216.\nGideon S Mann and Andrew McCallum. 2010. General-\nized expectation criteria for semi-supervised learning\nwith weakly labeled data. Journal of machine learn-\ning research, 11(2).\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n2018. Weakly-supervised neural text classification.\nIn proceedings of the 27th ACM International Con-\nference on information and knowledge management,\npages 983–992.\nBonan Min, Ralph Grishman, Li Wan, Chang Wang,\nand David Gondek. 2013. Distant supervision for re-\nlation extraction with an incomplete knowledge base.\nIn Proceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 777–782.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016a.\nSemeval-2016 task 6: Detecting stance in tweets. In\nProceedings of the 10th international workshop on\nsemantic evaluation (SemEval-2016), pages 31–41.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016b.\nSemEval-2016 task 6: Detecting stance in tweets.\nIn Proceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016), pages 31–\n41, San Diego, California. Association for Computa-\ntional Linguistics.\nMatúš Pikuliak. 2023. Chatgpt sur-\nvey: Performance on nlp datasets.\nhttps://www.opensamizdat.com/posts/chatgpt_survey.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nMohammad Sadegh Rasooli and Joel R. Tetreault. 2015.\nYara parser: A fast and accurate dependency parser.\nComputing Research Repository, arXiv:1503.06733.\nVersion 2.\nAlexander Ratner, Stephen H Bach, Henry Ehrenberg,\nJason Fries, Sen Wu, and Christopher Ré. 2017.\nSnorkel: Rapid training data creation with weak su-\npervision. In Proceedings of the VLDB Endowment.\nInternational Conference on Very Large Data Bases,\nvolume 11, page 269. NIH Public Access.\nMichael Reiss. 2023. Testing the reliability of chatgpt\nfor text annotation and classification: A cautionary\nremark.\nRion Snow, Brendan O’connor, Dan Jurafsky, and An-\ndrew Y Ng. 2008. Cheap and fast–but is it good?\nevaluating non-expert annotations for natural lan-\nguage tasks. In Proceedings of the 2008 conference\non empirical methods in natural language processing,\npages 254–263.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan\nHu, Yongrui Chen, and Guilin Qi. 2023. Evalu-\nation of chatgpt as a question answering system\nfor answering complex questions. arXiv preprint\narXiv:2303.07992.\nSerra Sinem Tekiroglu, Yi-Ling Chung, and Marco\nGuerini. 2020. Generating counter narratives against\nonline hate speech: Data and strategies. arXiv\npreprint arXiv:2004.04216.\nPetter Törnberg. 2023. Chatgpt-4 outperforms experts\nand crowd workers in annotating political twitter\nmessages with zero-shot learning. arXiv preprint\narXiv:2304.06588.\nMarcos Treviso, António Góis, Patrick Fernandes, Er-\nick Fonseca, and Andre Martins. 2022. Predicting\nattention sparsity in transformers. In Proceedings\nof the Sixth Workshop on Structured Prediction for\nNLP, pages 67–81, Dublin, Ireland. Association for\nComputational Linguistics.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce\nlabeling cost? gpt-3 can help. arXiv preprint\narXiv:2108.13487.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nAnn Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris\nCallison-Burch, Andy Coenen, and Sebastian\nGehrmann. 2021. Synthbio: A case study in human-\nai collaborative curation of text datasets. arXiv\npreprint arXiv:2111.06467.\nJieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang,\nand Alexander Ratner. 2022. A survey on pro-\ngrammatic weak supervision. arXiv preprint\narXiv:2202.05433.\nJieyu Zhang, Bohan Wang, Xiangchen Song, Yujing\nWang, Yaming Yang, Jing Bai, and Alexander Rat-\nner. 2021. Creating training sets via weak indirect\nsupervision. arXiv preprint arXiv:2110.03484.\nJustine Zhang, Jonathan P Chang, Cristian Danescu-\nNiculescu-Mizil, Lucas Dixon, Yiqing Hua, Nithum\nThain, and Dario Taraborelli. 2018. Conversations\ngone awry: Detecting early signs of conversational\nfailure. arXiv preprint arXiv:1805.05345.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too?\na comparative study on chatgpt and fine-tuned bert.\narXiv preprint arXiv:2302.10198.\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen,\nZhehao Zhang, and Diyi Yang. 2023. Can large lan-\nguage models transform computational social sci-\nence? arXiv preprint arXiv:2305.03514.",
  "topic": "Annotation",
  "concepts": [
    {
      "name": "Annotation",
      "score": 0.8876821398735046
    },
    {
      "name": "Computer science",
      "score": 0.7399824857711792
    },
    {
      "name": "Scalability",
      "score": 0.5921745300292969
    },
    {
      "name": "Natural language processing",
      "score": 0.48205432295799255
    },
    {
      "name": "Work (physics)",
      "score": 0.4375784695148468
    },
    {
      "name": "Scale (ratio)",
      "score": 0.437051385641098
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4297906756401062
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4095023274421692
    },
    {
      "name": "Data science",
      "score": 0.40935614705085754
    },
    {
      "name": "Machine learning",
      "score": 0.3340544104576111
    },
    {
      "name": "Database",
      "score": 0.15728291869163513
    },
    {
      "name": "Political science",
      "score": 0.07576131820678711
    },
    {
      "name": "Geography",
      "score": 0.06536710262298584
    },
    {
      "name": "Cartography",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3005327000",
      "name": "Institute for Infocomm Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 1
}