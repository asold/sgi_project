{
  "title": "Unbiasing Retrosynthesis Language Models with Disconnection Prompts",
  "url": "https://openalex.org/W4296393188",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2972467232",
      "name": "Amol Thakkar",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2708601231",
      "name": "Alain Vaucher",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A3211741165",
      "name": "Andrea Byekwaso",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2769065378",
      "name": "Philippe Schwaller",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A3100450207",
      "name": "Alessandra Toniato",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2209038682",
      "name": "Teodoro Laino",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091789543",
    "https://openalex.org/W3138801267",
    "https://openalex.org/W3043647281",
    "https://openalex.org/W3157679602",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6790978476",
    "https://openalex.org/W3208007731",
    "https://openalex.org/W4213314063",
    "https://openalex.org/W2999373911",
    "https://openalex.org/W2325811289",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W29374554",
    "https://openalex.org/W4230201210",
    "https://openalex.org/W3151478830",
    "https://openalex.org/W3097758405",
    "https://openalex.org/W4251572712",
    "https://openalex.org/W2888349794",
    "https://openalex.org/W2324964582",
    "https://openalex.org/W3146384714",
    "https://openalex.org/W3088999551",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3010145447",
    "https://openalex.org/W4242308659",
    "https://openalex.org/W1997974358",
    "https://openalex.org/W2970764640",
    "https://openalex.org/W2551217916",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W3046864710",
    "https://openalex.org/W4254949296",
    "https://openalex.org/W2968071222",
    "https://openalex.org/W4233119898",
    "https://openalex.org/W3035505522",
    "https://openalex.org/W4247259022",
    "https://openalex.org/W4229920677",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4251095682",
    "https://openalex.org/W1975147762"
  ],
  "abstract": "Data-driven approaches to retrosynthesis have thus far been limited in user interaction, in the diversity of their predictions, and the recommendation of unintuitive disconnection strategies. Herein, we extend the notions of prompt- based inference in natural language processing to the task of chemical language modeling. We show that by using a prompt describing the disconnection site in a molecule, we can steer the model to propose a wider set of precursors, overcoming training data biases in retrosynthetic recommendations and achiev- ing a 39 % performance improvement over the baseline. For the first time, the use of a disconnection prompt empowers chemists by giving them back greater control over the disconnection predictions, resulting in more diverse and creative recommendations. In addition, in place of a human-in-the-loop strategy, we propose a schema for automatic identification of disconnection sites, followed by prediction of reactant sets, achieving a 100 % improvement in class diversity as compared to the baseline. The approach is effective in mitigating prediction biases deriving from training data. In turn, this provides a larger variety of usable building blocks, which improves the end-user digital experience. We demonstrate its application to different chemistry domains, from traditional to enzymatic reactions, in which substrate specificity is key.",
  "full_text": "Unbiasing Retrosynthesis Language\nModels with Disconnection Prompts\nAmol Thakkar1, *, Alain C. Vaucher1, Andrea Byekwaso1, Philippe\nSchwaller1, Alessandra Toniato1, and Teodoro Laino1\n1IBM Research Europe, Saümerstrasse 4, 8803 Rüschlikon, Switzerland\n*tha@zurich.ibm.com\nData-driven approaches to retrosynthesis have thus far been limited in user\ninteraction, in the diversity of their predictions, and the recommendation of\nunintuitive disconnection strategies. Herein, we extend the notions of prompt-\nbased inference in natural language processing to the task of chemical language\nmodeling. We show that by using a prompt describing the disconnection site\nin a molecule, we can steer the model to propose a wider set of precursors,\novercoming training data biases in retrosynthetic recommendations and achiev-\ning a 39 % performance improvement over the baseline. For the ﬁrst time,\nthe use of a disconnection prompt empowers chemists by giving them back\ngreater control over the disconnection predictions, resulting in more diverse\nand creative recommendations. In addition, in place of a human-in-the-loop\nstrategy, we propose a schema for automatic identiﬁcation of disconnection\nsites, followed by prediction of reactant sets, achieving a 100 % improvement\nin class diversity as compared to the baseline. The approach is eﬀective in\nmitigating prediction biases deriving from training data. In turn, this provides\na larger variety of usable building blocks, which improves the end-user digital\nexperience. We demonstrate its application to diﬀerent chemistry domains,\nfrom traditional to enzymatic reactions, in which substrate speciﬁcity is key.\n1 Introduction\nRetrosynthesis is the task of determining the optimal sequence of steps required to\nsynthesise a given molecule of interest starting from readily available building blocks. It\nwas Corey in the 1960s [1] who pioneered the digitization of the process, followed by a\n1\nrange of approaches from heuristics based expert systems [2, 3, 4], to data-driven deep\nlearning [5, 6, 7, 8, 9, 10]. When performed by domain experts, single-step retrosynthetic\nanalysis, i.e. the breakdown of a target product into its constituent set of precursors, can\nbe seen as a two-step process. First, the expert identiﬁes a suitable site of disconnection,\nconsidering the competitiveness of forming that speciﬁc chemical bond (Figure 1) across\nall others present. Thereafter, the attention focuses on choosing an optimal transformation\nbased on chemo-, regio-, and stereo-selective considerations, while optimizing yields,\nsustainability, and costs. Although the choice of a disconnection site should be based solely\non the downstream synthetic route, it is frequently heavily inﬂuenced by the practitioner’s\nchemistry knowledge. The same is true for data-driven methods that incorporate the\ninherent chemical reactivity bias of training datasets.\nAmong the diﬀerent computer-assisted planning schemes for chemical synthesis[11],\ndeep-learning-based approaches using natural language processing (NLP) have become\npopular [8, 9, 12, 13] thanks to their high prediction accuracy[14], ease of adoption[15],\nseamless extension to novel reaction classes[16], and application to a wide range of\ndigital chemistry tasks[17, 18, 19]. Overall, language models oﬀer the great advantage of\nlearning the rules governing chemical transformations directly from raw data [20] instead\nof requiring the explicit encoding of humanly crafted logic. Commonly relying on the\nuse of the Transformer architecture [21] and the simpliﬁed molecular-input line-entry\nsystem (SMILES) notation [22, 23], NLP models treat the prediction of chemical species\nas a translation task. Given a target molecule, language models suggest the best set of\nprecursors (i.e. reactants, and possibly other reagents) as the translation’s outcome(s),\nwith the possibility to generate multiple such sets. Nevertheless, similar to the human\nbias of favoring more familiar reaction classes, data-driven models exhibit a bias inherited\nfrom reaction datasets used for training. This leads to a poor diversity of predictions, with\nthe proposed retrosynthetic disconnections often belonging to the most abundant reaction\nclasses in training datasets, such as Protection/Deprotection or Oxidation/Reduction for\nthose derived from patents [24, 25]. The inherent bias in these recommendations conceals\nthe broader options encompassed by multiple disconnection sites, thereby restricting\nthe variety of precursors which in turn reduces the eﬀectiveness of any computer-aided\nsynthesis plan. In their current format, data-driven approaches to retrosynthesis aﬀord\nlittle control to users in steering the prediction of the translation’s outcome(s). Here, we\nexplore the use of prompt-based learning to mitigate models biases inherited from training\ndatasets. Prompt-based learning is an emerging paradigm in computer science that opens\nup the possibility of enhancing interaction with AI-based systems to guide inference along\ndirections determined by the input prompt. GPT-3 and DALL-E, capable of generating\nhuman-like text and realistic images given a set of human- or machine-generated inputs\n[26, 27] are a few examples of the success of this technology.\nCompared to previous uses of language models for the prediction of chemical reactions,\n2\nwe introduce, for the ﬁrst time, the concept of steering with human or machine inputs the\nchemical predictions in retrosynthesis inference tasks (Figure 1). We introduce prompts\nthat specify the disconnection site to integrate deep learning algorithms with domain\nknowledge and experience. The disconnection prompts can be human or machine labeled,\nand they are used to steer the translation of a product into a set of precursors, thus\nleading to an improvement in predicted reaction class diversity exceeding 100 %. We\nvalidate this novel scheme both with traditional chemical transformations and enzymatic\nreactions for biocatalysis. The results demonstrate a 39 % performance improvement over\nbaseline models and conﬁrm the possibility to use human-in-the-loop approaches across\nchemical synthesis and biocatalysis for an improved retrosynthesis experience. The use of\nprompt-based learning demonstrates to be an easy to adopt and very eﬀective approach\nfor mitigating biases inherited from training datasets. Ultimately, the use of prompts\nhas the potential to open up new training data acquisition campaigns, that will facilitate\nperformance improvements, diversity, and interactivity of future retrosynthesis models.\nO\nO O\nOH\nO OH\nS OH\nO\nHO\nO\nO\nO OH\nO\nCC(=O)OC(C)=O.O=C(O)c1ccccc1O.O=S(=O)(O)O>>CC(=O)Oc1ccccc1C(=O)O\nReaction SMILES:\nChemical reactions\nprecursors (reactants & agents) product(s)\nAtom-mapped reaction SMILES:\nCC(=O)O[C:2]([CH3:1])=[O:3].[OH:4][c:5]1[cH:6][cH:7][cH:8][cH:9][c:10]1[C:11](=[O:12])\n[OH:13].O=S(=O)(O)O>>[CH3:1][C:2](=[O:3])[O:4][c:5]1[cH:6][cH:7][cH:8][cH:9]\n[c:10]1[C:11](=[O:12])[OH:13]\nCC(=O)OC(C)=O.O=C(O)c1ccccc1O.O=S(=O)(O)O>>C[C:1](=O)[O:1]c1ccccc1C(=O)O\nReaction SMILES with tagged atoms:\ntagging product atom with bond changes\nO\nO OH\nO\nproduct\nArbitrary precursors\nCC(=O)Oc1ccccc1C(=O)O\nRetrosynthesis (how to make a target molecule)\n[previous work]\nO\nO OH\nO\nC[C:1](=O)[O:1]c1ccccc1C(=O)O\nhuman or automatically curated prompt\n(labelled disconnection)\nDisconnection-aware retrosynthesis [this work]\nPrompt-driven\ndisconnection precursor\ngeneration\nTRANSFORMER\nMODEL\n1\n1\natom-mapping (e.g. RXNMapper)\nFigure 1: Left: Overview of the processing pipeline of the chemical reactions represented\nas SMILES. The steps taken to tag the product atoms using atom-mapping\nnotation are outlined. Right: A single-step retrosynthesis (upper) compared\nwith prompt-driven disconnection-aware retrosynthesis (lower).\n2 Results and Discussion\nChemical language modeling predominantly makes use of sequence-to-sequence Transformer\narchitectures. When applied to retrosynthesis, a SMILES string representing the molecule\nof interest is used as input and the model generates the set of precursors (reactants\nand reagents) as the translation’s outcome(s). Henceforth, we consider the ‘Molecular\n3\nTransformer’ (MT) developed by Schwaller et. al. [14,9] as a reference point for development\nof our prompt-drivendisconnection awaremodel (Figure 2) and refer to it as thebaseline\nmodel. The disconnection awaremodel can direct the outcome of a translation using an\nadditional input prompt, as opposed to thebaseline model which generates predictions\ntowards certain disconnections based solely on the underlying probability distribution of\nchemical transformations in training data sets. The use of guiding prompts resulted in\na 39% increased accuracy, and a 100% (2-fold) increase in reaction class diversity of the\ndisconnection awaremodel as opposed to thebaseline.\nN\nO\nOH\nN N\nO\nOH\nN\nN\nO\nOH\nN\nN\nO\nOH\nN\nN\nO\nOH\nN\nO\nOH\nN\nH\nNH2\nO O O\nO\nOH\nN\nH\nNH2\nO O\nHO\nO\nA) \nProduct\nB) \nPermuted Product Tags\n(Incomplete Input)\nC) \nTagged Product\n(Prompt)\nD) \nGround Truth Precursors\nE) \nPredicted Precursors\nBaseline (A -> D)\nAutoPrompt (A -> C)\nDisconnection Aware (C -> D)\nAutocomplete Tags (B -> C)\nPermuted Disconnection Aware (B -> D)\nTrained Models\nData\n1\n1\n1\n1\n11\n1\n11\n1\n1\n1\n1\nPrevious \nWork\nThis \nWork\nFigure 2:Overview of the experiments conducted and models trained illustrated with an\nexample for a heterocycle formation.\n2.1 Disconnection Aware Retrosynthesis\nThe original MT does not natively support user-deﬁned prompts, nor the tokens used for\ntheir representation. It follows that across all datasets examined thebaseline model was\nnot predictive when used in combination with inputs containing tagged disconnection sites\n(prompts). Therefore, we trained thedisconnection awaremodel using product SMILES\ncontaining labeled prompts in the form of atom-tags as shown in Figure 2C as input and\nusing the ground truth precursors as labels (Figure 2D).\n4\nFigure 3:A) Comparing thedisconnection awaremodel against thebaseline across the\nnumber of atoms that constitute the disconnection site. The disconnection\naccuracy reﬂects the ability of the model to predict the correct bond changes,\nfor which we observe thedisconnection awaremodel outperforms thebaseline.\nB) The round trip accuracy refers to the ability to regenerate the required\nproduct from the precursors predicted by thebaseline and disconnection aware\nmodels respectively. The baseline model exhibits slightly higher round trip\naccuracy although the precursors predicted are diﬀerent from the ground truth\nas evidenced by the lower disconnection accuracy. The number of tagged atoms\nhas been re-calibrated to the predicted bond changes. C) Tag distribution across\nthe Pistachio, USPTO, and USPTO50k datasets. The performance of the models\nis seen to correlate with the availability of training data for a given number of\ntags.\nOf primary interest were two metrics. First, the round trip accuracy as developed by\nSchwaller et. al. [9] to determine whether the desired product could be regenerated from\nthe predicted precursors, and second the disconnection accuracy, to determine whether the\npredicted precursors corresponded to disconnection at the user-deﬁned position (Figure 3).\nIn addition, the accuracy metric was reported across the number of tagged atoms, rather\nthan taking an overall topN accuracy. This metric oﬀers a more granular understanding\nof the types of disconnections present in the dataset, how well they are reproduced, and\nwhether there is a tendency for the model to be biased towards a given disconnection type,\n5\ninformation that cannot be obtained from the topN metric alone.\nWe determined that thedisconnection awaremodel outperformed thebaseline model,\nby an average of 39 % across all number of atom-tags and datasets with respect to the\ndisconnection accuracy (Figure 3A). This demonstrates that thedisconnection awaremodel\nis better able to reconstruct reactions corresponding to a user-speciﬁed disconnection\nthan thebaseline model. In addition, we observe that the disconnection and round trip\naccuracies correlate with the availability of training data for a given number of atom tags\n(Figure 3A & B). Most notable is the performance drop when the number of tags equals\n4, due to no examples being present in the USPTO50k dataset we processed. However,\ndespite there being no training examples containing 4 atom-tags in the USPTO50k dataset,\nthe model was still able to recover 20 % of the reactions, thereby demonstrating the model\ncan extrapolate to unseen reactions. Similarly, low performance was observed when the\nnumber of tags was greater than 5, owing to a decrease in the availability of training data.\nFurthermore, we observed a bias in the patent data towards disconnection sites involving\ntwo atoms (i.e. one bond change). These ﬁndings are in line with that observed from\nsurveys conducted by Böstrom et. al. and others studying the frequency of the types\nof reactions that are reported in both public and proprietary datasets [28, 29]. Despite\nthe bias towards reactions involving one bond change, thedisconnection awaremodel\ncan produce predictions across all number of atom tags, including that for heterocycle\nformation as shown in Figure 4. Unlike previous approaches developed for predicting ring\ndisconnections which aﬀord the user no control over which ring system to disconnect [30],\nthe disconnection awaremodel allows chemists to target speciﬁc ring systems via the\ntagging mechanism.\n2.1.1 Assessing the Impact of Reagents\nWhile the correct disconnection and reactants as the ground truth are often predicted,\nthere remain a set of variable species, so-called reagents, that may not match the ground\ntruth (see Figure 4). This can be explained considering the role of the diﬀerent chemical\nspecies in sequence-based model training. Usually, the major components (i.e. those that\ncontribute atoms to the product) can be atom-mapped [31, 20]. Thus, these must be\npresent for the reaction to occur. The unmapped components correspond to solvents, bases,\nadditives, catalysts, etc., which can be variable. For the disconnection aware retrosynthesis\ntask, we performed two diﬀerent analyses: one including all species, and the other excluding\nunmapped species. The results when removing the unmapped species show an increase\nin the percentage of predictions matching the ground truth from 14 % to 41 % for the\nUSPTO dataset. Comparable performance increases were seen across the other datasets\nexamined. Although the percentage of predictions matching the ground truth increased\nsigniﬁcantly, both round trip and disconnection accuracy remained comparable to the\nbaseline values regardless of the removal of the unmapped species. Thus, we can conclude\n6\nO\nNNF F\nF\nO\nOH\nOH\nO\nO\nNNF F\nF\nO\nHCl\nNa+OH-\nO\nOH\nO\nO\nNNF F\nF\nO Na+OH-\nOHO\nO\nO O\n1\nO\nH2O\nO\nS\nO\nHO OH O\nCrO O\nOHO\nO O\nO\nH2O\nO\nS\nO\nHO OH O\nCrO O\nOHO\nO O\n1\n1\nN\nNCl Br\nH2O\nNa+OH- 11\n1\nN O\nN Cl Br\nH- Na+\nO\nN\nO\nN\nH\nN\nN\nO\nO\nO\n1\n1 1 1\nN NH2N\nO\nO\nO\nON\nO\nHO\nO\nN\nC\nN\nN\nHClOH\nN\nN\nN\nN NH2N\nO\nO\nO\nON\nO\nHO\nO\nO\nN HN\nN\nO\nOH\nO\nN\nH\nO H\nN\nNH1 1\n1\n1\n1\nO\nO\nO\nN\nH\nO H\nN\nNH\nHCl\nH2O\nN\nCF3\nN\nH\nNN\nN\nH\nH2N\n1 1\n1 1\n11\nN\nCF3\nN\nH\nNNC\nS\nH2N NH2\nOH\nN\nCF3\nN\nH\nNNC\nS OH\nN2H2\nNumber of\nTagged\nAtoms\n1\n2\n3\n4\n5\n6\nGround Truth Precursors Tagged Product Predicted Precursors\nSame reactants \nand \ndisconnection\nExact Match\nSame reactants \nand \ndisconnection\nSame reactants \nand \ndisconnection\nSame\ndisconnection\nWrong Prediction\nFigure 4:Examples comparing the predicted precursors from thedisconnection aware\nmodel to the ground truth as extracted from the test set. Diﬀerent sizes of\ndisconnection site are shown as as labelled by atom-tags. Thedisconnection aware\nmodel is able to target speciﬁc sites of disconnection and produce appropriate\nprecursors in line with the ground truth. Although the major components are\npredicted as per the ground truth in most cases, some variability exists in the\nreagents that are predicted for the transformation.\nthat the diﬀerences in predictive performance do not arise from variations in the prediction\nof species that contribute to the product but are a result of the unmapped species which\nare variable. We also observe that this has negligible inﬂuence on the ability to predict\nthe correct product using the forward model, most likely due to the sparse sampling of\nthe chemical space in datasets used for training.\n2.2 Auto-complete Tags - Accounting for Incomplete Prompts\nGiven that the transformer-based models trained in this study are created to be interactive,\nthe model should be robust to user input. This means that a user may input a set of\nincomplete tags. To simulate this event, we permuted the tags up to a pre-set limit\nof 4-tags as shown in Figure 2. Three experiments were conducted to determine which\n7\napproach to take when dealing with incomplete user input. First (Expt-1), we inferred\ndirectly from incomplete input using thedisconnection awaremodel. Second (Expt-2), we\nauto-completed the disconnection site using the model termedauto-complete tagsfollowed\nby inference with thedisconnection awaremodel. Third (Expt-3), we inferred using a\nvariation of the disconnection aware model trained using permuted tags as input and the\nground truth precursors as training labels, aptly named thepermuted disconnection aware\nmodel (outlined in Figure 2). The results for these approaches are shown in Figure 5\nand are compared to thedisconnection awaremodel with complete tags for the USPTO\ndataset. Comparable results were obtained for the Pistachio and USPTO50k datasets (as\nshown in the SI).\nFigure 5:A) Thedisconnection awaremodel with complete tags remains the most per-\nformant for disconnection accuracy, while the approaches used for dealing with\nincomplete user input exhibit comparable performance. B) The round trip accu-\nracy across the number of tagged atoms is comparable up to the number of tags\nequals 6, thereafter a deviation occurs, whereby thedisconnection awaremodel\ndecreases in performance and the Expt-1, inference directly from incomplete\nuser input, maintains almost 50 % round trip accuracy.\nAll three strategies outlined for handling incomplete atom-tags showed comparable\nperformance for disconnection and round trip accuracy when the number of tagged atoms\nis below 6. Beyond 6 tagged atoms, a deviation occurs in the round trip accuracy, which\nfavors the direct inference from incomplete input using thedisconnection awaremodel\n(Expt-1). Furthermore, all three strategies exhibit a lower disconnection accuracy than the\nmodel given complete tags. Therefore, thedisconnection awaremodel shows a certain level\n8\nof robustness to incomplete disconnection sites albeit with a slight decrease in disconnection\naccuracy, and comparable round trip metric.\n2.3 Prompt-Driven Steering of Retrosynthesis Prediction\nHaving demonstrated that thedisconnection awaremodel outperforms thebaseline Molec-\nular Transformer, and can reproduce the required set of precursors when prompted with a\nlabel tagging the disconnection site, we studied whether valid sets of precursors would be\ngenerated for arbitrary disconnections. In doing so we investigated whether thedisconnec-\ntion awaremodel could be steered to produce alternative outputs for the same molecule,\nan example of which is shown in Figure 6. The results show that thedisconnection aware\nmodel is capable of distinguishing between similar sites of disconnection and producing\nvalid sets of precursors as shown for sites labelled 1 and 2 in Figure 6. The ability to\narbitrarily specify sites of disconnection, and prompt the model towards alternative outputs\nopens up new avenues for chemical language modelling, which serve to facilitate user\ninteraction with the model. For example, current models are criticised for their lack of\ndiversity in the choice of disconnection site, resulting in predictions with identical building\nblocks and a wide variety of reagents. ‘Human-in-the-loop’ prompting of thedisconnection\naware model is an eﬀective strategy to mitigate this behaviour by enabling the steering of\nthe model towards alternative reaction classes, therefore improving prediction diversity as\nshown in Figure 6. Furthermore, as a consequence of uncovering the ability to steer model\npredictions, we propose that prompting thedisconnection awaremodel can be used to\ninvestigate chemical language modelling more generally.\n9\nN\nNH\nN\nO\nF F\nF S\nN\nO\nH\nN\nO\nN\nOH\nO\nO\nF\nF F\nO\nN\nCl\nS\nO\nO\n-O O- Cs+Pd\nOH\nN\nOH\nN\nO\nN\nH2N\nN\nF\nF F\nO\nN\nNH2\nS\nOH\nN\nOH\nN\nO\nN\nBr\nN\nTarget Molecule - Prompts Annotated\nPrompt Driven Predicted Precursors\nXantphos\n11\n2 2\nO\nO\nO\nO\n-O O- Cs+Pd\nXantphos\n3 3 4\n4\n5\nN\nO\nN\nN\nN\nN+N\nNH\nN\nO\nF F\nF S\nN\nO\nOH FP- FFF\nF F\nOH\nN\nO\nO N\nOH\nDPEA\n1\n2\n3\nN\nNH\nN\nO\nF F\nF S\nN\nO\nH\nN\nO\nOH\nOH\nHN N\nO\nN\nN\nN\nN+\nFP- FFF\nF F\nDPEA\nN+N\nNH\nN\nO\nF F\nF S\nN\nO\nH\nN\nO\nN\nO Si\nF-\nTHF\nnBunBu\nnBu nBu\n4\n5\nFigure 6:Human curated prompts for steering retrosynthetic prediction. Prompts are\nlabelled on the target molecule and the predictions listed with the corresponding\nlabel (left column).\n10\n2.4 Improved Class Diversity for Automatic Retrosynthesis\nWhile the focus thus far has been on ‘human-in-the-loop’ retrosynthesis, thedisconnection\naware model can be adapted to cases where human interaction is not possible or not\ndesired, such as automatic or multi-step retrosynthesis prediction. We show that using a\nmodel trained to automatically label disconnection sites, which we callAutoTag, followed\nby inference with thedisconnection awaremodel improves predicted reaction class diversity\nby at least a factor of 2 (100% increase) for the USPTO dataset in comparison to the\nbaseline model(Figure 7B).\nThe number of tags predicted by theAutoTag model follows approximately the same\ndistribution as the ground truth data and inherits the same type of disconnection bias. No-\ntably, the model tends to predict disconnection sites that are larger than those represented\nin the ground truth dataset, as shown in Figure 7A. Although performance decreases with\nthe size of the disconnection site as represented by the number of tagged atoms (Figure\n3), we found that SMILES length, thus the size of the molecule, can be a detrimental\nfactor (Figure 7C). As the number of predictions for a given SMILES increases (topN), the\nperformance of theAutoTag model deteriorates concerning the ability to reproduce the\noriginal input. However, we found that for 96 % of the reactions in the datasets examined\n(SMILES length between 25 and 100 characters) the error was within an acceptable range\nfor the top10 predictions as shown in Figure 7. Figure 7D shows selected examples of the\nAutoTag predicted disconnection sites and precursors for a given molecule (more details\nfor all predictions are available in the SI).\n11\nPrediction\nO\nN+\nO\nO-\nO\nHN NH2\nOH\nO\nOHN+\nO\n-O\nF\nO\nN+\nO\nO-\nO\nHN\nO\nH2N\nO\nN+\nO\n-O\nO\nOH O\nNN N\nN\n1, 3\n2\nO\nN+\nO\nO-\nO\nHN\nS\nO\nN\nH\nO\nN+\nO\n-O\nCl\nHO\nH2O O\n-O O-K+\n5\nO\nN+\nO\nO-\nO\nHN\nN\nH\nO\nN+\nO\n-O\nF\nS\nO O\n-O O- K+7\nAutoTagged Product Predicted Precursors\n11, 3\n1, 3\n1, 3\n1, 3\n2\n2\n5 5\n7\n7\nD) AutoTag Predictions\nFigure 7:A) Distribution of predicted atom-tags as a function of the fraction of the dataset\ncompared to the ground truth. Disconnections sites consisting of 3 or more\ntagged atoms are over-sampled by theAutoTag model, although the general\ndistribution follows the same trend as the ground truth. B) Class diversity\ncorresponds to the number of unique reaction classes predicted per sample\nacross the top10 predictions. At least a two-fold improvement in class diversity\nis observed when using theAutoTag workﬂow for automatic retrosynthesis.\nThe disconnection aware model predicts only one reaction class by speciﬁc\ndisconnection site labelling. C) The percentage of predictions across the top10\nmatching the input as a function of SMILES length. In 96 % of cases, the errors\nfall within an acceptable range. D) Predictions obtained using theAutoTag\nworkﬂow for a given sample taken from the test set. The prediction rank is used\nfor labelling the disconnection site for the Figure.\n2.5 Extension to Enzyme Catalysis\nProbst et. al. [32] and Kreutter et. al. [33] have previously demonstrated that sequence to\nsequence transformer models can be applied to enzymatic reactions, thus broadening the\nscope of language models in chemistry to biocatalysis. Here, we extended the disconnection-\naware retrosynthesis approach by applying our atom-tagging procedure to enzymatic\nreactions using the ECReact data set [32], and subsequently training models following\nthe approach described by Probst et. al. [32]. A key distinction in the treatment of the\nenzymatic reaction SMILES was the inclusion of the enzymeEC number. To tag the\ndisconnection site, the enzymeEC number was omitted to facilitate atom-mapping with\n12\nRXNMapper [20]. However, it was reintroduced into the reaction SMILES for training\nand inference tasks. The trained model achieved an average disconnection accuracy of 79\n%, exceeding the performance on the Pistachio, USPTO, and USPTO50k datasets, and an\naverage round trip accuracy of 52 % across all number of tagged atoms. Figure 8 shows a\nfew examples of some of the predictions from thedisconnection awaremodel trained on\nenzymatic data.\nNH2\nHS\nO\nOH\nNH2\nS\nNH2\nO\nHO\nO\nOH\nH2O\nEC: 4.4.1.8\nNH2\nS\nNH2\nO\nHO\nO\nOH\nH2O\nEC: 4.4.1.1\nOH\nO\nO- O\nO\nH2OO\nO\nH2O\nEC: 3.1.1.83\nEC: 3.1.1.83\nO\nHO\nNH2O\nN+\nO\nOP\nO\nO-OP\nO\nO-O\nO\nNN\nH2N N\nN\nOH\nOH\nHO\nOH\nHO\nNH2O\nN+\nO\nOP\nO\nO-OP\nO\nO-O\nO\nNN\nH2N\nN\nN\nO P\nO\n-O O-\nOH\nHO\nOH\nEC: 1.1.1.145 EC: 1.1.1.145\nNumber of \nTagged \nAtoms\n1\n3\n5\nGround Truth \nPrecursors\nProduct\nDifferent EC \nNumber\nPredicted\nPrecursors\nExact Match\nDifferent \nPrecursors\n1\n1\n1\n1\n1\n1\n1 1 1\nFigure 8: Examples from the enyzme test set with diﬀerent numbers of tagged atoms.\n13\n3 Conclusion\nThis study introduces for the ﬁrst time the use of a prompt-driven language model for\nmitigating the prediction biases inherited from trained data. We demonstrate that a\nprompt-driven disconnection awaremodel is very eﬀective in steering the outcome of a\nsequence-to-sequence language model achieving up to 73 % prediction accuracy, which is a\n39% performance improvement over the baseline model when trained on the same dataset.\nSimilar improvements extend to enzymatic data, for which we achieve an overall 79 %\nprediction accuracy.\nWe also extended thedisconnection awaremodel to be compatible with automatic or\nmulti-step predictions, utilizing a model trained to automatically label the disconnection\nsite, which we termAutoTag. The use of thedisconnection awaremodel in combination\nwith theAutoTag model demonstrates an improved performance on reaction class diver-\nsity by a 2-fold factor (100% increase), providing a very eﬀective solution to overcome\ncriticisms towards the reduced diversity performance of traditional Molecular Transformer\narchitectures.\nThis work marks a shift from previous ideas exploring the use of sequence-to-sequence\nTransformer architectures for chemical language modelling. Similar to the approach\nfollowed by human experts when performing a retrosynthetic analysis, the use of a prompt-\nbased language guides the inference of reaction prediction models towards those reactions\nthat involve the creation of a selected bond or set of bonds. The use of prompt-based\nlanguage models opens up the possibility to use human-in-the-loop approaches across\nchemical synthesis and biocatalysis for an improved retrosynthesis experience. Finally,\nthe use of the human-designed prompts opens the door to systematic improvements in\nretrosynthetic planning tools, thanks to the eﬀective decision-making combination of expert\nknowledge and deep learning.\n4 Methodology\nData and Preprocessing: The United States Patent Oﬃce extracts comprised of the\nPistachio dataset (2022Q1) from NextMove software [24], the publicly available subset\nUSPTO extracted by Lowe [34, 25], and the USPTO50k subset from Schneider et. al. were\nused in this study [35]. An extension of the method to ECReact containing enzyme data\nas curated by Probst et. al. was used for the enzymatic model [32]. Each dataset consisted\nof reactions in the SMILES notation, atom-mapped using RXNMapper as shown in Figure\n1 [20]. Atom-mapping was required in order to determine which atoms and bonds had\nchanged in the reaction for subsequent labelling of the disconnection site. The datasets\nwere pre-processed using an internal preprocessing pipeline to ﬁlter for reactions with\none product, between 2 and 10 reactants, and token constraints as speciﬁed in the SI.\n14\nReactions with no tagged atoms were removed, as they imply no atom-bond changes were\ndetected, therefore no reaction occurred. Reactions with greater than 10 tagged atoms\nwere also removed. The datasets were split into training, validation, and test sets in a\n90:5:5 ratio respectively and tokenized using a regex pattern as described by Schwaller et.\nal. [14].\nPrompt Generation - Extracting Atom-Tags:Prompts can be both human-curated, or\nautomatically extracted if relevant training data is present. In this study, we automatically\nextract a prompt from a reaction SMILES by identiﬁcation of the changed atoms and bonds\nin the product. The automatically extracted prompts are solely used for model training in\nplace of human-curated prompts. The prompt corresponds to the site of disconnection\nand is labelled in the product SMILES by identiﬁcation of the atoms for which the bond\norder diﬀered between reactants and products for a given atom-mapped reaction SMILES.\nAtom-mapping was removed from the precursors and products after the disconnection\nsite was identiﬁed, and atom-tags introduced to the product using the SMARTS notation\n[*:1],[36] where ‘*’ resembles any atom, to signify atoms to be changed as shown in Figure\n1 and 2. The pseudo-code is outlined in the supporting information. The prompt labelled\ndata containing atom-tags were used to train thedisconnection awaremodel as shown in\nFigure 2C using the ground truth precursors as labels (Figure 2D). We additionally tested\nthe baseline model to determine if prompts were tolerated.\nTag Permutation: Given that thedisconnection awaremodel is designed to introduce a\nhuman-in-the-loop component for retrosynthesis, we took into consideration that there may\nbe incompletely tagged disconnection sites. Therefore, tag permutations were generated\nfor all products to emulate incomplete user input. Products with the number of tags equal\nto one were omitted, given no permutations are possible. All remaining products were\npermuted up to a pre-set limit of four tagged atoms upon sampling to avoid a too large\nnumber of permutations. For example, a ﬁve-membered ring system would have up to\nfour of the atom-tags permuted, an example of which is shown in Figure 2. The permuted\ndata as shown in Figure 2B was used as input to train a model using the ground truth\nprecursors as labels (Figure 2D), so-called thepermuted disconnection awaremodel. We\nadditionally used the permuted data to test whether thebaseline and disconnection aware\nmodels could tolerate incomplete input.\nTag Completion:An alternative approach to deal with incompletely tagged disconnection\nsites is to introduce an auto-completion model as a step between user input and prediction\nof a set of precursors, henceforth referred to as theautocomplete tagmodel. The model was\ntrained as a supervised learning task, where the input was our generated set of permuted\natom-tags that emulate incomplete user input. The corresponding labels consisted of the\ncompleted atom-tags as extracted automatically from the reaction SMILES.\nAutoPrompt - automatic labelling of disconnection sites:Although thedisconnection\naware model was originally intended to be used in a human-in-the-loop manner, we\n15\nobserved notable improvements to predicted reaction class diversity arising from the fact\nthat multiple disconnection sites can be labelled, thus generating alternative precursor sets.\nThis led us to develop a mechanism by which thedisconnection awaremodel can be used\nfor automated retrosynthesis. We introduce theAutoTag model trained on the product\nSMILES (Figure 2A) as input and the atom-tagged SMILES (Figure 2B) as labels.\nTraining: All models used supervised learning and a seq-seq Transformer architecture as\nimplemented in the OpenNMT-py library version 1.0.0 [37, 38]. The transformer models\nfor the enzyme dataset were trained using the adaptation and commands outlined by\nProbst et. al. [32].\nMetrics and Analysis: Evaluation was performed on the hold-out test set using the\nround-trip metric as established by Schwaller et. al. [14] in addition to metrics deﬁned in\nthis manuscript to evaluate the disconnection accuracy. All evaluations were conducted\nafter standardisation of the reaction components. The disconnection accuracy is determined\nby running a retrosynthetic translation to obtain the set of predicted precursors. The top1\npredicted precursor is then fed into a forward reaction prediction model trained on the\nPistachio dataset [24], in this case the default model used by the Molecular Transformer,\nand returns a predicted product [9]. The predicted product is evaluated to determine\nwhether it matches the ground truth, which constitutes the round trip accuracy. By\nextension the disconnection accuracy is computed by reconstituting the reaction using\nthe top1 predicted precursors and corresponding predicted product from the forward\nmodel, remapping with RXNMapper [20], and recomputing the tagged atoms, thus the\ndisconnection site. Evaluation is then performed to determine if the disconnection site\nobtained from the prediction matches the ground truth. For a complete set of models\ntrained and evaluations refer the supporting information.\nEvaluations were conducted to examine the topN accuracy across a variety of metrics,\nin addition to examining the performance across the number of tagged atoms.\n5 Author Contributions\nThe project was conceived and planned by Amol Thakkar, Alain C. Vaucher and Teodoro\nLaino. Andrea Byekwaso performed preliminary studies on the disconnection aware model,\nwith support from Alain C. Vaucher, Philippe Schwaller and Alessandra Toniato. Amol\nThakkar extended the analysis and improved the original ideas. Amol Thakkar trained all\nthe models presented in this work. Amal Thakkar wrote the manuscript with input from\nall authors.\n6 Competing Interests\nThe authors declare no competing interests.\n16\n7 Supplementary Information\n7.1 Code availability\nCode to identify the disconnection sites and train the models can be found on our GitHub\nrepository:\nhttps://github.com/rxn4chemistry/disconnection_aware_retrosynthesis\n7.2 Data availability\nThe US patent extract text mined by Lowe are publicly available and can be processed\nusing code on our GitHub repository [25, 34].\nReferences\n[1] Corey, E. J. & Wipke, W. T. Computer-Assisted Design of Complex Organic Syntheses.\nScience 166, 178 (1969). Number: 3902.\n[2] Thakkar, A.et al. Artiﬁcial intelligence and automation in computer aided synthesis\nplanning. Reaction Chemistry & Engineering6, 27–51 (2021). Number: 1 Publisher:\nThe Royal Society of Chemistry.\n[3] Szymkuć, S.et al. Computer-Assisted Synthetic Planning: The End of the Beginning.\nAngewandte Chemie International Edition55, 5904–5937 (2016). Number: 20.\n[4] Mikulak-Klucznik, B. et al. Computational planning of the synthesis of complex\nnatural products. Nature (2020).\n[5] Segler, M. H. S., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep\nneural networks and symbolic AI.Nature 555, 604–610 (2018). Number: 7698.\n[6] Genheden, S.et al. AiZynthFinder: a fast, robust and ﬂexible open-source software\nfor retrosynthetic planning.Journal of Cheminformatics12, 70 (2020). Number: 1.\n[7] Chen, B., Li, C., Dai, H. & Song, L. Retro*: Learning Retrosynthetic Planning with\nNeural Guided A* Search. In III, H. D. & Singh, A. (eds.)Proceedings of the 37th\nInternational Conference on Machine Learning, vol. 119 ofProceedings of Machine\nLearning Research, 1608–1616 (PMLR, 2020).\n[8] Karpov, P., Godin, G. & Tetko, I. V. A Transformer Model for Retrosynthesis. In\nTetko, I. V., Kůrková, V., Karpov, P. & Theis, F. (eds.)Artiﬁcial Neural Networks\nand Machine Learning – ICANN 2019: Workshop and Special Sessions, 817–830\n(Springer International Publishing, Cham, 2019).\n17\n[9] Schwaller, P.et al.Predicting retrosynthetic pathways using transformer-based models\nand a hyper-graph exploration strategy.Chemical Science 11, 3316–3325 (2020).\nNumber: 12 Publisher: The Royal Society of Chemistry.\n[10] Coley, C. W.et al. A Robotic Platform for Flow Synthesis of Organic Compounds\nInformed by AI Planning.Science 365, eaax1566 (2019). Number: 6453.\n[11] Shen, Y.et al. Automation and computer-assisted planning for chemical synthesis.\nNature Reviews Methods Primers1, 23 (2021).\n[12] Duan, H., Wang, L., Zhang, C., Guo, L. & Li, J. Retrosynthesis with attention-based\nNMT model and chemical analysis of “wrong” predictions.RSC Adv.10, 1371–1378\n(2020). Publisher: The Royal Society of Chemistry.\n[13] Lee, A. A.et al. Molecular Transformer uniﬁes reaction prediction and retrosynthesis\nacross pharma chemical space.Chem. Commun.55, 12152–12155 (2019). Publisher:\nThe Royal Society of Chemistry.\n[14] Schwaller, P.et al. Molecular Transformer: A Model for Uncertainty-Calibrated\nChemical Reaction Prediction.ACS Central Science5, 1572–1583 (2019). Number:\n9.\n[15] Healy, E. F. & Blade, G. Tips and Tools for Teaching Organic Synthesis Online.\nJournal of Chemical Education97, 3163–3167 (2020).\n[16] Pesciullesi, G., Schwaller, P., Laino, T. & Reymond, J.-L. Transfer learning enables the\nmolecular transformer to predict regio- and stereoselective reactions on carbohydrates.\nNature Communications11, 4874 (2020).\n[17] Vaucher, A. C. et al. Automated extraction of chemical synthesis actions from\nexperimental procedures. Nature Communications11, 3601 (2020).\n[18] Vaucher, A. C.et al.Inferring experimental procedures from text-based representations\nof chemical reactions.Nature Communications12, 2573 (2021).\n[19] Toniato, A., Schwaller, P., Cardinale, A., Geluykens, J. & Laino, T. Unassisted noise\nreduction of chemical reaction datasets. Nature Machine Intelligence 3, 485–494\n(2021).\n[20] Schwaller Philippe, Hoover Benjamin, Reymond Jean-Louis, Strobelt Hendrik &\nLaino Teodoro. Extraction of organic chemistry grammar from unsupervised learning\nof chemical reactions.Science Advances7, eabe4166 (2022). Publisher: American\nAssociation for the Advancement of Science.\n18\n[21] Vaswani, A.et al. Attention is all you need. InAdvances in neural information\nprocessing systems, 5998–6008 (2017).\n[22] Weininger, D. SMILES, a chemical language and information system. 1. Introduction\nto methodology and encoding rules.Journal of Chemical Information and Computer\nSciences 28, 31–36 (1988). Publisher: American Chemical Society.\n[23] Weininger, D., Weininger, A. & Weininger, J. L. SMILES. 2. Algorithm for Generation\nof Unique SMILES Notation.Journal of Chemical Information and Computer Sciences\n29, 97–101 (1989). Number: 2 Publisher: American Chemical Society.\n[24] Software, N. Pistachio. URL https://www.nextmovesoftware.com/pistachio.\nhtml.\n[25] Lowe, D. Chemical Reactions from US Patents (1976-Sep2016) (2018).\nURL https://figshare.com/articles/Chemical_reactions_from_US_patents_\n1976-Sep2016_/5104873.\n[26] Brown, T.et al. Language models are few-shot learners.Advances in neural informa-\ntion processing systems33, 1877–1901 (2020).\n[27] Ramesh, A.et al. Zero-shot text-to-image generation. InInternational Conference on\nMachine Learning, 8821–8831 (PMLR, 2021).\n[28] Boström, J., Brown, D. G., Young, R. J. & Keserü, G. M. Expanding the medicinal\nchemistry synthetic toolbox.Nature Reviews Drug Discovery17, 709–727 (2018).\n[29] Schneider, N., Lowe, D. M., Sayle, R. A., Tarselli, M. A. & Landrum, G. A. Big Data\nfrom Pharmaceutical Patents: A Computational Analysis of Medicinal Chemists’\nBread and Butter.Journal of Medicinal Chemistry59, 4385–4402 (2016). Number:\n9 Publisher: American Chemical Society.\n[30] Thakkar, A., Selmi, N., Reymond, J.-L., Engkvist, O. & Bjerrum, E. J. ‘Ring Breaker’:\nNeural Network Driven Synthesis Prediction of the Ring System Chemical Space.\nJournal of Medicinal Chemistry63, 8791–8808 (2020). Number: 16.\n[31] Lin, A.et al. Atom-to-atom Mapping: A Benchmarking Study of Popular Mapping\nAlgorithms and Consensus Strategies.Molecular Informatics41, 2100138 (2022).\n[32] Probst, D.et al. Biocatalysed synthesis planning using data-driven learning.Nature\nCommunications 13, 964 (2022).\n[33] Kreutter, D., Schwaller, P. & Reymond, J.-L. Predicting enzymatic reactions with a\nmolecular transformer. Chemical Science12, 8648–8659 (2021).\n19\n[34] Lowe, D.Extraction of chemical structures and reactions from the literature. Doctoral\nthesis, University of Cambridge (2012).\n[35] Schneider, N., Stieﬂ, N. & Landrum, G. A. What’s What: The (Nearly) Deﬁnitive\nGuide to Reaction Role Assignment.Journal of Chemical Information and Modeling\n56, 2336–2346 (2016). Publisher: American Chemical Society.\n[36] Daylight Theory: SMARTS - A Language for Describing Molecular Patterns. URL\nhttps://www.daylight.com/dayhtml/doc/theory/theory.smarts.html.\n[37] Klein, G., Kim, Y., Deng, Y., Senellart, J.&Rush, A. OpenNMT:Open-SourceToolkit\nfor Neural Machine Translation. InProceedings of ACL 2017, System Demonstrations,\n67–72 (Association for Computational Linguistics, Vancouver, Canada, 2017).\n[38] OpenNMT-py. URLhttps://github.com/OpenNMT/OpenNMT-py.\n20",
  "topic": "Disconnection",
  "concepts": [
    {
      "name": "Disconnection",
      "score": 0.9697365760803223
    },
    {
      "name": "Computer science",
      "score": 0.7004907727241516
    },
    {
      "name": "Retrosynthetic analysis",
      "score": 0.6287816762924194
    },
    {
      "name": "USable",
      "score": 0.5734078288078308
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5693497657775879
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4743686616420746
    },
    {
      "name": "Code refactoring",
      "score": 0.4649113118648529
    },
    {
      "name": "Natural language processing",
      "score": 0.43414685130119324
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.42514508962631226
    },
    {
      "name": "Machine learning",
      "score": 0.42407679557800293
    },
    {
      "name": "Training set",
      "score": 0.4105425477027893
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3468095660209656
    },
    {
      "name": "Software",
      "score": 0.1923966109752655
    },
    {
      "name": "Programming language",
      "score": 0.11261692643165588
    },
    {
      "name": "World Wide Web",
      "score": 0.10264846682548523
    },
    {
      "name": "Chemistry",
      "score": 0.09482750296592712
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Total synthesis",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    }
  ]
}