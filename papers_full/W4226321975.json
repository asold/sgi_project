{
  "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language",
  "url": "https://openalex.org/W4226321975",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2643549849",
      "name": "Liangke Gui",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2224734793",
      "name": "Borui Wang",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2146202550",
      "name": "Qiuyuan Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227866007",
      "name": "Alexander Hauptmann",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2296285037",
      "name": "Yonatan Bisk",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W3119323185",
    "https://openalex.org/W2563399268",
    "https://openalex.org/W2963477107",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W3196798856",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4309067651",
    "https://openalex.org/W2964303913",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4205191348",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3101703188",
    "https://openalex.org/W3093200502",
    "https://openalex.org/W2962967746",
    "https://openalex.org/W3172845486",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3209118325",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W102708294",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, Jianfeng Gao. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 956 - 968\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nKAT: A Knowledge Augmented Transformer for Vision-and-Language\nLiangke Gui§‡ Borui Wang†‡ Qiuyuan Huang‡\nAlex Hauptmann§ Yonatan Bisk§‡ Jianfeng Gao‡\n§Carnegie Mellon University †Yale University ‡Microsoft Research\n{liangkeg, alex, ybisk}@cs.cmu.edu\nborui.wang@yale.edu, {qihua, jfgao}@microsoft.com\nAbstract\nThe primary focus of recent work with large-\nscale transformers has been on optimizing the\namount of information packed into the model’s\nparameters. In this work, we ask a complemen-\ntary question: Can multimodal transformers\nleverage explicit knowledge in their reasoning?\nExisting, primarily unimodal, methods have\nexplored approaches under the paradigm of\nknowledge retrieval followed by answer predic-\ntion, but leave open questions about the qual-\nity and relevance of the retrieved knowledge\nused, and how the reasoning processes over\nimplicit and explicit knowledge should be in-\ntegrated. To address these challenges, we pro-\npose a - Knowledge Augmented Transformer\n(KAT) - which achieves a strong state-of-the-\nart result (+6% absolute) on the open-domain\nmultimodal task of OK-VQA. Our approach\nintegrates implicit and explicit knowledge in\nan encoder-decoder architecture, while still\njointly reasoning over both knowledge sources\nduring answer generation. Additionally, ex-\nplicit knowledge integration improves inter-\npretability of model predictions in our analysis.\nCode and pre-trained models are released at\nhttps://github.com/guilk/KAT.\n1 Introduction\nThere has been a revival of interest in knowledge-\nintensive tasks which require an external knowl-\nedge source for humans to perform. Many applica-\ntions in real-world scenarios, such as autonomous\nAI agents, need to seamlessly integrate implicit\n(i.e., commonsense) and explicit knowledge (e.g.,\nWikidata) to answer questions. In this work, we\ninvestigate how to effectively integrate implicit and\nexplicit knowledge for reasoning. Tasks like Out-\nside Knowledge Visual Question Answering (OK-\nVQA) (Marino et al., 2019) require that models use\nknowledge not present in the input to answer ques-\nWork done when Liangke and Borui interned at Microsoft\nResearch.\nFigure 1: Examples of knowledge-based VQA that re-\nquires external knowledge. Success on this task requires\nnot only visual recognition, but also logical reasoning\nto incorporate external knowledge about the world.\ntions, making it an ideal test bed for investigating\nthis implicit-explicit knowledge trade-off.\nConsider the examples from OK-VQA shown in\nFigure 1. To answer the question in the left exam-\nple, the system needs to both ground organism to\nbird through explicit knowledge and then apply the\nimplicit knowledge birds evolved from reptilesto\nanswer the question. Similarly for the question in\nthe right example, the system needs to recognize\nboats and harbor and requires the implicit knowl-\nedge anchors are used to stop boats from moving.\nA key challenge here is to accurately link image\ncontent to abstract external knowledge. There have\nbeen a number of recent developments demonstrat-\ning the feasibility of incorporating external knowl-\nedge into Question Answering models (Wang et al.,\n2017b; Li et al., 2020b; Marino et al., 2021; Wu\net al., 2022; Garderes et al., 2020). Existing meth-\nods first retrieve external knowledge from external\nknowledge resources, such as DBPedia (Auer et al.,\n2007) and ConceptNet (Liu and Singh, 2004) be-\nfore jointly reasoning over the retrieved knowledge\nand image content to predict an answer.\nHowever, most existing approaches have several\ndrawbacks. First, explicit knowledge retrieved us-\ning keywords from questions or image tags may be\n956\ntoo generic, which leads noise or irrelevant knowl-\nedge during knowledge reasoning. Second, exist-\ning work mainly focuses on explicit knowledge\nwhich is often in the form of encyclopedia articles\nor knowledge graphs. While this type of knowl-\nedge can be useful, it is insufficient to answer many\nknowledge-based questions. As shown in Figure 1,\nquestions require the system to jointly reason over\nexplicit and implicit knowledge, which is analo-\ngous to the way humans do. To address these\nchallenges, we propose an approach, KAT, to ef-\nfectively integrate implicit and explicit knowledge\nduring reasoning. The main contributions of our\nwork are as follows:\ni) Knowledge extraction. We adopt two novel\nmethods for knowledge extraction that significantly\nimprove the quality and relevance of extracted\nknowledge: for implicit knowledge, we design\nnew prompts to extract both tentative answers and\nsupporting evidence from a frozen GPT-3 model;\nfor explicit knowledge, we design a contrastive-\nlearning-based explicit knowledge retriever using\nthe CLIP model, where all the retrieved knowledge\nare centered around visually-aligned entities.\nii) Reasoning in an encoder-decoder trans-\nformer. We design a novel reasoning module\nin KAT to perform jointly reasoning over explicit\nand implicit knowledge during answer generation,\nwhich is trained by using an end-to-end encoder-\ndecoder transformer architecture.\niii) OK-VQA performance. KAT sets a new state\nof the art on the challenging OK-VQA (Marino\net al., 2019) benchmark, and significantly outper-\nforms existing approaches.\n2 Related Work\nVision-Language Transformer. Multimodal\ntransformers have made significant progress\nover the past few years, by pre-trained on large-\nscale image and text pairs, then finetuned on\ndownstream tasks. VisualBERT (Li et al., 2019),\nUnicoder-VL (Li et al., 2020a), NICE (Chen\net al., 2021b), and VL-BERT (Su et al., 2020)\npropose the single-stream architecture to work\non both images and text. ViLBERT (Lu et al.,\n2019) and LXMERT (Tan and Bansal, 2019)\npropose a two-stream architecture to process\nimages and text independently and fused by a\nthird transformer in ta later stage. While these\nmodels have shown to store in-depth cross-modal\nknowledge and achieved impressive performance\non knowledge-based VQA (Marino et al., 2021;\nWu et al., 2022; Luo et al., 2021), this type of\nimplicitly learned knowledge is not sufficient to\nanswer many knowledge-based questions (Marino\net al., 2021). Another line of work for multimodal\ntransformers, such as CLIP (Radford et al., 2021)\nor ALIGN (Jia et al., 2021), aligns visual and\nlanguage representations by contrastive learning.\nThese models achieve state-of-the-art performance\non image-text retrieval tasks. Different from\nexisting work that uses multimodal transformers as\nimplicit knowledge bases, we focus primarily on\nhow to associate images with external knowledge.\nImportantly, our model only relies on multimodal\ntransformers learned by contrastive learning which\ndo not require any labeled images. This makes our\nmodel more flexible in real-world scenarios.\nKnowledge-based VQA. Some Knowledge-\nbased visual language tasks requires external\nknowledge beyond the image to answer a ques-\ntion. Early exploration, such as FVQA (Wang\net al., 2017a), creates a fact-based VQA dataset\nby selecting a fact (e.g., <Cat, CapableOf, Climb-\ningTrees>) from a fixed knowledge base. A recent\nOutside Knowledge VQA (OK-VQA) dataset is a\nmore challenging dataset, covering a wide range of\nknowledge categories. In our work, we focus on\nOK-VQA due to its large-scale knowledge-based\nquestions as well as its open-ended nature.\nRecent approaches have shown a great potential\nto incorporate external knowledge for knowledge-\nbased VQA. Several methods explore aggregat-\ning the external knowledge either in the form\nof structured knowledge graphs (Garderes et al.,\n2020; Narasimhan et al., 2018; Li et al., 2020b;\nWang et al., 2017a,b), unstructured knowledge\nbases (Marino et al., 2021; Wu et al., 2022; Luo\net al., 2021), and neural-symbolic inference based\nknowledge (Chen et al., 2020; West et al., 2021).\nIn these methods, object detectors (Ren et al., 2015)\nand scene classifiers (He et al., 2016) are used to\nassociate images with external knowledge. Fur-\nther, external APIs, such as Google (Wu et al.,\n2022; Luo et al., 2021), Microsoft (Chen et al.,\n2021a; Yang et al., 2022) and OCR (Luo et al.,\n2021; Wu et al., 2022) are used to enrich the asso-\nciated knowledge. Finally, pre-trained transformer-\nbased language models (Chen et al., 2021a; Yang\net al., 2022), or multimodal models (Wu et al.,\n2022; Luo et al., 2021; Wu et al., 2022; Garderes\net al., 2020; Marino et al., 2021) are leveraged as\n957\nimplicit knowledge bases for answer predictions.\nDifferent from previous approaches, Our work\naims to develop a single, unified architecture,\nby jointly reasoning over explicit and implicit\nknowledge to augment generative language models.\nWhile part of our approach is similar to PICa (Yang\net al., 2022) which considers GPT-3 as implicit\nknowledge base, our model takes one step further\nby showing that how explicit and implicit knowl-\nedge can be integrated during knowledge reasoning.\nAnother similar work Vis-DPR (Luo et al., 2021)\ncollects a knowledge corpus from training set by\nGoogle Search which is specific to a certain dataset.\nOur proposed model is more generic by collecting\nentities from Wikidata and not limited to the train-\ning set.\nOpen-Domain Question Answering (ODQA).\nODQA is the NLP task of answering general do-\nmain questions, in which the evidence is not given\nas input to the system. Several approaches (Chen\net al., 2017; Karpukhin et al., 2020) propose to\npredict the answers by first retrieving support doc-\nument from Wikipedia, before extracting answers\nfrom the retrieved document. Recent works (Izac-\nard and Grave, 2020; Lewis et al., 2020b) combine\ntext retrieval models with language generative mod-\nels which achieve state-of-the-art performance on\nknowledge-intensive natural language processing\ntasks. Similar to these works as part of our method,\nwe extend this framework to VQA domain and\nshow the effectiveness of aggregating explicit and\nimplicit knowledge for knowledge-based VQA.\n3 Method\n3.1 Overview\nWhen humans reason about the world, they process\nmultiple modalities and combine external and inter-\nnal knowledge related to these inputs. Inspired by\nthis idea, we introduce a new KAT approach. The\noverview of the proposed KAT model is shown in\nFigure 2. We define the knowledge from explicit\nknowledge bases as the explicit knowledge, and\nthe knowledge stored in large-scale language mod-\nels as the implicit knowledge ( i.e., implicit com-\nmonsense knowledge). We describe the retrieval\nmethod of our explicit knowledge (§3.2) and the\nretrieval method of our implicit knowledge (§3.3).\nNext, we introduce the details of our knowledge\nreasoning module which jointly reasons over both\nexplicit and implicit knowledge (§3.4).\nProblem Formulation. We apply our KAT on\nOK-VQA task in this paper. Formally, given a\ntraining dataset D = {(vi,qi,ai)}s\ni=1, where vi\ndenotes the ith training image; sis the total num-\nber of the training images; qi and ai represent the\nith question and its corresponding answer, respec-\ntively. We use a sequence-to-sequence model that\nis composed of an encoder and a decoder, which\nis a comparison method of T5 (Raffel et al., 2020)\nor BART (Lewis et al., 2020a). Let θ be the pa-\nrameters of the model pthat needs to be trained.\nUnlike previous approaches that treat this task as\na classification problem (Wu et al., 2022; Marino\net al., 2021), our model is to takevi and qi as inputs\nand generate the answer ai in an auto-regressive\nmanner. It should be noted that our proposed model\ntackles a more challenging problem. As the gen-\nerated answer may contain an arbitrary number of\nwords from the entire vocabulary.\n3.2 Explicit Knowledge Retrieval\n3.2.1 Explicit Knowledge Extraction\nGiven an image vi and corresponding question qi,\nit is important to ground image regions with fine-\ngrained descriptions, which is conducive to under-\nstanding both the image content and the question\nwith the referred items. Existing approaches (Rad-\nford et al., 2021; Jia et al., 2021) on OK-VQA apply\nobject detectors to generate image tags which are\nused for explicit knowledge retrieval. Such image\ntags can be generic and have a limited vocabulary\nsize, leading noise or irrelevant knowledge. Mo-\ntivated by the recent progress of visual-semantic\nmatching approaches (Radford et al., 2021; Jia\net al., 2021), we leverage a contrastive-learning-\nbased model to associate image regions with exter-\nnal knowledge bases.\nSimilar to the previous work (Marino et al., 2021;\nLuo et al., 2021) which uses a subset of exter-\nnal knowledge, we construct an explicit knowl-\nedge base that covers the 8 categories of animals,\nvehicles and other common objects from Wiki-\ndata (Vrandecic and Krotzsch, 2014). The details\ncan be found in Section 3.2.2. We denote the con-\nstructed knowledge base as K. Each knowledge\nentry efrom Kis a concatenation of the entity and\nits corresponding description.\nThe goal of our explicit knowledge retriever is\nto index all knowledge entries in dr-dimensional\ndense representations by a dense encoder Eent(·),\nsuch that it can efficiently retrieve the topmknowl-\n958\nFigure 2: Our KAT model uses a contrastive-learning-based module to retrieve knowledge entries from an explicit\nknowledge base, and uses GPT-3 to retrieve implicit knowledge with supporting evidence. The integration of\nknowledge is processed by the respective encoder transformer, and jointly with reasoning module and the decoder\ntransformer as an end-to-end training with the answer generation.\nedge entries relevant to each input image. Given\nan image vi, we use a sliding window with a stride\nto generate N image regions {v1\ni,...,v N\ni }. Then\nan image encoder Eimg(·) is applied to map each\npatch to adr-dimensional dense representation, and\nretrieves kknowledge entries from Kwhose rep-\nresentations are closest to the patch-level represen-\ntation. To define the similarity score between the\nimage region vj\ni and the entity e, we use the inner\nproduct of their normalized representations:\nsim(vj\ni,e) = Eent(e)TEimg(vj\ni). (1)\nIn total, we retrieve the top N ×kknowledge en-\ntries relevant to image vi. We keep top-mknowl-\nedge entries ranked by similarity scores as explicit\nknowledge source xexp.\nIn principle, the image and knowledge entry en-\ncoders can be implemented by any multimodal\ntransformer. We use the CLIP model (ViT-B/16\nvariant) (Radford et al., 2021) in our work and take\nthe [CLS] as representations. We pre-extract rep-\nresentations of the knowledge entries in the knowl-\nedge base Kusing the entity encoder Eent and\nindex them using FAISS (Johnson et al., 2019).\nThe qualitative example for the extracting explicit\nknowledge model is presented in Appendix A.\n3.2.2 Knowledge Base Construction\nWe use the English Wikidata (Vrandecic and\nKrotzsch, 2014) dump from Sep. 20, 2021 as\nthe explicit knowledge source base which contains\n95,870,584 entities. Each data item is stored in\na structured format constituted of property-value\npairs. Properties are objects and have their own\nWikidata pages with labels, aliases, and descrip-\ntions. We extract a subset that covers common\nobjects in real-world scenarios. We remove all\nentities whose string labels or corresponding de-\nscriptions are empty or non-English. This results\nin a total of 423,520 entity triplets in the end (e.g.,\n<Q2813, Coca-Cola, carbonated brown colored\nsoft drink>) (See Table 1).\nSubclass Number\nRole (Q214339) 162,027\nPoint of interest (Q960648) 85,900\nTool (Q39546) 78,621\nVehicle (Q42889) 44,274\nAnimal (Q729) 18,581\nClothing (Q11460) 17,711\nCompany (Q891723) 12,173\nSport (Q349) 4,233\nTotal 423,520\nTable 1: We collect a subset of Wikidata that covers com-\nmon objects in real-life scenarios as our explicit knowl-\nedge base. Above are statistics of these subclasses.\n3.3 Implicit Knowledge Retrieval\nWhile our explicit knowledge retriever focuses\non semantic matching between image regions and\nknowledge entries, it lacks implicit commonsense\nknowledge (e.g., Lemons are sour) which is usu-\nally stored in large-scale language models (Brown\net al., 2020). In this section, we retrieve implicit\n959\nknowledge with supporting evidence by prompting\nfrom a large-scale pre-trained language model.\nWe design our implicit knowledge retriever with\ninspirations from the previous work (Yang et al.,\n2022). We leverage GPT-3 as an implicit language\nknowledge base and treat VQA as an open-ended\ntext generation task. For each image-question pair,\nwe first convert the image vi into a textual de-\nscription C via a state-of-the-art image caption-\ning model (Li et al., 2020c), and then construct\na carefully designed text prompt consisting of a\ngeneral instruction sentence, the textual descrip-\ntion C, the question, and a set of context-question-\nanswer triplets taken from the training dataset that\nare semantically most similar to the current image-\nquestion pair (see Figure 7 in Appendix B for a\nconcrete example). We then input this text prompt\nto the GPT-3 model in its frozen version and ob-\ntain the output from GPT-3 as the tentative answer\ncandidate to the current image-question pair.\nTo gain deeper insights from the implicit knowl-\nedge coming out of GPT-3 and its rationale, we\ndesign another prompt to query GPT-3 for support-\ning evidence behind the tentative answer candidate\nthat it generates. More specifically, for each image-\nquestion pair (vi,qi), and for a tentative answer a\ngenerated by GPT-3, we construct the prompt in\nthe form of: “(question qi)? (answer a). This is\nbecause” to query GPT-3 for supporting evidence\n(see Figure 6 in Appendix B for a concrete exam-\nple). We finally compile both the tentative answers\nand the corresponding supporting evidence from\nGPT-3 as implicit knowledge source ximp.\n3.4 KAT Model\nAs showed in the Figure 2, the explicit knowl-\nedge entries are from an image, which are con-\ncerned with semantic matching of the image re-\ngions. These knowledge entries could be noisy or\nirrelevant to its corresponding question. Moreover,\nsome of the supporting evidence prompted from\nGPT-3 is generic or not related to image content.\nSimple concatenation of different knowledge may\nintroduce noise during model training. We design\na knowledge reasoning module with inspirations\nfrom the previous work (Karpukhin et al., 2020).\nOur knowledge reasoning module encodes each\nquestion and knowledge pair separately, and jointly\nreason over both explicit and implicit knowledge\nwhen generating an answer.\nEncoder. We concatenate question qi with each\nknowledge as a question-knowledge pair. Firstly,\nwe add sentinel tokens question:, entity:\nand description: before the question, the\nretrieved entity, and its description separately.\nSimilarly, we add sentinel tokens question:,\ncandidate: and evidence: before the ques-\ntion, the tentative answer, and its evidence. Sec-\nondly, we use an embedding layer followed by a\nsequence of encoder layers to encode the question-\nknowledge pairs separately. We average the token\nembeddings of each question-knowledge pair from\nthe last encoder layer, which results in an embed-\nding matrix of explicit knowledge Xexp ∈Rm×d\nand implicit knowledge Ximp ∈Rp×d, where d,\nmand pare the embedding dimension, the num-\nber of explicit knowledge xexp, and the number of\nimplicit knowledge ximp, respectively.\nReasoning Module. To jointly reason over im-\nplicit and explicit knowledge, we concatenate the\nembeddings of explicit and implicit knowledge\nform a global representation X ∈R(m+p)×d. The\ncross-attention module takes the global represen-\ntation X of the encoder as the input. Let H ∈Rd\nbe the output of the previous self-attention layer of\nthe decoder. By definition (Vaswani et al., 2017),\nthe scaled dot-product attention can be expressed\nas:\nQv = softmax(QKT\n√\nd\n)V, (2)\nwhere queries Q, keys K, and values V are com-\nputed by applying linear transformations: Q =\nWQH,K = WKX,V = WVX. The attended\nrepresentation Qv is a weighted sum of the values,\nand implies that our model performs a joint rea-\nsoning over explicit and implicit knowledge when\ngenerating answers.\nDecoder. We feed the embeddings of explicit and\nimplicit knowledge to a sequence of decoder layers\nfor answer generation. We train our model with a\ncross-entropy loss:\nLCE = −\nn∑\nt=1\nlog pθ(yt|y<t,xexp; ximp), (3)\nwhere yt is predicted autoregressively.\n960\nMethod Knowledge Resources Acc (%)\nNo knowledge\nQ only (Marino et al., 2019) - 14.93\nVanilla T5 - 18.56\nMLP (Marino et al., 2019) - 20.67\nBAN (Marino et al., 2019) - 25.1\nMUTAN (Marino et al., 2019) - 26.41\nWith knowledge\nBAN+AN (Marino et al., 2019) Wikipedia 25.61\nBAN+KG-AUG (Li et al., 2020b) Wikipedia+ConceptNet 26.71\nMUTAN+AN (Marino et al., 2019) Wikipedia 27.84\nConceptBERT (Garderes et al., 2020) ConceptNet 33.66\nKRISP (Marino et al., 2021) Wikipedia+ConceptNet 38.35\nVis-DPR (Luo et al., 2021) Google Search 39.2\nMA VEx (Wu et al., 2022) Wikipedia+ConceptNet+Google Images 39.4\nGPT-3\nPICa-Base (Yang et al., 2022) Frozen GPT-3 (175B) 43.3\nPICa-Full (Yang et al., 2022) Frozen GPT-3 (175B) 48.0\nKAT-explicit (w/ reasoning) Wikidata 44.25\nKAT-implicit (w/ reasoning) Frozen GPT-3 (175B) 49.72\nKAT (w/o reasoning) Wikidata+Frozen GPT-3 (175B) 51.97\nKAT (single) Wikidata+Frozen GPT-3 (175B) 53.09\nKAT (ensemble) Wikidata+Frozen GPT-3 (175B) 54.41\nTable 2: Results of OK-VQA comparing to standard baselines show that our KAT (large size) model achieves\nstate-of-the-art performance on OK-VQA full testing set. It is important (see table sections) to compare methods\nbased on their access to increasingly large implicit sources of knowledge and utilization of explicit knowledge\nsources. Our five KAT models variants make the relative importance of these decisions explicit. We train our model\nwith 3 random seeds and the result is denoted as ensemble.\n4 Experiment\n4.1 Dataset\nOK-VQA (Marino et al., 2019) is currently the\nlargest knowledge-based VQA dataset, The ques-\ntions are crowdsourced from Amazon Mechani-\ncal Turkers and require outside knowledge beyond\nthe images in order to be answered correctly. The\ndataset contains 14,031 images and 14,055 ques-\ntions covering a variety of knowledge categories.\nWe follow the standard evaluation metric recom-\nmended by the VQA challenge (Antol et al., 2015).\n4.2 Implementation Details\nFor the knowledge reasoning module, we initialize\nour model with the pre-trained T5 model (Raffel\net al., 2020). We compare two model sizes, base\nand large, each containing 220M and 770M pa-\nrameters respectively. We fine-tune the models on\nOK-VQA dataset, using AdamW (Loshchilov and\nHutter, 2019). We use a learning rate of 3e−5\nto warm up for 2K iterations and train for 10K\niterations. Limited by the computational resources,\nwe set the number of retrieved entities to 40. The\nmodel is trained with a batch size of 32, using\n16 V100 GPUs with 32Gb of memory each. Un-\nless otherwise specified, all results reported in this\npaper as KAT use this model which we found to\nperform best. We evaluate our predictions with\nground-truth after normalization. The normaliza-\ntion step consists of lowercasing, and removing arti-\ncles, punctuation and duplicated whitespace (Chen\net al., 2017; Lee et al., 2019). To be consistent with\nprevious work (Marino et al., 2021), we train our\nmodel with 3 different random seeds and use the\naverage results for the leaderboard submission.\n4.3 Comparison with Existing Approaches\nWe compare our model against existing approaches\non the OK-VQA dataset and the results are summa-\nrized in Table 2. Our model outperforms state-of-\nthe-art methods by significant margins. We com-\npare our model with existing approaches from two\naspects. (1) If we only consider using explicit\nknowledge, our model achieves 44.25% which is\n4.85% and 5.9% higher than MA VEx and KRISP,\n961\nrespectively. Our model uses contrastive-learning-\nbased model to extract knowledge, leaving head-\nroom by incorporating supervised pre-trained mod-\nels, such as pre-trained object detectors. It should\nbe noted that our proposed model is working on a\nmore challenging problem. As the generated an-\nswer could contain an arbitrary number of words\nfrom the entire vocabulary. Our model is slightly\nbetter than PICa-Base which is a plain version of\nPICa-Full without example engineering. It implies\nthat our single, unified architecture can effectively\nassociate images with the explicit knowledge base.\n(2) If we take the implicit knowledge from GPT-\n3 as the additional input, our model outperforms\nPICa-Full by 6.41% which indicates it is important\nto integrate knowledge of different types when gen-\nerating answers. The detailed comparison can be\nfound in Table 3.\n5 Ablation Study\nTo unpack the performance gain and understand\nthe impact of different components, we ablate and\ncompare different model architectures, types of\nknowledge and the number of explicit knowledge.\nModel architecture Knowledge Accuracy (%)\nBase Large Explicit Implicit\n√ 18.56√ √ 40.93√ √ 44.25√ √ 47.60√ √ 49.72√ √ √ 50.58√ √ √ 54.41\nTable 3: Ablation study on model architectures and\ntypes of knowledge. Our experiments show that larger\nmodel has more capacity for implicit knowledge reason-\ning and jointly reasoning over both knowledge sources\nhas a consistent improvement with baselines.\nSpecifically, as shown in Table 3, our KAT-large\nshows a consistent improvement over using KAT-\nbase. This larger model has more capacity for\nimplicit knowledge reasoning. The integration of\nexplicit and implicit knowledge achieves a perfor-\nmance gain of ∼4%, supporting the intuition that\nthese two types of knowledge provide complemen-\ntary pieces of knowledge.\n5.1 Effectiveness of Knowledge Reasoning\nTo verify the effectiveness of our knowledge reason-\ning module, we use a KAT without the knowledge\nreasoning module which is denoted as KAT (w/o\nreasoning). This model concatenates explicit and\nMethod Accuracy (%)\nKAT (w/o reasoning) 51.97\nKAT 54.41\nTable 4: Comparison with KAT (w/o reasoning) which\nuses the concatenated knowledge as inputs without the\nknowledge reasoning module.\nimplicit knowledge as a sentence and adopts a max-\nimum length of 256 tokens. We train this variant\nwith the same parameter settings. As shown in Ta-\nble 4, simply concatenating knowledge sources is\n2.43% lower than our proposed model. It indicates\nthat KAT (w/o reasoning) may introduce noise to\nrelevant knowledge during encoding. Our model\nadaptively attend different knowledge sources for\nanswer generation that can reduce the influence of\nirrelevant knowledge.\n5.2 Extracting Explicit Knowledge\nFigure 3: Our model achieves consistent improvement\nwhen aggregating more knowledge entries from an ex-\nplicit knowledge base. However, as CLIP-ViT/16 and\nRN50 are very different explicit knowledge retrieval\nbackbones we see the choice of backbone and number\nof sources to include are intimately related. Here we\nuse KAT-base for demonstration.\nFrom Figure 3 we can see, the performance of\nour model is directly affected by the size of re-\ntrieved explicit knowledge. When only consider-\ning the implicit knowledge ( i.e., the number of\nretrieved entities is 0), our model achieves 47.6%\nwhich is slightly worse than PICa-Full baseline. It\nindicates that solely increasing model complexity\ncannot improve the performance. This also demon-\nstrates the importance of explicit knowledge. Our\nmodel shows a consistent improvement by incor-\nporating more explicit knowledge. While a more\n962\nQuestion:Whatispaintedonthebench?Category:Brands,CompaniesandProductsAnswer:Exp:strandKAT(w/oKRM):redImp:redKAT:Cocacola\nExplicitKnowledge:Tactilepaving:systemoftexturedgroundsurfaceindicatorstoassistpedestrianswhoareblindorvisuallyimpaired.CocaCola:carbonatedbrown-coloredsoftdrink.Bench:pieceoffurnitureonwhichseveralpeoplemaysitatthesametime.Streetfurniture:collectivetermforobjectsandpiecesofequipmentinstalledoutdoorsforvariouspurposes.ImplicitKnowledge:Red:thebenchispaintedred.\nQuestion:Whatkindofboardisthis?Category:SportsandrecreationAnswer:Exp:wakeboardKAT(w/oKRM):surfboardImp:surfboardKAT:surfboard\nExplicitKnowledge:Wakeboardboat:boatdesignedtocreateawakeforwakeboarding.Wakeboarder:someonepracticingwakeboarding.Kitesurfer:practitionerofkitesurfing.Skiboarding:freestyleskiingusingshortskisandnopoles.Boardsport:sportsthatarepracticedwithsomesortofboardastheprimaryequipment.ImplicitKnowledge:Surfboard:Thissportissurfboard.\nFigure 4: Two examples from OK-VQA dataset that our model generates correct answers by jointly reasoning over\nboth implicit and explicit knowledge. (exp: predictions by using explicit knowledge only and imp: predictions by\nusing implicit knowledge only). More examples and analysis can be found in Appendix C.\nextensive knowledge set may include more distract-\ning knowledge, retrieved knowledge entries can\nshare either visually or semantically similar knowl-\nedge as the relevant ones. Thus this can massively\nreduce the search space and/or reduce spurious am-\nbiguity.\nWe compare different explicit knowledge re-\ntrieval module. Though ViT/16 has a large classifi-\ncation improvement over ResNet-50 (e.g., 6.9% on\nImageNet) (Radford et al., 2021), there is a less gap\nbetween these two backbones. As the number of re-\ntrieved entities increases, our knowledge reasoning\nmodule can further migrate this gap by adaptively\nattending to different explicit knowledge.\n5.3 Category Results on OK-VQA\nHere we present quantitative analyses to illustrate\nhow explicit and implicit knowledge influence the\nfinal predictions. Based on the types of knowledge\nrequired, questions in OK-VQA are categorized\ninto 11 categories and the accuracy results of each\ncategory are reported in Table 5. We re-train our\nmodel under the same settings with only either\nexplicit or implicit knowledge, denoted as “exp”\nand “imp” respectively.\nFor most categories, the model using only ex-\nplicit knowledge performs worse than that using\nonly implicit knowledge. As implicit knowledge\ncomes from the results of state-of-the-art object\ndetection, image captioning models and support-\ning evidence by prompting GPT-3. While explicit\nknowledge is retrieved based on semantic match-\ning between images and entities from knowledge\nbases, it contains richer but more distracting knowl-\nedge. Note that using explicit knowledge performs\nbetter for category “Brands, Companies, and Prod-\nucts\" and “Weather and Climate\". It indicates that\naccurately recognizing objects with fine-grained\ndescriptions in the images is important for these\ncategories to answer corresponding questions.\nQuestion Type Exp Imp Ours ∆\nPlants and Animals 42.2 51.5 54.7 +3.2\nScience and Technology 44.4 43.3 52.8 +8.3\nSports and Recreation 49.7 53.8 60.4 +6.7\nGeo, History, Lang, and Culture 45.6 45.4 55.8 +10.2\nBrands, Companies, and Products 41.7 38.2 48.5 +6.8\nVehicles and Transportation 41.5 42.9 51.3 +8.4\nCooking and Food 47.9 47.7 52.7 +4.8\nWeather and Climate 51.7 46.3 54.8 +3.1\nPeople and Everyday 43.1 44.4 51.5 +7.1\nObjects, Material and Clothing 42.9 45.4 49.3 +3.9\nOther 41.5 50.2 51.2 +1.0\nTable 5: Accuracy (%) of question types in OK-VQA\nfull testing set. Our models outperforms exp and imp\nmodels by a large margin on all categories. (exp:\nexplicit-only model and imp: implicit-only model)\n5.4 Qualitative Analysis\nAnalyzed in previous sections, jointly reasoning\nover both knowledge sources during answer gener-\nation improves the explicit-only and implicit-only\nmodels by large margins. Figure 4 shows two ex-\namples comparing answers generated by different\nmodels along with retrieved knowledge. The left\nexample shows that while explicit knowledge re-\ntrieved from the knowledge base contains the nec-\nessary knowledge entries for reasoning, it fails to\ngenerate the answer which requires the relation be-\ntween bench and Coca Cola logos. On the other\nside, implicit knowledge retrieved from GPT-3 can\nonly infer the bench is painted red, failing to rec-\nognize its logo. By jointly considering both knowl-\nedge sources, our model can associate the color of\n963\nCoca Cola logo with the painted color of the bench\nwhich derives the correct answer. The right ex-\nample shows that though explicit knowledge does\nnot contain the right knowledge entries, it provides\nvisually similar descriptions of this sport which fur-\nther constrains the search space of our model and\nverifies the correctness of the implicit knowledge.\n6 Conclusion\nThis paper takes a step towards understanding the\ncomplementary role of implicit knowledge gained\nfrom continuing to scale models and explicit knowl-\nedge from structured knowledge bases. Impor-\ntantly, it appears that there is headroom in both\ndirections (i.g. improving retrieval and reasoning).\nOur conceptually simple yet effective approach for\nknowledge-based VQA makes these relationships\nexplicit while still achieving a significant improve-\nment against state-of-the-art results. Additional\nchallenges remain, for example how best to align\nimage regions with meaningful external semantics\ndeserves and how to efficiently and accurately inte-\ngrate multiple knowledge bases.\nAcknowledgement\nWe are especially grateful to Jianwei Yang, Daniel\nMcDuff, Dragomir Radev, Harkirat Behl, Hao\nChen, Chunyuan Li, Baolin Peng, Kezhen Chen,\nTejas Srinivasan for their for the early insightful dis-\ncussions, suggestion, and their pointers to the mod-\neling generation and literature. We thank Zhe Gan,\nZhengyuan Yang, Lijuan Wang from cognition ser-\nvice team of Microsoft for their work and their gen-\nerous helps and feedback for the project. We appre-\nciate Subhojit Som from Turing team of Microsoft\nfor his enormous support and encouragement. The\nauthors gratefully acknowledge Kenneth Marino\nfrom DeepMind, and Roozbeh Mottaghi from the\nAllenAI for their comments, supporting and helps\nof the work. This research was supported in part by\nthe Defense Advanced Research Projects Agency\n(DARPA) under contract number D17PC00340 and\nalso supported by the US DARPA KAIROS Pro-\ngram No. FA8750-19-2-1004.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn CVPR.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDbpedia: A nucleus for a web of open data. In The\nsemantic web, pages 722–735. Springer.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. arXiv preprint arXiv:1704.00051.\nKezhen Chen, Qiuyuan Huang, Yonatan Bisk, Daniel\nMcDuff, and Jianfeng Gao. 2021a. Kb-vlp: Knowl-\nedge based vision and language pretraining. In ICML,\nworkshop.\nKezhen Chen, Qiuyuan Huang, Daniel McDuff, Xiang\nGao, Hamid Palangi, Jianfeng Wang, Kenneth For-\nbus, and Jianfeng Gao. 2021b. Nice: Neural image\ncommenting with empathy. In EMNLP.\nKezhen Chen, Qiuyuan Huang, Paul Smolensky, Ken-\nneth Forbus, and Jianfeng Gao. 2020. Learning in-\nference rules with neural tp-reasoner. In NeurIPS,\nworkshop.\nFrançois Garderes, Maryam Ziaeefard, Baptiste Abe-\nloos, and Freddy Lecue. 2020. Conceptbert:\nConcept-aware representation for visual question an-\nswering. In EMNLP.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In CVPR.\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In EACL.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V Le, Yunhsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In ICML.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In EMNLP.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2020a. Bart:\n964\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In ACL.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. In NeurIPS.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and\nMing Zhou. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. Proceedings of AAAI.\nGuohao Li, Xin Wang, and Wenwu Zhu. 2020b. Boost-\ning visual question answering with context-aware\nknowledge aggregation. In ACM MM.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020c. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In ECCV.\nHugo Liu and Push Singh. 2004. Conceptnet—a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211–226.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nNeurIPS.\nMan Luo, Yankai Zeng, Pratyay Banerjee, and Chitta\nBaral. 2021. Weakly-supervised visual-retriever-\nreader for knowledge-based question answering. In\nEMNLP.\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav\nGupta, and Marcus Rohrbach. 2021. Krisp: Inte-\ngrating implicit and symbolic knowledge for open-\ndomain knowledge-based vqa. In CVPR.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\nquestion answering benchmark requiring external\nknowledge. In CVPR.\nMedhini Narasimhan, Svetlana Lazebnik, and Alexan-\nder Schwing. 2018. Out of the box: Reasoning with\ngraph convolution nets for factual visual question\nanswering. NeurIPS.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In ICML.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21(140):1–67.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. NeurIPS.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-training\nof generic visual-linguistic representations. In ICLR.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. In EMNLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nDenny Vrandecic and Markus Krotzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM.\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and\nAnton Van Den Hengel. 2017a. Fvqa: Fact-based vi-\nsual question answering. TPAMI, 40(10):2413–2427.\nPeng Wang, Qi Wu, Chunhua Shen, Anton van den Hen-\ngel, and Anthony Dick. 2017b. Explicit knowledge-\nbased reasoning for visual question answering. In\nIJCAI.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D.\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2021. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In ArXiv.\nJialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh\nMottaghi. 2022. Multi-modal answer validation for\nknowledge-based vqa. In AAAI.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In AAAI.\n965\nAppendix\nA Figure of Explicit Knowledge\nIn this section, we show one example Figure 5 to\nextract explicit knowledge from an image, which\nuse the CLIP model to conduct the explicit knowl-\nedge retrieval with the image and a wiki knowledge\nbase.\nFigure 5: Overview of the explicit knowledge extraction.\nWe use a sliding window to crop image regions and\nretrieve knowledge entries from an explicit knowledge\nbase by CLIP.\nB Examples of Prompts of Implicit\nKnowledge\nIn this Section B of the Appendix, we show two\nconcrete examples (Figure 6 and Figure 7) for the\nprompts that constructed to query GPT-3 for im-\nplicit knowledge in our experiments:\nFigure 6: An example of the evidence of rationale that\nwe obtain from GPT-3 by using a combination of ques-\ntion and answer candidate to query it.\nC Analysis on More Examples\nIn this section, we showcase more predictions from\nvariants of our model. As shown in Figure 8, we\nanalyze the predictions based on different type of\nknowledge from several aspects:\nEffectiveness of explicit knowledge retriever.\nOur explicit knowledge retriever can retrieve fine-\ngrained knowledge entries from the explicit knowl-\nedge base, such as golden retriever(a fine-grained\nbreed of dogs), cucumber sandwich(a specific type\nof sandwich) and Macbook Pro(a specific model\nFigure 7: An example of the prompts that we use to\nquery GPT-3 in our knowledge-aumented GPT-3 query\nsystem.\nof Apple products). These fine-grained entities are\nhardly obtained from existing object detection mod-\nels, which can constraint the search space of our\nmodel and are beneficial to our answer generation\nprocess.\nEffectiveness of implicit knowledge retriever.\nOur implicit knowledge retriever can retrieve sup-\nporting evidence from GPT-3, such asThomas: the\ntrain is named after the man who designed it.and\nRefrigerator: the refrigerator is used to keep food\ncold. These kinds of knowledge are highly related\nto commonsense knowledge which needs further\ninference based on entities and provide comple-\nmentary explanation to explicit knowledge.\nAnswer generation & classification. As most\nprevious work on OK-VQA task, such as KRISP or\nMA VEx method, implement OK-VQA as a classi-\nfication task. The prediction vocabulary is dataset-\nspecific and assumes the training and test set are\nsharing a similar vocabulary. The limitation of\nthese methods is the generalization ability. Our pro-\nposed KAT model treats OK-VQA as an open-end\ngeneration task. From these examples we found,\nour model can generate answers likeIphone or Her-\ncules that are visually and semantically reasonable.\nOur proposed novel KAT model using the explicit\n966\nQuestion:Canyouguessthematerialusedtomakethebagshowninthispicture?Category:Objects,MaterialandClothingAnswer:Exp:canvasKAT(w/oKRM):leatherImp:leatherKAT:canvas\nExplicitKnowledge:(entity:description)AcerAspireone:lineofnotebooksbyAcerInc.Drawinginstrument:toolusedfordrawingordrafting.Writingimplement:toolusedforwritingBookbag:abag,usuallyabackpack,usedbystudentstocarrytheirtextbooks.\nImplicitKnowledge:(candidate:evidence)leather:thebagismadeofleather.\nQuestion:Whatbreedarethedogs?Category:OtherAnswer:Exp:goldenretrieverKAT(w/oKRM):huskyImp:huskyKAT:goldenretriever\nExplicitKnowledge:(entity:description)Snowpillow:measuringdeviceforsnowpack.Searchandrescuedog:dogtrainedtolocateorretrieveamissingortrappedperson.Goldenretriever:dogbreed.Mushing:Sportordogpoweredtransportmethod.\nImplicitKnowledge:(candidate:evidence)Husky:Thehuskyisaveryintelligentdog.Theyareindependentandwilldowhattheywanttodo.\nQuestion:Whattypeofsandwichisbeingserved?Category:CookingandFoodAnswer:Exp:cucumberKAT(w/oKRM):subImp:sub KAT:cucumber\nExplicitKnowledge:(entity:description)Salad:dishconsistingofamixtureofsmallpiecesoffood,usuallyvegetablesorfruit.Cucumbersandwich:thetraditionalcucumbersandwichiscomposedofthinslicesofcucumberplacedbetweentwothinslicesofcrustless,lightlybutterdwhitebread.Vegetablechip:cookedchippreparedusingvegetables.\nImplicitKnowledge:(candidate:evidence)Sub:thesubisatypeofsandwich.\nQuestion:Whatsortofphonewouldyouassociatewiththiscomputer?Category:Brands,CompaniesandProductsAnswer:Exp:IphoneKAT(w/oKRM):cellImp:smartphoneKAT:Iphone\nExplicitKnowledge:(entity:description)Floorlamp:lampstandingonthefloor,oftenwithahightreachinguptotheverticalmiddleoftheroom.MacbookPro:laptopmadebyApple.MacOS:operatingsystemforApplecomputers,launchedin2001asMacOSX.Smartmattress:Mattressmonitoringsleeppatterns.\nImplicitKnowledge:(candidate:evidence)Smartphone:thecomputerisnotasmartphone.\nQuestion:Whatisthenameofthefamoustrainpictured?Category:VehiclesandTransportationAnswer:Exp:SmootKAT(w/oKRM):ThomasImp:ThomasKAT:Thomas\nExplicitKnowledge:(entity:description)Fogmachine:devicethatemitsadensevaporthatappearssimilartofog.Draisine:smallpoweredrailvehicleusedbytrackmaintenanceworkers.Oasthouse:buildingsdesignedforkilning(drying)hopsaspartofthebrewingprocess.Cloudingagent:typeofemulsifierusedtomakebeveragesuchasfruitjuicetolookmorecloudy.ImplicitKnowledge:(candidate:evidence)Thomas:thetrainisnamedafterthemanwhodesignedit.\nQuestion:Whatisthisdogrunningafter?Category:PlantsandAnimalsAnswer:Exp:personKAT(w/oKRM):ballImp:ballKAT:ball\nExplicitKnowledge:(entity:description)Sighthound:dogbreed.AmericanStaffordshireTerrier:dogbreed.Greyhoundracing:canineracingsportinvolvingtheGreyhounddogbreed.Whipperracing:dogsport.\nImplicitKnowledge:(candidate:evidence)Ball:thedogischasingaftertheball.\nQuestion:Howoftenshouldsomeoneusethis?Category:Objects,MaterialandClothingAnswer:Exp:twicedayKAT(w/oKRM):dailyImp:dailyKAT:daily\nExplicitKnowledge:(entity:description)Bathroomlinen:householdlinenusedspecificallyforthebathroom.Toothbrush:oralhygieneinstrumentusedtocleanthecleantheteeth,gums,andtongue.Toothbrushholder:containerorrackfortoothbrushes.Laubwerk:delicatefoliageornamentwithinterlacingstraps.ImplicitKnowledge:(candidate:evidence)Daily:theproductismadewithnaturalingredients.Thisiswhyitissafetousedaily.\nQuestion:Whathobbymightthisdepict?Category:Objects,MaterialandClothingAnswer:Exp:papercraftKAT(w/oKRM):paintingImp:scrapbookKAT:scrapbook\nExplicitKnowledge:(entity:description)Embroideryworkshop:workshopwhereembroideryiscreated.Scissors:hand-operatedcuttinginstrument.Paperknife:animplementusedforcuttingopensealedenvelopes.Leathercutter:craftman.\nImplicitKnowledge:(candidate:evidence)Scrapbooking:thebobbyisaformofart.\nQuestion:Whattypeofplaneisthis?Category:VehiclesandTransportationAnswer:Exp:HerculesKAT(w/oKRM):jetImp:jet KAT:jet\nExplicitKnowledge:(entity:description)AvroShackleton:maritimepatrolaircraftfamilybyAvro.MC-130Hercules:airlifterseriesbyLockheed.P-3BOrion:anti-submarinemaritimepatrolaircraft.C-130BHercules:airlifterseriesbyLockheed.\nImplicitKnowledge:(candidate:evidence)Jet:theplaneisflyingatahighspeed.\nQuestion:Whatisthismachineusedfor?Category:Brands,CompaniesandProductsAnswer:Exp:refrigeratefoodKAT(w/oKRM):freezeImp:freezerKAT:keepfoodcold\nExplicitKnowledge:(entity:description)Shelf-stablefood:foodofatypethatcanbesafelystoredatroomtemperatureinasealedcontainer.Freebox:boxorlocationusedtoallowforpeopletoridthemselvesofexcessitems.Icebox:non-mechanicalhouseholdapplicanceforcoolingfoodstuffs.Refrigenration:processofmovingheatfromonelocationtoanotherincontrolledconditions.ImplicitKnowledge:(candidate:evidence)Refrigerator:therefrigeratorisusedtokeepfoodcold.\nFigure 8: More examples from OK-VQA dataset that our model generates answers by jointly reasoning over both\nimplicit and explicit knowledge.\n967\nand implicit knowledge is designed to enhance se-\nmantic alignment and generate representations with\nstronger knowledge-awareness.\n968",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6793127059936523
    },
    {
      "name": "Transformer",
      "score": 0.6043241024017334
    },
    {
      "name": "Computational linguistics",
      "score": 0.4353334307670593
    },
    {
      "name": "Natural language processing",
      "score": 0.43251699209213257
    },
    {
      "name": "Human language",
      "score": 0.41813600063323975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40999871492385864
    },
    {
      "name": "Linguistics",
      "score": 0.3700132966041565
    },
    {
      "name": "Engineering",
      "score": 0.2069673240184784
    },
    {
      "name": "Electrical engineering",
      "score": 0.11308103799819946
    },
    {
      "name": "Philosophy",
      "score": 0.07448068261146545
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ]
}