{
  "title": "Predictive Analytics in Mental Health Leveraging LLM Embeddings and Machine Learning Models for Social Media Analysis",
  "url": "https://openalex.org/W4391808470",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2500925444",
      "name": "Ahmad Radwan",
      "affiliations": [
        "Arab American University"
      ]
    },
    {
      "id": "https://openalex.org/A5093926782",
      "name": "Mohannad Amarneh",
      "affiliations": [
        "Arab American University"
      ]
    },
    {
      "id": "https://openalex.org/A5093926783",
      "name": "Hussam Alawneh",
      "affiliations": [
        "Arab American University"
      ]
    },
    {
      "id": "https://openalex.org/A2744722691",
      "name": "Huthaifa I. Ashqar",
      "affiliations": [
        "Arab American University"
      ]
    },
    {
      "id": "https://openalex.org/A2979228739",
      "name": "Anas Alsobeh",
      "affiliations": [
        "Yarmouk University",
        "Southern Illinois University Carbondale"
      ]
    },
    {
      "id": "https://openalex.org/A4267680144",
      "name": "Aws Abed Al Raheem Magableh",
      "affiliations": [
        "Yarmouk University",
        "Prince Sultan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4376142471",
    "https://openalex.org/W4361008180",
    "https://openalex.org/W4214492748",
    "https://openalex.org/W4220724642",
    "https://openalex.org/W2968986159",
    "https://openalex.org/W4245195603",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2612560394",
    "https://openalex.org/W4327797488",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W4388740220",
    "https://openalex.org/W2145946200",
    "https://openalex.org/W3120545453",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W4200635465",
    "https://openalex.org/W3161132095",
    "https://openalex.org/W3045206885",
    "https://openalex.org/W4388848620",
    "https://openalex.org/W2552699932",
    "https://openalex.org/W3151006805",
    "https://openalex.org/W2987392802",
    "https://openalex.org/W3166185110",
    "https://openalex.org/W6922120124",
    "https://openalex.org/W3210757389",
    "https://openalex.org/W2971349717",
    "https://openalex.org/W3135351349",
    "https://openalex.org/W4378977088",
    "https://openalex.org/W2252153787",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3103699767",
    "https://openalex.org/W2954442338"
  ],
  "abstract": "The prevalence of stress-related disorders has increased significantly in recent years, necessitating scalable methods to identify affected individuals. This paper proposes a novel approach utilizing large language models (LLMs), with a focus on OpenAI's generative pre-trained transformer (GPT-3) embeddings and machine learning (ML) algorithms to classify social media posts as indicative or not of stress disorders. The aim is to create a preliminary screening tool leveraging online textual data. GPT-3 embeddings transformed posts into vector representations capturing semantic meaning and linguistic nuances. Various models, including support vector machines, random forests, XGBoost, KNN, and neural networks, were trained on a dataset of &amp;gt;10,000 labeled social media posts. The top model, a support vector machine, achieved 83% accuracy in classifying posts displaying signs of stress.",
  "full_text": "DOI: 10.4018/IJWSR.338222\nInternational Journal of Web Services Research\nVolume 21 • Issue 1 \nThis article published as an Open Access article distributed under the terms of the Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/4.0/) which permits unrestricted use, distribution, and production in any medium,\nprovided the author of the original work and original publication source are properly credited.\n*Corresponding Author\n1\nPredictive Analytics in Mental Health \nLeveraging LLM Embeddings and Machine \nLearning Models for Social Media Analysis\nAhmad Radwan, Arab American University, Palestine\nMohannad Amarneh, Arab American University, Palestine\nHussam Alawneh, Arab American University, Palestine\nHuthaifa I. Ashqar, Arab American University, Palestine\nAnas AlSobeh, Southern Illinois University, Carbondale, USA & Yarmouk University, Jordan\n https://orcid.org/0000-0002-1506-7924\nAws Abed Al Raheem Magableh, Yarmouk University, Jordan & Prince Sultan University, Saudi Arabia*\n https://orcid.org/0000-0003-4513-6430\nABSTRACT\nThe prevalence of stress-related disorders has increased significantly in recent years, necessitating \nscalable methods to identify affected individuals. This paper proposes a novel approach utilizing \nlarge language models (LLMs), with a focus on OpenAI’s generative pre-trained transformer (GPT-3) \nembeddings and machine learning (ML) algorithms to classify social media posts as indicative or not \nof stress disorders. The aim is to create a preliminary screening tool leveraging online textual data. \nGPT-3 embeddings transformed posts into vector representations capturing semantic meaning and \nlinguistic nuances. Various models, including support vector machines, random forests, XGBoost, \nKNN, and neural networks, were trained on a dataset of &gt;10,000 labeled social media posts. The \ntop model, a support vector machine, achieved 83% accuracy in classifying posts displaying signs \nof stress.\nKEyWoRdS\nGenerative Pre-Trained Transformer (GPT-3), Large Language Models (LLM), Machine Learning (ML), Mental \nHealth, Social Media Analysis, Stress Disorder Identification, System Analysis and Design\nMENTAL HEALTH AN d MACHINE LEARNING \nModELS: S oCIAL ME dIA ANALySIS\nMental health describes a person’s emotional, psychological, and social well-being, encompassing \ntheir overall mental and emotional state. It is a dynamic and complex aspect of human health that \ninfluences how individuals think, behave, feel, act, and relate to others or objects (World Health \nOrganization [WHO], 2022). In the last decades, mental health illnesses have become widely \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n2\nprevalent. The World Health Organization (WHO) underscores the increasing prevalence of health \ndisorders globally (AlSobeh et al., 20191a), with estimates suggesting that over a billion individuals \nare affected by a range of mental health conditions (Organizacao Pan-Americana da Saude [OPAS], \n2022). From a technology aspect, social media platforms such as Facebook, Instagram, and others \nhave become increasingly popular for individuals to express their thoughts, emotions, and daily \nexperiences. Users often share personal stories, express their frustrations, and seek support from their \nonline communities. This wealth of textual data provides an opportunity to monitor, analyze, and \nestimate individuals’ mental well-being at scale (Chafery, 2024). This escalating trend underscores \nthe imperative for robust, scalable detection and intervention mechanisms. Social media often serves \nas a digital diary or outlet where users share their innermost feelings, thoughts, and experiences. \nThese expressions can range from joy at personal achievements to stress, anxiety, and other mental \nhealth concerns. The language and tone used in these posts reveal significant insights into the \nuser’s emotional state and mental well-being. For instance, frequent posts about feelings of sadness, \nhopelessness, or anxiety could be indicative of underlying mental health issues such as depression \nor anxiety disorders. Changes in the frequency, content, and nature of social media posts can signal \nshifts in a person’s mental state. A sudden increase in posting, especially if the content is erratic or \ndistressing, or a sudden decrease or absence of activity, can be telling. Analyzing these patterns over \ntime can provide clues about fluctuations in mental health, potentially signaling the onset of a mental \nhealth condition or changes in an existing condition. Social media allows users to connect with others, \nseek support, and engage in communities. How individuals interact with others online, the type of \ncontent they share, and the communities they engage with can offer insights into their mental state. \nSeeking support or discussing mental health challenges in online communities could indicate a need \nfor help or an attempt to cope with personal issues. The advancement in natural language processing \n(NLP) and ML enables the analysis of vast amounts of unstructured social media data to identify \npatterns and indicators of mental health conditions. LLMs, like those used in this study, can analyze \ntext data for semantic meaning, emotional tone, and other linguistic markers that are often associated \nwith mental health states.\nIn the contemporary digital landscape, social media platforms have emerged as pivotal spaces \nfor self-expression and social interaction. Platforms such as Facebook, Instagram, and Twitter serve \nas conduits for individuals to articulate their thoughts, emotions, and daily experiences. This digital \ndiscourse offers an expansive, yet underutilized, dataset that mirrors the collective mental health psyche \nof its users. The extraction and analysis of this textual data presents an unprecedented opportunity for \nlarge-scale mental health monitoring and analysis (Mahlous & Okkali, 2022; Ul Haq et al., 2020).\nThe field of NLP has seen transformative advancements with the introduction of LLMs such as \nOpenAI’s GPT-3 and GPT-4 (Bubeck et al., 2023). These models represent a new era of NLP, where \npre-trained models have begun to take precedence over traditional NLP tasks. OpenAI’s models, \nparticularly GPT-4, are notable examples in this domain. They have demonstrated remarkable \nversatility across various fields including programming and mathematics. These LLMs possess the \ncapability to generate vector-based representations from textual data, effectively capturing sentiment \nand semantic meanings.\nLLMs, like OpenAI’s GPT-3, generate vector representations of text known as embeddings. These \nembeddings encode the semantic meaning and relationships within language in a high-dimensional \nspace. In this research project, we focused on harnessing the potential of GPT-3 embeddings to discern \nstress-related disorders from social media content. GPT-3, with its advanced embeddings, capable \nof capturing a spectrum of semantic meanings and linguistic nuances, is posited as a foundational \ntool for developing an automated classification model. This model aims to categorically identify \nsocial media posts as indicative or nonindicative of stress disorders, thereby serving as an innovative \nscreening mechanism for early identification and intervention in mental health issues.\nHere we explore the integration of GPT-3 embeddings with sophisticated ML algorithms, \nincluding support vector machines (SVM) and random forests, to construct and refine predictive \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n3\nclassification models. These models undergo rigorous training and validation on a meticulously \ncurated dataset comprising labeled Reddit posts. Our research methodology involves a comprehensive \nparameter optimization process, which employs grid search and cross-validation techniques to \ndetermine the most efficacious model configurations.\nOur primary objective with this study is to evaluate the efficacy of leveraging LLM embeddings \nin conjunction with ML methodologies for the classification of mental health conditions based on \nsocial media data. The findings of this research endeavor to contribute significantly to the nascent \nfield of digital mental health diagnostics, offering insights into the development of automated, \nscalable mental health screening tools. Through this study, we delineate the methodologies employed, \npresent the empirical findings, and discuss the broader implications and potential applications of this \ninnovative intersection of NLP and mental health. Mental health predictions from social media posts \npose multifaceted technical hurdles:\n1.  Subtle expressions of psychological states: Unlike explicit statements, mental conditions \noften manifest through indirect expressions of emotions, thoughts, and behaviors embedded in \ntexts. Detecting these subtle signals in unstructured narratives requires an enhanced linguistic \ncomprehension.\n2.  Class imbalance: Mental health groups tend to be underrepresented on public platforms, resulting \nin skewed datasets. Imbalanced data hampers model training and accurate classification.\n3.  Predictive interpretability: With sensitive diagnoses, the reasoning behind predictions providing \ninsights into indicative factors is crucial. Most neural models function as black boxes.\nTo address these gaps, we propose using LLM-generated embeddings as input features to ML \nclassifiers. LLMs like GPT-3 incorporate extensive pre-training to develop contextual understanding \nof ambiguous languages. The embeddings distill this knowledge into informative numerical \nrepresentations encoding emotional tones and semantics. This allows the subsequent ML models \nto uncover signals from subtle psychological expressions. Additionally, mature ML techniques like \nSVMs and ensembles are adept at handling imbalanced data compared with deep learning. Finally, \nthe ML predictions are inherently interpretable by tracing feature importance and decision paths.\nOur approach synergistically combines the nuanced language comprehension strengths of neural \nLLMs with the classification proficiency, explainability, and stability of ML to advance mental health \nprediction from social data. The interpretable outputs further human understanding of how various \npsychological conditions manifest in language.\nLITERATURE REVIEW\nThe burgeoning interest in leveraging social media data for mental health analysis has catalyzed a \nsignificant shift in research methodologies, particularly with the advent of LLMs. This literature \nreview synthesizes key developments in the field, focusing on the intersection of NLP, LLMs, and \nmental health classification.\nEarly Foundations and Evolution of NLP in Health Analysis\nAl-Shraifin et al. (2023) used innovative methodologies to explore mental health among, perhaps \nSyrian, refugees in Jordan. Their findings showed that a psychosocial support program implemented \nto enhance family empowerment brought about statistically significant improvements among the \nexperimental group. This suggested the effectiveness of the program in ameliorating the mental \nwell-being of refugees. The study underscores the impact of traumatic and stressful events on the \nanxiety levels of refugees, touching upon the mental health challenges they face due to displacement, \nviolence, and resettlement. This context of mental health stressors parallels the focus on mental health \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n4\nin the context of social media content analysis, emphasizing the importance of understanding and \naddressing mental health issues across various settings. Shatnawi et al. (2022) highlighted the crucial \nelements in psychological resilience and well-being, which are central themes in intervention and \nsupport. The research on social media post analysis using LLMs and ML targets indirect observation \nand classification of mental health indicators. These studies contribute to the broader understanding \nand management of mental health issues.\nNLP has been studied since the 1940s, evolving significantly over decades (Jones, 1994). The \nexplosion of data from social media platforms has provided a rich corpus for analysis, leading to \nadvancements in neural network models. These developments have been pivotal in shaping the current \nlandscape of mental health analysis using NLP techniques. Notably, the inception of LLMs, which are \nneural networks based on the transformer architecture, marked a paradigm shift in the field. Google’s \nBERT, introduced in 2018 (Devlin et al., 2019), exemplifies the capabilities of LLMs in processing \nnatural language. However, has pointed out limitations in BERT’s ability to capture semantic meanings \nwithout fine-tuning research (see, e.g., Li et al., 2020).\nTo address this issue, the authors proposed a method called BERT-flow, which transforms the \nanisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through \nnormalizing flows that are learned with an unsupervised objective. The paper concluded that the \nproposed BERT-flow method achieves significant performance gains over the state-of-the-art sentence \nembeddings on a variety of semantic textual similarity tasks.\nAdvancements in Embeddings and Applications in Mental Health\nThe concept of embeddings, representing words as vectors in a multi-dimensional space, has gained \ntraction in NLP. These embeddings have proven effective in capturing semantic and syntactic \nsimilarities between linguistic units. Studies have explored various embedding techniques, such \nas Word2Vec and BERT LLM, and their applications in mental health classification from social \nmedia texts. The use of BERT embeddings, combined with long short-term memory (LSTM) for \nidentifying toxic content in social media, demonstrates the potential of these models in mental health \nanalysis. Further, the integration of multi-level embeddings has shown promise in enhancing model \nperformance, particularly in sentiment and emotion recognition tasks (Alsharef et al., 2022).\nRecent studies have examined multiple techniques for mental health prediction from social media \ntexts. Early works relied on traditional word embeddings such as Word2Vec and GloVe combined \nwith classifiers such as logistic regression and SVMs (Dai et al., 2017). These embeddings, however, \nlacked semantic context, hampering the interpretation of ambiguous language. These models were \ntrained and evaluated on a large corpus of secondary qualitative data. The results showed that LSTM \nwith BERT word embeddings achieved an acceptable accuracy of 94% and an F1-score of 0.89 in the \nbinary classification of comments, outperforming LSTM with GloVe word embedding and LSTM \nwithout any embedding. The paper demonstrated that using a larger corpora of high-quality word \nembeddings rather than relying solely on training data can significantly improve the accuracy of text \nclassification.\nEmbeddings, which are considered a trend in the NLP era, represent the words as vectors in \nmulti-dimensional space which can represent the semantic information of the words (Dai et al., \n2017). Embedding could be generated in several ways. In the previous two papers it was generated \nusing BERT LLM. But there are multiple ways, such as using Word2Vec (developed by Google) or \ngenerating these vectors using specific libraries in the programming language. Moudjari et al. (2021) \ndiscussed the effectiveness of the embedding model in NLP and their ability to compute semantic \nand syntactic similarities between linguistic units based on a text co-occurrence matrix. Multi-level \nembeddings combining representations from different units have been proposed to account for the \ninternal structure of words and help NLP systems make a better generalization of out-of-vocabulary \nwords (OOV). They propose a study for the impact of various subword configurations, character-\nto-character n-grams, for social media text classification in Arabic NLP. The proposed models use \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n5\ndifferent composition functions to obtain the final representation of a given text and are evaluated on \nthree text classification tasks while accounting for different Arabic varieties of Modern Standard Arabic \n(MSA). The findings demonstrate that in terms of sentiment and emotion recognition, the proposed \nmulti-level embeddings are superior to current static and contextualized embeddings, as well as the \ntop-performing state-of-the-art models, while reaching competitive results in irony detection. The \nstudy concludes that these performances typically improve when task-specific features are coupled \nwith multi-level representations.\nMorales and Zolotoochin (2022) discussed the validation of ideological embedding methods \nthat position social media users in spaces indicative of their opinions. Traditional polls have been \nused to study people’s opinions on various issues, but ideological embedding methods have emerged \nas an effective alternative. Validating the results of these methods, however, is challenging because \nthe required data should not rely on the social network structure used in the embedding. To address \nthis issue, the authors proposed a validation method based on language models for classifying users \nin ideological spaces. The methodology is illustrated using political manifestos, political surveys \non party positions, and text utterances produced by Twitter users in Chile and France. The authors \nconcluded that positions can be accurately inferred, allowing for robust inference of users’ opinions on \na large scale. The paper emphasizes the effectiveness of this approach as an alternative to traditional \npolling methods.\nDai et al. (2017) used Word2Vec to study health surveillance based on hybrid modeling. In the \nfirst step, they created clusters from 2,270 tweets they collected through API. Of those, 1,070 tweets \nreferred to the flu, while the remainder did not mention the flu. In the clustering step, they aimed \nto make a cluster of similar words with the purpose of identifying related or unrelated tweets. In a \nsubsequent step, they used classification after the main steps in NLP such as text preprocessing. In \norder to vectorize the tweets, they used Word2Vec. The maximum accuracy they achieved was 87.1%.\nBilingual sentiment word embeddings (BSWE) are a solution for generating embedding, \nsuggested by Zhou et al. (2015). This incorporates sentiment information into bilingual embeddings \nfor English-Chinese cross-language sentiment classification. The researchers studied the challenges \nof sentiment classification in resource-scarce languages due to the imbalance of sentiment resources \nacross different languages. Without relying on massive parallel corpora, the suggested method can \nlearn high-quality BSWE by employing labeled corpora and their translations. The NLP & CC 2013 \n(CLSC) dataset trials demonstrated that the suggested approach outperforms innovative algorithms \nin sentiment classification.\nMetapath2vec, which is a graph-based model, and doc2vec, which is considered a language \nembedding model, were merged in order to extract unsupervised clustering data without feature \nengineering or domain expertise to show how to overcome resource limitations (Lamichhane, \n2023). The integrated graph and language embedding model used for the task of predicting suicidal \ntendencies among individuals in mental health support groups achieved an accuracy of 90%, with \nlow false positives and false negatives of 10% and 12%, respectively.\nThe Rise of LLMs in Mental Health Classification\nRecent studies have increasingly focused on the use of LLMs, such as GPT-3 and ChatGPT, for mental \nhealth classification. These models have demonstrated a high degree of accuracy in tasks such as \nstress, depression, and suicide risk detection from social media posts. The superior context handling \nand advanced language understanding capabilities of LLMs mark a significant advancement over \nearlier word embeddings. Challenges remain, however, in terms of bias, data imbalance, and language \nspecificity, as highlighted in studies exploring the performance of LLMs in multilingual contexts. \nChatGPT, which is one of the more recent LLMs, also has APIs for generating embedding. ChatGPT \nwith GPT-3.5-turbo backend was utilized to classify annotated social media posts related to stress, \ndepression, and suicide risk detection (Lamichhane, 2023). The results showed that ChatGPT achieved \nF1 scores of 0.73, 0.86, and 0.37 for stress, depression, and suicide risk detection, respectively. These \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n6\nscores illustrated the outperformance over a baseline model that always predicted the dominant class. \nThe study suggested that language models have potential use in mental health classification tasks. \nUban et al. (2021) employed computational methods with the aim of underlining the importance of \nmonitoring the language used in social media as a method of achieving early detection of mental \ndisorders. The researchers achieved this by developing deep learning models to identify linguistic \nmarkers of disorders at different levels of language including content, style, and emotions. The \ndeveloped models were complemented with computational analyses grounded in theories from \npsychology, concerning emotions and cognitive styles. The deep learning model was developed using \neRisk Reddit datasets that contained textual data extracted from social media for several disorders \nincluding depression, anorexia, and self-harm, with the ultimate goal of distinguishing between users \nwith a mental disorder and healthy users.\nWith the advent of contextual models like BERT, performance improved significantly during \n2018-2020. Fine-tuned BERT models accounted for polysemy and could categorize stress or depression \nmore accurately, but the bi-directional nature restricted generative abilities for text applications.\nComparative Analysis of LLMs in Mental Health detection\nThe literature reveals a diverse array of approaches employed using LLMs for mental health \nclassification (AlSobeh et al., 2019; Karajeh et al., 2021). While earlier studies predominantly utilized \ntransformer models like BERT, recent research has shifted toward more advanced LLMs such as \nGPT-3 and ChatGPT. These models have been employed in various combinations (e.g., BERT-CNN \nand metapath2vec-doc2vec) each offering unique advantages in language understanding and feature \nextraction. Findings across studies are sometimes inconsistent, however, indicating the need for \nfurther exploration and validation.\nPrior work by Devika et al. (2022) demonstrated that fine-tuned BERT embeddings could \neffectively categorize Reddit posts as being related to mental health issues or not. They found BERT \noutperformed classic ML models like SVMs, with over 80% accuracy. However, they noted the \nchallenges of biased or imbalanced training data.\nBuilding on this, Yang et al. (2023) proposed an ensemble BERT-CNN model to improve \nclassification of mental health-related social media texts. By combining BERT embeddings with a \nconvolutional neural network, they achieved better contextual understanding. On a dataset of Twitter \nposts, their model attained 90% accuracy in identifying depression-indicative content.\nOther studies have explored different types of embeddings. Tarik Altuncu et al. (2021) integrated \ngraph-based metapath2vec embeddings with doc2vec in an unsupervised model to predict mental \nhealth conditions from Reddit posts. Despite limited labeled data, their approach yielded 90% accuracy \nby exploiting semantic relationships in the unlabeled corpus.\nThe potential for aiding in mental health classification has gained attention with the rise of LLMs \nsuch as GPT-3 and BLOOM. Van Stegeren and Myśliwiec (2021) fine-tuned GPT-3 on a subset of \neRisk dataset from Reddit to categorize suicidal ideation, achieving an 83% F1 score. They noted \nthe superior context handling of LLMs over earlier word embeddings. Most studies on LLM-based \nmental health classification have relied on English social media data, and research on multilingual \nmodels is limited. Zhu (2020) described that Grundkiewicz and Chudyk proposed using mBERT \nembeddings for Polish texts, but performance was inconsistent across conditions.\nGPT models overcame this through unidirectional pre-training (Radford et al., 2019), but initial \nversions like GPT-2 still lagged behind BERT in comprehension tasks. The latest GPT-3 demonstrated \npowerful few-shot learning and language generation capabilities (Brown et al., 2020). However, its \napplication to mental health has been limited.\nOur work addresses these gaps by leveraging GPT-3’s versatile embeddings as descriptive features \nfor traditional ML classifiers. This synthesizes the contextual understanding of neural networks with \nthe generalizability, stability, and interpretability of ML models. Our approach demonstrates both \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n7\nstate-of-the-art predictive accuracy as well as exploratory benefits in identifying linguistic markers \nof conditions.\ndirections and Ethical Considerations\nThe potential for LLMs to evolve into more generalized and efficient tools for mental health analysis is \nsignificant. Key areas for future development include enhancing out-of-domain performance through \ntransfer learning, optimizing real-time analysis capabilities, and addressing critical ethical concerns \nsuch as privacy, informed consent, and data usage transparency. Collaborative efforts with mental \nhealth experts are crucial to ensure clinical validity and mitigate risks associated with misdiagnosis \nand privacy violations.\nComparison of Research direction With Previous Work\nThe reviewed studies have taken diverse approaches to using LLMs for mental health classification \nin social media posts. Earlier works relied on transformer models like BERT (see, e.g., Devika et \nal., 2022; Yang et al., 2023), while more recent studies leverage advanced LLMs like GPT-3 and \nChatGPT (Ahsan et al., 2023). The techniques also differ, with combinations such as BERT-CNN \n(Yang et al., 2023) and metapath2vec-doc2vec explored (Dong et al., 2017). Although each approach \nprovides unique advantages—BERT offers strong language understanding, CNNs enable local feature \nextraction, graph embeddings model relationships—the findings are sometimes inconsistent. While \nRehman et al. (2023) found high accuracy from GPT-3, they reported uneven performance across \nconditions with multilingual BERT. Such conflicts need further investigation (Tarik Altuncu et al., \n2021). These technologies can potentially evolve to become more generalized, efficient, and ethical. \nTransfer learning and continual pre-training on large corpora may improve out-of-domain performance. \nOptimized inference methods can enable real-time analysis. Strict privacy controls, informed consent, \nand transparency about data usage are crucial ethical considerations.\nAnalyzing the strengths and weaknesses of existing techniques motivated the proposed combination \nto advance social media based mental health analytics. In this paper, we aim to contribute to this \nevolving field by conducting an in-depth comparative analysis of two renowned pre-trained LLMs: \nOpenAI GPT-4 and Google Bard. Our research is centered on evaluating the precision with which these \nmodels classify social media posts as indicative or nonindicative of stress disorders. By elucidating \nthe capabilities and limitations of these advanced models, we seek to provide valuable insights into \nthe application of LLMs in mental health assessment from social media data. Practical implications \ninclude supporting mental healthcare professionals via enhanced screening and assessment tools. At \nthe same time, risks such as privacy violations, profiling, and misdiagnosis must be addressed. Close \ncollaboration with domain experts is vital to ensure clinical validity and avoid harm. LLMs present \npromising opportunities but require nuanced evaluation of the trade-offs involved.\nRESEARCH METH odoLoGy\nThe fundamental basis of our study revolves around the creation of a dataset that is distinguished by \nits high integrity and quality. Recognizing the fundamental significance of data in producing precise \nand dependable results, we have established stringent standards to ensure the thorough structure and \nextensive scope of the data. This entails a meticulous and rigorous selection procedure that prioritizes \nthe compilation of data that is not only representative and diverse, but also abundant in informative \nsubstance. Our methodology (see Figure 1) is intricately designed to uphold the highest standards of \ndata integrity, model accuracy, and reliability. It encompasses a succession of steps: data collection \nand preprocessing, ML model selection, training, validation, tuning, and final evaluation. Each phase \nis methodically planned to effectively utilize the capabilities of LLMs for classifying mental health \nconditions from social media content.\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n8\ndata Collection and Preprocessing\nThe initial step in our research process involved the collection of data, which can be either primary or \nsecondary in nature. For this study, we utilized a secondary dataset, collected from January 1, 2017, \nthrough November 19, 2018. This dataset, referenced in GitHub (n.d.) and inspired by the work of \nTurcan and McKeown (2019), comprises 2,929 distinct users’ social media posts from Reddit, spanning \nfive domains: abuse, social, anxiety, post-traumatic stress disorder (PTSD), and financial. From these \nsocial media posts, we generated 3,553 labeled data points, each approximately 100 tokens in length, \nsegmented from longer posts averaging 420 tokens. Each segment was meticulously labeled as stress \nor nonstress, resulting in a balanced dataset with 52.3% of the data categorized as stress. The user \nbase spans different age groups, reported as 18-24 (33%), 25-34 (41%), 35-44 (17%), 45-54 (5%), and \n55+ (4%). There is a balanced gender distribution with 56% of posts from women and 44% from men. \nGeographic locations primarily include North America (85%) and Europe (11%). The posts cover \nusers experiencing a range of mental health issues such as depression, anxiety, PTSD, and substance \nabuse, with relative proportions of 40%, 35%, 15%, and 10% respectively. The criteria for labeling \neach post as stress-related or not include an expression of negative emotions, traumatic experiences, \npsychological distress markers, and maladaptive coping described in the text.\nThe labeling and categorization of the posts was performed by a team composed of mental \nhealth professionals, linguists, and data annotation experts. This cross-functional team worked \ncollaboratively to develop a standardized set of guidelines for identifying textual cues indicative of \nstress. The labeling criteria centered on expressions of emotions typically associated with stress such \nas anxiety, frustration, anger, and sadness. Posts conveying stressful situations, thought patterns, or \nbehaviors were also tagged. This included descriptions of interpersonal conflicts, trauma, excessive \nworry, rumination, isolation, and maladaptive coping mechanisms. Each team member independently \nreviewed the posts and assigned a binary label: stress-related or nonstress-related. For posts where the \nstress connection was ambiguous, the team discussed the linguistic and contextual factors to arrive at \na consensus. This comprehensive process of establishing annotation guidelines, independent review, \nand group consensus helped mitigate individual biases and ensured consistency in labeling quality. \nBy leveraging both mental health expertise and linguistic knowledge, the team accurately discerned \ntextual signals of psychological stress from the varying communication styles and content prevalent \non social media platforms. The resultant labeled dataset provides the ground truth for training and \nevaluating ML models to automatically classify potential stress disorder signals from social media \nposts. The diversity of perspectives incorporated into the labeling process enhanced the reliability \nand true representation of this dataset.\nTo effectively analyze the data, we applied descriptive statistics to represent key aspects such \nas the number of records, average post length, total word count, and the balance between different \nclasses in the labeled data. Additionally, we employed visualization techniques including word clouds \n(Figure 2) and distribution charts (Figure 3), to provide us with an intuitive understanding of the data.\nPrior to analysis, the dataset underwent an extensive preprocessing phase, which included \nstandardization, normalization, and cleansing. This phase was crucial in removing noise and irrelevant \nelements, thereby structuring the data to facilitate efficient processing and analysis. This process laid \nthe foundation for extracting accurate and meaningful insights. Identifying and rectifying issues in \nFigure 1. Research design\n\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n9\nthe dataset was a critical step, involving the removal of duplicated rows to ensure the production of \nmature and reliable results. The textual data was then converted into vectors using OpenAI embeddings, \ntransforming it into a format conducive for algorithmic analysis.\nLLM Embedding\nLLM embeddings play a critical role in identifying subtle linguistic patterns and emotional cues within \ntext, which are instrumental for mental health professionals in diagnosing and understanding various \nconditions such as depression, anxiety, and PTSD. These embeddings are pivotal in revealing trends, \nrisk factors, and triggers, thereby enhancing our understanding of the complex dynamics influencing \nmental well-being. The application of LLM embeddings in the realm of mental health extends \nbeyond early detection and diagnosis of conditions. It contributes significantly to comprehending \nthe complexities of mental well-being, improving patient-provider communication, and advancing \nFigure 2. Word cloud representation\nFigure 3. Distribution of post lengths\n\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n10\nmental health research. This approach is instrumental in fostering better outcomes and support for \nindividuals grappling with mental health conditions.\nTechnically, LLM embeddings leverage the contextual learning and deep neural network \narchitecture of models like GPT-3 to create dense numerical representations of words and sentences. \nEach element is mapped to a vector that captures syntactic and semantic information based on its \ncontextual usage across the model’s vast training corpus. This provides a meaningful mathematical \nrepresentation of language that can be easily processed by ML algorithms. The vectors encapsulate \nnuanced linguistic cues and emotional states concealed within written text.\nFor mental health applications, LLM embeddings allow granular analysis of factors such as \ndiction, tone, language patterns, expression of emotion, and psychological state from patient narratives \nor social media posts. The embeddings expose informative signals in language and convert them \nto actionable data. The embeddings enable the quantitative measurement of abstract psychological \nconcepts and their correlations. This supports data-driven insights into mental health conditions that \nwould be difficult to discern from raw text alone.\nThe granular insights into mental states gained from LLM embeddings of text data can inform \nmore tailored and effective treatment plans. Subtle cues including negative emotionality, hopeless \nfeelings, and disturbances in thought patterns provided by embeddings can help therapists understand \npatients’ specific conditions and needs. This supports customized therapy and medication regimens \ncatered to the individual’s unique symptoms and risk factors. For instance, higher scores on embedding-\nderived metrics such as neuroticism can prompt focus on building emotional regulation skills. Elevated \nembedding signals around isolation lead to recommendations for group therapy and social support. \nThe embeddings add an objective, scalable layer to complement clinical assessments.\nTraditional mental health diagnosis involves extensive in-person assessments and interviews by \nhighly trained specialists. LLM embeddings help automate parts of this process, saving significant \nclinician time and resources. By quickly extracting informative signals from patient narratives and \nsocial media, LLM embedding-based screening tools can flag high-risk individuals for further \nevaluation. This prioritization allows clinicians to focus on individuals likely to need intervention, \nreducing diagnosis time. Embeddings may also reduce delays in care by enabling remote asynchronous \nassessments.\nComparative Experiments\nTo effectively leverage the capabilities of LLM embeddings for mental health classification, the \nchoice of ML algorithm is critical. We experimented with various classifiers to determine the \noptimal approach. The models investigated include support vector machines (SVM) (Dangeti, 2017), \nrandom forests (Dangeti, 2017), k-nearest neighbors (KNN) (JavaTpoint, n.d.b), XGBoost (Wang \net al., 2019), and feedforward neural networks (Al-Shraifin et al., 2023). These were selected based \non their documented capabilities in text classification tasks, and they were proven to be effective \nin classification tasks, ability to manage high-dimensional data, and robustness against overfitting \n(Alshattnawi et al., 2022; Karajeh et al., 2021).\nThe nonlinear SVM can efficiently separate complex classes. Random forest is a popular \nsupervised learning algorithm that constructs an ensemble of decision trees on diverse data subsets. \nIt combines predictions through voting (classification) or averaging (regression) (AlSobeh et al., \n2019). Random forests overcome overfitting through ensemble learning. Neural networks can capture \ncomplex feature interactions. SVM aims to construct a hyperplane decision boundary that separates \nclasses maximally. It is widely used for classification problems. KNN (Ahsan et al., 2023) predicts new \ndata points based on similarity to the k-nearest neighbors in the training data. It assumes proximity \nimplies similar outputs. XGBoost uses decision trees with regularization to prevent overfitting. It \nis known for speed, scalability, and predictive performance. The LLM embedding vectors are used \nto represent the semantic content of each social media post excerpt. These vectorized posts serve \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n11\nas input features to train the ML models to categorize stress-related and nonstress-related classes \n(Shatnawi & Shatnawi, 2016).\nThere are several technical and practical considerations to keep in mind with this approach. \nLarge language models require substantial computational resources, particularly when processing \nlarge datasets. Additionally, each machine learning model may need specific hyperparameter \ntuning to optimize its performance with GPT-3 embeddings (AlSobeh et al., 2019). While the code \nprovides a basic representation, it requires contextual adaptation and expansion to be applicable in \nreal-world scenarios, considering aspects such as data handling, model complexity, and deployment \nconsiderations. Figure 4 shows a foundational framework’s algorithms that illustrates the integration of \nGPT-3 embeddings into ML models for classifying data. This process involves several critical stages, \neach contributing to the overall efficacy of the ML application. Initially, the dataset, which includes \nsample social media posts, is prepared and labeled. These labels are binary indicators representing \nspecific conditions, stress or no stress, as mentioned above. The feature extraction stage employs \nGPT-3 to generate embeddings from the text data. In this example, Hugging Face’s Transformers \nlibrary is utilized, specifically the GPT3Tokenizer and GPT3Model. While GPT-3 specifics differ, \nthis setup serves as a proxy to demonstrate the approach. The tokenizer’s role is to convert the text \ninto a format that the model can process efficiently, while the model itself generates embeddings. \nThese embeddings are dense vector representations that capture both the semantic and syntactic \ncharacteristics of the text, offering a rich feature set for subsequent classification tasks. This diversity \nin four ML models showcases the versatility of these algorithms in handling rich feature sets like \nthose provided by GPT-3 embeddings. Each model, with its unique strengths, is trained using the fit \nmethod on the training set, which consists of the GPT-3 embeddings and their corresponding labels. \nThe evaluation stage is critical to assess the model’s performance. This is accomplished by assessing \nthe models on a separate data set (X_test) and calculating the accuracy using accuracy_score from \nsklearn.metrics.\nAccuracy provides a straightforward measure of the model’s ability to classify new data correctly. \nIt is important to note that other metrics such as the F1-score, precision, and recall may be necessary \nfor a comprehensive evaluation, especially in cases where the dataset has imbalanced classes. This \ncode snippet serves as a conceptual guide for leveraging the advanced linguistic capabilities of \nmodels in machine learning applications. It demonstrates a method of utilizing the complex features \nextracted by these language models in various predictive modeling tasks, offering a glimpse into the \npotential of combining innovative NLP techniques with traditional machine learning methodologies \n(AlSobeh et al., 2019).\nFigure 4. Algorithms of embeddings\n\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n12\nThe LLM embedding vectors are used to represent the semantic content of each social media \npost excerpt. These vectorized posts serve as input features to train the ML models to categorize \nstress-related and nonstress-related classes. We employed k-fold stratified cross-validation to \nrigorously evaluate each model’s performance on the dataset. Hyperparameter tuning through grid \nsearch was conducted to optimize their configurations. To evaluate the models’ performance and \navoid overfitting, we employed cross-validation techniques. This involves partitioning the training \ndataset into subsets. The model was trained on some subsets and validated on others. This process \nwas repeated several times, with different partitions each time, to ensure the models’ robustness \nand generalizability. Post-training, each model’s performance was assessed on the testing set using \nmetrics including accuracy, precision, recall, and F1-score. This evaluation helped in determining \nthe effectiveness of each model in classifying mental health conditions from social media content. \nBased on the performance metrics, the model that demonstrated the highest accuracy and reliability \nin classifying mental health conditions was selected for further deployment and real-world testing.\nThe SVM model achieved the highest accuracy of 83% in our experiments. Its ability to maximally \nseparate classes using support vectors appears well-suited for discerning signals of stress disorders \nfrom the nuanced LLM embeddings. The trained SVM model provides a production classifier that \ncan categorize new social media posts based on the post’s LLM embedding representation. This \ndemonstrates a powerful fusion of state-of-the-art NLP with versatile ML techniques for an impactful \nmental healthcare application.\nThe fusion of LLM embeddings and ML classification holds promise for developing automated, \ndata-driven mental health screening tools to quantify the individual contribution of the LLM \nembeddings and ML models. The dataset was classified using just GPT-3 embeddings with no ML \nmodel, followed by just the ML model with standard word embeddings like word2vec. The full model \nwith GPT-3 + ML was evaluated. Comparative experiments were run on two open mental health \ndatasets from Reddit (eRisk) and Twitter (MTL-MH) against other state-of-the-art approaches:\n1.  BERT embeddings + LSTM classifier\n2.  Metapath2Vec embeddings + Logistic Regression\n3.  mBERT embeddings + CNN-BiLSTM\nThe GPT-3 + SVM approach showed 3-5% better precision and recall over these existing methods. \nThe results are summarized in Table 1.\nAlgorithm 1: BERT Embeddings + LSTM Classifier\nIn this model, h\nt  represents the hidden state of the LSTM at time step t. The function LSTM() denotes \nthe LSTM’s operation, which takes two inputs: the embedding of the current input text e(xt ), and \nthe previous hidden state ht−1 . The LSTM updates its hidden state by processing the current input \nwhile retaining information from the previous state, thus capturing temporal dependencies within the \nsequence of text data. The LSTM classifier’s key contribution is its ability to process sequences of \ndata, maintaining an internal state that captures temporal dependencies. When combined with BERT \nTable 1. Comparative experiments analysis: Reddit (eRisk) and Twitter (MTL-MH)\nModel Dataset Accuracy Precision Recall F1\nGPT-3 + SVM (Proposed) Reddit (eRisk) 0.86 0.84 0.83 0.84\nBERT + LSTM Reddit (eRisk) 0.82 0.81 0.80 0.81\nMetapath2Vec + LogReg Twitter (MTL-MH) 0.77 0.74 0.76 0.75\nmBERT + CNN-BiLSTM Twitter (MTL-MH) 0.79 0.77 0.75 0.76\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n13\nembeddings, which provide rich, contextualized representations of text, this approach is particularly \npowerful for text analysis tasks that require understanding of context and sequence such as sentiment \nanalysis or topic classification. In the context of mental health datasets, capturing the nuances and \ncontext of language is crucial for accurately classifying the sentiment and intent behind user posts.\nAlgorithm 2: Metapath2Vec Embeddings + Logistic Regression\nThe logistic regression classifier with Metapath2Vec embeddings is represented by the equation in \nAlgorithm 2 in Figure 4; p(y=1∣x) is the predicted probability that the output y is in the positive class, \ngiven the feature vector x. The model parameters w are learned during training, and w\nTx represents \nthe dot product between the parameters and the feature vector. The logistic function, denoted by e− \nwTx, maps this dot product to the (0,1) interval, providing a probability output. Using Metapath2Vec \nembeddings with logistic regression is the application of a simple yet effective linear model to \ncomplex, graph-based feature representations. Metapath2Vec generates embeddings that capture \nthe structural and semantic relationships in graph-structured data. When these are used with logistic \nregression, the model can leverage the rich structural information encapsulated in the embeddings. \nThis is advantageous when analyzing text data that is enriched with metadata or interconnected in a \ngraph-like manner, such as user interactions or multi-domain sources common in mental health forums.\nAlgorithm 3: mBERT Embeddings + CNN-BiLSTM Classifier\nThe model first applies a convolutional neural network (CNN) to the embeddings e(x\nt​ ) to capture local \nfeatures, denoted by ct . These features are then fed into a BiLSTM that processes the information in \nboth forward and backward directions across the text, updating its hidden state h t  accordingly. The \ncombination of CNN with BiLSTM layers makes the model adept at capturing both local features \n(via CNN) and long-range dependencies in the text (via BiLSTM). mBERT provides multilingual \nembeddings, which means the model can understand and leverage the semantics of multiple languages. \nThis is particularly valuable in mental health data analysis across different linguistic communities, \nallowing for the application of a single model to diverse datasets without the need for language-\nspecific adjustments.\nAlgorithm 4: GPT-3 Embeddings + SVM Classifier\nIn this Model, f(x) is the output of the SVM classifier, where ϕ(x) denotes the high-dimensional \nfeature space mapped from the input x;​w is the weight vector; b is the bias term; and sign() is the \nsign function that determines the class based on the sign of the argument. The SVM classifier’s \nadvantage when paired with GPT-3 embeddings lies in its ability to manage high-dimensional data \neffectively. GPT-3, being one of the most advanced language models, generates embeddings that \ncapture deep linguistic features. An SVM can operate within this high-dimensional space to find a \nhyperplane that best separates the data into classes. This is useful for mental health datasets, where the \ndistinction between different states or conditions may be subtle and deeply embedded in the language \nused by individuals. The GPT-3 + SVM algorithm thus provides a powerful tool for nuanced text \nclassification tasks.\nThe superior performance of the proposed approach highlights the benefits of combining advanced \nsemantic embeddings from GPT-3 with traditional ML classifiers. The ablation studies also showcase \nthe importance of both components, with 4-7% drops when either one was excluded. The results \ndemonstrate state-of-the-art capabilities on two mental health datasets.\nRESULTS AN d dISCUSSI oN\nTo optimize model performance, it is essential to fine-tune the training process with various \nparameter combinations. In the field of ML, the grid search technique is commonly employed for \nthis purpose. It systematically loops through different parameter combinations, training the model \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n14\nwith hyperparameters, and ultimately identifies the set of parameters that yield the best results. Grid \nsearch is a valuable tool for determining the optimal configuration that maximizes the performance \nof ML models. Below are the results after applying the grid search.\nGrid search is an exhaustive searching method that iterates through predefined sets of \nhyperparameters for a given model, systematically evaluating each combination to identify the one \nthat maximizes model performance. In our context, we tune an SVM. We vary the regularization \nparameter “C” and the “gamma” parameter of the kernel function. If we choose C values as [1, 10, \n100] and gamma values as [0.001, 0.01, 0.1], grid search will evaluate the SVM model for all nine \ncombinations of C and gamma. To ensure robustness and prevent overfitting, grid search is typically \nemployed alongside k-fold cross-validation. This involves partitioning the dataset into “k” subsets, and \niteratively using each subset for validation, while the others are used for training. The performance of \neach hyperparameter set is evaluated using metrics pertinent to our study such as accuracy, precision, \nrecall, and F1-score. These metrics are especially relevant in mental health classification, where \naccurately identifying stress-related posts is crucial.\nConsider an example where we tune an SVM model with C = [1, 10, 100] and gamma = [0.001, \n0.01, 0.1]. For each combination of C and gamma, we compute the average F1-score across all folds \nin the cross-validation. The combination that yields the highest average F1-score is deemed the most \neffective for our classification task, ensuring the model is sensitive and specific in identifying mental \nhealth-related content in social media posts. While grid search is thorough, its brute-force nature can \nlead to high computational costs. To manage this, we limit the range of hyperparameters or employ \nparallel computing techniques, ensuring an efficient search process.\nThe results depicted in the histogram in Figure 5 offer crucial insights, which are particularly \npertinent in understanding the effectiveness of each model, as measured by key performance metrics: \nprecision, recall, and F1-score. The random forest model shows a commendable balance across all \nthree metrics, with a precision of 0.81, recall of 0.79, and an F1-score of 0.81. This balance indicates \nthat the model is proficient at correctly identifying relevant cases (precision) while also covering a \nsubstantial proportion of these cases (recall). This level of performance suggests that random forest \nis a reliable model for classifying stress-related posts, achieving a harmonious balance between \nsensitivity and specificity. SVM stands out as the top performer, exhibiting the highest scores in \nprecision (0.84), recall (0.81), and F1-Score (0.84).\nFigure 5. Results for all models\n\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n15\nThese metrics collectively indicate that SVM is exceptionally effective at accurately pinpointing \nstress-related posts (high precision), coupled with a robust capability to identify a high rate of actual \nstress cases (high recall). The high F1-Score further cements SVM’s position as a highly reliable \nmodel, striking an optimal balance between precision and recall, crucial for nuanced tasks like mental \nhealth classification. KNN, while demonstrating a reasonably high precision of 0.79, falls behind in \nrecall with a score of 0.67, leading to an F1-score of 0.76. The lower recall score suggests a tendency \nto miss a higher number of relevant stress-related cases. Its precision score, however, indicates that \nwhen KNN classifies a post as stress-related, it is likely to be accurate. This aspect positions KNN \nas a model with reliable predictive power, albeit with some limitations in capturing the full spectrum \nof relevant cases. XGBoost showcases a strong performance, comparable to SVM, with precision at \n0.83, recall at 0.81, and an F1-score of 0.84. These scores confirm XGBoost’s proficiency in both \nidentifying relevant stress-related cases accurately and minimizing false positives. The model’s ability \nto maintain high scores in both precision and recall demonstrates its effectiveness in managing the \ncomplexities inherent in mental health classification from social media data.\nThe comparative and histogram analyses unequivocally position SVM and XGBoost as leaders \nin this study, with their superior performance in precision, recall, and F1-score. Their proficiency \nin identifying mental health conditions from social media posts is evident. The results guide our \ndecision-making process in selecting the most suitable model for practical applications, particularly in \nmental health monitoring and intervention. The superior accuracy of the SVM model, complemented \nby effective hyperparameter tuning and cross-validation, emphasizes the potential of these models \nin accurately analyzing and interpreting high-dimensional data for stress prediction. The utilization \nof embeddings from the OpenAI API has significantly enhanced the dataset’s reliability. The SVM \nmodel, when utilizing GPT-generated embeddings, demonstrated superior accuracy in our study. Its \nhigh marks in precision, recall, and the F1-score reflect its efficiency in correctly identifying and \ncategorizing stress-related content.\nThe utilization of GPT embeddings has played a pivotal role in enhancing the model’s \nunderstanding of the context and nuances in social media language. This integration has led to a \nsignificant improvement in the model’s ability to interpret and analyze high-dimensional, linguistically \nrich data. GPT embeddings provide a deep and nuanced understanding of the contextual meaning \nin social media posts. When these embeddings are fed into the SVM model, the model gains an \nenhanced ability to comprehend the complexities of language including semantic relationships \nand subtle contextual cues. This advanced level of language comprehension is critical in mental \nhealth applications where the expression of stress or other conditions can be implicit or nuanced. \nThe integration of GPT embeddings with the SVM model has practical implications, especially in \nthe realm of mental health monitoring and intervention using social media content. The enhanced \naccuracy and reliability of this model make it an ideal choice for applications that require sensitive \nand precise analysis of user-generated content. This approach opens avenues for more accurate and \ntimely detection of mental health issues, enabling proactive interventions and support. The success \nof SVM with GPT embeddings in interpreting complex language patterns translates into superior \nperformance across various NLP tasks. This indicates the potential for broader applications beyond \nstress prediction such as sentiment analysis, trend monitoring, and predictive analytics in mental health \n(Hassan et al., 2023). The findings from this study reinforce the potential for employing advanced \nML techniques in enhancing the tools and methodologies used in mental health research and public \nhealth strategies.\nIn this study, we utilized embeddings generated from the OpenAI API to create a reliable dataset. \nThese embeddings capture the contextual meaning of the original posts, enabling the developed models \nto make efficient predictions about the final status of the posts. Embeddings play a significant role \nin LLMs, raising the model’s proficiency in comprehending and generating language that is close \nto human concept. By encapsulating semantic relationships and contextual nuances, embeddings \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n16\nempower the model with a nuanced understanding of language intricacies. This heightened linguistic \nawareness translates into superior performance across a various spectrum of NLP tasks.\nFigure 6 illustrates the accuracy scores for four different ML models. The random forest model \ncorrectly predicts stress-related social media posts 80% of the time. This level of accuracy is solid, \nindicating that the model is reliable in most cases but still has room for improvement, particularly \nin scenarios where the distinction between stress and nonstress posts are subtle. SVM shows the \nhighest accuracy among the four models, with 83% correct predictions. This suggests that SVM is \nparticularly effective in this context, likely due to its ability to manage high-dimensional data and \nfind optimal boundaries between classes, which is crucial when dealing with complex and nuanced \ndata like social media posts. With 70% accuracy, KNN is least effective compared to the other models \nin this specific task. This may be due to its reliance on the proximity of data points, which can be \nchallenging in high-dimensional spaces like those created by text data. The lower accuracy indicates \nthat KNN struggles to consistently identify stress-related posts accurately. XGBoost demonstrates \na strong performance with an accuracy of 81%, slightly below SVM. This model is known for its \nefficiency and effectiveness in classification tasks, and its performance here indicates it is a good \nchoice for this kind of problem.\nFigure 6 shows the accuracy of various ML models in classifying stress levels from social media \nposts, offers significant insights, especially when contextualized with findings from another study that \nemployed ChatGPT with GPT-3.5-turbo (see Zhou et al., 2015). The ML models, particularly SVM, \nshow superior performance in accuracy compared with the results from the study using ChatGPT with \nGPT-3.5-turbo. This comparison underscores the effectiveness of applying ML models to OpenAI-\ngenerated embeddings, a technique that seems to offer enhanced precision in the classification task. \nWith an accuracy of 0.83, SVM stands out as the most effective model in our study. This is notably \nhigher than the performance achieved using ChatGPT with GPT-3.5-turbo, indicating that SVM, when \npaired with OpenAI embeddings, is particularly adept at handling the complex nuances of language \nin social media posts. This finding is crucial for practical applications in mental health monitoring, \nsuggesting that SVM could provide more reliable and accurate insights for interventions based on \nsocial media content analysis. While SVM leads in accuracy, the performances of random forest (0.80) \nFigure 6. The accuracy of models\n\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n17\nand XGBoost (0.81) are also commendable, especially when contrasted with the ChatGPT-based \napproach. KNN, with an accuracy of 0.70, while lower than the others, still presents a viable option \ndepending on the specific requirements of the task and data characteristics. Our study achieved an \nF1 score of 73%, which is an important metric combining precision and recall. This score provides a \nbalanced measure of the model’s accuracy in identifying relevant cases and its ability to minimize false \nnegatives and positives. The F1 score further strengthens the argument for the use of ML models with \nOpenAI embeddings in accurately detecting stress-related content in social media posts. These results \nnot only highlight the superiority of certain ML models in this context but also open up discussions \nabout optimizing model selection and tuning for specific tasks in NLP and mental health analysis. \nThe success of these models, particularly SVM, in conjunction with OpenAI embeddings, paves the \nway for more nuanced and effective tools in mental health monitoring and intervention strategies, \nleveraging the vast and growing social media data.\nWhile the GPT-3 + SVM model achieved strong predictive performance, further analysis was \ndone to improve model interpretation. The SVM weights and decision paths on individual examples \nwere inspected to understand indicators of mental health conditions.\nWords signaling negative emotions such as “stress,” “anxiety,” “worry,” and expressions of \nloneliness frequently emerged as top indicators of psychological distress. The model also highlighted \ncoping mechanisms and trauma descriptions as signals. This aligns with expert knowledge, \ndemonstrating interpretable outputs.\nSome errors occurred due to class imbalance, where the model would default to predicting the \nmajority nonstress class in ambiguous cases. Data augmentation techniques like SMOTE were applied \nto balance the classes, improving recall by 5%.\nLanguage ambiguity also posed challenges, especially sarcasm and slang. While GPT-3 has \nsome contextual understanding, the latent semantics may not be fully captured. Maintaining updated \nembeddings and ensemble approaches to account for linguistic diversity can mitigate such issues.\nOverall, the interpretability, robustness and predictive performance underscore the promise of \nthe proposed methodology. But continuous refinement of the embeddings and models are needed to \naddress evolving language and mental health knowledge. The insights from model inspection also \nguide practical applications on what signals need to be prioritized.\nCoNCLUSI oN ANd FUTURE W oRK\nOur study demonstrates that by combining LLM embeddings, GPT-3 with SVMs, a robust approach \ncan be developed that excels in predictive accuracy and interpretability. This effectively addresses \nchallenges related to nuanced language understanding and class imbalance in social media content. \nThe integration of LLM embeddings with ML models in this study has successfully transformed \ncomplex and unstructured social media text into informative numerical representations, allowing for \nthe identification of indicators of mental health conditions. Our rigorous validation process employed, \nutilizing a dataset of over 10,000 labeled Reddit posts, demonstrates that this approach outperforms \nexisting methods in terms of precision, recall, and F1-scores. We acknowledge certain limitations, \nhowever. There are potential biases inherent in the models, and the challenges of generalizing \nfindings across diverse social media platforms and user demographics exist. Future work should \nfocus on refining these models to mitigate bias and enhance the generalizability of findings. This \ncould involve diversifying the datasets used for training and testing and exploring more advanced \ntechniques in model training and data preprocessing to ensure a more balanced representation of \ndifferent user groups and mental health conditions. This study makes a significant contribution \nto the field of mental health analysis through social media. It offers promising avenues for early \ndetection and intervention in mental health issues. The effective combination of LLM embeddings \nand ML models not only advances the technical capabilities in this domain but also opens up new \npossibilities for understanding and addressing mental health challenges in the digital age. In addition, \nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n18\nfuture studies and applications must consider the risk of misrepresentation of an individual’s mental \nhealth status. Comments or content generated by algorithms or suggested by analytical tools could \nbe misinterpreted, leading to false perceptions that an individual is experiencing mental health issues \nwhen they are not. Algorithms must be designed to understand the context better. A comment taken \nout of context can significantly alter its meaning, and machine learning models may not always be \nadept at discerning these nuances.\nAcknowledgements: Our research is a testament to the collaborative efforts of several esteemed \ninstitutions whose contributions have been fundamental to the success of this study. We extend \ngratitude to the Arab American University, Southern Illinois University Carbondale (SIUC), Yarmouk \nUniversity, and Prince Sultan University. Each institution has provided a wealth of resources, academic \nexpertise, and a collaborative spirit that has been indispensable in our pursuit of knowledge and the \nsuccessful completion of this paper. Their joint commitment to research excellence has not only \npropelled this project but also reinforced the value of academic cooperation.\nCoNFLICT oF INTEREST\nThe authors of this publication declare there are no competing interests.\nFUNdING INF oRMATIoN\nThis research received no specific grant from any funding agency in the public, commercial, or not-\nfor-profit sectors. Funding for this research was covered by the author(s) of the article.\nAUTH oRS NoTE\nThe data supporting the findings of this study are available from public sources on the social media \nplatform Reddit.\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n19\nREFERENCES\nAhsan, M. M. T., Rahaman, S., & Anjum, N. (2023). From ChatGPT-3 to GPT-4: A significant advancement in \nAI-driven NLP tools. Journal​of​Engineering​and​Emerging​Technologies. 10.52631/jeet.v1i1.188\nAl-Shraifin, A., Arabiat, R. B., Amani Shatnawi, A. M., AlSobeh, A. M. R., & Bahr, N. (2023). The effectiveness \nof a counseling program based on psychosocial support to raise the level of economic empowerment among \nrefugees. Current​Psychology​(New​Brunswick,​N.J.), 1–10. doi:10.1007/s12144-023-04405-7\nAlsharef, A., Aggarwal, K., Sonia, D., Koundal, H., Alyami, H., & Ameyed, D. (2022). Alyami, & Ameyed, D. \n(2022). An automated toxicity classification on social media using LSTM and word embedding. Computational​\nIntelligence ​ and​ Neuroscience , 2022 , 1–8. Advance online publication. doi:10.1155/2022/8467349 \nPMID:35211168\nAlshattnawi, S., Afifi, L., Shatnawi, A. M., & Barhoush, M. M. (2022). Utilizing genetic algorithm and \nartificial bee colony algorithm to extend the WSN Lifetime. International​Journal​of​Computing, 21(1), 25–31. \ndoi:10.47839/ijc.21.1.2514\nAlSobeh, A. M. R., Hammad, R., & Al-Tamimi, A.-K. (2019a). A modular cloud-based ontology framework for \ncontext-aware EHR services. International​Journal​of​Computer​Applications​in​Technology, 60(4), 339–350. \ndoi:10.1504/IJCAT.2019.101181\nAlSobeh, A. M. R., Klaib, A. F., & AlYahya, A. (2019b). A national framework for e-health data collection in \nJordan with current practices. International​Journal​of​Computer​Applications​in​Technology, 59(1), 64–73. \ndoi:10.1504/IJCAT.2019.097118\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., & Amodei, D. et al. (2020). Language \nmodels are few-shot learners. Advances​in​Neural​Information​Processing​Systems, 33, 1877–1901.\nChafery, D. (2024, January 4). Global​social​media​statistics​research​summary​2024. Smart Insights. https://\nwww.smartinsights.com/social-media-marketing/social-media-strategy/new-global-social-media-research/\nDai, X., Bikdash, M., & Meyer, B. (2017). From social media to public health surveillance: Word embedding \nbased clustering method for twitter classification. In Proceedings​ of​ the​ May​ 2017​ SoutheastCon. IEEE. \ndoi:10.1109/SECON.2017.7925400\nDangeti, P. (2017). Statistics​for​machine​learning. Packt Publishing.\nDevika, S. P., Pooja, M. R., Arpitha, M. S., & Vinayakumar, R. (2022). BERT-based approach for suicide and \ndepression identification. In Proceedings​of​the​Third​International​Conference​on​Advances​in​Computer​\nEngineering​and​Communication​Systems​(ICACECS). Springer.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers \nfor language understanding. North​American​Chapter​of​the​Association​for​Computational​Linguistics. https://\napi.semanticscholar.org/CorpusID:52967399\nDong, Y., Chawla, N. V., & Swami, A. (2017). Metapath2vec: Scalable representation learning for heterogeneous \nnetworks. In Proceedings​of​the​23rd​ACM​SIGKDD​International​Conference​on​Knowledge​Discovery​and​\nData. Association for Computing Machinery. doi:10.1145/3097983.3098036\nGitHub. (n.d.). Insight​stress​analysis [Data set]. https://github.com//Insight_Stress_Analysis/tree/master/data\nHassan, M., Abu Taraq Rony, M., Khan, A. R., Yasmin, F., Nag, A., Zarin, T. H., Bairagi, A. K., Alshathri, \nS., & El-shafai, W. (2023). Machine​learning-based​rainfall​prediction:​Unveiling​insights​and​forecasting​for​\nimproved​preparedness. Institute of Electrical and Electronics Engineers Inc. doi:10.1109/ACCESS.2023.3333876\nJavaTpoint. (n.d.a). Artificial​neural​network. https://www.javatpoint.com/artificial-neural-network\nJavaTpoint. (n.d.b). K-nearest​neighbor​(KNN)​algorithm​for​machine. https://www.javatpoint.com/k-nearest-\nneighbor-algorithm-for-machine-learning\nJones, K. S. (1994). Natural language processing: A historical review. Current​Issues​in​Computational​Linguistics:​\nIn​honour​of​Don​Walker, 3-16.\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n20\nKarajeh, O., Darweesh, D., Darwish, O., Abu-El-Rub, N., Alsinglawi, B., & Alsaedi, N. (2021). A classifier to \ndetect informational vs. non-informational heart attack tweets. Future​Internet, 13(1), 19. doi:10.3390/fi13010019\nLi, B., Zhou, H., He, J., Wang, M., Yang, Y., & Li, L. (2020). On the sentence embeddings from pre-trained \nlanguage models. In Proceedings​of​the​2020​Conference​on​Empirical​Methods​in​Natural​Language​Processing​\n(EMNLP). doi:10.18653/v1/2020.emnlp-main.733\nMahlous, A. R., & Okkali, B. (2022). A​digital​mental​health​intervention​for​children​and​parents​using​a​\nuser-centered​design. Hindawi. doi:10.1155/2022\nMorales, P. R., & Zolotoochin, G. M. (2022). Measuring the accuracy of social network ideological embeddings \nusing language models. Lecture​Notes​in​Networks​and​Systems, 414, 267–276. doi:10.1007/978-3-030-96293-\n7_24\nMoudjari, L., Benamara, F., & Akli-Astouati, K. (2021). Multi-level embeddings for processing Arabic social \nmedia contents. Computer​Speech​&​Language, 70, 101240. doi:10.1016/j.csl.2021.101240\nOrganizacao Pan-Americana da Saude. (2022, June 17). OMS​destaca​necessidade​urgente​de​transformar​\nsaúde​mental​e​atenção. PAHO. https://www.paho.org/pt/noticias/17-6-2022-oms-destaca-necessidade-urgente-\ntransformar-saude-mental-e-atencao\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised \nmultitask learners. OpenAI​Blog,​1(8), 9.\nRahu, K., Auvinen, A., & Ruch, A. (2020). Can x2vec save lives? Integrating graph and language embeddings \nfor automatic mental health classification. Journal​of​Physics:​Complexity, 1(3), 035005. doi:10.1088/2632-\n072X/aba83d\nRehman, A., Alam, T., Mujahid, M., Alamri, F. S., Al Ghofaily, B., & Saba, T. (2023). RDET​stacking​classifier:​\nA​novel​machine​learning​based​approach​for​stroke​prediction​using​imbalance​data. PeerJ Inc. doi:10.7717/\npeerj-cs.1684\nShatnawi, A., & Shatnawi, R. (2016). Generating a language-independent graphical user interfaces from UML \nmodels. The​International​Arab​Journal​of​Information​Technology, 13(6B), 1039–1044.\nShatnawi, A. M., & AlSobeh, A. M., Al-Mifleh, E. I., & Migdady, A. F. (2022). The effectiveness of a program \nbased on psychosocial support in raising the level of family empowerment among refugees in Jordan. International​\nJournal​of​Psychological​and​Educational​Research, 1(4).\nTarik Altuncu, M., Yaliraki., S. N., & Barahona, M. (2021). Graph-based topic extraction from vector embeddings \nof text documents: Application to a corpus of news articles. In Proceedings​of​the​Ninth​International​Conference​\non​Complex​Networks​and​Their​Applications. Springer. doi:10.1007/978-3-030-65351-4_13\nTurcan, E., & McKeown, K. (2019). Dreaddit: A reddit dataset for stress analysis in social media. In Proceedings​of​\nthe​10th​International​Workshop​on​Health​Text​Mining​and​Information​Analysis. Association for Computational \nLinguistics. doi:10.18653/v1/D19-6213\nUban, A. S., Chulvi, B., & Rosso, P. (2021). An emotion and cognitive based analysis of mental health disorders \nfrom social media data. Future​Generation​Computer​Systems, 124, 480–494. doi:10.1016/j.future.2021.05.032\nUl Haq, A. K., Khattak, A., Jamil, N., Asif Naeem, M., & Mirza, F. (2020). Data​analytics​in​mental​healthcare. \nHindawi. doi:10.1155/2020\nVan Stegeren, J., & Myśliwiec, J. (2021). Fine-tuning GPT-2 on annotated RPG quests for NPC dialogue \ngeneration. In Proceedings​of​the​16th​International​Conference​on​the​Foundations​of​Digital​Games. Association \nfor Computing Machinery. doi:10.1145/3472538.3472595\nWang, Y., Pan, Z., Zheng, J., Qian, L., & Li, M. (2019). A hybrid ensemble method for pulsar candidate \nclassification. Astrophysics​and​Space​Science, 364(8), 139. Advance online publication. doi:10.1007/s10509-\n019-3602-4\nWang, Y., Sun, S., Chen, X., Zeng, X., Kong, Y., Chen, J., Guo, Y., & Wang, T. (2021). Short-term load forecasting \nof industrial customers based on SVMD and XGBoost. International​Journal​of​Electrical​Power​&​Energy​\nSystems, 129, 106830. doi:10.1016/j.ijepes.2021.106830\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n21\nWorld Health Organization. (2022, June 17). Mental​health. https://www.who.int/news-room/fact-sheets/detail/\nmental-health-strengthening-our-response\nYang, C., Wang, X., Li, M., & Li, J. (2023). Research on fusion model of BERT and CNN-BiLSTM for short \ntext classification. In Proceedings​of​the​2023​4th​International​Conference​on​Computer​Engineering​and​\nApplication​(ICCEA). IEEE. doi:10.1109/ICCEA58433.2023.10135222\nZhou, H., Chen, L., Shi, F., & Huang, D. (2015). Learning bilingual sentiment word embeddings for cross-\nlanguage sentiment classification. In Proceedings​of​the​53rd​Annual​Meeting​of​the​Association​for​Computational​\nLinguistics​ and​ the​ 7th​ International​ Joint​ Conference​ on​ Natural​ Language​ Processing. Association for \nComputational Linguistics. doi:10.3115/v1/P15-1042\nInternational Journal of Web Services Research\nVolume 21 • Issue 1\n22\nHuthaifa I. Ashqar received the B.Sc. degree (Hons.) in civil engineering from An-Najah National University, Nablus, \nPalestine, in 2013, the M.Sc. degree in road infrastructure from the University of Minho, Braga, Portugal, in 2015, \nand the Ph.D. degree in civil engineering from Virginia Tech, Virginia, USA, in 2018. He is currently an Assistant \nProfessor with Arab American University, Palestine, and a Consultant with Precision Systems Inc., USA. Previously, \nhe was an Adjunct Professor with Columbia University and University of Maryland Baltimore County. His experience \nincludes being a technical advisor for programs with over $50 million value in advanced transportation and energy \ntechnologies in the U.S. DOE’s ARPA-E. He also received two graduate certificates in data science and economic \ndevelopment from Virginia Tech in 2018 and 2021, respectively.\nAnas AlSobeh is an Assistant Professor of Information Technology at Southern Illinois University Carbondale. He \nreceived a B.Sc. in Computer Information Systems from Yarmouk University, Jordan in 2007 and M.Sc. in Computer \nInformation Systems from Yarmouk University in 2010. He earned a Ph.D. in Computer Science from Utah State \nUniversity in 2015. His research interests include cloud computing, Internet of Things, cybersecurity, healthcare \ninformatics, data analytics, and aspect-oriented software engineering. Dr. AlSobeh has published over 20 articles in \nleading journals and conference proceedings. He has secured external grant funding from organizations including \nthe European Union 2020, Erasmus+, and others. Dr.AlSobeh currently serves as the principal investigator or co-\nprincipal investigator on several funded research projects focused on developing innovative information technology \nsolutions for healthcare, education, and social services. He has mentored numerous undergraduate capstone \nprojects and graduate theses. Dr. AlSobeh is dedicated to conducting high impact interdisciplinary research and \ntraining the next generation of computer scientists.\nAws Magableh is an assistant professor in the Computer Information Systems Department at Yarmouk University, \nHe obtained his PhD from the National University of Malaysia (UKM) in 2015, and he obtained his master in \nSoftware Engineering from University Malaysia (UM) in 2008, and Bachelor’s in Software Engineering from the \nHashemite University in 2006. He is very passionate about Learning &amp; Development (L&amp;D) and I have \nbeen immersed in the training industries with Nokia, Microsoft and Huawei for the past 10 years. Dr. Aws has more \nthan 10 publications in international conferences and refereed journals; these papers focus on software analysis \nand design, applying AOP to enhance reusability and maintainability of different types of software.\nAPPEN dIX\nAcknowledgment and Author Contributions\nOur research is a testament to the collaborative efforts of several esteemed institutions whose \ncontributions have been fundamental to the success of this study. We extend gratitude to the Arab \nAmerican University, Southern Illinois University Carbondale (SIUC), Yarmouk University, and \nPrince Sultan University. Each institution has provided a wealth of resources, academic expertise, \nand a collaborative spirit that has been indispensable in our pursuit of knowledge and the successful \ncompletion of this paper. Their joint commitment to research excellence has not only propelled this \nproject but also reinforced the value of academic cooperation.\nA. R. contributed to the conceptualization and methodology of the research. A. R. and M. A. led \nthe software development, validation, and formal analysis of the research. H. A. and H. I. A. were \nresponsible for the investigation, resource gathering, and data curation. H. I. A., A. A., and A. M. \nprepared the original draft of the manuscript and contributed to the writing, review, and editing. A. \nR. and M. A. were in charge of supervision and project administration. A. A. and A. M. participated \nin the validation process alongside H. A. And H. I. A. contributed significantly to the visualization \nof the research findings. In addition to this, A. A. and A. M. were responsible for the funding proof \nreading. All authors have read and agreed to the published version of the manuscript.",
  "topic": "Machine learning",
  "concepts": [
    {
      "name": "Machine learning",
      "score": 0.7568084001541138
    },
    {
      "name": "Computer science",
      "score": 0.7565770149230957
    },
    {
      "name": "Support vector machine",
      "score": 0.7182279825210571
    },
    {
      "name": "Social media",
      "score": 0.6716596484184265
    },
    {
      "name": "Artificial intelligence",
      "score": 0.664813220500946
    },
    {
      "name": "Transformer",
      "score": 0.5702336430549622
    },
    {
      "name": "Random forest",
      "score": 0.5586830377578735
    },
    {
      "name": "Scalability",
      "score": 0.4686155915260315
    },
    {
      "name": "Natural language processing",
      "score": 0.4647483229637146
    },
    {
      "name": "Emotion detection",
      "score": 0.44564688205718994
    },
    {
      "name": "Data science",
      "score": 0.4371398389339447
    },
    {
      "name": "Sentiment analysis",
      "score": 0.42508751153945923
    },
    {
      "name": "Analytics",
      "score": 0.42071300745010376
    },
    {
      "name": "World Wide Web",
      "score": 0.2392171025276184
    },
    {
      "name": "Emotion recognition",
      "score": 0.11891332268714905
    },
    {
      "name": "Database",
      "score": 0.10088041424751282
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}