{
  "title": "SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
  "url": "https://openalex.org/W4226142803",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1983828941",
      "name": "Liang Wang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2008438337",
      "name": "Wei Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2165683487",
      "name": "Zhuoyu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137724882",
      "name": "Jingming Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3173795766",
    "https://openalex.org/W2965538726",
    "https://openalex.org/W2798385737",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W205829674",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W2971155257",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4311565391",
    "https://openalex.org/W4287614078",
    "https://openalex.org/W3175627818",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W3034239155",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W4287552504",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3198052459",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2997545008",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3103296573",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4309083387",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3145807616",
    "https://openalex.org/W4287591152",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3154735894",
    "https://openalex.org/W3096655658",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at https://github.com/intfloat/SimKGC .",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4281 - 4294\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nSimKGC: Simple Contrastive Knowledge Graph Completion with\nPre-trained Language Models\nLiang Wang1∗and Wei Zhao2 and Zhuoyu Wei2 and Jingming Liu2\n1Microsoft Research Asia\n2Yuanfudao AI Lab, Beijing, China\nwangliang@microsoft.com\n{zhaowei01,weizhuoyu,liujm}@yuanfudao.com\nAbstract\nKnowledge graph completion (KGC) aims to\nreason over known facts and infer the miss-\ning links. Text-based methods such as KG-\nBERT (Yao et al., 2019) learn entity represen-\ntations from natural language descriptions, and\nhave the potential for inductive KGC. How-\never, the performance of text-based methods\nstill largely lag behind graph embedding-based\nmethods like TransE (Bordes et al., 2013)\nand RotatE (Sun et al., 2019b). In this pa-\nper, we identify that the key issue is efﬁcient\ncontrastive learning. To improve the learning\nefﬁciency, we introduce three types of nega-\ntives: in-batch negatives, pre-batch negatives,\nand self-negatives which act as a simple form\nof hard negatives. Combined with InfoNCE\nloss, our proposed model SimKGC can sub-\nstantially outperform embedding-based meth-\nods on several benchmark datasets. In terms\nof mean reciprocal rank (MRR), we advance\nthe state-of-the-art by +19% on WN18RR,\n+6.8% on the Wikidata5M transductive set-\nting, and +22% on the Wikidata5M inductive\nsetting. Thorough analyses are conducted to\ngain insights into each component. Our code\nis available at https://github.com/\nintfloat/SimKGC.\n1 Introduction\nLarge-scale knowledge graphs (KGs) are important\ncomponents for knowledge-intensive applications,\nsuch as question answering (Sun et al., 2019a),\nrecommender systems (Huang et al., 2018), and in-\ntelligent conversational agents (Dinan et al., 2019)\netc. KGs usually consist of a set of triples ( h, r,\nt), where his the head entity, ris the relation, and\nt is the tail entity. Popular public KGs include\nFreebase (Bollacker et al., 2008), Wikidata (Vran-\ndeˇci´c and Krötzsch, 2014), Y AGO (Suchanek et al.,\n2007), ConceptNet (Speer et al., 2017), and Word-\nNet (Miller, 1992) etc. Despite their usefulness\n∗Work done while at Yuanfudao AI Lab.\nin practice, they are often incomplete. Knowledge\ngraph completion (KGC) techniques are necessary\nfor the automatic construction and veriﬁcation of\nknowledge graphs.\ncountry of citizenship\ncontinent\nnamed after\ninstance of\nMount Everest is Earth's highest \nmountain above sea level...\nMount Everest\nAsia is Earth's largest and \nmost populous continent...\nAsia\nA mountain is an elevated \nportion of the Earth's crust…\nMountain\n... was a British surveyor \nand geographer who...\nGeorge Everest\nThe United Kingdom ... is a \nsovereign country in north-\nwestern Europe...\nUnited Kingdom\nFigure 1: An example of knowledge graph. Each entity\nhas its name and textual descriptions.\nExisting KGC methods can be categorized into\ntwo families: embedding-based and text-based\nmethods. Embedding-based methods map each\nentity and relation into a low-dimensional vector,\nwithout using any side information such as entity\ndescriptions. This family includes TransE (Bor-\ndes et al., 2013), TransH (Wang et al., 2014), Ro-\ntatE (Sun et al., 2019b), and TuckER (Balaze-\nvic et al., 2019) etc. By comparison, text-based\nmethods (Yao et al., 2019; Xie et al., 2016; Wang\net al., 2021c) incorporate available texts for entity\nrepresentation learning, as shown in Figure 1. In-\ntuitively, text-based methods should outperform\nembedding-based counterparts since they have ac-\ncess to additional input signals. However, results\non popular benchmarks (e.g., WN18RR, FB15k-\n237, Wikidata5M) tell a different story: text-based\nmethods still lag behind even with pre-trained lan-\nguage models.\nWe hypothesize that the key issue for such per-\nformance degradation is the inefﬁciency in con-\ntrastive learning. Embedding-based methods do\nnot involve the expensive computation of text en-\n4281\ncoders and thus can be extremely efﬁcient to train\nwith a large negative sample size. For example,\nthe default conﬁguration of RotatE 1 trains 1000\nepochs with a negative sample size of 64 on the\nWikidata5M dataset. While the text-based method\nKEPLER (Wang et al., 2021c) can only train 30\nepochs with a negative sample size of 1 due to the\nhigh computational cost incurred by RoBERTa.\nIn this paper, inspired by the recent progress on\ncontrastive learning, we introduce three types of\nnegatives to improve the text-based KGC method:\nin-batch negatives, pre-batch negatives, and self-\nnegatives. By adopting bi-encoder instead of cross-\nencoder (Yao et al., 2019) architecture, the number\nof in-batch negatives can be increased by using a\nlarger batch size. Vectors from previous batches are\ncached and act as pre-batch negatives (Karpukhin\net al., 2020). Additionally, mining hard negatives\ncan be beneﬁcial for improving contrastive learning.\nWe ﬁnd that the head entity itself can serve as hard\nnegatives, which we call “self-negatives”. As a\nresult, the negative sample size can be increased to\nthe scale of thousands. We also propose to change\nthe loss function from margin-based ranking loss\nto InfoNCE, which can make the model focus on\nhard negatives.\nOne advantage of text-based methods is that\nthey enable inductive entity representation learn-\ning. Entities that are not seen during training can\nstill be appropriately modeled, while embedding-\nbased methods like TransE can only reason under\nthe transductive setting 2. Inductive knowledge\ngraph completion is important in the real world as\nnew entities are coming out every day. Moreover,\ntext-based methods can leverage state-of-the-art\npre-trained language models to learn better rep-\nresentations. A line of recent work (Shin et al.,\n2020; Petroni et al., 2019) attempts to elicit the\nimplicitly stored knowledge from BERT. The task\nof KGC can also be regarded as a way to retrieve\nsuch knowledge.\nTwo entities are more likely to be related if con-\nnected by a short path in the graph. Empirically,\nwe ﬁnd that text-based models heavily rely on the\nsemantic match and ignore such topological bias\nto some degree. We propose a simple re-ranking\nstrategy by boosting the scores of the head entity’s\nk-hop neighbors.\nWe evaluate our proposed model SimKGC by\n1https://github.com/DeepGraphLearning/\ngraphvite\n2All entities in the test set also appear in the training set.\nconducting experiments on three popular bench-\nmarks: WN18RR, FB15k-237, and Wikidata5M\n(both transductive and inductive settings). Ac-\ncording to the automatic evaluation metrics (MRR,\nHits@{1,3,10}), SimKGC outperforms state-of-\nthe-art methods by a large margin on the WN18RR\n(MRR 47.6 →66.6), Wikidata5M transductive set-\nting (MRR 29.0 →35.8), and inductive setting\n(MRR 49.3 →71.4). On the FB15k-237 dataset,\nour results are also competitive. To help better\nunderstand our proposed method, we carry out a\nseries of analyses and report human evaluation re-\nsults. Hopefully, SimKGC will facilitate the future\ndevelopment of better KGC systems.\n2 Related Work\nKnowledge Graph Completion involves mod-\neling multi-relational data to aid automatic\nconstruction of large-scale KGs. In translation-\nbased methods such as TransE (Bordes et al.,\n2013) and TransH (Wang et al., 2014), a triple\n(h, r, t) is a relation-speciﬁc translation from the\nhead entity h to tail entity t. Complex number\nembeddings are introduced by Trouillon et al.\n(2016) to increase the model’s expressiveness.\nRotatE (Sun et al., 2019b) models a triple as\nrelational rotation in complex space. Nickel\net al. (2011); Balazevic et al. (2019) treat KGC\nas a 3-D binary tensor factorization problem and\ninvestigate the effectiveness of several factorization\ntechniques. Some methods attempt to incorporate\nentity descriptions. DKRL (Xie et al., 2016) uses\na CNN to encode texts, while KG-BERT (Yao\net al., 2019), StAR (Wang et al., 2021a), and\nBLP (Daza et al., 2021) both adopt pre-trained\nlanguage models to compute entity embeddings.\nGraIL (Teru et al., 2020) and BERTRL (Zha\net al., 2021) conduct inductive relation prediction\nby utilizing subgraph or path information. In terms\nof benchmark performance (Wang et al., 2021c),\ntext-based methods still underperform methods\nlike RotatE.\nPre-trained Language Modelsincluding BERT\n(Devlin et al., 2019), GPT (Radford et al., 2018),\nand T5 (Raffel et al., 2019) have led to a learning\nparadigm shift in NLP. Models are ﬁrst pre-trained\non large amounts of unlabeled text corpora with\nlanguage modeling objectives, and then ﬁne-tuned\non downstream tasks. Considering their good\nperformance in few-shot and even zero-shot\n4282\nscenarios (Brown et al., 2020), one interesting\nquestion is: “Can pre-trained language models\nbe used as knowledge bases?” Petroni et al.\n(2019) proposed to probe language models with\nmanually designed prompts. A series of following\nwork (Shin et al., 2020; Zhong et al., 2021; Jiang\net al., 2020) focus on ﬁnding better prompts\nto elicit the knowledge implicitly stored in the\nmodel parameters. Another line of work (Zhang\net al., 2019; Liu et al., 2020; Wang et al., 2021c)\ninjects symbolic knowledge into language model\npre-training, and shows some performance boost\non several knowledge-intensive tasks.\nContrastive Learning learns useful representa-\ntions by contrasting between positives and nega-\ntives (Le-Khac et al., 2020). The deﬁnitions of\npositives and negatives are task-speciﬁc. In self-\nsupervised vision representation learning (Chen\net al., 2020; He et al., 2020; Grill et al., 2020), a\npositive pair is two augmented views of the same\nimage, while a negative pair is two augmented\nviews of different images. Recently, contrastive\nlearning paradigm has witnessed great successes\nin many different ﬁelds, including multi-modal\npre-training (Radford et al., 2021), video-text re-\ntrieval (Liu et al., 2021), and natural language\nunderstanding (Gunel et al., 2021) etc. In the\nNLP community, by leveraging the supervision sig-\nnals from natural language inference data (Gao\net al., 2021), QA pairs (Ni et al., 2021), and par-\nallel corpora (Wang et al., 2021b), these methods\nhave surpassed non-contrastive methods (Reimers\nand Gurevych, 2019) on semantic similarity bench-\nmarks. Karpukhin et al. (2020); Qu et al. (2021);\nXiong et al. (2021) adopt contrastive learning to\nimprove dense passage retrieval for open-domain\nquestion answering, where the positive passages\nare the ones containing the correct answer.\n3 Methodology\n3.1 Notations\nA knowledge graph Gis a directed graph, where\nthe vertices are entitiesE, and each edge can be rep-\nresented as a triple (h,r,t), where h, r, and tcorre-\nspond to head entity, relation, and tail entity, respec-\ntively. The link prediction task of KGC is to infer\nthe missing triples given an incomplete G. Under\nthe widely adopted entity ranking evaluation proto-\ncol, tail entity prediction (h, r, ?) requires ranking\nall entities given hand r, similarly for head entity\nprediction (?, r, t). In this paper, for each triple\n(h,r,t), we add an inverse triple ( t,r−1,h), where\nr−1 is the inverse relation of r. Based on such\nreformulation, we only need to deal with the tail\nentity prediction problem (Malaviya et al., 2020).\n3.2 Model Architecture\nOur proposed model SimKGC adopts a bi-\nencoder architecture. Two encoders are initialized\nwith the same pre-trained language model but do\nnot share parameters.\nGiven a triple (h,r,t), the ﬁrst encoder BERThr\nis used to compute the relation-aware embedding\nfor the head entity h. We ﬁrst concatenate the tex-\ntual descriptions of entity hand relation rwith a\nspecial symbol [SEP] in between. BERT hr is ap-\nplied to get the last-layer hidden states. Instead of\ndirectly using the hidden state of the ﬁrst token,\nwe use mean pooling followed by L 2 normaliza-\ntion to get the relation-aware embedding ehr, as\nmean pooling has been shown to result in better\nsentence embeddings (Gao et al., 2021; Reimers\nand Gurevych, 2019). ehr is relation-aware since\ndifferent relations will have different inputs and\nthus have different embeddings, even though the\nhead entity is the same.\nSimilarly, the second encoder BERTt is used to\ncompute the L2-normalized embedding et for the\ntail entity t. The input for BERTt only consists of\nthe textual description for entity t.\nSince the embeddings ehr and et are both L 2\nnormalized, the cosine similarity cos(ehr,et) is\nsimply the dot product between two embeddings:\ncos(ehr,et) = ehr ·et\n∥ehr∥∥et∥= ehr ·et (1)\nFor tail entity prediction (h, r, ?), we compute\nthe cosine similarity between ehr and all entities in\nE, and predict the one with the largest score:\nargmax\nti\ncos(ehr,eti ), ti ∈E (2)\n3.3 Negative Sampling\nFor knowledge graph completion, the training\ndata only consists of positive triples. Given a\npositive triple (h, r, t), “negative sampling” needs\nto sample one or more negative triples to train\ndiscriminative models. Most existing methods\nrandomly corrupt h or t and then ﬁlter out false\nnegatives that appear in the training graph G. The\n4283\nnegatives for different triples are not shared and\ntherefore independent. The typical number of\nnegatives are ∼64 for embedding-based methods\n(Sun et al., 2019b), and ∼5 for text-based methods\n(Wang et al., 2021a). We combine three types\nof negatives to improve the training efﬁciency\nwithout incurring signiﬁcant computational and\nmemory overhead.\nIn-batch Negatives (IB) This is a widely\nadopted strategy in visual representation learning\n(Chen et al., 2020) and dense passage retrieval\n(Karpukhin et al., 2020) etc. Entities within\nthe same batch can be used as negatives. Such\nin-batch negatives allow the efﬁcient reuse of\nentity embeddings for bi-encoder models.\nPre-batch Negatives (PB)The disadvantage of\nin-batch negatives is that the number of negatives is\ncoupled with batch size. Pre-batch negatives (Lee\net al., 2021) use entity embeddings from previous\nbatches. Since these embeddings are computed\nwith an earlier version of model parameters, they\nare not consistent with in-batch negatives. Usually,\nonly 1 or 2 pre-batches are used. Other methods\nlike MoCo (He et al., 2020) can also provide more\nnegatives. We leave the investigation of MoCo as\nfuture work.\nSelf-Negatives (SN) Besides increasing the\nnumber of negatives, mining hard negatives (Gao\net al., 2021; Xiong et al., 2021) is also important\nfor improving contrastive representation learning.\nFor tail entity prediction ( h, r, ?), text-based\nmethods tend to assign a high score to the head\nentity h, likely due to the high text overlap. To\nmitigate this issue, we propose self-negatives that\nuse the head entity has hard negatives. Including\nself-negatives can make the model rely less on the\nspurious text match.\nWe use NIB, NPB, and NSN to denote the afore-\nmentioned three types of negatives. During train-\ning, there may exist some false negatives. For ex-\nample, the correct entity happens to appear in an-\nother triple within the same batch. We ﬁlter out\nsuch entities with a binary mask 3. Combining\nthem all, the collection of negatives N(h,r) is:\n{t′|t′∈NIB ∪NPB ∪NSN,(h,r,t′) /∈G} (3)\n3False negatives that do not appear in the training data will\nnot be ﬁltered.\nAssume the batch size is1024, and 2 pre-batches\nare used, we would have |NIB| = 1024 −1,\n|NPB|= 2×1024, |NSN|= 1, and |N(h,r)|=\n3072 negatives in total.\n3.4 Graph-based Re-ranking\nKnowledge graphs often exhibit spatial locality.\nNearby entities are more likely to be related than\nentities that are far apart. Text-based KGC methods\nare good at capturing semantic relatedness but may\nnot fully capture such inductive bias. We propose\na simple graph-based re-ranking strategy: increase\nthe score of candidate tail entity ti by α≥0 if ti\nis in k-hop neighbors Ek(h) of the head entity h\nbased on the graph from training set:\nargmax\nti\ncos(ehr,eti ) +α1 (ti ∈Ek(h)) (4)\n3.5 Training and Inference\nDuring training, we use InfoNCE loss with additive\nmargin (Chen et al., 2020; Yang et al., 2019):\nL= −log e(φ(h,r,t)−γ)/τ\ne(φ(h,r,t)−γ)/τ + ∑|N|\ni=1 eφ(h,r,t′\ni)/τ\n(5)\nThe additive marginγ >0 encourages the model\nto increase the score of the correct triple ( h,r,t).\nφ(h,r,t) is the score function for a candidate triple,\nhere we deﬁne φ(h,r,t) = cos(ehr,et) ∈[−1,1]\nas in Equation 1. The temperature τ can adjust the\nrelative importance of negatives, smaller τ makes\nthe loss put more emphasis on hard negatives, but\nalso risks over-ﬁtting label noise. To avoid tuning\nτ as a hyperparameter, we re-parameterize log 1\nτ as\na learnable parameter.\nFor inference, the most time-consuming part is\nO(|E|) BERT forward pass computation of entity\nembeddings. Assume there are |T| test triples. For\neach triple (h, r, ?) and (t, r−1, ?), we need to com-\npute the relation-aware head entity embedding and\nuse a dot product to get the ranking score for all en-\ntities. In total, SimKGC needs |E|+ 2×|T| BERT\nforward passes, while cross-encoder models like\nKG-BERT (Yao et al., 2019) needs |E|× 2 ×|T|.\nBeing able to scale to large datasets is important for\npractical usage. For bi-encoder models, we can pre-\ncompute the entity embeddings and retrieve top-k\nentities efﬁciently with the help of fast similarity\nsearch tools like Faiss (Johnson et al., 2021).\n4284\ndataset #entity #relation #train #valid #test\nWN18RR 40,943 11 86 ,835 3034 3134\nFB15k-237 14,541 237 272 ,115 17 ,535 20 ,466\nWikidata5M-Trans 4,594,485 822 20 ,614,279 5 ,163 5 ,163\nWikidata5M-Ind 4,579,609 822 20 ,496,514 6 ,699 6 ,894\nTable 1: Statistics of the datasets used in this paper. “Wikidata5M-Trans” and “Wikidata5M-Ind” refer to the\ntransductive and inductive settings, respectively.\n4 Experiments\n4.1 Experimental Setup\nDatasets We use three datasets for evaluation:\nWN18RR, FB15k-237, and Wikidata5M (Wang\net al., 2021c). The statistics are shown in Table\n1. Bordes et al. (2013) proposed the WN18 and\nFB15k datasets. Later work (Toutanova et al.,\n2015; Dettmers et al., 2018) showed that these two\ndatasets suffer from test set leakage and released\nWN18RR and FB15k-237 datasets by removing the\ninverse relations. The WN18RR dataset consists\nof ∼41ksynsets and 11 relations from WordNet\n(Miller, 1992), and the FB15k-237 dataset consists\nof ∼15kentities and 237 relations from Freebase.\nThe Wikidata5M dataset is much larger in scale\nwith ∼5 million entities and ∼20 million triples.\nIt provides two settings: transductive and inductive.\nFor the transductive setting, all entities in the test\nset also appear in the training set, while for the in-\nductive setting, there is no entity overlap between\ntrain and test set. We use “Wikidata5M-Trans” and\n“Wikidata5M-Ind” to indicate these two settings.\nFor textual descriptions, we use the data\nprovided by KG-BERT (Yao et al., 2019) for\nWN18RR and FB15k-237 datasets. The Wiki-\ndata5M dataset already contains descriptions for\nall entities and relations.\nEvaluation MetricsFollowing previous work, our\nproposed KGC model is evaluated with entity rank-\ning task: for each test triple (h,r,t), tail entity\nprediction ranks all entities to predict t given h\nand r, similarly for head entity prediction. We use\nfour automatic evaluation metrics: mean recipro-\ncal rank (MRR), and Hits@k(k∈{1,3,10}) (H@k\nfor short). MRR is the average reciprocal rank of\nall test triples. H@ kcalculates the proportion of\ncorrect entities ranked among the top-k. MRR and\nH@kare reported under the ﬁltered setting (Bor-\ndes et al., 2013), The ﬁltered setting ignores the\nscores of all known true triples in the training, val-\nidation, and test set. All metrics are computed by\naveraging over two directions: head entity predic-\ntion and tail entity prediction.\nWe also conduct a human evaluation on the\nWikidata5M dataset to provide a more accurate\nestimate of the model’s performance.\nHyperparameters The encoders are initialized\nwith bert-base-uncased (English). Using better\npre-trained language models is expected to improve\nperformance further. Most hyperparameters except\nlearning rate and training epochs are shared across\nall datasets to avoid dataset-speciﬁc tuning. We\nconduct grid search on learning rate with ranges\n{10−5, 3×10−5, 5×10−5}. Entity descriptions are\ntruncated to a maximum of 50 tokens. Temperature\nτ is initialized to 0.05, and the additive margin\nfor InfoNCE loss is 0.02. For re-ranking, we set\nα= 0.05. 2 pre-batches are used with logit weight\n0.5. We use AdamW optimizer with linear learning\nrate decay. Models are trained with batch size1024\non 4 V100 GPUs. For the WN18RR, FB15k-237,\nand Wikidata5M (both settings) datasets, we train\nfor 50, 10, and 1 epochs, respectively. Please see\nAppendix A for more details.\n4.2 Main Results\nWe reuse the numbers reported by Wang et al.\n(2021c) for TransE and DKRL, and the results\nfor RotatE are from the ofﬁcial GraphVite 4\nbenchmark. In Table 2 and 3, our proposed\nmodel SimKGCIB+PB+SN outperforms state-of-the-\nart methods by a large margin on the WN18RR,\nWikidata5M-Trans, and Wikidata5M-Ind datasets,\nbut slightly lags behind on the FB15k-237 dataset\n(MRR 33.6% vs 35.8%). To the best of our knowl-\nedge, SimKGC is the ﬁrst text-based KGC method\nthat achieves better results than embedding-based\ncounterparts.\n4https://graphvite.io/docs/latest/\nbenchmark\n4285\nMethod Wikidata5M-Trans Wikidata5M-Ind\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nembedding-based methods\nTransE (Bordes et al., 2013) 25.3 17.0 31.1 39.2 - - - -\nRotatE (Sun et al., 2019b) 29.0 23.4 32.2 39.0 - - - -\ntext-based methods\nDKRL (Xie et al., 2016) 16.0 12.0 18.1 22.9 23.1 5.9 32.0 54.6\nKEPLER (Wang et al., 2021c) 21.0 17.3 22.4 27.7 40.2 22.2 51.4 73.0\nBLP-ComplEx (Daza et al., 2021) - - - - 48.9 26.2 66.4 87.7\nBLP-SimplE (Daza et al., 2021) - - - - 49.3 28.9 63.9 86.6\nSimKGCIB 35.3 30.1 37.4 44.8 60.3 39.5 77.8 92.3\nSimKGCIB+PB 35.4 30.2 37.3 44.8 60.2 39.4 77.7 92.4\nSimKGCIB+SN 35.6 31.0 37.3 43.9 71.3 60.7 78.7 91.3\nSimKGCIB+PB+SN 35.8 31.3 37.6 44.1 71.4 60.9 78.5 91.7\nTable 2: Main results for the Wikidata5M dataset. “IB”, “PB”, and “SN” refer to in-batch negatives, pre-batch\nnegatives, and self-negatives respectively. Embedding-based methods are inherently unable to perform inductive\nKGC. According to the evaluation protocol by Wang et al. (2021c), the inductive setting only ranks 7,475 entities\nin the test set, while the transductive setting ranks ∼4.6 million entities, so the reported metrics for the inductive\nsetting are much higher. Results are statistically signiﬁcant under paired student’s t-test with p-value0.05.\nMethod WN18RR FB15k-237\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nembedding-based methods\nTransE (Bordes et al., 2013)† 24.3 4.3 44.1 53.2 27.9 19.8 37.6 44.1\nDistMult (Yang et al., 2015)† 44.4 41.2 47.0 50.4 28.1 19.9 30.1 44.6\nRotatE (Sun et al., 2019b)† 47.6 42.8 49.2 57.1 33.8 24.1 37.5 53.3\nTuckER (Balazevic et al., 2019)† 47.0 44.3 48.2 52.6 35.8 26.6 39.4 54.4\ntext-based methods\nKG-BERT (Yao et al., 2019) 21.6 4.1 30.2 52.4 - - - 42.0\nMTL-KGC (Kim et al., 2020) 33.1 20.3 38.3 59.7 26.7 17.2 29.8 45.8\nStAR (Wang et al., 2021a) 40.1 24.3 49.1 70.9 29.6 20.5 32.2 48.2\nSimKGCIB 67.1 58.5 73.1 81.7 33.3 24.6 36.2 51.0\nSimKGCIB+PB 66.6 57.8 72.3 81.7 33.4 24.6 36.5 51.1\nSimKGCIB+SN 66.7 58.8 72.1 80.5 33.4 24.7 36.3 50.9\nSimKGCIB+PB+SN 66.6 58.7 71.7 80.0 33.6 24.9 36.2 51.1\nTable 3: Main results for WN18RR and FB15k-237 datasets. †: numbers are from Wang et al. (2021a).\nWe report results for various combinations of\nnegatives. With in-batch negatives only, the perfor-\nmance of SimKGCIB is already quite strong thanks\nto the large batch size (1024) we use. Adding self-\nnegatives tends to improve H@1 but hurt H@10.\nWe hypothesize that self-negatives make the model\nrely less on simple text match. Thus they have neg-\native impacts on metrics that emphasize recall, such\nas H@10. Combining all three types of negatives\ngenerally has the best results but not always.\nCompared to other datasets, the graph for the\nFB15k-237 dataset is much denser (average de-\ngree is ∼37 per entity), and contains fewer en-\ntities (∼15k). To perform well, models need to\nlearn generalizable inference rules instead of just\nmodeling textual relatedness. Embedding-based\nmethods are likely to hold an advantage for this\nscenario. It is possible to ensemble our method\nwith embedding-based ones, as done by Wang et al.\n(2021a). Since this is not the main focus of this\npaper, we leave it as future work. Also, Cao et al.\n(2021) points out that many links in the FB15k-237\ndataset are not predictable based on the available\ninformation. These two reasons help explain the\nunsatisfactory performance of SimKGC.\nAdding self-negatives is particularly helpful for\nthe inductive setting of Wikidata5M dataset, with\nMRR rising from 60.3% to 71.3%. For inductive\nKGC, text-based models rely more heavily on text\nmatch than the transductive setting. Self negatives\n4286\ncan prevent the model from simply predicting the\ngiven head entity.\nIn terms of inference time, the most expen-\nsive part is the forward pass with BERT. For the\nWikidata5M-Trans dataset, SimKGC requires∼40\nminutes to compute ∼ 4.6 million embeddings\nwith 2 GPUs, while cross-encoder models such as\nKG-BERT (Yao et al., 2019) would require an es-\ntimated time of 3000 hours. We are not the ﬁrst\nwork that enables fast inference, models such as\nConvE (Dettmers et al., 2018) and StAR (Wang\net al., 2021a) also share similar advantages. Here\nwe just want to re-emphasize the importance of\ninference efﬁciency and scalability when designing\nnew models.\n5 Analysis\nWe conduct a series of analyses to gain further\ninsights into our proposed model and the KGC\ntask.\n5.1 What Makes SimKGC Excel?\nCompared to existing text-based methods,\nSimKGC makes two major changes: using more\nnegatives, and switching from margin-based\nranking loss to InfoNCE loss. To guide the future\nwork on knowledge graph completion, it is crucial\nto understand which factor contributes most to the\nsuperior performance of SimKGC.\nloss # of neg MRR H@1 H@3 H@10\nInfoNCE 255 64.4 53.8 71.7 82.8\nInfoNCE 5 48.8 31.9 60.2 80.3\nmargin 255 39.5 28.5 44.4 61.2\nmargin 5 38.0 27.5 42.8 58.7\nmargin-τ 255 57.8 48.5 63.7 74.9\nTable 4: Analysis of loss function and the number of\nnegatives on the WN18RR dataset.\nIn Table 4, we use SimKGC IB with batch size\n256 as a baseline. By reducing the number of nega-\ntives from 255 to 5, MRR drops from 64.4 to 48.8.\nChanging the loss function from InfoNCE to the\nfollowing margin loss makes MRR drop to 39.5:\n1\n|N|\n|N|∑\ni=1\nmax(0,λ + φ(h,r,t′\ni) −φ(h,r,t)) (6)\nConsistent with Equation 5, φ(h,r,t′\ni) is cosine\nsimilarity score for a candidate triple, and λ= 0.8.\nTo summarize, both InfoNCE loss and a large\nnumber of negatives are important factors, while\nthe loss function seems to have bigger impacts. For\nInfoNCE loss, the hard negatives naturally con-\ntribute larger gradients, and adding more negatives\ncan lead to more robust representations. Wang and\nLiu (2021) also draws a similar conclusion: such\nhardness-aware property is vital for the success of\ncontrastive loss.\nWe also propose a variant “margin- τ” loss by\nchanging the weight in Equation 6 from 1\n|N|\nto exp(s(t′\ni)/τ)\n∑|N|\nj=1 exp(s(t′\nj )/τ)\n, where s(t′\ni) = max(0,λ +\nφ(h,r,t′\ni) −φ(h,r,t)) and τ = 0.05. Similar to\nInfoNCE loss, “margin-τ” loss makes the model\npay more attention to hard negatives and leads to\nbetter performance as shown in Table 4. It is\nsimilar to the “self-adversarial negative sampling”\nproposed by Sun et al. (2019b). Most hyperparam-\neters are tuned based on InfoNCE loss. We expect\nthe margin-τ loss to achieve better results with a\nbit more hyperparameter optimization.\n5 16 32 64 128 256 512 1024\n# of negatives (log-scale)\n45\n50\n55\n60\n65\n70MRR\n48.8\n53.9\n57.1\n59.9\n61.8\n64.4\n66.0\n67.1\nMRR\nFigure 2: MRR on the WN18RR dataset w.r.t the num-\nber of negatives with SimKGCIB. We use a batch size\nof 1024 for all experiments, and change the number of\nnegatives with a binary mask over the softmax logits.\nIn Figure 2, we quantitatively illustrate how\nMRR changes as more negatives are added. There\nis a clear trend that the performance steadily im-\nproves from 48.8 to 67.1. However, adding more\nnegatives requires more GPU memory and may\ncause optimization difﬁculties (You et al., 2020;\nChen et al., 2020). We do not experiment with\nbatch size larger than 1024.\n5.2 Ablation on Re-ranking\nOur proposed re-ranking strategy is a simple way to\nincorporate topological information in the knowl-\nedge graph. For graphs whose connectivity patterns\nexhibit spatial locality, re-ranking is likely to help.\n4287\ntriple (Rest Plaus Historic District, is located in,New York)\nevidence . . . a national historic district located at Marbletown in Ulster County, New York. . .\nSimKGC Marbletown\ntriple (Timothy P. Green, place of birth, St. Louis)\nevidence William Douglas Guthrie (born January 17, 1967 in St. Louis, MO) is a professional boxer. . .\nSimKGC William Douglas Guthrie\ntriple (TLS termination proxy, instance of,networked software)\nevidence . . . a proxy server that is used by an institution to handle incoming TLS connections. . .\nSimKGC http server\ntriple (1997 IBF World Championships, followed by,1999 IBF World Championships)\nevidence The 10th IBF World Championships (Badminton) were held in Glasgow, Scotland,\nbetween 24 May and 1 June 1997. . .\nSimKGC 2000 IBF World Junior Championships\nTable 5: Examples of SimKGC prediction results on the test set of the Wikidata5M-Trans dataset. The entity to\npredict is in bold font. We only show a snippet of relevant texts in the row of “evidence” for space reason.\nMRR H@1 H@3 H@10\nw/ re-rank 35.8 31.3 37.6 44.1\nw/o re-rank 35.5 31.0 37.3 43.9\nTable 6: Ablation of re-ranking on the Wikidata5M-\nTrans dataset.\nIn Table 6, we see a slight but stable increase for\nall metrics on the Wikidata5M-Trans dataset. Note\nthat this re-ranking strategy does not apply to induc-\ntive KGC since entities in the test set never appear\nin the training data. Exploring more effective ways\nsuch as graph neural networks (Wu et al., 2019)\ninstead of simple re-ranking would be a future di-\nrection.\n5.3 Fine-grained Analysis\n1-1 1-n\nspouse\ncapital of\nlake inﬂows\nhead of government\nchild\nhas part\nnotable work\nside effect\nn-1 n-n\ninstance of\nplace of birth\ngiven name\nwork location\ncast member\nmember of\ninﬂuenced by\nnominated for\nTable 7: Examples for different categories of relations\non the Wikidata5M-Trans dataset.\nWe classify all relations into four categories\nbased on the cardinality of head and tail arguments\nfollowing the rules by Bordes et al. (2013): one-\nto-one(1-1), one-to-many(1-n), many-to-one(n-1),\nand many-to-many(n-n). Examples are shown in\nDataset 1-1 1-n n-1 n-n\nWikidata5M-Trans 30.4 8.3 71.1 10.6\nWikidata5M-Ind 83.5 71.1 80.0 54.7\nTable 8: MRR for different kinds of relations on the\nWikidata5M dataset with SimKGCIB+PB+SN.\nTable 7. As shown in Table 8, predicting the\n“n” side is generally more difﬁcult, since there are\nmany seemingly plausible answers that would con-\nfuse the model. Another main reason is the incom-\npleteness of the knowledge graph. Some predicted\ntriples might be correct based on human evaluation,\nespecially for 1-n relations in head entity predic-\ntion, such as “instance of”, “place of birth” etc.\nIn Table 5, for the ﬁrst example, “Marbletown”,\n“Ulster County”, and “New York” are both cor-\nrect answers. The second example illustrates the\ncase for relation “place of birth”: a lot of people\nshare the same place of birth, and some triples may\nnot exist in the knowledge graph. This helps ex-\nplain the low performance of “1-n” relations for the\nWikidata5M-Trans dataset. In the third example,\nSimKGC predicts a closely related but incorrect\nentity “http server”.\n5.4 Human Evaluation\nThe analyses above suggest that automatic evalu-\nation metrics such as MRR tend to underestimate\nthe model’s performance. To have a more accurate\nestimation of the performance, we conduct human\nevaluation and list the results in Table 9. An aver-\nage of 49% of the wrong predictions according to\nH@1 are correct according to human annotators. If\nwe take this into account, the H@1 of our proposed\nmodel would be much higher. How to accurately\n4288\ncorrect wrong unknown\n(h, r, ?) 24% 54% 22%\n(?, r, t) 74% 14% 12%\nAvg 49% 34% 17%\nTable 9: Human evaluation results on the Wikidata5M-\nTrans dataset. ( h, r, ?) and ( ?, r, t) denote tail entity\nand head entity prediction respectively. We randomly\nsample 100 wrong predictions according to H@1 from\ntest set. The “unknown” category indicates annotators\nare unable to decide whether the prediction is correct\nor wrong based on the textual information.\nmeasure the performance of KGC systems is also\nan interesting future research direction.\n5.5 Entity Visualization\n20\n 10\n 0 10 20\nx\n30\n20\n10\n0\n10\n20\n30\n40\ny\nEntity embedding visualization\nentity type\nHuman\nBook\nVillage\nT axon\nAlbum\nMovie\nCommunity\nCompany\nFigure 3: 2-D visualization of the entity embed-\ndings from the Wikidata5M-Trans dataset with t-SNE\n(Maaten and Hinton, 2008).\nTo examine our proposed model qualitatively,\nwe visualize the entity embeddings from 8 largest\ncategories 5 with 50 randomly selected entities per\ncategory. Entity embeddings are computed with\nBERTt in Section 3.2. In Figure 3, different cate-\ngories are well separated, demonstrating the high\nquality of the learned embeddings. One interesting\nphenomenon is that the two categories “Commu-\nnity” and “Village” have some overlap. This is\nreasonable since these two concepts are not mutu-\nally exclusive.\n6 Conclusion\nThis paper proposes a simple method SimKGC to\nimprove text-based knowledge graph completion.\nWe identify that the key issue is how to perform\n5We utilize the “instance of” relation to determine the entity\ncategory.\nefﬁcient contrastive learning. Leveraging the re-\ncent progress in the ﬁeld of contrastive learning,\nSimKGC adopts a bi-encoder architecture and com-\nbines three types of negatives. Experiments on the\nWN18RR, FB15k-237, and Wikidata5M datasets\nshow that SimKGC substantially outperforms state-\nof-the-art methods.\nFor future work, one direction is to improve the\ninterpretability of SimKGC. In methods like Ro-\ntatE (Sun et al., 2019b) and TransE (Bordes et al.,\n2013), a triple can be modeled as rotation in com-\nplex space or relational translation, while SimKGC\ndoes not enable such easy-to-understand interpre-\ntations. Another direction is to explore effective\nways to deal with false negatives (Huynh et al.,\n2020) resulting from the incompleteness of knowl-\nedge graphs.\n7 Broader Impacts\nFuture work could use SimKGC as a solid base-\nline to keep improving text-based knowledge graph\ncompletion systems. Our experimental results and\nanalyses also reveal several promising research di-\nrections. For example, how to incorporate global\ngraph structure in a more principled way? Are there\nother loss functions that perform better than the In-\nfoNCE loss? For knowledge-intensive tasks such\nas knowledge base question answering (KBQA),\ninformation retrieval, and knowledge-grounded re-\nsponse generation, etc., it would be interesting to\nexplore the new opportunities brought by the im-\nproved knowledge graph completion systems.\nAcknowledgements\nWe would like to thank anonymous reviewers and\narea chairs for their valuable comments, and ACL\nRolling Review organizers for their efforts.\nReferences\nIvana Balazevic, Carl Allen, and Timothy Hospedales.\n2019. TuckER: Tensor factorization for knowledge\ngraph completion. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5185–5194, Hong Kong, China. As-\nsociation for Computational Linguistics.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\n4289\nSIGMOD international conference on Management\nof data, pages 1247–1250.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , pages 2787–\n2795.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nYixin Cao, Xiang Ji, Xin Lv, Juanzi Li, Yonggang Wen,\nand Hanwang Zhang. 2021. Are missing links pre-\ndictable? an inferential benchmark for knowledge\ngraph completion. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 6855–6865, Online. Associa-\ntion for Computational Linguistics.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey E. Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nProceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 1597–1607. PMLR.\nDaniel Daza, Michael Cochez, and Paul Groth. 2021.\nInductive entity representations from text via link\nprediction. In Proceedings of the Web Conference\n2021, pages 798–808.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d\nknowledge graph embeddings. In Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, (AAAI-18), the 30th innovative Ap-\nplications of Artiﬁcial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 1811–\n1818. AAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. ArXiv preprint, abs/2104.08821.\nJean-Bastien Grill, Florian Strub, Florent Altché,\nCorentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Ávila Pires,\nZhaohan Guo, Mohammad Gheshlaghi Azar, Bilal\nPiot, Koray Kavukcuoglu, Rémi Munos, and Michal\nValko. 2020. Bootstrap your own latent - A new\napproach to self-supervised learning. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Veselin\nStoyanov. 2021. Supervised contrastive learning for\npre-trained language model ﬁne-tuning. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 9726–9735. IEEE.\nJin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong\nWen, and Edward Y . Chang. 2018. Improving se-\nquential recommendation with knowledge-enhanced\nmemory networks. In The 41st International ACM\nSIGIR Conference on Research & Development in\nInformation Retrieval, SIGIR 2018, Ann Arbor, MI,\nUSA, July 08-12, 2018, pages 505–514. ACM.\nTri Huynh, Simon Kornblith, Matthew R. Wal-\nter, Michael Maire, and Maryam Khademi. 2020.\nBoosting contrastive self-supervised learning with\nfalse negative cancellation. ArXiv preprint ,\nabs/2011.11765.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\n4290\nJeff Johnson, M. Douze, and H. Jégou. 2021. Billion-\nscale similarity search with gpus. IEEE Transac-\ntions on Big Data, 7:535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nBosung Kim, Taesuk Hong, Youngjoong Ko, and\nJungyun Seo. 2020. Multi-task learning for knowl-\nedge graph completion with pre-trained language\nmodels. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n1737–1743, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nPhuc H Le-Khac, Graham Healy, and Alan F Smeaton.\n2020. Contrastive representation learning: A frame-\nwork and review. IEEE Access.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021. Learning dense representations of\nphrases at scale. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6634–6647, Online. Association for\nComputational Linguistics.\nSong Liu, Haoqi Fan, Shengsheng Qian, Yiru\nChen, W. Ding, and Zhongyuan Wang. 2021.\nHit: Hierarchical transformer with momentum\ncontrast for video-text retrieval. ArXiv preprint ,\nabs/2103.15049.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nQi Ju, Haotang Deng, and Ping Wang. 2020. K-\nBERT: enabling language representation with knowl-\nedge graph. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 2901–2908. AAAI Press.\nL. V . D. Maaten and Geoffrey E. Hinton. 2008. Visual-\nizing data using t-sne. Journal of Machine Learning\nResearch, 9:2579–2605.\nChaitanya Malaviya, Chandra Bhagavatula, Antoine\nBosselut, and Yejin Choi. 2020. Commonsense\nknowledge base completion with structural and se-\nmantic context. In AAAI.\nGeorge A. Miller. 1992. WordNet: A lexical database\nfor English. In Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New\nYork, February 23-26, 1992.\nJianmo Ni, Noah Constant, Ji Ma, Keith B Hall, Daniel\nCer, Yinfei Yang, et al. 2021. Sentence-t5: Scalable\nsentence encoders from pre-trained text-to-text mod-\nels. ArXiv preprint, abs/2108.08877.\nMaximilian Nickel, V olker Tresp, and Hans-Peter\nKriegel. 2011. A three-way model for collective\nlearning on multi-relational data. In Proceedings\nof the 28th International Conference on Machine\nLearning, ICML 2011, Bellevue, Washington, USA,\nJune 28 - July 2, 2011, pages 809–816. Omnipress.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5835–5847, Online. Association for Computational\nLinguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research , pages\n8748–8763. PMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam M. Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, W. Li, and Peter J. Liu. 2019. Ex-\nploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. ArXiv preprint ,\nabs/1910.10683.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\n4291\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, Febru-\nary 4-9, 2017, San Francisco, California, USA ,\npages 4444–4451. AAAI Press.\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2007. Yago: a core of semantic knowledge.\nIn Proceedings of the 16th International Conference\non World Wide Web, WWW 2007, Banff, Alberta,\nCanada, May 8-12, 2007, pages 697–706. ACM.\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.\n2019a. PullNet: Open domain question answering\nwith iterative retrieval on knowledge bases and text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2380–\n2390, Hong Kong, China. Association for Computa-\ntional Linguistics.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019b. Rotate: Knowledge graph embedding\nby relational rotation in complex space. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nKomal Teru, Etienne Denis, and Will Hamilton. 2020.\nInductive relation prediction by subgraph reasoning.\nIn Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 9448–9457. PMLR.\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\nfung Poon, Pallavi Choudhury, and Michael Gamon.\n2015. Representing text for joint embedding of text\nand knowledge bases. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1499–1509, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Com-\nplex embeddings for simple link prediction. In\nProceedings of the 33nd International Conference\non Machine Learning, ICML 2016, New York City,\nNY, USA, June 19-24, 2016 , volume 48 of JMLR\nWorkshop and Conference Proceedings, pages 2071–\n2080. JMLR.org.\nDenny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou,\nYing Wang, and Yi Chang. 2021a. Structure-\naugmented text representation learning for efﬁcient\nknowledge graph completion. In Proceedings of the\nWeb Conference 2021, pages 1737–1748.\nFeng Wang and Huaping Liu. 2021. Understanding the\nbehaviour of contrastive loss. 2021 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 2495–2504.\nLiang Wang, Wei Zhao, and Jingming Liu. 2021b.\nAligning cross-lingual sentence representations with\ndual momentum contrast. In EMNLP.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021c. Kepler: A uniﬁed model for knowledge\nembedding and pre-trained language representation.\nTransactions of the Association for Computational\nLinguistics, 9:176–194.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In Proceedings of the Twenty-\nEighth AAAI Conference on Artiﬁcial Intelligence,\nJuly 27 -31, 2014, Québec City, Québec, Canada ,\npages 1112–1119. AAAI Press.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, C. Zhang, and Philip S. Yu. 2019. A com-\nprehensive survey on graph neural networks. IEEE\nTransactions on Neural Networks and Learning Sys-\ntems, 32:4–24.\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and\nMaosong Sun. 2016. Representation learning of\nknowledge graphs with entity descriptions. In Pro-\nceedings of the Thirtieth AAAI Conference on Arti-\nﬁcial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA, pages 2659–2665. AAAI Press.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings.\nYinfei Yang, Gustavo Hernández Ábrego, Steve Yuan,\nMandy Guo, Qinlan Shen, Daniel Cer, Yun-Hsuan\nSung, Brian Strope, and Ray Kurzweil. 2019. Im-\nproving multilingual sentence embedding using bi-\ndirectional dual encoder with additive margin soft-\nmax. In Proceedings of the Twenty-Eighth Interna-\ntional Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2019, Macao, China, August 10-16, 2019, pages\n5370–5378. ijcai.org.\n4292\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-\nbert: Bert for knowledge graph completion. ArXiv\npreprint, abs/1909.03193.\nYang You, Jing Li, Sashank J. Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learn-\ning: Training BERT in 76 minutes. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nHanwen Zha, Zhiyu Chen, and Xifeng Yan. 2021. In-\nductive relation prediction by bert. ArXiv preprint,\nabs/2103.07102.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033, Online. Association for\nComputational Linguistics.\nA Details on Hyperparameters\nHyperparameter value\n# of GPUs 4\ninitial temperatureτ 0.05\ngradient clip 10\nwarmup steps 400\nbatch size 1024\nmax # of tokens 50\nweightαfor re-ranking 0.05\ndropout 0.1\nweight decay 10−4\nInfoNCE margin 0.02\npooling mean\nTable 10: Shared hyperparameters for our proposed\nSimKGC model.\nIn Table 10, we show the hyperparameters that\nare shared across all the datasets. For learning\nrate, we use 5 ×10−5, 10−5, and 3 ×10−5 for\nWN18RR, FB15k-237, and Wikidata5M datasets,\nrespectively. For re-ranking, we use 5-hop neigh-\nbors for WN18RR and 2-hop neighbors for other\ndatasets. Each epoch takes ∼ 3 minutes for\nWN18RR, ∼12 minutes for FB15k-237, and ∼12\nhours for Wikidata5M (both settings). Our imple-\nmentation is based on open-source project trans-\nformers 6.\nFor inverse relation r−1, we add a preﬁx word\n“inverse” to the description ofr. For examples, if r\n= “instance of”, then r−1 = “inverse instance of”.\nSome entities in the WN18RR and FB15k-237\ndataset have very short textual descriptions. We\nconcatenate them with the entity names of its neigh-\nbors in the training set. To avoid label leakage dur-\ning training, we dynamically exclude the correct\nentity in the input text.\nB More Analysis Results\nbatch size MRR H@1 H@3 H@10\n256 33.8 28.7 35.8 43.1\n512 34.6 29.4 36.7 43.7\n1024 35.3 30.1 37.4 44.8\nTable 11: Effects of batch size on the Wikidata5M-\nTrans dataset with SimKGCIB.\nbatch size MRR H@1 H@3 H@10\n256 32.4 23.3 35.4 50.9\n512 32.7 23.7 35.6 51.0\n1024 33.3 24.6 36.2 51.0\nTable 12: Effects of batch size on the FB15k-237\ndataset with SimKGCIB.\nmarginγ MRR H@1 H@3 H@10\n0 33.4 24.8 36.0 50.9\n0.02 33.6 24.9 36.2 51.1\n0.05 33.6 25.0 36.2 50.9\nTable 13: Ablation for the additive margin γ of In-\nfoNCE loss on the FB15k-237 dataset.\nIn Table 11 and 12, we show how the batch\nsize affects model performance on the Wikidata5M-\nTrans and FB15k-237 dataset.\nIn Equation 5, we use a variant of InfoNCE loss\nthat has an additive margin γ. In our experiments,\nsuch a variant performs consistently better than the\nstandard InfoNCE loss, though the improvement is\nquite marginal, as shown in Table 13.\nIn Table 14, we show more examples of\nSimKGC predictions on the Wikidata5M-Trans\n6https://github.com/huggingface/\ntransformers\n4293\ntriple (captive state (ﬁlm), instance of,movie)\nevidence Captive State is a 2019 American crime science ﬁction thriller ﬁlm directed by Rupert Wyatt\nand co-written by Wyatt and Erica Beeney.. . .\nSimKGC 3-D movies\ntriple (Lionel Belasco, occupation,composer)\nevidence Lionel Belasco (1881 – c. 24 June 1967) was a prominent pianist, composer and bandleader,\nbest known for his calypso recordings.\nSimKGC bandleaders\ntriple (Johan Nordhagen, country of citizenship, Norway)\nevidence Waqas Ahmed (born 9 June 1991) is a Norwegian cricketer. . . .\nSimKGC Waqas Ahmed\ntriple (Carlos Peña Romulo, position held, philippine resident commissioner)\nevidence Francis Burton Harrison was an American-born Filipino statesman who served in the United States\nHouse of Representatives and was appointed Governor-General of the Philippines . . .\nSimKGC Francis Burton Harrison\nTable 14: More examples of SimKGC prediction results on the test set of Wikidata5M-Trans.\ndataset to help better understand our model’s be-\nhavior. Full model predictions on test datasets are\navailable in our public code repository.\n4294",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7743602991104126
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5509011149406433
    },
    {
      "name": "Mean reciprocal rank",
      "score": 0.5349822044372559
    },
    {
      "name": "Embedding",
      "score": 0.5342309474945068
    },
    {
      "name": "Graph",
      "score": 0.5322418212890625
    },
    {
      "name": "Natural language processing",
      "score": 0.5196762681007385
    },
    {
      "name": "Machine learning",
      "score": 0.34831058979034424
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3154611885547638
    }
  ]
}