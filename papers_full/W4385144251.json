{
  "title": "Large Language Models for Epidemiological Research via Automated Machine Learning: Case Study Using Data From the British National Child Development Study",
  "url": "https://openalex.org/W4385144251",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2632200507",
      "name": "RASMUS WIBAEK",
      "affiliations": [
        "Steno Diabetes Center"
      ]
    },
    {
      "id": "https://openalex.org/A2796117530",
      "name": "Gregers Stig Andersen",
      "affiliations": [
        "Steno Diabetes Center"
      ]
    },
    {
      "id": "https://openalex.org/A2046349481",
      "name": "Christina C. Dahm",
      "affiliations": [
        "Aarhus University"
      ]
    },
    {
      "id": "https://openalex.org/A2110865553",
      "name": "Daniel R. Witte",
      "affiliations": [
        "Aarhus University",
        "Steno Diabetes Center",
        "Aarhus University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2677273714",
      "name": "Adam Hulman",
      "affiliations": [
        "Aarhus University",
        "Aarhus University Hospital",
        "Steno Diabetes Center"
      ]
    },
    {
      "id": "https://openalex.org/A2632200507",
      "name": "RASMUS WIBAEK",
      "affiliations": [
        "Steno Diabetes Center"
      ]
    },
    {
      "id": "https://openalex.org/A2796117530",
      "name": "Gregers Stig Andersen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2046349481",
      "name": "Christina C. Dahm",
      "affiliations": [
        "Aarhus University"
      ]
    },
    {
      "id": "https://openalex.org/A2110865553",
      "name": "Daniel R. Witte",
      "affiliations": [
        "Steno Diabetes Center",
        "Aarhus University Hospital",
        "Aarhus University"
      ]
    },
    {
      "id": "https://openalex.org/A2677273714",
      "name": "Adam Hulman",
      "affiliations": [
        "Aarhus University Hospital",
        "Aarhus University",
        "Steno Diabetes Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2169818249",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2973639036",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4226269719",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2972198316",
    "https://openalex.org/W3116042400",
    "https://openalex.org/W3198980621",
    "https://openalex.org/W2735580341",
    "https://openalex.org/W3013605954",
    "https://openalex.org/W2131715067",
    "https://openalex.org/W3090365452",
    "https://openalex.org/W4226332741",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W3194569768",
    "https://openalex.org/W2214691690",
    "https://openalex.org/W2159235877",
    "https://openalex.org/W2100741073",
    "https://openalex.org/W2136085913",
    "https://openalex.org/W2134431449",
    "https://openalex.org/W1998392635",
    "https://openalex.org/W2624697962",
    "https://openalex.org/W3092398080",
    "https://openalex.org/W2302501749",
    "https://openalex.org/W2913997948",
    "https://openalex.org/W3133678827",
    "https://openalex.org/W3156027765"
  ],
  "abstract": "Abstract Background Large language models have had a huge impact on natural language processing (NLP) in recent years. However, their application in epidemiological research is still limited to the analysis of electronic health records and social media data. Objectives To demonstrate the potential of NLP beyond these domains, we aimed to develop prediction models based on texts collected from an epidemiological cohort and compare their performance to classical regression methods. Methods We used data from the British National Child Development Study, where 10,567 children aged 11 years wrote essays about how they imagined themselves as 25-year-olds. Overall, 15% of the data set was set aside as a test set for performance evaluation. Pretrained language models were fine-tuned using AutoTrain (Hugging Face) to predict current reading comprehension score (range: 0-35) and future BMI and physical activity (active vs inactive) at the age of 33 years. We then compared their predictive performance (accuracy or discrimination) with linear and logistic regression models, including demographic and lifestyle factors of the parents and children from birth to the age of 11 years as predictors. Results NLP clearly outperformed linear regression when predicting reading comprehension scores (root mean square error: 3.89, 95% CI 3.74-4.05 for NLP vs 4.14, 95% CI 3.98-4.30 and 5.41, 95% CI 5.23-5.58 for regression models with and without general ability score as a predictor, respectively). Predictive performance for physical activity was similarly poor for the 2 methods (area under the receiver operating characteristic curve: 0.55, 95% CI 0.52-0.60 for both) but was slightly better than random assignment, whereas linear regression clearly outperformed the NLP approach when predicting BMI (root mean square error: 4.38, 95% CI 4.02-4.74 for NLP vs 3.85, 95% CI 3.54-4.16 for regression). The NLP approach did not perform better than simply assigning the mean BMI from the training set as a predictor. Conclusions Our study demonstrated the potential of using large language models on text collected from epidemiological studies. The performance of the approach appeared to depend on how directly the topic of the text was related to the outcome. Open-ended questions specifically designed to capture certain health concepts and lived experiences in combination with NLP methods should receive more attention in future epidemiological studies.",
  "full_text": "Original Paper\nLarge Language Models for Epidemiological Research via\nAutomated Machine Learning: Case Study Using Data From\nthe British National Child Development Study\nRasmus Wibaek 1, PhD; Gregers Stig Andersen 1, PhD; Christina C Dahm 2, PhD; Daniel R Witte 2,3, PhD; Adam\nHulman2,3, PhD\n1Steno Diabetes Center Copenhagen, Herlev, Denmark\n2Department of Public Health, Aarhus University, Aarhus, Denmark\n3Steno Diabetes Center Aarhus, Aarhus University Hospital, Aarhus, Denmark\nCorresponding Author:\nAdam Hulman, PhD\nSteno Diabetes Center Aarhus, Aarhus University Hospital\nPalle Juul-Jensens Boulevard 11\nAarhus, 8200\nDenmark\nPhone: 45 23707481\nEmail: ADAHUL@rm.dk\nAbstract\nBackground: Large language models have had a huge impact on natural language processing (NLP) in recent years.\nHowever, their application in epidemiological research is still limited to the analysis of electronic health records and social\nmedia data.\nObjectives: To demonstrate the potential of NLP beyond these domains, we aimed to develop prediction models based on\ntexts collected from an epidemiological cohort and compare their performance to classical regression methods.\nMethods: We used data from the British National Child Development Study, where 10,567 children aged 11 years wrote\nessays about how they imagined themselves as 25-year-olds. Overall, 15% of the data set was set aside as a test set for\nperformance evaluation. Pretrained language models were fine-tuned using AutoTrain (Hugging Face) to predict current\nreading comprehension score (range: 0-35) and future BMI and physical activity (active vs inactive) at the age of 33 years. We\nthen compared their predictive performance (accuracy or discrimination) with linear and logistic regression models, including\ndemographic and lifestyle factors of the parents and children from birth to the age of 11 years as predictors.\nResults: NLP clearly outperformed linear regression when predicting reading comprehension scores (root mean square error:\n3.89, 95% CI 3.74-4.05 for NLP vs 4.14, 95% CI 3.98-4.30 and 5.41, 95% CI 5.23-5.58 for regression models with and without\ngeneral ability score as a predictor, respectively). Predictive performance for physical activity was similarly poor for the 2\nmethods (area under the receiver operating characteristic curve: 0.55, 95% CI 0.52-0.60 for both) but was slightly better than\nrandom assignment, whereas linear regression clearly outperformed the NLP approach when predicting BMI (root mean square\nerror: 4.38, 95% CI 4.02-4.74 for NLP vs 3.85, 95% CI 3.54-4.16 for regression). The NLP approach did not perform better\nthan simply assigning the mean BMI from the training set as a predictor.\nConclusions: Our study demonstrated the potential of using large language models on text collected from epidemiological\nstudies. The performance of the approach appeared to depend on how directly the topic of the text was related to the outcome.\nOpen-ended questions specifically designed to capture certain health concepts and lived experiences in combination with NLP\nmethods should receive more attention in future epidemiological studies.\nJMIR Med Inform 2023;11:e43638; doi: 10.2196/43638\nKeywords: natural language processing; deep learning; language model; epidemiology; cohort study; prediction; NLP;\nprediction model; child development; regression model; large language model; LLM\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 1\n(page number not for citation purposes)\nIntroduction\nUnderstanding human language is not a trivial task for\nmachines. Natural language processing (NLP), that is, the\nanalysis of free text with computational methods, has existed\nas a scientific field for more than half a century [ 1]. The\nintroduction of large language models was a major leap\nfor the field around the millennium [ 2]. The essentials of\nNLP have been reviewed recently with a clinical target\naudience in mind [ 3]. Text can be considered as a sequence\nof characters or words. These linguistic building blocks are\nreferred to as tokens. Once a text is parsed into tokens, a\nmathematical representation is generated. The most common\napproach is to use word embeddings—mapping tokens to\nnumerical vectors. These embeddings are trained (assessed\nin a data-driven manner) on large data sets, and their key\nfeature is that they preserve the relationships between related\nwords. Transformers, introduced in 2017 [ 4], are currently\nthe most popular underlying model architecture as they excel\nat contextualizing words in sentences. The vast amount of\neasily accessible textual data on the internet represents a\nmassive resource for training language models. As compared\nto supervised machine learning (ML) approaches where the\noutcome (target) is available in the training data set, the\nsituation is more complicated with language modeling, where\nthe assignment of labels is not always straightforward. One\npopular approach, also applied in one of the most influ-\nential language models—Bidirectional Encoder Representa-\ntions from Transformers (BERT) [ 5], is masked language\nmodeling, that is, masking a certain proportion of words and\nconsidering them as outcomes to be predicted based on the\npreceding sequences of words. Another approach used in the\ndevelopment of BERT is the prediction of the next sentence\nin a text out of several options. This is a semisupervised\ntraining strategy that makes it possible to turn vast amounts\nof texts, for example, the English-language Wikipedia corpus,\ninto a training set for a language model [ 5]. Technological\nadvancements in computational tools (eg, graphical process-\ning units and parallelization) have allowed language models\nto increase massively in size to hundreds of billions of\nparameters in recent years and have pushed performance\ncloser and closer to human level in various NLP tasks [ 5-7].\nThese language models, developed by tech giants or their\nsubsidiaries, are used in search engines, language transla-\ntors, and auto-correct functions, among others, affecting our\neveryday lives.\nLarge language models have a broad scientific potential\nas well, and with the advent of transfer learning, they are\nmore and more available for those who do not necessarily\nhave the computational resources of tech giants. Transfer\nlearning is the reuse of a pretrained model for a new data\nset or even a new prediction task that is different from\nthe one it was originally trained for [ 8]. This approach\nunlocks the potential of ML for smaller studies by using\nknowledge representations (in a form of pretrained param-\neters) learned in large data sets. The significance of the\nmethod for NLP was first demonstrated by Howard and\nRuder [ 9], who improved the predictive performance on\nseveral NLP benchmarks by ~30% by training a universal\nlanguage model and reused it for specialized tasks via transfer\nlearning. Even though transfer learning broadens the group of\npotential users of large language models and deep learning\nin general, it still requires specialized skills to apply these\nmodels. Web services to automate the training and deploy-\nment of ML models (automated ML [AutoML]) have been\ndeveloped to overcome this barrier and unlock the potential in\ndeep learning for researchers without specialized ML skills;\nhowever, their use is not common in the clinical research\ncommunity [10,11].\nIn addition to knowledge identification (named-entity\nrecognition), synthetization, or discovery in the scientific\nliterature [ 12], NLP has had an impact on clinical research\nwith applications mostly focusing on the analysis of\nelectronic health records or social media data [ 13,14], most\nlikely due to the large size of these data sources. However,\nthe potential in free-text data and NLP are to date not fully\nexploited in classical epidemiological studies. It is likely\nthat NLP performs better than classical regression prediction\nmodels in certain settings, but not all, depending on the\ncontent of the input text and the outcome to be predicted.\nWe designed a case study to evaluate the performance\nof large language models, trained via AutoML, in predicting\ncurrent reading comprehension and future BMI and physi-\ncal activity based on essays written by 11-year-old children\nabout how they imagine themselves as 25-year-olds. We then\ncompared this with a classical regression approach, including\ndemographic and lifestyle factors that were selected based\non prior domain knowledge as predictors. We explicitly\naimed to study and compare the predictive ability of the\nmodels (accuracy or discrimination), without the considera-\ntion of etiology as it is only on this benchmark that ML and\ntraditional models can currently be compared.\nMethods\nData Source\nThe National Child Development Study (NCDS) originally\nincluded 17,415 individuals born in the same week of\n1958 in England, Wales, or Scotland [15]. In a total\nof 12 sweeps, cohort members have been followed since\nthen via interviews, surveys, and biomedical measure-\nments, mostly focusing on health and sociodemographic\ninformation not only of the participants but also to some\nextent their parents. In this study, we used information\nfrom baseline (at birth in 1958), sweep 1 (age 7 years in\n1965), sweep 2 (age 11 years in 1969), and sweep 5 (age\n33 years in 1991) [ 16-18].\nThe three outcomes are (1) reading comprehension\nscore (continuous) at age 11 years; (2) BMI (continu-\nous) at age 33 years; and (3) physical activity (binary)\nat age 33 years. Reading comprehension (score range:\n0-35) was assessed using a test filled out at school. The\noriginal test is available on the web on the UK Data\nService portal [17]. BMI was calculated as weight (kg)\ndivided by height (m) squared based on anthropometric\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 2\n(page number not for citation purposes)\nmeasurements taken at the time of the interview. Physical\nactivity was assessed with 2 questions, asking whether\nparticipants do any sport or exercise, and if so, how\noften. Participants were considered as physically active if\nthey reported exercising at least once a week.\nAt the age of 11 years, the children were asked to write\nan essay about how they imagined themselves as 25-year-olds\n[16]. The instructions were the following: “Imagine that you\nare 25 years old now. Write about the life you are leading,\nyour interests, your home life and your work at the age of\n25. (You have 30 min to do this.).” Out of the 13,669 essays,\n10,567 (77.31%) were transcribed [ 19], which served as the\ninput for the deep learning analyses.\nWe had access to the following variables that were\navailable at the birth of the participants: sex, ethnicity, birth\nweight, gestational age at birth, parity, age and BMI of\nthe mother and father, whether the mother spoke English\nat home, mother’s smoking habit prior to pregnancy, and\nsocial class of the head of the household. Moreover, there was\ninformation available on the children’s eating habits at age 7\nyears (appetite and overeating) and BMI, lifestyle (how often\nthey read books, used parks, and did sports activities), and\ngeneral ability score (similar to an IQ test) at age 11 years.\nThese variables, selected based on prior knowledge in relation\nto the outcomes, are only a minor subset of those available in\nthe cohort. Extensive descriptions of the different sweeps of\nthe study are available on the web on the UK Data Service\nportal [16-18].\nPredictive Modeling and Performance\nEvaluation Strategy\nAn analytical sample was defined for each of the 3 outcomes.\nA random sample of approximately 15% of the participants\nwas reserved as a test set in each of the 3 analytical sam-\nples before developing the models, and the remaining 85%\nconstituted the development set. In the AutoML approach, the\ndevelopment set was further split into a training set (80%) and\na validation set (20%). All reported performance metrics were\nevaluated on the test sets.\nThe root mean square error (RMSE) was used as a\nperformance metric for the continuous outcomes, that is,\nreading comprehension and BMI. Additionally, 95% CIs for\nRMSE were calculated using the basic bootstrap method with\nthe boot package (version 1.3-28) in R (The R Foundation for\nStatistical Computing). To provide a benchmark RMSE score\nfor comparison, we applied and evaluated a naive approach,\nthat is, assigned the mean value of the outcome from the\ndevelopment data set as predictions in the test set.\nDiscrimination, measured by the area under the receiver\noperating characteristics curve (AUC ROC), was used as a\nperformance metric for the binary outcome: physical activity.\nThe naive benchmark was random assignment, and thus, an\nAUC ROC of 0.5 was defined.\nClassical Approach: Regression Models\nRegression models included predefined sets of variables\nthat could vary for the 3 outcomes based on prior\nepidemiological knowledge. Models were fitted using the\nentire development set after applying multiple imputation\n(within the development set for each particular outcome)\nby chained equations to impute missing predictors (mice\nR package; version 3.14.0). We generated 10 imputed\nsamples with the maximum number of iterations set to 30.\nEstimates were then pooled from the 10 resulting models.\nThe mice models derived in the development sets were\nsubsequently applied to the test sets to avoid information\nleakage.\nReading comprehension score and BMI were modeled\nusing linear regression. For the reading comprehension\noutcome, we fitted 2 models, with and without including\nthe general ability score among the predictors. The binary\noutcome physical activity at age 33 years was modeled\nwith logistic regression. The complete list of variables\nincluded in each model are shown in Tables 1 and 2.\nTable 1. Linear regression coefficients from prediction models for reading comprehension score and BMI.\nPredictor Reading comprehension score at age 11 years (n=8890) BMI at age 33 years (n=6010)\nImputed, n (%)\nModel 1 coefficients\n(95% CI)\nModel 2 coefficients\n(95% CI) Imputed, n (%)\nModel coefficients\n(95% CI)\nSex (reference: male) 0 –0.07 (–0.31 to 0.18) –0.63 (–0.81 to –0.45) 0 –1.1 (–1.3 to –0.9)\nEthnicity (reference: European) 1358 (15.28) 794 (13.21)\n  African –3.20 (–4.41 to –2.00) –0.60 (–1.41 to 0.22) 1.62 (0.27 to 2.97)\n  Asian –2.90 (–4.43 to –1.37) –1.40 (–2.41 to –0.38) 1.11 (–0.26 to 2.49)\nMother’s age (10 years) 423 (4.76) 1.59 (1.23 to 1.95) 0.92 (0.66 to 1.18) 234 (3.89) –0.22 (–0.56 to 0.12)\nFather’s age (10 years) 732 (8.23) 0.53 (0.22 to 0.84) 0.37 (0.16 to 0.59) 415 (6.91) –0.19 (–0.48 to 0.10)\nMother’s BMI N/Aa N/A N/A 615 (10.23) 0.12 (0.09 to 0.14)\nFather’s BMI N/A N/A N/A 752 (12.51) 0.10 (0.06 to 0.14)\nMother does not speak English at\nhome\n1015 (11.42) –0.06 (–0.46 to 0.34) –0.14 (–0.42 to 0.14) N/A N/A\nBirth weight (100 g) 694 (7.81) 0.11 (0.08 to 0.13) 0.03 (0.01 to 0.05) 414 (6.89) 0.00 (–0.03 to 0.02)\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 3\n(page number not for citation purposes)\nPredictor Reading comprehension score at age 11 years (n=8890) BMI at age 33 years (n=6010)\nImputed, n (%)\nModel 1 coefficients\n(95% CI)\nModel 2 coefficients\n(95% CI) Imputed, n (%)\nModel coefficients\n(95% CI)\nGestational age 1267 (14.25) 0.00 (–0.02 to 0.01) 0.00 (–0.01 to 0.01) 764 (12.71) 0.00 (–0.01 to 0.01)\nParity (reference: nulliparous) 421 (4.74) 232 (3.86)\n  Primiparous –1.33 (–1.64 to –1.03) –0.72 (–0.94 to –0.49) –0.12 (–0.38 to 0.14)\n  Multiparous –3.45 (–2.81 to –1.48) –1.53 (–1.78 to –1.27) 0.19 (–0.11 to 0.50)\nMaternal smoking 450 (5.06) –0.41 (–0.66 to –0.17) 0.02 (–0.15 to 0.20) 250 (4.16) 0.38 (0.17 to 0.58)\nSocioeconomic status (reference: I) 945 (10.63) 562 (9.35)\n  II –1.24 (–1.85 to –0.63) –0.27 (–0.72 to 0.18) –0.30 (–0.80 to 0.20)\n  III (nonmanual) –2.14 (–2.81 to –1.48) –0.56 (–1.05 to –0.08) 0.03 (–0.56 to 0.62)\n  III (manual) –4.17 (–4.73 to –3.60) –1.34 (–1.7 to –0.92) 0.32 (–0.16 to 0.79)\n  IV –5.11 (–5.72 to –4.49) –1.68 (–2.15 to –1.22) 0.26 (–0.27 to 0.79)\n  V –6.39 (–7.14 to –5.65) –1.89 (–2.45 to –1.33) 0.54 (–0.12 to 1.20)\n  No male head of household –4.64 (–5.45 to –3.83) –1.46 (–2.04 to 0.88) 0.27 (–0.39 to 0.93)\nPoor appetite N/A N/A N/A 607 (10.1) –0.15 (–0.43 to 0.13)\nOvereating N/A N/A N/A 612 (10.18) 0.06 (–0.39 to 0.52)\nReading books (reference: often) 276 (3.1) N/A\n  Sometimes –0.61 (–0.86 to –0.36) –0.20 (–0.39 to –0.02) N/A\n  Hardly ever –2.27 (–2.71 to –1.83) –0.72 (–1.04 to –0.41) N/A\nSport (reference: often) 238 (2.68) 146 (2.43)\n  Sometimes 0.09 (–0.17 to 0.34) –0.07 (–0.25 to 0.12) –0.15 (–0.38 to 0.08)\n  Hardly ever –0.06 (–0.47 to 0.35) 0.10 (–0.19 to 0.39) –0.07 (–0.41 to 0.27)\nPark use (reference: often) 847 (9.53) 488 (8.12)\n  Sometimes 0.38 (0.12 to 0.65) 0.10 (–0.10 to 0.30) –0.30 (–0.54 to –0.06)\n  Never 0.36 (–0.15 to 0.87) 0.32 (–0.04 to 0.69) 0.05 (–0.44 to 0.54)\n  Not available 0.06 (–0.34 to 0.46) –0.04 (–0.33 to 0.25) –0.11 (–0.45 to 0.24)\nGeneral ability score 1 (0.01) N/A 0.26 (0.26 to 0.27) N/A N/A\nBMI at age 11 years N/A N/A N/A 886 (14.74) 0.67 (0.62 to 0.72)\naN/A: not applicable.\nTable 2. Odds ratios (OR) from the prediction model for physical activity.\nPredictor Outcome: physical activity at age 33 years (n=6204)\nImputed, n (%) OR (95% CI)\nSex (reference: male) 0 (0) 1.13 (1.01-1.27)\nEthnicity (reference: European) 846 (14.04)\nAfrican 0.68 (0.34-1.35)\nAsian 0.76 (0.40-1.45)\nMother’s age (10 years) 234 (3.88) 0.98 (0.83-1.15)\nFather’s age (10 years) 431 (7.15) 1.05 (0.91-1.21)\nMother’s BMI 652 (10.82) 0.98 (0.97-1.00)\nFather’s BMI 809 (13.43) 0.99 (0.98-1.01)\nBirth weight (100 g) 411 (6.82) 1.00 (0.99-1.01)\nGestational age 796 (13.21) 1.00 (0.99-1.01)\nParity (reference: nulliparous) 232 (3.85)\nPrimiparous 0.99 (0.86-1.13)\nMultiparous 0.94 (0.80-1.11)\nMaternal smoking 253 (4.2) 0.92 (0.81-1.04)\nSocioeconomic status (reference: I) 579 (9.61)\nII 0.91 (0.69-1.19)\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 4\n(page number not for citation purposes)\nPredictor Outcome: physical activity at age 33 years (n=6204)\nImputed, n (%) OR (95% CI)\nIII (nonmanual) 0.83 (0.61-1.12)\nIII (manual) 0.82 (0.63-1.06)\nIV 0.77 (0.59-1.02)\nV 0.64 (0.45-0.92)\nNo male head of household 0.72 (0.51-1.02)\nPoor appetite 631 (10.47) 0.87 (0.71-1.05)\nOvereating 630 (10.46) 0.92 (0.74-1.15)\nSport (reference: often) 148 (2.46)\nSometimes 0.90 (0.79-1.02)\nHardly ever 0.70 (0.59-0.84)\nPark use (reference: often) 510 (8.47)\nSometimes 0.97 (0.85-1.10)\nNever 0.77 (0.61-0.97)\nNot available 0.73 (0.59-0.90)\nBMI at age 11 years 934 (15.50) 1.01 (0.97-1.04)\nDeep Learning Approach: NLP Using\nLarge Language Models\nWe used an AutoML tool, AutoTrain by Hugging Face\n[20], to develop our NLP prediction model. AutoTrain is a\nweb-based service to train and deploy state-of-the-art ML\nmodels (text or tabular as of June 2022). The data sets\nwere uploaded as comma-separated values files including 2\ncolumns: the essays (as text) and the outcome. AutoTrain then\nsplit this data set into a training set (80%) and a validation set\n(20%) and started training (fine-tuning) a variety of pretrained\nlarge language models. The number of models can be defined\nby the user. We chose n=15 for this study. After the train-\ning process for all 15 models was complete, we accessed\nthe best-performing model through Hugging Face’s applica-\ntion programming interface from Python (Python Software\nFoundation) and evaluated predictive performance on the\nreserved 15% in the test set. We did this for all 3 outcomes.\nEthical Considerations\nWe analyzed a publicly available, anonymized data set;\ntherefore, our study did not require ethical approval.\nResults\nReading Comprehension Score (Age 11\nYears)\nOut of 10,567 participants with transcribed essays, 10,490\n(99.27%) completed the reading comprehension test,\nforming the analytical sample for this outcome. From the\n10,490 participants, a random sample of 1600 (15.25%)\nparticipants were set aside for testing (test set), leaving\ndata from 8890 (84.75%) participants for model develop-\nment (development set). Reading comprehension scores\nranged from 0 to 34, with a median value of 16 (IQR\n12-20). The distribution was similar in the test set and\nonly differed slightly in the maximum (35) and the\nupper-quartile (21) values.\nThe main results are shown in Figure 1. The naive\nbenchmark had an RMSE of 6.07 (95% CI 5.89-6.26),\nwhich was outperformed by both the classical regression\nand the deep learning approach. The linear regression\nmodel without the general ability score had an 11% better\nperformance than the naive benchmark with an RMSE of\n5.41 (95% CI 5.23-5.58). This was further improved when\nincluding the general ability score in the model (4.14,\n95% CI 3.98-4.30). The best performance and thus lowest\nRMSE was achieved by the deep learning approach (3.89,\n95% CI 3.74-4.05), corresponding to a 36% lower RMSE\nthan the naive benchmark.\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 5\n(page number not for citation purposes)\nFigure 1. Performance of the prediction models versus the benchmark approach (naive prediction: assignment of the mean value from the training\nset) for (A) reading comprehension score and (B) BMI. Root mean square errors (RMSEs) are presented with 95% CIs. Percentages represent\ndifferences compared to the benchmark approach. NLP: natural language processing.\nThe linear regression models revealed that several predic-\ntors were associated with the reading comprehension score.\nMale sex, European ethnicity, having older parents, being\nthe first child in the family, higher birth weight, higher\nsocioeconomic status, reading books often, and having a\nhigher general ability score were all positively associated with\nreading comprehension. Regression coefficients are presented\nin Table 1.\nBMI (Age 33 Years)\nThe analytical sample for the BMI analysis consisted of 7060\nparticipants who later had their weight and height measured at\nage 33 years. From the 7060 participants, a random sample\nof 1050 (14.87%) participants were set aside for testing\nmodel performance, leaving 6010 (85.13%) participants in the\ndevelopment set. BMI values ranged from 12.3 to 50.6 kg/m 2\nin the development set and from 15.0 to 50.8 kg/m 2 in the test\nset. Median values were similar: 24.3 (IQR 22.3-27.1) and\n24.4 (IQR 22.2-26.8) kg/m2, respectively.\nPerformance metrics are shown in Figure 1 . The naive\nbenchmark had an RMSE of 4.45 (95% CI 4.09-4.78),\nwhich was similar to the performance of the deep learning\napproach (4.38, 95% CI 4.02-4.74). The regression model\nperformed ~13% better, achieving an RMSE of 3.85 (95% CI\n3.54-4.16).\nSeveral variables were associated with BMI at age 33\nyears according to the regression model, including sex,\nethnicity, parental BMI, parity, maternal smoking before\npregnancy, use and access to parks, and BMI at age 11 years.\nRegression coefficients are presented in Table 1.\nPhysical Activity (Age 33 Years)\nWe had information on physical activity at age 33 years\nfrom 7304 participants. We selected 1100 (15.06%) of them\nrandomly for the test set, leaving 6204 (84.94%) participants\nfor model development. Overall, 68.75% (4265/6204) and\n69.55% (765/1100) were physically active in the development\nand test sets, respectively. The logistic regression and NLP\napproaches had the same performance (AUC ROC=0.55,\n95% CI 0.52-0.60), representing poor discriminatory power.\nThere were a few variables associated with the outcome\nin the logistic regression model: sex, socioeconomic status,\nmother’s BMI, sport activities, and use or access to parks at\nage 11 years. Odds ratios are presented in Table 2.\nDiscussion\nOur study demonstrated the potential of using deep learn-\ning–based large language models for text prediction in\nepidemiological studies and compared it to classical statistical\nmethods. We observed different rankings of predictive\nperformance between the deep learning and classical\napproaches across the 3 outcomes. The performance of the\ndeep learning approach appeared to depend on how closely\nthe actual task, that is, writing an essay about the future,\nwas related to the outcome. Writing and reading skills among\nchildren are expected to be associated with each other, so the\nlanguage model could have picked up on linguistic features\nsuch as grammatical correctness, vocabulary, complexity\nof sentences, etc, which led to the NLP method clearly\noutperforming linear regression when predicting the reading\ncomprehension score. This was still true when the general\nability score was added to the regression model as a pre-\ndictor, despite its high correlation with reading comprehen-\nsion. However, this performance came with a computational\nprice. Large language models include hundreds of millions or\neven billions of parameters, whereas our regression model\nincluded 26. In addition to simplicity, interpretability is\nanother positive feature of linear regression. The model\nrevealed several strong predictors and quantified associations\nvia interpretable regression coefficients, for example, a social\ngradient with about a 5-point estimated difference between\nthe highest and lowest socioeconomic classes. Although the\ncoefficients are expressed in easily understandable units, they\nshould not be interpreted in the etiological sense, unless a\ncausal framework is applied. With the increasing interest in\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 6\n(page number not for citation purposes)\nML and causal inference, the development of ML methods\nintegrating causal structures is warranted [21].\nEpidemiologists and clinicians are comfortable with\ninterpreting the usual measures of association: linear\nregression coefficients, odds ratios, or hazard ratios. Although\nwe are far from understanding the overall nature of large\nlanguage models, there are emerging methods in explainable\nartificial intelligence (AI) that can help to understand the\ndriving factors of at least individual predictions (eg, which\nfeatures or specific expression in a text led to a prediction).\nHowever, they are yet to be integrated into AutoML tools.\nAccess to explainable AI tools (eg, LIME [ 22]) as part of\nAutoML solutions is likely to contribute to a more wide-\nspread use of deep learning in epidemiological research,\nwhere we often ask etiological questions and predictive\nperformance is not necessarily the main focus.\nChildren were directly asked about their interests as\n25-year-olds as part of the essay task, which could poten-\ntially include information on physical activity. We therefore\nexpected a similar performance for the NLP and regression\napproaches. Both approaches picked up some signals in the\ndata, demonstrated by discrimination nominally exceeding\nrandom assignment (AUC ROC=0.5), but their performance\nwas still poor and statistically not different from each other.\nA previous study from the NCDS reported that 42% of boys\nand 34% of girls mentioned physical activity in their essays\n[19]. The authors then used this information to predict their\nphysical activity patterns during adulthood, and they found a\npositive association among boys, but not girls. Pongiglione et\nal [19] used a 2-step approach: first, they applied a supervised\nML method (support vector machines) to extract information\non physical activity identity from the essays and, second, used\nthat variable to predict the physical activity in adulthood with\na separate logistic regression model. The drawback of this\napproach is that it needs a subset of the data set to have\nlabels for the intermediate outcome (whether physical activity\nwas mentioned in the text or not), which can be time-con-\nsuming and labor-intensive for large data sets. Once some\nlabels are available and the prediction model has reasonable\nperformance, the approach can handle large amounts of data\nto classify the rest of the essays. We have demonstrated that\nlarge language models can be directly applied on the data\nwithout first generating new intermediate labels.\nThe major difference between the study by Pongiglione\net al [ 19] and ours, and in general between many epidemio-\nlogical and data science approaches, is whether the focus\nis on the causal understanding of associations (etiology) or\non prediction. Although the 2 approaches require different\nstudy designs and interpretation, the conflation of etiology\nand prediction is still common in clinical research (eg,\ncausal interpretation of strong predictors) [ 23]. Our study\nshowed that despite identifying variables strongly associated\nwith the outcome, overall predictive performance might be\npoor. Therefore, we should be careful when interpreting\nand drawing causal conclusions from the results of models\ndeveloped with a predictive aim and avoid mistakenly stating\nthat altering the level of a component of a predictive model\nwould change the risk of the outcome.\nSimilar evidence also exists regarding the prevention\nof obesity. In a meta-analysis of 15 prospective studies,\nSimmonds et al [ 24] reported that children or adolescents\nwith obesity were about 5 times more likely to be obese in\nadulthood than those without obesity. In our study, we also\nfound a strong association between BMI in childhood and\nadulthood; however, the linear regression model performed\nonly slightly better than the naive benchmark, whereas the\nNLP approach did not outperform the benchmark at all. We\nwere not surprised that NLP performed worse than regression,\nconsidering that these approaches had matching performance\nin predicting physical activity, and obesity was not expected\nto be directly mentioned in the essays, in contrast to physical\nactivity. In general, the results for this outcome strengthen\nour previous argument that prediction can be difficult even\nif well-established associations are present at the population\nlevel.\nThe development of prediction models, regardless of\nthe use of ML or classical methods, is not a trivial task\n(handling of missing data, variable selection, reporting,\netc) [25-28]. This is often reflected in the quality of\nprediction studies and the fact that only a small propor-\ntion of published prediction models are actually used in\nclinical practice [29]. AutoML does not offer a solution\nfor this, as careful study design is still crucial. However,\nit makes the use of deep learning techniques (includ-\ning pretrained models) more feasible for epidemiologists,\nwho can use their resources on study design instead\nof programming tasks. Faes et al [10] recently repor-\nted a study where physicians (non-AI experts) achieved\nsimilar performance to expert-tuned algorithms in several\nmedical image classification tasks [10]. We only needed\nto use programming in the NLP analysis to preprocess the\nessays and for the evaluation of the results, whereas the\nrest of the process was completed in a browser environ-\nment (model evaluation became available in AutoTrain\nby Hugging Face soon after we finished our analyses).\nAutoML solutions are often claimed to democratize ML;\nhowever, the financial costs are still not negligible. It\nis indeed a positive development that technical skills\nand computational resources no longer pose as strong a\nbarrier as before. We should be vigilant that this increased\naccessibility is accompanied by an increased focus on\ngood study design and research quality. An aspect that\nAutoML might have a positive influence on is knowl-\nedge translation. With the AutoML approach we used, the\ndeep learning model became available right after training\nand could be used to make predictions for new samples\neither in the browser or via an application programming\ninterface. The developer can choose to keep the model\nprivate or make it public so that the research community\ncan reuse it as a pretrained model, either directly or after\nfine-tuning, thus potentially leading to multistep, incremen-\ntal transfer learning.\nA major strength of our work is the use of deep\nlearning methods that are currently state of the art in NLP\nto exploit an innovative data source—in this case, text\nwritten by participants in a cohort study. We compared\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 7\n(page number not for citation purposes)\nthese models with standard methods in epidemiology and\ndiscussed similarities and differences between the classical\nand data science approaches. A strength of deep learn-\ning methods in general is the potential reuse of extant\ntrained models. Although the interest in transfer learning\nis rapidly increasing in clinical research, it is still an\nalmost unknown concept in the epidemiology commun-\nity, despite some studies demonstrating major benefits,\neven for tabular data [30,31]. To increase the impact of\nprediction studies, especially those using ML and deep\nlearning methods, authors should be encouraged to deposit\ntheir models on the web and make them openly available.\nThis is a common practice in the data science commun-\nity, as most developers depend heavily on pretrained deep\nlearning models due to computational requirements. The\nHugging Face Model Hub has >50,000 pretrained models,\nwhich fits well with the FAIR (findability, accessibil-\nity, interoperability, and reusability) principles on reusing\ndigital assets in an open and inclusive manner [32]. In a\nclinical research setting, even if data accompany publica-\ntions, which is still rarely the case, sharing resources is\nalmost exclusively restricted to data sets and analysis code.\nThe children’s essays used in our NLP models were\nnot designed to be used for specific prediction tasks. Our\nmain aim was to demonstrate the use of deep learn-\ning–based large language models and to compare them\nto the classical statistical methods used in epidemiologi-\ncal research. In showing that NLP methods can extract\nfeatures from these texts that are associated with certain\ntraits, our study points toward the potential for extracting\nmeaningful additional data from other extant free-text data\nsources. Each text data source will have its own historical\npeculiarities and specific characteristics. In our case, the\nessays were written half a century ago by children. The\npractical utility of the presented models outside the context\nof the UK 1958 birth cohort is consequently likely limited\nwithout transfer learning via fine-tuning for adaptation to a\nnew context. It should be noted that language models are\nusually trained on texts from the internet (eg, Wikipedia)\nand, as such, mostly represent texts written in the past\nfew decades. Where older texts are included—for example,\nfrom older, digitized books—sources will represent texts\nselected for publication at the time. In all cases, texts\nwritten by children are likely to be severely underrepresen-\nted in training sets.\nA previous review of the clinical literature found no\nevidence for ML having better predictive performance\nthan traditional statistical methods [33]. Considering the\ntrade-off in the loss of easy interpretability, in most\nstudies, the use of ML does not offer any benefits\nas long as clinical researchers mostly work with tabu-\nlar data. However, the integration of new data sources\nin epidemiological studies (text, medical images, and\ntime series) is only possible by applying deep learning\nand often transfer learning, which also gives us the\nopportunity to reuse knowledge between studies. With\nregard to NLP, large language models have almost\nachieved human-level performance for various specific\ntasks; therefore, it may become possible for open-ended\nquestions or essays to replace or at least complement\nlong questionnaires (eg, on diet) in large epidemiologi-\ncal studies. Moreover, NLP offers computational methods,\nfor example, for the analysis of interview transcripts in\nqualitative studies, which might contribute to closing the\ngap between qualitative and quantitative research. Byrsell\net al [34] analyzed transcribed emergency calls to detect\nout-of-hospital cardiac arrests using deep learning, and\nFagherazzi et al [35] recently gave an overview of the\npotential of vocal biomarkers (containing both linguistic\nand acoustic features) in clinical research and practice.\nWith the large-scale collection of such and other novel\ndata types, potentially in combination with tabular data,\nthe role of deep learning in epidemiological research is\nlikely to increase as well. However, we can only exploit\nits potential and develop high-quality prediction models for\nclinical or public health use in close collaboration between\nthe data science and clinical research communities.\nAcknowledgments\nAH is supported by a Data Science Emerging Investigator grant (NNF22OC0076725) by the Novo Nordisk Foundation.\nDRW and AH are employed at Steno Diabetes Center Aarhus, which is partly funded by a donation from the Novo Nordisk\nFoundation. The funders had no role in the design of the study. The authors are grateful to all participants of the National Child\nDevelopment Study and the investigators for making the data openly available.\nData Availability\nThe data set can be accessed through the UK Data Service after a simple registration process [16-18].\nConflicts of Interest\nGSA owns shares in Novo Nordisk A/S. GSA is currently employed by Novo Nordisk A/S. During contribution to the\nmanuscript, GSA was employed by Steno Diabetes Center Copenhagen.\nReferences\n1. Nadkarni PM, Ohno-Machado L, Chapman WW. Natural language processing: an introduction. J Am Med Inform\nAssoc. 2011;18(5):544-551. [doi: 10.1136/amiajnl-2011-000464] [Medline: 21846786]\n2. Bengio Y, Ducharme R, Vincent P, Janvin C. A neural probabilistic language model. J Mach Learn Res. 2003 Mar\n2;3(6):1137-1155. [doi: 10.1162/153244303322533223]\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 8\n(page number not for citation purposes)\n3. Chen P-H. Essential elements of natural language processing: what the radiologist should know. Acad Radiol. 2020\nJan;27(1):6-12. [doi: 10.1016/j.acra.2019.08.010] [Medline: 31537505]\n4. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Presented at: 31st\nInternational Conference on Neural Information Processing Systems; June 12, 2017; Long Beach, CA p. 6000-6010.\n[doi: 10.5555/3295222.3295349]\n5. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep Bidirectional transformers for language\nunderstanding. Presented at: 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT; June 2-7, 2019; Minneapolis, MN p. 4171-4186. [doi: 10.\n18653/v1/N19-1423]\n6. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few-shot learners.\nPresented at: 34th International Conference on Neural Information Processing Systems; December 6–12, 2020;\nVancouver, BC p. 1877-1901. [doi: 10.5555/3495724.3495883]\n7. Rae JW, Borgeaud S, Cai T, Millican K, Hoffmann J, Song F, et al. Scaling language models: methods, analysis &\ninsights from training gopher. arXiv. Preprint posted online on December 1, 2021. [doi: 10.48550/arXiv.2112.11446]\n8. Ebbehoj A, Thunbo MØ, Andersen OE, Glindtvad MV, Hulman A. Transfer learning for non-image data in clinical\nresearch: a scoping review. PLOS Digit Health. 2022 Feb 17;1(2):e0000014. [doi: 10.1371/journal.pdig.0000014]\n[Medline: 36812540]\n9. Howard J, Ruder S. Universal language model fine-tuning for text classification. Presented at: 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1); July 15–20, 2018; Melbourne, Australia p. 328-339. [doi: 10.\n18653/v1/P18-1031]\n10. Faes L, Wagner SK, Fu DJ, Liu X, Korot E, Ledsam JR, et al. Automated deep learning design for medical image\nclassification by health-care professionals with no coding experience: a feasibility study. Lancet Digit Health. 2019\nSep;1(5):e232-e242. [doi: 10.1016/S2589-7500(19)30108-6] [Medline: 33323271]\n11. Wan KW, Wong CH, Ip HF, Fan D, Yuen PL, Fong HY, et al. Evaluation of the performance of traditional machine\nlearning algorithms, convolutional neural network and AutoML Vision in ultrasound breast lesions classification: a\ncomparative study. Quant Imaging Med Surg. 2021 Apr;11(4):1381-1393. [doi: 10.21037/qims-20-922] [Medline:\n33816176]\n12. Bose P, Srinivasan S, Sleeman WC, Palta J, Kapoor R, Ghosh P. A survey on recent named entity recognition and\nrelationship extraction techniques on clinical texts. Appl Sci. 2021 Sep;11(18):8319. [doi: 10.3390/app11188319]\n13. Kreimeyer K, Foster M, Pandey A, Arya N, Halford G, Jones SF, et al. Natural language processing systems for\ncapturing and standardizing unstructured clinical information: a systematic review. J Biomed Inform. 2017\nSep;73:14-29. [doi: 10.1016/j.jbi.2017.07.012] [Medline: 28729030]\n14. Spasic I, Nenadic G. Clinical text data in machine learning: systematic review. JMIR Med Inform. 2020 Mar\n31;8(3):e17984. [doi: 10.2196/17984] [Medline: 32229465]\n15. Power C, Elliott J. Cohort profile: 1958 British birth cohort (National Child Development Study). Int J Epidemiol. 2006\nFeb;35(1):34-41. [doi: 10.1093/ije/dyi183] [Medline: 16155052]\n16. University College London IOE, Centre for Longitundinal Studies. National Child Development Study: Age 11, Sweep\n2, “Imagine You Are 25” Essays, 1969. London, United Kingdom: UK Data Service; 1696.\n17. University College London IOE, Centre for Longitudinal Studies. National Child Development Study: Childhood Data\nfrom Birth to Age 16, Sweeps 0-3, 1958-1974. 3rd edition. National Children’s Bureau NBTF; 1974.\n18. University College London IOE, Centre for Longitundinal Studies. National Child Development Study: Age 33, Sweep\n5, 1991. 2nd edition. City University SSRU; 1991.\n19. Pongiglione B, Kern ML, Carpentieri JD, Schwartz HA, Gupta N, Goodman A. Do children's expectations about future\nphysical activity predict their physical activity in adulthood? Int J Epidemiol. 2020 Oct 1;49(5):1749-1758. [doi: 10.\n1093/ije/dyaa131] [Medline: 33011758]\n20. Hugging Face. AutoTrain. URL: https://huggingface.co/autotrain [Accessed 2022-10-17]\n21. Rieckmann A, Dworzynski P, Arras L, Lapuschkin S, Samek W, Arah OA, et al. Causes of outcome learning: a causal\ninference-inspired machine learning approach to disentangling common combinations of potential causes of a health\noutcome. Int J Epidemiol. 2022 Oct 13;51(5):1622-1636. [doi: 10.1093/ije/dyac078] [Medline: 35526156]\n22. Ribeiro MT, Singh S, Guestrin C. “\"Why should I trust you?\": explaining the predictions of any Classifier”. Presented at:\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; August 13, 2016; San\nFrancisco, CA p. 1135-1144. [doi: 10.1145/2939672.2939778]\n23. Ramspek CL, Steyerberg EW, Riley RD, Rosendaal FR, Dekkers OM, Dekker FW, et al. Prediction or causality? a\nscoping review of their conflation within current observational research. Eur J Epidemiol. 2021 Sep;36(9):889-898. [doi:\n10.1007/s10654-021-00794-w] [Medline: 34392488]\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 9\n(page number not for citation purposes)\n24. Simmonds M, Llewellyn A, Owen CG, Woolacott N. Predicting adult obesity from childhood obesity: a systematic\nreview and meta-analysis. Obes Rev. 2016 Feb;17(2):95-107. [doi: 10.1111/obr.12334] [Medline: 26696565]\n25. Hemingway H, Croft P, Perel P, Hayden JA, Abrams K, Timmis A, et al. Prognosis research strategy (PROGRESS) 1: a\nframework for researching clinical outcomes. BMJ. 2013 Feb 5;346(1):e5595. [doi: 10.1136/bmj.e5595] [Medline:\n23386360]\n26. Riley RD, Hayden JA, Steyerberg EW, Moons KGM, Abrams K, Kyzas PA, et al. Prognosis research strategy\n(PROGRESS) 2: prognostic factor research. PLOS Med. 2013 Feb 5;10(2):e1001380. [doi: 10.1371/journal.pmed.\n1001380] [Medline: 23393429]\n27. Steyerberg EW, Moons KGM, van der Windt DA, Hayden JA, Perel P, Schroter S, et al. Prognosis research strategy\n(PROGRESS) 3: prognostic model research. PLoS Med. 2013 Feb 5;10(2):e1001381. [doi: 10.1371/journal.pmed.\n1001381] [Medline: 23393430]\n28. Hingorani AD, van der Windt DA, Riley RD, Abrams K, Moons KGM, Steyerberg EW, et al. Prognosis research\nstrategy (PROGRESS) 4: stratified medicine research. BMJ. 2013 Feb 5;346:e5793. [doi: 10.1136/bmj.e5793] [Medline:\n23386361]\n29. Bouwmeester W, Zuithoff NPA, Mallett S, Geerlings MI, Vergouwe Y, Steyerberg EW, et al. Reporting and methods in\nclinical prediction research: a systematic review. PLoS Med. 2012;9(5):e1001221. [doi: 10.1371/journal.pmed.1001221]\n[Medline: 22629234]\n30. Desautels T, Calvert J, Hoffman J, Mao Q, Jay M, Fletcher G, et al. Using transfer learning for improved mortality\nprediction in a data-scarce hospital setting. Biomed Inform Insights. 2017 Jun;9:1178222617712994. [doi: 10.1177/\n1178222617712994] [Medline: 28638239]\n31. Gao Y, Cui Y. Deep transfer learning for reducing health care disparities arising from biomedical data inequality. Nat\nCommun. 2020 Oct 12;11(1):5131. [doi: 10.1038/s41467-020-18918-3] [Medline: 33046699]\n32. Wilkinson MD, Dumontier M, Aalbersberg IJJ, Appleton G, Axton M, Baak A, et al. The FAIR guiding principles for\nscientific data management and stewardship. Sci Data. 2016 Mar 15;3:160018. [doi: 10.1038/sdata.2016.18] [Medline:\n26978244]\n33. Christodoulou E, Ma J, Collins GS, Steyerberg EW, Verbakel JY, Van Calster B. A systematic review shows no\nperformance benefit of machine learning over logistic regression for clinical prediction models. J Clin Epidemiol. 2019\nJun;110:12-22. [doi: 10.1016/j.jclinepi.2019.02.004] [Medline: 30763612]\n34. Byrsell F, Claesson A, Ringh M, Svensson L, Jonsson M, Nordberg P, et al. Machine learning can support dispatchers to\nbetter and faster recognize out-of-hospital cardiac arrest during emergency calls: a retrospective study. Resuscitation.\n2021 May;162:218-226. [doi: 10.1016/j.resuscitation.2021.02.041] [Medline: 33689794]\n35. Fagherazzi G, Fischer A, Ismael M, Despotovic V. Voice for health: the use of vocal biomarkers from research to\nclinical practice. Digit Biomark. 2021 Apr 16;5(1):78-88. [doi: 10.1159/000515346] [Medline: 34056518]\nAbbreviations\nAI: artificial intelligence\nAUC ROC: area under the receiver operating characteristic curve\nAutoML: automated machine learning\nBERT: Bidirectional Encoder Representations from Transformers\nFAIR: findability, accessibility, interoperability, and reusability\nML: machine learning\nNCDS: National Child Development Study\nNLP: natural language processing\nRMSE: root mean square error\nEdited by Christian Lovis; peer-reviewed by Anders Bjorkelund, Nasmin Jiwani, Yuli Wang; submitted 21.10.2022; final\nrevised version received 29.06.2023; accepted 22.07.2023; published 19.09.2023\nPlease cite as:\nWibaek R, Andersen GS, Dahm CC, Witte DR, Hulman A\nLarge Language Models for Epidemiological Research via Automated Machine Learning: Case Study Using Data From the\nBritish National Child Development Study\nJMIR Med Inform 2023;11:e43638\nURL: https://medinform.jmir.org/2023/1/e43638\ndoi: 10.2196/43638\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 10\n(page number not for citation purposes)\n© Rasmus Wibaek, Gregers Stig Andersen, Christina C Dahm, Daniel R Witte, Adam Hulman. Originally published in JMIR\nMedical Informatics ( https://medinform.jmir.org), 19.09.2023. This is an open-access article distributed under the terms of\nthe Creative Commons Attribution License ( https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use,\ndistribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is\nproperly cited. The complete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as\nwell as this copyright and license information must be included.\nJMIR MEDICAL INFORMATICS Wibaek et al\nhttps://medinform.jmir.org/2023/1/e43638 JMIR Med Inform 2023 | vol. 11 | e43638 | p. 11\n(page number not for citation purposes)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5976048707962036
    },
    {
      "name": "Data science",
      "score": 0.46471843123435974
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41923248767852783
    },
    {
      "name": "Natural language processing",
      "score": 0.3810153305530548
    },
    {
      "name": "Machine learning",
      "score": 0.3787792921066284
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1339947260",
      "name": "Steno Diabetes Centers",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I204337017",
      "name": "Aarhus University",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I2802335433",
      "name": "Aarhus University Hospital",
      "country": "DK"
    }
  ],
  "cited_by": 4
}