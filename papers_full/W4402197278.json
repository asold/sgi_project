{
  "title": "Large language model (LLM) comparison between GPT-3 and PaLM-2 to produce Indonesian cultural content",
  "url": "https://openalex.org/W4402197278",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A24480810",
      "name": "Deni Erlansyah",
      "affiliations": [
        "Universitas Bina Darma"
      ]
    },
    {
      "id": "https://openalex.org/A1856832976",
      "name": "Amirul Mukminin",
      "affiliations": [
        "Jambi University"
      ]
    },
    {
      "id": "https://openalex.org/A5079793277",
      "name": "Dedek Julian",
      "affiliations": [
        "Universitas Bina Darma"
      ]
    },
    {
      "id": "https://openalex.org/A2896163798",
      "name": "Edi Surya Negara",
      "affiliations": [
        "Universitas Bina Darma"
      ]
    },
    {
      "id": "https://openalex.org/A5106968285",
      "name": "Ferdi Aditya",
      "affiliations": [
        "Universitas Bina Darma"
      ]
    },
    {
      "id": "https://openalex.org/A3010107481",
      "name": "Rezki Syaputra",
      "affiliations": [
        "Universitas Bina Darma"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3132699755",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3161970973",
    "https://openalex.org/W4312189463",
    "https://openalex.org/W4389925871",
    "https://openalex.org/W4389502564",
    "https://openalex.org/W4297776519",
    "https://openalex.org/W4313479509",
    "https://openalex.org/W3118985401",
    "https://openalex.org/W4385218994",
    "https://openalex.org/W4383473194",
    "https://openalex.org/W4386122586",
    "https://openalex.org/W3017131514",
    "https://openalex.org/W3157327767",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2744403582",
    "https://openalex.org/W4282823458",
    "https://openalex.org/W4210255930",
    "https://openalex.org/W4386724197",
    "https://openalex.org/W4392737179",
    "https://openalex.org/W4384264317",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4382991888",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4389520362",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3106339673",
    "https://openalex.org/W3119940386",
    "https://openalex.org/W4389464005",
    "https://openalex.org/W4389431826",
    "https://openalex.org/W4384389802",
    "https://openalex.org/W2245005275",
    "https://openalex.org/W2794546026",
    "https://openalex.org/W4386165096"
  ],
  "abstract": "Large language models can help to compile content with a cultural theme. However, any information generated by large language models needs to be evaluated to see the truth/fact of the information generated. With many studies discussing the comparison of the capabilities of large language models, there is not much research that directly discusses the comparison of the performance of large language models in producing Indonesian cultural content. This research compares the correctness of the information generated by the large language model using the expert judgment method when creating Indonesian cultural content and its fine-tuning capabilities evaluated using BERTScore. The evaluation method was successfully applied and the results show that in this case, PaLM-2 included less misinformation while GPT-3 excelled in fine-tuning. Using the combination of expert judgment and BERTScore makes it possible to evaluate large language models and obtain additional valid training data to correct deficiencies. The results showed that PaLM-2 produced more valid content with a score of 27 points, while GPT-3 scored 8 points. For training on new datasets/fine-tuning, it was found that the GPT-3 language model was able to learn the dataset more quickly, with a time of 50 minutes and a cost of IDR 27,000, while PaLM-2 took 2 hours 10 minutes and a cost of IDR 1,377,204. For the training dataset evaluation results, GPT-3 is superior with an average of all scores reaching 0.85205. Meanwhile, the PaLM-2 Tuned Model got an average overall score of 0.78942. In this case, the GPT-3 Tuned Model is superior by 8 %. In practice, this method can be used if the assessment is descriptive and requires direct assessment from experts",
  "full_text": null,
  "topic": "Indonesian",
  "concepts": [
    {
      "name": "Indonesian",
      "score": 0.9356851577758789
    },
    {
      "name": "Palm",
      "score": 0.6905635595321655
    },
    {
      "name": "Content (measure theory)",
      "score": 0.5610029697418213
    },
    {
      "name": "Mathematics",
      "score": 0.25395098328590393
    },
    {
      "name": "Linguistics",
      "score": 0.24415737390518188
    },
    {
      "name": "Philosophy",
      "score": 0.1219380795955658
    },
    {
      "name": "Physics",
      "score": 0.09390521049499512
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113646",
      "name": "Universitas Bina Darma",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I147791725",
      "name": "Jambi University",
      "country": "ID"
    }
  ],
  "cited_by": 3
}