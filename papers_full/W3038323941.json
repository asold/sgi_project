{
  "title": "Birds of a Feather Flock Together: Satirical News Detection via Language Model Differentiation",
  "url": "https://openalex.org/W3038323941",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2352341956",
      "name": "Zhang Yigeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095316391",
      "name": "Yang Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147249798",
      "name": "Zhang Yifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747565272",
      "name": "Dragut, Eduard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287191029",
      "name": "Mukherjee, Arjun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2984541361",
    "https://openalex.org/W2149340438",
    "https://openalex.org/W2966295280",
    "https://openalex.org/W2038634595",
    "https://openalex.org/W2845859134",
    "https://openalex.org/W2252350410",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2953252284",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W2963942654",
    "https://openalex.org/W2132210327",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2251357059",
    "https://openalex.org/W2092939357",
    "https://openalex.org/W2516338333"
  ],
  "abstract": "Satirical news is regularly shared in modern social media because it is entertaining with smartly embedded humor. However, it can be harmful to society because it can sometimes be mistaken as factual news, due to its deceptive character. We found that in satirical news, the lexical and pragmatical attributes of the context are the key factors in amusing the readers. In this work, we propose a method that differentiates the satirical news and true news. It takes advantage of satirical writing evidence by leveraging the difference between the prediction loss of two language models, one trained on true news and the other on satirical news, when given a new news article. We compute several statistical metrics of language model prediction loss as features, which are then used to conduct downstream classification. The proposed method is computationally effective because the language models capture the language usage differences between satirical news documents and traditional news documents, and are sensitive when applied to documents outside their domains.",
  "full_text": "Birds of a Feather Flock Together:\nSatirical News Detection via\nLanguage Model Differentiation\nYigeng Zhang1, Fan Yang1, Yifan Zhang1, and Eduard Dragut2\nand Arjun Mukherjee1\n1 University of Houston, Houston, TX 77004, USA\n{yzhang168,fyang11,yzhang114}@uh.edu, arjun@cs.uh.edu\n2 Temple University, Philadelphia, PA 19122, USA\nedragut@temple.edu\nAbstract. Satirical news is regularly shared in modern social media because it\nis entertaining with smartly embedded humor. However, it can be harmful to so-\nciety because it can sometimes be mistaken as factual news, due to its deceptive\ncharacter. We found that in satirical news, the lexical and pragmatical attributes\nof the context are the key factors in amusing the readers. In this work, we propose\na method that differentiates the satirical news and true news. It takes advantage\nof satirical writing evidence by leveraging the difference between the prediction\nloss of two language models, one trained on true news and the other on satirical\nnews, when given a new news article. We compute several statistical metrics of\nlanguage model prediction loss as features, which are then used to conduct down-\nstream classiﬁcation. The proposed method is computationally effective because\nthe language models capture the language usage differences between satirical\nnews documents and traditional news documents, and are sensitive when applied\nto documents outside their domains.\nKeywords: Satirical news detection ·Text classiﬁcation ·Deception detection.\n1 Introduction\nSatirical news is a kind of literary work that consists of parodies of mainstream jour-\nnalism, mundane events, or other humor. In the modern world, satirical news can be\nharmful to a society because it is deceptive in nature and hard to distinguish on many\noccasions. The creative approach of satire is witty, metaphorical, and subtle and peo-\nple without a related cultural or contextual background may have difﬁculty in telling\nit apart from factual news items. Satirical news may have unintentional consequences\nsimilar to fake news [11] and, thus, investigating the methods of ﬁltering satirical news\nhas drawn people’s attention.\nIn recent years, there have been a surge of works on fake news detection; however,\nthe aim of producing news satire is not to contradict the truth and misleading people as\nfake news does, but rather to entertain as form of parody. Satirical news articles have\nthese characteristics:\narXiv:2007.02164v1  [cs.CL]  4 Jul 2020\n2 Y . Zhang et al.\n– Imaginative Content: Similar to fake news, satirical news also entails ﬁctional\ncontent [11]. Fake news, pretending to report a real story, is intended to deliver false\ninformation to mislead people. The fake stories are created seemingly reasonable,\nthus people who are fooled will take it as fact without being skeptical. Although\nsatirical news is also ﬁctional, the purpose of creating satirical fake content is to\nmake the readers aware of an irony and the humor behind it.\n– Seemingly Formal and Serious Writing Style: Satirical news is usually written\nin a formal form and subjective tone in the same way as true news. Yang et al.\nreported that news satire is written in subjective tones, suggesting a formal form and\nmimicking true news [16]. This makes satire hold a kind of humor by contrasting\nthe serious writing style and ridiculous story.\n– Contradicting Common Sense: Once a reader deciphers the irony and deadpan\nhumor in a satire article, the reader will realize its ridiculousness since the con-\ntent violates common sense. Satirical news story sometimes combines irrelevant\nsubjects together to create unexpectedness and humor [10]. For example:\n‘Father spends joyful afternoon throwing son around backyard. ’—Onion\nThis sentence gives a sense of ridiculousness because the object ‘son’ is not used\nto be ‘thrown’ for fun around the backyard. Sometimes the stories are made up of\nimpossible events that will never happen in the real world if one knows the context.\nFor example:\n‘Vice President Mike Pence reportedly visited his conversion therapist Thurs-\nday for a routine gay-preventative checkup. ’—Onion\n– Humoristic and Amusing: The purpose of creating news satire is to criticize or\ncomment on some social affair in a humorous way [1]. The readers usually ﬁnd\nfunny metaphors and comical stories in it. This entertaining property increases the\npopularity of news satire in social media outlets.\nAlthough this seemingly formal writing style makes satirical news hard to detect,\nthe breach of lexical and pragmatical information can address the problem. Since satir-\nical news usually makes up stories with preposterous content, it is easy for people with\ncorresponding knowledge and cultural background to recognize it. Using a similar idea\nas how people discern illogical information, we can leverage the computational models\nwhich have the ability to gain domain knowledge to ‘judge’ like human beings. Lan-\nguage models (LM) is one possible solution because an LM encodes knowledge from\nthe context it is trained upon [15]. In this work, by making use of the characteristics of\nthe satirical content, we propose a new method to detect satirical news, where language\nmodels play an important part.\n2 Related Work\nIn recent years, there have been many works in news and sentiment analysis [5][14]\n[13][4]. One particular area has been fake and satirical news detection using machine\nlearning methods. Burfoot & Baldwin conducted a research on differentiating satire\nnews and true news using SVM with targeted lexical features and semantic validity on\na 4000-article dataset [1]. Rubin et al. categorized news satire as a kind of ‘humorous\nTitle Suppressed Due to Excessive Length 3\nfakes’ in deceptive news [12]. They proposed an SVM-based algorithm in satire detect-\ning and tested with 360 pieces of news [11]. Yang et al. built a dataset for satirical news\ndetection [16], which contains a much larger number of satirical and true news from\nvarious sources. They also proposed a method with Hierarchical Attention Networks\nwith many linguistic features such as writing stylistic features and readability features.\nThey argued that satirical cues often appear in certain paragraphs instead of the whole\narticle. De Sarkar et al. works at the sentence level embeddings and document level\nembeddings, and they also use many syntax features such as part-of-speech tags and\nnamed entity features [3].\nCompared to the existed satirical news detection works, we propose an approach\nthat seeks to capture the characteristics mentioned in Section 1 about news satire and\nuse it to distinguish it from other news genre. Our method utilizes two separately trained\nlanguage models from satirical news and true news as the ‘brain’ of domain knowledge.\nThen we obtain a series of satire/non-satire measurement scores—surprise scores for\neach news article from the language models. With these scores as features, a classiﬁer\nis trained for future prediction. As the idiom ‘Birds of a feather ﬂock together’ goes,\nwe ﬁnd that news of the same category will have similar feature representations while\nnews from different categories will show a signiﬁcant difference when viewed through\ntheir corresponding language models. The dataset we use for satirical news detection is\nfrom the work by Yang et al. [16]. Experimental results on this dataset show that our\nmethod can achieve state-of-the-art performance on the validation dataset and compet-\nitive results on the test data. Moreover, it only uses classical neural language models\nwith shallow layers and a small number of features from several basic statistics of the\nlanguage model output instead of sophisticated feature engineering.\n3 Method\nIn this section, we ﬁrst present the underlying hypothesis of our approach. Then we\npresent our model and classiﬁcation pipeline.\n3.1 Hypothesis\nSatirical news is written in a seemingly formal and serious way just like true news.\nBut people can distinguish satirical news from true news because its content ‘violates\npeople’s common sense’. As discussed in Section 1, satirical content contains stories\nwhich can hardly happen in real life. Looking at this phenomenon at the language level,\nthe pairing of subject-object and the word collocations in a large number of satirical\nnews articles appears to be signiﬁcantly different from a true news collection. Since\nlanguage models (LM) encodes knowledge from the data they are trained from, they\nare expected to act differently (i.e., present different scores) when fed with uncommon\ntext. We expect this to be reﬂected in entropy (logarithm of perplexity) when a pre-\ntrained true news LM is used to ﬁt true news from satire news.\nWe assume that, because of the lexical and pragmatical differences, when true/satire\nnews is applied to a pre-trained language model, news samples from a different category\nwill result in an obvious difference as judged based on the output of the language model.\n4 Y . Zhang et al.\nEncoder \n2-Layer \nLSTM \nDecoder \nOne-hot\nWord Index\nWord\nEmbedding\nRecurrent\nModule\nProjection\nOutput\nFig. 1: Basic encoder-decoder LM.\nTrue\tNews\t\nLanguage\tModel\nSatirical\tNews\t\nLanguage\tModel\nNews\tarticle\nConcat\nClassifier\nTrain/Test\nSentence\t1\nSentence\t2\n...\nTrue\tScore\nStatistics\nSatire\tScore\nStatistics\nFeature\tVector Fig. 2: Pipeline.\nBy applying the Wilcoxon signed-rank test onto the output pairs from different language\nmodels, we expect to prove the result of signiﬁcant difference [7].\nHere we deﬁne a metric named surprise score. A surprise score is the arithmetic\nmean of the entropy loss values on token-level that the LM produces when fed in with a\npiece of sequential text. The more distinct the new text piece (from the training data) is,\nthe higher the surprise score obtained with the LM we expect to be. For example, a piece\nof satirical news will have a higher surprise score than a true news after being applied\nto a language model trained on true news documents. By leveraging this surprise scores\nas features, we can perform the classiﬁcation effectively.\n3.2 Word-Level Neural Language Model with LSTM\nLanguage model (LM) can be deﬁned as a probability distribution over a sequence of\nwords. It is usually trained to describe the likelihood of occurrence of one next wordwt\nin a sequence by having seen the previous k words of the context:\np (wt|wt−k−1:t−1) = LM (wt−k−1:t−1) (1)\nwhere ∀wt ∈V , the ﬁnite vocabulary of the context.\nThe recurrent neural network based language model (RNN LM) was ﬁrstly pro-\nposed by Mikolov et al. [9]. The neural language model used in this work follows a\nbasic encoder-decoder language model with LSTM as the recurrent module (shown in\nFigure 1). The prediction procedure is derived by:\nwt = Ly∗\nt−1 (2)\nht = f (wt, ht−1) (3)\nyt = Wht + b (4)\nThe matrix L ∈Rdx×|V |is for word embedding.f refers to the LSTM module. The\nW is the wt matrix. After linear transformation from the decoder, outputyt is obtained.\nThe cross-entropy loss on sequence is calculated as below. In this loss function, x is\na raw output score vector from the linear projection layer for each class, and i is the\ndictionary index of each corresponding word, which indicates the class index number.\nTitle Suppressed Due to Excessive Length 5\nL(ytarget, ˆ y) = −\nV∑\ni=1\nytarget\ni log ˆyi (5)\n3.3 Pipeline\nWe favor a less complex classiﬁcation pipeline in this work. The input news article is\nfed as a word sequence into the two language models, which are each trained on true\nnews and satirical news documents, respectively. From each language model, we get\nan output sequence of loss numbers corresponding to each sentence of one article. Now\neach article of n sentences is represented with two corresponding sequences, describing\nthe impression of true and satirical news language models:\nArticletrue\ni = scoret\n1, scoret\n2, scoret\n3, ..., scoret\nn\nArticlesatire\ni = scores\n1, scores\n2, scores\n3, ..., scores\nn\nWe calculate the mathematical statistics: sample sizeN, arithmetic mean ¯X, me-\ndian ˜X, sample variance s2, and range R, of each sequence of surprise scores as\nsatire/true features, where median is the middle value and sample sizerepresents the\nnumber of sentences of each news article. Then we concatenate all of the features as an\n9-dimension vector to represent one news article. For each article, we ﬁnally obtained\na low-dimension feature vector:\n[N, ¯Xt, ˜Xt, s2\nt , Rt, ¯Xs, ˜Xs, s2\ns, Rs] (6)\nwhere the subscript s (for satirical) or t (for true) indicates which language model the\ncorresponding feature comes from. An SVM classiﬁer is trained and tested using the\nabove feature vectors. Figure 2 shows the proposed classiﬁcation pipeline.\n4 Experiment and Evaluation\nIn this section, we ﬁrst present the dataset and hypothesis testing. Then we introduce\nthe implementation details. Finally, we dive into the evaluation and analysis.\n4.1 Dataset\nThe news dataset we use in this paper is from the work by Yang et al. [16]. The news\narticles are crawled from both satirical news providers (such as Onion) and true news\nwebsites (such as CNN). The news headline, creation time, and author information are\nremoved in order not to introduce many obvious features of a news source.\nIn this work, we concentrate on the binary classiﬁcation task. The usage of this news\ndataset could be found in Table 1. We divide the original training data from [16] into two\nparts: the part to train the two language models and the part to train the classiﬁer. The\nformer part takes 2/3 of the original training data, while the latter takes the rest 1/3. The\ndata for validation and test remains the same. The part of data for training the classiﬁer\n6 Y . Zhang et al.\nTable 1: The usage distribution of the News dataset.\nOriginal Train Train LM Train SVM Validation Test\nTrue 101268 67512 33756 33756 33756\nSatire 9538 6358 3180 3103 3608\nis roughly the same size as the validation data and test data. This practice of division\nis to ensure the balance of each part of data usage and also to prove the effectiveness\nand generalization ability of the classiﬁer. Moreover, this dataset is from various news\nsources – Train: Onion, the Spoof ; Validation: Daily Current, Daily Report , Endur-\ning Vision, Gomerblog, National Report, Satire Tribune, Satire Wire, Syruptrap, and\nUnconﬁrmed Source; Test: Satire World, Beaverton and Ossurworld, which makes the\ndata distribution and its characteristics not uniform. In this case, the adaptability of the\nproposed method will be tested since the language models and the classiﬁer are trained\non different news sources. Therefore, because of the diversity of the news sources in this\ndataset, it can ﬁnally help to disclose whether our method has the ability to generally\ncatch the knowledge difference behind satirical content and true content.\n4.2 Statistical Hypothesis Testing\nAs mentioned in section 3, we should prove that the statistics from true/satire surprise\nscores of each given article produced from both true and satire LMs have a signiﬁcant\ndifference. Therefore, we need statistical hypothesis testing to examine the score pairs\nare deemed statistically signiﬁcant.\nNull Hypothesis 0 (H0): For all news articles, each statistic feature calculated out of\nthe surprise scores from the true LM has no statistically signiﬁcant difference with the\ncorresponding one from the satire LM pairwisely.\nHere we use the Wilcoxon signed-rank test [7], which is often used to determine\nwhether two related samples have the same distribution or not. By applying this test on\nevery pair of statistic features, we obtained the p-value ≪0.001 from both satire and\ntrue news sample test pairs, which means the Null Hypothesis H0 is rejected and there\nis a signiﬁcant difference between true/satire surprise score statistics from both true and\nsatire LMs.\n4.3 Implementation Details\nIn this work, we implement the pipeline using typical neural network modules and\nmediocre settings in each part, comparing to other classiﬁcation methods with complex\nnetwork structures or delicate embeddings. For the language model part, the size of the\nword embeddings of the encoder is 200. The RNN module is a typical 2-layer LSTM\nwith 200 hidden units per layer. A dropout rate of 0.2 is applied when training the\nlanguage model. Both of the true news and satirical news LM are trained with 6 epochs.\nFor the classiﬁer, a typical SVM3 with linear/polynomial kernel is used.\n3 sklearn.svm.SVC\nTitle Suppressed Due to Excessive Length 7\nTrue News Sentence/Paragraph Examples True LM Score Satire LM Score\nWe are not going to comment on the timeline of the evidence that comes in, Lynch said. 3.458497763 4.547165394\nDaley Blind rescued Manchester United with a strike deep into injury time on Sunday 5.857816696 7.925971508\nIt is risky. Both situations defy easy solution. The Iranians have changed their tone but must \ngo a long way to prove they are changing their intent, embracing transparency and \nadhering to international standards. Even if they do, if they continue to support terrorist \ngroups like Hezbollah they will be at loggerheads with the United States.\n4.679250264 5.665326705\nSatirical News Sentence/Paragraph Examples True LM Score Satire LM Score\nHis job is raking the ocean waves flat so the sun can shine through. 6.591171265 6.478577614\nAfter moving several shelves of canned goods to his garage workbench area, Svoboda \nattempted to break open a can of lentil soup using a pair of needle nose pliers and a \nblowtorch.\n7.182848454 6.970126152\nAlthough not a regular reader of Der Spiegel, Matherson said he gleaned information about \nthe publication from the celebrity news program Insider, which he typically watches alone in \nhis room while eating cold cereal.\n6.212657452 6.450323237\nFig. 3: Some examples of some selected news sentences/paragraphs with their surprise\nscores speciﬁed by the true news LM and the satirical news LM.\n4.4 Evaluation Results and Analysis\nTwo different language models of true/satirical news giving surprise scores to each sen-\ntences, which ﬁnally forms a feature vector for one piece of news. Figure 3 shows some\nexample sentences with their surprise scores on two kinds of LMs. For the true news\nsamples, the surprise scores are generally lower than the satirical news samples. Mean-\nwhile, the scores from true news LM are lower than they are from the satire LM, which\nconﬁrmed our hypothesis. For the satirical news samples, with their characteristics, the\nscores not only rise higher but become difﬁcult to distinguish on both sides. This is also\nabstractly reﬂected in Figure 5.\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.012.5\n10\n5\n0510152025\n3\n2\n1\n0\n1\n2\n3\nFig. 4: A t-SNE visualization of 1000\nrandomly sampled feature vectors of\ntrue news (red)/satirical news (blue)\nfrom Train SVM part of data.\nNscaled Xt\n Xt\n s2\nt Rt Xs\n Xs\n s2\ns Rs\nFeature\n0\n1\n2\n3\n4\n5\n6Statistic Metrics\nArithmetic Mean with Standard Deviation of the Features\nTrue\nSatire\nFig. 5: An illustration of the compar-\nison of avg. and std on each feature\nbetween true news (red) and satirical\n(blue). Feature N is scaled by 0.1.\nThe statistics calculated from the score sequences depict the different behavior on\ndifferent LMs from a higher perspective. Figure 4 shows that by using the proposed\nfeature vector, it is clear to see the separation of true news samples and satirical news\n8 Y . Zhang et al.\nTable 2: Performance results with increasing number of feature/feature-pairs. 1: Mean,\n2: Mean + Median, 3: Mean + Median + Sample Variance, 4: Mean + Median + Sample\nVariance + Range, 5: Mean + Median + Sample Variance + Range + Sample Size.\nValidation Acc Pre Rec F1 Test Acc Pre Rec F1\n1 96.35 92.01 82.83 86.74 1 94.82 90.51 77.37 82.35\n2 96.39 92.08 83.09 86.93 2 94.76 90.45 77.04 82.08\n3 96.90 92.61 86.32 89.16 3 95.23 91.32 79.35 84.06\n4 97.38 93.17 89.26 91.10 4 96.06 92.23 83.92 87.50\n5 97.97 94.55 92.00 93.23 5 96.82 93.67 87.34 90.19\nTable 3: Experimental result comparison of four different methods on satirical news\ndetection task in Accuracy, Precision, Recall, and F1 Score. Results being compared\nare originally listed in the work by Yang et al. [16] and De Sarkar et al. [3].\nValidation Test\nMethod Acc Pre Rec F1 Acc Pre Rec F1\nRubin et al. [11] 97.73 90.21 81.92 85.86 97.79 93.47 82.95 87.90\nYang et al. [16] 98.54 93.31 89.01 91.11 98.39 93.51 89.5 91.46\nDe Sarkar et al. [3] 98.18 94.15 86.55 90.19 98.31 93.45 86.01 89.57\nThis work SVM-Linear 97.93 94.46 91.79 93.07 96.67 93.41 86.62 89.65\nThis work SVM-Poly 97.97 94.55 92.00 93.23 96.82 93.67 87.34 90.19\nsamples even in 3D t-SNE illustration. Table 2 providing a further evidence: with fea-\ntures incrementing, the classiﬁcation performance improves accordingly. Meanwhile,\nit also reﬂects the feature importance when controlling different selection of features.\nHere we report the experimental results on both validation dataset and test dataset, in or-\nder to present the upper bound as well as the generalization performance of this method.\nThe results shows a coherent behavior on both dataset as the number of feature/feature-\npairs increments. The features generated using this method can reﬂect some statistical\ndifferences between two kinds of news data. Figure 5 is an illustration from a macro\nperspective using the arithmetic mean for each feature. Visible difference on these two\nseries of data could be found from the histogram plot.\nA further consideration is concerning the level of importance of each investigated\nfeature. Here we look into a uni-variate feature selection metric mutual information\n(MI) [6], which is described in Eq. 2.28 of [2]:\nI(X; Y ) =\n∑\ny∈Y\n∑\nx∈X\np(x, y) log\n( p(x, y)\np(x)p(y)\n)\n(7)\nwhere feature X and target Y are discrete random variables. The MI score here depicts\nhow much the uncertainty of the classiﬁcation is eliminated when given feature X. We\ncalculate the MI scores between each of the features and the classiﬁcation target. Thus\nthe higher the MI score is, the more contribution of the corresponding feature will make\nin classiﬁcation.\nThe MI scores are illustrated in pairs in Figure 6 for the training LM data, validation\ndata, and test data. By interpreting the scores, we found that features such as N and\nall of the features from true LM are of signiﬁcant importance on validation data, and\nTitle Suppressed Due to Excessive Length 9\nRs for all dataset play a great role in making decisions in determining news category,\nwhile features such as ¯Xs and ˜Xs of validation data are obviously of less utility for\nclassiﬁcation. Although as shown in Figure 5 there is a less signiﬁcant difference in\nthe feature pair sample variance, our model has the potential to distinguish and utilize\nthese features. Therefore, the contribution of each feature varies in the classiﬁcation on\ndifferent dataset. Furthermore, there is also a visible complementarity shown on each\nfeature item in pairs: if one feature from true news LM has a low MI score, its paired\ncorresponding feature will raise.\nN Xt\n Xt\n s2\nt Rt Xs\n Xs\n s2\ns Rs\nFeature\n10 3\n10 2\n10 1\nMI score\nSentence Count\nFrom True LM\nFrom Satire LM\nFig. 6: An illustration of Mutual Information feature analysis on the feature sample size,\nwhich is sentence (or paragraph) count N (yellow) and paired features from true news\nLM (red)/satirical news LM (blue). Each feature of Train/Validation/Test data appears\nfrom left to right accordingly across each group of the histograms.Y -axis is log-scaled.\nAs shown in Table 3, our method outperforms the other three methods in Rubin\net al. [11], Yang et al. [16] and De Sarkar et al. [3] precision, recall, and F1 score on\nthe validation dataset and achieves competitive results on the test dataset. Upon further\ninvestigation of the corpora we use, we found that one of the satirical news source\nproposed by Yang et al. [16] and De Sarkar et al. [3],Ossurworld4 from the test dataset,\nis questionable to be categorized and utilized as ‘satirical news’. This website is a blog\nof Irreverence, Irony, Insouciance & Iconoclasmas mentioned on their headline instead\nof news. Although it truly focuses on irony and publishes some satire content and fake\nnews, a considerable number of blog posts such as ironic ﬁlm reviews are neither satire\nnor in the form of news. Therefore, it is likely some of this irrelevance in data results in\na negative impact on the performance of our method.\nMoreover, our method is not inﬂuenced by the potential problems mentioned by\nMcHardy et al. [8] in Section 2, because the language models outputs, surprise scores,\nare just numerical values, which will not contain any semantic information or learn any\nﬁne-grained details like the models [16] and [3] possibly did. Also, the satire training\nand testing data are from diverse sources. For the satire part, the LM training and clas-\nsiﬁer data are from Onion and the Spoof. The validation data and testing data are from\nmany satire sources listed in Section 4. By using data from different news sources, the\nobjectivity of the data and the method can be mutually guaranteed.\n4 https://ossurworld.com/\n10 Y . Zhang et al.\n5 Conclusion\nInspired by the idiom ‘Birds of a feather ﬂock together’, we proposed a new method\nfor satirical news classiﬁcation that leverages language model output distribution diver-\ngence. By leveraging the surprise scores from different language models, the satirical\nnews was differentiated from true news articles effectively. This method is not only free\nfrom extracting numerous linguistic features as previous works did, but also it does not\nrequire any sophisticated neural network structures or advanced embeddings. More im-\nportantly, this proposed method proves the value of the selected statistical features from\na language model output, and shows the effectiveness of these features in depicting the\ncharacteristics of the corresponding document category.\nThis work is supported in part by the U.S. NSF grants 1838147 and 1838145. We\nalso thank anonymous reviewers for their helpful feedback.\nReferences\n1. Burfoot, C., Baldwin, T.: Automatic satire detection: Are you having a laugh? In: Proceed-\nings of the ACL-IJCNLP 2009 conference short papers. pp. 161–164 (2009)\n2. Cover, T.M., Thomas, J.A.: Elements of information theory (2006)\n3. De Sarkar, S., Yang, F., Mukherjee, A.: Attending sentences to detect satirical fake news. In:\nProceedings of COLING 2018. pp. 3371–3380 (2018)\n4. Dragut, E.C., Yu, C.T., Sistla, A.P., Meng, W.: Construction of a sentimental word dictionary.\nIn: CIKM. pp. 1761–1764 (2010)\n5. He, L., Han, C., Mukherjee, A., Obradovic, Z., Dragut, E.C.: On the dynamics of user en-\ngagement in news comment media. Wiley Interdiscip. Rev. Data Min. Knowl. Discov.10(1)\n(2020)\n6. Kraskov, A., St ¨ogbauer, H., Grassberger, P.: Estimating mutual information. Physical review\nE 69(6), 066138 (2004)\n7. McDonald, J.H.: Handbook of biological statistics, vol. 2 (2009)\n8. McHardy, R., Adel, H., Klinger, R.: Adversarial training for satire detection: Controlling for\nconfounding variables. In: Proceedings of NAACL-HLT 2019. pp. 660–665 (2019)\n9. Mikolov, T., Karaﬁ ´at, M., Burget, L., ˇCernock`y, J., Khudanpur, S.: Recurrent neural network\nbased language model. In: Proceedings of Interspeech 2010 (2010)\n10. Reyes, A., Rosso, P., Buscaldi, D.: From humor recognition to irony detection: The ﬁgurative\nlanguage of social media. Data & Knowledge Engineering 74, 1–12 (2012)\n11. Rubin, V ., Conroy, N., Chen, Y ., Cornwell, S.: Fake news or truth? using satirical cues to de-\ntect potentially misleading news. In: Proceedings of the second workshop on computational\napproaches to deception detection. pp. 7–17 (2016)\n12. Rubin, V .L., Chen, Y ., Conroy, N.J.: Deception detection for news: three types of fakes.\nIn: Proceedings of the 78th ASIS&T Annual Meeting: Information Science with Impact:\nResearch in and for the Community. p. 83. American Society for Information Science (2015)\n13. Schneider, A.T., Dragut, E.C.: Towards debugging sentiment lexicons. In: ACL. pp. 1024–\n1034 (2015)\n14. Stanojevic, M., Alshehri, J., Dragut, E.C., Obradovic, Z.: Biased news data inﬂuence on\nclassifying social media posts. In: NEwsIR@SIGIR. vol. 2411, pp. 3–8 (2019)\n15. Trinh, T.H., Le, Q.V .: A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847 (2018)\n16. Yang, F., Mukherjee, A., Dragut, E.: Satirical news detection and analysis using attention\nmechanism and linguistic features. In: Proceedings of EMNLP 2017. pp. 1979–1989 (2017)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7273167967796326
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6386477947235107
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6263697147369385
    },
    {
      "name": "Key (lock)",
      "score": 0.5817688703536987
    },
    {
      "name": "Language model",
      "score": 0.4805334210395813
    },
    {
      "name": "Natural language processing",
      "score": 0.41250497102737427
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39223772287368774
    },
    {
      "name": "History",
      "score": 0.16426050662994385
    },
    {
      "name": "Computer security",
      "score": 0.0775892436504364
    },
    {
      "name": "Mathematics",
      "score": 0.07081040740013123
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I44461941",
      "name": "University of Houston",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801004183",
      "name": "Temple College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I84392919",
      "name": "Temple University",
      "country": "US"
    }
  ]
}