{
  "title": "Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators",
  "url": "https://openalex.org/W3176468671",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5066202892",
      "name": "Peiyu Liu",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5090248956",
      "name": "Ze-Feng Gao",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5037145565",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5002437867",
      "name": "Z. Y. Xie",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5047454826",
      "name": "LU Zhong-yi",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5025631695",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W204991446",
    "https://openalex.org/W1798945469",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3103616906",
    "https://openalex.org/W3103454714",
    "https://openalex.org/W2069840277",
    "https://openalex.org/W2524428287",
    "https://openalex.org/W3101731278",
    "https://openalex.org/W4297781745",
    "https://openalex.org/W1993482030",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3116594510",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3105645800",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W2014237348",
    "https://openalex.org/W3110271845",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W3104371525",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W2619959423",
    "https://openalex.org/W3104223418",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970213198",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952902402",
    "https://openalex.org/W2919207648",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2765932895",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3035281298",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W3023285645",
    "https://openalex.org/W3101066076",
    "https://openalex.org/W1963826206",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2258054274",
    "https://openalex.org/W3104929588",
    "https://openalex.org/W2022419246"
  ],
  "abstract": "Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan Xie, Zhong-Yi Lu, Ji-Rong Wen. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5388‚Äì5398\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n5388\nEnabling Lightweight Fine-tuning for Pre-trained Language Model\nCompression based on Matrix Product Operators\nPeiyu Liu1,4‚àó, Ze-Feng Gao2,1‚àó, Wayne Xin Zhao1,4,5‚Ä†,\nZ.Y. Xie2, Zhong-Yi Lu2‚Ä† and Ji-Rong Wen1,3,4\n1Gaoling School of ArtiÔ¨Åcial Intelligence, Renmin University of China\n2Department of Physics, Renmin University of China\n3 School of Information, Renmin University of China\n4Beijing Key Laboratory of Big Data Management and Analysis Methods\n5Beijing Academy of ArtiÔ¨Åcial Intelligence, Beijing, 100084, China\n{liupeiyustu,zfgao,qingtaoxie,zlu,jrwen}@ruc.edu.cn, batmanÔ¨Çy@gmail.com\nAbstract\nThis paper presents a novel pre-trained lan-\nguage models (PLM) compression approach\nbased on the matrix product operator (short\nas MPO) from quantum many-body physics.\nIt can decompose an original matrix into cen-\ntral tensors (containing the core information)\nand auxiliary tensors (with only a small pro-\nportion of parameters). With the decomposed\nMPO structure, we propose a novel Ô¨Åne-tuning\nstrategy by only updating the parameters from\nthe auxiliary tensors, and design an optimiza-\ntion algorithm for MPO-based approximation\nover stacked network architectures. Our ap-\nproach can be applied to the original or the\ncompressed PLMs in a general way, which\nderives a lighter network and signiÔ¨Åcantly re-\nduces the parameters to be Ô¨Åne-tuned. Exten-\nsive experiments have demonstrated the effec-\ntiveness of the proposed approach in model\ncompression, especially the reduction in Ô¨Åne-\ntuning parameters (91% reduction on average).\nThe code to reproduce the results of this pa-\nper can be found at https://github.com/\nRUCAIBox/MPOP.\n1 Introduction\nRecently, pre-trained language models (PLMs) (De-\nvlin et al., 2019; Peters et al., 2018; Radford et al.,\n2018) have made signiÔ¨Åcant progress in various\nnatural language processing tasks. Instead of train-\ning a model from scratch, one can Ô¨Åne-tune a PLM\nto solve some speciÔ¨Åc task through the paradigm\nof ‚Äúpre-training and Ô¨Åne-tuning‚Äù.\nTypically, PLMs are constructed with stacked\nTransformer layers (Vaswani et al., 2017), involv-\ning a huge number of parameters to be learned.\nThough effective, the large model size makes it im-\npractical for resource-limited devices. Therefore,\nthere is an increasing number of studies focused\n‚àóAuthors contributed equally.\n‚Ä†Corresponding author.\non the parameter reduction or memory reduction\nof PLMs (Noach and Goldberg, 2020), including\nparameter sharing (Lan et al., 2020), knowledge\ndistillation (Sanh et al., 2019), low-rank approxima-\ntion (Ma et al., 2019) and data quantization (Hubara\net al., 2017). However, these studies mainly apply\nparameter reduction techniques to PLM compres-\nsion, which may not be intrinsically appropriate for\nthe learning paradigm and architecture of PLMs.\nThe compressed parameters are highly coupled so\nthat it is difÔ¨Åcult to directly manipulate different\nparts with speciÔ¨Åc strategies. For example, most\nPLM compression methods need to Ô¨Åne-tune the\nwhole network architecture, although only a small\nproportion of parameters will signiÔ¨Åcantly change\nduring Ô¨Åne-tuning (Liu et al., 2020).\nIn this paper, we introduce a novel matrix prod-\nuct operator (MPO) technique from quantum many-\nbody physics for compressing PLMs (Gao et al.,\n2020). The MPO is an algorithm that factorizes\na matrix into a sequential product of local tensors\n(i.e., a multi way array). Here, we call the tensor\nright in the middle as central tensor and the rest\nas auxiliary tensors. An important merit of the\nMPO decomposition is structural in terms of infor-\nmation distribution: the central tensor with most\nof the parameters encode the core information of\nthe original matrix, while the auxiliary tensors with\nonly a small proportion of parameters play the role\nof complementing the central tensor. Such a prop-\nerty motivates us to investigate whether such an\nMPO can be applied to derive a better PLM com-\npression approach: can we compress the central\ntensor for parameter reduction and update the aux-\niliary tensors for lightweight Ô¨Åne-tuning? If this\ncould be achieved, we can derive a lighter network\nmeanwhile reduce the parameters to be Ô¨Åne-tuned.\nTo this end, we propose an MPO-based com-\npression approach for PLMs, called MPOP. It is\ndeveloped based on the MPO decomposition tech-\n5389\nnique (Gao et al., 2020; Pirvu et al., 2010). We\nhave made two critical technical contributions for\ncompressing PLMs with MPO. First, we introduce\na new Ô¨Åne-tuning strategy that only focuses on the\nparameters of auxiliary tensors, so the number of\nÔ¨Åne-tuning parameters can be largely reduced. We\npresent both theoretical analysis and experimental\nveriÔ¨Åcation for the effectiveness of the proposed\nÔ¨Åne-tuning strategy. Second, we propose a new\noptimization algorithm, called dimension squeez-\ning, tailored for stacked neural layers. Since main-\nstream PLMs usually consist of multiple Trans-\nformer layers, this will produce accumulated re-\nconstruction error by directly applying low-rank\napproximation with MPO at each layer. The di-\nmension squeezing algorithm is able to gradually\nperform the dimension truncation in a more sta-\nble way so that it can dramatically alleviate the\naccumulation error in the stacked architecture.\nTo our knowledge, it is the Ô¨Årst time that MPO\nis applied to the PLM compression, which is well\nsuited for both the learning paradigm and the archi-\ntecture of PLMs. We construct experiments to eval-\nuate the effectiveness of the proposed compression\napproach for ALBERT, BERT, DistillBERT and\nMobileBERT, respectively, on GLUE benchmark.\nExtensive experiments have demonstrated the ef-\nfectiveness of the proposed approach in model com-\npression, especially dramatically reducing the Ô¨Åne-\ntuning parameters (91% reduction on average).\n2 Related Work\nWe review the related works in three aspects.\nPre-trained Language Model Compression .\nSince the advent of large-scale PLMs, several vari-\nants were proposed to alleviate its memory con-\nsumption. For example, DistilBERT (Sanh et al.,\n2019) and MobileBERT (Sun et al., 2020c) lever-\naged knowledge distillation to reduce the BERT\nnetwork size. SqueezeBERT (Iandola et al., 2020)\nand Q8BERT (Zafrir et al., 2019) adopted special\ntechniques to substitute the operations or quantize\nboth weights and activations. ALBERT (Lan et al.,\n2020) introduced cross-layer parameter sharing and\nlow-rank approximation to reduce the number of\nparameters. More studies (Jiao et al., 2020; Hou\net al., 2020; Liu et al., 2020; Wang et al., 2020;\nKhetan and Karnin, 2020; Xin et al., 2020; Pappas\net al., 2020; Sun et al., 2020a) can be found in the\ncomprehensive survey (Ganesh et al., 2020).\nTensor-based Network Compression. Tensor-\nbased methods have been successfully applied to\nneural network compression. For example, MPO\nhas been utilized to compress linear layers of deep\nneural network (Gao et al., 2020). Sun et al.\n(2020b) used MPO to compress the LSTM model\non acoustic data. Novikov et al. (2015) coined\nthe idea of reshaping weights of fully-connected\nlayers into high-dimensional tensors and represent-\ning them in Tensor Train (TT) (Oseledets, 2011)\nformat, which was extended to other network ar-\nchitectures (Garipov et al., 2016; Yu et al., 2017;\nTjandra et al., 2017; Khrulkov et al., 2019). Ma\net al. (2019) adopted block-term tensor decomposi-\ntion to compress Transformer layers in PLMs.\nLightweight Fine-tuning.In the past, lightweight\nÔ¨Åne-tuning was performed without considering pa-\nrameter compression. As a typical approach, train-\nable modules are inserted into PLMs. For example,\na ‚Äúside‚Äù network is fused with PLM via summation\nin (Zhang et al., 2020), and adapter-tuning inserts\ntask-speciÔ¨Åc layers (adapters) between each layer\nof PLMs (Houlsby et al., 2019; Lin et al., 2020;\nRebufÔ¨Å et al., 2017). On the contrary, several stud-\nies consider removing parameters from PLMs. For\nexample, several model weights are ablated away\nby training a binary parameter mask (Zhao et al.,\n2020; Radiya-Dixit and Wang, 2020).\nOur work is highly built on these studies, while\nwe have a new perspective by designing the PLM\ncompression algorithm, which enables lightweight\nÔ¨Åne-tuning. It is the Ô¨Årst time that MPO is applied\nto PLM compression, and we make two major tech-\nnical contributions for achieving lightweight Ô¨Åne-\ntuning and stable optimization.\n3 Preliminary\nIn this paper, scalars are denoted by lowercase let-\nters (e.g., a), vectors are denoted by boldface low-\nercase letters ( e.g., v), matrices are denoted by\nboldface capital letters (e.g., M), and high-order\n(order three or higher) tensors are denoted by bold-\nface Euler script letters (e.g., T). An n-order tensor\nTi1,i2,...in can be considered as a multidimensional\narray with nindices {i1,i2,...,i n}.\nMatrix Product Operator. Originating from\nquantum many-body physics, matrix product op-\nerator (MPO) is a standard algorithm to factorize\na matrix into a sequential product of multiple lo-\ncal tensors (Gao et al., 2020; Pirvu et al., 2010).\n5390\nMPO\nùëë!\nùëë\"\n#\nùëë$\nùëë$ùëé$\nùëé# ùëë#\nùëë# ùëë!ùëé! ùëé% ùëé&\nùëë! ùëë%\nùëë%\nTruncate dimension ùëë# > ùëë\"\n#\nùëë!\nùëë%\nùëë%\nùëë$\nùëë$\nùëë\"\n#\nùëÄ$√ó&\nùëé$\nùëé# ùëé! ùëé% ùëé&\nùê¥$ ùê¥# ùê¥! ùê¥%\nùê¥%ùê¥!ùê¥#\n\"ùê¥$\nùê∂\"\nùê∂\nFigure 1: MPO decomposition for matrix MI√óJ with Ô¨Åve local tensors, where ‚àèn\nk=1 ik = I,‚àèn\nk=1 jk = J, and\nak = ik √ójk (n = 5 here). Auxiliary tensors ( {Ai}4\ni=1) and central tensor ( C) are marked in blue and orange,\nrespectively. Dash line linking adjacent tensors denotes virtual bonds.\nFormally, given a matrixM ‚ààRI√óJ, its MPO de-\ncomposition into a product of nlocal tensors can\nbe represented as:\nMPO (M) =\nn‚àè\nk=1\nT(k)[dk‚àí1,ik,jk,dk], (1)\nwhere the T(k)[dk‚àí1,ik,jk,dk] is a 4-order tensor\nwith size dk‚àí1 √óik √ójk √ódk in which ‚àèn\nk=1 ik =\nI,‚àèn\nk=1 jk = Jand d0 = dn = 1. We use the con-\ncept of bond to connect two adjacent tensors (Pirvu\net al., 2010). The bond dimension dk is deÔ¨Åned by:\ndk = min\n( k‚àè\nm=1\nim √ójm,\nn‚àè\nm=k+1\nim √ójm\n)\n. (2)\nFrom Eq. (2), we can see thatdk is going to be large\nin the middle and small on both sides. We present\na detailed algorithm for MPO decomposition in\nAlgorithm 1. In this case, we refer to the tensor\nright in the middle as central tensor, and the rest as\nauxiliary tensor. Figure 1 presents the illustration\nof MPO decomposition, and we use n= 5 in this\npaper.\nAlgorithm 1MPO decomposition for a matrix.\nInput: matrix M, the number of local tensors n\nOutput : MPO tensor list {T(k)}n\nk=1\n1: for k= 1 ‚Üín‚àí1 do\n2: M[I,J ] ‚àí‚ÜíM[dk‚àí1 √óik √ójk,‚àí1]\n3: UŒªV‚ä§= SVD (M)\n4: U[dk‚àí1 √óik √ójk,dk] ‚àí‚ÜíU[dk‚àí1,ik,jk,dk]\n5: T(k) := U\n6: M := ŒªV‚ä§\n7: end for\n8: T(n) := M\n9: Normalization\n10: return {T(k)}n\nk=1\nMPO-based Low-Rank Approximation. With\nthe standard MPO decomposition in Eq.(1), we can\nexactly reconstruct the original matrix Mthrough\nthe product of the derived local tensors. Follow-\ning (Gao et al., 2020), we can truncate the k-th\nbond dimension dk (see Eq. (1)) of local tensors\nto d‚Ä≤\nk for low-rank approximation: dk > d‚Ä≤\nk. We\ncan set different values for {dk}n\nk=1 to control the\nexpressive capacity of MPO-based reconstruction.\nThe truncation error induced by the k-th bond di-\nmension dk is denoted by œµk (called local trunca-\ntion error) which can be efÔ¨Åciently computed as:\nœµk =\ndk‚àë\ni=dk‚àíd‚Ä≤\nk\nŒªi, (3)\nwhere {Œªi}dk\ni=1 are the singular values of\nM[i1j1...ikjk,ik+1jk+1...injn].\nThen the total truncation error satisÔ¨Åes:\n‚à•M‚àíMPO(M)‚à•F ‚â§\nÓµ™Óµ´Óµ´‚àö\nn‚àí1‚àë\nk=1\nœµ2\nk. (4)\nThe proof can be found in the supplementary ma-\nterials 1. Eq. (1) indicates that the reconstruction\nerror is bounded by the sum of the squared local\ntruncation errors, which is easy to estimate in prac-\ntice.\nSuppose that we have truncated the dimensions\nof local tensors from{dk}n\nk=1 to {d‚Ä≤\nk}n\nk=1, the com-\npression ratio introduced by quantum many-body\nphysics (Gao et al., 2020) can be computed as fol-\nlows:\nœÅ=\n‚àën\nk=1 d‚Ä≤\nk‚àí1ikjkd‚Ä≤\nk‚àèn\nk=1 ikjk\n. (5)\n1https://github.com/RUCAIBox/MPOP\n5391\nLayers (0,1e-4] (1e-4,1e-3] (1e-3, ‚àû)\nWord embedding 0.66 0.26 0.09\nFeed-forward 0.09 0.64 0.27\nSelf-attention 0.09 0.64 0.27\nTable 1: Distribution of parameter variations for BERT\nwhen Ô¨Åne-tuned on SST-2 task.\nThe smaller the compression ratio is, the fewer pa-\nrameters are kept in the MPO representation. On\nthe contrary, the larger the compression ratio œÅis,\nand the more parameters there are, and the smaller\nthe reconstruction error is. When œÅ >1, it indi-\ncates the decomposed tensors have more parame-\nters than the original matrix.\n4 Approach\nSo far, most of pre-trained language models (PLM)\nare developed based on stacked Transformer lay-\ners (Vaswani et al., 2017). Based on such an archi-\ntecture, it has become a paradigm to Ô¨Årst pre-train\nPLMs and then Ô¨Åne-tunes them on task-speciÔ¨Åc\ndata. The involved parameters of PLMs can be\ngenerally represented in the matrix format. Hence,\nit would be natural to apply MPO-based approxi-\nmation for compressing the parameter matrices in\nPLMs by truncating tensor dimensions.\nIn particular, we propose two major improve-\nments for MPO-based PLM compression, which\ncan largely reduce the Ô¨Åne-tuning parameters and\neffectively improve the optimization of stacked ar-\nchitecture, respectively.\n4.1 Lightweight Fine-tuning with Auxiliary\nTensors\nDue to the high coupling of parameters, previous\nPLM compression methods usually need to Ô¨Åne-\ntune all the parameters. As a comparison, the MPO\napproach decomposes a matrix into a list of local\ntensors, which makes it potentially possible to con-\nsider Ô¨Åne-tuning different parts with speciÔ¨Åc strate-\ngies. Next, we study how to perform lightweight\nÔ¨Åne-tuning based on MPO properties.\nParameter Variation from Pre-Training. To ap-\nply our solution to lightweight Ô¨Åne-tuning, we Ô¨Årst\nconduct an empirical experiment to check the varia-\ntion degree of the parameters before and after Ô¨Åne-\ntuning. Here, we adopt the standard pre-trained\nBERT (Devlin et al., 2019) and then Ô¨Åne-tune it\non the SST-2 task (Socher et al., 2013). We Ô¨Årst\ncompute the absolute difference of the variation for\neach parameter value and then compute the ratio\nof parameters with different variation levels. The\nstatistical results are reported in Table 1. As we\ncan see, most of parameters vary little, especially\nfor the word embedding layer. This Ô¨Ånding has\nalso been reported in a previous studies (Khetan\nand Karnin, 2020). As discussed in Section 3, after\nMPO decomposition, the central tensor contains\nthe majority of the parameters, while the auxiliary\ntensors only contain a small proportion of the pa-\nrameters. Such merit inspires us to consider only\nÔ¨Åne-tuning the parameters in the auxiliary tensors\nwhile keeping the central tensor Ô¨Åxed during Ô¨Åne-\ntuning. If this approach was feasible, this will\nlargely reduce the parameters to be Ô¨Åne-tuned.\nTheoretical Analysis. Here we introduce entan-\nglement entropy from quantum mechanics (Cal-\nabrese and Cardy, 2004) as the metric to measure\nthe information contained in MPO bonds, which\nis similar to the entropy in information theory but\nreplaces probabilities by normalized singular val-\nues produced by SVD. This will be more suitable\nfor measuring the information of a matrix as sin-\ngular values often correspond to the important in-\nformation implicitly encoded in the matrix, and the\nimportance is positively correlated with the magni-\ntude of the singular values. Following (Calabrese\nand Cardy, 2004), the entanglement entropy Sk\ncorresponding to the k-th bond can be calculated\nby:\nSk = ‚àí\ndk‚àë\nj=1\nvj ln vj, k = 1,2,...,n ‚àí1, (6)\nwhere {vj}dk\nj=1 denote the normalized SVD eigen-\nvalues of M[i1j1...ikjk,ik+1jk+1...injn]. The en-\ntanglement entropy Sk is an increasing function\nof dimension dk as described in (Gao et al., 2020).\nBased on Eq. (2), the central tensor has the largest\nbond dimension, corresponding to the largest en-\ntanglement entropy. This indicates that most of\nthe information in an original matrix will be con-\ncentrated in the central tensor. Furthermore, the\nlarger a dimension is, the larger the updating effect\nwill be. According to (Pirvu et al., 2010), it is also\nguaranteed in principle that any change on some\ntensor will be transmitted to the whole local tensor\nset. Thus, it would have almost the same effect\nafter convergence by optimizing the central tensor\nor the auxiliary tensors for PLMs.\nBased on the above analysis, we speculate\n5392\nthat the affected information during Ô¨Åne-tuning is\nmainly encoded on the auxiliary tensors so that\nthe overall variations are small. Therefore, for\nlightweight Ô¨Åne-tuning, we Ô¨Årst perform the MPO\ndecomposition for a parameter matrix, and then\nonly update its auxiliary tensors according to the\ndownstream task with the central tensor Ô¨Åxed. Ex-\nperimental results in Section 5.2 will demonstrate\nthat such an approach is indeed effective.\n4.2 Dimension Squeezing for Stacked\nArchitecture Optimization\nMost of PLMs are stacked with multiple Trans-\nformer layers. Hence, a major problem with di-\nrectly applying MPO to compressing PLMs is that\nthe reconstruction error tends to be accumulated\nand ampliÔ¨Åed exponentially by the number of lay-\ners. It is thus urgent to develop a more stable opti-\nmization algorithm tailored to the stacked architec-\nture.\nFast Reconstruction Error Estimation. Without\nloss of generality, we can consider a simple case\nin which each layer contains exactly one param-\neter matrix to be compressed. Assume that there\nare Llayers, so we have Lparameter matrices in\ntotal, denoted by {M(l)}L\nl=1. Let C(l) denote the\ncorresponding central tensor with a speciÔ¨Åc dimen-\nsion d(l) after decomposing M(l) with MPO. Our\nidea is to select a central tensor to reduce its di-\nmension by one at each time, given the selection\ncriterion that this truncation will lead to the least re-\nconstruction error. However, it is time-consuming\nto evaluate the reconstruction error of the original\nmatrix. According to Eq. (3), we can utilize the\nerror bound\n‚àö‚àën‚àí1\nk=1 œµ2\nk for a fast estimation of the\nyielded reconstruction error. In this case, only one\nœµk changes, and it can be efÔ¨Åciently computed via\nthe pre-computed eigenvalues.\nFast Performance Gap Computation. At each\ntime, we compute the performance gap before and\nafter the dimension reduction (d(l) ‚Üíd(l) ‚àí1) with\nthe stop criterion. To obtain the performance Àúpaf-\nter dimension reduction, we need to Ô¨Åne-tune the\ntruncated model on the downstream task. We can\nalso utilize the lightweight Ô¨Åne-tuning strategy in\nSection 4.1 to obtain Àúpby only tuning the auxil-\niary tensors. If the performance gap ‚à•p‚àíÀúp ‚à•is\nsmaller than a threshold ‚àÜ or the iteration number\nexceeds the predeÔ¨Åned limit, the algorithm will end.\nSuch an optimization algorithm is more stable to\noptimize stacked architectures since it gradually\nreduces the dimension considering the reconstruc-\ntion error and the performance gap. Actually, it\nis similar to the learning of variable matrix prod-\nuct states (Iblisdir et al., 2007) in physics, which\noptimizes the tensors one by one according to the\nsequence. As a comparison, our algorithm dynam-\nically selects the matrix to truncate and is more\nsuitable to PLMs.\nAlgorithm 2 presents a complete procedure for\nour algorithm. In practice, there are usually mul-\ntiple parameter matrices to be optimized at each\nlayer. This can be processed in a similar way: we\nselect some matrices from one layer to optimize\namong all the considered matrices.\nAlgorithm 2Training with dimension squeezing.\nInput: : Llayers with corresponding central tensor C(l) and\ndimension d(l), threshold ‚àÜ and iteration step iter\n1: Evaluate loss p= model(Inputs)\n2: Perform MPO decomposition for each layer\n3: for step= 1 ‚Üíiterdo\n4: Find the layer ( l‚àó) with the least reconstruction error\n5: Compress MPO tensor by truncating d(l‚àó)\n6: Fine-tuning auxiliary tensors with {C(l)}L\nl=1 Ô¨Åxed\n7: Evaluate loss Àúp= model(Inputs)\n8: if ‚à•p‚àíÀúp‚à•>‚àÜ then\n9: break\n10: end if\n11: end for\n12: return Compressed model\n4.3 Overall Compression Procedure\nGenerally speaking, our approach can compress\nany PLMs with stacked architectures consisting of\nparameter matrices, even the compressed PLMs.\nIn other words, it can work with the existing PLM\ncompression methods to further achieve a better\ncompression performance. Here, we select AL-\nBERT (Lan et al., 2020) as a representative com-\npressed PLM and apply our algorithm to ALBERT.\nThe procedure can be simply summarized as fol-\nlows. First, we obtain the learned ALBERT model\n(complete) and perform the MPO-decomposition to\nthe three major parameter matrices, namely word\nembedding matrix, self-attention matrix and feed-\nforward matrix2. Each matrix will be decomposed\ninto a central tensor and auxiliary tensors. Next,\nwe perform the lightweight Ô¨Åne-tuning to update\nauxiliary tensors until convergence on downstream\ntasks. Then, we apply the dimension squeezing\n2It introduces a parameter sharing mechanism to keep only\none copy for both self-attention and feed-forward matrices.\n5393\nCategory Method Inference Time\nTucker Tucker (d=1) (CP) O(nmd 2 )\nTucker (d> 1) O(nmd + dn )\nMPO MPO (n=2) (SVD) O(2 md3 )\nMPO (n> 2) O(nmd 3 )\nTable 2: Inference time complexities of different low-\nrank approximation methods. Here, ndenotes the num-\nber of the tensors, mdenotes max({ik}n\nk=1) means the\nlargest ik in input list, and d denotes max({d‚Ä≤\nk}n\nk=0)\nmeans the largest dimension d‚Ä≤\nk in the truncated dimen-\nsion list.\noptimization algorithm to the three central tensors,\ni.e., we select one matrix for truncation each time.\nAfter each truncation, we Ô¨Åne-tune the compressed\nmodel and further stabilize its performance. This\nprocess will repeat until the performance gap or the\niteration number exceeds the pre-deÔ¨Åned threshold.\nIn this way, we expect that ALBERT can be\nfurther compressed. In particular, it can be Ô¨Åne-\ntuned in a more efÔ¨Åcient way, with only a small\namount of parameters to be updated. Section 5.2\nwill demonstrate this.\n4.4 Discussion\nIn mathematics, MPO-based approximation can be\nconsidered as a special low-rank approximation\nmethod. Now, we compare it with other low-rank\napproximation methods, including SVD (Henry\nand Hofrichter, 1992), CPD (Hitchcock, 1927) and\nTucker decomposition (Tucker, 1966).\nWe present the categorization of these methods\nin Table 2. For PLM compression, low-rank decom-\nposition is only performed once, while it repeatedly\nperforms forward propagation computation. Hence,\nwe compare their inference time complexities. In-\ndeed, all the methods can be tensor-based decom-\nposition (i.e., a list of tensors for factorization) or\nmatrix decomposition, and we characterize their\ntime complexities with common parameters. In-\ndeed, MPO and Tucker represent two categories of\nlow-rank approximation methods. Generally, the\nalgorithm capacity is larger with the increase of n\n(more tensors). When n >3, MPO has smaller\ntime complexity than Tucker decomposition. It can\nbe seen that SVD can be considered as a special\ncase of MPO when tensor dimension n = 2 and\nCPD is a special case of Tucker when the core\ntensor is the super-diagonal matrix.\nIn practice, we do not need to strictly follow\nthe original matrix size. Instead, it is easy to pad\nadditional zero entries to enlarge matrix rows or\ncolumns, so that we can obtain different MPO de-\ncomposition results. It has demonstrated that dif-\nferent decomposition plans always lead to almost\nthe same results (Gao et al., 2020). In our exper-\niments, we adopt an odd number of local tensors\nfor MPO decomposition, i.e., Ô¨Åve local tensors (see\nsupplementary materials). Note that MPO decom-\nposition can work with other compression methods:\nit can further reduce the parameters from the matri-\nces compressed by other methods, and meanwhile\nlargely reduce the parameters to be Ô¨Åne-tuned.\n5 Experiments\nIn this section, we Ô¨Årst set up the experiments, and\nthen report the results and analysis.\n5.1 Experimental Setup\nDatasets. We evaluate the effectiveness of com-\npressing and Ô¨Åne-tuning PLMs of our approach\nMPOP on the General Language Understanding\nEvaluation (GLUE) benchmark (Wang et al., 2019).\nGLUE is a collection of 9 datasets for evaluating\nnatural language understanding systems. Follow-\ning (Sanh et al., 2019), we report macro-score (aver-\nage of individual scores, which is slightly different\nfrom ofÔ¨Åcial GLUE score, since Spearman correla-\ntions are reported for STS-B and accuracy scores\nare reported for the other tasks) on the development\nsets for each task by Ô¨Åne-tuning MPOP.\nBaselines. Our baseline methods include:\n‚Ä¢BERT (Devlin et al., 2019): The 12-layer\nBERT-base model was pre-trained on Wikipedia\ncorpus released by Google.\n‚Ä¢ALBERT (Lan et al., 2020): It yields a highly\ncompressed BERT variant with only 11.6M param-\neters, while maintains competitive performance,\nwhich serves as the major baseline.\n‚Ä¢DistilBERT (Sanh et al., 2019): It is trained\nvia knowledge distillation with 6 layers.\n‚Ä¢ MobileBERT (Sun et al., 2020c): It is\nequipped with bottleneck structures and a carefully\ndesigned balance between self-attentions and feed-\nforward networks.\nAll these models are released by Huggingface 3.\nWe select these baselines because they are widely\nadopted and have a diverse coverage of compres-\nsion techniques. Note that we do not directly com-\n3https://huggingface.co/\n5394\nExperiments Score SST-2\n(acc)\nMNLI\n(m_cc)\nQNLI\n(acc)\nCoLA\n(mcc)\nSTS-B\n(œÅ)\nQQP\n(acc)\nMRPC\n(acc)\nRTE\n(acc)\nWNLI\n(acc)\nAvg.\n#Pr/#To(M)\nALBERTpub - 90.3 81.6 - - - - - - - 11.6/11.6\nALBERTrep 78.9 90.6 84.5 89.4 53.4 88.2 89.1 88.5 71.1 54.9 11.6/11.6\nMPOP 79.7 90.8 83.3 90.5 54.7 89.2 89.4 89.2 73.3 56.3 1.1/9\nMPOPfull 80.3 92.2 84.4 91.4 55.7 89.2 89.6 87.3 76.9 56.3 12.7/12.7\nMPOPfull+LFA 80.4 93.0 84.3 91.3 56.0 89.2 89.0 88.0 78.3 56.3 1.2/12.7\nMPOPdir 68.6 86.6 79.2 81.9 15.0 82.5 87.0 74.3 54.2 56.3 1.1/9\nTable 3: Performance on GLUE benchmark obtained by Ô¨Åne-tuning ALBERT and MPOP. ‚ÄúALBERT pub‚Äù and\n‚ÄúALBERTrep‚Äù denote the results from the original paper (Lan et al., 2020) and reproduced by ours, respectively.\n‚Äú#Pr‚Äù and ‚Äú#To‚Äù denote the number (in millions) of pre-trained parameters and total parameters, respectively.\npare our approach with other competitive meth-\nods (Tambe et al., 2020) that require special opti-\nmization tricks or techniques (e.g., hardware-level\noptimization).\nImplementation. The original paper of ALBERT\nonly reported the results of SST-2 and MNLI in\nGLUE. So we reproduce complete results denoted\nas ‚ÄúALBERTrep‚Äù with the Huggingface implemen-\ntation (Wolf et al., 2020). Based on the pre-trained\nparameters provided by Huggingface, we also re-\nproduce the results of BERT, DistilBERT and Mo-\nbileBERT. To ensure a fair comparison, we adopt\nthe same network architecture. For example, the\nnumber of self-attention heads, the hidden dimen-\nsion of embedding vectors, and the max length\nof the input sentence are set to 12, 768 and 128,\nrespectively.\n5.2 Experimental Results\nNote that our focus is to illustrate that our approach\ncan improve either original (uncompressed) or com-\npressed PLMs. In our main experiments, we adopt\nALBERT as the major baseline, and report the com-\nparison results in Table 3.\nComparison with ALBERT. As shown in Table 3,\nour approach MPOP is very competitive in the\nGLUE benchmark, and it outperforms ALBERT in\nall tasks (except MNLI) with a higher overall score\nof 79.7. Looking at the last column, compared with\nALBERT, MPOP reduces total parameters by 22%\n(#To). In particular, it results in a signiÔ¨Åcant reduc-\ntion of pre-trained parameters by 91% (#Pr). Such\na reduction is remarkable in lightweight Ô¨Åne-tuning,\nwhich dramatically improves the Ô¨Åne-tuning efÔ¨Å-\nciency. By zooming in on speciÔ¨Åc tasks, the im-\nprovements over ALBERT are larger on CoLA,\nRTE and WNLI tasks. An interesting explanation\nis that RTE and WNLI tasks have small training\nsets (fewer than 4ksamples). The lightweight Ô¨Åne-\ntuning strategy seems to work better with limited\ntraining data, which enhances the capacity of PLMs\nand prevents overÔ¨Åtting on downstream tasks.\nAblation Results. Our approach has incorporated\ntwo novel improvements: lightweight Ô¨Åne-tuning\nwith auxiliary tensors and optimization with di-\nmension squeezing. We continue to study their\neffect on the Ô¨Ånal performance. Here we con-\nsider three variants for comparison: (1) MPOPfull\nand MPOPfull+LFA are full-rank MPO representa-\ntion (without reconstruction error), and Ô¨Åne-tune\nall the tensors and only auxiliary tensors, respec-\ntively. This comparison is to examine whether\nonly Ô¨Åne-tuning auxiliary tensors would lead to\na performance decrease. (2) MPOPdir directly opti-\nmizes the compressed model without the dimension\nsqueezing algorithm. This variant is used to exam-\nine whether our optimization algorithm is more\nsuitable for stacked architecture. Table 3 (last three\nrows) shows the results when we ablate these. In\nparticular, the dimension squeezing algorithm plays\na key role in improving our approach (a signiÔ¨Åcant\nperformance decrease for MPOPdir), since it is tai-\nlored to stacked architecture. Comparing MPOPfull\nwith MPOPfull+LFA, it is noted that Ô¨Åne-tuning all\nthe parameters seems to have a negative effect on\nperformance. Compared with ALBERT, we specu-\nlate that Ô¨Åne-tuning a large model is more likely to\noverÔ¨Åt on small datasets (e.g., RTE and MRPC).\nThese results show that our approach is able to\nfurther compress ALBERT with fewer Ô¨Åne-tuning\nparameters. Especially, it is also helpful to improve\nthe capacity and robustness of PLMs.\n5.3 Detailed Analysis\nIn this section, we perform a series of detailed\nanalysis experiments for our approach.\nEvaluation with Other BERT Variants. In gen-\neral, our approach can be applied to either uncom-\n5395\nModels WNLI\n(acc)\nMRPC\n(acc)\nRTE\n(acc)\nAvg.\n#Pr/#To(M)\nBERT 56.3 85.5 70.0 110/110\nMPOPB 56.3 84.3 70.8 7.7/70.4\nDistilBERT 56.3 84.1 61.4 66/66\nMPOPD 56.3 84.3 61.7 4.0/43.4\nMobileBERT 56.2 86.0 63.5 25.3/25.3\nMPOPM 56.2 85.3 65.7 4.4/15.4\nTable 4: Evaluation with different BERT variants.\npressed or compressed PLMs. We have evaluated\nits performance with ALBERT. Now, we continue\nto test it with other BERT variants, namely origi-\nnal BERT, DistilBERT and MobileBERT. The lat-\nter two BERT variants are knowledge distillation\nbased methods, and the distilled models can also be\nrepresented in the format of parameter matrix. We\napply our approach to the three variants. Table 4\npresents the comparison of the three variants before\nand after the application of MPOP. As we can see,\nour approach can substantially reduce the network\nparameters, especially the parameters to be Ô¨Åne-\ntuned. Note that DistilBERT and MobileBERT are\nhighly compressed models. These results show that\nour approach can further improve other compressed\nPLMs.\nEvaluation on Different Fine-Tuning Strategies.\nExperiments have shown that our approach is able\nto largely reduce the number of parameters to\nbe Ô¨Åne-tuned. Here we consider a more simple\nmethod to reduce the Ô¨Åne-tuning parameters, i.e.,\nonly Ô¨Åne-tune the last layers of BERT. This experi-\nment reuses the settings of BERT (12 layers) and\nour approach on BERT (i.e., MPOPB in Table 4).\nWe Ô¨Åne-tune the last 1-3 layers of BERT, and com-\npare the performance with our approach MPOPB.\nFrom Table 5, we can see that such a simple way is\nmuch worse than our approach, especially on the\nRTE task. Our approach provides a more princi-\npled way for lightweight Ô¨Åne-tuning. By updating\nauxiliary tensors, it can better adapt to task-speciÔ¨Åc\nloss, and thus achieve better performance.\nEvaluation on Low-Rank Approximation. As\nintroduced in Section 4.4, MPO is a special low-\nrank approximation method, and we Ô¨Årst com-\npare its compression capacity with other low-rank\napproximation methods. As shown in Table 2,\nMPO and Tucker decomposition represent two\nmain categories of low-rank approximation meth-\nods. We select CPD (Henry and Hofrichter, 1992)\nModels SST-2 MRPC RTE Avg.\n#Pr(M)\nBERT10‚àí12 91.9 76.5 67.2 45.7\nBERT11‚àí12 91.7 75.3 62.8 38.6\nBERT12 91.4 72.1 61.4 31.5\nMPOPB 92.6 84.3 70.8 10.1\nTable 5: Comparison of different Ô¨Åne-tuning strategies\non three GLUE tasks. The subscript number in BERT(¬∑)\ndenotes the index of the layers to be Ô¨Åne-tuned.\n0.0 0.2 0.4 0.6 0.8 1.0\nCompression Ratio\n0\n50\n100\n150Reconstruction Error\nCPD\nMPO\n(a) CPD v.s. MPO.\n0.0 0.2 0.4 0.6 0.8 1.0\nCompression Ratio\n0\n50\n100\n150Reconstruction Error\nMPO-2\nMPO-3\nMPO-5\nMPO-7 (b) # of local tensors.\nFigure 2: Comparison of different low-rank approxima-\ntion variants. x-axis denotes the compression ratio ( œÅ\nin Eq. (5)) and y-axis denotes the reconstruction error,\nmeasured in the Frobenius norm.\nfor comparison because general Tucker decompo-\nsition (Tucker, 1966) cannot obtain results with\nreasonable memory. Our evaluation task is to com-\npress the word embedding matrix of the released\n‚Äúbert-base-uncased‚Äù model4. As shown in Fig-\nure 2(a), MPO achieves a smaller reconstruction\nerror with all compression ratios, which shows that\nMPO is superior to CPD. Another hyper-parameter\nin our MPO decomposition is the number of lo-\ncal tensors ( n). We further perform the same\nevaluation with different numbers of local tensors\n(n= 3,5,7). From Figure 2(b), it can be observed\nthat our method is relatively stable with respect to\nthe number of local tensors. Overall, a larger nre-\nquires a higher time complexity and can yield Ô¨Çexi-\nble decomposition. Thus, we set n= 5 for making\na trade-off between Ô¨Çexibility and efÔ¨Åciency.\n6 Conclusion\nWe proposed an MPO-based PLM compression\nmethod. With MPO decomposition, we were able\nto reorganize and aggregate information in central\ntensors effectively. Inspired by this, we designed a\nnovel Ô¨Åne-tuning strategy that only needs to Ô¨Åne-\ntune the parameters in auxiliary tensors. We also\ndeveloped a dimension squeezing training algo-\nrithm for optimizing low-rank approximation over\n4https://huggingface.co/bert-base-uncased\n5396\nstacked network architectures. Extensive experi-\nments had demonstrated the effectiveness of our\napproach, especially on the reduction of Ô¨Åne-tuning\nparameters. We also empirically found that such\na Ô¨Åne-tuning way was more robust to generalize\non small training datasets. To our knowledge, it is\nthe Ô¨Årst time that MPO decomposition had been\napplied to compress PLMs. In future work, we will\nconsider exploring more decomposition structures\nfor MPO.\nAcknowledgments\nThis work was partially supported by the National\nNatural Science Foundation of China under Grants\nNo. 61872369, 61832017 and 11934020, Bei-\njing Academy of ArtiÔ¨Åcial Intelligence (BAAI) un-\nder Grant No. BAAI2020ZJ0301, Beijing Out-\nstanding Young Scientist Program under Grant\nNo. BJJWZYJH012019100020098, the Funda-\nmental Research Funds for the Central Universities\nand the Research Funds of Renmin University of\nChina under Grant No. 18XNLG22, 19XNQ047,\n20XNLG19 and 21XNH027. Xin Zhao and Zhong-\nYi Lu are the corresponding authors.\nReferences\nPasquale Calabrese and John Cardy. 2004. Entangle-\nment entropy and quantum Ô¨Åeld theory. Journal\nof Statistical Mechanics: Theory and Experiment ,\n2004(06):P06002.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171‚Äì4186. Association for Computa-\ntional Linguistics.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali\nKhan, Yin Yang, Deming Chen, Marianne Winslett,\nHassan Sajjad, and Preslav Nakov. 2020. Compress-\ning large-scale transformer-based models: A case\nstudy on bert. arXiv preprint arXiv:2002.11985.\nZe-Feng Gao, Song Cheng, Rong-Qiang He, ZY Xie,\nHui-Hai Zhao, Zhong-Yi Lu, and Tao Xiang.\n2020. Compressing deep neural networks by ma-\ntrix product operators. Physical Review Research ,\n2(2):023300.\nTimur Garipov, Dmitry Podoprikhin, Alexander\nNovikov, and Dmitry Vetrov. 2016. Ultimate ten-\nsorization: compressing convolutional and fc layers\nalike. arXiv preprint arXiv:1611.03214.\nER Henry and J Hofrichter. 1992. [8] singular value de-\ncomposition: Application to analysis of experimen-\ntal data. Methods in enzymology, 210:129‚Äì192.\nFrank L Hitchcock. 1927. The expression of a tensor\nor a polyadic as a sum of products. Journal of Math-\nematics and Physics, 6(1-4):164‚Äì189.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic\nBERT with adaptive width and depth. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efÔ¨Åcient transfer learning for nlp.\nIn International Conference on Machine Learning ,\npages 2790‚Äì2799. PMLR.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2017. Quantized\nneural networks: Training neural networks with low\nprecision weights and activations. The Journal of\nMachine Learning Research, 18(1):6869‚Äì6898.\nForrest N Iandola, Albert E Shaw, Ravi Krishna, and\nKurt W Keutzer. 2020. Squeezebert: What can\ncomputer vision teach nlp about efÔ¨Åcient neural net-\nworks? arXiv preprint arXiv:2006.11316.\nS Iblisdir, R Orus, and JI Latorre. 2007. Matrix product\nstates algorithms and continuous systems. Physical\nReview B, 75(10):104305.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. Tinybert: Distilling BERT for natural lan-\nguage understanding. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: Findings, EMNLP 2020, Online\nEvent, 16-20 November 2020, pages 4163‚Äì4174. As-\nsociation for Computational Linguistics.\nAshish Khetan and Zohar Karnin. 2020. schuBERT:\nOptimizing elements of BERT. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics , pages 2807‚Äì2818, Online.\nAssociation for Computational Linguistics.\nValentin Khrulkov, Oleksii Hrinchuk, Leyla Mir-\nvakhabova, and Ivan Oseledets. 2019. Tensorized\nembedding layers for efÔ¨Åcient model compression.\narXiv preprint arXiv:1901.10787.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\n5397\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2020. Exploring versatile generative language\nmodel via parameter-efÔ¨Åcient transfer learning. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings,\nEMNLP 2020, Online Event, 16-20 November 2020,\npages 441‚Äì459. Association for Computational Lin-\nguistics.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. Fastbert: a self-\ndistilling BERT with adaptive inference time. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 6035‚Äì6044. Associa-\ntion for Computational Linguistics.\nXindian Ma, Peng Zhang, Shuai Zhang, Nan Duan,\nYuexian Hou, Ming Zhou, and Dawei Song. 2019.\nA tensorized transformer for language modeling. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada , pages 2229‚Äì\n2239.\nMatan Ben Noach and Yoav Goldberg. 2020. Com-\npressing pre-trained language models by matrix de-\ncomposition. In Proceedings of the 1st Confer-\nence of the Asia-PaciÔ¨Åc Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 884‚Äì889.\nAlexander Novikov, Dmitry Podoprikhin, Anton Os-\nokin, and Dmitry P. Vetrov. 2015. Tensorizing neu-\nral networks. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neu-\nral Information Processing Systems 2015, Decem-\nber 7-12, 2015, Montreal, Quebec, Canada , pages\n442‚Äì450.\nIvan V Oseledets. 2011. Tensor-train decomposition.\nSIAM Journal on ScientiÔ¨Åc Computing, 33(5):2295‚Äì\n2317.\nNikolaos Pappas, Phoebe Mulcaire, and Noah A Smith.\n2020. Grounded compositional outputs for adaptive\nlanguage modeling. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1252‚Äì1267.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227‚Äì2237. Association for\nComputational Linguistics.\nBogdan Pirvu, Valentin Murg, J Ignacio Cirac, and\nFrank Verstraete. 2010. Matrix product operator rep-\nresentations. New Journal of Physics, 12(2):025012.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nEvani Radiya-Dixit and Xin Wang. 2020. How Ô¨Åne\ncan Ô¨Åne-tuning be? learning efÔ¨Åcient language mod-\nels. In International Conference on ArtiÔ¨Åcial Intelli-\ngence and Statistics, pages 2435‚Äì2443. PMLR.\nSylvestre-Alvise RebufÔ¨Å, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA , pages\n506‚Äì516.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631‚Äì1642.\nSiqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang\nWang, and Jingjing Liu. 2020a. Contrastive distil-\nlation on intermediate representations for language\nmodel compression. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 498‚Äì508.\nXingwei Sun, Ze-Feng Gao, Zhong-Yi Lu, Junfeng Li,\nand Yonghong Yan. 2020b. A model compression\nmethod with matrix product operators for speech\nenhancement. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 28:2837‚Äì2847.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020c. Mobilebert:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020 , pages 2158‚Äì\n2170. Association for Computational Linguistics.\nThierry Tambe, Coleman Hooper, Lillian Pentecost,\nEn-Yu Yang, Marco Donato, Victor Sanh, Alexan-\nder M Rush, David Brooks, and Gu-Yeon Wei. 2020.\nEdgebert: Optimizing on-chip inference for multi-\ntask nlp. arXiv preprint arXiv:2011.14203.\nAndros Tjandra, Sakriani Sakti, and Satoshi Nakamura.\n2017. Compressing recurrent neural network with\ntensor train. In 2017 International Joint Confer-\nence on Neural Networks (IJCNN) , pages 4451‚Äì\n4458. IEEE.\n5398\nLedyard R Tucker. 1966. Some mathematical notes\non three-mode factor analysis. Psychometrika,\n31(3):279‚Äì311.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38‚Äì45.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exiting\nfor accelerating bert inference. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2246‚Äì2251.\nRose Yu, Stephan Zheng, Anima Anandkumar, and\nYisong Yue. 2017. Long-term forecasting using\ntensor-train rnns. Arxiv.\nOÔ¨År Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\narXiv preprint arXiv:1910.06188.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Sch√ºtze. 2020. Masking as an efÔ¨Åcient alterna-\ntive to Ô¨Ånetuning for pretrained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 2226‚Äì\n2241. Association for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.604041337966919
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5254960656166077
    },
    {
      "name": "Product (mathematics)",
      "score": 0.5234205722808838
    },
    {
      "name": "Joint (building)",
      "score": 0.4943709373474121
    },
    {
      "name": "Computational linguistics",
      "score": 0.46706634759902954
    },
    {
      "name": "Natural language processing",
      "score": 0.43405431509017944
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34555500745773315
    },
    {
      "name": "Linguistics",
      "score": 0.3344680070877075
    },
    {
      "name": "Engineering",
      "score": 0.19687247276306152
    },
    {
      "name": "Mathematics",
      "score": 0.1530258059501648
    },
    {
      "name": "Philosophy",
      "score": 0.14378800988197327
    },
    {
      "name": "Physics",
      "score": 0.11562353372573853
    },
    {
      "name": "Structural engineering",
      "score": 0.07151278853416443
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ]
}