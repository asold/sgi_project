{
  "title": "Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
  "url": "https://openalex.org/W4213025374",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2495941695",
      "name": "Daoquan Chen",
      "affiliations": [
        "Zhejiang Institute of Mechanical and Electrical Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2150764534",
      "name": "Wei-Cong Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288866134",
      "name": "Xiuze Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2729150869",
    "https://openalex.org/W2942016856",
    "https://openalex.org/W2405191498",
    "https://openalex.org/W3171724330",
    "https://openalex.org/W2975620261",
    "https://openalex.org/W2768148617",
    "https://openalex.org/W3131411182",
    "https://openalex.org/W2790625295",
    "https://openalex.org/W2038012452",
    "https://openalex.org/W2744067593",
    "https://openalex.org/W3046666071",
    "https://openalex.org/W2960071712",
    "https://openalex.org/W1986907389",
    "https://openalex.org/W2071280205",
    "https://openalex.org/W2047152377",
    "https://openalex.org/W4210328444",
    "https://openalex.org/W3166577266",
    "https://openalex.org/W3089160862",
    "https://openalex.org/W2794284562",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W2884001105",
    "https://openalex.org/W3019166713",
    "https://openalex.org/W2028140375",
    "https://openalex.org/W2342265232",
    "https://openalex.org/W3131591720",
    "https://openalex.org/W6678662110",
    "https://openalex.org/W3136688211",
    "https://openalex.org/W3196731055",
    "https://openalex.org/W2967407971",
    "https://openalex.org/W3001479960",
    "https://openalex.org/W3137613462",
    "https://openalex.org/W2963641307",
    "https://openalex.org/W2984811147",
    "https://openalex.org/W2546416483",
    "https://openalex.org/W3040694753",
    "https://openalex.org/W3085700062",
    "https://openalex.org/W2899280876",
    "https://openalex.org/W2079985616",
    "https://openalex.org/W2115233498",
    "https://openalex.org/W2531298611",
    "https://openalex.org/W3097439325",
    "https://openalex.org/W2573443283",
    "https://openalex.org/W6634022385",
    "https://openalex.org/W2023975183",
    "https://openalex.org/W2012660988",
    "https://openalex.org/W2744550671",
    "https://openalex.org/W2351263822",
    "https://openalex.org/W3047949753",
    "https://openalex.org/W3012264837",
    "https://openalex.org/W3216881118",
    "https://openalex.org/W3089341989",
    "https://openalex.org/W3049495830",
    "https://openalex.org/W3005071803",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W3159081310",
    "https://openalex.org/W2945078999",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2019732711",
    "https://openalex.org/W2078279667",
    "https://openalex.org/W2144352195",
    "https://openalex.org/W2973897293",
    "https://openalex.org/W2940722387"
  ],
  "abstract": "Accurately predicting the Remaining Useful Life (RUL) of a Li-ion battery plays an important role in managing the health and estimating the state of a battery. With the rapid development of electric vehicles, there is an increasing need to develop and improve the techniques for predicting RUL. To predict RUL, we designed a Transformer-based neural network. First, battery capacity data is always full of noise, especially during battery charge/discharge regeneration. To alleviate this problem, we applied a Denoising Auto-Encoder (DAE) to process raw data. Then, to capture temporal information and learn useful features, a reconstructed sequence was fed into a Transformer network. Finally, to bridge denoising and prediction tasks, we combined these two tasks into a unified framework. Results of extensive experiments conducted on two data sets and a comparison with some existing methods show that our proposed method performs better in predicting RUL. Our projects are all open source and are available at <uri>https://github.com/XiuzeZhou/RUL</uri>.",
  "full_text": "Received January 29, 2022, accepted February 8, 2022, date of publication February 15, 2022, date of current version February 24, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3151975\nTransformer Network for Remaining Useful Life\nPrediction of Lithium-Ion Batteries\nDAOQUAN CHEN\n 1, WEICONG HONG\n 2, AND XIUZE ZHOU\n2\n1School of Intelligent Transportation, Zhejiang Institute of Mechanical and Electrical Engineering, Hangzhou 310053, China\n2Shuye Technology Company Ltd., Hangzhou, China\nCorresponding author: Xiuze Zhou (zhouxiuze@foxmail.com)\nABSTRACT Accurately predicting the Remaining Useful Life (RUL) of a Li-ion battery plays an important\nrole in managing the health and estimating the state of a battery. With the rapid development of electric\nvehicles, there is an increasing need to develop and improve the techniques for predicting RUL. To predict\nRUL, we designed a Transformer-based neural network. First, battery capacity data is always full of noise,\nespecially during battery charge/discharge regeneration. To alleviate this problem, we applied a Denoising\nAuto-Encoder (DAE) to process raw data. Then, to capture temporal information and learn useful features,\na reconstructed sequence was fed into a Transformer network. Finally, to bridge denoising and prediction\ntasks, we combined these two tasks into a uniﬁed framework. Results of extensive experiments conducted on\ntwo data sets and a comparison with some existing methods show that our proposed method performs better\nin predicting RUL. Our projects are all open source and are available at https://github.com/XiuzeZhou/RUL.\nINDEX TERMSLi-ion battery, remaining useful life, transformer, denoising auto-encoder, neural network.\nI. INTRODUCTION\nHaving light weight, high-energy density, good performance\nand a long lifetime, the rechargeable Lithium-ion (Li-ion)\nbattery is widely applied in various devices [1]–[4]. However,\nas the charge-discharge cycle increases, capacity generally\ndegrades. Prognostics and Health Management (PHM) meth-\nods, of which the prediction of Remaining Useful Life (RUL)\nis a very important component, are necessary to ensure\nthe reliability and safety of an electronic device [5]–[7].\nTo ensure safety, prediction of RUL in advance provides\nsome key information about the maintenance and replace-\nment of batteries [8]–[10]. Fig. 1 illustrates a toy-example of\nbattery use.\nAn accurate prediction of lifetime and estimation of\nhealth for batteries are crucial for durable electronic devices.\nRecent advancements and achievements in machine learning\nin various ﬁelds have piqued interest in the estimation\nof data-driven battery health [4], [11], [12]. For exam-\nple, to account for the effects of discharge current and\nambient temperature, Ng et al. [13] proposed a naive\nBayes model to predict the RUL. Regarding the prognos-\ntics of battery health, Nuhic et al. [14] explored applying\nSupport Vector Machine (SVM) to learn the decay pro-\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Zhe Zhang\n.\nFIGURE 1. Example of battery capacity degradation.\ncess. Also, for prognostics and to model battery degradation,\nLiu et al. [15], by online learning, developed the Relevance\nVector Machine (RVM).\nRecently, remarkable success has been achieved by\ndeep learning in various ﬁelds, such as recommender sys-\ntems [16]–[18], Computer Vision (CV) [19]–[21], and Nat-\nural Language Processing (NLP) [22]–[24]. To learn about\nthe nonlinear nature of battery capacity, deep learning\nmodels are also widely applied to RUL prediction. For\nexample, to capture the relationship between RUL and a\ncharge curve, Multi-Layer Perceptron (MLP) was used to\ndescribe the charge process and the terminal voltage curve of\na battery [2], [25], [26]. To assess the State Of Health (SOH),\nRecurrent Neural Network (RNN) was developed to simulate\nthe nonlinear trend [27]–[29]. To learn about inclination of\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 19621\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\nbattery degradation, Long Short-Term Memory (LSTM) was\napplied to the capacity sequence [8], [30], [31].\nWhen predicting RUL, RNN-based frameworks, including\nGate Recurrent Unit (GRU) and LSTM, are effective solu-\ntions for modeling sequential data. Although most existing\nRNN-based frameworks have shown promising performance,\nthey have the following three major problems:\n(1) Using RNN-based networks to model sequential data\nin a recurrent manner not only results in high time costs\nfor training, but also degrades performance due to long-term\ndependency [32]–[34];\n(2) To learn representation, raw data is fed directly to\nthe neural networks; however, the training data are always\nfull of noise, especially when capacity regeneration occurs.\nThe highly dynamic and nonlinear capacity curve affects\nRNN-based methods [28], [31], [35].\n(3) In most methods, data denoising and model prediction\nare two separate tasks; thus, the correlation between the two\ntasks is ignored [28], [36], [37].\nTo address these problems, we designed a novel neural net-\nwork to model sequential capacity patterns. In the network,\na Transformer, which effectively and efﬁciently captures use-\nful information of the sequences, serves as the body of the\nmodel. To learn trends from the sequences, the multi-head\nattention network of the Transformer accelerates the train-\ning performance of the neural networks. To the best of our\nknowledge, this is the ﬁrst Transformer-based architecture to\npredict RUL in the ﬁeld of Li-ion batteries.\nAlso, to build a robust network, it is necessary to deal\nwith noise, outliers, and irrelevant data. The representation\nability of a neural network heavily relies on the quality of\nthe source. Thus, to accurately predict RUL, the Denoising\nAuto-Encoder (DAE), with its powerful ability to learn rep-\nresentation from noisy raw data, is used to reconstruct input\ndata.\nFinally, for better generalization, we propose an objec-\ntive function to bridge denoising and prediction, instead\nof solving these two tasks separately. The learning pro-\ncedure optimizes both tasks simultaneously in a uniﬁed\nframework.\nII. RELATED WORK\nA. PROBLEM DEFINITION\nAn accurate, timely RUL predictor is important for a battery\nto maintain advance warning of potential risk [11]. For bat-\nteries, SOH, a health indicator for battery aging, represents\nthe states of battery in each charge-discharge cycle [31], [38].\nRUL is deﬁned by the following capacity ratio:\nSOH(t) =Ct\nC0\n×100%, (1)\nwhere C0 denotes rated capacity, and Ct denotes the mea-\nsured capacity of cycle, t. As the number of times a battery\nis charged/discharged increases, capacity degrades. For a\nbattery, End of Life (EOL) which is closely related to its\ncapacity [39], is deﬁned as the point when remaining capacity\nFIGURE 2. Example of RUL prediction.\nreaches 70-80% of initial capacity [31], [40]. Fig. 2 illustrates\nan example of RUL prediction.\nB. DEEP LEARNING FOR RUL\nBecause Li-ion batteries are a source of power for many\ndevices, it is critical to ensure their reliability and safety.\nRUL prediction and SOH evaluation have become increas-\ningly important topics and have received considerable\nattention in recent years. Methods to predict RUL for\nbatteries are classiﬁed into two kinds: model-based and\ndata-driven [4], [41], [42].\n1) MODEL-BASED\nTo ﬁt the degradation curve of a battery, mathematical mod-\nels are built to describe the physical properties. However,\nin practice, for a battery working under some noisy and\nuncertain environments, it is difﬁcult for mathematical mod-\nels to accurately assess the SOH [27], [43]–[46].\n2) DATA-DRIVEN\nData-driven methods are modeled on historical data without\nconsidering any properties of the battery. Because of their\nﬂexibility and ease of operation, data-driven methods receive\nmore popular attention [47]–[50].\nNeural network based data-driven methods possess\ngood generalization and powerful feature extraction\nability [2], [51]. To predict RUL for Li-ion batteries, many\ndeep learning models have been proposed. For RUL pre-\ndiction, MLP is applied to learn nonlinear degradation [2],\n[25]. However, it poorly captures the temporal information\nfrom the input sequence. To deal with the sequence data,\nmany RNN-based frameworks, including RNN, LSTM and\nGRU, have been designed [8], [31], [52]. However, RNN-\nbased frameworks with a recurrent manner have a high time\ncost for training and degrade performance due to long-term\ndependency. To speed up training, CNN is used [36], [53].\nBut when it comes to time series, CNN, as MLP, runs into the\nsame problem: it achieves limited performance in degradation\ntrend.\n19622 VOLUME 10, 2022\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\nTABLE 1. Major notations used in this paper.\nTransformers, which perform well in encoding text, have\nbeen explored for various applications, such as recommenda-\ntion systems [54], [55] and CV [56], [57]. They parallelly and\neffectively capture long-range dependencies by an attention\nmechanism. Owing to the effectiveness and efﬁciency of\nTransformers in modeling long sequences, we explore using\nthem to capture the weight of capacities at different time steps\nin the prediction of RUL.\nAlso, to further improve accuracy, various modules are\ncombined to gain their advantages [11], [36], [58], [59].\nAlthough those deep learning methods have achieved great\nsuccess in exploring battery decay trends, they train directly\non noisy data, which limits the model to learn accurate repre-\nsentations. To denoise and get clean input data, Lu et al. [37]\nproposed AE-GRU, in which an autoencoder was used in the\ndata pre-processing step to extract the features of the original\ndata, and GRU was used to learn the long-term inclination.\nHowever, in AE-GRU, data denoising and RUL prediction are\ntwo separate tasks; thus, the correlation between the two tasks\nis ignored. In this paper, we propose an objective function\nto bridge denoising and prediction and optimize both tasks\nsimultaneously in a uniﬁed framework.\nIII. THE PROPOSED METHOD\nThe main goal of our model is to predict RUL from histor-\nical records. Therefore, ﬁrst we describe our architecture in\ndetail. Then, we describe the objective function, which jointly\ncombines DAE and prediction loss.\nFIGURE 3. Denoising transformer network for RUL prediction.\nA. DENOISING TRANSFORMER NETWORK FOR RUL\nTo provide for uninterrupted battery operation and determine\nappropriate maintenance, accurate and timely prediction of\nRUL is important. To solve the problem of most existing\nRNN-based methods, we designed a deep learning archi-\ntecture, Denoising Transformer (DeTransformer) network,\nconsisting of four parts: input and normalization, denoising,\nTransformer, and prediction. The architecture is shown in\nFig. 3. Table 1 lists the major notations used throughout this\npaper.\n1) INPUT AND NORMALIZATION\nTo reduce the inﬂuence of input data distribution changes\non neural networks, the data must be normalized. Let x =\n{x1,x2,..., xn}denote the input sequence of capacity with\nlength n, which is mapped to (0,1]:\nx′= x\nC0\n, (2)\nwhere C0 denotes rated capacity.\n2) DENOISING\nRaw input is always full of noise, especially when\ncharge/discharge regeneration occurs. In most methods, raw\ndata is fed directly to the neural networks without any denois-\ning. These noise data seriously affect the prediction accuracy\nof the methods. To maintain stability and robustness, input\ndata must be denoised before being fed into deep neural\nnetworks. DAE, an unsupervised method in learning useful\nfeatures, which is adopted by our method, reconstructs input\ndata from lower-dimensional representation preserving as\nmuch information as possible in the process.\nLet x′\nt =\n{\nx′\nt+1,x′\nt+2,..., x′\nt+m\n}\n∈x′denote the slice of\ninput with m samples of a sequence. Gaussian noise is added\nto the normalized input to obtain the corrupted vector, ˜xt .\nDAE serves two purposes: denoising the raw input and learn-\ning nonlinear representation:\nz =a\n(\nWT˜xt +b\n)\n, (3)\nVOLUME 10, 2022 19623\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\nwhere W, b, a(·), and z denote weight, bias, activation func-\ntion, and output of the DAE encoder, respectively.\nThen, to reconstruct the input vector, the latent representa-\ntion is mapped back to the input space, deﬁned as follows:\nˆxt =f ′(\nW′z +b′)\n, (4)\nwhere W′, b′, z and f ′(·) denote weight, bias, output, and map\nfunction of the output layer of the DAE encoder, respectively.\nIn our network, identity and ReLU functions are used as\nthe decoding and encoding activation, respectively. Finally,\nthe objective function is deﬁned as follows:\nLd =1\nn\nn∑\nt=1\nℓ(˜xt −ˆxt ) +λ\n(\n∥W∥2\nF +\nW′\n2\nF\n)\n, (5)\nwhere ℓ(·)denotes a loss function.\nBecause the structure of DAE is symmetrical, some\nweights can be tied, i.e. W = W′, thereby accelerating\ntraining by reducing the number of weights of the model.\n3) TRANSFORMER\nThe standard Transformer is a sequence-to-sequence archi-\ntecture, consisting of an encoder and a decoder. The encoder\ntakes the input sequence and maps it into a higher dimensional\nvector, which is then fed into the decoder to generate an\noutput sequence. In this paper, the encoder of the Transformer\nis used to learn long-term dependencies of the capacity degra-\ndation from battery working records.\nThe Transformer layers are a stack of Transformer\nencoders that extract the degradation features from the recon-\nstructed data, with two sub-layers: Multi-Head Self-Attention\nand Feed-Forward. To fully use the position information of\nthe sequence, we inject some relative position tokens into the\nsequence. In this paper, we use sine and cosine functions of\ndifferent frequencies [60]:\nPE(t,2k) =sin(t/100002k/m) (6)\nPE(t,2k +1) =cos(t/100002k/m), (7)\nwhere t denotes the time step.\nThe Multi-Head Self-Attention sub-layer aims to capture\nthe dependencies between features and ignores their distances\nin the sequence [54]–[57]. Given the representation of the\n(l −1)-th layer, Hl−1 and h parallel attention functions, the\ni-th (i ∈[1,h]) attention is deﬁned:\nheadi =Attention\n(\nHl−1Wl\nQ,Hl−1Wl\nK ,Hl−1Wl\nV\n)\n, (8)\nwhere Wl\nQ, Wl\nK , and Wl\nV ∈Rd×dh are projection weights.\nLet Q, K, and V denote query, key, and value, Scaled Dot-\nProduct Attention deﬁned as follows::\nAttention (Q,K,V)=softmax\n(\nQKT\n√dh\n)\nV, (9)\nwhere dh = d/h, which avoids avoiding extremely small\ngradients and produces a softer attention distribution [60].\nThen, the Multi-Head Attention is deﬁned as follows:\nMultiHead\n(\nHl−1\n)\n=[head1;head2;···; headh]WO, (10)\nwhere WO is a trainable weight.\nFeed-Forward, which has two different mappings (linear\nand ReLU nonlinear), is applied to each time step identi-\ncally and separately. Then, we obtain Hl from the previous\nMultiHead\n(\nHl−1)\nas follows:\nHl =FFN\n(\nMultiHead\n(\nHl−1\n))\n, (11)\nFFN (x)=ReLU (xW1 +b1)W2 +b2. (12)\n4) PREDICTION\nFinally, to predict unknown capacity, a full connection layer\nis used to map the representation learned by the last Trans-\nformer cell to arrive at the ﬁnal prediction ˆxt , namely,ˆxi+m+1:\nˆxt =f\n(\nWpHh +bp\n)\n, (13)\nwhere Wp, bp, Hh, and f (·) denote weight, bias, input, and\nmap function of the prediction layer, respectively.\nB. LEARNING\nThere are two tasks in our model: denoising and prediction.\nInstead of solving these two tasks separately, we propose\nan objective function to bridge these tasks. The learning\nprocedure optimizes both tasks simultaneously in a uniﬁed\nframework. Mean Square Error (MSE) is used to evaluate\nloss, and the objective function is deﬁned as follows:\nL =\nn∑\nt=T +1\n(xt −ˆxt )2 +α\nn∑\ni=1\nℓ(˜xi −ˆxi) +λ(2), (14)\nwhere α denotes a parameter to control the relative contri-\nbution of each task; λ denotes a regularization parameter;\n(·) denotes the regularization; 2 denotes the learning\nparameters of our model.\nIV. EXPERIMENTS\nA. EXPERIMENTAL SETTINGS\n1) DATA SETS\nWe conducted experiments using two public data sets: NASA\nand CALCE. The NASA data set, available from the NASA\nAmes Research Center web site, 1 contains the record of four\ndifferent Li-ion batteries, with each Li-ion battery repeating\nthree operations: charge, discharge, and impedance measure-\nments [61], [62]. Similarly, the CALCE data set is avail-\nable from the Center for Advanced Life Cycle Engineer-\ning (CALCE) of the University of Maryland 2 [63]–[65].\n2) BASELINE APPROACHES\nWe compared our models to the following methods:\n• MLP [2], with multiple fully connected layers, is used\nto learn the dynamic and nonlinear degradation trend of\na battery.\n• RNN [27], with multiple RNN units, is used to\npredict RUL.\n1https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-\nrepository/#battery\n2https://calce.umd.edu/data#CS2\n19624 VOLUME 10, 2022\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\nTABLE 2. Overall performance on NASA and CALCE data sets.\n• LSTM [8], with multiple LSTM units, is used to learn\nthe degradation trend from the input sequence.\n• GRU [66], with multiple GRU units, is used to learn\nfeatures from sequences.\n• Dual-LSTM [52], with two different LSTM cells in\npoint detection, is used to capture the non-linearity\nbetween capacities.\n3) EVALUATION METRICS\nFirst, the three evaluation metrics used to evaluate the predic-\ntion performance of RUL are the following: Relative Error\n(RE), Mean Absolute Error (MAE) and RMSE. The three\nevaluation metrics are deﬁned as follows:\nRE =\n⏐⏐RULpred −RULtrue⏐\n⏐\nRULtrue (15)\nRMSE =\n\n√\n1\nn −T\nn∑\nt=T +1\n(xt −ˆxt )2 (16)\nMAE = 1\nn −T\nn∑\nt=T +1\n∥xt −ˆxt ∥ (17)\nwhere n denotes the length of a sequence, and T denotes the\nlength of samples generated from a sequence for training.\nThen, a leave-one-out evaluation is used to evaluate our\nmodels: one battery is sampled randomly; the remainder are\nused for training. Finally, after ﬁve iterations, the average\nscore over all batteries is determined.\n4) PARAMETER SETTINGS\nOur model has six key parameters: sampler size (m), learning\nrate (τ), depth (l ) and hidden size (s) of Transformer, regular-\nization for learning (λ), and ratio of each task (α ). m can be set\nabout 5∼10% of the length of a sequence. In our experiments,\nm is ﬁxed at 16 and 64 for NASA and CALCE, respectively.\nThe rest parameters were determined by grid-search on the\nvalidation error: τ is chose from {10−4,5 ×10−4,10−3,5 ×\n10−3,10−2}; s is chose from {8,16,32,64}; l is chose from\n{1,2,3,4}; λis chose from {10−6,10−5,10−4,10−3}; α is\nset from (0, 1].\nBecause RE is highly related to the RUL of a battery,\nwe chose RE as our major evaluation metric. In terms of\nthe RE, optimal parameters of all methods for the two data\nTABLE 3. Optimal parameters of RE scores for two data sets.\nsets are listed in Table 3. All codes are run on Pytorch 1.8.0,\nPython 3.7, and Cenos 7 Systems with i9 CPU.\nB. RESULTS AND ANALYSIS\n1) OVERALL PERFORMANCE\nFirst, experiments were conducted to verify the performance\nof our methods on different data sets. Table 2 shows the RE,\nMAE and RMSE scores obtained for all methods. The best\nresults are shown in bold.\nFrom the results shown in Table 2, we conclude the fol-\nlowing: (1) Among all methods, our models achieve the best\nexperimental results. The results demonstrate that our model\nextracts useful temporal information in the modeling capacity\nsequences. (2) On both data sets, DeTransformer is stable\nand robust and always makes good predictions, regardless of\nwhether a capacity sequence is long or short. Also, DeTrans-\nformer shows an even greater improvement on NASA.\nPossibly, for networks, a long sequence offers sufﬁcient infor-\nmation to train; however, representation ability is limited\nwhen only one feature is fed into the network for a short\nsequence. (3) Among the baseline methods, MLP performs\nthe worst, because it fails to take into account the effects of\ntemporal information. Our model and all RNN-based models\npredict trends better than MLP, which means that it is nec-\nessary to add sequential information to predict RUL well.\nThe attention networks of Transformer capture the overall\nVOLUME 10, 2022 19625\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\nFIGURE 4. Effect of autoencoder.\ninclination by modeling correlations among historical capac-\nity features. Thus, our model simulates well the effects of his-\ntorical capacities in sequence states. (4)RNN achieves better\nscores for MAE and RMSE on NASA, but, worse on CALCE\nwhen compared with LSTM, GRU, and Dual-LSTM. The\nbest possible reason is that the sequence length is different\nin the two data sets. LSTM and GRU are better at learning\nfeatures from long sequences than RNN, which is also a deﬁ-\nciency in the nature of RNN. In all metrics, DeTransformer\ndoes exceptionally well on RE, which is directly related to\nthe RUL of a battery. Potentially, the reason is that battery\ncharge/discharge regeneration degrades the learning of the\nmodel on the trends. To reﬁne the representation, our models\nreduce noise in a raw sequence with an autoencoder. In sum-\nmary, our method outperforms other competitive approaches,\nwhich suggests that our method is effective for extracting\nmeaningful temporal features to more accurately predict the\nRUL of a battery.\n2) EFFECT OF AUTOENCODER\nThen, we demonstrate the improvement in performance by\nusing an autoencoder. We compared our models with their\nsimpliﬁed versions without an autoencoder by setting differ-\nent values for the hidden size. The average scores of RE,\nMAE, and RMSE with changing hidden sizes are shown\nin Fig. 4.\nFrom Fig. 4, it is seen that all scores ﬁrst increase and then\ndecrease with an increase in hidden size. The most probable\nreason is that Transformer has limited weights to obtain\nsufﬁcient temporal information, leading to under-ﬁtting when\nhidden size is too small. When hidden size is too large,\nTransformer has too many weights to learn temporal infor-\nmation. Also, for all metrics, with an increase in the hidden\nsize of Transformer in most cases, our models performed\nbetter than their simpliﬁed versions, indicating that an added\nautoencoder improves performance in the prediction of RUL.\nFIGURE 5. Time cost (seconds) of all neural networks on two data sets.\nThe nonlinear capacity curve contains much noise, especially\nwhen capacity regeneration occurs. Most neural networks are\ntrained directly on the raw data, which inﬂuences the model in\nlearning representation. However, to make better predictions,\nour models are trained on the reﬁned data generated by DAE,\nwhich has the powerful ability to learn useful features from\ninput with much noise. Therefore, an autoencoder shows\nstrong improvement over our methods.\n3) TIME COST\nFinally, we studied the time cost of all neural networks on\ntwo data sets (See Fig. 5). From Fig. 5, it is seen that the\ntraining time of LSTM, GRU, and Dual-LSTM, is much\nlonger than for the other models. Potentially, the reason is that\nall RNN-based networks modeled on sequences in a recurrent\nmanner lead to higher time costs for training and inference.\nAlthough MLP is rapid, it does not work very well experimen-\ntally on sequential data. RNN and DeTransformer are very\nclose in training speed. However, our approach always yields\nthe best results. To learn trends from sequences, a multi-head\nattention network of a Transformer accelerates the training\nperformance of neural networks. Finally, we conclude that,\nwith a multi-head attention network applied in our model,\n19626 VOLUME 10, 2022\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\nTransformer learns features in parallel, which are more suit-\nable for predicting RUL.\nV. CONCLUSION\nWith a RUL predictor, an accurate estimation of RUL,\na safer battery system, and longer battery service life can\nbe achieved. We proposed a novel neural network model for\nRUL prediction. First, DAE was used to learn representation\nfrom corrupted input and then used to reconstruct input.\nSecond, from the reconstructed input, Transformer networks\nwere used to learn the feature for capacity fading. Finally,\nwe designed an objective function, which combines jointly\nDAE loss and prediction loss. Compared with existing RUL\nmethods, our models achieve better performance as indicated\nby lower RE, MAE, and RMSE scores.\nIn the future, we plan to extend our methods to more practi-\ncal applications. First, training a model using part of a record\nmay not be robust enough and may be lopsided. Thus, to fully\ntrain, more charge-discharge data will be added to our model.\nAlso, in practice, a battery will be examined under operational\nconditions, such as different working temperatures and cur-\nrents, which have a large impact on degradation trends. Thus,\nan estimation of RUL for a battery under different operating\nconditions will be studied further.\nACKNOWLEDGMENT\nThe authors would like to thank Michael McAllister for\nproofreading this manuscript.\nREFERENCES\n[1] Y. Song, D. Liu, C. Yang, and Y. Peng, ‘‘Data-driven hybrid remaining\nuseful life estimation approach for spacecraft lithium-ion battery,’’ Micro-\nelectron. Rel., vol. 75, pp. 142–153, Aug. 2017.\n[2] Y. Wu, W. Li, Y. Wang, and K. Zhang, ‘‘Remaining useful life prediction\nof lithium-ion batteries using neural network and bat-based particle ﬁlter,’’\nIEEE Access, vol. 7, pp. 54843–54854, 2019.\n[3] L. Wu, X. Fu, and Y. Guan, ‘‘Review of the remaining useful life prog-\nnostics of vehicle lithium-ion batteries using data-driven methodologies,’’\nAppl. Sci., vol. 6, no. 6, p. 166, 2016.\n[4] A. Samanta, S. Chowdhuri, and S. S. Williamson, ‘‘Machine learning-\nbased data-driven fault detection/diagnosis of lithium-ion battery: A criti-\ncal review,’’Electronics, vol. 10, no. 11, p. 1309, May 2021.\n[5] H. Meng and Y.-F. Li, ‘‘A review on prognostics and health management\n(PHM) methods of lithium-ion batteries,’’ Renew. Sustain. Energy Rev.,\nvol. 116, Dec. 2019, Art. no. 109405.\n[6] D. Wang, K.-L. Tsui, and Q. Miao, ‘‘Prognostics and health management:\nA review of vibration based bearing and gear health indicators,’’ IEEE\nAccess, vol. 6, pp. 665–676, 2018.\n[7] J.-Z. Kong, F. Yang, X. Zhang, E. Pan, Z. Peng, and D. Wang, ‘‘Voltage-\ntemperature health feature extraction to improve prognostics and health\nmanagement of lithium-ion batteries,’’ Energy, vol. 223, May 2021,\nArt. no. 120114.\n[8] Y. Zhang, R. Xiong, H. He, and M. G. Pecht, ‘‘Long short-term memory\nrecurrent neural network for remaining useful life prediction of lithium-\nion batteries,’’ IEEE Trans. Veh. Technol., vol. 67, no. 7, pp. 5695–5705,\nJul. 2018.\n[9] H. Dong, X. Jin, Y. Lou, and C. Wang, ‘‘Lithium-ion battery state of\nhealth monitoring and remaining useful life prediction based on support\nvector regression-particle ﬁlter,’’ J. Power Sources, vol. 271, pp. 114–123,\nDec. 2014.\n[10] S. Zheng, K. Ristovski, A. Farahat, and C. Gupta, ‘‘Long short-term\nmemory network for remaining useful life estimation,’’ in Proc. IEEE Int.\nConf. Prognostics Health Manage. (ICPHM), Jun. 2017, pp. 88–95.\n[11] B. Zhou, C. Cheng, G. Ma, and Y. Zhang, ‘‘Remaining useful life pre-\ndiction of lithium-ion battery based on attention mechanism with posi-\ntional encoding,’’ IOP Conf. Ser., Mater. Sci. Eng., vol. 895, no. 1, 2020,\nArt. no. 012006.\n[12] Y. Zhang, R. Xiong, H. He, X. Qu, and M. Pecht, ‘‘Aging characteristics-\nbased health diagnosis and remaining useful life prognostics for lithium-\nion batteries,’’ eTransportation, vol. 1, Aug. 2019, Art. no. 100004.\n[13] S. S. Y. Ng, Y. Xing, and K. L. Tsui, ‘‘A naive Bayes model for robust\nremaining useful life prediction of lithium-ion battery,’’ Appl. Energy,\nvol. 118, pp. 114–123, Apr. 2014.\n[14] A. Nuhic, T. Terzimehic, T. Soczka-Guth, M. Buchholz, and K. Dietmayer,\n‘‘Health diagnosis and remaining useful life prognostics of lithium-\nion batteries using data-driven methods,’’ J. Power Sources, vol. 239,\npp. 680–688, Oct. 2013.\n[15] D. Liu, J. Zhou, H. Liao, Y. Peng, and X. Peng, ‘‘A health indicator\nextraction and optimization framework for lithium-ion battery degradation\nmodeling and prognostics,’’ IEEE Trans. Syst., Man, Cybern. Syst, vol. 45,\nno. 6, pp. 915–928, Jun. 2015.\n[16] M. Chen, T. Ma, and X. Zhou, ‘‘CoCNN: Co-occurrence CNN for recom-\nmendation,’’Expert Syst. Appl., vol. 195, Jun. 2022, Art. no. 116595.\n[17] M. Chen, Y. Li, and X. Zhou, ‘‘CoNet: Co-occurrence neural networks\nfor recommendation,’’Future Gener. Comput. Syst., vol. 124, pp. 308–314,\nNov. 2021.\n[18] M. Chen and X. Zhou, ‘‘DeepRank: Learning to rank with neural net-\nworks for recommendation,’’ Knowl.-Based Syst., vol. 209, Dec. 2020,\nArt. no. 106478.\n[19] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, ‘‘Deep\nlearning for computer vision: A brief review,’’ Comput. Intell. Neurosci.,\nvol. 2018, pp. 1–13, Feb. 2018.\n[20] N. Akhtar and A. Mian, ‘‘Threat of adversarial attacks on deep learning in\ncomputer vision: A survey,’’ IEEE Access, vol. 6, pp. 14410–14430, 2018.\n[21] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi,\nM. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez,\n‘‘A survey on deep learning in medical image analysis,’’ Med. Image Anal.,\nvol. 42, pp. 60–88, Dec. 2017.\n[22] T. Young, D. Hazarika, S. Poria, and E. Cambria, ‘‘Recent trends in deep\nlearning based natural language processing,’’ IEEE Comput. Intell. Mag.,\nvol. 13, no. 3, pp. 55–75, Aug. 2018.\n[23] D. W. Otter, J. R. Medina, and J. K. Kalita, ‘‘A survey of the usages of\ndeep learning for natural language processing,’’ IEEE Trans. Neural Netw.\nLearn. Syst., vol. 32, no. 2, pp. 604–624, Feb. 2021.\n[24] E. Cambria and B. White, ‘‘Jumping NLP curves: A review of natural\nlanguage processing research,’’ IEEE Comput. Intell. Mag., vol. 9, no. 2,\npp. 48–57, May 2014.\n[25] J. Wu, C. Zhang, and Z. Chen, ‘‘An online method for lithium-ion battery\nremaining useful life estimation using importance sampling and neural\nnetworks,’’Appl. Energy, vol. 173, pp. 134–140, Jul. 2016.\n[26] A. Khalid and A. I. Sarwat, ‘‘Uniﬁed univariate-neural network models\nfor lithium-ion battery state-of-charge forecasting using minimized Akaike\ninformation criterion algorithm,’’ IEEE Access , vol. 9, pp. 39154–39170,\n2021.\n[27] J. Liu, A. Saxena, K. Goebel, B. Saha, and W. Wang, ‘‘An adaptive\nrecurrent neural network for remaining useful life prediction of lithium-ion\nbatteries,’’ in Proc. Annu. Conf. Prognostics Health Manage. Soc., 2010,\npp. 1–9.\n[28] N. Gugulothu, V. Tv, P. Malhotra, L. Vig, P. Agarwal, and G. Shroff,\n‘‘Predicting remaining useful life using time series embeddings based on\nrecurrent neural networks,’’ Int. J. Prognostics Health Manage., vol. 9,\nno. 1, pp. 1–10, Nov. 2020.\n[29] M. Catelani, L. Ciani, R. Fantacci, G. Patrizi, and B. Picano, ‘‘Remaining\nuseful life estimation for prognostics of lithium-ion batteries based on\nrecurrent neural network,’’ IEEE Trans. Instrum. Meas., vol. 70, pp. 1–11,\n2021.\n[30] A. Khalid, A. Sundararajan, I. Acharya, and A. I. Sarwat, ‘‘Prediction of Li-\nion battery state of charge using multilayer perceptron and long short-term\nmemory models,’’ in Proc. IEEE Transp. Electriﬁc. Conf. Expo. (ITEC),\nJun. 2019, pp. 1–6.\n[31] K. Park, Y. Choi, W. J. Choi, H.-Y. Ryu, and H. Kim, ‘‘LSTM-based battery\nremaining useful life prediction with multi-channel charging proﬁles,’’\nIEEE Access, vol. 8, pp. 20786–20798, 2020.\n[32] Y. Mo, Q. Wu, X. Li, and B. Huang, ‘‘Remaining useful life estimation\nvia transformer encoder enhanced by a gated convolutional unit,’’ J. Intell.\nManuf., vol. 32, no. 7, pp. 1997–2006, 2021.\nVOLUME 10, 2022 19627\nD. Chenet al.: Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries\n[33] J. Hao, X. Wang, B. Yang, L. Wang, J. Zhang, and Z. Tu, ‘‘Modeling\nrecurrence for transformer,’’ in Proc. Conf. North, 2019, pp. 1198–1207.\n[34] E. Egonmwan and Y. Chali, ‘‘Transformer and seq2seq model for para-\nphrase generation,’’ in Proc. 3rd Workshop Neural Gener. Transl., 2019,\npp. 249–255.\n[35] T. Qin, S. Zeng, J. Guo, and Z. Skaf, ‘‘A rest time-based prognostic\nframework for state of health estimation of lithium-ion batteries with\nregeneration phenomena,’’ Energies, vol. 9, no. 11, p. 896, Nov. 2016.\n[36] L. Ren, J. Dong, X. Wang, Z. Meng, L. Zhao, and M. J. Deen, ‘‘A data-\ndriven auto-CNN-LSTM prediction model for lithium-ion battery remain-\ning useful life,’’ IEEE Trans. Ind. Informat., vol. 17, no. 5, pp. 3478–3487,\nMay 2021.\n[37] Y.-W. Lu, C.-Y. Hsu, and K.-C. Huang, ‘‘An autoencoder gated recurrent\nunit for remaining useful life prediction,’’ Processes, vol. 8, no. 9, p. 1155,\nSep. 2020.\n[38] C. Wang, N. Lu, S. Wang, Y. Cheng, and B. Jiang, ‘‘Dynamic long short-\nterm memory neural-network-based indirect remaining-useful-life prog-\nnosis for satellite lithium-ion battery,’’ Appl. Sci., vol. 8, no. 11, p. 2078,\nOct. 2018.\n[39] L. Lu, X. Han, J. Li, J. Hua, and M. Ouyang, ‘‘A review on the key issues\nfor lithium-ion battery management in electric vehicles,’’ J. Power Sources,\nvol. 226, pp. 272–288, Mar. 2013.\n[40] K. Goebel, B. Saha, A. Saxena, J. R. Celaya, and J. P. Christophersen,\n‘‘Prognostics in battery health management,’’ IEEE Instrum. Meas. Mag.,\nvol. 11, no. 4, pp. 33–40, Aug. 2008.\n[41] Y. Li, P. Chattopadhyay, S. Xiong, A. Ray, and C. D. Rahn, ‘‘Dynamic\ndata-driven and model-based recursive analysis for estimation of battery\nstate-of-charge,’’Appl. Energy, vol. 184, pp. 266–275, Dec. 2016.\n[42] X. Lai, W. Yi, Y. Cui, C. Qin, X. Han, T. Sun, L. Zhou, and Y. Zheng,\n‘‘Capacity estimation of lithium-ion cells by combining model-based\nand data-driven methods based on a sequential extended Kalman ﬁlter,’’\nEnergy, vol. 216, Feb. 2021, Art. no. 119233.\n[43] L. Ungurean, G. Cârstoiu, M. V. Micea, and V. Groza, ‘‘Battery state of\nhealth estimation: A structured review of models, methods and commercial\ndevices,’’Int. J. Energy Res., vol. 41, no. 2, pp. 151–181, 2017.\n[44] K. Sun and Q. Shu, ‘‘Overview of the types of battery models,’’ in Proc.\n30th Chin. Control Conf., Jul. 2011, pp. 3644–3648.\n[45] X. Hu, F. Sun, and Y. Zou, ‘‘Comparison between two model-based\nalgorithms for Li-ion battery SOC estimation in electric vehicles,’’ Simul.\nModel. Pract. Theory, vol. 34, pp. 1–11, 2013.\n[46] H. He, Y. Zhang, R. Xiong, and C. Wang, ‘‘A novel Gaussian model based\nbattery state estimation approach: State-of-energy,’’Appl. Energy, vol. 151,\npp. 41–48, Aug. 2015.\n[47] G. Zhao, G. Zhang, Y. Liu, B. Zhang, and C. Hu, ‘‘Lithium-ion battery\nremaining useful life prediction with deep belief network and relevance\nvector machine,’’ in Proc. IEEE Int. Conf. Prognostics Health Manage.\n(ICPHM), Jun. 2017, pp. 7–13.\n[48] G.-W. You, S. Park, and D. Oh, ‘‘Real-time state-of-health estimation for\nelectric vehicle batteries: A data-driven approach,’’ Appl. Energy, vol. 176,\npp. 92–103, Aug. 2016.\n[49] B. Gou, Y. Xu, and X. Feng, ‘‘State-of-health estimation and remaining-\nuseful-life prediction for lithium-ion battery using a hybrid data-driven\nmethod,’’ IEEE Trans. Veh. Technol., vol. 69, no. 10, p. 10854–10867,\n2020.\n[50] K. Liu, Y. Shang, Q. Ouyang, and W. D. Widanage, ‘‘A data-driven\napproach with uncertainty quantiﬁcation for predicting future capacities\nand remaining useful life of lithium-ion battery,’’ IEEE Trans. Ind. Elec-\ntron., vol. 68, no. 4, pp. 3170–3180, Apr. 2021.\n[51] X. Wu, W. Zeng, F. Lin, and X. Zhou, ‘‘NeuRank: Learning to rank\nwith neural networks for drug–target interaction prediction,’’ BMC Bioinf.,\nvol. 22, no. 1, pp. 1–17, Dec. 2021.\n[52] Z. Shi and A. Chehade, ‘‘A dual-LSTM framework combining change\npoint detection and remaining useful life prediction,’’ Rel. Eng. Syst. Saf.,\nvol. 205, Jan. 2021, Art. no. 107257.\n[53] J. Hong, D. Lee, E.-R. Jeong, and Y. Yi, ‘‘Towards the swift prediction\nof the remaining useful life of lithium-ion batteries with end-to-end deep\nlearning,’’Appl. Energy, vol. 278, Nov. 2020, Art. no. 115646.\n[54] L. Wu, S. Li, C.-J. Hsieh, and J. Sharpnack, ‘‘SSE-PT: Sequential rec-\nommendation via personalized transformer,’’ in Proc. 14th ACM Conf.\nRecommender Syst., Sep. 2020, pp. 328–337.\n[55] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang, ‘‘BERT4Rec:\nSequential recommendation with bidirectional encoder representations\nfrom transformer,’’ in Proc. 28th ACM Int. Conf. Inf. Knowl. Manage.,\nNov. 2019, pp. 1441–1450.\n[56] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and\nD. Tran, ‘‘Image transformer,’’ in Proc. Int. Conf. Mach. Learn., 2018,\npp. 4055–4064.\n[57] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, ‘‘Meshed-memory\ntransformer for image captioning,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2020, pp. 10578–10587.\n[58] J. W. Song, Y. I. Park, J.-J. Hong, S.-G. Kim, and S.-J. Kang, ‘‘Attention-\nbased bidirectional LSTM-CNN model for remaining useful life estima-\ntion,’’ in Proc. IEEE Int. Symp. Circuits Syst. (ISCAS), May 2021, pp. 1–5.\n[59] Y. Song, L. Li, Y. Peng, and D. Liu, ‘‘Lithium-ion battery remaining\nuseful life prediction based on GRU-RNN,’’ in Proc. 12th Int. Conf. Rel.,\nMaintainability, Saf. (ICRMS), Oct. 2018, pp. 317–322.\n[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[61] B. Saha and K. Goebel, Battery Data Set. Moffett Field, CA, USA: NASA\nAmes Prognostics Data Repository, NASA Ames Research Center, 2008.\n[62] B. Saha and K. Goebel, ‘‘Uncertainty management for diagnostics and\nprognostics of batteries using Bayesian techniques,’’ in Proc. IEEE Aerosp.\nConf., Mar. 2008, pp. 1–8.\n[63] W. He, N. Williard, M. Osterman, and M. Pecht, ‘‘Prognostics of\nlithium-ion batteries based on Dempster–Shafer theory and the Bayesian\nMonte Carlo method,’’ J. Power Sour., vol. 196, pp. 10314–10321,\nDec. 2011.\n[64] Y. Xing, E. W. M. Ma, K.-L. Tsui, and M. Pecht, ‘‘An ensemble model\nfor predicting the remaining useful performance of lithium-ion batteries,’’\nMicroelectron. Rel., vol. 53, pp. 811–820, Jun. 2013.\n[65] N. Williard, W. He, M. Osterman, and M. Pecht, ‘‘Comparative analysis\nof features for determining state of health in lithium-ion batteries,’’ Int. J.\nPrognostics Health Manage., vol. 4, no. 1, pp. 1–7, Oct. 2020.\n[66] B. Xiao, Y. Liu, and B. Xiao, ‘‘Accurate state-of-charge estimation\napproach for lithium-ion batteries by gated recurrent unit with ensemble\noptimizer,’’IEEE Access, vol. 7, pp. 54192–54202, 2019.\nDAOQUAN CHENreceived the B.S. degree from\nthe Zhejiang University of Science and Technol-\nogy, in 2012, and the M.S. degree from Zhejiang\nUniversity, in 2015. He is currently a Researcher\nwith the Zhejiang Institute of Mechanical and\nElectrical Engineering. His current research inter-\nests include machine learning, deep learning, and\nautomatic control.\nWEICONG HONGreceived the M.S. degree from\nthe Zhejiang University of Technology, in 2018.\nHe is currently a Cloud Computing Engineer with\nShuye Technology Company Ltd., China. His cur-\nrent research interests include cloud computing\nand the IoT applications.\nXIUZE ZHOU received the M.S. degree from\nXiamen University, in 2016. He is currently work-\ning as the Chief Technology Ofﬁcer at Shuye\nTechnology Company Ltd., China. His current\nresearch interests include machine learning, com-\nputer vision, and recommender systems.\n19628 VOLUME 10, 2022",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7256689071655273
    },
    {
      "name": "Computer science",
      "score": 0.6614000201225281
    },
    {
      "name": "Battery capacity",
      "score": 0.6330565214157104
    },
    {
      "name": "Noise reduction",
      "score": 0.5415042638778687
    },
    {
      "name": "Encoder",
      "score": 0.5058736205101013
    },
    {
      "name": "Artificial neural network",
      "score": 0.4800114035606384
    },
    {
      "name": "Battery (electricity)",
      "score": 0.47470417618751526
    },
    {
      "name": "Reliability engineering",
      "score": 0.3965941071510315
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36863958835601807
    },
    {
      "name": "Data mining",
      "score": 0.3520020842552185
    },
    {
      "name": "Electrical engineering",
      "score": 0.24706339836120605
    },
    {
      "name": "Voltage",
      "score": 0.21646961569786072
    },
    {
      "name": "Engineering",
      "score": 0.18342596292495728
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210147759",
      "name": "Zhejiang Institute of Mechanical and Electrical Engineering",
      "country": "CN"
    }
  ],
  "cited_by": 297
}