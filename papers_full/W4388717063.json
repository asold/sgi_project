{
  "title": "AI LANGUAGE MODELS, STANDARDIZED TESTS, AND ACADEMIC INTEGRITY: A CHAT (GPT)",
  "url": "https://openalex.org/W4388717063",
  "year": 2023,
  "authors": [],
  "references": [
    "https://openalex.org/W3177938779",
    "https://openalex.org/W4388143676"
  ],
  "abstract": "Language models’ popularity is on the rise, and with that, concerns about academic integrity in the times of such advanced Artificial intelligence (AI) tools are on the rise, too. Considering such concerns, this small study, which employs both qualitative and quantitative methods, thoroughly examines the role of language models, particularly ChatGPT, in the context of academic integrity. By assessing the accuracy of test answers generated by said language model, on questions from the state-issued high-school graduation English exam in N. Macedonia, and analyzing parts of essays generated using various prompts, the study aims to explore the potential implications of such AI tools on academic integrity in this new tech era. The study shows that ChatGPT's accuracy in providing test answers is satisfactory, with a minimal number of mistakes and over 80% accuracy on average, on both tests! As for the text/parts of essays generated by the model, the study has shown that the quality of the generated text differed based on the prompts that the user provided and their proficiency in articulating their specific demands. The study also showed that current AI detection remains unreliable at best. These findings contribute to the ongoing discourse on AI's influence on education and academic integrity, especially in regard to ChatGPT’s capabilities to generate content that can pass standardized tests and excel in open-ended writing tasks.",
  "full_text": " \n  \n\nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola  \n \n \n \n \n \n \n \n \n \n \n \n \n \nTEACHER \nInternational Journal of Education \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nVol. 26 \nBitola, November, 2023 \n \n \n \nPublisher \nFaculty of Education  ̶  Bitola \n \nExecutive and Editor-in-chief \nLjupco Kevereski, PhD, University St. Kliment Ohridski, Republic of North Macedonia \n \nEditorial Team \nLjupco Kevereski, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \nljupco.kevereski@uklo.edu.mk (Editor in-chief) \nJove Dimitrija Talevski, PhD, University St. Kliment Ohridski, Republic of North \nMacedonia, jove.talevski@uklo.edu.mk \nDance Sivakova-Neshkovski, PhD, University St. Kliment Ohridski, Republic of North \nMacedonia, dance.sivakova@uklo.edu.mk \nVioleta Janusheva, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \nvioleta.januseva@uklo.edu.mk \nJasminka Kochoska, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \njasminka.kochoska@uklo.edu.mk \nJosif Petrovski, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \njosif.petrovski@uklo.edu.mk \n  \nEditorial Board \nGrozdanka Gojkov, Higher Professional School for Education of Teachers „Mihailo \nPalov“ Serbia, g_gojkov@mts.rs \nMarjan Blažič, PhD, Faculty of Health Sciences, University of Novo Mesto, Slovenia, \nmarjan.blazic@uni-nm.si \nAnte Kolak, PhD, University of Zagreb, Croatia, akolak@ffzg.unizg.hr \nSvetlana Kurtesh, PhD, Faculty of Arts and Humanities, University of Madeira, Portugal \nDanimir Mandić, PhD, University of Belgrade, Serbia,danimir.mandic@uf.bg.ac.rs \nDanijela Kostadinović, PhD, University of Nish, Serbia, \ndanijela.kostadinovic@filfak.ni.ac.rs \nJasmina Starc, PhD, University of Novo Mesto, Slovenia, jasmina.starc@uni-nm.si \nMojca Jurishevich, PhD, University of Ljubljana, Slovenia, mojca.jurisevic@pef.uni-lj.si \nAnton Ilica, PhD, University „Aurel Vlaicu“, Romania, ilica_anton@yahoo.com \nEva Szórádová, PhD, Pedagogical faculty, Constantine the Philosopher University in Nitra, \nSlovakia, eszoradova@ukf.sk \nLazar Stoshić, PhD, University UNION Nikola Tesla,  Serbia, lazarstosic@yahoo.com \nAlla Belousova, PhD, Don State Technical University, Russia, belousovaak@gmail.com \nIrina Abakumova, PhD, Don State Technical University, Russia, abakira@mail.ru \nTom Jovanovski, PhD, USA \nZlatko Zhoglev, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \nzlatko.zhoglev@uklo.edu.mk \nValentina Gulevska, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \nvalentina.gulevska@uklo.edu.mk \nBiljana Gramatkovski, PhD, University St. Kliment Ohridski, Republic of North \nMacedonia, biljana.gramatkovski@uklo.edu.mk \nMilena Pejchinovska-Stojkovikj, PhD, University St. Kliment Ohridski, Republic of North \nMacedonia, milena.pejcinovska@uklo.edu.mk \nNikoleta Momčilović,  Faculty of Philosophy, University of Belgrade, Serbia, \nnikoleta.momcilovic@filfak.ni.ac.rs \nNamita Subiotto, Faculty of Arts, University of Ljubljana, Slovenia, \nnamita.subiotto@ff.uni-lj.si \n \nEnglish Language Editor \nStela Bosilkovska, MA, University St. Kliment Ohridski, Republic of North Macedonia, \nstela.bosilkovska@uklo.edu.mk \n  \nTechnical Editor \nJosif Petrovski, PhD, University St. Kliment Ohridski, Republic of North Macedonia, \njosif.petrovski@uklo.edu.mk  \n \n \n \n \nCIP - Cataloging in Publication, \nNational and University Library “St. Kliment Ohridski” – Skopje. \n \nTEACHER: Journal of the Faculty of Education  − Bitola / \n[Editorial Board Acad. Grozdanka Gojkov ... ] Year XXI, No. 2 (2023) -. \n- Bitola: Faculty of Education, 2023 -. - 29 cm., 65 p. \n \nUnspecified \n \nISSN 1857- 8888 (online) \n \n \n  \nUniversity “St. Kliment Ohridski“ – Bitola, North Macedonia \nFaculty of Education – Bitola, North Macedonia \n \n \n \n \n \n \n \nAddress: \nFaculty of Education \nul “Vasko Karangelevski“ b. b. \n7000 Bitola, North Macedonia \nTel/Fax. ++ 389 47 253 652; 203 385 \n \n \n \n \n \n \n \n \n \nWith the opinion of the Ministry of Culture no. 07 -2699/2 from 15.04.1998, for \nthe journal “Teacher” is paid preferential tax rate. In accordance with Article \n20, paragraph 8 of the VAT Law (Official Gazette 44/99), for the journal \n“Teacher” is paid a tax of 5%. \n \n \n \n \n \n \n \n \n \n \nThe journal has no commercial nature. \nTable of contents \n \n \n \n  \n\n \nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola \neISSN: 1857- 8888  \nUDK: 378.147.091.33:004.43(497.7) \nDOI: 10.20544/teacher.26.03 \n \n17 \n \n \nAI LANGUAGE MODELS, STANDARDIZED TESTS, AND ACADEMIC \nINTEGRITY: A CHAT (GPT) \n \nElena Shalevska \nUniversity “St. Kliment Ohridski” – Bitola, North Macedonia \nelena.shalevska@uklo.edu.mk \n \n \nAbstract  \nLanguage models’ popularity is on the rise, and with that, concerns about academic integrity in the \ntimes of such advanced Artificial intelligence (AI) tools are on the rise, too. Considering such concerns, this \nsmall study, which employs both qualitative and quantitative methods, thoroughly examines the role of language \nmodels, particularly ChatGPT, in the context of academic integrity. By assessing the accuracy of test answers \ngenerated by said language model, on questions from the state -issued high-school graduation English exam in \nN. Macedonia, and analyzing parts of essays generated using various prompts, the study aims to explore the \npotential implications of such AI tools on academic integrity in this new tech era.  \nThe study shows that ChatGPT's accuracy in providing test answers is satisfactory, with a minimal \nnumber of mistakes and over 80% accuracy on average, on both tests! As for the text/parts of essays generated \nby the model, the study has shown that the quality of the generated text differed based on the prompts that the \nuser provided and their proficiency in articulating their specific demands. The study also showed that current AI \ndetection remains unreliable at best. \nThese findings contribute to the ongoing discourse on AI's influence on education and academic \nintegrity, especially in regard to ChatGPT’s capabilities to generate content that can pass standardized tests and \nexcel in open-ended writing tasks. \n \nKeywords: Language models, ChatGPT, academic integrity, standardized tests, ESL. \n \n \n1.  Introduction: Language Models and Academic Integrity  \nWith the recent introduction of sophisticated language models like ChatGPT (Conversational \nGenerative Pre-training Transformer), the field of artificial intelligence (AI) has seen somewhat of a \nrevolution. These models, primarily based on the GPT -3.5 and GPT -4 architecture created by \nOpenAI, have notably changed how people interact with technology. They have given us a glimpse of \nthe future and the language an AI can produce and have shown us how these language models, unlike \nanything we have seen thus far, can be coherent, contextually relevant, and most of all, similar to \nhuman writing.  ChatGPT, InstructGPT's sibling model, was made to carry out instructions and offer \ndetailed responses to various prompts.  Using deep learning and unsupervised learning techniques, as \nwell as training the model on large text datasets, before continuing with task -specific fine -tuning, \nOpenAI has created the most sophisticated, free AI chatbot to date. Trained to understand and \ngenerate human-like text across multiple languages, ChatGPT aces question answering and sentence \ncompletion. Its capabilities have taken the world by storm and according to Mollman (2022), the \nmodel attracted over a million subscribers in just five days after going live.  \n \n2. Broader research context \nAs technology and AI have evolved, so have the ethical and academic concerns about the use \nof language models like ChatGPT. Academic integrity, a fundamental principle in education and \nresearch, encompasses the values of honesty, trust, fairness, respect, and responsibility. (Fishman, \n2014). It emphasizes the importance of proper attribution, accurate citing, objectivity, and originality \nin education, in general. Academic integrity is something that is and should be highlighted in all \nlevels of education, especially in high schools, as students are old enough to understand these \nprinciples of ethical conduct. Citing several authors working in the field, in her literature review, \nHolden et al. (2021) define academic dishonesty (sometimes referred to as cheating) as the use of \nunauthorized materials, helping others cheat, misrepresentation of self, and ultimately, all kinds of \nplagiarism. These definitions clearly show why academic integrity has become a critical concern with \n \nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola \neISSN: 1857- 8888  \nUDK: 378.147.091.33:004.43(497.7) \nDOI: 10.20544/teacher.26.03 \n \n18 \n \nthe widespread use of ChatGPT and similar language models. The ease with which the models can \ngenerate text that is indistinguishable from authentic student work is worrisome, to say the least. \nSome authors, like Susnjak (2022) even go as far as to claim that such language models may \nforever end the, albeit fragile, online exam integrity. Susnjak study has found that ChatGPT poses a \npotential risk to the integrity of online exams, particularly in tertiary education where these kinds of \nexams are becoming more common. The language model, in his view, exhibits critical thinking skills \non par with numerous students, and, more worryingly, can produce very realistic, human -like text \nwith little input and prompting. What Susnjak suggests to aid this issue is the implementation of oral \nexams, vigorous academic integrity measures, as well as the use of AI detectors. He cautions, though, \nthat these AI -text detectors, although useful, as of now, are still not a foolproof way to detect \nacademic dishonesty. \nThough exams that are fully online seem to be the ones facing major challenges, other non -\nonline assignments can be subject to ChatGPT -enabled cheating, as well. Take, for example, essay \nwriting. Having the students write an essay on a book or an argumentative essay on a given topic, has \nbeen the cornerstone of instruction for decades. Essay writing, or even report writing, has thus far \ninvolved meticulous research, hours upon hours of writing and editing, before submitting the final \nversion to one’s professor. Yet, with language models such as ChatGPT, essay writing became \nsomething that can be done in seconds, within clicks, with nothing but a prompt. As stated, ChatGPT \ncan generate human-like text as an answer to a prompt. The prompt can be as detailed or as simple as \nthe user decides and the chatbot’s answers will vary based on the user input. The chief problem with \nthese AI-generated essays is their detection. As mentioned, though valuable efforts have thus far been \nmade by AI -detection software such as GPTZero and Turnitin's AI writing indicator, false positives \nmake these detectors unreliable, which in turn, helps students cheat. It must be noted, that such \ncheating not only compromises the educational process, as a whole, but also devalues the efforts of \nthe students who don’t engage in academic dishonesty and invest time and effort into producing their \nown, authentic writings. And though according to some papers, ChatGPT can be successfully used in \ndifferent activities in the language classroom to improve students’ c ommunication and writing skills  \n(Shalevska, 2023), ethical considerations remain. \n \n2.1. Standardized Exams in N. Macedonia and Academic Dishonesty \nStandardized exams in multiple subjects mark the end of high school education in North \nMacedonia. These state-issued exit exams are essential for evaluating students’ academic progress in \nthe four years of compulsory gymnasium or vocational schools throughout the country. They \ndetermine how prepared high schoolers are for higher education and can influence students’ chances \nof being accepted into their higher education institution of choice. One important part of the language \nexams for both the native Macedonian language, as well as the second, English language, is the essay \nwriting section. These essays test students' communication and language skills as well as their critical \nthinking and analytical writing skills. Students are expected to write about their views clearly and \nconvincingly, showcasing their ability to craft well-organized arguments with minimal mistakes. \nThe Macedonian language exam, a compulsory component of the standardized testing regime \nof N. Macedonia, assesses students' proficiency in the native language. Yet, the essay writing task in \nthis exam evaluates not only language skills but also students’ literary interpretation skills. Students \nare expected to be able to write about a vast number of books and convey their thoughts about given \nauthors, styles, periods, and works through their essays. On the other hand, the English language \nexam evaluates students' English language skills in general. The inclusion of an essay writing task in \nthis exam does not focus on works of literature but allows students to showcase their language \nproficiency, as well as their ability to write about diverse topics in non-native English. \nStandardized exams everywhere seem to always involve certain fraudulent behaviours, and \nNorth Macedonia is no exception. The 2022/2023 English graduation exam caused a l ot of \ncontroversies as around 7 000 Macedonian high school seniors were discovered to be part of a Viber \ngroup in which they shared answers to the test. The students seemed to communicate during the exam \nitself, which shows worrisome misconduct and alarming disrespect to th e no -phones-during-exams \npolicy (Sitel, 2023).  \n \nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola \neISSN: 1857- 8888  \nUDK: 378.147.091.33:004.43(497.7) \nDOI: 10.20544/teacher.26.03 \n \n19 \n \nAs for the cheating itself, students seemed to cheat on all exam tasks. What makes cheating \neven easier now, provided that they have access to their phone, are the AI -powered chatbots such as \nChatGPT, which can produce high-quality, coherent essays in a matter of seconds.  \n \n3. Methodology \nThis small -scale study uses a mixed -methods approach, combining qualitative and \nquantitative techniques, to comprehensively investigate the link between academic integrity and \nlanguage models such as ChatGPT. The study aims to shed light on the potential impact of AI tools on \nacademic integrity by evaluating their ability to provide correct test answers and “help” students with \ntheir writing tasks. \n \n3.1. Data Collection \n1. Test Answer Accuracy Assessment : A subset of questions from the standardized exam in \nEnglish in N. Macedonia is selected. ChatGPT’s answers are then compared with expert -reviewed \nanswers to evaluate the accuracy of responses produced by the language model. Results are \nstatistically analyzed. \n2. Essay and Text Analysis: Three different prompts are used to generate an essay introduction \nusing the free, 3.5 ChatGPT platform, which is deemed the most accessible. These outputs are then \nanalyzed qualitatively to identify possible patterns, overall coherence, and potential differences in \ngenerated texts due to user input. Furthermore, the AI -generated introductions are tested by AI -\ndetection software to test current detectors' capabilities. \nThis study was conducted with no external funding, ensuring unbiased analysis of the subject \nmatter. The author declares no conflicting interests that could influence the outcomes or compromise \nthis study’s objectivity. \n \n4. Results and discussion \n4.1. Test Answer Accuracy Results \nTo test the accuracy of the answers generated by ChatGPT 3.5, the two most recent \nstandardized high school exit exams were used: the English language exam from June and the one \nfrom August 2023. Both tests are identically composed, and include several sections that aim to test \nstudents’ language skills: \n  \n● Reading and Comprehension: Multiple choice questions to do with a given text \n● Reading and Comprehension: In-text gap completition with missing sentences \n● Use of English section: Word formation exercise \n● Use of English section: Keyword transformations \n● Use of English section: Sentence transformations \n● Open Cloze exercise  \n● Written Task: Email \n● Written Task: Essay \n \nThis study’s focus is on the language -use exam tasks as well as the reading and \ncomprehension tasks, as they are both machine -graded and follow a set of given answers deemed \ncorrect by experts. In terms of the first task, the language model produced satisfactory results in terms \nof accuracy, with only a few simple prompts: \n \n[1] Can you help me answer some questions to do with a text? \n[2] This is the text. I'll provide the questions in the next prompt. *text sent* \n[3] The questions are as follows: *questions provided* Please provide the answers only. No \nexplanations needed. \n \nUsing these comands, ChatGPT was then able to provide the answers to all 8 questions from \nthe June exam in less than 2 seconds. The answers provided were then compared to the provided key. \nCorrect answers were marked by the author with a “✔”, while incorrect ones – with an “X”: \n \nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola \neISSN: 1857- 8888  \nUDK: 378.147.091.33:004.43(497.7) \nDOI: 10.20544/teacher.26.03 \n \n20 \n \n \nJune exam results: \n(1.1) B. ✔ (1.2) C. ✔ \n(1.3) D. ✔ (1.4) A. ✔ \n(1.5) C. ✔ (1.6) A. X \n(1.7) B. X (1.8) B. ✔ \n \nAugust exam results:  \n(1.1) B. X (1.2) C. X \n(1.3) A. X (1.4) D. ✔ \n(1.5) C. ✔ (1.6) D. X \n(1.7) A. ✔ (1.8) D. X\nOne can easily note that ChatGPT’s accuracy in the first task was significantly lower in the \nAugust exam (37.5% accuracy) than in the June one (65% accuracy). Though the June results proved \nsatisfactory, ChatGPT seemed to struggle with the text from the August exam, due to possible text \ncomplexity. A similar, yet not as prominent discrepancy was noted in the second task as well, for \nwhich the following prompts were used: \n[1] Can you help me fill in some missing sentences in a text? \n[2] This is the text. I'll provide a list of sentences in the next prompt. Note that there are two \nextra sentences that you will not need to use.  *text sent* \n[3] The missing sentences are as follows: *questions provided* Please provide the answers \nonly. No explanations needed. Note that one answer can be used only once. \n \nGenerated answers for the June exam: \n(1) B. ✔ (2) E. X \n(3) F. ✔ (4) I. ✔ \n(5) H. X (6) C.✔ \n(7) D.✔ \n \nResults for the August exam: \n(1) G. ✔ (2) E. ✔ \n(3) D. X (4) A. ✔ \n(5) C. ✔ (6) F. ✔ \n(7) H. ✔\nWith 65% accuracy for the June exam and 87.5% for the August one, ChatGPT provided  \nmore than satisfactory results in the second task, though the complexity of the task itself seemed to \nchallenge the AI model more than changing single words. \nThe following, third section, which focused on single -word word formation, showed how \nmuch better the model is at simpler tasks. In this task, students are given a word that they have to \nchange (usually by adding prefixes or suffixes) to create a word that will seamlessly fit into the given \nsentence. To prompt ChatGPT to do this task, the following text-based command was used “Complete \nthe sentences (1 -10) by changing the given word (in brackets) to the correct form. Consider the \nexample. Provide answers only.” The answers were as follows: \n \nJune exam: \n1. understanding✔ \n2. ability✔ \n3. performance✔ \n4. impractical✔ \n5. approval✔ \n6. diligently✔ \n7. familiarize X \n8. infrequent ✔ \n9. encouraged✔ \n10. applicants✔ \n \nAugust exam: \n1. healthier✔ \n2. annoying✔ \n3. careless✔ \n4. satisfaction✔ \n5. indecisive✔ \n6. unimpressive✔ \n7. acceptance✔ \n8. sympathetic✔ \n9. reasonable✔ \n10. beautifully✔ \n \nChatGPT clearly aced this section with a near -perfect (90%) accuracy score in the June exam \nand a perfect score in the August one. This suggests that ChatGPT has a great ability to understand \nword formation rules and apply them accurately, to form a corresponding word that will fit the given \n \nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola \neISSN: 1857- 8888  \nUDK: 378.147.091.33:004.43(497.7) \nDOI: 10.20544/teacher.26.03 \n \n21 \n \ncontext. Being pre -trained on a large corpus and being adept at contextual understanding, ChatGPT \nused its pattern recognition abilities to excel in this task. \nSimilar results were observed in the fourth section, the so -called, keyword transformation \nexercise. In this task, students had to transform a series of sentences, using a given keyword, to ensure \nboth sentences have the same/near -same meaning and the keyword isn’t changed. To prompt \nChatGPT to do these transformations, the following command was used: “Using the capitalized word, \ncomplete the second sentence so that its meaning is the same as that of the first sentence. Do not \nchange the key word. Use between 2 to 5 words. Contractions are also allowed. Consider the  \nexample.” \n \nJune results: \n1. It’s the MOST exciting match we have ever been to. ✔ \n2. It took her a while USED TO helping with the household chores. X \n3. Jenny is so scared of dogs that she MUST have been bitten by a dog when she was little. ✔ \n4. In SPITE OF not feeling well yesterday, he went to work. ✔ \n5. My grandfather takes PRIDE in his collection of watches. ✔ \n \nAugust results: \n1. My friend ADVISED me NOT TO spend so much money on a gym subscription. ✔ \n2. The view from the skyscraper IS AS GOOD AS the one from the mountaintop. X \n3. If ONLY YOU HAD COME to the planetarium with us. ✔ \n4. If you hadn't forgotten your glasses, you WOULD HAVE BEEN able to read the subtitles now. ✔ \n5. It IS BELIEVED that robots will replace many jobs in the future. ✔ \n \nWith an 80% accuracy rate and only 2/10 mistakes, the AI model exhibited great ability to \nmanipulate language and maintain the meaning of sentences while preserving the provided keyword. \nOther than some tense mix-ups that can be attributed to vague grammar rules, no other major \nissues were encountered in the fifth task, as well, which again focused on sentence transformations, \nhowever, with no given keyword. To complete the task, ChatGPT was given the following prompt: \n“Fill in the gap in the second sentence so that its meaning is the same as that of the first sentence. Use \nbetween 2 to 5 words. Contractions are acceptable. Consider the example. Provide answers only.” \n \n \nJune results: \n1. She can’t go out unless she finishes her \nhomework.✔ \n2. Tom has been accused of breaking into \nthe company’s system. ✔ \n3. Sarah said that she didn't cheat on the \nexam the day before. X \n4. Only then did we realize that the vehicle \nhad been stolen. ✔ \n5. If only he would stop making so much \nnoise upstairs.✔ \nAugust results: \n1. I stopped drinking coffee a week ago.✔ \n2. I had my eyesight checked last week.✔ \n3. My best friend has been in Paris for five \nyears. X \n4. Daniel said he had had that book for \nages.✔ \n5. Some remarkable discoveries have been \nmade by the research team. ✔\n \nWith an 80% accuracy, and only two mistakes due to subtle grammatical nuances and \nselecting the most appropriate verb forms, ChatGPT yet again demonstrated a superb understanding \nof the task.  As for the mistakes, according to the key, Past Perfect should have been used in the \nsentence no. 3 in the June exam and the Present Perfect Continuous should have been used in sentence \n3 in the August exam.  \n The last section of interest, the sixth section called open cloze, demanded that students fill in \nthe gaps in the given text with words they will think of themselves. ChatGPT was prompted to do this \n \nUniversity “St. Kliment Ohridski“ – Bitola  \nFaculty of Education – Bitola \neISSN: 1857- 8888  \nUDK: 378.147.091.33:004.43(497.7) \nDOI: 10.20544/teacher.26.03 \n \n22 \n \ntask using the following command: “Fill in each gap in the text using ONE word only. Provide \nanswers only.” \n \nJune results: \n1. is✔   2. to✔  3. comes✔ \n4. what✔  5. with✔ 6. of✔ \n7. taking✔ 8. towards✔ 9. worth✔ \n10. more✔ \n \nAugust results: \n1. an✔  2. back✔ 3. spend✔ \n4. getting✔ 5. amount✔ 6. our✔ \n7. through✔ 8. with✔ 9. put✔ \n10. by ✔\n \n23 \n \nChatGPT demonstrated a truly remarkable accuracy of 100%. The clear prompting and the \nnature of the task itself, which requires less creativity, fits well with GPT -3.5's inherent capabilities. \nIn addition, unlike some of the previous exam tasks, which involved more complex language skills, \nthe open cloze task seemed to be relatively straightforward, as gaps are to be filled with words that are \nan unambiguous fit. \nAll in all, with an average test accuracy score of about 80% on both exams, ChatGPT \nexcelled in almost all tasks, with no major issues other than some minor mistakes due to nuanced \ngrammar rules, and overall text complexity. Other than the tasks that required a deeper understanding \nof the content and context of the text (which, in turn, may demand a higher level of reasoning, even \nfrom humans) the AI model excelled in all tasks provided, as shown in Graph 1. Still, it's important to \nnote that while the model aced these tasks, there can still be instances where its answers might diverge \nfrom human evaluators due to the intricacies of language and interpretation.  \n \n \n \nGraph 1: Test Answers Accuracy \n \n \n4.2. Text Analysis \nDue to the limited length and scope of this paper, this research will only analyze AI-generated \nessay introductions for the June exam, based on user input and prompt detail. For this test, the essay \ntopic from the June exam was used, and 3 prompts were written by the author, all different in length \nand complexity, to examine the difference in AI -generated text based on user input. As such, the \nintroductions in Table 1 were generated: \nPrompt [1] Write an introduction for an \nessay on: \nDo you believe that a sincere \napology is the only way to resolve \nan argument and \npreserve a good friendship?  \n[2] Write a 50-word introduction \nfor an essay on: Do you believe \nthat a sincere apology is the only \nway to resolve an argument and \npreserve a good friendship? \nTake a YES or NO position and \nmention two reasons that \nsupport it. \n[3] Write an introduction (45-\n50 words in lenght) for an \nessay on: Do you believe that a \nsincere apology is the only way \nto resolve an argument and \npreserve a good friendship? \nState your position and mention \ntwo reasons that support it. Use \nformal yet simple English, \nwithout any difficult words, on \na  B1 level. Make 1 grammar \nand 1 spelling mistake. \n\n \n24 \n \nChatGPT-\ngenerated \ntext \n[1] In the intricate tapestry of \nhuman relationships, conflicts and \ndisagreements are inevitable. As \nwe navigate the complexities of \nour interactions, we often find \nourselves grappling with the \nquestion of how to mend the \nruptures that occur between us. At \nthe heart of this inquiry lies the \nconcept of a sincere apology—a \nseemingly simple gesture imbued \nwith profound implications. In the \ncontext of both resolving \narguments and preserving \ncherished friendships, the act of \napologizing stands as a pivotal \nbridge between hurt and healing, \nmisunderstanding and empathy. \nThis essay delves into the \nmultifaceted dynamics \nsurrounding the idea of a sincere \napology, exploring its potential as \nthe linchpin for conflict resolution \nand the safeguard for the \nendurance of meaningful \nconnections. By scrutinizing the \nrole of apology in these dual \ncontexts, we can unravel whether \na sincere apology is indeed the \nsole pathway to not only resolving \ndisputes but also nurturing the \nlongevity of valuable friendships. \n[2] In the intricate dynamics of \nhuman relationships, the pivotal \nrole of sincere apologies in \nconflict resolution and \nfriendship preservation is a \nsubject of contention. This essay \ntakes a firm stance, arguing that \na sincere apology is indeed the \nprimary means to mend disputes \nand uphold strong friendships. \nThe assertion is substantiated by \nits potential to heal emotional \nwounds and its capacity to \nrebuild trust on foundations of \nhumility and understanding. \n[3] In this essay, I will discuss \nwhether a sincere apology is \nthe sole solution for resolving \nconflicts and maintaining \nstrong friendships. I firmly \nbelieve that offering a genuine \napology plays a crucial role in \nrestoring harmony after \ndisputes. This viewpoint is \nsupported by its ability to mend \nhurt feelings and its potential to \nrebuild trust in relationships. \nTable 1: Introductions generated by ChatGPT \n \nThe first introduction, which follows a not -very-detailed prompt, appears to be the most \nformal. It is comprehensive and eloquently written, with a strong focus on a sophisticated vocabulary \nthat isn’t typically seen in spontaneous writings of non -native high school seniors. This formality \nmakes this introduction stand out and may be an indicator for the one doing the assessment that the \nstudent has used ChatGPT or similar software to write it.  Introductions such as this one, which \ndeviate significantly from what is typical for a non-native speaker at a certain age, may be flagged for \nfurther scrutiny. Similar observations can be made for Introduction 2, as well.  \nYet, Introduction 3 doesn’t seem to have such issues. Due to the well -crafted, detailed \nprompt, the text appears not only human -like but also more likely to have been written by a non -\nnative high school senior. This is due to its simpler language that corresponds with the B1 level. The \nslight grammar and spelling mistakes, which follow the prompt, may “trick” the assessor as they \nmight make the text seem like it has been composed by someone who is still developing their \nlanguage skills. \nAs one can easily note from the introductions above, user input significantly influences AI -\ngenerated texts. ChatGPT tailors its responses based on the prompts given, including formality, \nvocabulary, and language complexity. For instance, Introduction 1 clearly showcases a high level of \neloquence, likely influenced by the sparse prompt that didn't specify simplicity. On the other hand, \nIntroduction 3 demonstrates the impact of specific user requests for simplicity, formality, and even \nerrors. In this case, the AI responds to the prompt that explicitly mentions non -native language \nproficiency, and generates a simpler text. \nThe use of AI to generate human -like content raises concerns about academic integrity, \nespecially when text is generated using detailed prompts that may help it go unnoticed by assessors. \n \n25 \n \nAs AI tools like ChatGPT become more sophisticated, ethical considerations become even more \ncrucial in maintaining academic integrity. \n \n4.2.1. AI Detection \nThough AI detection as of now is said to be unreliable, for the purposes of this research, the \nthree introductions detailed above were checked by the ChatGPT-based detector called GPTZero – an \nAI tool that is meant to not only detect ChatGPT -generated texts but also texts generated by GPT4, \nBARD, Llama, and other AI models. The results obtained were as follows: \n● Introduction 1: A 50% probability this text was entirely written by AI; \n● Introduction 2: A 51% probability this text was entirely written by AI; \n● Introduction 3: A 52% probability this text was entirely written by AI. \n \nThe results obtained from GPTZero are interesting as they differ from the original analysis of \nthe three introductions presented above. Namely, although Introduction 3 appeared to be the one \nwhich is most likely to be written by a human (due to the spelling and grammar mistakes and the \noverall simpler vocabulary and sentence structure), the AI -detector results suggest that all three \nintroductions have a relatively close probability of being generated by AI. This difference between the \ninitial analysis and the AI detector's assessment further emphasizes the evolving landscape of AI \ntechnology and the challenges it poses to accurate detection. While AI detection such as the one \nprovided by GPTZero plays an important part in identifying potential academic dishonesty, the \nevolving capabilities of AI language models highlight the importance of critical thinking, creativity, \nand original thought in assignments – things that AI will have trouble replicating. \n \n5. Conclusion  \nIn the ongoing quest to determine the impact of AI on education, tests, and academic \nintegrity, this study focused on ChatGPT's prowess in generating accurate test answers to the \nstandardized English tests in North Macedonia and writing essay introductions.  While excelling in \nvarious tasks, ChatGPT made minor errors in some instances due to nuanced grammar rules and \noverall task complexity. As for the generated texts, the study showed that user input (in terms of \nprompts) significantly impacted generated text. These further highlights AI's adaptability in the \nlandscape of current technology.  \nEthical concerns to do with ChatGPT -generated content's resemblance to human writing \nfurther emphasize the need for clear guidelines and strict rules, as well as a focus on nurturing critical \nthinking, creativity, and authenticity. Along the same line, the GPTZero -based AI detection results \nalso point out the evolving challenges in accurate AI identification, which, as of now, remains \nunreliable. All in all, this small study details a part of the evolving relationship between AI and \nacademics, prompting ongoing dialogue on ethics and AI. \n   \nReferences  \nFisher (2014). The fundamental values of academic integrity (2nd ed). International Center for \nAcademic Integrity.  \nHolden, O. L., Norris, M. E., & Kuhlmeier, V. A. (2021). Academic integrity in online assessment: A \nresearch review. Frontiers in Education, 6. \nMollman, S. (2022). ChatGPT gained 1 million users in under a week. Here's why the AI chatbot is \nprimed to disrupt search as we know it.  Retrieved from: https://finance.yahoo.com/news/chatgpt-gained-1-\nmillion-followers-224523258.html on 22.8.2023. \nShalevska, E. (2023). English teaching just got an upgrade: How AI is revolutionizing TESOL . \nInternational Journal of Education “Teacher”, 25. \nSitel. (2023). Grupno prepishuvanje vo Viber grupa na drzhavnata matura . Retrieved from: \nhttps://sitel.com.mk/grupno-prepishuvanje-vo-viber-grupa-na-drzhavnata-matura on 22.8.2023. \nSusnjak, T. (2022).  ChatGPT: The End of Online Exam Integrity?  Retrieved from : \nhttps://arxiv.org/abs/2212.09292 on 21.8.2023. \n \n  ",
  "topic": "Academic integrity",
  "concepts": [
    {
      "name": "Academic integrity",
      "score": 0.7641000747680664
    },
    {
      "name": "Computer science",
      "score": 0.6733802556991577
    },
    {
      "name": "Graduation (instrument)",
      "score": 0.6507079005241394
    },
    {
      "name": "Popularity",
      "score": 0.6476989984512329
    },
    {
      "name": "Context (archaeology)",
      "score": 0.639765739440918
    },
    {
      "name": "Test (biology)",
      "score": 0.5766521096229553
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.47841960191726685
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4506634473800659
    },
    {
      "name": "Natural language processing",
      "score": 0.44620364904403687
    },
    {
      "name": "Mathematics education",
      "score": 0.4310978651046753
    },
    {
      "name": "Psychology",
      "score": 0.3091188371181488
    },
    {
      "name": "Social psychology",
      "score": 0.10996899008750916
    },
    {
      "name": "Engineering",
      "score": 0.0911494791507721
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Library science",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": []
}