{
    "title": "Improving Context Aware Language Models",
    "url": "https://openalex.org/W2608912814",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5082970015",
            "name": "Aaron Jaech",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5087215613",
            "name": "Mari Ostendorf",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2557508245",
        "https://openalex.org/W2100664567",
        "https://openalex.org/W2294684023",
        "https://openalex.org/W1951216520",
        "https://openalex.org/W2005555645",
        "https://openalex.org/W2606347107",
        "https://openalex.org/W2514713644",
        "https://openalex.org/W2109816866",
        "https://openalex.org/W2020073413",
        "https://openalex.org/W2275625487",
        "https://openalex.org/W1965154800",
        "https://openalex.org/W2963773292",
        "https://openalex.org/W2311783643",
        "https://openalex.org/W2950473876",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W2197913429",
        "https://openalex.org/W1521626219",
        "https://openalex.org/W2155794909",
        "https://openalex.org/W2411934291",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2123845384",
        "https://openalex.org/W2399550240",
        "https://openalex.org/W2513257850",
        "https://openalex.org/W2472014052",
        "https://openalex.org/W2165279024"
    ],
    "abstract": "Increased adaptability of RNN language models leads to improved predictions that benefit many applications. However, current methods do not take full advantage of the RNN structure. We show that the most widely-used approach to adaptation (concatenating the context with the word embedding at the input to the recurrent layer) is outperformed by a model that has some low-cost improvements: adaptation of both the hidden and output layers. and a feature hashing bias term to capture context idiosyncrasies. Experiments on language modeling and classification tasks using three different corpora demonstrate the advantages of the proposed techniques.",
    "full_text": "Improving Context Aware Language Models\nAaron Jaech and Mari Ostendorf\n{ajaech, ostendor}@uw.edu\nAbstract\nIncreased adaptability of RNN language\nmodels leads to improved predictions that\nbeneﬁt many applications. However, cur-\nrent methods do not take full advantage\nof the RNN structure. We show that the\nmost widely-used approach to adaptation\n(concatenating the context with the word\nembedding at the input to the recurrent\nlayer) is outperformed by a model that has\nsome low-cost improvements: adaptation\nof both the hidden and output layers. and\na feature hashing bias term to capture con-\ntext idiosyncrasies. Experiments on lan-\nguage modeling and classiﬁcation tasks\nusing three different corpora demonstrate\nthe advantages of the proposed techniques.\n1 Introduction\nThe dominant paradigm for language model adap-\ntation relies on the notion of a domain. Domains\nare in many ways inadequate representations of\ncontext due to being ill-deﬁned, discrete and in-\ncomparable, and not reﬂective of the diversity of\nhuman language (Ruder et al., 2016). In context\naware language models, the notion of a domain\nis replaced with a set of context variables that\neach describe some aspect of the associated lan-\nguage such as the topic, time, or language. These\nvariables can be dynamically combined to create\na continuous representation of context as a low-\ndimensional embedding (Tang et al., 2016). The\ncontext variables and context embedding can then\nbe used to adapt a recurrent neural network lan-\nguage model (RNNLM).\nThe standard approach for using a context em-\nbedding to adapt an RNNLM is to simply con-\ncatenate the context representation with the word\nembedding at the input to the RNN (Mikolov and\nZweig, 2012). Optionally, the context embed-\nding is also concatenated with the output from\nthe recurrent layer so that the output layer can\nbe adapted as well. This basic strategy has been\nadopted for various types of adaptation such as for\nLM personalization (Wen et al., 2013; Li et al.,\n2016), adapting an LM to different genres of tele-\nvision shows (Chen et al., 2015), adapting to long\nrange dependencies in a document (Ji et al., 2015),\nsharing information in generative text classiﬁers\n(Yogatama et al., 2017), and in other cases as well.\nIn this paper, we study methods of improv-\ning the mechanism for using context variables\nfor adapting an RNNLM. The standard approach\nof adapting the hidden layer is equivalent to an\nadditive transformation of the hidden state. We\npropose complimenting this with a multiplicative\nrescaling at the hidden layer and show that it con-\nsistently helps when the language model is used\nas a generative text classiﬁer and can sometimes\nimprove perplexity.\nUsing context dependent bias vectors is one\nway to adapt the output layer but it becomes in-\nfeasible when both the vocabulary size and the\nnumber of contexts are large. The method from\nMikolov and Zweig (2012) of using the low-\ndimensional context embedding to adapt the out-\nput layer avoids the excessive memory issue of\ncontext-dependent bias vectors but our experi-\nments show that it does not capture isolated but\nimportant details. We propose a hashing technique\nto simultaneously beneﬁt from context-dependent\nweights and avoid the high memory cost. The\ncombination of the low-rank and hashing tech-\nniques for adapting the output layer shows a con-\nsistent improvement across our experiments on\nthree different corpora.\narXiv:1704.06380v1  [cs.CL]  21 Apr 2017\n2 Model\nOur model is built on top of a standard RNN lan-\nguage model. There are three key parts which we\ndiscuss below: how we represent context using a\nlow-dimensional embedding, the mechanism for\nusing the context embedding for adapting the re-\ncurrent layer, and the mechanisms for adapting the\noutput layer.\n2.1 Representing outside context\nWe assume access to one or more indicator vari-\nables, c1:n = c1,c2,...c n, that hold informa-\ntion about the outside context for each sentence.\nThese can be indicators for topic, geographic re-\ngion, time period, or other meta-data. In (Mikolov\nand Zweig, 2012) LDA topic vectors are used for\nthe outside context. In (Tang et al., 2016) the out-\nside context is a sentiment score and a product\nid for a product review dataset. We adopt their\nmethod of combining information from multiple\ncontext variables using a simple neural network.\nThis strategy is well-suited for the types of con-\ntext variables that we will see in our experiments,\nsuch as speaker identity. In other cases, it may be\nmore appropriate to use topic models (Chen et al.,\n2015; Ghosh et al., 2016) or an RNN (Hoang et al.,\n2016) to build the context representation.\nFor each context variableci, we learn an associ-\nated embedding matrix Ei, i= 1,...,n . If n= 1\nthen the embedding can directly be used as the\ncontext representation. Otherwise, a single layer\nneural network is used to combine the embeddings\nfrom the individual variables.\n⃗ c= tanh(\n∑\ni\nMiEici + b0)\nMi and b0 are parameters learned by the model.\nThe context embedding, ⃗ c, is used for adapting\nboth the hidden and the output layer of the RNN.\n2.2 Adapting the hidden layer\nThe equation for the hidden layer of an RNN is\nst = σ(U⃗ wt + Sst−1 + b1)\nwhere ⃗ wt is the word embedding of the t-th word,\nst−1 is the hidden state from the previous time step\nand σ is the activation function. To make use of\nthe context embedding, ⃗ c, for adapting the hidden\nlayer the term F⃗ cis inserted resulting in\nst = σ(U⃗ wt + Sst−1 + F⃗ c+ b1)\nWe refer to the insertion of theF⃗ cterm as an addi-\ntive adaptation of the hidden layer. It is equivalent\nto the unadapted version except with an adapted\nbias term. It can be implemented by simply con-\ncatenating the context vector ⃗ cwith the word em-\nbedding ⃗ wt at each timestep at the input to the re-\ncurrent layer.\nTo increase the adaptability of the hidden layer\nwe use a context-dependent multiplicative rescal-\ning of the hidden layer weights. The method is\nborrowed from Ha et al. (2016) where it is used\nfor dynamically adjusting the parameters of a lan-\nguage model in response to the previous words in\nthe sentence. Using this row rescaling technique\non top of the additive adaptation from above, the\nequation becomes\nst = σ(Cu⃗ c⊙U⃗ wt + Cw⃗ c⊙Sst−1 + F⃗ c+ b1)\nwhere Cu and Cw are parameters of the model and\n⊙is the elementwise multiplication operator. The\nelement-wise multiplication is a low-cost opera-\ntion and can even be pre-calculated so that model\nevaluation can happen with no extra computation\ncompared to a vanilla RNN.\n2.3 Adapting the output layer\nThe output probabilities of an RNN are given by\nyt = softmax(Vst + b2). In our case, we tie\nthe weights between the word embeddings in the\ninput and output layer: WT = V (Press and Wolf,\n2016; Inan et al., 2016).\nOne way of adapting the output layer is to let\neach context have its own bias vector. This re-\nquires the use of a matrix of size |V|×|C|, which\nmay be intractable when both |V|and |C|are\nlarge. Here, |V|is the size of the vocabulary\nand |C|is the total number of possible contexts.\nMikolov and Zweig (2012) use a low-rank factor-\nization of of the adaptation matrix, replacing the\n|V|×|C|matrix with the product of a matrixG of\nsize |V|×kand a context embedding ⃗ cof size k.\nyt = softmax(Vst + G⃗ c+ b2)\nThe total number of parameters is now a much\nmore manageable O(|V|+ ∑\ni |Ci|) instead of\nO(∑\ni |V||Ci|). The advantage of a low-rank\nadaptation is that it forces the model to share infor-\nmation between similar contexts. The disadvan-\ntage, is that important differences between similar\ncontexts can be lost.\nWe employ feature hashing to reduce the mem-\nory requirements but retain some of the beneﬁts of\nhaving an individual bias term for each context-\nword pair. The context-word pairs are hashed into\nbuckets and individual bias terms are learned for\neach bucket. The hashing technique relies on hav-\ning direct access to the context variablesc1:n. Rep-\nresenting context as a latent topic distribution pre-\ncludes the use of this hashing adaptation.\nThe choice of hashing function is motivated by\nwhat is easy and fast to perform inside the Ten-\nsorﬂow computation graph framework. If w is a\nword id and c1n are context variable ids then the\nhash table index is computed as\nhi(w,ci) = wr0 + ciri mod l\nwhere lis the size of the hash table and r0 and the\nri’s are all ﬁxed random integers. The value oflis\nusually set to a large prime number. The function\nH : Z →R maps hash indices to hash values and\nis implemented as a simple array.\nSince l is much smaller than the total num-\nber of inputs, there will be many hash collu-\nsions. Hash collusions are known to negatively\neffect the perplexity (Mikolov et al., 2011). To\ndeal with this issue, we restrict the hash table to\ncontext-word pairs that are observed in the training\ndata. A Bloom ﬁlter data structure records which\ncontext-word pairs are eligible to have entries in\nthe hash table. The design of this data structure\ntrades off a compact representation of set mem-\nbership against a small probability of false posi-\ntives (Bloom, 1970; Talbot and Brants, 2008; Xu\net al., 2011). A small amount of false positives\nis relatively harmless in this application because\nthey do not impair the ability of the Bloom ﬁlter\nto eliminate almost all of the hash collusions.\nThe function β : Z → [0,1] is used by the\nBloom ﬁlter to map hash indices to binary values.\nB(w,ci) =\n16∏\nj=1\nβ(hi,j(w,ci))\nThe hash functions hi,j are deﬁned in the same\nway as the hi’s above except that they use distinct\nrandom integers and the size of the table, l, can be\ndifferent. Because βis a binary function, the prod-\nuct B(w,ci) will always be zero or one. Thus, any\nword-context pairs not found in the Bloom ﬁlter\nwill have their hash values set to zero.\nSource Size Vocab. Context\nReddit 8,000K 68,000 Subreddit\nTwitter 77K 194 Language\nSCOTUS 864K 18,000 Case, Spkr., Role\nTable 1: Number of sentences, vocabulary size and\ncontext variables for the three corpora.\nThe ﬁnal expression for the hashed adaptation\nterm is given by\nHash(w,c1:n) =\nn∑\ni=1\nH(hi(w,ci))B(w,ci)\nyt = softmax(Vst +G⃗ c+b2 +Hash(wt,c1:n))\n3 Data\nThe experiments make use of three corpora cho-\nsen to give a diverse prospective on adaptation in\nlanguage modeling. Summary information on the\ndatasets (Reddit, Twitter, and SCOTUS) is pro-\nvided in Table 1 and each source is discussed in-\ndividually below. The Reddit and SCOTUS data\nare tokenized and lower-cased using the standard\nNLTK tokenizer (Bird et al., 2009).\nReddit Reddit is the world’s largest online dis-\ncussion forum and is comprised of thousands of\nactive subcommunities dedicated to a wide variety\nof themes. Our training data is 8 million sentences\nfrom Reddit comments during the month of April\n2015. The 68,000 word vocabulary is selected by\ntaking all tokens that occur at least 20 times in the\ntraining data. The remaining tokens are mapped to\na special UNK token leaving us with an OOV rate\nof 2.3%.\nThe context variable is the identity of the sub-\nreddit, i.e. community, that the comment came\nfrom. There are 5,800 subreddits with at least\n50 training sentences. The remaining ones are\ngrouped together in an UNK category. The largest\nsubreddit occupies just 4.5% of the data and the\nperplexity of the subreddit distribution is 742. By\nusing a large number of subreddits, we highlight\nan advantage of model adaptation which is to be\nable to use a single uniﬁed model instead of train-\ning thousands of separate models for each individ-\nual community. Similarly, using context depen-\ndent bias vectors for this data instead of the hash\nadaptation would require learning 400 million ad-\nditional parameters.\nTwitter The Twitter training data has 77,000\nTweets each annotated with one of nine languages:\nEnglish, German, Italian, Spanish, Portuguese,\nBasque, Catalan, Galician, and French. The cor-\npus was collected by combining resources from\npublished data for language identiﬁcation tasks\nduring the past few years. Sentences labeled as un-\nknown, ambiguous, or containing code-switching\nwere not included. The data is unbalanced across\nlanguages with more than 32% of the Tweets being\nSpanish and the smallest four languages (Italian,\nGerman, Basque, and Galician) all individually\nless than 1.5% of the total. There are 194 unique\ncharacter tokens in the vocabulary. Graphemes\nthat are surrogate-pairs in the UTF-16 encoding,\nsuch as emoji, are split into multiple vocabulary\ntokens. No preprocessing or tokenization is per-\nformed on this data except that newlines were re-\nplaced with spaces for convenience.\nSCOTUS Approximately 864,000 sentences of\ntraining data spanning arguments from 1990-\n2011. These are speech transcripts from the\nUnited States Supreme Court. Utterances are la-\nbeled with the case being argued (n=1,765), the\nspeaker id (n=2,276), and the speaker role (jus-\ntice, advocate, or unidentiﬁed). These three con-\ntext variables are deﬁned in the same way as in\nHutchinson et al. (2013), where a small portion\nof this data was used in language modeling ex-\nperiments. The vocabulary size is around 18,000\nwords. Utterances longer than 45 words were split\ninto smaller utterances.\n4 Experiments\nIn these experiments we ﬁx the size of the word\nembedding dimensions and recurrent layers so as\nnot to exhaust our computational resources and\nthen vary the different mechanisms for adapting\nthe model. We used an LSTM with coupled input\nand forget gates for a 20% reduction in computa-\ntion time (Greff et al., 2016). Dropout was used as\na regularizer on the input and outputs of the recur-\nrent layer as described in Zaremba et al. (2014).\nWhen the vocabulary is large, computing the full\ncross-entropy loss can be prohibitively expensive.\nFor the large vocabulary experiments, we used a\nsampled softmax strategy with a unigram distribu-\ntion to speed up training (Jean et al., 2015).\nA summary of the key hyperparameters for each\nclass of experiments if given in Table 2. The total\nparameter column in this table is based on the un-\nParameter Reddit SCOTUS Twitter\nBatch Size 400 300 200\nWord Embed. 200 200 30\nLSTM Size 240 240 200\nDropout 0% 15% 10%\nNeg. Samples 100 100 NA\nTotal Params 14M 4M 300K\nTable 2: Summary of Key Hyperparamters\nadapted model. Adapted models will have more\nparameters depending on the type of adaptation.\nWhen using hash adaptation of the output layer,\nthe size of the Bloom ﬁlter is 100 million and the\nsize of the hash table is 80 million. The model is\nimplemented using the Tensorﬂow library.1 Opti-\nmization is done using Adam with a learning rate\nof 0.001. Each model trained in under three days\nusing 8 CPU threads.\nAlthough the model is trained as a language\nmodel, it can be used as a generative text classiﬁer.\nWhen there are multiple context variables, we treat\nall but one of them as known values and attempt\nto identify the unknown one. It is not necessary\nto compute the probabilities over the full vocabu-\nlary. The sampled softmax criteria can be used to\ngreatly speed up evaluation of the classiﬁer.\n4.1 Reddit Experiments\nThe size of the subreddit embeddings was set to\n25. Table 3 gives the perplexities and average\nAUCs for subreddit detection for different adapted\nmodels.The evaluation data contains 60,000 sen-\ntences. For comparison, an unadapted 4-gram\nKneser-Ney model trained on the same data has\na perplexity of 119. The models with the best per-\nplexity do not use multiplicative adaptation of the\nhidden layer, but it is useful in the detection exper-\niments.\nWe can inspect the context embeddings learned\nby the model to see if it is exploiting similarities\nbetween subreddits in the way that we expect. Ta-\nble 4 lists the nearest neighbors by Euclidean dis-\ntance to three selected subreddits. We can see\nthat the nearest neighbors match our intuitions.\nThe closest subreddits to Pittsburgh are commu-\nnities created for other big cities and states. The\nPython subreddit is close to other programming\nlanguages’ communities, and the NBA subreddit\n1See https://github.com/ajaech/calm for\ncode.\nHidd. Output\n× + LR Hash PPL ∆PPL AUC\nN N N N 75.2 – –\nN N N Y 69.6 7.3% 76.5\nN N Y N 68.0 9.5% 75.5\nN Y N Y 66.9 11.0% 78.9\nN Y Y N 68.0 9.6% 75.3\nN Y Y Y 66.5 11.5% 78.4\nY N Y Y 67.2 10.6% 78.9\nY Y Y N 68.3 9.1% 75.7\nY Y Y Y 67.1 10.7% 79.2\nTable 3: Perplexities and Classiﬁcation Avg.\nAUCs for Reddit Models\nPittsburgh Python NBA\nAtlanta CSharp Warriors\nMontana JavaScript Rockets\nMadisonWI CPP Questions Mavericks\nBaltimore CPP NBASpurs\nTable 4: Nearest neighbors to selected subreddits\nin the context embedding space.\nis close to the communities for individual NBA\nteams.\nThe subreddit detection involves predicting the\nsubreddit a given comment came from with eight\nsubreddits to choose from (AskMen, AskScience,\nAskWomen, Atheism, ChangeMyView, Fitness,\nPolitics, and Worldnews) and nine distractors\n(Books, Chicago, NYC, Seattle, ExplainLikeIm-\nFive, Science, Running, NFL, and Today-\nILearned).2 To make a classiﬁcation decision we\nevaluate the perplexity of each comment under the\nassumption that it belongs to each of the eight\nsubreddits. We use z-score normalization across\nthe eight perplexities to create a score for each\nclass. The predictions are evaluated by averaging\nthe AUC of the eight individual ROC curves. The\nbest model for the classiﬁcation task uses all four\ntypes of adaptation. Interestingly, the multiplica-\ntive adaptation of the hidden layer is clearly useful\nfor classiﬁcation even though it does not help with\nperplexity.\nThe perplexities for selected large subreddits\nare listed in Table 5. It can be seen that the\nrelative gain from adaptation is largest when the\ntopic of the subreddit is more narrowly focused.\nThe biggest gains were achieved for subreddits\n2These are the same subreddit used in Tran and Ostendorf\n(2016) for a related but not comparable classiﬁcation task.\ndedicated to speciﬁc sports, tv shows, or video\ngames. Whereas, the gains were smallest for sub-\nreddits like Videos or Funny whose content tends\nto be more diverse. The knowledge that a sen-\ntence came from a pro-wrestling subreddit effec-\ntively provides more information about the text\nthan the analogous piece of knowledge for the Pics\nor Videos subreddit. This would seem to indicate\nthat further gains could be possible if additional\ncontextual information could be provided. An\nalternative explanation, that subreddits with less\nsentences in the training data receive more beneﬁt\nfrom adaptation, is not supported by the data.\n4.2 Twitter experiments\nThe Twitter evaluation was done on a set of 14,960\nTweets. The language context embedding vector\ndimensionality was set to 8. When both the vo-\ncabulary and the number of contexts are small, as\nin this case, there is no danger of hash collusions.\nWe disable the bloom ﬁlter making the hash adap-\ntation essentially equivalent to having context de-\npendent bias vectors.\nTable 6 reports the results of the experiments\non the Twitter corpus. We compute both the per-\nplexity and measure the performance of the mod-\nels on a language identiﬁcation task. In terms of\nperplexity, the best models do not make use of the\nmultiplicative hidden layer adaptation, consistent\nwith the results from the Reddit corpus. In general,\nthe improvement in perplexity from adaptation is\nsmall (less than 5%) on this corpus compared to\nour other experiments where we saw relative im-\nprovements two to four times as big. This is likely\nbecause the LSTM can ﬁgure out by itself which\nlanguage it is modeling early on in the sequence\nand adjust its predictions accordingly.\nTo investigate this further, we trained a logistic\nregression classiﬁer to predict the language using\nthe state from the LSTM at the last time step on\nthe unadapted model as a feature vector. Using\njust 30 labeled examples per class it is possible to\nget 74.6% accuracy and a 49.3 F1 score. Further-\nmore, we ﬁnd that a single dimension in the hid-\nden state of the unadapted model is often enough\nto distinguish between different languages even\nthough the model was not given any supervision\nsignal (Karpathy et al., 2015; Radford et al., 2017).\nFigure 1 visualizes the value of the dimension of\nthe hidden layer that is the strongest indicator of\nSpanish on three different code-swtiched tweets.\nSubreddit Base. PPL Adapt. PPL ∆PPL Description\nFlashTV 90.5 68.2 24.6% A popular TV show\nshield 99.4 77.3 22.2% A tv show\nGlobalOffensive 97.1 79.3 18.3% A PC video game\nnba 103.3 86.4 16.3% National Basketball Association\nSquaredCircle 85.7 71.7 16.3% Professional Wrestling\nFitness 50.1 42.3 15.5% Exercise and ﬁtness\nhockey 85.5 72.4 15.2% Professional hockey\nleagueoﬂegends 71.1 61.0 14.3% A PC video game\npcmasterrace 71.7 62.0 13.5% PC gaming\nnﬂ 84.2 74.0 12.2% National Football League\nAskWomen 62.1 55.3 10.9% Questions for women\nnews 70.8 65.0 8.2% General news stories and discussion\nworldnews 85.7 79.7 7.1% Global news discussion\nAskMen 69.4 66.7 3.9% Questions for men\ngaming 79.0 76.1 3.7% General video games interest group\npics 74.0 71.8 3.0% Funny or interesting pictures\nvideos 62.9 61.1 2.9% Funny or interesting videos\nfunny 72.6 70.8 2.5% Sharing humorous content\nTable 5: Comparison of perplexities per subreddit\nHidden Output\n× + LR Hash PPL Acc. F1\nN N N N 6.44 – –\nN N N Y 6.43 56.1 44.0\nN N Y N 6.37 49.7 36.6\nN Y Y N 6.21 91.4 82.9\nN Y N Y 6.25 92.5 84.4\nN Y Y Y 6.15 92.8 85.2\nY N Y N 6.28 93.2 85.1\nY Y Y N 6.54 94.2 86.3\nY Y Y Y 6.35 93.3 85.9\nTable 6: Results on Twitter data.\nCode-switching is not a part of the training data\nfor the model but it provides a compelling visual-\nization of the ability of the unsupervised model to\nquickly recognize the language. The fact that it is\nso easy for the unadapted model to pick-up on the\nidentity of the contextual variable ﬁts with our ex-\nplanation for the small relative gain in perplexity\nfrom the adapted models.\nOur best model, using multiplicative adaptation\nof the hidden layer, achieves an accuracy of 94.2%\non this task. That is a 19% relative reduction in the\nerror rate from the best model without multiplica-\ntive adaptation.\nFigure 1: The value of the dimension of the hidden\nvector that is most correlated with Spanish text for\nthree different code-switched Tweets.\n4.3 SCOTUS experiments\nTable 7 lists the results for the experiments on the\nSCOTUS corpus. The size of the context embed-\ndings are 9, 15, and 8 for the case, speaker, and\nrole variables respectively. For calculating per-\nplexity we use 60,000 sentence evaluation set. For\nthe classiﬁcation experiment we selected 4,000\nsentences from the test data from eleven different\njustices and attempted to classify the identity of\nthe justice. The perplexity of the distribution of\njudges over those sentences is 8.9 (11.0 would be\nuniform). So, the data is roughly balanced. When\nclassifying justices, the model is given the case\ncontext variable, but we do not make any special\nHidden Output\n× + LR Hash PPL ∆PPL ACC\nN N N N 37.3 – –\nN N N Y 31.2 16.5% 29.6\nN N Y N 32.9 12.0% 26.2\nN Y Y N 32.7 12.4% 25.4\nN Y Y Y 29.8 20.3% 31.1\nY N Y N 32.3 13.4% 24.5\nY Y Y N 32.2 13.7% 26.1\nY N Y Y 29.2 21.7% 32.4\nY Y Y Y 29.4 21.1% 31.9\nTable 7: Results on the SCOTUS data in terms of\nperplexity and classiﬁcation accuracy (ACC) for\nthe justice identiﬁcation task.\nCase Spkr. Role PPL\nN N N 37.3\nN N Y 36.5\nN Y N 33.6\nN Y Y 33.3\nY N N 31.5\nY N Y 30.3\nY Y N 29.6\nY Y Y 29.4\nTable 8: Perplexities for different combinations of\ncontext variables on the SCOTUS corpus.\neffort to ﬁlter candidates based on who was serv-\ning on the court during that time, i.e. all eleven\njustices are considered for every case.\nFor both the perplexity and classiﬁcation met-\nrics, the hash adaptation makes a big difference.\nThe model that uses only hash adaptation and\nno hidden layer adaptation has a better perplex-\nity than any of the model variants that use both\nhidden adaptation and low-rank adaptation of the\noutput layer.\nTo ascertain which of the context variables have\nthe most impact, we trained additional models\nwith using different combinations of context vari-\nables. The model architecture is the one that uses\nall four forms of adaptation. Results are listed\nin Table 8. The most useful variable is the in-\ndicator for the case. The role variable is highly\nredundant— almost every speaker only appears in\na single role. The experiments indicate that the\nrole variable provides useful information to the\nmodel, and the knowledge of the speaker identity\nseems to not convey much useful information be-\nyond what is provided by the role.\nIn Table 9 we list sentences generated from the\nfully adapted model (same one as the last line in\nTable 7) using beam search. The value of the con-\ntext variable for the Case is held ﬁxed while we\nexplore different values for the Speaker and Role\nvariables. Anecdotally, we see that the model cap-\ntures some information about John Roberts role\nas chief justice. The model learns that Justice\nBreyer tends to start his questions with the phrase\n“I mean” while Justice Kagan tends to start with\n“Well”. Roberts and Kagan appear in our data both\nas justices and earlier as advocates.\n5 Related Work\nMultiple survey papers cover the early history of\nlanguage model adaptation (DeMori and Federico,\n1999; Bellegarda, 2004). We mention just the\nmost recent closely related work here.\nThe multiplicative rescaling of the recurrent\nlayer weights is used in the Hypernetwork model\n(Ha et al., 2016). The focus of this model is to al-\nlow the LSTM to adjust automatically depending\non the context of the previous words. This is dif-\nferent from our work in that we are adapting based\non contextual information external to the word se-\nquence. Gangireddy et al. (2016) also use a rescal-\ning of the hidden layer for adaptation but it is done\nas a ﬁne-tuning step and not during training like\nour model.\nThe RNNME model from Mikolov et al. (2011)\nuses feature hashing to train a maximum entropy\nmodel alongside an RNN language model. The\nsetup is similar to our method of using hashing\nto learn context-dependent biases. However, there\nare a number of differences. The motivation for\nthe RNNME model was to speed-up training of\nthe RNN not to compensate for the inadequacy of\nlow-rank output layer adaptation, which had yet to\nbe invented. Furthermore, Mikolov et al. (2011)\ndo not use context dependent features in the max-\nent component of the RNNME model nor do they\nhave a method for dealing with hash collusions\nsuch as our use of Bloom ﬁlters.\nThe idea of having one part of a language model\nbe low-rank and another part to be an additive\ncorrection to the low-rank model has been in-\nvestigated in other work (Eisenstein et al., 2011;\nHutchinson et al., 2013). In both of these cases,\nthe correction term is encouraged to be sparse\nby including an L1 penalty. Our implementation\ndid not promote sparsity in the hash adaptation\nSpkr. Role Sentence\nRoberts J. We’ll hear argument first this morning in Ayers.\nBreyer J. I mean, I don’t think that’s right.\nKagan J. Well, I don’t think that’s right.\nKagan A. Mr. Chief Justice, and may it please the court:\nBork A. --No, I don’t think so, your honor.\nTable 9: Sentences generated from the adapted model using beam search under different assumptions for\nspeaker and role contexts.\nfeatures but this idea is worth further considera-\ntion. The hybrid LSTM and count based language\nmodel is an alternative way of correcting for a low-\nrank approximation (Neubig and Dyer, 2016).\nHoang et al. (2016) studies how to incorporate\nside information into an RNN language model.\nFor their data, they claim a bigger win by adapt-\ning at the output layer rather than the hidden layer.\n(This matches our own observations on the Red-\ndit and SCOTUS data.) Their work did not ad-\ndress adapting at both the hidden and output lay-\ners simultaneously. Most work on adaptation does\nnot consider combining multiple context factors\nbut there are some exceptions (Hutchinson et al.,\n2013; Tang et al., 2016; Hoang et al., 2016).\n6 Conclusions & Future Work\nWhile our results suggest that there is not a one-\nsize-ﬁts-all approach to language model adapta-\ntion, it is clear that we improve over the standard\nadaptation approach. The model from Mikolov\nand Zweig (2012), equivalent to using just additive\nadaptation on the hidden layer and low-rank adap-\ntation of the output layer, is outperformed for all\nthree datasets at both the language modeling and\nclassiﬁcation tasks. For language modeling, the\nmultiplicative hidden layer adaptation was only\nhelpful for the SCTOUS dataset. However, the\ncombined low-rank and hash adaptation of the out-\nput layer consistently gave the best perplexity. For\nthe classiﬁcation tasks, the multiplicative hidden\nlayer adaptation is clearly useful, as is the com-\nbined low-rank and hash adaptation of the output\nlayer.\nImportantly, there is not always a strong re-\nlationship between perplexity and classiﬁcation\nscores. Our results may have implications for\nwork on text generation where it can be more de-\nsirable to have more control over the generation\nrather than the lowest perplexity model. More\nstudies are needed to get intuition about what\ntypes of context variables will provide the most\nbeneﬁt. Our investigation of the language con-\ntext in the Twitter experiments gives a useful take-\naway: context variables that are easily predictable\nfrom the text alone are unlikely to be helpful.\nIn future work, we would like to consider ad-\nditional mechanisms for using the context embed-\nding ⃗ cto adapt the LSTM parameters. We also\nplan to extend our hash adaptation to incorporate\nlonger word histories, rather than just unigrams\ncombined with context.\nReferences\nJerome R Bellegarda. 2004. Statistical language model\nadaptation: review and perspectives. Speech Com-\nmunication 42(1):93–108.\nSteven Bird, Ewan Klein, and Edward Loper.\n2009. Natural Language Processing with Python.\nO’Reilly Media.\nBurton H Bloom. 1970. Space/time trade-offs in hash\ncoding with allowable errors. Communications of\nthe ACM13(7):422–426.\nXie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,\nMoquan Wan, Mark JF Gales, and Philip C Wood-\nland. 2015. Recurrent neural network language\nmodel adaptation for multi-genre broadcast speech\nrecognition. In Proceedings of InterSpeech.\nRenato DeMori and Marcello Federico. 1999. Lan-\nguage model adaptation. In Computational models\nof speech pattern processing, Springer, pages 280–\n303.\nJacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.\nSparse additive generative models of text .\nSiva Reddy Gangireddy, Pawel Swietojanski, Peter\nBell, and Steve Renals. 2016. Unsupervised adap-\ntation of recurrent neural network language models.\nInterspeech 2016pages 2333–2337.\nShalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,\nTom Dean, and Larry Heck. 2016. Contextual\nLSTM (CLSTM) models for large scale NLP tasks.\narXiv preprint arXiv:1602.06291.\nKlaus Greff, Rupesh K Srivastava, Jan Koutn ´ık,\nBas R Steunebrink, and J ¨urgen Schmidhuber. 2016.\nLSTM: A search space odyssey. IEEE transactions\non neural networks and learning systems.\nDavid Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-\nnetworks. arXiv preprint arXiv:1609.09106.\nCong Duy Vu Hoang, Trevor Cohn, and Gholamreza\nHaffari. 2016. Incorporating side information into\nrecurrent neural network language models. In HLT-\nNAACL.\nBrian Hutchinson, Mari Ostendorf, and Maryam Fazel.\n2013. Exceptions in language as learned by the\nmulti-factor sparse plus low-rank language model.\nIn 2013 IEEE International Conference on Acous-\ntics, Speech and Signal Processing. IEEE, pages\n8580–8584.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large tar-\nget vocabulary for neural machine translation. In\nACL.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document context lan-\nguage models. CoRR abs/1511.03962.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A persona-based neural con-\nversation model. preprint arXiv:1603.06155.\nTom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk ´aˇs\nBurget, and Jan ˇCernock`y. 2011. Strategies for\ntraining large scale neural network language mod-\nels. In Automatic Speech Recognition and Under-\nstanding (ASRU), 2011 IEEE Workshop on. IEEE,\npages 196–201.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn SLT. pages 234–239.\nGraham Neubig and Chris Dyer. 2016. Generalizing\nand hybridizing count-based and neural language\nmodels. arXiv preprint arXiv:1606.00499.\nOﬁr Press and Lior Wolf. 2016. Using the output\nembedding to improve language models. arXiv\npreprint arXiv:1608.05859.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nSebastian Ruder, Parsa Ghaffari, and John G Bres-\nlin. 2016. Towards a continuous modeling\nof natural language domains. arXiv preprint\narXiv:1610.09158 .\nDavid Talbot and Thorsten Brants. 2008. Randomized\nlanguage models via perfect hash functions. InACL.\nvolume 8, pages 505–513.\nJian Tang, Yifan Yang, Sam Carton, Ming Zhang, and\nQiaozhu Mei. 2016. Context-aware natural lan-\nguage generation with recurrent neural networks.\narXiv preprint arXiv:1611.09900.\nTrang Tran and Mari Ostendorf. 2016. Character-\nizing the language of online communities and its\nrelation to community reception. arXiv preprint\narXiv:1609.04779 .\nTsung-Hsien Wen, Aaron Heidel, Hung-yi Lee,\nYu Tsao, and Lin-Shan Lee. 2013. Recurrent neural\nnetwork based language model personalization by\nsocial network crowdsourcing. In INTERSPEECH.\npages 2703–2707.\nPuyang Xu, Sanjeev Khudanpur, and Asela Gunawar-\ndana. 2011. Randomized maximum entropy lan-\nguage models. In Automatic Speech Recognition\nand Understanding (ASRU), 2011 IEEE Workshop\non. IEEE, pages 226–230.\nDani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-\nsom. 2017. Generative and discriminative text clas-\nsiﬁcation with recurrent neural networks.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329."
}