{
  "title": "Earthquake magnitude and location estimation from real time seismic waveforms with a transformer network",
  "url": "https://openalex.org/W3120604493",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4222765634",
      "name": "Münchmeyer, Jannes",
      "affiliations": [
        "Humboldt-Universität zu Berlin",
        "Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences"
      ]
    },
    {
      "id": null,
      "name": "Bindi, Dino",
      "affiliations": [
        "Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences"
      ]
    },
    {
      "id": "https://openalex.org/A2247720008",
      "name": "Leser, Ulf",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A3091305792",
      "name": "Tilmann, Frederik",
      "affiliations": [
        "Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences",
        "Freie Universität Berlin"
      ]
    },
    {
      "id": null,
      "name": "M\\\"unchmeyer, Jannes",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6713134421",
    "https://openalex.org/W6886164677",
    "https://openalex.org/W2112393517",
    "https://openalex.org/W6634817459",
    "https://openalex.org/W6923606586",
    "https://openalex.org/W3042453446",
    "https://openalex.org/W2789097411",
    "https://openalex.org/W2887617884",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6894266613",
    "https://openalex.org/W6903573648",
    "https://openalex.org/W2769419644",
    "https://openalex.org/W6923691062",
    "https://openalex.org/W6921505035",
    "https://openalex.org/W2047325657",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W2135293965",
    "https://openalex.org/W6940865724",
    "https://openalex.org/W6960345870",
    "https://openalex.org/W6922507753",
    "https://openalex.org/W3006351417",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2903747695",
    "https://openalex.org/W1540953370",
    "https://openalex.org/W6755477022",
    "https://openalex.org/W2891833294",
    "https://openalex.org/W1205391184",
    "https://openalex.org/W2913447624",
    "https://openalex.org/W2146871287",
    "https://openalex.org/W6959712758",
    "https://openalex.org/W2760189625",
    "https://openalex.org/W2992786058",
    "https://openalex.org/W2987173766",
    "https://openalex.org/W2895546528",
    "https://openalex.org/W6955750435",
    "https://openalex.org/W3084541745",
    "https://openalex.org/W2982591115",
    "https://openalex.org/W6936915202",
    "https://openalex.org/W6918474135",
    "https://openalex.org/W6921704639",
    "https://openalex.org/W6940105827",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W6744750712",
    "https://openalex.org/W6921633356",
    "https://openalex.org/W2899283552",
    "https://openalex.org/W6943415444",
    "https://openalex.org/W6962522155",
    "https://openalex.org/W6739365718",
    "https://openalex.org/W1970831581",
    "https://openalex.org/W2810984347",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W2938085975",
    "https://openalex.org/W6940514951",
    "https://openalex.org/W6921365956",
    "https://openalex.org/W3082436677",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6942484111",
    "https://openalex.org/W1579853615",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2921147132",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W3102041920",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W4231276321",
    "https://openalex.org/W3008601985",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2762410434",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3021034800",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "SUMMARY Precise real time estimates of earthquake magnitude and location are essential for early warning and rapid response. While recently multiple deep learning approaches for fast assessment of earthquakes have been proposed, they usually rely on either seismic records from a single station or from a fixed set of seismic stations. Here we introduce a new model for real-time magnitude and location estimation using the attention based transformer networks. Our approach incorporates waveforms from a dynamically varying set of stations and outperforms deep learning baselines in both magnitude and location estimation performance. Furthermore, it outperforms a classical magnitude estimation algorithm considerably and shows promising performance in comparison to a classical localization algorithm. Our model is applicable to real-time prediction and provides realistic uncertainty estimates based on probabilistic inference. In this work, we furthermore conduct a comprehensive study of the requirements on training data, the training procedures and the typical failure modes. Using three diverse and large scale data sets, we conduct targeted experiments and a qualitative error analysis. Our analysis gives several key insights. First, we can precisely pinpoint the effect of large training data; for example, a four times larger training set reduces average errors for both magnitude and location prediction by more than half, and reduces the required time for real time assessment by a factor of four. Secondly, the basic model systematically underestimates large magnitude events. This issue can be mitigated, and in some cases completely resolved, by incorporating events from other regions into the training through transfer learning. Thirdly, location estimation is highly precise in areas with sufficient training data, but is strongly degraded for events outside the training distribution, sometimes producing massive outliers. Our analysis suggests that these characteristics are not only present for our model, but for most deep learning models for fast assessment published so far. They result from the black box modeling and their mitigation will likely require imposing physics derived constraints on the neural network. These characteristics need to be taken into consideration for practical applications.",
  "full_text": "EARTHQUAKE MAGNITUDE AND LOCATION ESTIMATION FROM\nREAL TIME SEISMIC WAVEFORMS WITH A TRANSFORMER\nNETWORK\nA PREPRINT\nJannes Münchmeyer1,2,∗, Dino Bindi1, Ulf Leser2, Frederik Tilmann1,3\n1 Deutsches GeoForschungsZentrum GFZ, Potsdam, Germany\n2 Institut für Informatik, Humboldt-Universität zu Berlin, Berlin, Germany\n3 Insitut für geologische Wissenschaften, Freie Universität Berlin, Berlin, Germany\n∗To whom correspondence should be addressed: munchmej@gfz-potsdam.de\nApril 15, 2021\nABSTRACT\nPrecise real time estimates of earthquake magnitude and location are essential for early warning and\nrapid response. While recently multiple deep learning approaches for fast assessment of earthquakes\nhave been proposed, they usually rely on either seismic records from a single station or from a\nﬁxed set of seismic stations. Here we introduce a new model for real-time magnitude and location\nestimation using the attention based transformer networks. Our approach incorporates waveforms\nfrom a dynamically varying set of stations and outperforms deep learning baselines in both magnitude\nand location estimation performance. Furthermore, it outperforms a classical magnitude estimation\nalgorithm considerably and shows promising performance in comparison to a classical localization\nalgorithm. Our model is applicable to real-time prediction and provides realistic uncertainty estimates\nbased on probabilistic inference. In this work, we furthermore conduct a comprehensive study of\nthe requirements on training data, the training procedures and the typical failure modes. Using three\ndiverse and large scale data sets, we conduct targeted experiments and a qualitative error analysis.\nOur analysis gives several key insights. First, we can precisely pinpoint the effect of large training\ndata; for example, a four times larger training set reduces average errors for both magnitude and\nlocation prediction by more than half, and reduces the required time for real time assessment by a\nfactor of four. Second, the basic model systematically underestimates large magnitude events. This\nissue can be mitigated, and in some cases completely resolved, by incorporating events from other\nregions into the training through transfer learning. Third, location estimation is highly precise in\nareas with sufﬁcient training data, but is strongly degraded for events outside the training distribution,\nsometimes producing massive outliers. Our analysis suggests that these characteristics are not only\npresent for our model, but for most deep learning models for fast assessment published so far. They\nresult from the black box modeling and their mitigation will likely require imposing physics derived\nconstraints on the neural network. These characteristics need to be taken into consideration for\npractical applications.\n1 Introduction\nRecently, multiple studies investigated deep learning on raw seismic waveforms for the fast assessment of earthquake\nparameters, such as magnitude (e.g. Lomax et al. (2019), Mousavi & Beroza (2020), van den Ende & Ampuero (2020)),\nlocation (e.g. Kriegerowski et al. (2019), Mousavi & Beroza (2019), van den Ende & Ampuero (2020)) and peak ground\nacceleration (e.g. Jozinovi´c et al. (2020)). Deep learning is well suited for these tasks, as it does not rely on manually\nselected features, but can learn to extract relevant information from the raw input data. This property allows the models\nto use the full information contained in the waveforms of an event. However, the models published so far use ﬁxed time\nwindows and can not be applied to data of varying length without retraining. Similarly, except the model by van den\narXiv:2101.02010v2  [physics.geo-ph]  14 Apr 2021\nEarthquake magnitude and location estimation with TEAM-LM\nEnde & Ampuero (2020), all models process either waveforms from only a single seismic station or rely on a ﬁxed\nset of seismic stations deﬁned at training time. The model by van den Ende & Ampuero (2020) enables the use of a\nvariable station set, but combines measurements from multiple stations using a simple pooling mechanism. While it has\nnot been studied so far in a seismological context, it has been shown in the general domain that set pooling architectures\nare in practice limited in the complexity of functions they can model (Lee et al., 2019).\nHere we introduce a new model for magnitude and location estimation based on the architecture recently introduced\nfor the transformer earthquake alerting model (TEAM) (Münchmeyer et al., 2020a), a deep learning based earthquake\nearly warning model. While TEAM estimated PGA at target locations, our model estimates magnitude and hypocentral\nlocation of the event. We call our adaptation TEAM-LM, TEAM for location and magnitude estimation. We use\nTEAM as a basis due to its ﬂexible multi-station approach and its ability to process incoming data effectively in\nreal-time, issuing updated estimates as additional data become available. Similar to TEAM, TEAM-LM uses mixture\ndensity networks to provided probability distributions rather than merely point estimates as predictions. For magnitude\nestimation, our model outperforms two state of the art baselines, one using deep learning (van den Ende & Ampuero,\n2020) and one classical approach (Kuyuk & Allen, 2013). For location estimation, our model outperforms a deep\nlearning baseline (van den Ende & Ampuero, 2020) and shows promising performance in comparison to a classical\nlocalization algorithm.\nWe note a further deﬁciency of previous studies for deep learning in seismology. Many of these pioneering studies\nfocused their analysis on the average performance of the proposed models. Therefore, little is known about the conditions\nunder which these models fail, the impact of training data characteristics, the possibility of sharing knowledge across\nworld regions, and of speciﬁc training strategies. All of these are of particular interest when considering practical\napplication of the models.\nTo address these issues and provide guidance for practitioners, we perform a comprehensive evaluation of TEAM-LM\non three large and diverse data sets: a regional broadband data set from Northern Chile, a strong motion data set from\nJapan, and another strong motion data set from Italy. These data sets differ in their seismotectonic environment (North\nChile and Japan: subduction zones; Italy: dominated by both convergent and divergent continental deformation), their\nspatial extent (North Chile: regional scale; Italy and Japan: national catalogs), and the instrument type (North Chile:\nbroadband, Italy and Japan: strong motion). All three data sets contain hundreds of thousands of waveforms. North\nChile is characterized by a relatively sparse station distribution, but a large number of events and a low magnitude of\ncompleteness. There are far more stations in the Italy and Japan data sets, but a smaller number of earthquakes. This\nselection of diverse data sets allows for a comprehensive analysis, giving insights for different use cases. Our targeted\nexperiments show that the characteristics are rooted in the principle structure used by TEAM-LM, i.e., the black box\napproach of learning a very ﬂexible model from data, without imposing any physical constraints. As this black box\napproach is common to all current fast assessment models using deep learning, they can be transferred to these models.\nThis ﬁnding is further backed by comparison to the results from previous studies.\n2 Data and Methods\n2.1 Datasets\nFor this study we use three data sets (Table 1, Figure 1): one from Northern Chile, one from Italy and one from Japan.\nThe Chile data set is based on the catalog by Sippl et al. (2018) with the magnitude values from Münchmeyer et al.\n(2020b). While there were minor changes in the seismic network conﬁguration during the time covered by the catalog,\nthe station set used in the construction of this catalog had been selected to provide a high degree of stability of the\nlocations accuracy throughout the observational period (Sippl et al., 2018). Similarly, the magnitude scale has been\ncarefully calibrated to achieve a high degree of consistency in spite of signiﬁcant variations of attenuation (Münchmeyer\net al., 2020b). This data set therefore contains the highest quality labels among the data sets in this study. For the Chile\ndata set, we use broadband seismogramms from the ﬁxed set of 24 stations used for the creation of the original catalog\nand magnitude scale. Although the Chile data set has the smallest number of stations of the three data sets, it comprises\nthree to four times as many waveforms as the other two due to the large number of events.\nThe data sets for Italy and Japan are more focused on early warning, containing fewer events and only strong motion\nwaveforms. They are based on catalogs from the INGV (ISIDe Working Group, 2007) and the NIED KiKNet (National\nResearch Institute For Earth Science And Disaster Resilience, 2019), respectively. The data sets each encompass a\nlarger area than the Chile data set and include waveforms from signiﬁcantly more stations. In contrast to the Chile data\nsets, the station coverage differs strongly between different events, as only stations recording the event are considered.\nIn particular, KiKNet stations do not record continuous waveforms, but operate in trigger mode, only saving waveforms\nif an event triggered at the station. For Japan each station comprises two sensors, one at the surface and one borehole\nsensor. Therefore for Japan we have 6 component recordings (3 surface, 3 borehole) available instead of the 3 component\n2\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 1: Overview of the data sets. The top row shows the spatial station distribution, the second tow the spatial\nevent distribution. The event depth is encoded using color. Higher resolution versions of the maps can be found in the\nsupplementary material (Figures SM 1, SM 2, SM 3). The bottom row shows the distributions of the event magnitudes.\nThe magnitude scales are the peak displacement based MA, local magnitude ML, moment magnitude MW, body wave\nmagnitude mb and MJMA, a magnitude primarily using peak displacement.\n3\nEarthquake magnitude and location estimation with TEAM-LM\nTable 1: Overview of the data sets. The lower boundary of the magnitude category is the 5th percentile of the magnitude;\nthis limit is chosen as each data set contains a small number of unrepresentative very small events. The upper boundary\nis the maximum magnitude. Magnitudes are given with two digit precision for Chile, as the precision of the underlying\ncatalog is higher than for Italy and Japan. The Italy data set uses different magnitudes for different events, which are\nML (>90 % of the events), MW (<10 %) and mb (<1 %). For depth and distance minimum, median and maximum\nare stated. Distance refers to the epicentral distance between stations and events. Note that the count of traces refers\nto the number of waveform-triplets (for three components, or group of six waveforms for the Japanese stations). The\nsensor types are broadband (BB) and strong motion (SM).\nChile Italy Japan\nYears 2007 - 2014 2008 - 2019 1997 - 2018\nTraining 01/2007-08/2011 01/2008 - 12/2015 01/1997 - 03/2012\n& 01/2017 - 12/2019\nTest 08/2012 - 12/2014 01/2016 - 12/2016 08/2013 - 12/2018\nMagnitudes 1.21 - 8.27 2.7 - 6.5 2.7 - 9.0\nMagnitude scale MA ML, MW, mb MJMA\nDepth [km] 0 - 102 - 183 0 - 10 - 617 0 - 19 - 682\nDistance [km] 0.1 - 180 - 640 0.1 - 180 - 630 0.2 - 120 - 3190\nEvents 96,133 7,055 13,512\nUnique stations 24 1,080 697\nTraces 1,605,983 494,183 372,661\nTraces per event 16.7 70.3 27.6\nSensor type BB SM SM & SM-borehole\nCatalog source Münchmeyer et al. (2020b) INGV NIED\nrecordings for Italy and Chile. A full list of seismic networks used in this study can be found in the appendix (Table\nSM 1).\nFor each data set we use the magnitude scale provided in the catalog. For the Chile catalog, this is MA, a peak\ndisplacement based scale, but without the Wood-Anderson response and therefore saturation-free for large events\n(Münchmeyer et al., 2020b; Deichmann, 2018). For Japan MJMA is used. MJMA combines different magnitude scales,\nbut similarly to MA primarily uses horizontal peak displacement (Doi, 2014). For Italy the catalog provides different\nmagnitude types approximately dependent on the size of the event: ML (>90 % of the events), MW (<10 %) and mb\n(<1 %). We note that while the primary magnitude scales for all data sets are peak-displacement based, the precision\nof the magnitudes vary, with the highest precision for Chile. This might lead to slightly worse magnitude estimation\nperformance for Italy and Japan.\nFor all data sets the data were not subselected based on the type of seismicity but only based on the location (for\nChile and Italy) or depending if they triggered (Japan). This guarantees that, even though we made use of a catalog to\nassemble our training data, the resulting data sets are suitable for training and assessing methods geared at real-time\napplications without any prior knowledge about the earthquakes. We focus on earthquake characterization and do not\ndiscuss event detection or separation from noise; we refer the interested reader to Perol et al. (2018); Mousavi et al.\n(2018).\nWe split each data set into training, development and test set. For Chile and Japan we apply a simple chronological\nsplit with approximate ratios of 60:10:30 between training, development and test set, with the most recent events in\nthe test set. As the last 30% of the Italy data set consist of less interesting events for early warning, we instead use\nall events from 2016 as test set and the remaining events as training and development sets. We reserve all of 2016 for\ntesting, as it contains a long seismic sequence in central Italy with two mainshocks in August (MW = 6.5) and October\n(MW = 6.0). Notably, the largest event in the test set is signiﬁcantly larger than the largest event in the training set\n(Mw = 6.1 L’Aquila event in 2007), representing a challenging test case. For Italy, we assign the remaining events to\ntraining and development set randomly with a 6:1 ratio.\n2.2 The transformer earthquake alerting model for magnitude and location\nWe build a model for real time earthquake magnitude and location estimation based on the core ideas of the transformer\nearthquake alerting model (TEAM), as published in Münchmeyer et al. (2020a). TEAM is an end-to-end peak ground\nacceleration (PGA) model calculating probabilistic PGA estimates based on incoming waveforms from a ﬂexible set of\nstations. It employs the transformer network method (Vaswani et al., 2017), an attention based neural network which\nwas developed in the context of natural language processing (NLP), at the core of its algorithm. Here, we adapt TEAM\n4\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 2: Overview of the adapted transformer earthquake alerting model, showing the input, the feature extraction, the\nfeature combination, the magnitude/location estimation and the output. For simplicity, not all layers are shown, but\nonly their order and combination is visualized schematically. For the exact number of layers and the size of each layer\nplease refer to tables SM 2 to SM 4. Please note that the number of input stations is variable, due to the self-attention\nmechanism in the feature combination.\n5\nEarthquake magnitude and location estimation with TEAM-LM\nto calculate real time probabilistic estimates of event magnitude and hypocentral location. As our model closely follows\nthe architecture and key ideas of TEAM, we use the name TEAM-LM to refer to the location and magnitude estimation\nmodel.\nSimilar to TEAM, TEAM-LM consists of three major components (Figure 2): a feature extraction, which generates\nfeatures from raw waveforms at single stations, a feature combination, which aggregates features across multiple\nstations, and an output estimation. Here, we brieﬂy discuss the core ideas of the TEAM architecture and training and\nput a further focus on the necessary changes for magnitude and location estimation. For a more detailed account of\nTEAM and TEAM-LM we refer to Münchmeyer et al. (2020a), Tables SM 2 to SM 4 and the published implementation.\nThe input to TEAM consists of three component seismogramms from multiple stations and their locations. TEAM\naligns all seismogramms to start and end at the same times t0 and t1. We choose t0 to be 5 seconds before the ﬁrst P\narrival at any station. This allows the model to understand the noise conditions at all stations. We limit t1 to be at latest\nt0 + 30s. In a real-time scenario t1 is the current time, i.e., the available amount of waveforms, and we use the same\napproach to imitate real-time waveforms in training and evaluation. The waveforms are padded with zeros to a length of\n30 s to achieve constant length input to the feature extraction.\nTEAM uses a CNN architecture for feature extraction, which is applied separately at each station. The architecture\nconsists of several convolution and pooling layers, followed by a multi-layer perceptron (Table SM 2). To avoid scaling\nissues, each input waveform is normalized through division by its peak amplitude. As the amplitude is expected to\nbe a key predictor for the event magnitude, we provide the logarithm of the peak amplitude as a further input to the\nmulti-layer perceptron inside the feature extraction network. We ensure that this transformation does not introduce a\nknowledge leak by calculating the peak amplitude only based on the waveforms until t1. The full feature extraction\nreturns one vector for each station, representing the measurements at the station.\nThe feature vectors from multiple stations are combined using a transformer network (Vaswani et al., 2017). Trans-\nformers are attention based neural networks, originally introduced for natural language processing. A transformer\ntakes a set of nvectors as input, and outputs again nvectors which now incorporate the context of each other. The\nattention mechanism allows the transformer to put special emphasis on inputs that it considers particularly relevant and\nthereby model complex inter-station dependencies. Importantly, the parameters of the transformer are independent of\nthe number of input vectors n, allowing to train and apply a transformer on variable station sets. To give the transformer\na notion of the position of the stations, TEAM encodes the latitude, longitude and elevation of the stations using a\nsinusoidal embedding and adds this embedding to the feature vectors.\nTEAM adds the position embeddings of the PGA targets as additional inputs to the transformer. In TEAM-LM, we aim\nto extract information about the event itself, where we do not know the position in advance. To achieve this, we add\nan event token, which is a vector with the same dimensionality as the positional embedding of a station location, and\nwhich can be thought of as a query vector. This approach is inspired by the so-called sentence tokens in NLP that are\nused to extract holistic information on a sentence (Devlin et al., 2018). The elements of this event query vector are\nlearned during the training procedure.\nFrom the transformer output, we only use the output corresponding to the event token, which we term event embedding\nand which is passed through another multi-layer perceptron predicting the parameters and weights of a mixture of\nGaussians (Bishop, 1994). We use N = 5Gaussians for magnitude and N = 15Gaussians for location estimation.\nFor computational and stability reasons, we constrain the covariance matrix of the individual Gaussians for location\nestimation to a diagonal matrix to reduce the output dimensionality. Even though uncertainties in latitude, longitude\nand depth are known to generally be correlated, this correlation can be modeled with diagonal covariance matrices by\nusing the mixture.\nThe model is trained end-to-end using a log-likelihood loss with the Adam optimizer (Kingma & Ba, 2014). We train\nseparate models for magnitude and for location. As we observed difﬁculties in the onset of the optimization when\nstarting from a fully random initialization, we pretrain the feature extraction network. To this end we add a mixture\ndensity network directly after the feature extraction and train the resulting network to predict magnitudes from single\nstation waveforms. We then discard the mixture density network and use the weights of the feature extraction as\ninitialization for the end-to-end training. We use this pretraining method for both magnitude and localization networks.\nSimilarly to the training procedure for TEAM we make extensive use of data augmentation during training. First, we\nrandomly select a subset of up to 25 stations from the available station set. We limit the maximum number to 25 for\ncomputational reasons. Second, we apply temporal blinding, by zeroing waveforms after a random time t1. This type\nof augmentation allows TEAM-LM to be applied to real time data. We note that this type of temporal blinding to\nenable real time predictions would most likely work for the previously published CNN approaches as well. To avoid\nknowledge leaks for Italy and Japan, we only use stations as inputs that triggered before time t1 for these data sets. This\nis not necessary for Chile, as there the maximum number of stations per event is below 25 and waveforms for all events\n6\nEarthquake magnitude and location estimation with TEAM-LM\nare available for all stations active at that time, irrespective of whether the station actually recorded the event. Third,\nwe oversample large magnitude events, as they are strongly underrepresented in the training data set. We discuss the\neffect of this augmentation in further detail in the Results section. In contrast to the station selection during training, in\nevaluation we always use the 25 stations picking ﬁrst. Again, we only use stations and their waveforms as input once\nthey triggered, thereby ensuring that the station selection does not introduce a knowledge leak.\n2.3 Baseline methods\nRecently, van den Ende & Ampuero (2020) suggested a deep learning method capable of incorporating waveforms from\na ﬂexible set of stations. Their architecture uses a similar CNN based feature extraction as TEAM-LM. In contrast to\nTEAM-LM, for feature combination it uses maximum pooling to aggregate the feature vectors from all stations instead\nof a transformer. In addition they do not add predeﬁned position embeddings, but concatenate the feature vector for\neach station with the location coordinates and apply a multi-layer perceptron to get the ﬁnal feature vectors for each\nstation. The model of van den Ende & Ampuero (2020) is both trained and evaluated on 100 s long waveforms. In its\noriginal form it is therefore not suitable for real time processing, although the real time processing could be added with\nthe same zero-padding approach employed for TEAM and TEAM-LM. The detail differences in the CNN structure\nand the real-time processing capability make a comparison of the exact model of van den Ende & Ampuero (2020) to\nTEAM-LM difﬁcult.\nTo still compare TEAM-LM to the techniques introduced in this approach, we implemented a model based on the key\nconcepts of van den Ende & Ampuero (2020). As we aim to evaluate the performance differences from the conceptual\nchanges, rather than different hyperparameters, e.g., the exact size and number of the convolutional layers, we use the\nsame architecture as TEAM-LM for the feature extraction and the mixture density output. Additionally we train the\nmodel for real time processing using zero padding. In comparison to TEAM-LM we replace the transformer with a\nmaximum pooling operation and remove the event token.\nWe evaluate two different representations for the position encoding. In the ﬁrst, we concatenated the positions to the\nfeature vectors as proposed by van den Ende & Ampuero (2020). In the second, we add the position embeddings\nelement-wise to the feature vectors as for TEAM-LM. In both cases, we run a three-layer perceptron over the combined\nfeature and position vector for each station, before applying the pooling operation.\nWe use the fast magnitude estimation approach (Kuyuk & Allen, 2013) as a classical, i.e., non deep-learning, baseline\nfor magnitude. The magnitude is estimated from the horizontal peak displacement in the ﬁrst seconds of the P wave. As\nthis approach needs to know the hypocentral distance, it requires knowledge of the event location. We simply provide\nthe method with the catalog hypocenter. While this would not be possible in real time, and therefore gives the method\nan unfair advantage over the deep learning approaches, it allows us to focus on the magnitude estimation capabilities.\nFurthermore, in particular for Italy and Japan, the high station density usually allows for sufﬁciently well constrained\nlocation estimates at early times. For a full description of this baseline, see supplement section SM 2.\nAs a classical location baseline we employ NonLinLoc (Lomax et al., 2000) with the 1D velocity models from Graeber\n& Asch (1999) (Chile), Ueno et al. (2002) (Japan) and Matrullo et al. (2013) (Italy). For the earliest times after the\nevent detection usually only few picks picks are available. Therefore we apply two heuristics. Until at least 3/5/5\n(Chile/Japan/Italy) picks are available, the epicenter is estimated as the arithmetic mean of the stations with picked\narrivals so far, while the depth is set to the median depth in the training data set. Until at least 4/7/7 picks are available,\nwe apply NonLinLoc, but ﬁx the depth to the median depth in the data set. We require higher numbers of picks for\nItaly and Japan, as the pick quality is lower than in Chile but the station density is higher. This leads to worse early\nNonLinLoc estimates in Italy and Japan compared to Chile, but improves the performance of the heuristics.\n3 Results\n3.1 Magnitude estimation performance\nWe ﬁrst compare the estimation capabilities of TEAM-LM to the baselines in terms of magnitude (Figure 3). We\nevaluate the models at ﬁxed times t= 0.5 s, 1 s, 2 s, 4 s, 8 s, 16 s, 25 s after the ﬁrst P arrival at any station in the\nnetwork. In addition to presenting selected results here, we provide tables with the results of further experiments in the\nsupplementary material (Tables SM 6–SM 16).\nTEAM-LM outperforms the classical magnitude baseline consistently. On two data sets, Chile and Italy, the performance\nof TEAM-LM with only 0.5 s of data is superior to the baseline with 25 s of data. Even on the third data set, Japan,\nTEAM-LM requires only approximately a quarter of the time to reach the same precision as the classical baseline\nand achieves signiﬁcantly higher precision after 25 s. The RMSE for TEAM-LM stabilizes after 16 s for all data sets\n7\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 3: RMSE of the mean magnitude predictions from TEAM-LM, the pooling model with sinusoidal location\nembeddings (POOL-E), the pooling model with concatenated positions (POOL-C) and the classical baseline method.\nThe time indicates the time since the ﬁrst P arrival at any station, the RMSE is provided in magnitude units [m.u.]. Error\nbars indicate ±1 standard deviation when training the model with different random initializations. For better visibility\nerror bars are provided with a small x-offset. Standard deviations were obtained from six realisations. Note that the\nuncertainty of the provided means is by a factor\n√\n6 smaller than the given standard deviation, due to the number of\nsamples. We provide no standard deviation for the baseline, as it does not depend on a model initialization.\nFigure 4: RMSE comparison of the TEAM-LM mean magnitude predictions for different magnitude buckets. Linestyles\nindicate the model type: trained only on the target data (solid line), using transfer learning (dashed), classical baseline\n(dotted). For Chile/Italy/Japan we count events as small if their magnitude is below 3.5/3.5/4 and as large if their\nmagnitude is at least 5.5/5/6. The time indicates the time since the ﬁrst P arrival at any station, the RMSE is provided in\nmagnitude units [m.u.].\nwith ﬁnal values of 0.08 m.u. for Chile, 0.20 m.u. for Italy and 0.22 m.u. for Japan. The performance differences\nbetween TEAM-LM and the classical baseline result from the simpliﬁed modelling assumptions for the baseline. While\nthe relationship between early peak displacement and magnitude only holds approximately, TEAM-LM can extract\nmore nuanced features from the waveform. In addition, the relationship for the baseline was originally calibrated for a\nmoment magnitude scale. While all magnitude scales have an approximate 1:1 relationship with moment magnitude,\nthis might introduce further errors.\nWe further note that the performance of the classical baseline for Italy are consistent with the results reported by Festa\net al. (2018). They analyzed early warning performance in a slightly different setting, looking only at the 9 largest\nevents in the 2016 Central Italy sequence. However, they report a RMSE of 0.28 m.u. for the PRESTO system 4 s\nafter the ﬁrst alert, which matches approximately the 8 s value in our analysis. Similarly, Leyton et al. (2018) analyze\nhow fast magnitudes can be estimated in subductions zones, and obtain values of 0.01 ±0.28 across all events and\n−0.70 ±0.30 for the largest events (Mw >7.5) at 30 s after origin time. This matches the observed performance of the\nclassical baseline for Japan. For Chile, our classical baseline performs considerably worse, likely caused by the many\nsmall events with bad SNR compared to the event set considered by Leyton et al. (2018). However, TEAM-LM still\noutperforms the performance numbers reported by Leyton et al. (2018) by a factor of more than 2.\n8\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 5: True and predicted magnitudes without upsampling or transfer learning (left column), with upsampling\nbut without transfer learning (middle column) and with upsampling and transfer learning (right column). All plots\nshow predictions after 8 seconds. In the transfer column for Chile and Japan we show results after ﬁne-tuning on the\ntarget data set; for Italy we show results from the model without ﬁne-tuning as this model performed better. For the\nlargest events in Italy (M >4.5) we additionally show the results after ﬁne-tuning with pale blue dots. We suspect\nthe degraded performance in the ﬁne tuned model results from the fact, that the largest training event (MW = 6.1) is\nconsiderably smaller than the largest test event (MW = 6.5). Vertical lines indicate the borders between small, medium\nand large events as deﬁned in Figure 4. The orange lines show the running 5th, 50th and 95th percentile in 0.2 m.u.\nbuckets. Percentile lines are only shown if sufﬁciently many data points are available. The very strong outlier for Japan\n(true ∼7.3, predicted ∼3.3) is an event far offshore (>2000 km).\n9\nEarthquake magnitude and location estimation with TEAM-LM\nImprovements for TEAM-LM in comparison to the deep learning baseline variants are much smaller than to the classical\napproach. Still, for the Japan data set at late times, TEAM-LM offers improvements of up to 27 % for magnitude.\nFor the Italy data set, the baseline variants are on par with TEAM-LM. For Chile, only the baseline with position\nembeddings is on par with TEAM-LM. Notably, for the Italy and Japan data sets, the standard deviation between\nmultiple runs with different random model initialization is considerably higher for the baselines than for TEAM-LM\n(Figure 3, error bars). This indicates that the training of TEAM-LM is more stable with regard to model initialization.\nThe gains of TEAM-LM can be attributed to two differences: the transformer for station aggregation and the position\nembeddings. In our experiments we ruled out further differences, e.g. size and structure of the feature extraction CNN,\nby using identical network architectures for all parts except the feature combination across stations. Regarding the\nimpact of position embeddings, the results do not show a consistent pattern. Gains for Chile seem to be solely caused by\nthe position embeddings; gains for Italy are generally lowest, but again the model with position embeddings performs\nbetter; for Japan the concatenation model performs slightly better, although the variance in the predictions makes the\ndifferences non-signiﬁcant. We suspect these different patterns to be caused by the different catalog and network sizes\nas well as the station spacing.\nWe think that gains from using a transformer can be explained with its attention mechanism. The attention allows the\ntransformer to focus on speciﬁc stations, for example the stations which have recorded the longest waveforms so far.\nIn contrast, the maximum pooling operation is less ﬂexible. We suspect that the high gains for Japan result from the\nwide spatial distribution of seismicity and therefore very variable station distribution. While in Italy most events are in\nCentral Italy and in Chile the number of stations are limited, the seismicity in Japan occurs along the whole subduction\nzone with additional onshore events. This complexity can likely be handled better with the ﬂexibility of the transformer\nthan using a pooling operation. This indicates that the gains from using a transformer compared to pooling with position\nembeddings are likely modest for small sets of stations, and highest for large heterogeneous networks.\nIn many use cases, the performance of magnitude estimation algorithms for large magnitude events is of particular\nimportance. In Figure 4 we compare the RMSE of TEAM-LM and the classical baselines binned by catalog magnitude\ninto small, medium and large events. For Chile/Italy/Japan we count events as small if their magnitude is below\n3.5/3.5/4 and as large if their magnitude is at least 5.5/5/6. We observe a clear dependence on the event magnitude. For\nall data sets the RMSE for large events is higher than for intermediate sized events, which is again higher than for small\nevents. On the other hand the decrease in RMSE over time is strongest for larger events. This general pattern can also\nbe observed for the classical baseline, even though the difference in RMSE between magnitude buckets is smaller. As\nboth variants of the deep learning baseline show very similar trends to TEAM-LM, we omit them from this discussion.\nWe discuss two possible causes for these effects: (i) the magnitude distribution in the training set restricts the quality\nof the model optimization, (ii) inherent characteristics of large events. Cause (i) arise from the Gutenberg-Richter\ndistribution of magnitudes. As large magnitudes are rare, the model has signiﬁcantly less examples to learn from\nfor large magnitudes than for small ones. This should impact the deep learning models the strongest, due to their\nhigh number of parameters. Cause (ii) has a geophysical origin. As large events have longer rupture durations, the\ninformation gain from longer waveform recordings is larger for large events. At which point during the rupture the ﬁnal\nrupture size can be accurately predicted is a point of open discussion (e.g., Meier et al. (2017), Colombelli et al. (2020)).\nWe probe the likely individual contributions of these causes in the following.\nEstimations for large events not only show lower precision, but are also biased (Figure 5, middle column). For Chile\nand Italy a clear saturation sets in for large events. Interestingly the saturation starts at different magnitudes, which are\naround 5.5 for Italy and 6.0 for Chile. For Japan, events up to magnitude 7 are predicted without obvious bias. This\nsaturation behavior is not only visible for TEAM-LM, but has also been observed in prior studies, e.g., in Mousavi &\nBeroza (2020) (their Fig. 3, 4). In their work, with a network trained on signiﬁcantly smaller events, the saturation\nalready set in around magnitude 3. The different saturation thresholds indicate that the primary cause for saturation is\nnot the longer rupture duration of large events or other inherent event properties, as in cause (ii), but is instead likely\nrelated to the low number of training examples for large events, rendering it nearly impossible to learn their general\ncharacteristics, as in cause (i). This explanation is consistent with the much higher saturation threshold for the Japanese\ndata set, where the training data set contains a comparably large number of large events, encompassing the year 2011\nwith the Tohoku event and its aftershocks.\nAs a further check of cause (i), we trained models without upsampling large magnitude events during training, thereby\nreducing the occurrence of large magnitude events to the natural distribution observed in the catalog (Figure 5, left\ncolumn). While the overall performance stays similar, the performance for large events is degraded on each of the data\nsets. Large events are on average underestimated even more strongly. We tried different upsampling rates, but were not\nable to achieve signiﬁcantly better performance for large events than the conﬁguration of the preferred model presented\nin the paper. This shows that upsampling yields improvements, but can not solve the issue completely, as it does not\nintroduce actual additional data. On the other hand, the performance gains for large events from upsampling seem to\n10\nEarthquake magnitude and location estimation with TEAM-LM\ncause no observable performance drop for smaller event. As the magnitude distribution in most regions approximately\nfollows a Gutenberg-Richter law with b≈1, upsampling rates similar to the ones used in this paper will likely work for\nother regions as well.\nThe expected effects of cause (ii), inherent limitations to the predictability of rupture evolutions, can be approximated\nwith physical models. To this end, we look at the model from Trugman et al. (2019), which suggests a weak rupture\npredictability, i.e., predictability after 50 % of the rupture duration. Trugman et al. (2019) discuss the saturation of\nearly peak displacement and the effects for magnitude predictions based on peak displacements. Following their model,\nwe would expect magnitude saturation at approximately magnitude 5.7 after 1 s; 6.4 after 2 s; 7.0 after 4 s; 7.4 after\n8 s. Comparing these results to Figure 5, the saturation for Chile and Italy clearly occurs below these thresholds, and\neven for Japan the saturation is slightly below the modeled threshold. As we assumed a model with only weak rupture\npredictability, this makes it unlikely that the observed saturation is caused by limitations of rupture predictability.\nThis implies that our result does not allow any inference on rupture predictability, as the possible effects of rupture\npredictability are masked by the data sparsity effects.\n3.2 Location estimation performance\nWe evaluate the epicentral error distributions in terms of the 50th, 90th, 95th and 99th error percentiles (Figure 6). In\nterms of the median epicentral error, TEAM-LM outperforms all baselines in all cases, except for the classical baseline\nat late times in Italy. For all data sets, TEAM-LM shows a clear decrease in median epicentral error over time. The\ndecrease is strongest for Chile, going from 19 km at 0.5 s to 2 km at 25 s. For Italy the decrease is from 7 km to 2 km,\nfor Japan from 22 km to 14 km. For all data sets the error distributions are heavy tailed. While for Chile even the errors\nat high quantiles decrease considerably over time, these quantiles stay nearly constant for Italy and Japan.\nSimilar to the difﬁculties for large magnitudes, the characteristics of the location estimation point to insufﬁcient\ntraining data as source of errors. The Chile data set covers the smallest region and has by far the lowest magnitude\nof completeness, leading to the highest event density. Consequently the location estimation performance is best and\noutliers are very rare. For the Italy and Japan data sets, signiﬁcantly more events occurred in regions with only few\ntraining events, causing strong outliers. The errors for the Japanese data set are highest, presumably related to the large\nnumber of offshore events with consequently poor azimuthal coverage.\nWe expect a further difference from the number of unique stations. While for a small number of unique stations, as in the\nChile data set, the network can mostly learn to identify the stations using their position embeddings, it might be unable\nto do so for a larger number of stations with fewer training examples per station. Therefore the task is signiﬁcantly\nmore complicated for Italy and Japan, where the concept of station locations has to be learned simultaneously to the\nlocalization task. This holds true even though we encode the station locations using continuously varying position\nembeddings. Furthermore, whereas for moderate and large events waveforms from all stations of the Chilean network\nwill show the earthquake and can contribute information, the limitation to 25 stations of the current TEAM-LM\nimplementation does not allow a full exploitation of the information contained in the hundreds of recordings of larger\nevents in the Japanese and Italian data sets. This will matter in particular for out-of-network events, where the wavefront\ncurvature and thus event distance can only be estimated properly by considering stations with later arrivals.\nLooking at the classical baseline, we see that it performs considerably worse than TEAM-LM in the Chile data set in all\nlocation quantiles, better than TEAM-LM in all but the highest quantiles at late times in the Italy data set, and worse\nthan TEAM-LM at late times in the Japan data set. This strongly different behavior can largely be explained with the\npick quality and the station density in the different data sets. While the Chile data set contains high quality automatic\npicks, obtained using the MPX picker (Aldersons, 2004), the Italy data set uses a simple STA/LTA and the Japan data\nset uses triggers from KiKNet. This reduces location quality for Italy and Japan, in particular in the case of a low\nnumber of picks available for location. On the other hand, the very good median performance of the classical approach\nfor Italy can be explained from the very high station density, giving a strong prior on the location. An epicentral error of\naround 2 km after 8 s is furthermore consistent with the results from Festa et al. (2018). Considering the reduction in\nerror due to the high station density in Italy, we note that the wide station spacing in Chile likely caused higher location\nerrors than would be achievable with a denser seismic network designed for early warning.\nIn addition to the pick quality, the assumption of a 1D velocity model for NonLinLoc introduces a systematic error into\nthe localization, in particular for the subduction regions in Japan and Chile where the 3D structure deviates considerably\nfrom the 1D model. Because of these limitations the classical baseline could be improved by employing more proﬁcient\npickers or ﬁne-tuned velocity models. Nonetheless, in particular the results from Chile, where the classical baseline\nhas access to high quality P-picks, suggest that TEAM-LM can, given sufﬁcient training data, outperform classical\nreal-time localization algorithms.\n11\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 6: Violin plots and error quantiles of the distributions of the epicentral errors for TEAM-LM, the pooling\nbaseline with position embeddings (POOL-E), the pooling baseline with concatenated position (POOL-C), TEAM-LM\nwith transfer learning (TEAM-TRA) and a classical baseline. Vertical lines mark the 50th, 90th, 95th and 99th error\npercentiles, with smaller markers indicating higher quantiles. The time indicates the time since the ﬁrst P arrival at any\nstation. We compute errors based on the mean location predictions. A similar plot for hypocentral errors is available in\nthe supplementary material (Figure SM 4).\n12\nEarthquake magnitude and location estimation with TEAM-LM\nFor magnitude estimation no consistent performance differences between the baseline approach with position embed-\ndings and the approach with concatenated coordinates, as originally proposed by van den Ende & Ampuero (2020), are\nvisible. In contrast, for location estimation, the approach with embeddings consistently outperforms the approach with\nconcatenated coordinates. The absolute performance gains between the baseline with concatenation and the baseline\nwith embeddings is even higher than the gains from adding the transformer to the embedding model. We speculate\nthat the positional embeddings might show better performance because they explicity encode information on how to\ninterpolate between locations at different scales, enabling an improved exploitation of the information from stations\nwith few or no training examples. This is more important for location estimation, where an explicit notion of relative\nposition is required. In contrast, magnitude estimation can use further information, like frequency content, which is less\nposition dependent.\n3.3 Transfer learning\nA common strategy for mitigating data sparsity is the injection of additional information from related data sets through\ntransfer learning (Pan & Yang, 2009), in our use case waveforms from other source regions. This way the model is\nsupposed to be taught the properties of earthquakes that are consistent across regions, e.g., attenuation due to geometric\nspreading or the magnitude dependence of source spectra. Note that a similar knowledge transfer implicitly is part of\nthe classical baseline, as it was calibrated using records from multiple regions.\nHere, we conduct a transfer learning experiment inspired by the transfer learning used for TEAM. We ﬁrst train a\nmodel jointly on all data sets and then ﬁne-tune it to each of the target data sets. This way, the model has more training\nexamples, which is of special relevance for the rare large events, but still is adapted speciﬁcally to the target data set.\nAs the Japan and Italy data sets contain acceleration traces, while the Chile data set contains velocity traces, we ﬁrst\nintegrate the Japan and Italy waveforms to obtain velocity traces. This does not have a signiﬁcant impact on the model\nperformance, as visible in the full results tables (Tables SM 6 to SM 9).\nTransfer learning reduces the saturation for large magnitudes (Figure 5, right column). For Italy the saturation is even\ncompletely eliminated. For Chile, while the largest magnitudes are still underestimated, we see a clearly lower level\nof underestimation than without transfer learning. Results for Japan for the largest events show nearly no difference,\nwhich is expected as the Japan data set contains the majority of large events and therefore does not gain signiﬁcant\nadditional large training examples using transfer learning. The positive impact of transfer learning is also reﬂected\nin the lower RMSE for large and intermediate events for Italy and Chile (Figure 4). These results do not only offer a\nway of mitigating saturation for large events, but also represent further evidence for data sparsity as the reason for the\nunderestimation.\nWe tried the same transfer learning scheme for mitigating mislocations (Figure 6). For this experiment we shifted the\ncoordinates of stations and events such that the data sets spatially overlap. We note that this shifting is not expected\nto have any inﬂuence on the single data set performance, as the relative locations of events and stations within a data\nset stay unchanged and nowhere the model uses absolute locations. The transfer learning approach is reasonable, as\nmislocations might result from data sparsity, similarly to the underestimation of large magnitudes. However, none of\nthe models shows signiﬁcantly better performance than the preferred models, and in some instances performance even\ndegrades. We conducted additional experiments where shifts were applied separately for each event, but observed even\nworse performance.\nWe hypothesize that this behaviour indicates that the TEAM-LM localization does not primarily rely on travel time\nanalysis, but rather employs some form of ﬁngerprinting of earthquakes. These ﬁngerprints could be speciﬁc scattering\npatterns for certain source regions and receivers. Note that similar ﬁngerprints are exploited in the traditional template\nmatching approaches (e.g. Shelly et al. (2007)). While the travel time analysis should be mostly invariant to shifts and\ntherefore be transferable between data sets, the ﬁngerprinting is not invariant to shifts. This would also explain why the\ntransfer learning, where all training samples were already in the pretraining data set and therefore their ﬁngerprints could\nbe extracted, outperforms the shifting of single events, where ﬁngerprints do not relate to earthquake locations. Similar\nﬁngerprinting is presumably also used by other deep learning methods for location estimation, e.g., by Kriegerowski\net al. (2019) or Perol et al. (2018), however further experiments would be required to prove this hypothesis.\n4 Discussion\n4.1 Multi-task learning\nAnother common method to improve the quality of machine learning systems in face of data sparsity is multi-task\nlearning (Ruder, 2017), i.e., having a network with multiple outputs for different objectives and training it simultaneously\n13\nEarthquake magnitude and location estimation with TEAM-LM\non all objectives. This approach has previously been employed for seismic source characterisation (Lomax et al., 2019),\nbut without an empirical analysis on the speciﬁc effects of multi-task learning.\nWe perform an experiment, in which we train TEAM-LM to predict magnitude and location concurrently. The feature\nextraction and the transformer parts are shared and only the ﬁnal MLPs and the mixture density networks are speciﬁc\nto the task. This method is known as hard parameter sharing. The intuition is that the individual tasks share some\nsimilarity, e.g., in our case the correct estimation of the magnitude likely requires an assessment of the attenuation\nand geometric spreading of the waves and therefore some understanding of the source location. This similarity is then\nexpected to drive the model towards learning a solution for the problem that is more general, rather than speciﬁc to the\ntraining data. The reduced number of free parameters implied by hard parameter sharing is also expected to improve\nthe generality of the derived model, if the remaining degrees of freedom are still sufﬁcient to extract the relevant\ninformation from the training data for each sub-task.\nUnfortunately, we actually experience a moderate degradation of performance for either location or magnitude in any\ndata set (Tables SM 6 to SM 12) when following a multi-task learning strategy. The RMSE of the mean epicenter\nestimate increases by at least one third for all times and data sets, and the RMSE for magnitude stays nearly unchanged\nfor the Chile and Japan data sets, but increases by ∼20% for the Italy data set. Our results therefore exhibit a case of\nnegative transfer.\nWhile it is generally not known, under which circumstances multi-task learning shows positive or negative inﬂuence\n(Ruder, 2017), a negative transfer usually seems to be caused by insufﬁciently related tasks. In our case we suspect that\nwhile the tasks are related in a sense of the underlying physics, the training data set is large enough that similarities\nrelevant for both tasks can be learned already from a single objective. At the same time, the particularities of the\ntwo objectives can be learned less well. Furthermore, we earlier discussed that both magnitude and location might\nnot actually use travel time or attenuation based approaches, but rather frequency characteristics for magnitude and a\nﬁngerprinting scheme for location. These approaches would be less transferable between the two tasks. We conclude\nthat hard parameter sharing does not improve magnitude and location estimation. Future work is required to see if other\nmulti-task learning schemes can be applied beneﬁcially.\n4.2 Location outlier analysis\nAs all location error distributions are heavy tailed, we visually inspect the largest deviations between predicted and\ncatalog locations to understand the behavior of the localization mechanism of TEAM-LM. We base this analysis on the\nChile data set (Figure 7), as it has generally the best location estimation performance, but observations are similar for\nthe other data sets (Figures SM 5 and SM 6).\nNearly all mislocated events are outside the seismic network and location predictions are generally biased towards the\nnetwork. This matches the expected errors for traditional localization algorithms. In contrast to traditional algorithms,\nevents are not only predicted to be closer to the network, but they are also predicted as lying in regions with a higher\nevent density in the training set (Figure 7, inset). This suggests that not enough similar events were included in the\ntraining data set. Similarly, Kriegerowski et al. (2019) observed a clustering tendency when predicting the location of\nswarm earthquakes with deep learning.\nWe investigated two subgroups of mislocated events: the Iquique sequence, consisting of the Iquique mainshock,\nforeshocks and aftershocks, and mine blasts. The Iquique sequence is visible in the north-western part of the study area.\nAll events are predicted approximately 0.5◦too far east. The area is both outside the seismic network and has no events\nin the training set. This systematic mislocation may pose a serious threat in applications, such as early warning, when\nconfronted with a major change in the seismicity pattern, as is common in the wake of major earthquakes or during\nsudden swarm activity, which are also periods of heightened seismic hazard.\nFor mine blasts, we see one mine in the northeast and one in the southwest (marked by red circles in Figure 7). While\nall events are located close by, the location are both systematically mispredicted in the direction of the network and\nexhibit scatter. Mine-blasts show a generally lower location quality in the test set. While they make up only ∼1.8% of\nthe test set, they make up 8% of the top 500 mislocated events. This is surprising as they occur not only in the test set,\nbut also in similar quantities in the training set. We therefore suspect that the difﬁculties are caused by the strongly\ndifferent waveforms of mine blasts compared to earthquakes. One waveform of each a mine blast and an earthquake,\nrecorded at similar distances are shown as inset in Figure 7. While for the earthquake both a P and S wave are visible,\nthe S wave can not be identiﬁed for the mine blast. In addition, the mine blast exhibits a strong surface wave, which is\nnot visible for the earthquake. The algorithm therefore can not use the same features as for earthquakes to constrain the\ndistance to a mine blast event.\n14\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 7: The 200 events with the highest location errors in the Chile data set overlayed on top of the spatial event\ndensity in the training data set. The location estimations use 16 s of data. Each event is denoted by a yellow dot for the\nestimated location, a green cross for the true location and a line connecting both. Stations are shown by black triangles.\nThe event density is calculated using a Gaussian kernel density estimation and does not take into account the event\ndepth. The inset shows the event density at the true event location in comparison to the event density at the predicted\nevent location for the 200 events. Red circles mark locations of mine blast events. The inset waveforms show one\nexample of a waveform from a mineblast (top) and an example waveform of an earthquake (bottom, 26 km depth) of\nsimilar magnitude (MA = 2.5) at similar distance (60 km) on the transverse component. Similar plots for Italy and\nJapan can be found in the supplementary material (Figures SM 5 and SM 6).\n15\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 8: RMSE for magnitude and epicentral location at different times for models trained on differently sized subsets\nof the training set in Chile. The line color encodes the fraction of the training and validation set used in training. All\nmodels were evaluated on the full Chilean test set. We note that the variance of the curves with fewer data is higher, due\nto the increased stochasticity from model training and initialization.\n4.3 The impact of data set size and composition\nOur analysis so far showed the importance of the amount of training data. To quantify the impact of data availability on\nmagnitude and location estimation, we trained models only using fractions of the training and validation data (Figure\n8). We use the Chile data set for this analysis, as it contains by far the most events. We subsample the events by only\nusing each kth event in chronological order, with k = 2,4,8,16,32,64. This strategy approximately maintains the\nmagnitude and location distribution of the full set. We point out, that TEAM-LM only uses information of the event\nunder consideration and does not take the events before or afterwards into account. Therefore, the ‘gaps’ between\nevents introduced by the subsampling do not negatively inﬂuence TEAM-LM.\nFor all times after the ﬁrst P arrival, we see a clear increase in the magnitude-RMSE for a reduction in the number of\ntraining samples. While the impact of reducing the data set by half is relatively small, using only a quarter of the data\nalready leads to a twofold increase in RMSE at late times. Even more relevant in an early warning context, a fourfold\nsmaller data sets results in an approximately fourfold increase in the time needed to reach the same precision as with\nthe full data. This relationship seems to hold approximately across all subsampled data sets: reducing the data set kfold\nincreases the time to reach a certain precision by a factor of k.\nWe make three further observations from comparing the predictions to the true values (Figure SM 7). First, for nearly all\nmodels the RMSE changes only marginally between 16 s and 25 s, but the RMSE of this plateau increases signiﬁcantly\nwith a decreasing number of training events. Second, the lower the amount of training data, the lower is the saturation\nthreshold above which all events are strongly underestimated. In addition, for 1/32 and 1/64 of the full data set, an\n‘inverse saturation’ effect is noticeable for the smallest magnitudes. Third, while for the full data set and the largest\nsubsets all large events are estimated at approximately the saturation threshold, if at most one quarter of the training data\nis used, the largest events even fall signiﬁcantly below the saturation threshold. For the models trained on the smallest\nsubsets (1/8 to 1/64), the higher the true magnitude the lower the predicted magnitude becomes. We assume that the\nlarger the event is, the further away from the training distribution it is and therefore it is estimated approximately at the\nmost dense region of the training label distribution. These observations support the hypothesis that underestimations of\nlarge magnitudes for the full data set are caused primarily by insufﬁcient training data.\nWhile the RMSE for epicenter estimation shows a similar behavior as the RMSE for magnitude, there are subtle\ndifferences. If the amount of training data is halved, the performance only degrades mildly and only at later times.\nHowever, the performance degradation is much more severe than for magnitude if only a quarter or less of the training\ndata are available. This demonstrates that location estimation with high accuracy requires catalogs with a high event\ndensity.\nThe strong degradation further suggests insights into the inner working of TEAM-LM. Classically, localization should\nbe a task where interpolation leads to good results, i.e., the travel times for an event in the middle of two others should\n16\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 9: Magnitude predictions and uncertainties in the Chile data set as a function of time since the ﬁrst P arrival.\nSolid lines indicate median predictions, while dashed lines (left panel only) show 20th and 80th quantiles of the\nprediction. The left panel shows the predictions, while the right panel shows the differences between the predicted\nand true magnitude. The right panel is focused on a shorter timeframe to show the early prediction development in\nmore detail. In both plots, each color represents a different magnitude bucket. For each magnitude bucket, we sampled\n1,000 events around this magnitude and combined their predictions. If less than 1,000 events were available within\n±0.5 m.u. of the bucket center, we use all events within this range. We only use events from the test set. To ensure that\nthe actual uncertainty distribution is visualized, rather than the distribution of magnitudes around the bucket center,\neach prediction is shifted by the magnitude difference between bucket center and catalog magnitude.\nbe approximately the average between the travel times for the other events. Following this argument, if the network\nwould be able to use interpolation, it should not suffer such signiﬁcant degradation when faced with fewer data. This\nprovides further evidence that the network does not actually learn some form of triangulation, but only an elaborate\nﬁngerprinting scheme, backing the ﬁnding from the qualitative analysis of location errors.\n4.4 Training TEAM-LM on large events only\nOften, large events are of the greatest concerns, and as discussed, generally showed poorer performance because they\nare not well represented in the training data. It therefore appears plausible that a model optimized for large events might\nperform better than a model trained on both large and small events. In order to test this hypothesis, we employed an\nextreme version of the upscaling strategy by training a set of models only on large events, which might avoid tuning\nthe model to seemingly irrelevant small events. In fact, these models perform signiﬁcantly worse than the models\ntrained on the full data set, even for the large events (Tables SM 6 to SM 12). Therefore even if the events of interest\nare only the large ones, training on more complete catalogs is still beneﬁcial, presumably by giving the network more\ncomprehensive information on the regional propagation characteristics and possibly site effects.\n4.5 Interpretation of predicted uncertainties\nSo far we only analyzed the mean predictions of TEAM-LM. As for many application scenarios, for example early\nwarning, quantiﬁed uncertainties are required, TEAM-LM outputs not only these mean predictions, but a probability\ndensity. Figure 9 shows the development of magnitude uncertainties for events from different magnitude classes in\nthe Chile data set. The left panel shows the absolute predictions, while the right panel shows the difference between\nprediction and true magnitude and focuses on the ﬁrst 2 s. As we average over multiple events, each set of lines can be\nseen as a prototype event of a certain magnitude.\nFor all magnitude classes the estimation shows a sharp jump at t = 0, followed by a slow convergence to the ﬁnal\nmagnitude estimate. We suspect that the magnitude estimation always converges from below, as due to the Gutenberg-\nRichter distribution, lower magnitudes are more likely a priori. The uncertainties are largest directly after t= 0and\nsubsequently decrease, with the highest uncertainties for the largest events. As we do not use transfer learning in this\napproach, there is a consistent underestimation of the largest magnitude events, visible from the incorrect median\npredictions for magnitudes 5 and 6.\nWhile the Gaussian mixture model is designed to output uncertainties, it cannot be assumed that the predicted\nuncertainties are indeed well calibrated, i.e., that they actually match the real error distribution. Having well calibrated\nuncertainties is crucial for downstream tasks that rely on the uncertainties. Neural networks trained with a log-likelihood\nloss generally tend to be overconﬁdent (Snoek et al., 2019; Guo et al., 2017), i.e., underestimate the uncertainties.\nThis overconﬁdence is probably caused by the strong overparametrization of neural network models. To assess the\nquality of our uncertainty estimations for magnitude, we use the observation that for a speciﬁc event i, the predicted\nGaussian mixture implies a cumulative distribution function Fi\npred : R →[0,1]. Given the observed magnitude yi\ntrue,\nwe can calculate ui = Fi\npred(yi\ntrue). If yi\ntrue is indeed distributed according to Fi\npred, then ui needs to be uniformly\ndistributed on [0,1]. We test this based on the ui of all events in the test set using P-P plots (Figure 10). Further details\n17\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 10: P-P plots of the CDFs of the empirical quantile of the magnitude predictions compared to the expected\nuniform distribution. The P-P plot shows(CDFui (z),CDFuniform(z)) for z∈[0,1]. The expected uniform distribution\nis shown as the diagonal line, the misﬁt is indicated as shaded area. The value in the upper corner provides d∞, the\nmaximum distance between the diagonal and the observed CDF. d∞ can be interpreted as the test statistic for a\nKolmogorov-Smirnov test. Curves consistently above the diagonal indicate a bias to underestimation, and below the\ndiagonal to overestimation. Sigmoidal curves indicate over-conﬁdence, mirrored sigmoids indicate under-conﬁdence.\nSee supplementary section SM 3 for a further discussion of the plotting methodology and its connection to the\nKolmogorov-Smirnov test.\n18\nEarthquake magnitude and location estimation with TEAM-LM\nFigure 11: The ﬁgure shows 90th percent conﬁdence areas for sample events around 5 example locations. For each\nlocation the 5 closest events are shown. Conﬁdence areas belonging to the same location are visualized using the\nsame color. Conﬁdence areas were chosen as curves of constant likelihood, such that the probability mass above the\nlikelihood equals 0.9. To visualize the result in 2D we marginalize out the depth. Triangles denote station locations\nfor orientation. The top row plots show results from a single model, while the bottom row plots show results from an\nensemble of 10 models.\n19\nEarthquake magnitude and location estimation with TEAM-LM\non the method can be found in the supplementary material (Section SM 3). Note that good calibration is a necessary\nbut not sufﬁcient condition for a good probabilistic forecast. An example of a perfectly calibrated but mostly useless\nprobabilistic prediction would be the marginal probability of the labels.\nFigure 10 shows the P-P plots of uin comparison to a uniform distribution. For all data sets and all times the model\nis signﬁcantly miscalibrated, as estimated using Kolmgorov-Smirnov test statistics (Section SM 3). Miscalibration\nis considerably stronger for Italy and Japan than for Chile. More precisely, the model is always overconﬁdent, i.e.,\nestimates narrower conﬁdence bands than the actually observed errors. Further, in particular at later times, the model is\nbiased towards underestimating the magnitudes. This is least visible for Chile. We speculate that this is a result of the\nlarge training data set for Chile, which ensures that for most events the density of training events in their magnitude\nrange is high.\nTo mitigate the miscalibration, we trained ensembles (Hansen & Salamon, 1990), a classical method to improve\ncalibration. Instead of training a single neural network, a set of nneural networks, in our case n = 10, are trained,\nwhich all have the same structure, but different initialization and batching in training. The networks therefore represent\na sample of size nfrom the posterior distribution of the model parameters given the training data. For Italy and Japan,\nthis improves calibration considerably (Figure 10). For Chile, the ensemble model, in contrast to the single model,\nexhibits underconﬁdence, i.e., estimates too broad uncertainty bands.\nThe maximum distance between the empirical cumulative distribution function of uand a uniformly distributed variable\nd∞is the test statistic of the Kolmogorov-Smirnov test. While d∞is reduced by nearly half for some of the ensemble\nresults, the Kolmogorov-Smirnov test indicates, that even the distributions from the ensemble models deviate highly\nsigniﬁcantly from a uniform distribution ( p ≪10−5). A table with d∞ for all experiments can be found in the\nsupplementary material (Table SM 9).\nTo evaluate the location uncertainties qualitatively, we plot conﬁdence ellipses for a set of events in Chile (Figure 11).\nAgain we compare the predictions from a single model to the predictions of an ensemble. At early times, the uncertainty\nregions mirror the seismicity around the station with the ﬁrst arrival, showing that the model correctly learned the\nprior distribution. Uncertainty ellipses at late times approximately match the expected uncertainty ellipses for classical\nmethods, i.e., they are small and fairly round for events inside the seismic network, where there is good azimuthal\ncoverage, and larger and elliptical for events outside the network. Location uncertainties are not symmetric around\nthe mean prediction, but show higher likelihood towards the network than further outwards. Location errors for the\nensemble model are more smooth than from the single model, but show the same features. The uncertainty ellipses are\nslightly larger, suggesting that the single model is again overconﬁdent.\nIn addition to improving calibration, ensembles also lead to slight improvements regarding the accuracy of the mean\npredictions (Tables SM 5 to SM 11). Improvements in terms of magnitude RMSE range up to ∼10%, for epicentral\nlocation error up to ∼20%. Due to the high computational demand of training ensembles, all other results reported in\nthis paper are calculated without ensembling. We note that in addition to ensembles a variety of methods have been\ndeveloped to improve calibration or obtain calibrated uncertainties. For a quantitative survey, see for example Snoek\net al. (2019). One of these methods, Monte-Carlo Dropout, has already been employed in the context of fast assessment\nby van den Ende & Ampuero (2020).\n5 Conclusion\nIn this study we adapted TEAM to build TEAM-LM, a real time earthquake source characterization model, and used it\nto study the pitfalls and particularities of deep learning for this task. We showed that TEAM-LM achieves state of the\nart in magnitude estimation, outperforming both a classical baseline and a deep learning baseline. Given sufﬁciently\nlarge catalogs, magnitude can be assessed with a standard deviation of ∼0.2 magnitude units within 2 s of the ﬁrst P\narrival and a standard deviation of 0.07 m.u. within the ﬁrst 25 s. For location estimation, TEAM-LM outperforms a\nstate of the art deep learning baseline and compares favorably with a classical baseline.\nOur analysis showed that the quality of model predictions depends crucially on the training data. While performance\nin regions with abundant data is excellent, in regions of data sparsity, prediction quality degrades signiﬁcantly. For\nmagnitude estimation this effect results in the underestimation of large magnitude events; for location estimation events\nin regions with few or no training events tend to be mislocated most severely. This results in a heavy tailed error\ndistribution for location estimation. Large deviations in both magnitude and location estimation can have signiﬁcant\nimpact in application scenarios, e.g., for early warning where large magnitudes are of the biggest interest.\nFollowing our analysis, we propose a set of best practices for building models for fast earthquake source characterisation:\n20\nEarthquake magnitude and location estimation with TEAM-LM\n1. Build a comprehensive evaluation platform. Put a special focus on outliers and rare or large events. Analyze\nwhich impact outliers or out of distribution events will have for the proposed application.\n2. Use very large training catalogs, spanning long time spans and having a low magnitude of completeness. If\npossible, employ transfer learning. We hope the catalogs used in this study can give a starting point for transfer\nlearning.\n3. Use training data augmentation, especially upsampling of large events, which improves prediction performance\nin face of label sparsity at virtually no cost.\n4. If probabilistic estimates are required, use deep ensembles to improve the model calibration.\n5. When using deep learning for location estimation, put special emphasis on monitoring possible distribution\nshifts between training data and application.\nWhile these points give guidance for training current models they also point to further directions for methodological\nadvances. First, our transfer learning scheme is fairly simple. More reﬁned and targeted schemes could increase the\namount of information sharable across data sets considerably. We further expect major improvements from training\nwith simulated data, but are aware that generating realistic, synthetic seismogramms, especially for large events, poses\nmajor challenges. Another promising alternative might be to move away from the paradigm of black box modeling,\ni.e., training algorithms that are built solely by ﬁtting recorded data. Instead, incorporation of physical knowledge and\na move towards physics informed deep learning methods seems promising (Raissi et al., 2019). However, physics\ninformed neural networks are still in their infancy and the application to seismic tasks still needs to be developed.\nData availability\nThe Italy data set has been published as Münchmeyer et al. (2020). The Chile data set has been published as\nMünchmeyer et al. (2021a). An implementation of TEAM-LM and TEAM has been published as Münchmeyer et al.\n(2021b). Download instructions for the Japan data set are available in the code publication.\nAcknowledgments\nWe thank the National Research Institute for Earth Science and Disaster Resilience for providing the catalog and\nwaveform data for our Japan data set. We thank the Istituto Nazionale di Geoﬁsica e Vulcanologia and the Dipartimento\ndella Protezione Civile for providing the catalog and waveform data for our Italy data set. We thank Christian Sippl for\nproviding the P picks for the Chile catalog. We thank Sebastian Nowozin for insightful discussions on neural network\ncalibration and probabilistic regression. Jannes Münchmeyer acknowledges the support of the Helmholtz Einstein\nInternational Berlin Research School in Data Science (HEIBRiDS). We thank Martijn van den Ende for his comments\nthat helped improve the manuscript. We use obspy (Beyreuther et al., 2010), tensorﬂow (Abadi et al., 2016) and color\nscales from Crameri (2018).\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.,\n2016. Tensorﬂow: A system for large-scale machine learning, in 12th {USENIX}symposium on operating systems\ndesign and implementation ({OSDI}16), pp. 265–283.\nAldersons, F., 2004. Toward three-dimensional crustal structure of the dead sea region from local earthquake tomography.\nAsch, G., Tilmann, F., Schurr, B., & Ryberg, T., 2011. Seismic network 5e: Minas project (2011/2013).\nBeyreuther, M., Barsch, R., Krischer, L., Megies, T., Behr, Y ., & Wassermann, J., 2010. Obspy: A python toolbox for\nseismology, Seismological Research Letters, 81(3), 530–533.\nBishop, C. M., 1994. Mixture density networks, Tech. rep., Aston University.\nCesca, S., Sobiesiak, M., Tassara, A., Olcay, M., Günther, E., Mikulla, S., & Dahm, T., 2009. The iquique local network\nand picarray.\nColombelli, S., Festa, G., & Zollo, A., 2020. Early rupture signals predict the ﬁnal earthquake size, Geophysical\nJournal International, 223(1), 692–706.\nCrameri, F., 2018. Geodynamic diagnostics, scientiﬁc visualisation and staglab 3.0, Geoscientiﬁc Model Development,\n11(6), 2541–2562.\nDeichmann, N., 2018. Why Does ML Scale 1:1 with 0.5logES?, Seismological Research Letters, 89(6), 2249–2255.\n21\nEarthquake magnitude and location estimation with TEAM-LM\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding, arXiv preprint arXiv:1810.04805.\nDipartimento di Fisica, Università degli studi di Napoli Federico II, 2005. Irpinia seismic network (isnet).\nDoi, K., 2014. Seismic network and routine data processing-japan meteorological agency, Summary of the Bulletin of\nthe International Seismological Centre, 47(7-12), 25–42.\nEMERSITO Working Group, 2018. Seismic network for site effect studies in amatrice area (central italy) (sesaa).\nFesta, G., Picozzi, M., Caruso, A., Colombelli, S., Cattaneo, M., Chiaraluce, L., Elia, L., Martino, C., Marzorati, S.,\nSupino, M., & Zollo, A., 2018. Performance of Earthquake Early Warning Systems during the 2016–2017 Mw 5–6.5\nCentral Italy Sequence, Seismological Research Letters, 89(1), 1–12.\nGEOFON Data Center, 1993. Geofon seismic network.\nGeological Survey-Provincia Autonoma di Trento, 1981. Trentino seismic network.\nGraeber, F. M. & Asch, G., 1999. Three-dimensional models of P wave velocity and P-to-S velocity ratio in the southern\ncentral Andes by simultaneous inversion of local earthquake data, Journal of Geophysical Research: Solid Earth,\n104(B9), 20237–20256.\nGuo, C., Pleiss, G., Sun, Y ., & Weinberger, K. Q., 2017. On calibration of modern neural networks,arXiv preprint\narXiv:1706.04599.\nHansen, L. K. & Salamon, P., 1990. Neural network ensembles, IEEE transactions on pattern analysis and machine\nintelligence, 12(10), 993–1001.\nISIDe Working Group, 2007. Italian seismological instrumental and parametric database (iside).\nIstituto Nazionale di Geoﬁsica e Vulcanologia (INGV), 2008. Ingv experiments network.\nIstituto Nazionale di Geoﬁsica e Vulcanologia (INGV), Istituto di Geologia Ambientale e Geoingegneria (CNR-IGAG),\nIstituto per la Dinamica dei Processi Ambientali (CNR-IDPA), Istituto di Metodologie per l’Analisi Ambientale\n(CNR-IMAA), & Agenzia Nazionale per le nuove tecnologie, l’energia e lo sviluppo economico sostenibile (ENEA),\n2018. Centro di microzonazione sismica network, 2016 central italy seismic sequence (centromz).\nIstituto Nazionale di Geoﬁsica e Vulcanologia (INGV), Italy, 2006. Rete sismica nazionale (rsn).\nJozinovi´c, D., Lomax, A., Štajduhar, I., & Michelini, A., 2020. Rapid prediction of earthquake ground shaking intensity\nusing raw waveform data and a convolutional neural network,Geophysical Journal International, 222(2), 1379–1389.\nKingma, D. P. & Ba, J., 2014. Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980.\nKriegerowski, M., Petersen, G. M., Vasyura-Bathke, H., & Ohrnberger, M., 2019. A Deep Convolutional Neural\nNetwork for Localization of Clustered Earthquakes Based on Multistation Full Waveforms, Seismological Research\nLetters, 90(2A), 510–516.\nKuyuk, H. S. & Allen, R. M., 2013. A global approach to provide magnitude estimates for earthquake early warning\nalerts, Geophysical Research Letters, 40(24), 6329–6333.\nLee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., & Teh, Y . W., 2019. Set transformer: A framework for attention-based\npermutation-invariant neural networks, in International Conference on Machine Learning, pp. 3744–3753, PMLR.\nLeyton, F., Ruiz, S., Baez, J. C., Meneses, G., & Madariaga, R., 2018. How Fast Can We Reliably Estimate the\nMagnitude of Subduction Earthquakes?, Geophysical Research Letters, 45(18), 9633–9641.\nLomax, A., Virieux, J., V olant, P., & Berge-Thierry, C., 2000. Probabilistic earthquake location in 3d and layered\nmodels, in Advances in seismic event location, pp. 101–134, Springer.\nLomax, A., Michelini, A., & Jozinovi ´c, D., 2019. An Investigation of Rapid Earthquake Characterization Using\nSingle-Station Waveforms and a Convolutional Neural Network,Seismological Research Letters, 90(2A), 517–529.\nMatrullo, E., De Matteis, R., Satriano, C., Amoroso, O., & Zollo, A., 2013. An improved 1-d seismic velocity model\nfor seismological studies in the campania–lucania region (southern italy), Geophysical Journal International, 195(1),\n460–473.\nMedNet Project Partner Institutions, 1990. Mediterranean very broadband seismographic network (mednet).\nMeier, M.-A., Ampuero, J. P., & Heaton, T. H., 2017. The hidden simplicity of subduction megathrust earthquakes,\nScience, 357(6357), 1277–1281.\nMousavi, S. M. & Beroza, G. C., 2019. Bayesian-Deep-Learning Estimation of Earthquake Location from Single-Station\nObservations, arXiv:1912.01144 [physics].\n22\nEarthquake magnitude and location estimation with TEAM-LM\nMousavi, S. M. & Beroza, G. C., 2020. A Machine-Learning Approach for Earthquake Magnitude Estimation,\nGeophysical Research Letters, 47(1), e2019GL085976.\nMousavi, S. M., Zhu, W., Sheng, Y ., & Beroza, G. C., 2018. CRED: A Deep Residual Network of Convolutional and\nRecurrent Units for Earthquake Signal Detection, arXiv:1810.01965.\nMünchmeyer, J., Bindi, D., Leser, U., & Tilmann, F., 2020. Fast earthquake assessment and earthquake early warning\ndataset for italy.\nMünchmeyer, J., Bindi, D., Leser, U., & Tilmann, F., 2020a. The transformer earthquake alerting model: A new\nversatile approach to earthquake early warning, Geophysical Journal International, (ggaa609).\nMünchmeyer, J., Bindi, D., Sippl, C., Leser, U., & Tilmann, F., 2020b. Low uncertainty multifeature magnitude\nestimation with 3-D corrections and boosting tree regression: Application to North Chile, Geophysical Journal\nInternational, 220(1), 142–159.\nMünchmeyer, J., Bindi, D., Leser, U., & Tilmann, F., 2021a. Fast earthquake assessment dataset for chile.\nMünchmeyer, J., Bindi, D., Leser, U., & Tilmann, F., 2021b. Team – the transformer earthquake alerting model.\nNational Research Institute For Earth Science And Disaster Resilience, 2019. Nied k-net, kik-net.\nOGS (Istituto Nazionale di Oceanograﬁa e di Geoﬁsica Sperimentale), 2016. North-east italy seismic network (nei).\nOGS (Istituto Nazionale di Oceanograﬁa e di Geoﬁsica Sperimentale) and University of Trieste, 2002. North-east italy\nbroadband network (ni).\nPan, S. J. & Yang, Q., 2009. A survey on transfer learning, IEEE Transactions on knowledge and data engineering,\n22(10), 1345–1359.\nPerol, T., Gharbi, M., & Denolle, M., 2018. Convolutional neural network for earthquake detection and location,\nScience Advances, 4(2), e1700578.\nPresidency of Counsil of Ministers - Civil Protection Department, 1972. Italian strong motion network (ran).\nRaissi, M., Perdikaris, P., & Karniadakis, G. E., 2019. Physics-informed neural networks: A deep learning framework\nfor solving forward and inverse problems involving nonlinear partial differential equations,Journal of Computational\nPhysics, 378, 686–707.\nRESIF - Réseau Sismologique et géodésique Français, 1995a. Resif-rlbp french broad-band network, resif-rap strong\nmotion network and other seismic stations in metropolitan france.\nRESIF - Réseau Sismologique et géodésique Français, 1995b. Réseau accélérométrique permanent (french ac-\ncelerometrique network) (rap).\nRuder, S., 2017. An overview of multi-task learning in deep neural networks, arXiv preprint arXiv:1706.05098.\nShelly, D. R., Beroza, G. C., & Ide, S., 2007. Non-volcanic tremor and low-frequency earthquake swarms, Nature,\n446(7133), 305–307.\nSippl, C., Schurr, B., Asch, G., & Kummerow, J., 2018. Seismicity Structure of the Northern Chile Forearc From\n>100,000 Double-Difference Relocated Hypocenters, Journal of Geophysical Research: Solid Earth , 123(5),\n4063–4087.\nSnoek, J., Ovadia, Y ., Fertig, E., Lakshminarayanan, B., Nowozin, S., Sculley, D., Dillon, J., Ren, J., & Nado, Z., 2019.\nCan you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift, inAdvances in Neural\nInformation Processing Systems, pp. 13969–13980.\nTrugman, D. T., Page, M. T., Minson, S. E., & Cochran, E. S., 2019. Peak Ground Displacement Saturates Exactly\nWhen Expected: Implications for Earthquake Early Warning, Journal of Geophysical Research: Solid Earth, 124(5),\n4642–4653.\nUeno, H., Hatakeyama, S., Aketagawa, J., Funasaki, J., & Hamada, N., 2002. Improvement of hypocenter determination\nprocedures in the japan meteorological agency, Quart. J. Seism., 65, 123–134.\nUniversidad de Chile, 2013. Red sismologica nacional.\nUniversita della Basilicata, 2005. Unibas.\nUniversity of Genova, 1967. Regional seismic network of north western italy. international federation of digital\nseismograph networks.\nvan den Ende, M. P. A. & Ampuero, J.-P., 2020. Automated Seismic Source Characterization Using Deep Graph Neural\nNetworks, Geophysical Research Letters, 47(17), e2020GL088690.\n23\nEarthquake magnitude and location estimation with TEAM-LM\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I., 2017.\nAttention is all you need, in Advances in neural information processing systems, pp. 5998–6008.\nWigger, P., Salazar, P., Kummerow, J., Bloch, W., Asch, G., & Shapiro, S., 2016. West–ﬁssure- and atacama-fault\nseismic network (2005/2012).\n24\nEarthquake magnitude and location estimation with TEAM-LM\nTable SM 1: Seismic networks\nRegion Network Reference\nChile GE GEOFON Data Center (1993)\nC, C1 Universidad de Chile (2013)\n8F Wigger et al. (2016)\nIQ Cesca et al. (2009)\n5E Asch et al. (2011)\nItaly 3A Istituto Nazionale di Geoﬁsica e Vulcanologia (INGV) et al. (2018)\nBA Universita della Basilicata (2005)\nFR RESIF - Réseau Sismologique et géodésique Français (1995a)\nGU University of Genova (1967)\nIT Presidency of Counsil of Ministers - Civil Protection Department (1972)\nIV Istituto Nazionale di Geoﬁsica e Vulcanologia (INGV), Italy (2006)\nIX Dipartimento di Fisica, Università degli studi di Napoli Federico II (2005)\nMN MedNet Project Partner Institutions (1990)\nNI OGS (Istituto Nazionale di Oceanograﬁa e di Geoﬁsica Sperimentale) and University of Trieste\n(2002)\nOX OGS (Istituto Nazionale di Oceanograﬁa e di Geoﬁsica Sperimentale) (2016)\nRA RESIF - Réseau Sismologique et géodésique Français (1995b)\nST Geological Survey-Provincia Autonoma di Trento (1981)\nTV Istituto Nazionale di Geoﬁsica e Vulcanologia (INGV) (2008)\nXO EMERSITO Working Group (2018)\nJapan KiK-Net National Research Institute For Earth Science And Disaster Resilience (2019)\nSM 1 Data sources\nSM 2 Classical magnitude estimation baseline\nFor magnitude estimation we compare TEAM-LM to a classical baseline. To this end we use the peak displacement\nbased approach proposed by Kuyuk & Allen (2013). At each station, we bandpass ﬁlter the signal between 0.5 Hz and\n3 Hz and discard traces with insufﬁcient signal to noise ratio. We extract peak displacement PD from the horizontal\ncomponents in the ﬁrst 6 s of the P wave, while only including samples before the S onset. We use the relationship\nM = c1 log(PD) +c2 log(R) +c3 + N (0,σ2) (1)\nfrom Kuyuk & Allen (2013) to estimate magnitudes from peak displacement. We use c1 = 1.23, c2 = 1.38 and\nσ= 0.31 from Kuyuk & Allen (2013). These parameters were calibrated using data from California and Japan, but the\nauthors state that the relationship can be applied to earthquake source zones around the world. To account for a constant\noffset between different magnitude scales, we optimized c3 separately for each dataset such that the predictions do not\nhave a systematic bias compared to the ground truth.\nWe average the predictions from multiple stations, effectively assuming independence between the predictions. To\nobtain earlier predictions, we already calculate magnitude estimates at a station once at least 1 s of P wave data has\nbeen recorded. We assign higher weights to stations with longer P wave records, with weights linearly increasing from\n0.11 for 1 s of waveforms, to 1.0 for 6 s of data. Thereby, while getting early estimates from the ﬁrst stations, new data\nfrom later stations does not perturb the prediction strongly until enough data has been recorded.\nAs the estimation relies on the hypocentral distance Rbetween station and event, the method requires an estimate of the\nhypocentral location. We provide the method with the cataloged hypocentral location. While this is an unrealistically\noptimistic assumption for an actual real time determination, it allows us to put our focus on the magnitude estimation\ncapabilities. We note that this gives the baseline an advantage compared to TEAM-LM, which has no information on\nthe earthquake location.\nFor some events in the Chile catalog, the SNR criterion is not fulﬁlled at any station due to the inclusion of smaller\nmagnitude events and the higher distances between stations and events. For these events the baseline does not issue a\nmagnitude estimation. We exclude these events from the evaluation of the baseline, leading to an optimistic assessment\nof the performance of the baseline.\n25\nEarthquake magnitude and location estimation with TEAM-LM\nTable SM 2: Architecture of the feature extraction network. The input shape of the waveform data is (time, channels).\nFC denotes fully connected layers. As FC layers can be regarded as 0D convolutions, we write the output dimensionality\nin the ﬁlters column. The “Concatenate scale” layer concatenates the log of the peak amplitude to the output of the\nconvolutions. We want to mention that depending on the existence of borehole data the number of input ﬁlters for the\nﬁrst Conv1D varies.\nLayer Filters Kernel size Stride\nConv2D 8 5, 1 5, 1\nConv2D 32 16, 3 1, 3\nFlatten to 1D\nConv1D 64 16 1\nMaxPool1D 2 2\nConv1D 128 16 1\nMaxPool1D 2 2\nConv1D 32 8 1\nMaxPool1D 2 2\nConv1D 32 8 1\nConv1D 16 4 1\nFlatten to 0D\nConcatenate scale\nFC 500\nFC 500\nFC 500\nTable SM 3: Architecture of the transformer network.\nFeature Value\n# Layers 6\nDimension 500\nFeed forward dimension 1000\n# Heads 10\nMaximum number of stations 25\nDropout 0\nActivation GeLu\nSM 3 Calibration estimation\nCalibration of a model describes whether the predicted uncertainties match the observed values, i.e., if the observation\nytrue was drawn from a distribution with cumulative distribution function (CDF) Fpred. Unfortunately, for each event i,\nonly one prediction observation of the magnitude yi\ntrue and one prediction Fi\npred is available. To this end, we deﬁne the\nrandom variable ui = Fi\npred(yi\ntrue). If yi\ntrue is distributed according to Fi\npred than ui must be uniformly distributed\non [0,1]. This follows from the deﬁnition of the CDF. If F is a CDF and U a uniform random variable on [0,1], than\nF−1(U) is distributed according to F.\nWe take the ui of all events as samples of a random variable U and compare U to a uniform random variable on\n[0,1]. The maximum difference between the empirical CDF of U and a uniform variable is the test statistic of a\nKolmogorov-Smirnov test, d∞. As the number of events nis large, critical values dα to a conﬁdence threshold αcan\nbe estimated as:\ndα =\n√\n−1\n2 log α\n2\n√n (2)\nFor α= 10−5, this gives values dα of 0.015 (Chile), 0.054 (Japan) and 0.039 (Italy). This is considerably below the\nobserved values d∞, even using ensembles, indicating that U differs highly signiﬁcantly from a uniform distribution.\nSM 4 Figures and tables\n26\nEarthquake magnitude and location estimation with TEAM-LM\nTable SM 4: Architecture of the mixture density network.\nFeature Value\nDimensions fully connected layers (magnitude) 150, 100, 50, 30, 10\nDimensions fully connected layers (location) 150, 100, 50, 50, 50\nMixture size (magnitude) 5\nMixture size (location) 15\nBase distribution Gaussian\nFigure SM 1: Event and station distribution for Chile. In the map, events are indicated by dots, stations by triangles.\nThe event depth is encoded using color.\n27\nEarthquake magnitude and location estimation with TEAM-LM\nFigure SM 2: Event and station distribution for Italy. In the map, events are indicated by dots, stations by triangles. The\nevent depth is encoded using color.\nTable SM 5: Experiment names for the results tables\nName Explanation\nBaseline Baseline method\nPlain Model trained with a single loss for magnitude or location\nMulti-task Model trained with both a loss for magnitude and for location\nHigh magnitudes Model only trained on events with magnitude above 4.5 (Chile), 3.5 (Italy), 5.5 (Japan)\nTransfer Model trained with transfer learning and a single loss\nMulti-task transfer Model trained with transfer learning and both losses\nJoint Model trained on all datasets jointly with a single loss\nJoint multi-task Model trained on all datasets jointly with both losses\nVelocity Model trained with acceleration traces integrated to velocity\nEnsemble Model trained as an ensemble of size 10\nNo upsampling Model trained without upsampling large magnitudes\nPooling (Emb) Model trained with the pooling architecture using position embeddings\nPooling (Concat) Model trained with the pooling architecture and concatenated coordinates\n28\nEarthquake magnitude and location estimation with TEAM-LM\nFigure SM 3: Event and station distribution for Japan. In the map, events are indicated by dots, stations by triangles.\nThe event depth is encoded using color. There are ∼20 additional events far offshore in the catalog, which are outside\nthe displayed map region.\n29\nEarthquake magnitude and location estimation with TEAM-LM\nFigure SM 4: Distribution of the hypocentral errors for TEAM-LM, the pooling baseline with position embeddings\n(POOL-E), the pooling baseline with concatenated position (POOL-C), TEAM-LM with transfer learning (TEAM-TRA)\nand a classical baseline. Vertical lines mark the 50th, 90th, 95th and 99th error percentiles. The time indicates the time\nsince the ﬁrst P arrival at any station. We use the mean predictions.\n30\nEarthquake magnitude and location estimation with TEAM-LM\nFigure SM 5: The 100 events with the highest location error in the Italy dataset overlayed on top of the spatial event\ndensity in the training dataset. The estimations use 16 s of data. Each event is denoted by a dot for the estimated\nlocation, a cross for the true location and a line connecting both. Stations are not shown as station coverage is dense.\nThe event density is calculated using a Gaussian kernel density estimation and does not take into account the event\ndepth. The inset shows the event density at the true event location in comparison to the event density at the predicted\nevent location.\n31\nEarthquake magnitude and location estimation with TEAM-LM\nFigure SM 6: The 200 events with the highest location error in the Japan dataset overlayed on top of the spatial event\ndensity in the training dataset. The estimations use 16 s of data. Each event is denoted by a dot for the estimated\nlocation, a cross for the true location and a line connecting both. Stations are not shown as station coverage is dense.\nThe event density is calculated using a Gaussian kernel density estimation and does not take into account the event\ndepth. The inset shows the event density at the true event location in comparison to the event density at the predicted\nevent location.\n32\nEarthquake magnitude and location estimation with TEAM-LM\nFigure SM 7: True and predicted magnitudes after 8 seconds using only parts of the datasets for training. All plots show\nthe Chile dataset. The fraction in the corner indicates the amount of training and validation data used for model training.\nAll models were evaluated on the full test dataset.\n33\nTable SM 6: Test set RMSE magnitude estimate across all magnitudes. For some experiments we additionally provide standard deviation. The standard deviations were\nobtained from six runs with different random model initialization. In this case the provided mean value is the mean over six runs. Note that the provided standard deviation\ndenotes the empirical standard deviation of a single run, therefore the uncertainty of the mean expected to be smaller by a factor of\n√\n6. Due to computational constraints\nwe are only able to provide standard deviations for a selected set of experiments.\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - 0.55 0.52 0.47 0.42 0.42 0.43 - 0.77 0.68 0.46 0.35 0.35 0.35 - 0.65 0.51 0.44 0.38 0.31 0.30\nMulti-task 0.31 0.25 0.20 0.15 0.11 0.08 0.07 0.36 0.34 0.30 0.26 0.24 0.23 0.23 0.65 0.47 0.38 0.33 0.27 0.23 0.23\nHigh magnitudes 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.46 0.46 0.45 0.45 0.44 0.43 0.43 0.51 0.50 0.47 0.43 0.38 0.34 0.36\nPlain 0.31 0.25 0.21 0.16 0.12 0.09 0.08 0.34 0.33 0.29 0.24 0.21 0.20 0.20 0.65 0.47 0.37 0.33 0.26 0.22 0.22\n±0.02 ±0.02 ±0.02 ±0.01 ±0.00 ±0.00 ±0.00 ±0.01 ±0.01 ±0.01 ±0.02 ±0.02 ±0.02 ±0.02 ±0.02 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01\nEnsemble 0.29 0.23 0.19 0.15 0.11 0.08 0.08 0.32 0.31 0.26 0.21 0.18 0.17 0.18 0.64 0.43 0.35 0.30 0.24 0.21 0.21\nNo Upsampling 0.31 0.25 0.21 0.16 0.12 0.09 0.08 0.33 0.31 0.28 0.23 0.19 0.18 0.18 0.54 0.42 0.35 0.31 0.25 0.21 0.22\nPooling (Emb) 0.31 0.25 0.21 0.16 0.13 0.09 0.09 0.36 0.33 0.29 0.25 0.21 0.20 0.21 0.66 0.49 0.41 0.36 0.31 0.28 0.28\n±0.01 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.02 ±0.02 ±0.02 ±0.03 ±0.03 ±0.03 ±0.03 ±0.03 ±0.05 ±0.05 ±0.05 ±0.06 ±0.07 ±0.07\nPooling (Concat) 0.35 0.28 0.24 0.20 0.16 0.11 0.11 0.37 0.33 0.29 0.24 0.21 0.21 0.22 0.71 0.50 0.41 0.35 0.29 0.26 0.26\n±0.01 ±0.01 ±0.02 ±0.01 ±0.01 ±0.01 ±0.01 ±0.03 ±0.02 ±0.01 ±0.01 ±0.01 ±0.02 ±0.02 ±0.01 ±0.02 ±0.03 ±0.02 ±0.03 ±0.04 ±0.04\nTransfer 0.32 0.25 0.20 0.15 0.12 0.09 0.08 0.36 0.33 0.28 0.22 0.19 0.19 0.22 0.71 0.52 0.40 0.35 0.30 0.25 0.25\nMulti-task transfer 0.31 0.24 0.19 0.14 0.11 0.08 0.07 0.36 0.33 0.27 0.20 0.17 0.17 0.17 0.63 0.47 0.39 0.34 0.27 0.23 0.23\nVelocity - - - - - - - 0.37 0.35 0.28 0.22 0.18 0.18 0.19 0.64 0.47 0.38 0.35 0.29 0.24 0.25\nJoint multi-task 0.32 0.25 0.20 0.15 0.11 0.08 0.08 0.35 0.32 0.27 0.21 0.17 0.16 0.17 0.65 0.50 0.40 0.35 0.28 0.24 0.25\nJoint 0.34 0.27 0.22 0.18 0.14 0.11 0.10 0.34 0.32 0.27 0.21 0.17 0.17 0.18 0.62 0.49 0.40 0.35 0.30 0.25 0.25\nTable SM 7: Test set mean absolute error (MAE) magnitude estimate across all magnitudes\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - 0.43 0.41 0.37 0.33 0.33 0.34 - 0.63 0.53 0.34 0.25 0.24 0.24 - 0.50 0.40 0.34 0.28 0.23 0.22\nMulti-task 0.23 0.18 0.14 0.10 0.07 0.04 0.04 0.26 0.24 0.21 0.18 0.16 0.15 0.15 0.48 0.33 0.26 0.21 0.17 0.15 0.15\nHigh magnitudes 0.53 0.52 0.52 0.52 0.52 0.52 0.52 0.30 0.30 0.29 0.27 0.26 0.26 0.26 0.40 0.39 0.37 0.32 0.30 0.27 0.28\nPlain 0.23 0.18 0.14 0.11 0.08 0.05 0.04 0.26 0.24 0.21 0.17 0.15 0.15 0.15 0.50 0.34 0.26 0.22 0.17 0.15 0.14\n±0.01 ±0.01 ±0.01 ±0.01 ±0.00 ±0.00 ±0.00 ±0.01 ±0.01 ±0.02 ±0.02 ±0.02 ±0.02 ±0.02 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01\nEnsemble 0.22 0.17 0.13 0.10 0.07 0.05 0.04 0.24 0.23 0.19 0.15 0.13 0.12 0.13 0.48 0.31 0.24 0.19 0.15 0.13 0.13\nNo Upsampling 0.23 0.18 0.14 0.11 0.08 0.05 0.04 0.24 0.23 0.20 0.16 0.14 0.12 0.12 0.40 0.30 0.24 0.20 0.16 0.14 0.14\nPooling (Emb) 0.23 0.18 0.15 0.11 0.08 0.06 0.05 0.27 0.25 0.22 0.18 0.15 0.15 0.15 0.51 0.37 0.31 0.26 0.23 0.21 0.20\n±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.02 ±0.02 ±0.02 ±0.03 ±0.02 ±0.02 ±0.04 ±0.05 ±0.06 ±0.06 ±0.07 ±0.08 ±0.08\nPooling (Concat) 0.25 0.20 0.16 0.13 0.10 0.07 0.06 0.28 0.25 0.21 0.17 0.16 0.15 0.16 0.55 0.38 0.29 0.25 0.20 0.18 0.18\n±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.01 ±0.01 ±0.01 ±0.02 ±0.02 ±0.02 ±0.01 ±0.02 ±0.03 ±0.03 ±0.04 ±0.05 ±0.05\nTransfer 0.24 0.18 0.14 0.11 0.08 0.05 0.05 0.26 0.24 0.20 0.16 0.14 0.14 0.15 0.53 0.37 0.27 0.22 0.18 0.15 0.15\nMulti-task transfer 0.23 0.18 0.13 0.10 0.07 0.04 0.04 0.24 0.22 0.18 0.13 0.11 0.11 0.11 0.46 0.33 0.26 0.21 0.17 0.14 0.14\nVelocity - - - - - - - 0.25 0.24 0.19 0.14 0.12 0.13 0.13 0.47 0.33 0.26 0.21 0.17 0.14 0.14\nJoint multi-task 0.23 0.18 0.14 0.10 0.07 0.05 0.04 0.24 0.21 0.18 0.13 0.11 0.11 0.11 0.48 0.35 0.27 0.22 0.17 0.15 0.15\nJoint 0.26 0.20 0.16 0.13 0.10 0.08 0.07 0.25 0.23 0.19 0.14 0.12 0.12 0.12 0.46 0.35 0.28 0.22 0.18 0.16 0.16\nTable SM 8: Test set R2 score across all magnitudes\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - 0.53 0.56 0.60 0.62 0.59 0.57 - -2.26 -1.83 -0.34 0.21 0.14 0.10 - 0.26 0.54 0.66 0.75 0.82 0.84\nMulti-task 0.74 0.84 0.90 0.94 0.97 0.98 0.99 0.17 0.25 0.41 0.55 0.64 0.65 0.66 0.31 0.64 0.76 0.82 0.88 0.91 0.91\nHigh magnitudes 0.03 0.03 0.02 0.03 0.03 0.02 0.01 -0.29 -0.29 -0.27 -0.26 -0.17 -0.14 -0.15 0.02 0.07 0.17 0.32 0.45 0.57 0.50\nPlain 0.74 0.83 0.89 0.93 0.96 0.98 0.98 0.24 0.29 0.45 0.62 0.72 0.74 0.73 0.29 0.64 0.77 0.82 0.89 0.92 0.92\n±0.03 ±0.02 ±0.02 ±0.01 ±0.00 ±0.00 ±0.00 ±0.05 ±0.04 ±0.04 ±0.05 ±0.05 ±0.04 ±0.04 ±0.04 ±0.01 ±0.01 ±0.01 ±0.01 ±0.00 ±0.00\nEnsemble 0.77 0.86 0.91 0.94 0.97 0.98 0.98 0.33 0.39 0.55 0.70 0.79 0.81 0.80 0.33 0.70 0.80 0.85 0.90 0.93 0.93\nNo Upsampling 0.74 0.83 0.88 0.93 0.96 0.98 0.98 0.29 0.37 0.50 0.65 0.76 0.79 0.79 0.52 0.71 0.79 0.84 0.89 0.92 0.92\nPooling (Emb) 0.74 0.83 0.88 0.93 0.96 0.98 0.98 0.17 0.28 0.44 0.61 0.71 0.73 0.71 0.27 0.60 0.72 0.78 0.84 0.86 0.86\n±0.01 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.11 ±0.09 ±0.09 ±0.09 ±0.10 ±0.08 ±0.08 ±0.07 ±0.08 ±0.07 ±0.06 ±0.06 ±0.07 ±0.07\nPooling (Concat) 0.68 0.78 0.85 0.90 0.94 0.96 0.97 0.13 0.28 0.46 0.62 0.70 0.72 0.69 0.17 0.58 0.73 0.79 0.86 0.88 0.89\n±0.02 ±0.02 ±0.02 ±0.01 ±0.01 ±0.01 ±0.01 ±0.14 ±0.07 ±0.03 ±0.03 ±0.04 ±0.06 ±0.06 ±0.03 ±0.03 ±0.04 ±0.03 ±0.03 ±0.04 ±0.04\nTransfer 0.73 0.83 0.89 0.94 0.96 0.98 0.98 0.18 0.28 0.49 0.67 0.76 0.76 0.69 0.18 0.56 0.73 0.80 0.86 0.90 0.90\nMulti-task transfer 0.75 0.84 0.91 0.95 0.97 0.98 0.99 0.16 0.30 0.53 0.73 0.81 0.82 0.81 0.35 0.64 0.75 0.81 0.88 0.91 0.91\nVelocity - - - - - - - 0.11 0.21 0.48 0.69 0.79 0.78 0.77 0.32 0.63 0.76 0.80 0.86 0.90 0.90\nJoint multi-task 0.73 0.83 0.89 0.94 0.97 0.98 0.98 0.22 0.35 0.54 0.72 0.81 0.83 0.82 0.31 0.59 0.73 0.80 0.87 0.90 0.90\nJoint 0.69 0.80 0.87 0.92 0.95 0.97 0.97 0.25 0.34 0.51 0.72 0.82 0.81 0.78 0.37 0.60 0.74 0.80 0.85 0.90 0.89\nTable SM 9: Test set test statistic dα for the Kolmogorov-Smirnov test across all magnitudes.\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - - - - - - - - - - - - - - - - - - - - -\nMulti-task 0.05 0.03 0.03 0.06 0.04 0.03 0.04 0.09 0.10 0.14 0.16 0.17 0.22 0.22 0.22 0.17 0.17 0.18 0.19 0.19 0.20\nHigh magnitudes 0.15 0.16 0.15 0.15 0.14 0.13 0.14 0.13 0.16 0.19 0.22 0.22 0.25 0.24 0.16 0.14 0.12 0.09 0.12 0.11 0.11\nPlain 0.04 0.04 0.04 0.05 0.04 0.04 0.03 0.11 0.13 0.16 0.15 0.16 0.15 0.16 0.19 0.13 0.12 0.13 0.14 0.12 0.11\n±0.02 ±0.02 ±0.02 ±0.02 ±0.01 ±0.01 ±0.01 ±0.03 ±0.04 ±0.05 ±0.07 ±0.06 ±0.08 ±0.07 ±0.04 ±0.04 ±0.05 ±0.06 ±0.07 ±0.06 ±0.05\nEnsemble 0.04 0.04 0.03 0.03 0.03 0.04 0.04 0.06 0.07 0.06 0.07 0.09 0.09 0.10 0.19 0.12 0.09 0.08 0.09 0.08 0.08\nNo Upsampling 0.04 0.03 0.04 0.05 0.05 0.05 0.04 0.07 0.09 0.11 0.15 0.17 0.09 0.09 0.11 0.10 0.09 0.11 0.14 0.15 0.14\nPooling (Emb) 0.03 0.03 0.03 0.03 0.04 0.03 0.05 0.14 0.14 0.16 0.17 0.15 0.16 0.18 0.19 0.13 0.16 0.16 0.18 0.17 0.17\n±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.02 ±0.04 ±0.03 ±0.05 ±0.08 ±0.05 ±0.07 ±0.08 ±0.03 ±0.04 ±0.02 ±0.02 ±0.03 ±0.04 ±0.04\nPooling (Concat) 0.04 0.04 0.05 0.05 0.05 0.04 0.05 0.14 0.12 0.12 0.14 0.17 0.17 0.19 0.20 0.12 0.11 0.12 0.13 0.12 0.13\n±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.02 ±0.02 ±0.02 ±0.04 ±0.04 ±0.07 ±0.06 ±0.05 ±0.02 ±0.03 ±0.03 ±0.02 ±0.05 ±0.07 ±0.06\nTransfer 0.05 0.02 0.03 0.02 0.02 0.02 0.02 0.08 0.06 0.10 0.15 0.25 0.24 0.30 0.17 0.15 0.15 0.15 0.14 0.11 0.10\nMulti-task transfer 0.02 0.03 0.02 0.04 0.04 0.06 0.03 0.14 0.14 0.13 0.12 0.18 0.20 0.17 0.13 0.12 0.15 0.17 0.19 0.18 0.18\nVelocity - - - - - - - 0.08 0.08 0.11 0.14 0.20 0.25 0.23 0.15 0.10 0.11 0.12 0.13 0.13 0.14\nJoint multi-task 0.05 0.05 0.04 0.04 0.07 0.02 0.04 0.13 0.12 0.12 0.13 0.16 0.18 0.18 0.12 0.10 0.13 0.12 0.14 0.18 0.18\nJoint 0.04 0.03 0.03 0.04 0.10 0.13 0.15 0.08 0.09 0.14 0.17 0.19 0.22 0.24 0.11 0.11 0.11 0.08 0.06 0.06 0.06\nTable SM 10: Test set RMSE of magnitude estimate for large events\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - 0.95 0.71 0.58 0.49 0.45 0.52 - 1.32 1.18 0.86 0.53 0.51 0.50 - 1.09 0.73 0.57 0.54 0.58 0.50\nMulti-task 1.38 1.06 0.83 0.67 0.60 0.46 0.41 1.12 1.04 0.90 0.81 0.68 0.67 0.66 1.31 1.09 0.90 0.91 0.69 0.40 0.38\nHigh magnitudes 0.89 0.89 0.90 0.90 0.90 0.90 0.91 0.81 0.83 0.85 0.87 0.84 0.84 0.85 0.59 0.58 0.54 0.52 0.42 0.35 0.41\nPlain 1.43 1.11 0.88 0.70 0.60 0.46 0.40 1.02 0.96 0.82 0.62 0.47 0.42 0.40 1.21 1.01 0.85 0.86 0.66 0.38 0.39\n±0.02 ±0.03 ±0.06 ±0.04 ±0.05 ±0.03 ±0.01 ±0.04 ±0.02 ±0.04 ±0.05 ±0.04 ±0.04 ±0.05 ±0.03 ±0.05 ±0.04 ±0.04 ±0.05 ±0.04 ±0.04\nEnsemble 1.41 1.09 0.84 0.68 0.59 0.45 0.39 0.98 0.93 0.80 0.59 0.44 0.38 0.36 1.17 0.98 0.85 0.84 0.66 0.35 0.35\nNo Upsampling 1.56 1.30 1.05 0.82 0.74 0.56 0.51 1.03 0.91 0.80 0.64 0.48 0.43 0.43 1.46 1.22 1.02 0.98 0.78 0.52 0.50\nPooling (Emb) 1.40 1.11 0.88 0.68 0.55 0.42 0.37 1.02 0.94 0.82 0.62 0.49 0.45 0.44 1.23 1.00 0.87 0.87 0.71 0.44 0.41\n±0.02 ±0.07 ±0.03 ±0.03 ±0.03 ±0.03 ±0.02 ±0.12 ±0.12 ±0.11 ±0.13 ±0.13 ±0.15 ±0.15 ±0.04 ±0.04 ±0.05 ±0.08 ±0.05 ±0.06 ±0.06\nPooling (Concat) 1.37 1.05 0.88 0.68 0.54 0.42 0.39 0.96 0.88 0.75 0.56 0.46 0.40 0.40 1.19 0.98 0.84 0.83 0.66 0.39 0.39\n±0.02 ±0.06 ±0.02 ±0.06 ±0.06 ±0.04 ±0.04 ±0.04 ±0.04 ±0.03 ±0.04 ±0.04 ±0.03 ±0.04 ±0.03 ±0.03 ±0.03 ±0.03 ±0.03 ±0.01 ±0.01\nTransfer 1.33 1.01 0.83 0.67 0.55 0.37 0.34 1.12 1.05 0.85 0.62 0.38 0.40 0.37 1.32 1.15 0.96 0.93 0.80 0.44 0.41\nMulti-task transfer 1.32 1.02 0.74 0.61 0.50 0.42 0.37 1.13 1.02 0.82 0.61 0.48 0.46 0.48 1.45 1.10 0.93 0.87 0.63 0.36 0.38\nVelocity - - - - - - - 1.13 1.02 0.82 0.67 0.50 0.50 0.50 1.46 1.18 0.98 0.91 0.72 0.41 0.42\nJoint multi-task 2.57 2.00 1.37 0.99 0.87 0.82 0.71 2.52 2.64 1.66 1.32 1.06 1.02 1.02 1.49 1.15 1.01 0.92 0.72 0.35 0.37\nJoint 2.50 2.08 1.68 1.37 1.26 1.01 0.77 2.06 2.61 1.48 1.14 0.26 0.13 0.16 1.38 1.12 0.91 0.87 0.75 0.44 0.46\nTable SM 11: Test set MAE of magnitude estimate for large events\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - 0.74 0.57 0.46 0.35 0.30 0.34 - 1.16 0.98 0.73 0.45 0.40 0.37 - 0.79 0.55 0.43 0.41 0.42 0.39\nMulti-task 0.98 0.73 0.55 0.44 0.38 0.30 0.27 0.94 0.86 0.74 0.63 0.54 0.54 0.53 1.02 0.76 0.58 0.52 0.38 0.28 0.27\nHigh magnitudes 0.58 0.58 0.58 0.57 0.58 0.58 0.58 0.63 0.67 0.68 0.70 0.69 0.69 0.70 0.44 0.43 0.40 0.38 0.30 0.27 0.30\nPlain 1.05 0.78 0.60 0.45 0.38 0.29 0.25 0.81 0.74 0.61 0.43 0.34 0.30 0.29 0.94 0.73 0.57 0.48 0.38 0.26 0.27\n±0.04 ±0.03 ±0.04 ±0.04 ±0.02 ±0.02 ±0.02 ±0.05 ±0.04 ±0.04 ±0.06 ±0.05 ±0.05 ±0.06 ±0.03 ±0.04 ±0.03 ±0.04 ±0.03 ±0.03 ±0.03\nEnsemble 1.04 0.76 0.57 0.44 0.38 0.29 0.24 0.75 0.71 0.59 0.40 0.30 0.26 0.25 0.90 0.69 0.53 0.45 0.36 0.24 0.24\nNo Upsampling 1.19 0.97 0.73 0.55 0.44 0.37 0.33 0.81 0.68 0.57 0.42 0.33 0.29 0.29 1.17 0.95 0.75 0.63 0.47 0.37 0.36\nPooling (Emb) 1.00 0.77 0.59 0.44 0.35 0.27 0.23 0.81 0.73 0.63 0.44 0.36 0.33 0.33 0.95 0.74 0.59 0.53 0.43 0.33 0.31\n±0.02 ±0.05 ±0.02 ±0.02 ±0.02 ±0.02 ±0.02 ±0.16 ±0.16 ±0.14 ±0.15 ±0.15 ±0.15 ±0.16 ±0.03 ±0.04 ±0.06 ±0.08 ±0.06 ±0.07 ±0.06\nPooling (Concat) 0.98 0.74 0.59 0.45 0.36 0.28 0.26 0.74 0.66 0.55 0.38 0.31 0.28 0.28 0.92 0.72 0.57 0.50 0.38 0.27 0.26\n±0.02 ±0.05 ±0.03 ±0.06 ±0.04 ±0.04 ±0.03 ±0.03 ±0.03 ±0.03 ±0.02 ±0.02 ±0.02 ±0.03 ±0.03 ±0.04 ±0.03 ±0.02 ±0.02 ±0.01 ±0.01\nTransfer 0.98 0.70 0.55 0.45 0.34 0.23 0.21 0.93 0.87 0.62 0.40 0.23 0.25 0.24 0.94 0.76 0.62 0.50 0.41 0.29 0.27\nMulti-task transfer 0.94 0.70 0.48 0.40 0.35 0.29 0.24 0.91 0.80 0.62 0.44 0.36 0.34 0.35 1.05 0.72 0.55 0.49 0.34 0.25 0.27\nVelocity - - - - - - - 0.95 0.82 0.63 0.51 0.39 0.38 0.38 1.11 0.87 0.63 0.52 0.41 0.26 0.26\nJoint multi-task 2.42 1.76 1.09 0.76 0.69 0.65 0.53 2.46 2.60 1.64 1.30 0.99 0.95 0.95 1.09 0.76 0.63 0.51 0.37 0.24 0.26\nJoint 2.40 1.94 1.36 1.11 1.04 0.81 0.59 1.80 2.61 1.36 1.14 0.26 0.11 0.16 1.04 0.77 0.60 0.50 0.39 0.31 0.32\nTable SM 12: Test set R2 score of magnitude estimate for large events\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline - -0.95 -0.09 0.26 0.49 0.57 0.41 - -4.15 -4.46 -1.77 -0.09 -0.21 -0.67 - -4.21 -1.37 -0.47 -0.41 -0.55 -0.16\nMulti-task -3.14 -1.45 -0.51 0.04 0.22 0.55 0.63 -3.99 -3.27 -2.24 -1.58 -0.83 -0.78 -0.72 -7.01 -4.52 -2.80 -2.83 -1.22 0.27 0.31\nHigh magnitudes -0.45 -0.45 -0.48 -0.46 -0.47 -0.47 -0.50 -1.62 -1.76 -1.85 -2.03 -1.83 -1.80 -1.84 -0.41 -0.36 -0.18 -0.12 0.27 0.48 0.31\nPlain -3.41 -1.69 -0.71 -0.05 0.20 0.53 0.65 -3.14 -2.67 -1.67 -0.52 0.13 0.29 0.34 -5.83 -3.77 -2.37 -2.42 -1.07 0.30 0.27\n±0.15 ±0.13 ±0.24 ±0.12 ±0.12 ±0.06 ±0.02 ±0.29 ±0.17 ±0.25 ±0.23 ±0.17 ±0.15 ±0.16 ±0.35 ±0.49 ±0.32 ±0.30 ±0.33 ±0.13 ±0.13\nEnsemble -3.31 -1.57 -0.55 0.00 0.24 0.55 0.67 -2.85 -2.44 -1.53 -0.41 0.25 0.43 0.49 -5.36 -3.51 -2.32 -2.30 -1.01 0.44 0.42\nNo Upsampling -4.26 -2.68 -1.39 -0.44 -0.19 0.32 0.44 -3.18 -2.30 -1.55 -0.62 0.08 0.28 0.27 -8.93 -5.95 -3.88 -3.48 -1.81 -0.25 -0.17\nPooling (Emb) -3.24 -1.69 -0.69 -0.02 0.35 0.61 0.71 -3.18 -2.59 -1.69 -0.60 -0.03 0.13 0.15 -6.01 -3.69 -2.55 -2.52 -1.36 0.10 0.19\n±0.12 ±0.36 ±0.12 ±0.10 ±0.07 ±0.06 ±0.04 ±1.07 ±0.98 ±0.73 ±0.70 ±0.63 ±0.64 ±0.66 ±0.51 ±0.42 ±0.45 ±0.65 ±0.32 ±0.27 ±0.22\nPooling (Concat) -3.06 -1.41 -0.69 -0.02 0.35 0.62 0.66 -2.64 -2.09 -1.21 -0.26 0.17 0.35 0.37 -5.53 -3.47 -2.31 -2.24 -1.03 0.28 0.29\n±0.12 ±0.27 ±0.08 ±0.17 ±0.15 ±0.08 ±0.06 ±0.31 ±0.25 ±0.20 ±0.17 ±0.14 ±0.11 ±0.12 ±0.37 ±0.26 ±0.27 ±0.23 ±0.21 ±0.04 ±0.05\nTransfer -2.82 -1.22 -0.48 0.03 0.36 0.70 0.75 -3.95 -3.39 -1.89 -0.51 0.43 0.37 0.44 -7.15 -5.19 -3.32 -3.03 -1.99 0.11 0.21\nMulti-task transfer-2.79 -1.27 -0.19 0.21 0.45 0.61 0.70 -4.11 -3.14 -1.69 -0.50 0.08 0.15 0.10 -8.72 -4.65 -3.03 -2.53 -0.83 0.39 0.34\nVelocity - - - - - - - -4.04 -3.17 -1.67 -0.79 0.01 0.01 0.01 -8.92 -5.50 -3.44 -2.87 -1.39 0.22 0.18\nJoint multi-task -13.69 -7.88 -3.17 -1.18 -0.67 -0.51 -0.10 -100.77 -110.26 -42.87 -26.91 -16.89 -15.79 -15.52 -9.29 -5.10 -3.71 -2.97 -1.44 0.43 0.38\nJoint -12.85 -8.63 -5.23 -3.17 -2.53 -1.28 -0.32 -66.63 -108.03 -34.05 -19.94 -0.08 0.74 0.59 -7.90 -4.79 -2.89 -2.53 -1.58 0.10 0.04\nTable SM 13: Test set root squared mean for hypocentral error. We note that only 4 out of 10 models for the Italy location ensemble converged. We used only the\nconverged models for the ensemble evaluation.\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline 78.97 77.62 74.32 69.53 44.28 17.51 17.58 66.92 75.15 91.77 189.05 172.65 69.88 56.09 82.89 88.03 90.80 98.55 184.61 373.34 332.11\nMulti-task 48.49 40.17 33.10 24.79 17.69 12.63 11.86 67.06 65.52 61.91 58.09 55.13 54.93 56.25 140.83 141.21 144.12 139.64 135.31 134.77 134.13\nHigh magnitudes124.94 113.56 113.02 119.20 116.61 90.28 93.75 192.27 190.97 189.82 188.28 186.77 187.36 188.10 393.78 377.11 365.31 383.16 352.42 344.42 352.45\nPlain 43.96 36.99 28.66 20.74 12.52 8.97 8.83 51.94 50.35 45.49 41.07 35.33 35.89 38.68 77.08 73.51 72.16 69.93 68.18 66.60 66.10\nEnsemble 39.36 32.31 25.04 18.24 10.77 7.90 7.81 63.28 61.57 57.77 54.52 51.95 52.00 53.38 64.15 61.48 60.09 58.64 56.45 54.26 54.07\nPooling (Emb) 44.04 37.00 30.42 25.30 18.56 15.10 14.40 65.73 65.00 61.25 59.02 56.91 57.07 58.21 98.95 95.17 93.88 91.29 88.20 87.26 87.13\nPooling (Concat)53.19 44.53 38.02 33.01 27.09 22.31 21.80 181.39 181.41 181.47 181.52 181.46 181.47 181.52 163.43 155.61 154.13 152.15 149.51 148.23 148.29\nTransfer 44.92 37.96 30.76 21.24 13.78 9.05 9.25 52.63 51.21 44.56 41.04 36.99 36.73 39.38 64.80 60.76 60.72 56.79 53.61 51.32 52.27\nMulti-task transfer51.07 41.46 32.91 23.00 14.56 10.80 10.46 55.78 54.26 50.60 47.10 43.82 45.37 49.68 98.96 91.05 90.85 87.61 86.29 87.75 88.27\nVelocity - - - - - - - 75.50 74.75 71.02 68.44 66.97 66.67 68.56 77.41 69.06 67.91 64.79 62.96 61.95 60.64\nJoint multi-task 52.36 43.38 35.14 25.23 17.32 14.82 14.64 62.87 61.27 56.59 54.38 51.56 51.81 53.81 111.41 100.82 100.55 97.64 97.20 97.92 97.39\nJoint 49.55 42.75 35.52 24.32 15.69 10.12 9.76 67.38 65.06 60.95 58.38 58.10 59.13 60.29 198.47 196.29 195.19 196.00 194.68 194.46 195.26\nTable SM 14: Test set mean absolute hypocentral error. We note that only 4 out of 10 models for the Italy location ensemble converged. We used only the converged\nmodels for the ensemble evaluation.\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline 73.30 71.60 67.38 58.00 28.15 13.80 13.92 26.03 32.11 50.13 111.88 71.80 18.57 15.70 43.66 46.38 49.29 54.04 78.95 154.71 124.94\nMulti-task 33.78 27.71 22.39 15.81 10.99 8.20 7.89 27.09 25.25 22.88 20.27 18.87 18.76 19.22 67.48 66.47 65.97 63.09 59.05 57.13 56.76\nHigh magnitudes106.43 98.18 99.49 103.49 104.08 82.75 85.67 120.07 119.24 119.07 117.63 116.63 116.50 116.93 229.31 221.00 218.40 218.67 208.64 203.95 207.15\nPlain 30.41 25.01 19.06 12.40 7.26 5.42 5.30 20.47 18.84 16.59 14.22 11.86 11.85 12.91 40.88 37.35 35.43 33.15 30.28 29.20 29.06\nEnsemble 27.20 22.09 16.94 11.06 6.55 4.90 4.77 28.88 27.68 25.78 24.20 23.11 23.09 23.48 35.25 31.59 29.49 27.50 24.47 23.11 22.88\nPooling (Emb) 30.75 25.44 20.86 16.72 12.61 9.60 9.04 28.16 26.93 24.85 23.37 22.20 22.23 22.59 51.57 45.70 43.15 40.34 37.48 35.94 35.75\nPooling (Concat) 37.68 30.62 25.96 21.94 17.89 14.11 13.68 83.53 83.47 83.47 83.58 83.51 83.42 83.43 125.56 118.22 117.21 116.03 113.29 111.72 111.59\nTransfer 31.39 26.03 20.29 12.75 7.47 5.27 5.16 21.09 19.39 16.22 13.42 11.55 11.02 11.79 31.86 27.84 25.81 22.87 20.11 18.49 18.16\nMulti-task transfer35.86 28.93 22.46 14.69 9.10 6.77 6.60 24.66 23.51 21.80 19.75 18.74 19.00 20.18 48.75 43.28 40.07 37.48 35.21 34.39 34.21\nVelocity - - - - - - - 32.69 31.80 29.85 28.20 27.17 27.25 27.85 43.11 39.53 37.60 34.71 32.58 31.59 31.09\nJoint multi-task 38.31 31.64 25.02 17.28 11.90 9.78 9.65 30.48 28.98 27.13 25.74 24.53 24.56 25.30 54.41 48.50 44.99 42.34 40.29 39.34 38.94\nJoint 34.75 29.17 23.04 14.61 8.66 5.95 5.70 24.61 22.52 19.82 17.05 15.81 15.72 16.22 124.42 121.32 118.92 117.11 113.93 112.59 112.77\nTable SM 15: Test set root squared mean for epicentral error. We note that only 4 out of 10 models for the Italy location ensemble converged. We used only the converged\nmodels for the ensemble evaluation.\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline 65.49 63.86 59.74 52.07 29.72 13.82 13.82 55.77 64.82 67.69 84.57 116.35 66.82 53.17 73.93 79.82 82.69 86.91 160.74 360.57 325.00\nMulti-task 44.19 36.51 29.74 22.46 16.20 11.29 10.46 61.81 60.39 58.22 56.28 53.67 53.46 54.20 138.89 139.48 142.85 138.34 134.03 133.63 133.03\nHigh magnitudes121.57 111.06 110.52 115.92 114.20 88.24 90.81 187.58 186.33 185.25 183.85 182.34 182.95 183.69 387.59 367.30 352.89 375.67 338.82 331.22 338.71\nPlain 39.48 32.95 25.25 18.34 11.24 7.73 7.58 45.62 44.65 40.95 38.06 33.15 33.51 35.20 73.10 70.09 69.37 67.40 66.22 64.86 64.30\nEnsemble 35.68 29.21 22.28 16.12 9.30 6.70 6.64 58.05 57.09 53.99 52.09 49.95 49.92 50.59 59.82 57.60 56.75 55.52 53.94 52.15 51.98\nPooling (Emb) 40.11 33.68 27.41 22.72 16.53 13.79 13.27 59.18 58.42 54.67 52.83 50.91 51.20 52.29 95.42 91.99 91.39 88.92 86.01 85.25 85.14\nPooling (Concat)49.57 41.56 35.44 30.80 25.52 21.06 20.77 178.40 178.41 178.43 178.46 178.43 178.45 178.51 161.48 153.83 152.69 151.03 148.65 147.48 147.56\nTransfer 39.80 33.04 26.20 18.72 12.51 7.95 8.21 46.87 45.63 41.84 39.46 35.66 35.80 37.94 59.28 55.24 55.34 52.53 50.00 48.05 49.31\nMulti-task transfer46.17 37.03 28.98 20.61 12.97 9.45 9.12 49.79 48.63 45.65 43.73 40.68 42.06 45.25 95.24 87.75 87.95 84.75 83.85 85.67 86.23\nVelocity - - - - - - - 70.61 70.14 66.79 65.30 63.80 63.76 64.77 72.34 64.27 63.83 60.69 59.33 58.74 57.48\nJoint multi-task 47.10 38.58 30.60 22.14 14.93 13.05 12.80 56.11 54.73 50.60 48.94 46.58 47.09 48.61 108.26 97.65 97.53 94.64 94.43 95.36 94.80\nJoint 44.46 37.75 30.83 21.41 14.44 8.86 8.53 62.76 61.32 58.91 57.21 57.06 57.98 58.71 197.03 195.09 194.19 195.16 194.27 194.13 194.85\nTable SM 16: Test set mean absolute epicentral error. We note that only 4 out of 10 models for the Italy location ensemble converged. We used only the converged models\nfor the ensemble evaluation.\nChile Italy Japan\n0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s 0.5s 1.0s 2.0s 4.0s 8.0s 16.0s 25.0s\nBaseline 58.27 56.23 51.33 40.82 18.73 10.25 10.28 21.58 26.72 33.11 45.59 46.45 14.30 10.58 34.11 37.77 41.37 45.18 64.04 141.80 114.48\nMulti-task 30.01 24.41 19.21 12.94 8.79 6.38 6.05 24.34 22.67 20.70 18.78 17.57 17.47 17.52 64.06 63.41 63.32 60.74 57.21 55.60 55.31\nHigh magnitudes103.82 96.17 97.49 100.94 102.08 80.90 83.17 116.66 115.99 115.90 114.44 113.44 113.30 113.71 219.98 209.35 206.58 207.45 196.24 191.91 195.00\nPlain 26.76 21.67 16.00 9.63 5.52 3.96 3.86 17.45 16.05 14.20 12.38 10.23 10.19 11.01 37.39 34.24 32.54 30.44 28.43 27.65 27.50\nEnsemble 23.74 18.94 13.98 8.33 4.85 3.56 3.44 25.85 24.95 23.36 22.24 21.42 21.38 21.36 31.78 28.46 26.57 24.99 22.89 21.76 21.51\nPooling (Emb) 27.15 22.27 17.82 13.70 10.32 7.93 7.45 24.01 22.75 20.68 19.47 18.48 18.54 18.79 47.74 42.31 40.22 37.77 35.36 34.04 33.88\nPooling (Concat) 34.09 27.53 23.11 19.18 15.77 12.50 12.11 82.19 82.10 82.01 82.02 82.02 81.96 81.98 122.65 116.01 115.44 114.68 112.38 111.04 110.94\nTransfer 27.44 22.33 16.81 9.94 5.77 3.93 3.84 18.38 16.85 14.44 12.30 10.55 10.12 10.67 27.90 23.94 22.08 19.80 18.12 16.67 16.36\nMulti-task transfer31.86 25.37 19.18 11.73 7.09 5.17 5.02 20.92 19.86 18.63 17.33 16.53 16.71 17.45 44.96 39.92 37.04 34.72 33.03 32.51 32.31\nVelocity - - - - - - - 29.32 28.63 26.89 25.71 24.69 24.93 25.05 39.14 35.83 34.14 31.43 30.23 29.57 29.07\nJoint multi-task 34.01 27.81 21.39 13.99 9.18 7.56 7.42 25.12 23.68 22.14 21.25 20.34 20.43 20.83 50.75 45.09 41.85 39.56 37.90 37.11 36.70\nJoint 30.62 25.28 19.39 11.55 6.78 4.44 4.20 21.85 20.16 18.04 15.83 14.63 14.58 14.76 120.16 117.03 115.00 113.76 112.04 110.99 111.15",
  "topic": "Magnitude (astronomy)",
  "concepts": [
    {
      "name": "Magnitude (astronomy)",
      "score": 0.6774495840072632
    },
    {
      "name": "Computer science",
      "score": 0.6697087287902832
    },
    {
      "name": "Warning system",
      "score": 0.5310326814651489
    },
    {
      "name": "Transfer of learning",
      "score": 0.505706787109375
    },
    {
      "name": "Waveform",
      "score": 0.4732773005962372
    },
    {
      "name": "Transformer",
      "score": 0.4690316915512085
    },
    {
      "name": "Deep learning",
      "score": 0.446557879447937
    },
    {
      "name": "Data mining",
      "score": 0.40812861919403076
    },
    {
      "name": "Machine learning",
      "score": 0.35076904296875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3249719738960266
    },
    {
      "name": "Engineering",
      "score": 0.0848502516746521
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Radar",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210152878",
      "name": "GFZ Helmholtz Centre for Geosciences",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I75951250",
      "name": "Freie Universität Berlin",
      "country": "DE"
    }
  ],
  "cited_by": 96
}