{
    "title": "Interactively Providing Explanations for Transformer Language Models",
    "url": "https://openalex.org/W4296634084",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2106763651",
            "name": "Felix Friedrich",
            "affiliations": [
                null,
                "Technical University of Darmstadt"
            ]
        },
        {
            "id": "https://openalex.org/A2789883686",
            "name": "Patrick Schramowski",
            "affiliations": [
                null,
                "Technical University of Darmstadt"
            ]
        },
        {
            "id": "https://openalex.org/A2805654723",
            "name": "Christopher Tauchmann",
            "affiliations": [
                "Technical University of Darmstadt"
            ]
        },
        {
            "id": "https://openalex.org/A2252032993",
            "name": "Kristian Kersting",
            "affiliations": [
                "Technical University of Darmstadt",
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2811104224",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3035989815",
        "https://openalex.org/W6695661434",
        "https://openalex.org/W3157894658",
        "https://openalex.org/W3100931974",
        "https://openalex.org/W3165941178",
        "https://openalex.org/W2516809705",
        "https://openalex.org/W4287589258",
        "https://openalex.org/W3158023256",
        "https://openalex.org/W2958514452",
        "https://openalex.org/W3048549109",
        "https://openalex.org/W3101349400",
        "https://openalex.org/W2050482109",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2970641574"
    ],
    "abstract": "Transformer language models (LMs) are state of the art in a multitude of NLP tasks.Despite these successes, their opaqueness remains problematic, especially as the training data might be unfiltered and contain biases.As a result, ethical concerns about these models arise, which can have a substantial negative impact on society as they get increasingly integrated into our lives [1].Therefore, it is not surprising that a growing body of work aims to provide interpretability and explainability to black-box LMs [2]: Recent evaluations of saliency or attribution methods [3,4] find that, while intriguing, different methods assign importance to different inputs for the same outputs, thus encouraging misinterpretation and reporting bias [5,6].Moreover, these methods primarily focus on post-hoc explanations of (sometimes spurious) input-output correlations.Instead, we emphasize using (interactive) prototype networks directly incorporated into the model architecture and hence explain the reasoning behind the network's decisions.",
    "full_text": null
}