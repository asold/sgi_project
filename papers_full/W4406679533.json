{
  "title": "What large language models know and what people think they know",
  "url": "https://openalex.org/W4406679533",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A499903789",
      "name": "Mark Steyvers",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2806790889",
      "name": "Heliodoro Tejeda",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A3184244395",
      "name": "Aakriti Kumar",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A3092189991",
      "name": "Catarina Belém",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A5093800032",
      "name": "Sheer Karny",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2101917445",
      "name": "Xinyue Hu",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2165897775",
      "name": "Lukas W. Mayer",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2137074633",
      "name": "Padhraic Smyth",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A499903789",
      "name": "Mark Steyvers",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2806790889",
      "name": "Heliodoro Tejeda",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A3184244395",
      "name": "Aakriti Kumar",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A3092189991",
      "name": "Catarina Belém",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A5093800032",
      "name": "Sheer Karny",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2101917445",
      "name": "Xinyue Hu",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2165897775",
      "name": "Lukas W. Mayer",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2137074633",
      "name": "Padhraic Smyth",
      "affiliations": [
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2034317619",
    "https://openalex.org/W4386836971",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4387851331",
    "https://openalex.org/W4319332853",
    "https://openalex.org/W4388585881",
    "https://openalex.org/W6839548382",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4399803256",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4381797997",
    "https://openalex.org/W4402671787",
    "https://openalex.org/W2144200470",
    "https://openalex.org/W2056533510",
    "https://openalex.org/W4388787677",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4390578173",
    "https://openalex.org/W2132256294",
    "https://openalex.org/W4388624604",
    "https://openalex.org/W3030350595",
    "https://openalex.org/W2896487960",
    "https://openalex.org/W3163443091",
    "https://openalex.org/W3159250634",
    "https://openalex.org/W4225117847",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4388717366",
    "https://openalex.org/W4389523793",
    "https://openalex.org/W4389518686",
    "https://openalex.org/W2000803197",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W2143841415",
    "https://openalex.org/W6906376699",
    "https://openalex.org/W4404534210"
  ],
  "abstract": "Abstract As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. Whereas recent work has focused on LLMs’ internal confidence, less is understood about how effectively they convey uncertainty to users. Here we explore the calibration gap, which refers to the difference between human confidence in LLM-generated answers and the models’ actual confidence, and the discrimination gap, which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Moreover, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models’ internal confidence, both the calibration gap and the discrimination gap narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in artificial-intelligence-assisted decision-making environments.",
  "full_text": "Nature Machine Intelligence | Volume 7 | February 2025 | 221–231\n 221\nnature machine intelligence\nhttps://doi.org/10.1038/s42256-024-00976-7\nArticle\nWhat large language models know and what \npeople think they know\nMark Steyvers    1 , Heliodoro Tejeda1, Aakriti Kumar1, Catarina Belem2, \nSheer Karny1, Xinyue Hu1, Lukas W. Mayer1 & Padhraic Smyth2\nAs artificial intelligence systems, particularly large language models \n(LLMs), become increasingly integrated into decision-making processes, \nthe ability to trust their outputs is crucial. T o earn human trust, LLMs must \nbe well calibrated such that they can accurately assess and communicate \nthe likelihood of their predictions being correct. Whereas recent work \nhas focused on LLMs’ internal confidence, less is understood about \nhow effectively they convey uncertainty to users. Here we explore the \ncalibration gap, which refers to the difference between human confidence \nin LLM-generated answers and the models’ actual confidence, and the \ndiscrimination gap, which reflects how well humans and models can \ndistinguish between correct and incorrect answers. Our experiments \nwith multiple-choice and short-answer questions reveal that users tend \nto overestimate the accuracy of LLM responses when provided with \ndefault explanations. Moreover, longer explanations increased user \nconfidence, even when the extra length did not improve answer accuracy. \nBy adjusting LLM explanations to better reflect the models’ internal \nconfidence, both the calibration gap and the discrimination gap narrowed, \nsignificantly improving user perception of LLM accuracy. These findings \nunderscore the importance of accurate uncertainty communication \nand highlight the effect of explanation length in influencing user trust in \nartificial-intelligence-assisted decision-making environments.\nUncertainty communication plays a critical role in decision-making \nand policy development. Uncertainties are often expressed verbally to \nhelp stakeholders understand risks and make informed choices across \na wide range of domains, including climate policy, law, medicine and \nintelligence forecasting. Psychology research has investigated percep-\ntions of verbally expressed uncertainty (for example, phrases such as \n‘very unlikely’ or ‘almost certain’) in these domains1–5. Despite their \nlack of precision in communicating probabilities, verbal probability \nphrases provide a simple and effective way to communicate uncer-\ntainty in natural language contexts. The emergence of large language \nmodels (LLMs) introduces new complexities in the area of uncertainty \ncommunication. These models are increasingly integrated into areas \nsuch as public health6, coding7 and education8. However, the question \nof how effectively LLMs communicate uncertainty is unexplored. As \nthe primary mode of communication with LLMs is through natural \nlanguage, it is critical to understand whether LLMs are able to accu-\nrately convey through verbal means what they know or do not know.\nRecent research raises doubts about the reliability of the infor -\nmation that LLMs generate. One notable issue is the possibility of \ngenerating responses that, while convincing, may be inaccurate or \nnonsensical9,10. The unreliability of LLMs has led developers of LLMs to \ncaution against the uncritical acceptance of model outputs11, suggest-\ning that it is not always clear when the models are or are not confident \nin the knowledge communicated to the user.\nReceived: 2 May 2024\nAccepted: 13 December 2024\nPublished online: 21 January 2025\n Check for updates\n1Department of Cognitive Sciences, University of California, Irvine, CA, USA. 2Department of Computer Science, University of California, Irvine, CA, USA. \n e-mail: mark.steyvers@uci.edu\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231 222\nArticle https://doi.org/10.1038/s42256-024-00976-7\nof this confidence. In addition, we investigate the discrimination gap, \nwhich relates to the difference in the ability to discriminate between \nprobably correct and incorrect answers. Any discrimination gap shows \nthat whatever internal LLM representation is used to tell the differ -\nence between probably correct and incorrect answers is not conveyed \neffectively to humans. We address two specific research questions in \nthis context. First, how large are the calibration and discrimination \ngaps? That is, is there a notable gap between LLM model confidence \nand human confidence in terms of how each assesses the true accuracy \nof the LLM? Second, can the calibration and discrimination gaps be \nreduced? Can the quality of human confidence in an LLM be improved \nby adapting the textual output of the LLM to internal model confi -\ndence? These questions have important implications for the design of \nreliable LLM assistants. By aligning the LLM’s internal confidence with \nhuman perception of this confidence, we can bridge the gap between \nwhat LLMs know and what people think they know, which is crucial for \nthe development of effective and trustworthy assistants\nOur contributions in this context are twofold. First, we present a \nset of experimental studies and a dataset that directly captures human \nassessment of LLM confidence in a question-answering context, provid-\ning insight into human perceptions of LLM textual responses. Second, \nwe test and suggest ways of generating LLM responses that improve the \ncalibration quality of human confidence relative to the LLM assistant’s \nmodel confidence and the LLM’s true accuracy.\nLLMs\nWe use three publicly available LLMs in our studies: GPT-3.5 (ref. 20), \nPaLM2 (ref. 21) and GPT-4o. We apply the GPT-3.5 and PaLM2 models \nto a subset of multiple-choice questions from the Massive Multitask \nAt the same time, recent research has also indicated that LLMs \nhave the ability, to a certain degree, to accurately discern their own \nknowledge boundaries. LLMs in particular can exhibit a reasonable level \nof calibration for multiple-choice questions such that the probability \nthe model assigns to a selected answer tracks with the probability that \nthis answer is correct12–14. In addition, recent studies show that LLMs \ncan distinguish between answerable and unanswerable questions13,15 \nand the internal state of an LLM can distinguish between truthful state-\nments and lies16 and truthful statements and confabulations17. These \nfindings suggest that LLMs may possess an internal mechanism that is \nreflective of self-knowledge.\nIn the specific context of question-answering, an LLM’s model \nconfidence is typically equated to the probability assigned by the LLM \nto the selected answer relative to other possible answers (for example, \nrefs. 18,19). However, from the perspective of a human interacting \nwith the LLM, this internal model confidence is not usually displayed \nto human users as part of LLM output. Instead, in current practice, \nhumans rely solely on the language produced by the LLM to assess \nLLM confidence. T o contrast with model confidence, in this article \nwe use the term human confidence to refer to a human’s assessment \n(expressed as a probability) of how likely it is that the LLM’s answer is \ncorrect based only on the language produced by the LLM without any \nknowledge of the LLM’s internal model confidence.\nSurprisingly, studies focused on investigating human confidence \nin LLM outputs are lacking. In this article, we take a step in addressing \nthis issue and investigate what we term the calibration gap, namely \nthe difference in the reliability of LLM model confidence and human \nconfidence. In effect, the calibration gap represents the gap between an \nLLM’s own internal confidence of what it knows and human perception \nQ uest i o n:  T he signif i cance o f  \nFrere's discovery  w a s t hat  w hat  he \nf ound w as locat ed:\nChoose f r om t he f ollow i n g  opt ions:\nA. beneath the remai n s of (...)\nB. on the surface, above the (...)\nC . i n  a  r egi on o f  A f r i c a  w her e  (...)\nD . on t op o f  t he r u i n s  o f  (...)\nP l ease answ er thi s  questi on.\nA n s w er A ,  B , C ,  o r  D . T h e  a n s w er\ni s :\nMode l c onf i den c e\nT he a ns w er is  [C] because Frere's\ndiscovery  o f s t one t ools in t he\nSomme Valley  o f  France w as\nsignif i cant  because i t w as in a\nregion of Africa (...) possibility  of\nhuman habit a t i on in A f rica much\nearlier t han previously  t hought . \nA\nB\nC\nD\n0 . 1 3\n0 . 0 1\n0 . 4 0\n0 . 4 6\n95%\nW hat  i s  t he p r obability\nt hat t he  LLM  a ns w er i s\ncorrect? \nLLM\nMode l  e x p l anati o n\nM ultiple- c h o i c e  ques tio n H u m an c onfidenc e\nS hort - ans we r quest ion Model \nconfidence I am not  ent irely  sure t hat  t he\nansw e r is dove,  as sy mbols of\nhappiness can vary  across\ncult ures.  W hile doves are o ften\nassociat ed w i t h  peace and love,\nw hich can cont ribut e  t o  f eelings\no f  happiness,  t here  a re  o t her\nbirds like robins or canaries t hat\nmight  also be seen as s y mbols\no f  joy  in diﬀ erent  t r adit i ons.  … \nT RUE\nFALSE\n0 . 1 8\n0 . 8 2\n75%\nW hat  i s  t he p r obability\nthat the LLM answ er i s\ncorrect? \nQ uest i o n:  W hich bird is the\nint ernat ional s y mbol f or happiness ?  \nD o v e\nModel \nanswer\nQ uest i o n:  W hich bird is the\nint ernat ional s y mbol f or happiness ?  \nProposed answ e r:  D o v e\nI s  t he  pr oposed a ns w er:\nA )  t r ue or B )  fal s e.\nT he answ e r is:  \nMode l  e x p l anati o n\nFig. 1 | Overview of the evaluation methodology for assessing the calibration \ngap between model confidence and human confidence in the model. The \nmultiple-choice questions, the approach works as follows: (1) prompt the LLM \nwith a question to obtain the model’s internal confidence for each answer choice; \n(2) select the most likely answer and prompt the model a second time to generate \nan explanation for the given answer; (3) obtain the human confidence by showing \nusers the question and the LLM's explanation and asking users to indicate the \nprobability that the model is correct. In this toy example, the model confidence for \nthe multiple-choice question is 0.46 for answer C, whereas the human confidence \nis 0.95. For short-answer questions, the approach is similar except that internal \nmodel confidence is obtained by an additional step where the LLM is prompted to \nevaluate whether the previously provided answer to the question is true or false\n13. \nIn the short-answer question example, the LLM model explanation was modified \nwith uncertainty language to convey the low model confidence (0.18). For the two \ntoy examples, the correct answers are ‘ A’ and ‘blue bird’ .\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231\n 223\nArticle https://doi.org/10.1038/s42256-024-00976-7\nLanguage Understanding (MMLU) dataset, a comprehensive dataset that \ncontains multiple-choice questions from various knowledge domains, \nsuch as science, technology, engineering and mathematics (STEM), \nhumanities, social sciences and more19. We apply the GPT-4o model \nto a subset of short-answer questions from the Trivia QA dataset22. For \neach multiple-choice and short-answer question, we assess the model \nconfidence by computing the token likelihoods (see the Methods for \ndetails). This method for reading out model confidence allows a direct \ncomputation of the relative probabilities of different possible answers \nin multiple-choice questions12,13,18,19,23 and the probability that the answer \nto an open-ended question is correct13,17. We investigate the relationship \nbetween model confidence and accuracy to determine whether the LLM \nis reasonably well calibrated, independent of the LLM’s ability to elicit \nwell-calibrated confidence from humans who use the LLM.\nBehavioural experiments\nWe designed behavioural experiments to evaluate human perceptions of \nLLM confidence. In these experiments, participants estimate the prob-\nability that the LLM’s answer to a multiple-choice or short-answer ques-\ntion is correct based on the explanation that the LLM provided (Fig. 1). \nParticipants are not provided any direct access to the LLM’s numerical \nmodel confidence, allowing us to make inferences about participants’ \nperceptions of the confidence of the LLM based on model explana-\ntions alone. In addition, for the multiple-choice questions part of the \nexperiment only, with the assistance of the LLM, participants provided \nanswers to the questions. Previous research has demonstrated that the \nMMLU multiple-choice questions are difficult for participants who lack \ndomain expertise, resulting in near-chance accuracy19. We anticipate that \nthese questions will be difficult to answer without the assistance of the \nLLM because the majority of the participants in our experiments lack \ndomain expertise, and their perception of the explanation’s content will \ninfluence their evaluation more than their own knowledge.\nWe conducted two experiments each involving the three types of \nLLMs and two types of questions (see Table 1 for an overview). Experi-\nment 1 assesses human perceptions of LLM accuracy using the LLM’s \ndefault explanations for either multiple-choice or short-answer ques-\ntions. The results from this experiment allow us to address the first \nresearch question regarding the size of the calibration and discrimina-\ntion gap between model and human confidence. Experiment 2 manipu-\nlates the prompts to produce three levels of uncertainty language (low, \nmedium and high confidence) and three levels of explanation length, \nresulting in nine different types of explanations presented to partici-\npants. The prompts are designed to include uncertainty language cor-\nresponding to model confidence at the start of the explanation. Table 2 \nillustrates explanations from a particular multiple-choice question \nused in the experiments (see Supplementary Table 2 for the full model \nexplanations). The results from this experiment serve two purposes. \nFirst, we establish that human confidence varies with the uncertainty \nlanguage and the length of the explanation. Next, we use the results \nfrom experiment 2 to answer the second research question, which is to \nunderstand how the calibration and discrimination gap can be reduced \nby aligning the uncertainty language with model confidence—showing \na low-, medium- and high-confidence explanation when the model has \nlow, medium and high confidence, respectively. The Supplementary \nInformation (‘experiment 3’) reports the results from an additional \nexperiment with a different prompting approach that alters the default \nexplanations from experiment 1. We use the two metrics to assess the \nrelationship between human and model confidence and model accu-\nracy. See Methods for details.\nTable 1 | Overview of experiments\nExperiment Question type LLM Explanation type Number of \nparticipants\n1a Multiple choice GPT-3.5 Default explanations 41\n1b Multiple choice PaLM2 Default explanations 39\n1c Short answer GPT-4o Default explanations 42\n2a Multiple choice GPT-3.5 Modified explanations 60\n2b Multiple choice PaLM2 Modified explanations 60\n2c Short answer GPT-4o Modified explanations 59\nTable 2 | Example explanations for the answer by GPT-3.5 to \na multiple-choice question used in experiments 1a and 2a\nPrompt style Explanation\nExperiment 1a\nBaseline The answer is [A] Velocity. This is because in an \ninelastic collision, the two objects involved will stick \ntogether and move with a common velocity after the \ncollision. […] In this case, the initial momentum of \nthe 1.5-kg cart is transferred to the combined mass of \nthe two carts after the collision, resulting in the same \nvelocity for both carts. Therefore, the correct answer is \n[A] Velocity.\nExperiment 2a\nLow confidence \nand long\nI am not completely sure that the answer is [A] \nbecause velocity is a vector quantity that includes \nboth magnitude and direction. In this scenario, the \ncarts may have the same magnitude of velocity, but \nthey may not necessarily have the same direction. \nTherefore, I am not completely convinced that the \nanswer is [A]. […] In this case, the carts would indeed \nhave the same velocity as each other.\nMedium confidence \nand long\nI am somewhat sure that the answer is [A] because \nvelocity is a vector quantity that takes into account \nboth the speed and direction of an object’s motion. In \nthis scenario, the two carts are colliding and sticking \ntogether, […] Therefore, I am somewhat sure that the \nanswer is [A] because the velocity of the two carts will \nbe the same after the collision due to the conservation \nof momentum principle.\nHigh confidence \nand long\nI am sure that the answer is [A] because velocity is a \nvector quantity that describes the speed and direction \nof an object’s motion. In this scenario, the two carts are \ncolliding and sticking together, meaning they will have \nthe same final velocity after the collision. […] This is \nalso supported by the fact that the carts have different \nmasses, so their kinetic energies and linear momenta \nwill not be the same. Thus, the correct answer is [A] \nvelocity.\nLow confidence \nand short\nI am not sure that the answer is [A] because the carts \nmay have different velocities before and after the \ncollision due to the conservation of momentum.\nMedium confidence \nand short\nI am somewhat sure that the answer is [A] because \nvelocity is a vector quantity that takes into account \nboth the speed and direction of an object’s motion. \nIn this scenario, the carts will have the same velocity \nafter the collision because they are stuck together and \nmoving in the same direction.\nHigh confidence \nand short\nI am sure that the answer is [A] because velocity is a \nvector quantity that is conserved in collisions, meaning \nthat the final velocity of the two carts will be the same.\nLow confidence \nand uncertainty \nonly\nI am not sure that the answer is [A].\nMedium confidence \nand uncertainty \nonly\nI am somewhat sure that the answer is [A].\nHigh confidence \nand uncertainty \nonly\nI am sure that the answer is [A].\nThe particular question is: “A moving 1.5 kg cart collides with and sticks to a 0.5 kg cart which \nwas initially at rest. Immediately after the collision, the carts each have the same ______ as \neach other. Choose from the following options: A) Velocity B) Kinetic energy C) Mass D) Linear \nmomentum.” The correct answer to this question is A.\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231 224\nArticle https://doi.org/10.1038/s42256-024-00976-7\nResults\nWe start by examining the results from experiment 1 and compare \nhuman and model confidence in the case where LLMs generate default \nexplanations for participants. We present the results for two different \nmetrics: (1) expected calibration error (ECE), which assesses the degree \nto which confidence scores from the model or the human reflect the \ntrue accuracy of the LLM, and (2) area under the curve (AUC), which \nassesses the degree to which confidence scores discriminate between \ncorrect and incorrect responses (see the Methods for details). The find-\nings indicate that there is a significant gap, as measured by calibration \nand discrimination, between what LLMs know and what humans believe \nthey know based on default explanations.\nCalibration gap\nFigure 2 (left) shows the ECE for both model and human confidence. The \nresults show a calibration gap; across the different types of LLMs and \ntypes of questions (multiple choice and short answer), the ECE metric \nis much lower for model confidence (in grey) than for human confi-\ndence with baseline explanations (in green). This gap demonstrates that \nstandard explanations provided by the LLM do not enable participants \nto judge the likelihood of correctness of the LLM’s answers, leading to \na misalignment between perceived accuracy and actual LLM accuracy.\nFigure 3 expands on the calibration results in Fig. 2 to show detailed \ncalibration results for each LLM and each experimental condition. The \ndiagrams show how well model confidence (left column) and human \nconfidence (right two columns) are calibrated. The ideal calibration (that \nis, ECE of 0) would yield results along the diagonal. For multiple-choice \nquestions, both LLMs have a tendency to be overconfident, resulting in \ncalibration lines below the diagonal. For the short-answer questions, the \nLLM is somewhat underconfident. Comparing the LLM with the human \ncalibration in experiment 1 (middle column), the results show that, for \nthe multiple-choice questions, human miscalibration is primarily due \nto overconfidence, indicating that people generally believe that LLMs \nare more accurate than they actually are. The histograms (inset panels) \ndemonstrate that a substantial portion of the calibration error is due \nto participants’ propensity to produce high-confidence scores, even \nthough the model accuracy for the associated questions is much lower \nthan expected based on confidence.\nDiscrimination gap\nParticipants are not very good, relative to the LLM, at discriminating \nbetween which answers are probably correct or incorrect based on the \ndefault explanation. We assess discrimination using the AUC metric \napplied to the human confidence ratings. Figure 2 (right) shows the \nAUC for both model and human confidence. The results show a gap \nbetween how well model and human confidence discriminate between \ncorrect and incorrect answers. The LLM model confidence discrimi-\nnates between correct and incorrect answers well above chance (GPT-\n3.5 AUC 0.751 and PaLM2 AUC 0.746 for the multiple-choice questions \nand GPT-4o AUC 0.781 for the short-answer questions). By contrast, \nparticipants who view the default explanations in experiment 1 were \nonly slightly better than random guessing (AUC 0.589 and AUC 0.602 \nfor the multiple-choice explanations by GPT-3.5 and PaLM2, respec-\ntively, and AUC 0.592 for the short-answer explanations by GPT-4o). \nTherefore, default explanations lead to a discrimination gap as well.\nExplanation style and length affect human confidence\nExperiment 2 evaluates how human confidence is affected by the degree \nof uncertainty expressed in LLM explanations (across three levels of \nconfidence) as well as the overall length of the LLM explanation (across \nthree levels of length).\nFigure 4 shows that the type of uncertainty language used in the \nexplanations has a strong influence on human confidence regardless \nof the type of LLM that produced the explanation or the type of ques-\ntion. Low-confidence explanations (‘I am not sure’) produced signifi-\ncantly lower human confidence than medium-confidence explanations  \n(‘I am somewhat sure’); Bayes factor (BF) >100 across experiments \n2a, 2b and 2c. Similarly, medium-confidence explanations produced \nlower human confidence than high-confidence explanations; BF >100 \nacross experiments 2a, 2b and 2c. The Supplementary Information \n(‘human confidence agreement’) shows an analysis of the reliability \nof the confidence ratings across participants.\nIn addition, the length of the explanations also affected the human \nconfidence in the LLM answers. Long explanations led to significantly \nhigher confidence than the short explanations (BF of 25 with data com-\nbined across experiments 2a, 2b and 2c), and short explanations led \nto significantly higher confidence than the responses that contained \nonly the uncertainty expression (BF >100 with data combined across \nexperiments 2a, 2b and 2c). The additional information presented in \nlonger explanations did not enable participants to better discriminate \nbetween probably correct and incorrect answers for longer explana-\ntions. Across experiments 2a, 2b and 2c, the mean participant AUC is \n0.54 and 0.57 for long and uncertainty-only explanations, respectively \n(BF of 0.23). Therefore, the length of the answer led to an increase in \nCalibration error Discrimination\n0 0.1 0.2 0.3 0.5 0.6 0.7 0.8\nE C E AUC\nB as eline e x planat ions\nModel confidence\nHuman confidence\nModif ied ex planat ions\nGPT­3.5\nmultiple choice\nPaLM2\nmultiple choice\nGPT­4o\nshort answer\n(n = 60)\n(n = 60)\n(n = 59)\n(n = 41)\n(n = 39)\n(n = 42)\nFig. 2 | Calibration error and discrimination for model confidence and human \nconfidence across the behavioural experiments and LLMs. Calibration error is \nassessed by ECE (lower is better), while discrimination is assessed by AUC (higher \nis better). The vertical dashed lines represent the calibration and discrimination \ngap between model confidence and human confidence for unmodified \nexplanations (experiments 1a, 1b and 1c). For human confidence, the data points \nrepresent the AUC values computed separately for each participant (n shown \nin figure) and the error bars represent the 95% confidence interval of the mean \nacross participants. Because of data sparsity, the ECE values were computed at \nthe group level.\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231\n 225\nArticle https://doi.org/10.1038/s42256-024-00976-7\nhuman confidence without any corresponding change in sensitivity to \ndiscriminating between correct and incorrect answers.\nThe results confirm that people can appropriately interpret verbal \ncues about uncertainty and that manipulating the length of the expla-\nnation can directly impact human confidence.\nReducing the calibration and discrimination gap\nHaving established in experiment 2 that the uncertainty language in \nthe LLM explanation can modify human confidence, we now evaluate \nwhether linking the type of uncertainty language to the LLM model \nconfidence (that is, showing a low-, medium- and high-confidence \nexplanation when the model confidence is low, medium and high, \nrespectively) can reduce the calibration and discrimination gap.\nSelecting explanations on the basis of model confidence. We simu-\nlated the effect of aligning the explanation style to model confidence by \na simple decision rule. With this rule, we select the type of explanation \ns ∈ {low confidence, medium confidence, high confidence} on the basis \nof the LLM model confidence score p\ns =\n⎧⎪\n⎨⎪\n⎩\nlowconfidence if p ≤ θ1\nmediumconfidence if θ1 < p ≤ θ2\nhighconfidence if θ2 < p\n. (1)\nThe parameters θ1 and θ2 determine the ranges where low-, medium- and \nhigh-confidence explanations are chosen. The application of this rule \nto a given parameter setting leads to a participant’s estimates being \nfiltered out if the explanation style used for a specific question does \nnot match the selected style. This allowed us to simulate the effect of \nparticipants receiving different types of explanations based on model \nconfidence (that is, lower-confidence explanations for low model confi-\ndence and higher-confidence explanations for high model confidence). \nThe Supplementary Information (‘optimization procedure’) provides \ndetails on the optimization procedure and also a demonstration that \nthe results are not particularly sensitive to the parameter settings.\nCalibration and discrimination results. Figure 2 shows the calibration \nand discrimination results when the selection rule is applied to the \nresults from experiment 2. The results in Fig. 2 (left, red bars) show that \nthe calibration gap has narrowed substantially. While there is still gener-\nally a higher calibration error for human confidence relative to model \nconfidence, the calibration gap has decreased for all three LLMs relative \nto the baseline explanations in experiment 1. Furthermore, Fig. 2 (right) \nshows that the discrimination gap (as measured by AUC) has also been \nnarrowed relative to the baseline explanations across LLMs and ques-\ntion types (BF >100, BF 6.48 and BF >100 for experiments 2a, 2b and \n2c, respectively). Therefore, the results show that selecting the type \nof explanation on the basis of LLM model confidence improves both \n0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nE CE 0. 10 4\nE CE 0. 29 1\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nE CE 0. 15 4\n E CE 0. 22 5\nE CE 0. 264 E CE 0. 150\nGPT-3.5\nmultiple-choice\nquestions \nPaLM2\nmultiple-choice\nquestions\nHuman calibration\nModel calibration Default explanations Modifi ed  explan ations\nM odel a cc urac y M odel a cc urac y M odel a cc urac y\nGPT-4o\nshort-answer\nquestions\n0 0.5\nModel confidence Human confidence Human confidence\n1.0 0 0.5 1.0 0 0.5 1.0\nE CE 0. 165E CE 0. 141 E CE 0. 111\nFig. 3 | Calibration diagrams for model confidence and human confidence \nacross experiments 1 and 2. The top and middle rows show results for multiple-\nchoice questions with the GPT-3.5 and PaLM2 models, respectively. The bottom \nrow shows results for short-answer questions with the GPT-4o model. The \nhistograms at the bottom of each plot show the proportion of observations in \neach confidence bin (values are scaled by 30% for visual clarity). The shaded \nregions represent the 95% confidence interval of the mean computed across \nparticipants and questions.\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231 226\nArticle https://doi.org/10.1038/s42256-024-00976-7\ncalibration and discrimination performance, as human confidence in \nthe LLM becomes more closely aligned with the LLM’s actual accuracy.\nParticipants lack specialized knowledge\nFor the experiments with multiple-choice questions (1a, 1b, 2a and \n2b), participants provided their own answer after seeing the answer \nfrom the LLM. This allowed us to analyse whether participants have any \nindependent knowledge from the LLM that allowed them to improve \non LLM accuracy. In experiments 1a and 2a with GPT-3.5, participants’ \naverage answer accuracy was 51%, closely aligning with LLM’s 52% accu-\nracy rate. Similarly, for the multiple-choice experiments 1b and 2b \nwith PaLM2, average participant accuracy was 45%, similar to the 47% \naccuracy rate for the LLM. In the majority (82%) of responses across all \nmultiple-choice experiments, participants selected the response that \nagreed with the LLM’s explanation.\nWhen participants chose to alter the answer, the average accuracy \nwas 33% which is lower than the LLM’s accuracy of 39% for these particu-\nlar questions. These findings suggest limited success in participants’ \nability to accurately answer the questions independent of the LLM’s \nexplanation. This is consistent with findings from ref. 19, showing that \nMechanical Turk workers without specialized knowledge (akin to our \nparticipant pool) scored 35% accuracy on similar questions.\nWhen we applied the selection rule and the explanation type was \naligned with model confidence, human decision accuracy in experi -\nments 2a and 2b did not improve for the selected questions (even \nthough discrimination and calibration improved). This shows that accu-\nrate uncertainty communication by the LLM allowed participants to \nrecognize when the LLM was providing a probably correct or incorrect \nanswer, but the lack of accurate human knowledge independent from \nthe LLM prevented participants from improving on the LLM answer.\nAt the end of the experiment, participants estimated the perfor-\nmance they would achieve on similar questions for each of the ten topics \nin the sample of MMLU questions. The median of these self-assessed \nexpertise estimates did not substantially vary between topics: from \n30% (for example, high school physics) to 45% (for example, high \nschool world history). Examining the impact of perceived expertise \non accuracy estimation, we divided participants into two groups based \non whether their self-rated expertise was above or below 50% sepa -\nrately for each of the ten topics. For the experiments with GPT-3.5, the \nhigher-expertise groups generally had better discrimination (AUC \n0.600 versus AUC 0.579), but there was no evidence that this difference \nwas significant (BF <1). In addition, the calibration error was comparable \nbetween the two groups (ECE 0.289 versus 0.292). Similarly, no effect of \nexpertise was found for the experiments with PaLM2. Therefore, partici-\npants who considered themselves more knowledgeable about a topic \nwere not more adept at estimating the LLM’s performance in that area.\nDiscussion\nOur research focused on bridging the gap between what an LLM knows \nand what users perceive it knows. This gap is critical, especially as reli-\nance on LLMs for decision-making across various domains is rapidly \nincreasing.\nResearch on LLMs has begun to address these challenges, with \na focus on improving uncertainty communication and the quality \nof explanations. Several studies have explored LLM confidence in \nanswering multiple-choice questions, focusing on how well the models’ \nself-reported confidence aligns with their actual accuracy12,13,19,24 and \nwhether users can accurately assess the reliability of the explanations \nprovided25. The work by ref. 26 investigates how users respond to verbal \nphrases of uncertainty in a simulated trivia task but does not use actual \nLLM outputs. Overall, there has been little research examining user \nconfidence in LLM output. Our work uses actual LLM outputs and its \nconfidence in an attempt to quantify the calibration and discrimination \ngap. As a result, we directly address the issue of miscommunication of \nuncertainty from LLMs to humans.\nOur results showed that users consistently overestimated how \naccurate LLM outputs were, especially when they relied on the models’ \ndefault explanations. This was true for three different LLMs and two \ndifferent types of questions (multiple choice and short answer). This \ntendency towards overconfidence in LLM capabilities is an impor -\ntant concern, particularly in scenarios where critical decisions rely \non LLM-generated information. The inability of users to discern the \nreliability of LLM responses not only undermines the utility of these \nmodels but also poses risks in situations where user understanding of \nmodel accuracy is critical.\nIn addition, the results also showed a length bias where longer \nexplanations led to higher human confidence levels even though they \ndid not contain any additional information to help users to better \ndiscriminate between probably correct and incorrect answers. This \nsuggests that users were processing the explanations at a shallow \nlevel, relying on simple textual cues such as overall length to predict \nLLM accuracy. This result is consistent with studies in social psychol-\nogy and communication research that suggest that longer answers or \nexplanations may be perceived as more persuasive or credible, even \nwhen they do not contain more meaningful information27,28. This length \nbias has also been found in domains such as peer reviews, where longer \nExperiment 2c\nshort- answ er questions\nGPT-4o\nExperiment 2a\nmultiple- choice questions\nGPT-3.5\nExperiment 2b\nmultiple- choice questions\nPaLM2\nE x planation s t y l e\n0\n0 . 1\n0 . 2\n0 . 3\n0 . 4\n0 . 5\n0 . 6\n0 . 7\n0 . 8\n0 . 9\n1.0\nE x planation s t y l e\n0\n0 . 1\n0 . 2\n0 . 3\n0 . 4\n0 . 5\n0 . 6\n0 . 7\n0 . 8\n0 . 9\n1.0\nH u m an c onfidence\nE x pla n a tion l e ngt h\nLon g\nS hor t\nU n certainty  o n l y\nNot sure Somewhat sure SureNot sure Somewhat sure SureNot sure Somewhat sure Sure\nE x planation s t y l e\n0\n0 . 1\n0 . 2\n0 . 3\n0 . 4\n0 . 5\n0 . 6\n0 . 7\n0 . 8\n0 . 9\n1.0\nFig. 4 | Mean human confidence across LLM explanation styles varying \nin uncertainty language and length. Data are presented as mean values of \nparticipant confidence in experiments 2a (n = 60), 2b (n = 60) and 2c (n = 59). For \nreference, the dashed lines show the average human confidence for the baseline \nexplanations in experiments 1a, 1b, and 1c. The error bars represent the 95% \nconfidence interval of the mean across participants.\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231\n 227\nArticle https://doi.org/10.1038/s42256-024-00976-7\nreviews are perceived as more persuasive and informative even if the \ninformation content remains the same29.\nAlthough default LLM explanations do not enable users to perceive \nwhat the models truly know, this research shows that a simple approach \nbased on tailored explanations can bridge this perception gap. This was \nachieved by altering the prompts used to generate explanations based \non model confidence, allowing better control over how uncertainty was \nexpressed in the responses. Specifically, we designed these prompts \nto induce varying degrees of certainty in the explanations, ranging \nfrom expressions of low confidence (for example, ‘I am not sure the \nanswer is [B] because’) to high confidence (for example, ‘I am confident \nthe answer is [B] because’). By modifying the language of the LLM’s \nresponses to better reflect model confidence, users showed improved \ncalibration in their assessment of the LLM’s reliability and were bet -\nter able to discriminate between correct and incorrect answers. This \nimprovement underscores the importance of transparent communica-\ntion from LLMs, suggesting a need for researchers to investigate how \nmodel explanations affect user perception.\nOne limitation of the current study is the focus on a specific type of \nquestion involving a small number of response alternatives (multiple \nchoice) and short answers to open-ended questions. The extent to which \nthese results apply to longer open-ended questions remains an open \nquestion. Further research could investigate the applicability of our find-\nings across a broader range of scenarios. Another limitation of this study \nis that our approach to modifying the prompt on the basis of internal \nuncertainty required the LLM to be prompted twice: once to read out \nthe answer and model confidence, and again to produce an explanation \nmodified by the model confidence. Future research could investigate \nhow to produce confidence-modified explanations in a single step.\nAnother important area for future research is to understand the \nfundamental causes of the miscommunication of uncertainty. Why \ndo LLMs generate calibrated model confidences while also produc -\ning explanations that are not consistent with those confidences? One \nhypothesis is that current LLMs are aligned to human preferences \nusing reinforcement learning from human feedback (RLHF)30, which \nproduces some built-in biases. In these RLHF procedures, various \ntypes of explanations are presented to human participants, who can \nthen choose their preferred explanations. LLMs are then fine-tuned \non the basis of human preference data, making them more likely to \nproduce explanations that people prefer. While RLHF encourages \nhuman-aligned output, it inevitably reproduces any human prefer -\nence biases. For example, people prefer detailed and generally longer \nexplanations31,32. As a result, LLMs trained on these human preferences \nmay produce explanations that are overly convincing, potentially \nmisleading users about the reliability of the information.\nAn alternative hypothesis to the production of overconfident \nexplanations lies in the autoregressive nature of well-established \nLLMs. In particular, we conjecture that, after committing to an answer \n(encoded as a sequence of tokens), the model will generate a sequence \nof tokens (explanation) that maximizes the likelihood of the previous \nanswer, effectively resulting in an assertive answer. A similar hypothesis \nwas also presented in ref. 16. Interestingly, the possibility that the LLM’s \nchoice of a particular answer inflates the rationale for that answer resem-\nbles the phenomenon of choice-supportive biases in psychology 33.  \nAfter making a decision, people tend to overestimate the desirability \nof the chosen option while underestimating the desirability of rejected \nalternatives. This can make them feel more confident in their decision \nthan they were when they first made it.\nOur work shares some parallels with prior studies on the human \nperception and evaluation of artificial intelligence (AI)-generated \nexplanations in the domain of machine learning classifiers (see ref. 34 \nfor an overview). These studies frequently use feature highlighting to \nexplain what areas of the image35 or what fragments of documents36 \ncan support the suggested classification. Studies have found mixed \nevidence for the effectiveness of these types of AI explanations in human \ndecision-making37–40. These results highlight the challenge of ensuring \nthat AI-generated explanations align with human expectations and \nallow humans to distinguish between correct and incorrect answers.\nIn conclusion, our research highlights the critical role of clear and \naccurate communication in the interaction between users and LLMs. \nEnhancing the alignment between model confidence and the user’s \nperception of model confidence can lead to more responsible and \ntrustworthy use of LLMs, particularly in areas where the accuracy of \nAI-generated information is critical.\nMethods\nQuestion datasets\nMMLU dataset for multiple-choice questions.  The MMLU dataset \nis a comprehensive multitask dataset that contains multiple-choice \nquestions from various knowledge domains, such as STEM, humanities, \nsocial sciences and more19. In total, there are 14,042 test set questions \nfrom 57 categories curated by undergraduate and graduate students \nfrom freely available online resources such as the Graduate Record \nExaminations and United States Medical Licensing Examination. These \nquestions range in difficulty from high school to the professional level. \nThe MMLU dataset is widely used to measure a text model’s multitask \naccuracy, as it challenges models on their real-world text understand-\ning beyond mere linguistic comprehension, thus making it a robust \nbenchmark for model evaluation19,41,42. For this research, we sampled \na subset of 350 questions from a range of model confidence levels in \nten select categories from the full dataset to comprehensively evaluate \npeople’s assessment of LLM model confidence.\nTrivia QA dataset for short-answer questions. Trivia QA is a dataset \nof trivia questions that can be answered in short answers22. Similar to \nmethodology by ref. 17, contextual information was excluded to make \nthe question answering more challenging for LLMs and more suitable \nfor our behavioural experiments. For this research, we assessed model \nconfidence for 5,000 questions from the original 650,000-question \ndataset before selecting a final sample of 336 questions from a range of \nmodel confidence levels. The final set of questions was categorized into \nseven different topics (culture and society, entertainment, geography, \nhistory, politics, science and technology, and sports).\nAssessing model confidence and creating question subsets\nSeveral approaches have been developed to elicit confidence in LLMs \nand to assess the degree to which the elicited confidence scores are \ncalibrated (see ref. 43  for an overview). In this research, we use an \napproach commonly used to access internal model information \nbased on token likelihoods, allowing the direct computation of rela -\ntive probabilities of different possible answers in multiple-choice \nquestions12,13,18,19,23. In addition, the token-likelihood approach can be \nextended to short-answer questions such that the token-likelihood \nreflects the model confidence that the LLM answer is correct13.\nMethods that do not require access to internal model representa-\ntions have used prompting strategies designed to elicit verbal expres-\nsions of uncertainty24,44. Confidence is expressed in natural language \nas numeric strings (for example, ‘80%’)24,45 or more qualitative expres-\nsions of confidence (for example, ‘I am not confident the answer is X’).  \nPrompts can be designed to emphasize step-by-step reasoning about \nthe correctness of individual steps and clarify the space of possible \nanswers to lead to better calibration than simple prompts that sim -\nply ask for a confidence rating 24. For short-form question answer -\ning, prompting strategies can lead to calibrated confidence levels 46. \nHowever, prompting approaches have been found to be less accurate \ncompared with methods that read out model confidence\n24.\nMultiple-choice questions.  For the multiple-choice questions, we \nfollowed the procedures based on reading out the internal token likeli-\nhoods as described in the GPT-4 T echnical Report12. We used a zero-shot \nNature Machine Intelligence | Volume 7 | February 2025 | 221–231 228\nArticle https://doi.org/10.1038/s42256-024-00976-7\nprompting approach, in which the model was prompted only with the \ntarget question and its associated answer options (Extended Data \nFig. 1). We first assessed the LLM model confidence of GPT-3.5 and \nPaLM2 language models to 14,042 MMLU multiple-choice questions. \nThis allowed us to then select questions with (somewhat) evenly dis-\ntributed confidence levels. We read out the log-probabilities for the \ntop five tokens completed by the model using the APIs for the GPT3.5 \n(gpt-3.5-turbo-instruct) and the PaLM2 (text-bison@002) models. \nThe temperature parameter was set to 0. The answer was considered \ncomplete if the tokens included the single letters A, B, C and D. The log \nscores were then converted and normalized to probabilities across the \nfour answer options (so that the sum of the scores equalled one). In this \nresearch, internal uncertainties, referred to in this Article as model \nconfidence, were represented by these probabilities in all experiments, \na common technique in calibration assessment with LLMs12,13,18,19,23.\nBased on the model confidence levels of each LLM for all MMLU \nquestions, we created a subset separately for each LLM. In total, 35 ques-\ntions were sampled for each of 10 topics, for a total of 350 questions. \nFor each topic, the 35 questions were sampled to approximately create \na uniform distribution over model confidence using the confidence \nbins 0.2–0.4, 0.4–0.6, 0.6–0.8 and 0.8–1.0. However, owing to the \nsmall number of questions that lead to model confidence in the lowest \nconfidence bin, fewer questions were sampled for the 0.2–0.4 confi-\ndence range. Supplementary Fig. 1 shows the distribution over model \nconfidence levels for the entire MMLU dataset as well as the question \nsubset sampled for our study. Model accuracy across the 350 questions \nis 55% and 50% for GPT-3.5 and PaLM2, respectively.\nShort-answer questions. For the short-answer questions, we used a \nprocedure based on the pTrue method13 to assess internal model con-\nfidence. All experiments with short-answer questions were performed \nwith the API for the GPT-4o model (gpt-4o-mini) with the tempera -\nture parameter set to 0.7 (similar to refs. 13 ,17). The model was first \nprompted to generate the answer to each of the 5,000 trivia questions \nin the sample. T o ensure that the model response was restricted to \nshort answers, we used a ten-shot prompting approach where the \nprompt contained the target question preceded by a random sample \nof ten trivia question with the reference answers. The median answer \nlength was two words.\nT o assess the model confidence for short-answer questions, as \nshown in Fig. 1 (bottom), we prompted the model with the question and \nthe proposed answer and asked it to determine whether the proposed \nanswer is true or false (see Extended Data Fig. 1 for an example of the \nexact prompt). The log scores for the true and false answer options were \nthen converted and normalized to probabilities across the two answer \noptions. The model confidence in our experiments corresponds to the \nprobability for the true answer option.\nFor the behavioural experiments, we created a subset of 336 ques-\ntions to ensure a uniform distribution across four confidence bins: \n0–0.25, 0.25–0.50, 0.50–0.75 and 0.75–1.0. Supplementary Figs. 1 \nand 2 show the distribution of model confidence levels for the 5,000 \nsample and the 336 subset used in our behavioural experiments. The \nmodel accuracy across the 336 questions is 63%.\nWe used both automatic and human scoring methods to assess \nmodel accuracy. For the 5,000-question sample, we prompted an LLM \n(GPT-4o) to determine whether the reference answer from the Trivia \nQA had the same meaning as the LLM answer within the context of the \nquestion. For the 336-question sample, we also applied human scoring. \nFor 97% of questions, automatic and human scoring agreed. The model \naccuracy for the 336-question subset was based on human evaluation.\nBehavioural experiments\nThis section describes the methodology we used for our behavioural \nexperiments. Experiment 1 presented default explanations from LLMs \nto participants, whereas experiment 2 presented explanations that \nwere altered by different types of uncertainty language and overall \nlength (see Table 1 for an overview of all experiments). Within each \nexperiment, across different groups of participants, we varied the \ntype of question as well as the type of LLM. Experiments 1a and 2a used \nexplanations from GPT-3.5 for the MMLU multiple-choice questions. \nExperiments 1b and 2b used explanations from PaLM2 for the MMLU \nmultiple questions. Finally, experiments 1c and 2c used explanations \nfrom GPT-4o for the Trivia QA short-answer questions. The Supple -\nmentary Information (‘experiment 3’) describes the results from an \nadditional experiment 3, which was conducted to verify that our results \ngeneralize to different ways to vary the type of uncertainty language \nin the explanations.\nParticipants. A total of 301 participants completed the study across \nexperiments 1 and 2 (Table 1 presents the breakdown by experiment). \nThe participants were native English speakers residing in the USA, \nrecruited through Prolific ( www.prolific.com). Demographic data \nwere obtained for 284 participants. There were 146 female and 138 male \nparticipants. The median age was 34 years (age range from 18 to 79). \nSupplementary Table 1 presents the breakdown of demographic infor-\nmation by experiment. The University of California, Irvine institutional \nreview board approved the experimental protocol. The participants \nwho completed experiments 1a, 1b, 2a or 2b were paid US$8 for their \nparticipation. The participants in experiments 1c and 2c required less \ntime to complete the study and were paid US$5. The payments across \nexperiments corresponded to a rate of approximately US$12 per hour. \nBefore participating, all individuals provided informed consent and \nreceived detailed instructions outlining the experimental procedure \nas well as how to understand and interact with the user interface. Par-\nticipants were also asked to sign an integrity pledge after reading all of \nthe instructions, stating that they would complete the experiment to \nthe best of their abilities. After submitting their integrity pledge, the \nparticipants were granted access to the experiment.\nExperimental procedure. Across all experiments, participants were \nrandomly assigned 40 questions (from the pool of 350 multiple-choice \nquestions or the pool of 336 short-answer questions). The questions \nwere sampled to balance across model confidence bins ensuring that \nall participants were exposed to questions at all levels of difficulty.\nFurthermore, in experiments 2a, 2b and 2c, we balanced the types \nof explanation style across questions so that each question was pre -\nsented approximately the same number of times with each style. It \nshould be noted that, for each subject, each question was presented \nonly once, and each question received only one explanation style. \nThe counterbalancing ensured that the same question had (roughly) \nan equal number of observations for each explanation style (across \nparticipants).\nFor the multiple-choice questions, the participant’s task was \ndivided into two phases for each question (Extended Data Fig. 2). In \nthe first stage, the participants had to provide a probability estimate \nthat the LLM’s answer was correct. In the second phase, the partici -\npants had to answer the question with the assistance of the LLM. The \nparticipants were instructed to use their own knowledge as well as the \nLLM’s response when making their own answer selection for this phase. \nFor the short-answer questions, the participants only had to provide \na probability estimate that the LLM’s answer was correct. They were \ninstructed not to look up the answer. For the short-answer questions, \nthe question-answering phase was omitted to prevent participants \nfrom looking up the answers.\nAt the end of the experiments, we administered a brief survey in \nwhich the participants self-assessed their knowledge of topics associ-\nated with the multiple-choice and short-answer questions. The partici-\npants were asked to estimate the expected accuracy for each topic if \nthey were presented with questions similar to those they encountered \nduring the experiment.\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231\n 229\nArticle https://doi.org/10.1038/s42256-024-00976-7\nCreating explanation styles with varying degrees of uncertainty. This \nsection describes how we constructed prompts to elicit explanations with \nvarying levels of uncertainty language in the experiments. Table 2 con-\ntains examples of explanations from the two experiments and styles. For \nadditional details on the exact prompts used, see Supplementary Table 3.\nExperiment 1. T o generate the baseline explanations for experiment 1, \nwe used a simple prompt that asked for an explanation, which would \nalso repeat the answer.\nExperiment 2. In experiment 2, explanations were manipulated in terms \nof the level of confidence expressed in the answer as well as the length of \nthe answer. In total, the experiment included nine types of explanations \n(three levels of uncertainty × three levels of length). The three levels of \nconfidence (low, medium and high) were generated by prompts that \ninstructed the LLM to ‘mention you are not sure/somewhat sure/sure’ in \nthe explanations, respectively. The prompts elicited responses in which \nthe beginning of each explanation indicated the level of uncertainty (for \nexample, ‘I am not sure the answer is [B] because’ for the low-confidence \nprompt). Note that expressions of uncertainty were not limited to the \nstart of the explanation. Answers often contained additional explana-\ntions for why the LLM lacked confidence (for example, ‘further research \nmay be required to confirm this’ and ‘it is not possible to definitively \nstate that… ’). Experiment 2 also varied the length of the explanation \nacross three levels: long, short and uncertainty only. The long explana-\ntions were generated by not including any instruction regarding the \nlength of the answer. The short explanations were generated by adding \nan instruction to use as few words as possible in the explanation. The \nuncertainty-only explanations were generated by removing the ration-\nale for the answer and including only the expression of uncertainty and \nthe answer (for example, ‘I am not sure the answer is [B]’).\nFor experiment 2, the median lengths of the long and short expla-\nnations were as follows: 115 and 34 words (GPT-3.5, multiple choice), 64 \nand 24 words (PaLM2, multiple choice) and 95 and 24 words (GPT-4o, \nshort answer). By comparison, the uncertainty-only responses con -\ntained a median of nine words across all variants of experiment.\nMetrics\nT o investigate the relationship between the accuracy of answers to \nthe multiple-choice and short-answer questions and the confidence \n(either human confidence or model confidence) associated with them, \nwe utilize a range of metrics to evaluate this association. The primary \nfocus is on understanding how well confidence levels correlate with the \ncorrectness of answers. T o achieve this, we use both ECE and the AUC \nmetric. These metrics assess the extent of overconfidence in predic-\ntions as well as the diagnostic effectiveness of confidence scores in \ndistinguishing between correct and incorrect answers 13,18,23,24,46. The \nuse of AUC in this context parallels various metrics in psychology \nfor metacognitive discrimination or sensitivity, which similarly aim \nto evaluate the effectiveness of confidence scores in distinguishing \nbetween correct and incorrect answers 47. In addition, in the Supple -\nmentary Information (‘Overconfidence Error’), we also show results \nfor the additional metric of Overconfidence Error (OE).\nECE. We evaluate miscalibration using the ECE, as detailed in refs. 48,49. \nThe ECE is calculated by averaging the absolute differences between \naccuracy and confidence across M equal-width probability bins\nECE =\nM\n∑\nm=1\n|Bm|\nN |conf(Bm)−acc(B m)|, (2)\nwhere N represents the total sample count, B m represents the m th \nconfidence bin, and acc(B m) and conf(B m) respectively denote the \naccuracy and average confidence for samples in the mth bin. ECE does \nnot account for the direction of deviations between the accuracy and \nthe confidence per bin, respectively, so a non-zero ECE can indicate a \nmix of over- and underconfidence. Although recent work50,51 has shown \nthat ECE can underestimate the true calibration error, the potential for \nunderestimation should not be an issue given that we are interested \nin analysing differences in ECE rather than unbiased estimates of the \nerror itself.\nAUC. The AUC metric is used to assess the diagnostic ability of con -\nfidence scores in distinguishing between correct and incorrect \nanswers. Utilizing the Mann–Whitney U  statistic approach, the AUC \nrepresents the probability that a randomly chosen correct answer \nhas a higher-confidence score compared with a randomly chosen \nincorrect answer\nAUC= 1\nNpos ×Nneg\nNpos\n∑\ni=1\nNneg\n∑\nj=1\nI(Ci > C j), (3)\nwhere Npos and Nneg denote the counts of correct (positive) and incorrect \n(negative) answers, respectively. Ci and Cj represent the confidence \nscores of the ith and jth correct and incorrect answers, respectively. I \nis an indicator function, which equals 1 if Ci > Cj and 0 otherwise. This \nmethod evaluates each pair of correct and incorrect answers to deter-\nmine if the confidence score for the correct answer surpasses that of \nthe incorrect one. The AUC is then the fraction of these pairs satisfying \nthis criterion, measuring the capability of confidence scores to dif -\nferentiate between correct and incorrect responses, with AUC values \nranging from 0.5 (indicating no better than chance discrimination) to \n1 (signifying perfect discrimination).\nStatistical analysis\nT o assess statistical significance, we utilize BFs to determine the extent \nto which the observed data adjust our belief in the alternative and null \nhypotheses. Values of 3 < BF < 10 and BF >10 indicate moderate and \nstrong evidence against the null hypothesis, respectively. Similarly, \nvalues of 1/10 < BF < 1/3 and BF <1/10 indicate moderate and strong \nevidence in favour of the null hypothesis, respectively. We report BFs \nfor Bayesian t-tests using the default priors as recommended by ref. 52.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nAll behavioural data as well as data produced by the LLMs used in this \nstudy are publicly available via the Open Science Framework (OSF) at \nhttps://osf.io/y7pr6/ (ref. 53). Source data are provided with this paper.\nCode availability\nThe code used for data analysis and extracting LLM model confidence \nis available via OSF at https://osf.io/y7pr6/ (ref. 53).\nReferences\n1. Budescu, D. V., Por, H.-H., Broomell, S. B. & Smithson, M. The \ninterpretation of IPCC probabilistic statements around the world. \nNat. Clim. Change 4, 508–512 (2014).\n2. Ho, E. H., Budescu, D. V., Dhami, M. K. & Mandel, D. R. Improving \nthe communication of uncertainty in climate science and \nintelligence analysis. Behav. Sci. Policy 1, 43–55 (2015).\n3. Karelitz, T. M., Dhami, M. K., Budescu, D. V. & Wallsten, T. S. \nToward a universal translator of verbal probabilities. In Proc. \n15th International Florida Artificial Intelligence Research Society \nConference (eds Haller, M. S. & Simmons, G.) 498–502 (AAAI \nPress, 2002).\n4. Wallsten, T. S., Shlomi, Y. & Ting, H. Final Report for Research \nContract ‘Expressing Probability in Intelligence Analysis’ (2008).\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231 230\nArticle https://doi.org/10.1038/s42256-024-00976-7\n5. O’Brien, B. J. Words or numbers? The evaluation of probability \nexpressions in general practice. J. R. Coll. Gen. Pract. 39, \n98–100 (1989).\n6. Ali, S. R., Dobbs, T. D., Hutchings, H. A. & Whitaker, I. S. Using \nChatGPT to write patient clinic letters. Lancet Digit. Health 5, \n179–181 (2023).\n7. Zambrano, A. F. et al. From nCoder to ChatGPT: from automated \ncoding to refining human coding. In Proc. International \nConference on Quantitative Ethnography (eds Arastoopour Irgens, \nG. & Knight, S.) 470–485 (Springer, 2023).\n8. Whalen, J. et al. ChatGPT: challenges, opportunities, and \nimplications for teacher education. Contemp. Iss. Technol. Teach. \nEduc. 23, 1–23 (2023).\n9. Jo, A. The promise and peril of generative AI. Nature 614, 214–216 \n(2023).\n10. Huang, L. et al. A survey on hallucination in large language \nmodels: principles, taxonomy, challenges, and open questions. \nPreprint at https://arxiv.org/abs/2311.05232 (2024).\n11. Introducing ChatGPT (OpenAI, 2022).\n12. Achiam, J. et al. GPT-4 technical report. Preprint at https://arxiv.org/\nabs/2303.08774 (2023).\n13. Kadavath, S. et al. Language models (mostly) know what  \nthey know. Preprint at https://arxiv.org/abs/2207.05221  \n(2022).\n14. Srivastava, A. et al. Beyond the imitation game: quantifying and \nextrapolating the capabilities of language models. Trans. Mach. \nLearn. Res. (2023).\n15. Yin, Z. et al. Do large language models know what they don’t \nknow? In Proc. Findings of the Association for Computational \nLinguistics (eds Rogers, A. et al.) 8653–8665 (ACL, 2023).\n16. Azaria, A. & Mitchell, T. in Findings of the Association for \nComputational Linguistics: EMNLP 2023 (eds Bouamor, H. et al.) \n967–976 (ACL, 2023).\n17. Farquhar, S., Kossen, J., Kuhn, L. & Gal, Y. Detecting hallucinations \nin large language models using semantic entropy. Nature 630, \n625–630 (2024).\n18. Jiang, Z., Araki, J., Ding, H. & Neubig, G. How can we know when \nlanguage models know? On the calibration of language models \nfor question answering. Trans. Assoc. Comput. Linguist. 9, \n962–977 (2021).\n19. Hendrycks, D. et al. Measuring massive multitask language \nunderstanding. In Proc. International Conference on Learning \nRepresentations (2021).\n20. GPT-3.5 (OpenAI, 2022).\n21. Anil, R. et al. Palm 2 Technical Report (2023).\n22. Joshi, M., Choi, E., Weld, D. S. & Zettlemoyer, L. Triviaqa: a \nlarge scale distantly supervised challenge dataset for reading \ncomprehension. In Proc. 55th Annual Meeting of the Association \nfor Computational Linguistics Vol. 1 (eds Barzilay, R. & Kan, M.-Y.) \n1601–1611 (ACL, 2017).\n23. Xiao, Y. et al. in Findings of the Association for Computational \nLinguistics: EMNLP 2022 (eds Goldberg, Y. et al.) 7273–7284 \n(ACL, 2022).\n24. Xiong, M. et al. Can LLMs express their uncertainty? an empirical \nevaluation of confidence elicitation in LLMs. In The Twelfth \nInternational Conference on Learning Representations  \n(2024).\n25. Tanneru, S. H., Agarwal, C. & Lakkaraju, H. Quantifying uncertainty \nin natural language explanations of large language models. In \nInternational Conference on Artificial Intelligence and Statistics \n1072–1080 (PMLR, 2024).\n26. Zhou, K., Hwang, J., Ren, X. & Sap, M. in Relying on the Unreliable: \nThe Impact of Language Models’ Reluctance to Express  \nUncertainty 3623–3643 (Association for Computational \nLinguistics, 2024).\n27. Petty, R. E. & Cacioppo, J. T. The effects of involvement on \nresponses to argument quantity and quality: central and peripheral \nroutes to persuasion. J. Person. Soc. Psychol. 46, 69 (1984).\n28. Oppenheimer, D. M. Consequences of erudite vernacular utilized \nirrespective of necessity: problems with using long words \nneedlessly. Appl. Cogn. Psychol. 20, 139–156 (2006).\n29. Goldberg, A. et al. Peer reviews of peer reviews: a randomized \ncontrolled trial and other experiments. Preprint at https://arxiv.org/\nabs/2311.09497 (2023).\n30. Ouyang, L. et al. Training language models to follow instructions \nwith human feedback. Adv. Neural Inf. Process. Syst. 35,  \n27730–27744 (2022).\n31. Bower, A. H., Han, N., Soni, A., Eckstein, M. P. & Steyvers, M. How \nexperts and novices judge other people’s knowledgeability from \nlanguage use. Psychonom. Bull. Rev. 1–11 (2024).\n32. Saito, K., Wachi, A., Wataoka, K. & Akimoto, Y. Verbosity bias in \npreference labeling by large language models. Preprint at  \nhttps://arxiv.org/abs/2310.10076 (2023).\n33. Mather, M., Shafir, E. & Johnson, M. K. Misremembrance of \noptions past: source monitoring and choice. Psychol. Sci. 11, \n132–138 (2000).\n34. Rong, Y. et al. Towards human-centered explainable AI: a survey \nof user studies for model explanations. In Proc. IEEE Transactions \non Pattern Analysis and Machine Intelligence Vol. 46 1–20  \n(IEEE, 2023).\n35. Smith-Renner, A. et al. No explainability without accountability: an \nempirical study of explanations and feedback in interactive ML. \nIn Proc. 2020 CHI Conference on Human Factors in Computing \nSystems 1–13 (Association for Computing Machinery, 2020).\n36. Feng, S. & Boyd-Graber, J. What can AI do for me? Evaluating \nmachine learning interpretations in cooperative play. In Proc. 24th \nInternational Conference on Intelligent User Interfaces  \n(eds Fu, W.-T. & Pan, S.) 229–239 (ACL, 2019).\n37. Steyvers, M. & Kumar, A. Three challenges for AI-assisted \ndecision-making. Perspect. Psychol. Sci. 19, 722–734 (2023).\n38. Bansal, G. et al. Does the whole exceed its parts? The effect of AI \nexplanations on complementary team performance. In Proc. 2021 \nCHI Conference on Human Factors in Computing Systems  \n(eds Kitamura, Y. & Quigley, A.) 1–16 (ACL, 2021).\n39. Buçinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: \ncognitive forcing functions can reduce overreliance on AI in \nAI-assisted decision-making. Proc. ACM Hum. Comput. Interact. 5, \n1–21 (2021).\n40. Wang, X. & Yin, M. Effects of explanations in AI-assisted decision \nmaking: principles and comparisons. ACM Trans. Interact. Intell. \nSyst. 12, 1–36 (2022).\n41. Hoffmann, J. et al. Training compute-optimal large language \nmodels. Preprint at https://arxiv.org/abs/2203.15556 (2022).\n42. Rae, J. W. et al. Scaling language models: methods, analysis \n& insights from training gopher. Preprint at https://arxiv.org/\nabs/2112.11446 (2021).\n43. Geng, J. et al. A survey of language model confidence estimation \nand calibration. Preprint at https://arxiv.org/abs/2311.08298 (2023).\n44. Zhou, K., Jurafsky, D. & Hashimoto, T. Navigating the grey area: \nhow expressions of uncertainty and overconfidence affect \nlanguage models. In Proc. 2023 Conference on Empirical Methods \nin Natural Language Processing 5506–5524 (Association for \nComputational Linguistics, 2023).\n45. Lin, S., Hilton, J. & Evans, O. Teaching models to express their \nuncertainty in words. Trans. Mach. Learn. Res. (2022).\n46. Tian, K. et al. Just ask for calibration: strategies for eliciting \ncalibrated confidence scores from language models fine-tuned \nwith human feedback. In Proc. 2023 Conference on Empirical \nMethods in Natural Language Processing 5433–5442 (Association \nfor Computational Linguistics, 2023).\nNature Machine Intelligence | Volume 7 | February 2025 | 221–231\n 231\nArticle https://doi.org/10.1038/s42256-024-00976-7\n47. Fleming, S. M. & Lau, H. C. How to measure metacognition. Front. \nHum. Neurosci. 8, 443 (2014).\n48. Guo, C., Pleiss, G., Sun, Y. & Weinberger, K. Q. On calibration of \nmodern neural networks. In Proc. 34th International Conference \non Machine Learning (eds Precup, D. & Teh, Y. W.) vol. 70 of \nProceedings of Machine Learning Research 1321–1330  \n(PMLR, 2017).\n49. Naeini, M. P., Cooper, G. & Hauskrecht, M. Obtaining well \ncalibrated probabilities using Bayesian binning. In Proc. AAAI \nConference on Artificial Intelligence Vol. 29 2901–2907 \n(AAAI, 2015).\n50. Kumar, A., Liang, P. S. & Ma, T. Verified uncertainty calibration. \nAdv. Neural Inf. Process. Syst. 32, (2019).\n51. Gruber, S. & Buettner, F. Better uncertainty calibration via proper \nscores for classification and beyond. Adv. Neural Inf. Process. Syst. \n35, 8618–8632 (2022).\n52. Rouder, J. N., Morey, R. D., Speckman, P. L. & Province, J. M. \nDefault Bayes factors for ANOVA designs. J. Math. Psychol. 56, \n356–374 (2012).\n53. Steyvers, M., Tejeda, H. & Belem, C. What large language  \nmodels know and what people think they know. OSF  \nhttps://doi.org/10.17605/OSF.IO/Y7PR6 (2024).\nAcknowledgements\nThis research was supported by the National Science Foundation \nunder award number 1900644 (P.S. and M.S.)\nAuthor contributions\nM.S. and P.S. conceptualized and designed the study. M.S., A.K., C.B., \nS.K., L.W.M. and X.H. developed methodology for the behavioural \nexperiments. H.T. implemented and conducted the behavioural \nexperiments. M.S. analysed the data. M.S. and A.K. wrote the first draft. \nH.T., C.B., S.K., L.W.M. and P.S. reviewed and edited the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at https://doi.org/10.1038/\ns42256-024-00976-7.\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s42256-024-00976-7.\nCorrespondence and requests for materials should be addressed to \nMark Steyvers.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended use \nis not permitted by statutory regulation or exceeds the permitted use, you \nwill need to obtain permission directly from the copyright holder. To view \na copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00976-7\nExtended Data Fig. 1 | Example prompts for multiple choice and short-answer \nquestions. Example prompt to elicit model confidence for a multiple-choice \nquestion (top) and short-answer question (bottom). For the multiple-choice \nquestion, the prompt elicits the answer and model confidence across answer \noptions. Note that for the short-answer question, the LLM answer (dove) was \nelicited through another prompt.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00976-7\nExtended Data Fig. 2 | Illustration of the behavioral experiment interface. \nIllustration of the behavioral experiment interface used to conduct the multiple-\nchoice experiments. The left panel displays the experimental interface during \nphase 1 of the task, where participants evaluate the probability that the LLM is \ncorrect based on the explanation. The right panel displays the experimental \ninterface during phase 2 of the task, where participants answer the multiple-\nchoice question with the assistance of the LLM.\n\n\n",
  "topic": "Perception",
  "concepts": [
    {
      "name": "Perception",
      "score": 0.6009929180145264
    },
    {
      "name": "Computer science",
      "score": 0.5475724935531616
    },
    {
      "name": "Calibration",
      "score": 0.4818671643733978
    },
    {
      "name": "Self-confidence",
      "score": 0.4237607419490814
    },
    {
      "name": "Psychology",
      "score": 0.3551517724990845
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35335737466812134
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33329296112060547
    },
    {
      "name": "Social psychology",
      "score": 0.2767537236213684
    },
    {
      "name": "Statistics",
      "score": 0.14090391993522644
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}