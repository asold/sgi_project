{
  "title": "Better Language Models with Model Merging",
  "url": "https://openalex.org/W1515897616",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5076738898",
      "name": "Thorsten Brants",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2061271742",
    "https://openalex.org/W2441154163",
    "https://openalex.org/W2096498841",
    "https://openalex.org/W2046224275"
  ],
  "abstract": "This paper investigates model merging, a technique for deriving Markov models from text or speech corpora. Models are derived by starting with a large and specific model and by successively combining states to build smaller and more general models. We present methods to reduce the time complexity of the algorithm and report on experiments on deriving language models for a speech recognition task. The experiments show the advantage of model merging over the standard bigram approach. The merged model assigns a lower perplexity to the test set and uses considerably fewer states.",
  "full_text": "arXiv:cmp-lg/9604005v1  17 Apr 1996\nBetter Language Models with Model Merging\nThorsten Brants\nUniversit¨ at des Saarlandes, Computational Linguistics\nP.O.Box 151150, D-66041 Saarbr¨ ucken, Germany\nthorsten@coli.uni-sb.de\nConference on Empricial Methods in NLP\nMay 17 – 18, 1996, Philadelphia, PA.\nAbstract\nThis paper investigates model merging, a tech-\nnique for deriving Markov models from text or\nspeech corpora. Models are derived by starting\nwith a large and speciﬁc model and by succes-\nsively combining states to build smaller and more\ngeneral models. We present methods to reduce\nthe time complexity of the algorithm and report\non experiments on deriving language models for\na speech recognition task. The experiments show\nthe advantage of model merging over the standard\nbigram approach. The merged model assigns a\nlower perplexity to the test set and uses consider-\nably fewer states.\nIntroduction\nHidden Markov Models are commonly used for\nstatistical language models, e.g. in part-of-speech\ntagging and speech recognition (Rabiner, 1989).\nThe models need a large set of parameters which\nare induced from a (text-) corpus. The parameters\nshould be optimal in the sense that the resulting\nmodels assign high probabilities to seen training\ndata as well as new data that arises in an applica-\ntion.\nThere are several methods to estimate model\nparameters. The ﬁrst one is to use each word\n(type) as a state and estimate the transition prob-\nabilities between two or three words by using the\nrelative frequencies of a corpus. This method is\ncommonly used in speech recognition and known\nas word-bigram or word-trigram model. The rel-\native frequencies have to be smoothed to handle\nthe sparse data problem and to avoid zero proba-\nbilities.\nThe second method is a variation of the\nﬁrst method. Words are automatically grouped,\ne.g. by similarity of distribution in the corpus\n(Pereira et al., 1993). The relative frequencies of\npairs or triples of groups (categories, clusters) are\nused as model parameters, each group is repre-\nsented by a state in the model. The second method\nhas the advantage of drastically reducing the num-\nber of model parameters and thereby reducing the\nsparse data problem; there is more data per group\nthan per word, thus estimates are more precise.\nThe third method uses manually deﬁned cate-\ngories. They are linguistically motivated and usu-\nally called parts-of-speech. An important diﬀer-\nence to the second method with automatically de-\nrived categories is that with the manual deﬁni-\ntion a word can belong to more than one cate-\ngory. A corpus is (manually) tagged with the cat-\negories and transition probabilities between two or\nthree categories are estimated from their relative\nfrequencies. This method is commonly used for\npart-of-speech tagging (Church, 1988).\nThe fourth method is a variation of the third\nmethod and is also used for part-of-speech tagging.\nThis method does not need a pre-annotated corpus\nfor parameter estimation. Instead it uses a lexicon\nstating the possible parts-of-speech for each word,\na raw text corpus, and an initial bias for the tran-\nsition and output probabilities. The parameters\nare estimated by using the Baum-Welch algorithm\n(Baum et al., 1970). The accuracy of the derived\nmodel depends heavily on the initial bias, but with\na good choice results are comparable to those of\nmethod three (Cutting et al., 1992).\nThis paper investigates a ﬁfth method for es-\ntimating natural language models, combining the\nadvantages of the methods mentioned above. It\nis suitable for both speech recognition and part-\nof-speech tagging, has the advantage of automat-\nically deriving word categories from a corpus and\nis capable of recognizing the fact that a word be-\nlongs to more than one category. Unlike other\ntechniques it not only induces transition and out-\nput probabilities, but also the model topology, i.e.,\nthe number of states, and for each state the out-\nputs that have a non-zero probability. The method\nis called model merging and was introduced by\n(Omohundro, 1992).\nThe rest of the paper is structured as fol-\nlows. We ﬁrst give a short introduction to Markov\nmodels and present the model merging technique.\nThen, techniques for reducing the time complex-\nity are presented and we report two experiments\nusing these techniques.\nMarkov Models\nA discrete output, ﬁrst order Markov Model con-\nsists of\n• a ﬁnite set of states Q∪{ qs, q e}, qs, q e ̸∈ Q, with\nqs the start state, and qe the end state;\n• a ﬁnite output alphabet Σ;\n• a ( |Q| + 1) × (|Q| + 1) matrix, specifying the\nprobabilities of state transitions p(q′|q) between\nstates q and q′ (there are no transitions into qs,\nand no transitions originating in qe); for each\nstate q ∈ Q ∪ { qs}, the sum of the outgoing\ntransition probabilities is 1, ∑\nq′∈Q∪{qe}\np(q′|q) =\n1;\n• a |Q| × | Σ | matrix, specifying the output prob-\nabilities p(σ|q) of state q emitting output σ; for\neach state q ∈ Q , the sum of the output proba-\nbilities is 1, ∑\nσ ∈Σ\np(σ|q) = 1.\nA Markov model starts running in the start\nstate qs, makes a transition at each time step, and\nstops when reaching the end state qe. The transi-\ntion from one state to another is done according\nto the probabilities speciﬁed with the transitions.\nEach time a state is entered (except the start and\nend state) one of the outputs is chosen (again ac-\ncording to their probabilities) and emitted.\nAssigning Probabilities to Data\nFor the rest of the paper, we are interested in the\nprobabilities which are assigned to sequences of\noutputs by the Markov models. These can be cal-\nculated in the following way.\nGiven a model M, a sequence of outputs σ =\nσ1 . . . σ k and a sequence of states Q = q1 . . . q k (of\nsame length), the probability that the model run-\nning through the sequence of states and emitting\nthe given outputs is\nPM (Q, σ ) =\n( k∏\ni=1\npM (qi|qi−1)pM (σi|qi)\n)\npM (qe|qi)\n(with q0 = qs). A sequence of outputs can be emit-\nted by more than one sequence of states, thus we\nhave to sum over all sequences of states with the\ngiven length to get the probability that a model\nemits a given sequence of outputs:\nPM (σ) =\n∑\nQ\nPM (Q, σ ).\nThe probabilities are calculated very eﬃciently\nwith the Viterbi algorithm (Viterbi, 1967). Its\ntime complexity is linear to the sequence length\ndespite the exponential growth of the search space.\nPerplexity\nMarkov models assign rapidly decreasing proba-\nbilities to output sequences of increasing length.\nTo compensate for diﬀerent lengths and to make\ntheir probabilities comparable, one uses the per-\nplexity PP of an output sequence instead of its\nprobability. The perplexity is deﬁned as\nPPM (σ) = 1\nk\n√\nPM (σ)\n.\nThe probability is normalized by taking the kth\nroot ( k is the length of the sequence). Similarly,\nthe log perplexity L P is deﬁned:\nL PM (σ) = log P PM (σ) = − log PM (σ)\nk .\nHere, the log probability is normalized by dividing\nby the length of the sequence.\nPP and L P are deﬁned such that higher per-\nplexities (log perplexities, resp.) correspond to\nlower probabilities, and vice versa. These mea-\nsures are used to determine the quality of Markov\nmodels. The lower the perplexity (and log perplex-\nity) of a test sequence, the higher its probability,\nand thus the better it is predicted by the model.\nModel Merging\nModel merging is a technique for inducing model\nparameters for Markov models from a text cor-\npus. It was introduced in (Omohundro, 1992)\nand (Stolcke and Omohundro, 1994) to induce\nmodels for regular languages from a few sam-\nples, and adapted to natural language models in\n(Brants, 1995). Unlike other techniques it not\nonly induces transition and output probabilities\nfrom the corpus, but also the model topology, i.e.,\nthe number of states and for each state the out-\nputs that have non-zero probability. In n-gram\napproaches the topology is ﬁxed. E.g., in a pos-\nn-gram model, the states are mostly syntactically\nmotivated, each state represents a syntactic cate-\ngory and only words belonging to the same cate-\ngory have a non-zero output probability in a par-\nticular state. However the n-gram-models make\nthe implicit assumption that all words belonging\nto the same category have a similar distribution\nin a corpus. This is not true in most of the cases.\nBy estimating the topology, model merging\ngroups words into categories, since all words that\ncan be emitted by the same state form a cate-\ngory. The advantage of model merging in this re-\nspect is that it can recognize that a word (the\na)\n✒✑\n✓✏\nStart\n✒✑\n✓✏\n5\n✒✑\n✓✏\n3\n✒✑\n✓✏\n1\n✒✑\n✓✏\n6\n✒✑\n✓✏\n4\n✒✑\n✓✏\n2\n✒✑\n✓✏\n7 ✒✑\n✓✏\n8\n✒✑\n✓✏\nEnd✲\n0.33\n✑✑✑✑✑ ✸0.33\n◗◗◗◗◗ s0.33\n✲\n✲\n✲\n✲\n✲\n◗◗◗◗◗◗ s\n✲ ✑✑✑✑✑ ✸\na b\na c\na b a c\np(S|Ma) = 1\n27 ≃ 3.7 · 10− 2\nb)\n✒✑\n✓✏\nStart\n✒✑\n✓✏\n5\n✒✑\n✓✏\n1,3\n✒✑\n✓✏\n6\n✒✑\n✓✏\n4\n✒✑\n✓✏\n2\n✒✑\n✓✏\n7 ✒✑\n✓✏\n8\n✒✑\n✓✏\nEnd✲\n0.67◗◗◗◗◗ s0.33\n✲\n✲\n0.5\n✑✑✑✑✑ ✸0.5\n✲\n✲\n◗◗◗◗◗◗ s\n✲ ✑✑✑✑✑ ✸\nb\na c\na b a c\np(S|Mb) = 1\n27 ≃ 3.7 · 10− 2\nc)\n✒✑\n✓✏\nStart\n✒✑\n✓✏\n1,3,5\n✒✑\n✓✏\n2,6 ✒✑\n✓✏\n7 ✒✑\n✓✏\n4,8 ✒✑\n✓✏\nEnd✲ ✲0.67✟✟✟✟✟ ❍❍❍❍❍ ❥\n0.33\n✲0.5\n❍❍❍❍❍ ✟✟✟✟✟ ✯\n0.5\n✲ ✲\na b a c\np(S|Mc) = 1\n27 ≃ 3.7 · 10− 2\nd)\n✒✑\n✓✏\nStart\n✒✑\n✓✏\n1,3\n5,7 ✒✑\n✓✏\n2,6 ✒✑\n✓✏\n4,8 ✒✑\n✓✏\nEnd\n✲ ✲0.5✟✟✟✟✟ ❍❍❍❍❍ ❥\n0.5\n✛\n0.5 ❍❍❍❍❍ ✟✟✟✟✟ ✯\n0.5\n✲\na b c\np(S|Md) = 1\n64 ≃ 1.6 · 10− 2\ne)\n✒✑\n✓✏\nStart\n✒✑\n✓✏\n1,3\n5,7 ✒✑\n✓✏\n2,4\n6,8 ✒✑\n✓✏\nEnd\n✲ ✲\n✛\n0.25\n✲0.75\na ❅❅ ■\nb\n0.5\n\u0000\u0000 ✒\nc\n0.5\np(S|Me) = 27\n4096 ≃ 6.6 · 10− 3\nFigure 1: Model merging for a corpus S = {ab, ac, abac }, starting with the trivial model in a) and ending\nwith the generalization ( a(b|c))+ in e). Several steps of merging between model b) and c) are not sh own.\nUnmarked transitions and outputs have probability 1.\ntype) belongs to more than one category, while\neach occurrence (the token) is assigned a unique\ncategory. This naturally reﬂects manual syntac-\ntic categorizations, where a word can belong to\nseveral syntactic classes but each occurrence of a\nword is unambiguous.\nThe Algorithm\nModel merging induces Markov models in the fol-\nlowing way. Merging starts with an initial, very\ngeneral model. For this purpose, the maximum\nlikelihood Markov model is chosen, i.e., a model\nthat exactly matches the corpus. There is one\npath for each utterance in the corpus and each\npath is used by one utterance only. Each path\ngets the same probability 1 /u , with u the number\nof utterances in the corpus. This model is also\nreferred to as the trivial model. Figure 1.a shows\nthe trivial model for a corpus with words a, b, c and\nutterances ab, ac, abac . It has one path for each of\nthe three utterances ab, ac, and abac, and each\npath gets the same probability 1 / 3. The trivial\nmodel assigns a probability of p(S|Ma) = 1 / 27\nto the corpus. Since the model makes an im-\nplicit independence assumption between the ut-\nterances, the corpus probability is calculated by\nmultiplying the utterance’s probabilities, yielding\n1/ 3 · 1/ 3 · 1/ 3 = 1 / 27.\nNow states are merged successively, except for\nthe start and end state. Two states are selected\nand removed and a new merged state is added.\nThe transitions from and to the old states are redi-\nrected to the new state, the transition probabilities\nare adjusted to maximize the likelihood of the cor-\npus; the outputs are joined and their probabilities\nare also adjusted to maximize the likelihood. One\nstep of merging can be seen in ﬁgure 1.b. States 1\nand 3 are removed, a combined state 1,3 is added,\nand the probabilities are adjusted.\nThe criterion for selecting states to merge is\nthe probability of the Markov model generating\nthe corpus. We want this probability to stay as\nhigh as possible. Of all possible merges (gener-\nally, there are k(k − 1)/ 2 possible merges, with k\nthe number of states exclusive start and end state\nwhich are not allowed to merge) we take the merge\nthat results in the minimal change of the probabil-\nity. For the trivial model and u pairwise diﬀerent\nutterances the probability is p(S|Mtriv ) = 1 /u u.\nThe probability either stays constant, as in Figure\n1.b and c, or decreases, as in 1.d and e. The prob-\nability never increases because the trivial model is\nthe maximum likelihood model, i.e., it maximizes\nthe probability of the corpus given the model.\nModel merging stops when a predeﬁned\nthreshold for the corpus probability is reached.\nSome statistically motivated criteria for ter-\nmination using model priors are discussed in\n(Stolcke and Omohundro, 1994).\nUsing Model Merging\nThe model merging algorithm needs several opti-\nmizations to be applicable to large natural lan-\nguage corpora, otherwise the amount of time\nneeded for deriving the models is too large. Gen-\nerally, there are O(l2) hypothetical merges to be\ntested for each merging step ( l is the length of the\ntraining corpus). The probability of the training\ncorpus has to be calculated for each hypothetical\nmerge, which is O(l) with dynamic programming.\nThus, each step of merging is O(l3). If we want\nto reduce the model from size l + 2 (the trivial\nmodel, which consists of one state for each token\nplus initial and ﬁnal states) to some ﬁxed size, we\nneed O(l) steps of merging. Therefore, deriving a\nMarkov model by model merging is O(l4) in time.\n(Stolcke and Omohundro, 1994) discuss sev-\neral computational shortcuts and approximations:\n1. Immediate merging of identical initial and ﬁnal\nstates of diﬀerent utterances. These merges do\nnot change the corpus probability and thus are\nthe ﬁrst merges anyway.\n2. Usage of the Viterbi path (best path) only in-\nstead of summing up all paths to determine the\ncorpus probability.\n3. The assumption that all input samples retain\ntheir Viterbi path after merging. Making this\napproximation, it is no longer necessary to re-\nparse the whole corpus for each hypothetical\nmerge.\nWe use two additional strategies to reduce the\ntime complexity of the algorithm: a series of cas-\ncaded constraints on the merges and the variation\nof the starting point.\nConstraints\nWhen applying model merging one can observe\nthat ﬁrst mainly states with the same output are\nmerged. After several steps of merging, it is no\nlonger the same output but still mainly states that\noutput words of the same syntactic category are\nmerged. This behavior can be exploited by intro-\nducing constraints on the merging process. The\nconstraints allow only some of the otherwise pos-\nsible merges. Only the allowed merges are tested\nfor each step of merging.\nWe consider constraints that divide the states\nof the current model into equivalence classes. Only\nstates belonging to the same class are allowed to\nmerge. E.g., we can divide the states into classes\ngenerating the same outputs. If the current model\nhas N states and we divide them into k > 1\nnonempty equivalence classes C1 . . . C k, then, in-\nstead of N(N − 1)/ 2, we have to test\nk∑\ni=1\n|Ci|(|Ci| − 1)\n2 < N(N − 1)\n2\nmerges only.\nThe best case for a model of size N is the\ndivision into N/ 2 classes of size 2. Then, only N/ 2\nmerges must be tested to ﬁnd the best merge.\nThe best division into k > 1 classes for some\nmodel of size N is the creation of classes that all\nhave the same size N/k (or an approximation if\nN/k ̸∈I N). Then,\nN\nk (N\nk − 1)\n2 · k = N(N\nk − 1)\n2\nmust be tested for each step of merging.\nThus, the introduction of these constraints\ndoes not reduce the order of the time complexity,\nbut it can reduce the constant factor signiﬁcantly\n(see section about experiments).\nThe following equivalence classes can be used\nfor constraints when using untagged corpora:\n1. States that generate the same outputs (unigram\nconstraint)\n2. unigram constraint, and additionally all prede-\ncessor states must generate the same outputs\n(bigram constraint)\n3. trigrams or higher, if the corpora are large\nenough\n4. a variation of one: states that output words be-\nlonging to one ambiguity class, i.e. can be of a\ncertain number of syntactic classes.\nMerging starts with one of the constraints. Af-\nter a number of merges have been performed, the\nconstraint is discarded and a weaker one is used\ninstead.\nThe standard n-gram approaches are special\ncases of using model merging and constraints.\nE.g., if we use the unigram constraint, and merge\nstates until no further merge is possible under this\nconstraint, the resulting model is a standard bi-\ngram model, regardless of the order in which the\nmerges were performed.\nIn practice, a constraint will be discarded be-\nfore no further merge is possible (otherwise the\nmodel could have been derived directly, e.g., by\nthe standard n-gram technique). Yet, the ques-\ntion when to discard a constraint to achieve best\nresults is unsolved.\nThe Starting Point\nThe initial model of the original model merging\nprocedure is the maximum likelihood or trivial\nmodel. This model has the advantage of directly\nrepresenting the corpus. But its disadvantage is\nits huge number of states. A lot of computation\ntime can be saved by choosing an initial model\nwith fewer states.\nThe initial model must have two properties:\n1. it must be larger than the intended model, and\n2. it must be easy to construct.\nThe trivial model has both properties. A class of\nmodels that can serve as the initial model as well\nare n-gram models. These models are smaller by\none or more orders of magnitude than the trivial\nmodel and therefore could speed up the derivation\nof a model signiﬁcantly.\nThis choice of a starting point excludes a lot\nof solutions which are allowed when starting with\nthe maximum likelihood model. Therefore, start-\ning with an n-gram model yields a model that is\nat most equivalent to one that is generated when\nstarting with the trivial model, and that can be\nmuch worse. But it should be still better than\nany n-gram model that is of lower of equal order\nthan the initial model.\nExperiments\nModel Merging vs. Bigrams\nThe ﬁrst experiment compares model merging\nwith a standard bigram model. Both are trained\non the same data. We use Ntrain = 14 , 421\nwords of the Verbmobil corpus. The corpus\nconsists of transliterated dialogues on business\nappointments1. The models are tested on Ntest =\n2, 436 words of the same corpus. Training and test\nparts are disjunct.\nThe bigram model yields a Markov model with\n1,440 states. It assigns a log perplexity of 1.20 to\nthe training part and 2.40 to the test part.\nModel merging starts with the maximum like-\nlihood model for the training part. It has 14,423\nstates, which correspond to the 14,421 words (plus\nan initial and a ﬁnal state). The initial log per-\nplexity of the training part is 0.12. This low value\nshows that the initial model is very specialized in\nthe training part.\nWe start merging with the same-output (uni-\ngram) constraint to reduce computation time. Af-\nter 12,500 merges the constraint is discarded and\nfrom then on all remaining states are allowed to\n1Many thanks to the Verbmobil project for pro-\nviding these data. We use dialogues that were\nrecorded in 1993 and 94, and which are now avail-\nable from the Bavarian Archive for Speech Signals\nBAS (http:// www.phonetik.uni-muenchen.de/Bas/\nBasHomeeng.html).\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ×103 merges\n14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 ×103 states\n0.5\n1.0\n1.5\n2.0\n2.5\n− log10 p/N train\nlp\nconstraint\nchange\ndlp\nFigure 2: Log Perplexity of the training part during merging. Constr aints: same output until 12,500 / none\nafter 12,500. The thin lines show the further development if we reta in the the same-output constraint until\nno further merge is possible. The length of the training part is Ntrain = 14 , 421.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ×103 merges\n14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 ×103 states\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n− log10 p/N test\nlp\nlpbigram (1440 states)◦\nlpmin (113 states)◦\nconstraint\nchange\nFigure 3: Log Perplexity of Test Part During Merging. Constraints: Same Output until 12,500 / none after\n12,500. The thin line shows the further development if we retain the s ame-output constraint, ﬁnally yielding\na bigram model. The length of the test part is Ntest = 2 , 436.\nmerge. The constraints and the point of changing\nthe constraint are chosen for pragmatic reasons.\nWe want the constraints to be as week as possi-\nble to allow the maximal number of solutions but\nat the same time the number of merges must be\nmanageable by the system used for computation\n(a SparcServer1000 with 250MB main memory).\nAs the following experiment will show, the exact\npoints of introducing/discarding constraints is not\nimportant for the resulting model.\nThere are Ntrain(Ntrain − 1)/ 2 ∼ 108 hypo-\nthetical ﬁrst merges in the unconstraint case. This\nnumber is reduced to ∼ 7 · 105 when using the\nunigram constraint, thus by a factor of ∼ 150.\nBy using the constraint we need about a week of\ncomputation time on a SparcServer 1000 for the\nwhole merging process. Computation would not\nhave been feasible without this reduction.\nFigure 2 shows the increase in perplexity dur-\ning merging. There is no change during the ﬁrst\n1,454 merges. Here, only identical sequences of\ninitial and ﬁnal states are merged (compare ﬁgure\n1.a to c). These merges do not inﬂuence the prob-\nability assigned to the training part and thus do\nnot change the perplexity.\nThen, perplexity slowly increases. It can never\ndecrease: the maximum likelihood model assigns\nthe highest probability to the training part and\nthus the lowest perplexity.\nFigure 2 also shows the perplexity’s slope. It is\nlow until about 12,000 merges, then drastically in-\ncreases. At about this point, after 12,500 merges,\nwe discard the constraint. For this reason, the\ncurve is discontinuous at 12,500 merges. The eﬀect\nof further retaining the constraint is shown by the\nthin lines. These stop after 12,983 merges, when\nall states with the same outputs are merged (i.e.,\nwhen a bigram model is reached). Merging with-\nout a constraint continues until only three states\nremain: the initial and the ﬁnal state plus one\nproper state.\nNote that the perplexity changes very slowly\nfor the largest part, and then changes drastically\nduring the last merges. There is a constant phase\nbetween 0 and 1,454 merges. Between 1,454 and\n∼11,000 merges the log perplexity roughly linearly\nincreases with the number of merges, and it ex-\nplodes afterwards.\nWhat happens to the test part? Model merg-\ning starts with a very special model which then is\ngeneralized. Therefore, the perplexity of some ran-\ndom sample of dialogue data (what the test part is\nsupposed to be) should decrease during merging.\nThis is exactly what we ﬁnd in the experiment.\nFigure 3 shows the log perplexity of the test\npart during merging. Again, we ﬁnd the disconti-\nTable 1: Number of states and Log Perplexity for\nthe derived models and an additional, previously\ntest part, consisting of 9,784 words. (a) stan-\ndard bigram model, (b) constrained model merg-\ning (ﬁrst experiment), (c) model merging starting\nwith a bigram model(second experiment)\n(a) (b) (c)\nmodel MM start\ntype bigrams merging with bigrams\n# states 1,440 113 113\nLog PP 2.78 2.41 2.39\nnuity at the point where the constraint is changed.\nAnd again, we ﬁnd very little change in perplex-\nity during about 12,000 initial merges, and large\nchanges during the last merges.\nModel merging ﬁnds a model with 113 states,\nwhich assigns a log perplexity of 2.26 to the test\npart. Thus, in addition to ﬁnding a model with\nlower log perplexity than the bigram model (2.26\nvs. 2.40), we ﬁnd a model that at the same time\nhas less than 1/10 of the states (113 vs. 1,440).\nTo test if we found a model that predicts new\ndata better than the bigram model and to be sure\nthat we did not ﬁnd a model that is simply very\nspecialized to the test part, we use a new, previ-\nously unseen part of the Verbmobil corpus. This\npart consists of 9,784 words. The bigram model\nassigns a log perplexity of 2.78, the merged model\nwith 113 states assigns a log perplexity of 2.41 (see\ntable 1). Thus, the model found by model merging\ncan be regarded generally better than the bigram\nmodel.\nImprovements\nThe derivation of the optimal model took about\na week although the size of the training part was\nrelatively small. Standard speech applications do\nnot use 14,000 words for training as we do in this\nexperiment, but 100,000, 200,000 or more. It is\nnot possible to start with a model of 100,000 states\nand to successively merge them, at least it is not\npossible on today’s machines. Each step would\nrequire the test of ≈ 109 merges.\nIn the previous experiment, we abandoned\nthe same-output constraint after 12,500 merges to\nkeep the inﬂuence on the ﬁnal result as small as\npossible. It can not be skipped from the begin-\nning because somehow the time complexity has to\nbe reduced. But it can be further retained, until\nno further merge under this constraint is possible.\nThis yields a bigram model. The second experi-\nment uses the bigram model with 1,440 states as\nits starting point and imposes no constraints on\n10 11 12 13 14 ×103 merges\n4 3 2 1 0 ×103 states\n0.5\n1.0\n1.5\n2.0\n2.5\n− log10 p/N train\n◦\nlp\n10 11 12 13 14 ×103 merges\n4 3 2 1 0 ×103 states\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n− log10 p/N test\nlp\nlpbigram◦\nlpmin\nFigure 4: Log Perplexity of training and test parts when starting wit h a bigram model. The starting point\nis indicated with ◦, the curves of the previous experiment are shown in thin lines.\nthe merges. The results are shown in ﬁgure 4.\nWe see that the perplexity curves approach\nvery fast their counterparts from the previous ex-\nperiment. The states diﬀer from those of the pre-\nviously found model, but there is no diﬀerence in\nthe number of states and corpus perplexity in the\noptimal point. So, one could in fact, at least in the\nshown case, start with the bigram model without\nloosing anything. Finally, we calculate the per-\nplexity for the additional test part. It is 2.39,\nthus again lower than the perplexity of the bigram\nmodel (see table 1). It is even slightly lower than\nin the previous experiment, but most probably due\nto random variation.\nThe derived models are not in any case\nequivalent (with respect to perplexity), regardless\nwhether we start with the trivial model or the bi-\ngram model. We ascribe the equivalence in the\nexperiment to the particular size of the training\ncorpus. For a larger training corpus, the optimal\nmodel should be closer in size to the bigram model,\nor even larger than a bigram model. In such a case\nstarting with bigrams does not lead to an optimal\nmodel, and a trigram model must be used.\nConclusion\nWe investigated model merging, a technique to in-\nduce Markov models from corpora. The original\nprocedure is improved by introducing constraints\nand a diﬀerent initial model. The procedures are\nshown to be applicable to a transliterated speech\ncorpus. The derived models assign lower perplex-\nities to test data than the standard bigram model\nderived from the same training corpus. Addition-\nally, the merged model was much smaller than the\nbigram model.\nThe experiments revealed a feature of model\nmerging that allows for improvement of the\nmethod’s time complexity. There is a large ini-\ntial part of merges that do not change the model’s\nperplexity w.r.t. the test part, and that do not in-\nﬂuence the ﬁnal optimal model. The time needed\nto derive a model is drastically reduced by abbrevi-\nating these initial merges. Instead of starting with\nthe trivial model, one can start with a smaller,\neasy-to-produce model, but one has to ensure that\nits size is still larger than the optimal model.\nAcknowledgements\nI would like to thank Christer Samuelsson for\nvery useful comments on this paper. This work\nwas supported by the Graduiertenkolleg Kogni-\ntionswissenschaft, Saarbr¨ ucken.\nReferences\n[Bahl et al., 1983] Lalit R. Bahl, Frederick Je-\nlinek, and Robert L. Mercer. 1983. A maximum\nlikelihood approach to continuous speech recog-\nnition. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 5(2):179–190.\n[Baum et al., 1970] Leonard E. Baum, Ted Petrie,\nGeorge Soules, and Norman Weiss. 1970. A\nmaximization technique occuring in the statisti-\ncal analysis of probabilistic functions in markov\nchains. The Annals of Methematical Statistics,\n41:164–171.\n[Brants, 1995] Thorsten Brants. 1995. Estimat-\ning HMM topologies. In Tbilisi Symposium\non Language, Logic, and Computation, Human\nCommunication Research Centre, Edinburgh,\nHCRC/RP-72.\n[Church, 1988] Kenneth Ward Church. 1988.\nA stochastic parts program and noun phrase\nparser for unrestricted text. In Proc. Second\nConference on Applied Natural Language Pro-\ncessing, pages 136–143, Austin, Texas, USA.\n[Cutting et al., 1992] Doug Cutting, Julian Ku-\npiec, Jan Pedersen, and Penelope Sibun. 1992.\nA practical part-of-speech tagger. In Proceed-\nings of the 3rd Conference on Applied Natural\nLanguage Processing (ACL), pages 133–140.\n[Jelinek, 1990] F. Jelinek. 1990. Self-organized\nlanguage modeling for speech recognition. In\nA. Waibel and K.-F. Lee, editors, Readings in\nSpeech Recognition, pages 450–506. Kaufmann,\nSan Mateo, CA.\n[Omohundro, 1992] S. M. Omohundro. 1992.\nBest-ﬁrst model merging for dynamic learning\nand recognition. In J. E. Moody, S. J. Han-\nson, and R. P. Lippmann, editors, Advances in\nNeural Information Processing Systems 4, pages\n958–965. Kaufmann, San Mateo, CA.\n[Pereira et al., 1993] Fernando Pereira, Naftali\nTishby, and Lillian Lee. 1993. Distributional\nclustering of english words. In Proceedings of\nthe 31st ACL, Columbus, Ohio.\n[Rabiner, 1989] L. R. Rabiner. 1989. A tutorial\non hidden markov models and selected applica-\ntions in speech recognition. In Proceedings of\nthe IEEE, volume 77(2), pages 257–285.\n[Stolcke and Omohundro, 1994] Andreas Stolcke\nand Stephen M. Omohundro. 1994. Best-ﬁrst\nmodel merging for hidden markov model in-\nduction. Technical Report TR-94-003, Inter-\nnational Computer Science Institute, Berkeley,\nCalifornia, USA.\n[Viterbi, 1967] A. Viterbi. 1967. Error bounds\nfor convolutional codes and an asymptotically\noptimum decoding algorithm. In IEEE Trans-\nactions on Information Theory, pages 260–269.",
  "topic": "Bigram",
  "concepts": [
    {
      "name": "Bigram",
      "score": 0.9533941745758057
    },
    {
      "name": "Perplexity",
      "score": 0.9462490081787109
    },
    {
      "name": "Computer science",
      "score": 0.8541109561920166
    },
    {
      "name": "Language model",
      "score": 0.8504878282546997
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5976078510284424
    },
    {
      "name": "Task (project management)",
      "score": 0.5584530234336853
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5580332279205322
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5510578155517578
    },
    {
      "name": "Markov model",
      "score": 0.5134001970291138
    },
    {
      "name": "Speech recognition",
      "score": 0.43428510427474976
    },
    {
      "name": "Natural language processing",
      "score": 0.39136800169944763
    },
    {
      "name": "Machine learning",
      "score": 0.3538323640823364
    },
    {
      "name": "Markov chain",
      "score": 0.2682959735393524
    },
    {
      "name": "Trigram",
      "score": 0.16627469658851624
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}