{
  "title": "Gapformer: Graph Transformer with Graph Pooling for Node Classification",
  "url": "https://openalex.org/W4385767854",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2095878594",
      "name": "Chuang LIU",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2130251626",
      "name": "Yibing Zhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2729669276",
      "name": "Xueqi Ma",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2072270871",
      "name": "Liang, Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2310734762",
      "name": "Dapeng Tao",
      "affiliations": [
        "Convergence",
        "Communication, Information, Médias",
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2099029284",
      "name": "Jia Wu",
      "affiliations": [
        "Macquarie University"
      ]
    },
    {
      "id": "https://openalex.org/A2119135000",
      "name": "Wenbin Hu",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6863994431",
    "https://openalex.org/W6779733120",
    "https://openalex.org/W3034065136",
    "https://openalex.org/W3128443161",
    "https://openalex.org/W4323066703",
    "https://openalex.org/W4285600445",
    "https://openalex.org/W3034492151",
    "https://openalex.org/W4221163422",
    "https://openalex.org/W4306175892",
    "https://openalex.org/W3090016979",
    "https://openalex.org/W4287074646",
    "https://openalex.org/W3020880807",
    "https://openalex.org/W2173027866",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W3205705101",
    "https://openalex.org/W2918342466",
    "https://openalex.org/W6757634740",
    "https://openalex.org/W2980633415",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4287027946",
    "https://openalex.org/W6789042779",
    "https://openalex.org/W6790825729",
    "https://openalex.org/W4284896159",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3169575312",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W2244807774",
    "https://openalex.org/W4327652436",
    "https://openalex.org/W3093499132",
    "https://openalex.org/W2995509042",
    "https://openalex.org/W4281706128",
    "https://openalex.org/W2942681667",
    "https://openalex.org/W3084428871",
    "https://openalex.org/W2173183968",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W3213068071",
    "https://openalex.org/W4380993339",
    "https://openalex.org/W2811124557",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W2947537487",
    "https://openalex.org/W4304699884",
    "https://openalex.org/W6755811877",
    "https://openalex.org/W3093814892",
    "https://openalex.org/W3188906027",
    "https://openalex.org/W4285602050",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2966750432",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W4288363255",
    "https://openalex.org/W4287757758",
    "https://openalex.org/W4286902310",
    "https://openalex.org/W3190020173",
    "https://openalex.org/W4283746919",
    "https://openalex.org/W2996268457",
    "https://openalex.org/W4226356708",
    "https://openalex.org/W2963716836",
    "https://openalex.org/W4286795917",
    "https://openalex.org/W4382203217",
    "https://openalex.org/W2945591540",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4287754915",
    "https://openalex.org/W2951659295",
    "https://openalex.org/W3187966659",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W3005644236",
    "https://openalex.org/W3209214366",
    "https://openalex.org/W4226208698",
    "https://openalex.org/W4295728955",
    "https://openalex.org/W2939208918",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2964926209",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4385764404",
    "https://openalex.org/W4288419263",
    "https://openalex.org/W3035010690",
    "https://openalex.org/W3122063025",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4221166193",
    "https://openalex.org/W3034998639",
    "https://openalex.org/W3095883070"
  ],
  "abstract": "Graph Transformers (GTs) have proved their advantage in graph-level tasks. However, existing GTs still perform unsatisfactorily on the node classification task due to 1) the overwhelming unrelated information obtained from a vast number of irrelevant distant nodes and 2) the quadratic complexity regarding the number of nodes via the fully connected attention mechanism. In this paper, we present Gapformer, a method for node classification that deeply incorporates Graph Transformer with Graph Pooling. More specifically, Gapformer coarsens the large-scale nodes of a graph into a smaller number of pooling nodes via local or global graph pooling methods, and then computes the attention solely with the pooling nodes rather than all other nodes. In such a manner, the negative influence of the overwhelming unrelated nodes is mitigated while maintaining the long-range information, and the quadratic complexity is reduced to linear complexity with respect to the fixed number of pooling nodes. Extensive experiments on 13 node classification datasets, including homophilic and heterophilic graph datasets, demonstrate the competitive performance of Gapformer over existing Graph Neural Networks and GTs.",
  "full_text": "Gapformer: Graph Transformer with Graph Pooling for Node Classification\nChuang Liu1∗ , Yibing Zhan2 , Xueqi Ma3 , Liang Ding2 ,\nDapeng Tao4,5 , Jia Wu6 and Wenbin Hu1†\n1School of Computer Science, Wuhan University, Wuhan, China\n2JD Explore Academy, JD.com, China\n3School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia\n4School of Computer Science, Yunnan University, Kunming, China\n5Yunnan Key Laboratory of Media Convergence, Kunming, China\n6School of Computing, Macquarie University, Sydney, Australia\n{chuangliu, hwb}@whu.edu.cn, zhanyibing@jd.com, xueqim@student.unimelb.edu.au,\nliangding.liam@gmail.com, dptao@ynu.edu.cn, jia.wu@mq.edu.au\nAbstract\nGraph Transformers (GTs) have proved their ad-\nvantage in graph-level tasks. However, existing\nGTs still perform unsatisfactorily on the node clas-\nsification task due to 1) the overwhelming unre-\nlated information obtained from a vast number of\nirrelevant distant nodes and 2) the quadratic com-\nplexity regarding the number of nodes via the fully\nconnected attention mechanism. In this paper, we\npresent Gapformer, a method for node classifica-\ntion that deeply incorporates Graph Transformer\nwith Graph Pooling. More specifically, Gapformer\ncoarsens the large-scale nodes of a graph into a\nsmaller number of pooling nodes via local or global\ngraph pooling methods, and then computes the at-\ntention solely with the pooling nodes rather than all\nother nodes. In such a manner, the negative influ-\nence of the overwhelming unrelated nodes is mit-\nigated while maintaining the long-range informa-\ntion, and the quadratic complexity is reduced to lin-\near complexity with respect to the fixed number of\npooling nodes. Extensive experiments on 13 node\nclassification datasets, including homophilic and\nheterophilic graph datasets, demonstrate the com-\npetitive performance of Gapformer over existing\nGraph Neural Networks and GTs.\n1 Introduction\nGraph Neural Networks (GNNs), which are based on local\nmessage-passing [Kipf and Welling, 2017 ], have achieved\nnotable success in a variety of applications, including biol-\nogy [Xu et al., 2019b] and recommendation [Zhang et al.,\n2019]. In contrast to GNNs, Graph Transformers (GTs) al-\nlow each node in a graph to directly attend to all other nodes,\nwhich enables the aggregation of information from arbitrary\n∗This work was done when Chuang Liu worked as an intern at\nJD Explore Academy.\n†Corresponding Author\n7\n98\n6 5\n11\n10\n4\n3\n2\n12\n1\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1-layer2-layer3-layer\nVanilla Graph Transformer\nComplexity: \nRF: \n            GAT\nComplexity: \nRF:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n               Gapformer \nComplexity: \nRF:  \nInput Graph\n1-layer2-layer3-layer 1-layer2-layer3-layer\nFigure 1: Complexity and receptive fields (RF) in Vanilla Graph\nTransformer (GT), GAT, and our Gapformer. Here, n denotes the\nnumber of nodes in the original graph, |E| denotes the number of\nedges, and n′ denotes the number of pooling nodes, which is con-\nstant and significantly smaller than n. r(l) denotes the number of\nthe l-hop neighboring nodes, where l is the number of layers. The\nvanilla GT has the maximum receptive field, which comes with a\nquadratic complexity. In comparison, our Gapformer is computa-\ntionally efficient, meanwhile maintaining a large receptive field size.\nnodes. Along with normalization and residual connection,\nGTs overcome the deficiencies of GNNs in dealing with over-\nsmoothing [Rong et al., 2020b], over-squashing [Alon and\nYahav, 2021], heterophily [Zhu et al., 2020], and long-range\ndependencies [Zhang et al., 2022].\nHowever, existing GTs [Dwivedi and Bresson, 2021;\nKreuzer et al., 2021; Ying et al., 2021] are exploited pri-\nmarily for graph-level tasks (e.g., graph classification) with\na small number of nodes in a graph. Developing GTs for\nnode classification, where the number of nodes in a graph is\nrelatively large (up to around one million), remains a chal-\nlenging proposition for the following two reasons. First, the\nquadratic computational complexity O(n2) of self-attention\nin vanilla GTs, in regards to the number of nodes, inhibits\ntheir application to node classification in real-world scenar-\nios. Second, vanilla GTs calculate the full connected atten-\ntion and aggregate messages from arbitrary nodes, including\nnumerous irrelevant nodes; this results in ambiguous atten-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2196\ntion weights and the aggregation of noise information from\nincorrectly correlated nodes.\nOnly a few existing works have attempted to consider GTs\nfor node classification. GT-sparse [Dwivedi and Bresson,\n2021] and SAN [Kreuzer et al., 2021] confine the receptive\nfield of each node to its 1-hop neighboring nodes; as a result,\nexpressiveness is sacrificed when important interactions are\nmultiple hops away, especially in the large-scale graph corre-\nspondingly requiring a large receptive field. LiteGT [Chen\net al., 2021a ], DGT [Park et al., 2022 ], and DET [Guo\net al., 2022 ] propose selecting important nodes for atten-\ntion using certain specific, fixed node sampling strategies,\nwhich may result in the selection of uninformative nodes.\nGraphGPS [Rampasek et al., 2022] and TokenGT[Kim et al.,\n2022] directly adopt efficient approximations [Choromanski\net al., 2021; Beltagy et al., 2020] in Transformers [Vaswani\net al., 2017] to improve the efficiency of vanilla GTs; never-\ntheless, they neglect unique characteristics of graph data and\ntend to yield dense attention, causing an enormous amount of\nnoise messages to be aggregated from irrelevant nodes.\nIn light of the above analysis, we propose Gapformer,\nwhich combines Graph Transformer with Graph Pooling, to\ncapture long-range dependencies and improve the efficiency\nof vanilla GTs. In vanilla GTs, self-attention converts nodes\ninto queries and keys/values, after which each query attends\nto all the keys. Specifically, self-attention involves computing\nthe inner product between the query and key vectors to gen-\nerate attention scores. These scores are then used to perform\na weighted aggregation of value vectors. To reduce the com-\nplexity of the dense inner product, Gapformer first utilizes\ngraph pooling to group key and value nodes into a smaller\nnumber of pooling nodes. For graph pooling, we propose two\ntypes of strategies to compress the original graph efficiently\nand effectively: 1) global graph pooling, which condenses\nthe entire graph into several significant pooling nodes; 2)\nlocal graph pooling, which compresses the k-hop neighbor-\ning nodes of each query node into the corresponding pooling\nnodes. Subsequently, each query node interacts with pooling\nkeys (fewer in number) and generates representation with the\npooling values. In conclusion, our Gapformer transforms the\nfully connected attention in vanilla GTs into a sparse atten-\ntion schema by decreasing the number of attended tokens via\ngraph pooling.\nAs shown in Figure 1, Gapformer has the following ad-\nvantages. 1) Gapformer enables a larger attention field per\nnode and thus allows to compute multi-hop correlations via\ngraph pooling between each node and its corresponding dis-\nconnected nodes. 2) Since the number of pooling nodes is\nsignificantly smaller than that of nodes in the original graph,\nthe computational complexity of Gapformer only increases\nlinearly with the number of nodes in a graph, hence making\nGapformer suitable for processing large-scale datasets in the\nnode classification task.\nOur main contributions are summarized as follows:\n1. We propose Gapformer, a deeper combination of Trans-\nformer and Graph Neural Networks. Specifically, Gap-\nformer utilizes Graph Pooling to group the attended nodes\nof each node into pooling nodes (fewer in number) and\ncomputes its attention using only the pooling nodes. This\ndesign mitigates the overwhelming unrelated informa-\ntion and quadratic complexity issues associated with GTs\nwhile preserving long-range interactions.\n2. We conduct extensive experiments to compare Gapformer\nwith 20 GNN and GT baseline models in the node clas-\nsification task on 13 real-world graph datasets, including\nhomophilic and heterphilic datasets. Experimental results\nconsistently validate the effectiveness and efficiency of our\nproposed Gapformer.\n2 Related Work\nGraph Pooling. Graph Neural Networks (GNNs)[Kipf and\nWelling, 2017; Hamilton et al., 2017] are networks that per-\nform on graph domain. As an essential component of GNNs,\nGraph Pooling condenses the input graph with node rep-\nresentations into a smaller graph or a holistic graph-level\nrepresentation. There are two main types of designs pro-\nposed for graph pooling: flat and hierarchical. Flat pool-\ning directly generates a graph-level representation in one\nstep, mostly by taking the average or sum over all node\nembeddings as the graph representation [Duvenaud et al.,\n2015]. On the other hand, hierarchical pooling gradually\ncoarsens a graph into a smaller one using either node clus-\ntering pooling [Ying et al., 2018; Bianchi et al., 2020] or\nnode drop pooling [Gao and Ji, 2019; Lee et al., 2019;\nLiu et al., 2023]. Node clustering requires significant com-\nputational resources to cluster nodes into clusters, while node\ndrop selects a subset of nodes from the original graph to con-\nstruct a coarsened version that is more efficient and suitable\nfor large-scale graphs [Lee et al., 2019]. For further details,\nplease refer to [Liu et al., 2022b].\nGraph Transformers. In recent years, many Transformer\nvariants have been successfully applied to graph model-\ning, displaying competitive or even superior performance on\nmany tasks when compared to GNNs. Dwivedi et al.[2021]\nwere the first to extend the transformer architecture to graphs\nand propose position encoding [Ding et al., 2020] for nodes\nin a graph. Subsequently, Kreuzer et al.[2021] enabled the\nposition encoding by making it learnable, and further divided\nthe fully connected edges into true edges and virtual edges.\nThere are many other existing GTs [Rong et al., 2020a;\nChen et al., 2021b; Wu et al., 2021; Hussain et al., 2022;\nChen et al., 2022; Nguyen et al., 2022] and the applications\nof GTs [Xu et al., 2019a; Zhu et al., 2021; Zhu et al., 2022;\nCai et al., 2022], for a more detailed introduction, please refer\nto the recent reviews of GTs [Rampasek et al., 2022; Min et\nal., 2022]. However, the above methods are mostly designed\nfor graph-level tasks due to the time and memory constraints\nimposed by the self-attention layer, which requires O(n2)\ncomplexity. Therefore, several works [Zhao et al., 2021;\nChoromanski et al., 2022; Guoet al., 2022; Parket al., 2022;\nWu et al., 2022] have been proposed to make graph trans-\nformers more scalable and efficient, but they still suffer from\nsome issues such as long-range information loss or noise ag-\ngregation.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2197\nVanila Attention\nQuery\nKey\nPooling-enhanced \nAttention\nQuery\nPooling\nKey\n(b)\nMulti-Head \nAttention\nAdd & Norm\nFFN\nAdd & Norm\nQ K V\nLinear\nVanilla GT\nMulti-Head \nAttention\nAdd & Norm\nFFN\nAdd & Norm\nQ\nK V\nLinear\nPooling\nK’ V’\nGapformer\nH H\n(a)\nFigure 2: Comparison of Vanilla Graph Transformer (GT) and our Gapformer. (a) One core of GT models is the self-attention layer, which\ncomputes the pairwise inner product between the input node tokens Q and K. (b) Before calculating self-attention, Gapformer utilizes the\ngraph pooling operation to coarsen the key (K) and value (V) vectors into the pooling key (K\n′\n) and value (V\n′\n) vectors, respectively, which\ndecreases the number of attended tokens (n → n′).\n3 Methodology\n3.1 Preliminaries\nNotations. A graph G can be represented by an adjacency\nmatrix A ∈ {0,1}n×n and a node feature matrix X ∈ Rn×d,\nwhere n is the number of nodes, d is the dimension of the\nnode features, and A[i, j] = 1if there exits an edge between\nnode vi and node vj, otherwise, A[i, j] = 0.\nGraph Pooling. Let a graph pooling operator be defined as\nany function Pooling that maps a graph G = (V, E) to a new\npooled graph G′ = (V′, E′) :\nG′ = Pooling(G), (1)\nwhere |V′| < |V|. The main objective of graph pooling is to\ndecrease the number of nodes in a graph while maintaining\nits semantic information.\nTransformer. The vanilla Transformer [Vaswani et al.,\n2017] consists of two essential parts: a multi-head self-\nattention (MHA) module and a position-wise feed-forward\nnetwork (FFN). To build a deeper model, a residual connec-\ntion [He et al., 2016] is employed to each module, followed\nby a layer normalization (LN) [Ba et al., 2016]. The self-\nattention mechanism calculates attention scores by taking the\ninner product of query vectors (Q) and key vectors (K). It\nthen uses these scores to aggregate value vectors (V ) in a\nweighted manner, resulting in contextualized representations,\nthat is,\nQ = HWQ, K = HWK, V = HWV ; (2)\nH′ = softmax\n\u0012QK⊤\n√\nd′\n\u0013\nV, (3)\nwhere WQ ∈ Rd×d′\n, WK ∈ Rd×d′\n, and WV ∈ Rd×d′\nare\nprojection matrices, H =\nh\nh⊤\n1 , . . . ,h⊤\nn\ni\n∈ Rn×d denotes\nthe input matrix of node embeddings, H′ ∈ Rn×d′\nis the out-\nput matrix, and d′ is the output hidden dimension. Note that,\nEquation (3) denotes the single-head self-attention module,\nwhich can straightforwardly generalize to MHA.\n3.2 Proposed Method: Gapformer\nIn this section, we present the model architecture of Gap-\nformer. First, we introduce the base architecture of Gap-\nformer and its core module; that is, the attention enhanced\nwith graph pooling (AGP). We then provide a comprehensive\ndescription of AGP from both global and local perspectives.\nArchitecture\nAs shown in Figure 2 (a), self-attention in vanilla GT calcu-\nlates the dot product between each pair of nodes after projec-\ntion (QK⊤). Therefore, the computation of full self-attention\ncomes with potential noises from long-distance neighbors and\nO(n2) complexity, limiting its capacity to analyze large-scale\ngraphs. Graph pooling manages to reduce the number of\ngraph nodes while maintaining semantic information. This\nencourages our employment of graph pooling to overcome\nthe deficiencies of GTs. To our knowledge, no efforts have\nbeen made to integrate GTs and graph pooling.\nAttention Enhanced with Graph Pooling. In light of the\nabove analysis, Gapformer uses a sparse attention schema\nbased on graph pooling to replace the full self-attention mech-\nanism. Specifically, as shown in Figure 2 (b), Gapformer first\nproduces query, key, and value matrices (Linear Module); that\nis,\neQ = HW\neQ, eK = HW\neK, eV = HW\neV . (4)\nWe define the query vector of nodevi as eqi, while its corre-\nsponding key and value matrices in the vanilla self-attention\nare eK ∈ Rn×d′\nand eV ∈ Rn×d′\n, respectively. Subsequently,\nwe apply graph pooling to compress eK and eV, which is de-\nfined as follows:\nKS(i) = Pooling\n\u0010\n˜K\n\u0011\n; (5)\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2198\nPooling\nPooling\nExtract k-hop \nSubgraphs\nA\nC\nB\nA\nC\nB\nPooling\nPooling\nLocal Graph \nPooling\nGlobal Graph Pooling\nPooling \nSet\nFigure 3: Two types of graph pooling methods to enhance attention\nin Gapformer. Here, qa is the query vector of nodeva, and N (a, k)\nrefers to the neighbor set within k hops of node va. Left: each node\nin the original graph attends with all nodes in the pooling set (S ).\nRight: each node attends with nodes in its own pooling set, which is\ngenerated from its k-hop neighbors.\nVS(i) = Pooling\n\u0010\n˜V\n\u0011\n, (6)\nwhere KS(i) ∈ Rn′×d′\nand VS(i) ∈ Rn′×d′\nare the com-\npressed key and value matrices of node vi respectively, and\nthe size of pooled node sets S (i) for node vi is n′, signif-\nicantly smaller than n. Pooling(·) refers to graph pooling,\nwhich is discussed in detail in the next section. The attention\nenhanced with graph pooling is then calculated as:\nhi = softmax\n\u0000\nα˜qT\ni KS(i)\n\u0001\nV\nT\nS(i), (7)\nwhere α is a constant scalar (α = 1√\nd′ ).\nFollowing [Guo et al., 2022; Zhao et al., 2021], we also\nmaintain the message-passing with the neighboring nodes.\nThe process is defined as follows:\nzi = softmax\n\u0010\nα˜qT\ni ˜KN(i)\n\u0011\n˜VT\nN(i), (8)\nwhere ˜KN(i) and ˜VN(i) are the key and value matrices of\nneighboring nodes, respectively. Therefore, the node vi’s fi-\nnal output of single attention module (AGP) in Gapformer is\ncalculated as follows:\nh′\ni = hi + β ∗ zi, (9)\nwhere β is a balanced hyper-parameter which controls the\ncombination for the attention enhanced with graph pooling\nand the neighboring attention.\nOther Modules. In addition to AGP, Gapformer also con-\ntains layer normalization (LN(·)) applied after the multi-\nhead self-attention (MHA(·)) and the feed-forward blocks\n(FFN(·)), as illustrated in Figure 2. We formalize the Gap-\nformer layer as below:\nh′(l) = LN\n\u0010\nMHA\n\u0010\nh(l−1)\n\u0011\u0011\n+ h(l−1); (10)\nh(l) = LN\n\u0010\nFNN\n\u0010\nh′(l)\n\u0011\u0011\n+ h′(l). (11)\nAs with most GT methods, our Gapformer also adopts\nthe positional encodings (PEs), i.e., Laplacian eigenvec-\ntors encodings (LapPE) [Dwivedi and Bresson, 2021;\nKreuzer et al., 2021] and random-walk positional encodings\n(RWPE) [Dwivedi et al., 2022].\n100 500 1000 1500 2000 2500\n# Nodes\n0.0\n1.0\n2.0Memory (Mib)\n 160MB\n1e4\nVanilla GTs\nGapformer\n100 500 1000 1500 2000 2500\n# Nodes\n0\n20\n40Time (sec)\n0.40s\nVanilla GTs\nGapformer\nFigure 4: Running time and GPU memory of the full self-attention\nin vanilla GTs and sparse self-attention in our proposed Gapformer.\nWe evaluate the performance of Gapformer on the synthetic datasets.\nThe time and memory usages of Gapformer scale linearly with the\nnumber of nodes, unlike the full self-attention mechanism in vanilla\nGTs whose values scale exponentially.\nTwo Types of Attention Enhanced with Graph Pooling\nIn this section, we discuss how to implement the attention\nenhanced with graph pooling from the global and local views.\nAttention Enhanced with Global Graph Pooling (AGP-\nG). AGP-G reduces the number of attended nodes by com-\npressing the original nodes into new pooling nodes in smaller\nsizes. Intuitively, all information in a graph is compressed\ninto the new pooling nodes. As shown in Figure 3, given\nnode features H ∈ Rn×d with their adjacency information\nA ∈ {0,1}n×n, we first construct new keys and values using\ngraph pooling operations, as follows:\nK\nGlobal\nS = Pooling\n\u0010\nHW\neK, A\n\u0011\n; (12)\nV\nGlobal\nS = Pooling\n\u0010\nHW\neV , A\n\u0011\n. (13)\nThen, as shown in the left part of Figure 3, each node (Q)\nin the original graph attends to the pooling nodes (\nK\nGlobal\nS )\nin the new setsS to generate attention scores. For the pooling\noperation (Pooling(·)), we empirically explore several dif-\nferent pooling methods [Liu et al., 2022b] to perform com-\npressions, including flat pooling methods (e.g., the mean and\nmax pooling), and trainable pooling mechanisms (e.g., Set-\nPool [Vinyals et al., 2016] and SAGPool [Lee et al., 2019]).\nAttention Enhanced with Local Graph Pooling (AGP-L).\nDifferent from AGP-G, AGP-L works by compressing the\nneighbor information of each node. Specifically, as shown\nin the right part of Figure 3, for each node (i.e., each query\nQi), we execute graph pooling on its neighboring nodes ex-\ntracted from its k-hop subgraphs. Formally, the new keys and\nvalues for each node (i.e., node vi) are generated by\nK\nLocal\nS(i) = Pooling\n\u0010\n˜KN(i,k)\n\u0011\n; (14)\nV\nLocal\nS(i) = Pooling\n\u0010\n˜VN(i,k)\n\u0011\n, (15)\nwhere N (i, k) refers to the neighbor set within k hops of\nnode vi. The nodes in a graph then perform attention with\nthose in the corresponding pooled sets. Formally, the output\nof the AGP-L for node vi is calculated by Eq. (7). Note that\nthe operation of extractingk-hop subgraphs can be performed\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2199\nCora Citeseer Pubmed DBLP\nCS Physics Photo CoraFull ogbn-arxiv Cornell Texas\nWisconsin Actor\n# Nodes 2,708 3,327 19,717 17,716\n18,333 34,493 7,650 19,793 169,343 183 183 251 7,600\n#\nEdges 5,429 4,732 44,338 105,734\n81,894 247,962 119,081 126,842 1,166,343 280 195 466 26,752\nHomo. 0.83 0.72 0.79 0.70\n0.83 0.91 0.85 0.57 0.63 0.30 0.11 0.21 0.22\nTable 1: Statistics of benchmark datasets.\nin the preprocessing stage without consuming additional re-\nsources in the training stage. Finally, as with the global pool-\ning, we empirically explore several different pooling meth-\nods for use in performing compression (Pooling( ·)), includ-\ning the flat pooling methods (e.g., the mean and max pooling)\nand trainable pooling mechanisms (e.g., SetPool [Vinyals et\nal., 2016] and SAGPool [Lee et al., 2019]).\n3.3 Merits of Gapformer\nReducing Computational Complexity. We first analyze\nthe complexity of Gapformer. The computational complex-\nity of the attention enhanced with graph pooling (Eq. (7) )\nis O (n′n). Since n′ is a constant and usually much smaller\nthan n, the computational complexity can be simplified as\nO(n). Moreover, the computational complexity of the neigh-\nboring attention (Eq. (8) ) is O (|E|). Therefore, the overall\ncomplexity of Gapformer is O(n + |E|). To illustrate the su-\nperiority of Gapformer, we conduct experiments on synthetic\ndatasets. The results in Figure 4 demonstrate that the time\nand memory usages of Gapformer do indeed scale linearly\nwith the number of nodes, unlike the full self-attention mech-\nanism that scales exponentially, which enables the application\nof Gapformer to extremely large-scale datasets.\nReducing the Ratio of Noisy Connections. In most exist-\ning graph transformer models, each node aggregates infor-\nmation from all nodes in a graph, which provides the global\nreceptive field. This approach may pose a challenge for node\nclassification tasks since the aggregated information could\npotentially contain noise and irrelevant data that is not use-\nful for the target node. Consequently, this can hinder the\nmodel’s ability to perform effectively. To address this is-\nsue, our proposed Gapformer modifies the standard full self-\nattention to a sparse schema, which helps greatly reduce the\nratio of noisy connections, thereby enhancing the capacity of\ngraph transformer-based models in node classification.\nHandling Long-range Dependency. As shown in Figure 1,\nthe receptive field of Gapformer is flexible and ranges from\nlinear growth to exponential growth. Thus, it requires fewer\nlayers than traditional GNN models to capture long-distance\nconnections. This is a remarkable benefit for the case when\nsignificant correlations are multiple hops away.\n4 Experiments\n4.1 Experimental Settings\nDatasets. We employ a total of 13 real-world datasets,\nincluding nine homophilic graph datasets (Cora, Citeseer,\nPubmed, DBLP, CoraFull, CS, Physics, Photo, and ogbn-\narxiv) and four heterophilic graph datasets (Cornell, Texas,\nWisconsin, and Actor), involving diverse domains (cita-\ntion, co-authorship, co-purchase, and web pages) and sizes\n(ogbn-arxiv is a large-scale dataset). The dataset statis-\ntics are summarized in Table 1. Please note that in ref-\nerence to [Zhu et al., 2020], Homo. refers to the ratio of\nedges linking nodes with identical labels. We use differ-\nent training, validation, and test splits for various datasets.\nSpecifically, for Cora, Citeseer, and Pubmed datasets we\nfollow the (48%/32%/20%) split as proposed in [Pei et\nal., 2020 ]. The same splits used by [Zhu et al., 2020 ]\nand [Liu et al., 2022a] are adopted for the four heterophilic\ngraph datasets. For all other datasets, we randomly split\nthem into 60%/20%/20% training/validation/test sets follow-\ning [Zhang et al., 2022]. All the adopted graph datasets, ex-\ncept ogbn-arxiv, can be downloaded from PyTorch Geomet-\nric (PyG) [Fey and Lenssen, 2019 ] 1, and obgn-arxiv can be\ndownloaded from Open Graph Benchmark (OGB) 2.\nBaseline. To demonstrate the effectiveness of our pro-\nposed method, we compare Gapformer with the follow-\ning 20 baselines: (I) 7 standard GCN-based models:\nGCN [2017], GatedGCN [2016], APPNP [2019], GC-\nNII [2020], GAT [2018], GATv2 [2022], and Super-\nGAT [2021]; (II) 5 heterophilic-graph-oriented mod-\nels: MLP [2015], MixHop [2019], FAGCN [2021],\nH2GCN [2020], and GPRGNN [2021]; (III) 8 transformer-\nbased models for graphs: GT-sparse [Dwivedi and Bres-\nson, 2021], SAN [2021], Graphormer [2021], UniMP [2021],\nLiteGT [2021a], DET [2022], NAGformer [2023], and ANS-\nGT [2022]. The last five transformer-based models are effi-\ncient graph transformer models.\nImplementation Details. We assess the effectiveness of\nour proposed model by measuring its accuracy in node clas-\nsification. To ensure reliability, we conduct 10 trials for each\nmodel using random seeds. We utilize Adam optimizer for\nGCN-based and heterophily-based methods, while Adamw\nis adopted for all graph transformer-based models. Each\nmethod and dataset are run for 200 epochs, with the test ac-\ncuracy reported based on the epoch that achieves the highest\nvalidation accuracy. For ease of tuning work, we set some\nhyperparameters: dropout at 0.5, weight decay at 5e−4, posi-\ntion encoding dimension at 20, and hidden dimension within\n{64, 128, 256}. Our implementation of Gapformer is devel-\noped using Python (3.7.0), Pytorch (1.11.0), and Pytorch Ge-\nometric (2.2.0). All experiments are conducted on a Linux\nserver with two NVIDIA A100s.\n4.2 Overall Performance\nWe evaluate the effectiveness of the proposed model in terms\nof accuracy. For each model and dataset, we conduct 10 tri-\n1https://github.com/pyg-team/pytorch geometric\n2https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2200\nCora Citeseer Pubmed DBLP\nPhoto Physics CS CoraFull ogbn-arxiv\nGCN-based methods\nGCN [Kipf and Welling,\n2017] 86.92±1.33 76.13±1.51 87.01±0.62 85.13±0.44 85.94±1.18 95.38±0.20 89.11±0.70 24.49±0.47 70.40±0.10\nGatedGCN [Li et al., 2016] 85.49±1.32 74.94±1.68 86.19±0.46 85.50±0.57 57.84±14.6 95.89±0.21 89.94±2.24 49.59±7.57 62.71±1.76\nAPPNP [Gasteiger et al., 2019] 87.75±1.30 76.53±1.61 86.52±0.61 85.22±0.56 84.71±1.25 95.04±0.31 87.49±0.48 20.61±0.78 70.20±0.16\nGCNII [Chen et al., 2020] 86.08±2.18 74.75±1.76 85.98±0.61 83.26±0.49 67.06±1.74 94.88±0.32 84.23±0.78 9.10±0.62 69.78±0.16\nGAT [Veliˇcko\nvi´c et al., 2018] 87.34±1.14 75.75±1.86 85.37±0.56 83.86±0.44 87.13±1.00 95.14±0.28 88.53±0.54 25.32±1.43 67.56±0.12\nGATv2 [Brody et al.,\n2022] 87.25±0.89 75.72±1.30 85.75±0.55 84.96±0.47 81.52±3.23 95.02±0.32 88.46±0.61 31.62±0.71 68.84±0.13\nSuperGAT [Kim and Oh,\n2021] 87.22±1.24 75.41±1.78 85.30±0.52 83.64±0.40 85.83±1.29 95.11±0.26 88.11±0.43 23.52±0.85 66.99±0.07\nHeterophily-based methods\nMLP [LeCun et al., 2015] 70.32±2.68 68.64±1.98 86.46±0.35 72.54±0.95 88.66±0.85 95.12±0.26 92.99±0.51 53.63±0.96 52.63±0.12\nMixHop [Sami et al., 2019] 84.47±1.37 72.04±1.49 88.44±0.47 82.23±0.65 93.24±0.59 96.34±0.22 93.88±0.63 56.66±1.19 70.83±0.30\nH2GCN [Zhu et al., 2020] 83.48±2.29 75.16±1.48 88.86±0.45 83.10±0.27 91.56±0.70 96.28±0.13 94.02±0.31 50.38±0.82 68.29±0.67\nFAGCN [Bo et al.\n, 2021] 85.17±1.08 75.60±2.37 87.71±0.44 83.80±0.47 87.53±0.75 95.86±0.12 91.82±0.54 30.14±0.45 66.12±0.02\nGPRGNN [Chien et al., 2021] 86.82±1.15 75.45±1.40 86.83±0.48 84.97±0.64 92.27±0.44 96.06±0.21 93.60±0.36 64.11±0.80 68.28±0.21\nGraph Tr\nansformer-based methods\nSAN [Kreuzer et al., 2021] 81.91±3.42 69.63±3.76 81.79±0.98 – 94.17±0.65 96.83±0.18 94.16±0.36 45.61±5.25 69.17±0.15\nGraphormer [Ying et al.,\n2021] 67.71±0.78 73.30±1.21 OOM OOM 85.20±\n4.12 OOM OOM OOM OOM\nLiteGT [Chen et al., 2021a] 80.62±2.69 69.09±2.03 85.45±0.69 – – OOM 92.\n16±0.44 56.86±0.69 OOM\nUniMP [Shi et al., 2021] 84.18±1.39 75.00±1.59 88.56±0.32 84.25±0.42 92.49±0.47 96.82±0.13 94.20±0.34 67.93±0.56 73.19±0.18\nDET [Guo et al., 2022] 86.30±1.41 75.37±1.41 86.28±0.44 84.96±0.39 91.44±0.49 96.30±0.18 93.34±0.31 67.12±0.93 55.70±0.30\nNAGphormer [Chen et al.\n, 2023] 85.77±1.35 73.69±1.48 87.87±0.33 – 94.64±0.60 96.66±0.16 95.00±0.14 66.75±0.79 –\nANS-GT [Zhang et al., 2022] 86.71±1.45 74.57±1.51 89.76±0.46 85.19±0.47 94.41±0.62 96.22±0.15 94.64±0.24 61.66±1.85 –\nGapformer (w/o GP) 81.69±2.03 70.90±3.05 87.35±0.51 83.54±0.48 94.06±0.81 96.68±0.14 93.62±0.72 54.95±1.37 70.20±0.21\nGapformer (AGP-G) 87.37±0.76 76.21±1.47 88.98±0.46 85.50±0.43 94.81±0.45 97.10±0.12 95.13±0.40 68.22±0.70 71.90±0.19\nGapformer (AGP-L) 87.04±1.14 75.24±1.44 88.49±0.44 85.31±0.49 92.34±0.63 96.42±0.20 94.48±0.36 67.59±0.66 71.70±0.33\nNotations: 1) Gapformer (w/o GP) refers to Gapformer without graph pooling, which is also the GT-sparse baseline model [Dwivedi and\nBresson, 2021]. 2) The results of Graphormer are taken from NAGphormer [2023] and Specformer [2023]. 3) The full-version of GT [2021]\nand SAN [2021] is OOM even on the small-scale datasets. 4) Another recent graph Transformer, SAT [Chen et al., 2022], is not considered,\nas it reports OOM even on the small-scale datasets.\nTable 2: Experimental results for the node classification task on eight common datasets (mean accuracy (%) and standard deviation over 10\ndifferent runs). Red: the best performance per dataset. Blue: the second best performance per dataset. OOM denotes out-of-memory.\nals with random seeds, and then take the mean accuracy and\nstandard deviation, which are reported in Tables 2 and 3.\nPerformance on Homophilic Graphs. From the results in\nTable 2, we can observe: 1) Gapformer achieves the state-\nof-the-art performance on five datasets and competitive per-\nformance on three datasets, which demonstrates the effec-\ntiveness of our proposed method. Gapformer has a signif-\nicant advantage over its variant that does not include the\ngraph pooling module, referred to as Gapformer (w/o GP).\n2) Compared with GCN-based methods, Gapformer performs\nbetter on graphs with more nodes (e.g., Photo, Physics,\nand CS). This is likely because local message-passing based\nGCN methods neglect long-range dependencies, but our Gap-\nformer enables to learn more informative node representa-\ntions from the multi-hop neighborhoods, which is a remark-\nable benefit for the bigger graphs, where the required re-\nceptive field is large [Guo et al., 2022; Park et al., 2022;\nWu et al., 2022]. 3) The performance of Gapformer surpasses\nthat of graph transformer-based methods on the small-scale\ndatasets (e.g., Cora and Citeseer). The reason may be that\nvanilla GTs with full connected attention (e.g., Graphormer)\nand sampling-based GTs (e.g., LiteGT [Chen et al., 2021a]\nand DET [Guo et al., 2022] ) both introduce more noises from\nmassive unrelated nodes. However, GCN-based models per-\nform better than GT-based methods on small-scale datasets.\nThis is likely because, on small-scale datasets, local infor-\nmation is more important. Moreover, GTs, including our\nGapformer, have more parameters than GCN-based meth-\nods, meaning that they may suffer from over-fitting on small\ndatasets. 4) Our Gapformer can be applied to large-scale\ngraphs, such as ogbn-arxiv, while some other transformer-\nbased methods cannot be applied to such graphs due to their\npoor scalability. We have noticed that Graphormer [Ying et\nal., 2021 ] and LiteGT [Chen et al., 2021a] encounter out-\nof-memory errors, even when processing small graphs. This\nhighlights the need for a graph Transformer that can scale ef-\nfectively to handle large-scale graphs.\nPerformance on Heterophilic Graphs. Table 3 summa-\nrizes the results of models on heterophilic graphs. From these\nresults, we can make the following observations: 1) GCN-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2201\nCornell T exas\nWisconsin Actor\nGCN-based methods\nGCN [Kipf and Welling,\n2017] 45.67±7.96 60.81±8.03 52.55±4.27 28.73±1.17\nGatedGCN [Li et al., 2016] 72.70±5.33 75.40±4.26 81.37±3.31 35.13±1.10\nAPPNP [Gasteiger et al., 2019] 41.35±7.15 61.62±5.37 55.29±3.90 29.42±0.81\nGCNII [Chen et al., 2020] 44.32±5.81 58.91±4.32 52.54±7.32 25.40±0.97\nGAT [Veliˇcko\nvi´c et al., 2018] 47.02±7.66 62.16±4.52 57.45±3.51 28.33±1.13\nGATv2 [Brody et al.,\n2022] 50.27±8.97 60.54±4.55 52.74±3.96 28.79±1.47\nSuperGAT [Kim and Oh,\n2021] 43.51±6.55 59.99±4.64 53.52±4.64 28.08±1.03\nHeterophily-based methods\nMLP [LeCun et al., 2015] 71.62±5.57 77.83±5.24 82.15±6.93 33.26±0.91\nMixHop [Sami et al., 2019] 76.48±2.97 83.24±4.48 85.48±3.06 34.92±0.91\nH2GCN [Zhu et al., 2020] 75.40±4.09 79.73±3.25 77.57±4.11 36.18±0.45\nFAGCN [Bo et al.\n, 2021] 67.56±5.26 75.67±4.68 75.29±3.06 32.13±1.33\nGPRGNN [Chien et al., 2021] 76.76±2.16 81.08±4.35 82.66±5.62 35.30±0.80\nGraph Tr\nansformer-based methods\nSAN [Kreuzer et al., 2021] 50.85±8.54 60.17±6.66 51.37±3.08 27.12±2.59\nUniMP [Shi et al., 2021] 66.48±12.5 73.51±8.44 79.60±5.41 35.15±0.84\nDET [Guo et al., 2022] 41.35±7.45 56.76±4.98 54.90±6.56 28.94±0.64\nNAGphormer [Chen et al.\n, 2023] 56.22±8.08 63.51±6.53 62.55±6.22 34.33±0.94\nGapformer (w/o GP) 61.89±5.85 70.54±4.75 75.29±5.12 33.86±0.79\nGapformer (AGP-G) 77.57±3.43 80.27±4.01 83.53±3.42 36.90±0.82\nGapformer (AGP-L) 76.22±2.65 79.73±5.16 82.15±2.22 36.47±1.02\nNotations: 1) Gapformer (w/o GP) refers to Gapformer without\ngraph pooling, which is also the GT-sparse baseline model[Dwivedi\nand Bresson, 2021 ]. 2) The official codes of LiteGT [Chen et\nal., 2021a] and ANS-GT [2022] fail to handle the above four het-\nerophilic datasets.\nTable 3: Experimental results for the node classification task on\nfour heterophilic datasets (mean accuracy (%) and standard devia-\ntion over 10 different runs). Red: the best performance per dataset.\nBlue: the second best performance per dataset.\nbased models exhibit relatively inferior performance on het-\nerophilic graphs. This is because most GCNs utilize directly\nconnected nodes for aggregation even in heterophilic graphs.\n2) Surprisingly, transformer-based models show poor perfor-\nmance, which implies that GTs fail to filter out irrelevant\nmessages. 3) Instead, our proposed Gapformer achieves su-\nperior performance on heterophilic graph datasets. In partic-\nular, Gapformer significantly outperforms transformer-based\nbaselines. This phenomenon is probably because Gapformer\nsummarizes the graph structure or neighbor information from\na global view instead of a similarity view.\n4.3 Further Discussions\nEfficiency of Gapformer. To validate the efficiency of\nGapformer, we compare its training cost in terms of running\ntime (s) and GPU memory (MB) with such representative\nmethods as GT [Dwivedi and Bresson, 2021], SAN [Kreuzer\net al., 2021], and ANS-GT [Zhang et al., 2022]. The re-\nsults are summarized in Table 4. From these results, we can\nobserve that Gapformer with AGP-G shows high efficiency\ncompared with all existing GTs, especially when dealing with\nlarge-scale graphs. Moreover, Gapformer with AGP-L incurs\nlower memory costs compared to vanilla GTs, although its\ntime cost is high, comparable to NAGphormer [Chen et al.,\n2023] and ANS-GT [Zhang et al., 2022].\nCora Photo\nMemory\n(MB)\nTraining\nTime (s)\nMemory\n(MB)\nT\nraining\nTime (s)\nGAT [Veliˇcko\nvi´c et al., 2018] 1,672 2.64 2,189 17.27\nGT-Full [Dwivedi and Bresson, 2021] 13,375 48.80 OOM OOM\nSAN-Sparse [Kreuzer et al., 2021] 2,936 16.64 4,878 82.77\nSAN-Full [Kreuzer et al., 2021] 13,410 372.94 OOM OOM\nLiteGT [Chen et al., 2021a] 4,414 23.96 OOM OOM\nUniMP [Shi et al., 2021] 1,861 4.85 2,437 20.88\nDET [Guo et al., 2022] 1,961 11.93 4,827 222.73\nNAGphormer [Chen et al., 2023] 1,879 12.02 1923 1,936.22\nANS-GT [Zhang et al., 2022] 1,909 805.89 1,883 19,709.21\nGapformer (w/o GP) 1,827 3.81\n2,725 7.89\nGapformer (AGP-G) 1,829 5.56 2,727 9.40\nGapformer (AGP-L) 1,843 40.52 6,983 3,038.38\nTable 4: Comparison of training time and GPU memory costs of\nGapformer to graph transformer-based models. GT-Full and SAN-\nFull denote the dense self-attention versions of GT and SAN, re-\nspectively. OOM denotes out-of-memory.\n70\n74Acc (%)\nCiteseer\n86\n88\nPubmed\n68\n78\nTexas\nBase w/o FFN w/o Residual w/o LN\nFigure 5: Ablation: Components of Graph Transformer architecture.\n75\n76Acc (%)\nCiteseer\n88.6\n88.7\nPubmed\n78\n80\nTexas\nMeanPool MaxPool SetPool SAGPool\nFigure 6: Performance of Gapformer with different pooling types.\nAblation on Transformer Components and Pooling Type.\nWe next study the effects of the three components of the trans-\nformer and different pooling types. We conduct experiments\nwith AGP-G on three graph datasets. Please note that, apart\nfrom the selected components, all other parts remain identical\nto the complete model. We can observe in Figure 5 that the\nperformance drops after the LayerNorm and Residual com-\nponents are removed. However, the performance increases\nafter removing the FFN module, which indicates that this\nmodule may cause over-fitting. Moreover, from the results in\nFigure 6, we can determine that SAGPool [Lee et al., 2019]\nperforms better than other simple pooling methods. This en-\ncourages our search for more effective and efficient pooling\nmethods to improve the performance of Gapformer.\nImpact of Number of Layers and Balance Parameter.\nWe analyzed the effects of l and β using AGP-G on three\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2202\n2 3 4 5 6 7 8 9 10\n# Layers\n0.6\n0.8Accuracy\n0.01 0.1 1.0 10.0\n0.6\n0.8\nCiteseer Pubmed Texas\nFigure 7: Performance of Gapformer with different parameters.\ngraph datasets (Citeseer, Pubmed, and Texas) and presented\nthe results in Figure 7. Specifically, we investigated how the\nnumber of layers impacts node classification performance.\nOur findings indicate that as l increases from low to high val-\nues, test accuracy decreases due to over-fitting. Additionally,\nwhen β is relatively small, our model’s accuracy curve re-\nmains smooth indicating less sensitivity to hyper-parameters.\n5 Conclusion\nWe propose Gapformer, which combines Graph Transform-\ners (GTs) with Graph Pooling for efficient node classification.\nOur Gapformer addresses the two main issues of existing\nGTs: potential noises from long-distance neighbors and the\nquadratic computational complexity in regards to the number\nof nodes. Extensive experiments on 13 graph datasets demon-\nstrate that Gapformer outperforms existing GTs and Graph\nNeural Networks. Despite its competitive performance, Gap-\nformer still has room for improvement. For instance, 1) de-\nvising an effective manner to combine the proposed local\npooling enhanced attention and global pooling enhanced at-\ntention, and 2) incorporating useful techniques to further en-\nhance the performance on large-scale graph datasets.\nAcknowledgments\nThis work was supported in part by the Natural Sci-\nence Foundation of China (Nos. 61976162, 82174230,\n62002090, 62172354), Artificial Intelligence Innovation\nProject of Wuhan Science and Technology Bureau (No.\n2022010702040070), and Science and Technology Major\nProject of Hubei Province (Next Generation AI Technologies)\n(No. 2019AEA170). Dr Wu is partially supported by ARC\nProjects LP210301259 and DP230100899.\nReferences\n[Alon and Yahav, 2021] Uri Alon and Eran Yahav. On the\nbottleneck of graph neural networks and its practical im-\nplications. In ICLR, 2021.\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Ge-\noffrey E Hinton. Layer normalization. arXiv:1607.06450,\n2016.\n[Beltagy et al., 2020] Iz Beltagy, Matthew E. Peters, and Ar-\nman Cohan. Longformer: The long-document trans-\nformer. arXiv:2004.05150, 2020.\n[Bianchi et al., 2020] Filippo Maria Bianchi, Daniele Grat-\ntarola, and Cesare Alippi. Spectral clustering with graph\nneural networks for graph pooling. In ICML, 2020.\n[Bo et al., 2021] Deyu Bo, Xiao Wang, Chuan Shi, and\nHuawei Shen. Beyond low-frequency information in graph\nconvolutional networks. In AAAI, 2021.\n[Bo et al., 2023] Deyu Bo, Chuan Shi, Lele Wang, and Ren-\njie Liao. Specformer: Spectral graph neural networks meet\ntransformers. In ICLR, 2023.\n[Brody et al., 2022] Shaked Brody, Uri Alon, and Eran Ya-\nhav. How attentive are graph attention networks? InICLR,\n2022.\n[Cai et al., 2022] Weishan Cai, Wenjun Ma, Jieyu Zhan, and\nYuncheng Jiang. Entity alignment with reliable path\nreasoning and relation-aware heterogeneous graph trans-\nformer. In IJCAI, 2022.\n[Chen et al., 2020] Ming Chen, Zhewei Wei, Zengfeng\nHuang, Bolin Ding, and Yaliang Li. Simple and deep\ngraph convolutional networks. In ICML, 2020.\n[Chen et al., 2021a] Cong Chen, Chaofan Tao, and Ngai\nWong. Litegt: Efficient and lightweight graph transform-\ners. In CIKM, 2021.\n[Chen et al., 2021b] Jianwen Chen, Shuangjia Zheng, Ying\nSong, Jiahua Rao, and Yuedong Yang. Learning attributed\ngraph representation with communicative message passing\ntransformer. In IJCAI, 2021.\n[Chen et al., 2022] Dexiong Chen, Leslie O’Bray, and\nKarsten Borgwardt. Structure-aware transformer for graph\nrepresentation learning. In ICML, 2022.\n[Chen et al., 2023] Jinsong Chen, Kaiyuan Gao, Gaichao Li,\nand Kun He. NAGphormer: A tokenized graph trans-\nformer for node classification in large graphs. In ICLR,\n2023.\n[Chien et al., 2021] Eli Chien, Jianhao Peng, Pan Li, and Ol-\ngica Milenkovic. Adaptive universal generalized pagerank\ngraph neural network. In ICLR, 2021.\n[Choromanski et al., 2021] Krzysztof Marcin Choromanski,\nValerii Likhosherstov, David Dohan, Xingyou Song, An-\ndreea Gane, Tamas Sarlos, Peter Hawkins, et al. Rethink-\ning attention with performers. In ICLR, 2021.\n[Choromanski et al., 2022] Krzysztof Choromanski, Han\nLin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish, Va-\nlerii Likhosherstov, Jack Parker-Holder, Tamas Sarlos,\nAdrian Weller, and Thomas Weingarten. From block-\ntoeplitz matrices to differential equations on graphs: to-\nwards a general theory for scalable masked transformers.\nIn ICML, 2022.\n[Ding et al., 2020] Liang Ding, Longyue Wang, and\nDacheng Tao. Self-attention with cross-lingual position\nrepresentation. In ACL, 2020.\n[Duvenaud et al., 2015] David Duvenaud, Dougal Maclau-\nrin, Jorge Aguilera-Iparraguirre, Rafael G ´omez-\nBombarelli, Timothy Hirzel, Al ´an Aspuru-Guzik,\nand Ryan P. Adams. Convolutional networks on graphs\nfor learning molecular fingerprints. In NeurIPS, 2015.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2203\n[Dwivedi and Bresson, 2021] Vijay Prakash Dwivedi and\nXavier Bresson. A generalization of transformer networks\nto graphs. AAAI Workshop, 2021.\n[Dwivedi et al., 2022] Vijay Prakash Dwivedi, Anh Tuan\nLuu, Thomas Laurent, Yoshua Bengio, and Xavier Bres-\nson. Graph neural networks with learnable structural and\npositional representations. In ICLR, 2022.\n[Fey and Lenssen, 2019] Matthias Fey and Jan E. Lenssen.\nFast graph representation learning with PyTorch Geomet-\nric. In ICLR Workshop, 2019.\n[Gao and Ji, 2019] Hongyang Gao and Shuiwang Ji. Graph\nu-nets. In ICML, 2019.\n[Gasteiger et al., 2019] Johannes Gasteiger, Aleksandar Bo-\njchevski, and Stephan G ¨unnemann. Combining neural\nnetworks with personalized pagerank for classification on\ngraphs. In ICLR, 2019.\n[Guo et al., 2022] Lingbing Guo, Qiang Zhang, and Huajun\nChen. Unleashing the power of transformer for graphs.\narXiv:2202.10581, 2022.\n[Hamilton et al., 2017] Will Hamilton, Zhitao Ying, and Jure\nLeskovec. Inductive representation learning on large\ngraphs. In NeurIPS, 2017.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\n[Hussain et al., 2022] Md Shamim Hussain, Mohammed J.\nZaki, and Dharmashankar Subramanian. Global self-\nattention as a replacement for graph convolution. In\nSIGKDD, 2022.\n[Kim and Oh, 2021] Dongkwan Kim and Alice Oh. How to\nfind your friendly neighborhood: Graph attention design\nwith self-supervision. In ICLR, 2021.\n[Kim et al., 2022] Jinwoo Kim, Dat Tien Nguyen, Seonwoo\nMin, Sungjun Cho, Moontae Lee, Honglak Lee, and Se-\nunghoon Hong. Pure transformers are powerful graph\nlearners. In NeurIPS, 2022.\n[Kipf and Welling, 2017] Thomas N. Kipf and Max Welling.\nSemi-supervised classification with graph convolutional\nnetworks. In ICLR, 2017.\n[Kreuzer et al., 2021] Devin Kreuzer, Dominique Beaini,\nWilliam L. Hamilton, Vincent L´etourneau, and Prudencio\nTossou. Rethinking graph transformers with spectral at-\ntention. In NeurIPS, 2021.\n[LeCun et al., 2015] Yann LeCun, Yoshua Bengio, and Ge-\noffrey Hinton. Deep learning. Nature, pages 436–444,\n2015.\n[Lee et al., 2019] Junhyun Lee, Inyeop Lee, and Jaewoo\nKang. Self-attention graph pooling. In ICML, 2019.\n[Li et al., 2016] Yujia Li, Richard Zemel, Marc\nBrockschmidt, and Daniel Tarlow. Gated graph se-\nquence neural networks. In ICLR, 2016.\n[Liu et al., 2022a] Chuang Liu, Xueqi Ma, Yinbing Zhan,\nLiang Ding, Dapeng Tao, Bo Du, Wenbin Hu, and Danilo\nMandic. Comprehensive graph gradual pruning for sparse\ntraining in graph neural networks. arXiv:2207.08629,\n2022.\n[Liu et al., 2022b] Chuang Liu, Yibing Zhan, Chang Li,\nBo Du, Jia Wu, Wenbin Hu, Tongliang Liu, and Dacheng\nTao. Graph pooling for graph neural networks: Progress,\nchallenges, and opportunities. arXiv:2204.07321, 2022.\n[Liu et al., 2023] Chuang Liu, Yibing Zhan, Xueqi Ma,\nDapeng Tao, Bo Du, and Wenbin Hu. Masked graph\nauto-encoder constrained graph pooling. In ECML PKDD,\n2023.\n[Min et al., 2022] Erxue Min, Runfa Chen, Yatao Bian,\nTingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin\nZhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong.\nTransformer for graphs: An overview from architecture\nperspective. arXiv:2202.08455, 2022.\n[Nguyen et al., 2022] Dai Quoc Nguyen, Tu Dinh Nguyen,\nand Dinh Phung. Universal graph transformer self-\nattention networks. In WWW, 2022.\n[Park et al., 2022] Jinyoung Park, Seongjun Yun, Hyeonjin\nPark, Jaewoo Kang, Jisu Jeong, Kyung-Min Kim, Jung-\nwoo Ha, and Hyunwoo J Kim. Deformable graph trans-\nformer. arXiv:2206.14337, 2022.\n[Pei et al., 2020] Hongbin Pei, Bingzhe Wei, Kevin Chen-\nChuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo-\nmetric graph convolutional networks. In ICLR, 2020.\n[Rampasek et al., 2022] Ladislav Rampasek, Mikhail\nGalkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy\nWolf, and Dominique Beaini. Recipe for a general,\npowerful, scalable graph transformer. In NeurIPS, 2022.\n[Rong et al., 2020a] Yu Rong, Yatao Bian, Tingyang Xu,\nWeiyang Xie, Ying WEI, Wenbing Huang, and Junzhou\nHuang. Self-supervised graph transformer on large-scale\nmolecular data. In NeurIPS, 2020.\n[Rong et al., 2020b] Yu Rong, Wenbing Huang, Tingyang\nXu, and Junzhou Huang. Dropedge: Towards deep graph\nconvolutional networks on node classification. In ICLR,\n2020.\n[Sami et al., 2019] Sami, Bryan Perozzi, Amol Kapoor,\nNazanin Alipourfard, Kristina Lerman, Hrayr Harutyun-\nyan, Greg Ver Steeg, and Aram Galstyan. MixHop:\nHigher-order graph convolutional architectures via sparsi-\nfied neighborhood mixing. In ICML, 2019.\n[Shi et al., 2021] Yunsheng Shi, Zhengjie Huang, Shikun\nFeng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked\nlabel prediction: Unified message passing model for semi-\nsupervised classification. In IJCAI, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, 2017.\n[Veliˇckovi´c et al., 2018] Petar Veliˇckovi´c, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Li `o, and\nYoshua Bengio. Graph attention networks. In ICLR, 2018.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2204\n[Vinyals et al., 2016] Oriol Vinyals, Samy Bengio, and Man-\njunath Kudlur. Order matters: Sequence to sequence for\nsets. In ICLR, 2016.\n[Wu et al., 2021] Zhanghao Wu, Paras Jain, Matthew\nWright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion\nStoica. Representing long-range context for graph neural\nnetworks with global attention. In NeurIPS, 2021.\n[Wu et al., 2022] Qitian Wu, Wentao Zhao, Zenan Li, David\nWipf, and Junchi Yan. Nodeformer: A scalable graph\nstructure learning transformer for node classification. In\nNeurIPS, 2022.\n[Xu et al., 2019a] Chengfeng Xu, Pengpeng Zhao, Yanchi\nLiu, Victor S. Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua\nFang, and Xiaofang Zhou. Graph contextualized self-\nattention network for session-based recommendation. In\nIJCAI, 2019.\n[Xu et al., 2019b] Nuo Xu, Pinghui Wang, Long Chen, Jing\nTao, and Junzhou Zhao. Mr-gnn: Multi-resolution and\ndual graph neural network for predicting structured entity\ninteractions. In IJCAI, 2019.\n[Ying et al., 2018] Zhitao Ying, Jiaxuan You, Christopher\nMorris, Xiang Ren, Will Hamilton, and Jure Leskovec.\nHierarchical graph representation learning with differen-\ntiable pooling. In NeurIPS, 2018.\n[Ying et al., 2021] Chengxuan Ying, Tianle Cai, Shengjie\nLuo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,\nand Tie-Yan Liu. Do transformers really perform badly\nfor graph representation? In NeurIPS, 2021.\n[Zhang et al., 2019] Jiani Zhang, Xingjian Shi, Shenglin\nZhao, and Irwin King. Star-gcn: Stacked and recon-\nstructed graph convolutional networks for recommender\nsystems. In IJCAI, 2019.\n[Zhang et al., 2022] Zaixi Zhang, Qi Liu, Qingyong Hu, and\nChee-Kong Lee. Hierarchical graph transformer with\nadaptive node sampling. In NeurIPS, 2022.\n[Zhao et al., 2021] Jianan Zhao, Chaozhuo Li, Qianlong\nWen, Yiqi Wang, Yuming Liu, Hao Sun, Xing Xie, and\nYanfang Ye. Gophormer: Ego-graph transformer for node\nclassification. arXiv:2110.13094, 2021.\n[Zhu et al., 2020] Jiong Zhu, Yujun Yan, Lingxiao Zhao,\nMark Heimann, Leman Akoglu, and Danai Koutra. Be-\nyond homophily in graph neural networks: Current limita-\ntions and effective designs. In NeurIPS, 2020.\n[Zhu et al., 2021] Yiran Zhu, Xing Xu, Fumin Shen, Yanli Ji,\nLianli Gao, and Heng Tao Shen. Posegtac: Graph trans-\nformer encoder-decoder with atrous convolution for 3d hu-\nman pose estimation. In IJCAI, 2021.\n[Zhu et al., 2022] Yangfu Zhu, Linmei Hu, Xinkai Ge, Wan-\nrong Peng, and Bin Wu. Contrastive graph transformer\nnetwork for personality detection. In IJCAI, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n2205",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.8788214325904846
    },
    {
      "name": "Computer science",
      "score": 0.6397542953491211
    },
    {
      "name": "Graph",
      "score": 0.541573166847229
    },
    {
      "name": "Theoretical computer science",
      "score": 0.47695958614349365
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2668917179107666
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210103986",
      "name": "Jingdong (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I189210763",
      "name": "Yunnan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99043593",
      "name": "Macquarie University",
      "country": "AU"
    }
  ],
  "cited_by": 20
}