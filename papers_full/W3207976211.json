{
    "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey",
    "url": "https://openalex.org/W3207976211",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2742924505",
            "name": "Wei, Xiaokai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102372205",
            "name": "Wang Shen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3104954816",
            "name": "Zhang, Dejiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3196134010",
            "name": "Bhatia, Parminder",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112596608",
            "name": "Arnold Andrew",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2983102021",
        "https://openalex.org/W3113819910",
        "https://openalex.org/W2126433015",
        "https://openalex.org/W3134979179",
        "https://openalex.org/W3039578880",
        "https://openalex.org/W3014521650",
        "https://openalex.org/W3005441132",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W2972167903",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W3093871960",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W3104415840",
        "https://openalex.org/W2759211898",
        "https://openalex.org/W2952984539",
        "https://openalex.org/W3106234277",
        "https://openalex.org/W193524605",
        "https://openalex.org/W2996604169",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2251648804",
        "https://openalex.org/W3083494020",
        "https://openalex.org/W3099178230",
        "https://openalex.org/W2983995706",
        "https://openalex.org/W3020775333",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W3133849783",
        "https://openalex.org/W3035153870",
        "https://openalex.org/W3154735894",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3114219454",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2783538964",
        "https://openalex.org/W3113280695",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2984100107",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W2951561177",
        "https://openalex.org/W3033176962",
        "https://openalex.org/W2609826708",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3105082862",
        "https://openalex.org/W1512387364",
        "https://openalex.org/W1991418309",
        "https://openalex.org/W1713614699",
        "https://openalex.org/W2970780738",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3080997787",
        "https://openalex.org/W3105111366",
        "https://openalex.org/W3100353583",
        "https://openalex.org/W3098266846",
        "https://openalex.org/W2963777632",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3040558716",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2962948632",
        "https://openalex.org/W2123142779",
        "https://openalex.org/W3022116759",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2593560537",
        "https://openalex.org/W2968908603",
        "https://openalex.org/W3212218773",
        "https://openalex.org/W3105601320",
        "https://openalex.org/W3035419191",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3115729981",
        "https://openalex.org/W3126974869",
        "https://openalex.org/W3034834768",
        "https://openalex.org/W3089060322",
        "https://openalex.org/W3114916066",
        "https://openalex.org/W2962891712",
        "https://openalex.org/W2739716023",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2250861254",
        "https://openalex.org/W2969788713",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W3182352988",
        "https://openalex.org/W2968289784",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W3100283070",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2964207259",
        "https://openalex.org/W3090656107",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W3104007871",
        "https://openalex.org/W2990070951",
        "https://openalex.org/W2998374885",
        "https://openalex.org/W3117246841",
        "https://openalex.org/W2964116313"
    ],
    "abstract": "Pretrained Language Models (PLM) have established a new paradigm through learning informative contextualized representations on large-scale text corpus. This new paradigm has revolutionized the entire field of natural language processing, and set the new state-of-the-art performance for a wide variety of NLP tasks. However, though PLMs could store certain knowledge/facts from training corpus, their knowledge awareness is still far from satisfactory. To address this issue, integrating knowledge into PLMs have recently become a very active research area and a variety of approaches have been developed. In this paper, we provide a comprehensive survey of the literature on this emerging and fast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs). We introduce three taxonomies to categorize existing work. Besides, we also survey the various NLU and NLG applications on which KE-PLM has demonstrated superior performance over vanilla PLMs. Finally, we discuss challenges that face KE-PLMs and also promising directions for future research.",
    "full_text": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey\nXIAOKAI WEI, AWS AI, USA\nSHEN WANG,AWS AI, USA\nDEJIAO ZHANG, AWS AI, USA\nPARMINDER BHATIA,AWS AI, USA\nANDREW ARNOLD, AWS AI, USA\nPretrained Language Models (PLM) have established a new paradigm through learning informative contextualized representations\non large-scale text corpus. This new paradigm has revolutionized the entire field of natural language processing, and set the new\nstate-of-the-art performance for a wide variety of NLP tasks. However, though PLMs could store certain knowledge/facts from training\ncorpus, their knowledge awareness is still far from satisfactory. To address this issue, integrating knowledge into PLMs have recently\nbecome a very active research area and a variety of approaches have been developed. In this paper, we provide a comprehensive survey\nof the literature on this emerging and fast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs). We introduce\nthree taxonomies to categorize existing work. Besides, we also survey the various NLU and NLG applications on which KE-PLM has\ndemonstrated superior performance over vanilla PLMs. Finally, we discuss challenges that face KE-PLMs and also promising directions\nfor future research.\n1 INTRODUCTION\nIn recent years, large-scale pretrained language models (PLM), which are pretrained on huge text corpus typically with\nunsupervised objectives, have revolutionized the field of NLP. Pretrained models such as BERT [16], RoBERTa [50],\nGPT2/3 [68] [7] and T5 [69] have gained huge success and greatly boosted state-of-the-art performance on various NLP\napplications [67]. The wide success of pretraining in NLP also inspires the adoption of self-supervised pretraining in\nother fields, such as graph representation learning [30] [31] and recommender system [81][98].\nTraining on large textual data also enables these PLMs memorize certain facts and knowledge contained in the\ntraining corpus. As demonstrated in recent work, these pretrained language models could possess decent amount of\nlexical knowledge [48] [92] as well as factual knowledge [63] [71] [95]. However, further study reveals that PLMs also\nhave the following limitations in terms of knowledge awareness:\n‚Ä¢For NLU, recent study have found PLMs tend to rely on superficial signals/statistical cues [62] [55] [58], and can\nbe easily fooled with negated (e.g., ‚ÄúBirds can [MASK]‚Äù v.s. ‚ÄúBirds cannot [MASK]‚Äù) and misprimed probes [35].\nBesides, It has been found that PLMs often fail in reasoning tasks [84].\n‚Ä¢For NLG, although PLMs are able to generate grammatically correct sentence, the generated text might not be\nlogical or sensible [46] [51]. For example, as noted in [46], given a set of concepts {dog, frisbee, catch, throw},\nGPT2 generates ‚ÄúA dog throws a frisbee at a football player ‚Äù and T5 generates ‚Äúdog catches a frisbee and throws it\nto a dog ‚Äù, neither of which aligns with human‚Äôs commonsense.\nThese observations motivate work on designing more knowledge-aware pre-trained models. Recently, an ever-\ngrowing body of work aims at explicitly incorporating knowledge into PLMs [ 100] [108][61][90][96][49][33]. They\nexploit knowledge from various sources such as encyclopedia knowledge, commonsense knowledge and linguistic\nknowledge with different injection strategies. Such knowledge integration mechanism have successfully enhance\n1\narXiv:2110.08455v1  [cs.CL]  16 Oct 2021\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\nexisting PLMs‚Äô knowledge awareness, and lead to improved performance on a variety of tasks, including but not limited\nto entity typing [100], question answering [101][45], story generation [22] and knowledge graph completion [102].\nIn this paper, we aim to provide a comprehensive survey on this emerging field of Knowledge Enhanced Pretrained\nLanguage Models (KE-PLMs). Existing work on KE-PLMs have developed a diverse set of techniques for knowledge\nintegration on different knowledge sources. To provide insights on these models and facilitate future research, we build\nthree taxonomies to categorize the existing KE-PLMs. Figure 1 illustrates our proposed taxonomies on Knowledge\nEnhanced Pretrained Language Models (KE-PLMs).\nIn existing KE-PLMs, there are different types of knowledge sources (e.g., linguistic, commonsense, encyclopedia,\napplication-specific) have been explored to enhance the capability of PLMs in different aspects. The first taxonomy helps\nus to understand what knowledge sources have been considered for constructing KE-PLMs. In the second taxonomy,\nwe recognize that a knowledge source can be exploited to different extents, and categorize the existing work based\non the knowledge granularity: text-chunk based, entity-based, relation triple-based and subgraph-based. Finally, we\nintroduce a third taxonomy that groups the methods by their application areas. This taxonomy presents a range of\napplications that existing KE-PLMs have targeted to improve with the help of knowledge integration. By recognizing\nwhat application areas have been well addressed/under addressed by KE-PLMs, we believe this could shed light on\nfuture research opportunities on applying KE-PLMs to under-addressed areas.\nRelated workTwo contemporaneous reviews [73] and [12] also investigate incorporating relational knowledge\ngraph into pretrained language models. We cover a larger scope by discussing more types of knowledge (e.g., domain\nspecific knowledge) and present in-depth discussion on the applications and potential future directions.\nThe rest of this survey is organized as follows. In section 2, we present our taxonomy based on the knowledge source\nof KE-PLMs, and discuss representative approaches in each category. In section 3, we categorize the different knowledge\ngranularities that existing KE-PLMs exploit, and summarize the common techniques employed for incorporating such\nknowledge. In section 4, we present the applications that benefit from the development of KE-PLMs and introduce\ncorresponding datasets. In section 5, we discuss the challenges facing the design of highly effective KE-PLMs and the\nopportunities for future research in this area. Lastly, we conclude our survey in 6.\n2 KNOWLEDGE SOURCE\nExisting works have explored the integration of knowledge from diverse sources: linguistic knowledge, encyclopedia\nknowledge, and commensense knowledge and domain-specific knowledge. In this section, we categorize existing work\nby their knowledge source. For each category of knowledge source, we also introduce several representative methods\nand the corresponding knowledge they exploit.\n2.1 Linguistic Knowledge\nLexcial relationLexically Informed BERT (LIBERT) [39] incorporate lexical relation knowledge by predicting whether\ntwo words in a sentence exhibit semantic similarity (i.e., whether they are synonyms or hypernym-hyponym pairs).\nWord senseSenseBERT [41] exploits word-sense knowledge from WordNet as weak supervision by including\nadditional training task on predicting word-supersense (e.g., noun.food and noun.state) based on the masked word‚Äôs\ncontext.\nSyntax treeSyntax-BERT[2] employs syntax parsers to extract both dependency [9] and constituency parsing [110]\ntrees. Syntax-related masks are designed to incorporate information from the syntax trees. K-Adapter[96] also includes\n2\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\nKE-PLM\nKnowledgeSource\nKnowledgeGranularity\nApplication\nLinguisticKnowledge\nEncyclopediaKnowledge\nCommonsenseKnowledge\nApplication/Domain-specific\nTextEntityRelationSubgraph\nText GenerationEntity-relatedTasksQuestion AnsweringKGCompletion\nLIBERT, SenseBERT, Syntax-BERT\nERNIE, WKLM, KgPLM, LUKE, ERICA\nGRF, KG-BART, COMET\nSKEP, BERT-MK, Ecommerce-BERT\nRAG,KIFERNIE, LUKE, KALM,  CoLake, WKLMFaE, BERT-MK, KGLM, K-BERTCokeBERT, GRF, KG-BART\nGRF, KG-BART, LRLMCoLake, K-Adapter, LUKE, WKLM, KALMKT-NET, AMS, GLM, EaE, FaEKG-BERT, BLP, KEPLER, COMET\nFig. 1. Taxonomy of Knowledge Enhanced Pretrained Langauge Models (KE-PLMs)\na linguistic adapter that incorporates dependency parsing information, by predicting the head index for each token in a\nsentence.\nPart-of-Speech tagSentiLARE [36] and LIMIT-BERT [109] consider POS tag as additional knowledge. For example,\nLIMIT-BERT incorporates multiple types of linguistic knowledge simultaneously in a multi-task manner. In addition to\nPOS tags and syntactical parsing tree, LIMIT-BERT also explores span and semantic role labeling (SRL) information.\nWith such additional linguistic knowledge incorporated, these approaches are able to demonstrate superior perfor-\nmance on general NLU benchmark dataset such as GLUE [94] and SuperGLUE[93], or specific applications such as\nsentiment analysis.\n2.2 Encyclopedia Knowledge\nEncyclopedia KG such as Freebase [4], NELL [8] and Wikidata [91] contain facts/world knowledge in the following\nform:\n(‚Ñéùëíùëéùëë_ùëíùëõùë°ùëñùë°ùë¶,ùëüùëíùëôùëéùë°ùëñùëúùëõ,ùë°ùëéùëñùëô _ùëíùëõùë°ùëñùë°ùë¶)\n3\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\nFor example, (‚ÄôJoe Biden‚Äô, ‚ÄôPresidentOf‚Äô, ‚ÄôUSA‚Äô) . These encyclopedia KG are able to provide abundant knowledge for\nPLMs to integrate. A majority of exiting work on KE-PLMs[108][26][105] [96][90][82][66] uses Wikidata1 as knowledge\nsource. Typically, entities in Wikidata are linked with entity mentions in the text of Wikipedia. Then entity-aware\ntraining could be performed on such linked data to learn the parameters for KE-PLMs.\n2.3 Commonsense Knowledge\nTo empower PLMs with more commonsense reasoning capability, existing models typically resort to the following two\ncommonsense knowledge graphs: ConceptNet [79] and ATOMIC [74].\n‚Ä¢ConceptNet2 [79] is a multilingual knowledge graph consisting of triples in 34 relations, such as CapableOf,\nCauses and HasProperty. For a knowledge triple (ùëêùëúùëõùëêùëíùëùùë°1,ùëü,ùëêùëúùëõùëêùëíùëùùë° 2), it represents that head ùëêùëúùëõùëêùëíùëùùë°1 has\nthe relation ùëü with tail ùëêùëúùëõùëêùëíùëùùë°2. For example, the triple (cooking, requires, food) means that ‚Äúthe prerequisite of\ncooking is food‚Äù.\n‚Ä¢ATOMIC [74] contains inferential knowledge in the form if-then triples. It covers a variety of social common-\nsense knowledge around specific event prompts (e.g., ‚ÄúX goes to the store‚Äù). Specifically, ATOMIC distills its\ncommonsense in nine dimensions, covering the event‚Äôs causes and effects on the agent.\nIncorporating knowledge from ConceptNet and ATOMIC helps PLMs gain stronger capability on commonsense reason-\ning [45][77][103][53] [22][33][104][51][44]. We will discuss in more detail about how the knowledge is incorporated\nin section 3 and how they benefit multiple commonsense-related downstream tasks such commonsenseQA and text\ngeneration in section 4.\n2.4 Application/Domain Specific Knowledge\nIn this subsection, we introduce KE-PLMs which utilizes domain-specific knowledge for their particular vertical\napplications.\nSentiment knowledgeIn addition to the POS information mentioned in section 2.1, SentiLARE[36] also utilizes\nsentiment word polarity from SentiWordNet[1]. SKEP[86] incorporates sentiment knowledge from self-supervised\ntraining, including sentiment word detection, word polarity and aspect-sentiment pair.\nMedical knowledgeMedical domain knowledge [27] integrate biomedical ontology from Unified Medical Language\nSystem (UMLS) [3] to facilitate tasks in medical domain. K-BERT [49] also exploits knowledge from a medical concept\nKG for higher quality NER in medical domain.\nE-commerce Product GraphE(commerce)-BERT[106] utilizes product association graph (i.e., whether two products\nare substitutable and complementary) [54] constructed from consumer shopping statistics. It introduces additional\ntasks for reconstructing a product given its neighbor products in association graph.\nThough most existing approaches exploit only one knowledge source, it is worth noting that certain methods attempt\nto incorporate from more than one knowledge source. For example, K-Adapter [ 96] incorporate knowledge from\nmultiple sources by learning a different adapter for each knowledge source. It exploits both dependency relation as\nlinguistic knowledge and relation/fact knowledge from Wikidata.\n1https://www.wikidata.org/\n2https://conceptnet.io/\n4\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\nTable 1. Summarization of entity-related objectives. ùë†ùëñ is the similarity score between mention ùëöùëñ and entity ùëíùëñ (e.g., the inner\nproduct between them) given the contextulized embedding C. ùë†‚àí\nùëñ is the similarity on negative(i.e. distractor) mention/entity pairs. ùëüùëñ\nrepresents whether the entity has been replaced with any negative sample.\nLoss Type Typical Form Reprensentative Methods\nCross entropy\nentity linking (EL) loss √ç\nùëñ I(ùëöùëñ = ùëíùëñ )log(ùëíùëñ |C)\nERNIE(THU)[108], JAKET[105]\nEaE[21], LUKE[100], CokeBERT[80]\nMax margin EL loss √ç\nùëñ ùëöùëéùë•(ùúÜ‚àíùë†ùëñ +ùë†‚àí\nùëñ ,0) GLM[77], KALM[72]\nEntity replacement detection\n√ç\nùëó I(ùëüùëñ = 1)¬∑ log ùëù(ùëüùëñ = 1|C)+\nI(ùëüùëñ = 0)¬∑ log ùëù(ùëüùëñ = 0|C) WKLM[99], GLM[77]\nSqured Loss √ç\nùëñ ||Wùëíùëñ ‚àíùëöùëñ ||2 E-BERT [64]\n3 KNOWLEDGE GRANULARITY\nA majority of approaches resort to knowledge graphs (enclopedia, commonsense or domain-specific) as source of\nknowledge. In this section, we group these models by the granularity of knowledge they incorporate from KG: text-based\nknowledge, entity knowledge, relation triples and KG subgraphs.\n3.1 Text Chunk-based Knowledge\nRAG [43] builds an index in which each entry is a text chunk from Wikipedia. It first retrieve top-k documents/chunks\nfrom the memory using kNN-based retrieval, and the BART model [42] is employed to generate the output, conditioned\non these retrieved documents. Similarly, KIF [18] uses KNN-based retrieval on an external non-parametric memory\nstoring wikipedia sentences and dialogue utterances to improve generative dialog modeling.\n3.2 Entity Knowledge\nEntity-level information can be highly useful for a variety of entity-related tasks such as NER, entity typing, relation\nclassification and machine reading comprehension. Hence, many existing KE-PLM models target this type of simple yet\npowerful knowledge.\nA popular approach to making PLMs more entity aware is to introduce the entity-aware objectives while pretraining.\nSuch strategy is adopted by multiple existing approaches: ERNIE (BAIDU)[ 83] ERNIE(THU)[108] CokeBERT[80]\nKgPLM[26], LUKE[100], GLM[77], KALM[72], CoLAKE [82], JAKET[105] and AMS[103]. A typical choice of entity-\nrelated objective is an entity linking loss which predicts the entity mention in text to entity in KG with a cross entropy\nloss or max-margin loss on the prediction [108][100][105][72]. There is also method that employs replacement detection\nloss in which the goal is the predict whether an entity mention has been replaced with a distractor [99]. Also, certain\nmethods adopts more than one type of losses. For example, KgPLM[26] show through ablation study that employing\nboth a cross-entropy entity prediction loss and an entity replacement detection loss (which were referred to as generative\nand discriminative loss respectively) lead to better performance than only using one loss. We summarize the different\nentity-aware loss functions in Table 1.\nBesides named entities, a few existing approaches find it helpful to incorporate phrase information into pretraining,\nsuch as ERNIE (BAIDU) [83] and E(commerce)-BERT[106].\n5\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\n3.3 Relation Knowledge\nKE-PLMs by incorporating entity information alone have already demonstrated impressive performance gains over\nvanilla PLMs. Recently, an increasing amount of work have considered integrating knowledge beyond entities, such as\nthe relation triples in KG to make the models even more powerful.\nFor example, FaE (Fact-as-Experts) [90] further extends EaE [21] by building fact (i.e., relation triples in KG) memory\nin addition to the entity memory. The set of tail entities retrieved from fact memory are aggregated with attention\nmechanism, and then fused with token embeddings. BERT-MK [27] and adopts similar technique as ERNIE-THU[108]\nfor knowledge fusion. ERICA[66] applies contrastive loss on entity and relation, which pulls neighbor entity/relation\nclose and pushes non-neighbors far apart in the embedding space. K-Adapter[96] introduces a factual adapter which\nincorporates relation information by performing relation classification based on the entity context. K-BERT[49] injects\nknowledge by augmenting sentence with the triplets from KG to transform it into a knowledge-rich sentence tree. To\nprevent changing the original semantic of the sentence, K-BERT adopts a visible matrix to control the visibility of each\ninjected token.\nTo better predict the plausibility of a triple, KG-BERT [102] fine-tune a BERT model on a training format as in (1)\nderived from KG triplets. Similarly, COMET [6] and [22] verbalize the concepts and relations from commonsense KG\ninto text tokens for training a knowledge enhanced PLM. KEPLER [97] and BLP (BERT for Link Prediction) [13] further\ninclude a TransE based objective [5] for the PLM to incorporate information from the triples in KG.\n[ùê∂ùêøùëÜ]HeadEntityTokens[ùëÜùê∏ùëÉ]ReltionTokens[ùëÜùê∏ùëÉ]TailEntityTokens[ùëÜùê∏ùëÉ] (1)\nLRLM [25] and KGLM [52] are two language models that utlize relations in KG when generating the next token in\nan autoregressive manner. They introduce a latent variable to determine whether the next token should be a text-based\ntoken or entity/relation induced token.\n3.4 Subgraph Knowledge\nAs knowledge graphs provide richer knowledge than simply entity/relation triplets, recently, an increasing number of\napproaches start to explore integration of more sophisticated knowledge, such as subgraphs in KG. The approaches in\nthis category typically have two stages: subgraph construction and subgraph representation learning. In the subgraph\nconstruction stage, they extract the 1-hop or K-hop relations. As the subgraph can be large (particularly when ùêæ ‚â•2)\nand many entities/relations involved might not be highly relevant to the text context, usually certain pruning/filtering\nare conducted and/or attention mechanism are applied on the nodes/relations. In the second stage, for representation\nlearning on extracted subgraphs, most existing approaches adopt certain variants of Graph Neural Networks with\nattention mechanism.\n3.4.1 Encylopedia KG. CokeBERT [80] designs Semantic-driven GNN (S-GNN) for representing knowledge subgraphs.\nS-GNN employs attention mechanism to direct the model to better focus on the most relevant entities/relations.\nSubsequently, conditioned on pre-trained entity embedding and subgraph embedding, the entity mention embedding is\ncomputed and used in a way similar to ERNIE (THU) [108].\nCoLake [82] also uses GNN to aggregate information from the extracted knowledge subgraphs in both pretraining\nand inference. Instead of constructing GNN in an explicit way, CoLake convert the subgraph into token sequence and\nappend it to input sequence for PLM, by recognizing the fact that self-attention mechanism in Transformer [ 88] is\nsimilar to Graph Attention Networks [89] in spirit.\n6\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\n3.4.2 Commonsense KG. Knowledge subgraphs are also very useful for commonsense based applications, such as\nCommonsenseQA and text/story generation.\nKag-Net [45] employs path-finding based algorithm for subgraph construction. It first applies GCN on the subgraph\nto generate node embeddings, then the output of GCNs is passed to LSTM-based [29] path encoder. Finally, the path\nembeddings on the subgraphs are combined through attention-based pooling to obtain the knowledge representation.\nGRF [33] also adopts a GNN-based module for concept subgraph encoding, and it designs a dynamic reasoning\nmodule to propogate information on this graph at each decoding step.\nKG-BART [51] first constructs a knowledge subgraph from commonsense KG, and use Glove embedding [60] based\nword similarity to prune irrelevant concepts. Then graph attention is employed to learn concept embeddings, which\nare integrated with token embedding using concept-to-subword and subword-to-concept fusion. Such scheme enables\nKG-BART to generate sentences with more logical and natural even with unseen concepts. [ 53] also uses GNN to\ngenerate embedding for concepts graphs.\nIn Table 2, we summarize the existing approaches with their characteristics.\n4 APPLICATIONS\nKnowledge enhanced pretrained language models have benefited a variety of NLP applications, especially those\nknowledge-intensive ones. In the following, we discuss in more detail how KE-PLM improves the performance of\nvarious NLG and NLU tasks. To facilitate future research in the area, we also briefly introduce a few widely used\nbenchmark datasets for evaluating the efficacy of these models.\n4.1 Text Generation\nThe KE-PLMs designed for natural language generation can be roughly categorized into following two types:\n‚Ä¢To improve the logical soundness of generated text by incorporating commonsense knowledge graphs.\n‚Ä¢To improve the factualness of generated text by incorporating encyclopedia knowledge\nIn the following, we introduce the commonly used datasets for evaluating these two aspects.\nROCStories [57] consists of 98,162 coherent stories (each with five sentences) as the unlabeled training dataset.\nROCStories is widely used for story understanding and generation tasks. By integrating subgraph information from\nCommonsense KG (e.g., ConceptNet and ATOMIC), knowledge enhanced PLMs [22][33][104] are able to generate story\nendings that is more logical and aligns better with the commonsense of human.\nCommonGen [46] is a popular dataset for constrained text generation task. It combines crowdsourced and existing\ncaption corpora to produce commonsense descriptions for over 35k concept sets. The goal of CommonGen is to assess\nthe model‚Äôs capability on commonsense reasoning when generating text. In this task, given a concept set with typically\nthree to five concepts, the model is expected to generate coherent and sensible text description containing these concepts.\nKG-BART [51] and [44], via incorporating commonsense knowledge, show better capability on generating more natural\nand sensible text for the given concept set.\n4.2 Entity-related Tasks\nSince many existing approaches incorporate entity-level knowledge, entity related tasks (e.g., entity typing and relation\nclassification) become natural testbeds for evaluating the efficacy of these KE-PLMs. By injecting entity information\n7\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\nTable 2. Comparisons of different knowledge enhanced pretrained langauge models\nMethod\nKnowledge\nAware\nPretraining\nKnowledge\nAware\nAuxiliary Loss Knowledge Source\nERICA [66] Yes entity/relation discrimination Wikipedia, Wikidata\nERNIE (THU) [108] Yes entity prediction Wikipedia/Wikidata\nERNIE 2.0 (Baidu) [83] Yes masked entity/phrase N/A\nE-BERT [64] Yes entity/wordpiece alignment Wikipedia2Vec\nE(commerce)-BERT [106] Yes neighbor Product Reconstruction product graph/AutoPhrase[75]\nEaE [21] Yes mention detection/linking Wikipedia\nCokeBERT [80] Yes entity prediction Wikipedia/Wikidata\nCOMET [6] No autoregressive ATOMIC, ConceptNet\nK-Adapter [96] No dependency relation Wikipedia, Wikidata, Stanford Parser\nKnowBERT [61] Yes entity linking WordNet, Wikipedia\nK-BERT [49] No finetuning\nWikiZh, WebtextZh, CN-DBpedia\nHowNet, MedicalKG\nKEPLER [97] Yes TransE scoring Wikipedia/Wikidata\nKG-BERT [102] Yes relation cross-entropy ConceptNet\nKG-BART [51] Yes masked concept ConceptNet\nKgPLM [26] Yes generative/discriminative masked entity Wikipedia/Wikidata\nFaE [90] Yes masked entity et.al Wikipedia/Wikidata\nJAKET [105] Yes entity category/relation type/masked entity Wikipedia/Wikidata\nLUKE [100] Yes entity prediction Wikipedia\nWKLM [99] Yes entity replacement detection Wikipedia/Wikidata\nCoLAKE [82] Yes masked entity prediction Wikipedia/Wikidata\nKT-NET [101] No finetuning N/A\nLIBERT [40] Yes lexical relation prediction WordNet\nSenseBERT [41] Yes supersense prediction WordNet\nSyntax-BERT [2] No masks induced by syntax tree parsing syntax tree\nSentiLARE [36] Yes POS/ word level polarity/sentiment polarity SentiWordNet\n[44] No finetuning ConceptNet\nCOCOLM [104] Yes discourse relation/co-occurrence relation ASER\n[22] Yes autoregressive ConceptNet/ATOMIC\nAMS [103] Yes distractor-based loss ConceptNet\nGLM [77] No distractor-based entity linking loss ConceptNet\nGRF [33] No finetuning ConceptNet\nKagNet [45] No finetuning ConceptNet\nLIMIT-BERT [109] Yes semantics/syntax pretrained model\nKGLM [52] Yes autoregressive WikiText-2/wikidata\nLRLM [25] Yes autoregressive Wikidata/Freebase\n[59] No finetune Wikidata\nSKEP [86] Yes Word Polarity Prediction auto-mined\ninto the model, they are able to be more entity-aware and outperform these vanilla PLMs (such as BERT and RoBERTa)\non different benchmark datasets.\n8\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\n4.2.1 Entity Typing. The goal of entity typing is to classify entity mentions to predefined categories. Two popular\nbenchmark datasets used for entity typing are Open Entity[11] and FIGER[47] (sentence-level entity typing dataset)).\nTo perform entity typing, existing approaches [80][97][108][100][82][64][96] typically insert special tokens [E] and\n[/E] to surround the entity mention and employ the contextual representation of the [E] token to predict the category.\n4.2.2 Relation Classification. Relation classification/relation typing aims at classifying the relationship between two\nentities mentioned in text. TACRED[107] is widely used benchmark dataset for relation classification. It consists of over\n10ùëò sentences with a total of 42 relations. FewRel[23] is a dataset constructed from Wikipedia text and Wikidata facts.\nIt contains 100 relations can be used for evaluating few-shot relation classification.\nTo perform relation classification task, existing PLMs [ 80][108][97][108][100][82][64][96] typically perform the\nfollowing during fine-tuning: special tokens [HE], [/HE], [TE] and [/TE] are added to mark the beginning and end for\nhead entity and tail entity. Then they usually concatenate the contextual embeddings for [HE] and [TE] to predict the\nrelation category.\n4.3 Question Answering\n4.3.1 Cloze-style Question Answering/Knowledge Probing. LAMA (LAnguage Model Analysis) probe [63] is a set of\ncloze-style questions with single-token answers. It is generated from the relation triplets in KG with templates which\ncontain variables s and o for subject and object (e.g, ‚Äús was born in o‚Äù). This dataset aims at measuring how much factual\nknowledge is stored in pretrained models. [64] further constructs LAMA-UHN, a more difficult subset of LAMA, by\nfiltering out easy-to-answer questions with overly informative entity names.\nCoLAKE[82], E-BERT[64], KgPLM[26], EaE [ 21], KALM[72], KEPLER[97] and K-Adapter[ 96] adopt LAMA for\nknowledge probing. With the injected knowledge, they are able to generate more factual answers than vanilla PLMs.\n4.3.2 Open Domain Question Answering. PLMs equiped with Knowledge could lead to better performance on Open\nDomain Question Answering (ODQA), as the context and answer often involve entities. A few KE-PLMs evaluate their\napproaches on ODQA dataset to showcase their improved capability. For example, KT-NET[101] and LUKE[100] demon-\nstrate their efficacy on SQuAD1.1[70]. TriviaQA[34] and SearchQA[17] were used by EaE[21] and K-ADAPTER[96],\nrespectively.\n4.3.3 Commonsense QA. CommonsenseQA [85] is a dataset question answering constructed from ConceptNet [79]\nto test model‚Äôs capability on understanding several commonsense types (e.g., causal, spatial, social). Each question is\nequipped with one correct answer and four distractors by human annotators. The distractors are typically also related\nto the question but less aligned with human commonsense. One example question is \"what do all humans want to\nexperience in their own home? {feel comfortable, work hard, fall in love, lay eggs, live forever}\"3. Through integrating\nknowledge from commonsense KG, KagNet[45], GLM[77], AMS[103] and [53] are able to perform reasonably well\non CommonsenseQA. CosmosQA[32] is a dataset with multiple-choice questions for commonsense-based reading\ncomprehension, and K-Adapter[96] outperforms vanilla RoBERTa[50] on this dataset.\n4.4 Knowledge Graph Completion\nKnowledge graphs usually suffer from the problem of incompleteness and many relations between entities are missing\nin KG. This might negatively impact multiple downstream tasks, such as knowledge base QA [24]. Pretrained language\n3https://www.tau-nlp.org/commonsenseqa\n9\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\nmodels enhanced by knowledge graph can in turn help infer missing links and improve the completeness of knowledge\ngraphs. Some popular benchmark datasets for evaluating link prediction on knowledge graphs are WN18RR [15] (subset\nof WordNet), FB15K-237 [15] (subset of FreeBase [4]) and also Wikidata5M (recently introduced in [97]).\nKG-BERT [102], BLP [13], GLM [77] and KEPLER [97] by including margin-based or cross-entropy loss derived from\nrelation triplet, demonstrated competitive performance on the datasets mentioned above. Besides, it has been shown\nthat COMET [6] is able to generate new high-quality commonsense knowledge, and hence can be employed to further\ncomplete commonsense KGs such as ConceptNet [79] and ATOMIC [74].\n4.5 Sentiment Analysis Tasks\nKE-PLMs also demonstrate their effectiveness on sentiment analysis tasks such as Amazon QA dataset [56], Stanford\nSentiment Treebank (SST) [78], Semantic Eval 2014 Task4 [65]. With the help of additional knowledge such as word\nsentiment polarity, SentiLARE [36], SKEP [86], E(commerce)-BERT [106] have achieved than their vanilla counterpart\non sentence-level and aspect-level sentiment classification. E(commerce)-BERT [ 106] also demonstrates superior\nperformance on review based QA and review aspect extraction.\n5 CHALLENGES AND FUTURE DIRECTIONS\nIn this section, we present the common challenges that face the development of KE-PLMs, and discuss possible research\ndirections to address these challenges.\n5.1 Exploring More Applications\nAs discussed in previous section, knowledge enhanced PLMs have achieved success on multiple NLU and NLG tasks.\nBesides these tasks, there still remain many other applications that can potentially benefit from KE-PLMs such as the\nfollowing:\n‚Ä¢Text SummarizationKE-PLMs have shown impressive performance on generating more logical and factual\ntext. Factual inconsistency has been an issue that affects the performance of existing document summarization\napproaches for a long time [37]. Adopting the techniques from knowledge enhanced PLMs for improving the\nfactualness of document summarization would be a promising field to explore.\n‚Ä¢Machine TranslationHow KE-PLMs can improve the quality of machine translation or is not yet explored,\nto our best knowledge. Integrating linguistic and commonsense knowledge into the PLMs might further their\nperformance.\n‚Ä¢Semantic parsingbased applications might also benefit from entity-aware KE-PLMs since it often relies on the\nentity information in the text.\n5.2 Exploring More Knowledge Sources\n5.2.1 Application/Domain-specifc Knowledge. Since most existing work focuses on encyclopedia KG, commonsense KG\nor linguistic knowledge, exploiting domain-specific knowledge is still a relatively under-explored area. As we present in\nprevious sections, several papers have investigated knowledge integration in medical domain and E-commerce domain.\nAs future work, one could investigate the effectiveness of knowledge injection from other domains to PLMs, such as\nfinance, sports and entertainment. It would be exciting to see how KE-PLMs could help improve the NLU and NLG\nperformance in diverse fields.\n10\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\n5.2.2 Temporal Knowledge Graph. It would be beneficial to study how to effectively integrate temporal knowledge\ngraphs [87] into PLMs. For example, the personName in relation triple (personName, presidentOf, USA) would not be the\nsame during different time periods. Handling such temporal knowledge could be helpful for further enhancing the\ncapability of KE-PLMs but also poses additional challenges to knowledge representation and integration.\n5.3 Integrating Complex Knowledge More Effectively\nThere have been some recent attempts to incorporate knowledge with complex model architecture or perform pretraining\nin a more sophisticated manner (such as joint training of PLM and KG embedding) [80][97]. However, on benchmark\ndatasets, the reported performance of these more sophisticated schemes does not seem to always outperform approaches\nthat incorporate only entity-level information such as LUKE [100]. Hence, we believe there is still great potential on\nexploring more effective knowledge integration schemes.\n5.4 Optimizing Computational Burden\nDespite the success of KE-PLM on a variety of applications, one should not overlook the fact that incorporating\nknowledge might incur more computational burden and storage overhead. Most existing work only reports accuracy\ngains but the incurred cost of knowledge integration has not been thoroughly studied. Designing more time and space\nefficient solutions would be critical to practical adoption of these knowledge enhanced models.\n5.4.1 Time Overhead. Many existing methods involves pre-training with auxiliary tasks on large scale linked data\n[108][82][100]. How to minimize the amount of pretraining required while still achieving strong performance requires\nmore investigation. Methods such as K-BERT[49], K-Adapter [96] and Syntax-BERT [2] make progress towards this end\nby being pretraining-free. We believe there is still potential on further striking a balance between pretraining workload\nand model performance. When it comes to inference, there is usually additional cost for KE-PLMs as well. For example,\na few approaches [33][51][81] involve constructing knowledge subgraphs and learn graph embeddings on the fly, which\nmight increase the inference time. It is worth developing more efficient inference strategy for KE-PLMs to facilitate\ntheir adoption in real-world applications.\n5.4.2 Space Overhead. The additional space consumption of KE-PLMs might also be a concern for their practical\ndeployment. For instance, Fact-as-Experts (FaE) [90] builds external entity memory and fact memory (with 1.54 million\nKB triples) to complement pretrained model. Retrieval Augmented Generation (RAG) [43] relies on a non-parametric\nknowledge corpus with 21 million documents. These integrated entities/facts are not equally useful and some entities\nmight play a more important role in enhancing the model‚Äôs capability than others. Thus, how to select a succinct set of\nmost important knowledge entries out of a potentially huge space could be a promising future direction.\nAnother direction worth exploring is to employ model compression techniques [10] on both knowledge integration\nparameters (e.g., entity memory) and language model parameters of KE-PLMs. For example, popular model compression\nmethods such as quantization [ 76], knowledge distillation [ 28] and parameter sharing [ 38] [14] can be applied to\nKE-PLMs for improving time and space efficiency.\n5.5 Noise Resilient KE-PLMs\nWhile existing approaches benefit from the additional information provided by knowledge sources, they might also\nincorporate noise from potentially noisy sources. This is especially true for methods that rely on off-the-shelf toolkit\nfor knowledge extraction:\n11\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\n‚Ä¢Entity recognition and entity linking. Existing entity-aware PLMs typically depend on third-party tools or\nsimple heuristics for performing entity extraction and disambiguation. For example, ERNIE [108], CoLake [82],\nCokeBERT [80] use TAGME [20] to perform entity extraction and linking. ERICA [66] uses SpaCy to perform\nNER and then link the entity mentions to Wikidata. [72] relies on a (fuzzy) frequency-based dictionary look-up\nfor entity linking. All of these entity linkers would inevitably introduce certain amount of noise to the training\nprocess, which might in turn degrade the PLM‚Äôs performance.\n‚Ä¢Syntax treeSyntax-BERT[2] and K-Adapter [96] employ the Stanford Parser4 for generating dependency parsing\ntrees, which is further used to construct auxiliary loss.\nAfter the knowledge integration, such noise might propagate to other sub-components of the model and hence\nnegatively impact the performance on downstream tasks. How different noise levels affect the model‚Äôs performance\nwould be an interesting direction to explore. It would be highly desirable if new KE-PLMs developed in future could\ndemonstrate their robustness to such noise, in both pretraining phase and inference phase.\n5.6 Upper Bound Study\nRecently, gigantic pretrained models with hundreds of billions of parameters, such as Switch Transformers [19] and\nGPT-3 [7], have demonstrated their impressive capability for producing human-like text in a wide range of NLP tasks.\nThough the prohibitive training and inference cost make them less practical in large-scale real-world applications, they\nshowcase the huge upside of PLMs‚Äôs performance once equipped with larger training corpus, more model parameters\nand training iterations.\nHowever, such upper bound study for knowledge enhanced PLMs still seems to be lacking, and there has not been\nas much effort on pushing the limit for KE-PLMs. It would be interesting to study how well a KE-PLM can perform\nwith massive parameters and knowledge incorporated from a large number of knowledge sources and types (e.g.,\nencyclopedia, commonsense, linguistic). This would be a challenging problem that requires more careful design of\nknowledge integration schemes, to effectively integrate from multiple knowledge sources and mitigate forgetting.\nBesides, more advanced distributed training techniques are also critical to empowering the training of such gigantic\nKE-PLMs.\n6 CONCLUSION\nIntegrating knowledge into pretrained language models has been an active research area. We have witnessed an\never-growing interest on this topic, since BERT [16] popularized the application of PLMs. In this survey, we thoroughly\nsurvey and categorize the existing KE-PLMs from a methodological point of view, and establish taxonomy based on\nknowledge source, knowledge granularity and applications. Finally, we highlight several challenges on this topic and\ndiscuss potential research directions in this area. We hope that this survey will facilitate future research and help\npractitioners to further explore this promising field.\nREFERENCES\n[1] S. Baccianella, A. Esuli, and F. Sebastiani. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. InProceedings\nof the Seventh International Conference on Language Resources and Evaluation (LREC‚Äô10) , Valletta, Malta, May 2010. European Language Resources\nAssociation (ELRA).\n[2] J. Bai, Y. Wang, Y. Chen, Y. Yang, J. Bai, J. Yu, and Y. Tong. Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees.arXiv e-prints ,\npage arXiv:2103.04350, Mar. 2021.\n4https://nlp.stanford.edu/software/lex-parser.shtml\n12\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\n[3] O. Bodenreider. The unified medical language system (umls): integrating biomedical terminology. Nucleic Acids Res. , 32(Database-Issue):267‚Äì270,\n2004.\n[4] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: A collaboratively created graph database for structuring human knowledge.\nIn Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data , SIGMOD ‚Äô08, page 1247‚Äì1250, New York, NY, USA, 2008.\nAssociation for Computing Machinery.\n[5] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for modeling multi-relational data. In C. J. C.\nBurges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,Advances in Neural Information Processing Systems , volume 26. Curran\nAssociates, Inc., 2013.\n[6] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi. COMET: Commonsense transformers for automatic knowledge graph\nconstruction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4762‚Äì4779, Florence, Italy, July 2019.\nAssociation for Computational Linguistics.\n[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 1877‚Äì1901. Curran Associates, Inc., 2020.\n[8] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. H. Jr., and T. Mitchell. Toward an architecture for never-ending language learning. In Proceedings of\nthe Conference on Artificial Intelligence (AAAI) , pages 1306‚Äì1313. AAAI Press, 2010.\n[9] D. Chen and C. Manning. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages 740‚Äì750, Doha, Qatar, Oct. 2014. Association for Computational Linguistics.\n[10] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and acceleration for deep neural networks: The principles, progress, and challenges.\nIEEE Signal Processing Magazine , 35(1):126‚Äì136, 2018.\n[11] E. Choi, O. Levy, Y. Choi, and L. Zettlemoyer. Ultra-fine entity typing. InProceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 87‚Äì96, Melbourne, Australia, July 2018. Association for Computational Linguistics.\n[12] P. Colon-Hernandez, C. Havasi, J. Alonso, M. Huggins, and C. Breazeal. Combining pre-trained language models and structured knowledge. arXiv\ne-prints, page arXiv:2101.12294, Jan. 2021.\n[13] D. Daza, M. Cochez, and P. Groth. Inductive Entity Representations from Text via Link Prediction. arXiv e-prints , page arXiv:2010.03496, Oct. 2020.\n[14] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n[15] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel. Convolutional 2D knowledge graph embeddings. In Thirty-Second AAAI Conference on\nArtificial Intelligence, 2018.\n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. InProceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 4171‚Äì4186, Minneapolis, Minnesota, June 2019.\n[17] M. Dunn, L. Sagun, M. Higgins, V. Ugur Guney, V. Cirik, and K. Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search\nEngine. arXiv e-prints , page arXiv:1704.05179, Apr. 2017.\n[18] A. Fan, C. Gardent, C. Braud, and A. Bordes. Augmenting Transformers with KNN-Based Composite Memory for Dialogue. arXiv e-prints , page\narXiv:2004.12744, Apr. 2020.\n[19] W. Fedus, B. Zoph, and N. Shazeer. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. arXiv e-prints ,\npage arXiv:2101.03961, Jan. 2021.\n[20] P. Ferragina and U. Scaiella. Tagme: On-the-fly annotation of short text fragments (by wikipedia entities). In Proceedings of the 19th ACM\nInternational Conference on Information and Knowledge Management , CIKM ‚Äô10, page 1625‚Äì1628, New York, NY, USA, 2010. Association for\nComputing Machinery.\n[21] T. F√©vry, L. Baldini Soares, N. FitzGerald, E. Choi, and T. Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 4937‚Äì4951, Online, Nov. 2020. Association\nfor Computational Linguistics.\n[22] J. Guan, F. Huang, Z. Zhao, X. Zhu, and M. Huang. A knowledge-enhanced pretraining model for commonsense story generation. Transactions of\nthe Association for Computational Linguistics , 8:93‚Äì108, 2020.\n[23] X. Han, H. Zhu, P. Yu, Z. Wang, Y. Yao, Z. Liu, and M. Sun. FewRel: A large-scale supervised few-shot relation classification dataset with\nstate-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 4803‚Äì4809, Brussels,\nBelgium, Oct.-Nov. 2018. Association for Computational Linguistics.\n[24] Y. Hao, Y. Zhang, K. Liu, S. He, Z. Liu, H. Wu, and J. Zhao. An end-to-end model for question answering over knowledge base with cross-attention\ncombining global knowledge. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 221‚Äì231, Vancouver, Canada, July 2017. Association for Computational Linguistics.\n[25] H. Hayashi, Z. Hu, C. Xiong, and G. Neubig. Latent Relation Language Models. arXiv e-prints , page arXiv:1908.07690, Aug. 2019.\n[26] B. He, X. Jiang, J. Xiao, and Q. Liu. KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning. arXiv\ne-prints, page arXiv:2012.03551, Dec. 2020.\n13\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\n[27] B. He, D. Zhou, J. Xiao, X. Jiang, Q. Liu, N. J. Yuan, and T. Xu. BERT-MK: Integrating graph contextualized knowledge into pre-trained language\nmodels. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 2281‚Äì2290, Online, Nov. 2020. Association for Computational\nLinguistics.\n[28] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop ,\n2015.\n[29] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation , 9(8):1735‚Äì1780, 1997.\n[30] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec. Strategies for pre-training graph neural networks. InInternational Conference\non Learning Representations , 2020.\n[31] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun. Gpt-gnn: Generative pre-training of graph neural networks. InProceedings of the 26th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining , 2020.\n[32] L. Huang, R. Le Bras, C. Bhagavatula, and Y. Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages 2391‚Äì2401, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.\n[33] H. Ji, P. Ke, S. Huang, F. Wei, X. Zhu, and M. Huang. Language generation with multi-hop reasoning on commonsense knowledge graph. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 725‚Äì736, Online, Nov. 2020. Association for\nComputational Linguistics.\n[34] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1601‚Äì1611, Vancouver,\nCanada, July 2017. Association for Computational Linguistics.\n[35] N. Kassner and H. Sch√ºtze. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics , pages 7811‚Äì7818, Online, July 2020. Association for Computational Linguistics.\n[36] P. Ke, H. Ji, S. Liu, X. Zhu, and M. Huang. SentiLARE: Sentiment-aware language representation learning with linguistic knowledge. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6975‚Äì6988, Online, Nov. 2020. Association for\nComputational Linguistics.\n[37] W. Kryscinski, B. McCann, C. Xiong, and R. Socher. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP) , pages 9332‚Äì9346, Online, Nov. 2020. Association for Computational\nLinguistics.\n[38] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. In\nInternational Conference on Learning Representations , 2020.\n[39] A. Lauscher, O. Majewska, L. F. R. Ribeiro, I. Gurevych, N. Rozanov, and G. Glava≈°. Common sense or world knowledge? investigating adapter-based\nknowledge injection into pretrained transformers. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction\nand Integration for Deep Learning Architectures , pages 43‚Äì49, Online, Nov. 2020. Association for Computational Linguistics.\n[40] A. Lauscher, I. Vuliƒá, E. M. Ponti, A. Korhonen, and G. Glava≈°. Specializing unsupervised pretraining models for word-level semantic similarity. In\nProceedings of the 28th International Conference on Computational Linguistics , pages 1371‚Äì1383, Barcelona, Spain (Online), Dec. 2020. International\nCommittee on Computational Linguistics.\n[41] Y. Levine, B. Lenz, O. Dagan, O. Ram, D. Padnos, O. Sharir, S. Shalev-Shwartz, A. Shashua, and Y. Shoham. SenseBERT: Driving some sense into\nBERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , July 2020.\n[42] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 7871‚Äì7880, Online, July 2020. Association for Computational Linguistics.\n[43] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K√ºttler, M. Lewis, W.-t. Yih, T. Rockt√§schel, S. Riedel, and D. Kiela. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems , volume 33, pages 9459‚Äì9474. Curran Associates, Inc., 2020.\n[44] Y. Li, P. Goel, V. Kuppur Rajendra, H. Simrat Singh, J. Francis, K. Ma, E. Nyberg, and A. Oltramari. Lexically-constrained Text Generation through\nCommonsense Knowledge Extraction and Injection. arXiv e-prints , page arXiv:2012.10813, Dec. 2020.\n[45] B. Y. Lin, X. Chen, J. Chen, and X. Ren. KagNet: Knowledge-aware graph networks for commonsense reasoning. InProceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,\npages 2829‚Äì2839, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.\n[46] B. Y. Lin, W. Zhou, M. Shen, P. Zhou, C. Bhagavatula, Y. Choi, and X. Ren. CommonGen: A constrained text generation challenge for generative\ncommonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 1823‚Äì1840, Online, Nov. 2020. Association\nfor Computational Linguistics.\n[47] X. Ling, S. Singh, and D. S. Weld. Design challenges for entity linking. Transactions of the Association for Computational Linguistics , 3:315‚Äì328, 2015.\n[48] N. F. Liu, M. Gardner, Y. Belinkov, M. E. Peters, and N. A. Smith. Linguistic knowledge and transferability of contextual representations. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 1073‚Äì1094, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n14\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\n[49] W. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang. K-BERT: enabling language representation with knowledge graph. In The\nThirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pages\n2901‚Äì2908. AAAI Press, 2020.\n[50] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 , 2019.\n[51] Y. Liu, Y. Wan, L. He, H. Peng, and P. S. Yu. KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning. arXiv\ne-prints, page arXiv:2009.12677, Sept. 2020.\n[52] R. Logan, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh. Barack‚Äôs wife hillary: Using knowledge graphs for fact-aware language modeling. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 5962‚Äì5971, Florence, Italy, July 2019. Association for\nComputational Linguistics.\n[53] S. Lv, D. Guo, J. Xu, D. Tang, N. Duan, M. Gong, L. Shou, D. Jiang, G. Cao, and S. Hu. Graph-based reasoning over heterogeneous external knowledge\nfor commonsense question answering. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, New York, USA , pages 8449‚Äì8456.\nAAAI Press, 2020.\n[54] J. McAuley, R. Pandey, and J. Leskovec. Inferring networks of substitutable and complementary products. In Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining , KDD ‚Äô15, page 785‚Äì794, New York, NY, USA, 2015. Association for Computing\nMachinery.\n[55] T. McCoy, E. Pavlick, and T. Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics , pages 3428‚Äì3448, Florence, Italy, July 2019. Association for Computational\nLinguistics.\n[56] J. Miller, K. Krauth, B. Recht, and L. Schmidt. The effect of natural distribution shift on question answering models. In Proceedings of the 37th\nInternational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research ,\npages 6905‚Äì6916. PMLR, 2020.\n[57] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A corpus and cloze evaluation for deeper\nunderstanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages 839‚Äì849, San Diego, California, June 2016. Association for Computational Linguistics.\n[58] T. Niven and H.-Y. Kao. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages 4658‚Äì4664, Florence, Italy, July 2019. Association for Computational Linguistics.\n[59] M. Ostendorff, P. Bourgonje, M. Berger, J. Moreno-Schneider, and G. Rehm. Enriching BERT with Knowledge Graph Embedding for Document\nClassification. In Proceedings of the GermEval 2019 Workshop , Erlangen, Germany, 2019.\n[60] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , pages 1532‚Äì1543, Doha, Qatar, Oct. 2014. Association for Computational Linguistics.\n[61] M. E. Peters, M. Neumann, R. Logan, R. Schwartz, V. Joshi, S. Singh, and N. A. Smith. Knowledge enhanced contextual word representations.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages 43‚Äì54, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.\n[62] F. Petroni, P. Lewis, A. Piktus, T. Rockt√§schel, Y. Wu, A. H. Miller, and S. Riedel. How context affects language models‚Äô factual predictions. In\nAutomated Knowledge Base Construction , 2020.\n[63] F. Petroni, T. Rockt√§schel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller. Language models as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2463‚Äì2473, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.\n[64] N. Poerner, U. Waltinger, and H. Sch√ºtze. E-BERT: Efficient-yet-effective entity embeddings for BERT. InFindings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 803‚Äì818, Online, Nov. 2020. Association for Computational Linguistics.\n[65] M. Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou, I. Androutsopoulos, and S. Manandhar. SemEval-2014 task 4: Aspect based sentiment\nanalysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014) , pages 27‚Äì35, Dublin, Ireland, Aug. 2014. Association\nfor Computational Linguistics.\n[66] Y. Qin, Y. Lin, R. Takanobu, Z. Liu, P. Li, H. Ji, M. Huang, M. Sun, and J. Zhou. ERICA: Improving Entity and Relation Understanding for Pre-trained\nLanguage Models via Contrastive Learning. arXiv e-prints , page arXiv:2012.15022, Dec. 2020.\n[67] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang. Pre-trained models for natural language processing: A survey.Science in China E: Technological\nSciences, 63(10):1872‚Äì1897, Oct. 2020.\n[68] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019.\n[69] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the Limits of Transfer Learning with a\nUnified Text-to-Text Transformer. arXiv e-prints , page arXiv:1910.10683, Oct. 2019.\n[70] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing , pages 2383‚Äì2392, Austin, Texas, Nov. 2016. Association for Computational\nLinguistics.\n15\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold\n[71] A. Roberts, C. Raffel, and N. Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP) , Online, Nov. 2020.\n[72] C. Rosset, C. Xiong, M. Phan, X. Song, P. Bennett, and S. Tiwary. Knowledge-Aware Language Model Pretraining. arXiv e-prints , page\narXiv:2007.00655, June 2020.\n[73] T. Safavi and D. Koutra. Relational world knowledge representation in contextual language models: A review. In To Appear in EMNLP , Hong Kong,\nChina, Nov. 2021. Association for Computational Linguistics.\n[74] M. Sap, R. L. Bras, E. Allaway, C. Bhagavatula, N. Lourie, H. Rashkin, B. Roof, N. A. Smith, and Y. Choi. ATOMIC: an atlas of machine commonsense\nfor if-then reasoning. In AAAI, pages 3027‚Äì3035. AAAI Press, 2019.\n[75] J. Shang, J. Liu, M. Jiang, X. Ren, C. R. Voss, and J. Han. Automated phrase mining from massive text corpora. IEEE Transactions on Knowledge and\nData Engineering , 30(10):1825‚Äì1837, 2018.\n[76] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Q-BERT: hessian based ultra low precision quantization of\nBERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,\n2020, pages 8815‚Äì8821. AAAI Press, 2020.\n[77] T. Shen, Y. Mao, P. He, G. Long, A. Trischler, and W. Chen. Exploiting structured knowledge in text via graph-guided representation learning. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 8980‚Äì8994, Online, Nov. 2020. Association\nfor Computational Linguistics.\n[78] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pages 1631‚Äì1642, Seattle, Washington, USA,\nOct. 2013. Association for Computational Linguistics.\n[79] R. Speer, J. Chin, and C. Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In S. P. Singh and S. Markovitch, editors,\nProceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA , pages 4444‚Äì4451. AAAI\nPress, 2017.\n[80] Y. Su, X. Han, Z. Zhang, P. Li, Z. Liu, Y. Lin, J. Zhou, and M. Sun. CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced\nPre-Trained Language Models. arXiv e-prints , page arXiv:2009.13964, Sept. 2020.\n[81] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from\ntransformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management , CIKM ‚Äô19, pages 1441‚Äì1450, New\nYork, NY, USA, 2019. ACM.\n[82] T. Sun, Y. Shao, X. Qiu, Q. Guo, Y. Hu, X. Huang, and Z. Zhang. Colake: Contextualized language and knowledge embedding. In Proceedings of the\n28th International Conference on Computational Linguistics, COLING , 2020.\n[83] Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang. ERNIE 2.0: A continual pre-training framework for language understanding. In The\nThirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pages\n8968‚Äì8975. AAAI Press, 2020.\n[84] A. Talmor, Y. Elazar, Y. Goldberg, and J. Berant. oLMpics-on what language model pre-training captures. Transactions of the Association for\nComputational Linguistics , 8:743‚Äì758, 2020.\n[85] A. Talmor, J. Herzig, N. Lourie, and J. Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. InProceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 4149‚Äì4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[86] H. Tian, C. Gao, X. Xiao, H. Liu, B. He, H. Wu, H. Wang, and F. Wu. SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4067‚Äì4076, Online, July 2020. Association for\nComputational Linguistics.\n[87] R. Trivedi, H. Dai, Y. Wang, and L. Song. Know-evolve: Deep temporal reasoning for dynamic knowledge graphs. In D. Precup and Y. W. Teh, editors,\nProceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 3462‚Äì3471,\nInternational Convention Centre, Sydney, Australia, 06‚Äì11 Aug 2017. PMLR.\n[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30.\nCurran Associates, Inc., 2017.\n[89] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li√≤, and Y. Bengio. Graph attention networks. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018.\n[90] P. Verga, H. Sun, L. Baldini Soares, and W. W. Cohen. Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge.\narXiv e-prints , page arXiv:2007.00849, July 2020.\n[91] D. Vrandeƒçiƒá and M. Kr√∂tzsch. Wikidata: A free collaborative knowledgebase. Commun. ACM, 57(10):78‚Äì85, Sept. 2014.\n[92] I. Vuliƒá, E. M. Ponti, R. Litschko, G. Glava≈°, and A. Korhonen. Probing pretrained language models for lexical semantics. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7222‚Äì7240, Online, Nov. 2020. Association for Computational\nLinguistics.\n16\nKnowledge Enhanced Pretrained Language Models: A Compreshensive Survey\n[93] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems , volume 32. Curran Associates, Inc., 2019.\n[94] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language\nunderstanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353‚Äì355,\nBrussels, Belgium, Nov. 2018. Association for Computational Linguistics.\n[95] C. Wang, X. Liu, and D. Song. Language Models are Open Knowledge Graphs. arXiv e-prints , page arXiv:2010.11967, Oct. 2020.\n[96] R. Wang, D. Tang, N. Duan, Z. Wei, X. Huang, J. ji, G. Cao, D. Jiang, and M. Zhou. K-Adapter: Infusing Knowledge into Pre-Trained Models with\nAdapters. arXiv e-prints , page arXiv:2002.01808, Feb. 2020.\n[97] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang. KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language\nRepresentation. arXiv e-prints , page arXiv:1911.06136, Nov. 2019.\n[98] X. Xie, F. Sun, Z. Liu, S. Wu, J. Gao, B. Ding, and B. Cui. Contrastive Learning for Sequential Recommendation.arXiv e-prints, page arXiv:2010.14395,\nOct. 2020.\n[99] W. Xiong, J. Du, W. Y. Wang, and V. Stoyanov. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. In 8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.\n[100] I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Matsumoto. Luke: Deep contextualized entity representations with entity-aware self-attention. In\nEMNLP, 2020.\n[101] A. Yang, Q. Wang, J. Liu, K. Liu, Y. Lyu, H. Wu, Q. She, and S. Li. Enhancing pre-trained language representations with rich knowledge for machine\nreading comprehension. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2346‚Äì2357, Florence, Italy,\nJuly 2019. Association for Computational Linguistics.\n[102] L. Yao, C. Mao, and Y. Luo. KG-BERT: BERT for Knowledge Graph Completion. arXiv e-prints , page arXiv:1909.03193, Sept. 2019.\n[103] Z.-X. Ye, Q. Chen, W. Wang, and Z.-H. Ling. Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language\nRepresentation Models. arXiv e-prints , page arXiv:1908.06725, Aug. 2019.\n[104] C. Yu, H. Zhang, Y. Song, and W. Ng. CoCoLM: COmplex COmmonsense Enhanced Language Model. arXiv e-prints , page arXiv:2012.15643, Dec.\n2020.\n[105] D. Yu, C. Zhu, Y. Yang, and M. Zeng. JAKET: Joint Pre-training of Knowledge Graph and Language Understanding. arXiv e-prints , page\narXiv:2010.00796, Oct. 2020.\n[106] D. Zhang, Z. Yuan, Y. Liu, Z. Fu, F. Zhuang, P. Wang, H. Chen, and H. Xiong. E-BERT: A Phrase and Product Knowledge Enhanced Language\nModel for E-commerce. arXiv e-prints , page arXiv:2009.02835, Sept. 2020.\n[107] Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning. Position-aware attention and supervised data improve slot filling. In Proceedings\nof the 2017 Conference on Empirical Methods in Natural Language Processing , pages 35‚Äì45, Copenhagen, Denmark, Sept. 2017. Association for\nComputational Linguistics.\n[108] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu. ERNIE: Enhanced language representation with informative entities. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics , pages 1441‚Äì1451, Florence, Italy, July 2019. Association for Computational\nLinguistics.\n[109] J. Zhou, Z. Zhang, H. Zhao, and S. Zhang. LIMIT-BERT : Linguistics informed multi-task BERT. In Findings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 4450‚Äì4461, Online, Nov. 2020. Association for Computational Linguistics.\n[110] M. Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 434‚Äì443, Sofia, Bulgaria, Aug. 2013. Association for Computational\nLinguistics.\n17"
}