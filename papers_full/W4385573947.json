{
  "title": "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks",
  "url": "https://openalex.org/W4385573947",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2904925844",
      "name": "Fatemehsadat Mireshghallah",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2250880705",
      "name": "Kartik Goyal",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A3027362765",
      "name": "Archit Uniyal",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A4208177027",
      "name": "Taylor Berg-Kirkpatrick",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2081295151",
      "name": "Reza Shokri",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4225713323",
    "https://openalex.org/W2899478368",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2778284298",
    "https://openalex.org/W3153896080",
    "https://openalex.org/W2190333735",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W3042943646",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W2950943617",
    "https://openalex.org/W3154109599",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W3106051020",
    "https://openalex.org/W2963378725",
    "https://openalex.org/W3120094169",
    "https://openalex.org/W2930926105",
    "https://openalex.org/W3018763859",
    "https://openalex.org/W4308410483",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3217478986",
    "https://openalex.org/W3048684575",
    "https://openalex.org/W3096214574",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2795435272",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3158753499",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3150395569",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3166854338",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2963956191",
    "https://openalex.org/W4287125387",
    "https://openalex.org/W3173769540"
  ],
  "abstract": "The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying potential robustness of MLMs to privacy attacks.In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM‚Äôs model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are indeed susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8332‚Äì8347\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nQuantifying Privacy Risks of Masked Language Models\nUsing Membership Inference Attacks\nFatemehsadat Mireshghallah1, Kartik Goyal2, Archit Uniyal3\nTaylor Berg-Kirkpatrick1, Reza Shokri4\n1 University of California San Diego,2 Toyota Technological Institute at Chicago (TTIC)\n3 University of Virginia,4 National University of Singapore\n[fatemeh, tberg]@ucsd.edu,\nkartikgo@ttic.edu,a.uniyal@virginia.edu,reza@comp.nus.edu.sg\nAbstract\nThe wide adoption and application of Masked\nlanguage models (MLMs) on sensitive data\n(from legal to medical) necessitates a thorough\nquantitative investigation into their privacy\nvulnerabilities. Prior attempts at measuring\nleakage of MLMs via membership inference\nattacks have been inconclusive, implying\npotential robustness of MLMs to privacy at-\ntacks. In this work, we posit that prior attempts\nwere inconclusive because they based their\nattack solely on the MLM‚Äôs model score. We\ndevise a stronger membership inference attack\nbased on likelihood ratio hypothesis testing\nthat involves an additional reference MLM to\nmore accurately quantify the privacy risks of\nmemorization in MLMs. We show that masked\nlanguage models are indeed susceptible to\nlikelihood ratio membership inference attacks:\nOur empirical results, on models trained on\nmedical notes, show that our attack improves\nthe AUC of prior membership inference attacks\nfrom 0.66 to an alarmingly high0.90 level.\n1 Introduction\nBERT-based encoders with Masked Language Mod-\neling (MLM) Objectives (Devlin et al., 2018; Liu\net al., 2019) have become models of choice for use\nas pre-trained models for various Natural Language\nProcessing (NLP) classification tasks (Wang et al.,\n2018; Zhang et al., 2019; Rogers et al., 2020) and\nhave been applied to diverse domains such as dis-\nease diagnosis, insurance analysis on financial data,\nsentiment analysis for improved user experience,\netc (Yang et al., 2020; Gu et al., 2021; Lee et al.,\n2020). Given the sensitivity of the data used to train\nthese models, it is crucial to conceive a framework\nto systematically evaluate the leakage of training\ndata from these models (Shokri, 2022; Carlini et al.,\n2019; Murakonda and Shokri, 2020; Mireshghallah\net al., 2020), and limit the leakage. The conventional\nway to measure the leakage of training data from\nmachine learning models is by performing mem-\nbership inference attacks (Shokri et al., 2017; Nasr\net al., 2021), in which the attacker tries to determine\nwhether a given sample was part of the training data\nof the target model or not. These attacks expose the\nextent of memorization by the model at the level of\nindividual samples. Prior attempts at performing\nmembership inference and reconstruction attacks on\nmasked language models have either been inconclu-\nsive (Lehman et al., 2021), or have (wrongly) con-\ncluded that memorization of sensitive data in MLMs\nis very limited and these models are more private\nthan their generative counterparts (e.g., autoregres-\nsive language models) (Vakili and Dalianis, 2021;\nJagannatha et al., 2021; Nakamura et al., 2020).\nWe hypothesize that prior MLM attacks have\nbeen inconclusive because they rely solely on the\ntarget model‚Äôs (model under attack) loss on each\nindividual sample as a proxy for how well the model\nhas memorized that sample. If the loss is lower than\na threshold, the sample is predicted to be a member\nof the training set. However, the target model‚Äôs\nloss includes confounding factors of variation like\nthe intrinsic complexity of the sample ‚Äì and thus\nprovides a limited discriminative signal for mem-\nbership prediction. This scheme has either a high\nfalse-negative rate (with a conservative threshold)\n‚Äì classifying many hard-to-fit samples from the\ntraining set as non-members, or a high false-positive\nrate (with a generous threshold) ‚Äì failing to identify\neasy-to-fit samples that are not in the training set.\nReference-based likelihood ratio attacks, on the\nother hand, when applied to certain probabilistic\ngraphical models and classifiers, have been shown\nto alleviate this problem and more accurately dis-\ntinguish members from non-members (Murakonda\net al., 2021; Ye et al., 2021). In such attacks, instead\nof the loss of the model under attack, we look at the\nratio of the likelihood of the sample under the target\nmodel and a reference model trained on samples\nfrom the underlying population distribution that\ngenerates the training data for the target model.\n8332\nThis ratio recalibrates the test statistic to explain\naway spurious variation in model‚Äôs loss for different\nsamples due to the intrinsic complexity of the\nsamples. Unlike most other models (e.g., generative\nmodels), however, computing the likelihood of\nMLMs is not straightforward. In this paper, we\npropose a principled framework for measuring\ninformation leakage of MLMs through likelihood\nratio-based membership inference attacks and\nperform an extensive analysis of memorization in\nsuch models. To compute the likelihood ratio of the\nsamples under the target and the reference MLMs,\nwe view the MLMs as energy-based probabilistic\nmodels (Goyal et al., 2022) over the sequences.\nThis enables us to perform powerful inference\nattacks on conventionally non-probabilistic models\nlike masked language models.\nWe evaluate our proposed attack on a suite\nof masked clinical language models, follow-\ning (Lehman et al., 2021). We compare our attack\nwith the baseline from the prior work that relies\nsolely on the loss of the target model (Yeom et al.,\n2018; Song and Raghunathan, 2020; Jagannatha\net al., 2021). We empirically show that our\nattack improves the AUC from0.66 to 0.90 on the\nClinicalBERT-Base model, and achieves a true\npositive rate (recall) of 79.2% (for a false positive\nrate of 10%), which is a substantial improvement\nover the baseline with 15.6% recall. This shows\nthat, contrary to prior results, masked language\nmodels are significantly susceptible to attacks\nexploiting the leakage of their training data. In low\nerror regions (at 1% false positive rate)our attack\nis 51√ómore powerful than the prior work.\nWe also present analyses of the effect of the\nsize of the model, the length of the samples, and\nthe choice of the reference model on the success\nof the attack. Finally, we attempt to identify\nfeatures of samples that are more exposed (attack\nis more successful on), and observe that samples\nwith multiple non-alphanumeric symbols (like\npunctuation) are more prone to being memorized.\nWe provide instructions on how to request access\nto the data and code in Appendix A.2.1.\n2 Membership Inference Attacks\nIn this section, we first formally describe the mem-\nbership inference attack, how it can be conducted\nusing likelihood ratio tests and how we apply the\ntest for masked language models (MLMs) which do\nnot explicitly offer an easy-to-compute probability\ndistribution over sequences. Finally, we describe all\nthe steps in our attack, as summarized in Figure 1.\n2.1 Problem Formulation\nLet MŒ∏ denote a model with parameters Œ∏ that\nhave been trained on data setD, sampled from the\ngeneral population distribution p. Our goal is to\nquantify the privacy risks of releasing MŒ∏ for the\nmembers of training setD.\nWe consider an adversary who has access to the\ntarget model MŒ∏. We assume this adversary can\ntrain a (reference) modelMŒ∏R with parameters Œ∏R\non independently sampled data from the general\npopulation p. In a Membership Inference Attack\n(MIA), the objective of the adversary is to create\na decision rule that determines whether a given\nsample s was used for training MŒ∏. To test the\nadversary, we perform the following experiment.\nWe sample a datapoint sfrom either the general\npopulation or the training data with a0.5 probability,\nand challenge the adversary to tell if sis selected\nfrom the training set (it is a member) or not (it is a\nnon-member) (Murakonda et al., 2021). The pre-\ncision of the membership inference attack indicates\nthe degree of information leakage from the target\nmodel about the members of its training set. We\nmeasure the adversary‚Äôs success using two metrics:\n(1) the adversary‚Äôs power (the true positive rate),\nand (2) the adversary‚Äôs error (the false positive rate).\n2.2 Likelihood Ratio Test\nBefore discussing our proposed attack for MLMs\nin the next section, we summarize the likelihood\nratio test here which forms the core of our approach.\nA likelihood ratio test distinguishes between a null\nhypothesis and an alternative hypothesis via a test\nstatistic based on the ratio of likelihoods under the\ntwo hypotheses. Prior work demonstrated an MIA\nattack based on the likelihood ratio to be optimal\nfor probabilistic graphical models (Bayesian net-\nworks) (Murakonda et al., 2021). Given a samples\nfrom the training data of the target model, the adver-\nsary aims at distinguishing between two hypotheses:\n1. Null hypothesis (Hout): The target sample s\nis drawn from the general population p,\nindependently from the training setD.\n2. Alternative hypothesis ( Hin): The target\nsample s is drawn from the target model‚Äôs\ntraining set D.\nThe goal of hypothesis testing is to find whether\nthere is enough evidence to reject Hout in favor\nof Hin. We use a likelihood ratio for this purpose\n8333\nTraining Data (ùê∑~ùëù)\nEnergy LM for ùúÉ\nEnergy LM forùúÉ!General Data Distribution (ùëù)\nTarget Model M!\nReference Model ùëÄ!!\nPr(ùë†;ùúÉ)\nPr(ùë†;ùúÉ!) Likelihood Ratio Test\nMember (ùêª!\")\nNon-member (ùêª\"#$)\nTarget Sample (ùë†)\nMr. Smith has lung Cancer.\nùë†\nAttack Procedure\nùêøùë†=logP(ùë†;ùúÉ\")P(ùë†;ùúÉ)<t\nFigure 1: Overview of our attack: to determine whether a target samplesis a member of the training data (D‚àºp)\nof the target model (MŒ∏), we feed it to the energy function formulation ofMŒ∏ so that we can compute Pr(s;MŒ∏),\nthe probability of sunder MŒ∏. We do the same with a reference modelMŒ∏R which is trained on a disjoint data set\nfrom the same distribution as the training data. Then, we compute likelihood ratioL(s), and based on this ratio and\na given test thresholdt, we decide ifsis a member ofD(Hin) or not (Hout).\nwhich involves comparison of the likelihood of the\ntarget sample under the settings for Hout and Hin\nrespectively. For Hin, we already have access to\nthe target model, which is parameterized byŒ∏and\ntrained onD. For Hout, we require access to a model\ntrained on the general population. As mentioned ear-\nlier, the adversary has access to a reference model\nparameterized by Œ∏R. Therefore, the likelihood\nratio test is characterized by the following statistic:\nL(s)=log\n(p(s; Œ∏R)\np(s; Œ∏)\n)\n(1)\nThe Likelihood Ratio (LR) test is a comparison\nof the log-likelihood ratio statistic L(s) with a\nthreshold t. If L(s) ‚â§t, then the adversary rejects\nHout (decides in favor of membership of s‚ààD);\notherwise the adversary fails to reject Hout. We\ndiscuss the details of selecting the threshold and\nquantifying the attack‚Äôs success in Section 2.4.\n2.3 Likelihood Ratio Test for MLMs\nPerforming a likelihood ratio test with masked\nlanguage models is difficult because these models\ndo not explicitly define an easy-to-compute\nprobability distribution over natural language\nsequences. Following prior work (Goyal et al.,\n2022), we alternatively view pre-trained MLMs\nas energy-based probability distributions on se-\nquences, allowing us to directly apply the likelihood\nratio formalism. An energy-based sequence model\ndefines the probability distribution over the space\nof possible sequencesSas:\np(s;Œ∏)= e‚àíE(s;Œ∏)\nZŒ∏\n,\nwhere E(s; Œ∏) refers to the scalar energy of\na sequence s that is parametrized by Œ∏, and\nZŒ∏ = ‚àë\ns‚Ä≤‚ààS e‚àíE(s‚Ä≤;Œ∏) denotes the intractable\nnoramlization constant. Under this framework, the\nlikelihood ratio test statistic (Eq. 1) is:\nL(s)=log\n(p(s; Œ∏R)\np(s; Œ∏)\n)\n=log\n(\ne‚àíE(s; Œ∏R)\nZŒ∏R\n)\n‚àí\n(\nloge‚àíE(s; Œ∏)\nZŒ∏\n)\n=‚àíE(s; Œ∏R)‚àílog(ZŒ∏R)+E(s; Œ∏)+log(ZŒ∏)\n=E(s; Œ∏)‚àíE(s; Œ∏R)+constant\nAbove, we make use of the fact that for two fixed\nmodels (i.e., target model Œ∏, and reference model\nŒ∏R), the intractable term log(ZŒ∏) ‚àílog(ZŒ∏R) is\na global constant and can be ignored in the test.\nTherefore, computation of the test statistic only\nrelies on the difference between the energy values\nassigned to sample sby the target model MŒ∏, and\nthe reference modelMŒ∏R.\nIn practice, we cast a traditional MLM as an\nenergy-based language model using a slightly\ndifferent parameterization than explored by Goyal\net al. (2022). Since the training of most MLMs (in-\ncluding the ones we attack in experiments) involves\nmasking 15% of the tokens in a training sequence,\nwe define our energy parameterization on these\n15% chunks. Specifically, for a sequence of length\nT, and the subset sizel= ‚åà0.15√óT‚åâ, we consider\ncomputing the energy with the setCconsisting of\nall\n(T\nl\n)\ncombinations of masking patterns.\nE(s; Œ∏)= ‚àí1\n|C|\n‚àë\nI‚ààC\n‚àë\ni‚ààI\nlog\n(\npmlm(si|s\\I; Œ∏)\n)\n(2)\nwhere s\\I is the sequenceswith the lpositions in\nImasked. Computing this energy, which involves\nrunning |C|=\n(T\nl\n)\nforward passes of the MLM, is\nexpensive. Hence, we further approximate this\nparametrization by summing up over K random\nmasking patterns whereK‚â™|C|.\n8334\nPopulation Data distribution ofùêø(ùë•)over population\nChoose threshold tso that FPR is ùõº, e.g.10%\nLR Test for (s,t) \nùë•\nùë†\nAll  threshold values t corresponding to 0‚â§ùõº‚â§1Target Samples (S={s|s~ùëù,Ps‚ààùê∑=0.5}) \nùêø(ùë•)\nFalse Positive Rate (ùêπùëÉùëÖ)\nTrue Positive Rate (ùëáùëÉùëÖ)\n(a) Selecting thresholdt\nPopulation Data (ùëÖ~ùëù) distribution ofùêø(ùëü)over ùëÖ\nChoose threshold tso that FPR is ùõº, e.g.10%\nLR Test for (s,t) \nùëü\nùë†\nAll  threshold values t corresponding to 0‚â§ùõº‚â§1Target Samples (S={s|s~ùëù,Ps‚ààùê∑=0.5}) \nùêø(ùëü)\nFalse Positive Rate (ùêπùëÉùëÖ)\nTrue Positive Rate (ùëáùëÉùëÖ)\n(b) Plotting the ROC curve\nFigure 2: (a) Selecting a threshold for the attack using\npopulation data and (b) plotting the ROC curve to show\nthe true-positive vs. false-positive rate trade-off, given\ndifferent thresholds.\n2.4 Quantifying the Privacy Risk\nGiven the form of the likelihood ratio test statistic\n(Eq. 2) and energy function formulation for MLM\nlikelihood (Eq. 2), we conduct the attack as follows\n(shown in Figure 1):\n1. Given a sampleswhose membership we want\nto determine, we calculate its energyE(s; Œ∏)\nunder the model under attack (MŒ∏) using Eq. 2.\nWe calculate the energyE(s; Œ∏R) under the ref-\nerence model. Using Eq. 1, we compute the test\nstatistic L(s) by subtracting the two energies.\n2. We compare L(s) to a threshold t, and if\nL(s) ‚â§t, we reject the null hypothesis (Hout)\nand mark the sample as a member. Otherwise,\nwe mark it as a non-member.\nChoosing the threshold. The threshold determines\nthe (false positive) error the adversary is willing to\ntolerate in the membership inference attack. Thus,\nfor determining the threshold t, we select a false\npositive rate Œ±, and empirically compute tas the\ncorresponding percentile of the likelihood ratio\nstatistic over random samples from the underlying\ndistribution. This process is visualized in Figure 2a.\nWe empirically estimate the distribution of the test\nstatistic L(x) using all the sequencesxdrawn from\nthe general population distribution. This yields\nthe distribution ofLunder the null hypothesis. We\nthen select the threshold such that the tolerance of\nattack‚Äôs error i.e. the rate at which attack falsely\nclassifies the population data as ‚Äúmembers‚Äù isŒ±%.\nQuantifying the Privacy Risk. The attacker‚Äôs\nsuccess (i.e. the privacy loss of the model) can be\nquantified using the relation between the attack‚Äôs\npower (the true positive rate) versus its error (the\nfalse positive rate). Higher power for lower errors\nindicates larger privacy loss. To compare two\nattack algorithms (e.g., our method versus the target\nmodel loss based methods), we can compute their\npower for all different error values, which can be\nillustrated in an ROC curve (as in Figure 2b and\nFigure 4). This enables a complete comparison\nbetween two attack algorithms. The Area Under\nthe Curve (AUC) metric for each attack provides\nan overall threshold independent evaluation of the\nprivacy loss under each attack.\n3 Experimental Setup\nWe conduct our experiments using the pre-\nprocessed data, and pre-trained models provided\nby Lehman et al. (2021). We use this medical-based\nsetup as medical notes are sensitive and leakage of\nmodels trained on notes can cause privacy breaches.\nIn this section, we briefly explain the details of our\nexperimental setup. Appendix A.2 provides more\ndetails. Table 1 provides a summary.\n3.1 Datasets\nWe run our attack on two sets of target samples, in\nboth of which the ‚Äúmembers‚Äù portion is sampled\nfrom the training set ( D) of our target models,\nwhich is the MIMIC-III dataset. The non-members,\nhowever, are different. For the results shown under\n‚ÄúMIMIC‚Äù, the non-members are a held-out subset\nof the MIMIC data that was not used in training.\nFor i2b2, the non-members are from a different (but\nsimilar) dataset, i2b2. Below we elaborate on each\nof these datasets (full detail in Appendix A.2.2).\nBoth the datasets require a license for access, so we\ncannot show examples of the training data.\nMIMIC-III. The target models we attack are\ntrained on the pseudo re-identified MIMIC-III\nnotes which consist of1,247,291 electronic health\nrecords (EHR) of46,520 patients.\ni2b2. This dataset was curated for the i2b2\nde-identification of protected health information\n(PHI) challenge in 2014 (Stubbs and √ñzlem\nUzuner, 2015). We use this dataset as a secondary\nnon-member dataset since it is similar in domain\nto MIMIC-III (both are medical notes), is larger in\nterms of size than the held-out MIMIC-III set, and\nhas not been used as training data for our models.\n3.2 Models\nTarget Models. We perform our attack on 4\ndifferent pre-trained ClinicalBERT models, that are\nall trained on MIMIC-III, but with different training\n8335\nTable 1: Summary of model and baseline notations used in the results.\nNotation ExplanationModels\nBase ClinicalBERT-base target model, trained for 300k iterations w/ sequence length 128 and 100k iterations w/ sequence length 512.Base++ ClinicalBERT++ target model, same as the Base model but trained for longer: trained for 1M iterations w/ a sequence length of 128.Large ClinicalBERT-large target model, trained for 300k iterations w/ sequence length 128 and 100k iterations w/ sequence length 512.Large++ ClinicalBERT-large++ target model, same as the Large model but trained for longer: trained for 1M iterations w/ a sequence length of 128.Methods\n(A) w/¬µthresh. Baseline with threshold set to be the mean of training sample losses (¬µ) (for reporting threshold-dependant metrics)(A) w/ Pop. thresh. Baseline with threshold set so that there is10%false positive rate (for reporting threshold-dependant metrics)(B) w/ Pop. thresh. Our method with threshold set so that there is10%false positive rate ( for reporting threshold-dependant metrics)\nprocedures, summarized in Table 1 under Models.\nReference Models. We use Pubmed-BERT 1\ntrained on pre-processed PubMed texts containing\naround 4000M words extracted from PubMed\nASCII code version (Peng et al., 2019) as our\nmain domain-specific reference model, since its\ntraining data is similar to MIMIC-III in terms of\ndomain, however, it does not include MIMIC-III\ntraining data. We also use the standard pre-trained\nbert-base-uncased as a general-domain\nreference model for ablating our attack.\n3.3 Baselines\nWe compare our results with a popular prior method,\nwhich uses the loss of the target model as a signal to\npredict membership (Yeom et al., 2018; Jayaraman\net al., 2021; Ye et al., 2021). We show this baseline\nas Model lossin our tables. This baseline could have\ntwo variations, based on the way its threshold is cho-\nsen: (1) ¬µthreshold (Jagannatha et al., 2021), which\nassumes access to the mean of the training data loss,\n¬µand uses it as the threshold for the attack, and (2)\npopulation threshold (pop. thresh.) which calcu-\nlates the loss on a population set of samples (samples\nthat were not used in training but are similar to train-\ning data), and then selects the threshold that would\nresult in a10% false positive rate on that population.\n3.4 Metrics\nArea Under the ROC Curve (AUC). The ROC\ncurve is a plot of power (true positive rate) versus\nerror (false positive rate), measured across different\nthresholds t, which captures the trade-off between\npower and error. Thus, the area under the ROC\ncurve (AUC) is a single, threshold-independent\nmetric for measuring the strength of the attack. Fig-\nure 2b shows how we obtain the ROC curve. AUC\n= 1implies that the attacker can correctly classify\nall target samples as members or non-members.\nPrecision and Recall. We setŒ±=10% as the false\npositive rate and choose the threshold accordingly,\nas shown in Fig. 2a. For precision, we measure\nthe percentage of samples correctly inferred as\n1bionlp/bluebert_pubmed_uncased_L-12_\nH-768_A-12\n(a) Likelihood RatioL(s) Histogram\n(b) Target Model Loss Histogram\nFigure 3: (a) likelihood ratio histogram for training\ndata members and non-members. (b) loss histogram for\ntraining data members and non-members. The blue lines\nin the two figures correspond to the same target sample\n(which is a random member of training-set). The red line\nis is the threshold at Œ±= 10%false positive rate. The\nthreshold from our attack (a) is correctly able to label the\ntest sample as a training-set member but the thresholds\nfrom the baseline attack (b) fails to do so.\nmembers of the training set out of the total number\nof target samples inferred as members by the attack.\nFor recall, we measure the percentage of samples\ncorrectly inferred as members of the training set\nout of the total number of target samples that are\nactually members of the training set.\n4 Results\nIn this section, we discuss our experimental results\nand main observations. First, we explore the overall\nperformance improvement of our approach over\nbaselines. Later, we analyze the effectiveness of\nour approach across several factors of variation that\nhave an effect on the leakage of the model. (e.g.\nlength of samples, model size, including names\n8336\nTable 2: Overview of our attack on the ClinicalBERT-\nBase model, using PubMed-BERT as the reference.\nSample-level attack attempts to determine membership\nof a single sample, whereas patient-level determines\nmembership of a patient based on all their notes. The\nMIMIC and i2b2 columns determine which dataset was\nused as non-members in the target sample pool.\nSample-level Patient-level\nNon-members MIMIC i2b2 MIMIC i2b2\nAUC.\n(A) Model loss 0.662 0.812 0.915 1.000\n(B) Ours 0.900 0.881 0.992 1.000\nPrec.\n(A) w/¬µthresh. 61.5 77.6 87.5 100.0\n(A) w/ Pop. thresh. 61.2 79.6 87.5 92.5\n(B) w/ Pop. thresh. 88.9 87.5 93.4 92.5\nRec.\n(A) w/¬µthresh 55.7 55.8 49.5 49.5\n(A) w/ Pop. thresh. 15.6 39.0 49.5 100.0\n(B) w/ Pop. thresh. 79.2 69.9 100.0 100.0\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nOurs (Liklihood ratio)\nBaseline (Model loss)\nFigure 4: The ROC curve of sample-level attack on\nClinical-BERT with MIMIC used as non-member.\nGreen line shows our attack and the red line shows the\nbaseline loss-based attack. The blue dashed line shows\nAUC=0.5 (random guess). This figure corresponds to\nthe results presented in the first column of Table 2.\netc.) Finally, we explore correlations between\nsamples that are deemed to be exposed by our\napproach. We provide further studies and ablations\non choosing a lower false-positive rate of1%, using\ndifferent energy formulation and changing target\nsequence lengths in Appendix sections A.3.1, A.3.2,\nand A.3.3, respectively.\n4.1 Comparison with Baseline\nTable 2 shows the metrics for our attack and the\nbaseline‚Äôs on both sample and patient level, with\nheld-out MIMIC-III and i2b2 medical notes used\nas non-member samples (Figure 4 shows the ROC\ncurve). The table shows that our method signifi-\ncantly outperforms the target model loss-based base-\nlines (Jagannatha et al., 2021; Yeom et al., 2018),\nwhich threshold the loss of the target model based\non either the mean of the training samples‚Äô loss\n(¬µ), or the population samples‚Äô loss. Our attack‚Äôs\nimprovement over the baselines is more apparent in\nthe case where both the members and non-members\nTable 3: Effect of target sample length: Sample-level\nattack on the ClinicalBERT-Base model, using PubMed-\nBERT as the reference. The MIMIC and i2b2 columns\ndetermine which dataset was used as non-members in\nthe target sample pool. Short and long show a break\ndown of the length of target samples.\nShort Long\nNon-members MIMIC i2b2 MIMIC i2b2\nAUC.\n(A) Model loss 0.516 0.756 0.662 0.812\n(B) Ours 0.830 0.845 0.900 0.881\nPrec.\n(A) w/¬µthresh. 50.9 70.0 61.5 77.6\n(A) w/ Pop. thresh. 42.0 72.5 61.2 79.6\n(B) w/ Pop. thresh. 87.3 86.3 88.9 87.5\nRec.\n(A) w/¬µthresh. 55.3 55.3 55.7 55.8\n(A) w/ Pop. thresh. 7.2 26.3 15.6 39.0\n(B) w/ Pop. thresh. 68.2 62.9 79.2 69.9\nare from MIMIC-III. This case is harder for the\nbaselines since members and non-members are\nmuch more similar and harder to distinguish if we\nonly look at the loss of the target model. Our attack,\nhowever, is successful due to the use of a reference,\nwhich helps magnify the gap in the behavior of the\ntarget model towards members and non-members,\nthereby teasing apart similar samples.\nWe can also see that in terms of precision/recall\ntrade-off, our attack has a consistently higher recall,\nwith an average higher precision. Population loss\nbased thresholding ((A) w/ Pop. thresh.) has the\nlowest recall of 15.6%, which is due to members\nand non-members achieving similar losses from\nthe target model due to their similarity. This is also\nshown in Figure 3b. In Figure 3a, however, we\nsee a distinct separation between the member and\nnon-member histogram distributions when we use\nthe ratio statistic L(s) as the test criterion for our\nattack. This results in the estimation of a useful\nthreshold that correctly classifies the blue line sam-\nple as a member, as opposed to using only the target\nmodel loss (Figure 3b). Finally, we observe that all\nthe metrics have higher values on the patient-level\nattack, compared to sample-level, for both our\nattack and the baselines. This is due to the higher\ngranularity of the patient level attack, as it makes the\ndecision based onan aggregateof multiple samples.\n4.2 Effect of Sample Length and Model Size\nTables 3 and 4 show the metrics for our attack and\nthe baseline broken down based on the length of the\ntarget sample, and the size and training epochs of\nthe target model, respectively. In Table 3, the target\nmodel is same as that of Table 2, ClinicalBERT-base.\nShort samples are those that have between 10 to\n20 tokens, and long samples have20 to 60 tokens.\n8337\nTable 4: Effect of model size and training: Sample-level\nattack on the four different ClinicalBERT models, using\nPubMed-BERT as the reference and the MIMIC data\nas non-members. Base++ (Large++) is same as Base\n(Large), but trained for more epochs.\nTarget Model Base Base++ Large Large++\nAUC.\n(A) Model loss 0.662 0.656 0.679 0.700\n(B) Ours 0.900 0.894 0.904 0.905\nPrec.\n(A) w/¬µthresh. 61.5 61.0 62.4 64.9\n(A) w/ Pop. thresh. 61.2 61.2 63.9 69.1\n(B) w/ Pop. thresh. 88.9 88.8 88.9 88.9\nRec.\n(A) w/¬µthresh. 55.7 55.8 56.4 56.1\n(A) w/ Pop. thresh. 15.6 15.6 17.6 22.2\n(B) w/ Pop. thresh. 79.2 78.5 79.2 79.3\nWe can see that both the baseline and our attacks\nshow more leakage for long sentences than they\ndo for short sequences, which could be due to the\nlonger sentences being more unique and thus being\nmore likely to provide a discriminative signal for a\nsequence-level decision. Table 4 shows the attacks\nmounted on the four models from Table 1. We\nsee that leakage on all the models is very similar,\nhowever, the AUC on Large++ is consistently\nhigher than on Base, which hints at the observation\nmade by (Carlini et al., 2021b) that larger models\ntend to have a higher capacity for memorization.\n4.3 Effect of Changing the Reference Model\nTable 5 studies how changing the reference model\nwould affect the success of the attack. Here,Pubmed\nis the reference model that is used in the previous\nexperiments, and BERT-base is Huggingface‚Äôs\npre-trained BERT. We observe that the attack using\nBERT-base performs well, but is worse than using\nPubmed, especially in terms of recall (true positive\nrate). The main reason behind this is the domain\noverlap between the Pubmed reference model and\nthe model under attack. An ideal reference model\nfor this attack would be trained on data from a\ndomain that is similar to that of the target model‚Äôs\ntraining set so as to better characterize the intrinsic\ncomplexity of the samples. On the other hand, a ref-\nerence model trained on a different data distribution\n(in this case Wikipedia) would give the same score\nto easy and difficult samples, thereby decreasing\nthe true positive rate (recall), as shown in the table.\n4.3.1 Effect of Inserting Names\nTable 6 shows results for attacking the name inser-\ntion model (Lehman et al., 2021), shown as Base-b,\nwhere the patient‚Äôs first and last name are prepended\nto each training sample. We see that our attack‚Äôs\nperformance is better on the name-insertion model,\nTable 5: Effect of reference model: Sample-level attacks\non ClinicalBERT-Base model, using PubMed-BERT\nand standard bert-base-uncased as the reference and\nMIMIC data as non-member.\nBase Large++\nReference Model pubmed bert pubmed bert\nAUC.\n(A) Model loss 0.662 0.662 0.700 0.700\n(B) Ours 0.900 0.883 0.905 0.889\nPrec.\n(A) w/¬µthresh. 61.5 61.5 64.9 64.9\n(A) w/ Pop. thresh. 61.2 61.2 69.1 69.1\n(B) w/ Pop. thresh. 88.9 87.8 88.9 88.0\nRec.\n(A) w/¬µthresh. 55.7 55.7 56.1 56.1\n(A) w/ Pop. thresh. 15.6 15.6 22.2 22.2\n(B) w/ Pop. thresh. 79.2 71.5 79.3 72.6\nTable 6: Effect of inserting names: Sample and\nPatient-level attacks on ClinicalBERT-Base and Base-b\n(name insertion) model, using PubMed-BERT as the\nreference and MIMIC data as non-member. We study\nthe effect that inserting names into all training samples\nhas on the leakage of the model.\nSample-level Patient-level\nTarget model Base Base-b Base Base-b\nAUC.\n(A) Model loss 0.662 0.561 0.915 0.953\n(B) Ours 0.900 0.960 0.992 1.000\nPrec.\n(A) w/¬µthresh. 61.5 53.0 87.5 100.0\n(A) w/ Pop. thresh. 61.2 44.1 87.5 91.1\n(B) w/ Pop. thresh. 88.9 90.2 93.4 92.5\nRec.\n(A) w/¬µthresh. 55.7 54.0 49.5 48.5\n(A) w/ Pop. thresh. 15.6 7.8 49.5 82.8\n(B) w/ Pop. thresh. 79.2 91.3 100.0 100.0\ncompared to the base model, whereas the baseline at-\ntack performs worse (in the sample-level scenario).\nWe hypothesize that this is due to the ‚Äúdifficulty‚Äù\nof the samples. Adding names to the beginning\nof each sample actually increases the entropy of\nthe dataset overall, since in most cases they don‚Äôt\nhave a direct relation with the rest of the sentence\n(except for very few sentences that directly state a\nperson‚Äôs disease), therefore they might as well be\nrandom. This makes these sentences more difficult\nand harder to learn, as there is no easy pattern.\nHence, on average, these sentences have higher loss\nvalues (2.14 for name inserted samples, vs. 1.61 for\nregular samples). However, for the non-members,\nsince they don‚Äôt have names attached to them, the\naverage loss is the same (the 10% FPR threshold\nis 1.32), and that is why the attack performs poorly\non these samples, as most of the members get\nclassified as non-members. For our attack, since\nwe use the reference, we are able to tease apart such\nhard samples as they are extremely less likely given\nthe reference than they are given the target model.\n8338\n4.4 Correlations between Memorized Samples\nTo evaluate whether there are correlations between\nsamples that have high leakage based on our attack\n(i.e. training samples that are successfully detected\nas members), we conduct an experiment. In this\nexperiment, we create a new train and test dataset,\nby subsampling the main dataset and selecting\n5505 and 7461 samples, respectively. We label the\ntraining and test samples based on whether they are\nexposed or not, i.e. whether the attack successfully\ndetects them as training samples or not, and get\n2519 and 3283 samples labeled as ‚Äúmemorized‚Äù,\nfor the train and test set. Since our goal is to see if\nwe can find correlations between the memorized\nsamples of the training set and use those to predict\nmemorization on our test set, we create features\nfor each sample, and then use those features with\nthe labels to create a simple logistic regression\nclassifier that predicts memorization.\nTable 7 shows these results in terms of precision\nand recall for predicting if a sample is ‚Äúmemorized\"\nor not, with different sets of features. The first 4\nrows correspond to individual handcrafted feature\nsets: (A) the number of digits in the sample, (B)\nlength of a sample (in tokens), (C) the number of\nnon-alphanumeric characters (this would be char-\nacters like ‚Äô*‚Äô, ‚Äô-‚Äô, etc.). (D) corresponds to feature\nsets that are obtained by encoding the tokenized\nsample by the frequency of each of its tokens, and\nthen taking the 3 least frequent tokens‚Äô frequencies\nas features (the frequency comes from a frequency\ndictionary built on the training set). We can see\nthat among the hand-crafted features, (C) is most\nindicative, as it counts the characters that are more\nout-of-distribution and are possibly not determined\nby grammatical rules or consistent patterns. (C) and\n(D) concatenated together perform slightly better\nthan (C) alone, which could hint at the effect fre-\nquency of tokens and how common they are could\nhave on memorization. We also get a small improve-\nment over these by concatenating (B), (C), and (D),\nwhich shows the length has a slight correlation too.\n5 Related Work\nPrior work on measuring memorization and leakage\nin machine learning models can be classified into\ntwo main categories: (1) membership inference\nattacks and (2) training data extraction attacks.\nMembership inference. Membership Inference\nAttacks (MIA) try to determine whether or not\na target sample was used in training a target\nTable 7: Analysis of correlations between samples\nthat are leaked through our attack. We want to see\nwhat features are shared among all leaked samples\nby extracting a list of possible features and training\na simple logistic regression model on a subset of the\noriginal training data (D), and then testing it on another\nsubset. The logistic regression model tries to predict\nwhether a sample would be leaked or not (based on\nwhether our model has classified it as a member or not).\nThe precision and recall here are those of the logistic\nregression model, for predicting leaked training samples.\nFeatures Train Test\nPrec. Rec. Prec. Rec.\n(A) #Digits 0.0 0.0 0 .0 0 .0\n(B) Seq. Len 0.0 0.0 0 .0 0 .0\n(C) #Non-alphanumeric 71.246.6 69 .2 47 .5\n(D) 3 Least Frequent 68.9 40.5 63 .8 39 .2\n(C) & (D) 73.9 58.8 71 .1 57 .8\n(B) & (C) & (D) 74.3 61.3 72 .1 61 .3\n(A) & (B) & (C) & (D) 74.361.3 72 .1 61 .3\nmodel (Shokri et al., 2017; Yeom et al., 2018).\nThese attacks can be seen as privacy risk analysis\ntools (Murakonda and Shokri, 2020; Nasr et al.,\n2021; Kandpal et al., 2022), which help reveal how\nmuch the model has memorized the individual sam-\nples in its training set, and what the risk of individual\nusers is (Nasr et al., 2019; Long et al., 2017; Salem\net al., 2018; Ye et al., 2021; Carlini et al., 2021a).\nA group of these attacks rely on behavior of shadow\nmodels to determine the membership of given\nsamples (Jayaraman et al., 2021; Shokri et al., 2017).\nSong and Shmatikov mounts such an attack on\nLSTM-based text-generation models, Mahloujifar\net al. mounts one on word embedding, Hisamoto\net al. applies it to machine translation and more\nrecently, Shejwalkar et al. mounts it on transformer-\nbased NLP classification models. Mounting such\nattacks is usually costly, as their success relies\nupon training multiple shadow models on different\npartitionings of shadow data, and access to adequate\nshadow data for training such models.\nAnother group of MIAs relies solely on the\nloss value of the target sample, under the target\nmodel, and thresholds this loss to determine\nmembership (Jagannatha et al., 2021; Yeom et al.,\n2018). Song and Raghunathan mount such an\nattack on word embedding, where they try to infer\nif given samples were used in training different\nembedding models. Jagannatha et al. (2021), which\nis the work closest to ours, uses a thresholding\nloss-based attack to infer membership on MLMs.\nOur approach instead incorporates a reference\nmodel by using an energy-based formulation to\n8339\nmount a likelihood ratio based attack and achieves\nhigher AUC as shown in the results.\nTraining data extraction. Training data extrac-\ntion quantifies the risk of extracting training data\nby probing a trained language model (Salem et al.,\n2020; Carlini et al., 2019; Zanella-B√©guelin et al.,\n2020; Carlini et al., 2021b, 2022; Nakamura et al.,\n2020). One such prominent attacks on NLP models\nis that of Carlini et al. (2021b), where they take\nmore than half a million samples from different\nGPT-2 models, sift through the samples using a\nmembership inference method to find samples that\nare most likely to have been memorized. Lehman\net al. (2021) mount the same data extraction attack\non MLMs, but their results are inconclusive as\nto how much MLMs memorize samples. They\nalso mount other types of attacks, where they try\nto extract a person‚Äôs name given their disease,\nor disease given name, but in all their attacks,\nthey only use signals from the target model and\nconsistently find that a frequency-based baseline\n(i.e. one that would always guess the most frequent\nname/disease) is more successful.\n6 Conclusions\nIn this paper, we introduce a principled membership\ninference attack based on likelihood ratio testing\nto measure the training data leakage of Masked\nLanguage Models (MLMs). In contrast to prior\nwork on MLMs, we rely on signals from both\nthe model under attack and a reference model to\ndecide the membership of a sample. This enables\nperforming successful membership inference\nattacks on data points that are hard to fit, and\ntherefore cannot be detected using the prior work.\nWe also perform an analysis of why these models\nleak, and which data points are more susceptible\nto memorization. Our attack shows that MLMs\nare significantly prone to memorization. This\nwork calls for designing robust privacy mitigation\nalgorithms for such language models.\nLimitations\nMembership inference attacks form the foundation\nof privacy auditing and memorization analysis in\nmachine learning. As we show in this paper, and as\nit is shown in the recent work (Carlini et al., 2021a;\nYe et al., 2021), these attacks are very efficient in\nidentifying privacy vulnerabilities of models with\nrespect to individual data records. However, for a\nthorough analysis of data privacy, it is not enough to\nrely only on membership inference attacks. We thus\nwould need to extend our analysis to reconstruction\nattacks and property inference attacks.\nEthics Statement\nWe use two datasets in this paper, MIMIC-III and\ni2b2, both of which contain sensitive data and can\nonly be accessed by request 2 and after agreeing\nto the data usage and confidentiality terms 3 and\npassing proper training for ethical and privacy-\npreserving use of the data. For reproduction of our\nresults, code will be made available only by request\nand for research purposes, only to researchers who\nprovide proof of authorized access to the datasets\n(by forwarding the access granted emails from\nMIMIC-III and i2b2 to the first author4).\nTo protect models against membership inference\nattacks, like the one proposed in this work, differen-\ntially private training algorithms (Abadi et al., 2016;\nChaudhuri et al., 2011) can be used, as they are theo-\nretically designed to protect the membership of each\ndata record individually. Other methods such as\nadversarial training (Mireshghallah et al., 2021) and\npersonally identifiable information scrubbing (Der-\nnoncourt et al., 2017) can also be used, however,\nthey do not provide the worst-case guarantees that\ndifferential privacy does (Brown et al., 2022).\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers and meta-reviewers for their helpful\nfeedback. We also thank our colleagues at the\nUCSD Berg Lab and NUS for their helpful\ncomments and feedback.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan\nMcMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.\n2016. Deep learning with differential privacy. InPro-\nceedings of the 2016 ACM SIGSAC conference on com-\nputer and communications security, pages 308‚Äì318.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tram√®r.\n2Access can be requested through https:\n//mimic.mit.edu/docs/gettingstarted/\nand https://portal.dbmi.hms.harvard.edu/\nprojects/n2c2-nlp/\n3Data Usage Agreements (DUA) are available\nin https://physionet.org/content/mimiciii/\nview-dua/1.4/ and https://projects.iq.\nharvard.edu/files/n2c2/files/n2c2_data_\nsets_dua_preview_-_academic_user.pdf for\nthe datasets, respectively.\n4fmireshg@eng.ucsd.edu\n8340\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint arXiv:2202.05520.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang\nSong, Andreas Terzis, and Florian Tramer. 2021a.\nMembership inference attacks from first principles.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural\nlanguage models.\nNicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer:\nEvaluating and testing unintended memorization\nin neural networks. In 28th USENIX Security\nSymposium (USENIX Security 19), pages 267‚Äì284.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew\nJagielski, Ariel Herbert-V oss, Katherine Lee, Adam\nRoberts, Tom Brown, Dawn Song, Ulfar Erlingsson,\nAlina Oprea, and Colin Raffel. 2021b. Extracting\ntraining data from large language models.\nKamalika Chaudhuri, Claire Monteleoni, and Anand D\nSarwate. 2011. Differentially private empirical\nrisk minimization. Journal of Machine Learning\nResearch, 12(3).\nFranck Dernoncourt, Ji Young Lee, Ozlem Uzuner,\nand Peter Szolovits. 2017. De-identification of\npatient notes with recurrent neural networks.Journal\nof the American Medical Informatics Association ,\n24(3):596‚Äì606.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805.\nKartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick.\n2022. Exposing the implicit energy networks\nbehind masked language models via metropolis‚Äì\nhastings. In International Conference on Learning\nRepresentations.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific\nlanguage model pretraining for biomedical natu-\nral language processing. ACM Transactions on\nComputing for Healthcare (HEALTH), 3(1):1‚Äì23.\nSorami Hisamoto, Matt Post, and Kevin Duh. 2020.\nMembership Inference Attacks on Sequence-to-\nSequence Models: Is My Data In Your Machine\nTranslation System? Transactions of the Association\nfor Computational Linguistics, 8:49‚Äì63.\nAbhyuday Jagannatha, Bhanu Pratap Singh Rawat,\nand Hong Yu. 2021. Membership inference attack\nsusceptibility of clinical language models.\nBargav Jayaraman, Lingxiao Wang, Katherine Knip-\nmeyer, Quanquan Gu, and David Evans. 2021.\nRevisiting membership inference under realistic\nassumptions.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models. arXiv preprint arXiv:2202.06539.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234‚Äì1240.\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav\nGoldberg, and Byron Wallace. 2021. Does BERT\npretrained on clinical notes reveal sensitive data?\nIn Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 946‚Äì959, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nYunhui Long, Vincent Bindschaedler, and Carl A.\nGunter. 2017. Towards measuring membership\nprivacy. ArXiv, abs/1712.09136.\nSaeed Mahloujifar, Huseyin A Inan, Melissa Chase, Esha\nGhosh, and Marcello Hasegawa. 2021. Membership\ninference on word embedding and beyond. arXiv\npreprint arXiv:2106.11384.\nFatemehsadat Mireshghallah, Huseyin Inan, Marcello\nHasegawa, Victor R√ºhle, Taylor Berg-Kirkpatrick,\nand Robert Sim. 2021. Privacy regularization: Joint\nprivacy-utility optimization in languagemodels. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3799‚Äì3807.\nFatemehsadat Mireshghallah, Mohammadkazem\nTaram, Praneeth Vepakomma, Abhishek Singh,\nRamesh Raskar, and Hadi Esmaeilzadeh. 2020.\nPrivacy in deep learning: A survey. arXiv preprint\narXiv:2004.12254.\nSasi Kumar Murakonda and Reza Shokri. 2020. Ml\nprivacy meter: Aiding regulatory compliance by\nquantifying the privacy risks of machine learning.\nIn Workshop on Hot Topics in Privacy Enhancing\nTechnologies (HotPETs).\nSasi Kumar Murakonda, Reza Shokri, and George\nTheodorakopoulos. 2021. Quantifying the privacy\nrisks of learning high-dimensional graphical models.\nIn International Conference on Artificial Intelligence\nand Statistics, pages 2287‚Äì2295. PMLR.\nYuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura,\nNaoto Hayashi, Osamu Abe, Shuntaro Yada, Shoko\nWakamiya, and Eiji Aramaki. 2020. Kart: Privacy\nleakage framework of language models pre-trained\nwith clinical records.\n8341\nMilad Nasr, Reza Shokri, and Amir Houmansadr. 2019.\nComprehensive privacy analysis of deep learning:\nPassive and active white-box inference attacks against\ncentralized and federated learning. In 2019 IEEE\nsymposium on security and privacy (SP) , pages\n739‚Äì753. IEEE.\nMilad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas\nPapemoti, and Nicholas Carlin. 2021. Adversary\ninstantiation: Lower bounds for differentially private\nmachine learning. In 2021 IEEE Symposium on\nSecurity and Privacy (SP), pages 866‚Äì882. IEEE.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo on\nten benchmarking datasets. In Proceedings of the\n2019 Workshop on Biomedical Natural Language\nProcessing (BioNLP 2019), pages 58‚Äì65.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842‚Äì866.\nAhmed Salem, Apratim Bhattacharya, Michael Backes,\nMario Fritz, and Yang Zhang. 2020. Updates-Leak:\nData set inference and reconstruction attacks in\nonline learning. In 29th USENIX Security Symposium\n(USENIX Security 20), pages 1291‚Äì1308. USENIX\nAssociation.\nAhmed Salem, Yang Zhang, Mathias Humbert, Mario\nFritz, and Michael Backes. 2018. Ml-leaks: Model\nand data independent membership inference attacks\nand defenses on machine learning models. ArXiv,\nabs/1806.01246.\nVirat Shejwalkar, Huseyin A Inan, Amir Houmansadr,\nand Robert Sim. 2021. Membership inference attacks\nagainst nlp classification models. In NeurIPS 2021\nWorkshop Privacy in Machine Learning.\nReza Shokri. 2022. Auditing data privacy for machine\nlearning. Santa Clara, CA. USENIX Association.\nReza Shokri, Marco Stronati, Congzheng Song, and\nVitaly Shmatikov. 2017. Membership inference\nattacks against machine learning models. In 2017\nIEEE symposium on security and privacy (SP), pages\n3‚Äì18. IEEE.\nCongzheng Song and Ananth Raghunathan. 2020. Infor-\nmation leakage in embedding models. InProceedings\nof the 2020 ACM SIGSAC Conference on Computer\nand Communications Security, pages 377‚Äì390.\nCongzheng Song and Vitaly Shmatikov. 2018. The\nnatural auditor: How to tell if someone used your\nwords to train their model.ArXiv, abs/1811.00513.\nAmber Stubbs and √ñzlem Uzuner. 2015. Annotating lon-\ngitudinal clinical narratives for de-identification: The\n2014 i2b2/uthealth corpus. Journal of Biomedical In-\nformatics, 58:S20‚ÄìS29. Supplement: Proceedings of\nthe 2014 i2b2/UTHealth Shared-Tasks and Workshop\non Challenges in Natural Language Processing for\nClinical Data.\nThomas Vakili and Hercules Dalianis. 2021. Are\nclinical bert models privacy preserving? the difficulty\nof extracting patient-condition associations. In\nProceedings of the AAAI 2021 Fall Symposium on\nHuman Partnership with Medical AI : Design, Op-\nerationalization, and Ethics (AAAI-HUMAN 2021),\nnumber 3068 in CEUR Workshop Proceedings.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model\nfor financial communications. arXiv preprint\narXiv:2006.08097.\nJiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda,\nVincent Bindschaedler, and Reza Shokri. 2021. En-\nhanced membership inference attacks against machine\nlearning models. arXiv preprint arXiv:2111.09679.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and\nSomesh Jha. 2018. Privacy risk in machine learning:\nAnalyzing the connection to overfitting. In 2018\nIEEE 31st computer security foundations symposium\n(CSF), pages 268‚Äì282. IEEE.\nSantiago Zanella-B√©guelin, Lukas Wutschitz, Shruti\nTople, Victor R√ºhle, Andrew Paverd, Olga Ohri-\nmenko, Boris K√∂pf, and Marc Brockschmidt. 2020.\nAnalyzing Information Leakage of Updates to Natural\nLanguage Models, pages 363‚Äì375. Association for\nComputing Machinery, New York, NY , USA.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert.arXiv preprint\narXiv:1904.09675.\n8342\nTable 8: Summary of our attack‚Äôs notations.\nNotation Explanation\nD Training datasetS Set of target samples whose membership we want to determine.s A given target sample (s‚ààS,p(s‚ààD)=0.5))R Reference modelt Attack thresholdL(s) Likelihood ratio for given samples\nA Appendix\nA.1 Notations\nTo summarize and clarify the notations used in the\npaper for explaining our attack, we added Table 8.\nA.2 Detailed Experimental Setup\nA.2.1 Code and Data Access\nWe use two datasets in this paper, MIMIC-\nIII and i2b2, both of which contain sen-\nsitive data and can only be accessed\nby request through https://mimic.\nmit.edu/docs/gettingstarted/\nand https://portal.dbmi.hms.\nharvard.edu/projects/n2c2-nlp/,\nand after agreeing to the data usage and confidential-\nity terms5 and passing proper training for ethical and\nprivacy-preserving use of the data. For reproduction\nof our results, code will be made available only by re-\nquest and for research purposes, only to researchers\nwho provide proof of authorized access to the\ndatasets (by forwarding the access granted emails\nfrom MIMIC-III and i2b2 to the first author6).\nA.2.2 Datasets\nWe run our attack on two sets of target samples, one\nwe denote by ‚ÄúMIMIC‚Äù and the other by ‚Äúi2b2‚Äù in\nthe results (both medical notes). For both of these,\nthe ‚Äúmembers‚Äù portion of the target samples is from\nthe training set (D) of our target models, which is\nthe MIMIC-III dataset. However, the non-members\nare different. For the results shown under MIMIC,\nthe non-members are a held-out subset of the\nMIMIC data that was not used in training. For i2b2,\nthe non-members are from a different (but similar)\ndataset, i2b2. Below we elaborate on this setup and\neach of these datasets.\nMIMIC-III The target models we attack\nare trained (by Lehman et al.) on the pseudo\nre-identified MIMIC III notes which consist of\n5Data Usage Agreements (DUA) are available\nin https://physionet.org/content/mimiciii/\nview-dua/1.4/ and https://projects.iq.\nharvard.edu/files/n2c2/files/n2c2_data_\nsets_dua_preview_-_academic_user.pdf for\nthe datasets, respectively.\n6fmireshg@eng.ucsd.edu\n1,247,291 electronic health records (EHR) of\n46,520 patients. These records have been altered\nsuch that the original first and last names are\nreplaced with new first and last names sampled from\nthe US Census data. Only27,906 of these patients\nhad their names explicitly mentioned in the EHR.\nFor the attack‚Äôs target data, we use a held-out\nsubset of MIMIC-III consisting of 89 patients\n(4072 sample sequences) whose data was not used\nduring training of the ClinicalBERT target models\nand use them as ‚Äúnon-member‚Äù samples. For\n‚Äúmember‚Äù samples, we take a99 patient subsample\nof the entire training data, and then subsample4072\nsample sequences from that (we do this 10 times\nfor each attack and average the results over), so that\nthe number of member and non-member samples\nare the same and the target pool is balanced.\ni2b2 This dataset was curated for the i2b2\nde-identification of protected health information\n(PHI) challenge in 2014 (Stubbs and √ñzlem\nUzuner, 2015). We use this dataset as a secondary\nnon-member dataset, since it is similar in domain\nto MIMIC-III (both are medical notes), is larger in\nterms of size than the held-out MIMIC-III set, and\nhas not been used as training data for our models.\nWe subsample99 patients from i2b2, consisting of\n18561 sequences, and use them as non-members.\nThe population data that we use to evaluate\nthe distribution of likelihood ratio over the null\nhypothesis (which is used to compute the threshold)\nis disjoint with the non-member set that we use to\nevaluate the attack. We randomly select99 patients\nfrom the i2b2 dataset for this purpose.\nA.2.3 Target Models\nWe perform our attack on 5 different pre-trained\nmodels, that are all trained on MIMIC-III, but with\ndifferent training procedures:\nClinicalBERT (Base) BERT-base architecture\ntrained over the pseudo re-identified MIMIC-III\nnotes for 300k iterations for sequence length 128\nand for 100k iterations for sequence length 512.\nClinicalBERT++ (Base++) BERT-base architec-\nture trained over the pseudo re-identified MIMIC III\nnotes for 1M iterations at a sequence length of 128.\nClinicalBERT-Large (Large). BERT-large\narchitecture trained over the pseudo re-identified\nMIMIC-III notes for 300k iterations for sequence\nlength 128 and for 100k iterations for sequence\nlength 512.\n8343\nClinicalBERT-large++ (Large++) BERT-large\narchitecture trained over the pseudo re-identified\nMIMIC III notes for 1M iterations at a sequence\nlength of 128.\nClinicalBERT-b ((Base-b), Name Insertion).\nSame training and architecture as ClincalBERT-\nbase, but that the patient‚Äôs surrogate name is\nprepended to the beginning of every sentence.\nThis model is used to identify the effect of name-\ninsertion on the memorization of BERT-based\nmodels (Lehman et al., 2021).\nA.2.4 Computational Resources\nFor this paper, we did not train any models, so the\nGPU training time is0 hours. However, for getting\nthe likelihoods, we ran inference on the sequences\nin our target samples pool. For that, we used an\nRTX2080 GPU with 11GB of memory for 18 hours.\nA.3 Further Studies\nA.3.1 Lower False Positive Rate\nAll the threshold-dependant results (precision and\nrecalls) in Section 4 are reported with a threshold\nset for havingŒ±=10% false positive rate (using the\nmechanism shown in Figure 2a). In this section, we\nwant to look at lower false positive rates, like we do\nin Figure 5, and see how well our attack does when\nprecision is very important to us and we do not want\nto get any false positives. These results are shown in\nTable 10. (This table corresponds with Table 4 from\nSection 4.2 and the sample-level part of Table 6.)\nWe can see that compared to Table 10, as we are\ndecreasing the false positive rate, the performance\ngap between our attack and the baseline increases\ndrastically, showin that our attack performs really\nwell under tight false positive rate constraints. Note\nthat AUC is not threshold dependant therefore it has\nthe same value in both tables.\nA.3.2 Using Normalized Energy\nIn the paper, we useE(s;Œ∏), as shown in Equation 2\nfor finding the likelihood ratio. In other words,\nfor finding the likelihood ratio, we basically\ncalculate the loss of the target model and reference\nmodel (using 15% masking and averaging over 10\ntimes) on the given sequences, and subtract them.\nHowever, another way to approach this problem of\ncalculating likelihood ratio is to use the normalized\nenergy (instead of the loss) as introduced in (Goyal\net al., 2022), instead of the loss. For calculating\nthe normalized energy, we mask each token in the\nsequence, one token at a time (instead of15%), cal-\nculated the loss, and average over all the tokens. To\nsee how this energy does, we have used it to mount\nour attack, and we show the results in Table 11.\nCompared to using loss (Table 9) it seems like\nthe AUC for normalized energy is higher overall,\nA.3.3 Results for Short Sequences\nAll the results in Section 4 are reported for long\nsequences (more than 20 tokens long), except those\nreported in Table 3, where we ablate the privacy risks\nfor sequences of different lengths. In this section,\nfor the sake of completion, we are reporting results\nfor short sequences as well as long sequences, for all\nthe five models we study. These results are shown in\nTable 9. (This table corresponds with Table 4 from\nSection 4.2 and the sample-level part of Table 6.)\nA.3.4 Qualitative Comparison with Baseline\nFigures 3a and 3b show histogram visualizations\nof L(s) (likelihood ratio statistic) and model loss,\nover the target sample pool (i.e. mixture of members\nand non-members from the MIMIC dataset), respec-\ntively. Here the target model is ClinicalBERT-Base.\nThe blue line represents a target query sample drawn\nfrom the member set. The point of these visualiza-\ntions is to show a case in which a sample is misclassi-\nfied as a non-member by the model loss baseline, but\nis correctly classified as a member using our attack.\nThe member and non-member distributions‚Äô\nhistograms are shown via light blue and light\norange bars respective. The red line indicates the\nthreshold that is selected such thatŒ±=0.1, i.e. 10%\nfalse positive rate. In Figure 3a we see a distinct\nseparation between the member and non-member\nhistogram distributions when we use L(s) as the\ntest criterion for our attack. This results in the\nestimation of a useful threshold that correctly\nclassifies the blue line sample as a member. In\ncontrast, the baseline attack by (Jagannatha et al.,\n2021), in Figure 3b based solely upon the target\nmodel‚Äôs loss leads to a high overlap between\nthe member and non-member histograms which\nleads to a threshold that misclassifies the sample\nrepresented by the blue line. These histograms\nshow that the reference model used in our method\nhelps in getting a sense of how hard each sample\nis in general, and puts each point in perspective.\nA.3.5 ROC Curve Magnified\nIn Figure 5 We have plotted Figure 4 from the\nresults, but with logarithmic x-axis, to zoom in on\nthe low false positive rate section and really show\nthe differences between our attack and the baseline.\n8344\nTable 9: Sample-level attack results (withŒ±=10% false positive rate used for thresholding) on the four ClinicalBERT\nmodels, plus the Base-b model (name insertion). We use PubMed-BERT as the reference model and the MIMIC\ndata as non-members. Base++ (Large++) is same as Base (Large), but trained for more epochs. Base-b is the same\nas the Base model, but the training data was modified to prepend patient‚Äôs first and last name to each sequence. This\ntable studies effect of model size, training and sequence length on leakage.\nShort Long\nNon-members Base Base++ Large Large++ Base-b Base Base++ Large Large++ Base-b\nAUC.\n(A) Model loss 0.662 0.656 0.679 0.700 0.561 0.516 0.513 0 .509 0 .536 0 .391\n(B) Ours 0.900 0.894 0.904 0.905 0.960 0.830 0.827 0 .835 0 .843 0 .912\nPrec.\n(A) w/¬µthresh. 61.5 61.0 62.4 64.9 53.0 50.9 50.6 50 .7 52 .5 44 .3\n(A) w/ Pop. thresh. 61.2 61.2 63.9 69.1 44.1 42.0 40.6 40 .0 43 .4 25 .9\n(B) w/ Pop. thresh. 88.9 88.8 88.9 88.9 90.2 87.3 87.3 87 .3 87 .3 89 .3\nRec.\n(A) w/¬µthresh. 55.7 55.8 56.4 56.1 54.0 55.3 55.1 56 .2 55 .8 54 .3\n(A) w/ Pop. thresh. 15.6 15.6 17.6 22.2 7.8 7.2 6.8 6 .6 7 .6 3 .5\n(B) w/ Pop. thresh. 79.2 78.5 79.2 79.3 91.3 68.2 68.2 68 .4 68 .4 82 .7\nTable 10: Sample-level attack results (withŒ±=1% false positive rate used for thresholding) on the four ClinicalBERT\nmodels, plus the Base-b model (name insertion). We use PubMed-BERT as the reference model and the MIMIC\ndata as non-members. Base++ (Large++) is same as Base (Large), but trained for more epochs. Base-b is the same\nas the Base model, but the training data was modified to prepend patient‚Äôs first and last name to each sequence. This\ntable studies effect of model size, training and sequence length on leakage.\nShort Long\nNon-members Base Base++ Large Large++ Base-b Base Base++ Large Large++ Base-b\nAUC.\n(A) Model loss 0.662 0.656 0.679 0.700 0.561 0.516 0.513 0 .509 0 .536 0 .391\n(B) Ours 0.900 0.894 0.904 0.905 0.960 0.830 0.827 0 .835 0 .843 0 .912\nPrec.\n(A) w/¬µthresh. 61.5 61.0 62.4 64.9 53.0 50.9 50.6 50 .7 52 .5 44 .3\n(A) w/ Pop. thresh. 53.7 53.8 62.1 71.4 35.1 9.4 5.8 9 .3 18 .3 7 .5\n(B) w/ Pop. thresh. 98.5 98.4 98.4 98.4 98.8 97.9 97.9 98 .0 97 .9 98 .4\nRec.\n(A) w/¬µthresh. 55.7 55.8 56.4 56.1 54.0 55.3 55.1 56 .2 55 .8 54 .3\n(A) w/ Pop. thresh. 1.1 1.1 1.6 2.4 0.5 0.1 0.1 0 .1 0 .2 0 .1\n(B) w/ Pop. thresh. 60.4 58.6 57.6 56.2 78.3 43.0 43.1 45 .2 42 .9 58 .7\n10‚àí2 10‚àí1 100\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nOurs (Liklihood ratio)\nBaseline (Model loss)\nFigure 5: The ROC curve of sample-level attack, with log-\narithmic scale x-axis. performed on Clinical-BERT using\nMIMIC samples as non-members. Green line shows\nour attack and the red line shows the baseline loss-based\nattack. The blue dashed line shows AUC=0.5 (random\nuess). We find false positive and true positive rates over\nthe target sample pool, which is comprised of members\nand non-members. This curve corresponds to Table 2\nin the results, and is the zoomed-in version of Figure 4.\nA.4 Extended Related Works\nSince our work proposes an attack for quantifying\nleakage of masked language models (MLMs),\nbased on the likelihood ratio, there are two lines of\nwork that are related to ours: (1) work surrounding\nattacks/leakage on machine learning models\n(2) work on calculating sequence likelihood for\nMLMs. Prior work on measuring memorization\nand leakage in machine learning models, and\nspecifically NLP models can itself be classified into\ntwo main categories: (1) membership inference\nattacks and (2) training data extraction attacks.\nBelow we discuss each line of work in more detail.\nMembership inference. Membership Inference\nAttacks (MIA) try to determine whether or\nnot a target sample was used in training a target\nmodel (Shokri et al., 2017; Yeom et al., 2018). These\nattacks be seen as privacy risk analysis tools (Mu-\nrakonda and Shokri, 2020; Nasr et al., 2021;\nKandpal et al., 2022), which help reveal how much\nthe model has memorized the individual samples in\n8345\nTable 11: Sample-level attack results (withŒ±=10% false positive rate used for thresholding) on the four ClinicalBERT\nmodels, plus the Base-b model (name insertion). Here we use ‚Äúnormalized energy‚Äù as a proxy for likelihood, instead\nof using the 15% masked energy formulation of Equation 2. We use PubMed-BERT as the reference model and\nthe MIMIC data as non-members. Base++ (Large++) is same as Base (Large), but trained for more epochs. Base-b\nis the same as the Base model, but the training data was modified to prepend patient‚Äôs first and last name to each\nsequence. This table studies effect of model size, training and sequence length on leakage.\nShort Long\nNon-members Base Base++ Large Large++ Base-b Base Base++ Large Large++ Base-b\nAUC.\n(A) Model loss 0.685 0.675 0.686 0.714 0.547 0.487 0.482 0 .476 0 .522 0 .329\n(B) Ours 0.916 0.910 0.917 0.924 0.972 0.871 0.869 0 .873 0 .881 0 .950\nPrec.\n(A) w/¬µthresh. 63.02 62.18 62.43 63.70 53.00 49.24 49.23 48 .78 50 .44 42 .58\n(A) w/ Pop. thresh. 64.38 63.94 66.78 72.47 34.68 36.55 35.01 36 .49 50 .65 7 .74\n(B) w/ Pop. thresh. 89.07 89.05 89.09 89.21 90.40 88.07 88.10 88 .15 88 .19 89 .97\nRec.\n(A) w/¬µthresh. 58.11 58.27 58.57 58.77 55.29 59.17 59.13 59 .70 59 .72 55 .42\n(A) w/ Pop. thresh. 17.90 17.57 19.91 25.90 5.26 5.75 5.37 5 .73 10 .22 0 .84\n(B) w/ Pop. thresh. 80.66 80.48 80.82 81.82 93.23 73.40 73.61 73 .98 74 .21 89 .21\nits training set, and what the risk of individual users\nis (Nasr et al., 2019; Long et al., 2017; Salem et al.,\n2018; Ye et al., 2021; Carlini et al., 2021a) A group\nof these attacks rely on behavior of shadow models\n(models trained on data similar to training, to mimic\nthe target model) to determine the membership of\ngiven samples (Jayaraman et al., 2021; Shokri et al.,\n2017). In the shadow model training procedure the\nadversary trains a batch of modelsm1,m2,...,mk as\nshadow models, with data from the target user. Then,\nit trains m‚Ä≤\n1,m‚Ä≤\n2...,m‚Ä≤\nk without the data from the tar-\nget user and then tries to find some statistical dispar-\nity between these models (Mahloujifar et al., 2021).\nShadow-based attacks have been mounted on NLP\nmodels as well: (Song and Shmatikov, 2018) mounts\nsuch an attack on LSTM-based text-generation\nmodels, (Mahloujifar et al., 2021) mounts one on\nword embedding, (Hisamoto et al., 2020) applies\nit to machine translation and more recently, (She-\njwalkar et al., 2021) mounts it on transformer-based\nNLP classification models. Mounting such attacks\nis usually costly, as their success relies upon\ntraining multiple shadow models, and access to\nadequate shadow data for training such models.\nAnother group of MIAs relies solely on the loss\nvalue of the target sample, under the target model,\nand thresholds this loss to determine member-\nship (Jagannatha et al., 2021; Yeom et al., 2018).\nSong and Raghunathan mount such an attack on\nword embedding, where they try to infer if given\nsamples were used in training different embedding\nmodels. Jagannatha et al., which is the work closest\nto ours, uses a thresholding loss-based attack to infer\nmembership on MLMs. Although our proposed\nattack is also a threshold-based one, it is different\nfrom prior work by: (a) applying likelihood ratio\ntesting using a reference model and (b) calculating\nthe likelihood through our energy function formu-\nlation. These two components cause our attack to\nhave higher AUC, as shown in the results.\nWe refer the reader to the framework introduced\nby (Ye et al., 2021) that formalizes different\nmembership inference attacks and compares their\nperformance on benchmark ML tasks.\nTraining data extraction. Training data extrac-\ntion quantifies the risk of extracting training data\nby probing a trained language model (Salem et al.,\n2020; Carlini et al., 2019; Zanella-B√©guelin et al.,\n2020; Carlini et al., 2021b, 2022; Nakamura et al.,\n2020). The most prominent of such attacks, on NLP\nmodels is that of Carlini et al. (2021b), where they\ntake more than half a million samples from different\nGPT-2 models, sift through the samples using a\nmembership inference method to find samples that\nare more likely to have been memorized, and finally,\nonce they have narrowed down the samples to 1800,\nthey check the web to see if such samples might\nhave been in the GPT-2 training set. They find\nthat over 600 of those 1800 samples were verbatim\ntraining samples. Lehman et al. (2021) mount the\nsame data extraction attack on MLMs, but their\nresults are somehow inconclusive as to how much\nMLMs memorize samples, as only 4% of generated\nsentences with a patient‚Äôs name also contain one of\ntheir true medical conditions. They also mount other\ntype of attacks, where they try to extract a person‚Äôs\nname given their disease, or disease given name, but\nin all their attacks, they only use signals from the\ntarget model and consistently find that a frequency-\nbased baseline (i.e. one that would always guess the\n8346\nmost frequent name/disease) is more successful.\n8347",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.8250710368156433
    },
    {
      "name": "Computer science",
      "score": 0.6740255355834961
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5938257575035095
    },
    {
      "name": "Machine learning",
      "score": 0.46491438150405884
    },
    {
      "name": "Data mining",
      "score": 0.4611165523529053
    },
    {
      "name": "Artificial intelligence",
      "score": 0.413940966129303
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I160992636",
      "name": "Toyota Technological Institute at Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}