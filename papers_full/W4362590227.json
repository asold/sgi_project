{
  "title": "Swin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for Alzheimer‚Äôs Disease with Machine Learning",
  "url": "https://openalex.org/W4362590227",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4265167981",
      "name": "Nasr Gharaibeh",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A4211440571",
      "name": "Ashraf  A. Abu-Ein",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A4211440570",
      "name": "Obaida M. Al-Hazaimeh",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A2502375454",
      "name": "Khalid M.O. Nahar",
      "affiliations": [
        "Yarmouk University"
      ]
    },
    {
      "id": "https://openalex.org/A4364289435",
      "name": "Waleed A. Abu-Ain",
      "affiliations": [
        "Taibah University"
      ]
    },
    {
      "id": "https://openalex.org/A4284864685",
      "name": "Malek M. Al-Nawashi",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A4265167981",
      "name": "Nasr Gharaibeh",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A4211440571",
      "name": "Ashraf  A. Abu-Ein",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A4211440570",
      "name": "Obaida M. Al-Hazaimeh",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    },
    {
      "id": "https://openalex.org/A2502375454",
      "name": "Khalid M.O. Nahar",
      "affiliations": [
        "Yarmouk University"
      ]
    },
    {
      "id": "https://openalex.org/A4364289435",
      "name": "Waleed A. Abu-Ain",
      "affiliations": [
        "Taibah University",
        "Yanbu University College"
      ]
    },
    {
      "id": "https://openalex.org/A4284864685",
      "name": "Malek M. Al-Nawashi",
      "affiliations": [
        "Al-Balqa Applied University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283662297",
    "https://openalex.org/W3195192326",
    "https://openalex.org/W3125139346",
    "https://openalex.org/W3215630567",
    "https://openalex.org/W3173195087",
    "https://openalex.org/W3127433321",
    "https://openalex.org/W3134362034",
    "https://openalex.org/W3119018229",
    "https://openalex.org/W4306790045",
    "https://openalex.org/W4285097303",
    "https://openalex.org/W2318325670",
    "https://openalex.org/W2884680377",
    "https://openalex.org/W2995495466",
    "https://openalex.org/W3027152588",
    "https://openalex.org/W3007755486",
    "https://openalex.org/W2017753777",
    "https://openalex.org/W3028149444",
    "https://openalex.org/W2886104458",
    "https://openalex.org/W3205018926",
    "https://openalex.org/W3133909080",
    "https://openalex.org/W3017890102",
    "https://openalex.org/W3027483243",
    "https://openalex.org/W3092648605",
    "https://openalex.org/W2972307193",
    "https://openalex.org/W4285130568",
    "https://openalex.org/W3181068645",
    "https://openalex.org/W3129015452",
    "https://openalex.org/W3145812136",
    "https://openalex.org/W3155865340",
    "https://openalex.org/W3167864582",
    "https://openalex.org/W3006901343",
    "https://openalex.org/W3210926952",
    "https://openalex.org/W3125959229",
    "https://openalex.org/W3123936063",
    "https://openalex.org/W3171990888",
    "https://openalex.org/W4302028668",
    "https://openalex.org/W4285246255",
    "https://openalex.org/W3011893870",
    "https://openalex.org/W4236647340"
  ],
  "abstract": "Alzheimer Disease (AD) is the ordinary type of dementia which does not have any proper and efficient medication. Accurate classification and detection of AD helps to diagnose AD in an earlier stage, for that purpose machine learning and deep learning techniques are used in AD detection which observers both normal and abnormal brain and accurately detect AD in an early. For accurate detection of AD, we proposed a novel approach for detecting AD using MRI images. The proposed work includes three processes such as tri-level pre-processing, swin transfer based segmentation, and multi-scale feature pyramid fusion module-based AD detection.In pre-processing, noises are removed from the MRI images using Hybrid Kuan Filter and Improved Frost Filter (HKIF) algorithm, skull stripping is performed by Geodesic Active Contour (GAC) algorithm which removes the non-brain tissues that increases detection accuracy. Here, bias field correction is performed by Expectation-Maximization (EM) algorithm which removes the intensity non-uniformity. After completed pre-processing, we initiate segmentation process using Swin Transformer based Segmentation using Modified U-Net and Generative Adversarial Network (ST-MUNet) algorithm which segments the gray matter, white matter, and cerebrospinal fluid from the brain images by considering cortical thickness, color, texture, and boundary information which increases segmentation accuracy. After that, multi-scale feature extraction is performed by Multi-Scale Feature Pyramid Fusion Module using VGG16 (MSFP-VGG16) which extract the features in multi-scale which increases the detection and classification accuracy, based on the extracted features the brain image is classified into three classes such as Alzheimer Disease (AD), Mild Cognitive Impairment, and Normal. The simulation of this research is conducted by Matlab R2020a simulation tool, and the performance of this research is evaluated by ADNI dataset in terms of accuracy, specificity, sensitivity, confusion matrix, and positive predictive value.",
  "full_text": "Paper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nSwin Transformer-Based Segmentation and Multi-Scale \nFeature Pyramid Fusion Module for Alzheimer‚Äôs Disease \nwith Machine Learning\nhttps://doi.org/10.3991/ijoe.v19i04.37677\nNasr Gharaibeh1, Ashraf A. Abu-Ein1, Obaida M. Al-hazaimeh1(*),  \nKhalid M.O. Nahar2, Waleed A. Abu-Ain3, Malek M. Al-Nawashi1\n1Al-Balqa Applied University, Irbid, Jordan\n2Yarmouk University, Irbid, Jordan\n3Applied College, Taibah University, Yanbu, Saudi Arabia\ndr_obaida@bau.edu.jo\nAbstract‚ÄîAlzheimer Disease (AD) is the ordinary type of dementia which \ndoes not have any proper and efficient medication. Accurate classification \nand detection of AD helps to diagnose AD in an earlier stage, for that purpose \nmachine learning and deep learning techniques are used in AD detection which \nobservers both normal and abnormal brain and accurately detect AD in an early. \nFor accurate detection of AD, we proposed a novel approach for detecting AD \nusing MRI images. The proposed work includes three processes such as tri-\nlevel pre-processing, swin transfer based segmentation, and multi-scale fea -\nture pyramid fusion module-based AD detection. In pre-processing, noises are \nremoved from the MRI images using Hybrid KuanFilter and Improved Frost Filter \n(HKIF) algorithm, the skull stripping is performed by Geodesic Active Contour \n(GAC) algorithm which removes the non-brain tissues that increases detection \naccuracy. Here, bias field correction is performed by Expectation-Maximization \n(EM) algorithm which removes the intensity non-uniformity. After completed \npre-processing, we initiate segmentation process using Swin Transformer based \nSegmentation using Modified U-Net and Generative Adversarial Network \n(ST-MUNet) algorithm which segments the gray matter, white matter, and cere-\nbrospinal fluid from the brain images by considering cortical thickness, color, \ntexture, and boundary information which increases segmentation accuracy. The \nsimulation of this research is conducted by Matlab R2020a simulation tool, and \nthe performance of this research is evaluated by ADNI dataset in terms of accu-\nracy, specificity, sensitivity, confusion matrix, and positive predictive value. \nKeywords‚ÄîAlzheimer Disease (AD), Mild Cognitive Impairment (MCI),  \nskull striping, segmentation, VGG-16, swin transfer\n1 Introduction\nAlzheimer‚Äôs disease is one of the common causes of elderly death which is an \nirreversible and progressive neurological brain disorder that causes the brain to shrink. \n22\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nIt slowly destroys the memory and thinking skills then the ability to carry out the sim -\nplest task which is serious enough to hamper daily life. Alzheimer‚Äôs disease is mainly \ncaused by an abnormal build-up of protein in and around brain cells where one of the \nprotein amyloid deposit and form plaque around the brain [1, 2]. Due to aging, the diag-\nnoses of Alzheimer‚Äôs disease are tedious task; therefore, Magnetic Resonance Imaging \n(MRI) is used for disease detection to help doctors [3‚Äì6]. \nCertain processes are involved in Alzheimer‚Äôs disease detection from MRI images, \nthe processesare pre-processing, segmentation, feature extraction and classification. \nInitially, the MRI images are pre-processed due to noise-prone characteristics and con-\ntain non-brain tissues (eye, fat, muscles, dura, scalp, skin, etc.) where some existing \nworks nevermore perform skull stripping and several existing works don‚Äôt consider \nnoise reduction (i.e. Rician noise, Gaussian noise, salt and pepper noise, etc.) which \nmoderation them with less classification accuracy [7‚Äì12]. After pre-processing, seg-\nmentation was performed to increase the classification accuracy and reduce complexity. \nSegmentation is one of the major processes to segment the gray matter, white matter \nand cerebrospinal fluid to extract the significant features for classification. Some of the \nexisting work doesn‚Äôt perform segmentation and the majority of the existing works \naccomplished the segmentation using automated image analyzing tool such as statis -\ntical parametric mapping (SPM), FreeSurfer and FSL-FAST4 which consume more \ncomputational time to perform full segmentation, lack of validation, automated meth -\nods in estimating the volume will not accurate and the automated tools compare inten-\nsity to atlas on to guide segmentation this augment issues of loss of fine information \nwhich leads to segmentation problem and affect the segmentation accuracy [11, 13‚Äì16].\nIn most of the existing work, the neural network algorithms are used for feature \nextraction and classification which only extract single feature or limited features that \nare insufficient to perform Alzheimer‚Äôs disease classification [ 17, 18]. The existing \nworks utilize machine learning (ML), neural network and deep learning (DL) algo -\nrithms, the ML algorithms such as support vector machine, decision tree, k-nearest \nneighbor, random forest etc.., which has high training complexity due to large number \nof trees generated while feature extraction and some of the algorithms are not appro -\npriate for large dataset [12, 17, 19]. The ML limitations are resolved by deep learning \nwhich employs neural network for feature extraction and classification such as con -\nvolutional neural network, multilayer perceptron and radial basis function [20‚Äì24]. \nHowever, this algorithm generates large number of hidden layers, high convergence \nweight and consumes high computational time which increases the high complexity \nand affects the classification accuracy. The challenges faced by automated tool while \nsegmentation was addressed by swin transformer-based semantic segmentation then \nthe provocation and insufficiency while feature extraction was solved by multi-scale \nfeature extraction with effective architecture which reduces the high complexity and \nhigh false positive rate. \nFor Alzheimer‚Äôs disease detection, most of the existing works used neural networks \nfor feature extraction and classification. However, it increases the high complexity \nand reduces the accuracy of classification. Several important problems present in the \nexisting work are described as follows in Table 1 [1].\niJOE ‚Äí Vol. 19, No. 04, 2023\n23\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nTable 1. Most relevant problems ‚Äì state-of-the-art methods \nProblems Description \nLack of Pre- \nprocessing\nMost of the existing work performs pre-processing, whereas some works don‚Äôt perform \nskull stripping and noise reduction which affects the classification accuracy due to \npresence of noise and non-brain tissues (eye, scalp, fat, dura, skin etc.).\nInadequate \nfeatures\nMany of the existing works used the texture analysis to extract texture features. \nAnd some of them extracted only the limited features (low-level features) where these \nfeatures are insufficient to classify the disease and affect the true positive rate.\nHigh \nComplexity\nIn some of the existing works the features are extracted from the entire MRI image \nwithout segmenting the region of interest which increases the high complexity. Several \nexisting works utilized the convolutional neural network which generates numerous \nunwanted layers while feature extraction which leads to high complexity. \n Then most  \nof the works employed support vector machine for classification which are not suitable \nfor large dataset.\nWe are motivated the abovementioned problems with the aim of detecting and clas-\nsifying the Alzheimer‚Äôs disease (AD) using MRI images. In addition, this research \nidentifies the problem of considering lack of pre-processing, inadequate feature, high \ncomplexity and so on. The main objective of this research is to perform mathematical \nmodeling for classifying the Alzheimer‚Äôs disease using MRI images with low false \npositive rate and high accuracy. The remaining objectives of this research are described \nas follows:\n1. To enhance the quality of MRI images, pre-processing is initialized by noise reduc-\ntion, removal of non-brain tissues and bias field correction that increase the true \npositive rate.\n2. To extract the feature effectively, semantic segmentation is performed to segment the \ngray matter, white matter and cerebrospinal fluid which reduce the high complexity.\n3. To increase the classification accuracy, significant features are extracted in  \nmulti-scale from the segmented image which reduces the high false positive rate.\nTo make it clear, this research mainly focuses on detecting AD accurately using \nMRI images with high accuracy. The major contributions of this research are listed as \nfollows:\n1. Initially, we perform tri-level pre-processing, in first level we perform noise removal \nby HKIF algorithm which removes the noise and preserves the edge information of \nthe images, in second level we perform skull stripping by extracting brain images \nusing GAC which increases accuracy and reduces complexity. In third level, we \nperform bias filed correction to increase the intensity of the images which is done by \nEM algorithm that helps to increase the quality of the images. \n2. Swin transformer-based segmentation is performed to segment gray matter, white \nmatter, and cerebrospinal fluid of brain which is done byST-MUNet algorithm, this \nprocess increases the detection accuracy of AD, and MCI.\n3. Multi-scale feature extraction based AD detection is performed by MSFP-VGG16 \nwhich extracts multiple features (i.e, structural, statistical, edge, blobs, color,  \n24\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nand contour) in multi-scale format that provides additional information of the fea-\ntures which increases classification accuracy. The proposed MSFP-VGG16 classi-\nfies the images into three classes namely normal, AD, and MCI.\nThe performance of this research is evaluated by various performance metrics such \nas accuracy, sensitivity, specificity, confusion matrix, and positive predictive value.  \nThis paper is structured as follows; Section 3 explains the detailed summary of the litera-\nture review and its research gap. Section 2 explains the major problem statement which are \naddressed from the existing works and solved by the proposed work. Section 4 illustrates \nthe research methodology which explains the research methodology, pseudocode, algo-\nrithm, and mathematical formulations. Section 5 illustrates the detailed description of the \nexperimentation study of the proposed work. Section 6 concludes the conclusion.\n2 Problem statement \nIn [23], an automated detection model for Alzheimer‚Äôs detection using T2 weighted \nbrain MRI images is proposed. Here, Alzhiemer disease is classified by Support Vector \nMachine (SVM) Poly-1 and Random Forest algorithm. The major problems of this \nresearch are expressed as follows:\n1. In this work, the features are extracted from the entire image without focusing on \nregion of interest (GM, WM, and CSF) that increase the high complexity.\n2. The local binary pattern (LBP) was employed for feature fusion which produces \nrather long histogram that leads to slow down the model performance and affect the \ntrue positive rate. \n3. Here, the model does not allow for training larger dataset due to its low computation \ncapacity therefore the model does not suitable for large dataset that affects the clas-\nsification accuracy. \nThe author proposed a deep learning method for Alzheimer classification using T1 \nweighted MRI images [25]. The multilayer perceptron (MLP) was used for classifying \ncognitive normal (CN), early mild cognitive impairment (EMCI), late mild cognitive \nimpairment (LMCI) and AD. The research issues are listed as follows, \n1. In this work, the pre-processing was performed for bias field correction, B1 non-uni-\nformity and grad-warp where skull stripping is the main process, lack of consider -\nation leads to less true positive rate.\n2. Here, only texture features were considered for Alzheimer‚Äôs disease classification. \nHowever, lack of considering other significant features (i.e., structural, statistics, \nedge etc.) will affect the classification performance and its accuracy.\n3. The multilayer perceptron (MLP) was employed for classification, where it gen-\nerates many hidden nodes and convergence of the weight will be very slow which \ndegrades the model performance and it leads to high complexity.\nThe author proposed a deep fusion method for Alzheimer‚Äôs detection using T1 \nMRI image from ADNI dataset [26]. Searchlight was utilized for feature extraction \nfrom specific position of brain and the SVM classifies the images into AD and normal.  \nThe main research problems of this research are listed as follows,\niJOE ‚Äí Vol. 19, No. 04, 2023\n25\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n1. Here, the segmentation was performed on raw image, where the presence of non-\nbrain tissues, noise and intensity inhomogeneity affect the image quality and leads \nto high false positive rate.\n2. In this work, statistical parametric mapping (SPM) was utilized for segmentation, \nwhere it was an automatic image analysing tool, which is not suitable for any other \ndataset‚Äôs segmentation process this affects the classification accuracy. \n3. The support vector machine (SVM) was employed for Alzheimer‚Äôs disease classifi-\ncation which doesn‚Äôt perform well on large dataset due to its high training complica-\ntion that leads to high complexity.\nThe author proposed a method for early diagnosis of Alzheimer‚Äôs detection using \nT2 weighted MRI from ADNI database [27]. Convolutional Neural Network (CNN) \nwas utilized to perform feature extraction and classification into three classes such as \nnormal, MCI, and AD.\n1. In this work, the pre-processing was performed to enhance the image quality and \nskull stripping where bias filed correction and noise reduction were not considered \nwhich leads to affect the image quality and classification accuracy.\n2. Here, the clustering-based segmentation was performed where random initialization \nof centroid of cluster lack of consistency in determination of abnormalities that leads \nto high false positive rate.\n3. The feature extraction and classification were performed by Convolutional Neural \nNetwork (CNN) which generates large number of unwanted layers while feature \nextraction that leads to high complexity.\n3 Related work \nIn paper [28], the authors proposed a machine-learning model for Alzheimer‚Äôs dis -\nease diagnosis using T1 MRI images. Initially, the pre-processing was performed using \nFreeSurfer software for skull stripping, image registration, cortical segmentation, and \nthickness estimation then the brain regions are segmented using the same software and \n63 volumetric features are obtained. The Welch‚Äôs test was used for feature selection \nand 22 significant features are selected for classification. Finally, the classification \nwas performed by Random Forest which classifies into three classes as Normal, MCI \nand Alzheimer‚Äôs disease. Here, the classification was performed using Random Forest \nwhich generates a large number of trees for a large dataset during classification increases \nhigh complexity and high latency. Authors in [29], proposed a model for Alzheimer‚Äôs \ndisease detection using T1 weighted MRI images. Initially, the pre-processing was  \nperformed for skull removal using FreeSurfer, then sampling, clipping, and intensity \nnormalization are executed. The feature extraction and classification were accom -\nplished using Modified U-net, which has skip-connection to fuse low-level information \nand high-level semantic features. Finally, the structural features are used for classifi-\ncation which classifies normal, early cognitive impairment, late mild cognitive impair-\nment, and Alzheimer‚Äôs disease. In this work, the feature extraction was performed on \nthe overall image instead of focusing on the region of interest which leads to high \ncomplexity in extracting features from the overall image.\n26\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nThe authors proposed a deep learning model for Alzheimer‚Äôs and Dementia dis -\nease diagnosis using MRI images [5]. Initially, the pre-processing was performed to \nbalance the dataset using Synthetic minority over-sampling technique (SMOTE) and \nthe normalization was executed. The DEMNET network was implemented for feature \nextraction and disease detection where the convolutional layers extract the discrimi -\nnate features from the pre-processed image. Finally, the softmax layer classifies into \nfour classes such as non-AD, very mild AD, mild AD, and moderate AD. Here, the \npre-processing was only performed to balance the dataset where the presence of non-\nbrain tissues, noise, and intensity in homogeneity affects the classification accuracy. In \npaper [30], the author‚Äôs deep and conventional machine learning model for Alzheimer‚Äôs \ndisease detection using T1 MRI images. Initially, the N4 algorithm was used to correct \nthe non-uniformity and skull stripping, and then a non-rigid B-spline transformation \nmodel was implemented for pair wise registration. The gray matter was segmented and \nthe feature map was extracted using a statistical parametric mapping from the pre-pro-\ncessed image. Finally, the deep convolutional neural network and conventional SVM \nclassify into normal, mild cognitive impairment, and Alzheimer‚Äôs disease. The gray \nmatter segmentation and feature map were extracted by statistical parametric mapping \nwhich is an analyzing tool and it will not suitable for all the datasets that leads to high \nfalse positive rate. Authors in [31], proposed an automatic classification method for \nAlzheimer‚Äôs detection using T1 MRI images. Initially, the FMRIB‚Äôs software library \n(FSL) and brain extraction tool were used for skull stripping then the gray matter, white \nmatter, and CSF are segmented using FSL-FAST4 automated segmentation tool. Then \nFreeSurfer software was implemented for feature extraction based on cortical thickness \nand 63 features are extracted. The Six classifiers are executed separately for classifica-\ntion into normal, early mild cognitive impairment (EMCI), late mild cognitive impair-\nment (LMCI), and Alzheimer‚Äôs disease. Finally, the SVM with radial basis function \nobtained high accuracy. In this work, the feature extraction was performed using Free-\nSurfer, which resulted in errors in terms of precise borders of a different brain region, \nwhich will affect the true positive rate.\nIn paper [32], authors proposed a deep learning method for early diagnosis of  \nAlzheimer‚Äôs disease using MRI images. Initially, the pre-processing was performed \nusing non-local mean algorithm for normalization, standardization, and resize then the \ndata augmentation was performed to avoid data scarcity. The feature extraction and \nclassification were performed using two methods, the first method was employed by \nCNN and the second method was using the transfer learning method. Finally, the fea -\ntures are extracted and classified into normal, EMCI, LMCI, and AD. In this work, \nthe features are extracted from the overall images without segmenting the GM, WM, \nand CSF which leads to high complexity. In paper [27], the authors proposed a deep \nlearning method for the early detection of Alzheimer‚Äôs disease using T2 MRI images \nfrom two datasets. Initially, the pre-processing was performed to remove the non-brain \ntissues using the skull stripping algorithm. The Enhanced independent component anal-\nysis was used for segmenting the gray matter which is used to estimate the atrophy \nchanges. The CNN based on inception type of block was proposed for feature extraction \nand classification which extract the deep features. Finally, the softmax layer classifies \ninto normal, AD, and MCI. The convolutional neural network was used for feature \niJOE ‚Äí Vol. 19, No. 04, 2023\n27\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nextraction and classification, where it takes long time to train, especially with large \ndataset which leads to high false positive rate. The authors proposed a deep transfer \nlearning model for Alzheimer‚Äôs disease detection using MRI images [33]. Initially, \nthe images are channelized, resize and the images were cropped to remove the white \nspace and enhance the data. Then, the images are augmented to balance the dataset \nand the CNN was utilized for feature extraction, the convolutional layers at starting of \nCNN‚Äôs filters extract the low-level features such as edge, blobs, and color and the net -\nwork end learns the global, high level and complicated features. Finally, by using CNN \nbased transfer learning technique the softmax classifies into AD, MCI, and normal. The \npre-processing was performed to channelize, resized, and crop to remove the white \nspace, where lack of skull stripping, noise reduction, and bias field correction affects \nthe classification accuracy.\nFeature selection-based AD classification was performed by using optimization and \nmachine learning algorithms [34]. Initially, the input images were pre-processed and \nnormalized for improving the quality of the images. The pre-processed images were \nsent to feature selection phase which were done by genetic with logistic regression \nalgorithm. The selected features were sent to classification phase which was done by \nsupport vector machine, extreme gradient boosting, and random forest algorithm which \nclassified the images into AD/NC, AD/MCI. The performance of this research was \nevaluated by ADNI dataset that prove the proposed work achieved better performance \nin AD detection and classification. Here, machine learning algorithm was proposed for \nAD detection which takes high training time for large dataset that reduces the training \nperformance of AD detection and classification. \nDeep learning-based AD detection using MRI images [35]. This research includes \nthree processes such as pre-processing, feature extraction, and classification. Here, \n3D MRI images were taken as an input, and thenpre-processing was performed for \nimage slicing. The sliced images were sent to the CNN algorithm which performs \nfeature extraction. Here, transfer learning was applied to the CNN for efficient fea-\nture extraction. The extracted features were fed into RNN which classified the images \ninto AD or NC. The performance of this research was evaluated by ADNI dataset. \nAD detection was performed by incorporating CNN and VGG-16 using MRI images \n[36]. Initially, MRI images were collected from kaggle for AD detection. After that, \nfeature extraction was performed by VGG-16, based on the extracted features neural \nnetwork was classified the images into four classes such as mild, moderate, very mild, \nand normal. The experimental result demonstrates that the proposed work achieved \nsuperior performance in accuracy, recall, precision, f-measure, AUC and ROC curve. \nHere, features were extracted from the noisy images which lead to less classification \nand detection accuracy. Automatic detection of AD and MCI was performed using \nMRI images [37]. Initially, MRI images were collected from ADNI dataset for AD and \nMCI detection. Then perform image resizing and slicing for accurate detection of AD.  \nHere, deep convolutional neural network extracted the features automatically from the \nsliced MRI images. The proposed convolutional neural network includes depth wise \nand point wise convolutions for increasing the performance of feature extraction which \nextracts the features in multiple views. The classification includes two classes such as \nAD and MCI. Finally, the experimental results show that the proposed work achieved \ngood performance in terms of accuracy, AUC, and ROC curve. \n28\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n4 System model\nIn this research, we mainly focus on the Mathematical modeling of Alzheimer‚Äôs \ndisease detection using Magnetic Resonance Imaging (MRI). This proposed work \nimproves the classification accuracy by performing pre-processing, segmentation and \nfeature extraction. We have considered T1 weighted MRI image from Alzheimer‚Äôs \ndisease Neuro imaging Initiative (ADNI) database. Figure 1 represents the architecture \nof the proposed ST-Seg model. This proposed work consists of three sequential phases \nsuch as: Tri-level Pre-processing, Swin Transformer based Segmentation using MU-net \nand GAN, and Multi-scale feature pyramid fusion module and Classification.\nInput Image\nNoise\nReduction \nSkull\nStripping \nPre-processed Image\nTri-Level Pre-processing\nBias Field\nCorrection\nSwim Transformer Block \nMerging\nBlock\nMerging\nBlock\nMerging\nBlock\nMerging\nEncoder Decoder\nSkip Connection\nGenerator Network\nOR\nDiscriminator\nNetwork \nGround Truth Image\nSegmented Image\nPre-processed Image\nCorrect\nWrong\nMulti Feature Pyramid Feature Fusion and Classification\nSegmented Image\nVGG16\nFeature Pyramid\nFusion \nPosition\nAttention\nChannel\nAttention\nCC\nSoftmax\nFeature Attention Aware\nAlzheimer Normal Classification\nFeature Map\nPatch Partition\nPre-processed Image\nSwim Transformer based Segmentation using MU-net and GAN\nExpectation\nMaximization\nGeodesic\nActive\nContour \nHybrid of\nKuan and\nFrost Filter\nMild Cognitive\nImpairment \nFig. 1. Architecture of proposed ST-Seg model\niJOE ‚Äí Vol. 19, No. 04, 2023\n29\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n4.1 Tri-level Pre-processing\nIn this work, we have taken MRI images from ADNI dataset where we perform \ntri-level pre-processing to enhance the image quality by accomplishing skull stripping, \nnoise reduction and bias field correction.\nNoise reduction. The MRI images have certain noises such as Speckle noise, Gauss-\nian noise, and so on. Removal of these noises is required to enhance the MRI image \nquality. For this purpose, we proposed Hybrid of Kuan and Improved Frost filters \n(HKIF) where these filters remove noise and smoothen the image without removing \nedge features which is shown in Figure 2. Here, the noisy images are fed into HKIF \nwhereas the Kuan filter and frost filter provides the noise removed images which are \nincorporated and provided the final noise removal image. The Kuan filter depends on \nthe effective count of looks which is the main parameter since that needs to adjust \nfor every image to tune the parameters of the filter for obtaining efficient results in \nnoise removal. Here, the Kuan filter automatically evaluated the optimal parameter of \nthe filter based upon edge pixels that provides effective results. Let consider W is the \nwindow centre of the filter with pixel coordinates (p, q). C\nW is the centre pixels in W \nand Œ¥ is weighted value. The Kuan filter gray level pixel positions are given as follows,\n \nFp qC WW W(, )( )/g32/g14/g72/g71 /g72/g16  (1)\nwhere ŒµW represent the mean value inside the filter window W. The weight value of \nthe filter is evaluated as follows,\n \n/g71 /g72\n/g72\n/g32/g16\n/g14\n1\n1\n22\n2\nCC\nC\ni/  (2)\n \nCiW W= œÅŒµ /  (3)\nwhere œÅW is represent the standard deviation in W, and \nCl ookn/g72/g321/ . The lookn \nmetric efficiently manages the smoothing volume which is applied in the image by \nchanging the coefficient of noise variation C\nŒµ. The value of look n is represent the effi-\ncient number of looks ( Œ≥ ‚Äâ‚Äâ) of the given image which is closure to the actual lookn of \nthe noisy image. If the lookn has smaller values, then it represents more smoothening, \notherwise it preserves the edge information of the image. The value of lookn is adjusted \nto control the performance of the filter. The calculation of Œ≥ is defined as follows,\n \n/g74/g80/g85/g32(/ )rr\n2  (4)\nwhere mr represent the mean value of the even image region ( r), and œÅr represent \nthe standard deviation of the even image region. For identifying the uniform areas \nof the image, the image is divided into 25 √ó 25 sub images, and calculated the Œ≥ for \neach sub images, finally average two sub images value of Œ≥\n  for obtaining final Œ≥  value.  \nLet consider look n = Œ≥ at initial stage, and then the values are adjusted by manually \nwhich is calculated as follows,\n30\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n \nlook an =+()1 Œ≥  (5)\nwhere a represent the coefficient of adjustment parameter, and the value of a changes \nwith variance of noise (an) and detail of image (ad) which is formulated as follows,\n \naa adn=+  (6)\nIn this way, image noises are removed by using kuan filter. Parallelly, the noisy \nimages are sent to the improved frost filter for noise removal. Here, the window size \nand tuning factors are adaptively changed based on regional characteristics. The math-\nematical representation of frost filter is defined below,\n \nÔÅâ(, )/ij pe\nxy\nxy xy\nxy\nxy xy\nkh DIx y/g32/g32/g166/g166 /g166/g166\n/g16\n/g80/g80 /g80\n2  (7)\nwhere (i, j) represent the current pixel location, \nÔÅâ(, )ij  is filter output, and pxy is the \npixels values between window centre at \n(, ), ()ij kk >0  represent the factor of tuning \nparameter, and \nhI  represent the variation coefficient which is defined as ration between \nsample mean and standard deviation, and \nD xy  is distance between the pixels to cen -\ntre pixels. The value of k is small, then improved frost filter perform noise removal \nefficiently, but it does not preserve edge information accurately. The value of k is \nincreased, then it effectively preserves the edge information effectively. Hence, the \nsize of the window is adjusted dynamically based on adaptive windowing method.  \nLet assume (i\n, j) represent the current location of the pixel, and the threshold calculation \nof the current window is defined as follows,\n \nhij\nij\nij\n/g32\n/g85\n/g80  (8)\n \nTr\nS\nij\nf\nwi j\nf/g32/g14\n/g14\n/g16/g11/g12\n/g167\n/g169\n/g168\n/g168/g168\n/g183\n/g185\n/g184\n/g184/g184\n/g117/g69\n/g85\n/g851\n12\n1\n2  (9)\nwhere \n¬µij  represent the mean value, and \nœÅij  is the standard deviation of the current \nwindow. \nTrij  is represent the threshold value which effectively smoothened the image. \nœÅf\n2\n denotes the noise variance and \nSwi j  represent the size of the current window, \nŒ≤  is \ndenotes the system parameter. Here, window centre size is changed dynamically that is \nexpressed as follows,\n \nS\nMi nS Si fS Tr\nMa xS Si f\nwij\nwijw Ma xw ij ij\nwijw Mi n\n/g32\n/g14/g170/g172 /g186\n/g188 /g100\n/g16/g170\n/g172 /g186\n/g188\n2\n2\n,,\n,, S\nST rwiji j/g33\n/g173\n/g174/g176\n/g175/g176  (10)\niJOE ‚Äí Vol. 19, No. 04, 2023\n31\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nwhere \nSwM in  represent the minimum window size, and \nSwM ax  and maximum window \nsize. Here, variation coefficient \nhij  is not bigger than \nTrij , then the window size is grad-\nually increases for obtaining better results. In this way, improved frost filter performs \nnoise removal and edge preservation. Finally, two outputs of kuan filter and improved \nfrost filter are combined and produce final noise removed images with effective edge \npreservation.\nKuan Filter Improved Frost filter\nNoise removal 1 Noise removal 2\nFinal Result (1 + 2)\nNoise Removed Image\nInput Image\nFig. 2. Process of noise reduction\nSkull stripping. The main process of pre-processing is skull stripping in Alzheimer‚Äôs \ndisease detection, which is the process of segmenting the brain tissues by removing non-\nbrain tissues (fat, scalp, muscles, dura etc.) from surrounding region which improves \nthe robustness of the disease detection. For that purpose, we proposed GAC is a contour \nmodel that adjusts the smooth curve and constructs closed contour for region. Figure 3 \nrepresents the skull stripping input and output. Initially, we defined the edge indicator \nof the image \n()E I\n which is expressed as follows,\n32\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n \nE\ngI\nI /g32\n/g14/g146/g117\n1\n1\n2\n/g85  (11)\nwhere \ngI/g85/g117  is used to remove the noise by performing Gaussian smoothing, and \n‚àá  \nrepresent the function of gradient. The function of edge indicator takes small values on \nimage boundaries compared to other regions. So, we can introduce level set function\n() :lsfr /g45/g58/g111\n which is formulated as follows, \n \n/g45()\n,\n,\n,\ns\nInfs ts B\nsB\nInfs ts B\ntB in\ntB out\n/g32\n/g16/g16 /g143\n/g143/g119\n/g16/g143\n/g173\n/g174\n/g176/g176\n/g175\n/g176\n/g176\n/g143/g119\n/g143/g119\n2\n2\n0  (12)\nwhere \nst‚àí 2  is represent the distance between s and t, and \nBin , \nBout  and \n‚àÇB  represent \nthe outside, boundary, and inside of the object respectively. Here, segmentation contour \nœë\n has the value of zero. The energy calculation of proposed GAC is defined as follows,\n \nZ /g45/g45 /g45/g11/g12/g32/g146/g179/g143\n/g58\nn\ndxE I ()  (13)\n ·º°\n/g143/g32 /g143\n/g14/g143/g11/g12/g83/g4522  (14)\nwhere \n‚àá  represent the gradient operator, here energy is calculated the edge indicator \nline integral with zero level set contour, which results slight length curve. The GAC \nenergy function is integrated with penalty and area which are expressed as follows,\n »¥ \n/g45/g45/g11/g12/g32/g16\n/g58/g179Ed xI\nn\nh()  (15)\n \nN /g45/g45/g11/g12/g32/g146/g16/g11/g12/g179\n1\n2 1\n2\n/g58\nn\ndx  (16)\nwhere \nh()œë  represent the function of Heaviside, the weight value of area and \ninitial contour values are essential. If the initial value of contour is present inside the \nobjects, the weight values of area must be positive so that shrink the zero \nlsf\n of contour \nthrough evolution. On the divergence, if the initial contour is pinched inside the object, \nthe area weight values are negative so that it enlarges to the zero-level set contour.  \nThese types of shrinking and expanding procedure are followed in GAC for extracting \nbrain regions, and removing skull regions in the MRI images. \n·º°\niJOE ‚Äí Vol. 19, No. 04, 2023\n33\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n(a) (b)\nFig. 3. (a) Noise removed image (b) Skull stripping image\nPseudocode for Pre-processing\nInput: Input MRI image \nI\nOutput: Pre-processed Image (ùìÖI)\nBegin\n  For eachIdo\n   Initialize Kuan filter \n   Perform noise removal by Kuan filter from Eq. (1)‚ÄìEq. (6)\n   Adjust \nŒ≥  by manually using Eq. (4)\n   Obtain noise removed image \nNR I()1  by Kuan filter\n   Initialize improved frost filter \n()IF\n   Perform noise removal by \nIF  using Eq. from (7)‚ÄìEq. (10)\n   Adjust the \nS wi j  dynamically using Eq. (10)\n   Obtain noise removed image image \nNR I()2  by IF\n \nNR NR NRII I/g109/g14im agei ma ge() ()11\nEnd For\n  Return \nNR I\nFor each \nNR I do\n   Perform skull stripping using Eq. (11)‚ÄìEq. (16)\n   Perform bias field correction by EM using Eq. (12) and Eq. (13)\n   Obtain pre-processed image ùìÖI\n End for\n Return ùìÖI\nEnd\nBias field correction. The bias field correction is the step of removing intensity \nin homogeneity or non-uniformity that occurs from magnetic field. The bias filed is \nremoved by cleaning the spatial frequencies. In MRI images, the useful information \nis presented in the higher spatial frequency compared to intensity non-uniformity, \nhence we need to eliminate low spatial frequency non-uniformity. In this research, \nthe bias field correction is performed using by Expectation-Maximization (EM) algo-\nrithm which makes the difference of brain tissues clearly and it is mainly performed \nto reduce classification errors. The main objective of EM is to detect maximum proba-\nbility solution for bias field correction. The observed intensity non-uniformity from N  \nimages which are expressed as \nNn nn mIi ii/g32/g125{, ,, }() () ()12\n. The latent elements of the \nobserved non-uniformity are \nVV VVmm mm N/g32/g125{, ,. .() () () }12 1  Let \n{, }NVIm  is a complete \ndata and its likelihood function is expressed as \nln (, |)lN VIm Œ± , whereas \nŒ±  represent \nthe total metrics of the model. The proposed EM algorithm is used to maximize the \n34\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nprobability and acquire entire metrics in the model by performing two steps which \nare \nistepa nd jstep . In \nistep , the distribution of posterior for \nVm  is obtained by using \nthe present model metric \nŒ±old .  The complete posterior distribution-based likelihood \nfunction is defined as follows,\n \nZi lN VNIm I(, )[ log( ,| )| ,]/g68/g68 /g68/g68oldo ld/g32  (17)\nIn stepj, the likelihood function is maximized to inform the model metrics which are \nexpressed as follows,\n \n/g68/g68 /g68\n/g68\noldo ldAr g/g32ma x( ,)Z  (18)\nwhere \nŒ±old  represent the current metric of the model, step i and j are performed \nalternatively until the meeting constraint is met. In this way, bias filed correction is \nperformed in this research which increases the intensity of the brain tissues that helps \nto detect AD accurately.\nSwin transformer based segmentation.  After successful pre-processing, we per -\nform Swin transformer-based segmentation to improve the classification accuracy. In \nthis research, Swin Transformer based Segmentation using Modified U-net and Gener-\native Adversarial Network was implemented for segmentation by considering cortical \nthickness, color, texture and boundary information, the Gray matter (GM), White matter \n(WM) and Cerebrospinal fluid (CSF) are segmented from the pre-processed image. The \ndetailed architecture of proposed GAN is shown in Figure 4. In this research, we pro -\nposed a hierarchical transformer that is computed with shifted windows which consist \nof swin based Multi-head Self Attention (MSA) module. The Swin transformer is a type \nof vision transformer, it built the hierarchical feature map by merging image patches \nin deep layers and it has low computation complexity and high accuracy. The input of \nthe swin is divided into multiple patches of \n(, ,)‚Ä≤‚Ä≤ ‚Ä≤hw d\n and dimension of \n/g99/g117/g99/g117/g99/g117hw du .  \nInitially, we used the patch partition to made an order of 3D tokens with dimension \nof \nh\nh\nw\nw\nd\nd/g99\n/g170\n/g172/g171\n/g186\n/g188/g187/g117/g99\n/g170\n/g172/g171\n/g186\n/g188/g187/g117/g99\n/g170\n/g172/g171\n/g186\n/g188/g187  and develop them into implanting space with dimension c.  \nThe swin transfer windowing method used a window size \nHHH√ó√ó  to regularly divide the \n3D tokens into \n/g99/g170\n/g171/g171\n/g186\n/g187/g187/g117/g99/g170\n/g171/g171\n/g186\n/g187/g187/g117/g99/g170\n/g171/g171\n/g186\n/g187/g187\nhw d\nHH H  sections at a given layer in the transformer encoder.  \nLater in layer \nL +1 , then the window is partitioned and shifted by \nHHH\n222\n/g171\n/g172/g171\n/g187\n/g188/g187\n/g171\n/g172/g171\n/g187\n/g188/g187\n/g171\n/g172/g171\n/g187\n/g188/g187\n/g167\n/g169\n/g168\n/g183\n/g185\n/g184,,  \nvoxels. The output of layers in encoder is expressed as follows,\n \n)ÀÜ( ) MSA( ( ( 1))) ( 1)LOL w N OL OL=‚àí ‚àí + ‚àí  (19)\n \n)ÀÜ ) ÀÜ() M L P ( ( () ) (LOL N OL OL=+  (20)\n \n( 1) MSA( ( ( ))) ( )ÀÜ\nLOL s w N OL OL+= ‚àí +  (21)\niJOE ‚Äí Vol. 19, No. 04, 2023\n35\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n \n(1 ) M L P ( ( (1 )ÀÜÀÜ ))) ( 1LOL N OL OL+= + + +  (22)\nwhere w ‚Äì MSA, and sw ‚Äì MSA represent the even and window partitioning MSA \nmodule respectively, and \nÀÜ ()OL  represent the output of \nw‚àíMSA , and \nÀÜ ( 1)OL +  is the \noutput of sw ‚Äì MSA, NL represent the normalization of layer. The output of the swin \ntransfer is fed to the encode which is placed in the Generator.\nThe Modified U-net (MU-net) is used for semantic segmentation where the skip- \nconnection characteristics will append every upsampled feature map at decoder with \ncorresponding encoder and the Generative adversarial network (GAN) is deep learning- \nbased architecture that can train and classify the medical data more effectively. The \nGAN‚Äôs architecture consists of generator and discriminator network, the generator net-\nwork includes MU-net where the swin transformer technique is applied in encoder \nand the decoder executes the result of segmentation. The proposed MUNet per -\nformed down sampling in encoder and up sampling in decoder with skip connections.  \nThe generator pursues to learn a map \nGe f: ‚Üí\n which produces a binary map of seg -\nmentation f from input image e based on training data. The map of discriminator \n{, }ef  \nis covered of segmentation map and input image with the range in between 0 to 1, in \nwhich the discriminators evaluate the f is a ground truth mask or predicted map from \ngenerator. For segmentation, the objective function of GAN is expressed as follows, \n \nZi lN VNIm I(, )[ log( ,| )| ,]/g68/g68 /g68/g68oldo ld/g32  (23)\nInput\nConv\nLeaky ReLU\nConv\nBN\nLeaky ReLU\nReLU\nBN\nDeconv\nReLU\nDeconv\nTanh\nDeconv\nConv\nSigmoid\nConv\nBN\nLeaky ReLU\nLeaky ReLU\nInput\nConv BN\nSegmented Image\nGround\nTruth Image\nPre-processed\nimage\nor Predicted\nLabels\n1\n0\nReal\nFake\n4X4X64\n4X4X128\n4X4X128\n4X4X256\n4X4X256\n4X4X512\n4X4X32 4X4X64 4X4X128 4X4X256 4X4X1\nGenerator\nDiscriminator\n4X4X1\nConcat\n4X4X32\n4X4X32\n4X4X64\nFig. 4. Architecture of GAN for segmentation\n36\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nThe objective function aligns with maximum of ƒé(e,f  ), and minimum of ƒé(e, ƒú(e)) \nthat pursuesto train the discriminator ƒé for making correct decision. The generator \ngenerates the output from real data to hinder the discriminator ƒé that makes correct \ndecision and also minimizes the objective function. Here, loss function is calculated \nbased on binary cross entropy which is common in segmentation which is defined as \nfollows, \n \nseg , ~pdata ( , )\nÀÜ ÀÜÀÜ(G) [ .log G( ) (1 ).log(1 G( ))]ef e fL fef e= ‚àí ‚àí‚àí ‚àíÔÅÖ  (24)\nBy integrating both segmentation loss and Objective function of GAN, we get the \nsegmentation results from optimal generator is defined as follows, \n \nƒúƒé\n= Arg min [max LGAN (ƒú, ƒé)] + Œ≥ Lseg (ƒú)ƒú *  (25)\nwhere Œ≥  represent the weighting  parameters which can balanced the objective func-\ntions. Here, the skip connections of the U-Net are connected between decoder and \nencoder allowing inserting of information in the encoder for every resolution. The pro-\nposed MU-Net includes batch normalization, Leaky ReLU function in encoder and \nReLU function for decoder, and pooling layers. Here, convolutional layers with size of  \n4 √ó 4 tracked by leaky ReLU function and batch normalization, in the expanding arm. \nFor mapping every feature vector to chosen number of classes such as affected region \nand non-affected region. We used the final convolutional layer with the size of 4 √ó 4 and \ntanh activation function to map the estimated values for classification. The mathemati-\ncal formulation of ReLU and Leaky ReLU are expressed as follows,\n \nRe ()LU if\nifj j\njj/g32 /g100\n/g33\n/g173\n/g174\n/g175\n00\n0  (26)\n \nL eakyR eLU( ) if\nifj jj\nj/g32 /g33\n/g118/g100\n/g173\n/g174\n/g175\n0\n0j  (27)\nwhere \n‚àù  denotes the small constant range between 0.1 to 0.3. The ground truth \nimage and estimated segmentation image is given to the discriminator for detecting \noutput is real (one) of fake (zero). The discriminator also used seventeen convolu -\ntional layers with the size of 4 √ó 4, and batch normalization and leaky ReLU activation \nfunction with the size of 2 √ó 2. The final output of discriminator is between zero to \none that determines the probability of segmentation results. Then, the discriminator \nnetwork will evaluate the segmentation result with ground-truth image whether the \nsegmentation performs accurately. And if the segmentation doesn‚Äôt perform precisely \nthen the process repeated again to implement accurate segmentation, by this way the \nsegmentation accuracy increase.\niJOE ‚Äí Vol. 19, No. 04, 2023\n37\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nPseudocode for Segmentation\nInput: Pre-processed Image (ùìÖI) \nOutput: Segmented Image\nBegin\n Initialize segmentation features\nFor each ùìÖI do\n   T rain G and D based on features \n   Initialize Encoder \nÔÅÖ\n   Perform swin transfer mechanism ()\n   Divide ùìÖI into multiple patches with \n‚Ä≤‚Ä≤ ‚Ä≤hw d,,\n   Output of encoder \n   Learn binary map of segmentation by G\n   Compute objective function of GAN \n   Compute loss function of GAN\n   Obtain segmentation results by G \n   Compute activation function for encoder \n   Compute activation function for decoder \n   Evaluate the segmentation results by D \n  End For\n Return \nS I\nEnd\n4.2 Multi-scale feature pyramid fusion module based AD detection\nThe feature extraction is performed from the segmented region, to improve the \nclassification accuracy multi-scale feature extraction was performed. For that purpose, \nwe proposed MSFP-VGG16 which is shown in Figure 5. Here, the VGG16 is the back-\nbone of the architecture, which utilizes for feature extraction and classification. The \nVGG16 is type of CNN architecture that performs efficiently with deep network and \nsmall convolution filter which has low training cost and high performance. Initially, \nthe VGG16 was implemented to extract features such as textural, statistical, structural, \nedge, blobs, color and contour then the feature map was obtained from feature pyra -\nmid fusion module which was implemented to fuse the local and global information \nbased on multi-scale feature extraction. This will improve the feature representation \nand classification accuracy. Initially, we select first ten layers of VGG16 for feature \nextraction layer in MSFP network. After that, feature maps are attained through the fea-\nture extraction layer and forward to fusion of feature pyramid network which perform \nmulti scale feature extraction. The multi scale feature maps are sent to the attention \nmodule for fusing both global and local information of the features. The generated fea-\ntures maps are divided into multiple blocks and perform group convolutions (GC) 3 √ó 3  \nwith various dilations. The Figure 5 represents the architecture of feature pyramid \nfusion model, which divide the feature maps into four blocks with the size of 3 √ó 3 and \ndilation rate of \nRR(, ,, )=1 234\n whereas the group number is increased with 23 in pyra-\nmid structure. The output feature map (fm) is defined as follows,\n38\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n \nfb\nbn bG Ri\nfb nb GR i\nf\nmi\nii i\nmi ii\nmk\n()\n(, ,, ),\n(, ,, ),\n(\n()/g32\n/g32\n/g32\n/g16\nGC\nGC\nGC\n1\n2\n1\nÔÅç\n1 1() ,, ,) ,bn bG Ri kii i /g32\n/g173\n/g174\n/g176/g176\n/g175\n/g176\n/g176  (28)\nwhere \nGC (, ,, )bn bG Rii i  represent the pyramid group convolution, k is the count \nof layer, nbi is denotes the count of blocks, Ri is represent the dilation rate, G i count \nof groups in every convolution layer. In this research, we reduce the computational \ncost by using group convolution operation as a replacement of vanilla convolution.  \nThe computational cost of GC is defined as follows,\n \nCost ()\n()\nGt ff\ntf fh w\nGmi nm out\nmi nm out\n,, ,\n()\n()/g11/g12 /g32\n/g117/g117 /g117/g1172  (29)\nwhere t represent the kernel size of convolution, and fm(in) represent the count of input \nin feature map, and fm(out) represent the count of output in feature map, h √ó w represent \nthe height and weight of the feature map. The feature maps are fed into attention layers \nwhich helps to enhance the robustness of feature extraction. In this research, we used \nboth position and channel attention mechanism. \nConY-2\nConY-3\nConY-4\nConY-5\nCNN\nGC1\nGC2\nGC4\nGC8\nGC1\nGC2\nGC4\nGC1\nGC2\nGC1\nC1\nC2\nC3\nC4\nC1\nC2\nC3\nC1\nC2\nPosition\nAttention\nChannel\nAttention\nFeature Map\nC\nCS S SC C\nS Split\nC Concat\nGC2\nGroup 1\nGC3\nGroup 2\nGC1 GC4\nGroup 3\nGroup 4\nR = 1\nR = 2\nR = 3\nR = 4\nDilated Convolution\nSoftmax\nConv +\nReLU\nmaxpool\nFully connected +\nReLU\nSoftmax\nNormal MCIA D\nFig. 5. AD detection using MSFP-VGG16\nThe position attention is used to shows the spatial relationship between every posi -\ntion in the image. The channel attention provides the relationship between various \nchannels in the feature map by selecting different weight values. The output feature \niJOE ‚Äí Vol. 19, No. 04, 2023\n39\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nmap from feature pyramid fusion model is send to convolutional layer for obtaining \ntwo feature maps \n(, )GG12  which are reshaped to Rnc√ón whereas nc represent the number \nof feature map channels, and n = h √ó w represent the image pixel counts. The final \nfeature map output is sent to the softmax layer for getting spatial relationship between \nmatrix ∆∫\n()sR nn/g143/g117\n which is expressed as follows,\n ∆∫ij\ns  \n/g32\n/g166\neG G\neG G\nji\ni\nn jj\n(. )\n(. )\n21\n21  (30)\nwhere ∆∫ij\ns  represent the relationship between ith and jth position of the feature map. \nWe multiplied the G1 √ó ∆∫(s) and reshaped the result to Rh√ów√ónc for obtaining position \nattention-based map. Next the spatial parameter \n()ÔÉ≥  is multiplied with \nGs()  which is \ndefined as follows,\n \n/g99/g32/g117 /g117/g14/g117/g117 /g117/g117 /g117Gs() () ()ÔÉ≥ GG GGnc nn cn Tn cn nc hw\n11 1  (31)\nFinally, we got two feature maps \n(, )FF Rnc hw\n12 /g143/g117/g117  from feature pyramid fusion \nmodel. The final global connection matrix ’° \n()cR cc/g143/g117  is calculated by softmax layer \nthat is defined as follows,\n ∆∫ij\nc\n/g32 /g152\n/g152/g166\neF F\neF F\nji\ni\nn jj\n()\n()\n21\n21  (32)\nwhere ∆∫ij\nc represent the weight value of channel, after we perform matrix multi -\nplication ’° \n()cF√ó 2 . We multiply the scale parameter of the channel \nI  and perform \nelement-based addition which is defined as follows,\n \n/g99/g32/g117 /g117/g14/g117/g117 /g117/g117 /g117FC FF FFcn cn Tc nc hw() (( () ))I 21 2  (33)\nThe final feature map of feature attention-based module is defined as follows,\n \nA 2ch w FC Gs/g117/g117/g32/g134/g99/g99() ()  (34)\nwhere \n‚Ä≤‚Ä≤FC Gs() ,( )an d  represent the channel connection feature map and spatial \nconnection feature map respectively, and \n‚äï  is the concatenation operation. Finally, \nbased on the feature extraction the softmax layer classify into three classes such as \nnormal, AD, and MCI.\n40\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nPseudocode for AD detection\nInput: Segmented image \n()S I\nOutput: Disease Detection\nBegin\n Initialize features ·∫ù\n     ·∫ù ‚Üê {tf , stf , sf , ef , bf , cf , Crf } \n Initialize MSFP-VGG16\nForeach SI do\n  Extract ·∫ù from SI using VGG16\n  Divide feature map into 4 blocks\n  Obtain feature map output using eqn (23)\n  Calculate computation cost of GC\n  Forward the output feature map into convolution layer\n  Get two feature map \n(, )GG12\n  Get spatial relationship of feature map \n  Get channel relationship of feature map\n  Get final feature map \n  Obtain classification results (N, AD, MCI)\nEnd For\nEnd\n5 Experimental results\nThis section illustrates the experimental results of proposed ST-Seg model which \nalso includes four sub divisions such as dataset description, simulation setup, compar -\native analysis, and research summary. This section also proved that the proposed work \nachieved superior performance in AD detection compared to existing approaches.\n5.1 Dataset description\nIn this research, we have used Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI) \ndataset for AD detection. This dataset includes T1 weighted brain MRI images. This \ndataset provides the data of AD which also includes 3D volumetric data. That becomes \nlightly difficult to work on 3D images, hence we have considered only 2D images \nwhich are extracted from the ADNI dataset that includes three classes of data such as \nAD, MCL, and normal subjects. The details of dataset are shown in table. Figure 6 \nrepresents the sample brain images of ADNI dataset. Table 2 depicts the training and \ntesting images presented in the ADNI dataset. \nTable 2. Training and test images in ADNI\nClasses Test Train Total\nAD 26 145 171\nNormal 87 493 580\nMCI 35 198 233\nTotal 148 836 984\niJOE ‚Äí Vol. 19, No. 04, 2023\n41\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n5.2 Simulation setup\nThis section illustrates the simulation setup of the proposed ST-Seg model which \nis implemented by Matlab R2020a simulation tool that helps to enhance the perfor -\nmance of the simulation results. The simulation of this research is conducted for pre- \nprocessing, segmentation, and AD detection by extracting muti scale features from \nthe segmented image. Table 3 illustrates the system specification which includes both \nhardware and software specifications.\nTable 3. System specifications\nHardware  \nSpecifications\nCPU 3.00GHz\nRAM 6GB\nStorage of Hard Disk 1 TB\nSoftware  \nSpecifications\nOperating system Windows 10 pro\nSimulation tool Matlab R2020a\nProcessor Intel (R) core‚Ñ¢ i5-4590S\n(a) (b) (c)\n(d) (e) (f )\n(g) (h) (i)\nFig. 6. (a)‚Äì(c) sampled normal brain images, (d)‚Äì(f ) sample  \nAD brain images, (g)‚Äì(i) sample MCI images\n5.3 Comparative analysis \nThis section explains the comparative analysis of the proposed and existing models. \nIn this research, we compare two existing works such as BEMD  [1], and Ex-PFE [38] \nwith our proposed ST-Seg model. The performance of the proposed work is compared \n42\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nwith existing works in terms of accuracy, sensitivity, specificity, positive predictive \nvalue, and confusion matrix. \nImpact of accuracy. This metric is used to evaluate the accuracy of AD detection \nand classification. It is calculated by dividing the sum of true positive rate and true \nnegative by sum of all samples. The mathematical expression of accuracy is defined \nas follows,\n «û\n/g32\n/g14\n/g14/g14/g14\ntt\ntt ff\npn\npn pn  (35)\nwhere \ntp  is true positive, \ntn  is true negative, \nfp  represent the false positive rate, and \nfn  \nrepresent the false negative. Figure 7 represents the comparison of accuracy with num-\nber of images. The comparison proved that the proposed ST-Seg model achieves high \naccuracy compared to existing models. The existing BEMD model does not perform \nnoise removal or skull stripping, hence it performed AD detection with the presence of \nnoise and non-brain regions that leads to poor detection accuracy. In addition, features \nare extracted by local binary pattern algorithm to extract the features from the whole \nbrain image which leads to high complexity and misclassification that reduces detection \naccuracy. In Ex-PFE model perform AD detection by considering both brain and non-\nbrain images which leads to misclassification and poor accuracy. Additionally, it per -\nforms AD detection by considering single feature (i.e., textural) which is not enough for \naccurate AD detection which leads to less accuracy compared to our proposed ST-Seg \nmodel. In our proposed ST-Seg model consider many features (i.e., textural, structural, \nedge, blobs, color, and contour) for detecting AD which leads to high accuracy, addition-\nally, we remove non-brain (i.e., skull stripping) tissues from the MRI image for increas-\ning accuracy. The proposed ST-Seg model achieves (0.98) higher than existing models. \n0.7\n0.74\n0.78\n0.82\n0.86\n0.9\n0.94\n0.98\n20 40 60 80 100\nAccuracy\n# of images\nBEMD Ex-PFE ST-Seg\nFig. 7. Comparison of accuracy\nImpact of specificity. This metric is used to measure the correctness of detecting \nsuch as AD, normal, and MCI using MRI images. The mathematical representation of \nspecificity is defined as follows,\n √ü \n/g32/g14\nt\ntf\nn\nnp  (36)\niJOE ‚Äí Vol. 19, No. 04, 2023\n43\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nwhere √ü represent specificity, the definition of specificity is denoted as the true  \npositive rate is divided by the sum of true negative and false positive. Figure 8 \nillustrates the comparison of specificity with respect to number of images which proved \nthat the proposed work achieved superior performance in terms of specificity com -\npared to existing works. The proposed ST-Seg model perform noise removal, skull \nstripping, and bias field correction before initiating segmentation which increases true \npositive rate because of reducing unwanted tissues and noises form the image that \nleads to high specificity. The existing BEMD model perform only contrast enhance -\nment and Ex-PFE model perform only bias field correction which reduces correctness \nof detection due to poor quality of images. The Ex-PFE model consider only texture \nfeature for disease detection which increases misclassification and reduces specificity. \nIn addition, the BEMD model does not focusing on segmentation for detection which \nis one of the significant processes to accurately classified the disease, hence it achieves \nless specificity. The proposed ST-Seg model achieves high (0.93) specificity compared \nto existing models.\n20 40 60 80 100\n# of images\nBEMD Ex-PFE ST-Seg\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nSpecificity\nFig. 8. Comparison of specificity\nImpact of sensitivity. This metric is used to calculate the detection ability of positive \n(i.e., affected by disease) samples, which is also known as recall. In addition, it helps to \ncalculate the applicability from the detected samples. The mathematical expression of \nsensitivity is defined as below,\n ’á \n/g32/g14\nt\ntf\np\npn  (37)\nwhere ’á represent the sensitivity, Figure 9 represents the comparison of sensitiv -\nity with respect to number of images which shows that the proposed ST-Seg model \nachieves high sensitivity compared to existing models. The proposed ST-Seg model \nextract multi scale features for improving detection accuracy, but the existing works \nused single scale features which provide limited information about the features that \nleads to less detection accuracy and sensitivity. In addition, we have used attention lay-\ners (i.e., spatial and channel) in VGG-16 for given an importance for every feature \nthat increases sensitivity value during detection and classification of Alzheimer disease.  \nThe proposed ST-Seg model achieves high (0.9) sensitivity compared to existing models.\n44\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n20 40 60 80 100\n# of images\nBEMD Ex-PFE ST-Seg\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nSensitivity\nFig. 9. Comparison of sensitivity\nImpact of positive predictive value. This metric is used to successfully identify the \npositive results. In other words, the proportion between truly identified as positive to \nall the samples that had positive results (i.e., normal persons). The mathematical repre-\nsentation of positive predictive value (‚Ç±) is defiend as follows,\n ‚Ç± \n/g32/g14\nt\ntf\np\npp  (38)\nFigure 10 represents the comparison of proposed and existing positive predictive \nvalue with respect to number of images. The comparison result shows that the proposed \nwork achieved better performance in positive predictive value compared to existing \nworks. In this research, we have taken multiple features for segmenting GM, WM, and \nCSF using ST-MUNet algorithm which performs semantic segmentation that improves \nhigh positive predictive value, because discriminator evaluate the segmentation result \nto ground truth to improve segmentation accuracy which also increases positive \npredictive values. In addition, the proposed MSFP-VGG-16 takes multiple features \n(i.e., statistical, textural, structural, edge, color, blobs, and contour) in multi scale for \nimproving classification accuracy. But the existing works only consider limited features \nfor segmentation and classification with single scale for detecting AD which reduces the \npositive predictive value compared to out proposed work. The ST-Seg model achieves \nhigh positive prediction value (0.7) in AD detection.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.96 0.97 0.98 0.99 1\nPositive Predictive Value\nNegative Predictive Value\nST-Seg\nFig. 10. Comparison of positive predictive value\niJOE ‚Äí Vol. 19, No. 04, 2023\n45\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nImpact of confusion matrix. It is the matrix to represent the performance of AD \ndetection and classification achieved by the proposed ST-Seg model. It helps to pre -\ndict the tiny errors in the detection or classification. The confusion matrix represent \nthe matrix of square format which includes \nCi jI ,/g11/g12  represent the number of images \nwhich are detected in the dataset whereas i represent the true label and j represent the \npredicted label. Table 4 represents the confusion matrix of the proposed ST-Seg model.\nTable 4. Confusion matrix\nTrue Label\nAD 0.98 0 3\nNormal 0 0.97 0\nMCI 3 0 0.95\nAD Normal MCI\nPredicted Label\nTo make it clear, Table 5 illustrates the average values of the performance metrics of \nboth proposed and existing models.\nTable 5. Numerical analysis of performance metrics\nPerformance Metrics Scenario\nProposed vs. Existing Systems\nBEMD Ex-PFE ST-Seg\nAccuracy # of images 0.80 0.9 0.98\nSpecificity # of images 0.73 0.84 0.93\nSensitivity # of images 0.69 0.79 0.9\nPositive predictive value Negative predictive value ‚Äì ‚Äì 0.7\n6 Conclusion \nIn this research, we proposed a deep learning approach for detecting AD using MRI \nimages. Initially, preprocessing is performed to increases the quality of the images, for \nthat noise removal is performed by HKIF algorithm which removes the noise from the \nbrain MRI images, and then perform skull stripping to eliminate non-brain tissues and \nextracting brain tissues which increases the detection accuracy which is done by GAC \nalgorithm. After completed skull stripping, bias field correction is performed to correct \nthe intensity of the non-uniformity which increases the quality of the brain images \nwhich is done by EM algorithm. The preprocessed images are fed into segmentation for \nsegmenting gray matter, white matter, and cerebrospinal fluid by considering cortical \nthickness, color, texture, and boundary information which is done by ST-MUNet. Here, \nthe preprocessed images are divided into multiple patches for performing segmenta -\ntion which increases the accuracy of segmentation. After completed segmentation, AD \ndetection and classification is done by MSFP-VGG16 algorithm which extracts the \nfeatures from multi scales and given an attention to features in terms of spatial and \nchannel which increases the accuracy of detection. Finally, the MRI brain images are \nclassified into three classes such as normal, AD, and MCI. \n46\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n7 References\n [1]  A. Deshmukh, M. V Karki, B. SR, and H. JP, ‚ÄúDeep neural network model for automated \ndetection of Alzheimer‚Äôs disease using EEG signals,‚Äù International Journal of Online & \nBiomedical Engineering, vol. 18, 2022. https://doi.org/10.3991/ijoe.v18i08.29867\n [2]  S. PC, V . Sherimon, P. SP, R. V . Nair, and R. Mathew, ‚ÄúA systematic review of clinical \ndecision support systems in Alzheimer‚Äôs disease domain,‚Äù International Journal of Online \n& Biomedical Engineering, vol. 17, 2021. https://doi.org/10.3991/ijoe.v17i08.23643\n [3]  R. Hedayati, M. Khedmati, and M. Taghipour-Gorjikolaie, ‚ÄúDeep feature extraction method \nbased on ensemble of convolutional auto encoders: Application to Alzheimer‚Äôs disease \ndiagnosis,‚Äù Biomedical Signal Processing and Control, vol. 66, p. 102397, 2021. https://doi.\norg/10.1016/j.bspc.2020.102397\n [4]  B. A. Mohammed, E. M. Senan, T. H. Rassem, N. M. Makbol, A. A. Alanazi, Z. G. \nAl-Mekhlafi, et al., ‚ÄúMulti-method analysis of medical records and MRI images for early \ndiagnosis of dementia and Alzheimer‚Äôs disease based on deep learning and hybrid methods,‚Äù \nElectronics, vol. 10, p. 2860, 2021. https://doi.org/10.3390/electronics10222860\n [5]  S. Murugan, C. Venkatesan, M. Sumithra, X.-Z. Gao, B. Elakkiya, M. Akila , et al., \n‚ÄúDEMNET: A deep learning model for early diagnosis of Alzheimer diseases and dementia \nfrom MR images,‚Äù IEEE Access, vol. 9, pp. 90319‚Äì90329, 2021. https://doi.org/10.1109/\nACCESS.2021.3090474\n [6]  M. Raju, T. Sudila, V . P. Gopi, and V . Anitha, ‚ÄúClassification of mild cognitive impairment and \nAlzheimer‚Äôs disease from magnetic resonance images using deep learning,‚Äù in 2020 Interna-\ntional Conference on Recent Trends on Electronics, Information, Communication & Tech -\nnology (RTEICT), 2020, pp. 52‚Äì57. https://doi.org/10.1109/RTEICT49044.2020.9315695\n [7]  S. Aruchamy, V . Mounya, and A. Verma, ‚ÄúAlzheimer‚Äôs disease classification in brain MRI \nusing modified kNN algorithm,‚Äù in 2020 IEEE International Symposium on Sustainable \nEnergy, Signal Processing and Cyber Security (iSSSC),  2020, pp. 1‚Äì6. https://doi.\norg/10.1109/iSSSC50941.2020.9358867\n [8]  R. De Feo, A. Shatillo, A. Sierra, J. M. Valverde, O. Gr√∂hn, F. Giove , et al., ‚ÄúAutomated \njoint skull-stripping and segmentation with Multi-Task U-Net in large mouse brain \nMRI databases,‚Äù NeuroImage, vol. 229, p. 117734, 2021. https://doi.org/10.1016/  \nj.neuroimage.2021.117734\n [9]  O. M. Al-hazaimeh, A. A. Abu-Ein, N. M. Tahat, M. m. A. Al-Smadi, and M. M. Al-Nawashi, \n‚ÄúCombining Artificial Intelligence and Image Processing for Diagnosing Diabetic \nRetinopathy in Retinal Fundus Images,‚Äù International Journal of Online & Biomedical \nEngineering, vol. 18, 2022. https://doi.org/10.3991/ijoe.v18i13.33985\n [10] A. Hoopes, J. S. Mora, A. V . Dalca, B. Fischl, and M. Hoffmann, ‚ÄúSynthStrip: Skull-Stripping \nfor Any Brain Image,‚Äù arXiv preprint arXiv:2203.09974, 2022. https://doi.org/10.1016/ \nj.neuroimage.2022.119474\n [1 1]  O. Al-hazaimeh, S. A. Alomari, J. Alsakran, and N. Alhindawi, ‚ÄúCross correlation‚Äìnew \nbased technique for speaker recognition,‚Äù Int J Acad Res, vol. 6, pp. 232‚Äì239, 2014. https://\ndoi.org/10.7813/2075-4124.2014/6-3/A.33\n [12] N. Gharaibeh, O. M. Al-Hazaimeh, B. Al-Naami, and K. M. Nahar, ‚ÄúAn effective image pro-\ncessing method for detection of diabetic retinopathy diseases from retinal fundus images,‚Äù \nInternational Journal of Signal and Imaging Systems Engineering, vol. 11, pp. 206‚Äì216, \n2018. https://doi.org/10.1504/IJSISE.2018.093825\n[13] M. Liu, F . Li, H. Yan, K. Wang, Y . Ma, L. Shen , et al., ‚ÄúA multi-model deep convolu-\ntional neural network for automatic hippocampus segmentation and classification in \nAlzheimer‚Äôs disease,‚Äù Neuroimage, vol. 208, p. 116459, 2020. https://doi.org/10.1016/ \nj.neuroimage.2019.116459\niJOE ‚Äí Vol. 19, No. 04, 2023\n47\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n[14] E. Dicks, W. M. van der Flier, P. Scheltens, F. Barkhof, B. M. Tijms, and A. s. D. N. \nInitiative, ‚ÄúSingle-subject gray matter networks predict future cortical atrophy in preclin -\nical Alzheimer‚Äôs disease,‚Äù Neurobiology of Aging, vol. 94, pp. 71‚Äì80, 2020. https://doi.\norg/10.1016/j.neurobiolaging.2020.05.008\n[15] S. Basheera and M. S. S. Ram, ‚ÄúA novel CNN based Alzheimer‚Äôs disease classification using \nhybrid enhanced ICA segmented gray matter of MRI,‚Äù Computerized Medical Imaging and \nGraphics, vol. 81, p. 101713, 2020. https://doi.org/10.1016/j.compmedimag.2020.101713\n[16] A. Ma‚Äômoun, O. M. Al-hazaimeh, N. Alhindawi, and S. M. Hayajneh, ‚ÄúA dual curvature \nshell phased array simulation for delivery of high intensity focused ultrasound,‚Äù Computer \nand Information Science, vol. 7, p. 49, 2014. https://doi.org/10.5539/cis.v7n3p49\n[17] Z. Xia, G. Yue, Y . Xu, C. Feng, M. Yang, T. Wang, et al., ‚ÄúA novel end-to-end hybrid net -\nwork for Alzheimer‚Äôs disease detection using 3D CNN and 3D CLSTM,‚Äù in 2020 IEEE \n17th International Symposium on Biomedical Imaging (ISBI),  2020, pp. 1‚Äì4. https://doi.\norg/10.1109/ISBI45749.2020.9098621\n[18] O. M. Al-Hazaimeh, M. Al-Nawashi, and M. Saraee, ‚ÄúGeometrical-based approach for \nrobust human image detection,‚Äù Multimedia Tools and Applications, vol. 78, pp. 7029‚Äì7053, \n2019. https://doi.org/10.1007/s11042-018-6401-y\n[19] N. Gharaibeh, O. M. Al-hazaimeh, A. Abu-Ein, and K. M. Nahar, ‚ÄúA hybrid svm na√Øve-\nbayes classifier for bright lesions recognition in eye fundus images,‚Äù Int J Electr Eng Inform, \nvol. 13, pp. 530‚Äì545, 2021. https://doi.org/10.15676/ijeei.2021.13.3.2\n[20] P . Khan, M. F. Kader, S. R. Islam, A. B. Rahman, M. S. Kamal, M. U. Toha , et al., \n‚ÄúMachine learning and deep learning approaches for brain disease diagnosis: principles and \nrecent advances,‚Äù IEEE Access, vol. 9, pp. 37622‚Äì37655, 2021. https://doi.org/10.1109/\nACCESS.2021.3062484\n[21] C. S. Eke, E. Jammeh, X. Li, C. Carroll, S. Pearson, and E. Ifeachor, ‚ÄúEarly detection of \nAlzheimer‚Äôs disease with blood plasma proteins using support vector machines,‚Äù IEEE \nJournal of Biomedical and Health Informatics, vol. 25, pp. 218‚Äì226, 2020. https://doi.\norg/10.1109/JBHI.2020.2984355\n[22] E. Hussain, M. Hasan, S. Z. Hassan, T. H. Azmi, M. A. Rahman, and M. Z. Parvez, ‚ÄúDeep \nlearning based binary classification for Alzheimer‚Äôs disease detection using brain MRI \nimages,‚Äù in 2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA), \n2020, pp. 1115‚Äì1120. https://doi.org/10.1109/ICIEA48937.2020.9248213\n[23] T . A. Tuan, T. B. Pham, J. Y . Kim, and J. M. R. Tavares, ‚ÄúAlzheimer‚Äôs diagnosis using \ndeep learning in segmenting and classifying 3D brain MR images,‚Äù International Journal of \nNeuroscience, pp. 1‚Äì10, 2020. https://doi.org/10.1080/00207454.2020.1835900\n[24] O. M. Al-Hazaimeh and M. Al-Smadi, ‚ÄúAutomated pedestrian recognition based on deep \nconvolutional neural networks,‚Äù International Journal of Machine Learning and Comput-\ning, vol. 9, pp. 662‚Äì667, 2019. https://doi.org/10.18178/ijmlc.2019.9.5.855\n[25] H. S. Zaina, S. B. Belhaouari, T. Stanko, and V . Gorovoy, ‚ÄúAn exemplar pyramid fea -\nture extraction based Alzheimer disease classification method,‚Äù IEEE Access, vol. 10,  \npp. 66511‚Äì66521, 2022. https://doi.org/10.1109/ACCESS.2022.3183185\n[26] J. E. Arco, J. Ram√≠rez, J. M. G√≥rriz, M. Ruz, and A. s. D. N. Initiative, ‚ÄúData fusion based \non searchlight analysis for the prediction of Alzheimer‚Äôs disease,‚Äù Expert Systems with \nApplications, vol. 185, p. 115549, 2021. https://doi.org/10.1016/j.eswa.2021.115549\n[27] S. Basheera and M. S. S. Ram, ‚ÄúDeep learning based Alzheimer‚Äôs disease early diagnosis \nusing T2w segmented gray matter MRI,‚Äù International Journal of Imaging Systems and \nTechnology, vol. 31, pp. 1692‚Äì1710, 2021. https://doi.org/10.1002/ima.22553\n[28] M. Song, H. Jung, S. Lee, D. Kim, and M. Ahn, ‚ÄúDiagnostic classification and biomarker \nidentification of Alzheimer‚Äôs disease with random forest algorithm,‚Äù Brain Sciences, vol. 11, \np. 453, 2021. https://doi.org/10.3390/brainsci11040453\n48\nhttp://www.i-joe.org\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\n[29] Z. Fan, J. Li, L. Zhang, G. Zhu, P. Li, X. Lu , et al., ‚ÄúU-net based analysis of MRI for Alzhei-\nmer‚Äôs disease diagnosis,‚Äù Neural Computing and Applications, vol. 33, pp. 13587‚Äì13599, \n2021. https://doi.org/10.1007/s00521-021-05983-y\n[30] E. E. Bron, S. Klein, J. M. Papma, L. C. Jiskoot, V . Venkatraghavan, J. Linders , et al., \n‚ÄúCross-cohort generalizability of deep and conventional machine learning for MRI-based \ndiagnosis and prediction of Alzheimer‚Äôs disease,‚Äù NeuroImage: Clinical, vol. 31, p. 102712, \n2021. https://doi.org/10.1016/j.nicl.2021.102712\n[31] V . S. Rallabandi, K. Tulpule, M. Gattu, and A. s. D. N. Initiative, ‚ÄúAutomatic classification \nof cognitively normal, mild cognitive impairment and Alzheimer‚Äôs disease using structural \nMRI analysis,‚Äù Informatics in Medicine Unlocked, vol. 18, p. 100305, 2020. https://doi.\norg/10.1016/j.imu.2020.100305\n[32] H. A. Helaly, M. Badawy, and A. Y . Haikal, ‚ÄúDeep learning approach for early detection \nof Alzheimer‚Äôs disease,‚Äù Cognitive computation, vol. 14, pp. 1711‚Äì1727, 2022. https://doi.\norg/10.1007/s12559-021-09946-2\n[33] A. Ashraf, S. Naz, S. H. Shirazi, I. Razzak, and M. Parsad, ‚ÄúDeep transfer learning for \nAlzheimer neurological disorder detection,‚Äù Multimedia Tools and Applications, vol. 80,  \npp. 30117‚Äì30142, 2021. https://doi.org/10.1007/s11042-020-10331-8\n[34] R. Divya and R. Shantha Selva Kumari, ‚ÄúGenetic algorithm with logistic regression fea -\nture selection for Alzheimer‚Äôs disease classification,‚Äù Neural Computing and Applications,  \nvol. 33, pp. 8435‚Äì8444, 2021. https://doi.org/10.1007/s00521-020-05596-x\n[35] A. Ebrahimi, S. Luo, R. Chiong, and A. s. D. N. Initiative, ‚ÄúDeep sequence modelling for \nAlzheimer‚Äôs disease detection using MRI,‚Äù Computers in Biology and Medicine, vol. 134, \np. 104537, 2021. https://doi.org/10.1016/j.compbiomed.2021.104537\n[36] S. Sharma, K. Guleria, S. Tiwari, and S. Kumar, ‚ÄúA deep learning based convolutional neu-\nral network model with VGG16 feature extractor for the detection of Alzheimer Disease \nusing MRI scans,‚Äù Measurement: Sensors, vol. 24, p. 100506, 2022. https://doi.org/10.1016/ \nj.measen.2022.100506\n[37] F . U. R. Faisal and G.-R. Kwon, ‚ÄúAutomated detection of Alzheimer‚Äôs disease and mild cog -\nnitive impairment using whole brain MRI,‚Äù IEEE Access, vol. 10, pp. 65055‚Äì65066, 2022. \nhttps://doi.org/10.1109/ACCESS.2022.3180073\n[38] J. E. W. Koh, V . Jahmunah, T.-H. Pham, S. L. Oh, E. J. Ciaccio, U. R. Acharya , et al., \n‚ÄúAutomated detection of Alzheimer‚Äôs disease using bi-directional empirical model decom -\nposition,‚Äù Pattern Recognition Letters, vol. 135, pp. 106‚Äì113, 2020. https://doi.org/10.1016/ \nj.patrec.2020.03.014\n8 Authors\nNasr Gharaibeh is an Assistance Professor in the Department of electrical \nEngineering. He received his PhD in Electrical engineering in 1990. Now, he is a lec -\nturer at Al-Balqa Applied University ‚Äì Al-huson University College, Jordan. He can be \ncontacted at email: nas@bau.edu.jo.\nAshraf A. Abu-Ein  is an Associate Professor in the Department of Electrical \nEngineering. He has completed his PhD at National Technical University of Ukraine, \nComputer Engineering. ‚ÄúComputers, Computing Systems and Networks‚Äù, 2007.  \nNow, he is a lecturer at Al-Balqa Applied University  ‚Äì Al-huson University College, \nJordan. He can be contacted at email: ashraf.abuain@bau.edu.jo.\niJOE ‚Äí Vol. 19, No. 04, 2023\n49\nPaper‚ÄîSwin Transformer-Based Segmentation and Multi-Scale Feature Pyramid Fusion Module for‚Ä¶\nObaida M. Al-hazaimeh earned a BSc in Computer Science from Jordan‚Äôs Applied \nScience University in 2004 and an MSc in Computer Science from Malaysia‚Äôs University \nScience Malaysia in 2006. In 2010, he earned a PhD in Network Security (Cryptogra -\nphy) from Malaysia. He is an Full professor at Al-Balqa Applied University‚Äôs Depart -\nment of Computer Science and Information Technology. Cryptology, image processing, \nmachine learning, and chaos theory are among his primary research interests. He has \npublished around 55 papers in international refereed publications as an author or co-  \nauthor. He can be contacted at email: dr_obaida@bau.edu.jo.\nKhalid M.O. Nahar is an Associate Professor in the Department of Computer \nSciences-Faculty of IT, Yarmouk University, Irbid-Jordan. He received his BS and  \nMS in Computer Sciences from Yarmouk University in Jordan, in 1992 and 2005, respec-\ntively. He was awarded a full scholarship to continue his PhD in Computer Sciences \nand Engineering from King Fahd University of Petroleum and Minerals (KFUPM), \nKSA. In 2013, he completed his PhD and started his job as an Assistant Professor at \nTabuk University, KSA for two years. In 2015, he backs to Yarmouk University and \nfrom now he is the Assistant Dean for Quality Control, Jordan. He can be contacted at \nemail: Khalids@yu.edu.jo.\nWaleed A. Abu-Ain his PhD in Artificial Intelligence in 2016 from the Artificial \nIntelligence Technology Center of the National University of Malaysia, Department \nof Computer Science (UKM). Since 2016, he is currently working as a full-time assis-\ntant professor in the Department of Computer Science at Taibah University, Applied \nCollege, Saudi Arabia. His research interests include artificial intelligence, machine \nlearning and optimization, data science, computer vision, deep learning, and the Internet \nof Things. He can be contacted at email: wabuain@taibahu.edu.sa.\nMalek M. Al-Nawashi is an a Lecturer in the Department of Computer Science and \nInformation Technology at Al-Balqa Applied University‚ÄìAl-huson University College, \nJordan. He has completed his PhD at University of Salford Manchester in Computer \nScience in 2019. His main research interests are image processing and machine learn -\ning. He can be contacted at email: nawashi@bau.edu.jo.\nArticle submitted 2022-12-30. Resubmitted 2023-02-01. Final acceptance 2023-02-02. Final version \npublished as submitted by the authors.\n50\nhttp://www.i-joe.org",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.8470250368118286
    },
    {
      "name": "Computer science",
      "score": 0.689649224281311
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6298357248306274
    },
    {
      "name": "Segmentation",
      "score": 0.5972546339035034
    },
    {
      "name": "Feature extraction",
      "score": 0.5699739456176758
    },
    {
      "name": "Computer vision",
      "score": 0.5017755031585693
    },
    {
      "name": "Image segmentation",
      "score": 0.45685508847236633
    }
  ]
}