{
  "title": "Transformer Reasoning Network for Image- Text Matching and Retrieval",
  "url": "https://openalex.org/W3016333779",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2169874835",
      "name": "Nicola Messina",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
        "National Research Council"
      ]
    },
    {
      "id": "https://openalex.org/A2032466494",
      "name": "Fabrizio Falchi",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
        "National Research Council"
      ]
    },
    {
      "id": "https://openalex.org/A97551770",
      "name": "Andrea Esuli",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
        "National Research Council"
      ]
    },
    {
      "id": "https://openalex.org/A2102836602",
      "name": "Giuseppe Amato",
      "affiliations": [
        "National Research Council",
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A2169874835",
      "name": "Nicola Messina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2032466494",
      "name": "Fabrizio Falchi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A97551770",
      "name": "Andrea Esuli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102836602",
      "name": "Giuseppe Amato",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6694395031",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6767617715",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6685183736",
    "https://openalex.org/W2346425926",
    "https://openalex.org/W2552579943",
    "https://openalex.org/W2963040148",
    "https://openalex.org/W2778100917",
    "https://openalex.org/W2963467339",
    "https://openalex.org/W2964120214",
    "https://openalex.org/W2221837760",
    "https://openalex.org/W6740863234",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2886970679",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2913618459",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W6773248631",
    "https://openalex.org/W2810482788",
    "https://openalex.org/W6749537441",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2988823324",
    "https://openalex.org/W2467557055",
    "https://openalex.org/W1957706851",
    "https://openalex.org/W6747225742",
    "https://openalex.org/W6738893770",
    "https://openalex.org/W6759118546",
    "https://openalex.org/W2972895044",
    "https://openalex.org/W2962716332",
    "https://openalex.org/W2963224792",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W6754778999",
    "https://openalex.org/W2963899908",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W2973978812",
    "https://openalex.org/W2949474740",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3001555892",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2912177220",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2737766105",
    "https://openalex.org/W3104180728",
    "https://openalex.org/W2963499204",
    "https://openalex.org/W2613718673"
  ],
  "abstract": "Image-text matching is an interesting and fascinating task in modern AI research. Despite the evolution of deep-learning-based image and text processing systems, multi-modal matching remains a challenging problem. In this work, we consider the problem of accurate image-text matching for the task of multi-modal large-scale information retrieval. State-of-the-art results in image-text matching are achieved by inter-playing image and text features from the two different processing pipelines, usually using mutual attention mechanisms. However, this invalidates any chance to extract separate visual and textual features needed for later indexing steps in large-scale retrieval systems. In this regard, we introduce the Transformer Encoder Reasoning Network (TERN), an architecture built upon one of the modern relationship-aware self-attentive architectures, the Transformer Encoder (TE). This architecture is able to separately reason on the two different modalities and to enforce a final common abstract concept space by sharing the weights of the deeper transformer layers. Thanks to this design, the implemented network is able to produce compact and very rich visual and textual features available for the successive indexing step. Experiments are conducted on the MS-COCO dataset, and we evaluate the results using a discounted cumulative gain metric with relevance computed exploiting caption similarities, in order to assess possibly non-exact but relevant search results. We demonstrate that on this metric we are able to achieve state-of-the-art results in the image retrieval task. Our code is freely available at https://github.com/mesnico/TERN.",
  "full_text": "Transformer Reasoning Network for Image-Text\nMatching and Retrieval\nNicola Messina, Fabrizio Falchi, Andrea Esuli, Giuseppe Amato\nInstitute of Information Science and Technologies\nNational Research Council\nPisa, Italy\n{name.surname}@isti.cnr.it\nAbstract—Image-text matching is an interesting and fasci-\nnating task in modern AI research. Despite the evolution of\ndeep-learning-based image and text processing systems, multi-\nmodal matching remains a challenging problem. In this work,\nwe consider the problem of accurate image-text matching for\nthe task of multi-modal large-scale information retrieval. State-\nof-the-art results in image-text matching are achieved by inter-\nplaying image and text features from the two different processing\npipelines, usually using mutual attention mechanisms. However,\nthis invalidates any chance to extract separate visual and textual\nfeatures needed for later indexing steps in large-scale retrieval\nsystems. In this regard, we introduce the Transformer Encoder\nReasoning Network (TERN), an architecture built upon one\nof the modern relationship-aware self-attentive architectures,\nthe Transformer Encoder (TE). This architecture is able to\nseparately reason on the two different modalities and to enforce\na ﬁnal common abstract concept space by sharing the weights\nof the deeper transformer layers. Thanks to this design, the\nimplemented network is able to produce compact and very rich\nvisual and textual features available for the successive indexing\nstep. Experiments are conducted on the MS-COCO dataset,\nand we evaluate the results using a discounted cumulative gain\nmetric with relevance computed exploiting caption similarities,\nin order to assess possibly non-exact but relevant search results.\nWe demonstrate that on this metric we are able to achieve state-\nof-the-art results in the image retrieval task. Our code is freely\navailable at https://github.com/mesnico/TERN.\nI. I NTRODUCTION\nRecent advances in deep learning research brought to life\ninteresting tasks and applications which include joint process-\ning of data from different domains. Image-text matching is an\ninteresting task that consists in aligning information coming\nfrom visual and textual worlds, in order to beneﬁt from the\ncomplementary richness of these two very different domains.\nVisuals and texts are two important modalities used by hu-\nmans to fully understand the real world. While text is already a\nwell-structured description developed by humans in hundreds\nof years, images are basically nothing but raw matrices of\npixels hiding very high-level concepts and structures. If we\nwant to obtain an informative textual description of a visual\nscene we are required not only to understand what are the\nsalient entities in the image, but we need also to reason about\nthe relationships between the different entities, e.g. ”The kid\nkicks the ball”. In this respect, it is necessary not only to\nperceive objects on their own but also understanding spatial\nand even abstract relationships linking them together.\nMatching\nTransformer Encoder \nReasoning\nA tennis player serving \na ball on the court ...\nFig. 1. Overview of the presented architecture. Image and text are seen\nrespectively as sets of image regions and sequences of words, and they are\nprocessed using a transformer-based reasoning engine.\nThis has important implications in many modern AI-\npowered systems, where perception and reasoning play both\nimportant roles. In this work, we concentrate our effort on\nthe cross-modal information retrieval research ﬁeld, in which\nwe are asked to produce compact yet very informative object\ndescriptions coming from very different domains (visual and\ntextual in this scenario).\nVision and language matching has been extensively stud-\nied [1]–[5]. Many works employ standard architectures for\nprocessing images and text, such as CNNs-based models\nfor image processing and recurrent networks for language.\nUsually, in this scenario, the image embeddings are extracted\nfrom standard image classiﬁcation networks, such as ResNet\nor VGG, by employing the network activations before the\nclassiﬁcation head. Usually, descriptions extracted from CNN\nnetworks trained on classiﬁcation tasks are able to only capture\nglobal summarized features of the image, ignoring important\nlocalized details.\nAlthough these networks demonstrated noticeable perfor-\nmances in the image-text matching task, they are not able\nto infer what an object really is. The objectness prior is an\nimportant feature of the perception system that helps ﬁltering\nout irrelevant zones in the images while focusing the attention\narXiv:2004.09144v3  [cs.CV]  25 Jan 2021\non entities of interest. As far as the matching problem is\nconcerned, ﬁnding entities of interest inside the image helps\nin creating a representation that has a level of abstraction\ncomparable with the related text. In fact, a visual object present\nin an image, such as a dog, can be matched in an almost one-\nto-one relationship with the nouns dog, or animal present in\nthe corresponding image caption. Furthermore, the objectness\nprior is the ﬁrst step towards higher-level abstraction tasks\nsuch as reasoning about inter-object relationships.\nWe are to tackle this important problem with the goal\nof ﬁnding compact cross-modal descriptions of images and\ntexts which can incorporate detailed relational insights of\nthe scene. Compact and informative descriptions are required\nin the context of large scale retrieval systems, where image\nand text embeddings can be compared and indexed using a\nsimple similarity function (e.g., cosine similarity) deﬁned on\na common embedding space.\nSome works have recently tackled the matching problem\nusing a relational approach, trying to reason on substructures\nof images and texts (regions and words respectively) using\nattention and self-attention mechanisms [3], [5], [6], or graph\nnetworks [7].\nIn particular [3], [4], [6] try to learn a scoring function\ns = φ(I,C) measuring the afﬁnity between an image and a\ncaption, where I is an image, C is the caption and s is a\nnormalized score in the range [0,1]. The problem with this\napproach is that it is not possible to extract compact features\ndescribing images and texts separately. In this setup, if we\nwant to retrieve images related to a given query text, we have\nto compute all the similarities through the φfunction, and then\nsort the resulting scores in descending order. This is unfeasible\nif we want to retrieve images from a large database in few\nmilliseconds.\nFor this reason, we propose the Transformer Encoder Rea-\nsoning Network (TERN), a transformer-based architecture able\nto map images and texts into the same common space while\npreserving important relational aspects of both modalities. In\ndoing so, we avoid cross-talking between the two pipelines,\nso that it remains possible to separately forward the visual\nand the language pipeline to obtain compact image/caption\ndescriptors.\nThe general transformer architecture [8] was introduced to\nprocess sequential data, like natural languages. However, the\nencoder part of the transformer has no sequential prior hard-\ncoded in its architecture. Therefore, it is a good candidate for\nprocessing also image regions: with the very desirable self-\nattention mechanism it incorporates, the transformer encoder\ncan be employed to link together different image regions,\neffectively constructing a powerful visual reasoning pipeline.\nConcerning the evaluation of the proposed matching proce-\ndure in an information retrieval setup, the Recall@K metric is\nusually employed, where typically K = {1,5,10}. However,\nin common search engines where the user is searching for\nrelated images and not necessarily exact matches, the Re-\ncall@K evaluation is too rigid and unable to capture high-level\nsemantic similarities between the retrieval results.\nFor this reason, we propose to measure the retrieval abilities\nof the system through a discounted cumulative gain metric\nwith relevance computed exploiting caption similarities work-\ning at a high conceptual level, proceeding in a similar way to\n[2].\nThe contributions of this paper are:\n• We introduce the Transformer Encoder Reasoning Net-\nwork (TERN), a transformer-based architecture able to\nmap both visual and textual modalities into the same\ncommon space, preserving the relational content of both\nmodalities. The learned representations can be used for\nefﬁcient and scalable multi-modal retrieval.\n• We introduce a novel evaluation metric able to capture\nnon-exact search results, by weighting different results\nthrough a relevance measure computed on the caption\nsimilarities.\n• We show that our architecture reaches state-of-the-art\nperformances with respect to other architectures on the\nintroduced metric, for the image retrieval task.\nII. R ELATED WORK\nIn this section, we review some of the previous works\nrelated to image-text matching and high-level relational rea-\nsoning. Also, we brieﬂy summarize the evaluation metrics\navailable in the literature for the image-caption retrieval task.\nImage-Text matching\nImage-text matching is often cast to the problem of inferring\na similarity score among an image and a sentence. Usually,\none of the common approaches for computing this cross-\ndomain similarity is to project images and texts into a common\nrepresentation space on which some kind of similarity measure\ncan be deﬁned (e.g.: cosine or dot-product similarities).\nConcerning image processing, the standard approach con-\nsists in using Convolutional Neural Networks (CNNs), usu-\nally pre-trained on image classiﬁcation tasks. In particular,\nsome works [9]–[13] used VGGs, others [1], [14]–[16] used\nResNets. The problem with these kinds of CNNs is that they\nusually are able to extract extremely summarized and global\ndescriptions of images. Therefore, a lot of useful ﬁne-grained\ninformation needed to reconstruct inter-object relationships for\nprecise image-text alignment is permanently lost. To overcome\nthis problem, the authors in [17] worked on a ﬁne-grained\nlevel, deﬁning the image-sentence matching score on the basis\nof the similarities among the common abstract concepts found\nin the two modalities.\nRecent works, instead, exploited the availability of pre-\ncomputed region-level features extracted from state-of-the-art\nobject detectors. In particular, following the work by [18],\nthe authors in [5], [7] used bottom-up features extracted from\nFaster-RCNN. The bottom-up attention mechanism resembles\nthe attentive mechanism present in the human visual system,\nand it is an important feature for ﬁltering out unimportant\ninformation. This lays the foundations for a more precise and\nlightweight reasoning mechanism, downstream of the bottom-\nup perception module, which should carefully process the\nresulting image regions to obtain an expressive representation\nof the overall scene.\nConcerning sentence processing, many works [1], [4], [5],\n[7], [16] employed GRU or LSTM recurrent networks to\nprocess natural language.\nRecently, the transformer architecture [8] achieved state-\nof-the-art results in many natural language processing tasks,\nsuch as next sentence prediction or sentence classiﬁcation. In\nparticular, the BERT embeddings [19] emphasized the power\nof the attention mechanism to produce accurate context-aware\nword descriptions.\nGiven the enormous ﬂexibility of the transformer encoder\narchitecture, some works [3], [6] tried to apply the attention\nmechanism of the transformer encoder architecture to process\nvisual inputs and natural language together. The main idea\nbehind visual processing using the transformer encoder is to\nleverage its self-attention mechanism to link together different\nimage regions to catch important inter-object relationships.\nHowever, the systems proposed in this direction are not able to\nextract separate visual and textual features for use in similarity\nsearch applications.\nThe authors in [7] were able to achieve very good results\nin caption/image retrieval learning separate visual and textual\nfeatures. They introduced a visual reasoning pipeline built of\na Graph Convolution Networks (GCNs) and a GRU to se-\nquentially reason on the different image regions. Furthermore,\nthey impose a sentence reconstruction loss to regularize the\ntraining process. Differently from their work, we leverage on\nthe reasoning power of the transformer encoder, both for the\nvisual and linguistic pipelines.\nHigh-level reasoning\nAnother branch of research is focused on the study of\nrelational reasoning models for high-level understanding. The\nauthors in [20] proposed an architecture that separates per-\nception from reasoning. They tackled the problem of Visual\nQuestion Answering by introducing a particular layer called\nRelation Network (RN), which is specialized in comparing\npairs of objects. Object representations are learned using a\nfour-layer CNN, and the question embedding is generated\nthrough an LSTM. Recently, the authors in [21], [22] extended\nthe RN for producing compact features for relation-aware\nimage retrieval. However, they did not explore the multi-modal\nretrieval setup.\nOther solutions try to stick more to a symbolic-like way\nof reasoning. In particular, some works [23], [24] introduced\ncompositional approaches able to explicitly model the reason-\ning process by dynamically building a reasoning graph that\nstates which operations must be carried out and in which order\nto obtain the right answer.\nRecent works employed Graph Convolution Networks\n(GCNs) to reason about the interconnections between con-\ncepts. In particular, the works by [25]–[27] used GCNs to\nreason on the image regions for image captioning, while others\n[28], [29] used GCN with attention mechanisms to produce the\nscene graph from plain images.\nRetrieval evaluation metrics\nAll the works involved with image-caption matching eval-\nuate their results by measuring how good the system is\nat retrieving relevant images given a query caption (image-\nretrieval) and vice-versa (caption-retrieval). In other words,\nthey evaluate their proposed models using a retrieval setup.\nUsually the Recall@K metric is used [1], [3], [6], [7], [30],\nwhere typically K = {1,5,10}. On the other hand, the authors\nin [2] introduced a novel metric able to capture non-exact\nresults by weighting the ranked documents using a caption-\nbased similarity measure. We embrace their idea, and we\nextend it bringing to life a powerful evaluation metric able\nto capture high-level semantic aspects. Furthermore, relaxing\nthe constraints of exact-match similarity search is an important\nstep towards an effective evaluation of modern search engines.\nIII. R EVIEW OF TRANSFORMER ENCODERS (TE S)\nOur proposed architecture is based on the well established\nTransformer Encoder (TE) architecture, which relies heavily\non the concept of self-attention. The basic attention mecha-\nnism, as described in [8], is built upon three quantities: queries,\nkeys, and values. The attention mechanism maps a query and\na set of key-value pairs to an output, where the query, keys,\nvalues, and output are all vectors. The output is computed as\na weighted sum of the values, where the weight assigned to\neach value is computed using a softmax activation function\nover the inner product of the query with the corresponding\nkey. More formally,\nAtt(Q,K,V ) = softmax\n(QKT\n√dk\n)\nV, (1)\nwhere Q ∈ Rt×dk ,K ∈ Rs×dk and V ∈ Rs×dv ; s is the\ninput sequence length and t is the length of the conditioning\nsequence that drives the attention. The factor √dk is used\nto mitigate the vanishing gradient problem of the softmax\nfunction in case the inner product assumes too large values.\nThe self-attention derives trivially from the general attention\nmechanism when either V, K, and Q are computed from the\nsame input set, i.e., when the set that we use to drive the\nattention is the same as the input set. In this case, in fact,\nt= s and the scalar product QKT ∈Rs×s is a square matrix\nthat encodes the afﬁnity that each element of the set has with\nall the others elements of the same set.\nIn the self-attention case, Q, K, and V are computed by\nlinearly projecting the same input embeddings using three\ndifferent matrices WQ ∈ Rdk×di ,WK ∈ Rdk×di and\nWV ∈Rdv×di , where di is the dimensionality of the input\nembeddings.\nThen, a simple feedforward layer on the Att (Q,K,V )\nvectors, with a ReLU activation function, further processes\nthe vectors produced by the self-attention mechanism. A\nschematic view of a transformer encoder layer is shown in\nFigure 2.\nWe argue that the transformer encoder self-attention mech-\nanism is able to drive a simple but powerful reasoning mecha-\nnism able to spot hidden links between the vector entities given\nquery \nFFN\nkey\nFFN\nvalues\nFFN\ns ⨉ s attention matrixFFN\n.\n.\n.\ns input vectors\ns output vectors\n.\n.\n.\nQ\nK\nV\nAtt(Q, K, V)\nsoftmax\n(per row)\nFig. 2. Simpliﬁed view of a transformer encoder layer. Add&Norm skip\nconnections present in the original architecture are not shown here for ease\nof viewing.\nin input to the encoder, whatever nature they have. Also, the\nencoder is designed in a way that multiple instances of the\nsame architecture could be stacked in sequence, producing a\ndeeper reasoning pipeline.\nIV. V ISUAL -TEXTUAL REASONING USING TRANSFORMER\nENCODERS\nOur work relies almost entirely on the TE architecture, both\nfor the visual and the textual data pipelines.\nThe TE takes as input sequences or sets of entities, and\nit can reason upon these entities disregarding their intrinsic\nnature. In particular, we consider the salient regions in an\nimage as visual entities, and the words present in the caption\nas language entities.\nMore formally, the input to our reasoning pipeline is a set\nI = {r0,r1,...rn}of nimage regions representing an image I\nand a sequence C = {w0,w1,...wm}of mwords representing\nthe corresponding caption C. Following, we will describe the\nmethodology we adopted to extract ri from images and wj\nfrom captions.\nRegion and Word Features\nI and C descriptions come from state-of-the-art visual and\ntextual pre-trained networks, Faster-RCNN with Bottom-Up\nattention, and BERT respectively.\nFaster-RCNN [31] is a state-of-the-art object detector. It\nhas been used in many downstream tasks requiring salient\nobject regions extracted from images. The authors in [32]\nintroduced bottom-up visual features by training Faster-RCNN\nwith a Resnet-101 backbone on the Visual Genome dataset\n[33]. Using these features, they were able to reach remarkable\nresults on the two downstream tasks of image captioning and\nvisual question answering. Therefore, in our work we employ\nthe bottom-up features extracted from every image as image\ndescription I = {r0,r1,...rn}.\nConcerning text processing, we used BERT [19] for ex-\ntracting word embeddings. BERT already uses a multi-layer\ntransformer encoder to process words in sentences and capture\ntheir functional relationships through the same powerful self-\nattention mechanism. BERT embeddings are trained on some\ngeneral natural language processing tasks such as sentence\nprediction or sentence classiﬁcation and demonstrated state-\nof-the-art results in many downstream natural language tasks.\nBERT embeddings, unlike word2vec [34], capture the context\nin which each word appears. Therefore, every word embedding\ncarries information about the surrounding context, that could\nbe different from caption to caption.\nTransformer Encoder Reasoning Network (TERN)\nOur reasoning engine is built using a stack of transformer\nencoder layers; the overall architecture is shown in Figure 3.\nThe reasoning module continuously operates on sets and\nsequences of n and m objects respectively for images and\ncaptions. The objective is to produce a compact representation\nof the n processed regions and of the m processed words\nsuitable for the downstream task of image-text matching in a\ncommon space with ﬁxed dimensionality. One of the easiest\nways to proceed is to pool the elements of the set/sequence\nusing symmetric functions like sum or avg, or, like [7],\ngrowing a meaningful aggregated representation inside the\nhidden state of a recurrent network (GRU or LSTM).\nOur method, instead, follows the approach by BERT [19]:\nwe reserve a special token both at the beginning of the regions\nset and of the words sequence (I-CLS and T-CLS) devoted to\ncarrying global information along the two pipelines. For this\nreason, we effectively expand the number of image regions\nto n+ 1 and the number of words to m+ 1, with r0 and\nw0 reserved for this purpose. Initially, w0 is set to the T-\nCLS BERT token, while r0, i.e., I-CLS, is a zero vector. At\nevery reasoning step, this information is updated attentively\nby the self-attention mechanism of the TEs. In the end, our\nﬁnal image and caption features will be r0 and w0 in output\nfrom the last transformer encoder layer. In the last layers of\nthe TERN architecture, the abstracted representations of the\nvisual and textual pipelines should be comparable. To enforce\nthis constraint, we share the weights of the last layers of the\nTEs before computing the matching loss Lm on the common\nspace.\nIf we use only bottom-up features without any spatially\nrelated information, the visual reasoning engine is not able to\nreason about spatial relationships. This is a fairly important as-\npect to capture since lot of textual descriptions contain spatial\nindications (e.g. on top ofor above). In order to include spatial\nawareness also in the visual reasoning process, we condition\nthe early visual pipeline with the bounding-boxes coordinates.\nTo this aim, we compute the normalized coordinates and the\nnormalized area for each region, as follows:\nc=\n{x1\nW, y1\nW,x2\nH,y2\nH,(x2 −x1)(y2 −y1)\nWH\n}\n. (2)\nThen, we concatenate cwith the original bottom-up feature.\nIn the end, we forward this information through a simple\nLinear-ReLU-Linear stack (sharing weights among all the n\nregions) to obtain the ﬁnal spatial-aware bottom-up feature.\nBERT\n(pretrained)\nTEs\nTEs\nI-CLS\nTEs\nTEs\nShared \nWeights Lm\nFC\nFaster-RCNN\nReasoning\nT-CLS\nRegion \nFeatures\nFC\nFC\nFC\nRegion \nBounding \nBoxes\nx1, y1, \nx2, y2, \nArea\nw0\nr0\nA tennis player serving \na ball on the court ... ... ...\nFig. 3. The proposed TERN architecture. TE stands for Transformer Encoder and its architecture is explained in detail in [8]. Region and words are extracted\nthrough a bottom-up attention model based on Faster-RCNN and BERT respectively. BERT already employs positional encoding for representing the sequential\nnature of words, therefore this step is not reported in the ﬁgure. Concerning regions, the extracted bottom-up features are conditioned with the information\nrelated to the geometry of the bounding-boxes. This is done through a simple fully connected stack in the early visual pipeline, before the reasoning steps.\nLm is the matching loss, deﬁned as in [1]. The ﬁnal weight sharing between TE modules guarantees consistent processing of the high-level concepts..\nLearning\nIn order to match images and captions in the same common\nspace, we use a hinge-based triplet ranking loss, focusing the\nattention on hard negatives, as in [1], [7]. Therefore, we used\nthe following loss function:\nLm(i,c) = max\nc′\n[α+ S(i,c′) −S(i,c)]++\nmax\ni′\n[α+ S(i′,c) −S(i,c)]+, (3)\nwhere [x]+ ≡max(0,x). The hard negatives i′ and c′ are\ncomputed as follows:\ni′= arg max\nj̸=i\nS(j,c)\nc′= arg max\nd̸=c\nS(i,d), (4)\nwhere (i,c) is a positive pair. S(i,j) is the similarity function\nbetween image and caption features. We used the standard\ncosine similarity as S(·,·). As in [1], the hard negatives are\nsampled from the mini-batch and not globally, for performance\nreasons.\nV. E VALUATION METRIC FOR NON-EXACT MATCHING\nMany works measure the retrieval abilities of their visual-\nlinguistic matching system by employing the well known\nRecall@K metric. The Recall@K measures the percentage\nof queries able to retrieve the correct item among the ﬁrst\nk results.\nHowever, in common search engines where the user is\nsearching for related images/captions and not necessarily exact\nmatches, the Recall@K evaluation often proves to be too\nrigid, especially when K is small. In fact, in the scenarios\nwhere K = {1,5,10}, we are measuring the ability of the\nsystem to retrieve exact results at the top of the ranked list\nof images/captions. Doing so, we are completely ignoring\nother relevant but not exact-matching elements retrieved in the\nﬁrst positions. These elements still contribute to a good user\nexperience in the context of search engines. The Recall@K\nmetric is fully unable to capture this simple yet important\naspect. Furthermore, Recall@K is unable to capture high-\nlevel semantics of sentences and images during the retrieval\nevaluation phase.\nFor this reason, inspired by the evaluation method pre-\nsented in [2], we employed a common metric often used in\ninformation retrieval applications, the Normalized Discounted\nCumulative Gain (NDCG). The NDCG is able to evaluate the\nquality of the ranking produced by a certain query by looking\nat the ﬁrst pposition of the ranked elements list. The premise\nof NDCG is that highly relevant items appearing lower in a\nsearch result list should be penalized as the graded relevance\nvalue is reduced proportionally to the position of the result.\nThe non-normalized DCG until position p is deﬁned as\nfollows:\nDCGp =\np∑\ni=1\nreli\nlog2(i+ 1), (5)\nwhere reli is a positive number encoding the afﬁnity that the\ni-th element of the retrieved list has with the query element.\nThe DCG is agnostic upon how the relevance is computed. The\nNDCGp is computed by normalizing the DCGp with respect to\nthe Ideal Discounted Cumulative Gain (IDCG), that is deﬁned\nas the DCG of the list obtained by sorting all its elements by\ndescending relevance:\nNDCGp = DCGp\nIDCGp\n. (6)\nIDCGp is the best possible ranking. Thanks to this normal-\nization, NDCGp acquires values in the range [0,1].\nComputing reli values\nWe concentrate our attention on image-retrieval, given that\nis the most common scenario in real-world search engines.\nTherefore, in our work, we consider a caption as a query,\nwhile the retrieved elements are images.\nBeing a cross-modal retrieval setup, the relevance should\nbe a value obtained from a function operating on an image\nIi and a caption Cj. In principle, it could be possible to use\nthe φ(Ii,Cj) learned by methods like in [3], [6]. The problem\nis that φ is a complex neural network, and Ii, Cj are drawn\nfrom a dataset of thousands of elements, in the best case.\nThis means that constructing a Nc ×Ni relevance matrix is\ncomputationally unfeasible, where Nc is the number of total\ncaptions and Ni is the total number of images in the dataset.\nUsually, in the considered datasets, images come with a cer-\ntain number of associated captions. Thus, instead of computing\nφ(Ii,Cj), we could think of computing τ( ¯Ci,Cj) instead,\nwhere ¯Ci is the set of all captions associated with the image Ii.\nWith this simple expedient, we could efﬁciently compute quite\nlarge relevance matrices using similarity between captions,\nwhich in general are computationally much cheaper.\nAs a result, for our image-retrieval objective we deﬁne\nreli = τ( ¯Ci,Cj), where Cj is the query caption and ¯Ci are\nthe captions associated with the i-th retrieved image.\nIn our work, we use ROUGE-L [35] and SPICE [36] as\nfunctions τ for computing captions similarities. These two\nmetrics capture different aspects of the sentences. In particular,\nROUGE-L operates on the longest common sub-sequences,\nwhile SPICE exploits graphs associated with the syntactic\nparse trees, and has a certain degree of robustness against\nsynonyms. In this way, SPICE is more sensitive to high-level\nfeatures of the text and semantic dependencies between words\nand concepts rather than to pure syntactic constructions of the\nsentences.\nVI. E XPERIMENTS\nWe train the Transformer Encoder Reasoning Network and\nwe measure its performance on the MS-COCO [37] dataset,\nby measuring the effectiveness of our approach on the image\nretrieval task. We compare our results against state-of-the-art\napproaches on the same dataset, using the introduced metric.\nThe MS-COCO dataset comes with a total of 123,287\nimages. Every image has associated a set of 5 human-written\ncaptions describing the image.\nWe follow the splits introduced by [4] and followed by\nthe subsequent works in this ﬁeld [1], [7], [15]. In particular,\n113,287 images are reserved for training, 5000 for validating,\nand 5000 for testing.\nAt test time, results for both 5k and 1k image sets are\nreported. In the case of 1k images, the results are computed\nby performing 5-fold cross-validation on the 5k test split and\naveraging the outcomes. We set the NDCG parameter p= 25\nas in [2] in our experiments.\nA. Implementation Details\nWe employ the BERT model pre-trained on the masked\nlanguage task on English sentences, using the PyTorch im-\nplementation by HuggingFace 1. These pre-trained BERT\nembeddings are 768-D. For the visual pipeline, we used the\nalready available bottom-up features extracted on the MS-\nCOCO dataset. They are freely available on GitHub 2 and\nthey are 2048-D. In the experiments we used the ﬁxed-size\ndescriptors, selecting for each image the features of the top\n36 most conﬁdent detections. However, our pipeline can work\nwith a variable-length set of regions for each image, by\nappropriately masking the attention weights in the TE layers.\nConcerning the reasoning steps, we used a stack of 4 non-\nshared TE layers for visual reasoning. Instead, we found the\nbest results when ﬁne-tuning the pre-trained BERT, so we\ndid not introduce any further non-shared TE layers for the\nlanguage pipeline. We used 2 ﬁnal TE layers with weights\nshared among the visual and textual pipelines. All the TEs\nfeed-forward layers are 2048-dimensional and the dropout is\nset to 0.1. Weight sharing in the last TE layers is possible if the\ninput vectors from both visual and textual pipelines share the\nsame number of dimensions. For this reason, before entering\nthe last shared-weight TEs, both the visual and textual vectors\nare linearly projected to a 1024-D space, which is also the\ndimensionality of the ﬁnal common space, as in [1].\nWe trained for 30 epochs using Adam optimizer with a\nlearning rate of 0.00002. The α parameter of the hinge-based\ntriplet ranking loss is set to 0.2, as in [1], [7].\nWe used a batch size of 90, instead of 128 as in previous\nworks, due to hardware limitations.\nB. Results and Discussion\nWe report the results obtained on the MS-COCO dataset on\nboth 5k and 1k image test sets, and we compare them against\nthe state-of-the-art on the image retrieval task. For VSRN [7]\nand VSE [1] we used the original code and pre-trained models\nprovided by the authors, updating the evaluation protocol by\nincluding the NDCG metric.\nConcerning VSRN, in the original paper the results are\ngiven for an ensemble of two independently trained models.\nIn our case, we did not consider model ensembling. For this\nreason, we evaluated VSRN using the best snapshot among\nthe two provided by the authors.\nResults are reported in Table I. For the sake of completeness,\nwe report also the values for the Recall@K metric. Our\nTERN architecture can reach top performance on the NDCG\nmetric with the SPICE-based relevance. Due to the high-level\nabstraction nature of the SPICE metric, this result conﬁrms\n1https://github.com/huggingface/transformers\n2https://github.com/peteanderson80/bottom-up-attention\nQuery: A large jetliner sitting on top of an airport runway.\nQuery: An eating area with a table and a few chairs.\nFig. 4. Example of image retrieval results for a couple of query captions. The red marked images represent the MS-COCO ground truths, and they are not\nnecessarily the best results in these scenarios. In fact, in the very ﬁrst positions, we ﬁnd non-matching yet relevant images. These are common examples\nwhere NDCG really succeed over the Recall@K metric. .\nTABLE I\nIMAGE RETRIEVAL RESULTS ON THE MS-COCO DATASET.\nRecall@K NDCG\nModel K=1 K=5 K=10 ROUGE-L SPICE\n1K Test Set\nVSE0 [1] 43.7 79.4 89.7 0.702 0.616\nVSE++ [1] 52.0 84.3 92.0 0.712 0.617\nVSRN [7] 60.8 88.4 94.1 0.723 0.620\nTERN (Ours) 51.9 85.6 93.6 0.725 0.653\n5K Test Set\nVSE0 [1] 22.0 50.2 64.2 0.633 0.549\nVSE++ [1] 30.3 59.4 72.4 0.656 0.577\nVSRN [7] 37.9 68.5 79.4 0.676 0.596\nTERN (Ours) 28.7 59.7 72.7 0.665 0.600\nthe ability of our system to understand complex patterns and\nabstract concepts both in the visual and textual inputs. The\nNDCG metric with the SPICE relevance tries to measure the\nhigh-level perception and relational understanding abilities of\nthe model directly on the downstream image-retrieval task. We\nobtain the best gap on the 1K test set, where we improve the\ncurrent state-of-the-art by 5.3%.\nConcerning the NDCG metric with the ROUGE-L computed\nrelevance, our TERN architecture performs slightly worse\nthan VSRN. Overall, the gap between VSRN and our TERN\narchitecture is very subtle, conﬁrming the ability of those\narchitectures to be comparable when we focus on the syntactic\nand less abstract features of the language.\nDespite VSRN performing better in terms of Recall@K, we\ndemonstrated through the NDCG metric that our architecture\nis better at ﬁnding non-exact matching yet relevant elements\nin the top p positions of the ranked images list. The results\non the new evaluation metric conﬁrm the power of the TERN\narchitecture to construct high-level visual-textual descriptions\nuseful for similarity search in cross-modal environments.\nFigure 4 shows an example of image retrieval using fea-\ntures computed through our TERN architecture. The reported\nexamples show two typical situations in which the NDCG\nevaluation succeeds over the Recall@K. The ﬁgure show\nhow the ground truth images from the MS-COCO dataset,\nhighlighted in red, are not necessarily the best retrieval results\nfor the probed query sentences.\nHaving observed the qualitative and quantitative results,\nwe argue that the Recall@K and NDCG metrics should\nbe weighted appropriately when evaluating a cross-modal\nretrieval system. A system validated and tested only using\nRecall@K becomes strongly sensible to exact-matching ele-\nments only. This would result in a scenario where non-exact\nyet relevant matches are pulled away during the training phase,\nwith no indicators to monitor this strongly unwanted scenario.\nWe think that these are very important considerations to keep\nin mind during the validation of cross-modal search engines.\nVII. C ONCLUSIONS\nIn this work, we addressed the problem of image-text\nmatching in the context of efﬁcient multi-modal information\nretrieval. We argued that many state-of-the-art methods do\nnot extract compact features separately for images and text.\nThis is a problem if we want to employ relationship-aware\nvisual and textual features in the subsequent indexing stage\nfor efﬁcient and scalable cross-modal information retrieval.\nTo this aim, we developed a relationship-aware architecture\nbased on the Transformer Encoder (TE) architecture to reason\nabout the spatial and abstract relationships between elements\nin the image and the text separately. The ﬁnal weight sharing\nbetween TE modules guarantees consistent processing of the\nhigh-level concepts.\nIn the vision of employing this architecture for efﬁcient\nmulti-modal information retrieval in real-world search engines,\nwe measured our results using an NDCG metric assessing\npossibly non-exact but relevant search results. The relevance\namong images and captions has been evaluated by employing\nsimilarity measures deﬁned over captions, ROUGE-L and\nSPICE respectively. We demonstrated that our relation-aware\napproach for reasoning and matching visual and textual con-\ncepts achieved state-of-the-art results with respect to current\nmulti-modal matching architectures on the proposed retrieval\nmetric, for the task of image retrieval.\nIn the near future, we manage to enforce some reconstruc-\ntion constraints for better shaping the common space, like\nreconstructing the sentences from the visual features, as in\n[7], or recovering the image regions from the captions. Also,\nmajor interest should be given to the optimization objective.\nIn particular, it would be interesting to attenuate the very\naggressive behavior of the hinge-based triplet ranking loss for\nbetter appreciating non-exact matches at training time.\nACKNOWLEDGMENT\nThis work was partially funded by: AI4Media - A European\nExcellence Centre for Media, Society and Democracy (EC,\nH2020 n. 951911); AI4EU project (EC, H2020, n. 825619);\nAI4ChSites, CNR4C program (Tuscany POR FSE 2014-2020\nCUP B15J19001040004).\nREFERENCES\n[1] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, “VSE++: improving\nvisual-semantic embeddings with hard negatives,” in BMVC 2018.\nBMV A Press, 2018, p. 12.\n[2] F. Carrara, A. Esuli, T. Fagni, F. Falchi, and A. M. Fern ´andez, “Picture\nit in your mind: generating high level visual representations from textual\ndescriptions,” Inf. Retr. J., vol. 21, no. 2-3, pp. 208–229, 2018.\n[3] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks,”\nin NeurIPS 2019, 2019, pp. 13–23.\n[4] A. Karpathy and F. Li, “Deep visual-semantic alignments for generating\nimage descriptions,” in CVPR 2015. IEEE Computer Society, 2015,\npp. 3128–3137.\n[5] K. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for\nimage-text matching,” in ECCV 2018, ser. Lecture Notes in Computer\nScience, vol. 11208. Springer, 2018, pp. 212–228.\n[6] D. Qi, L. Su, J. Song, E. Cui, T. Bharti, and A. Sacheti, “Imagebert:\nCross-modal pre-training with large-scale weak-supervised image-text\ndata,” CoRR, vol. abs/2001.07966, 2020.\n[7] K. Li, Y . Zhang, K. Li, Y . Li, and Y . Fu, “Visual semantic reasoning\nfor image-text matching,” in ICCV 2019. IEEE, 2019, pp. 4653–4661.\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS\n2017, 2017, pp. 5998–6008.\n[9] B. Klein, G. Lev, G. Sadeh, and L. Wolf, “Associating neural word\nembeddings with deep image representations using ﬁsher vectors,” in\nCVPR 2015. IEEE Computer Society, 2015, pp. 4437–4446.\n[10] I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun, “Order-embeddings of\nimages and language,” in 4th International Conference on Learning\nRepresentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings, Y . Bengio and Y . LeCun, Eds., 2016.\n[11] X. Lin and D. Parikh, “Leveraging visual question answering for image-\ncaption ranking,” in ECCV 2016, ser. Lecture Notes in Computer\nScience, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds., vol. 9906.\nSpringer, 2016, pp. 261–277.\n[12] Y . Huang, W. Wang, and L. Wang, “Instance-aware image and sentence\nmatching with selective multimodal LSTM,” in CVPR 2017. IEEE\nComputer Society, 2017, pp. 7254–7262.\n[13] A. Eisenschtat and L. Wolf, “Linking image and text with 2-way nets,”\nin CVPR 2017. IEEE Computer Society, 2017, pp. 1855–1865.\n[14] Y . Liu, Y . Guo, E. M. Bakker, and M. S. Lew, “Learning a recurrent\nresidual fusion network for multimodal matching,” inIEEE International\nConference on Computer Vision, ICCV 2017. IEEE Computer Society,\n2017, pp. 4127–4136.\n[15] J. Gu, J. Cai, S. R. Joty, L. Niu, and G. Wang, “Look, imagine and match:\nImproving textual-visual cross-modal retrieval with generative models,”\nin CVPR 2018. IEEE Computer Society, 2018, pp. 7181–7189.\n[16] Y . Huang, Q. Wu, C. Song, and L. Wang, “Learning semantic concepts\nand order for image and sentence matching,” in CVPR 2018. IEEE\nComputer Society, 2018, pp. 6163–6171.\n[17] C. Sun, C. Gan, and R. Nevatia, “Automatic concept discovery from par-\nallel text and visual corpora,” in Proceedings of the IEEE international\nconference on computer vision, 2015, pp. 2596–2604.\n[18] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\nL. Zhang, “Bottom-up and top-down attention for image captioning and\nVQA,” CoRR, vol. abs/1707.07998, 2017.\n[19] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT 2019. Association for Computational Linguistics, 2019, pp. 4171–\n4186.\n[20] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu,\nP. Battaglia, and T. Lillicrap, “A simple neural network module for\nrelational reasoning,” in Advances in neural information processing\nsystems, 2017, pp. 4967–4976.\n[21] N. Messina, G. Amato, F. Carrara, F. Falchi, and C. Gennaro, “Learning\nvisual features for relational cbir,” International Journal of Multimedia\nInformation Retrieval, Sep 2019.\n[22] ——, “Learning relationship-aware visual features,” in ECCV 2018\nWorkshops, ser. Lecture Notes in Computer Science, vol. 11132.\nSpringer, 2018, pp. 486–501.\n[23] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko, “Learning to\nreason: End-to-end module networks for visual question answering,” in\nThe IEEE International Conference on Computer Vision (ICCV), 2017.\n[24] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, L. Fei-\nFei, C. Lawrence Zitnick, and R. Girshick, “Inferring and executing\nprograms for visual reasoning,” in The IEEE International Conference\non Computer Vision (ICCV), 2017.\n[25] T. Yao, Y . Pan, Y . Li, and T. Mei, “Exploring visual relationship for\nimage captioning,” in ECCV 2018, ser. Lecture Notes in Computer\nScience, vol. 11218. Springer, 2018, pp. 711–727.\n[26] X. Yang, K. Tang, H. Zhang, and J. Cai, “Auto-encoding scene graphs\nfor image captioning,” in CVPR 2019. Computer Vision Foundation /\nIEEE, 2019, pp. 10 685–10 694.\n[27] X. Li and S. Jiang, “Know more say less: Image captioning based on\nscene graphs,” IEEE Trans. Multimedia, vol. 21, no. 8, pp. 2117–2130,\n2019.\n[28] J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh, “Graph R-CNN for\nscene graph generation,” in ECCV 2018, ser. Lecture Notes in Computer\nScience, vol. 11205. Springer, 2018, pp. 690–706.\n[29] Y . Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang, “Factorizable\nnet: An efﬁcient subgraph-based framework for scene graph generation,”\nin ECCV 2018, ser. Lecture Notes in Computer Science, vol. 11205.\nSpringer, 2018, pp. 346–363.\n[30] K. Lee, H. Palangi, X. Chen, H. Hu, and J. Gao, “Learning visual relation\npriors for image-text matching and image captioning with neural scene\ngraph generators,” CoRR, vol. abs/1909.09953, 2019.\n[31] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards\nreal-time object detection with region proposal networks,” in Advances\nin Neural Information Processing Systems 28, 2015, pp. 91–99.\n[32] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\nL. Zhang, “Bottom-up and top-down attention for image captioning and\nvisual question answering,” in CVPR 2018. IEEE Computer Society,\n2018, pp. 6077–6086.\n[33] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY . Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein, and F. Li, “Visual\ngenome: Connecting language and vision using crowdsourced dense\nimage annotations,” CoRR, vol. abs/1602.07332, 2016.\n[34] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in 1st International Conference\non Learning Representations, ICLR 2013, 2013.\n[35] C.-Y . Lin, “ROUGE: A package for automatic evaluation of summaries,”\nin Text Summarization Branches Out. Barcelona, Spain: Association\nfor Computational Linguistics, Jul. 2004, pp. 74–81.\n[36] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “SPICE: semantic\npropositional image caption evaluation,” in ECCV 2016, ser. Lecture\nNotes in Computer Science, vol. 9909. Springer, 2016, pp. 382–398.\n[37] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll ´ar, and C. L. Zitnick, “Microsoft COCO: common objects in\ncontext,” in ECCV 2014, ser. Lecture Notes in Computer Science, vol.\n8693. Springer, 2014, pp. 740–755.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.78114914894104
    },
    {
      "name": "Search engine indexing",
      "score": 0.6911401748657227
    },
    {
      "name": "Transformer",
      "score": 0.6430357694625854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5408748388290405
    },
    {
      "name": "Encoder",
      "score": 0.5088377594947815
    },
    {
      "name": "Image retrieval",
      "score": 0.46663814783096313
    },
    {
      "name": "Information retrieval",
      "score": 0.4565943479537964
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37253284454345703
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2754572033882141
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155236",
      "name": "National Research Council",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I122991210",
      "name": "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
      "country": "IT"
    }
  ],
  "cited_by": 10
}