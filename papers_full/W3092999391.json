{
  "title": "Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering",
  "url": "https://openalex.org/W3092999391",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5023633048",
      "name": "Jeroen Offerijns",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027124439",
      "name": "Suzan Verberne",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5058670993",
      "name": "Tessa Verhoef",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3022039293",
    "https://openalex.org/W2964087080",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W2275056699",
    "https://openalex.org/W3101798106",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2891946694",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3017596848",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W2129496160",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3001393026",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2998196817"
  ],
  "abstract": "For the field of education, being able to generate semantically correct and educationally relevant multiple choice questions (MCQs) could have a large impact. While question generation itself is an active research topic, generating distractors (the incorrect multiple choice options) receives much less attention. A missed opportunity, since there is still a lot of room for improvement in this area. In this work, we train a GPT-2 language model to generate three distractors for a given question and text context, using the RACE dataset. Next, we train a BERT language model to answer MCQs, and use this model as a filter, to select only questions that can be answered and therefore presumably make sense. To evaluate our work, we start by using text generation metrics, which show that our model outperforms earlier work on distractor generation (DG) and achieves state-of-the-art performance. Also, by calculating the question answering ability, we show that larger base models lead to better performance. Moreover, we conducted a human evaluation study, which confirmed the quality of the generated questions, but showed no statistically significant effect of the QA filter.",
  "full_text": "Better Distractions: Transformer-based Distractor Generation\nand Multiple Choice Question Filtering\nJeroen Offerijns, Suzan Verberne, Tessa Verhoef\nLeiden Institute of Advanced Computer Science, Leiden University\nAbstract\nFor the ﬁeld of education, being able to gen-\nerate semantically correct and educationally\nrelevant multiple choice questions (MCQs)\ncould have a large impact. While question\ngeneration itself is an active research topic,\ngenerating distractors (the incorrect multiple\nchoice options) receives much less attention.\nA missed opportunity, since there is still a lot\nof room for improvement in this area. In this\nwork, we train a GPT-2 language model to gen-\nerate three distractors for a given question and\ntext context, using the RACE dataset. Next,\nwe train a BERT language model to answer\nMCQs, and use this model as a ﬁlter, to select\nonly questions that can be answered and there-\nfore presumably make sense. To evaluate our\nwork, we start by using text generation met-\nrics, which show that our model outperforms\nearlier work on distractor generation (DG) and\nachieves state-of-the-art performance. Also,\nby calculating the question answering ability,\nwe show that larger base models lead to bet-\nter performance. Moreover, we conducted a\nhuman evaluation study, which conﬁrmed the\nquality of the generated questions, but showed\nno statistically signiﬁcant effect of the QA ﬁl-\nter.\n1 Introduction\nOver the last two years, Transformer-based lan-\nguage models have progressed from initial devel-\nopment to being adopted in all parts of natural lan-\nguage processing (NLP). This started with ULM-\nFiT (Howard and Ruder, 2018) and BERT (Devlin\net al., 2019), which showed the potential of pre-\ntraining a large neural network using unsupervised\nlearning. After pre-training, these neural networks\ncan be ﬁne-tuned on speciﬁc tasks. During ﬁne-\ntuning, the weights of the model are tweaked to\nperform well on a speciﬁc task, building upon the\nknowledge learned during pre-training. This has\nled to substantial improvements in the state of the\nart for tasks such as sentiment classiﬁcation, ques-\ntion answering, and many others. When GPT-2\n(Radford et al., 2019) was released, a huge im-\nprovement in text generation ability was obtained.\nThe performance has even been shown to continue\nto improve with an increase in the size of the lan-\nguage models, ranging from 117M parameters for\nthe smallest GPT-2 model, to 175B parameters for\nthe largest of the GPT-3 (Brown et al., 2020) mod-\nels.\nWithin natural language processing, question an-\nswering (QA) is a heavily researched ﬁeld, while\nthe inverse task receives much less attention: ques-\ntion generation (QG) (Pan et al., 2019). For edu-\ncation, being able to generate semantically correct\nand educationally relevant questions is a challeng-\ning task with clear applications. Yet most of the\nwork in this ﬁeld focuses on QG for generating\nsynthetic datasets for question answering, rather\nthan seeing it as a goal on its own. For this rea-\nson, these papers tend to concentrate only on the\ntask of generating a question from a given context\nand answer, while the other elements required for\nmultiple choice questions (MCQs) receive much\nless attention. These elements include selecting the\nanswer and generating the incorrect answers. It is\nthis last part that we focus on: generating incorrect\nanswers, also known as distractors.\nFor the distractor generation task, we use the\nRACE dataset (Lai et al., 2017), which contains\nalmost 100,000 questions. Each of these questions\nis paired with a context of a single paragraph, the\ncorrect answer, and three distractors. We use this to\ncreate a distractor generation model, which gives\nus the ability to generate complete multiple choice\nquestions. Previous work on distractor generation\nwith the RACE dataset Gao et al. (2019); Zhou\net al. (2020) use sequence-to-sequence models to\ngenerate the distractors, which leads to low-quality\narXiv:2010.09598v1  [cs.CL]  19 Oct 2020\nRACE dataset\nQuestion Generator\nGPT-2\nDistractor Generator\nGPT-2\nSQuAD dataset\nQA Filtering\nDistilBERT\nRACE dataset\nTrain Train TrainCONTEXT + ANSWER\nInput\nMULTIPLE CHOICE \nQUESTION\nRepeat N times for N questions\nInput Input If correct\nFigure 1: Overview of the model architecture: we take a context and answer as input, generate a question, generate\nthree distractors, and use the QA model to ﬁlter only correctly answered questions. The distractor and question\ngenerator models are based on GPT-2, which is ideal for text generation, while the QA ﬁltering model is based on\nBERT, which is better for classiﬁcation problems.\ntext. The generation of complete multiple choice\nquestions opens up other possibilities, including\nthe ability to create a QA model which chooses the\ncorrect answer from four options. We will investi-\ngate whether such a multiple choice QA model can\nbe used to ﬁlter only correctly answered questions\nin order to improve the overall quality of question\ngeneration models.\nThe key contributions of this work are:\n• We ﬁne-tuned a GPT-2 language model for\ndistractor generation on the RACE dataset.\n• We ﬁne-tuned a BERT language model for\nmultiple choice question answering on the\nRACE dataset.\n• We proposed a new QA ﬁltering method for\nimproving QG results, by ﬁltering using a mul-\ntiple choice QA model.\n2 Related work\nQuestion generation Early question generation\nmodels were mainly rule-based: deﬁning patterns\nof word types and using these to extract phrases\nfrom the text, which would be transformed into\nquestions (Mitkov and Ha, 2003; Chen et al., 2006;\nHeilman, 2011). In the last decade, these rule-\nbased models were mostly replaced by neural net-\nworks, primarily sequence-to-sequence architec-\ntures (Du et al., 2017; Kim et al., 2019). However,\nin the last year, these again are being replaced, now\nwith Transformer-based language models.\nThe ﬁrst of such works used BERT to generate\nquestions (Alberti et al., 2019). By now, GPT-2\n(Radford et al., 2019) has mostly replaced BERT\nfor QG tasks (Klein and Nabi, 2019; Liu et al.,\n2020; Cho et al., 2019; Lopez et al., 2020). GPT-2\nis a better text generator overall (Wang and Cho,\n2019) due to it being trained solely in a left-to-right\nfashion, predicting the next word in a sequence of\nwords. This is in contrast with bidirectional mod-\nels such as BERT, which are trained primarily by\npredicting masked words. Such masked language\nmodeling training leads to better performance on\nmany NLP classiﬁcation tasks, due to the bidirec-\ntional nature, but is worse at the speciﬁc task of\ntext generation.\nDistractor generation Several previous solu-\ntions for distractor generation (DG) are actually\nranking models. These include the work by Liang\net al. (2018), which ranks distractors from a given\ncandidate set using both feature-based and neural\nnetwork-based ranking models, and Ren and Zhu\n(2020), who use a knowledge base to generate a dis-\ntractor candidate set and a learning-to-rank model\nfor selecting distractors.\nIn 2017, the English language RACE dataset\n(Lai et al., 2017) was published. This was the ﬁrst\nlarge dataset to include distractors along with the\nquestions. Several papers since then have used\nthis to create distractor generation models, includ-\ning Gao et al. (2019), which used a hierarchical\nencoder-decoder model with attention to generate\ndistractors. Zhou et al. (2020) improved upon this\nmodel by adding co-attention layers and using more\ntricks to gain better performance. Our works uses\nTransformer-based language models instead, lead-\ning to higher quality outputs.\nMultiple choice QA The original RACE paper\nused several models to establish baselines on the\nmultiple choice QA task. Their Gated AR model\nachieved an accuracy of 44.1%, which showed the\nlimitations of the models available at that time of\npublication (2017) for such a complex dataset. Re-\ncently, language models have been able to greatly\nsurpass this accuracy, with BERT achieving an ac-\ncuracy of 73.9% (Lan et al., 2019), and the largest\nvariant of ALBERT (Lan et al., 2019) even achiev-\ning an accuracy of 82.3%. We use these advances\nin question answering models to create a multiple\nchoice QA model and employ this in the context of\ndistractor generation.\nQA ﬁltering Alberti et al. (2019) introduced the\nconcept of QA ﬁltering to the domain of question\ngeneration. They generate a question, then answer\nthat question using an extractive text QA model.\nOnly when the QA model generates the correct\nanswer, they keep it. This is to ensure roundtrip\nconsistency. Liu et al. (2020) also used a similar\nﬁltering method, but with the explicit goal of gen-\nerating human-like questions. These approaches\ndiffer from our method since we do not generate\na textual answer, but we check whether a multiple\nchoice QA model can choose the correct option out\nof four answers.\n3 Method\nOur system consists of three separate models: a\nquestion generator, a distractor generator, and a QA\nﬁlter. We will outline how we created and trained\nthese models separately, and then we will explain\nhow we used these jointly to improve the overall\nresults. Figure 1 provides a high-level overview of\nour complete architecture.\n3.1 Question generation\nWhile question generation is not the goal of our\nresearch, we do use it as input for the other\ntwo models. It is used to evaluate the abil-\nity of the QA model to ﬁlter generated ques-\ntion—answer—distractor tuples. Similar to many\nrecent works (Klein and Nabi, 2019; Liu et al.,\n2020; Lopez et al., 2020), we decided to ﬁne-tune\na GPT-2 model, in particular the “small” variant\nwith 117 million parameters. For this task, we\nused the English SQuAD dataset (Rajpurkar et al.,\n2016), speciﬁcally the training dataset of SQuAD\nv2. We used SQuAD rather than the RACE dataset\nfor this task, in order to create a model which is\nsimilar to most recent works in question generation,\nwhich almost exclusively use the SQuAD dataset.\nWe remove questions which are highlighted as be-\ning impossible to answer (as speciﬁed by humans\nwhen the dataset was created), because we want\nour model to generate answerable questions. After\nremoving these, 86,821 questions remained.\nWe extract context—answer—question tuples\nfrom the SQuAD dataset, and tokenize these using\nthe Byte-Pair-Encoding (BPE) tokenizer (Sennrich\net al., 2016) that GPT-2 uses. Since GPT-2 is a\nmodel that learns to generate the next word after a\nsequence of words, we use special tokens to iden-\ntify the segments of the inputs. This forces the\nmodel to learn to generate the correct elements.\nThe input format is shown in Figure 2.\n[context] answer_start [answer] answer_end [question] endoftext\nFigure 2: Format of the input to the QG model. The\nblack boxes denote special tokens supplied to the tok-\nenizer.\nThis model was implemented in PyTorch\n(Paszke et al., 2019) using the Transformers library\n(Wolf et al., 2019). The model was already pre-\ntrained by OpenAI on a large text corpus, and we\nﬁne-tuned it on our dataset. It was ﬁne-tuned for 3\nepochs on the full dataset, using a batch size of 4.\nThe Adam optimizer (Kingma and Ba, 2015) was\nused with a learning rate of 5 × 10−5 and an ep-\nsilon value of 1 × 10−8. This optimizer improves\nupon classical stochastic gradient descent by us-\ning ﬁrst and second moments of the gradients to\nspeed up convergence. Using the Adam optimizer\nis standard practice for Transformer-based models.\nThe learning rate and epsilon values are based on\nrecommendations from Wolf et al. (2019).\n3.2 Distractor generation\nSimilar to the question generation model, we again\nﬁne-tune GPT-2, but this time to generate distrac-\ntors. Since the SQuAD dataset does not contain\ndistractors, we used the RACE dataset (Lai et al.,\n2017) for this model. We do not do any ﬁltering, so\nwe use the full training dataset of87,866 questions.\nWe provide the context, question, and answer as\ninput. The context is where the model can draw\nstylistic inﬂuence and semantic information from.\nThe question is what the distractors should be writ-\nten in relation to. And ﬁnally, the answer should\nbe used to make sure that the distractors are differ-\nent from the answer. The input format is shown in\nFigure 3.\n[context] question_start [question] answer_start [answer] distractor_start\n[distractor1] endoftextdistractor_start [distractor2] distractor_start [distractor3]\n...\nFigure 3: Format of the input to the DG model.\nThis is again tokenized using the BPE tokenizer,\nand we train the model with the same settings.\nHowever, besides training the small GPT-2 model,\nwe also train another model based on the medium\nGPT-2 variant, with 355 million parameters. We\nkeep the settings the same, except for the batch size\nwhich we reduce to 1, since we are limited by the\nmemory usage.\nDuring generation, we also apply a repetition\npenalty, as proposed by the authors of the CTRL\nlanguage model (Keskar et al., 2019). This penal-\nizes the model for generating similar texts, which\nhelps to generate syntactically dissimilar distrac-\ntors. Moreover, we noticed that the model could\nsometimes generate less than three distractors, gen-\nerate non-unique distractors, or generate empty\nstrings as distractors. To alleviate this, we decided\nto ﬁlter non-unique and empty distractors, and to\nrepeat the generation step until three unique and\nnon-empty distractors were found.\n3.3 QA ﬁltering\nIn order to be able to ﬁlter multiple choice ques-\ntions, we need to have a model which can answer\nthem. To create this, we decided to ﬁne-tune the\nDistilBERT model (Sanh et al., 2019), with 66 mil-\nlion parameters. This is a distilled version of BERT,\nretaining 97% of the performance of the small\nBERT model, with 40% less parameters. Most QA\nresearch focuses on extractive QA: models where\nthe output is a string, which is extracted from the\nsource document. In our case, we want a QA model\nwhich chooses one of the multiple choice options\nas the correct answer. To accomplish this, we feed\ncontext—question—answer tuples into BERT. We\nthen combine the four outputs and feed it through\na dropout layer (Srivastava et al., 2014) for regu-\nlarization, a fully connected layer for classiﬁcation,\nand ﬁnally a softmax layer in order to model it as\na multi-class classiﬁcation problem. The input for-\nmat and the model architecture is shown in Figure\n4.\n[context] [question] [answer]CLS SEP SEP SEP\n[context] [question] [answer]CLS SEP SEP SEP\n[context] [question] [answer]CLS SEP SEP SEP\n[context] [question] [answer]CLS SEP SEP SEP\nDropout\nFully connected\nSoftmax\nDistilBERT\nFigure 4: Overview of the input and architecture of the\nQA ﬁltering model. We feed each distractor separately\ninto the DistilBERT model, then use the four outputs to\ndetermine the answer.\nThis model was trained for 3 epochs, with a fully\nconnected layer dimension of 768, a dropout ratio\nof 10%, a batch size of 2, and 8 gradient accumula-\ntion steps per batch.1 Again, the Adam optimizer\nwas used, with a learning rate of 3 × 10−5 and an\nepsilon value of 1 × 10−8.\nOnce we have the multiple choice QA model,\nwe can use it to ﬁlter question—answer—distractor\ntuples. The intuition behind this QA ﬁlter is that\nwhen a multiple choice QA model is given per-\nfect information, it should almost always be able\nto answer a generated question correctly. If not,\nthere could be two type of errors: either (A) the\nQA model does not have the capability to answer it,\nor (B) the question or distractors are somehow in-\ncorrect (i.e. this is a bad question). As for the type\nA errors, this should be unlikely because the model\nreceives the exact context which is needed to an-\nswer the question. Imagine if you had a test and the\nstudents would be provided the paragraph which\ncontained the answer for the question right next to\nevery question: students would surely receive high\ngrades. Moreover, QA models have already sur-\npassed human performance on the SQuAD dataset\n(Zhang et al., 2020) and are nearing human perfor-\nmance on the RACE dataset (Lan et al., 2019), fur-\nther decreasing the chance of type A errors. Type B\nerrors are exactly what the QA model aims to ﬁlter.\nTherefore, whether the QA model can answer the\nquestion should be a good ﬁlter for high-quality\nquestions.\n4 Results\nTo evaluate our work, we used three different ap-\nproaches: evaluating the text generation quality us-\ning standardized metrics, evaluating the ability for\nthe QA model to answer the generated questions,\nand using a human evaluation to complement these\ntwo automatic metrics with a human perspective.\n4.1 Quantitative evaluation\nWe compare our models against three baselines:\nthe basic sequence-to-sequence distractor generator\nmodel from Gao et al. (2019), the improved hierar-\nchical encoder-decoder model with static attention\n(HSA ) from Gao et al. (2019), and the hierarchi-\ncal model enhanced with co-attention (CHN) from\nZhou et al. (2020).\n4.1.1 Text generation quality\nAs a high-level overview, we use several metrics\nto calculate the quality of the generated distractors.\n1This simulates a larger batch size, which is required for\ngood performance with a QA model on the RACE dataset (Liu\net al., 2019).\nBLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L\nDataset questions\nSEQ2SEQ (Gao et al., 2019) 25.25 11.99 6.54 3.92 13.34\nHSA (Gao et al., 2019) 26.93 13.57 8.00 5.21 14.45\nCHN (Zhou et al., 2020) 27.53 13.80 8.46 5.80 15.11\nGPT-2 SMALL 60.12 26.56 13.64 9.17 12.36\nGPT-2 MEDIUM 60.85 26.52 13.20 8.70 12.01\nGPT-2 MEDIUM (after QA ﬁltering) 60.21 26.38 13.29 8.84 12.00\nGenerated questions\nGPT-2 SMALL 57.08 24.14 11.73 7.59 10.40\nGPT-2 MEDIUM 57.66 24.00 11.29 7.14 9.79\nGPT-2 MEDIUM (after QA ﬁltering) 56.60 23.50 11.05 7.03 9.69\nTable 1: Text generation quality of the distractor generation model. The DG scores are calculated separately for\neach distractor, and then averaged over all three distractors.\nSpeciﬁcally, we use the BLEU metric, which uses\nmodiﬁed2 precision of n-grams to determine the\ncorrespondence to human-written text; and we use\nthe ROUGE-L metric, which looks at the longest\ncommon subsequence and is a measure of recall.\nThe results of this evaluation can be found in Table\n1. By default, we use questions from the dataset as\ninput to the distractor generator. As a comparison,\nwe also show the case where we are generating the\nquestions as well, to show what the impact is on\nthe results of the distractor generator. This should\nshow lower text generation quality scores, since\nthe question generator will at times generate low\nquality questions, which would make it harder for\nthe distractor generator to generate high quality\ndistractors.\nThe distractors in the RACE dataset are on av-\nerage 5.7 words long, with a standard deviation of\n3.3. This means that for evaluating distractors, the\nBLEU-1 and BLEU-2 scores are more relevant than\nBLEU-3 and BLEU-4, since 3-grams and 4-grams\noccur much less.\nLooking at the quantitative results in Table 1, the\nBLEU scores are substantially higher than those\nreported in previous work. This is in line with\nwhat other studies have shown with the use of\nTransformer-based language models for text gen-\neration: these are much better at generating coher-\nent text than previous sequence-to-sequence model\nbased approaches were. However, interestingly, the\nROUGE-L score is actually slightly lower than the\n2BLEU’s modiﬁed version of precision accounts for over-\ngeneration of words by clipping based on the maximum refer-\nence word count.\nROUGE-L scores of prior work. While the BLEU\nscore is a measure of precision, ROUGE-L is a mea-\nsure of recall. ROUGE measures how many words\nin the human references appear in the generated\ndistractors.\nWhen looking at the differences between our\nown models, these seem to be relatively minor. The\nlarger GPT-2 MEDIUM model, which has twice the\nnumber of parameters as the GPT-2 SMALL model,\nonly gains less than a percentage point (when look-\ning at BLEU-1). This minor change is likely due to\nthe dataset size: the small model is already able to\nmodel the distribution well and can already learn\nto generate distractors like the outputs from the\ndataset. Furthermore, it appears that only rating\ndistractors after the QA ﬁltering step does not lead\nto better results. Lastly, the scores for when we\ngenerate questions are on average several percent-\nage points lower than when we use questions from\nthe dataset. This makes sense: the question genera-\ntor will occasionally generate incoherent questions,\nwhich will complicate the work of the distractor\ngenerator, and lead to outputs which differ more\nfrom the reference dataset. But it is worth men-\ntioning that the scores for generated questions are\nnot dramatically lower, which means that the solu-\ntion for distractor generation proposed here seems\nto generalise well to the harder task of end-to-end\nmultiple choice question and distractor generation.\n4.1.2 Question answering ability\nAs a second quantitative evaluation, we decided\nto measure the number of questions answered cor-\nrectly by the QA model, when the distractors are\nDataset questions Generated questions\nGPT-2 SMALL 51.15% 54.29%\nGPT-2 MEDIUM 53.36% 55.90%\nTable 2: Accuracy of the QA model for generated distractors by both DG models.\ngenerated by our model. The better the distrac-\ntors, the higher this percentage should be, as good\ndistractors should be clearly incorrect answers to\nthe QA model, given the fact that the model has\nfull access to the context. However, as previously\nnoted, the error rate of the QA model is a summa-\ntion of two errors: errors due to bad distractors or\nquestions, as well as errors made by the QA model\nitself due to other reasons. Therefore, the accuracy\non its own is not meaningful to evaluate the distrac-\ntors, but it is meaningful as a relative number to\ncompare models.\nFor the results, see Table 2. We compare the\nGPT-2 SMALL and MEDIUM models. Again, we\nalso compare the case for which we generate the\nquestions with our question generator, with the case\nwhere we use the questions provided by the dataset\nand only generate the distractors. We can clearly\nsee that using GPT-2 MEDIUM for distractor gener-\nation, which has twice the number of parameters as\nGPT-2 SMALL , results in more accurate question\nanswering than the smaller model. Interestingly,\nthe scores when the the questions are also gener-\nated, are better than when the questions are taken\nfrom the dataset. An explanation for this could\nbe that the question generation model generated\nquestions which are simpler for the QA model to\nanswer, thus leading to higher QA model accuracy.\n4.2 Human evaluation\nMetrics such as BLEU and ROUGE are based\nmerely on comparing text similarity to reference\nsentences and are therefore limited in their ability\nto measure the quality of generated text as a hu-\nman would (Callison-Burch et al., 2006). Good\ndistractors could deﬁnitely be different from the\nreferences, which is not accounted for in the text\ngeneration quality metrics. Moreover, the text gen-\neration quality is calculated on a per-distractor ba-\nsis, i.e. the ﬁrst generated distractor for a question\nis compared with the ﬁrst reference distractor, and\nso on. We would argue it makes more sense to com-\npare a generated distractor with all three reference\ndistractors, but we chose this type of comparison\nto follow the same evaluation methodology as pre-\nvious works (Gao et al., 2019; Zhou et al., 2020).\nSimply reordering distractors would lead to lower\ntext generation quality scores. To account for these\nlimitations in the quantitative evaluation methods,\nwe decided to run a human evaluation. Speciﬁcally,\nwe wanted to test the ability of the QA ﬁltering\nmodel to ﬁlter high quality questions which are\nanswerable by a human. We set up a human evalu-\nation with 4 assessors, each rating 100 generated\nquestions (leading to a total of 310 assessed ques-\ntions) with the following questions:\n1. Is the question well-formed and can you\nunderstand the meaning? Possible an-\nswers include “Both understandable and well-\nformed”, “Understandable, but not well-\nformed.”, and “Neither”.\n2. If the question is at least understandable,\ndoes the answer make sense in relation to\nthe question? This is a yes, no, or I don’t\nknow question.\nThese questions are based on work done by Liu\net al. (2020), but we removed the relevancy ques-\ntion since it did not provide for a good indica-\ntor of quality in their results, and we rewrote the\nquestions and answers to improve clarity. Of the\n100 generated questions rated by each assessor, 30\nquestions were the same for each assessor, while\nthe other 70 were unique questions. This enabled\nus to estimate inter-rater reliability, while still rat-\ning a large number of questions overall. Of these\n310 unique questions, 155 are questions that the\nQA ﬁltering model accepted, while the other 155\nare questions that the QA ﬁltering model rejected.\nThis should highlight the effect of the QA ﬁltering\nmodel and show whether it is a good measure of\nthe quality of questions. 10 example questions used\nas part of the evaluation are shown in Figure 5.\nWe estimated the inter-rater reliability of the\ndata using the Fleiss’ kappa measure (Fleiss, 1971).\nThis led to a κvalue of 0.413 for question 1 and a\nκvalue of -0.147 for question 2. Using the inter-\npretation table3 from Landis and Koch (1977), the\n3It should be noted that there is extensive debate about the\nAccepted questions\n-  What did the Wahhabism mean for the Muslims?\n      wars\n-  What does the climate change report do?\n      does not carry out research nor does it monitor\n      climate related data\n-  What do Wankel engines use?\n      cylinders and valve gear\n      \nRejected questions\n-  Who was the composer for Destiny's Child?\n      Philip Glass\n-  What was Zia-ul-Haq's primary ideology?\n      Islamism\n-  Who was the Duke Yansheng Kong Duanyou's brother?\n      Kong Duancao\nFigure 5: Randomly chosen examples of generated questions used for the human evaluation. The difference in\nquality appears small, which aligns with the results of the human evaluation.\nAccepted Rejected\nQuestion 1 (question quality)\nWell-formed and understandable 70% 69%\nOnly understandable 18% 14%\nNeither 12% 18%\nQuestion 2 (answer compatibility)\nYes 50% 56%\nNo 41% 37%\nI don’t know 8% 7%\nTable 3: Results from the human evaluation. We compare the quality of the questions which were accepted by the\nQA ﬁltering model with those which were rejected.\nassessors would appear to be in moderate agree-\nment for question 1, but in slight disagreement for\nquestion 2. Note that we did not ﬁlter the results\nfrom question 2 for when the assessors chose the\n‘Neither’ option in question 1, which might have in-\nﬂuenced the results if the assessors misunderstood\nthe question.\nThe output of the human evaluation can be found\nin Table 3. The questions which the QA ﬁlter-\ning model accepted are overall 5% point better\nthan those it rejected. 88% of accepted questions\nare either only understandable (18%) or are both\nwell-formed and understandable (70%). This is\na bit higher than the 83% for rejected questions.\nHowever, this is still a pretty small difference. A\nPearson’s chi-squared test indicated that the differ-\nence between the accepted and rejected questions\nwas not signiﬁcant ( p = 0.21 for question 1 and\np= 0.40 for question 2).\n5 Discussion\nThe results show that the text quality of generated\ndistractors is substantially higher than previous\nworks, that using a larger model has a small ef-\nfect on the question answering ability, and that the\ndifference in quality when applying the QA ﬁlter-\ning model is statistically insigniﬁcant, as evaluated\nvalidity of these ranges of interpretation, but it seems to be the\nmost commonly used.\nby humans. To put these results into context, we\nneed to be aware of the limitations of the differ-\nent evaluation methods. As for the text generation\nquality measures such as BLEU and ROUGE, the\nmain issue is that they do not consider the meaning\nof the text, but only literal word overlap. There is\nsome recent work in using neural language models\nfor evaluating the text quality (Sellam et al., 2020),\nwhich should better incorporate meaning into the\nscore. This could be considered for future work in\nthis area. Moreover, these metrics do not evaluate\nsentence structure as part of their calculation.\nAs for the question answering ability, the main\nissue is that the model can accept bad questions\nor reject good questions. These types of errors are\nincluded in the total score. Ideally, we would need\na QA model which always answers a good ques-\ntion correctly and always answers a bad question\nincorrectly. This means that the absolute values\nfrom Table 2 contain some noise, but they do give\na general indication of relative quality.\nAs for the human evaluation, the main issue is\nthe low number of total assessed questions, lead-\ning to a lack of statistical power. Since there is\nsome subjectivity in how the generated questions\nare rated by the assessors, we would say that mod-\nerate agreement for question 1 is a positive result.\nThe low score for question 2 can be explained by\na combination of the question being even more\nsubjective, as well as the fact that question 2 was\nperhaps not explained well in the evaluation setup.\nTherefore, we focus primarily on the results for\nquestion 1.\nThe known limitations of evaluation metrics for\ntext generation have led us to use three different\nevaluation methods. The combined results suggest\nthat whether the question is answerable by the mul-\ntiple choice QA model, is only a minor indicator\nof question quality. There was only a small differ-\nence in the quantitative results and no statistically\nsigniﬁcant difference in the qualitative results. One\npossible reason for this result is that the QA model\nwill guess one of the four options if it does not\nknow the answer for certain, leading to a high false\npositive rate. This could potentially be resolved by\nusing bayesian neural networks to determine the\nQA uncertainy and set a threshold, ensuring that\nthe model is sure about its prediction. Or a ﬁfth\n“I don’t know” option could be added to the QA\noutput and we could teach the model to choose this\noption when it is not certain.\n6 Conclusions\nOverall, we can conclude that distractor generation\nusing GPT-2 works well: the proposed method\nbeats the state-of-the-art baselines on all BLEU\nmetrics. In addition, we proposed question ﬁltering\nusing a multiple choice QA model. This additional\nstep does not give a signiﬁcant improvement in the\nhuman-experienced question quality.\nWe have a number of suggestions for future re-\nsearch. First, besides being applied to our own\nquestion generator, we could apply our QA ﬁlter-\ning model to improve the results of other ques-\ntion generation models. Second, larger pretrained\nTransformer-based language models could be ex-\nperimented with on this task. It would be interest-\ning to see how much of an improvement such larger\npretrained models could bring.\nSpeciﬁcally, in the near future, we plan to im-\nprove the distractor generation model by setting\nup an end-to-end training pipeline with the ques-\ntion answering model. Inspired by Klein and Nabi\n(2019), the idea is to generate distractors for a ques-\ntion, then feed this to the QA model, and back-\npropagate the loss of the QA model with regards to\nthe weights of the DG model. This way, we could\nteach the DG model to generate distractors such\nthat the QA model could still correctly identify the\ncorrect answer, as the current DG model does not\nhave enough inductive bias to generate distractors\nwhich are actually incorrect answers.\nIn summary, we have shown that generating mul-\ntiple choice questions with distractors is technically\npossible using Transformer-based language models.\nThis opens up many new possibilities and interest-\ning applications. For example, it could be used\nto assist teachers in creating multiple choice ex-\nams. Or it could be used to automatically quiz\nstudents when they are learning. These develop-\nments are getting closer to reality and we aimed\nfor this work to provide a valuable contribution\ntowards this hopeful future.\nAcknowledgements\nThis work was performed using resources provided\nby the Academic Leiden Interdisciplinary Cluster\nEnvironment (ALICE).\nReferences\nChris Alberti, Daniel Andor, Emily Pitler, Jacob De-\nvlin, and Michael Collins. 2019. Synthetic QA cor-\npora generation with roundtrip consistency. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6168–\n6173, Florence, Italy. Association for Computa-\ntional Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluating the role of Bleu in ma-\nchine translation research. In 11th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics, Trento, Italy. Association for\nComputational Linguistics.\nChia-Yin Chen, Hsien-Chin Liou, and Jason S. Chang.\n2006. FAST – an automatic generation system for\ngrammar tests. In Proceedings of the COLING/ACL\n2006 Interactive Presentation Sessions , pages 1–4,\nSydney, Australia. Association for Computational\nLinguistics.\nWoon Sang Cho, Yizhe Zhang, Sudha Rao, Asli Celiky-\nilmaz, Chenyan Xiong, Jianfeng Gao, Mengdi Wang,\nand Bill Dolan. 2019. Contrastive multi-document\nquestion generation.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Association for Computational\nLinguistics (ACL).\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin ,\n76(5):378.\nYifan Gao, Lidong Bing, Piji Li, Irwin King, and\nMichael R. Lyu. 2019. Generating distractors for\nreading comprehension questions from real exami-\nnations. In AAAI-19 AAAI Conference on Artiﬁcial\nIntelligence.\nMichael Heilman. 2011. Automatic factual question\ngeneration from text. Ph.D. thesis, Carnegie Mellon\nUniversity.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCtrl: A conditional transformer language model for\ncontrollable generation.\nYanghoon Kim, Hwanhee Lee, Joongbo Shin, and Ky-\nomin Jung. 2019. Improving neural question gen-\neration using answer separation. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 33, pages 6602–6609.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nTassilo Klein and Moin Nabi. 2019. Learning to an-\nswer by learning to ask: Getting the best of gpt-2\nand bert worlds.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785–794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nbiometrics, pages 159–174.\nChen Liang, Xiao Yang, Neisarg Dave, Drew Wham,\nBart Pursel, and C. Lee Giles. 2018. Distractor gen-\neration for multiple choice questions using learning\nto rank. In Proceedings of the Thirteenth Work-\nshop on Innovative Use of NLP for Building Educa-\ntional Applications , pages 284–290, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nBang Liu, Haojie Wei, Di Niu, Haolan Chen, and\nYancheng He. 2020. Asking questions the human\nway: Scalable question-answer generation from text\ncorpus. In Proceedings of The Web Conference\n2020, WWW ’20, page 2032–2043, New York, NY ,\nUSA. Association for Computing Machinery.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nLuis Enrico Lopez, Diane Kathryn Cruz, Jan Chris-\ntian Blaise Cruz, and Charibeth Cheng. 2020.\nTransformer-based end-to-end question generation.\nRuslan Mitkov and Le An Ha. 2003. Computer-aided\ngeneration of multiple-choice tests. In Proceedings\nof the HLT-NAACL 03 Workshop on Building Edu-\ncational Applications Using Natural Language Pro-\ncessing, pages 17–22.\nLiangming Pan, Wenqiang Lei, Tat-Seng Chua, and\nMin-Yen Kan. 2019. Recent advances in neural\nquestion generation.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in neural information processing systems ,\npages 8026–8037.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nSiyu Ren and Kenny Q. Zhu. 2020. Knowledge-driven\ndistractor generation for cloze-style multiple choice\nquestions.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nThibault Sellam, Dipanjan Das, and Ankur Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7881–7892, Online. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation , pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning.\nZhuosheng Zhang, Junjie Yang, and Hai Zhao. 2020.\nRetrospective reader for machine reading compre-\nhension.\nXiaorui Zhou, Senlin Luo, and Yunfang Wu. 2020. Co-\nattention hierarchical network: Generating coherent\nlong distractors for reading comprehension. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artiﬁcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020 , pages 9725–\n9732. AAAI Press.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.639514684677124
    },
    {
      "name": "Computer science",
      "score": 0.4583626687526703
    },
    {
      "name": "Psychology",
      "score": 0.43404895067214966
    },
    {
      "name": "Engineering",
      "score": 0.19148293137550354
    },
    {
      "name": "Electrical engineering",
      "score": 0.13163012266159058
    },
    {
      "name": "Voltage",
      "score": 0.05519998073577881
    }
  ],
  "institutions": []
}