{
  "title": "A Masked Segmental Language Model for Unsupervised Natural Language Segmentation",
  "url": "https://openalex.org/W3156299562",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5058978201",
      "name": "Carlton Downey",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5100676785",
      "name": "Fei Xia",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5031004017",
      "name": "Gina‐Anne Levow",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5017484646",
      "name": "Shane Steinert‐Threlkeld",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952125979",
    "https://openalex.org/W2130042265",
    "https://openalex.org/W3133631758",
    "https://openalex.org/W2079735306",
    "https://openalex.org/W4226246098",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2126377586",
    "https://openalex.org/W2962875366",
    "https://openalex.org/W2158266063",
    "https://openalex.org/W2105738468",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2074546930",
    "https://openalex.org/W2140991203",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2287914047",
    "https://openalex.org/W179875071",
    "https://openalex.org/W4304111904",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2117621558",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963077280",
    "https://openalex.org/W2295297373",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035229572",
    "https://openalex.org/W3097485645",
    "https://openalex.org/W3158607076",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2008225289",
    "https://openalex.org/W2251563156",
    "https://openalex.org/W2096204319",
    "https://openalex.org/W2963357986",
    "https://openalex.org/W3035193825",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2891546148",
    "https://openalex.org/W2952343510",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2010906431",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2609370997",
    "https://openalex.org/W3093427098",
    "https://openalex.org/W2592647456",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W1533169541",
    "https://openalex.org/W2963899393",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2101711363",
    "https://openalex.org/W2039133703",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3144818841"
  ],
  "abstract": "We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world's languages are both morphologically complex, and have no large dataset of \"gold\" segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.",
  "full_text": "19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 39 - 50\nJuly 14, 2022 ©2022 Association for Computational Linguistics\nA Masked Segmental Language Model for Unsupervised Natural\nLanguage Segmentation\nC.M. Downey Fei Xia Gina-Anne Levow Shane Steinert-Threlkeld\nDepartment of Linguistics, University of Washington\n{cmdowney, fxia, levow, shanest}@uw.edu\nAbstract\nWe introduce a Masked Segmental Language\nModel (MSLM) for joint language model-\ning and unsupervised segmentation. While\nnear-perfect supervised methods have been de-\nveloped for segmenting human-like linguis-\ntic units in resource-rich languages such as\nChinese, many of the world’s languages are\nboth morphologically complex, and have no\nlarge dataset of “gold” segmentations for su-\npervised training. Segmental Language Mod-\nels offer a unique approach by conducting un-\nsupervised segmentation as the byproduct of\na neural language modeling objective. How-\never, current SLMs are limited in their scalabil-\nity due to their recurrent architecture. We pro-\npose a new type of SLM for use in both unsu-\npervised and lightly supervised segmentation\ntasks. The MSLM is built on a span-masking\ntransformer architecture, harnessing a masked\nbidirectional modeling context and attention,\nas well as adding the potential for model scal-\nability. In a series of experiments, our model\noutperforms the segmentation quality of recur-\nrent SLMs on Chinese, and performs similarly\nto the recurrent model on English.\n1 Introduction\nOutside of the orthography of English and lan-\nguages with similar writing systems, natural lan-\nguage is rarely overtly segmented into meaningful\nunits. Languages such as Chinese, are written with\nno spaces in between characters, and Chinese Word\nSegmentation remains an active ﬁeld of study (e.g.\nTian et al., 2020). Running speech is also highly\nﬂuent with no meaningful pauses existing between\n“words” like in orthography.\nTokenization schemes for large modern lan-\nguage models are now largely passed off to greedy\ninformation-theoretic algorithms like Byte-Pair\nEncoding (Sennrich et al., 2016) and the subse-\nquent SentencePiece (Kudo and Richardson, 2018),\nwhich create subword vocabularies of a desired size\nby iteratively joining commonly co-occuring units.\nHowever, these segmentations are usually not sen-\nsical to human readers (Park et al., 2021). Given\nthe current performance of models using BPE-type\ntokenization, the nonsensical nature of these seg-\nmentations does not necessarily seem to inhibit the\nsuccess of neural models.\nNevertheless, BPE does not necessarily help\nin situations where knowing a sensical segmen-\ntation of linguistic-like units is important, such as\nattempting to model the ways in which children\nacquire language (Goldwater et al., 2009), segment-\ning free-ﬂowing speech (Kamper et al., 2016; Rasa-\nnen and Blandon, 2020), creating linguistic tools\nfor morphologically complex languages (Moeng\net al., 2021), or studying the structure of an endan-\ngered language with few or no current speakers\n(Dunbar et al., 2020).\nWhile near-perfect supervised models have been\ndeveloped for resource-rich languages like Chinese,\nmost of the world’s languages do not have large\ncorpora of training data (Joshi et al., 2020). Es-\npecially for morphologically complex languages,\nlarge datasets containing “gold” segmentations into\nunits like morphemes are very rare.\nTo help mitigate this problem, we propose a\nnovel variant of the unsupervised Segmental Lan-\nguage Model (Sun and Deng, 2018; Kawakami\net al., 2019). Segmental Language Models (SLMs)\nfunction as neural LMs that can also be used for un-\nsupervised segmentation correlating with units like\nwords and morphemes (Kawakami et al., 2019).\nTraditional (recurrent) SLMs provide a good\ntradeoff between language-modeling performance\nand segmentation quality. However, in order to em-\nbrace a fully bidirectional modeling context, atten-\ntion, and the scalability afforded by parallelization,\nwe present a Masked Segmental Language Model\n(MSLM), built on a span-masking transformer ar-\nchitecture (Vaswani et al., 2017). As far as we are\naware, we are the ﬁrst to introduce a non-recurrent\n39\narchitecture for segmental modeling.\nIn this paper, we seek to compare our model\nto recurrent baselines across two standard word-\nsegmentation datasets in Chinese and English, with\nthe hope of expanding to more languages and do-\nmains (such as speech) in future work. We con-\nstrain the scope of our work to comparison with\nrecurrent SLMs both because standard Bayesian\nmodels have been compared to SLMs elsewhere\n(Kawakami et al., 2019, Section 2), and because\nSLMs have different use cases from Bayesian algo-\nrithms, which tend to be weaker language models\nand lack continuous character representations that\nare invaluable in settings such as transfer learning.\nIn what follows, we overview baselines in unsu-\npervised segmentation as well as other precursors\nto SLMs (Section 2), provide a formal character-\nization of SLMs in general, as well as the archi-\ntecture and modeling assumptions that make the\nMSLM distinct (Section 3), present our experimen-\ntal method comparing recurrent and masked SLMs\n(Section 4), and ﬁnally show that the MSLM out-\nperforms its recurrent counterpart on Chinese seg-\nmentation, and performs similarly to the recurrent\nmodel on English (Sections 5-6). Section 7 lays\nout directions for future work.\n2 Related Work\nSegmentation Techniques and SLM Precursors\nAn early application of machine learning to unsu-\npervised segmentation is Elman (1990), who shows\nthat temporal surprisal peaks in RNNs provide a\nheuristic for inferring word boundaries. Subse-\nquently, Minimum Description Length (MDL) (Ris-\nsanen, 1989) was widely used. The MDL model\nfamily underlies well-known segmentation tools\nsuch as Morfessor (Creutz and Lagus, 2002) and\nother notable works (de Marcken, 1996; Goldsmith,\n2001).\nMore recently, Bayesian models have proved\nsome of the most accurate in their ability to model\nword boundaries. Some of the best examples are\nHierarchical Dirichlet Processes (Teh et al., 2006),\ne.g. those applied to natural language by Goldwater\net al. (2009), as well as Nested Pitman-Yor (Mochi-\nhashi et al., 2009; Uchiumi et al., 2015). However,\nKawakami et al. (2019) notes most of these do not\nadequately account for long-range dependencies in\nthe same capacity as modern neural LMs.\nSegmental Language Models follow a variety of\nrecurrent models proposed for ﬁnding hierarchi-\ncal structure in sequential data. Inﬂuential among\nthese are Connectionist Temporal Classiﬁcation\n(Graves et al., 2006), Sleep-Wake Networks (Wang\net al., 2017), Segmental RNNs (Kong et al., 2016),\nand Hierarchical Multiscale Recurrent Neural Net-\nworks (Chung et al., 2017).\nIn addition, SLMs draw heavily from character\nand open-vocabulary language models. For exam-\nple, Kawakami et al. (2017) and Mielke and Eisner\n(2019) present open-vocabulary language models\nin which words are represented either as atomic\nlexical units, or built out of characters. While the\nhierarchical nature and dual-generation strategy of\nthese models did inﬂuence SLMs (Kawakami et al.,\n2019), both assume that word boundaries are avail-\nable during training, and use them to form word\nembeddings from characters on-line. In contrast,\nSLMs usually assume no word boundary informa-\ntion is available in training.\nSegmental Language Models The next section\nhas a more technical description of SLMs; here we\ngive a short overview of related work. The term\nSegmental Language Model seems to be jointly\ndue to Sun and Deng (2018) and Kawakami et al.\n(2019). Sun and Deng (2018) demonstrate strong\nresults for Chinese Word Segmentation using an\nLSTM-based SLM and greedy decoding, competi-\ntive with and sometimes exceeding state of the art\nfor the time. This study tunes the model for seg-\nmentation quality on a validation set, which we will\ncall a “lightly supervised” setting (Section 4.3).\nKawakami et al. (2019) use LSTM-based SLMs\nin a strictly unsupervised setting in which the\nmodel is only trained to optimize language-\nmodeling performance on the validation set, and is\nnot tuned on segmentation quality. Here they report\nthat “vanilla” SLMs give sub-par segmentations\nunless combined with one or more regularization\ntechniques, including a character n-gram “lexicon”\nand length regularization.\nFinally, Wang et al. (2021) very recently intro-\nduce a bidirectional SLM based on a Bi-LSTM.\nThey show improved results over the unidirectional\nSLM of Sun and Deng (2018), test over more su-\npervision settings, and include novel methods for\ncombining decoding decisions over the forward and\nbackward directions. This study is most similar to\nour own work, though our transformer-based SLMs\nutilize a bidirectional context in a qualitatively dif-\nferent way, and do not require an additional layer\nto capture the reverse context.\n40\n3 Model\n3.1 Recurrent SLMs\nA schematic of the original Recurrent SLM can be\nfound in Figure 1. Within an SLM, a sequence of\nsymbols or time-steps x can further be modeled as\na sequence of segments y, which are themselves\nsequences of the input time-steps, such that the\nconcatenation of segments π(y) =x.\nSLMs are broken into two levels: a Context En-\ncoder and a Segment Decoder. The Segment De-\ncoder estimates the probability of the jth character\nin the segment starting at index i, yi\nj, as:\np(yi\nj|yi\n0:j,x0:i) =Decoder(hi\nj−1,yi\nj−1)\nwhere the indices for xi:j are [i,j). The Context\nEncoder encodes information about the input se-\nquence up to index i. The hidden encoding hi is\nhi = Encoder(hi−1,xi)\nFinally, the Context Encoder “feeds” the Seg-\nment Decoder: the initial character of a segment\nbeginning at iis decoded using (transformations\nof) the encoded context as initial states (gh(x) and\ngstart(x) are single feed-forward layers):\np(yi\n0|x0:i) =Decoder(hi\n∅,starti)\nhi\n∅= gh(hi−1)\nstarti = gstart (hi−1)\nFor inference, the probability of a segmentyi:i+k\n(starting at index iand of length k) is modeled as\nthe log probability of generating yi:i+k with the\nSegment Decoder given the left context π(y0:i) =\nx0:i. Note that the probability of a segment is\nnot conditioned on other segments / segmentation\nchoice, but only on the unsegmented input time-\nseries. Thus, the probability of the segment is\np(yi\n0|hi\n∅,starti)\nk∏\nj=1\np(yi\nj|hi\nj−1,yi\nj−1)\nwhere yi\nk is the end-of-segment symbol.\nThe probability of a sentence is thus modeled\nas the marginal probability over all possible seg-\nmentations of the input, as in equation (1) below\n(where Z(|x|) is the set of all possible segmenta-\ntions of an input x). However, since there are 2|x|−1\npossible segmentations, directly marginalizing is\nintractable. Instead, dynamic programming over\na forward-pass lattice can be used to recursively\ncompute the marginal as in (2) given the base con-\ndition that α0 = 1. The maximum-probability seg-\nmentation can then be read off of the backpointer-\naugmented lattice through Viterbi decoding.\np(x) =\n∑\nz∈Z(|x|)\n∏\ni\np(yi:i+zi ) (1)\np(x0:i) =αi =\nL∑\nk=1\np(yi−k:i|x0:i−k)αi−k (2)\nFigure 1: Recurrent Segmental Language Model\n3.2 New Model: Masked SLM\nWe present a Masked Segmental Language Model,\nwhich leverages a non-directional transformer as\nthe Context Encoder. This reﬂects recent ad-\nvances in bidirectional (Schuster and Paliwal, 1997;\nGraves and Schmidhuber, 2005; Peters et al., 2018)\nand adirectional language modeling (Devlin et al.,\n2019). Such modeling contexts are also psycholog-\nically plausible: Luce (1986) shows that in acoustic\nperception, most words need some following con-\ntext to be recognizable.\nA key difference between our model and stan-\ndard Masked LMs like BERT is that the latter pre-\ndict single tokens based on the rest, while for SLMs\nwe must predict a segment of tokens based on all\nother tokens outside the segment. For instance, to\npredict the three-character segment starting at xt,\nthe modeled distribution is p(xt:t+3|x<t,x≥t+3).\nSome recent pre-training techniques for trans-\nformers, such as MASS (Song et al., 2019) and\n41\nBART (Lewis et al., 2020) mask out spans to be\npredicted. A key difference between our model and\nthese approaches is that the pre-training data for\nlarge transformer models is usually large enough\nthat only about 15% of training tokens are masked,\nwhile we need to estimate the generation probabil-\nity for every possible segment of x. Since the usual\nmethod for masking is to replace the masked to-\nken(s) with a special symbol, only one span can be\npredicted with each forward pass. However, each\nsequence contains O(|x|) possible segments, so re-\nplacing each one with a mask token and recovering\nit would require as many forward passes.\nThese design considerations motivate our Seg-\nmental Transformer Encoder, and the Segmen-\ntal Attention Maskaround which it is based. Each\nforward pass of the encoder generates an encoding\nfor every possible start-position in x, for a segment\nof up to length k. The encoding at timestep t−1\ncorresponds to every possible segment whose ﬁrst\ntimestep is at index t. Thus with maximum seg-\nment length of kand total sequence length n, the\nencoding at each index t−1 will approximate\np(xt:t+1,xt:t+2,...xt:t+k|x<t,x≥t+k)\nThis encoder leverages an attention mask that\nconditions predictions only on indices outside the\npredicted segment. An example of this mask with\nk = 3 is shown in Figure 2. For max segment\nlength k, the mask is given by:\nαi,j =\n{\n−∞ if 0 <j −i≤k\n0 else\nFigure 2: Segmental Attention Mask with segment-\nlength (k) of 3. Blue squares are equal to 0, orange\nsquares are equal to −∞. This mask blocks the posi-\ntion encoding the segment in the Queries from attend-\ning to segment-internal positions in the Keys.\nThis solution is similar to that of Shin et al.\n(2020), developed independently and concurrently\nwith our work, which uses a custom attention mask\nto “autoencode” each position without needing a\nspecial mask token. One key difference is that their\nmasking scheme is used to predict single tokens,\nrather than spans. In addition, their mask runs di-\nrectly along the diagonal of the attention matrix,\nrather than being offset. This means that to pre-\nserve self-masking in the ﬁrst layer, the Queries are\nthe “pure” positional embeddings.\nTo prevent information leaking “from under the\nmask”, our encoder uses a different conﬁguration\nin its ﬁrst layer than in subsequent layers. In the\nﬁrst layer, Queries, Keys, and Values are all learned\nfrom the original input embeddings. In subsequent\nlayers, the Queries come from the hidden encod-\nings output by the previous layer, while Keys and\nValues are learned directly from the original em-\nbeddings. If Queries and either Keys or Values both\ncome from the previous layer, information can leak\nfrom positions that are supposed to be masked for\na particular query position. Shin et al. (2020) come\nto a similar solution to preserve their auto-encoder\nmasking.\nThe encodings learned by the segmental encoder\nare then input to an SLM decoder in exactly the\nsame way as previous models (Figure 3).\nFigure 3: Masked Segmental Language Model, k= 2.\nTo tease apart the role of an adirectional model-\ning assumption itself, vs the role of attention, we\nadditionally deﬁne a Directional MSLM, which\nuses a directional (“causal”) mask instead of the\nspan masking type. Using the directional mask, the\nencoder is still attention-based, but the language\nmodeling context is strictly “directional”, in that\n42\npositions are only allowed to attend over a mono-\ntonic “leftward” context (Figure 4).\nFinally, to add positional information to the en-\ncoder, we use static sinusoidal encodings (Vaswani\net al., 2017) and additionally employ a linear map-\nping f to the concatenation of the original and\npositional embeddings to learn the ratio at which\nto add the two together.\ng= 1.0 +ReLU(f([embedding,position]))\nembedding ← −g∗embedding + position\n4 Experiments\nOur experiments assess SLMs across three dimen-\nsions: (1) network architecture and language mod-\neling assumptions, (2) evaluation metrics, speciﬁ-\ncally segmentation quality and language-modeling\nperformance, and (3) supervision setting (if and\nwhere gold segmentation data is available).\n4.1 Architecture and Modeling\nTo analyze the importance of the self-attention ar-\nchitecture versus the bidirectional conditioning con-\ntext, we test SLMs with three different encoders:\nthe standard R(ecurrent)SLM based on an LSTM,\nthe M(asked)SLM introduced in 3.2 with a seg-\nmental or “cloze” mask, and a D(irectional)MSLM,\nwith a “causal” or directional mask. The RSLM\nis thus (+recurrent context, +directional), the DM-\nSLM is (-recurrent context, +directional), and the\nMSLM is (-recurrent context, -directional).\nFigure 4: Directional MSLM\nFor all models, we use an LSTM for the segment\ndecoder, as a control and because the decoded se-\nquences are relatively short and may not beneﬁt\nas much from an attention model. See also Chen\net al. (2018) for hybrid models with transformer\nencoders and recurrent decoders.\n4.2 Evaluation Metrics\nPart of the motivation for SLMs is to create strong\nlanguage models that can also be used for segmen-\ntation (Kawakami et al., 2019). Because of this,\nwe report both segmentation quality and language\nmodeling performance.\nFor segmentation quality, we get the word-F1\nscore for each corpus using the script from the\nSIGHAN Bakeoff (Emerson, 2005). Following\nKawakami et al. (2019), we report this measure\nover the entire corpus. For language modeling per-\nformance, we report the average Bits Per Character\n(bpc) loss over the test set.\n4.3 Supervision Setting\nBecause previous studies have used SLMs both in\n“lightly supervised” settings (Sun and Deng, 2018)\nand totally unsupervised ones (Kawakami et al.,\n2019), and because we expect SLMs to be deployed\nin either use case, we test both. For all model types,\nwe conduct a hyperparameter sweep and select both\nthe conﬁguration that maximizes the validation seg-\nmentation quality (light supervision) and the one\nthat minimizes the validation bpc (unsupervised).\n4.4 Datasets\nWe evaluate our SLMs on two datasets used in\nKawakami et al. (2019). For each, we use the same\ntraining, validation, and test split. The sets were\nchosen to represent two relatively different writing\nsystems: Chinese (PKU) and English (PTB). Statis-\ntics for each are in Table 1. One striking difference\nbetween the two writing systems can be seen in the\ncharacter vocabulary size: phonemic-type writing\nsystems like English have a much smaller vocabu-\nlary of tokens, with words being built out of longer\nsequences of characters that are not meaningful on\ntheir own.\nCorpus PKU PTB\nTokens/Characters 1.93M 4.60M\nWords 1.21M 1.04M\nLines 20.78k 49.20k\nAvg. Characters per Word 1.59 4.44\nCharacter V ocabulary Size 4508 46\nTable 1: Statistics for the datasets\n43\nPeking University Corpus (PKU) PKU has\nbeen used as a Chinese Word Segmentation bench-\nmark since the International Chinese Word Seg-\nmentation Bakeoff (Emerson, 2005). One minor\nchange we make to this dataset is to tokenize En-\nglish, number, and punctuation tokens using the\nmodule from Sun and Deng (2018), to make our\nresults more comparable to theirs. Unlike them, we\ndo not pre-split sequences on punctuation.\nPenn Treebank (PTB) For English, we use\nthe version of the Penn Treebank corpus from\n(Kawakami et al., 2019; Mikolov et al., 2010).\n4.5 Parameters and Trials\nFor all models, we tune among six learning rates on\na single random seed. After the parameter sweep,\nthe conﬁguration that maximizes validation seg-\nmentation quality and the one that minimizes vali-\ndation bpc are run over an additional four random\nseeds. All models are trained using Adam (Kingma\nand Ba, 2015) for 8192 steps.\nAll models have one encoder layer and one de-\ncoder layer, as well as an embedding and hidden\nsize of 256. The transformer-based encoder has a\nnumber of trainable parameters less than or equal\nto the number in the LSTM-based encoder.1\nOne important parameter for SLMs is the max-\nimum segment length k. Sun and Deng (2018)\ntune this as a hyperparameter, with different val-\nues for kﬁtting different CWS standards more or\nless well. In practice, this parameter can be chosen\nempirically to be an upper bound on the maximum\nsegment length one expects to ﬁnd, so as to not\nrule out long segments. We follow Kawakami et al.\n(2019) in choosing k= 5for Chinese and k= 10\nfor English. For a more complete characterization\nof our training procedure, see Appendix A.2\n5 Results\n5.1 Chinese\nFor PKU (Table 2), Masked SLMs yield better seg-\nmentation quality in both the lightly-supervised\nand unsupervised settings, though the advantage\nin the former setting is much larger (+12.4 median\nF1). The Directional MSLM produces similar qual-\nity segmentations to the MSLM, but it has worse\nlanguage modeling performance in both settings\n1592,381 trainable parameters in the former, 592,640 in\nthe latter\n2The code used to build SLMs and conduct these experi-\nments can be found at (url redacted)\n(+0.23 bpc for lightly supervised and +0.11 bpc\nfor unsupervised); the RSLM produced the second-\nbest bpc in the unsupervised setting.\nThe RSLM gives the best bpc in the lightly-\nsupervised setting. However for this setting, the\nstrict division of the models that maximize segmen-\ntation quality and those that minimize bpc can be\nmisleading. In between these two conﬁgurations,\nmany have both good segmentation quality and low\nbpc, and if the practitioner has gold validation data,\nthey will be able to pick a conﬁguration with the\ndesired tradeoff.\nIn addition, there is some evidence that “under-\nshooting” the objective in the unsupervised case\nwith a slightly lower learning rate may lead to\nmore stable segmentation quality. The unsuper-\nvised MSLM in the table was trained at rate 2e-3,\nand achieved 5.625 bpc (validation). An MSLM\ntrained at rate 1e-3 achieved only a slightly worse\nbpc (5.631) and resulted in better and more stable\nsegmentation quality (69.4 ±2.0 / 70.4).\n5.2 English\nResults for English (PTB) can also be found in\nTable 2. By median, results remain comparable be-\ntween the recurrent and transformer-based models,\nbut the RSLM yields better segmentation perfor-\nmance in both settings (+4.0 and +4.7 F1). How-\never, both types of MSLM are slightly more sus-\nceptible to random seed variation, causing those\nmeans to be skewed slightly lower. The DMSLM\nseems more susceptible than the MSLM to outlier\nperformance based on random seeds, as evidenced\nby its large standard deviation. Finally, the RSLM\ngives considerably better bpc performance in both\nsettings (-0.29 and -0.31 bpc).\n6 Analysis and Discussion\n6.1 Error Analysis\nWe conduct an error analysis for our models based\non the overall Precision and Recall scores for each\n(using the character-wise binary classiﬁcation task,\ni.e. word-boundary vs no word-boundary).\nAs can be seen in Table 3, all model types trained\non Chinese have a Precision that approaches 100%,\nmeaning almost all boundaries that are inserted\nare true boundaries. On ﬁrst glance the main dif-\nference in the unsupervised case seems to be the\nRSLM’s relatively higher Recall. However, the\nhigher Precision of both MSLM types seems to\nbe more important for the overall segmentation\n44\nDataset Model Tuned on Gold Unsupervised\nF1 Mean / Median BPC F1 Mean / Median BPC\nPKU\nRSLM 61.2 ±3.6 / 60.2 5.67 ±0.01 59.4 ±1.9 / 58.7 5.63 ±0.01\nDMSLM 72.2 ±2.0 / 72.7 6.08 ±0.31 62.9 ±2.6 / 63.4 5.67 ±0.03\nMSLM 72.3 ±0.7 / 72.6 5.85 ±0.12 62.9 ±2.8 / 64.1 5.56 ±0.01\nPTB\nRSLM 77.4 ±0.7 / 77.6 2.10 ±0.04 75.7 ±2.6 / 76.2 1.96 ±0.00\nDMSLM 70.6 ±6.4 / 73.3 2.36 ±0.07 67.9 ±10.6 / 73.8 2.27 ±0.04\nMSLM 71.1 ±5.6 / 73.6 2.39 ±0.06 69.3 ±5.6 / 71.5 2.27 ±0.01\nTable 2: Results on the Peking University Corpus and English Penn Treebank (over 5 random seeds)\nperformance.3 In the lightly-supervised case, the\nMSLM variants learn to trade off a small amount\nof Precision for a large gain in Recall, allowing\nthem to capture more of the true word boundaries\nin the data. Given different corpora have different\nstandards for the coarseness of Chinese segmenta-\ntion, this reinforces the need for studies on a wider\nselection of datasets.\nBecause the English results (also in Table 3)\nare similar between supervision settings, we only\nshow the unsupervised variants. Here, the RSLM\nshows a deﬁnitive advantage in Recall, leading to\noverall better performance. The transformer-based\nmodels show equal or higher Precision, but tend to\nunder-segment, i.e. produce longer words. Exam-\nple model segmentations for PTB can be found in\nTable 4. Some intuitions from our error analysis\ncan be seen here: the moderate Precision of these\nmodels yields some false splits like be + fore\nand quest + ion, but all models also seem to\npick up some valid morphological splits not present\nin the gold standard (e.g. +able in questionable).\nPredictably, rare words with uncommon structure\nremain difﬁcult to segment (e.g. asbestos).\n6.2 Discussion\nFor Chinese, the transformer-based SLM exceeds\nthe recurrent baseline for segmentation quality, by a\nmoderate amount for the unsupervised setting, and\nby a large amount when tuned on gold validation\nsegmentations. The MSLM also gives stronger\nlanguage modeling. Given the large vocabulary\nsize for Chinese, it is intuitive that the powerful\ntransformer architecture may make a difference\n3This table also shows that though character-wise segmen-\ntation quality (i.e. classifying whether a certain character has\na boundary after it) is a useful heuristic, it does not always\nscale straightforwardly to word-wise F1 like is traditionally\nused (e.g. by the SIGHAN script).\nin this difﬁcult language-modeling task. Further,\nthough the DMSLM achieves similar segmentation\nquality, the bidirectional context of the MSLM does\nseem to be the source of the best bpc modeling\nperformance.\nIn English, on the other hand, recurrent SLMs\nseem to retain a slight edge. By median, segmen-\ntation quality remains fairly similar between the\nthree model types, but the RSLM holds a major\nlanguage-modeling advantage in our experiments.\nOur main hypothesis for the disparity in modeling\nperformance between Chinese and English comes\ndown to the nature of the orthography for each. As\nnoted before, Chinese has a much larger charac-\nter vocabulary. This is because in Chinese, almost\nevery character is a morpheme itself (i.e. it has\nsome meaning). English, on the other hand, has a\nroughly phonemic writing system, e.g. the letter c\nhas no inherent meaning outside of a context like\ncat.\nIntuitively, one can see why this might pose a\nlimitation on transformers. Without additive or\nlearned positional encodings, they are essentially\nadirectional. In English, cat is completely differ-\nent from act, but this might be difﬁcult to model\nfor an attention model without robust positional\ninformation. To try to counteract this, we added\ndynamic scaling to our static positional encodings,\nbut without deeper networks or more robust po-\nsitional information, the discrepancy in character-\nbased modeling for phonemic systems may remain.\n7 Conclusion\nThis study provides strong proof-of-concept for\nthe viability of transfomer-based Masked Segmen-\ntal Language Models as an alternative to recurrent\nSLMs in their ability to perform joint language\nmodeling and unsupervised segmentation. MSLMs\n45\nDataset Model Avg. Word Length Precision Recall\nPKU\nGold 1.59 - -\nRSLM (unsup.) 1.93 ±0.02 98.2 ±0.1 80.8 ±0.6\nDMSLM (unsup.) 1.99 ±0.04 98.6 ±0.1 78.5 ±1.8\nMSLM (unsup.) 2.00 ±0.05 98.5 ±0.1 78.1 ±1.9\nRSLM (sup.) 1.92 ±0.02 98.2 ±0.1 81.3 ±0.7\nDMSLM (sup.) 1.83 ±0.04 97.5 ±0.5 84.6 ±1.5\nMSLM (sup.) 1.83 ±0.01 97.6 ±0.1 84.5 ±0.4\nPTB\nGold 4.44 - -\nRSLM (unsup.) 4.02 ±0.08 86.1 ±1.9 95.5 ±0.1\nDMSLM (unsup.) 4.27 ±0.17 85.4 ±5.4 88.9 ±4.6\nMSLM (unsup.) 4.29 ±0.12 86.2 ±1.5 89.5 ±3.5\nTable 3: Error analysis statistics (over 5 random seeds)\nExamples\nGold we ’re talking about years ago before anyone heard of asbestos having any questionable...\nRSLM Median we’retalking about years ago be fore any oneheard of as best oshaving any question able\nDMSLM Median we’retalking about years ago be fore any oneheard of as bestoshaving any quest ion able\nMSLM Median we’retalking about years ago be fore any oneheard of as bestoshaving any quest ion able\nTable 4: Example model segmentations from the Penn Treebank\nprovide the advantage of a parallelizable architec-\nture, and have several open avenues for extending\ntheir utility. To close, we lay out directions for\nfuture work.\nThe most obvious next step is evaluating\nMSLMs on additional segmentation datasets. As\nmentioned, the criteria for “wordhood” in Chi-\nnese are not agreed upon, thus more experiments\nare warranted using corpora with different stan-\ndards. Prime candidates include the Chinese Penn\nTreebank (Xue et al., 2005), as well as those in-\ncluded in the SIGHAN segmentation bakeoff: Mi-\ncrosoft Research, City University of Hong Kong,\nand Academia Sinicia (Emerson, 2005).\nThe sets used here are also relatively formal\northographic datasets. An eventual use of SLMs\nmay be in speech segmentation, but a smaller step\nin that direction could be using phonemic tran-\nscript datasets like the Brent Corpus, also used in\nKawakami et al. (2019). This set consists of phone-\nmic transcripts of child-directed English speech\n(Brent, 1999). SLMs could also be applied to the\northographies of more typologically diverse lan-\nguages, especially ones with complicated systems\nof morphology (e.g. Swahili, Turkish, Hungarian,\nFinnish).\nFurther, though we only test shallow models\nhere, one of the main advantages of transformers\nis their ability to scale to deep architectures due to\ntheir short derivational chains. Thus, extending seg-\nmental models to “deep” settings would be more\nfeasible using MSLMs than RSLMs.\nLastly, Kawakami et al. (2019) propose regular-\nization techniques for SLMs due to low segmen-\ntation quality from their “vanilla” models. They\nreport good ﬁndings using a character n-gram “lex-\nicon” jointly with expected segment length regu-\nlarization based on Eisner (2002) and Liang and\nKlein (2009). Both techniques are implemented\nin our codebase, and we have tested them in pilot\nsettings. Oddly, neither has given us any gain in\nperformance over our “vanilla” models. A more ex-\nhaustive hyperparameter search with these methods\nmay produce a future beneﬁts as well.\nIn conclusion, the present study shows strong\npotential for the use of MSLMs. They show par-\nticular promise for writing systems with a large in-\nventory of semantic characters (e.g. Chinese), and\nwe believe that they could be stable competitors of\nrecurrent models in phonemic-type writing systems\ngiven some mitigation of the relative weakness of\nthe positional information available in transform-\ners.\n46\nReferences\nMichael R. Brent. 1999. An Efﬁcient, Probabilistically\nSound Algorithm for Segmentation and Word Dis-\ncovery. Machine Learning, 34:71–105.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The Best of Both Worlds: Combining Recent\nAdvances in Neural Machine Translation. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 76–86, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nJ. Chung, Sungjin Ahn, and Yoshua Bengio. 2017.\nHierarchical Multiscale Recurrent Neural Networks.\nIn 5th International Conference on Learning Repre-\nsentations, ICLR 2017, Conference Track Proceed-\nings, Toulon, France.\nMathias Creutz and Krista Lagus. 2002. Unsupervised\nDiscovery of Morphemes. In Proceedings of the\nACL-02 Workshop on Morphological and Phonolog-\nical Learning, pages 21–30. Association for Compu-\ntational Linguistics.\nCarl de Marcken. 1996. Linguistic Structure as Com-\nposition and Perturbation. In 34th Annual Meet-\ning of the Association for Computational Linguistics,\npages 335–341, Santa Cruz, California, USA. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEwan Dunbar, Julien Karadayi, Mathieu Bernard,\nXuan-Nga Cao, Robin Algayres, Lucas Ondel,\nLaurent Besacier, Sakriani Sakti, and Emmanuel\nDupoux. 2020. The Zero Resource Speech Chal-\nlenge 2020: Discovering discrete subword and word\nunits. In Proceedings of INTERSPEECH 2020.\nJason Eisner. 2002. Parameter Estimation for Prob-\nabilistic Finite-State Transducers. In Proceedings\nof the 40th Annual Meeting of the Association for\nComputational Linguistics, pages 1–8, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nThomas Emerson. 2005. The Second International Chi-\nnese Word Segmentation Bakeoff. InProceedings of\nthe Fourth SIGHAN Workshop on Chinese Language\nProcessing.\nJohn Goldsmith. 2001. Unsupervised Learning of the\nMorphology of a Natural Language. Computational\nLinguistics, 27(2):153–198.\nSharon Goldwater, Thomas L. Grifﬁths, and Mark\nJohnson. 2009. A Bayesian framework for word seg-\nmentation: Exploring the effects of context. Cogni-\ntion, 112(1):21–54.\nAlex Graves, Fernández Santiago, Faustino Gomez,\nand Jürgen Schmidhuber. 2006. Connectionist Tem-\nporal Classiﬁcation: Labelling Unsegmented Se-\nquence Data with Recurrent Neural Networks. In\nProceedings of the 23rd International Conference on\nMachine Learning, Pittsburgh, PA.\nAlex Graves and Jürgen Schmidhuber. 2005. Frame-\nwise phoneme classiﬁcation with bidirectional\nLSTM and other neural network architectures. In\nNeural Networks, volume 18, pages 602–610. Perg-\namon.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The State and\nFate of Linguistic Diversity and Inclusion in the\nNLP World. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 6282–6293, Online. Association for\nComputational Linguistics.\nHerman Kamper, Aren Jansen, and Sharon Goldwa-\nter. 2016. Unsupervised word segmentation and\nlexicon discovery using acoustic word embeddings.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing, 24(4):669–679.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to Create and Reuse Words in Open-\nV ocabulary Neural Language Modeling. InProceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1492–1502, Vancouver, Canada. Asso-\nciation for Computational Linguistics.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2019. Learning to Discover, Ground and Use Words\nwith Segmental Neural Language Models. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6429–\n6441, Florence, Italy. Association for Computa-\ntional Linguistics.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, Conference Track Proceedings , San\nDiego, CA, USA.\nLingpeng Kong, Chris Dyer, and Noah A Smith. 2016.\nSegmental Recurrent Neural Networks. In 4th Inter-\nnational Conference on Learning Representations,\nICLR 2016, Conference Track Proceedings , San\nJuan, Puerto Rico.\n47\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7871–7880, Online. As-\nsociation for Computational Linguistics.\nPercy Liang and Dan Klein. 2009. Online EM for\nUnsupervised Models. In Proceedings of Human\nLanguage Technologies: The 2009 Annual Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics , pages 611–619,\nBoulder, Colorado. Association for Computational\nLinguistics.\nPaul A. Luce. 1986. A computational analysis of\nuniqueness points in auditory word recognition. Per-\nception & Psychophysics, 39(3):155–158.\nSabrina Mielke and Jason Eisner. 2019. Spell\nOnce, Summon Anywhere: A Two-Level Open-\nV ocabulary Language Model. Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence ,\n33(01):6843–6850. Number: 01.\nTomas Mikolov, Kai Chen, G. Corrado, and J. Dean.\n2013. Efﬁcient Estimation of Word Representations\nin Vector Space. In 1st International Conference\non Learning Representations, ICLR 2013, Workshop\nTrack Proceedings, Scottsdale, AR, USA.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernocký, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. volume 2,\npages 1045–1048.\nDaichi Mochihashi, Takeshi Yamada, and Naonori\nUeda. 2009. Bayesian Unsupervised Word Segmen-\ntation with Nested Pitman-Yor Language Modeling.\nIn Proceedings of the Joint Conference of the 47th\nAnnual Meeting of the ACL and the 4th International\nJoint Conference on Natural Language Processing\nof the AFNLP , pages 100–108, Suntec, Singapore.\nAssociation for Computational Linguistics.\nTumi Moeng, Sheldon Reay, Aaron Daniels, and Jan\nBuys. 2021. Canonical and Surface Morphological\nSegmentation for Nguni Languages. ArXiv.\nHyunji Hayley Park, Katherine J. Zhang, Coleman Ha-\nley, Kenneth Steimel, Han Liu, and Lane Schwartz.\n2021. Morphology Matters: A Multilingual Lan-\nguage Modeling Analysis. Transactions of the\nAssociation for Computational Linguistics , 9:261–\n276. _eprint: https://direct.mit.edu/tacl/article-\npdf/doi/10.1162/tacl_a_00365/1924158/tacl_a_00365.pdf.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nO. Rasanen and M. A. Cruz Blandon. 2020. Unsuper-\nvised Discovery of Recurring Speech Patterns using\nProbabilistic Adaptive Metrics. In Proceedings of\nINTERSPEECH 2020.\nJorma Rissanen. 1989. Stochastic Complexity in Statis-\ntical Inquiry, volume 15 of Series in Computer Sci-\nence. World Scientiﬁc, Singapore.\nMike Schuster and Kuldip K. Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE Transactions\non Signal Processing, 45(11):2673–2681.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nJoongbo Shin, Yoonhyung Lee, Seunghyun Yoon, and\nKyomin Jung. 2020. Fast and Accurate Deep Bidi-\nrectional Language Representations for Unsuper-\nvised Learning. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 823–835, Online. Association for\nComputational Linguistics.\nK. Song, X. Tan, Tao Qin, Jianfeng Lu, and T. Liu.\n2019. MASS: Masked Sequence to Sequence Pre-\ntraining for Language Generation. In Proceedings\nof the 36th International Conference on Machine\nLearning, Long Beach, CA.\nZhiqing Sun and Zhi-Hong Deng. 2018. Unsupervised\nNeural Word Segmentation for Chinese via Segmen-\ntal Language Modeling. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4915–4920, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYee Whye Teh, Michael I. Jordan, Matthew J. Beal,\nand David M. Blei. 2006. Hierarchical Dirichlet Pro-\ncesses. Journal of the American Statistical Associa-\ntion, 101(476):1566–1581.\nYuanhe Tian, Yan Song, Fei Xia, Tong Zhang, and\nYonggang Wang. 2020. Improving Chinese Word\nSegmentation with Wordhood Memory Networks.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n48\n8274–8285, Online. Association for Computational\nLinguistics.\nKei Uchiumi, Hiroshi Tsukahara, and Daichi Mochi-\nhashi. 2015. Inducing Word and Part-of-Speech\nwith Pitman-Yor Hidden Semi-Markov Models. In\nProceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1774–1782, Beijing, China. Association for Compu-\ntational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Ilia Polosukhin. 2017. Attention is All\nYou Need. In Proceedings of the 31st Conference\non Neural Information Processing Systems , Long\nBeach, CA. Neural Information Processing Systems\nFoundation.\nChong Wang, Yining Wang, Po-Sen Huang, Abdel-\nrahman Mohamed, Dengyong Zhou, and Li Deng.\n2017. Sequence Modeling via Segmentations. In\nProceedings of the 34th International Conference\non Machine Learning , volume 70 of Proceedings\nof Machine Learning Research , pages 3674–3683,\nInternational Convention Centre, Sydney, Australia.\nPMLR.\nL. Wang, Zongyi Li, and Xiaoqing Zheng. 2021. Un-\nsupervised Word Segmentation with Bi-directional\nNeural Language Model. ArXiv.\nNianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha\nPalmer. 2005. The Penn Chinese TreeBank: Phrase\nStructure Annotation of a Large Corpus. Natural\nLanguage Engineering, 11(2):207–238.\n49\nA Training Details\nA.1 Data\nThe datasets used here are sourced from\nKawakami et al. (2019), and can be downloaded\nat https://s3.eu-west-2.amazonaws.\ncom/k-kawakami/seg.zip. Our PKU data\nis tokenized slightly differently, and all data used\nin our experiments can be found in our project\nrepository (url redacted).\nA.2 Architecture\nA dropout rate of 0.1 is applied leading into both\nthe encoder and the decoder. Transformers use 4\nattention heads and a feedforward size of 509 (cho-\nsen to come out less than or equal to the number\nof parameters in the standard LSTM). This also\nincludes a 512-parameter linear mapping to learn\nthe combination proportion of the word and sinu-\nsoidal positional embeddings. The dropout within\ntransformer layers is 0.15.\nA.3 Initialization\nCharacter embeddings are initialized using CBOW\n(Mikolov et al., 2013) on the given training set for\n32 epochs, with a window size of 5 for Chinese and\n10 for English. Special tokens like <eoseg> that\ndo not appear in the training corpus are randomly\ninitialized. These pre-trained embeddings are not\nfrozen during training.\nA.4 Training\nFor PKU, the learning rates swept are {6e-4, 7e-4,\n8e-4, 9e-4, 1e-3, 2e-3}, and for PTB we use {6e-\n4, 8e-4, 1e-3, 3e-3, 5e-3, 7e-3}. For Chinese, we\nfound a linear warmup for 1024 steps was useful,\nfollowed by a linear decay. For English, we apply\nsimple linear decay from the beginning. Check-\npoints are taken every 128 steps. A gradient norm\nclip threshold of 1.0 is used. Mini-batches are sized\nby number of characters rather than number of se-\nquences, with a size of 8192 (though this is not\nalways exact since we do not split up sequences).\nThe ﬁve random seeds used are {2, 3, 5, 8, 13}.\nEach model is trained on an Nvidia Tesla M10\nGPU with 8GB memory, with the average per-batch\nruntime of each model type listed in Table 5.\nA.5 Optimal Hyperparameters\nThe optimal learning rate for each model type,\ndataset, and supervision setting are listed in the\nTable 6. Parameters are listed by the validation\nModel s / step\nPKU PTB\nRSLM 2.942 2.177\nDMSLM 2.987 2.190\nMSLM 2.988 2.200\nTable 5: Average runtime per batch in seconds\nobjective they optimize: segmentation MCC or\nlanguage-modeling BPC.\nDataset Model by MCC by BPC\nPKU\nRSLM 6e-4 9e-4\nDMSLM 6e-4 2e-3\nMSLM 6e-4 2e-3\nPTB\nRSLM 7e-3 3e-3\nDMSLM 1e-3 8e-4\nMSLM 1e-3 6e-4\nTable 6: Optimum learning rates\n50",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8595841526985168
    },
    {
      "name": "Segmentation",
      "score": 0.703294575214386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6879021525382996
    },
    {
      "name": "Language model",
      "score": 0.6354559659957886
    },
    {
      "name": "Scalability",
      "score": 0.6113131046295166
    },
    {
      "name": "Transformer",
      "score": 0.5581160187721252
    },
    {
      "name": "Natural language processing",
      "score": 0.5566104650497437
    },
    {
      "name": "Natural language",
      "score": 0.5001909732818604
    },
    {
      "name": "Market segmentation",
      "score": 0.4446987807750702
    },
    {
      "name": "Speech recognition",
      "score": 0.33628466725349426
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 5
}