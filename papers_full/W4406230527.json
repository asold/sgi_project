{
    "title": "Evaluation of LLMs accuracy and consistency in the registered dietitian exam through prompt engineering and knowledge retrieval",
    "url": "https://openalex.org/W4406230527",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2168097569",
            "name": "Iman Azimi",
            "affiliations": [
                "DigiLens (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2999166040",
            "name": "Mo-Han Qi",
            "affiliations": [
                "DigiLens (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2017065509",
            "name": "Li Wang",
            "affiliations": [
                "DigiLens (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2123772918",
            "name": "Amir M Rahmani",
            "affiliations": [
                "University of California, Irvine",
                "Irvine University"
            ]
        },
        {
            "id": "https://openalex.org/A2102462568",
            "name": "Youlin Li",
            "affiliations": [
                "DigiLens (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4381805620",
        "https://openalex.org/W4389173934",
        "https://openalex.org/W4394865839",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4387583347",
        "https://openalex.org/W4384945770",
        "https://openalex.org/W4399452994",
        "https://openalex.org/W4377988719",
        "https://openalex.org/W4386781106",
        "https://openalex.org/W4393054226",
        "https://openalex.org/W4405489687",
        "https://openalex.org/W4389579018",
        "https://openalex.org/W4387047777",
        "https://openalex.org/W4386381115",
        "https://openalex.org/W4403019432",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4393335480",
        "https://openalex.org/W4387412164",
        "https://openalex.org/W4388592961",
        "https://openalex.org/W4390275274",
        "https://openalex.org/W4393327656",
        "https://openalex.org/W4391564557",
        "https://openalex.org/W4388453260",
        "https://openalex.org/W4396833396",
        "https://openalex.org/W4379278559",
        "https://openalex.org/W4401056155",
        "https://openalex.org/W4384484700",
        "https://openalex.org/W4391971084",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W6600882715",
        "https://openalex.org/W4399807068",
        "https://openalex.org/W4402860127",
        "https://openalex.org/W2749420742",
        "https://openalex.org/W4388157511",
        "https://openalex.org/W4392151399",
        "https://openalex.org/W4392045825",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W6601375367",
        "https://openalex.org/W4391640544",
        "https://openalex.org/W4402407635",
        "https://openalex.org/W4381930847",
        "https://openalex.org/W4368340908",
        "https://openalex.org/W2155243985",
        "https://openalex.org/W2053154970",
        "https://openalex.org/W1975879668",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W2613482391"
    ],
    "abstract": "Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient. In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries. Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels. In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses. Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains. GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency. For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency. RAP was particularly effective for GPT-4o to answer Expert level questions. Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots.",
    "full_text": "Evaluation of LLMs accuracy and \nconsistency in the registered \ndietitian exam through prompt \nengineering and knowledge \nretrieval\nIman Azimi1, Mohan Qi1, Li Wang2, Amir M. Rahmani3 & Youlin Li1\nLarge language models (LLMs) are fundamentally transforming human-facing applications in the \nhealth and well-being domains: boosting patient engagement, accelerating clinical decision-making, \nand facilitating medical education. Although state-of-the-art LLMs have shown superior performance \nin several conversational applications, evaluations within nutrition and diet applications are still \ninsufficient. In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a \nstandard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and \nGemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries. Our evaluation includes \n1050 RD exam questions encompassing several nutrition topics and proficiency levels. In addition, for \nthe first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought \nwith Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and \nconsistency of the responses. Our findings revealed that while these LLMs obtained acceptable \noverall performance, their results varied considerably with different prompts and question domains. \nGPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS \nrecorded the highest consistency. For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC \nimproved both accuracy and consistency. RAP was particularly effective for GPT-4o to answer Expert \nlevel questions. Consequently, choosing the appropriate LLM and prompting technique, tailored to \nthe proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition \nchatbots.\nKeywords Large Language Models, Registered Dietitian, Nutrition, Prompt Engineering, Knowledge \nRetrieval\nThere is growing interest in leveraging conversational models, commonly known as chatbots, in healthcare, \nparticularly in the areas of diet and nutrition 1–3. The rise of large language models (LLMs) is significantly \ntransforming human-machine interactions in this context, creating new opportunities for nutrition management \napplications and lifestyle enhancement that involve natural language understanding and generation 4–6. These \nchatbots can serve as assistants to health providers (e.g., dietitian or nurses) or as ubiquitous companions for \npatients, providing preventive care, personalized meal planning, and chronic disease management7.\nSince the release of ChatGPT8in November 2022, numerous nutrition management studies have developed \nor employed LLM-based chatbots to target different health conditions, such as type 2 diabetes, obesity, liver \ndiseases, kidney diseases, and cardiovascular diseases, to mention a few 1,7,9–16. These studies highlight the \npotential of chatbots interventions to enhance diet and promote lifestyle behavior changes.\nDue to the life-critical nature of these applications, they must provide high quality attributes, such as accuracy, \nconsistency, safety, and fairness, before being deployed in real-world settings for end-users17–19. Recent studies \nhave evaluated the LLM-based chatbots within nutritional and dietary contexts. For example, Sun et al.20 and \nBarlas et al.21assessed the performance of ChatGPT in providing nutritional management support for diabetic \n1Department of Engineering, iHealth Labs, Sunnyvale, CA 94085, United States. 2Department of Clinical Research, \niHealth Labs, Sunnyvale, CA 94085, United States. 3School of Nursing and Department of Computer Science, \nUniversity of California Irvine, Irvine, CA 92697, United States. email: iman.azimi@ihealthlabs.com\nOPEN\nScientific Reports |         (2025) 15:1506 1| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports\n\npatients. Other investigations focused on chatbots’ reliability in delivering accurate calorie and macronutrient \ninformation22,23. For non-communicable diseases, the accuracy of dietary advice generated by ChatGPT’s were \nassessed10,24. Other studies also examined ChatGPT’s ability to address common nutrition-related inquiries, \nhighlighting its strength and weakness in offering personalized and accurate nutritional information 25,26. \nHowever, the existing evaluation studies on nutrition-related chatbots face three major challenges.\nFirst, prior research on the LLMs application in nutrition has relied solely on ad-hoc or subjective evaluations. \nIn these studies, domain experts designed a set of questions focused on specific diseases or nutrition topics. \nSubsequently, human evaluators were instructed to grade the responses in terms of accuracy, comprehensiveness, \nor attractiveness20,21,27. Human-in-the-loop evaluation is widely recognized as a popular and well-established \nstrategy for assessing chatbots in the literature18,19. However, these evaluations are not comprehensive regarding \nnutrition problems and are prone to human errors or biases, as they depend on the opinion of an individual \nexpert, especially when no standard guidelines are followed in the evaluation process. Additionally, they are \ntime-consuming and costly. This limitation can be observed in the current nutrition chatbots evaluation, as their \nassessments are restricted to a few hundred interactions (i.e., prompts) at most.\nSecond, most of the nutrition and diet studies have focused only on ChatGPT-3.5 or ChatGPT-47,11,24,27. The \nlandscape of LLMs is rapidly evolving. New models and techniques are being released frequently, within weeks or \nmonths28. This rapid advancement requires the evaluation of a wide range of models to ensure the best possible \nsolutions for diet and nutrition management applications. In addition, existing research on nutrition evaluation \nhas ignored the impact of prompt engineering techniques20,24,29. They have been limited to zero-shot prompting \nmethods with either no instructions or fixed instructions. The zero-shot prompting instructs LLMs to perform \nspecific tasks without providing any prior examples. This technique is straightforward and is widely used. \nHowever, it might be insufficient for LLM response generation if problem-solving or contextual information is \nneeded. Chain of thought and chain of thought with self-consistency prompting techniques have shown their \npotential to enhance the performance of LLMs in multiple non-nutrition studies by enabling chatbots to address \ncomplex reasoning tasks 30–32. Retrieval augmented prompting models have also indicated their effectiveness \nin mitigating LLMs hallucination problems across generic scenarios. We hypothesize that these step-by-step \nreasoning techniques and retrieval models can surpass zero-shot prompting techniques, especially in breaking \ndown complex nutritional questions, handling uncertainties by providing external information, and enabling \nbetter decision-making33–36. It is essential to investigate the impact of these prompting techniques on LLMs \nperformance in handling various nutrition-related queries.\nThird, previous work merely focused on the overall accuracy of LLMs responses. Their findings indicated \nthat the models were generally accurate, but they still had errors 10,21,24,27. These studies did not examine the \nerrors, along with the strategies to enhance the LLMs’ responses. Wang et al.31highlights this issue in the context \nof clinical medicine. Moreover, the non-deterministic behavior of chatbots was ignored 37. The consistency \nand reliability of chatbots in answering nutrition-related questions must be evaluated to determine if their \nperformance varies with identical or different prompts. In the nutrition context, to the best of our knowledge, \nonly one study 22 has explored the consistency of ChatGPT-3.5 and ChatGPT-4 responses, using a zero-shot \nprompt for 222 food items across five repeated measurements.\nIn this paper, we thoroughly evaluate the accuracy and consistency of three leading LLMs chatbots, i.e., \nGPT-4o38, Claude 3.5 Sonnet39, and Gemini 1.5 Pro40, in addressing nutrition-related inquiries. To achieve this, \nwe leverage the Registered Dietitian (RD) exam41 for the first time, as a standard certification examination that \nserves to assess whether dietitians meet the qualifications required to practice in the dietetics and nutrition field. \nOur evaluation includes 1050 multiple-choice questions with different proficiency levels, covering four nutrition \ndomains: i.e., principles of dietetics, nutrition care, food service systems, and food and nutrition management. \nTo investigate the impact of prompts, the questions are presented to the LLMs using four different prompting \ntechniques. We then compare the responses with the ground truth answers, enabling an objective assessment of \nthe model’s performance. To examine the consistency of the responses, we perform repeated measurements by \nasking each model the same set of questions multiple times using each prompting technique. The responses for \neach technique and model are compared within and across groups.\nResults\nAccuracy\nOverall performance\nThe results show that all the approaches obtained a score of over 88% in selecting the correct option for the 1050 \nRD exam questions, as indicated in Figure 1 and Table 1. Table 2 also summarizes the error counts for each \nprompting technique across the three chatbots. Overall, GPT-4o achieved the highest score (the blue markers in \nthe figure) ranging between 91% and 95%, with the best score for CoT-SC. GPT-4o with CoT-SC obtained the \nlowest error count: i.e., 58 errors on average on the 1050 RD exam questions. On the other hand, Gemini 1.5 Pro \n(the green markers in Figure 1) had the lowest scores. The highest error count was for Gemini 1.5 Pro with CoT.\nIn both GPT-4o and Claude 3.5 Sonnet, the CoT and CoT-SC prompting techniques resulted in similar \npercentage scores, which were approximately 2.5 percent higher than the ZS prompting’s scores. The error \ncount of GPT-4o with ZS was approximately 25 higher than that observed for CoT and CoT-SC. However, the \ncombination of Gemini with CoT or CoT-SC did not improve the accuracy but produced wider percentage \nscores across repeated measurements, with ranges of 1.9 and 1.2. The error count of Gemini with ZS was \napproximately 9 and 18 fewer than that observed for CoT and CoT-SC. Moreover, RAP obtained better scores \n(less error counts), compared to ZS, in GPT-4o; however it slightly decreased the performance of Claude and \nGemini models.\nScientific Reports |         (2025) 15:1506 2| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nSubgroup error analysis\nWe categorize the RD exam questions into different subgroups, within which the LLMs’ inaccurate responses \nare assessed. To achieve this, we analyze the errors obtained in terms of proficiency levels and four nutrition \ndomains (i.e., topics).\nProficiency Levels:  The approaches are evaluated based on the questions’ proficiency levels, provided by \nthe Academy of Nutrition and Dietetics, eatrightPREP for the RDN Exam42. The exam consists of 149 Easy, 352 \nModerate, 392 Difficult, and 149 Expert levels questions. Figure 2a shows the average errors for each approach. \nIn addition, the mean and standard deviation of the error counts of the approaches per proficiency level are \nindicated in Supplementary Table S.1.\nGPT-4o obtained the lowest overall average error counts. The model with CoT-SC resulted in the fewest errors \nacross the proficiency levels, with the average errors of 0.6, 10.6, 22.4, and 24.4 for Easy, Moderate, Difficult, \nand Expert levels questions, respectively. Compared to ZS prompting, CoT and CoT-SC improved the model’s \nperformance at all levels, but RAP only enhanced the responses of the Difficult and Expert level questions.\nSimilar to the GPT-4o approaches, Claude  3.5 Sonnet performance was enhanced by CoT and CoT-SC. \nClaude 3.5 Sonnet with CoT and CoT-SC achieved similar average error rates. Conversely, using Claude 3.5 \nSonnet, RAP recorded the highest error counts, particularly with 5 more errors (on average) for Expert questions, \ncompared to the ZS prompting technique.\nGemini 1.5 Pro had the highest number of errors overall. The ZS prompting recorded the lowest average \nerrors with Gemini. Compared to ZS, CoT and CoT-SC improved the responses of the Moderate questions but \nBenchmark Prompt GPT-4o Claude 3.5 S. Gemini 1.5 P .\nRD Exam\nZero Shot 84.8 (2.93) 104.6 (1.02) 96.8 (1.17)\nChain of Thought 59.6 (1.85) 80.6 (2.87) 117.4 (6.62)\nChain of Thought w. Self Consistency 58.0 (2.28) 77.0 (1.67) 104.8 (4.12)\nRetrieval Augmented Prompting 75.8 (2.86) 113.2 (1.94) 108.6 (1.20)\nTable 2. The error counts (mean and standard deviation) of the LLMs’ responses on the 1050 RD exam \nquestions.\n \nBenchmark Prompt GPT-4o Claude 3.5 S. Gemini 1.5 P .\nRD Exam\nZero Shot 91.92% (0.28) 90.04% (0.10) 90.78% (0.11)\nChain of Thought 94.32% (0.18) 92.32% (0.27) 88.82% (0.63)\nChain of Thought w. Self Consistency 94.48% (0.22) 92.67% (0.16) 90.02% (0.39)\nRetrieval Augmented Prompting 92.78% (0.27) 89.22% (0.18) 89.66% (0.11)\nTable 1. The percentage scores (mean and standard deviation) of the LLMs’ responses on the RD exam \nquestions.\n \nFig. 1. Percentage Scores of the approaches on the RD exam. GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro \nare indicated with blue, orange, and green markers, respectively. The Zero Shot (ZS), Chain of Thought (CoT), \nChain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) techniques are \nindicated with circle, square, triangle, and star markers, respectively.\n \nScientific Reports |         (2025) 15:1506 3| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nobtained higher average errors for the Difficult and Expert level questions. RAP obtained higher error rates for \nModerate and Difficult questions.\nDomains: The inaccurate responses collected by each approach is evaluated based on four domains: D1) \nPrinciples of Dietetics , D2) Nutrition Care for Individuals and Groups , D3) Food Service Systems , and D4) \nManagement of Food and Nutrition Programs and Service. The exam consists of 237, 392, 185, and 236 questions \nfor D1, D2, D3, and D4, respectively. As illustrated in Figure 2b, the impact of prompt engineering techniques \nvaried across the domains for the three LLMs. The mean and standard deviation of the error counts of the \napproaches per domain are indicated in Supplementary Table S.2.\nGPT-4o with CoT-SC reduced the average error counts in D3 from 27.4 to 12 and in D4 from 28.4 to 18.2, \ncompared to GPT-4o with ZS. CoT and RAP also showed similar improvements in error rates although RAP \nrecorded more errors for D2. Using GPT-4o, different prompting techniques resulted in small changes in the \nerror rates observed in D1.\nClaude 3.5 Sonnet showed that transitioning from ZS prompting to CoT-SC or CoT reduced the average \nerrors across the four domains. On the other hand, RAP slightly improved D1 and D2 but obtained more errors \nin D3 and D4.\nWith Gemini 1.5 Pro, different prompts led to small variations in error counts, with changes of fewer than \n4 errors on average in D1, D3, and D4. However, ZS prompting obtained the lowest error count in D2, with an \naverage of 26.2 errors. Nevertheless, this outcome shows approximately 6 errors higher than the performance \nachieved by GPT-4o. In D2, Gemini and CoT obtained the highest error rates.\nConsistency\nInter-rater analysis\nThe inter-rater reliability of the responses from the approaches was analyzed to investigate their agreement. \nTo achieve this, Cohen’s Kappa coefficient was calculated for each pair of approaches to determine if they \nselected the same choices, whether accurate or inaccurate. Our study includes 12 distinct approaches (3 LLMs \nmultiplied by 4 prompting techniques), so the tests were performed for each of the 12 pairwise comparisons. \nSince each approach is repeated five times, one set of measurements per approach is randomly selected to \nassess the inter-rater reliability. Figure 3presents the Cohen’s Kappa coefficients, where dark blue indicates high \nlevels of agreement, and light blue represents lower agreement levels. Additionally, we utilized the McNemar-\nBowker test43 to determine if each pair of responses are statistically different. Table 3 indicates the test results \nfor responses collected from the three chatbots with different prompts. The detailed statistical data for the \nFig. 2. The LLMs’ inaccurate responses based on the RD exam questions’ proficiency levels and domains.\n \nScientific Reports |         (2025) 15:1506 4| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nLLMs with different prompts Test Statistic P value\nGPT-4o, ZS vs CoT 7.20 0.303\nGPT-4o, ZS vs CoT-SC 6.83 0.337\nGPT-4o, ZS vs RAP 9.20 0.162\nGPT-4o, CoT vs CoT-SC 7.53 0.274\nGPT-4o, CoT vs RAP 10.23 0.115\nGPT-4o, CoT-SC vs RAP 13.05 0.042*\nClaude 3.5 S., ZS vs CoT 17.24 0.008*\nClaude 3.5 S., ZS vs CoT-SC 19.82 0.003*\nClaude 3.5 S., ZS vs RAP 2.26 0.894\nClaude 3.5 S., CoT vs CoT-SC 10.13 0.119\nClaude 3.5 S., CoT vs RAP 20.56 0.002*\nClaude 3.5 S., CoT-SC vs RAP 23.22 0.001*\nGemini 1.5 P ., ZS vs CoT 7.92 0.244\nGemini 1.5 P ., ZS vs CoT-SC 8.07 0.233\nGemini 1.5 P ., ZS vs RAP 5.55 0.475\nGemini 1.5 P ., CoT vs CoT-SC 2.10 0.910\nGemini 1.5 P ., CoT vs RAP 13.09 0.042*\nGemini 1.5 P ., CoT-SC vs RAP 13.67 0.034*\nTable 3. McNemar-Bowker test results for responses collected from the LLMs with different prompts. * \nindicates that the differences are statistically significant (p < 0.05).\n \nFig. 3. The Cohen’s Kappa coefficients measured for each of the 12 pairwise comparisons using the RD exam. \nThe dark blue indicates high levels of agreement, while the light blue represents lower agreement levels.\n \nScientific Reports |         (2025) 15:1506 5| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nCohen’s kappa and McNemar-Bowker tests, including 95% confidence intervals and P-values, are presented in \nSupplementary Table S.3.\nThe approaches based on GPT-4o showed a high degree of agreement, indicated by a Cohen’s Kappa \ncoefficient of 0.98 between CoT and CoT-SC and a coefficient of 0.93 between RAP and the other three \nprompting techniques. The McNemar-Bowker tests also indicated that there were no statistically significant \ndifferences in the paired responses, except for CoT-SC and RAP (P value =0 .042). For the Claude 3.5-based \napproaches, the Cohen’s Kappa coefficients were slightly lower compared to GPT-4o. However, the statistical \ntests showed that the responses of ZS and RAP were significantly different from CoT and CoT-SC (see Table 3). \nThe Gemini 1.5 Pro’s approaches recorded relatively lower Cohen’s Kappa coefficients, despite maintaining high \noverall agreement. The Cohen’s Kappa coefficients of the Gemini-based approaches were from 0.84 to 0.93. The \nagreement level between RAP and CoT / CoT-SC were 0.85. The statistical tests also showed that the responses \nof RAP were significantly different from CoT and CoT-SC. Interestingly, among the prompting techniques, \nthe approaches (even with different LLMs) using CoT and CoT-SC obtained higher levels of agreement. The \nMcNemar-Bowker tests also indicated that there were no statistically significant differences between the CoT \nand CoT-SC responses in the three LLMs.\nIntra-rater analysis\nIn this study, each approach was repeated five times, resulting in five sets of responses. The intra-rater reliability \nof the responses was evaluated by measuring the repeatability of the approaches, determining how consistently \nthey agreed with themselves when receiving the same questions. For this purpose, Fleiss Kappa was employed to \nassess the intra-rater agreements. Table 4 and Figure 4 indicate the Fleiss Kappa coefficients, and Supplementary \nTable S.4 includes the detailed statistical data.\nFig. 4. The Fleiss Kappa coefficients of the 12 approaches.\n \nLLM Prompt Fleiss’ Kappa 95% CI\nGPT-4o\nZero Shot 0.980 0.973 – 0.987\nChain of Thought 0.969 0.960 – 0.977\nChain of Thought w. Self Consistency 0.977 0.970 – 0.985\nRetrieval Augmented Prompting 0.985 0.978 – 0.991\nClaude 3.5 S.\nZero Shot 0.987 0.981 – 0.992\nChain of Thought 0.975 0.967 – 0.983\nChain of Thought w. Self Consistency 0.982 0.975 – 0.988\nRetrieval Augmented Prompting 0.977 0.970 – 0.985\nGemini 1.5 P .\nZero Shot 0.996 0.993 – 0.999\nChain of Thought 0.902 0.887 – 0.917\nChain of Thought w. Self Consistency 0.938 0.926 – 0.951\nRetrieval Augmented Prompting 0.991 0.987 – 0.996\nTable 4. The Fleiss Kappa coefficients of the 12 approaches. Each approach was repeated 5 times.\n \nScientific Reports |         (2025) 15:1506 6| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nGemini 1.5 Pro combined with the ZS prompting achieved the highest agreement among all combinations, \nwhereas the Gemini with CoT produced the lowest agreement. The approaches based on Claude 3.5 Sonnet \ndemonstrated the highest overall agreement. For the three LLMs, the ZS prompting technique consistently \nresulted in the highest agreement, as indicated by Fleiss’s Kappa coefficients of 0.996, 0.987, and 0.980 for Gemini \n1.5 Pro, Claude 3.5 Sonnet, and GPT-4o, respectively. Similarly, the coefficients of the LLMs with RAP were high. \nThe CoT-SC recorded the third highest agreement, while the CoT obtained the lowest.\nDiscussion\nOur findings indicated that all the approaches, combining three LLMs with four prompt engineering techniques, \nsuccessfully passed the RD exam and obtained a score of above 88%. In our tests, the combination of GPT-4o \nwith CoT-SC prompting outperformed the other approaches in terms of accuracy, while Gemini 1.5 Pro with \nZS prompting showed the highest consistency. On the other hand, the lowest average percentage score was \n89.22% for Gemini 1.5 Pro with CoT, which also showed the lowest agreement in repeated measurements, with \na coefficient of 0.902. GPT-4o recorded the highest accuracy overall (see Table 1).\nIn practice for dietitians taking the RD exam, the exam is scored from 1 to 50, and the minimum score to \npass is 2544. In the exam, the questions might be weighted differently, and the score is calculated based on the \ncandidate’s performance as well as the difficulty levels of the questions. According to the RD Exam Pass/Fail \nStatistics published by the Commission on Dietetic Registration, the grand total first-attempt pass rate for the \nRD exam from January to June 2024 was 61.5% and the total first-attempt pass rate in 2023 was 88.4%45.\nDespite the success of the approaches to pass the RD exam, the three leading LLMs had different performance \nlevels in terms of the number of inaccurate responses and consistency. Particularly, the prompting techniques \nhad considerable impacts on the results. Such prompting impacts were also explored in other evaluation studies, \nfor example, in clinical medicine 31, mental health 46, and radiology 47. We observed that GPT-4o showed more \nconsistent behavior in changing prompts, as indicated by its higher overall Cohen’s Kappa values and only one \nsignificant difference in responses between CoT-SC and RAP . This is also supported by its high Fleiss Kappa \nvalues, which indicate its consistency in answering the same questions multiple times. The percentage scores of \nGPT-4o also indicate its robust accuracy when handling different prompt types. Claude 3.5 Sonnet demonstrated \nsimilar performance, with slightly lower Cohen’s Kappa values and percentage scores. However, the responses \nof ZS and RAP were significantly different from the responses of CoT and CoT-SC. In contrast, changing the \nprompts had the most impact on Gemini’s performance, with lowest Cohen’s Kappa coefficients. Although \nGemini with ZS was the most consistent approach, the model with CoT showed the lowest Fleiss Kappa values \nand percentage scores.\nIn addition to our findings, an overview of the models’ performance on existing non-nutrition-focused \nknowledge and reasoning benchmarks are indicated in Table 5. The performance scores of these three benchmarks \nwere collected from the following references 38,39,48. The GPQA benchmark 49includes 448 multiple-choice \nquestions on biology, physics, and chemistry. The MMLU benchmark50contains multiple-choice questions from \n57 topics, such as elementary mathematics, US history, computer science, and law; and the DROP benchmark51 \nconsists of 96,567 questions focusing on discrete reasoning over the content of paragraphs, including addition, \ncounting, and sorting. Claude 3.5 Sonnet outperformed the other LLMs in all scenarios, except for MMLU using \nthe ZS prompting.\nOur findings presented in Table 1contrasts with these previous non-nutrition research, except in MMLU50with \nZS prompting. Claude 3.5 with CoT obtained a 59.4% score on GPQA49. However, the three LLMs using CoT on \nthe RD exam achieved percentage scores above 90%. This difference might be due to the different difficulty levels \nof the exams. Particularly, 14.9% of the questions in the RD Exam are at the Expert level. However, as reported by \nRein et al.49, the GPQA questions are “extremely difficult, ”from which PhD students achieved a 65% score while \nnon-expert individuals achieved a 34% score. Moreover, DROP51 demonstrated that Claude 3.5 with Three Shot \nprompting outperformed in reasoning over text. Conversely, our results indicated that GPT-4o performed better \nusing the reasoning process of CoT prompting.\nPrior nutrition-focused research indicated that ChatGPT was accurate in most nutrition instances, but \nthe chatbot also recorded errors that could potentially harm and negatively impact the end-users. Therefore, \nachieving general accuracy is insufficient for practical real-world applications. For example, Sun et al. 20 \nindicated that ChatGPT-3.5 and ChatGPT-4 passed the Chinese RD exam (included 200 questions) and the food \nrecommendations were acceptable despite the presence of mistakes for specific foods, such as root vegetables \nand dry beans. Mishra et al.52 tested ChatGPT in eight medical nutritional therapy scenarios and discussed \nthat ChatGPT should be avoided for complex scenarios. Naja et al.29evaluated ChatGPT’s accuracy and quality \nin providing nutrition management for Type 2 diabetes. They highlighted that ChatGPT exhibited errors in \nresponses, for example, in weight loss recommendation and the adoption of specific dietary interventions. \nBenchmark Prompt GPT-4o Claude 3.5 S. Gemini 1.5 P .\nMMLU (Undergraduate\nLevel Knowledge)\nZero Shot 88.70% 88.30% -\nFive Shot - 88.70% 85.90%\nGPQA (Graduate Level Reasoning) Chain of Thought 53.60% 59.40% 46.20%\nDROP (Reasoning) Three Shot 83.40% 87.10% 74.90%\nTable 5. The performance of the LLMs on the MMLU50, GPQA49, and DROP51benchmarks, collected from the \nfollowing references38,39,48.\n \nScientific Reports |         (2025) 15:1506 7| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nSimilarly, other studies10,24discussed that ChatGPT has great potential for nutritional management focusing on \nnon-communicable diseases, but the model might be potentially harmful by providing inaccurate responses, \nparticularly in complex situations. Another study22leveraged ChatGPT-3.5 and ChatGPT-4 to provide nutritional \ninformation for eight menus. Their results indicated that responses had no significant differences compared \nto nutritionists’ recommendations in terms of energy, carbohydrate, and fat contents, but the difference was \nstatistically significant for protein. The potential of ChatGPT to generate dietary advice for patients with allergic \nto food allergens were also investigated 27. It was shown that although the model was generally accurate, it \nproduced harmful diets. These studies highlight the need for further investigation into LLM responses within \nthe context of food and nutrition.\nOur results confirmed previous findings about the overall accuracy of ChatGPT and the instances of \ninaccurate responses. However, unlike the existing work, our study is not merely restricted to ChatGPT or the \nZS prompting technique. We also focused on examining errors across various subcategories and mitigate them \nby employing prompting techniques (reasoning and ensemble) and external knowledge retrieval.\nIn summary, we observed that GPT-4o with CoT-SC and CoT obtained the best performance across all the \nproficiency levels. GPT-4o with CoT-SC resulted in 0.6 errors (on average) for the 149 easy questions, 22.4 errors \nfor the 392 difficult questions, and 24.4 errors for the 157 expert-level questions. CoT obtained the least errors \n(i.e., 9.6) for the 352 moderate-level questions. GPT-4o was also the most accurate model across all domains. \nGPT-4o with CoT-SC recorded only 7 errors in the D1) principles of dietetics questions and 18.2 errors in the \nD4) food and nutrition management questions. GPT-4o with CoT obtained the fewest errors (i.e., 11.0) in the \nD3) food service systems questions, and GPT-4o with ZS had the fewest errors (i.e., 19.4) in the D2) nutrition care \nquestions. Figure 2 and Supplementary Tables S.1 and S.2 indicate each model’s performance across proficiency \nlevels and domains. In the following, we discuss how the prompting technique influenced the LLMs responses \nin more detail.\n• CoT guided LLMs to perform a reasoning process when answering a question. Our findings showed that CoT, \ncompared to ZS prompting, enhanced the accuracy of GPT-4o and Claude 3.5 Sonnet but led to diminished \nconsistency. CoT did not consistently generate the same reasoning paths (lower Fleiss Kappa coefficients), \neven with identical prompts (see Figure 4). This variability indicates randomness in the selection of reasoning \npaths. We observed that the reasoning steps of CoT considerably reduced the LLMs’ (particularly GPT-4o \nand Claude 3.5 Sonnet) mistakes for the questions with easy, moderate, and difficult proficiency levels. This \ndemonstrates the effectiveness of this prompting technique in enhancing the chatbots’ performance to handle \na wide range of nutrition question complexities by breaking down problems into reasoning steps. However, \nthis improvement was less for the expert-level questions. At the expert proficiency level, where questions \nrequired deeper understanding and reasoning, only a small number of errors were corrected, indicating the \nlimitations of CoT in complex nutrition scenarios. It should be noted that the combination of Gemini 1.5 Pro \nwith CoT showed different patterns, where both accuracy and consistency decreased. Gemini with CoT was \nunable to select a choice from the given multiple-choice options for 20 out of 1050 questions (on average). Al-\nthough the errors on easy and moderate levels questions slightly decreased, the errors on difficult and expert \nlevels questions notably increased. Additionally, CoT notably improved the questions about D3) food service \nsystems, which involved calculations for food cost and portion estimation/forecasting. CoT also enhanced \nthe accuracy of D4) food and nutrition management , which included theoretical and conceptual questions \nrequiring an understanding of implicitly stated relationships. These improvements by CoT are consistent with \nexisting literature, indicating CoT enhances LLMs’ performance in arithmetic and commonsense tasks by \nestablishing logical connections53. Although CoT reduced errors in questions requiring calculations, our ob-\nservations indicate that CoT responses still include miscalculations and rounding errors. This issue may arise \ndue to the inherent characteristics of Transformer models, designed to generate text based on tokens rather \nthan numerical values. Potential solutions to address these issues include agentic approaches54,55, which inte-\ngrate LLMs with calculator tools or symbolic computing systems.\n• CoT-SC guided LLMs to perform multiple independent reasoning processes, then the responses were merged \nusing a majority voting method. Our findings revealed that CoT-SC (compared to CoT) improved accura -\ncy, particularly in Gemini 1.5 Pro. However, in GPT-4o and Claude 3.5, this improvement was small, as it \nonly led to the correction of a few errors. This small difference can also be observed in their high inter-rater \ncoefficient agreement, as illustrated in Figure 3. This finding does not support the literature suggesting that \nCoT-SC considerably enhances the accuracy of CoT56. Similar to CoT, the reasoning steps of CoT-SC notably \nreduced the GPT-4o errors for all the proficiency levels. GPT-4o with CoT-SC recorded fewest errors for the \neasy-, difficult-, and expert-levels questions. Claude 3.5 with CoT-SC improved the easy- moderate-, and \ndifficult-levels questions, compared with CoT. Gemini with CoT-SC was considerably better than CoT but \nworse than ZS. It is worth noting that, in our analysis, we observed that the impact of CoT-SC (compared with \nCoT) was more on consistency (intra-rater agreement) rather than accuracy. The ensemble process enabled \nby CoT-SC mitigates the randomness in the selection of reasoning paths (which was observed in CoT). For \nGPT-4o and Claude 3.5 Sonnet, the Fleiss’ Kappa agreements of CoT-SC were as robust as the agreements of \nZS prompting. The Gemini’s inability to select a choice from the given multiple-choice options also improved, \nreducing them from 20 in CoT to 6 in CoT-SC. This highlights the importance of employing such ensemble \ntechniques to enhance the consistency of LLM’s reasoning process by combining multiple reasoning paths \nrather than relying on a single path.\n• RAP integrated external relevant information from multiple references into the input prompts. GPT-4o effec-\ntively leveraged the retrieved information to reduce error rates, particularly for Difficult and Expert questions \nthat required more comprehensive understanding. Like CoT-SC, RAP recorded the fewest errors in the ex -\npert-level questions. Moreover, similar to CoT and CoT-SC, RAP improved D3) food service systems and D4) \nScientific Reports |         (2025) 15:1506 8| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nfood and nutrition management questions. Although relevant information was provided in our knowledge \nbase, RAP (compared to ZS) has recorded higher error rates for D2) nutrition care. D2 questions are mostly \nrelated to medical nutrition therapy, dietary guidelines, counseling skills, and nutrition care process. In con-\ntrast to GPT-4o, Gemini 1.5 Pro and Claude 3.5 Sonnet with RAP showed opposite behavior, as the accuracy \nfor the Difficult and Expert questions reduced. We noticed that, in some cases, Gemini was prioritizing exter-\nnal information over its own internal knowledge, even when that external information was irrelevant to the \nquestion. This resulted in incorrect interpretations and answers. For example, for two questions, the model \ngenerated “The provided text does not contain the answer to the question as it pertains to dietary restrictions for \npatients on Linezolid. ” and “The provided text focuses on Body Mass Index (BMI) but does not contain informa-\ntion about when weight and BMI peak. ”This issue was particularly observed in D2, where error rates increased \nfrom 26.2 (ZS) to 35.6 (RAP). It was anticipated that the LLMs’ performance improved when using the ex -\nternal information. However, our findings showed that RAP did not consistently enhance accuracy across \nthe three models. This higher error rates with RAP might arise from irrelevant retrieval, where the retrieval \nmodel fetches extraneous information57. As previously mentioned, we observed that the external information \nled Gemini and Claude to select a wrong option or fail to select an option. Additionally, the complexity or am-\nbiguity of the queries might contribute to this problem making it challenging for the retrieval model to find \nthe most relevant chunks.It is worth noting that the prompting techniques had less impact, whether positive \nor negative, on D1) principles of dietetics questions compared to the other domains. D1 questions primarily \nfocus on general food science, nutrients, biochemistry, and related research (e.g. which fruit has the highest \nfructose?), compared to the other domains that are more specialized in dietetics or involve more domain \nknowledge. For D1, GPT-4o achieved the best accuracy.\nConversational models including LLM chatbots are expected to be broadly used for various nutrition-related \ntasks, such as diet recommendations and recipes generation 1,3. Our findings offer insights into the readiness, \npotential, and limitations of these models, since the RD exam consists of a wide range of topics designed to \nassess a dietitian’s competency. The performance of the chatbots on the RD exam can reflect their ability to \ncomprehend nutrition queries, understand fundamentals of dietetics, and transform the knowledge into nutrition \nmanagement, care, and reliable advice. These findings show the potential of LLM chatbots to be effectively used \nto support decision-making in practice and enhance dietitian support applications, including patient assessment, \nnutrition plan generation, and providing accurate answers to nutrition inquiries. Additionally, our results \nhighlight the role of prompt engineering on improving the accuracy and consistency of the models. It can help \nclinicians interact with the models more effectively and provide insights for developers when designing nutrition \nchatbots. A high-performing and consistent LLM can enhance users trust and satisfaction while boosting safety \nby ensuring that generated responses are accurate and reliable. Consequently, using RD exam to benchmark \nLLMs can be a valuable approach to evaluate LLMs capabilities in the nutrition domain and their potential to be \nused in real-world dietitian support scenarios.\nHowever, the LLM evaluation using the RD exam might have multiple limitations. First, our findings cannot \nshow LLMs’ performance in handling open-ended questions. In our evaluation, the model was instructed \nto choose one answer from provided options. Second, the results cannot indicate the models contextual \nunderstanding and personalization aspects. The accuracy, trustworthiness, and safety of a response might be \nhighly dependent to user’s profile, medical history, and personal situation. Another limitation of this test is \nthe lack of assessment of nutrition literacy and the clarity of the responses. Additionally, the RD exam cannot \nassess bias and cultural insensitivity in responses that could lead to misunderstandings or mistrust, even though \ngenerated responses are accurate.\nFuture work in this direction will involve evaluating LLMs on open-ended questions and by leveraging \npatient-centric questions, answers, and conversations. Our evaluation has primarily concentrated on the \naccuracy and consistency of the models. Given the sensitivity of health and nutrition applications, ensuring high \naccuracy and consistency is essential. However, it is important to assess LLMs holistically and from multiple \nperspectives, such as safety, bias, privacy, and emotional support, to mention a few18,19,58.\nIn addition, future research should concentrate on the performance of open-source LLMs in the diet and \nnutrition field. Our study is limited to the leading proprietary LLM models. These models are user-friendly \nand highly powerful. Our results also confirm their significant potential in food and nutrition applications. \nY et, growing concerns are being raised about their lack of openness and limited access. The exact architecture \nand training data of these LLMs are not publicly known. In contrast, open-source LLMs are emerging rapidly, \noffering benefits, such as improved data security and privacy, decreased reliance on vendors, and the ability to \ncustomize models. Examples of the state-of-the-art open-source LLMs are Llama 359, Falcon 260, and Yi-34B61.\nAnother avenue for future research should evaluate the impact of various information retrieval, fine-tuning, \nand agentic approaches in nutrition management applications. Recent studies have explored the role of fine-\ntuning32,62,63and agentic methods12,23,64 in healthcare and nutrition applications. A solid benchmarking approach \nis required to comprehensively evaluate the effectiveness of these approaches in nutrition chatbots.\nIn conclusion, this study assessed the accuracy and consistency on the GPT-4o, Claude 3.5 Sonnet, and \nGemini 1.5 Pro in responding to diet and nutrition questions of the RD exam. In contrast to the previous LLM \nevaluation studies focusing on nutritional management, our experiments were not restricted to ChatGPT or \nZS prompting. We evaluated the models using the RD exam and analyzed their errors across various questions \ncomplexities and nutrition domains. Our findings highlighted the strengths and weaknesses of the three LLMs, \nshowing the influence of different prompting techniques on their responses to the RD exam questions. GPT-4o \nwith CoT-SC prompting outperformed other approaches, while Gemini 1.5 Pro with ZS indicated the highest \nconsistency. For GPT-4o and Claude 3.5, the application of CoT improved accuracy, while CoT-SC enhanced \nboth accuracy and consistency. RAP particularly improved GPT-4o performance in addressing difficult- \nScientific Reports |         (2025) 15:1506 9| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nexpert-level questions. Consequently, selecting the appropriate LLM and prompt engineering, tailored to the \nproficiency level and specific domain, can considerably reduce errors and mitigate potential risks in diet and \nnutrition chatbot applications.\nMethods\nIn this study, we use the RD exam questions to benchmark the performance of three leading LLMs chatbots, i.e., \nGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, in addressing nutrition-related inquiries. The RD exam was \nselected due to its comprehensive coverage across multiple nutrition topics. We define accuracy and consistency \nas key performance metrics in this experiment. The assessment is conducted by employing four distinct \nprompting techniques: 1) Zero Shot prompting (ZS), 2) Chain of Thought (CoT), 3) Chain of Thought with Self \nConsistency (CoT-SC), and 4) Retrieval Augmented Prompting (RAP) enabled by external nutrition knowledge. \nThe responses are then analyzed, by comparing them to the ground truth, to evaluate the performance of each \nLLM and prompting technique. We perform inter-rater and intra-rater analysis to identify the strengths and \nweaknesses of each approach. In the following, we, first, provide more details about the RD exam. Then, we \nbriefly describe the LLMs and prompting techniques used. Finally, we outline response collection and analysis \nin this benchmarking.\nRegistered dietitian exam\nThe Registration Examination for Dietitians is a required exam for individuals seeking to obtain the registered \ndietitian credential. To take the exam, candidates must successfully complete the eligibility requirements \nprovided by the Commission on Dietetic Registration (CDR) 65. The examination consists of 125 to 145 four-\nchoice questions44, covering four major domains: D1) Principles of Dietetics (21%), D2) Nutrition Care for \nIndividuals and Groups (45%), D3) Food Service Systems (13%), and D4) Management of Food and Nutrition \nPrograms and Services (21%) 44. D1 covers topics related to i) food, nutrition, and supporting sciences, ii) \neducation, communication and technology, and iii) research applications. D2 consists of the topics related to \ni) screening and assessment, ii) diagnosis, iii) planning and intervention, and iv) monitoring and evaluation. \nD3 includes topics related to i) menu development, ii) procurement, production, distribution, and service, \niii) sanitation and safety, and iv) equipment and facility planning. D4 includes topics related to i) functions of \nmanagement, ii) human resource management, iii) financial management, iv) marketing and public relations; \nand v) quality management and regulatory compliance44.\nThe RD exam consists of a wide range of topics designed to assess a dietitian’s professional competency. \nTherefore, we posit that the exam can be a valuable nutrition benchmark for evaluating the ability of the LLMs \nto respond nutrition queries. Nevertheless, it is worth noting that the RD exam might not consist of complex \nclinical scenarios encountered in real-world dietetic practice, including personalized dietary recommendations, \ncomorbidity management, and food-drug interactions. In addition, it cannot address open-ended conversations, \nas the exam only include multiple-choice questions. In real-life scenarios, conversations might include detailed \nand long responses describing complex nutrition or health issues.\nLarge language models\nWithin the state-of-the-art nutrition and diet studies, OpenAI models (i.e., ChatGPT-3.5 and ChatGPT-4) have \nbeen mostly employed and evaluated 7,11,24,27. In this study, we propose to employ the RD exam to conduct a \nstandard and comprehensive evaluation of the leading OpenAI model (i.e., GPT-4o). Therefore, our findings \nwould be comparable to the existing literature in this field. Moreover, we extend the evaluation to include the \nrecently proposed gold standard chatbots. In addition to GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro are other \nchatbots that showed top performance on industry benchmarks for graduate-level reasoning, undergraduate-\nlevel knowledge, text-based reasoning, and math problem-solving.\nMoreover, OpenAI, Anthropic, and Google generative LLM chatbots holds the largest market share (more \nthan 90% in total) as of October 2024, highlighting the usability of these chatbots66–68. OpenAI released GPT-4o, \ntheir new flagship model, on May 13, 202438, Claude 3.5 Sonnet was launched, by Anthropic, as their strongest \nvision model yet, on Jun 20, 202439, and Google announced Gemini 1.5 Pro as their next-generation model on \nFebruary 15, 2024 40. An overview of the models’ performance on other benchmarks are indicated in Table 5. \nFind more details in the following references38,39,48.\nIn this study, we set the temperature setting to 0 in all the experiments to better evaluate the chatbots’ \nknowledge and decision-making in nutrition and diet applications, minimizing the effect of external variables \non consistency. The temperature parameter, ranging from 0 to 2 for GPT-4o and Gemini 1.5 Pro and from 0 to \n1 for Claude 3.5 Sonnet, regulates the uncertainty or randomness in the output 69. A higher temperature value \nleads to more randomness in the output. In other words, it raises the likelihood of selecting tokens other than the \nmost probable ones. However, with a temperature setting of 0, chatbots generate responses by selecting the next \nwords with the highest probability. Therefore, this selection leads to more deterministic behavior of chatbots.\nPrompt engineering\nPrompt engineering enables the design and optimization of input prompts to instruct LLMs in performing \nspecific tasks. Studies show that various prompt engineering techniques can differently affect on the performance \nof LLMs and the quality of the outcome 33,34. These techniques can be applied in different tasks across multiple \nfields, including healthcare and nutrition 31,70. For example, they can be leveraged in diet recommendation \nchatbots to provide conversations with users while incorporating relevant nutritional information into their \nresponses. In this study, four prompting techniques are utilized for the models evaluation. \nScientific Reports |         (2025) 15:1506 10| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\n 1.  Zero Shot (ZS) prompting leverages precisely formulated prompts, including a task description, to guide the \nLLM in completing the task. The prompting technique is straightforward and do not require any prior ex -\namples, as the model leverages its internal knowledge to generate responses33. However, the accuracy of the \nLLM’s outputs may be diminished when handling more complex tasks. To the best of our knowledge, existing \nevaluations of LLM chatbots focusing on nutrition and diet have utilized ZS prompting for their assess -\nments20,24,29. In our study, the ZS prompt consists of a question, multiple choices, and a fixed task description.\n 2.  Chain of Thought (CoT) prompting, proposed by Wei et al.53, guides LLMs to carry out the task through a \nmulti-step reasoning process. This reasoning process might increase the accuracy of the LLM’s outputs in \ncomplex tasks, such as arithmetic and symbolic reasoning. However, the model might be susceptible to error \npropagation in scenarios where multiple reasoning paths are available, and it cannot explore all the paths and \nselect the most accurate outcome71. CoT has been widely used in medical studies30,31. In our study, the CoT \nprompt includes a question, multiple choices, and a description to the model to answer the question through \nintermediate reasoning steps.\n 3.  Chain of Thought with Self Consistency (CoT-SC)  guides the LLM to carry out the task through multiple \npaths, each of which is based on CoT. Subsequently, the outcomes are aggregated56. Although CoT-SC might \nimprove the performance of the model, it increases computational costs and response generation latency56. \nCoT-SC has been leveraged in clinical studies 32. In our study, the CoT-SC consists of three independent \nreasoning paths (CoT) and a majority voting method for the aggregation.\n 4.  Retrieval Augmented Prompting (RAP) fetches relevant information from a knowledge base in real-time and \nintegrates it into the input prompt57,72. In contrast to the other prompting techniques, using RAP , the model \ngenerates responses by relying not only on its internal knowledge but also on external information, which \nmight mitigate hallucination. However, its implementation is more complex and introduces dependencies \non external data sources. In our study, the knowledge base includes 125 documents (such as articles, books, \nand guidelines) recommended by the Academy of Nutrition and Dietetics42, as references for the RD exam. \nThe full list of the references used for RAP is provided in Supplementary Table S.7. For the implementation, \nwe leveraged a conventional Retrieval Augmented Generation (RAG) framework57. To achieve this, the ref-\nerences were divided into 512-token chunks, using the Amazon Titan Text Embeddings v2 model73 for text \nembeddings. Then, the Cosine Similarity method 74 was utilized to identify the most similar chunks.Sche -\nmatic illustrations of the four techniques are shown in Figure 5. Supplementary Table S.5 also includes a brief \nsummary of the four techniques, along with their strengths and weaknesses. Additionally, the instructions \nused for the prompting techniques in this study are presented in Supplementary Table S.6.\nData collection\nFor response collection, the questions were first stored in JSON (JavaScript Object Notation) format and saved \nin a MySQL Workbench database. Then, the questions were delivered to the three LLMs via their respective \nApplication Programming Interfaces (APIs). The data collection was performed in Python using OpenAI \nv.1.26.075, google-generativeai v.0.5.476, Boto3 v. 1.34.11477, and lxml.etree v.5.2.278 libraries. Details about the \ncode, along with its documentation, are available in the GitHub repository at  h t t p s : / / g i t h u b . c o m / i H e a l t h L a b / D \ni e t i t i a n E x a m E v a l     .  \nUsing this setup, the 1050 RD exam questions were delivered to the three models through the four prompting \ntechniques. Each question was asked five times. Consequently, we collected 60 (i.e., 3 × 4 × 5) sets of 1050 \nFig. 5. Schematic illustrations of the four prompting techniques used in the evaluation. The inputs include \nmultiple-choice questions, and the generated output includes the selected choice.\n \nScientific Reports |         (2025) 15:1506 11| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nresponses. As previously mentioned, the questions include four choices. We observed that sometimes the LLMs \nwere unable to select an option from the multiple choices and provided responses such as, “None of the above, ” \n“Since no option is correct, we cannot provide a final answer within the requested tags, ” or “Cannot be determined \nwith the given information. ” In summary, this issue occurred once for GPT-4o with CoT, once for GPT-4o with \nCoT-SC, 15 times for Claude 3.5 with RAP , 100 times for Gemini 1.5 with CoT, 30 times for Gemini 1.5 with \nCoT-SC, and 63 times for Gemini 1.5 with RAP . For these responses, we added another option, labeled “Others. ”\nIt is worth noting that five repeated measurements was chosen as it has been identified in recently published \nstudies as an acceptable number of repeated measurements 22,24,31,79. For examples, Wang et al. 31 evaluated \nthe consistency of the agreement LLMs with the American Academy of Orthopedic Surgeons osteoarthritis \nevidence-based guidelines. Each question was posed five times in this study. Hoang et al. 22 investigated the \nconsistency of ChatGPT-3.5 and ChatGPT-4 in providing the energy and macronutrient content of 222 food \nitems across five repeated measurements. A high number of repeated measurements increases the reliability of \nevaluations. However, it also significantly increases the costs and latency of response collection, particularly for \ntechniques such as the CoT-SC, which includes multiple multi-step reasoning paths, and retrieval-augmented \nprompting.\nThe collected responses were compared with the ground truth answers provided by the Academy of Nutrition \nand Dietetics, eatrightPREP42. It should be noted that we used a new chat session for each query to minimize bias \nin the evaluation caused by information leakage from other questions.\nStatistical analysis\nThe chatbots were evaluated in terms of accuracy and consistency. The responses consists of choices (i.e., four \ncategories) from the same set of questions. Accuracy measures how close a set of responses aligns with the \nground truth answers (i.e., if they are equal). To this end, we calculate the percentage score and error count. The \npercentage score indicates how well an LLM can detect the correct option and is obtained as follows.\n \nScore = Correct Responses\nAll Responses ∗ 100 (1)\nThe error count is the number of times the selected choice does not match the ground truth. As previously \nmentioned, each measurement is repeated five times. The five repeated measurements in each test are grouped, \nand the mean and standard deviation of the scores and error counts are calculated. We also assess the performance \nof the LLMs by considering the proficiency levels and domains of the questions. To do this, we compute the \naverage error counts for each subgroup.\nConsistency refers to the degree to which responses produce the same results. To assess consistency, we \nperform inter-rater and intra-rater analysis approaches80. For the former, the agreement between the responses \nobtained from different models / prompting techniques are evaluated. To this end, Cohen’s Kappa81 was utilized \nto measure the degree of agreement between paired sets of responses. For example, the agreement between \nresponses obtained from GPT-4o with ZS prompting and GPT-4o with CoT prompting are calculated. Cohen’s \nKappa was selected as the analysis included paired categorical responses, requiring a method that takes into \naccount the possibility of agreement by chance. We also utilize the McNemar-Bowker test 43 to investigate if \nthe collected paired sets of responses are statistically different. The test indicates if the contingency table is \nsymmetric, evaluating whether there are significant differences in patterns between two sets of responses. The \nMcNemar-Bowker test was selected, as it allows the analysis of paired categorical data involving more than two \ncategories (i.e., an extension of McNemar’s test).\nFurthermore, for the intra-rater analysis, Fleiss Kappa test 82 was used to indicate the degree of overall \nagreement between the repeated measurements under fixed conditions. For instance, we assess whether GPT-4o \nwith ZS prompting provides the same choices in repeated measurements. Fleiss Kappa was selected, since the \nanalysis consisted of multiple raters, requiring a method to measure agreement across multiple sets of responses.\nNote that the statistical analysis was conducted in R Programming using the irr v.0.84.183library to perform \nCohen’s Kappa and Fleiss Kappa tests, and the boot v.1.3–3084 library to compute Bootstrap confidence intervals. \nDetails about the code, along with its documentation, are available in the GitHub repository at  h t t p s : / / g i t h u b . c o \nm / i H e a l t h L a b / D i e t i t i a n E x a m E v a l     .  \nData availability\nThe RD exam questions used in this study are not publicly available and can be accessed via  h t t p s : / / w w w . e a t r i g \nh t p r e p . o r g     .  \nReceived: 15 August 2024; Accepted: 30 December 2024\nReferences\n 1. Singh, B. et al. Systematic review and meta-analysis of the effectiveness of chatbots on lifestyle behaviours. npj Digital Medicine 6, \n118 (2023).\n 2. Webster, P . Six ways large language models are changing healthcare. Nature Medicine 29, 2969–2971 (2023).\n 3. Ma, P . et al. Large language models in food science: Innovations, applications, and future. Trends in Food Science & Technology \n104488 (2024).\n 4. Clusmann, J. et al. The future landscape of large language models in medicine. Communications medicine 3, 141 (2023).\nScientific Reports |         (2025) 15:1506 12| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\n 5. Meskó, B. The impact of multimodal large language models on health care’s future. Journal of medical Internet research 25, e52865 \n(2023).\n 6. Bond, A., Mccay, K. & Lal, S. Artificial intelligence & clinical nutrition: What the future might have in store. Clinical nutrition \nESPEN (2023).\n 7. Dao, D., Teo, J. Y .  C., Wang, W . & Nguyen, H.  D. LLM-Powered Multimodal AI Conversations for Diabetes Prevention. In \nProceedings of the 1st ACM Workshop on AI-Powered Q &A Systems for Multimedia, 1–6 (2024).\n 8. OpenAI. Introducing ChatGPT. https://openai.com/index/chatgpt/ (2022). Accessed: August 2024.\n 9. Liu, Y . et al. Exploring the usability of a chatbot-based conversational dietary assessment tool among cardiovascular patients. \nEuropean Journal of Preventive Cardiology 30, zwad125–281 (2023).\n 10. Pugliese, N. et al. Accuracy, Reliability, and Comprehensibility of ChatGPT-generated Medical Responses for Patients with \nNonalcoholic Fatty Liver Disease. Clinical Gastroenterology and Hepatology 22, 886–889 (2024).\n 11. Kim, D. W . et al. Qualitative evaluation of artificial intelligence-generated weight management diet plans. Frontiers in Nutrition 11, \n1374834 (2024).\n 12. Abbasian, M. et al. Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients. In the \n46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE, 2024).\n 13. Haman, M., Školník, M. & Lošták, M. AI dietitian: Unveiling the accuracy of ChatGPT’s nutritional estimations. Nutrition 119, \n112325 (2024).\n 14. Qarajeh, A. et al. AI-Powered Renal Diet Support: Performance of ChatGPT, Bard AI, and Bing Chat. Clinics and Practice 13, \n1160–1172 (2023).\n 15. Tsai, C.-H. et al.  Generating Personalized Pregnancy Nutrition Recommendations with GPT-Powered AI Chatbot. In 20th \nInternational Conference on Information Systems for Crisis Response and Management (ISCRAM), vol. 2023, 263 (2023).\n 16. Zhou, P . et al. FoodSky: A Food-oriented Large Language Model that Passes the Chef and Dietetic Examination. arXiv:2406.10261 \n(2024).\n 17. Thirunavukarasu, A. J. et al. Large language models in medicine. Nature medicine 29, 1930–1940 (2023).\n 18. Abbasian, M. et al. Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI. NPJ \nDigital Medicine 7, 82 (2024).\n 19. Liang, P . et al. Holistic evaluation of language models. arXiv:2211.09110 (2022).\n 20. Sun, H. et al. An AI dietitian for type 2 diabetes mellitus management based on large language and image recognition models: \npreclinical concept validation study. Journal of Medical Internet Research 25, e51300 (2023).\n 21. Barlas, T., Altinova, A. E., Akturk, M. & Toruner, F . B. Credibility of ChatGPT in the assessment of obesity in type 2 diabetes \naccording to the guidelines. International Journal of Obesity 48, 271–275 (2024).\n 22. Hoang, Y . N. et al. Consistency and accuracy of artificial intelligence for providing nutritional information. JAMA network open 6, \ne2350367–e2350367 (2023).\n 23. Y ang, Z. et al. ChatDiet: Empowering personalized nutrition-oriented food recommender chatbots through an LLM-augmented \nframework. Smart Health 32, 100465 (2024).\n 24. Ponzo, V . et al. Is ChatGPT an Effective Tool for Providing Dietary Advice?. Nutrients 16, 469 (2024).\n 25. Kirk, D., van Eijnatten, E. & Camps, G. Comparison of answers between ChatGPT and human dieticians to common nutrition \nquestions. Journal of Nutrition and Metabolism 2023, 5548684 (2023).\n 26. Szymanski, A., Wimer, B.  L., Anuyah, O., Eicher-Miller, H.  A. & Metoyer, R.  A. Integrating Expertise in LLMs: Crafting a \nCustomized Nutrition Assistant with Refined Template Instructions. In Proceedings of the CHI Conference on Human Factors in \nComputing Systems, 1–22 (2024).\n 27. Niszczota, P . & Rybicka, I. The credibility of dietary advice formulated by ChatGPT: robo-diets for people with food allergies. \nNutrition 112, 112076 (2023).\n 28. Minaee, S. et al. Large language models: A survey. arXiv:2402.06196 (2024).\n 29. Naja, F . et al. Artificial intelligence chatbots for the nutrition management of diabetes and the metabolic syndrome. European \nJournal of Clinical Nutrition 1–10 (2024).\n 30. Holmes, J. et al. Evaluating large language models on a highly-specialized topic, radiation oncology physics. Frontiers in Oncology \n13, 1219326 (2023).\n 31. Wang, L. et al. Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. npj Digital Medicine \n7, 41 (2024).\n 32. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023).\n 33. Sahoo, P . et al. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv:2402.07927 \n(2024).\n 34. Chen, B., Zhang, Z., Langrené, N. & Zhu, S. Unleashing the potential of prompt engineering in large language models: a \ncomprehensive review. arXiv:2310.14735 (2023).\n 35. Wang, J. et al. Prompt engineering for healthcare: Methodologies and applications. arXiv:2304.14670 (2023).\n 36. Maharjan, J. et al. OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-\nsource large language models. Scientific Reports 14, 14156 (2024).\n 37. Ouyang, S., Zhang, J. M., Harman, M. & Wang, M. LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code \nGeneration. arXiv:2308.02828 (2023).\n 38. OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ (2024). Accessed: August 2024.\n 39. Anthropic. Introducing Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet (2024). Accessed: August 2024.\n 40. Google. Introducing Gemini 1.5, Google’s next-generation AI model.  h t t p s :  / / b l o g  . g o o g l  e / t e c  h n o l o g y / a i / g o o g l e - g e m i n i - n e x t - g e n e \nr a t i o n - m o d e l - f e b r u a r y - 2 0 2 4 /     (2024). Accessed: August 2024.\n 41. Commission on Dietetic Registration. Registered Dietitian Nutritionist. https://www.cdrnet.org/RDN. Accessed: August 2024.\n 42. Academy of Nutrition and Dietetics . https://www.eatrightprep.org/. Accessed: August 2024.\n 43. Fagerland, M., Lydersen, S. & Laake, P . Statistical analysis of contingency tables (Chapman and Hall/CRC, 2017).\n 44. Commission on Dietetic Registration. Candidate Handbook, RD Exam.  h t t p s :  / / a d m i  n . c d r n  e t . o r  g / v a u l t / 2 4 5 9 / w e b / / R D % 2 0 H a n d b \no o k % 2 0 f o r % 2 0 C a n d i d a t e s % 2 0 - % 2 0 6 - 2 0 2 4 . p d f     (2024).\n 45. Commission on Dietetic Registration. RD Exam Pass / Fail Statistics. https://www.cdrnet.org/RDExamStats. Accessed: October \n2024.\n 46. Grabb, D. The impact of prompt engineering in large language model performance: a psychiatric example. Journal of Medical \nArtificial Intelligence 6 (2023).\n 47. Russe, M.  F ., Reisert, M., Bamberg, F . & Rau, A. Improving the use of LLMs in radiology through prompt engineering: from \nprecision prompts to zero-shot learning. In RöFo-Fortschritte auf dem Gebiet der Röntgenstrahlen und der bildgebenden Verfahren \n(Georg Thieme Verlag KG, 2024).\n 48. Reid, M. et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530 (2024).\n 49. Rein, D. et al. GPQA: A Graduate-level Google-proof Q &A Benchmark. arXiv:2311.12022 (2023).\n 50. Hendrycks, D. et al. Measuring massive multitask language understanding. arXiv:2009.03300 (2020).\n 51. Dua, D. et al.  DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv:1903.00161 \n(2019).\nScientific Reports |         (2025) 15:1506 13| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\n 52. Mishra, V ., Jafri, F ., Abdul Kareem, N., Aboobacker, R. & Noora, F . Evaluation of accuracy and potential harm of ChatGPT in \nmedical nutrition therapy-a case-based approach. F1000Research 13, 137 (2024).\n 53. Wei, J. et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Advances in neural information processing \nsystems 35, 24824–24837 (2022).\n 54. Gou, Z. et al. ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. arXiv:2309.17452 (2023).\n 55. Abbasian, M., Azimi, I., Rahmani, A. M. & Jain, R. Conversational health agents: A personalized LLM-powered agent framework. \narXiv:2310.02374 (2023).\n 56. Wang, X. et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 (2022).\n 57. Gao, Y . et al. Retrieval-augmented generation for large language models: A survey. arXiv:2312.10997 (2023).\n 58. Sun, L. et al. TrustLLM: Trustworthiness in large language models. arXiv:2401.05561 (2024).\n 59. Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-llama-3/ \n(2024). Accessed: August 2024.\n 60. Technology Innovation Institute. Falcon LLM. https://falconllm.tii.ae/ (2024). Accessed: August 2024.\n 61. 01.AI. Yi-34B. https://huggingface.co/01-ai/Yi-34B (2024). Accessed: August 2024.\n 62. Xu, L., Xie, H., Qin, S.-Z. J., Tao, X. & Wang, F . L. Parameter-efficient fine-tuning methods for pretrained language models: A \ncritical review and assessment. arXiv:2312.12148 (2023).\n 63. Zhang, X. et al. Comparison of prompt engineering and fine-tuning strategies in large language models in the classification of \nclinical notes. AMIA Summits on Translational Science Proceedings 2024, 478 (2024).\n 64. Li, Y . et al. Personal LLM agents: Insights and survey about the capability, efficiency and security. arXiv:2401.05459 (2024).\n 65. Commission on Dietetic Registration, Registered Dietitian (RD) Or Registered Dietitian Nutritionist (RDN) Certification.  h t t p s : / \n/ w w w . c d r n e t . o r g / R D N     . Accessed: August 2024.\n 66. Chatbot Arena. Chat with Open Large Language Models. https://chat.lmsys.org/?leaderboard. Accessed: August 2024.\n 67. Artificial Analysis. Model & API Providers Analysis. https://artificialanalysis.ai/. Accessed: August 2024.\n 68. Evan Bailyn. Top Generative AI Chatbots by Market Share - October 2024.  h t t p s :  / / fi   r s  t p a g e s  a g e . c  o m / r e p o r t s / t o p - g e n e r a t i v e - a i - c h \na t b o t s /     (2024). Accessed: October 2024.\n 69. Peeperkorn, M., Kouwenhoven, T., Brown, D. & Jordanous, A. Is temperature the creativity parameter of large language models? \narXiv:2405.00492 (2024).\n 70. Zaghir, J. et al. Prompt engineering paradigms for medical applications: Scoping review. Journal of Medical Internet Research 26, \ne60501 (2024).\n 71. Saparov, A. & He, H. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv:2210.01240 \n(2022).\n 72. Li, Y . et al. ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain \nKnowledge. Cureus 15 (2023).\n 73. Sebastien Stormacq. Amazon Titan Text Embeddings V2 now available in Amazon Bedrock, optimized for improving RAG. \nhttps://aws. amazon.com/b logs/aws/ama zon-titan-t ext-v2-now- available-in -amazon-bedr ock-optimiz ed-for-improving-rag/  \n(2024). Accessed: August 2024.\n 74. Manning, C. D., Raghavan, P . & Schütze, H. Scoring, term weighting & the vector space model. In Introduction to Information \nRetrieval (Cambridge University Press, 2008).\n 75. Libraries, OpenAI API. https:   //platfo rm.ope nai .com /docs/lib raries  /python-library. Accessed: August 2024.\n 76. google-generativeai, Gemini API. https://pypi.org/project/google-generativeai/. Accessed: August 2024.\n 77. AWS SDK for Python (Boto3) Documentation. https://docs.aws.amazon.com/pythonsdk/. Accessed: August 2024.\n 78. Stefan Behnel. The lxml.etree Tutorial. https://lxml.de/tutorial.html. Accessed: August 2024.\n 79. Antaki, F ., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating the performance of chatgpt in ophthalmology: an analysis of \nits successes and shortcomings. Ophthalmology science 3, 100324 (2023).\n 80. Hallgren, K. A. Computing inter-rater reliability for observational data: an overview and tutorial. Tutorials in quantitative methods \nfor psychology 8, 23 (2012).\n 81. Cohen, J. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 37–46 (1960).\n 82. Fleiss, J. L. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 378 (1971).\n 83. Gamer, M., Lemon, J. & Singh, I. F . P . irr: Various Coefficients of Interrater Reliability and Agreement (2019). R package version \n0.84.1, https://CRAN.R-project.org/package=irr.\n 84. Canty, A., Ripley, B. & Brazzale, A. R. boot: Bootstrap Functions (2024). R package version 1.3-30,  h t t p s : / / C R A N . R - p r o j e c t . o r g / p a c \nk a g e = b o o t     .   \nAcknowledgements\nWe are grateful for iHealth Labs to sponsor this research, which will be used in their LLM-based nutrition \nassistant product. The authors would like to thank Prof. Penny M. Kris-Etherton for her thorough review and \nsuggestions. We acknowledge UCI Center for Statistical Consulting for their expert statistical consulting.\nAuthor contributions\nI.A. drafted the manuscript, contributed to the study design, analyzed and interpreted the LLM data, and pre -\npared Figures 1-4. M.Q. contributed to the acquisition of LLM data, contributed to the study design, analyzed \nand interpreted nutrition information, and co-drafted the manuscript. L.W . contributed to the analysis and in-\nterpretation of the nutrition information and critically revised the manuscript. A.R. contributed to the interpre-\ntation of the LLM responses and critically revised the manuscript. Y .L. contributed to the study design, analyzed \nand interpreted both the LLM data and nutrition information, and critically revised the manuscript. All authors \nreviewed the manuscript.\nDeclarations\nCompeting Interests\nThe authors declare no competing interests. Moreover, the funders of the study had no role in study design, \ndata collection and analysis, or interpretation of results and preparation of the manuscript.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 4 - 8 5 0 0 3 - w     .  \nScientific Reports |         (2025) 15:1506 14| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/\nCorrespondence and requests for materials should be addressed to I.A.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |         (2025) 15:1506 15| https://doi.org/10.1038/s41598-024-85003-w\nwww.nature.com/scientificreports/"
}