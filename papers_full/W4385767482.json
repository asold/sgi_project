{
  "title": "Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for Ultra-High Resolution Segmentation",
  "url": "https://openalex.org/W4385767482",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2920854880",
      "name": "Deyi Ji",
      "affiliations": [
        "Alibaba Group (United States)",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1233410411",
      "name": "Feng Zhao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2009627569",
      "name": "Hong-Tao Lu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096653763",
    "https://openalex.org/W2963222130",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3110850179",
    "https://openalex.org/W2922097204",
    "https://openalex.org/W3169770376",
    "https://openalex.org/W4312893741",
    "https://openalex.org/W4287198652",
    "https://openalex.org/W2609402060",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4386076151",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2964217532",
    "https://openalex.org/W3102785203",
    "https://openalex.org/W2950045474",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2894030891",
    "https://openalex.org/W4312832682",
    "https://openalex.org/W2946824094",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W3179066717",
    "https://openalex.org/W3034438741",
    "https://openalex.org/W3197006556",
    "https://openalex.org/W1512435323",
    "https://openalex.org/W2805735218",
    "https://openalex.org/W3109196706",
    "https://openalex.org/W2998317038",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4312272720",
    "https://openalex.org/W2804199516",
    "https://openalex.org/W1701879202",
    "https://openalex.org/W3172798612",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3035358681",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3198947145",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2412782625"
  ],
  "abstract": "Most existing ultra-high resolution (UHR) segmentation methods always struggle in the dilemma of balancing memory cost and local characterization accuracy, which are both taken into account in our proposed Guided Patch-Grouping Wavelet Transformer (GPWFormer) that achieves impressive performances. In this work, GPWFormer is a Transformer (T)-CNN (C) mutual leaning framework, where T takes the whole UHR image as input and harvests both local details and fine-grained long-range contextual dependencies, while C takes downsampled image as input for learning the category-wise deep context. For the sake of high inference speed and low computation complexity, T partitions the original UHR image into patches and groups them dynamically, then learns the low-level local details with the lightweight multi-head Wavelet Transformer (WFormer) network. Meanwhile, the fine-grained long-range contextual dependencies are also captured during this process, since patches that are far away in the spatial domain can also be assigned to the same group. In addition, masks produced by C are utilized to guide the patch grouping process, providing a heuristics decision. Moreover, the congruence constraints between the two branches are also exploited to maintain the spatial consistency among the patches. Overall, we stack the multi-stage process in a pyramid way. Experiments show that GPWFormer outperforms the existing methods with significant improvements on five benchmark datasets.",
  "full_text": "Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for\nUltra-High Resolution Segmentation\nDeyi Ji1,2 , Feng Zhao1∗ , Hongtao Lu3,4\n1University of Science and Technology of China\n2Alibaba Group\n3Department of Computer Science and Engineering, Shanghai Jiao Tong University\n4MOE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\njideyi@mail.ustc.edu.cn, fzhao956@ustc.edu.cn, htlu@sjtu.edu.cn\nAbstract\nMost existing ultra-high resolution (UHR) segmen-\ntation methods always struggle in the dilemma\nof balancing memory cost and local characteriza-\ntion accuracy, which are both taken into account\nin our proposed Guided Patch-Grouping Wavelet\nTransformer (GPWFormer) that achieves impres-\nsive performances. In this work, GPWFormer is a\nTransformer (T )-CNN (C ) mutual leaning frame-\nwork, where T takes the whole UHR image as\ninput and harvests both local details and fine-\ngrained long-range contextual dependencies, while\nC takes downsampled image as input for learning\nthe category-wise deep context. For the sake of\nhigh inference speed and low computation com-\nplexity, T partitions the original UHR image into\npatches and groups them dynamically, then learns\nthe low-level local details with the lightweight\nmulti-head Wavelet Transformer (WFormer) net-\nwork. Meanwhile, the fine-grained long-range con-\ntextual dependencies are also captured during this\nprocess, since patches that are far away in the\nspatial domain can also be assigned to the same\ngroup. In addition, masks produced by C are uti-\nlized to guide the patch grouping process, pro-\nviding a heuristics decision. Moreover, the con-\ngruence constraints between the two branches are\nalso exploited to maintain the spatial consistency\namong the patches. Overall, we stack the multi-\nstage process in a pyramid way. Experiments show\nthat GPWFormer outperforms the existing methods\nwith significant improvements on five benchmark\ndatasets.\n1 Introduction\nThe analysis of ultra-high resolution (UHR) geospatial im-\nage with millions or even billions of pixels has opened new\nhorizons for the computer vision community, playing an in-\ncreasingly important role in a wide range of geosciences and\nurban construction applications, such as disaster control, en-\nvironmental monitoring, land resource protection and urban\n∗Corresponding Author.\nplanning [Chen et al., 2019; Guo et al., 2022; Ji et al., 2023].\nThe focus of this paper is on semantic segmentation, provid-\ning a better understanding by assigning each pixel into a spec-\nified category.\nFully convolution neural networks (FCN) based methods\nhave driven rapid growth in segmentation for regular resolu-\ntion images, but overlook the feasibility of larger scale input.\nDue to the memory limitation, earliest works for UHR image\nsegmentation basically follow two paradigms: (1) downsam-\npling the image to a regular resolution, or (2) cropping the\nimage into small patches, feeding them to network sequen-\ntially and merging their predictions. Intuitively both the two\nparadigms will result in inaccurate results, the former loses\nmany local details while the latter lacks of global context. Af-\nter that, methods specially designed for UHR images are pro-\nposed and most of them follow the global-local collaborative\nframework to preserve both global and local information with\ntwo deep branches, taking the downsampled entire image and\ncropped local patches as inputs respectively. The most rep-\nresentative works are GLNet [Chen et al., 2019 ] and FCtL\n[Li et al., 2021]. Despite the considerable performance, their\nmemory cost is high and inference speed is very low, due to\nthe deep branches and sequentially inference. Later, follow-\ning the bilateral architecture[Yuet al., 2018], ISDNet [Guo et\nal., 2022] proposes to combine a shallow and a deep branch.\nThe shallow branch takes the whole UHR image as input and\nextracts multi-scale shallow features, while the deep branch\ntakes the highly downsampled image as input to extract one\ndeep feature, then the features are fused for final prediction\nwith a specially designed fusion module. This type of archi-\ntecture avoids local patches cropping and sequentially pre-\ndiction thus improves the inference speed with a large mar-\ngin, but also results in weaker performance, especially in lo-\ncal characterization, since the local details will never be fully\naddressed in the shallow branch with UHR input. In addi-\ntion, their proposed fusion module also introduces much ex-\ntra memory cost. In a word, due to enormous pixels, existing\nmethods for UHR segmentation usually struggle in the press-\ning dilemma of balancing memory cost and local characteri-\nzation accuracy.\nTo this end, in this paper we propose a novel Guided Patch-\nGrouping Wavelet Transformer (GPWFormer) network to ad-\ndress the above balance problem for UHR image segmenta-\ntion. In general, we formulate a hybrid CNN-Transformer\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n920\nframework in dual-branch style, where the Transformer\nbranch takes locally cropped UHR image as input and har-\nvests both local details and fine-grained long-range depen-\ndencies, while the CNN branch takes downsampled UHR\nimage as input for learning the category-wise deep context.\nFor the sake of high inference speed and low computation\ncomplexity, different from the classical global-local frame-\nwork, the local patches are fed into the Transformer branch\nall at once and dynamically grouped, then each group is fed\ninto a different Transformer head to extract local texture de-\ntails. Meanwhile fine-grained long-range dependencies are\nalso captured during this process, since patches that are far\naway in spatial domain can also be assigned to a same group.\nIn addition, inspired by [Yao et al., 2022 ], invertible down-\nsampling operations with dense wavelets are integrated into\nthe Transformer for lightweight memory cost, and masks pro-\nduced by CNN branch are utilized to guide the patch group-\ning process, providing a heuristics decision. Moreover, the\ncongruence constraint between the two branches are also ex-\nploited to maintain the spatial consistency among the patches.\nIn general, we stack the multi-stage process in a pyramid way.\nOverall, our contributions are summarized as follows:\n• We propose a novel Patch-Grouping Wavelet Trans-\nformer (GPWFormer) network for ultra-high resolu-\ntion image segmentation, which is a hybrid CNN-\nTransformer in dual-branch style to harvest fine-grained\nboth low-level and high-level context simultaneously in\nan efficient way.\n• Specifically, we introduce a Wavelet Transformer to\nintegrate the Transformer branch with dense wavelets\nfor lightweight memory cost, and further decrease its\ncomputation complexity with heuristics grouping masks\nguided by the corresponding deep features of CNN\nbranch.\n• Moreover, the congruence constraints between the two\nbranches are also exploited to maintain the spatial con-\nsistency among the patches.\n• Extensive experiments demonstrate that the proposed\nGPWFormer obtains an excellent balance between\nmemory cost and local segmentation accuracy, and out-\nperforms the existing methods with significant improve-\nments on five public datasets.\n2 Related Work\n2.1 Generic Semantic Segmentation\nDeep learning based methods have taken a big step forward\non computer vision [Goodfellow et al., 2016; He et al., 2016;\nJi et al., 2019; Feng et al., 2018; Wang et al., 2021a;\nWang et al. , 2021b; Ji et al. , 2021 ]. The development of\ndeep CNN and Transformer over the past few years has driven\nrapid growth of methods in generic semantic segmentation\nfor natural images and daily photos [Hu et al., 2020; Chen\net al., 2018; Zhu et al., 2021 ]. Earlier semantic segmenta-\ntion models for generic images was mainly based on the fully\nconvolutional networks (FCN) [Long et al., 2015 ] and re-\ncent ones followed the design of Transformer network. FCN-\nbased methods usually relied on large receptive field and fine-\ngrained deep features, such as DeepLab [Chen et al., 2017;\nChen et al., 2018], DANet[Fu et al., 2019], OCRNet [Yuanet\nal., 2020a]. While Transformer-based networks viewed seg-\nmentation as a Sequence-to-Sequence perspective and have\nbecome a new research hotspot. Representative works in-\ncluded SETR [Zheng et al., 2021 ] and Swin [Liu et al.,\n2021]. However, Transformer networks usually took amounts\nof memory cost and computation complexity, limiting its de-\nvelopment on UHR image segmentation. In this paper, we\naim to take advantage of the strong representation ability of\nTransformer meanwhile decrease its memory cost for UHR\nsegmentation. In addition, knowledge distillation methods\n[Ji et al., 2022] have also been applied to make the network\nlightweight.\n2.2 Ultra-High Resolution Image Segmentation\nBenefited from the advancement of photography and sensor\ntechnologies, the accessibility and analysis of ultra-high res-\nolution geospatial images has opened new horizons for the\ncomputer vision community, playing an increasingly impor-\ntant role in a wide range of geosciences and urban construc-\ntion applications, including but not limited to disaster control,\nenvironmental monitoring, land resource protection and ur-\nban planning [Ji et al., 2023]. According to [Ascher and Pin-\ncus, 2007; Chen et al., 2019], an image with at least4.1×106\npixels can reach the minimum bar of ultra-high definition me-\ndia, usually deriving from a wide range of scientific applica-\ntions, for example, geospatial and histopathological images.\nFor ultra-high resolution image segmentation, CascadePSP\n[Cheng et al., 2020] proposed to improve the coarse segmen-\ntation results with a pretrained model to generate high-quality\nresults. GLNet [Chen et al., 2019 ] incorporated both global\nand local information deeply in a two-stream branch manner.\nFCtL [Li et al., 2021] exploited a squeeze-and-split structure\nto fuse multi-scale features information. ISDNet [Guo et al.,\n2022] integrated the shallow and deep networks. These exist-\ning works lack of a further in-depth analysis and have obvious\ndrawbacks analyzed in the Introduction section. WSDNet [Ji\net al., 2023] was proposed as an efficient and effective frame-\nwork for UHR segmentation especially with ultra-rich con-\ntext.\n3 Method\nIn this section, we introduce the proposed Guided Patch-\nGrouping Wavelet Transformer (GPWFormer) in detail.\nFirstly, we introduce the overall structure. Subsequently, we\nillustrate the Guided Patch-Grouping strategy and Wavelet\nTransformer respectively. Next the details of spatial congru-\nence are explained.\n3.1 Overview\nFigure 1 shows the overall framework of the proposed GPW-\nFormer, which consists of dual branches. The upper branch is\na lightweight Transformer network taking the original UHR\nimage as input to harvest both local structured details and\nfine-grained long-range spatial dependencies, while the lower\nis a deep CNN network taking the downsampled UHR image\nas input to learn category-wise deep context. For simplicity,\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n921\nStage1\nDWT\nDWT\ngroup \nguidance\nre-cropping\npatch \ncropping\nWFormer\nWFormer\nWFormer\nWFormer\nWFormer\nWFormer\nWFormer\nWFormer\nDWT\ngrouping\nCongruence\ngrouping\nrestore &\ngroup \nguidance\nASPP\nIWT\nIWT\nIWT\nrestore\nfusion\nCongruence\nStage2\n-branch\n-branch\nWFormer\nWFormer\nWFormer\nWFormer\nFigure 1: The overview of the proposed GPWFormer. Upper: Transformer branch (T ); Lower: CNN branch (C ). In each stage of T , the\ninput UHR image/feature is evenly partitioned into patches and fed into the network all at once. Then, they are grouped guided by a mask\ngenerated from the corresponding features in C. Next, intra-group relations in each group are learned with their respective WFormer heads.\nIn C, DeeplabV3+ (ResNet18) integrated with pyramid wavelets is employed, taking downsampled UHR image with wavelet transform as\ninput to capture deep category-wise context (“ResNet-S1” means the first stage of ResNet and so on). After each stage ofT , we maintain the\ninter-patch relation consistency with a congruence constraint byC. Finally, the outputs of T and C are fused and supervised with a focal loss.\nwe denote the two branches asT and C respectively. In C, any\nclassical generic segmentation architecture can be utilized,\nhere DeepLabV3+ with a lightweight backbone ResNet18 is\nemployed. Besides, in order to further reduce its computation\ncomplexity, we integrate each stage of ResNet18 with dis-\ncrete wavelet transform (DWT) to reduce the dimension of\nintermediate features, followed by the Atrous Spatial Pyra-\nmid Pooling (ASPP) module and inverse wavelet transform\n(IWT).\nIn T , the input UHR image is firstly evenly partition into\nlocal patches. Different from existing global-local frame-\nworks that take into patches sequentially, all the patches are\nfed into T at once in our framework, which can greatly ac-\ncelerate the inference process. In order to decrease the mem-\nory cost in this situation, we divide these patches into mul-\ntiple groups, then intra-group relations for each group are\nlearned with a corresponding shallow Wavelet Transformer\n(WFormer) head respectively, which is proposed for more ef-\nfective and efficient learning. The relations consist of both\nlocal structured relations and long-range dependencies, since\nboth spatially adjacent and nonadjacent patches can be as-\nsigned to a same group. Meanwhile all the low-level pixel-\nwise details are also captured during the process. Multiple\nstages can be stacked in a pyramid way for fully characteriza-\ntion, and the stage number is set to 2 here for higher inference\nspeed, which can also be set to a flexible number according to\nthe practical needs. The number of groups among stages are\nset in a pyramid manner to obtain representations of different\ngranularity. It is worthy noted that the corresponding features\nfrom C are employed to guide the patch-grouping, providing a\nheuristics decision. Moreover, after the learning of each stage\nin T , we also add a congruence constraint to the inter-patch\nrelations by the corresponding stage of C to maintain the spa-\ntial consistency. Finally the output of T and C are fused and\nsupervised with a focal loss.\n3.2 Guided Patch-Grouping\nIn each stage ofT , we produce a guidance for patch-grouping\nfrom the corresponding feature from C, as shown in Figure\n1, and the first stage is taken as example for the following\nillustration. Given the UHR input I ∈ RHI×WI in T , we\nevenly partition it into m ×n patches, which are then divided\ninto G groups for the subsequent WFormer heads. Let h, w\ndenote the height and width of each patch respectively. Our\naim is to produce a guidance mask M ∈ Rm×n from the C,\nMi,j ∈ [1, G] denotes the group index of patch (i, j), where\n(i ∈ [1, m], j∈ [1, n]).\nConcretely, we employ the low-frequency subbandFLL ∈\nRC1×H1×W1 of wavelet transform for the feature after\nResNet-S1 in C to produce M, since low-frequency subband\nis able to preserve more spatial details. C1, H1, and W1 de-\nnote the channel, height, width of FLL respectively. Firstly,\nchannel of FLL is decreased to G with an intuitive PCA\nmethod instead of a typical convolution operator, so that the\nwhole mask-producing process is not necessary to be learn-\nable, thereby reducing the amount of calculation, while avoid-\ning the design of complex forward propagation and gradient\nbackward processes. Next following the same partition oper-\nations for I in T , we apply a patch-wise average pooling to\nFLL, resulting in the mask feature FM ∈ RG×m×n. For-\nmally, the process of patch-wise average pooling is denoted\nas,\nFM\ni,j = GAP(FLL\n{i×h1 : (i+1)×h1, j×w1 : (j+1)×w1})\nh1 = H1/m, w1 = W1/n, i∈ [1, m], j∈ [1, n],\n(1)\nwhere GAP(·) denotes global average pooling. The above\nprocess can also be viewed as an average pooling with\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n922\nConv\nDWT\nConv\nDWT\nConv\nDWT\nConv\nDWT\nConcat\nProjection\nConcat\nProjection\nConcat\nProjection\nConcat\nProjection\nWavelet Transformer Layer\nWavelet Transformer Layer\n…\nDecoder\nFigure 2: The details of WFormer.\nrow stride = h1 and column stride = w1. Then we ap-\nply a softmax function to FM , generate the score mask\nS ∈ RG×m×n, and argmax S along the last two dimension to\ngenerate the mask M,\nM = arg max\nm,n\n(S). (2)\nNoted that M may not divide the groups evenly, that is,\nnot all groups may have equal number of patches, and some\ngroups may have more than m×n\nG patches, thus we sort the\npatches in descending order of scores in these groups re-\nspectively, then re-distribute their last extra patches to other\ngroups based on scores.\n3.3 Wavelet Transformer\nIn our framework, the input to each Transformer head is a\ngroup of patch features. Taking any group in first stage of T\nas example for illustration, let {Fu ∈ Rh×w×C} denote the\nset of input patch features in the group, where u is patch in-\ndex. Following the design of [Yao et al., 2022], we introduce\nthe Wavelet Transformer to make the network lightweight.\nFor each Fu, we first reduce its channel to C/4 with a convo-\nlution layer, then 2D DWT is utilized to decompose its four\nwavelet subbands {FLL\nu , FLH\nu , FLL\nu , FLL\nu } with four filters\nfLL, fLH, fHL, fHH . The dimension of both the subbands\nis R\nh\n2 ×w\n2 ×C\n4 . FLL\nu is the low-frequency subbands while the\nothers are high-frequency ones, next the four subbands are\nconcatenated to ˘Fu ∈ R\nh\n2 ×w\n2 ×C. By this way, the dimen-\nsion of all patch features are reduced so that the computa-\ntion complexity is squarely decreased. Noted that despite the\ndownsampling operation is deployed, due to the biorthogonal\nproperty of DWT, the original feature can be accurately re-\nconstructed by the inverse wavelet transform (IWT) in sub-\nsequent steps. Finally, { ˘Fu} is projected into a sequence\n˘F ∈ RL×C and embedded spatial information as previous\ngeneral Transformer networks [Zheng et al., 2021], and for-\nmulated as input to corresponding Transformer head.\nAs shown in Figure 2, following the general design of\nTransformer for segmentation, WFormer also contains an en-\ncoder and a decoder, where encoder consists of Le layers of\nmulti-head self-attention (MSA) and MLP blocks. The input\nto self-attention of each layerl is in a triplet of (Q, K, V), cal-\nculated by the current input Y l−1 ∈ RL×C (the input for the\nfirst layer is ˘F), with three learn-able linear projection layers\nWQ, WK, WV ∈ RC×d (d is the dimension), as:\nQ = Y l−1WQ, K= Y l−1WK, V= Y l−1WV . (3)\nNoted that the computational complexity of ordinary Self-\nAttention (SA) grows quadratically with dimensions, so we\nfurther apply the similar procedure of wavelet transform to\nK and V to decrease the size, denoted as ˘K and ˘V , by\nwhich we propose the Wavelet-SA (WSA) and Wavelet-MSA\n(WMSA), formulated as:\nW SA(Y l−1) = Y l−1 + softmax(Q ˘K⊤\n√\nd\n)( ˘V ), (4)\nWM\nSA(Y l−1) = [SA1(Y l−1), SA2(Y l−1), ..., SAm(Y l−1)]WO,\n(5)\nwhere m is number\nof WSA operations in WMSA, WO is\nthe transformation matrix. Then IWT is utilized to restore the\nsame dimension of output as the input. In the light of basic\ntheory of DWT and IWT, WFormer is able to harvest stronger\nboth local details and spatial long-range dependencies with-\nout loss of essential information, compared to Transformer\nwith traditional pooling operations.\n3.4 Spatial Consistency Congruence\nWith multiple Transformer heads, the patches of input feature\nare learned in groups, which affects the translation-invariant\nand results in the spatial in-consistency among the patches.\nSince C learns the input image globally and is able to preserve\nthe overall spatial consistency, we propose to utilize congru-\nence constraints from C to each stage of T , to maintain the\nspatial congruence of inter-patch relations in T .\nConsidering the stage s in T , given the patch features set\n{PTs\nk } (k ∈ [1, ms × ns]) after WFormer heads, where\nms × ns is the number of patches in stage s of T (denoted\nas Ts), we first calculate their inter-patch relationsCT\ns , which\ndefines as the sum of relations between any two patch centers.\nThe center of a patch is defined as the average of its all pixel\nfeatures. To capture the high-order relations in an efficient\nmanner, inspired by the theory of SVM [Han et al., 2012 ],\nwe employ the metric with Gaussian Radial Based Function\n(RBF) kernel to calculate the inter-patch relations. Therefore\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n923\nCT\ns can be formulated as,\nCT\ns =\nms×nsX\nk1=1\nms×nsX\nk2=1\nR(A(PTs\nk1\n), A(PTs\nk2\n)), (6)\nwhere R(·), A(·) are the RBF kernel and Average function\nrespectively. To simplify the calculation, we replace the RBF\ncalculation with its sum of T-order Taylor series,\nR(A(PTs\nk1\n),A(PTs\nk2\n)) = e−θ·||A(PTs\nk1 )−A(PTs\nk2 )||2\n=\nTX\nt=0\n(2θ)t\nt! (A(PTs\nk1\n) · A(PTs\nk2\n)⊤)te−2θ,\n(7)\nwhere θ is temperature parameter, ⊤ is matrix transposition\noperator.\nSimilarly, given the feature in corresponding stage ofC, we\npartition it as the same style of {PTs\nk }, and calculate its inter-\npatch relations CC\ns , so the spatial consistency constraint can\nbe formulated,\nC =\nX\ns\n1\nms × ns\n||CC\ns − CT\ns ||2\n2, (8)\nwhich is implemented as a loss cooperating with the main\nfocal loss.\nSo the overall loss L is a weighted combination of a main\nfocal loss and the above spatial consistency constraint,\nL = Lfocal + αC, (9)\nwhere α is the loss weight and set to 0.8.\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nIn order to validate the effectiveness of our proposed method\nin a wide perspective, we perform experiments on five\ndatasets, including DeepGlobe, Inria Aerial, Cityscapes, ISIC\nand CRAG.\nDeepGlobe\nThe DeepGlobe dataset [Demir et al., 2018 ] have 803 UHR\nimages, with 455/207/142 images for training, validation and\ntesting. Each image contains 2448 × 2448 pixels and the\ndense annotation contains seven classes of landscape regions.\nInria Aerial\nThe Inria Aerial Challenge dataset [Maggiori et al., 2017 ]\nhas 180 UHR images captured from five cities. Each image\ncontains 5000 × 5000 pixels and is annotated with a binary\nmask for building/non-building areas, with 126/27/27 images\nfor training, validation and testing.\nCityScapes\nThe Cityscapes dataset [Cordts et al., 2016 ] has 5,000 im-\nages of 19 semantic classes, with 2,979/500/1,525 images for\ntraining, validation and testing.\nISIC\nThe ISIC Lesion Boundary Segmentation Challenge dataset\n[Tschandl et al., 2018 ] contains 2596 UHR images, with\n2077/260/259 images for training, validation and testing.\nCNN Trans\nformer P.G. Wave. Cong. mIoU\n(%)\nMem\n(M)\n✓ 62.7 -\n✓ ✓ ✓ ✓ 73.6 -\n✓ ✓ ✓ ✓ ✓ 75.8 2380\n✓ ✓ ✓ ✓ 74.5 2380\n✓ ✓ ✓ ✓ 75.9 3370\n✓ ✓ ✓ 76.1 6090\nTable 1: Effectiveness of each component in GPWFormer. “P.G.,\nWave., Cong.” indicate Patch-Grouping, Wavelet Transform, and\nSpatial Congruence, respectively.\nGrouping\nStrategy\nLinear\n(row)\nLinear\n(column) Rectangle Guided\nmIoU 74.1 74.0 74.8 75.8\nTable 2: Comparisons of patch-grouping strategy.\nCRAG\nCRAG [Graham et al., 2019] dataset includes two classes and\nexhibits different differentiated glandular morphology, with\n173/40 images for training/testing. Their average size is 1512\n× 1516.\nEvaluation Metrics\nIn all experiments, we adopt the mIoU, F1 score, Accuracy\nand memory cost to study the effectiveness.\n4.2 Implementation Details\nWe use the MMSegmentation codebase [MMSegmentation,\n2022] following the default augmentations without bells and\nwhistles, and train on a server with Tesla V100 GPUs with\nbatch size 8. During the image cropping of Transformer\nbranch, the size of patches are500×500 pixels and neighbor-\ning patches have 120 × 580 pixels overlap region in order to\navoid boundary vanishing. The input image of CNN branch\nis also downsampled to size 500 × 500 to trade-off perfor-\nmance and efficiency. We employ three layers in WFormer\nencoder and pre-train the Transformer on the Imagenet-1K\ndataset. During training, Focal loss [Lin et al., 2017 ] with\nλ = 2 is used to supervise training, and Adam [Kingma and\nBa, 2014] optimizer is used in the optimization process. We\nset the initial learning rate to 5 × 10−5, and it is decayed\nby a poly learning rate policy where the initial learning rate\nis multiplied by (1 − iter\ntotal iter )0.9 after each iteration. The\nmaximum iteration number set to 40k, 80k, 160k and 80k for\nInria Aerial, DeepGlobe, Cityscapes and ISIC respectively.\nWe use the command line tool “gpustat” to measure the GPU\nmemory, with the mini-batch size of 1 and avoid calculating\nany gradients.\n4.3 Ablation Study\nIn this section, we delve into the modules and settings of our\nproposed model and demonstrate their effectiveness. All the\nablation studies are performed on DeepGlobe test set.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n924\nMetric Mean Inner-Dot RBF\norder-1\nRBF\norder-2\nRBF\norder-3\nmIoU 74.7 74.7 75.0 75.5 75.8\nTable 3: The impact of spatial congruence.\nMethod Average\nPooling\nTransposed\nConvolution\nWavelet\nTransform\nmIoU 74.8 75.0 75.8\nMem 2219 2701 2380\nTable 4: Comparisons of downsampling methods.\nEffectiveness of GPWFormer\nWe conduct experiments to verify the effectiveness of dif-\nferent components in GPWFormer, as shown in Table 1.\nFirstly, only CNN branch or Transformer branch will lead to\nlightweight memory cost but lower performance, indicating\nthe effectiveness of the hybrid CNN-Transformer architec-\nture. Then dual-branch with all components achieves impres-\nsive and balanced results. Removing “Spatial Congruence”\nmodule leads to a lower mIoU since the spatial consistency\namong groups is mismatched, while memory cost is steady\nas this module is only used in training. Using an ordinary\nTransformer without wavelets has a less memory, showing\nthat WFormer is able to improve the efficiency while main-\ntaining a comparable performance. Applying WFormer on\nthe whole image results in better performance while severely\ndegraded inference speed, proving that patch-grouping strat-\negy indeed decreases the computation complexity.\nComparisons of Patch-Grouping Strategies\nWe show the superiority of the proposed guided patch-\ngrouping strategy in Table 3, and several other strategies are\nalso utilized for comparison. “Linear (row)” and “Linear\n(column)” mean grouping the patches in order of rows and\ncolumns respectively, and “Rectangle” means in the order of\nrectangular boxes. Experiments show that guided grouping\nshows best performance, proving that CNN branch indeed\nprovides an effective guidance for patch grouping of Trans-\nformer branch.\nComparisons of Downsampling Methods in WFormer\nWe show the comparison of self-attention block with dif-\nferent downsampling methods in Table 4. Classical self-\nattention with ordinary average pooling or transposed convo-\nlution shows much lower performance, since both of them are\nirreversible and lose much information during the downsam-\npling operations, while wavelet transform is invertible thus\nall the information can be persevered. A self-attention block\nwith transposed convolution achieves a high mIoU than with\naverage pooling, but also introducing extra memory cost.\nThe Impact of Settings in Spatial Congruence\nThe comparison of different congruence methods is shown in\nTable 3. Here besides the Gaussian RBF kernel, we also im-\nplement some other metrics, including “Mean” and “Inner-\nDot”. The former indicates calculating inter-patch relation of\nGeneric Model mIoU\n(%)↑\nF1\n(%)↑\nAcc\n(%)↑\nMem\n(M)↓\nLocal Inference\nU-Net 37.3 - - 949\nDeepLabv3+ 63.1 - - 1279\nFCN-8s 71.8 82.6 87.6 1963\nGlobal Inference\nU-Net 38.4 - - 5507\nICNet 40.2 - - 2557\nPSPNet 56.6 - - 6289\nDeepLabv3+ 63.5 - - 3199\nFCN-8s 68.8 79.8 86.2 5227\nBiseNetV1 53.0 - - 1801\nDANet 53.8 - - 6812\nSTDC 70.3 - - 2580\nUHR Model\nCascadePSP 68.5 79.7 85.6 3236\nPPN 71.9 - - 1193\nPointRend 71.8 - - 1593\nMagNet 72.9 - - 1559\nMagNet-Fast 71.8 - - 1559\nGLNet 71.6 83.2 88.0 1865\nISDNet 73.3 84.0 88.7 1948\nFCtL 73.5 83.8 88.3 3167\nWSDNet 74.1 85.2 89.1 1876\nGPWFormer (Ours) 75.8 85.4 89.9 2380\nTable 5: Comparison with state-of-the-arts on DeepGlobe test set.\ntwo patches as distance between their mean feature, while the\nlatter as the Inner-Product. Experiments shown that Gaussian\nRBF is more flexible and powerful in capturing the complex\nnon-linear relationship between high-dimensional patch fea-\ntures.\n4.4 Comparison with State-of-the-Arts\nIn this section, we compare the proposed framework with\nexisting state-of-the-art methods, including U-Net [Ron-\nneberger et al., 2015], ICNet [Zhao et al., 2018 ], PPN [Wu\net al., 2020 ], PSPNet [Zhao et al., 2017 ], SegNet [Badri-\nnarayanan et al., 2017 ], DeepLabv3+ [Chen et al., 2018 ],\nFCN-8s [Long et al., 2015 ], CascadePSP [Cheng et al.,\n2020], BiseNet [Yu et al., 2018 ], PointRend [Kirillov et al.,\n2020], DenseCRF [Kr¨ahenb¨uhl and Koltun, 2011], DGF [Wu\net al., 2018 ], DANet [Fu et al., 2019 ], SegFix [Yuan et al.,\n2020b], MagNet [Huynh et al., 2021 ], STDC [Fan et al.,\n2021], GLNet [Chen et al., 2019 ], FCtL [Li et al., 2021 ],\nISDNet [Guo et al., 2022 ] and WSDNet [Ji et al., 2023 ],\non DeepGlobe, Inria Aerial, Cityscapes, ISIC and CRAG\ndatasets, in terms of mIOU (%), F1 (%), Accuracy (%), Mem-\nory Cost (M).\nSome of these methods are specially designed for UHR im-\nages (denoted as “UHR Model”) and the others are not (de-\nnoted as “Generic Model”). We show the results of “Generic\nModel” on both “Global Inference” and “Local Inference”.\nThe former obtains the prediction with downsampled global\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n925\nGeneric Model mIoU\n(%)↑\nF1\n(%)↑\nAcc\n(%)↑\nMem\n(M)↓\nDeepLabv3+ 55.9 - - 5122\nFCN-8s 69.1 81.7 93.6 2447\nSTDC 72.4 - - 7410\nUHR Model\nCascadePSP 69.4 81.8 93.2 3236\nGLNet 71.2 - - 2663\nISDNet 74.2 84.9 95.6 4680\nFCtL 73.7 84.1 94.6 4332\nWSDNet 75.2 86.0 96.0 4379\nGPWFormer (Ours) 76.5 86.2 96.7 4710\nTable 6: Comparison with state-of-the-arts on Inria Aerial test set.\nGeneric Model mIoU (%)↑ Mem (M)↓\nBiseNetV1 74.4 2147\nBiseNetV2 75.8 1602\nPSPNet 74.9 1584\nDeepLabv3 76.7 1468\nUHR Model\nDenseCRF 62.9 1575\nDGF 63.3 1727\nSegFix 65.8 2033\nMagNet 67.6 2007\nMagNet-Fast 66.9 2007\nISDNet 76.0 1510\nGPWFormer (Ours) 78.1 1897\nTable 7: Comparison with state-of-the-arts on Cityscapes test set.\nimage, and the latter obtains the prediction with local cropped\npatches sequentially and then merges their results by post-\nprocessing.\nDeepGlobe\nAs shown in Table 5, we first compare GPWFormer with\nabove-mentioned methods on DeepGlobetest dataset. Due to\nthe diversity of land cover types and the high density of an-\nnotations, this dataset is very challenging. The experiments\nshow that GPWFormer outperforms all other methods on both\nmIoU, F1 and Accuracy. Specifically, we outperform GLNet,\nISDNet and FCtL by large margins on mIoU respectively,\ndirectly showing the segmentation effectiveness and perfor-\nmance improvement. Besides, the categories in the dataset\nare often seriously unbalanced distributed, so we exploit\nthe F1 score and Accuracy metrics to reflect the improve-\nments and experiment results show the proposed method also\nachieves the highest scores among all the models. With\nsuch impressive performance, our methods is economic in the\nmemory cost, attaining an excellent balance among accuracy\nand memory cost.\nInria Aerial\nWe also show the comparisons on Inria Aerial test dataset in\nTable 6. This dataset is more challenging, since the number\nMethod\nISIC CRAG\nmIoU (%) mIoU (%)\nPSPNet 77.0 88.6\nDeepLabV3+ 70.5 88.9\nDANet 51.4 82.3\nGLNet 75.2 85.9\nGPWFormer (Ours) 80.7 89.9\nTable 8: Comparison with state-of-the-arts on CRAG and ISIC test\nset.\nof pixels for each image reaches 25 million, which is around\nfour times than DeepGlobe, and the foreground regions are\nalso finer. Experiment results show that GPWFormer outper-\nforms GLNet, ISDNet and FCtL by large margins again on\nmIOU respectively, with comparable memory cost.\nCityscapes\nTo further validate the generality of our method, we also show\nthe results on Cityscapes dataset, as shown in Table 7. GPW-\nFormer also outperforms all other methods on mIoU, with a\nbright results on memory cost.\nISIC and CRAG\nThe image resolution of ISIC is comparable to Inria Aerial,\nand CRAG is of lower image resolution than other datasets.\nTable 8 shows the experimental results. GPWFormer once\nachieves excellent performances.\n5 Conclusion\nIn this paper, we focus on the ultra-high resolution image\nsegmentation and develop a novel Guided Patch-Grouping\nWavelet Transformer network. We firstly analyze the limita-\ntions of existing state-of-the-art methods, and the unique dif-\nficulties of ultra-high resolution image segmentation. With\nCNN-Transformer dual-branch, we propose the Wavelet\nTransformer with a Guided Patch-Grouping strategy to learn\nlocal details and long-range spatial dependencies simulta-\nneously, while the CNN branch takes the downsampled in-\nput UHR image with wavelet transform to capture deep\ncategory-wise context. Moreover, the congruence constraint\nis also introduced to maintain the spatial consistency from\nCNN branch to Transformer branch. Our proposed frame-\nwork achieves new state-of-the-art results on five benchmark\ndatasets.\nAcknowledgements\nThis work was supported by the JKW Research Funds un-\nder Grant 20-163-14-LZ-001-004-01, Anhui Provincial Natu-\nral Science Foundation under Grant 2108085UD12, National\nKey R&D Program of China under Grant 2020AAA0103902,\nNSFC (No. 62176155), Shanghai Municipal Science and\nTechnology Major Project, China (2021SHZDZX0102). We\nacknowledge the support of GPU cluster built by MCC Lab\nof Information Science and Technology Institution, USTC.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n926\nReferences\n[Ascher and Pincus, 2007] Steven Ascher and Edward Pin-\ncus. The filmmaker’s handbook: A comprehensive guide\nfor the digital age. Penguin, 2007.\n[Badrinarayanan et al., 2017] Vijay Badrinarayanan, Alex\nKendall, and Roberto Cipolla. Segnet: A deep convolu-\ntional encoder-decoder architecture for image segmenta-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39(12):2481–2495, 2017.\n[Chen et al., 2017] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 40(4):834–848, 2017.\n[Chen et al., 2018] Liang-Chieh Chen, Yukun Zhu, George\nPapandreou, Florian Schroff, and Hartwig Adam.\nEncoder-decoder with atrous separable convolution for se-\nmantic image segmentation. In European Conference on\nComputer Vision, pages 801–818, 2018.\n[Chen et al., 2019] Wuyang Chen, Ziyu Jiang, Zhangyang\nWang, Kexin Cui, and Xiaoning Qian. Collaborative\nglobal-local networks for memory-efficient segmentation\nof ultra-high resolution images. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8924–\n8933, 2019.\n[Cheng et al., 2020] Ho Kei Cheng, Jihoon Chung, Yu-Wing\nTai, and Chi-Keung Tang. Cascadepsp: Toward class-\nagnostic and very high-resolution segmentation via global\nand local refinement. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 8890–8899,\n2020.\n[Cordts et al., 2016] Marius Cordts, Mohamed Omran, Se-\nbastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele.\nThe cityscapes dataset for semantic urban scene under-\nstanding. In IEEE Conference on Computer Vision and\nPattern Recognition, 2016.\n[Demir et al., 2018] Ilke Demir, Krzysztof Koperski, David\nLindenbaum, Guan Pang, Jing Huang, Saikat Basu, For-\nest Hughes, Devis Tuia, and Ramesh Raskar. Deepglobe\n2018: A challenge to parse the earth through satellite im-\nages. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition Workshops, pages\n172–181, 2018.\n[Fan et al., 2021] Mingyuan Fan, Shenqi Lai, Junshi Huang,\nXiaoming Wei, Zhenhua Chai, Junfeng Luo, and Xiaolin\nWei. Rethinking bisenet for real-time semantic segmenta-\ntion. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9716–9725, 2021.\n[Feng et al., 2018] Weitao Feng, Deyi Ji, Yiru Wang,\nShuorong Chang, Hansheng Ren, and Weihao Gan. Chal-\nlenges on large scale surveillance video analysis. In IEEE\nConference on Computer Vision and Pattern Recognition\nWorkshops, pages 69–76, 2018.\n[Fu et al., 2019] Jun Fu, Jing Liu, Haijie Tian, Yong Li,\nYongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual at-\ntention network for scene segmentation. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 3146–3154, 2019.\n[Goodfellow et al., 2016] Ian Goodfellow, Yoshua Bengio,\nand Aaron Courville. Deep learning. MIT press, 2016.\n[Graham et al., 2019] Simon Graham, Hao Chen, Jevgenij\nGamper, Qi Dou, Pheng-Ann Heng, David Snead,\nYee Wah Tsang, and Nasir Rajpoot. Mild-net: Minimal in-\nformation loss dilated network for gland instance segmen-\ntation in colon histology images. Medical Image Analysis,\n52:199–211, 2019.\n[Guo et al., 2022] Shaohua Guo, Liang Liu, Zhenye Gan,\nYabiao Wang, Wuhao Zhang, Chengjie Wang, Guannan\nJiang, Wei Zhang, Ran Yi, Lizhuang Ma, and Ke Xu. Is-\ndnet: Integrating shallow and deep networks for efficient\nultra-high resolution segmentation. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n4361–4370, June 2022.\n[Han et al., 2012] Shunjie Han, Cao Qubo, and Han Meng.\nParameter selection in svm with rbf kernel function. In\nWorld Automation Congress, pages 1–4. IEEE, 2012.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 770–778, 2016.\n[Hu et al., 2020] Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai\nBai, Wei Wu, and Junjie Yan. Class-wise dynamic graph\nconvolution for semantic segmentation. In European Con-\nference on Computer Vision, pages 1–17. Springer, 2020.\n[Huynh et al., 2021] Chuong Huynh, Anh Tuan Tran, Khoa\nLuu, and Minh Hoai. Progressive semantic segmentation.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16755–16764, 2021.\n[Ji et al., 2019] Deyi Ji, Hongtao Lu, and Tongzhen Zhang.\nEnd to end multi-scale convolutional neural network for\ncrowd counting. In Eleventh International Conference on\nMachine Vision, volume 11041, pages 761–766, 2019.\n[Ji et al., 2021] Deyi Ji, Haoran Wang, Hanzhe Hu, Weihao\nGan, Wei Wu, and Junjie Yan. Context-aware graph con-\nvolution network for target re-identification. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 1646–1654, 2021.\n[Ji et al., 2022] Deyi Ji, Haoran Wang, Mingyuan Tao, Jian-\nqiang Huang, Xian-Sheng Hua, and Hongtao Lu. Struc-\ntural and statistical texture knowledge distillation for\nsemantic segmentation. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 16876–\n16885, 2022.\n[Ji et al., 2023] Deyi Ji, Feng Zhao, Hongtao Lu, Mingyuan\nTao, and Jieping Ye. Ultra-high resolution segmentation\nwith ultra-rich context: A novel benchmark. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1–10, 2023.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n927\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[Kirillov et al., 2020] Alexander Kirillov, Yuxin Wu, Kaim-\ning He, and Ross Girshick. Pointrend: Image segmenta-\ntion as rendering. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9799–9808, 2020.\n[Kr¨ahenb¨uhl and Koltun, 2011] Philipp Kr ¨ahenb¨uhl and\nVladlen Koltun. Efficient inference in fully connected\ncrfs with Gaussian edge potentials. In Advances in Neural\nInformation Processing Systems, volume 24, pages 1–9,\n2011.\n[Li et al., 2021] Qi Li, Weixiang Yang, Wenxi Liu, Yuan-\nlong Yu, and Shengfeng He. From contexts to local-\nity: Ultra-high resolution image segmentation via locality-\naware contextual correlation. In IEEE/CVF International\nConference on Computer Vision, pages 7252–7261, 2021.\n[Lin et al., 2017] Tsung-Yi Lin, Priya Goyal, Ross Girshick,\nKaiming He, and Piotr Doll´ar. Focal loss for dense object\ndetection. In IEEE International Conference on Computer\nVision, pages 2980–2988, 2017.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In IEEE/CVF International Conference\non Computer Vision, pages 10012–10022, 2021.\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\nTrevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3431–3440, 2015.\n[Maggiori et al., 2017] Emmanuel Maggiori, Yuliya Tara-\nbalka, Guillaume Charpiat, and Pierre Alliez. Can se-\nmantic labeling methods generalize to any city? the inria\naerial image labeling benchmark. In IEEE International\nGeoscience and Remote Sensing Symposium, pages 3226–\n3229, 2017.\n[MMSegmentation, 2022] MMSegmentation. Mmseg-\nmentation: Openmmlab semantic segmentation toolbox\nand benchmark. https://github.com/open-mmlab/\nmmsegmentation, 2022. Accessed: 2022-08-16.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, pages 234–241. Springer, 2015.\n[Tschandl et al., 2018] Philipp Tschandl, Cliff Rosendahl,\nand Harald Kittler. The ham10000 dataset, a large col-\nlection of multi-source dermatoscopic images of common\npigmented skin lesions. Scientific Data, 5(1):1–9, 2018.\n[Wang et al., 2021a] Haoran Wang, Licheng Jiao, Fang Liu,\nLingling Li, Xu Liu, Deyi Ji, and Weihao Gan. Ipgn: Inter-\nactiveness proposal graph network for human-object inter-\naction detection. IEEE Transactions on Image Processing,\n30:6583–6593, 2021.\n[Wang et al., 2021b] Haoran Wang, Licheng Jiao, Fang Liu,\nLingling Li, Xu Liu, Deyi Ji, and Weihao Gan. Learn-\ning social spatio-temporal relation graph in the wild and a\nvideo benchmark. IEEE Transactions on Neural Networks\nand Learning Systems, pages 1–14, 2021.\n[Wu et al., 2018] Huikai Wu, Shuai Zheng, Junge Zhang,\nand Kaiqi Huang. Fast end-to-end trainable guided fil-\nter. In IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1838–1847, 2018.\n[Wu et al., 2020] Tong Wu, Zhenzhen Lei, Bingqian Lin,\nCuihua Li, Yanyun Qu, and Yuan Xie. Patch proposal net-\nwork for fast semantic segmentation of high-resolution im-\nages. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, page 12402–12409, 2020.\n[Yao et al., 2022] Ting Yao, Yingwei Pan, Yehao Li, Chong-\nWah Ngo, and Tao Mei. Wave-vit: Unifying wavelet and\ntransformers for visual representation learning. In Eu-\nropean Conference on Computer Vision, pages 328–345.\nSpringer, 2022.\n[Yu et al., 2018] Changqian Yu, Jingbo Wang, Chao Peng,\nChangxin Gao, Gang Yu, and Nong Sang. Bisenet: Bi-\nlateral segmentation network for real-time semantic seg-\nmentation. In European Conference on Computer Vision,\npages 325–341, 2018.\n[Yuan et al., 2020a] Yuhui Yuan, Xilin Chen, and Jingdong\nWang. Object-contextual representations for semantic seg-\nmentation. In European Conference on Computer Vision,\npages 173–190. Springer, 2020.\n[Yuan et al., 2020b] Yuhui Yuan, Jingyi Xie, Xilin Chen, and\nJingdong Wang. Segfix: Model-agnostic boundary refine-\nment for segmentation. In European Conference on Com-\nputer Vision, pages 489–506. Springer, 2020.\n[Zhao et al., 2017] Hengshuang Zhao, Jianping Shi, Xiao-\njuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene\nparsing network. In IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2881–2890, 2017.\n[Zhao et al., 2018] Hengshuang Zhao, Xiaojuan Qi, Xiaoy-\nong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time\nsemantic segmentation on high-resolution images. In Eu-\nropean Conference on Computer Vision, pages 405–420,\n2018.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, Hengshuang\nZhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu,\nJianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang.\nRethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6877–6886, 2021.\n[Zhu et al., 2021] Lanyun Zhu, Deyi Ji, Shiping Zhu, Wei-\nhao Gan, Wei Wu, and Junjie Yan. Learning statistical\ntexture for semantic segmentation. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n12537–12546, 2021.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n928",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7143348455429077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5840021967887878
    },
    {
      "name": "Segmentation",
      "score": 0.5682836174964905
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5078528523445129
    },
    {
      "name": "Transformer",
      "score": 0.4983401298522949
    },
    {
      "name": "Wavelet",
      "score": 0.47741591930389404
    },
    {
      "name": "Inference",
      "score": 0.4168124198913574
    },
    {
      "name": "Image resolution",
      "score": 0.4146175980567932
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ],
  "cited_by": 13
}