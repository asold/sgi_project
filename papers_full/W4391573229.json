{
  "title": "A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification",
  "url": "https://openalex.org/W4391573229",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2624037926",
      "name": "Madhumita Sushil",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A2890023934",
      "name": "Travis Zack",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A2740786905",
      "name": "Divneet Mandair",
      "affiliations": [
        "UCSF Helen Diller Family Comprehensive Cancer Center",
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A2105495501",
      "name": "Zhiwei Zheng",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2052909661",
      "name": "Ahmed Wali",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5061052929",
      "name": "Yan-Ning Yu",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5113113430",
      "name": "Yuwei Quan",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A3051019213",
      "name": "Atul Butte",
      "affiliations": [
        "University of California, San Francisco"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4310964482",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4379474412",
    "https://openalex.org/W4390510163",
    "https://openalex.org/W4389524025",
    "https://openalex.org/W4386865118",
    "https://openalex.org/W4385714479",
    "https://openalex.org/W4385682300",
    "https://openalex.org/W4388608412",
    "https://openalex.org/W4390704928",
    "https://openalex.org/W4383223222",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4297734170",
    "https://openalex.org/W4386532529",
    "https://openalex.org/W4214673031",
    "https://openalex.org/W4378711639",
    "https://openalex.org/W4225335710",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W4383605243",
    "https://openalex.org/W4289531747",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4388011452",
    "https://openalex.org/W4320486724",
    "https://openalex.org/W4312113143",
    "https://openalex.org/W4389205282",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4392619666",
    "https://openalex.org/W4378976598",
    "https://openalex.org/W2098690513",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4385728371",
    "https://openalex.org/W4388525110",
    "https://openalex.org/W2153222072",
    "https://openalex.org/W4388007786",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W4391973028",
    "https://openalex.org/W4390745503"
  ],
  "abstract": null,
  "full_text": "A comparative study of zero-shot inference with\nlarge language models and supervised modeling in\nbreast cancer pathology classi\u0000cation\nMadhumita Sushil  (  madhumita.sushil@ucsf.edu )\nBakar Computational Health Sciences Institute, University of California, San Francisco, San Francisco,\nCA, USA https://orcid.org/0000-0001-7884-0526\nTravis Zack \nBakar Computational Health Sciences Institute, University of California, San Francisco, San Francisco,\nCA, USA\nDivneet Mandair \nHelen Diller Family Comprehensive Cancer Center, University of California, San Francisco, San\nFrancisco, CA, USA\nZhiwei Zheng \nUniversity of California, Berkeley, CA, USA\nAhmed Wali \nUniversity of California, Berkeley, CA, USA\nYan-Ning Yu \nUniversity of California, Berkeley, CA, USA\nYuwei Quan \nUniversity of California, Berkeley, CA, USA\nAtul Butte \nBakar Institute for Computational Health Sciences, University of California, San Francisco\nhttps://orcid.org/0000-0002-7433-2740\nArticle\nKeywords:\nPosted Date: February 6th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3914899/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: There is a con\u0000ict of interest MS, TZ, DM, ZZ, AW, YY, and YQ report no \u0000nancial\nassociations or con\u0000icts of interest. AJB is a co-founder and consultant to Personalis and NuMedii;\nconsultant to Mango Tree Corporation, and in the recent past, Samsung, 10x Genomics, Helix, Pathway\nGenomics, and Verinata (Illumina); has served on paid advisory panels or boards for Geisinger Health,\nRegenstrief Institute, Gerson Lehman Group, AlphaSights, Covance, Novartis, Genentech, and Merck, and\nRoche; is a shareholder in Personalis and NuMedii; is a minor shareholder in Apple, Meta (Facebook),\nAlphabet (Google), Microsoft, Amazon, Snap, 10x Genomics, Illumina, Regeneron, Sano\u0000, P\u0000zer, Royalty\nPharma, Moderna, Sutro, Doximity, BioNtech, Invitae, Paci\u0000c Biosciences, Editas Medicine, Nuna Health,\nAssay Depot, and Vet24seven, and several other non-health related companies and mutual funds; and\nhas received honoraria and travel reimbursement for invited talks from Johnson and Johnson, Roche,\nGenentech, P\u0000zer, Merck, Lilly, Takeda, Varian, Mars, Siemens, Optum, Abbott, Celgene, AstraZeneca,\nAbbVie, Westat, and many academic institutions, medical or disease speci\u0000c foundations and\nassociations, and health systems. Atul Butte receives royalty payments through Stanford University, for\nseveral patents and other disclosures licensed to NuMedii and Personalis. Atul Butte’s research has been\nfunded by NIH, Peraton (as the prime on an NIH contract), Genentech, Johnson and Johnson, FDA, Robert\nWood Johnson Foundation, Leon Lowenstein Foundation, Intervalien Foundation, Priscilla Chan and\nMark Zuckerberg, the Barbara and Gerson Bakar Foundation, and in the recent past, the March of Dimes,\nJuvenile Diabetes Research Foundation, California Governor’s O\u0000ce of Planning and Research, California\nInstitute for Regenerative Medicine, L ’Oreal, and Progenity. None of these entities had any bearing on this\nresearch or interpretation of the \u0000ndings.\n \n1 \nA comparative study of zero-shot inference with large language \nmodels and supervised modeling in breast cancer pathology \nclassification \n \nMadhumita Sushil, PhD [0000-0001-7884-0526]1,*, Travis Zack, MD, PhD1,2,*, Divneet Mandair, MD1,2,*,  \nZhiwei Zheng, MEng3,#, Ahmed Wali, MEng3,#, Yan-Ning Yu, MEng3,#, Yuwei Quan, MEng3,#, Atul J. \nButte, MD, PhD [0000-0002-7433-2740] 1,2, 4,5 \n \n1. Bakar Computational Health Sciences Institute, University of California, San Francisco, USA \n2. Helen Diller Family Comprehensive Cancer Center, University of California, San Francisco, USA \n3. University of California, Berkeley, USA \n4. Center for Data-driven Insights and Innovation, University of California, Office of the President, \nOakland, CA, USA \n5. Department of Pediatrics, University of California, San Francisco, CA, USA \n \n* co-first authorship \n# equal contribution as a single team for Master of Engineering Capstone project at UC Berkeley \n \nCorresponding Authors:  \nMadhumita Sushil, PhD \nEmail ID: \nMadhumita.Sushil@ucsf.edu \nBakar Computational Health Sciences Institute, 490 Illinois Street, Cubicle 2215, 2nd Fl, North Tower, \nSan Francisco, CA 94143 \n \nAtul Butte, MD, PhD \nEmail ID: \nAtul.Butte@ucsf.edu  \nBakar Computational Health Sciences Institute, 490 Illinois Street, #001, 2nd Fl, North Tower, San \nFrancisco, CA 94143 \n \n \nWord Count: 3375 words (excluding Tables, Figures, and Supplementary Materials)  \n \n  \n \n2 \nAbstract \nAlthough supervised machine learning is popular for information extraction from clinical notes, creating \nlarge, annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large \nlanguage models (LLMs) have demonstrated promising transfer learning capability. In this study, we \nexplored whether recent LLMs can reduce the need for large-scale data annotations. We curated a \nmanually labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare \nzero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised \nclassification performance of three model architectures: random forests classifier, long short-term \nmemory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-\n4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att \nmodel (average macro F1 score of 0.83 vs. 0.75). On tasks with a high imbalance between labels, the \ndifferences were more prominent. Frequent sources of GPT-4 errors included inferences from multiple \nsamples and complex task design. On complex tasks where large annotated datasets cannot be easily \ncollected, LLMs can reduce the burden of large-scale data labeling. However, if the use of LLMs is \nprohibitive, the use of simpler supervised models with large annotated datasets can provide comparable \nresults. LLMs demonstrated the potential to speed up the execution of clinical NLP studies by reducing \nthe need for curating large annotated datasets. This may increase the utilization of NLP-based variables \nand outcomes in observational clinical studies.  \n \n3 \nIntroduction  \nOver the last decade, supervised machine learning methods have been the most popular technique for \ninformation extraction from clinical notes1. However, supervised learning for clinical text is arduous, \nrequiring curation of large domain-specific datasets, interdisciplinary collaborations to design and execute \nstandardized annotation schema, and significant time from multiple domain experts for the meticulous \ntask of data annotation. Supervised modeling can often require subsequent iterative development driven \nby advanced technical expertise, which can be limiting for certain practitioners. The entire process thus \ntakes a significant amount of time between problem conception and obtaining final results. These \nchallenges, combined with the limited availability of clinical notes corpora, have contributed to an under-\nutilization of Natural Language Processing (NLP) in observational studies from Electronic Health \nRecords (EHRs)2.  \n \nRecently, language models have demonstrated promising transfer learning ability, which is encouraging \nfor information extraction from text without extensive task-specific model training3–5. Prompt-based \ninference is popular with generative language models, where practitioners can simply query the model in \nnatural language to obtain the desired information, sometimes by presenting a few examples of the task \nthey may be trying to solve. Prompt-based inference with large language models (LLMs) like the GPT-4 \nmodel have demonstrated varying levels of proficiency in medical inference tasks, such as diagnosing \ncomplex clinical cases6–8, radiology report interpretation9,10, clinical notes-based patient phenotyping11–13, \nautomated clinical trial matching14,15, and improving patient interaction with health systems16. However, \nto understand whether LLMs may be able to alleviate the need to curate large training datasets, few \nstudies have investigated whether zero-shot modeling with LLMs can perform as well as supervised \nlearning in low-resource settings. In this study, utilizing a large corpus of breast cancer pathology notes, \nwe investigate whether large language models can alleviate the need to curate large training datasets for \nsupervised learning to extract information from pathology text. To this end, we have a three-fold \ncontribution: \n1. We developed an annotation schema and detailed guidelines to create an expert-annotated dataset \nof 769 breast cancer pathology reports with document-level, treatment-relevant information. We \nfurther analyzed the curation process to identify frequent modes of disagreements in data \nannotation, which we additionally present here. \n2. To establish a baseline of automated breast cancer pathology classification against that of expert \nclinicians, using the newly curated dataset, we benchmarked the performance of supervised \nmachine learning models of varied levels of complexity, which include a random forest classifier, \na long short-term memory network (LSTM) classifier, and a transformers-based medically-\ntrained BERT classifier.  \n3. We finally prompted the GPT-4 and GPT-3.5-turbo models to obtain zero-shot classification \nresults, i.e., results without using any domain-specific manually labeled dataset, which we \ncompared to the supervised learning performance obtained earlier.  \n \n4 \nMaterials and Methods \nData  \nBreast cancer pathology reports between 2012 and March 2021 were retrieved from the University of \nCalifornia, San Francisco (UCSF) clinical data warehouse, deidentified and date-shifted with the Philter \nalgorithm as previously described 17. Patients with breast cancer were identified by querying for \nencounters with the ICD-9 codes 174, 175, 233.0, or V10.3, or the ICD-10 codes C50, D05, or Z85.3. The \ncohort was restricted to pathology reports by selecting the note type ‘Pathology and Cytology’. Notes \nshorter than 300 characters in length, and those unrelated to breast cancer, for example, those about \nregular cervical cancer screening through pap smears, were removed through keyword search. A flow \ndiagram for the inclusion and exclusion criteria is presented in Figure 1. Among the final set of notes, \n769 pathology reports were randomly selected for manual labeling with treatment-relevant breast cancer \npathology. \nFigure 1: Flow diagram representing inclusion and exclusion criteria for breast cancer pathology report \nselection before data annotation. Number of patients and number of clinical notes is represented at each \nstage. The final annotated subset represents a random sample of the final representative dataset obtained \nin this manner. \nAnnotation schema and guidelines were designed in collaboration with oncology experts to label \ntreatment-relevant breast cancer pathology details. To align with the clinical decision-making process, \nfocus was established on identifying the most aggressive, or “worst” specimen before making document-\nlevel inferences. To analyze categories relevant for prognostic inference, categories such as final tumor \nmargins and lymphovascular invasion were added in addition to commonly investigated categories of \nbiomarkers, histopathology, and grade. The final cohort of 769 breast cancer pathology reports was \nannotated through thirteen key tasks, including ten single-label tasks and three multi-label tasks (Figure \nAll patients in UCSF de-identified clinical data warehouse, until Mar 2021\n(n_patients = 2,576,590)\nPatients with breast cancer diagnosis\n(n_patients = 45,285)\nRetain ICD-9 codes:\n174* (malignant neoplasm of female breast)\n175* (malignant neoplasm of male breast),\n233.0 (carcinoma in-situ of breast), \n V10.3 (personal history of malignant neoplasm of breast) \nRetain ICD-10 codes:\nC50* (malignant neoplasm of breast)\nD05 (Carcinoma in-situ of breast)\nZ85.3 (Personal history of malignant neoplasm of breast)\nRetain notes >300 characters in length, after removing cervical \ncancer screening reports through keyword-based filtering\nRestrict to note_type \n‘Pathology and Cytology’\nPathology and cytology reports in patient encounters containing a breast \ncancer diagnosis\n(n_patients = 15,802, n_notes = 48,250)\nPathology and cytology reports longer than 300 characters in patient \nencounters containing a breast cancer diagnosis\n(n_patients = 15,792, n_notes = 47,986)\nAll patients in UCSF de-identified clinical data warehouse, \nwith clinical notes, until Mar 2021\n(n_patients = 2,530,956, n_notes = 107,052,224)\nAll pathology notes until Mar 2021\n(n_patients = 685,889, n_notes = 3,192,955)\nPathology and Cytology reports recorded in the \nsame encounter as a breast cancer diagnosis\n \n5 \n2). Each report mentioned metadata such as the report date and patient ID, along with the pathologist's \ncomments and the complete clinical diagnosis. Text spans corresponding to cancer stage, lymph node \ninvolvement, and tumor-related information were pre-highlighted within text with an external long-short \nterm networks model that had been previously trained for named entity recognition. To establish a good \ninter-annotator agreement, a group of 2 independent oncology fellows annotated the documents jointly in \nthe first phase. After achieving high inter-annotator agreement, the fellows further labeled the documents \nin the training subset (570 documents) independently. Furthermore, the test subset (100 reports) was \nestablished with documents that were annotated by both oncology fellows, and any disagreements \nbetween discordant labels were manually adjudicated by a third reviewer. Similarly, the validation subset \n(99 reports) was annotated in parallel by 3 medical students and any disagreements were independently \nadjudicated by the same reviewer. The complete annotation guidelines are provided in the Supplementary \nMaterials (Section S1).  \nZero-shot inference with LLMs \nTwo large language models, the GPT-3.5 model and the GPT-4 model18, were queried via the HIPAA-\ncompliant Azure OpenAI Studio1 to provide the requested category of breast cancer pathology \ninformation from a given pathology report. No data was permanently transferred to or stored by either \nOpenAI or Microsoft for any purposes, as previously described2. Model inputs were provided in the \nformat {system role description} {note section text, prompt}. The specific prompt, model version, and the \nmodel hyperparameters are provided in the Supplementary Materials, section S2. All thirteen \nclassification labels were requested through a single prompt, as one call to the model for each pathology \nreport. Prompt development was performed on the development set, and the final results were reported on \na held-out test set. Model outputs were requested in the JSON format, which were post-processed into \npython dictionaries to automatically evaluate model outputs.  \n \nSupervised Modeling \nSupervised machine learning classifiers were trained independently for each of the 13 breast cancer \npathology classification tasks. Three models of varied complexity were included in the analysis — a \nrandom forests classifier19, a Long Short Term Memory networks (LSTM) classifier with attention20,21, \nand a fine-tuned UCSF-BERT (base) model22,23. The random forests model was initialized with a TF-IDF \nvector of n-grams within pathology notes, and the word embeddings in the LSTM model were initialized \nwith fasttext24 embeddings of 250 dimensions, trained on a corpus of 110 million clinical notes at UCSF. \nThe UCSF-BERT model was pretrained from scratch on 75 million clinical notes at UCSF, and fine-\ntuned further on pathology classification-specific tasks. Pathology reports were pre-processed to remove \npunctuations and symbols, and were converted to lowercase before vectorization for the random forests \nand the LSTM models. For random forests single-label tasks, training data samples of the minority classes \nwere up-sampled to reflect a uniform distribution and address data imbalance. Validation and test data \nwere not modified and reflected the real-world distribution. To find the best parameters for the random \nforest model, a random grid search was performed, using 3-fold cross-validation on the training data and \n15 iterations. For deep learning classifiers, the validation subset was used for hyperparameter tuning. To \naddress the data imbalance in multi-label tasks, asymmetric loss25 was used by the deep learning models \n \n1 The AI framework to safely use OpenAI application programming interfaces is called Versa at UCSF. \n2 https://physionet.org/news/post/415 \n \n6 \nduring model training, and the categorical cross-entropy loss was used for single-label tasks. Further \ndetails on model settings and hyperparameter tuning are available in the Supplementary Materials, section \nS3. Source code for model implementation is further available through the github repository: \nhttps://github.com/MadhumitaSushil/BreastCaPathClassification. To obtain a reliable estimate of \nminority class performance, model performance was evaluated on a held-out test set with the metric \nmacro-averaged F1-score instead of accuracy or micro-averaged F1 score.  \nResults \nBreast cancer pathology information extraction dataset \n769 breast cancer pathology reports were annotated with detailed breast cancer pathology information \nacross 13 key tasks (Figure 2). Minimum, maximum, mean and median document length of the dataset \nwere 36, 4430, 723.4, and 560 words respectively, and the inter-quartile range was 508 words. The \ndataset included a diverse population across demographics and age, with nearly 1% of cases being male \nbreast cancer, which reflects the relative incidence of this disease (Table 1). Median patient age was 55 \nyears. To encourage reproducibility and further research, upon manuscript publication, the dataset will be \nfreely shared through the controlled-access repository PhysioNet. Average inter-annotator agreement, as \nquantified with Krippendorf’s alpha326, was 0.85 (Table 2), with variability across tasks. Classification of \nDCIS margins and the multi-label category of sites examined showed the most interannotator discordance, \nwhile lympho-vascular invasion and invasive carcinoma margin status showed the highest concordance.  \n \n \n \n \n3Krippendorf’s alpha was preferred over Cohen’s kappa to quantify inter-annotator agreement on multi-label \nannotation tasks in addition to single-label tasks. \n\n \n7 \nFigure 2: Sample of an annotated pathology report, along with the corresponding document-level \nannotation schema. Irrelevant note refers to those that are not related to a breast cancer diagnosis (further \ndetails in the annotation guidelines). The Unknown labels refer to the cases where a label could not be \ninferred based on the information provided in the pathology report. \n \nTable 1. Socio-demographic distribution of patients in the annotated dataset \n \nSample characteristic Count (percentage) \n(n=769) \nGender     \n  Male 7 (0.91%) \n  Female 762 (99.09%) \nAge     \n  Median [IQR] 55.0 [19.0] \nRace/ethnicity     \n  White 505 (65.67%) \n  Asian 101 (13.13%) \n  Latinx 42 (5.46%) \n  Black or African-American 36 (4.68%) \n  Native Hawaiian or Other Pacific \nIslander \n7 (0.91%) \n  Southwest Asian and North African 2 (0.26%) \n  Other 23 (2.99%) \n  Multi-Race/Ethnicity 15 (1.95%) \n  Unknown/Declined 38 (4.94%) \nLanguage     \n  English 702 (91.29%) \n  Russian 18 (2.34%) \n  Unknown/Declined 17 (2.21%) \n  Chinese - Cantonese 9 (1.17%) \n \n8 \n  Spanish 9 (1.17%) \n  Vietnamese 4 (0.52%) \n  Chinese - Mandarin 2 (0.26%) \n  Burmese 1 (0.13%) \n  Italian 1 (0.13%) \n  Cambodian 1 (0.13%) \n  Samoan 1 (0.13%) \n  Korean 1 (0.13%) \n  Farsi 1 (0.13%) \n  Sign Language 1 (0.13%) \n  Other 1 (0.13%) \nAbbreviation: IQR, Inter-quartile range \n \n \nTable 2: Inter-annotator agreement between expert clinical annotators, as quantified by Krippendorf’s \nalpha score for single-label and multilabel tasks. \n \nTask Inter-annotator agreement  \n(Krippendorf’s alpha) \nBiopsy type 0.80 \nNum lymph nodes involved 0.89 \nER 0.85 \nPR 0.90 \nHER2 0.80 \nGrade 0.85 \nLVI 0.97 \n \n9 \nMargins 0.93 \nDCIS Margins 0.77 \nHistology (Multilabel) 0.82 \nSites examined (Multilabel) 0.79 \nSites of disease (Multilabel) 0.85 \nAVERAGE 0.85 \n \nSources of disagreements between annotators in the development and the test sets were analyzed by an \nindependent adjudicator. Common sources of disagreements included differences in inferring the most \naggressive (“worst”) sample when multiple samples were analyzed, incorrectly including information \nfrom patient history for providing labels for the current report, linguistic or clinical ambiguity in the \npathology report, discordant interpretation of procedures involving excisions when differentiating \nbetween a histopathology report and a cytology report, inconsistencies in categorizing metastatic disease \nsites as “other tissues” and histology as “others”, and inconsistent execution of the annotation guidelines \nfor annotating molecular pathology reports and grade information.  \n \nThe class distribution across the annotated data was highly skewed, resulting in a highly imbalanced \ndataset. Certain low frequency categories, such as groups of histology codes or the low positive and \npositive categories of Estrogen Receptor status were combined before further automated classification, \nresulting in the final distribution presented in Figure 3. The Unknown class, which corresponded to the \ncase where the requested information could not be inferred from the given note, was the majority class \nacross 8 of 13 tasks. Among the remaining classes, high imbalances were observed in tasks of inferring \nthe category of the number of lymph nodes involved, lymphovascular invasion, tumor margins, and HER-\n2 receptor status.  \n \n \n10 \n \n \nFigure 3: Class distribution for all tasks in the training data for supervised classification. \n \nThe GPT-4 model is as good as or better than supervised models in breast cancer pathology \nclassification \n \nDespite no task-specific training, the GPT-4 model either outperformed or performed as well as our tasks-\nspecific supervised models trained on task-specific breast cancer pathology data (Figure 4). For both the \nGPT-4 model and the GPT-3.5 model, all model responses could be automatically parsed as JSON \nwithout any errors. The average macro F1 score of the GPT-4 model across all tasks was 0.83, of the \nLSTM model with attention was 0.75, of the random forests model was 0.61, of the UCSF-BERT model \nwas 0.58, and that of the GPT-3.5-turbo model (zero-shot) was 0.53. The GPT-4 model was significantly \nbetter than the LSTM model (the best supervised model) for the tasks of margins and estrogen receptor \n(ER) status classification (Approximate randomized testing for significance27, p < 0.01). These tasks \nencompass either a large training data imbalance resulting in a sparsity of class-specific training instances \n(margins) or could be frequently solved with an n-gram-matching approach (ER status). For all other \ntasks, no significant differences were obtained between the zero-shot GPT-4 model and the supervised \nLSTM model.  \n \nThe GPT-3.5-turbo model performed significantly worse than the GPT-4 model for all tasks. Similarly, \nthe UCSF-BERT model, which is a transformers model pre-trained on the UCSF notes corpus22,23, did not \noutperform simpler models like random forests or LSTM with attention for several tasks, potentially due \nto relatively small sample sizes and highly imbalanced training data. The random forests classifier \nperformed well on keyword-oriented tasks, like pathology type classification and biomarker status \nclassification, but under-performed on tasks requiring more advanced reasoning, like grade and margins \ninference. \n\n \n11 \n \nFigure 4: Classification performance, as measured by Macro F1, for different models for each \nclassification task. All models other than GPT-3.5 and GPT-4 are trained in a supervised setup on task-\nspecific training data. GPT-3.5 and GPT-4 models are evaluated zero-shot, i.e., in an unsupervised \nmanner. \n \nGPT-4 model error analysis \n \nAs demonstrated in Figure 5, the confusion matrix of the GPT-4 model revealed that it had difficulties in \ndifferentiating the unknown class from the class that indicated no lymph node involvement and no \nlympho-vascular invasion. Furthermore, margins inference was complex for the model, where more than \n2mm margins (negative margins) are confused with less than 2mm margins. Confusion between classes \nwere more prevalent in multi-label tasks than single-label tasks. Further errors from the GPT-4 model \nwere prevalent when the task design was ambiguous in model prompts, such as the grouping of sparse \nhistology into an “others” category, the assignment of metastatic sites for breast cancer as “other tissues \nthan breast or lymph nodes”, or the inference of pathology reports unrelated to breast cancer. The latter \nset of errors correspond to common sources of disagreements identified during the data annotation \nprocess.   \n\n \n12 \n \n \n \n(a) Single-label classification tasks \n\n \n13 \n \n(b) Multi-label classification tasks \n \nFigure 5: Confusion matrices for GPT-4 classification in (a) single-labeled tasks, (b) multi-labeled tasks. \n \nManual analysis of the GPT-4 model errors revealed several consistent sources of errors. Common \nsources of errors in biomarker reporting involved the reporting of results from clinical history or tests \nconducted at other sites that were not confirmed in the current report. Furthermore, the GPT-4 model \nincorrectly reported nuclear grade as the overall tumor grade when the overall grade was not discussed in \nthe note. Moreover, common errors in reporting tumor margins were concerned with mathematical \ninferences over multiple margins (for example, anterior, posterior, medial, etc.), where the category \nrepresenting the closest margin values needed to be provided. Manual analysis additionally uncovered \nseveral error sources for multi-label tasks. The model performed inconsistently when inferring sites of \nbenign findings; while the model frequently missed reporting the site of benign findings as a site \nexamined for tumors, it also sometimes included sites of benign findings as a site of cancer. Furthermore, \nsentinel and axillary lymph nodes were frequently reported as tissues other than breast or lymph nodes, \nalthough they were annotated as lymph node sites. Some errors related to complex cases were also found, \nfor example, 1% staining results for progesterone receptors were provided as negative by the model, \nwhereas they were annotated as positive. Finally, errors related to task setup were reflected in histology-\nrelated errors, where the model could not reliably abstain from providing histology from reports unrelated \nto breast cancer and from molecular pathology reports for ERBB2 despite being instructed as such, and \nerrors due to the grouping of histologies like LCIS into an “others” category.  \n\n \n14 \nDiscussion \nTask-specific supervised learning models trained on manually annotated data have been the standard \napproach in clinical NLP for over a decade1. Using a manually annotated dataset of 769 breast cancer \npathology reports focused on the most clinically relevant report features, our study compared the \nperformance of supervised learning models, including random forests classifier, LSTM models, and the \nUCSF-BERT model, with a zero-shot classification performance of two LLMs, the GPT-4 model, and the \nGPT-3.5-turbo model. We found that even in zero-shot setups, the GPT-4 model performs as well as or \nsignificantly better than simpler, task-specific supervised counterparts, although the GPT-3.5 model \nperforms significantly worse than the GPT-4 model on all classification tasks. Previous studies have \ndemonstrated similar results, showing that in zero-shot setups, LLMs consistently perform the same as or \noutperform fine-tuned models on biomedical NLP datasets with small training data sizes (fewer than 1000 \ntraining examples)28,29. Similar small datasets are common in medical informatics studies since domain \nexpertise is frequently required for reliably annotating clinical notes, making the process time-consuming \nand difficult to scale30. This study enhances previous findings on a new real-world clinical dataset, \nreinforcing that LLMs are promising for use in classification tasks in low-resource clinical settings. \n \n \nTasks where the training data contained high class imbalance or keyword-based tasks (i.e., could be \nlargely solved with simple lexical matching) were particularly conducive for using GPT-4 model over \ntask-specific supervised models. Given that the GPT-4 model is already trained on internet-scale corpora, \nthe model may already encode a fundamental understanding of breast cancer pathology-related \nterminology, which may explain its surprising zero-shot capability on these tasks, including that on \ncomplex and imbalanced tasks like margins inference. However, the reasons behind the striking \nperformance difference between the GPT-3.5 and GPT-4 models remain unclear due to the closed nature \nof these models, although similar trends have been observed in previous medical NLP studies31,32,13.  \n \nAn analysis of the GPT-4 model errors indicated several errors due to insufficient understanding of \nidiosyncratic task-design choices, for example differentiating between “Unknown” and “no lymph node \ninvolvement” categories, and grouping of less frequent histologies into an “others” category. It is possible \nthat these errors can be mitigated with strategies, such as few-shot learning to demonstrate a better \nunderstanding of annotation-specific choices, or chain-of-thought-prompting to elucidate reasoning and \navoid answering from incomplete or old information within text report. However, it has been \ndemonstrated earlier that the GPT-4 model cannot process long input contexts efficiently, particularly \nwhen the results are included in the central part of the context33. Hence, how to best integrate in-context \nlearning with long pathological notes such that the model can still make effective use of the supported \ncontext length remains to be investigated in future research. \n \nAlthough this study compared two proprietary LLMs with supervised classifiers on a real-world breast \ncancer dataset, several design choices may have impacted the findings. The dataset was curated from a \nsingle health system, and further validation of the findings on pathology reports from other health systems \nmay improve the reliability of the results. Although potential de-identification errors may have impacted \nthe capability of LLMs, the data reflects real-world setups for retrospective observational studies in a \nprivacy-preserving manner. Furthermore, although some model-specific hyperparameters were explored \nfor baseline models, all possible choices have not been explored, and it may be possible to improve \n \n15 \nbaseline model performance further with continued development. Moreover, LLMs used in the study \nwere evaluated using a single prompt and model setting, and the results could be sensitive to these design \nchoices. However, the findings of this study will inform future studies on the development of more \nadvanced prompting and few-shot strategies for LLMs to obtain even better performance, the \ndevelopment of effective annotated datasets for simpler supervised classification setups, the evaluation of \nnewer LLMs for clinical information extraction, and the analysis of output sensitivity to input prompts \nand model settings. While the findings from this study demonstrate the promising capability of LLMs for \nclinical research, if access to models like the GPT-4 model is prohibitive due to either privacy or \ncomputational constraints, comparable performance on EHR-based NLP tasks such as pathology \nclassification can be obtained with simpler deep learning classifiers, particularly if annotated sample sizes \nare sufficiently large and class imbalance can be controlled through targeted annotations of minority \nclasses for model training. Finally, the studied classifiers may exhibit biases against specific \ndemographics, and caution must be exercised when deploying them in clinical workflows. These biases \nneed to be investigated further in the future to establish concrete guidelines for their use. \n \nDespite widespread studies in oncology information extraction from textual clinical records34,35, annotated \ndatasets of breast cancer pathology reports are not publicly available. To make the findings of this study \nreplicable and promote further research on breast cancer pathology extraction, the dataset curated in this \nstudy along with corresponding source code for the supervised machine learning pipelines and zero-shot \nLLM inference will be shared publicly through a controlled-access repository PhysioNet, accessible via a \ndata use agreement. Providing this large, annotated dataset for future research increases the impact of this \nwork by allowing it to serve as a baseline of comparison as LLMs continue their rapid progress.  \nConclusions \nThe study compared breast cancer pathology classification abilities of five models of varying sizes and \narchitecture, finding that the GPT-4 model, even in zero-shot setups, performed similarly to or better than \nthe LSTM model with attention trained on nearly 550 pathology report examples. The GPT-4 model \noutperformed simpler baselines for classification tasks with high class imbalance or that required simpler \nkeyword-matching for inference. However, when large training datasets were available, no significant \ndifference was observed between the performance of simpler models like the LSTM model with attention \ncompared to the GPT-4 model. The results of this study demonstrated that while LLMs may relieve the \nneed for resource-intensive data annotations for creating large training datasets in medicine, if there are \nprivacy, computational, or cost-related concerns regarding the use of LLMs with patient data, it may be \npossible to obtain reliable performance with simpler models by developing large annotated datasets, with \nparticular focus on minority class labeling potentially in an active-learning setup. \nAcknowledgements \nThis research would not have been possible without support from several people. The authors thank the \nUCSF AI Tiger Team, Academic Research Services, Research Information Technology, and the \nChancellor’s Task Force for Generative AI for their software development, analytical and technical \nsupport related to the use of Versa API gateway (the UCSF secure implementation of large language \n \n16 \nmodels and generative AI via API gateway), Versa chat (the chat user interface), and related data asset \nand services. We are grateful to Dima (Dmytro) Lituiev for sharing LSTM-based token-level pathology \ninformation extraction pipeline for pre-annotating breast cancer pathology reports. We thank Boris \nOskotsky, the Information Commons team, and the Wynton high-performance computing platform team \nat UCSF for supporting high performance computing platforms that enable the use of language models \nwith de-identified patient data. We further thank Debajoyti Datta and Michelle Turski for feedback on \nbreast cancer pathology annotation schema, all members of the Butte lab for helpful discussions in the \ninternal presentations, and Gundolf Schenk and Lakshmi Radhakrishnan for discussions related to clinical \nnotes de-identification. Partial funding for this work is through the FDA grant U01FD005978 to the \nUCSF–Stanford Center of Excellence in Regulatory Sciences and Innovation (CERSI), through the NIH \nUL1 TR001872 grant to UCSF CTSI, through the National Cancer Institute of the National Institutes of \nHealth under Award Number P30CA082103, and from a philanthropic gift from Priscilla Chan and Mark \nZuckerberg and the Gerson Bakar Foundation. The content is solely the responsibility of the authors and \ndoes not necessarily represent the official views of the National Institutes of Health. \nFinancial Disclosures and Conflicts of Interest \nMS, TZ, DM, ZZ, AW, YY, and YQ report no financial associations or conflicts of interest. \nAJB is a co-founder and consultant to Personalis and NuMedii; consultant to Mango Tree Corporation, \nand in the recent past, Samsung, 10x Genomics, Helix, Pathway Genomics, and Verinata (Illumina); has \nserved on paid advisory panels or boards for Geisinger Health, Regenstrief Institute, Gerson Lehman \nGroup, AlphaSights, Covance, Novartis, Genentech, and Merck, and Roche; is a shareholder in Personalis \nand NuMedii; is a minor shareholder in Apple, Meta (Facebook), Alphabet (Google), Microsoft, Amazon, \nSnap, 10x Genomics, Illumina, Regeneron, Sanofi, Pfizer, Royalty Pharma, Moderna, Sutro, Doximity, \nBioNtech, Invitae, Pacific Biosciences, Editas Medicine, Nuna Health, Assay Depot, and Vet24seven, and \nseveral other non-health related companies and mutual funds; and has received honoraria and travel \nreimbursement for invited talks from Johnson and Johnson, Roche, Genentech, Pfizer, Merck, Lilly, \nTakeda, Varian, Mars, Siemens, Optum, Abbott, Celgene, AstraZeneca, AbbVie, Westat, and many \nacademic institutions, medical or disease specific foundations and associations, and health systems.  Atul \nButte receives royalty payments through Stanford University, for several patents and other disclosures \nlicensed to NuMedii and Personalis.  Atul Butte’s research has been funded by NIH, Peraton (as the prime \non an NIH contract), Genentech, Johnson and Johnson, FDA, Robert Wood Johnson Foundation, Leon \nLowenstein Foundation, Intervalien Foundation, Priscilla Chan and Mark Zuckerberg, the Barbara and \nGerson Bakar Foundation, and in the recent past, the March of Dimes, Juvenile Diabetes Research \nFoundation, California Governor’s Office of Planning and Research, California Institute for Regenerative \nMedicine, L’Oreal, and Progenity.  None of these entities had any bearing on this research or \ninterpretation of the findings. \n \n \n \n  \n \n17 \n \nReferences \n1. Wu, H. et al. A survey on clinical natural language processing in the United Kingdom from \n2007 to 2022. npj Digit. Med. 5, 1–15 (2022). \n2. Fu, S. et al. Recommended practices and ethical considerations for natural language \nprocessing-assisted observational research: A scoping review. Clin Transl Sci 16, 398–411 \n(2023). \n3. Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural \nInformation Processing Systems vol. 33 1877–1901 (Curran Associates, Inc., 2020). \n4. Kojima, T., Gu, S. (Shane), Reid, M., Matsuo, Y. & Iw asawa, Y. Large Language Models are \nZero-Shot Reasoners. Advances in Neural Information Processing Systems 35, 22199–\n22213 (2022). \n5. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & So ntag, D. Large language models are \nfew-shot clinical information extractors. in Proceedings of the 2022 Conference on Empirical \nMethods in Natural Language Processing 1998–2022 (Association for Computational \nLinguistics, Abu Dhabi, United Arab Emirates, 2022). \n6. Eriksen, A. V., Möller, S. & Ryg, J. Use of GPT-4 to D iagnose Complex Clinical Cases. \nNEJM AI 1, AIp2300031 (2023). \n7. Wang, Z. et al. Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? \nMaybe, but not today. Preprint at http://arxiv.org/abs/2306.01499 (2023). \n8. Barile, J. et al. Diagnostic Accuracy of a Large Language Model in Pediatric Case Studies. \nJAMA Pediatrics (2024) doi:10.1001/jamapediatrics.2023.5750. \n9. Liu, Q. et al. Exploring the Boundaries of GPT-4 in Radiology. in Proceedings of the 2023 \nConference on Empirical Methods in Natural Language Processing (eds. Bouamor, H., Pino, \nJ. & Bali, K.) 14414–14445 (Association for Computational Linguistics, Singapore, 2023). \n \n18 \ndoi:10.18653/v1/2023.emnlp-main.891. \n10. Fink, M. A. et al. Potential of ChatGPT and GPT-4 for Data Mining of Free-Text CT Reports \non Lung Cancer. Radiology 308, e231362 (2023). \n11. Alsentzer, E. et al. Zero-shot interpretable phenotyping of postpartum hemorrhage using \nlarge language models. npj Digit. Med. 6, 1–10 (2023). \n12. Guevara, M. et al. Large language models to identify social determinants of health in \nelectronic health records. npj Digit. Med. 7, 1–14 (2024). \n13. Sushil, M. et al. CORAL: Expert-Curated medical Oncology Reports to Advance Language \nModel Inference. Preprint at https://doi.org/10.48550/arXiv.2308.03853 (2024). \n14. Wong, C. et al. Scaling Clinical Trial Matching Using Large Language Models: A Case Study \nin Oncology. Preprint at https://doi.org/10.48550/arXiv.2308.02180 (2023). \n15. Datta, S. et al. AutoCriteria: a generalizable clinical trial eligibility criteria extraction system \npowered by large language models. Journal of the American Medical Informatics \nAssociation 31, 375–385 (2024). \n16. Mirza, F. N. et al. Using ChatGPT to Facilitate Truly Informed Medical Consent. NEJM AI 0, \nAIcs2300145 (2024). \n17. Radhakrishnan, L. et al. A certified de-identification system for all clinical text documents for \ninformation extraction at scale. JAMIA Open 6, ooad045 (2023). \n18. OpenAI. GPT-4 Technical Report. Preprint at https://doi.org/10.48550/arXiv.2303.08774 \n(2023). \n19. Breiman, L. Random Forests. Machine Learning 45, 5–32 (2001). \n20. Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Comput. 9, 1735–1780 \n(1997). \n21. Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align \nand Translate. Preprint at https://doi.org/10.48550/arXiv.1409.0473 (2016). \n22. Sushil, M., Ludwig, D., Butte, A. J. & Rudrapatna, V. A. Developing a general-purpose \n \n19 \nclinical language inference model from a large corpus of clinical notes. Preprint at \nhttp://arxiv.org/abs/2210.06566 (2022). \n23. Silverman, A. L. et al. Algorithmic identification of treatment-emergent adverse events from \nclinical notes using large language models: a pilot study in inflammatory bowel disease. \n2023.09.06.23295149 Preprint at https://doi.org/10.1101/2023.09.06.23295149 (2023). \n24. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T. Enriching Word Vectors with Subword \nInformation. Transactions of the Association for Computational Linguistics 5, 135–146 \n(2017). \n25. Ridnik, T. et al. Asymmetric Loss For Multi-Label Classification. in 2021 IEEE/CVF \nInternational Conference on Computer Vision (ICCV) 82–91 (IEEE, Montreal, QC, Canada, \n2021). doi:10.1109/ICCV48922.2021.00015. \n26. Krippendorff, K. Content Analysis: An Introduction to Its Methodology. (SAGE Publications, \n2018). \n27. Edgington, E. S. Approximate Randomization Tests. The Journal of Psychology 72, 143–\n149 (1969). \n28. Jahan, I., Laskar, M. T. R., Peng, C. & Huang, J. A Comprehensive Evaluation of Large \nLanguage Models on Benchmark Biomedical Text Processing Tasks. Preprint at \nhttp://arxiv.org/abs/2310.04270 (2023). \n29. Chen, Q. et al. Large language models in biomedical natural language processing: \nbenchmarks, baselines, and recommendations. Preprint at \nhttps://doi.org/10.48550/arXiv.2305.16326 (2024). \n30. Gao, Y. et al. A scoping review of publicly available language tasks in clinical natural \nlanguage processing. Journal of the American Medical Informatics Association 29, 1797–\n1806 (2022). \n31. Taloni, A. et al. Comparative performance of humans versus GPT-4.0 and GPT-3.5 in the \nself-assessment program of American Academy of Ophthalmology. Sci Rep 13, 18562 \n \n20 \n(2023). \n32. Nori, H. et al. Can Generalist Foundation Models Outcompete Special-Purpose Tuning? \nCase Study in Medicine. Preprint at http://arxiv.org/abs/2311.16452 (2023). \n33. Liu, N. F. et al. Lost in the Middle: How Language Models Use Long Contexts. Preprint at \nhttps://doi.org/10.48550/arXiv.2307.03172 (2023). \n34. Wang, L. et al. Assessment of Electronic Health Record for Cancer Research and Patient \nCare Through a Scoping Review of Cancer Natural Language Processing. JCO Clin Cancer \nInform e2200006 (2022) doi:10.1200/CCI.22.00006. \n35. Gholipour, M., Khajouei, R., Amiri, P., Hajesmaeel Gohari, S. & Ahmadian, L. Extracting \ncancer concepts from clinical notes using natural language processing: a systematic review. \nBMC Bioinformatics 24, 405 (2023). \n  \n \n21 \nSupplementary Materials \nSection S1: Annotation guidelines\n \nPathology Type  \n \nThis is a single-option selection regarding the TYPE of sample a pathology report is referring to. In \ngeneral, there are two main types of samples in pathology.  \n1. Cytology (Cyto=”cell”). In this type, cells are collected from either fluid or from a procedure \ncalled “Aspiration”. In either of these two techniques, the structure BETWEEN cells is lost, so \nthe only information gathered are what the individual cells look like under the microscope \n2. Histopathology: (histo=”tissue”): as the name suggests, a sample is collected as an entire \n“Chunk” of tissue. This allows for characterization of both cellular features but ALSO the \nstructure and organization of those cells within the larger tissue/organ/tumor.  \n3. Irrelevant: In some cases, we have collected pathology notes on breast cancer patients that are not \nRELATED to cancer diagnosis or treatment directly. For example notes about reconstruction \nsurgeries, or perhaps any notes related to cervical cancer screening pap smears. In these cases, \nplease mark notes as Irrelevant \n4. Unknown: In some cases, it can be very difficult to determine if the primary sample is cytology or \npathology. The most common example of this is for Molecular staining for a specific tumor \nmarker (in our case, specifically for HER2/ERBB2), where the pathologist is only looking for a \nsingle presence or absence. In these cases, please always mark Unknown. \n The pathology type can generally be determined at the beginning of a report.   \n Examples of Histopathology key words: \n- Surgical Pathology report \n- Lumpectomy \n- Core biopsy \n- Excisional biopsy \n- Mastectomy \n- Additionally, mention of specific forms of carcinoma require histopathology for \ndiagnosis, so if “adenocarcinoma”, ductal carcinoma, etc are described, this is a \nhistopathology note \n Examples of cytology key words: \n- Fine Needle Aspiration \n- Cytology \n- XXXX of XXX fluid \n Examples of Irrelevant notes \n- Capsulectomy or breast reconstructive surgery \n- Cytology report for pap smear in patient with history of breast cancer \nDisambiguation \nIn some notes, multiple tumor samples may be present (such as surgical reports). In these cases where \nboth histopathology and cytology reports are present, please label as histopathology.  \n \n\n \n22 \n \nSites examined and Sites of disease \n \n \nThis is a multi-class selection for which tissues are present in the report and which tissues contained \nmalignancy.  \nOften, much of this information can be gather at the beginning of the report under the section: Final \nPathological Diagnosis: \n \nFor sites examined, please check all sites in which there is a specimen that was examined for disease. In \nthe above example, left breast would be checked, even though both specimens were benign. Please make \nthe best guess when the laterality is not 100% clear. \n \nFor sites of disease, check all boxes where any type of disease has occurred in that site (this includes, \nDCIS, invasive cancer, etc.).  \n \nSites of disease is None when we know that tumors were not found among the sites that were examined. \nUnknown is to be selected only when we can’t make the inference from the data, for example, when the \nresults are not present in the report. \n \nAny lymph nodes, for example supraclavicular lymph node, should be considered as lymph nodes and not \n“other tissues”. Finally, we often find that skin is one of the specimens examined. If the specimen is \nreferring to skin overlying breast tissue, then this should NOT be labeled as ‘other tissue’. Instead, skin \noverlying breast tissue should just be grouped with the same labeling for the underlying breast tissue. The \nreason for this is that for breast cancer, if the skin overlying a breast also has evidence of disease, then \nthis reflects the degree of LOCAL INVASION of that cancer (ie it would have a T stage of 4). So we \nreally don’t want to count this as a separate ‘other tissue’ - the skin involvement would be reflected in the \nstaging information we capture about the disease (it’s not a separate site of disease). You would only \ncheck ‘other tissue’ for skin involvement if the site of skin involvement is some other area of the body, \nfor instance the arm, leg, back, etc. \n \nThere is also a text span “Tumor location” closely associated with this label. If an invasive disease is \npresent, you should highlight the text that corresponds to where this is located ONLY (do not highlight \nlocations of DCIS or benign samples if invasive disease is present). If only DCIS is present, then you \nshould highlight the text that corresponds to where the DCIS is located. Only if all specimens are benign \n\n \n23 \nshould you highlight the locations of benign disease with ‘tumor location.’ In the text span, you should \ninclude not only laterality (ie Left Breast) but also positional information often included alongside this, \ne.g. ‘10 o'clock position’. You should only need to label tumor location text spans in one of 2 sections in \nthe pathology note: in the ‘FINAL PATHOLOGIC DIAGNOSIS’ and the ‘Comments’ sections. \n \nHistology \n  \nHistology types - This is a multiclass selection for all histology types that correspond to the \nspecimens you’ve labeled in the text span. If there are multiple tumor specimens, at the document level \nwe are annotating the ‘worst’ of these for both invasive tumors AND DCIS. More below on how you \ndecide what’s the ‘worst’ invasive tumor. But the point to remember here is that you should select the \nhistology that corresponds to that worst case and if DCIS is present in any of the specimens, you should \nalso check that (DCIS still has significance if it occurs along with invasive tumors, but more on that \nbelow). ‘No malignancy’ should be selected only when there is no disease of any kind found in any of the \nsamples (note that in these cases, you should also have ‘None’ checked for sites of disease). Note that the \ncategory ‘Cribiform’ has been removed moving forward, and can be ignored in the guidelines. The \ncategory ‘Others’4 is reserved for cases where a histology is present, but it is not present in the options \nprovided by us in the ‘Histology’ class list. \n \nFor text spans, the histo type corresponds to the text span ‘Tumor type’. If an invasive tumor is present, \nthe text span highlighted should be for the corresponding histo type. Do not highlight benign samples as \n‘tumor type’ if invasive disease (or DCIS) is present. If DCIS is present, it also should be highlighted as \n‘tumor_type’. Only if there are no invasive samples nor DCIS in a path report should you highlight \nbenign samples that mention no tumor as ‘tumor type’.  \n \nThe histology almost always should be annotated from the “Final Pathological DIagnosis section” - many \ntimes it is duplicated within more technical portions of the text. It is not necessary to mark all the text \nspans where the histology is mentioned if it is the same as what is contained in the ‘Final Pathological \nDiagnosis’ Section. Even if you initially think it’s clear that a specimen is lobular invasive from the ‘Final \nPathology Diagnosis section”, you should quickly look over the path description to make sure no other \nhistology types were mentioned (you don’t have to be exhaustive about this, but a quick check here might \nshow the pathologist also saw some ductal components, for example). The key histology types to really \nfocus on are LCIS, DCIS, invasive ductal and invasive lobular, as these are by far the most frequent types \nof in situ and invasive carcinomas for breast  \ncancer. \n \n\"Mixed\" refers to a situation where ~50% of the cells are one type but there is a second population that \nmakes up >10% of the remaining cells: https://www.pathologyoutlines.com/topic/breastmixedNST.html. \n \n4Please note that in new iterations, Carcinoma NOS has been changed to an ‘Others’ category to represent all the \ntumors that don’t fit the remaining category. \n\n \n24 \nThis description is often contained in a separate descriptive paragraph. If the sample is described as mixed \nin that paragraph, typically it would be described as mixed in the Final Path diagnosis. In general, we \ndecided to select both if both are present in cases as neither is necessary \"more\" aggressive than the other. \nWe can leave out invasive ductal and invasive lobular individually if these refer to Mixed carcinoma in \nthe note.\n \nLymph Node Involvement \n \nThis includes the total number of lymph nodes that are involved (including left and right sides, e.g. for \nbilateral mastectomy). If there are multiple samples from the same date, we should sum those lymph node \nnumbers. If there are multiple samples from different dates, then the maximum number of lymph node \nshould be selected independently (this scenario becomes a bit tricky as you need to monitor sample \ndates). Any lymph node, for example supraclavicular lymph node, should be considered as lymph nodes \nand not “other tissues”. \n \nUnknown should be marked if the lymph nodes were not examined at all in the given pathology report. If \nthe lymph nodes were examined, but disease was not found on lymph nodes, the ‘0 involved’ category \nshould be marked. It might feel redundant (which means you’re annotating correctly :)) to check ‘0’ \nlymph nodes involved when you’ve already checked ‘None’ for sites of disease, but that’s just the way it \ngoes. Please make the best guess when the number of lymph nodes involved is not 100% clear. \n \nSome technical pointers: if the number of lymph nodes with disease is not clear (whether this is due to \nredaction or the note simply doesn’t mention), then either make the best guess or check ‘Unknown’.  \n \nThe corresponding text span-level annotations of ‘tumor_location’ will be highlighted only if a tumor was \nfound in a lymph node. \n \n \nBiopsy Type \n \nThis is a single item selection. Sometimes, a path report will refer to multiple specimens. These may be \nspecimens from different points over time or different sites where tissue was examined. Key things to \nremember when annotating the biopsy type (a label that’s only at the document level), is that this should \ncorrespond to the specimen you’ve chosen as the ‘worst’ disease. However, if all the samples are disease-\nfree, just mark the biopsy type to be the highest level of sample analyzed. \n \nMore technical things to remember are that ‘biopsy’ here refers to Fine Needle Aspiration (FNA) as well \nas core needle biopsies.  \n \nLumpectomies should be checked for any surgery with partial removal of breast tissue - in most cases, the \nnote will call the procedure a lumpectomy and this will be clear. Some notes, however, refer to \nlumpectomies as ‘excisional biopsies or re-excision biopsy’ - this will take a bit more detective work to \n\n \n25 \nconfirm but this generally is still referring to a lumpectomy, since portions of breast tissue were removed \nbut the specimen does not have true margins (which you’d only get with a mastectomy).  \n \nFor non breast tissue, some general rules are that Fine Needle Aspiration (FNA) or procedures involving \nneedle-like instruments should be grouped under biopsy: these include CSF or other percutaneous \nbiopsies. Other times a procedure may not fit nicely into our groupings (such as excision of a metastatic \nbrain tumor) and ‘Unknown’ should be checked.  \n \nIf mastectomy has been mentioned only in the history or only in reference to a cosmetic surgery, those \nshould be ignored.  \n \nRefer to the end of the document to read about how to choose the appropriate ‘worst’ invasive disease. \nThe same logic applies to grade, margins, ER/PR status below as well. \n \nGrade \n \n \n \n \nGrade should also be annotated in two ways simultaneously: 1) as document-level class, 2) corresponding \ntext spans should also be highlighted. \n \nLike the document-level annotations, the grade checked is the overall grade for the specimen that you \nidentified as the ‘worst’ - note, if there is any invasive disease in the report, this grade should correspond \nto invasive disease. If only DCIS is present in the report, then this grade would correspond to DCIS. If \ngrade is only present in history, it should be ignored. If it is present as results from another center but not \nconfirmed at UCSF, it should be marked. \n \nFor text spans of grade in the pathology report, we only annotate mentions of OVERALL grade for any \nparticular specimen. There are only two sections where you should be highlighting text areas \ncorresponding to grade (otherwise, you may be highlighting a number of random grade mentions). The \n‘Final Pathologic Diagnosis’ section often but not always mentions the overall grade for a specimen. \nSecondly, as seen above, there’s often a section in path notes ‘Comments’ that will mention the overall \ngrade twice (as depicted above). You should highlight in the text span all of these and ignore any others. \nMentions of the word ‘grade’ itself, then, are not included in these text spans and should be deleted if \nmarked already. Well/moderate/poorly differentiated tumors represent grades 1/2/3 respectively. Nuclear \ngrade is equivalent to overall grade only for DCIS and should only be highlighted in text as the grade if \n\n \n26 \nthere is only DCIS in the path report (otherwise, the grade highlighted in the text should correspond to the \ninvasive disease). Another point is that grade should never be annotated from cytology or Molecular \nHER2 study reports - often these reports might mention the nuclear grade but without tissue architecture, \na true pathologic grade can’t be determined. You should have ‘Unknown’ marked for grade in these \ncases.  \n \nOverall grade is often mentioned as a part of a larger text string that begins with ‘Total points/overall \ngrade’ - all of this, up until the mention of the overall grade of the sample, is included in the text \nannotation span. \n \nSometimes the grade that is mentioned as a fraction is replaced with a date, for example 3/5 being \nrepresented as September 3rd instead. This can happen due to the incorrect redaction of PHI data, where \nthe redaction algorithm assumed that a fraction is date, shifted it, and replaced it. In this case, make the \nbest guess of the grade from the surrounding text for a document-level category and mark those spans, \ninstead of using this incorrect information for inference. If an inference cannot be made, leave it as \nUnknown. \n \n \nER \n \n \n \n \nEstrogen receptor values should be selected at both text span level as well as document-level. When \nannotating at the text span-level, the percentage positive should be included in the text spans when \nmentioned, for example “ER was 90% positive”. \n \nAny mentions of ER in the history (for a previously treated disease) should be ignored, unless it has been \nconfirmed in the current state of the disease either at UCSF or at an external center. This is so because ER \nvalues can change over the course of the disease. However, if ER status is mentioned as finding from \nanother center (for the current disease), it should be included in the annotations even if it has not been \nconfirmed at UCSF. \n \nIf multiple values are present, make sure that the ER result corresponds to the same specimen you’ve \npicked as the ‘worst’ disease. For example, if there are two different statuses: one before a neoadjuvant \ntherapy, and one after, then we annotate the one after. \n \nWe have two categories for positive ER disease: Low positive and positive. Low positive is for cases \nunder 10% staining, also called “weakly positive”. This differentiation has been made to account for \ndifferences in treatment guidelines for the same. \n \nPR \n\n \n27 \n \n \n \n \nProgesterone receptor status is annotated in the same manner as estrogen receptors. The only difference is \nthat we do not differentiate between low positive and positive disease for the same. So even if PR status is \n“barely present”, it should be marked as positive. Please include %age positive within the text spans for \nthis. \n \nIf ER and PR values are mentioned within the common text, for example ‘ER and PR were found to be \npositive”, overlapping text spans should be added for both text-level labels. \n \nHER2 \n \n \n \n \nHER2 status is similar to the prior ER and PR, although a few specific issues arise at both the document \nand text level. For the document level, an original specimen will have IHC staining to test for HER2 - \nbased on these results (if HER2 is present in the report), you should check Positive, Negative (0 or 1+) or \nEquivocal (2+). The note will mention the level of the stain and often explicitly mention if it’s \nnegative/positive or indeterminate. Note that if HER2 results have been reported as FISH results, they \nshould always be marked as “Equivocal Positive”, “Equivocal Negative”, or “Equivocal”, and not directly \nas “Positive or Negative”. This is so because specimens that are equivocal will have follow up FISH \nstudies for HER2.  \n \nSometimes, these follow-up will look like a completely different style of path report (Molecular \npathology report). Buried in the text in these reports, you will see a line like above, that shows the status \nof the FISH HER2 test (called ERBB2 here). This will either be positive or negative (there will be no \nindeterminate result here) - check either Equivocal Positive or Equivocal Negative for these reports. We \ndo this since the FISH analysis was performed generally as a follow up test for equivocal cases (this is not \nalways true but a generalizing assumption that helps us break out what type of test was performed). For \ntext spans, make sure you include not only if the result was positive but all the description of the \ntest result (degree of IHC staining, etc.) \n \nMoreover, HER2 term itself may have been incorrectly redacted at times (example in the screenshot \nabove). In this case, if HER2 can be inferred from the context, it should still be annotated. \n \nAgain, any mentions of HER-2 in history should be ignored. \n \n\n \n28 \nMargins \n \nMargin status would be “positive” if an invasive tumor touches normal tissue, “close” if tumor was <2 \nmm from the normal tissue, and “negative” if tumor was ≥2 mm from the normal tissue. Here, the least \ndistance is considered to be the worst case. \n \nThere are often multiple specimens in the same path report, hence there may be multiple margins. At the \nnote level, the margin selected is the worst margin status (i.e. positive margin if present) for the \ndominant/worst disease site in the note. If there have been multiple resections for a given specimen (i.e. \nthe same invasive lobular cancer in the left breast), then the margin at the end of the final resections \nshould be selected. This is true even if a path report has multiple specimens from the same disease/site \nover time. \n \nTo help with text span annotations, margin information should be obtained from two places: the “Final \nPathologic Diagnosis’ Section and the ‘Comments’ section under the specimen of interest. In the \n“Comments’ section, there is often a detailed list of the margins for all directions of the sample \n(anterior/inferior, etc.). All of this should be highlighted in a text span with the label ‘tumor margin’.  \n \nThe document level class label for margin will reflect the closest margin status across all of these borders. \ni.e., if multiple specimens are present, the following priority order is established: the first choice would be \npositive margin if any tumor touches the normal tissue, the second choice would be < 2mm if the distance \nbetween the tumor and the normal tissue is <2mm, the third choice would be >= 2mm or negative \nmargins. Please pay attention that you do not make mistakes in converting between mm and cm; our \nmargin options are mentioned as mm. 1cm = 10mm, or vice versa, 1 mm = 0.1cm. Unknown should be \nselected only if the margin is not mentioned in the report. When the report only mentions \"margin clear\", \n\"free of tumor\", or \"margin negative\", it should be categorized into \"More than/eq to 2mm\". \n \nDCIS Margins \nAnnotated in the same manner as Margins, but for DCIS only. Note that the DCIS margins task has a \nseparate label for text annotations. Look for the same sections as with invasive tumor, but note that you \nshould only label DCIS margins for info corresponding to the DCIS sample. \n \nLVI \n \nLVI should be marked as present if lymphovascular invasion is known to be present, absent if it is known \nto be absent, and Unknown otherwise. Note that we are combining both extensive and non-extensive \nlymphovascular invasion into a common category of “present”. In the path note, you will generally find \nLVI in the ‘Comments’ section under the details of a specific specimen. Angiolymphatic invasion, \nlymphovascular invasion, lymphatic invasion, and vascular invasion are terms often used interchangeably \namong pathologists to describe the histologic finding of tumor cells within a vessel, and you should look \nout for either of these terms in the report. \n \n \nHow to determine the ‘worst’ disease specimen? \n \n29 \n \nWe want to ensure that the document level labels we select for a note correspond to the things we \nhighlight in the note text. If a path report has both DCIS and invasive disease, the invasive disease is the \n‘worse’ and all document level items and text highlights (unless explicitly related to DCIS) should refer to \nthe invasive disease - i.e. grade, ER/PR/HER2 should refer to the invasive disease. Determining the \n‘worst disease’ present can get tricky when the path note text has multiple path specimens with different \ntypes of invasive specimens. In general, if there are multiple invasive specimens, we aim to pick the \n‘worst’ of these and have our text labels for this sample correspond to the labels at the document level. \nThe ‘worst’ disease is tricky but general guides are that this will be the sample with the worst grade \nand/or size. If the path report has multiple samples but from different points in time (say from different \ninstitutions or before/after treatment), choose the sample that best reflects the patient's current disease \nstate at the time of the note - so after treatment or the ‘revised’ impressions.  \n \nIf there are two foci of invasive disease and they have different sizes – you should only highlight one of \nthese two foci. Which of the two foci should be annotated depends on the analysis of the ‘worst’ disease – \nthis is subjective but it'll generally be the larger of the two foci or the one with the worst grade. \nSection S2: GPT model prompts and settings \n0613 version of the GPT-3.5-turbo model and 0314 version of the GPT-4 model were used via the \nMicrosoft Azure OpenAI studio platform for all the experiments. The API version was 2023-05-15. The \nmost deterministic temperature setting of 0 was used. The outputs were retrieved via the ChatCompletion \nAPI using the prompts described next. Additional prompt engineering or hyperparameter tuning was not \nperformed. \n \nPrompts for extracting related pairs of entities for all the sub-tasks \nSystem role:  \nPretend you are a helpful Pathologist reading the given breast cancer pathology report.\"  \nProvide answers based on the pathology sample with the most aggressive or advanced cancer in the input \nreport. \nDo not use patient history to answer, only provide the current patient information as an answer. \nAnswer as concisely as possible in the given format. \nUser prompt template: \nProvide the type of pathology report, biopsy procedure type, sites examined, sites of cancer, histological subtype, \ntotal number of lymph nodes involved, estrogen receptor status, progesterone receptor status, her2 gene \namplification status, tumor grade, lympho-vascular invasion, final resection margins for invasive tumor, and final \n \n30 \nresection margin for DCIS tumor. Notes about breast reconstruction surgery, or those unrelated to breast cancer \nare irrelevant here. Unknown option refers to the case where the answer cannot be inferred from the input note. For \nall irrelevant notes, return all everything other than path_type as the numeric option for Unknown. For molecular \npathology report, report the path_type as the option for Unknown. Report the grade for treated tumors as Unknown, \nand do not report nuclear grade unless the most advanced tumor is of type DCIS. Numeric option for 'No \nmalignancy' should be reported only if none of the samples is malignant. Numeric option for DCIS should always be \nreported as a histological subtype if it is present. Numeric option for 'Others' should be reported for histological \nsubtype if a specific histological type is not discussed, but the tumor is not benign. For margins inference, if multiple \nmargins have been reported, the margins after the final resection and associated with the worst prognosis, that is \nthe one closest to the tumor, should be provided. Report DCIS margins as 'Unknown' if no DCIS tumor exists.  \nAnswer only with the most aggressive or advanced scenario in the current state. Do not use any history that has not \nbeen confirmed currently to answer.  \nAnswer from the given options for each output: \npathology type: 1: Cytology 2. Histopathology 3. Either a report for breast reconstruction surgery, or a report \nunrelated to any breast cancer 4. Unknown. \nbiopsy procedure type: 1. Biopsy 2. Lumpectomy 3. Mastectomy 4. Unknown. \nsites examined: 1. Left breast 2. Left lymph node 3. Other tissues than breast or lymph nodes 4. Right breast 5. Right \nlymph node 6. Unknown. \nsites of cancer: 1. Left breast 2. Left lymph node 3. None 4. Other tissues than breast or lymph nodes 5. Right breast \n6. Right lymph node 7. Unknown. \nhistological subtype: 1. DCIS 2. Invasive ductal carcinoma 3. Invasive lobular carcinoma 4. No malignancy was \nfound 5. Other types of carcinoma than those mentioned 6. Unknown. \ntotal number of lymph nodes involved: 1. 1 to 3 lymph nodes involved, 2. More than 10 lymph nodes involved 3. 4 to \n9 lymph nodes involved 4. No lymph nodes involved 5. Unknown. \nestrogen receptor status: 1. Negative 2. Positive 3. Unknown. \nprogesterone receptor status: 1. Negative 2. Positive 3. Unknown. \nher2 gene amplification status: 1. Equivocal or indeterminate findings 2. Negative by FISH test 3. Positive by FISH \ntest 4. Negative, but not with FISH test 5. Positive, but not with FISH test 6. Unknown. \n \n31 \ntumor grade: 1. 1 or low 2. 2 or intermediate 3: 3 or high, 4: Unknown. \nlympho-vascular invasion: 1. Absent 2. Present 3. Unknown \nfinal margins for invasive tumor: 1. Less than 2mm 2. More than or equal to 2mm 3. Positive margin 4. Unknown. \nfinal resection margins for DCIS tumor: 1. Less than 2mm 2. More than or equal to 2mm 3. Positive margin 4. \nUnknown. \nProvide the answers as a json in the following format, only using task-specific numeric options as specified above: \n            { \n            'path_type': option number for pathology type, \n            'biopsy': option number for biopsy procedure type, \n            'sites_examined': [list of all site option numbers that were examined for tumor], \n            'sites_cancer': [list of all site option numbers where cancer is found], \n            'histology': [list of all histological subtype option numbers for the most invasive tumor and DCIS], \n            'lymph_nodes_involved': option number for the group that includes the total number of lymph nodes \ninvolved, \n            'er': option number for estrogen receptor status, \n            'pr': option number for progesterone receptor status, \n            'her2': option number for her2 gene amplification status, \n            'grade': option number for tumor grade,  \n            'lvi': option number for lympho-vascular invasion, \n            'margins': option number for final margins for invasive tumor, \n            'dcis_margins': option number for final margins for dcis tumor, \n            } \nDo not provide answer as a list for anything except sites_examined, sites_cancer and histology. \nSection S3: Supervised classifier settings \nFor supervised classification models, the following settings were used:  \nEnglish language stop words were removed from pathology reports before further processing. To find the \nbest parameters for the random forests model, a random grid search was performed, using 3-fold cross-\nvalidation on the training data and 15 iterations. The explored parameters included the number of \nestimators, max depth, minimum samples split, minimum samples leaf, and maximum leaf nodes, as \n \n32 \nshown in Table ST1. n-grams in the range of [1–4] were tested to examine the impact of the length of the \nphrases on the model performance. The final model parameters were selected based on the best macro-\naverage F1 score on the validation data. The final selected length of the ngrams, depending on the \nclassification task, is provided in Table ST2. \n  \nTable ST1. Random grid search parameters for the random forests classifier. \n Range \nRF parameters minimum maximum Number of steps \nnumber of estimators 300 300 N/A \nmax depth None 50 6 \nminimum samples split 2 100 5 \nminimum leaf split 1 10 4 \nmaximum leaf nodes None 40 20 \n \nTable ST2. Random Forest final ngram count selected for each task \nTask Name n-grams stopword removal (y/n) \nPathology Type 2 n \nER 3 n \nHER2 3 y \nPR 3 n \nLVI 2 y \nMargins 3 n \nDCIS margins 3 y \nBiopsy 3 y \nGrade 3 y \nLymph node involvement 3 n \nSite Examined 2 n \nSite Disease 2 y \nHistology 3 y \n \n33 \n \nFor the LSTM model, an architecture with attention was used. Max sequence length was set as 4600 to \ninclude all pathology reports in the dataset. Furthermore, hidden layer dimensions were finally selected to \nbe 128, the model included 2 fully-connected layers, and was trained with a dropout value of 0.5 and the \nbatch size of 16 for a total of 70 epochs. Adam optimizer with 1e-5 weight decay was used for model \noptimization. Learning rate of 5e-4 was set for the tasks of pathology type classification, grade \nclassification, HER2 classification, and ER classification. For the tasks of classifying biopsy type, \nlymphovascular invasion, progesterone receptor status, margins and DCIS margins, the learning of 5e-3 \nwas used. For all other tasks, the learning rate was set as 1e-3.\n \n \nFor the UCSF-BERT model, the batch size of 16 along with 2 grad accumulation steps was used, the \nlearning rate was set to 2e-5, weight decay was set to 0., and the Adam optimizer was used with an \nepsilon of 1e-8, maximum gradient norm of 1.0, and the model was trained for 40 epochs. ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7129520773887634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6844725012779236
    },
    {
      "name": "Inference",
      "score": 0.6409745216369629
    },
    {
      "name": "Machine learning",
      "score": 0.6331713199615479
    },
    {
      "name": "Classifier (UML)",
      "score": 0.573531448841095
    },
    {
      "name": "Random forest",
      "score": 0.5395879149436951
    },
    {
      "name": "F1 score",
      "score": 0.5045892000198364
    },
    {
      "name": "Natural language processing",
      "score": 0.3956359922885895
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387154595",
      "name": "University of California Office of the President",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210167126",
      "name": "UCSF Helen Diller Family Comprehensive Cancer Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180670191",
      "name": "University of California, San Francisco",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 10
}