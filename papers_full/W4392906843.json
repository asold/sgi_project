{
  "title": "A Comparison Between GPT-3.5, GPT-4, and GPT-4V: Can the Large Language Model (ChatGPT) Pass the Japanese Board of Orthopaedic Surgery Examination?",
  "url": "https://openalex.org/W4392906843",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2946737212",
      "name": "Nozomu Nakajima",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2132148990",
      "name": "Takahito Fujimori",
      "affiliations": [
        "Sakai Municipal Hospital",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2105148188",
      "name": "Masayuki Furuya",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3136741223",
      "name": "Yuya Kanie",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2102197001",
      "name": "Hirotatsu Imai",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2142142809",
      "name": "Kosuke Kita",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2140955956",
      "name": "Keisuke Uemura",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1901710633",
      "name": "Seiji Okada",
      "affiliations": [
        "Sakai Municipal Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3203982446",
    "https://openalex.org/W1494192115",
    "https://openalex.org/W1997866278",
    "https://openalex.org/W2031184266",
    "https://openalex.org/W3209636687",
    "https://openalex.org/W2060482235",
    "https://openalex.org/W2273066798",
    "https://openalex.org/W2110293626",
    "https://openalex.org/W4307499977",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4386553312",
    "https://openalex.org/W4380685958",
    "https://openalex.org/W4385351447",
    "https://openalex.org/W4386448461",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4377220156",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4323360926",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4381953225",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4386510404",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4302603702",
    "https://openalex.org/W4292779060"
  ],
  "abstract": null,
  "full_text": "Review began\n 02/03/2024 \nReview ended\n 02/15/2024 \nPublished\n 03/18/2024\n© Copyright \n2024\nNakajima et al. This is an open access\narticle distributed under the terms of the\nCreative Commons Attribution License CC-\nBY 4.0., which permits unrestricted use,\ndistribution, and reproduction in any\nmedium, provided the original author and\nsource are credited.\nA Comparison Between GPT-3.5, GPT-4, and GPT-\n4V: Can the Large Language Model (ChatGPT)\nPass the Japanese Board of Orthopaedic Surgery\nExamination?\nNozomu Nakajima \n, \nTakahito Fujimori \n, \nMasayuki Furuya \n, \nYuya Kanie \n, \nHirotatsu Imai \n, \nKosuke Kita \n,\nKeisuke Uemura \n, \nSeiji Okada \n1.\n Orthopaedics, Sakai City Medical Center, Sakai, JPN \n2.\n Orthopaedic Surgery, Osaka University, Graduate School of\nMedicine, Suita, JPN\nCorresponding author: \nTakahito Fujimori, \ntakahito-f@hotmail.co.jp\nAbstract\nIntroduction\nRecently, large-scale language models, such as ChatGPT (OpenAI, San Francisco, CA), have evolved.\nThese models are designed to think and act like humans and possess a broad range of specialized knowledge.\nGPT-3.5 was reported to be at a level of passing the United States Medical Licensing Examination. Its\ncapabilities continue to evolve, and in October 2023, GPT-4V became available as a model capable of image\nrecognition. Therefore, it is important to know the current performance of these models because they will be\nsoon incorporated into medical practice. We aimed to evaluate the performance of ChatGPT in the field of\northopedic surgery.\nMethods\nWe used three years’ worth of Japanese Board of Orthopaedic Surgery Examinations (JBOSE) conducted in\n2021, 2022, and 2023. Questions and their multiple-choice answers were used in their original Japanese\nform, as was the official examination rubric. We inputted these questions into three versions of ChatGPT:\nGPT-3.5, GPT-4, and GPT-4V. For image-based questions, we inputted only textual statements for GPT-3.5\nand GPT-4, and both image and textual statements for GPT-4V. As the minimum scoring rate acquired to\npass is not officially disclosed, it was calculated using publicly available data.\nResults\nThe estimated minimum scoring rate acquired to pass was calculated as 50.1% (43.7-53.8%). For GPT-4, even\nwhen answering all questions, including the image-based ones, the percentage of correct answers was 59%\n(55-61%) and GPT-4 was able to achieve the passing line. When excluding image-based questions, the score\nreached 67% (63-73%). For GPT-3.5, the percentage was limited to 30% (28-32%), and this version could not\npass the examination. There was a significant difference in the performance between GPT-4 and GPT-3.5 (p\n< 0.001). For image-based questions, the percentage of correct answers was 25% in GPT-3.5, 38% in GPT-4,\nand 38% in GPT-4V. There was no significant difference in the performance for image-based questions\nbetween GPT-4 and GPT-4V.\nConclusions\nChatGPT had enough performance to pass the orthopedic specialist examination. After adding further\ntraining data such as images, ChatGPT is expected to be applied to the orthopedics field.\nCategories:\n Orthopedics\nKeywords:\n japanese board of orthopaedic surgery examination, gpt-4v, chatgpt, large language model, artificial\nintelligence\nIntroduction\nMachines have long been valuable partners to humanity. In our daily lives, we are surrounded by many\nmachines. Modern artificial intelligence (AI), which is attracting significant attention these days, is designed\nto think and act like humans \n[1-3]\n. In the 1970s, attention was drawn to expert systems that aimed to\nreplicate professional advice by teaching machines vast amounts of specialized knowledge \n[4,5]\n. A prime\nexample, Mycin, could suggest appropriate antibiotics based on the entered information \n[6-8]\n. However,\nthese systems required extensive manual data entry by experts, and due to their rigid rule-based structure,\nthey had limited application \n[1,9]\n. Nevertheless, these limitations are being overcome with advancements in\nmachine learning technologies. Introduced by OpenAI (San Francisco, CA) in October 2022, ChatGPT (GPT-\n3.5) is a prominent large language model. It can instantly generate text based on the vast knowledge\n1\n2\n2\n2\n2\n2\n2\n2\n \n Open Access Original\nArticle\n \nDOI:\n 10.7759/cureus.56402\nHow to cite this article\nNakajima N, Fujimori T, Furuya M, et al. (March 18, 2024) A Comparison Between GPT-3.5, GPT-4, and GPT-4V: Can the Large Language Model\n(ChatGPT) Pass the Japanese Board of Orthopaedic Surgery Examination?. Cureus 16(3): e56402. \nDOI 10.7759/cureus.56402\nacquired from web data and textbooks. It supports multiple languages and is characterized by its\nconversational skills, making interactions feel like one is speaking to a human \n[10]\n.\nSince GPT-3.5 was released, it has attracted significant attention worldwide for its usefulness, completeness,\nand high accuracy. As of October 2023, a search for “Large Language Model” in PubMed has yielded 8182\nhits, indicating a high level of attention in the medical field. GPT-3.5 was reported to be at a level of passing\nthe United States Medical Licensing Examination (USMLE) \n[11]\n. Furthermore, GPT-4, an upgraded model of\nGPT-3.5, was released in March 2023. Its accuracy was in the top 10% of the US bar exam (compared with the\nbottom 10% for the GPT-3.5) \n[10]\n. This AI is not only capable of natural conversation but also possesses a\nbroad range of specialized knowledge. It is inevitable that, in the future, it will be utilized in the medical\nfield as an AI that can provide expert advice.\nIn the field of orthopedics, GPT-4 corresponded to the average performance of post-graduate year (PGY)-5 in\nthe Orthopedic In-Training Examination and exceeded the passing score of the American Board of\nOrthopedic Surgery Part 1 Examination, while GPT-3.5 only corresponded to a PGY-1 level \n[12]\n. However,\nthese evaluations were conducted in English, and their performance inputted in Japanese has not yet been\nassessed. Furthermore, in October 2023, GPT-4V became available, enabling bimodal input through both\ntext and images. No studies have attempted image-based questions using GPT-4V. This study aimed to\nevaluate the performance of ChatGPT, one of the representative large-scale language models, on the\nJapanese Board of Orthopaedic Surgery Examination (JBOSE).\nMaterials And Methods\nThe Japanese Board of Orthopaedic Surgery Examination (JBOSE)\nThis examination is held once a year, consists of 100 questions, and tests a wide range of knowledge related\nto orthopedics from basic medicine, such as bone metabolism and pathology, to specialized fields, such as\nconservative therapy, surgical treatment, rehabilitation, and medical insurance. After passing the National\nMedical Examination in Japan, the applicant must complete two years of initial clinical training, followed by\nthree years and nine months of specialized orthopedic surgery training to be eligible for the examination.\nThe time limit is 120 minutes, and the questions are presented in computer-based testing (CBT) format. Each\nquestion consists of a statement and five options. There are three types of questions requiring the selection\nof one, two, or three correct answers. JBOSE 33rd, 34th, and 35th, conducted in 2021, 2022, and 2023, were\nused to evaluate the performance of ChatGPT. We focused on the examinations from the past three\nyears because the format changed after the 33rd session, shifting from a combination of written and\ninterview examinations to the CBT format. The questions and correct answers were obtained from the\nJapanese Orthopaedic Association journal \n[13-15]\n. Except for a few questions that were officially announced\nto be incomplete, all questions were inputted to ChatGPT (Figure \n1\n).\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n2\n of \n11\nFIGURE\n 1: Workflow of this study\nThe Japanese Board of Orthopaedic Surgery Examination (JBOSE) included text-based and image-based\nquestions. A few questions were officially excluded after the examination because of their incompleteness. GPT-4V\nenables bimodal input through both text and images. The 33rd, 34th, and 35th are ordinal numbers in exams, and\nthey were conducted in 2021, 2022, and 2023, respectively.\nJBOSE includes text-based and image-based questions. Image-based questions include both textual\nstatements and images. Although image information is helpful in predicting the correct answers, it is\npossible to guess the answer from the textual statements alone, even without images. Because GPT-3.5 and\nGPT-4 only accept textual input, image-based questions were inputted only as textual statements.\nLarge language model\nThe models used in this study were GPT-3.5 (standard model), GPT-4 (high-grade model), and GPT-4V (text\nand image input model), all of which were large language models developed by OpenAI (San Francisco, CA).\nWhile GPT-3.5 was available for free, GPT-4 and GPT-4V required a monthly fee of US$20 as of January 2024.\nThese models generated text based on vast knowledge across various fields and were characterized by their\nconversational skills, making interactions similar to those of humans. We examined its knowledge in the\northopedic field using JBOSE. The GPT-4V model became available in October 2023. GPT-4V is capable of\nrecognizing images as well as text, therefore we added a verification step to confirm GPT-4V’s performance\nin recognizing medical images.\nData input\nQuestions and their multiple-choice answers from the JBOSE were used in their original Japanese form, as\nwas the official examination rubric. Instructions for using ChatGPT were also provided in Japanese. Because\nGPT-3.5 and GPT-4 were not able to recognize images, we input text data only. In other words, for image-\nbased questions, GPT3.5 and GPT4 answered without images, as a reference, only text data for the questions.\nWe employed prompt engineering techniques to ensure consistency in the interaction and emulation of a\nclinical examination setting. The following statement was instructed, and then each question was entered\none by one: “You are an orthopedic specialist. We will present a test, so please answer the following\nquestions. Make sure to answer each question carefully without any mistakes.” This approach was designed\nto prime ChatGPT with a specific role and mindset as an orthopedic specialist. When multiple questions\nwere presented at once, there was a risk of information overload and scattered attention to specific\nquestions. Therefore, in each interaction, we inputted questions individually.\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n3\n of \n11\nEvaluation\nTo determine whether GPT-3.5 and GPT-4 could pass the orthopedic specialist examination, the accuracy\nwas calculated separately for the entire set of questions and the text-based questions. Accuracy was defined\nas the percentage of correct answers. As the minimum scoring rate acquired to pass is not officially\ndisclosed, it was determined using publicly available data (total examinees, number of passers, average\nscore, and standard deviation) using the following formula: passing score = Z × standard deviation + average\nscore. Z-score corresponds to the value derived from the standard normal distribution table for “1 − number\nof passers/total examinees.”\nModified question\nTo further assess the accuracy of GPT-4, we introduced a “Modified Question” format to the text-based\nquestions. In this approach, the number of correct options was concealed during the question. In other\nwords, the model was instructed to respond without knowing how many correct answers were present among\nthe options. For instance, a question like “Choose the three correct options” would be posed as “Select all\nthe correct options.” The model was assessed on its ability to judge all given options correctly. We presented\nGPT-4 with modified questions created from the text-based questions of the 33rd to 35th JBOSE and\nexamined its performance and reproducibility. “Reproducibility” was defined as the proportion of questions\nthat were answered correctly in the modified questions out of those that were correctly answered in the\nconventional questions.\nStatistical analysis\nStatistical analysis was conducted using Welch's t-test to compare the performance of GPT-3.5 and GPT-4.\nAll statistical tests were two-tailed, and a p-value of less than 0.05 was considered statistically significant.\nStatistical analysis was performed using SPSS Statistics version 20 software (IBM Corp., Armonk, NY).\nResults\nDemographic data\nThe overview of the examination is shown in Table \n1\n.\nJBOSE\n33rd\n34th\n35th\nNumber of examinees (passers/total)\n1021/1048\n603/653\n530/591\nPassing rate among all examinees (%)\n97\n92\n90\nNumber of questions (total/text-based/image-based)\n98/77/21\n99/73/26\n97/64/33\nAverage score (%)\n61\n66\n64\nStandard deviation (%)\n9\n8.5\n8.9\nEstimated minimum scoring rate acquired to pass (%)\n43.7\n53.8\n52.7\nTABLE\n 1: The overview of the 33rd-35th Japanese Board of Orthopaedic Surgery Examination\n(JBOSE) and the performance of GPT-3.5, GPT-4, and GPT-4V\nThe estimated minimum scoring rate acquired to pass was calculated as 43.7% for the 33rd, 53.8% for the\n34th, and 52.7% for the 35th examination. The pass rate among all examinees is typically around 90% each\nyear. However, the 33rd exam, being the first to adopt the CBT format, saw a higher pass rate of 97%,\nexceeding the usual average. Subsequently, the pass rates for the 34th and 35th exams were 92% and 90%,\nrespectively. There were 77 (79%) text-based questions and 21 (21%) image-based questions in the 33rd\nexamination, 73 (74%) and 26 (26%) in the 34th examination, and 64 (66%) and 33 (34%) in the 35th\nexamination, respectively.\nPerformance of each GPT model\nFor GPT-4, even when answering all questions, including the image-based ones, the accuracy was 60%, 55%,\nand 61% for each examination. When excluding image-based questions, it was 64%, 63%, and 73%, which\nwas comparable with the average score of examinees. In both cases, the scores achieved the pass line. The\naccuracy for image-based questions was lower than that for text-based questions. For GPT-4V, the accuracy\nin image-based questions was 38%, 35%, and 39%, which surpassed that of GPT-4 in the 34th (31%) and 35th\n(36%) examinations but fell below GPT-4’s performance in the 33rd (48%) examination. For GPT-3.5, the\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n4\n of \n11\naccuracy was limited to 28%, 32%, and 30%, which was unable to achieve a pass in any of the years (Table \n2\n).\n \nJBOSE\n33rd\n34th\n35th\nNumber of correct answers/total questions, accuracy (%)\nGPT-4\nAll questions\n59/98, (60)\n54/99, (55)\n59/97, (61)\nText-based questions\n49/77, (64)\n46/73, (63)\n47/64, (73)\nImage-based questions without image\n10/21, (48)\n8/26, (31)\n12/33, (36)\nGPT-4V\nImage-based questions with image\n8/21, (38)\n9/26, (35)\n13/33, (39)\nGPT-3.5\nAll questions\n27/98, (28)\n32/99, (32)\n29/97, (30)\nText-based questions\n20/77, (26)\n25/73, (34)\n23/64, (36)\nImage-based questions without image\n7/21, (33)\n7/26, (27)\n6/33, (18)\nTABLE\n 2: The accuracy of GPT-3.5, GPT-4, and GPT-4V for the 33rd-35th Japanese Board of\nOrthopaedic Surgery Examination (JBOSE)\nThe accuracy was defined as the percentage of correct answers. All questions include both text-based and image-based questions.\nFor GPT-4, the average accuracies were 59% for all questions, 66% for text-based questions, and 38% for\nimage-based questions. In comparison, GPT-3.5 achieved 30% for all questions, 32% for text-based\nquestions, and 25% for image-based questions. A significant difference was observed between GPT-4 and\nGPT-3.5 in text-based questions (p = 0.002; Table \n3\n).\n \nGPT-4\nGPT-3.5\nGPT-4V\np-value\nNumber of correct answers/total questions, accuracy (%)\nAll questions\n172/294, (59)\n88/294, (30)\nN.A.\n<0.001\n*§\nText-based questions\n142/214, (66)\n68/214, (32)\nN.A.\n0.002\n*§\nImage-based questions without image\n30/80, (38)\n20/80, (25)\nN.A.\n0.1\n§\nImage-based questions with image\nN.A.\nN.A.\n30/80, (38)\n0.9\n†\nTABLE\n 3: Average of three-year total accuracy of GPT-4, GPT-3.5, and GPT-4V for all questions,\ntext-based questions, and image-based questions\nThe accuracy was defined as the percentage of correct answers. All questions include both text-based and image-based questions.\n*\n p < 0.05. \n† \np-value was calculated between GPT-4 and GPT-4V. \n§ \np-value was calculated between GPT-4 and GPT-3.5. N.A.: not available.\nPerformance of GPT-4 for modified question\nWe made ChatGPT answer the modified questions that had no information about the number of correct\noptions. Modified questions were made for all text-based questions in the 33rd to 35th examination\n(n = 214). Out of the questions correctly answered in the conventional questions (n = 142), 79 were also\ncorrectly answered in the modified questions, resulting in a reproducibility of 56%. Additionally, there were\na few questions (n = 6) answered correctly in the modified questions, but answered incorrectly in the\nconventional questions (Figure \n2\n).\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n5\n of \n11\nFIGURE\n 2: GPT-4’s performance on the modified question and its\nreproducibility\nTo further assess the performance of GPT-4, a “Modified Question” was made for all text-based questions in the\n33rd-35th examination (n = 214), concealing the number of correct options. Reproducibility was defined as the\nproportion of questions answered correctly in the modified question format (A) out of those correctly answered in\nthe conventional question format (A+B).\nDiscussion\nWe evaluated ChatGPT's performance in the orthopedic field using the JBOSE. While GPT-3.5 did not reach\nthe passing line for the exam, GPT-4 achieved the passing line despite including image-based questions.\nAlthough GPT-4 could not recognize the image, the GPT-4’s accuracy for the image-based questions reached\n38%. This fact suggests that GPT-4 may have advanced reasoning skills. It has been reported that GPT-4’s\nadvancements were not merely due to increased training data; it can now interpret complex texts and grasp\nnuanced differences \n[10]\n. We think that GPT-4 was able to deduce correct answers even from incomplete\nquestions using its clinical reasoning skills. The percentage of correct answers on GPT-4 for all questions\nwas categorized by subject type (Figure \n3\n).\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n6\n of \n11\nFIGURE\n 3: Percentage of correct answers and image-based questions\ncategorized by subject type\nThe percentage of correct answers on GPT-4 for all questions was categorized by subject type. There was a\ntendency for the accuracy rate to increase with a lower proportion of image-based questions.\nWhile the accuracy rate was higher for the basic science, which asked for simple knowledge, it was lower for\nquestions involving image-based questions.\nSince October 2023, GPT-4V has become available and features image recognition capabilities. However, the\naccuracy of image-based questions showed no improvement. There was a tendency for questions incorrectly\nanswered by GPT-4 to also be answered incorrectly by GPT-4V. Specific examples of responses by GPT-4 and\nGPT-4V to image-based questions are shown in Figure \n4\n.\nFIGURE\n 4: The examples of responses of GPT-4 and GPT-4V to image-\nbased questions.\nEven if images were provided to GPT-4V, the responses were similar to GPT-4, in which no image was provided.\n(A) An example of a question that includes both textual statements and images. X-ray images were omitted for\ncopyright reasons. The textual statement was translated from Japanese.\n(B) The answer obtained from GPT-4.\n(C) The answer obtained from GPT-4V.\nImage credits: Nozomu Nakajima.\nWhile the presence of a fracture was recognized with the inclusion of an image, GPT-4V failed to identify the\nspecific location of the fracture or the characteristic changes necessary for diagnosis. Consequently, the\nresponses were similar to those given when no image was provided. We think that GPT-4V has been trained\nsufficiently for medical imaging due to the difficulties in acquiring extensive image data, given the concerns\nover personal privacy.\nThe utility of ChatGPT in the medical field is gathering attention, and several existing studies support this\nnotion \n[11,12,16-21]\n. We summarized in Table \n4\n research compiling examinations that evaluated knowledge\nusing ChatGPT.\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n7\n of \n11\n \nAuthor (year)\nExam\nLanguage\nModel\nResult\nGeneral\nMedicine\nKung et al. (2023) \n[11]\nUnited States Medical Licensing Examination\nEnglish\nGPT-\n3.5\nPass\nTakagi et al. (2023)\n[16]\nJapanese Medical Licensing Examination\nJapanese\nGPT-4\nPass\nOrthopedics\nKung et al. (2023) \n[12]\nOrthopaedic In-Training Examination\nEnglish\nGPT-4\nPass\nGPT-\n3.5\nFail\nSaad et al. (2023) \n[17]\nOrthopaedic Fellow of the Royal College of Surgeons\nEnglish\nGPT-4\nFail\nMassey et al. (2023)\n[18]\nResStudy Orthopaedic Examination Question Bank\nEnglish\nGPT-4\nFail\nNeurosurgery\nAli et al. (2023) \n[19]\nNeurosurgery Written Board Examinations\nEnglish\nGPT-4\nPass\nGPT-\n3.5\nPass\nGastroenterology\nSuchman et al. (2023)\n[20]\nAmerican College of Gastroenterology Self-Assessment Test\nEnglish\nGPT-4\nFail\nRadiology\nBhayana et al. (2023)\n[21]\nCanadian Royal College and American Board of Radiology\nexaminations\nEnglish\nGPT-\n3.5\nPass\nTABLE\n 4: The summary of existing studies compiling examinations that evaluate knowledge\nusing ChatGPT\nWhile there were slight variations in the prompts, we found that GPT-3.5 could pass the USMLE \n[11]\n.\nHowever, one of the notable limitations of GPT-3.5 was its lower accuracy in non-English languages \n[22]\n,\nwhich was particularly evident in the Japanese medical licensing exam where GPT-3.5 could not achieve a\npass level. In contrast, GPT-4 showed significant improvement, successfully passing the same exam \n[16]\n.\nThis enhancement in GPT-4’s performance in non-English languages marks a critical step in its applicability\nin global medical contexts. In studies of specialties, most of the exams were administered in English. It was\nreported that ChatGPT achieved the passing line in orthopedics, neurosurgery, and radiology examinations\n[12,19,21]\n. On the other hand, some reports reported that ChatGPT has not yet reached the level of a\nspecialist \n[17,18,20]\n. Our study indicated that GPT-4 may have knowledge at the level of the Japanese license\nof an orthopedic specialist who has trained for five years and nine months after obtaining their medical\nlicense \n[10,23]\n. Although it is not possible to make a simple comparison with examinations in English, as the\ndifficulty level and the passing line are different, it is apparent that ChatGPT has reached a certain level in\nthe Japanese orthopedic field.\nExpert systems, developed in the 1970s, stood as AI designed to mimic human knowledge in specific\ndomains. These systems aimed to emulate experts by absorbing vast amounts of precise information. Mycin\nwas the typical model. It could guess the identity of bacteria and suggest appropriate antibiotics from the\nprogrammed 200 rules by entering the characteristics of the patient (Figure \n5\n) \n[8]\n.\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n8\n of \n11\nFIGURE\n 5: An example of a conversation with Mycin\nTo enter the characteristics of the patient, Mycin can guess the identity of bacteria and suggest appropriate\nantibiotics from the 200 rules.\n(A) An example of a conversation with Mycin.\n(B) One example out of 200 rules.\nImage credits: Nozomu Nakajima.\nHowever, despite the innovative system, they were not widely used because they required experts to\nmanually input knowledge, making updates difficult. Additionally, being rule-based, these systems could\nonly process inputs in a predetermined manner, restricting their adaptability \n[4,5]\n. In contrast, ChatGPT\nboasts a wide-ranging knowledge base, allowing for flexible, rapid responses in conversational formats,\nhighlighting its superiority \n[23,24]\n. While remaining adaptable, ChatGPT has rapidly improved in accuracy to\na level that would pass the JBOSE \n[25]\n. ChatGPT has the potential to evolve into a state-of-the-art expert\nsystem by further increasing knowledge in specialized areas.\nIn addition to issues like response reliability, a challenge known as “Artificial Hallucination” is highlighted.\nThis refers to AI generating incorrect or unsubstantiated information as if it were factual, a significant\nconcern with large language models \n[26,27]\n. While the rate of hallucinations in GPT-4 is reportedly\ndecreasing, it is still unsatisfactory \n[28]\n. These hallucinations, essentially AI “misconceptions” based on\nfalse information, can be hard to discern without expert knowledge. This leads to potential misreading and\nunderscores the need for further refinement.\nAlthough GPT-4 corresponded to the passing level, the performance on modified questions (where the\nnumber of correct options was concealed) was lower compared with conventional questions. In some cases,\nonce a question was answered correctly, it would respond incorrectly when asked at a different time. There\nwas some randomness in ChatGPT’s responses because it is based on transformer architecture, which works\nby predicting the next word in a sequence based on probabilities. It analyzes the context and previous words\nto estimate which word might logically follow. This process, drawing from extensive training data, results in\nsome unpredictability or randomness in the responses \n[10,24]\n.\nThis study had some limitations. First, ChatGPT is continuously evolving through user feedback, which\nmeans that outcomes might vary depending on the timing of the test. The evaluations of GPT-3.5 and GPT-4\nwere conducted in August 2023, while GPT-4V was tested in October. Similarly, the randomness of\nChatGPT's responses could also affect the results, but each model was tested only once in this test. Second,\nthe quality of images of image-based questions was not very high. For this study, images were downloaded as\nmonochrome PDFs from the society's website. However, in the actual examination, more high-quality color\nimages are available on computer screens. This may lead to GPT-4V’s failure to produce a better\nperformance than GPT-4 in the image-based questions.\nConclusions\nThis study demonstrated that ChatGPT has the performance to pass the Japanese orthopedic specialist\nexamination. However, even if ChatGPT can pass the exam, it cannot replace the clinician as is. \"Artificial\nhallucinations\" can cause harm to patients, and the use of such technology in medical practice should be\ncarefully phased in.\nHowever, with its extensive knowledge across a wide range of fields beyond medicine, the user-friendly chat\nformat, and multimodal functionalities like image recognition and web search, ChatGPT has the potential to\nbe a potent support tool. Therefore, we continuously need to evaluate and improve the evolving capabilities\nof AI to ensure its beneficial and safe use in the medical field.\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n9\n of \n11\nAdditional Information\nAuthor Contributions\nAll authors have reviewed the final version to be published and agreed to be accountable for all aspects of the\nwork.\nConcept and design:\n  \nTakahito Fujimori, Nozomu Nakajima, Masayuki Furuya, Yuya Kanie, Hirotatsu Imai,\nKosuke Kita, Keisuke Uemura, Seiji Okada\nCritical review of the manuscript for important intellectual content:\n  \nTakahito Fujimori, Nozomu\nNakajima, Masayuki Furuya, Yuya Kanie, Hirotatsu Imai, Kosuke Kita, Keisuke Uemura, Seiji Okada\nSupervision:\n  \nTakahito Fujimori\nAcquisition, analysis, or interpretation of data:\n  \nNozomu Nakajima\nDrafting of the manuscript:\n  \nNozomu Nakajima\nDisclosures\nHuman subjects:\n All authors have confirmed that this study did not involve human participants or tissue.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n All authors have declared that no financial support was received from\nany organization for the submitted work. \nFinancial relationships:\n All authors have declared that they have\nno financial relationships at present or within the previous three years with any organizations that might\nhave an interest in the submitted work. \nOther relationships:\n All authors have declared that there are no\nother relationships or activities that could appear to have influenced the submitted work.\nReferences\n1\n. \nRussell S, Norvig P: \nArtificial Intelligence: A Modern Approach. Third Edition\n. Prentice Hall, Hoboken, NJ;\n2010.\n2\n. \nGhahramani Z: \nProbabilistic machine learning and artificial intelligence\n. Nature. 2015, 521:452-9.\n10.1038/nature14541\n3\n. \nRamesh AN, Kambhampati C, Monson JR, Drew PJ: \nArtificial intelligence in medicine\n. Ann R Coll Surg Engl.\n2004, 86:334-8. \n10.1308/147870804290\n4\n. \nDuda RO, Shortliffe EH: \nExpert systems research\n. Science. 1983, 220:261-8. \n10.1126/science.6340198\n5\n. \nAdami C: \nA brief history of artificial intelligence research\n. Artif Life. 2021, 27:131-7. \n10.1162/artl_a_00349\n6\n. \nDavis R, Buchanan B, Shortliffe E: \nProduction rules as a representation for a knowledge-based consultation\nprogram\n. Artif Intell. 1977, 8:15-45. \n10.1016/0004-3702(77)90003-0\n7\n. \nShortliffe EH: \nMycin: a knowledge-based computer program applied to infectious diseases\n. Proc Annu Symp\nComput Appl Med Care. 1977, 66-9.\n8\n. \nShortliffe EH: \nComputer-Based Medical Consultations: Mycin\n. Elsevier, Amsterdam, Netherlands; 1976.\n10.1016/B978-0-444-00179-5.X5001-X\n9\n. \nHasman A: \nMy journey through the field of medical informatics\n. Stud Health Technol Inform. 2022, 300:38-\n52. \n10.3233/SHTI220940\n10\n. \nOpen AI. GPT-4 Technical Report\n. (2023). Accessed: January 10, 2024: \nhttps://cdn.openai.com/papers/gpt-\n4.pdf\n.\n11\n. \nKung TH, Cheatham M, Medenilla A, et al.: \nPerformance of ChatGPT on USMLE: potential for AI-assisted\nmedical education using large language models\n. PLOS Digit Health. 2023, 2:e0000198.\n10.1371/journal.pdig.0000198\n12\n. \nKung JE, Marshall C, Gauthier C, Gonzalez TA, Jackson JB 3rd: \nEvaluating ChatGPT performance on the\northopaedic in-training examination\n. JB JS Open Access. 2023, 8:e23.00056. \n10.2106/JBJS.OA.23.00056\n13\n. \nYamada H, Hosogane N, Adachi N: \nAbout 35th orthopaedic specialist examination\n. J Jpn Orthop Assoc. 2023,\n97:317-53.\n14\n. \nYamada H, Taniguchi N, Hosogane N: \nAbout 34th orthopaedic specialist examination\n. J Jpn Orthop Assoc.\n2022, 96:309-49.\n15\n. \nMatsuyama Y, Kawano H, Yamada H: \nAbout 33rd orthopaedic specialist examination\n. J Jpn Orthop Assoc.\n2021, 95:305-47.\n16\n. \nTakagi S, Watari T, Erabi A, Sakaguchi K: \nPerformance of GPT-3.5 and GPT-4 on the Japanese Medical\nLicensing Examination: comparison study\n. JMIR Med Educ. 2023, 9:e48002. \n10.2196/48002\n17\n. \nSaad A, Iyengar KP, Kurisunkal V, Botchu R: \nAssessing ChatGPT's ability to pass the FRCS orthopaedic part\nA exam: a critical analysis\n. Surgeon. 2023, 21:263-6. \n10.1016/j.surge.2023.07.001\n18\n. \nMassey PA, Montgomery C, Zhang AS: \nComparison of ChatGPT-3.5, ChatGPT-4, and orthopaedic resident\nperformance on orthopaedic assessment examinations\n. J Am Acad Orthop Surg. 2023, 31:1173-9.\n10.5435/JAAOS-D-23-00396\n19\n. \nAli R, Tang OY, Connolly ID, et al.: \nPerformance of ChatGPT and GPT-4 on neurosurgery written board\nexaminations\n. Neurosurgery. 2023, 93:1353-65. \n10.1227/neu.0000000000002632\n20\n. \nSuchman K, Garg S, Trindade AJ: \nChat Generative Pretrained Transformer fails the multiple-choice\nAmerican College of Gastroenterology self-assessment test\n. Am J Gastroenterol. 2023, 118:2280-2.\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n10\n of \n11\n10.14309/ajg.0000000000002320\n21\n. \nBhayana R, Krishna S, Bleakney RR: \nPerformance of ChatGPT on a radiology board-style examination:\ninsights into current strengths and limitations\n. Radiology. 2023, 307:e230582. \n10.1148/radiol.230582\n22\n. \nSeghier ML: \nChatGPT: not all languages are equal\n. Nature. 2023, 615:216. \n10.1038/d41586-023-00680-3\n23\n. \nBrown T, Mann B, Ryder N, et al.: \nLanguage models are few-shot learners\n. Adv Neural Inf Process Syst.\n2020, 33:1877-901.\n24\n. \nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L: \nAttention is all you need\n. Adv\nNeural Inf Process Syst. 2017, 30:1-11.\n25\n. \nGPT-4 API general availability and deprecation of older models in the Completions API\n. (2023). Accessed:\nJanuary 10, 2024: \nhttps://openai.com/blog/gpt-4-api-general-availability\n.\n26\n. \nGoddard J: \nHallucinations in ChatGPT: a cautionary tale for biomedical researcher\n. Am J Med. 2023,\n136:1059-60. \n10.1016/j.amjmed.2023.06.012\n27\n. \nSallam M: \nChatGPT utility in healthcare education, research, and practice: systematic review on the\npromising perspectives and valid concerns\n. Healthcare (Basel). 2023, 11:887. \n10.3390/healthcare11060887\n28\n. \nWalters WH, Wilder EI: \nFabrication and errors in the bibliographic citations generated by ChatGPT\n. Sci Rep.\n2023, 13:14045. \n10.1038/s41598-023-41032-5\n2024 Nakajima et al. Cureus 16(3): e56402. DOI 10.7759/cureus.56402\n11\n of \n11",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.4206179678440094
    },
    {
      "name": "General surgery",
      "score": 0.35613301396369934
    },
    {
      "name": "Medical physics",
      "score": 0.34214985370635986
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210104391",
      "name": "Sakai Municipal Hospital",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I98285908",
      "name": "The University of Osaka",
      "country": "JP"
    }
  ]
}