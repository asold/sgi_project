{
  "title": "Neural Polysynthetic Language Modelling",
  "url": "https://openalex.org/W3024787440",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287567167",
      "name": "Schwartz, Lane",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228052136",
      "name": "Tyers, Francis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294508315",
      "name": "Levin, Lori",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287002470",
      "name": "Kirov, Christo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301740919",
      "name": "Littell, Patrick",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lo, Chi-kiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223751354",
      "name": "Prud'hommeaux, Emily",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225978889",
      "name": "Park, Hyunji Hayley",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Steimel, Kenneth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297419203",
      "name": "Knowles, Rebecca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378456134",
      "name": "Micher, Jeffrey",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Strunk, Lonny",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106760909",
      "name": "Liu Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4291966487",
      "name": "Haley, Coleman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223798088",
      "name": "Zhang, Katherine J.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jimmerson, Robbie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2808896290",
      "name": "Andriyanets Vasilisa",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Muis, Aldrian Obaja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2078731886",
      "name": "Otani Naoki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2293524961",
      "name": "Park， Jong Hyuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350210011",
      "name": "Zhang, Zhisong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953092638",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2545319977",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W2515737026",
    "https://openalex.org/W2803939019",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2024060665",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2949973181",
    "https://openalex.org/W2964034111",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2013494846",
    "https://openalex.org/W573135173",
    "https://openalex.org/W3172431930",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1485830925",
    "https://openalex.org/W2805910418",
    "https://openalex.org/W2250618788",
    "https://openalex.org/W2079145130",
    "https://openalex.org/W2897758760",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2059800182",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2007035566",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2996181438",
    "https://openalex.org/W2963359165",
    "https://openalex.org/W24718583",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W2903188467",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2164529863",
    "https://openalex.org/W2250523604",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2250761258",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2743686029",
    "https://openalex.org/W177261865",
    "https://openalex.org/W201288405",
    "https://openalex.org/W1922655562",
    "https://openalex.org/W2901203455",
    "https://openalex.org/W2887196178",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3089254180",
    "https://openalex.org/W3088689279",
    "https://openalex.org/W46679369",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2251994258",
    "https://openalex.org/W3109323240",
    "https://openalex.org/W2887920589",
    "https://openalex.org/W1491760260",
    "https://openalex.org/W2894551254",
    "https://openalex.org/W2805588039",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963091326",
    "https://openalex.org/W3029985558",
    "https://openalex.org/W2167925143",
    "https://openalex.org/W2778814079",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2555745756",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W2562606904",
    "https://openalex.org/W2897058237",
    "https://openalex.org/W2890225082"
  ],
  "abstract": "Research in natural language processing commonly assumes that approaches that work well for English and and other widely-used languages are \"language agnostic\". In high-resource languages, especially those that are analytic, a common approach is to treat morphologically-distinct variants of a common root as completely independent word types. This assumes, that there are limited morphological inflections per root, and that the majority will appear in a large enough corpus, so that the model can adequately learn statistics about each form. Approaches like stemming, lemmatization, or subword segmentation are often used when either of those assumptions do not hold, particularly in the case of synthetic languages like Spanish or Russian that have more inflection than English. In the literature, languages like Finnish or Turkish are held up as extreme examples of complexity that challenge common modelling assumptions. Yet, when considering all of the world's languages, Finnish and Turkish are closer to the average case. When we consider polysynthetic languages (those at the extreme of morphological complexity), approaches like stemming, lemmatization, or subword modelling may not suffice. These languages have very high numbers of hapax legomena, showing the need for appropriate morphological handling of words, without which it is not possible for a model to capture enough word statistics. We examine the current state-of-the-art in language modelling, machine translation, and text prediction for four polysynthetic languages: Guaraní, St. Lawrence Island Yupik, Central Alaskan Yupik, and Inuktitut. We then propose a novel framework for language modelling that combines knowledge representations from finite-state morphological analyzers with Tensor Product Representations in order to enable neural language models capable of handling the full range of typologically variant languages.",
  "full_text": "Final Report\nof the\nFrederick Jelinek Memorial Summer Workshop\non\nNeural Polysynthetic\nLanguage Modelling\narXiv:2005.05477v2  [cs.CL]  13 May 2020\nii\nMay 2020\niii\nSixth Frederick Jelinek Memorial Summer Workshop\n24 June – 02 August 2019\nÉcole de Technologie Supérieure\nMontréal, Québec, Canada\niv\nAcknowledgements\nThe work described herein was performed by the Neural Polysynthetic Language Modelling team at the Sixth\nFrederick Jelinek Memorial Summer Workshop, which was organized and sponsored by Johns Hopkins University\nwith unrestricted gifts from Amazon, Facebook, Google, and Microsoft. This work utilizes resources supported\nby the National Science Foundation’s Major Research Instrumentation program, grant #1725729, as well as the\nUniversity of Illinois at Urbana-Champaign. This article contains output of a research project implemented as\npart of the Basic Research Programme at the National Research University Higher School of Economics (HSE\nUniversity).\nThis workshop took place at École de technologie supérieure in Montréal, Québec, Canada on the traditional\nterritory of the Kanien’kehá:ka people. The ongoing research at our home institutions in Illinois, Indiana, Penn-\nsylvania, Maryland, Massachusetts, New York, Colorado, Washington, and Ontario takes place on the traditional\nterritories of numerous indigenous peoples. Our work at the University of Illinois takes place on the lands of\nthe Peoria, Kaskaskia, Piankashaw, Wea, Miami, Mascoutin, Odawa, Sauk, Mesquaki, Kickapoo, Potawatomi,\nOjibwe, and Chickasaw peoples. Our work at Indiana University Bloomington takes place on the lands of the\nMiami, Lenni Lenape, Potawatomi, and Shawnee peoples. Our work at Carnegie Mellon University takes place\non the lands of the Lenni Lenape, Shawnee, and Haudenosaunee Nations. Our work at NRC Canada in Ottawa\ntakes place on the traditional and unceded territory of the Algonquin Nation. Our work at Rochester Institute\nof Technology takes place on Onödawa’ga:’ land. Our work at the University of Colorado Boulder takes place\non the traditional lands of the Ute, Cheyenne, and Arapaho peoples. Our work at the University of Washington\ntakes place on the traditional lands of the Suquamish, Tulalip and Muckleshoot nations. Our work at Boston\nCollege takes place on the traditional lands of the Mashpee Wampanoag, Aquinnah Wampanoag, Nipmuc, and\nMassachusett tribal nations. Our work at Johns Hopkins University takes place on the traditional lands of the\nPiscataway Tribe. Our ﬁeldwork in Alaska takes place on the lands of St. Lawrence Island Yupik and Central\nAlaskan Yup’ik peoples. We acknowledge these and all of the indigenous peoples whose lands and waters we live\nand work upon.\nIn this work we are honored to work with the languages of the St. Lawrence Island Yupik, Central Alaskan\nYup’ik, Inuit, Chukchi, Crow, and Guaraní peoples. We hope and strive for our work to serve the communities\nwhose languages we work with. We honor and acknowledge the rich history, languages, and cultural legacies of\nall of these indigenous peoples.\nv\nvi ACKNOWLEDGEMENTS\nAbstract\nMany techniques in modern computational linguistics and natural language processing (NLP) make the assump-\ntion that approaches that work well on English and other widely used European (and sometimes Asian) languages\nare “language agnostic” – that is that they will also work across the typologically diverse languages of the world.\nIn high-resource languages, especially those that are analytic rather than synthetic, a common approach is to treat\nmorphologically-distinct variants of a common root (such asdog and dogs) as completely independent word types.\nDoing so relies on two main assumptions: that there exist a limited number of morphological inﬂections for any\ngiven root, and that most or all of those variants will appear in a large enough corpus (conditioned on assump-\ntions about domain, etc.) so that the model can adequately learn statistics about each variant. Approaches like\nstemming, lemmatization, morphological analysis, subword segmentation, or other normalization techniques are\nfrequently used when either of those assumptions are likely to be violated, particularly in the case of synthetic\nlanguages like Czech and Russian that have more inﬂectional morphology than English.\nWithin the NLP literature, agglutinative languages like Finnish and Turkish are commonly held up as extreme\nexamples of morphological complexity that challenge common modelling assumptions. Yet, when considering\nall of the world’s languages, Finnish and Turkish are closer to the average case in terms of synthesis. When\nwe consider polysynthetic languages (those at the extreme of morphological complexity), even approaches like\nstemming, lemmatization, or subword modelling may not sufﬁce. These languages have very high numbers of\nhapax legomena (words appearing only once in a corpus), underscoring the need for appropriate morphological\nhandling of words, without which there is no hope for a model to capture enough statistical information about\nthose words. Moreover, many of these languages have only very small text corpora, substantially magnifying\nthese challenges.\nTo this end, we examine the current state-of-the-art in language modelling, machine translation, and predic-\ntive text completion in the context of four polysynthetic languages: Guaraní, St. Lawrence Island Yupik, Central\nAlaskan Yup’ik, and Inuktitut. We have a particular focus on Inuit-Yupik, a highly challenging family of endan-\ngered polysynthetic languages that ranges geographically from Greenland through northern Canada and Alaska to\nfar eastern Russia. The languages in this family are extraordinarily challenging from a computational perspective,\nwith pervasive use of derivational morphemes in addition to rich sets of inﬂectional sufﬁxes and phonological\nchallenges at morpheme boundaries. Finally, we propose a novel framework for language modelling that com-\nbines knowledge representations from ﬁnite-state morphological analyzers with Tensor Product Representations\n(Smolensky, 1990) in order to enable successful neural language models capable of handling the full linguistic\nvariety of typologically variant languages.\nvii\nviii ABSTRACT\nTeam Members\nTeam Leader\n• Lane Schwartz\nAssistant Professor\nDepartment of Linguistics\nUniversity of Illinois at Urbana-Champaign\nlanes@illinois.edu\nLane Schwartz is an Assistant Professor of Computational Linguistics at the University of Illinois at Urbana-\nChampaign. His research centers on computational linguistics for endangered languages, with a focus on\nSt. Lawrence Island Yupik; this includes work in polysynthetic language modelling, cognitively-motivated\nunsupervised grammar induction, and machine translation. He is one of the original developers of Joshua, an\nopen source toolkit for tree-based statistical machine translation, and was a frequent contributor to Moses,\nthe de-facto standard for phrase-based statistical machine translation.\nSenior Members & Afﬁliates\n• Francis Tyers\nAssistant Professor\nDepartment of Linguistics\nIndiana University\nftyers@iu.edu\nFrancis Tyers is an Assistant Professor of Computational Linguistics at Indiana University Bloomington.\nHis research is focused on language technology for marginalized and indigenous languages and commu-\nnities and he has worked extensively on the Uralic languages and the Turkic languages. In language tech-\nnology his main interests are morphological modelling, using ﬁnite-state transducers and neural networks,\ndependency syntax and parsing, and machine translation. He is part of the core team of the Universal\nDependencies project and secretary of the Apertium project — a free/open-source platform for machine\ntranslation.\nix\nx TEAM MEMBERS\n• Lori Levin\nResearch Professor\nLanguage Technologies Institute\nCarnegie Mellon University\nlevin@andrew.cmu.edu\nLori Levin is a Research Professor at the Language Technologies Institute at Carnegie Mellon University.\nShe has 20 years experience in NLP for low-resource and endangered languages on several funded projects.\nShe specializes in morphosyntax, language typology, and Construction Grammar.\n• Christo Kirov\nGoogle\nckirov@gmail.com\nChristo Kirov is a Research Software Engineer at Google, and was previously a Postdoctoral Fellow in\nthe Center for Language and Speech Processing at Johns Hopkins University. His research has focused\non computational morphophonology, especially in cross-linguistic, low-resource settings. He is one of the\nfounders of the UniMorph project, which provides structured morphological paradigm data and related tools\nfor many languages.\n• Patrick Littell\nResearch Ofﬁcer\nNational Research Council of Canada\npatrick.littell@nrc-cnrc.gc.ca\nPatrick Littell is a Research Ofﬁcer in the Multilingual Text Processing team at the National Research Coun-\ncil of Canada (NRC-CNRC). His current research involves techniques for language technology development\nin very low-resource languages, by combining pre-trained multilingual models and knowledge-based rules\nand priors.\n• Chi-kiu (Jackie) Lo\nResearch Ofﬁcer\nNational Research Council of Canada\nchikiu.lo@nrc-cnrc.gc.ca\nChi-kiu Lo is a Research Ofﬁcer in the Multilingual Text Processing team at the National Research Council\nof Canada (NRC-CNRC). Her research interest is multilingual natural language processing with particular\nfocuses on semantics in machine translation (MT), its quality evaluation and estimation. She designs a\nuniﬁed semantic-oriented MT quality evaluation and estimation metric, YiSi, that is readily available for\nevaluating translation quality in any language.\n• Emily Prud’hommeaux\nAssistant Professor\nDepartment of Computer Science\nBoston College\nprudhome@bc.edu\nEmily Prud’hommeaux is an Assistant Professor of Computer Science at Boston College. Her research\narea is natural language and speech processing in low-resource settings, with a focus on developing tools\nto support the revitalization of the Haudenosaunee languages and other endangered languages of North\nAmerica.\nxi\nGraduate Students\n• Hyunji Hayley Park\nDepartment of Linguistics\nUniversity of Illinois at Urbana-Champaign\nhpark129@illinois.edu\nHayley Park is a PhD student in Computational Linguistics at the University of Illinois at Urbana-Champaign.\nHer research focuses on computational linguistics and natural language processing for low-resource lan-\nguages. Her recent projects include language modelling, grammar induction, morphological analysis and\ncorpus digitization for low-resource languages.\n• Kenneth Steimel\nDepartment of Linguistics\nIndiana University\nksteimel@iu.edu\nKenneth Steimel is a PhD candidate in Computational Linguistics at the University of Indiana Bloomington.\nHis primary research interests are data-driven tagging of morphologically complex languages, particularly\nBantu languages. His current research focuses on cross-language tagging for low resource Bantu languages.\n• Rebecca Knowles\nCenter for Language and Speech Processing, Johns Hopkins University &\nNational Research Council of Canada\nRebecca.Knowles@nrc-cnrc.gc.ca\nRebecca Knowles is a Research Associate at the National Research Council of Canada (NRC-CNRC).\nShe recently completed her Ph.D. in computer science at Johns Hopkins University. Her current research\nfocuses on machine translation and computer aided translation.\n• Jeffrey Micher\nArmy Research Laboratory &\nCarnegie Mellon University\njmicher@cs.cmu.edu\nJeffrey Micher is a computer science researcher at Army Research Lab and a Ph.D. student at Carnegie Mel-\nlon Universtiy. His research interests include machine translation and morphological analysis of polysyn-\nthetic languages, speciﬁcally Inuktitut.\n• Lonny Strunk\nDepartment of Linguistics\nUniversity of Washington\nlonny.strunk@gmail.com\nLonny Strunk is a Master’s student in the computational linguistics program at the University of Wash-\nington. His research interests focus on language technology for indigenous languages. His current project is\nin the creation of a ﬁnite state morphological analyzer for his heritage language of Central Alaskan Yup’ik.\n• Han Liu\nDepartment of Computer Science\nUniversity of Colorado Boulder\nhan.liu@colorado.edu\nHan Liu is a Ph.D. student in computer science at the University of Colorado Boulder. His research interests\ninclude natural language processing, human-centered machine learning, and human-AI collaboration.\nxii TEAM MEMBERS\nUndergraduate Students\n• Coleman Haley\nJohns Hopkins University\nchaley7@jhu.edu\nColeman Haley is an undergraduate senior at Johns Hopkins University majoring in Computer Science\nand Cognitive Science. His research interests include neural interpretability in natural language processing,\nas well as NLP for morphologically and typologically diverse languages.\n• Katherine J. Zhang\nCarnegie Mellon University\nkjzhang@alumni.cmu.edu\nKatherine Zhang is a member of the teaching staff at Carnegie Mellon University’s Language Technolo-\ngies Institute. She recently graduated from CMU with majors in Linguistics and Chinese Studies. Her\nresearch interests lie in corpus linguistics and Sino-Tibetan languages.\nGraduate Student Afﬁliates\n• Robbie Jimerson\nRochester Institute of Technology\nrcj2772@rit.edu\nRobbie Jimerson is a Ph.D. candidate in Computing and Information Sciences at the Rochester Institute\nof Technology and a member of the Seneca Nation of Indians. His dissertation research focuses on devel-\noping robust language technologies to support the documentation and revitalization of the Seneca language\nand other endangered indigenous languages.\n• Vasilisa Andriyanets\nMoscow Higher School of Economics\nblindedbysunshine@gmail.com\nVasilisa Adriyanets is a recently graduated Masters student in computational linguistics from Higher School\nof Economics in Moscow, Russia. She has worked on computational approaches to processing a variety of\nlanguages, and speciﬁcally on morphological analysis for Russian and Chukchi.\nRemote Student Afﬁliates\n• Aldrian Obaja Muis, Naoki Otani, Jong Hyuk (Jay) Park, Zhisong Zhang\nCarnegie Mellon University\n{amuis@cs, notani@cs, jp1@andrew, zhisongz@andrew}.cmu.edu\nContents\nAcknowledgements v\nAbstract vii\nTeam Members ix\nTeam Leader . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nSenior Members & Afﬁliates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nGraduate Students . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\nUndergraduate Students . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nGraduate Student Afﬁliates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nRemote Student Afﬁliates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii\nContents xv\n1 Introduction 1\n2 Background 3\n2.1 Finite-state morphology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Language modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.3 Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3 Languages & Resources 7\n3.1 Language selection & data collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.1.1 Chukchi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.1.2 St. Lawrence Island Yupik . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.1.3 Central Alaskan Yup’ik . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.1.4 Inuktitut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.1.5 Crow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.1.6 Guaraní . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Descriptive statistics of the corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.3 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.4 Estimating weights for ﬁnite-state morphological analyzers . . . . . . . . . . . . . . . . . . . . . 10\n4 Machine Translation 13\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4.2 Parallel Data Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2.1 Inuktitut–English Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2.2 Yupik–English Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2.3 Guaraní–Spanish Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.3 Inuktitut Machine Translation Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.3.1 Segmentation experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.3.2 Single source and multi-source experiments . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.3.3 Challenges in Evaluation of English-to-Inuktitut MT . . . . . . . . . . . . . . . . . . . . 19\nxiii\nxiv CONTENTS\n4.4 Low-Resource Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.4.1 Baselines and V ocabularies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.4.2 Yupik Language Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5 Language Modelling 25\n5.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n5.2 Tokenization strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.2.1 Word . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.2.2 Character . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.2.3 BPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.2.4 Morfessor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.2.5 FST segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.3 RNN-LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.4 Character-level perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.5 Results & Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.6 Future Direction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n6 Applications & Future Work 37\n6.1 On-device Text Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.1.1 Open Source Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.1.2 User Interface Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.1.3 Adapting Neural Language Models for Mobile Devices . . . . . . . . . . . . . . . . . . . 38\n6.1.4 Future Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n6.2 Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n6.2.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n6.2.2 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n6.2.3 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n6.2.4 Preliminary results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n6.2.5 Crow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n6.2.6 Guaraní . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n6.2.7 Future directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n7 Feature-rich Open-vocabulary Interpretable Language Model 43\n7.1 Language Model Desiderata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n7.1.1 Flexibility with respect to language typology . . . . . . . . . . . . . . . . . . . . . . . . 44\n7.1.2 Ability to incorporate external knowledge sources as features . . . . . . . . . . . . . . . . 45\n7.1.3 Open vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n7.1.4 Interpretability of predicted units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n7.2 Sub-word language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.2.1 Prediction of next morpheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.2.2 Prediction of next character . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.3 Neural morphological analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.4 Tensor Product Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n7.4.1 Unbinding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n7.5 Morpheme vector representations from TPRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.5.1 Morpheme TPRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.5.2 Learning morpheme vectors using an autoencoder . . . . . . . . . . . . . . . . . . . . . 48\n7.6 Unbinding loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\nCONTENTS xv\n8 Conclusions 51\n8.1 Contribution 1: Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n8.2 Contribution 2: Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n8.3 Contribution 3: Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n8.4 Contribution 4: Mobile & Speech Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n8.5 Contribution 5: Model Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\nBibliography 62\nxvi CONTENTS\nChapter 1\nIntroduction\nMany techniques in modern computational linguistics and natural language processing (NLP) make the assump-\ntion that approaches that work well on English and other widely used European (and sometimes Asian) languages\nare “language agnostic” – that is that they will also work across the typologically diverse languages of the world.1\nIn high-resource languages, especially those that are analytic rather than synthetic, a common approach is to treat\nmorphologically-distinct variants of a common root (such asdog and dogs) as completely independent word types.\nDoing so relies on two main assumptions: that there exist a limited number of morphological inﬂections for any\ngiven root, and that most or all of those variants will appear in a large enough corpus (conditioned on assump-\ntions about domain, etc.) so that the model can adequately learn statistics about each variant. Approaches like\nstemming, lemmatization, morphological analysis, subword segmentation, or other normalization techniques are\nfrequently used when either of those assumptions are likely to be violated, particularly in the case of synthetic\nlanguages like Czech and Russian that have more inﬂectional morphology than English.\nWithin the NLP literature, agglutinative languages like Finnish and Turkish are commonly held up as extreme\nexamples of morphological complexity that challenge common modelling assumptions. Yet, when considering\nall of the world’s languages, Finnish and Turkish are closer to the average case in terms of synthesis. When we\nconsider polysynthetic languages (those at the extreme of morphological complexity), approaches like stemming,\nlemmatization, or subword modelling may not sufﬁce. These languages have very high numbers of hapax legom-\nena (words appearing only once in a corpus), underscoring the need for appropriate morphological handling of\nwords, without which there is no hope for a model to capture enough statistical information about those words.\nMoreover, many of these languages have only very small text corpora, substantially magnifying these challenges.\nThe remainder of this work is structured as follows.\nIn Chapter 2 we brieﬂy review the relevant background literature in ﬁnite-state morphology, language mod-\nelling, and machine translation. We review ﬁnite-state approaches to morphological analysis. We review the\nmajor approaches to language modelling, including n-gram language models, feed-forward language models, and\nrecurrent neural language models.\nIn Chapter 3 we present a set of polysynthetic languages which we will consider throughout this work and\ndetail the resources available for each. We have a particular focus on Inuit-Yupik, a highly challenging family\nof endangered polysynthetic languages that ranges geographically from Greenland through northern Canada and\nAlaska to far eastern Russia. The languages in this family are extraordinarily challenging from a computational\nperspective, with pervasive use of derivational morphemes in addition to rich sets of inﬂectional sufﬁxes and\nphonological challenges at morpheme boundaries.\nIn Chapters 4–6 we examine the current state-of-the-art in language modelling, machine translation, and pre-\ndictive text completion in the context of four polysynthetic languages: Guaraní, St. Lawrence Island Yupik, Cen-\ntral Alaskan Yup’ik, and Inuktitut. In Chapter 4 we present experiments and results on machine translation into,\nout of, and between polysynthetic languages; we carry out experiments between various Inuit-Yupik languages\nand English, as well as between Guaraní and Spanish, showing that multilingual approaches incorporating data\nfrom higher-resource members of the language family can effectively improve translation into lower-resource lan-\n1Emily Bender provides a thorough discussion of this problem in https://thegradient.pub/\nthe-benderrule-on-naming-the-languages-we-study-and-why-it-matters/ .\n1\n2 CHAPTER 1. INTRODUCTION\nFigure 1.1: Overview of the tangible artefacts, models, and applications in this report. We start with all of the available\nresources for a given language, including (bi-)texts, grammars, and dictionaries. These are used to create ﬁnite-state morpho-\nlogical analyzers and MT systems (§4) directly. The ﬁnite-state morphological analyzers are then applied to corpora to create\nsegmented or analyzed corpora (§2). These are used both to build language models (§5) and machine translation systems\n(§4) based on the segmented morphemes and to create interpretable morpheme-based language models using tensor product\nrepresentations (§7). The ﬁnal results are predictive keyboards that use morphemes as the unit of prediction (§6), with potential\nfuture work (greyed out) including automatic speech recognition and morpheme-based machine translation.\nguages. In Chapter 5, we present language modelling experiments across a range of languages and vocabularies.\nIn Chapter 6 we present practical applications which we anticipate will beneﬁt from our language model and\nmultilingual approaches, along with preliminary experimental results and discussion of future work.\nFinally in Chapter 7 we present a core theoretical contribution of this work: a feature-rich open-vocabulary\ninterpretable language model designed to support a wide range of typologically and morphologically diverse\nlanguages. This approach uses a novel neural architecture that explicitly model characters and morphemes in\naddition to words and sentences, making explicit use knowledge representations from ﬁnite-state morphological\nanalyzers, in combination with Tensor Product Representations (Smolensky, 1990) to enable successful neural\nlanguage models capable of handling the full linguistic variety of typologically variant languages. We present our\nconclusions in Chapter 8.\nChapter 2\nBackground\nIn this chapter we provide a brief overview of the background technologies that underlie this report, namely ﬁnite-\nstate approaches to morphological analysis (§2.1), n-gram and neural language modelling techniques (§2.2), and\nneural machine translation (§2.3).\n2.1 Finite-state morphology\nInitial approaches to modelling the morphology of natural languages in the mid-20th century tended to focus on\nunidirectional algorithmic solutions to particular languages, implemented in general-purpose (rather than domain-\nspeciﬁc) programming languages. These included generators, which generated wordforms from an analysis speci-\nﬁcation, analyzers, which returned possible analyses for a given word, and lemmatizers or stemmers which aimed\nto return a baseform, stem, or lemma given a wordform. These approaches had a number of downsides, the ﬁrst\nbeing that the same code could not be used for analysis and generation, so for each language, separate code had\nto be written for these two tasks. In addition, descriptions could not be shared between related languages without\nmuch difﬁculty and there was little formalization.\nIn the early 1980s this changed with the introduction of ﬁnite-state morphology. In this formalization of\nmorphology, the set of potential strings (wordform-analysis pairs) in a language is represented by a ﬁnite-state\ntransducer. A ﬁnite-state transducer is a special class of ﬁnite-state automaton where each arc has both an in-\nput symbol and an output symbol. There are two main approaches to modelling morphophonological (or mor-\nphographemic) rules using ﬁnite-state approaches. The ﬁrst consists of applying a sequence of rewrite rules in the\nform α → β / γ_ δ, where the alphabet symbol αis rewritten as β between γ and δ. The second approach\nis referred to as two-level morphology (Koskenniemi, 1983). In this approach, phonological rules are unordered\nconstraints over possible symbol pairs. As Karttunen (1993) notes, the two approaches are formally equivalent\nand all phonological phenomena that can be described with one can be described with the other.\nGiven a description, a ﬁnite-state morphological analyzer can produce both analyses of surface tokens (e.g.\nsequences of tags and lemmas such as those found in interlinear glosses) and segmentations of surface tokens.\nConsider the output of the analyzer for the Guaraní sentence Rehótapa che rendápe. ‘Will you come with me’ in\nExample (1). The output includes the lemmas ho ‘come’,che ‘my’ andtenda ‘place’, person and number tags\nsuch as <p2> ‘second person’,<sg> ‘singular’, tags indicating word class,<n> ‘noun’ and<v> ‘verb’ among\nothers.\n(1) Input Rehótapa che rendápe.\nAnalysis re<prn><p2><sg>+ho<v><iv>+ta<fti>+pa<qst>\nche<prn><pos><p1><sg>\nr<det>+tenda<n>+pe<post>\nSegmentation Rehó>ta>pa che r>endá>pe\nThis is especially important for polysynthetic languages, as words can be made up of many morphemes, for ex-\nample the word ñaha’arõ’˜yetéva ‘that we did not expect at all’ in the sentenceOiko pete˜ı mba’e ñaha’arõ’˜yetéva.\n3\n4 CHAPTER 2. BACKGROUND\n“Something happened that we did not expect at all’ can be decomposed as in Example (2) below.\n(2) Input ñaha’arõ’ ˜yetéva\nAnalysis ña<prn><p1><pl>+ha’arõ+ ˜y<neg>+ete<emph>+va<subs>\nSegmentation ña>ha’arõ>’ ˜y>ete>va\nThe amount of time required to develop a ﬁnite-state description can vary widely, but can be anywhere from\ntwo weeks, given a trained developer and a description of a related language — e.g. Kumyk in Washington et al.\n(2014) — to a year for a developer completely unfamiliar with the tools and language. The speed is also affected\nby the available resources such as grammatical descriptions and machine-readable lexicons.\nOne shortcoming of many ﬁnite-state morphological analyzers is an inability to assign probabilities to analy-\nses. Table 2.1 depicts six example English sentences which each contain the word wound; each of these six uses\nis analyzed with a distinct linguistic analysis. When analyzing an English sentence that contains the word wound,\nan unweighted English morphological analyzer would posit all of these analyses, and would be unable to suggest\nwhich might be the most probable. Some ﬁnite-state morphological toolkits support the use of probabilities on\nAnalysis Example Frequency Rel. frequency\n‘wind-PAST’ She wound the watch. 4 0.66\n‘wind-PP’ She had wound the watch. 1 0.16\n‘wound-N.SG’ The wound healed quickly. 1 0.16\n‘wound-INF ’ Therefore I will wound you. 0 0\n‘wound-PRES ’ They wound and they heal. 0 0\n‘wound-IMPER ’ You wound me sir! 0 0\nTable 2.1: List of analyses for the wordform wound in English, along with example sentences and frequency according to the\nEnglish treebanks from the Universal Dependencies project (Nivre et al., 2016).\narcs in constructed ﬁnite-state transducers (Mohri, 2001). This means that it is possible to make analyzers and seg-\nmenters where the output is ranked, either by probability or by some other metric. Arc probability weights can be\nobtained from corpus statistics or from other measures. This is especially important for polysynthetic languages,\nwhere words may potential have many analyses. We describe the methods we used to weight our analyzers in\nSection 3.4.\n2.2 Language modelling\nA language model is any model that describes natural language. By that description, the ﬁnite-state models\nfrom the previous section could also be considered as a form of language model. In this section, however we use a\nnarrower deﬁnition of language model as being a model of a probability distribution over a sequence of vocabulary\nitems (characters, words).\nPerhaps the simplest approximation to determine the probability of a sentence would be to use a unigram\nmodel over words. In such a model, the probability of a sentence is deﬁned as the product of the probabilities of\nthe individual words, which could be estimated by taking their relative frequency in a given corpus. While such\na model could reasonably discriminate between the relative probabilities of sentences such as (a) “have a great\ntrip” and (b) “have a superannuated tardigrade”, it would not be able to distinguish the relative probability of (c)\n“great a have trip” and (a). A more accurate, but less tractable approximation would be to ask all speakers of a\ngiven language to rank all of the possible sentences in that language by some metric of ‘goodness’. So the idea of\nlanguage modelling is to ﬁnd a tractable way to model the distribution of probability for sequences of linguistic\nsymbols or tokens.\nThis simple model can be extended to n-gram language models (Shannon, 1948, 1951), whereby instead of\nmodelling single units (characters, words), what is modelled is sequences of units. Thus in a bigram word model,\nthe sequences modelled would be bigrams, e.g. {have a, a great, a trip} and {great a, a have, have trip} from\nexamples (a) and (c) respectively. For languages where large amounts of monolingual training data are available,\n2.3. MACHINE TRANSLATION 5\nlanguage models of order 5–7 have been widely used in applications such as machine translation and automatic\nspeech recognition.\nHowever, as the model is extended to cover longer sequences, the problem of out-of-vocabulary (OOV) items\nbecomes more severe. This happens when the sequence we are attempting to estimate the probability of does\nnot appear in our model. This can be illustrated with the example in (b) above. The sequence “superannuated\ntardigrade” does not return any results with a search engine query on several major search engines. It is therefore\nhighly likely that a bigram language model trained using all English text available on the internet would estimate\nthe probability of this sequence to be zero, and therefore the probability of the entire sentence would also be zero.\nThere are two techniques that have been developed to deal with this problem. Smoothing techniques reserve a\nsmall amount of the probability mass to distribute to unseen n-grams (Good, 1953; Jelineck and Mercer, 1980;\nKatz, 1987; Witten and Bell, 1991; Church and Gale, 1991; Ney et al., 1994; Kneser and Ney, 1995), while backoff\ntechniques allow combinations of lower-ordern-grams to be used to estimate the probability of higher-order ones.\nIn example (b) the probabilities of ‘superannuated’ and ‘tardigrade’ would be used to estimate the probability of\n‘superannuated tardigrade’.\nOne of the issues with n-gram language models is that parameters are not shared between tokens and se-\nquences. For example, the token ‘wonderful’ is as far from ‘great’ as is the token ‘superannuated’. So if we have\nthe sequence “have a wonderful trip”, the other shared contexts that ‘wonderful’ and ‘great’ appear in are not taken\ninto account. A way of dealing with this problem is to use distributional representations of individual tokens, as\nin Bengio et al. (2000, 2003). Here each token is represented by a vector of real numbers, embedding each token\nin a shared vector space. In these kind of language models it is still necessary to specify a ﬁxed n-gram context,\nwhich means that the amount of context that can be taken into account is limited to a ﬁxed-sized window for each\ntoken. Mikolov et al. (2010) describe using recurrent neural networks to model context to allow whole-sentence\ncontext to be taken into account. In addition they introduce efﬁcient methods of training the distributional vectors\nsuch that corpora numbering in the billions of words can be used in training. In both the models proposed by\nBengio et al. (2000) and Mikolov et al. (2010) each token is represented by a single vector. As evidenced from the\nexamples above this is not always tenable, words in natural language are ambiguous (cf. wound and trip – ‘to trip\nover something’ or ‘a nice trip’). In ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), each word vector\nis context dependent, both on external, sentence-level context, and on word-internal context, so even if a given\ntoken has not been seen before, the model can generalize from forms that have similar surface forms and appear\nin similar contexts. This would seem to be an ideal model for polysynthetic languages, however the downside is\nthat these models typically contain very large numbers of parameters which in turn require very large amounts of\ntraining data, far more than is available for most endangered languages.\n2.3 Machine Translation\nIn recent years, the machine translation community has gravitated toward neural approaches to machine trans-\nlation. Midway through the 2010s, these began outperforming phrase-based statistical and other approaches in\nlarge-scale evaluations (Bojar et al., 2016). This success has driven a rapid sequence of approaches to building\nneural machine translation models, from sequence-to-sequence models (Sutskever et al., 2014), to models with\nattention (Bahdanau et al., 2015), to models that primarily rely on attention (Vaswani et al., 2017). In preparation\nfor the workshop, we trained both statistical and neural machine translation models on the available training data.\nDuring the workshop, we focused solely on neural approaches to machine translation, and report those experi-\nments in Chapter 4. As our experiments tended to examine variations of the input to the translation models rather\nthan modiﬁcations to the networks themselves, we do not provide a thorough overview of the techniques here; for\nadditional detail, please see the cited code and papers.\nThere does exist prior work on machine translation for polysynthetic languages, though it has generally been\nlimited by small data sizes. In their recent overview of corpus resources for indigenous languages of the Americas,\nMager et al. (2018a) note that most of the parallel corpora they found were quite small (less than 250,000 lines\nof text). Homola (2012) proposed the use of rule-based systems for polysynthetic languages, but this approach is\nstill labor-intensive, as it requires the application of extensive linguistic knowledge or other tools. Monson et al.\n(2006) report on Mapudungun and Quechua to Spanish machine translation systems. Mager et al. (2018b) discuss\nchallenges of translating between polysynthetic and fusional languages. This is not a complete account of all such\nwork.\n6 CHAPTER 2. BACKGROUND\nOf special note for the purposes of this work is existing research on two of the languages we worked on\nthis summer: Inuktitut and Guaraní. For translation between Guaraní and Spanish, we are aware of an online\ngister (http://iguarani.com/) and Bible translations evaluated on stemmed output (Rudnick, 2018), and\na system for translators called Mainumby by Gasser (2018). Previous work on translation between Inuktitut\nand English can be found in Micher (2018b), in which results of statistical machine translation for English and\nInuktitut are reported. Micher makes use of a morphologically analyzed previous version of the Nunavut Hansard\ncorpus to enhance SMT systems. Details on developing this corpus can be found in Micher (2018a). The FST-\nbased analyzer (Farley, 2009) in combination with the neural analyzer (Micher, 2017) are used to morphologically\nanalyze this data set. Klavans et al. (2018a) discuss some of the challenges of building such translation systems.\nChapter 3\nLanguages & Resources\nA central issue that arises when conducting research on polysynthetic languages is the lack of resources: many\npolysynthetic languages are very low resource. Due to the need for corpora for use in language modelling efforts,\nan effort was directed towards locating existing corpora for polysynthetic languages and assessing their usability\nfor different experiments. While we used only a subset of what we collected for experiments, this chapter provides\nan overview of all linguistic resources we gained access to in the process in order to offer a glimpse into available\npolysynthetic language resources.\nIn what follows, we provide short descriptions of the language families and languages involved and the corpora\nwe collected. We brieﬂy discuss the characteristics of polysynthetic languages based on descriptive statistics and\nthe texts we selected for subsequent experiments. Details regarding corpus preprocessing are described in the\ncontext of experiments discussed in later chapters.\n3.1 Language selection & data collection\nWe obtained corpora and resources for six languages: Chukchi, St. Lawrence Island Yupik, Central Alaskan\nYup’ik, Inuktitut, Crow, and Guaraní. These languages were chosen from four different families, all of which are\nlow-resource and polysynthetic. There was a focus in particular on the Inuit-Yupik-Unangan family, from which\nthree of the languages were selected. The Inuit-Yupik-Unangan languages, historically known as Eskimo-Aleut,\nare a language family native to the Russian Far East, Alaska, Canada, and Greenland. The family is divided into\ntwo branches: Inuit-Yupik and Unangan. St. Lawrence Island Yupik, Central Alaskan Yup’ik, and Inuktitut belong\nto the Inuit-Yupik branch of the family.\nIn preparation for the workshop, we gathered spoken and written corpora for the selected polysynthetic lan-\nguages. In addition to written and spoken corpora, where available, we also gathered dictionaries, reference\ngrammars, and ﬁnite-state morphological analyzers. Table 3.1 provides a summary of the resources we had in\neach language. We refer to each language by name or by ISO 639-3 code.\nLanguage Code Mono. text Para. text FST Audio\nChukchi ckt ✓ ✓ ✓\nSt. Lawrence Island Yupik ess ✓ ✓ ✓ ✓\nCentral Alaskan Yup’ik esu ✓ ✓\nInuktitut iku ✓ ✓ ✓\nCrow cro ✓\nGuaraní grn ✓ ✓ ✓\nTable 3.1: Overview of languages and resources: monolingual text, parallel text, ﬁnite state transducers, and audio data.\n7\n8 CHAPTER 3. LANGUAGES & RESOURCES\n3.1.1 Chukchi\nChukchi (ckt) is the most widely spoken language in the Chukotko-Kamchatkan family, with approximately\n5000 speakers. The Chukotko-Kamchatkan languages are native to the Russian Far East, and Chukchi is spoken\nin the easternmost part, mainly on the Chukotka Peninsula.\nWe obtained audio data and transcripts for Chukchi from http://chuklang.ru, a website dedicated to\nmaterials and research on Chukchi funded by the Russian Science Foundation. The audio data contains two books\nof the Bible, the Book of Jonah and the Gospel of Luke, and short stories in the language. The stories represent a\nvaluable resource for the endangered language. The transcripts are in both Latin and Cyrillic scripts. There also\nexists a prototype ﬁnite-state morphological analyzer for Chukchi (Andriyanets and Tyers, 2018). This analyzer\nwas expanded on during the workshop using the transcripts of the audio data.\n3.1.2 St. Lawrence Island Yupik\nSt. Lawrence Island Yupik ( ess) is an endangered language in the Inuit-Yupik family spoken on St. Lawrence\nIsland, Alaska and on the Chukokta Peninsula of the Russian Far East. We collected a corpus consisting primar-\nily of scanned and digitized books, including educational materials (Apassingok et al., 1993, 1994, 1995), oral\nnarratives (Nagai, 2001; Apassingok et al., 1985, 1987, 1989; Slwooko, 1977, 1979) and a reference grammar\n(Jacobson, 2001). In addition, we made use of the Yupik translation of the New Testament1 (Wycliffe, 2018). We\nmade use of the Chen and Schwartz (2018) ﬁnite-state morphological analyzer, which was based on the Yupik\ngrammar of Jacobson (2001) and incorporated Yupik lexical entries from the Badten et al. (2008) dictionary.\n3.1.3 Central Alaskan Yup’ik\nCentral Alaskan Yup’ik (esu) is an ofﬁcial language of Alaska that is spoken by about 10,000 speakers in the\nwestern and southwestern parts of the state. There are ﬁve major dialects of Central Alaskan Yup’ik, of which\nGeneral Central Yup’ik (Yugtun) is the most widely spoken.\nThis workshop made use of a Yup’ik translation 2 of the Bible. As one of our team members speaks the\nlanguage, we were able to align it with a corresponding English Bible (Good News Translation, Today’s English\nVersion, Second Edition). The parallel data were used for both machine translation and language modelling\nexperiments. Additionally, the Yup’ik Bible and a dictionary (Jacobson, 1984) were used to begin development\non a Yup’ik ﬁnite-state morphological analyzer.\n3.1.4 Inuktitut\nInuktut (a term that includes the variants Inuktitut and Inuinnaqtun) is one of the ofﬁcial languages of Nunavut, the\nlargest territory of Canada, and is spoken by approximately 39,770 people in Canada (Statistics Canada, 2017). It\nalso has ofﬁcial recognition in several other areas and is part of the Inuit-Yupik-Unangan language family. Inuktut\ncan be written in syllabics or in roman orthography, and regional variations use different special characters and\nspelling conventions.\nAs Inuktut is an ofﬁcial language of government in Nunavut, there exist some resources that are available in\nthis language at a much larger scale than most other languages in the same family, notably a parallel corpus with\nEnglish. Since its formation in 1999, the Legislative Assembly of Nunavut has been publishing its proceedings\n(known as a Hansard) in both Inuktitut ( iku) and English. 3 In the subsequent 20 years, the collected Nunavut\nHansard has grown to be a substantial bilingual corpus (Martin et al., 2003, 2005; Farley, 2008; Joanis et al.,\n2020), putting Inuktitut in the perhaps unique position of a polysynthetic language with a parallel corpus of more\nthan a million sentence pairs. We discuss the different versions of this data, and their preprocessing for machine\ntranslation, in Section 4.2.\nWe also made use of a Inuktitut translation 4 of the Bible for language modelling experiments. We decided\nto exclude the Hansard in the language modelling experiments as including it would make the Inuktitut dataset\n1https://live.bible.is/bible/ESSWYI\n2bibles.org\n3It should be noted that Legislative Assembly of Nunavut discourse takes place in several Inuktut varieties, as well as English; a more\ndetailed description of the construction and dialect situation of the Hansard will be available in Joanis et al. (2020).\n4bible.com\n3.2. DESCRIPTIVE STATISTICS OF THE CORPORA 9\nLanguage Code Corpus Sentences Tokens Types TTR MDN\nCentral Alaskan Yup’ik esu Bible 59575 566544 138320 0.244 3.86\nEnglish eng Bible 62049 1057713 22201 0.021 42.90\nChukchi ckt Transcripts 1015 5309 2387 0.450 2.22\nInuktitut iku Bible 31103 459571 126165 0.275 3.64\nInuktitut iku Hansard 1300148 10869995 1563883 0.144 6.95\nEnglish eng Hansard 1300148 20367595 59234 0.003 343.81\nGuaraní grn Bible 30078 629099 45766 0.073 12.71\nSpanish spa Bible 30078 822192 31625 0.038 23.75\nSt. Lawrence Island Yupik ess Books 24456 214090 60414 0.282 3.32\nSt. Lawrence Island Yupik ess New Testament 8002 119482 32532 0.272 3.45\nEnglish eng New Testament 8002 273064 9071 0.033 28.37\nTable 3.2: Statistics of the written corpora, including type-token ratio (TTR) and mean distance to next novel type (MDN).\nsubstantially different from other datasets and thus making it hard to compare it with other languages. How we\npreprocessed the data for language modelling is discussed in Chapter 5.\n3.1.5 Crow\nCrow (Apsáalooke, language code cro) is one of the most widely spoken languages of the Siouan family, with\napproximately 3500 speakers. The Siouan languages are native primarily to the Great Plains of North America,\nand Crow speciﬁcally is spoken in southern Montana.\nOur primary resource for Crow was a series of audio recordings for a dictionary developed by the Language\nConservancy, an organization that protects and revitalizes Native American languages. This corpus consists of 11.7\nhours of recordings produced by 14 speakers. The data is entirely composed of single words and short phrases\nfrom the online Crow Dictionary project (The Crow Language Conservancy, 2019). This data was obtained on\nspecial permission from the Language Conservancy and is not publicly available.\n3.1.6 Guaraní\nGuaraní (grn) is a Tupian language native to South America. It is an ofﬁcial language of Paraguay and the most\nwidely spoken language in the country with almost 5 million speakers. It is also the only indigenous language of\nthe Americas with a large number of non-indigenous native speakers.\nWe were able to obtain Guaraní-Spanish parallel Bible translations. The Guaraní Bible was translated and pub-\nlished by the Sociedad Bíblica Paraguaya. The parallel translations were used for language modelling and machine\ntranslation experiments. A morphological analyser developed by Kuznetsova and Tyers (2019),apertium-grn,\nwas also used.\n3.2 Descriptive statistics of the corpora\nThe polysynthetic languages described above differ signiﬁcantly from languages such as English and Spanish.\nOne major point of difference is in the ratio of word types to word tokens; given the number of word tokens and\nthe number of unique word types, the type-token ratio is calculated as TTR = |types|\n|tokens|. Another useful metric,\nproposed by Hasegawa-Johnson et al. (2017a) and used for polysynthetic language by Schwartz et al. (2020),\ncalculates the mean distance to the next novel word type (MDN).\nTable 3.2 displays these text metrics for all textual corpora used. Large differences exist between different\nlanguages and between different corpora of the same language with respect to these metrics. The polysynthetic\nlanguages examined display higher type-token ratios and lower average distances to the next novel word type in\ncomparison to the non-polysynthetic languages (English and Spanish). This is particularly poignant for parallel\ncorpora. The New Testament in English has a type-token ratio approximately nine times lower than St. Lawrence\nIsland Yupik. This is somewhat expected as the central focus of this work is determining effective strategies for\n10 CHAPTER 3. LANGUAGES & RESOURCES\nAlgorithm 1: Mean distance to next novel type metric\nResult: Mean distance to next novel type\ndistances = list;\ntypes = list;\ncurrent_distance = 0; for word in text do\nif word in types then\ncurrent_distance++;\nend\nelse\ndistances.append(current_distance);\ncurrent_distance = 0;\nend\nend\ndistance = avg(distances)\nworking with highly morphologically complex polysynthetic languages and previous research (Kettunen, 2014)\nhas indicated that morphological complexity is correlated with metrics like TTR.\nThe datasets utilized cover a large number of different domains as well, including religious texts, parlimentary\nproceedings, audio transcriptions, and data scraped from internet resources. These domain differences contribute\nto the differences in corpus properties as well. For example, both the English Bible and the English Nunavut\nHansard corpus have lower type token ratios and higher mean distances to the next novel type. However, the\nformulaic language of parliamentary proceedings causes the English Hansard corpus to have a type-token ratio\nseven times lower than the English Bible used. These domain differences were controlled for the language mod-\nelling experiments described in Chapter 5 by using the New Testament for several different languages. For the\nother tasks, comparisons between languages are used sparingly if similar genres of text are not available for both\nlanguages.\n3.3 Preprocessing\nWe preprocessed the corpora for 1) machine translation and 2) language modelling experiments. The general\nprinciple and strategies we adapted for preprocessing for both experiments are very similar. We removed any\nredundant lines and verse numbers to clean up the corpora. We made sure to normalize apostrophes so that they\nremained as part of a word after we tokenized the data using Moses scripts (Koehn et al., 2007). As truecasing\nis a common practice in machine translation, we truecased the text for machine translation experiments, but not\nfor language modelling experiments. Using the cleaned-up datasets, we explored different tokenization strategies.\nFST and BPE segmentation methods were adapted for machine translation experiments, and character, BPE, Mor-\nfessor and FST segmentation levels were used for language modelling experiments. Details about how we selected\nand preprocessed the datasets for the two sets of experiments are discussed in Chapter 4 (Machine Translation)\nand Chapter 5 (Language modelling), respectively.\n3.4 Estimating weights for ﬁnite-state morphological analyzers\nWe used three approaches to estimate weights for our ﬁnite-state analysers, one supervised, one heuristic and\none unsupervised. The supervised method was the most simple. We had a small corpus of annotated (manually\ndisambiguated) text for Guaraní, the test corpus from Kuznetsova and Tyers (2019). We used this and assigned\na weight to all wordform:analyses pairs of 1. For the wordform-analysis pairs found in the corpus, a weight\nwas assigned equal to 1 −P(a|w), where P(a|w) is the number of times the analysis occurs with the particular\nwordform over the total number of times the wordform appears. This is necessarily a number between zero and\none and thus for wordforms seen in the corpus, their analysis received a lower weight than unseen wordform-\nanalysis pairs. Given the size of the corpus, 2020 wordforms, the majority of the wordforms seen in the corpora\nwere unseen. For both the Yupik analyser and the Guaraní analyser we added an additional heuristic, for each\n3.4. ESTIMATING WEIGHTS FOR FINITE-STATE MORPHOLOGICAL ANALYZERS 11\nmorpheme boundary, we increased the weight by 1. The motivation behind this heuristic is that we wanted to\nfavor lexicalized forms and defavor forms with very many derivations when there was a lexicalized alternative. In\naddition, we experimented with a novel unsupervised approach to weighting the transducers based on byte-pair\nencoding (BPE; Sennrich et al., 2016).\n12 CHAPTER 3. LANGUAGES & RESOURCES\nChapter 4\nMachine Translation\n4.1 Introduction\nThis chapter discusses neural machine translation (NMT) experiments for translation into, out of, and between\npolysynthetic languages. While polysynthetic and, more generally, morphologically complex languages are often\nconsidered to pose a greater challenge for machine translation research than languages with relatively simple\nmorphology (Oﬂazer and Durgar El-Kahlout, 2007; Bojar et al., 2015), this challenge is often entangled with the\nchallenges of low-resource machine translation. What really causes this challenge? Is it the length and complexity\nof the word forms? The type-token ratio and data sparsity? A lack of sufﬁcient training data or a need for more\ntraining data than morphologically simple languages? A matter of many evaluation metrics being ill-suited to\nmorphologically complex languages? Some combination of all of this?\nIn this work, we take steps towards answering two relevant questions through experiments on machine trans-\nlation between English, Inuktitut, and Yupik as well as Guaraní and Spanish. First, can we untangle the inﬂuences\nof small data and morphological complexity on the challenge of modelling these languages? Second, can we\nmake use of higher-resource languages in the same language family to improve machine translation of lower-\nresource languages? We examine the ﬁrst through translation of Inuktitut using a new, larger, pre-release version\nof the Nunavut Hansard,1 as described in Sections 3.1.4, 4.2.1 and 4.3. We examine the second through a series\nof experiments on low-resource machine translation (described in Section 4.4); our most promising experiments\nincorporate Inuktitut data into the translation of Yupik data (Table 4.8).\nWe ﬁrst discuss the data resources for machine translation, providing more detail about data size, prepro-\ncessing, and the like (Section 4.2). This is followed by descriptions of our machine translation experiments.\nSection 4.3.3 brieﬂy covers challenges of machine translation evaluation for polysynthetic languages.\nThe main contributions of our machine translation work during this workshop are as follows.\n• We achieved state-of-the-art performance on translation between Inuktitut and English (since surpassed by\nJoanis et al. (2020)).\n• With ﬁrst access to the beta version 3.0 of the Nunavut Hansard (Joanis et al., 2020), we were able to pro-\nvide feedback and best practices for preprocessing the dataset and contributed to knowledge about existing\ncharacter and spelling variations in the dataset.\n• We collected empirical evidence on several well-known but unresolved challenges, such as best practices in\ntoken segmentation for MT into and out of polysynthetic languages, as well as an examination of how to\nevaluate MT into polysynthetic languages.\n• We successfully used multilingual neural machine translation methods to improve translation quality into\nlow-resource languages using data from related languages. Notably, our “low-resource” languages were\nlower resource than much of the literature, and we produced improvements without the use of large mono-\nlingual corpora (which are unavailable for these languages and many other languages of interest). We\nobserved these improvements across both n-gram-oriented and semantic-oriented metrics.\n1While this was a pre-release at the time of this workshop, the data has now been made available publicly; see Joanis et al. (2020).\n13\n14 CHAPTER 4. MACHINE TRANSLATION\n4.2 Parallel Data Resources\nChapter 3 describes the general data resources used throughout the workshop. Here we provide a more in-depth\nlook at the resources used for machine translation speciﬁcally, including some notes on preprocessing.\nTrain Dev. Test\niku-eng 1300148 3088 2780\ness-eng 5838 1142 1750\nesu-eng 30724 1279 927\ngrn-spa 28050 1129 875\nTable 4.1: Preprocessed lines of parallel training, development/validation, and test data for machine translation experiments.\nThe machine translation resources available to us ranged from moderate to extremely low resource, as shown\nin Table 4.1.\n4.2.1 Inuktitut–English Data\nAs described is Section 3.1.4, there have been several releases of the Nunavut Hansard. The ﬁrst, version 1.0, was\nreleased to the natural language processing community in Martin et al. (2003), and consisted of 3.4 million English\ntokens and 1.6 million Inuktitut tokens of parallel data. A subsequent update, version 1.1, corrected some errors in\nversion 1.0 (Martin et al., 2005). Version 2.0 covered proceedings from 1999 through late 2007 (excluding 2003)\nwith about 5.5 million English tokens and 2.6 million Inuktitut tokens (Farley, 2008).\nFor the purposes of this workshop, we received pre-release access to a beta version of the Nunavut Hansard\nInuktitut–English parallel corpus version 3.0, which contains 17.3 million English tokens and 8.1 million Inuktitut\ntokens, a huge increase over the original data. We refer to this pre-release version as 3.0 or 3.0 beta. We use\ndeduplicated development and test sets in our experiments. The ﬁnal Nunavut Hansard Inuktitut–English parallel\ncorpus version 3.0 corpus is now available and is described in Joanis et al. (2020). Through our early access to this\ncorpus, we provided feedback on the corpus and on preprocessing best practices, which have been incorporated\ninto the data release.\nThe corpus contains 17.3 million English tokens and 8.1 million Inuktitut tokens, spanning 1999 to 2017, a\nmajor increase over the version 1.0 and 2.0 releases (Martin et al., 2003, 2005; Farley, 2008). This is the largest\ncorpus we had access to for this workshop, and is arguably no longer truly “low-resource” for machine translation\nresearch. It is, however very domain-speciﬁc, and differs in domain from the other parallel corpora we use in our\nexperiments.\nAs prior machine translation work performed translation on romanized Inuktitut (Micher, 2018b), we chose\nto do the same. We converted Inuktitut data from syllabics as follows: we ﬁrst applied uniconv,2 then repaired\nerrors (e.g., incorrectly handled accented French characters in the Inuktitut data) usingiconv, then identiﬁed and\ncorrected other characters using a hand-built preprocessing script (including treating word-internal apostrophes as\nnon-breaking characters on the Inuktitut side of the data).3\nWe ran standard preprocessing scripts from Moses (Koehn et al., 2007): punctuation normalization, tokeniza-\ntion, cleaning, and truecasing. We discuss subword segmentation in Section 4.3.\n4.2.2 Yupik–English Data\nWe had access to parallel data for two Yupik languages: St. Lawrence Island Yupik ( ess) and Central Alaskan\nYup’ik (esu). In both cases, all of the available data was verse-aligned data drawn from the Bible. For St. Lawrence\nIsland Yupik, we had access to New Testament data only. We used Luke for development and validation and used\nJohn for testing. The remainder of the data was used for training. The data was preprocessed for machine transla-\ntion experiments as follows: bracketed text was removed from the English data,4 apostrophes were normalized in\n2uniconv is distributed with Yudit: www.yudit.org\n3Joanis et al. (2020) provides slightly updated scripts; we note that neither those scripts nor the ones described here fully conform to spelling\nand romanization conventions as described in the Nunavut Utilities ( www.gov.nu.ca/culture-and-heritage/information/\ncomputer-tools).\n4This consisted of rephrasings of entire verses, and was not present in all verses.\n4.3. INUKTITUT MACHINE TRANSLATION EXPERIMENTS 15\nSt. Lawrence Island Yupik, and then all data was punctuation-normalized, tokenized, cleaned, and truecased using\nstandard Moses scripts (Koehn et al., 2007) with English default settings.\nFor Central Alaskan Yup’ik, we had access to the full Bible. For consistency, we still used Luke for devel-\nopment and validation and used John for testing. The remainder of the data was used for training. For Central\nAlaskan Yup’ik, we normalize apostrophes and convert characters with certain diacritics that would otherwise\nbe split by the Moses tokenizer. Both Central Alaskan Yup’ik and its corresponding English translations were\npunctuation-normalized, tokenized, cleaned, and truecased using standard Moses scripts (Koehn et al., 2007) with\nEnglish default settings. In the case of Central Alaskan Yup’ik, we performed tokenization without aggressive\nhyphen-splitting.5\nTable 4.1 shows the number of lines in the datasets; the Central Alaskan Yup’ik training data is more than 5\ntimes larger than the St. Lawrence Island Yupik training data.\n4.2.3 Guaraní–Spanish Data\nAs with the Yupik datasets, we had verse-aligned parallel Bible data available in Spanish and Guaraní. We used\nLuke for development and validation and used John for testing, with the remaining data used for training. Guaraní\ndata was ﬁrst preprocessed with quotation and apostrophe normalization, along with the removal of paragraph and\nother symbols that were artifacts of the initial data creation. Guaraní and Spanish data were then punctuation-\nnormalized, tokenized, cleaned, and truecased using standard Moses scripts (Koehn et al., 2007) using Spanish\ndefaults.\n4.3 Inuktitut Machine Translation Experiments\nOur Inuktitut-English machine translation efforts were largely concerned with doing initial experiments on the\npre-release version of the Nunavut Hansard parallel corpus. Being substantially larger than previous releases –\nto our knowledge, by far the largest aligned parallel corpus of a polysynthetic language to date – this corpus\noffered a unique opportunity to try contemporary NMT methods on Inuktitut, and consider whether methods of\nsegmentation like byte-pair encoding (BPE; Sennrich et al., 2016) are sufﬁcient to handle a language of this level\nof complexity.\nIn the experiments that follow, our baseline systems – that is, conventional Transformer (Vaswani et al., 2017)\nNMT systems, using BPE and standard hyperparameter settings – always outperformed the experimental systems\n(which included special segmentation procedures and multi-source attention). This suggests that contemporary\nmethods are indeed adequate for processing Inuktitut, although we do not consider the case closed as there are\nmany interesting possibilities for principled segmentation that we have not yet explored.\n4.3.1 Segmentation experiments\nIn this set of experiments, we contrast automatic segmentation (by byte-pair encoding) with more morpholog-\nical segmentations based on human knowledge of Inuktitut morphology, and also consider a simple method of\ncombining them. We perform our machine translation experiments contrasting these approaches in the Inuktitut-\nto-English direction.\nByte-Pair Encoding\nByte-pair encoding (BPE; Sennrich et al., 2016) – broadly, the segmentation of text at the character-level into\nlarger chunks by compressing the text and using the resulting compression units as word segmentation – has\nbecome a ubiquitous practice in current machine translation. While the units discovered are not guaranteed to\ncorrespond to morphemes as such, the resulting systems do end up working at a more morpheme-like level, with\nunits larger than a character but smaller than a word.\nTable 4.2 shows the segmentation of several words according to four BPE vocabulary sizes. The Inuktitut\nloanword siipiisiikkut (meaning CBC or Canadian Broadcasting Corporation) is frequent enough in the corpus\n5This keeps hyphenated sufﬁxes attached, but has the downside of non-ideal interactions with subword segmentation, occasionally breaking\nsufﬁxed biblical names into two parts, with the latter attached to the hyphen and Central Alaskan Yup’ik sufﬁx.\n16 CHAPTER 4. MACHINE TRANSLATION\nthat at 30000 merges it is represented as a single token. The word qimirruvita (meaning are we looking at ,\nas in the context Are we looking at trying to ﬁnd out? or qimirruvita qaujimanittinnuk ) can be split into the\nmorpheme qimirru- (to scan, to inspect 6) and the verb ending -vita? (are we (3+) ...? 7); we see that here BPE\nsuccessfully respects the morpheme boundary at all sizes, segmenting exactly and only along that boundary with\na vocabulary of 30000. For utaqqivita (meaning are we waiting for?, as in the context What are we waiting for?\nor kisumik utaqqivita?), the story is somewhat different. Though the word contains the same sufﬁx -vita? with\nthe verb root utaqqi- (to wait8), BPE does not segment the words along the expected morpheme boundaries; the\nonly segmentation that respects them (500) appears to oversegment. In these examples, we are able to see clear\nmorpheme splits in the surface form, but this is not always the case. In many cases, the underlying forms may\nundergo phonological changes at the boundaries where two morphemes meet, making it impossible to segment\nthe word such that the resulting units have a uniform representation across all examples of that morpheme.\nBPE vocab siipiisiikkut qimirruvita utaqqivita\n500 si |i |pi |i |si |i |kkut qi |mi |r |ru |vi |ta uta |qq |i |vi |ta\n5000 si |i |pii |si |i |kkut qimirru |vi |ta utaqq |ivi |ta\n15000 siipii |si |ikkut qimirru |vi |ta utaqqivi |ta\n30000 siipiisiikkut qimirru |vita utaqqivi |ta\nTable 4.2: Segmentation of three words according to BPE at four different vocabulary sizes.\nOne of our topics of investigation was whether this procedure alone would be sufﬁcient to pre-process Inuktitut\nfor machine translation, whether more sophisticated morphological processing would be necessary, or whether a\ncombination of the two (morphological processing where possible, BPE for the rest) might prevail.\nMorphological Analysis\nThe Nunavut Hansard version 1.1 was the starting point for morphological analysis of the larger, later-released\ncorpus (version 3.0). As version 1.1 is a subset of the days of debate included in version 3.0, we made use of prior\nmorphological processing of version 1.1 when possible (processing described in Micher (2018a) and summarized\nhere). Every word type of the version 1.1 corpus was processed with the Uqailaut analyzer (Farley, 2009), which\nprovides morpheme segmentation and labeling (including deep representation and morphological category tags).\nAbout 70% of the corpus was analyzable by this tool. The remaining 30% was subsequently processed using a\nneural morphological analyzer, which is trained on a subset of the Uqailaut processed data (Micher, 2017). Filter-\ning out noise (concatenations of numbers and alphanumerics), we were left with approximately 413K processed\nword types from version 1.1 of corpus.\nWe then extracted the word types from the larger corpus, using the same noise ﬁltering script as with version\n1.1 and omitting the word types that had been successfully processed already from version 1.1. We ended up\nwith ∼1.14M additional types. From these another ∼9K words were identiﬁed as English and removed, yield-\ning ∼1.13M types to process. However, we note a few differences between these corpora, which affected the\nprocessing pipeline. First, the romanization scheme performed for version 1.1 of the Hansard is not identical to\nthe romanization we performed on version 3.0 beta. In many cases, the resulting romanizations of words match,\nbut in the cases that do not, the morphological analysis needed to be performed anew. For example, there are\ndifferences in romanization between Hansard versions (e.g. “lh\" vs. “&\" for the lateral fricative) and between\ndialects (e.g. “s\" vs. “h\" for a particular phoneme); since Uqailaut presumes “&\" and “h\", these are substituted\nbefore re-processing. After all of the pre-processing, we followed the same procedure as with version 1.1 of the\ncorpus, ﬁrst processing what the Uqailaut analyzer would process, and sending the remaining types through the\nneural morphological analyzer. In total, we have 1,548,500 types, processed through one or the other analyzer.\nFor our work during the workshop, however, we are training and evaluating using only the Uqailaut segmenta-\ntions (that is to say, without using the neural parser), as the neural parses were not yet ﬁnished at the time of these\nexperiments. We expect that the more complete analyses of the neural parser will have a more positive effect on\ndownstream performance in future experiments.\n6https://uqausiit.ca/node/10333\n7https://uqausiit.ca/verb-ending/vita\n8https://uqausiit.ca/node/12189\n4.3. INUKTITUT MACHINE TRANSLATION EXPERIMENTS 17\nIn the following experiments, the morphologically processed text uses “deep\" forms, in the sense of Micher\n(2017), rather than the surface forms. Since Uqailaut, and thus the neural generalization of it, only parse surface\nwords into deep forms (and do not generate surface words from deep forms), we present our experiments with\ndifferent segmentation approaches solely in the Inuktitut to English translation direction.\nSystem conﬁguration\nThe model uses a 3-layer encoder, a 3-layer decoder, a model dimension of 512 and 2048 hidden units in the\nfeed-forward networks. The network was optimized using Adam (Kingma and Ba, 2014), with an initial learning\nrate of 1e−4, decreasing by a factor of 0.7 each time the development set BLEU did not improve for8000 updates,\nand stopping early when BLEU did not improve for 32,000 updates.\nIn addition to the most common automatic MT evaluation metric, BLEU 9 (Papineni et al., 2002), we also\nevaluated our MT experiments using two recently proposed metrics, chrF10 (Popovi´c, 2015) and YiSi (Lo, 2019),\nwhich were shown to correlate better with human judgments on translation quality in English by Ma et al. (2019).\nResults\niku segmentation eng segmentation BLEU chrF YiSi-0 YiSi-1\n5000 BPE 5000 BPE 27.7 47.1 62.9 70.8\nMorphological 5000 BPE 23.3 42.5 58.2 66.1\nMorph + 5000 BPE 5000 BPE 26.6 46.8 62.6 70.5\nTable 4.3: Results of Inuktitut-to-English NMT systems as evaluated by BLEU, chrF, YiSi-0 and YiSi-1 .\nWe compared BPE of various vocabulary sizes to the morphological analysis described above. In Table 4.3,\nwe observe that morphological analysis underperforms BPE across all metrics.\nWe think this is not due to a problem in the morphological analysis itself (e.g. identifying morphemes incor-\nrectly), but that the process left unanalyzable words intact, whereas BPE manages to segment all words into more\nmanageable pieces. We therefore also performed a preliminary attempt to combine them, in hopes of combining\nsome of the beneﬁts of true morphological analysis with the statistical advantages of BPE. First, we took the output\nof morphological analysis (i.e., the input corpus to the “Morphological\" system in Table 4.3), trained a new BPE\nmodel on it, and segmented it according to this model. Manual inspection of the results of this process suggest\nthat morphemes identiﬁed in morphological analysis were typically left intact by BPE – that is to say, they were\nidentiﬁed as units by BPE as well – and only unanalyzed words were further segmented.\nThis system also underperformed the BPE-only system, but only by small margins. We think that this avenue\nis still promising, as there are many possible ways to integrate BPE and morphology. Many questions remain:\n• Does one resegment only the unanalyzed words, or all words?\n• Does one train the BPE model on only unanalyzed words, or all words?\n• Do we use surface morphemes or underlying morphemes?\n• Do we rejoin underlying forms or keep them segmented?11\nAlso, as not all the corpus was fully analyzed, more development in neural analysis will probably lead to\nimprovements downstream.\n9BLEU scores were computed using SacreBLEU (Post, 2018), compared to untokenized but punctuation-normalized references.\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.2\n10chrF scores were computed against untokenized but punctuation-normalized references using SacreBLEU with\nchrF2+case.mixed+numchars.6+numrefs.1+space.False+version.1.4.2 settings.\n11Joanis et al. (2020) ﬁnds that using underlying forms, but rejoining them before BPE segmentation, gives a performance improvement\nover deep forms alone in corpus alignment.\n18 CHAPTER 4. MACHINE TRANSLATION\n4.3.2 Single source and multi-source experiments\nOne experimental theme we pursued in this workshop was whether multi-source techniques (Zoph and Knight,\n2016; Nishimura et al., 2018; Libovický and Helcl, 2017), typically used for training MT systems with multiple\nsource languages, could be of value when applied to multiple representations of the input text, as a potential way\nto combine the beneﬁts of two different kinds of analysis.\nA recent result in multilingual machine translation (Littell et al., 2019) suggested that it can be valuable, when\ntraining MT on a corpus that has undergone signiﬁcant processing (in that case, machine translation of the original\nsource into Russian), to attend to both the original text and its processed version. That is to say, “attention\" in\nMT makes it possible to avoid having to choose between using the original text or a process that may have been\nhelpful (or may have destroyed useful information); rather, we can allow the model to attend to the results of any\nstage in the pipeline, and learn for itself which representations to attend to the most. The above result concerned\na pre-processing step that was itself machine translation – that is to say, this was a “pivot\" system in which L1\nis translated to L2, and L2 is translated into L3. We were wondering whether the result might also apply for\nprocessing steps that were not machine translation. Would, for example, it be fruitful to attend to two different\npre-processings: say, BPE and morphological, syllabics or romanized, etc.?\nSystem conﬁguration\nThe following experiments were performed using the architecture in Littell et al. (2019), a variant of Transformer\n(Vaswani et al., 2017) with multi-source attention, implemented in the Sockeye framework (Hieber et al., 2017)\nfor machine translation.\nThe model uses two 3-layer encoders (one for each source type), a 3-layer decoder, a model dimension of512\nand 2048 hidden units in the feed-forward networks. The decoder attended to each decoder using “ﬂat\" attention\n(that is, attending to each and combining the result by simple addition, rather than an additional, hierarchical\nattention layer). The network was optimized using Adam (Kingma and Ba, 2014), with an initial learning rate of\n1e−4, decreasing by a factor of 0.7 each time the development set BLEU did not improve for 8000 updates, and\nstopping early when BLEU did not improve for 32,000 updates.\nResults\nAs an initial sanity check, we performed two tests of the idea:\n1. Source 1: BPE vocab size 5000, source 2: BPE vocab size 30000\n2. Source 1: Inuktitut in syllabics, BPE vocab size 5000; source 2: Inuktitut romanized, BPE vocab size 5000.\nWe did not expect these to show signiﬁcant gains, but we wanted to make sure the systems did not experience\na serious drop in scores. Unfortunately, Table 4.4 indeed showed such a performance drop, with the multi-source\nsystems performing very poorly.\nSource Target BLEU\nInuktitut, syllabics, BPE 5000 English, BPE 5000 30.3\nInuktitut, romanized, BPE 5000 English, BPE 5000 27.7\nInuktitut, syllabics, BPE 5000 + English, BPE 5000 6.3\nInuktitut, romanized, BPE 5000\nInuktitut, romanized, BPE 5000 + English, BPE 5000 2.5\nInuktitut, romanized, BPE 30000\nTable 4.4: Preliminary multi-source iku→eng\nWe believe this is because the multi-source source system greatly increases the number of parameters without\nan associated increase in information in the corpus. If we compare this to the positive results in Littell et al.\n(2019), the difference is that there the introduction of a third language greatly increases the amount of information\navailable to the system: it is not just another view of the same data. So, rather than continue exploring additional\nmonolingual multi-source setups (e.g., BPE and morphology together), we instead moved on to the multilingual\nmulti-source experiments detailed in Section 4.4.2.\n4.3. INUKTITUT MACHINE TRANSLATION EXPERIMENTS 19\n4.3.3 Challenges in Evaluation of English-to-Inuktitut MT\nFor questions of segmentation, we primarily looked at the Inuktitut-to-English direction, since our morphological\nanalyzer was only able to parse, rather than generate. (That is to say, while we could output segmented, underlying\nmorphemes, we could not, at that time, rejoin them into ﬂuent outputs.) For English-to-Inuktitut, we only looked\nat BPE-based systems, since these can trivially be de-segmented. In this translation direction, we focused on\nquestions of evaluation because morphologically complex languages pose a challenge in terms of the choice of\nautomatic evaluation metric.\nBLEU (Papineni et al., 2002) is a common metric for evaluation of machine translation output given refer-\nence translations. However, because BLEU score is (typically) computed at the word level, an error in a single\nmorpheme is penalized just as harshly as a completely incorrect choice of terminology. This can be expected to\nhave a particularly detrimental effect when evaluating translation output in morphologically complex languages;\neven if the system chooses the correct lemma, any errors of morphological inﬂection will be counted as whole-\nword errors, decreasing the count of correctly-predicted n-grams. BLEU score could also be computed over byte\npair encodings rather than words, but this poses challenges when trying to compare systems built with different\nvocabularies.\nchrF sidesteps the segmentation issue by ﬁrst removing whitespace before counting character n-grams and\ncomputes a precision/recall-balanced score over the character n-gram counts. On the other hand, YiSi-0 respects\nthe word boundaries in the MT output but uses the character-level longest common substring accuracy to evaluate\nthe word-level similarities and aggregates the word-level similarity scores into the sentence-level score. These\ntwo automatic evaluation metrics based on character-level information would be more suitable for evaluating MT\noutput in morphological complex languages. In fact, Ma et al. (2018) showed that chrF correlates the best with\nhuman in evaluating Finnish translation output and YiSi-0 correlates the best with human in evaluating Turkish\ntranslation output. However, we think it important to point out that the complexity of Inuktitut morphology is\nhigher than that of Finnish or Turkish and there is no existing work on MT evaluation for polysynthetic languages.\nThis remains an area for future work.\nSystem conﬁguration\nThe English-to-Inuktitut MT system was built using the same architecture as that of the system mentioned in\nSection 4.3.1. We evaluated the system at both word-level and 5k BPE-vocabulary segmentation using BLEU, 12\nchrF,13 and YiSi-0. Since YiSi-0 is a weighted harmonic mean of precision and recall, we also dissected YiSi-0\ninto pure precision and recall for further analysis.\nResults\nFirst and the foremost, we have to emphasize that MT system scores for different translation directions are not\ndirectly comparable. Thus, one should not conclude from Table 4.5 that translating Inuktitut into English is an\neasier task to the opposite direction, or the translation quality of a system in one direction is better than that in the\nother direction.\nTranslation Evaluation YiSi-0\ndirection unit BLEU chrF weighted-F precision recall\niku→eng word 27.7 47.1 62.9 66.2 62.1\neng→iku word 17.8 46.7 48.0 49.9 47.9\niku→eng 5000 BPE 29.5 47.4 64.1 67.6 63.3\neng→iku 5000 BPE 13.7 46.4 56.4 59.0 56.0\nTable 4.5: Results of English-to-Inuktitut NMT systems as evaluated by BLEU, chrF and YiSi-0 (with pure YiSi-0 precision,\ni.e. α=0.0 and recall, i.e. α=1.0 for supplementary analysis).\n12In this table and table 4.4, BLEU scores were computed against untokenized but punctuation-normalized references using SacreBLEU\nwith BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.2 settings.\n13chrF scores were computed against untokenized but punctuation-normalized references using SacreBLEU with\nchrF2+case.mixed+numchars.6+numrefs.1+space.False+version.1.4.2 settings.\n20 CHAPTER 4. MACHINE TRANSLATION\nInstead, we would like to point out that there is a notable difference in word-level BLEU scores for the sys-\ntems in two translation directions because BLEU penalizes systems on failing to correctly inﬂect a word form\nequally harshly as choosing an entirely incorrect word; thus MT systems translating into morphological complex\nlanguages are expected to achieve lower word-level BLEU scores. A huge difference can also be seen in YiSi-0\nscores using word segmentation in evaluation. However, the chrF score difference between the two translation\ndirections is marginal.\nWhen evaluating translation output at subword unit level, both BLEU and chrF showed a wider score difference\nwhen the translation direction was ﬂipped. However, YiSi-0 showed a smaller difference. The contradicting results\nshowed that evaluating translation output in polysynthetic languages itself is a challenging and unsolved research\nproblem.\nWithout human evaluation on translation output in polysynthetic languages, we could not conclude whether\nthe quality of the English-to-Inuktitut MT system is acceptable or not (or whether it is sufﬁcient for some use\ncases but not others). We hope that future human evaluation of machine translation into polysynthetic languages\nwill provide a basis for the examination of different evaluation approaches, allowing future researchers to select\nthe most appropriate evaluation metrics.\n4.4 Low-Resource Experiments\nIn keeping with the theme of the workshop, our low-resource machine translation experiments involve neural\nsystems rather than phrase-based ones, despite the fact that they are built from extremely small datasets. While\nwe perform our experiments with fairly simple modern neural models and minimal hyperparameter tuning, recent\nwork (Sennrich and Zhang, 2019) suggests that careful tuning of hyperparameters can result in NMT systems\noutperforming statistical machine translation systems even on datasets of around 5000 sentences (comparable to\nour smaller datasets).\nMost of the low-resource machine translation experiments were performed using Sockeye (Hieber et al., 2017),\nand the multi-source generalization of Sockeye introduced in Littell et al. (2019).\n4.4.1 Baselines and Vocabularies\nRNN Transf. Transf. Transf. Transf.\nBPE BPE Word FST FST+BPE\ness→eng 4.2 8.4 7.3\neng→ess 3.3 4.4 3.5\nesu→eng 10.7 13.9 6.5\neng→esu 5.4 5.3 3.3\ngrn→spa 10.5 7.4 7.1 9.6\nspa→grn 8.6 7.1 8.3 8.1\nTable 4.6: BLEU scores of baseline and vocabulary experiments for Yupik–English and Guaraní–Spanish machine translation\nexperiments. All BPE vocabularies in this table are of size 5000, learned separately.\nWe ﬁrst compare RNN and Transformer translation models using BPE vocabularies of 5000. The size of 5000\nwas selected for consistency with other experiments and because it was among the highest performing vocabulary\nsize on initial RNN experiments for several language pairs (not reported here). The RNN models were trained\nusing OpenNMT (Klein et al., 2017) with default settings, and the Transformer models were trained using Sockeye\n(Hieber et al., 2017) with a 3 layer encoder, 3 layer decoder, batch size 2048, optimized toward perplexity, and the\nremaining parameters set to defaults. As Table 4.6 shows, the Transformer system outperformed the RNN system\nin all but one case (which was within 0.1 BLEU); we use the Transformer system for all remaining experiments.\nWe compare using a BPE vocabulary of 5000 symbols to using a whole word vocabulary. In all cases, the BPE\nvocabulary outperforms the whole word vocabulary (by between 0.9 and 7.4 BLEU points). Using whole words,\nEnglish–St. Lawrence Island Yupik experiments were run with vocabulary sizes of 4787 and 26888 (respectively,\nincluding special characters), while English–Central Alaskan Yup’ik whole word vocabularies consisted of 13501\n4.4. LOW-RESOURCE EXPERIMENTS 21\nand 106736 types respectively. Given the small data sizes and large Yupik vocabulary sizes, it is unsurprising that\nBPE outperforms whole words; there may simply not be enough examples of many types in the long tail for the\nsystem to accurately translate them, and the word system includes a large number of out of vocabulary items in\nthe test set.\nFollowing the results of the Yupik experiments, we omit the RNN experiments for Guaraní–Spanish and in-\nstead start with a baseline of a Transformer model (3 layer encoder, 3 layer decoder, batch size 2048, optimized\ntoward perplexity, remaining parameters set to defaults), using separately learned BPE encodings for Spanish and\nGuaraní with vocabularies of 5000 types each. There does exist other work on machine translation for Guaraní–\nSpanish, notably an online gister 14 and work in Rudnick (2018). Though Rudnick (2018) also performs experi-\nments on Bible translation, we do not compare directly, as those results are measured on stemmed output.\nFor Guaraní–Spanish, we also experiment with full-word vocabularies, FST-segmented vocabulary (Guaraní\nside only; Spanish side BPE 5000), and an FST-segmented vocabulary with backoff to BPE (all Guaraní words\nleft unsegmented by the FST were segmented by a BPE model learned for a BPE 5000 vocabulary on Guaraní;\nSpanish side BPE 5000). As shown in Table 4.6, the baseline BPE model outperforms all other experiments.15\n4.4.2 Yupik Language Experiments\nOur Yupik language experiments begin with baseline RNN and Transformer models. Finding that the Transformer\nstrongly outperforms the RNN (Table 4.6), we perform the remainder of the experiments with the Transformer\narchitecture only.\nIn addition to the baseline, we perform two experiments: multi-source experiments on a multi-parallel subset\nof the data and multilingual NMT system experiments. BPE vocabularies of size 5000 were learned separately\non each language’s training data usingsubword-nmt (Sennrich et al., 2016). Our most promising low-resource\nexperiments, described in Section 4.4.2 involve the use of higher resource languages from the same language\nfamily to build multilingual neural machine translation systems which can then be ﬁnetuned for speciﬁc low-\nresource languages.\nMultisource\nIn order to experiment with multisource machine translation, we build a multiparallel verse-aligned corpus from\nthe intersection of all available Yupik Bible data. The resulting New Testament corpus has 5449 lines for training,\n1091 lines for development and validation, and 874 lines for testing. It contains data in St. Lawrence Island Yupik\nand Central Alaskan Yup’ik, as well as data from two English Bibles. We call the English Bibles engess (for\nthe English Bible originally aligned to St. Lawrence Island Yupik) and engesu (for the English Bible originally\naligned to Central Alaskan Yup’ik). We preprocessed them identically to the baseline experiments, with one\nchange: we removed verse numbers from Central Alaskan Yup’ik and its corresponding English ( engesu) as\nthose were not present in the St. Lawrence Island Yupik corpus.\nWe compared single-source (Sing.) and multi-source (Mult.) approaches, as described in §4.3.2, as well\nas separately learned and jointly learned 5000 symbol BPE representations (the joint BPE representations were\nlearned across all 4 sides of the multiparallel corpus). For the multi-source experiments, we tried translating into\nCentral Alaskan Yup’ik using its corresponding English and St. Lawrence Island Yupik, as well as translating\ninto St. Lawrence Island Yupik using its corresponding English and Central Alaskan Yup’ik. Without any major\nparameter search, we found that the joint BPE single-source systems performed the best.\nAs these BLEU scores are extremely low, it is quite difﬁcult to draw any conclusions from this set of exper-\niments; the following notes should be understood in that context. We do observe that for single-source, using a\njointly trained BPE vocabulary performs better than separately trained BPE vocabularies. This may be due in part\nto improved translation of copied terms (e.g., names). We do not observe the same consistency in multisource.\nPerhaps unintuitively, in single-source experiments, we ﬁnd that swapping the English Bibles (translatingengesu\ninto ess and engess into esu) performs better than the “correct” pairs. This highlights several challenges of\nperforming machine translation using Bible corpora: we do not have a guarantee in our case that the “source”\nEnglish Bible is the version from which the Yupik Bibles were translated, Bible translations may rely on metaphor\n14http://iguarani.com/\n15BLEU scores were computed against untokenized but punctuation-normalized references using SacreBLEU with\nBLEU+case.lc+numrefs.1+smooth.exp+tok.13.a+version.1.3.7 settings.\n22 CHAPTER 4. MACHINE TRANSLATION\nSing. Mult. Sing. Mult. Mult.\n(joint) (joint) (joint+tied)\ness–esu 3.8 4.7\nengess–esu 4.8 4.9\nengesu–esu 3.6 3.2 3.9 2.8 3.1\nengess–ess 4.0 3.1 4.4 3.4 3.4\nengesu–ess 4.7 5.4\nesu–ess 4.2 4.8\nTable 4.7: BLEU score results for experiments on joint and separate BPE learning, along with multisource experiments. Tested\non the multiparallel subset of Yupik corpora.\nor other non-literal phrases, and verse alignment provides additional challenges due to mismatches between sen-\ntence and verse boundaries. In some cases, we observe that a sentence spans more than one verse, with a name\nappearing in the ﬁrst verse in English and in the second verse in Yupik or vice versa, an impossible challenge for\nmachine translation without extrasentential context to overcome; this is a known challenge in parallel Bible cor-\npora (Mayer and Cysouw, 2014). We also did not perform hyperparameter optimization due to time constraints;\nmore extensively tuned models may show different results.\nMultilingual\nMultilingual neural machine translation has been proposed as a means of improving neural machine translation of\nlow-resource languages, using a variety of distinct approaches. These approaches depend are split into approaches\nto translate into or out of low-resource languages. Neubig and Hu (2018) explore the multilingual translation task\ntranslating from multiple low-resource languages into a single high-resource language. Gu et al. (2018) also work\nin the same translation direction, and incorporate large amounts of monolingual data and many closely-related\nsource languages.\nOur interest is on translation into low-resource languages. In that direction, Ha et al. (2016) perform mul-\ntilingual neural machine translation by tagging each subword with a language-speciﬁc tag, and then building a\nsystem based on available training data. Johnson et al. (2017) use a single special token at the beginning of input\nsentences to indicate the desired target language to translate into. Rikters et al. (2018) follow this approach to do\nmultilingual translation into and out of morphologically rich languages, though their low-resource setting consists\nof more than 3 million sentence pairs.\nSt. Lawrence Island Yupik, Central Alaskan Yup’ik, and Inuktitut belong to the same language family. Despite\nthis, they have very limited vocabulary overlap in our parallel data (less than 1% type overlap between Inuktitut\nand Yupik, and less than a 3% type overlap between St. Lawrence Island Yupik and Central Alaskan Yup’ik). This\nis certainly due in part to the different domains we had available: legislative text (Inuktitut) and Bible (Yupik).\nAs described in Section 4.2.2 and Section 4.2.1, our data spans a wide range in terms of size, from approximately\n5000 lines of text to approximately 1.3 million lines. We approximately follow the Johnson et al. (2017) approach\nin our approach to translating from English into Inuktitut and Yupik languages.\nBaseline Multilingual ess-Ad. Multi. esu-Ad. Multi.\neng–ess 4.4 5.8 6.5 1.3\neng–esu 5.3 5.7 1.9 6.0\nTable 4.8: BLEU scores for experiments on multilingual neural machine translation. The baseline is the original Transformer\nbaseline for each language pair. Multilingual is the single multilingual system (trained on Inuktitut and Yupik data), and the\nremaining two columns show that system ﬁne-tuned on a particular variety of Yupik.\nWe train joint BPE (vocabulary 5000) on Inuktitut, St. Lawrence Island Yupik, and Central Alaskan Yup’ik,\ndownsampling the Inuktitut and upsampling St. Lawrence Island Yupik to match the size of Central Alaskan\nYup’ik. We prepend a language tag (e.g. “<ess>”) to each source and target sentence in the three sub-corpora.\nNext we train a Transformer model (our “multilingual baseline”) on the concatenation of all available training\n4.4. LOW-RESOURCE EXPERIMENTS 23\nBaseline Multilingual ess-Ad. Multi. esu-Ad. Multi.\neng–ess 26.9 28.0 30.1 10.5\neng–esu 31.0 32.5 16.7 33.2\nTable 4.9: YiSi-1 scores (higher is better) computed using ess or esu BPE 5000 embeddings built by word2vec (Mikolov\net al., 2013) for experiments on multilingual neural machine translation. The baseline is the original Transformer baseline for\neach language pair. Multilingual is the single multilingual system (trained on Inuktitut and Yupik data), and the remaining two\ncolumns show that system ﬁne-tuned on a particular variety of Yupik.\ndata (with no sampling, 3 layer encoder, 3 layer decoder, 512 embedding size, early stopping on perplexity of the\nconcatenated development data). For St. Lawrence Island Yupik and Central Alaskan Yup’ik, we then ﬁne-tune\nthe multilingual baseline on all language-speciﬁc training data (with early stopping based on perplexity on the\nlanguage-speciﬁc development data). The BLEU score results are shown in Table 4.8. Table 4.9 reports YiSi\nresults, which follow the same trend as the BLEU score results. As expected, ﬁne-tuning on language speciﬁc\ndata boosts performance on that particular language (while the output on the other language appears to exhibit\ncatastrophic forgetting (Kirkpatrick et al., 2017)), giving us our best performance. However, with BLEU scores in\nthe single digits, it is clear that there is still a long way to go before the MT output may be genuinely useful (e.g.\nin post-editing or interactive translation) for these low-resource languages.\n24 CHAPTER 4. MACHINE TRANSLATION\nChapter 5\nLanguage Modelling\nIn this chapter, we report on language modelling experiments, comparing different tokenization strategies for\npolysynthetic languages. We trained a state-of-the-art RNN language model using the character, BPE, Morfessor\nand FST as the unit for segmenting text data. In order to facilitate comparisons across the tokenization strategies,\nwe carefully selected datasets for two experimental settings: 1) A setting where all the data available for a lan-\nguage is used and 2) a setting where only the New Testament in a language is used. The former setting provides\nus an opportunity to utilize all the data we have in a language while the latter allows us to draw a more precise\ncomparison across languages. We use the average perplexity per character or the character-level perplexity as a\nmetric to compare different models. The results show that the linguistically-oriented, FST segmentation strategy\nperformed the best in modelling polysynthetic languages when it was available. In addition, difﬁculty of mod-\nelling different languages is compared using the average perplexity per word or the word-level perplexity. The\npotential of FST in aiding language modelling of polysynthetic languages and implications on comparing models\nfor different languages are discussed.\n5.1 Data Preparation\nAfter much consideration, we selected four low-resource, polysynthetic languages for our language modelling\nexperiments (hereafter referred to by ISO 639-3 code): St. Lawrence Island Yupik (ess), Central Alaskan Yup’ik\n(esu), Inuktitut (iku) and Guaraní (grn). These languages were chosen because we had the most available text\ndata in them. We had at least the Bible, the Gospel books in New Testament in particular, in these languages,\nand that allowed us to have a commonality among the datasets to facilitate comparison across the languages. In\naddition to the polysynthetic languages, we included two well-researched, non-polysynthetic languages: English\n(eng) and Spanish ( spa). The eng and spa data were included to provide comparison between polysynthetic\nlanguages and non-polysynthetic languages as esu and eng and grn and spa were parallel translations.\nWe designed two experimental settings to fully utilize available data while ensuring comparability across dif-\nferent languages. As for the 1) all data setting, we included any available monolingual data in a given language,\nincluding but not limited to the New Testament. The second setting, the 2) New Testament only setting, focused\nonly on the New Testament data in order to further ensure comparability given the near-parallel data across differ-\nent languages. Regardless of the settings, Luke was used as the development set and John as the test set to further\nfacilitate fair comparison as we had the Gospel books in all languages. This ensured that different languages\nSplit Setting 1: All data Setting 2: New Testament\nTrain Rest of the data available Rest of New Testament\n(e.g. Old Testament, transcripts, stories)\nDev Luke Luke\nTest John John\nTable 5.1: Train-dev-test split\n25\n26 CHAPTER 5. LANGUAGE MODELLING\nLanguage Sentences Words Types Type/Token Mean distance to unseen\ness 20,899 206,691 58,637 0.28 3.28\nesu 33,102 474,499 106,381 0.22 4.15\niku 31,103 466,705 126,162 0.27 3.70\ngrn 30,078 622,999 38,944 0.06 14.63\neng 21,835 395,368 11,258 0.03 31.72\nspa 30,078 840,937 24,829 0.03 30.39\nTable 5.2: Descriptive statistics for setting 1 (all available data)\nshared a development set and a test set and a part of the train set (the rest of the New Testament) in common\neven though the exact train set available in each language may differ from one another. The train set in the 1) all\ndata setting included the New Testament, but may also include the Old Testament, transcripts and oral narratives\nif available. This setting, therefore, fully utilizes the data we had in each language. In the 2) New Testament\nsetting, the development and test sets stayed the same, but the train set included the rest of the New Testament\nonly. It should be noted that we did not align the Bibles at the sentence level, and there was some variability\namong different Bible translations as discussed in Chapter 4. However, esu and eng and grn and spa Bible\ntranslations were assumed to be parallel, and we assume that the other Bible translations provide comparable texts\nwith similar intensions overall. While the 2) New Testament only setting may provide a more precise comparison,\nthe 1) all data setting may be more representative of the reality given the limited size of the data for the former\nsetting. Table 5.1 summarizes the two experimental settings and the dataset split.\nGiven the data split, we preprocessed the datasets systematically to further ensure comparability among sub-\nsequent language models. We removed redundant, bracketed texts when applicable, and normalized apostrophes\nas they were meaningful in some languages and should not be tokenized separately from their surrounding words.\nThen, we normalized the punctuation and tokenized the texts using Moses scripts (Koehn et al., 2007) with default\nsettings. The overall preprocessing for language modelling experiments resembles that for machine translation\nexperiments discussed in Chapter 4 except that we did not truecase the data for langauge modelling experiments.\nTables 5.2 and 5.3 summarize descriptive statistics of the preprocessed data under each setting. Overall, it\nseems that the characteristics of a language as captured by the statistics are quite similar under the two settings.\nThis may not be surprising given that the two settings concern very similar domains. While it remains to be seen if\nthese descriptive statistics would be similar under a different setting for the languages, we observed the followings\nfor the languages given our datasets: As discussed in Chapter 3, the languages seem different in the TTR and\nmean distance to the next unseen word. ess, esu and iku consistently show a higher TTR and a lower mean\ndistance to the next unseen word than grn. While grn is considered as a polysynthetic language, it seems that\ngrn might be slightly different from the other polysynthetic languages spoken in Alaska (ess, esu, iku). Still,\ngrn is distinctive from spa and eng in that it still had a higher TTR and lower mean distance to the next unseen\nword. While the spa data seems more complex under the New Testament setting, eng and spa are consistently\nsimpler than polysynthetic languages in terms of TTR and mean distance to the next unseen word.\nIt is noted that, across languages, the datasets are similar in terms of sentence counts within each experimental\nsetting. While esu-eng and grn-spa differed slightly in terms of the exact sentence count, they are aligned at\nthe verse level. The rest of the data are not aligned at the verse level, but they seem to contain similar number\nof sentences under the respective data conditions. Note that we did not include the Hansard data for iku. We\nexclude the data because including it would increase the amount of available data and genre variability for the\nparticular language too much to allow comparison across languages.\nGiven the similar number of sentences present in each dataset, it is noteworthy that the word count and type\ncount are distinct across the languages. Again, ess, esu and iku seem similar to each other in that they have\na smaller number of words and a large number of types than others. This reﬂects their typological characteristic,\nthat they tend to have longer words with more morphemes, which may lead to more unique tokens. grn still\nseems distinct from the other polysynthetic languages in that the datasets in the language tend to have more words\nand less unique words. In fact, grn seems to have similarity with spa in terms of the descriptive statistics even\nthough grn still has a lower mean distance to the next unseen word than spa. eng seems to be clearly more\nanalytic than the other languages as it has more word counts and less type counts.\n5.2. TOKENIZATION STRATEGIES 27\nLanguage Sentences Words Types Type/Token Mean distance to unseen\ness 7,860 121,549 31,928 0.26 3.57\nesu 8,464 108,757 30,980 0.28 3.32\niku 7,858 110,977 36,573 0.33 3.03\ngrn 7,896 171,350 12,779 0.07 12.20\neng 7,870 210,395 5,067 0.02 38.82\nspa 7,896 206,707 11,371 0.06 16.01\nTable 5.3: Descriptive statistics for Setting 2 (New Testament only)\n5.2 Tokenization strategies\nWe considered ﬁve different tokenization strategies in modelling the languages: word, character, BPE, morfessor\nand FST segmentation methods. In what follows, we brieﬂy explain each tokenization strategy and why they\nmight be helpful in segmenting polysynthetic languages.\n5.2.1 Word\nA common tokenization strategy is to tokenize text by whitespace or by words. While it may be simple and seem\nintuitive, this tokenization strategy faces data sparsity and out-of-vocabulary (OOV) issues. For example, if we\ntokenize by words, dog and dogs will count as two separate tokens even though there is much shared information\nbetween the two. If the train set includes only the singular form and the test set contains only the plural form, the\nplural form in the test set will be considered as OOV .\n(3) aghnaaguq\naghnagh - ∼:(ng)u - ∼f(g/t)u- -q\nwoman -to.be -I NTR .IND -3SG\n‘she is a woman’ (Jacobson, 2001, p.25-26)\nThis tokenization method is particularly problematic for polysynthetic languages given their rich morphology.\nA word in polysynthetic languages may contain several morphemes to express a sentence-like intension. For ex-\nample, a word in ess, aghnaaguq, consists of four morphemes and is translated as ‘She is a woman’as shown\nin Example (3). Importantly, this results in a high rate of hapax legomena (words appearing only once), which\nresults in much higher OOV rates than observed in most non-polysynthetic languages. In modelling polysynthetic\nlanguages, the word-level tokenization is too unrealistic to be useful in predicting the next word, and its perfor-\nmance may be over-estimated or under-estimated depending on how we reward or penalize OOVs. For example,\nif we do not penalize a model for predicting an OOV symbol for the next word, it may predict an OOV symbol\nrepeatedly for a polysynthetic language to falsely record a good performance. If we do want to penalize OOV , we\nwill have to come up with a metric that does that fairly given our data. Given that the model we adapted did not\npenalize OOV , we opted to use language models that would not over-generate OOVs.\n5.2.2 Character\nOne possible solution to such issues of word-level tokenization is to tokenize text by the character. The character-\nlevel tokenization rarely has OOV issues because a text typically consists of a ﬁnite set of characters regardless\nof its morphological complexity. However, this tokenization method, again, cannot fully utilize the linguistic\ninformation present in a text as it reduces all words into a sequence of a ﬁnite set of characters. The relation-\nship between dog and dogs may be easily captured by a character-level model, but words with more complex\nmorphology like Example (3) may be hard to model using the character as the tokenization unit.\nWhile we report our results for character-level models as the baseline to compare other results to, we note\nthat character-level models may not be meaningful for downstream applications for polysynthetic languages such\n28 CHAPTER 5. LANGUAGE MODELLING\nas keyboard prediction: Predicting a character at a time when a word consists of several morphemes and a long\nsequence of characters may be too slow or too low-quality.\n5.2.3 BPE\nIf word-level tokenization is too coarse-grained and character-level tokenization is too ﬁne-grained, it may mean\nthat we need to utilize subword units to segment our data. As discussed in Section 4.3.1, byte pair encoding\n(BPE; Sennrich et al., 2016) is a unsupervised segmentation method that uses subword units. Originally a data\ncompression algorithm (Gage, 1994), BPE has become one of the standard techniques in neural machine transla-\ntion since Sennrich et al. (2016). Tokens segmented by BPE can represent texts with the minimum entropy by the\nﬁxed vocabulary size, which should be chosen as the hyperparameter. BPE segmentation may look like morpheme\nsegmentation for some words, but it is data-driven rather than based on linguistic information. For example, with\nenough support from a given data, BPE may segment ‘lower’ as ‘low@@ er’ (@@ represents a within-word mor-\npheme boundary), which may seem linguistically motivated, but it is also possible to get different segmentations\nsuch as ‘l@@ ow@@ er’ with different hyperparameters and different data conditions. Refer to Table 4.2 for ex-\namples of BPE segmentations for machine translation of iku, some of which respect morphological boundaries\nand some of which do not.\nWe trained a BPE model on the training data and applied the model to all data using subword-nmt1. We\nexperimented with different vocabulary sizes for BPE segmentation, and report results on two vocabulary sizes:\n500 and 5,000. While BPE provides an off-the-shelf method to segment words into subword units, it remains\nunclear whether the unsupervised method would prove useful in modelling polysynthetic languages.\n5.2.4 Morfessor\nWe adopted another unsupervised segmentation method called Morfessor to compare with BPE. Morfessor is a tool\nfor unsupervised (and semi-supervised) morphological segmentation and has been utilized in speech recognition,\nMT, and speech retrieval. While there is no literature on its efﬁciency in neural language modelling tasks for\npolysynthetic languages, it is said to be useful in modelling languages with rich morphology such as Finnish,\nEstonian, German and Turkish (Smit et al., 2014). Morfessor uses Maximum a Posteriori (MAP) estimation to\napproximate morpheme segmentation assuming that a word consists of one or more “morph”, yet its results may\nnot be the same as linguistically motivated morpheme segmentation. We used Morfessor 2.0 with the default\nsettings for Morfessor segmentation.\n5.2.5 FST segmentation\nThe last segmentation strategy we considered was segmentation based on FSTs. FST segmentation provides\nknowledge-based, rule-based segmentation based on linguistic knowledge and analysis. Several FST-based mor-\nphological analyzers or morphological segmenters have been developed for polysynthetic languages, and we were\nable to experiment with two of them for our experiments: ess (Chen and Schwartz, 2018) and grn (Kuznetsova\nand Tyers, 2019). The FST-based morphological analyzers produce zero or more morphological analyses for any\ngiven word. When there are more than one analysis available for a word, we used heuristics (e.g. choose the\nshortest analysis) to select one analysis to segment the given word. When there was no analysis available, we used\ncharacter (character backoff) or BPE (BPE backoff) segmentation for the word. The BPE backoff was performed\nusing the existing BPE segmentations with the vocabulary size of 500 and 5,000. While we were able to ob-\ntain this segmentation results only for two polysynthetic languages, this provides a point of comparison between\nsupervised, linguistically motivated segmentation and unsupervised, data-driven segmentation.\n5.3 RNN-LSTM\nWe used a state-of-the-art language model (Merity et al., 2017, 2018) for our language modelling experiments.\nThe RNN model with LSTM has shown to be competitive in modelling English benchmark datasets such as PTB\nand WikiText-2. We adapted the hyperparameters for WikiText-2 (WT2) with LSTM for Morfessor and FST\n1https://github.com/rsennrich/subword-nmt\n5.4. CHARACTER-LEVEL PERPLEXITY 29\nCharacter & BPE Morfessor & FST\nRNN Cell LSTM LSTM\nLayers 3 3\nRNN hidden size 1840 1150\nDropout (e/h/i/o) 0/0.1/0.1/0.4 0.1/0.2/0.65/0.4\nWeight drop 0.2 0.5\nWeight decay 1.2e-6 1.2e-6\nBPTT length 200 70\nBatch size 128 80\nInput embedding size 400 400\nLearning rate 1e-3 30\nEpochs 50 200\nRandom seed 1111 1882\nOptimizer Adam SGD\nLR reduction (lr/10) [25, 35] NA\nTable 5.4: Hyper-parameters for word- and character-level language modelling experiments\nLanguage Morfessor BPE (V=500) BPE (V=5k) Character\ness 2.53 2.64 3.34 2.51\nesu 2.72 2.82 2.84 2.64\niku 2.31 2.42 2.46 2.36\ngrn 2.93 3.07 3.49 3.03\neng 2.53 2.48 2.47 2.51\nspa 8.97 2.72 2.60 2.69\nTable 5.5: Character-level perplexity for setting 1 (all available data). V means the vocabulary size for BPE operation. Bold\nnumbers represent the best model for each language while underlined numbers show the best model for each tokenization.\nmodels and the hyperparameters for character level enwik8 for character and BPE models. Table 5.4 summarizes\nthe hyperparameters.\nWe acknowledge that none of these models (nor any other models to our knowledge) have been speciﬁcally\ndesigned to model polysynthetic languages or reported to be used to model polysynthetic languages. With a lack\nof a language model designed to model polysynthetic languages, we chose a state-of-the-art model that has proven\ncompetitive in modelling English instead.\n5.4 Character-level perplexity\nPerplexity is a measure of language modelling difﬁculty and calculated by taking the exponent of the average\nnegative log-likelihood per token. Because perplexity as it is depends on the tokenization strategy, we calculate\nthe character-level perplexity for each model to allow comparison among them. We deﬁne the character-level\nperplexity as the exponent of the average negative log-likelihood per character and calculate it by adding up the\ntoken-level loss for a given tokenization, multiplying the total loss by the number of tokens in the test set and\ndividing the value by the number of characters in the test set. We count whitespace and the end of a sentence\nsymbol as separate tokens. This ensures a fair comparison among different tokenization strategies. The choice\nof character as the common denominator is arbitrary, and it can be other tokenization methods such as the word.\nRefer to Mielke (2019) for detailed explanations.\n30 CHAPTER 5. LANGUAGE MODELLING\nLanguage Morfessor BPE (V=500) BPE (V=5k) Character\ness 2.77 3.14 3.23 2.64\nesu 2.98 3.74 3.61 2.89\niku 2.59 3.02 2.96 2.61\ngrn 3.16 3.44 3.41 2.97\neng 2.40 2.81 2.59 2.56\nspa 2.66 3.23 3.18 2.94\nTable 5.6: Character-level perplexity for setting 2 (New Testament only).\n5.5 Results & Discussion\nTables 5.5 and 5.6 summarize the language modelling experiment results excluding FST segmentation for the 1)\nall data setting and 2) New Testament only setting, respectively. It is suggested that the character and Morfessor\nmodels might work better than BPE models for polysynthetic languages. As for the 1)all data setting, tokenization\nby character resulted in the best performance in modelling ess and esu while Morfessor models performed the\nbest for iku and grn. BPE models with the vocabulary size of 5k worked the best with eng and spa. The same\ntrend was observed for the 2) New Testament setting for ess, esu and iku: character models performed the best\nfor ess and esu while Morfessor led to the lowest perplexity measure for iku. However, the character-level\nmodel resulted in the lowest character-level perplexity for grn while the Morfessor model was the best for eng\nand spa for the 2) New Testament setting. While it is unclear why a certain tokenization method worked better\nfor a language, it is speculated that BPE might not be well-suited in segmenting polysynthetic languages given\ntheir morphological richness. A word in a polysynthetic language might consists of several morphemes that are\nnot immediately retrievable based on the surface form. As shown in Example (3), a word in ess may contain\na root, a derivational sufﬁx and inﬂexional sufﬁxes, which may look different in the surface form depending on\nthe morphophonological rules that apply to the sufﬁxation. For example, the derivational sufﬁx ( -∼:(ng)u) in\nexample (3) has two morphophonological symbols ( ∼and :), the latter of which applies to delete the gh ending\nof the root (for details see Jacobson, 2001). Given such characteristics of polysynthetic languages, the fact that\ncharacter models worked the best for ess and esu might mean that those languages were hard to segment with\nunsupervised segmentation methods like Morfessor and BPE. Segmenting those languages might require getting\nat the underlying form with linguistically motivated segmentation rather than segmenting the surface form only.\nEven though Morfessor models worked the best for iku under both settings and for grn under the 1) all data\nsetting, the difference between the Morfessor models and character models are quite small.\nIt should be noted that the hyperparameters for Morfessor and BPE operations are not optimized. While the\nBPE models with the two hyperparameters (V=500 and V=5k) did not result in the best model for any of the\npolysynthetic languages, it is possible that different hyperparameters might result in better (or worse) perplexity\nmeasures. In a similar note, different datasets in a language might work differently with Morfessor tokenization:\nthe Morfessor segmentation was the best in modelling spa under the 2) New Testament only setting, but it was\nthe very worst under the 1) all data setting.\n5.5. RESULTS & DISCUSSION 31\nLanguage Setting Morfessor FST BPE Character\n- +BPE(V=500) +BPE(V=5k) +char V=500 V=5k\ness All 2.53 2.30 2.35 2.36 2.33 2.64 3.34 2.51\ness NT 2.77 2.25 2.41 5.38 2.42 3.14 3.23 2.64\ngrn All 2.93 2.74 2.70 2.70 2.69 3.07 3.49 3.03\ngrn NT 3.16 4.82 2.93 2.65 2.68 3.44 3.41 2.97\nTable 5.7: Character-level perplexity including FST segmentation\nLanguage Setting Morfessor FST BPE Character\n- +BPE(V=500) +BPE(V=5k) +char V=500 V=5k\ness All 1903.37 882.16 1037.10 1097.51 980.49 2718.52 18157.32 1790.03\ness NT 3986.77 739.92 1289.70 891605.81 1353.29 10993.00 13975.35 2689.33\ngrn All 287.71 203.99 187.76 189.23 185.96 372.84 725.33 348.70\ngrn NT 432.68 3988.57 288.63 171.56 181.21 673.22 644.65 313.79\nTable 5.8: Word-level perplexity including FST segmentation\n32 CHAPTER 5. LANGUAGE MODELLING\nLanguage Morfessor BPE (V=500) BPE (V=5k) Character\ness 1903.37 2718.52 18157.32 1790.03\nesu 2244.44 2969.89 3113.87 1783.40\niku 1469.02 2185.62 2503.41 1773.08\ngrn 287.71 372.84 725.33 348.70\neng 49.37 45.87 44.86 47.83\nspa 13051.98 75.10 62.01 71.97\nTable 5.9: Word-level perplexity for setting 1 (all available data). V denotes vocabulary size for BPE operation\nLanguage Morfessor BPE (V=500) BPE (V=5k) Character\ness 3986.77 10993.00 13975.35 2689.34\nesu 4562.23 26323.96 20053.28 3572.00\niku 3923.01 14970.43 12581.29 4231.44\ngrn 432.68 673.22 644.65 313.79\neng 39.62 77.30 55.01 52.03\nspa 67.93 158.86 148.47 106.16\nTable 5.10: Word-level perplexity for setting 2 (New Testament only). V denotes vocabulary size for BPE operation\nAs a way to utilize rich morphology in modelling polysynthetic languages, we trained FST-based models for\ness and grn. Table 5.7 summarizes the character-level perplexity values for all tokenization methods including\nFST segmentation only and FST segmentation with character or BPE backoff strategy for ess and grn. For all\nsettings, FST-based segmentation resulted in the best model for the two languages. The clear difference between\nFST-based models and non-FST-based models suggest that the Morfessor and BPE models failed to capture the\nmorphological information present in the data.\nThe fact that the FST segmentation only worked the best for ess might suggest that the FST segmentation\nfor the language might have been more robust than grn. Indeed, the FST segmentation only resulted in high\nperplexity in modelling grn under the 2) New Testament setting. With the BPE and character backoff, grn FST\nmodels still worked the best, but it is speculated that the FST morphological segmentation alone for grn might\nnot have been reliable or the coverage of the FST was not as good as the ess FST.\nAfter comparing different tokenization methods per language, we compared different languages to see which\nlanguage is easier or harder to model. This line of inquiry has been pursued by several recent studies (Cotterell\net al., 2018; Mielke et al., 2019; Gerz et al., 2018), where various languages are modeled using a state-of-the-art\nneural language model to compare relative difﬁculty of modelling a language with particular linguistic features.\nIt should be noted that our data per language were not parallel so the comparison has to be drawn with caution.\nHowever, we still attempted the comparison here as comparing our models may provide insights for future studies\ngiven that we used the same or very similar RNN language models as the previous literature and that polysynthetic\nlanguages have not been discussed in this line of inquiry. If we compared the character-level perplexity, Table 5.5\nand Table 5.6 show that iku was the easiest to model under the 1) all data setting and eng under the 2) New\nTestament setting. However, character-level perplexity may not be the right metric to use to compare different lan-\nguages. The problem with the character-level measure is that it does not tell us much about real-life applications,\nwhere the difﬁculty of predicting an entire word might be more meaningful. More importantly, the character-level\nperplexity underestimates the difﬁculty of modelling polysynthetic languages as they tend to have longer, morpho-\nlogically complex words. In fact, when we look at the word-level perplexity, the differences between polysynthetic\nlanguages and others become clearer. Table 5.9 and Table 5.10 show the word-level perplexity measures for the\ntwo experimental settings. When considering the difﬁculty of predicting the next word in the languages than the\nnext character, iku is no longer the easiest to model under any condition. The word-level measure clearly shows\nthat eng, followed by spa, was the easiest to model. Comparisons of the word-level perplexity values suggest\nthat ess, esu and iku are quite similarly hard to model while grn is less difﬁcult even though it is still quite\nharder to model than language like eng and spa. This observation agrees with our previous observation about\n5.5. RESULTS & DISCUSSION 33\nFigure 5.1: Model comparison in sentence-level negative log-likelihood for ess\nthe descriptive statistics of the datasets.\nOf course, it might be unrealistic to expect that a model for polysynthetic languages would result in a word-\nlevel perplexity comparable to that for eng given the linguistic difference. Polysynthetic languages tend to have\nlonger and diverse word forms because of their richer morphology. Therefore, they are likely to be harder to model\nthan other languages. However, comparing the character-level perplexity only may result in mistakenly arguing\nthat iku is easier to model than eng.\nWhile the relative performance of each tokenization method for a given language stays the same regardless, the\nchoice of the unit for the perplexity measure should be carefully made if we are to compare different languages.\nAs mentioned above, the datasets were not strictly parallel across the languages even under the 2) New Testament\nsetting. Parallel texts and different evaluation methods might facilitate comparison across languages. For example,\nMielke et al. (2019) uses the average surprisal (negative log-likelihood loss) per verse when comparing languages\nmodels using data fully aligned at the verse level and also suggests a statistical method to estimate the difﬁculty\ncoefﬁcient of a language given some missing verses. Aligning a parallel corpus of polysynthetic languages and\nothers at the verse or sentence level may lead to a more useful comparison in future research.\n34 CHAPTER 5. LANGUAGE MODELLING\nFigure 5.2: Model comparison in sentence-level negative log-likelihood for grn\n5.6. FUTURE DIRECTION 35\n5.6 Future Direction\nThe results clearly show that FST segmentation is helpful in modelling polysynthetic languages. While we had\nonly two languages to experiment with FST segmentation, FST segmentation with or without a backoff strategy re-\nsulted in the best model by a large margin. Figure 5.1 and Figure 5.2 visualize the relative performance of the FST\nmodel v. BPE or character models at the sentence level foress and grn, respectively. For both ﬁgures, points un-\nder the 45 degree line mean lower loss or better performance for the FST model than Morfessor, BPE or character\nmodel. For both languages, it is clear that the FST models resulted in lower loss (negative log-likelihood) per sen-\ntence overall as well as the entire text. This represents an opportunity to utilize an existing, linguistically-oriented\nsystem in aiding neural language modelling. While FSTs might not be as helpful in modelling high-resource\nlanguages with poor morphology, they will be essential in modelling low-resource polysynthetic languages.\nAnother line of inquiry we are currently pursuing is comparing polysynthetic languages with other languages\nin terms of language modelling difﬁculty. In order to compare different languages more precisely, we are using\naligned Bible datasets and comparing a perplexity measure per verse. By modelling 149 Bibles in 94 languages,\ncovering 24 language families, we aim to answer if polysynthetic languages are indeed harder to model than other\nlanguages and what kind of linguistic, typological features (if any) explain such difﬁculty.\n36 CHAPTER 5. LANGUAGE MODELLING\nChapter 6\nApplications & Future Work\n6.1 On-device Text Prediction\nOne of the goals of this workshop was to make progress in providing human language technologies that can\nactually be used by native speakers. As smartphones become ubiquitous in native communities, text entry is\nbecoming an increasingly important use case.\nIn particular, users should have access to text entry methods, namely custom keyboards, that allow them\nto enter text quickly and accurately. Currently, most of the languages we consider have no form of predictive\nkeyboard available.\nOur goal was to develop a pipeline for constructing custom predictive keyboards for polysynthetic languages.\nWe wanted the keyboards to allow both automatic completion of the current unit of text being typed by the user\n(where units could refer to morphemes or words) and prediction of the next unit when the user input reached a\nboundary. Both completion and prediction rely on language models to work, so the bulk of our efforts focused on\nadapting trained neural network language models for on-device use.\nUltimately, we successfully built functional prototype on-device keyboards for Guaraní (grn) and St. Lawrence\nIsland Yupik (ess). To our knowledge, these would be the ﬁrst open-source predictive keyboards available for\nthese languages on the Android platform.\n6.1.1 Open Source Stack\nWe chose to integrate our predictive LM models with the android branch of the open source Divvun toolkit 1.\nDivvun was chosen since it is actively developed, and the project has a stated goal of enabling text entry for low-\nresource languages. The toolkit provides base IME front end source code that handles on-device keyboard display\nand capturing of user input. We rewrote Divvun’s default back end to enable loading a trained neural LM that\ncould be used to make future predictions based on the text buffer content the user has already typed.\n6.1.2 User Interface Considerations\nPolysynthetic languages pose unique challenges for UI/UX design in the context of a predictive keyboard. A key\nquestion concerns the level of granularity at which predictions should be presented.\nExisting keyboards almost exclusively make predictions over whole words. For polysynthetic languages,\nword-level prediction is problematic. For reasons introduced in Chapters 1 and 2, it isn’t feasible to train an effec-\ntive language model over words in languages with extremely productive morphology. Most words are composed\non-the-ﬂy, and so would not have been seen during training. Furthermore, polysynthetic morphology permits\nextremely long words (e.g., “oñembohuguaipu’ã” in Guaraní). The small prediction strip present on device key-\nboards would not be able to comfortably accommodate so many characters in a single prediction.\n1https://github.com/divvun/giellakbd-android\n37\n38 CHAPTER 6. APPLICATIONS & FUTURE WORK\nFigure 6.1: Sample mobile keyboard interface.\nAs a compromise, we chose to use morphemes as the unit of prediction for our keyboard prototypes. As\nthe user types, the prediction bar presents them with either completions of the current morpheme they are in the\nmiddle of, or predictions for the next morpheme if the language model predicts they are at a morpheme boundary.\nThe use of morphemes as units of prediction implies that we have access to morphological analysis and seg-\nmentation tools that can generate morpheme-level training data for our language models. These tools may not be\navailable for all languages, in which case different subword units may need to be used. One option is do modelling\nand prediction over BPE word chunks. However, these would likely appear unnatural to most users, since BPE\nsegmentation is unsupervised and linguistically unaware, leading to segmentation that doesn’t correspond to any\nnatural boundaries. A better option would be to use syllables as units, since they can be extracted with a simple\nmodel that looks for consonant/vowel alternations, and do correspond to cognitively ‘natural’ linguistic units.\n6.1.3 Adapting Neural Language Models for Mobile Devices\nAs shown in Figure 6.1, we’d like to build an interface that uses the context typed into a buffer to present comple-\ntions and predictions to the keyboard user. To do this, we need to feed the context data into a language model.\nInitially, we attempted to use the SOTA PyTorch-based language models tested in Chapter 5 directly on-\ndevice. However, this proved to be technically prohibitive. First, device resources are limited, and keyboards\nshould be lightweight — they only account for text entry and shouldn’t have a signiﬁcant impact on other running\napplications. We set a goal of keeping keeping our model size on the order of 10Mb. Second, there is little\nbuilt-in support in Android for loading and running PyTorch models. In contrast, Google provides the TensorFlow\nLite(TFLite) framework for loading models trained via TensorFlow and converted for on-device use.\nWe attempted to convert our PyTorch models to TensorFlow using the ONNX, toolkit 2 but found that the\nautomatic converter did not support many of the operations used. Ultimately, we settled on training custom models\nfor keyboard operation building on TensorFlow sample code. 3 We trained our models using the full desktop\n2https://onnx.ai\n3https://www.tensorflow.org/tutorials/sequences/recurrent\n6.1. ON-DEVICE TEXT PREDICTION 39\nversion of TensorFlow, and successfully exported the portion of the resulting computation graph responsible for\ninference to TFLite.\nFor both Guaraní and Yupik, language models were trained on text from the Bible, that had been processed\nvia the FSTs described in Chapters 3 and 7 to include morpheme boundaries. The data was split as described in\nChapter 5 for consistency with the language modelling experiments described there. The training data covered all\navailable Bible verses except the gospel of Luke (which was reserved as development data), and John (which was\nreserved as test data). The models were built at the character level, but with morpheme boundaries (@) marked\ndirectly on predicted symbols, as shown in Figure 6.2. This modiﬁcation enabled the model to guess when a\nmorpheme boundary was reached (i.e., a symbol with @ was predicted/typed).4\nFigure 6.2: Language model training for keyboard.\nThe model consisted of the following architecture. A single LSTM with 2 layers, and 200 hidden units per\nlayer, read a 30-character context. The ﬁnal hidden state of the LSTM was passed through a dense layer fol-\nlowed by a softmax to assign probabilities to each possible next symbol. The LSTM was trained with dropout\n(keep_prob=0.75) between layers, with dropout disabled during inference. Batches of 20 contexts were used for\ntraining. Optimization was done via Adam, with initial learning rate 1.0 and learning rate decay 0.5.\nWhen the model was loaded on the device, our custom Divvun back end sent the last 30 chars of the input\nbuffer the user had typed through the model, and used the greedy algorithm below to generate continuations and\npredictions to display to the user in the keyboard’s prediction bar.\nAlgorithm 2: Greedy continuation/prediction generation\nResult: N prediction candidates\npredictions = list;\n/* Get the LM’s ranked predictions for the next char */\nnextFromLM = LM.predict(context[-X:];\n/* Loop over top N continuation points */\nfor c in top N from nextFromLMdo\n/* Greedy unroll to fill out prediction candidate */\nprediction = c;\ntmp = (context + c)[-X:];\nwhile boundary symbol (@,_) not yet reached do\nnextFromLM = LM.predict(tmp);\nc = top 1 char from nextFromLM;\nprediction += c;\ntmp = (tmp + c)[-X:];\nend\npredictions.append(prediction)\nend\nCurrently, prediction stops when the model predicts a morpheme or word boundary. This stopping condition\ncan be altered as needed to, for example, avoid stopping if the current prediction is too small (e.g., a single char-\nacter) or continue predicting until the total log probability of the predicted string drops below a given threshold.\nPredictions can also be reached by a different, less greedy search algorithm, such as a depth ﬁrst search start-\ning at the current context. However, this has a high chance of producing many candidates with the same preﬁx.\nThe method used here was chosen for its simplicity, and because it ensures candidates are diverse (no two can-\n4Note that morpheme boundaries never appear in the user’s input buffer according to this scheme. This is different from a system based\nentirely on words, as the relevant boundaries, spaces and punctuation symbols, are visible.\n40 CHAPTER 6. APPLICATIONS & FUTURE WORK\ndidates can share the same initial character). User testing might be able to determine if this bias towards diverse\npredictions is desirable.\n6.1.4 Future Development\nIn Chapter 5, we evaluate our underlying language model quality via perplexity measures. Unfortunately, we\ndid not have access to native speakers during the workshop and so could not perform direct user testing with our\nprototype keyboards.\nOur ultimate goal would be to push our development back to the main Divvun project, so that it can receive\nongoing support, and make it into the hands of native speakers. However, there are a number of evaluation\nmeasures that approximate the user experience related to prediction quality. Top-nprediction recall measures how\noften the correct prediction would have been shown to the user in the keyboard’s prediction strip (assuming the\nuser was typing a ﬁxed script). Similarly, we can measure how many keystrokes a user can save by selecting a\nprediction (1 touch) versus typing it out (# touches corresponding to characters in the prediction unit).\nOur prototype keyboards lack certain features that are standard on more mature offerings for languages like\nEnglish. First, we assume the users touch exactly the keys they intended, and that they don’t make spelling\nmistakes. The reality of using a touch device is that input is noisy and prone to error, with touches often sensed\nonly in the vicinity of the intended key. A noisy channel model applied to the sequence of touch points received\nby the keyboard can be used to auto-correct these mistakes.\nSecond, our keyboard’s predictions are at the mercy of the data used to train our language models. Without\na whitelist of acceptable units, or a blacklist of units that shouldn’t be predicted, there is nothing preventing the\nmodel from generating offensive language. Similarly, predictions can be signiﬁcantly biased towards the style of\nthe training data. In our case, the our LMs are noticeably ‘evangelical,’ being trained almost exclusively on text\nfrom the Bible.\n6.2 Speech Recognition\nWithin this section, we discuss two experiments with automatic speech recognition on polysynthetic languages:\npreliminary experiments with Crow (cro) word prediction and experiments with Guaraní speech recognition. First,\nwe describe previous work on speech recognition for polysynthetic languages as well as some of the inherent\ndifﬁculties that arise when constructing speech corpora. Then, we discuss our baseline approach to end to end\nneural speech recognition using the Deepspeech model (Hannun et al., 2014), the preliminary results obtained and\na discussion of future directions for polysynthetic speech recognition.\n6.2.1 Related work\nSpeech recognition for polysynthetic languages is a relatively new area of research. Much of this is due to the\nnecessity of large transcribed speech corpora.\nKlavans et al. (2018b) presents an overview of the challenges facing automatic speech recognition for polysyn-\nthetic languages. They note that there is a dearth of resources for polysynthetic languages, particularly transcribed\nspeech corpora. These corpora require large volumes of data from skilled native language speakers. The size of the\ncorpora required and the linguistic, technological and language speciﬁc knowledge required make this an difﬁcult\ntask for communities to accomplish on their own. Hasegawa-Johnson et al. (2017b) states that “transcribing even\none hour of speech may be beyond the reach of communities that lack large-scale government funding” (as cited\nin Klavans et al. (2018b)).\nFor Seneca, Jimerson et al. (2018) investigated the application of different ASR models to a small spoken\ncorpus of Seneca (consisting of approximately 155 minutes of recordings). They found that GMM ASR models\nfrom the Kaldi ASR toolkit Povey et al. (2011) yielded better results than neural approaches on this small dataset\nsize – requiring transfer learning from pretrained English ASR models and various augmentation procedures on\nboth the text data and audio data to even approach GMM performance.\nFor Guaraní, a relatively large speech corpus has been constructed as part of the IARPA Babel project. 5 This\n5Though as noted in Gales et al. (2017), the BABEL corpora are small in comparison to other corpora used in end to end neural ASR.\nHannun et al. (2014), for example, used 5,000 hours of data.\n6.2. SPEECH RECOGNITION 41\nLearning rate WER CER\n10−3 97.07 87.11\n10−4 98.97 85.76\nTable 6.1: Crow speech recognition\ndataset enabled the development of several existing speech recognition systems. Hartmann et al. (2016) experi-\nmented with GMM and DNN models on several of the BABEL languages including Guaraní, ﬁnding overall better\nperformance for DNN models. Their main contribution was innovative data augmentation techniques. They sam-\npled noise from sections of the BABEL dataset without speech data.6 This noise was then injected into the regular\ntranscribed data at a signal to noise ratio between 0 and 20 db. An additional data augmentation method employed\nby Hartmann et al. (2016) involved speed peturbation. Previous research Ko et al. (2015), found that sampling\nthe audio signal at different rates was an effective data augmentation technique. Using these two methods in\ncombination, Hartmann et al. (2016) see a reduction in word error rate from 46.7 to 45.2.\nGales et al. (2017) also worked with Guaraní. They use an end to end neural approach, as we do, but they\nleverage stimulated network training. Stimulated network training aims to train networks where nodes with similar\nactivation properties are grouped together Gales et al. (2017). Their paper also discusses a number of optimization\nmethods for keyword search in speech data. They obtain a WER of 49.5 for their Guaraní ASR system using\nstimulated network training.\n6.2.2 Methodology\nDeepspeech\nHannun et al. (2014) introduces the end to end neural speech recognition system used for the following experi-\nments. This system takes in short time fourier transform (STFT) features (referred to as ‘spectrogram‘ features in\nthe original work). These features go through three convolutional layers with ReLU activation, and then a single\nbidirectional RNN. Lastly, a softmax layer is used to give a probability distribution over the possible characters in\nthe dataset.\nWe borrow from this original implementation with some modiﬁcations: instead of a simple recurrent layer,\nwe utilize gated-recurrent units, and instead of a single hidden recurrrent layer, we utilize a number of different\nrecurrent layers. Hannun et al. (2014) use a non-gated recurrent ﬁnal layer as they were seeking to avoid computing\nand storing the update, input and output gates used in Long-Short-Term-Memory (LSTM) recurrent units. As a\ncompromise between LSTMs and non-gated RNNs, we utilize Gated Recurrent Units (GRUs). Gated Recurrent\nUnits have an update gate but no output gate, thus saving some computation in comparison to an LSTM but\nalso allowing the neural network to be less susceptible to exploding/vanishing gradients. We also introduce more\nrecurrent layers after the convolutional layers with signiﬁcant increases in performance at the cost of increased\nruntime.\n6.2.3 Decoding\nLanguage models can help improve automatic speech recognition systems by imposing constraints on the possible\ncharacter co-occurances. We present results for greedy decoding, where no language model is utilized and the\nnetwork’s predicted character sequence is not explicitly constrained. In the future, we will incorporate language\nmodels into the speech recognition system.\n6.2.4 Preliminary results\nInitial results for Crow word recognition and Guaraní speech recognition are shown in the following sections.\n6These sections are denoted as <no-speech> in the transcription ﬁles\n42 CHAPTER 6. APPLICATIONS & FUTURE WORK\nNumber of GRU layers WER CER\n1 layer 92.98 52.75\n2 layer 87.85 47.90\n3 layer 86.00 46.96\n4 layer 82.08 44.40\n5 layer 82.00 44.50\nTable 6.2: Guaraní results using greedy decoding\nNumber of GRU layers WER CER\n1 layer 92.36 51.89\n2 layer 86.44 47.18\n3 layer 83.74 45.49\n4 layer 82.73 44.46\n5 layer 81.80 44.45\nTable 6.3: Guaraní results using greedy decoding and data augmentation\n6.2.5 Crow\nAs noted in 3.1.5, the data available for Crow consists only of recordings of single words and small phrases. In\naddition, very little monolingual text data for Crow was available. Due to the lack of long phrases, as with the\nGuaraní data, and the lack of large monolingual language resources, only a single recurrent layer was used in our\nmodel, similar to the original DeepSpeech implementation. In addition, the language model created from a very\nsmall collection of Crow monolingual stories was given very little weight due to the low coverage of the model.\nInitial experiments at word prediction proved unsuccessful. The neural net simply produced all spaces for output.\nA pretrained English model trained on the Librispeech corpus was leveraged in an attempt to get any output at\nall from the Crow data. This pretrained model was then adapted to the available Crow data. The results from this\nadapted speech recognition model are shown in Table 6.1. While the results produced are very poor, the network\nwas at least producing some output at this point.\n6.2.6 Guaraní\nFor Guaraní, a number of different recurrent layers were used. Character and word error rates for the development\ndataset from the IARPA corpus using greedy decoding are shown in Table 6.2. Both the development and training\ndataset used only utterances between 1 and 15 seconds in length, thus the results shown are not directly comparable\nto Hartmann et al. (2016). Future experiments will be conducted on all the data for more direct comparison. All\nmodels were trained for 50 epochs with a starting learning rate of 10−4 and learning rate annealing each epoch.\n6.2.7 Future directions\nMoving forward, we will incorporate neural language models into the speech recognition systems. Currently,\nthe results displayed utilize simple greedy predictors with no explicit language modelling or conventional n-gram\nbased language models (Heaﬁeld, 2011) for decoding. Gales et al. (2017) use an RNN language model with\nPashto speech recognition and found that it had a minor effect on speech recognition but helped signiﬁcantly with\nkeyword search. However, their approach seems to involve a neural language model during the decoding stage.\nIncorporating a neural language model into the architecture using adversarial networks could enable still lower\nerror rates as the model\nChapter 7\nFeature-rich Open-vocabulary\nInterpretable Language Model\nIn this chapter, we present a novel general-purpose neural language modelling framework designed to be capable\nof handling a broad variety of typologically diverse languages, including languages whose morphology includes\nany or all of the following: preﬁxes, sufﬁxes, inﬁxes, circumﬁxes, templatic morphemes, derivational morphemes,\ninﬂectional morphemes, and clitics. In this chapter we motivate our language modelling framework using ex-\namples drawn primarily from St. Lawrence Island Yupik. St. Lawrence Island Yupik is a polysynthetic sufﬁxing\nlanguage in which words with 1 root, 0–3 derivational morphemes, and 1 inﬂectional are common, and words with\nup to 7 derivational morphemes have been attested (de Reuse, 1994).\n(4) Qikmighhaak neghtuk\nqikmigh -ghhagh -k negh -tuk\ndog -small -A BS.DU to.eat -I NTR .IND.3D U\n‘The two small dogs eat’\nIn Example (4) we observe a sample two-word sentence from St. Lawrence Island Yupik. The ﬁrst wordqikmi-\nghhaak is a noun composed of a noun root qikmigh, a derivational sufﬁx -ghhagh that serves as a diminutive, and\nan inﬂectional sufﬁx -k that indicates the noun’s case (absolutive) and number (dual). The second wordneghtuk is\na verb composed of a verb root negh and an inﬂectional sufﬁx -tuk that indicates the verb’s mood (indicative) and\nvalence (intransitive), as well as the person (3rd person) and number (dual) of the verb’s subject. Note that it is\ncommon for the form in which a morpheme surfaces in a word to differ from the underlying lexical form of that\nmorpheme. In the morphemes’ respective surface forms in this example, the ﬁnal uvular fricative of qikmigh and\n-ghhagh are each dropped, the vowel of -ghhagh is lengthened, and the ﬁnal uvular fricative of negh devoices to\nmatch the adjacent voiceless stop at the beginning of -tuk.\n(5) Mangteghaghrugllangllaghyunghitunga\nmangteghagh- -ghrugllag- -ngllagh- -yug- -nghite- -tu- -nga\nhouse- -big- -build- -want.to- -to.not- - INTR .IND - -1 SG\n‘I didn’t want to make a huge house’ (Jacobson, 2001, pg. 43)\nIn Example (5), a single Yupik word represents an entire sentence. The word consists of a noun rootmangteghagh,\na derivational sufﬁx ghrugllag that serves as an augmentative, a verbalizing derivational sufﬁx ngllagh, a verb-\nelaborating derivational sufﬁx yug, another verb-elaborating derivational sufﬁx nghite, and inﬂectional sufﬁxes\ntu and nga that mark mood (indicative) and valence (intransitive), as well as the person (1st person) and number\n(singular) of the verb’s subject.\n43\n44 CHAPTER 7. FEATURE-RICH OPEN-VOCABULARY INTERPRETABLE LM\n7.1 Language Model Desiderata\nA language model capable of effectively modelling the full linguistic diversity found in human languages, includ-\ning St. Lawrence Island Yupik and similar endangered and polysynthetic languages, should have the following\ndesiderata.\n7.1.1 Flexibility with respect to language typology\nTypical methods of categorizing languages by morphological type include isolating, fusional, agglutinative and\npolysynthetic. There are also morphological afﬁx types such as preﬁxes, sufﬁxes, circumﬁxes, inﬁxes and tem-\nplatic morphology, and processes such as compounding and incorporation.\nOne can think of isolating languages as those (almost) without productive morphology, such as Chinese and\nVietnamese. These languages are well served by existing approaches to language modelling which treat the word\nas the fundamental unit.\nFusional languages are those where a morpheme may represent multiple morphological or syntactic features.\nMost well-known Indo-European languages are of this type. They may also have complicated, irregular, or lex-\nicalised phonological processes occurring when morphemes are joined together. Consider for example Catalan\ntener ‘to have’— tinc ‘I have’— tinga ‘I have’. The stem is ten-, -er is the formant of the inﬁnitive, -c is the for-\nmant of the ﬁrst person singular present indicative and -nga is the formant of the ﬁrst and third person present\nsubjective. A vowel change in the stem occurs when the sufﬁxes are attached to the stem. This example has two\nfusional features: multiple features per morpheme and stem-internal phonological changes caused by afﬁxing.\nThese languages are fairly well dealt with in existing approaches, the number of forms that can be generated by\nthese processes may be larger than in isolating languages, but is essentially a ﬁnite-set.\nAs mentioned, current ad hoc methods work fairly well with isolating and fusional languages, where there are\na ﬁnite number of forms for a single word. Out of vocabulary items are a problem, but are typically related to\nunseen new stems rather than forms of seen stems. Agglutinating and polysynthetic languages have this problem\ntoo, but in addition they have the problem of unseen forms of previously seen stems.\nIn agglutinating languages — and in polysynthetic languages to an even greater extent — words are typically\nmade up of many morphemes concatenated together. These are typically with preﬁxes or sufﬁxes, or a combina-\ntion. The Yupik example in (4) is an example of sufﬁxing, and indeed Yupik is an exclusively sufﬁxing language.\nGuaraní combines sufﬁxes, which are primarily for tense, aspect, and mood (TAM) markers and subordination,\nwith preﬁxes for valency changing and agreement. This is illustrated in Example (6) where theai- preﬁx indicates\nﬁrst-person singular agreement, and the -se sufﬁx indicates volitional mood, and in Example (7) where the ña-\npreﬁx indicates agreement and the -va sufﬁx indicates nominalisation.\n(6) Aikosénte\nAi-ko-se-nte\nSG1-live-VOL-JUST\n‘I would just like to live’\n(7) ñaha’arõ’ ˜yetéva\nña-ha’arõ-’˜y-ete-va\nPL1-wait-NEG -INTS -REL\n‘that we did not expect at all’\nThe negative form of Guaraní verbs is formed by a circumﬁx of two morphemes,nd- and -i. These circumﬁxes\ngo around verbal derivations, agreement and (TAM) markers etc, as in (7.1.1).\n(8) ndojuhumo’ãi\nnd-o-juhu-mo’ã-i\nNEG -3-ﬁnd-FUT -NEG\nIn Chukchi the comitative case is made up of a circumﬁx of two morphemes, /γa/- and -/ma/. The noun /ławt/\n7.1. LANGUAGE MODEL DESIDERATA 45\n‘head’ forms the associative singular /γaławt˘ama/ by combining these and adding an epenthetic schwa.\nInﬁxes are morphemes that break a given stem and appear inside it. For example in Seri, a language spoken in\nthe north-west of Mexico. It uses inﬁxation after the ﬁrst vowel in the root to create forms with number agreement.\nFor example, ic ‘to plant’,i{tí}c i ‘did she plant it?’ vs.i{tí}{tóo}c ‘did they plant it?’.\nIn languages with templatic morphology, the root is typically represented as a consonant template, e.g. in\nMaltese, k-t-b ‘book’. Inﬂection takes place by “ﬁlling” the slots in the root with other templates, such that e.g.\nktieb ‘book’ (singular),kotba ‘books’, are formed by combining the root with the vowel templates {ø-ie, o-ø}, and\nin the plural the sufﬁx -a.\nAn ideal language model would be able to encode all of these types of morphology in a generic and composi-\ntional manner without using language- or typology-speciﬁc tricks or assumptions (e.g. productive morphological\nprocesses are exclusively sufﬁxing).1\nIt should allow for arbitrary subsets of characters in a given string to form meaningful, compositional units.\n7.1.2 Ability to incorporate external knowledge sources as features\nIn high-resource settings, neural networks commonly function as effective feature extractors (Goodfellow et al.,\n2016). In very low-resource settings such as St. Lawrence Island Yupik, extreme data sparsity means that neural\nmodels are likely to have insufﬁcient data to effectively extract such reliable features. To alleviate this issue, our\nlanguage model should be capable of incorporating a rich array of features from supplementary knowledge sources\nwhen insufﬁcient data conditions prevent learning them.\nFinite-state morphological analyzers (Beesley and Karttunen, 2003) in particular represent a mature technol-\nogy capable of serving as a reliable source of rich linguistic features. In the Yupik Example (4) above, we make\nuse of the ﬁnite-state morphological analyzer of Chen and Schwartz (2018). At a minimum, we expect such an\nanalyzer to decompose a Yupik word, providing morpheme boundary information and the associated constituent\nmorphemes. We expect that in most cases a morphological analyzer should also provide the underlying ortho-\ngraphic form of each root morpheme and each derivational morpheme, the set of linguistic features such as noun\ncase, verb mood, person, and number associated with each inﬂectional morpheme, and the underlying type of\neach morpheme (such as noun, verb, nominalizing sufﬁx, etc). In the some cases, an analyzer might also provide\ninformation regarding the phonemes in each morpheme.\n7.1.3 Open vocabulary\nIn high-resource languages, especially those that are analytic rather than synthetic, a common approach is to\ntreat morphologically-distinct variants (such as dog and dogs) as completely independent word types, rather than\ninﬂected variants of a common root. In polysynthetic languages in general, and in Yupik in particular, encountering\npreviously unseen word forms is pervasive and should be considered the norm rather than the exception. In very\nlow-resource settings, it is especially important that our language model be able to robustly handle and predict\nout-of-vocabulary tokens. Language models with a closed vocabulary are not viable in such settings. Instead, we\nrequire an open vocabulary language model in which the probability of a token given a history can be robustly\ncalculated even when that token was not present in the training data.\n7.1.4 Interpretability of predicted units\nBy deﬁnition, a language model provides a probabilistic model over a sequence of linguistic units. In other words,\na language model must be able to provide a probability distribution over the identity of the current linguistic unit\ngiven a history representing the preceding linguistic units in the sequence. We use the term linguistic unit to refer\nto an instance of any well-deﬁned linguistic level of analysis, such as a word, a morpheme, a syllable, a phoneme,\nor even a grapheme.\nIn our language model, we require that the computational mechanism that implements the linguistic unit be\ninterpretable. For example, consider the case of a trained instance of our language model randomly generating\na sequence of morphemes; when the model generates a morpheme, we should be able to recover whatever rich\n1We would note that treating words as basic units can also be considered to be a language-speciﬁc trick designed for isolating and fusional\nlanguages.\n46 CHAPTER 7. FEATURE-RICH OPEN-VOCABULARY INTERPRETABLE LM\nfeatures may be encoded therein (see §7.1.2), such as the underlying grapheme or phoneme sequence and the type\nof morpheme (root, derivational, inﬂectional, etc). This should be the case regardless of whether the generated\nunit was present in the training data or not (see §7.1.3).\n7.2 Sub-word language models\nThe rich morphology and phonology of Yupik and typologically similar languages results in an extreme type-token\nratio. This fact coupled with a very small corpus size make the use ofn-gram language models and recurrent neu-\nral language models over words highly unlikely to be effective. Schwartz et al. (2019) examined the number of\npotential word forms word forms in St. Lawrence Island Yupik, and estimated approximately1.27×1023 morpho-\ntactically licensed word forms. This number is approximately equal to current estimates of the number of stars in\nthe observable universe.2 While this estimate does not take into account restrictions imposed by semantic felicity,\nthe polysynthetic nature of the language ensures an extremely high fraction of hapax legomenon in Yupik texts,\nwith Schwartz et al. (2020) reporting that approximately every other Yupik word token establishes a previously\nunseen word type. In contrast to the astronomical number of potential Yupik word forms, the complete collection\nof fully digitized St. Lawrence Island Yupik texts available at the time of the 2019 JSALT workshop consisted\nof a corpus of slightly over 81,000 word tokens (see Chapter 3 for more details). In lieu of word-based language\nmodels, we consider language models that utilize sub-word units.\nLanguage models serve as an enabling technology for other downstream language technologies, including\nmobile text prediction. These technologies are mature and widespread for many high-resource languages, but\nrelatively immature and rare for polysynthetic languages. In this section, we present several motivating use cases\nof sub-word language models for polysynthetic language.\n7.2.1 Prediction of next morpheme\nThe core operation of a language model is estimating the conditional probability of a predicted next linguistic unit\ngiven a history of previous linguistic units. Figure 7.1 illustrates a recurrent neural network language model that\npredicts the most likely next morpheme given a history of four immediately preceding morphemes, where each\nmorpheme is encoded as a vector.\nFigure 7.1: A recurrent neural network language model over morphemes can be used to predict the next morpheme in a\nsequence. In this ﬁgure, the light green boxes represent Yupik morphemes from Example (4), each encoded as a vector.\n7.2.2 Prediction of next character\nA closely related task applicable in the context of mobile text completion is the prediction of the next character\ngiven a preceding sequence of characters. In the polysynthetic language setting, it may be beneﬁcial to augment\nsuch a model with a history of morphemes in situations where this information is available.\n7.3 Neural morphological analysis\nAs discussed in §2.1, ﬁnite-state morphological analyzers provide a mechanism for encoding linguistic knowledge\nin a ﬁnite-state transducer capable of analyzing a word and providing morpheme boundaries and other linguisti-\ncally salient information about the underlying morphemes that comprise the word. Recent work has explored how\na ﬁnite-state morphological analyzer can be used to bootstrap a neural morphological analyzer (Micher, 2018b;\nSchwartz et al., 2019; Silfverberg and Tyers, 2019). Building on that work, we propose a neural morphological\n2https://www.skyandtelescope.com/astronomy-resources/how-many-stars-are-there\n7.4. TENSOR PRODUCT REPRESENTATION 47\nFigure 7.2: In a text completion setting, a more sophisticated recurrent neural network language model could predict the next\ncharacter given a history of preceding characters and a history of preceding morphemes from Example (4). In this ﬁgure, the\nlight green boxes represent Yupik morphemes while the light purple boxes represent characters.\nanalyzer that directly predicts morpheme vectors, rather than predicting a sequence of strings representing an\nanalyzed form.\nFigure 7.3: In a morphological analysis setting, a sequence-to-sequence model predicts a sequence of morphemes from an\ninput sequence of Yupik characters from Example (4). In this ﬁgure, the light green boxes represent predicted Yupik morpheme\nvectors.\n7.4 Tensor Product Representation\nTo satisfy the language model desiderata speciﬁed in §7.1, we consider the Tensor Product Representation (TPR)\nproposed by Smolensky (1990). The use of TPRs provides a principled way of representing hierarchical symbolic\ninformation in vector spaces, such as those used as the input and output domains of neural networks. Developing\na tensor-product-based representational scheme begins by decomposing a symbolic structure into roles and ﬁllers.\nA symbolic structure can then be represented as the bindings of ﬁllers to roles. Once decomposed, both roles and\nﬁllers are embedded into a vector space such that all roles are linearly independent from one another. Let bbe a\nlist of ordered pairs (i,j) representing ﬁller i(with embedding vector ˆfi) being bound to role j (with embedding\nvector ˆrj). The tensor product representation T of the information is then given by\nT =\n∑\n(i,j)∈b\nˆfi ⊗ˆrj ∈Rd ⊗Rn. (7.1)\nThis TPR may itself be used as a ﬁller and subsequently be bound to another role vector. This process results in a\nTPR that represents hierarchical compositional structure.\n7.4.1 Unbinding\nTPRs are useful because they embed arbitrary symbolic structure in a vector space in such a way that simple\nlinear algebra operations may be used to retrieve the form of the symbolic structure, including its compositional\nstructure. The core operation in retrieving this structure is called unbinding. We may use unbinding to query\na role for its ﬁller. Unbinding may be accomplished by any of several exact or approximate strategies. Exact\nunbinding requires linear independence of the roles; however, recent (unpublished) work points to the accuracy\nof approximate unbinding even in densely packed TPRs. In this work, we use self-addressing unbinding, as it is\nquick to compute and proved sufﬁciently accurate for our purposes. Self-addressing unbinding retrieves the ﬁller\n˜fi for the role ˆri by simply computing the inner product between the role vector and the TPR:\n˜fi = T ·ˆri (7.2)\n48 CHAPTER 7. FEATURE-RICH OPEN-VOCABULARY INTERPRETABLE LM\nFigure 7.4: This sample word from Chukchi is composed of a root morpheme ławt˘ aand a circumﬁx γa. . . ma. The individual\ncharacters positions in the word comprise rolesr1 through r9, while the characters at those respective positions comprise ﬁllers\nf1 through f9. Roles rm1 and rm2 represent morpheme positions within the word, and are respectively ﬁlled byfm1 (denoting\nthe identity of the circumﬁx morpheme marking associative case) and fm2 (denoting the identity of the root morpheme).\nThis unbinding is exact if the role vectors are orthogonal to one another. Otherwise, the intrusion of the ﬁller of\nrole j, ˆfj, into the unbound ﬁller of the role i, ˜fi, is given by\nIn our case, since we have a ﬁxed ﬁller vocabulary, we were able to snap our unbindings to the ﬁller with the\nhighest cosine similarity to the unbound vector with sufﬁcient accuracy to render this intrusion irrelevant. Other\nunbinding strategies involve computing an inverse or pseudoinverse of a matrix of role vectors to perform a change\nof basis and decrease the intrusion.\n7.5 Morpheme vector representations from TPRs\nWe use TPRs (§7.4) to bridge the gap between the rich hierarchical symbolic information encoded in ﬁnite state\nmorphological transducers (such as Chen and Schwartz, 2018) and the morpheme vectors needed by the neural\nmodels described in §7.2 and §7.3.\n7.5.1 Morpheme TPRs\nGiven a language, a corpus of text in that language, and a ﬁnite-state morphological analyzer for that language,\nwe can use the ﬁnite-state analyzer to obtain a morphological analysis for each word in the corpus. For each\nmorpheme provided in an analysis, we extract a collection bof linguistically salient feature-value ordered pairs\n(i,j). Each linguistic feature j serves as a TPR role; each value iserves as a TPR ﬁller. For each such feature\nj (such as noun case), we deﬁne ˆrj to be a role vector representing that feature; for each value i(such as A BS)\nassociated with feature j, we deﬁne ˆfi to be a ﬁller vector representing that value. This use of TPRs enables us\nto jointly encode latent structural information provided by a ﬁnite state transducer with surface information in a\nprincipled manner. This process is depicted in Figure 7.5.\n7.5.2 Learning morpheme vectors using an autoencoder\nThe morpheme tensors constructed in §7.5.1 are potentially very high dimensional. Depending on how much\nlinguistic information is encoded in each tensor, the morpheme tensors may consist of approximately 103 to 109\nﬂoating point values per tensor. Tensors of this size are far too large to be directly usable as morpheme representa-\ntions in the neural models described in §7.2 and §7.3. To learn lower dimensional morpheme vectors, we make use\nof an autoencoder. The autoencoder is trained using the dictionary of previously constructed morpheme tensors.\nThe trained autoencoder can be used to encode a low-dimensional morpheme vector from a high-dimensional\nmorpheme tensor by running the morpheme tensor through the ﬁrst half of the autoencoder, and can be used to\nobtain a high-dimensional morpheme tensor from a morpheme vector by running the morpheme vector though the\nlatter half of the autoencoder.\n7.6. UNBINDING LOSS 49\nFigure 7.5: (a) Each word in a corpus is processed by a morphological analyzer. (b) A tensor product representation of each\nmorpheme is calculated, resulting in one tensor per morpheme. (c) The morpheme tensors extracted from the corpus are stored\nin a dictionary.\nFigure 7.6: An autoencoder trained is on the dictionary of morpheme tensors.\n7.6 Unbinding loss\nIn order to effectively train the autoencoder in §7.5.2, gold standard morpheme tensors must be compared against\npredicted morpheme tensors outputted by the autoencoder. However, the morpheme tensors are very high dimen-\nsional. In initial experiments, we used mean squared error as a loss function, but we found this was unable to\nconverge for auto-encoding sparse TPRs.\nTo enable effective training of the autoencoder, we therefore deﬁne a novel loss function that makes use of\nthe information encoded in the TPR. We deﬁne a loss function called unbinding loss that examines the unbinding\nproperties of a predicted morpheme tensor to answer the question, “What ﬁller is closest to the unbinding of each\nrole in the TPR?” For simplicity, we assume the use of self-addressing unbinding in this section (which we also\nused in the work presented here), but the computations are analogous with other unbinding strategies, relying only\non a ﬁxed role and ﬁller vocabulary and a ﬁxed number of bindings. We call the output TPR T.\nGiven a predicted tensor, the ﬁrst step to computing the unbinding loss is recursively unbind roles until the\nleaves of the structure are reached – that is, unbind each role until the result of unbinding is a single vector (rather\nthan a higher-dimensional tensor). When this point is reached, we compute the cosine similarity between the result\nof unbinding and all the ﬁllers in the vocabulary. For example, assume a depth-3 structure is encoded in a TPR,\nwhere the ﬁllers are character embeddings, the second level is left-to-right positional roles, and the highest level\n50 CHAPTER 7. FEATURE-RICH OPEN-VOCABULARY INTERPRETABLE LM\nis morpheme identity. If we want to see what is bound to the ﬁrst position of the English cat morpheme in T, we\nwould ﬁrst unbind from T as follows (assuming self-addressing unbinding):\nfcat,1 = T ·ˆrcat ·ˆr1 (7.3)\nWe then get the vector of similarities ˆscat,1 between this ﬁller and the each of character embedding vectors in\nthe vocabulary matrix V as follows:\nˆscat,1 = fcat,1 ·V\n||fcat,1||ViVi (7.4)\nwhere ViVi denotes the column-wise vector norm of the vocabulary matrix (using Einstein summation nota-\ntion).\nThis similarity vector can be used to deﬁne a probability distribution over possible ﬁllers through the use\nof a softmax. We take the logarithm of the result of this computation to obtain log-probabilities. We call this\ndistribution P.\nP = log\n( eˆscat,1\n∑eˆscat,1\n)\n(7.5)\nWe then treat each ﬁller vocabulary word (in this case, each character) as a class, and compute the negative log-\nlikelihood loss over this probability distribution. The resulting loss for the ﬁrst character of cat being c is then\nloss(ˆscat,1,c) =−ˆscat,1,c + log(\n∑\nj\neˆscat,1,j ). (7.6)\nIn this example, we focus on the loss for a single ﬁller; however, as we consider tree-structured representations,\nthe number of ﬁllers needing to be checked is exponential with the depth of our representation. In practice, we\nwere able to overcome this difﬁculty by parallelizing the independent matrix computations for the loss of all the\nposition roles for a given morpheme, trading space for time. For more complex TPRs, a potential avenue would be\nto exploit the fact that most roles will be empty (and their unbindings thus a matrix of zeros) by replacing the loss\ncomputations for unbound roles with mean squared error (which need only push that part of the representation to\n0).\nChapter 8\nConclusions\nIn motivating this JSALT workshop on neural polysynthetic language modelling, we observed the following major\nassumptions (usually unstated) that are pervasive in most computational linguistics and natural language process-\ning research:\n• If a technique works well on English, the technique is likely to be “language agnostic” and is likely to work\nwell on a large variety of other languages. Various other high-resource languages such as Spanish, French,\nGerman, or Chinese are sometimes used in place of English.\n• For any given word stem, there will be a relatively small number of morphological variants of that stem.\n• Most or all of the morphological variants of any given word stem will appear in a sufﬁciently large corpus\nto enable learning of robust statistics.\nOur work was built around explicitly challenging all of these assumptions, using a variety of polysynthetic\nlanguages and a variety of natural language tasks. The polysynthetic languages that we chose to work with\npresent numerous signiﬁcant challenges. These languages are typologically very different from English and other\nwidely-used high-resource languages. There is pervasive use of derivational and inﬂectional morphology. For\nmost word stems, there are very large numbers of potential morphological variants, very few of which occur in\nany given corpus. For all of the selected languages (with the exception of Inuktitut), the corpus sizes are very\nsmall (less than 60,000 sentences).\n8.1 Contribution 1: Resources\nOne contributing factor to the dearth of prior work on computational research on endangered polysynthetic lan-\nguages is the lack of easily available corpus resources. Nearly all endangered languages are very low resource.\nMost CL and NLP researchers do not have the personal connections with members of endangered language com-\nmunities that are often critical for obtaining data for use in research. In preparation for this workshop, our team\ngathered together text and speech data from various sources for a variety of polysynthetic languages. In cases\nwhere we have connections with indigenous community stakeholders and rights-holders, we have begun the pro-\ncess of discussions regarding community desires and possibilities for data distribution. For data that we have\nobtained permission to distribute, we have initiated a process of public data hosting.\n8.2 Contribution 2: Machine Translation\nThe main contributions of our machine translation work during this workshop are as follows. With ﬁrst access\nto the beta version 3.0 of the Nunavut Hansard (Joanis et al., 2020), we were able to provide feedback and best\npractices for preprocessing the dataset and shared knowledge about existing character and spelling variations in\nthe dataset. This work contributed to the data release and publication of Joanis et al. (2020); that data is now being\nused in the Fifth Conference on Machine Translation (WMT20) Inuktitut-English news translation shared task.\n51\n52 CHAPTER 8. CONCLUSIONS\nOur work at the time constituted state-of-the-art performance on translation between Inuktitut and English. It has\nsince been surpassed by Joanis et al. (2020), and we anticipate future improvements through the WMT20 shared\ntask.\nWe collected empirical evidence on several well-known but unresolved challenges, such as best practices in\ntoken segmentation for MT into and out of polysynthetic languages, as well as an examination of how to evalu-\nate MT into polysynthetic languages. We successfully used multilingual neural machine translation methods to\nimprove translation quality into low-resource languages (St. Lawrence Island Yupik and Central Alaskan Yup’ik)\nusing data from related languages (Inuktitut). Notably, our “low-resource” languages were lower resource than\nmuch of the literature, and we produced improvements without the use of large monolingual corpora (which are\nunavailable for these languages and many other languages of interest). We observed these improvements across\nboth n-gram-oriented and semantic-oriented metrics.\nThere remain a number of open challenges in this space. We encourage caution in interpreting the automatic\nquality metrics, as we do not yet have human judgments of translation quality for the languages examined; human\njudgements from the WMT20 shared task may prove particularly valuable. Our initial results, using fairly con-\nventional methods, for both multilingual and bilingual machine translation show promise, but we expect that there\nremains much room for improvement.\n8.3 Contribution 3: Language Models\nTo our best knowledge, this paper represents the ﬁrst attempt at modeling polysynthetic languages using a state-\nof-the-art RNN model and comparing their language modeling difﬁculty with that of other languages. We conduct\nlanguage modeling experiments on four low-resource, polysynthetic languages (St. Lawrence Island Yupik, Cen-\ntral Alaskan Yup’ik, Inuktitut, Guaraní) and two high-resource, morphologically poor languages (English, Span-\nish), using four different segmentation methods: character, BPE, Morfessor and FST. By comparing the perplexity\nmeasure at the character level, we show that the FST segmentation method worked the best for polysynthetic\nlanguages when it was available. While the Morfessor segmentation method might improve language modeling\nperformance for some polysynthetic languages, all the other segmentation method we considered—character, BPE\nand Morfessor—failed to capture the rich morphology of polysynthetic languages better than the FST segmenta-\ntion that is based on linguistic knowledge of the languages. We also compared the perplexity measure at the word\nlevel to illustrate how signiﬁcantly difﬁcult it is to model polysynthetic languages.\nAll in all, this presents an exciting starting point for a line of inquiries into modeling polysynthetic languages\nand utilizing the linguistic knowledge realized in FST in modeling such languages that are morphological rich and\nlow resource. At the same time, we invite future research into linguistic characteristics that contribute to language\nmodeling difﬁculty as we continue to investigate the effect of morphological complexity in our ongoing study.\n8.4 Contribution 4: Mobile & Speech Applications\nAs smartphones become ubiquitous in native communities, facilitating native-language communication through\nbetter technology will become an important aspect of language conservation and revitalization efforts. Building\non freely available open source tools, we developed a pipeline for training neural language models that can run on-\ndevice, and loading them as a predictive back-end for on-device keyboards. This effort lead to working keyboard\nprototypes for Guaraní ( grn) and St. Lawrence Island Yupik ( grn) — the ﬁrst ever input methods for these\nlanguage varieties to include intelligent next-unit prediction and completion. Building the prototypes highlighted\nthe unique requirements posed by polysynthetic languages. Their complex, productive morphology results in very\nlong words, many of which would never appear in the training data available for language modeling, and which\nwould be unwieldy to show to keyboard users as prediction candidates. We dealt with these problems by training\ncharacter-level models that were aware of morpheme boundaries, and using morphemes rather than words as units\nof prediction.\nThe low-resource nature of most polysynthetic languages is particularly poignant for automatic speech recog-\nnition. While transfer learning can help alleviate some of the issues with data poverty, neural approaches to ASR\nare still not sufﬁcient to enable usable systems.\n8.5. CONTRIBUTION 5: MODEL DEVELOPMENT 53\n8.5 Contribution 5: Model Development\nIn this workshop we proposed a novel framework for language modelling that combines knowledge representa-\ntions from ﬁnite-state morphological analyzers with Tensor Product Representations (Smolensky, 1990) in order\nto enable successful neural language models capable of handling the full linguistic variety of typologically variant\nlanguages. To support this framework, we also deﬁned and implemented a novel loss function called unbind-\ning loss that enables gold standard morpheme tensors to be compared against predicted morpheme tensors. We\nimplemented a prototype TPR framework that we are continuing development of as part of ongoing future work.\n54 CHAPTER 8. CONCLUSIONS\nBibliography\nVasilisa Andriyanets and Francis Tyers. A prototype ﬁnite-state morphological analyser for Chukchi. InProceed-\nings of the Workshop on Computational Modeling of Polysynthetic Languages , pages 31–40, Santa Fe, New\nMexico, USA, August 2018. Association for Computational Linguistics. URL https://www.aclweb.\norg/anthology/W18-4804.\nAnders Apassingok, (Iyaaka), Willis Walunga, (Kepelgu), and Edward Tennant, (Tengutkalek), editors. Sivuqam\nNangaghnegha — Siivanllemta Ungipaqellghat / Lore of St. Lawrence Island — Echoes of our Eskimo Elders ,\nvolume 1: Gambell. Bering Strait School District, Unalakleet, Alaska, 1985. URL http://www.uaf.edu/\nanla/collections/search/resultDetail.xml?id=SY980AWT1985.\nAnders Apassingok, (Iyaaka), Willis Walunga, (Kepelgu), and Edward Tennant, (Tengutkalek), editors. Sivuqam\nNangaghnegha — Siivanllemta Ungipaqellghat / Lore of St. Lawrence Island — Echoes of our Eskimo Elders ,\nvolume 2: Savoonga. Bering Strait School District, Unalakleet, Alaska, 1987. URL http://www.uaf.\nedu/anla/collections/search/resultDetail.xml?id=SY980AWT1985.\nAnders Apassingok, (Iyaaka), Willis Walunga, (Kepelgu), and Edward Tennant, (Tengutkalek), editors. Sivuqam\nNangaghnegha — Siivanllemta Ungipaqellghat / Lore of St. Lawrence Island — Echoes of our Eskimo Elders ,\nvolume 3: Southwest Cape. Bering Strait School District, Unalakleet, Alaska, 1989. URL http://www.\nuaf.edu/anla/collections/search/resultDetail.xml?id=SY980AWT1985.\nAnders Apassingok, (Iyaaka), Jessie Uglowook, (Ayuqliq), Lorena Koonooka, (Inyiyngaawen), and Edward Ten-\nnant, (Tengutkalek), editors. Kallagneghet / Drumbeats . Bering Strait School District, Unalakleet, Alaska,\n1993. URL http://www.uaf.edu/anla/item.xml?id=SY990AUKT1993.\nAnders Apassingok, (Iyaaka), Jessie Uglowook, (Ayuqliq), Lorena Koonooka, (Inyiyngaawen), and Edward Ten-\nnant, (Tengutkalek), editors. Akiingqwaghneghet / Echoes. Bering Strait School District, Unalakleet, Alaska,\n1994. URL http://www.uaf.edu/anla/item.xml?id=SY990AUKT1994.\nAnders Apassingok, (Iyaaka), Jessie Uglowook, (Ayuqliq), Lorena Koonooka, (Inyiyngaawen), and Edward Ten-\nnant, (Tengutkalek), editors. Suluwet / Whisperings. Bering Strait School District, Unalakleet, Alaska, 1995.\nURL http://www.uaf.edu/anla/item.xml?id=SY900AUKT1995.\nLinda Womkon Badten, Vera Oovi Kaneshiro, Marie Oovi, and Christopher Koonooka. St. Lawrence Island /\nSiberian Yupik Eskimo Dictionary. Alaska Native Language Center, University of Alaska Fairbanks, 2008.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align\nand translate. ICLR, 2015. URL https://arxiv.org/pdf/1409.0473v6.pdf.\nKenneth R. Beesley and Lauri Karttunen. Finite State Morphology. CSLI Studies in Computational Linguistics.\nCSLI Publications, Stanford, California, 2003.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. InProceedings of\nNeural Information Processing Systems, pages 932–938, 2000. URL http://papers.nips.cc/paper/\n1839-a-neural-probabilistic-language-model.pdf .\n55\n56 BIBLIOGRAPHY\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language\nmodel. Journal of Machine Learning Research, 3(Feb):1137–1155, 2003. URL http://www.jmlr.org/\npapers/volume3/bengio03a/bengio03a.pdf.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp\nKoehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and\nMarco Turchi. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth\nWorkshop on Statistical Machine Translation, pages 1–46, Lisbon, Portugal, September 2015. Association for\nComputational Linguistics. URL http://aclweb.org/anthology/W15-3001.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji-\nmeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves,\nMartin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and\nMarcos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Confer-\nence on Machine Translation, pages 131–198, Berlin, Germany, August 2016. Association for Computational\nLinguistics. URL http://www.aclweb.org/anthology/W/W16/W16-2301.\nEmily Chen and Lane Schwartz. A morphological analyzer for St. Lawrence Island / Central Siberian Yupik.\nIn Proceedings of the 11th Language Resources and Evaluation Conference (LREC 2018) , Miyazaki, Japan,\nMay 2018. European Languages Resources Association (ELRA). URL https://www.aclweb.org/\nanthology/L18-1416.pdf.\nKenneth W. Church and William A. Gale. A comparison of the enhanced Good-Turing and deleted estimation\nmethods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19–54, 1991.\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. Are all languages equally hard to language-\nmodel? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 536–541, New Or-\nleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2085. URL\nhttps://www.aclweb.org/anthology/N18-2085.\nWillem J. de Reuse.Siberian Yupik Eskimo — The Language and Its Contacts with Chukchi. Studies in Indigenous\nLanguages of the Americas. University of Utah Press, Salt Lake City, Utah, 1994.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguis-\ntics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\nBenoît Farley. Nunavut Hansard Inuktitut–English Parallel Corpus version 2.0.\nhttp://www.inuktitutcomputing.ca/NunavutHansard/info.php, 2008.\nBenoît Farley. The Uqailaut Project, 2009. URL http://www.inuktitutcomputing.ca/Uqailaut/\ninfo.php.\nPhilip Gage. A new algorithm for data compression. C Users J., 12(2):23–38, February 1994. ISSN 0898-9788.\nURL http://dl.acm.org/citation.cfm?id=177910.177914.\nMark J. F. Gales, Kate M. Knill, and Anton Ragni. Low-resource speech recognition and keyword-spotting. In\nAlexey Karpov, Rodmonga Potapova, and Iosif Mporas, editors, Speech and Computer , pages 3–19, Cham,\n2017. Springer International Publishing. ISBN 978-3-319-66429-3.\nMichael Gasser. Mainumby: un ayudante para la traducción Castellano-Guaraní. In Tercer Seminario Interna-\ntional sobre Traducción, Terminología y Lenguas Minorizadas, 2018.\nBIBLIOGRAPHY 57\nDaniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Roi Reichart, and Anna Korhonen. On the relation between\nlinguistic typology and (limitations of) multilingual language modeling. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Processing , pages 316–327, Brussels, Belgium, October-\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1029. URL https:\n//www.aclweb.org/anthology/D18-1029.\nIrving J. Good. The population frequencies of species and the estimation of population parameters. Biometrika,\n40(3–4):237–264, 1953.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press, 2016. http://www.\ndeeplearningbook.org.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. Universal neural machine translation for extremely low\nresource languages. In Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 344–354, New\nOrleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1032. URL\nhttps://www.aclweb.org/anthology/N18-1032.\nThanh-Le Ha, Jan Niehues, and Alex Waibel. Toward multilingual neural machine translation with universal\nencoder and decoder. 13th International Workshop on Spoken Language Translation (IWSLT 2016) , 2016.\nURL http://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_5.pdf.\nAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev\nSatheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition.\narXiv preprint arXiv:1412.5567, 2014.\nWilliam Hartmann, Tim Ng, Roger Hsiao, Stavros Tsakalidis, and Richard M Schwartz. Two-stage data augmen-\ntation for low-resourced speech recognition. In Interspeech, pages 2378–2382, 2016.\nMark Hasegawa-Johnson, Mohamed Elmahdy, and Eiman Mustafawi. Arabic speech and language technology. In\nElabbas Benmamoun and Reem Bassiouney, editors,Routledge Handbook of Arabic Linguistics, page 299–311.\nTaylor and Francis Group Ltd., Oxford, 2017a.\nMark A Hasegawa-Johnson, Preethi Jyothi, Daniel McCloy, Majid Mirbagheri, Giovanni M di Liberto, Amit\nDas, Bradley Ekin, Chunxi Liu, Vimal Manohar, Hao Tang, et al. Asr for under-resourced languages from\nprobabilistic transcription. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 25\n(1):50–63, 2017b.\nKenneth Heaﬁeld. Kenlm: Faster and smaller language model queries. In Proceedings of the Sixth Workshop\non Statistical Machine Translation, WMT ’11, pages 187–197, Stroudsburg, PA, USA, 2011. Association for\nComputational Linguistics. ISBN 978-1-937284-12-1. URL http://dl.acm.org/citation.cfm?\nid=2132960.2132986.\nFelix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, and Matt Post.\nSockeye: A Toolkit for Neural Machine Translation. arXiv preprint arXiv:1712.05690, December 2017. URL\nhttps://arxiv.org/abs/1712.05690.\nPetr Homola. A machine translation toolchain for polysynthetic languages. In Proceedings of the 16th EAMT\nConference, 2012. URL http://www.mt-archive.info/EAMT-2012-Homola.pdf .\nSteven A Jacobson. Yup’ik Eskimo Dictionary.Alaska Native Language Center, 1984.\nSteven A. Jacobson. A Practical Grammar of the St. Lawrence Island / Siberian Yupik Eskimo Language, Prelim-\ninary Edition. Alaska Native Language Center, Fairbanks, Alaska, 2nd edition, 2001.\nFrederick Jelineck and Robert L. Mercer. Interpolated estimation of Markov source parameters from sparse data.\nIn Proceedings of the Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands, May 1980.\n58 BIBLIOGRAPHY\nRobbie Jimerson, Kruthika Simha, Raymond Ptucha, and Emily Prudhommeaux. Improving ASR Output for\nEndangered Language Documentation. In Proc. The 6th Intl. Workshop on Spoken Language Technologies for\nUnder-Resourced Languages, pages 187–191, 2018. doi: 10.21437/SLTU.2018-39. URL http://dx.doi.\norg/10.21437/SLTU.2018-39.\nEric Joanis, Rebecca Knowles, Roland Kuhn, Samuel Larkin, Patrick Littell, Chi kiu Lo, Darlene Stewart, and Jef-\nfrey Micher. The Nunavut Hansard Inuktitut–English parallel corpus 3.0 with preliminary machine translation\nresults. In Proceedings of LREC-2020, Marseille, France, May 2020.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda\nViégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s multilingual neural\nmachine translation system: Enabling zero-shot translation. Transactions of the Association for Computational\nLinguistics, 5:339–351, 2017. doi: 10.1162/tacl\\_a\\_00065. URL https://doi.org/10.1162/tacl_\na_00065.\nLauri Karttunen. Finite-state constraints. In John Goldsmith, editor, The Last Phonological Rule: Reﬂections on\nconstraints and derivations. University of Chicago Press, 1993.\nSlava M. Katz. Estimation of probabilities from sparse data for the language model component of a speech\nrecognizer. IEEE Tranactions on Acoustics, Speech and Signal Processing, 35(3):400–401, March 1987.\nKimmo Kettunen. Can type-token ratio be used to show morphological complexity of languages? Journal of\nQuantitative Linguistics, 21(3):223–245, 2014. doi: 10.1080/09296174.2014.911506. URL https://doi.\norg/10.1080/09296174.2014.911506.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http:\n//arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference paper at the\n3rd International Conference for Learning Representations, San Diego, 2015.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dhar-\nshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the\nNational Academy of Sciences, 114(13):3521–3526, 2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114.\nURL https://www.pnas.org/content/114/13/3521.\nJudith Klavans, John Morgan, Stephen LaRocca, Jeffrey Micher, and Clare V oss. Challenges in speech recognition\nand translation of high-value low-density polysynthetic languages. InProceedings of the 13th Conference of the\nAssociation for Machine Translation in the Americas (Volume 2: User Papers) , pages 283–293, Boston, MA,\nMarch 2018a. Association for Machine Translation in the Americas. URL https://www.aclweb.org/\nanthology/W18-1921.\nJudith L Klavans, John Morgan, Stephen LaRocca, Jeffrey Micher, and Clare V oss. Challenges in speech recogni-\ntion and translation of high-value low-density polysynthetic languages. In Proceedings of the 13th Conference\nof the Association for Machine Translation in the Americas (Volume 2: User Papers), pages 283–293, 2018b.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: Open-source toolkit\nfor neural machine translation. In Proceedings of ACL 2017, System Demonstrations , pages 67–72, Vancou-\nver, Canada, July 2017. Association for Computational Linguistics. URL https://www.aclweb.org/\nanthology/P17-4012.\nReinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In Proceedings of the\nIEEE International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 181–184, 1995.\nTom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. Audio augmentation for speech recognition.\nIn Sixteenth Annual Conference of the International Speech Communication Association, 2015.\nBIBLIOGRAPHY 59\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke\nCowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan\nHerbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meet-\ning of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster\nSessions, pages 177–180, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL\nhttps://www.aclweb.org/anthology/P07-2045.\nKimmo Koskenniemi. Two-level Morphology: A General Computational Model for Word-Form Recognition\nand Production. PhD thesis, University of Helsinki, 1983. URL http://www.ling.helsinki.fi/\nkoskenni/doc/Two-LevelMorphology.pdf.\nAnastasia Kuznetsova and Francis M. Tyers. A ﬁnite-state morphological analyser for paraguayan guaraní. In\nsubmission, 2019.\nJindˇrich Libovický and Jind ˇrich Helcl. Attention strategies for multi-source sequence-to-sequence learning. In\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), pages 196–202, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.\n18653/v1/P17-2031. URL https://www.aclweb.org/anthology/P17-2031.\nPatrick Littell, Chi-kiu Lo, Samuel Larkin, and Darlene Stewart. Multi-source transformer for Kazakh-Russian-\nEnglish neural machine translation. In Proceedings of the Fourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1) , pages 267–274, Florence, Italy, August 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/W19-5326. URL https://www.aclweb.org/anthology/W19-5326.\nChi-kiu Lo. YiSi - a uniﬁed semantic MT quality evaluation and estimation metric for languages with different\nlevels of available resources. In Proceedings of the Fourth Conference on Machine Translation (Volume 2:\nShared Task Papers, Day 1) , pages 507–513, Florence, Italy, August 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/W19-5358. URL https://www.aclweb.org/anthology/W19-5358.\nQingsong Ma, Ondˇrej Bojar, and Yvette Graham. Results of the wmt18 metrics shared task: Both characters and\nembeddings achieve good performance. In Proceedings of the Third Conference on Machine Translation, Vol-\nume 2: Shared Task Papers, pages 682–701, Belgium, Brussels, October 2018. Association for Computational\nLinguistics. URL http://www.aclweb.org/anthology/W18-6451.\nQingsong Ma, Johnny Wei, Ondˇrej Bojar, and Yvette Graham. Results of the wmt19 metrics shared task: Segment-\nlevel and strong mt systems pose big challenges. In Proceedings of the Fourth Conference on Machine Trans-\nlation (Volume 2: Shared Task Papers, Day 1) , pages 62–90, Florence, Italy, August 2019. Association for\nComputational Linguistics. URL http://www.aclweb.org/anthology/W19-5302.\nManuel Mager, Ximena Gutierrez-Vasques, Gerardo Sierra, and Ivan Meza-Ruiz. Challenges of language tech-\nnologies for the indigenous languages of the Americas. In Proceedings of the 27th International Conference on\nComputational Linguistics, pages 55–69, Santa Fe, New Mexico, USA, August 2018a. Association for Compu-\ntational Linguistics. URL https://www.aclweb.org/anthology/C18-1006.\nManuel Mager, Elisabeth Mager, Alfonso Medina-Urrea, Ivan Vladimir Meza Ruiz, and Katharina Kann. Lost\nin translation: Analysis of information loss during machine translation between polysynthetic and fusional\nlanguages. In Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages, pages 73–\n83, Santa Fe, New Mexico, USA, August 2018b. Association for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/W18-4808.\nJoel Martin, Howard Johnson, Benoit Farley, and Anna Maclachlan. Aligning and using an English-Inuktitut\nparallel corpus. In Proceedings of the HLT-NAACL 2003 Workshop on Building and using parallel texts: Data\ndriven machine translation and beyond, Volume 3, pages 115–118. Association for Computational Linguistics,\n2003.\nJoel Martin, Rada Mihalcea, and Ted Pedersen. Word alignment for languages with scarce resources. In Proceed-\nings of the ACL Workshop on Building and Using Parallel Texts, pages 65–74. Association for Computational\nLinguistics, 2005.\n60 BIBLIOGRAPHY\nThomas Mayer and Michael Cysouw. Creating a massively parallel Bible corpus. In Proceedings of the Ninth\nInternational Conference on Language Resources and Evaluation (LREC’14) , pages 3158–3163, Reykjavik,\nIceland, May 2014. European Language Resources Association (ELRA). URL http://www.lrec-conf.\norg/proceedings/lrec2014/pdf/220_Paper.pdf.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM Language Mod-\nels. arXiv preprint arXiv:1708.02182, 2017.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. An Analysis of Neural Language Modeling at Multiple\nScales. arXiv preprint arXiv:1803.08240, 2018.\nJeffrey Micher. Improving coverage of an Inuktitut morphological analyzer using a segmental recurrent neural\nnetwork. In Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered\nLanguages, pages 101–106. Association for Computational Linguistics, 2017. URLhttp://aclweb.org/\nanthology/W17-0114.\nJeffrey Micher. Provenance and processing of an Inuktitut-English parallel corpus part 1: Inuktitut data preparation\nand factored data format. Technical Report ARL-TN-0923, US Army Research Laboratory, October 2018a.\nURL https://www.arl.army.mil/arlreports/2018/technical-report.cfm?id=6182.\nJeffrey Micher. Using the Nunavut Hansard data for experiments in morphological analysis and machine trans-\nlation. In Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages , pages\n65–72, Santa Fe, New Mexico, USA, August 2018b. Association for Computational Linguistics. URL\nhttps://www.aclweb.org/anthology/W18-4807.pdf.\nSabrina J. Mielke. Can you compare perplexity across different segmentations?, 2019. URL https://\nsjmielke.com/comparing-perplexities.htm.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner. What kind of language is\nhard to language-model? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 4975–4989, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1491. URL https://www.aclweb.org/anthology/P19-1491.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of\nwords and phrases and their compositionality. In Proceedings of the 26th International Conference on Neu-\nral Information Processing Systems , NIPS’13, pages 3111–3119, USA, 2013. Curran Associates Inc. URL\nhttp://dl.acm.org/citation.cfm?id=2999792.2999959.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan ˇCernocký, and Sanjeev Khudanpur. Recurrent neu-\nral network based language model. In Proceedings of the 11th Annual Conference of the International\nSpeech Communication Association (INTERSPEECH) , pages 1045–1048, Makuhari, Chiba, Japan, Septem-\nber 2010. URL https://www.isca-speech.org/archive/archive_papers/interspeech_\n2010/i10_1045.pdf.\nMehryar Mohri. Language processing with weighted transducers. In Proceedings of the 8th annual conference\nTraitement Automatique des Langues Naturelles (TALN 2001), 2001.\nChristian Monson, Ariadna Font Llitjós, Roberto Aranovich, Lori Levin, Ralf Brown, Eric Peterson, Jaime Car-\nbonell, and Alon Lavie. Building nlp systems for two resource-scarce indigenous languages: Mapudungun and\nquechua. Strategies for developing machine translation for minority languages; 5th SALTMIL Workshop on\nMinority Languages, page 15, 2006.\nKayo Nagai. Mrs. Della Waghiyi’s St. Lawrence Island Yupik Texts with Grammatical Analysis. Number A2-006\nin Endangered Languages of the Paciﬁc Rim. Nakanishi Printing, Kyoto, Japan, 2001.\nGraham Neubig and Junjie Hu. Rapid adaptation of neural machine translation to new languages. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing , pages 875–880, Brussels,\nBelgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1103.\nURL https://www.aclweb.org/anthology/D18-1103.\nBIBLIOGRAPHY 61\nHermann Ney, Ute Essen, and Reinhard Kneser. On structuring probabilistic dependencies in stochastic language\nmodeling. Computer Speech and Language, 8:1–38, 1994.\nYuta Nishimura, Katsuhito Sudoh, Graham Neubig, and Satoshi Nakamura. Multi-source neural machine trans-\nlation with data augmentation. CoRR, abs/1810.06826, 2018. URL http://arxiv.org/abs/1810.\n06826.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Haji ˇc, Chris Manning, Ryan Mc-\nDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Dan Zeman. Universal Dependencies\nv1: A Multilingual Treebank Collection. In Proceedings of Language Resources and Evaluation Conference\n(LREC’16), 2016.\nKemal Oﬂazer and Ilknur Durgar El-Kahlout. Exploring different representational units in English-to-Turkish\nstatistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation ,\npages 25–32, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/W/W07/W07-0204.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of ma-\nchine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,\npages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi:\n10.3115/1073083.1073135. URL https://www.aclweb.org/anthology/P02-1040.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers), pages 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/N18-1202. URL https://www.aclweb.org/anthology/N18-1202.\nMaja Popovi´c. chrF: character n-gram f-score for automatic MT evaluation. InProceedings of the Tenth Workshop\non Statistical Machine Translation, pages 392–395, Lisbon, Portugal, September 2015. Association for Com-\nputational Linguistics. doi: 10.18653/v1/W15-3049. URL https://www.aclweb.org/anthology/\nW15-3049.\nMatt Post. A call for clarity in reporting BLEU scores. InProceedings of the Third Conference on Machine Trans-\nlation: Research Papers , pages 186–191, Belgium, Brussels, October 2018. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/W18-6319.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukáš Burget, Ondˇrej Glembek, Nagendra Goel, Mirko Hanne-\nmann, Petr Mótliˇcek, Yanmin Qian, Petr Schwarz, Jan Silovský, Georg Stemmer, and Karel Veselý. The Kaldi\nspeech recognition toolkit. In Proceedings of the IEEE 2011 Workshop on Automatic Speech Recognition and\nUnderstanding. IEEE Signal Processing Society, 2011.\nMat¯ıss Rikters, M¯arcis Pinnis, and Rihards Krišlauks. Training and adapting multilingual NMT for less-resourced\nand morphologically rich languages. In Proceedings of the Eleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association\n(ELRA). URL https://www.aclweb.org/anthology/L18-1595.\nAlexander Rudnick. Cross-Lingual Word Sense Disambiguation for Low-Resource Hybrid Machine Translation.\nPhD thesis, Indiana University, 12 2018. URL https://scholarworks.iu.edu/dspace/handle/\n2022/22672.\nLane Schwartz, Emily Chen, Benjamin Hunt, and Sylvia L.R. Schreiner. Bootstrapping a neural morphological\nanalyzer for St. Lawrence Island Yupik from a ﬁnite-state transducer. In Proceedings of the 3rd Workshop on\nthe Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers) , pages 87–96,\nHonolulu, February 2019. Association for Computational Linguistics. URL https://www.aclweb.org/\nanthology/W19-6012.\n62 BIBLIOGRAPHY\nLane Schwartz, Sylvia Schreiner, and Emily Chen. Community-focused language documentation in support of\nlanguage education and revitalization for St. Lawrence Island Yupik. Études Inuit Studies, 2020. In press.\nRico Sennrich and Biao Zhang. Revisiting low-resource neural machine translation: A case study. InProceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics , pages 211–221, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1021. URL https://www.\naclweb.org/anthology/P19-1021.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 1715–1725, Berlin, Germany, August 2016. Association for Computational Linguistics.\ndoi: 10.18653/v1/P16-1162. URL https://www.aclweb.org/anthology/P16-1162.\nClaude Shannon. A mathematical theory of communication.Bell System Technical Journal, 27:379–423,623–656,\n1948.\nClaude Shannon. Prediction and entropy of printed English. Bell System Technical Journal, 30:50–64, 1951. URL\nhttp://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf.\nMiikka Silfverberg and Francis Tyers. Data-driven morphological analysis for Uralic languages. In Proceedings\nof the Fifth International Workshop on Computational Linguistics for Uralic Languages, pages 1–14, 2019.\nGrace Slwooko. Sivuqam Ungipaghaatangi I. University of Alaska, Anchorage, AK, 1977.\nGrace Slwooko. Sivuqam Ungipaghaatangi II. University of Alaska, Anchorage, AK, 1979.\nPeter Smit, Sami Virpioja, Stig-Arne Grönroos, and Mikko Kurimo. Morfessor 2.0: Toolkit for statistical morpho-\nlogical segmentation. In Proceedings of the Demonstrations at the 14th Conference of the European Chapter\nof the Association for Computational Linguistics , pages 21–24, Gothenburg, Sweden, 2014. Association for\nComputational Linguistics. doi: 10.3115/v1/E14-2006.\nPaul Smolensky. Tensor product variable binding and the representation of symbolic structures in connection-\nist systems. Artiﬁcial Intelligence , 46:159–216, January 1990. URL https://doi.org/10.1016/\n0004-3702(90)90007-M.\nStatistics Canada. Census in Brief: The Aboriginal languages of First Nations people, Métis and\nInuit, 2017. URL https://www12.statcan.gc.ca/census-recensement/2016/as-sa/\n98-200-x/2016022/98-200-x2016022-eng.cfm .\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Z. Ghahra-\nmani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information\nProcessing Systems 27 , pages 3104–3112. Curran Associates, Inc., 2014. URL http://papers.nips.\ncc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf .\nThe Crow Language Conservancy. Crow dictionary online, 2019. URL https://dictionary.\ncrowlanguage.org.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems , pages\n5998–6008, 2017.\nJ. N. Washington, I. Salimzyanov, and F. M. Tyers. Finite-state morphological transducers for three kypchak\nlanguages. In Proceedings of the 9th Conference on Language Resources and Evaluation, LREC2014, 2014.\nIan H. Witten and Timothy C. Bell. The zero-frequency problem: Estimating the probability of novel events in\nadaptive text compression. IEEE Transactions on Information Theory, 37(4):1085–1094, July 1991.\nWycliffe. Yupik New Testament. Wycliffe Bible Translators, Saint Lawrence Island, Alaska, 2018.\nBarret Zoph and Kevin Knight. Multi-source neural translation. In Proceedings of NAACL-HLT, pages 30–34,\n2016.",
  "topic": "Inflection",
  "concepts": [
    {
      "name": "Inflection",
      "score": 0.833665132522583
    },
    {
      "name": "Lemmatisation",
      "score": 0.794565737247467
    },
    {
      "name": "Computer science",
      "score": 0.7159333825111389
    },
    {
      "name": "Agglutinative language",
      "score": 0.6593260765075684
    },
    {
      "name": "Natural language processing",
      "score": 0.5957006812095642
    },
    {
      "name": "Linguistics",
      "score": 0.5876635313034058
    },
    {
      "name": "Turkish",
      "score": 0.541667640209198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5209461450576782
    },
    {
      "name": "Root (linguistics)",
      "score": 0.49548447132110596
    },
    {
      "name": "Affix",
      "score": 0.4144722521305084
    },
    {
      "name": "Parsing",
      "score": 0.30783331394195557
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}