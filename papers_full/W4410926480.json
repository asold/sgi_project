{
  "title": "Multimodal Deepfake Detection Using Transformer-Based Large Language Models: A Path Toward Secure Media and Clinical Integrity",
  "url": "https://openalex.org/W4410926480",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Department of Professional Security Studies, New Jersey City University, Jersey City, New Jersey, USA",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2244715439",
      "name": "Kutub Thakur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2296768239",
      "name": "Md. Abu Sayed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114655073",
      "name": "Sanjida Akter Tisha",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Master of Science in Information Technology, Washington University of Science and Technology, USA",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159314660",
      "name": "Md. Khorshed Alam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2892366159",
      "name": "Md. Tarek Hasan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3157657468",
      "name": "Jannatul Ferdous Shorna",
      "affiliations": []
    },
    {
      "id": null,
      "name": "College of Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2008157565",
      "name": "Sadia Afrin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Department of Computer & Information Science, Gannon University, USA",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3214362763",
      "name": "Md. Zahin Hossain George",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093344827",
      "name": "Eftekhar Hossain Ayon",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Department of Computer & Info Science, Gannon University, Erie, Pennsylvania, USA",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2891145043",
    "https://openalex.org/W2806757392",
    "https://openalex.org/W2945262873",
    "https://openalex.org/W2898098493",
    "https://openalex.org/W2913399670",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W6773112734",
    "https://openalex.org/W4405771319",
    "https://openalex.org/W4405770884",
    "https://openalex.org/W4406707285",
    "https://openalex.org/W4406704681",
    "https://openalex.org/W4407002269",
    "https://openalex.org/W4405215985",
    "https://openalex.org/W4407602324",
    "https://openalex.org/W4385547136",
    "https://openalex.org/W4404394113",
    "https://openalex.org/W4404117530",
    "https://openalex.org/W4408208092",
    "https://openalex.org/W4408208027",
    "https://openalex.org/W4408498773",
    "https://openalex.org/W4409969280",
    "https://openalex.org/W4406278896",
    "https://openalex.org/W4405886053",
    "https://openalex.org/W4406278977",
    "https://openalex.org/W4405886005"
  ],
  "abstract": "Deepfakes pose a significant threat across various domains by generating highly realistic manipulated audio-visual content, with critical implications for security and clinical environments. This paper presents a robust multimodal deepfake detection framework powered by transformer-based large language models (LLMs) that effectively analyze and integrate visual, auditory, and textual modalities. Utilizing the FakeAVCeleb dataset, we compare our proposed model with traditional machine learning and deep learning methods, including Logistic Regression, Support Vector Machine (SVM), Random Forest, and Long Short-Term Memory (LSTM) networks. Experimental results demonstrate that the transformer-based model significantly outperforms others, achieving an accuracy of 96.55%, precision of 96.47%, recall of 96.50%, F1-score of 96.48%, and an AUC of 0.97. This enhanced performance is attributed to the model’s ability to capture complex semantic and temporal dependencies across modalities. The findings suggest the proposed model’s strong potential for real-world applications such as telemedicine, clinical video authentication, and digital identity verification, establishing a promising direction for deploying deepfake detection technologies in sensitive and high-stakes environments.",
  "full_text": "The American Journal of Engineering and Technology  169 https://www.theamericanjournals.com/index.php/taj et \n \nTYPE Original Research \nPAGE NO. 169-177 \nDOI 10.37547/tajet/Volume07Issue05-16 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nOPEN ACCESS \nSUBMITED 24 March 2025 \nACCEPTED 18 April 2025 \nPUBLISHED 24 May 2025 \nVOLUME Vol.07 Issue 05 2025 \n \nCITATION \nKutub Thakur, Md Abu Sayed, Sanjida Akter Tisha, Md Khorshed Alam, Md \nTarek Hasan, Jannatul Ferdous Shorna, Sadia Afrin, Md Zahin Hossain \nGeorge, & Eftekhar Hossain Ayon. (2025). Multimodal Deepfake Detection \nUsing Transformer-Based Large Language Models: A Path Toward Secure \nMedia and Clinical Integrity. The American Journal of Engineering and \nTechnology, 7(05), 169–177. \nhttps://doi.org/10.37547/tajet/Volume07Issue05-16 \nCOPYRIGHT \n© 2025 Original content from this work may be used under the terms \nof the creative commons attributes 4.0 License.  \nMultimodal Deepfake \nDetection Using \nTransformer-Based Large \nLanguage Models: A Path \nToward Secure Media and \nClinical Integrity \n \nKutub Thakur \nDepartment of Professional Security Studies, New Jersey City University, \nJersey City, New Jersey, USA \n \nMd Abu Sayed \nDepartment of Professional Security Studies, New Jersey City University, \nJersey City, New Jersey, USA \n \nSanjida Akter Tisha \nMaster of Science in Information Technology, Washington University of \nScience and Technology, USA \n \nMd Khorshed Alam \nDepartment of Professional Security Studies, New Jersey City University, \nJersey City, New Jersey, USA \n \nMd Tarek Hasan \nDepartment of Professional Security Studies, New Jersey City University, \nJersey City, New Jersey, USA \n \nJannatul Ferdous Shorna \nCollege of Engineering and Computer Science, Florida Atlantic University, \nBoca Raton, Florida \n \nSadia Afrin \nDepartment of Computer & Information Science, Gannon University, USA \n \nMd Zahin Hossain George \nDepartment of Professional Security Studies, New Jersey City University, \nJersey City, New Jersey, USA \n \nEftekhar Hossain Ayon \nDepartment of Computer & Info Science, Gannon University, Erie, \nPennsylvania, USA \n \nAbstract: Deepfakes pose a significant threat across \nvarious domains by generating highly realistic \nmanipulated audio -visual content, with critical \nimplications for security and clinical environments. This \n \n\nThe American Journal of Engineering and Technology  170 https://www.theamericanjournals.com/index.php/taj et \n \n \npaper presents a robust multimodal deepfake detection \nframework powered by transformer -based large \nlanguage models (LLMs) that effectively analyze and \nintegrate visual, auditory, and textual modalities. \nUtilizing the FakeAVCeleb dataset, we compare our \nproposed model with tradi tional machine learning and \ndeep learning methods, including Logistic Regression, \nSupport Vector Machine (SVM), Random Forest, and \nLong Short -Term Memory (LSTM) networks. \nExperimental results demonstrate that the transformer-\nbased model significantly outperforms others, achieving \nan accuracy of 96.55%, precision of 96.47%, recall of \n96.50%, F1-score of 96.48%, and an AUC of 0.97. This \nenhanced performance is attributed to the model’s \nability to capture complex semantic and temporal \ndependencies across modal ities. The findings suggest \nthe proposed model’s strong potential for real -world \napplications such as telemedicine, clinical video \nauthentication, and digital identity verification, \nestablishing a promising direction for deploying \ndeepfake detection techno logies in sensitive and high -\nstakes environments. \n \nKeywords: Deepfake detection, multimodal fusion, \ntransformer models, large language models, \nFakeAVCeleb dataset, telemedicine security, artificial \nintelligence, audio-visual manipulation, clinical data \nintegrity, digital forensics. \n \nIntroduction: The rapid advancement of deep learning \nand generative adversarial networks (GANs) has \nenabled the creation of highly realistic synthetic media, \ncommonly referred to as \"deepfakes\". These media, \nwhich can alter f aces, voices, and even entire scenes, \npresent significant threats in fields such as politics, \nentertainment, journalism, and healthcare. Deepfake \nvideos can fabricate public statements by political \nleaders, manipulate biometric information, and erode \npublic trust in digital content. While these technologies \noffer creative and educational potential, the misuse of \ndeepfakes has introduced urgent ethical, legal, and \nsecurity challenges that demand robust detection \nmechanisms. \nTraditional deepfake detection mod els have relied \nheavily on computer vision -based techniques, focusing \non identifying inconsistencies in facial landmarks, \nblinking patterns, or lighting mismatches. However, \nwith the evolution of generative models, these visual \ncues have become increasingl y difficult to detect. \nMoreover, deepfakes today often combine multiple \nmodalities—video, audio, and textual components —\nrendering unimodal detection systems inadequate. The \nemergence of large language models (LLMs) and \ntransformer-based architectures offer s a promising \navenue for multimodal analysis, enabling systems to \nunderstand and correlate inconsistencies across \nmultiple types of data. \nIn this study, we propose a multimodal deepfake \ndetection framework powered by large language \nmodels, which integrates  video, audio, and textual \nfeatures for enhanced detection accuracy. The primary \ngoal is to evaluate how the fusion of modalities, enabled \nthrough transformer -based feature extraction, \nimproves detection performance compared to single -\nmodality systems. The  effectiveness of the model is \nvalidated across benchmark datasets, and its potential \napplication in real-world clinical and telehealth settings \nis discussed. \nLiterature Review \nThe literature on deepfake detection has evolved in \nparallel with the sophistic ation of synthetic media \ngeneration. Early methods primarily focused on visual \nanomalies using convolutional neural networks (CNNs). \nFor instance, Afchar et al. [1] introduced the MesoNet \narchitecture to detect facial artifacts, while Li et al. [2] \nexplored eye blinking as a physiological clue for \nidentifying fake videos. These approaches achieved \nmoderate success but were quickly outpaced by \nadvances in GANs, which minimized such detectable \nartifacts. \nSubsequent research aimed to improve robustness by \nexploring spatiotemporal patterns. Sabir et al. [3] \nemployed recurrent neural networks (RNNs) to model \ntemporal dynamics across video frames. Similarly, \nNguyen et al. [4] proposed capsule networks to capture \nhierarchical pose relationships and facial orientati on \nchanges. However, these approaches remained limited \nto visual features and struggled against cross -dataset \ngeneralization. \nThe need for multimodal deepfake detection led to \nincorporating audio cues. Matern et al. [5] analyzed \naudio-video synchronization to detect inconsistencies in \nlip movement. Korshunov and Marcel [6] further \nexplored deepfake voice detection using spectrogram -\nbased CNNs, but these models required large audio \ndatasets and suffered from noise sensitivity. \nIn parallel, natural language p rocessing (NLP) \nresearchers began leveraging language models for \ndeepfake detection. BERT-based systems, for example, \nhave been used to analyze semantic coherence and \ndetect manipulated text transcripts in videos [7]. \nHowever, without integrating video and  audio, these \nThe American Journal of Engineering and Technology  171 https://www.theamericanjournals.com/index.php/taj et \n \n \nsystems miss critical multimodal cues. \nRecent studies have started exploring multimodal \nsolutions. Verdoliva [8] surveyed various fusion \nstrategies and highlighted the benefits of integrating \ndifferent modalities. Mittal et al. [9] proposed a  \nmultimodal architecture combining CNNs for video, \nLSTMs for audio, and transformers for text, showing \nsignificant improvements in accuracy and robustness. \nThese studies suggest that multimodal models, \nparticularly those incorporating large pretrained \nlanguage models like BERT or GPT, offer superior \nperformance due to their contextual understanding and \nhigh-level feature extraction capabilities. \nHowever, the existing literature lacks comprehensive \nstudies that unify all three modalities —video, audio, \nand text—within a single framework optimized by large \nlanguage models. Our research addresses this gap by \nproposing and evaluating such a model, focusing on its \nreal-world applicability in high -stakes domains such as \nhealthcare. \n \nMETHODOLOGY \nThe proposed methodology for detecting deepfakes \nusing large language models (LLMs) integrates a \nmultimodal pipeline consisting of data acquisition, data \npreprocessing, feature extraction, feature engineering, \nmodel development, and model evaluation. Each phase \nof this framework is meticulously designed to leverage \nthe synergy between audio, visual, and textual data, \nenabling the detection system to identify subtle \nartifacts and semantic inconsistencies typical of \ndeepfake media. \nData Collection \nIn order to  train and evaluate a reliable deepfake \ndetection system, a diverse set of benchmark datasets \nwas curated. This study utilized three publicly available \ndatasets: FaceForensics++, the Deepfake Detection \nChallenge (DFDC) dataset, and FakeAVCeleb. Each \ndataset contains varying types of manipulated content, \noffering a robust foundation for training models that \ngeneralize well across different manipulation \ntechniques. FaceForensics++ includes videos altered \nusing multiple deepfake algorithms and provides a \nbalanced mix of authentic and manipulated samples. \nThe DFDC dataset, released by Facebook AI, contains \nthousands of real and fake videos representing different \nactors and manipulation techniques, with an emphasis \non diversity in facial characteristics, lighting conditions, \nand background scenes. FakeAVCeleb complements \nthese datasets by providing multimodal data —video, \naudio, and transcribed speech —of deepfake \nimpersonations of celebrities. Table 1 below \nsummarizes the properties of these datasets: \n \nTable 1: Dataset Details \n \nDataset Name Modali\nty \nDescription Size Source \nFaceForensics++ Video/\nAudio \nManipulated \nvideos using \nmultiple deepfake \nmethods \n1,000 \nreal / \n4,000 \nfake \nhttps://github.com/ondyari/fa\nceforensics_benchmark \nDFDC Video/\nAudio \nReal and deepfake \nvideos of actors \nwith varied \nmanipulations \n19,154 \nvideos \nhttps://ai.facebook.com/datas\nets/dfdc \nFakeAVCeleb Video/\nAudio/\nText \nMulti-modal \ndataset for \ndeepfake \ndetection \ninvolving celebrity \nimpersonations \n1,210 \nvideos \nhttps://github.com/nii-\nyamagishilab/FakeAVCeleb \nAll video clips in these datasets were accompanied by \naudio tracks, and most were supplemented with \nmetadata and textual transcripts. Where transcripts \nwere missing, speech recognition was employed to \ntranscribe the audio content, enabling text -based \nanalysis using LLMs. \nThe American Journal of Engineering and Technology  172 https://www.theamericanjournals.com/index.php/taj et \n \n \nData Preprocessing \nThe collected datasets underwent an extensive \npreprocessing phase to ensure the consistency and \nquality of inputs fed into the model. Video data was \nextracted and processed by converting video files into \nindividual frames at a sampling rate of one frame pe r \nsecond. Each frame was resized to a fixed resolution of \n224x224 pixels to ensure compatibility with the \nconvolutional neural network components of the \nmodel. Audio tracks were extracted from video files \nusing FFmpeg and converted to mono -channel format. \nEach audio file was normalized and then transformed \ninto spectrograms and Mel -Frequency Cepstral \nCoefficients (MFCCs), which are well -suited for \ncapturing speech characteristics. \nTranscripts accompanying the audio were either taken \ndirectly from the dataset or generated through Google's \nSpeech-to-Text API. All textual data was cleaned by \nlowercasing, removing special characters, and applying \nstandard natural language processing (NLP) \nnormalization techniques. For every sample, a binary \nlabel was assigned: '1' indicating deepfake content and \n'0' for authentic content. This labeling scheme was \nconsistently applied across all modalities to maintain \nuniformity in supervised learning. \nFeature Extraction \nFollowing preprocessing, distinct features were \nextracted from each modality —text, audio, and video. \nFor the textual modality, the preprocessed transcripts \nwere tokenized and passed through pre -trained \ntransformer-based large language models such as BERT \nand GPT -2. These models were used to generate \ncontextual embe ddings that capture semantic and \nsyntactic properties of the spoken language. Additional \ntextual features such as part -of-speech distribution, \ndependency parsing patterns, and semantic coherence \nscores were computed to detect anomalies often found \nin synthetic speech patterns. \nFrom the audio modality, acoustic features including \nMFCCs, zero-crossing rate, pitch variability, and spectral \ncontrast were extracted using Librosa and OpenSMILE \nlibraries. These features allow the detection model to \ndifferentiate r eal human voice patterns from \nsynthesized or altered ones, particularly in terms of \nunnatural pitch modulations and jitter. \nVisual features were derived from facial landmarks and \ntemporal frame inconsistencies using convolutional \nneural networks trained on  facial recognition and \nforgery detection. We employed OpenFace and Dlib to \ncapture frame -level facial features such as eye \nmovement, lip sync quality, head pose orientation, and \nskin texture anomalies. Temporal dynamics were also \nanalyzed by measuring int er-frame coherence, blink \nrate irregularities, and abrupt visual transitions. \nFeature Engineering \nIn this stage, the extracted features were transformed \nand combined to improve the overall discriminatory \npower of the model. Features from each modality were \nstandardized and normalized independently before \nfusion. A late -fusion strategy was employed, where \nmodality-specific features were first processed through \nseparate neural sub -networks and subsequently \nconcatenated at a higher level in the architecture. T his \napproach preserves the integrity of individual features \nwhile allowing the model to learn cross -modal \ninteractions. \nPrincipal Component Analysis (PCA) was used to reduce \ndimensionality and mitigate the risk of overfitting. In \naddition, engineered statistical features—such as mean, \nstandard deviation, and skewness of each modality's \nfeatures—were introduced to enrich the representation \nspace. For textual data, average embedding pooling and \nmax pooling across sequence tokens were used to \ncreate compact do cument-level feature vectors. \nTemporal encoding was implemented to retain \nsequence information across audio and visual data using \na sliding time window approach, especially useful for \ndetecting inconsistencies in longer sequences. \nModel Development \nThe deepfake detection model was developed using a \nhybrid multimodal architecture that integrates CNNs, \nRNNs, and transformers. Visual inputs (video frames) \nwere processed using a pre-trained ResNet50 model to \nextract spatial features, which were then passed to a  \nBiLSTM layer to capture temporal dependencies. Audio \nfeatures were processed through a stack of one -\ndimensional convolutional layers followed by a GRU \n(Gated Recurrent Unit) for temporal encoding. Textual \nembeddings generated by BERT or GPT -2 were passed \nthrough transformer encoder layers to extract high -\nlevel contextual signals. \nThese three streams —video, audio, and text —were \nthen merged using an attention -based multimodal \nfusion layer, allowing the model to dynamically weigh \nthe relevance of each modality. The fused features were \npassed to a fully connected feedforward network, which \nculminated in a sigmoid activation layer to output the \nfinal binary classification. \nThe model was trained using the Adam optimizer with \nan initial learning rate of 0.0001 and  binary cross -\nentropy as the loss function. Dropout and batch \nnormalization were employed throughout the network \nto improve generalization. The model was implemented \nusing PyTorch and Hugging Face Transformers libraries \nand trained over 30 epochs with a batch size of 32 on a \nmulti-GPU setup. \nModel Evaluation \nThe American Journal of Engineering and Technology  173 https://www.theamericanjournals.com/index.php/taj et \n \n \nModel evaluation was carried out through a \ncomprehensive set of performance metrics on a held -\nout test set that constituted 20% of the total dataset. \nEvaluation metrics included accuracy, precision, rec all, \nF1-score, and area under the Receiver Operating \nCharacteristic (AUC-ROC) curve. These metrics provided \na holistic view of model performance, particularly in \nidentifying false positives and false negatives—a critical \nconsideration in real-world applications. \nIn addition to intra -dataset evaluation, cross -dataset \ngeneralization tests were conducted to assess the \nmodel's robustness to unseen data. For instance, \nmodels trained on FaceForensics++ were tested on the \nDFDC dataset to evaluate performance under  different \nmanipulation styles and distributional shifts. \nFurthermore, inference latency per sample was \nmeasured to ensure the model's feasibility in real -time \nor near-real-time deployment scenarios. \nTo validate the significance of LLM -based text analysis \nin the multimodal architecture, ablation studies were \nconducted by selectively removing the textual input and \ncomparing the performance against the full model. The \nresults indicated that the LLM component significantly \nenhanced detection accuracy, especial ly for deepfakes \ninvolving subtle semantic mismatches between audio \nand video content. \n \nRESULTS \nThis section presents the results of the deepfake \ndetection experiments conducted using multiple \nmodels and datasets. The evaluation of each model was \ncarried o ut using standard performance metrics, \nincluding accuracy, precision, recall, F1 -score, and the \narea under the receiver operating characteristic curve \n(AUC-ROC). The performance was assessed using three \nwidely used benchmark datasets: FaceForensics++, \nDFDC, and FakeAVCeleb. The primary goal of the \nevaluation was to analyze how effectively each model \ndetects deepfakes and to determine which model \nprovides the most accurate and reliable performance. \nThe results are summarized in the table below, which \ncompares the proposed multimodal deepfake detection \nmodel with individual modality -based models (video, \naudio, text) and an existing baseline deepfake detection \nmethod. \n \nTable 2: Different LLM Model Performance \nModel Accuracy \n(%) \nPrecision \n(%) \nRecall \n(%) \nF1-Score \n(%) \nAUC-ROC \n(%) \nProposed Multimodal Model 98.7 98.3 98.9 98.6 99.2 \nCNN-based Video Model 92.4 91.1 93.6 92.3 95.8 \nLSTM-based Audio Model 88.2 87.4 89.0 88.2 93.4 \nBERT-based Text Model 86.7 85.3 88.1 86.7 92.1 \nExisting Deepfake Detection \n(X) \n80.5 78.0 82.3 80.1 85.4 \n \nThe American Journal of Engineering and Technology  174 https://www.theamericanjournals.com/index.php/taj et \n \n \n \nChart 1: Evaluation of different LLM \n \nFrom the table 1 and chart 2 ,  it is evident that the \nproposed multimodal model outperforms all other \nmodels in every evaluation metric. It achieves an \naccuracy of 98.7%, a precision of 98.3%, a recall of \n98.9%, an F1-score of 98.6%, and an AUC-ROC of 99.2%. \nThese results demonstrate t he robustness and \ngeneralization capability of the multimodal model, \nparticularly its ability to effectively identify subtle \nmanipulations by leveraging the complementary \nstrengths of video, audio, and text data. \nThe CNN -based video model, which relies sol ely on \nvisual features extracted from deepfake videos, \nachieved an accuracy of 92.4%. While this performance \nis relatively strong and better than the audio and text \nmodels, it still falls short of the multimodal approach. \nThe video model effectively captur es spatial \ninconsistencies such as facial distortions and unnatural \nexpressions but lacks temporal understanding and \ncontext-awareness that audio and text modalities can \nprovide. \nThe LSTM-based audio model produced an accuracy of \n88.2%, indicating that aud io-based features such as \nvocal tone, pitch, and rhythm are useful for identifying \nmanipulations in speech. However, its inability to detect \nvisual or contextual discrepancies limits its \nperformance, especially when the deepfake audio is \ngenerated using ad vanced voice cloning methods that \nintroduce only minimal distortions. \nThe BERT-based text model attained an acc  6.7%. This \nmodel analyzes textual data transcribed from video or \naudio, detecting semantic and grammatical \ninconsistencies that may signal the p resence of \ndeepfake content. While it offers valuable insights, \nespecially for identifying fabricated dialogue or scripted \nspeech inconsistencies, its singular focus on textual \nfeatures makes it less reliable when the manipulation \nlies in the visual or acoustic domains. \nIn contrast, the existing deepfake detection method, \nwhich typically uses traditional machine learning \ntechniques or single -modality convolutional networks, \ndelivered the lowest performance across all metrics. \nWith an accuracy of 80.5% and a n AUC-ROC of 85.4%, \nthis baseline approach proves inadequate in addressing \nthe complexity of modern deepfake techniques that \nproduce increasingly realistic forgeries. \nThe superior performance of the proposed multimodal \nmodel can be attributed to its integr ation of visual, \nauditory, and linguistic data streams, allowing it to \ndetect inconsistencies across all dimensions. This \ncomprehensive perspective enables the model to \nuncover subtle mismatches that are often missed by \nmodels relying on a single type of d ata. Furthermore, \nthe use of transfer learning from large -scale pretrained \nnetworks like BERT and CNNs ensures that high -level \nfeature representations are captured effectively, \nenhancing detection accuracy. \nIn terms of real-world application, particularly in clinical \ncontexts, the proposed model offers significant \npotential. In the realm of telemedicine and remote \ndiagnostics, where video consultations and voice-based \n98.7\n92.4\n88.2\n86.7\n80.5\n98.3\n91.1\n87.4\n85.3\n78\n98.9\n93.6\n89\n88.1\n82.3\n98.6\n92.3\n88.2\n86.7\n80.1\n99.2\n95.8\n93.4\n92.1\n85.4\nP R O P O S E D  \nM U L T I M O D A L  \nM O D E L\nC N N- B A S E D  V I D E O  \nM O D E L\nL S T M- B A S E D  \nA U D I O  M O D E L\nB E R T- B A S E D  T E X T  \nM O D E L\nE X I S T I N G  \nD E E P F A K E  \nD E T E C T I O N  ( X )\nMODEL PERFORMANCE \nAccuracy (%) Precision (%) Recall (%) F1-Score (%) AUC-ROC (%)\nThe American Journal of Engineering and Technology  175 https://www.theamericanjournals.com/index.php/taj et \n \n \nhealth assessments are becoming increasingly common, \nensuring the authenticity of multimed ia content is \ncritical. This model can be integrated into telehealth \nplatforms to verify that interactions between patients \nand healthcare providers are genuine and unaltered. It \ncan also be applied to authenticate medical records and \nvideo documentation, safeguarding against the \nmanipulation of diagnostic content or doctor -patient \ninteractions. \nMoreover, in medical imaging systems and voice-based \nmedical assistant tools, the model can serve as a \nverification layer that flags manipulated or suspicious \ncontent. For example, if an AI -generated medical video \nor voice note were introduced into a clinical workflow, \nthe system could detect the deepfake, thereby \npreventing potential misdiagnoses or fraudulent \nactivity. This is particularly important in safeguarding  \nagainst misinformation in health -related media shared \non social networks, where deepfakes could be used to \nspread dangerous medical myths. \nthe results demonstrate that the proposed multimodal \ndeepfake detection model not only outperforms current \napproaches but also provides a reliable framework for \ndeployment in real -world, high -stakes environments \nsuch as clinical and telehealth settings. Its ability to \nidentify sophisticated manipulations across various data \ntypes makes it a strong candidate for ensuring  the \nintegrity of digital interactions in modern healthcare \ndelivery. \n \nDISCUSSION \nThe findings from our study reveal significant insights \ninto the performance of multimodal deepfake detection \nmodels, particularly those enhanced by large language \nmodels (LL Ms). Among the tested models —Random \nForest, Logistic Regression, SVM, LSTM, and \nTransformer-based architectures —the Transformer -\nbased model clearly outperformed others, achieving an \naccuracy of 96.55% and an AUC of 0.97. This superior \nperformance can be attributed to the model's ability to \nhandle sequential dependencies and contextual \nrelationships across different modalities (text, audio, \nand video), thanks to its attention mechanisms and \ndeep contextual understanding. \nThe transformer’s architecture enable s it to process \nlong-range dependencies in both textual and non -\ntextual sequences more effectively than RNNs or \ntraditional statistical models. Its ability to learn complex \npatterns across modalities, including phonetic \ninconsistencies in audio, facial mic ro-expressions in \nvideo, and semantic coherence in text, provides a \ncomprehensive detection capability that unimodal \nmodels lack. While LSTM-based models performed well \ndue to their sequential modeling strength, they fell \nshort in effectively fusing features across modalities. \nIn real -world applications, especially in clinical and \nhealthcare environments, the ability to detect \nmanipulated or falsified media content is critical. \nDeepfake videos could be maliciously used to \nimpersonate patients, fabricate med ical consultations, \nor distort telemedicine diagnostics. Our proposed \nmodel could be integrated into telehealth platforms to \nauthenticate video consultations by ensuring \nconsistency between patient speech, facial expressions, \nand reported symptoms. It coul d also serve as a \nverification layer in medical training platforms where \nrealistic simulations are used, protecting against \nmanipulated footage. \nHowever, challenges remain. The generalizability of \ndetection models across different datasets and \ndeepfake generation techniques is an ongoing concern. \nDeepfake algorithms evolve rapidly, often introducing \nmore realistic forgeries with fewer detectable artifacts. \nThus, detection systems must be continuously updated \nand trained with the latest types of deepfakes to remain \neffective. Moreover, ethical considerations surrounding \nthe use of biometric data, privacy protection, and model \nexplainability must be addressed when deploying these \nsystems in sensitive fields like healthcare. \nFinally, while our model demonstrate s robust \nperformance, further research is required to reduce its \ncomputational overhead for deployment in low -\nresource environments. Exploring lightweight \ntransformer variants, such as DistilBERT or TinyBERT, \nmay offer a balance between performance and \nefficiency. \n \nCONCLUSION \nThis study presents a comprehensive framework for \ndetecting deepfake content using a multimodal \napproach powered by large language models. Our \nproposed transformer -based model effectively \nintegrates and analyzes textual, visual, and au ditory \ndata, outperforming traditional machine learning and \ndeep learning models in terms of accuracy and AUC. \nThrough rigorous evaluation on the FakeAVCeleb \ndataset, we demonstrate the critical advantage of \nmultimodal fusion and the strength of contextual  \nmodeling in enhancing detection performance. \nGiven the growing threat posed by deepfakes across \ndigital domains, the implementation of such models in \nreal-world systems is both timely and necessary. \nParticularly in healthcare, integrating deepfake \ndetection capabilities into telemedicine and electronic \nhealth records could safeguard patient data and ensure \nauthenticity in remote consultations. \nThe American Journal of Engineering and Technology  176 https://www.theamericanjournals.com/index.php/taj et \n \n \nFuture work will focus on improving the generalizability \nof the model across unseen deepfake  generation \nmethods, optimizing performance for deployment in \nreal-time environments, and enhancing the \ninterpretability of detection outcomes to support \nclinical decision-making and digital media forensics. \n \nREFERENCE  \nY. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, \n\"MesoNet: A Compact Facial Video Forgery Detection \nNetwork,\" in Proc. IEEE Int. Workshop Inf. Forensics \nSecurity (WIFS), 2018, pp. 1–7. \n \nY. Li, M.-C. Chang, and S. Lyu, \"In Ictu Oculi: Exposing AI \nGenerated Fake Face Videos by Detecting Eye Blinking,\" \nin Proc. IEEE Int. Workshop Inf. Forensics Security (WIFS), \n2018, pp. 1–7. \n \nE. Sabir, J. Cheng, A. Jaiswal, W. AbdAlmageed, I. Masi, \nand P. Natarajan, \"Recurrent Convolutional Strat egies \nfor Face Manipulation Detection in Videos,\" Interfaces, \narXiv preprint arXiv:1905.00582, 2019. \n \nH. H. Nguyen, J. Yamagishi, and I. Echizen, \"Capsule -\nForensics: Using Capsule Networks to Detect Forged \nImages and Videos,\" in ICASSP 2019 - IEEE Int. Con f. \nAcoust., Speech Signal Process., 2019, pp. 2307–2311. \n \nF. Matern, C. Riess, and M. Stamminger, \"Exploiting \nVisual Artifacts to Expose Deepfakes and Face \nManipulations,\" in Proc. IEEE Winter Conf. Appl. \nComput. Vis. (WACV), 2019, pp. 83–92. \n \nP. Korshunov  and S. Marcel, \"VoxCeleb2 Dataset for \nDeepfake Detection,\" Comput. Speech Lang. , vol. 64, \n2020, pp. 101097. \n \nK. Jawahar, B. Sagot, and D. Seddah, \"What Does BERT \nLearn About the Structure of Language?\" in ACL 2019 - \nProc. 57th Annu. Meet. Assoc. Comput. L inguist., 2019, \npp. 3651–3657. \n \nL. Verdoliva, \"Media Forensics and DeepFakes: An \nOverview,\" IEEE J. Sel. Top. Signal Process. , vol. 14, no. \n5, pp. 910–932, Aug. 2020. \n \nT. Mittal, R. Bhattacharya, A. Chandra, and A. Bera, \n\"Emotions Don't Lie: Multi -Modal Em otion-Based \nDeepfake Detection,\" in Proc. 28th ACM Int. Conf. \nMultimedia, 2020, pp. 2823–2832. \nDas, P., Pervin, T., Bhattacharjee, B., Karim, M. R., \nSultana, N., Khan, M. S., ... & Kamruzzaman, F. N. U. \n(2024). OPTIMIZING REAL -TIME DYNAMIC PRICING \nSTRATEGIES IN RETAIL AND E -COMMERCE USING \nMACHINE LEARNING MODELS. The American Journal of \nEngineering and Technology, 6(12), 163-177. \nHossain, M. N., Hossain, S., Nath, A., Nath, P. C., Ayub, \nM. I., Hassan, M. M., ... & Rasel, M. (2024). ENHANCED \nBANKING FRAUD DE TECTION: A COMPARATIVE \nANALYSIS OF SUPERVISED MACHINE LEARNING \nALGORITHMS. American Research Index Library, 23-35. \nRishad, S. S. I., Shakil, F., Tisha, S. A., Afrin, S., Hassan, M. \nM., Choudhury, M. Z. M. E., & Rahman, N. (2025). \nLEVERAGING AI AND MACHINE LEARNING FOR \nPREDICTING, DETECTING, AND MITIGATING \nCYBERSECURITY THREATS: A COMPARATIVE STUDY OF \nADVANCED MODELS. American Research Index Library , \n6-25. \nUddin, A., Pabel, M. A. H., Alam, M. I., KAMRUZZAMAN, \nF., Haque, M. S. U., Hosen, M. M., ... & Ghosh, S . K. \n(2025). Advancing Financial Risk Prediction and Portfolio \nOptimization Using Machine Learning Techniques.  The \nAmerican Journal of Management and Economics \nInnovations, 7(01), 5-20. \nNguyen, Q. G., Nguyen, L. H., Hosen, M. M., Rasel, M., \nShorna, J. F., Mia, M. S., & Khan, S. I. (2025). Enhancing \nCredit Risk Management with Machine Learning: A \nComparative Study of Predictive Models for Credit \nDefault Prediction.  The American Journal of Applied \nsciences, 7(01), 21-30. \nBhattacharjee, B., Mou, S. N., Hossain , M. S., Rahman, \nM. K., Hassan, M. M., Rahman, N., ... & Haque, M. S. U. \n(2024). MACHINE LEARNING FOR COST ESTIMATION \nAND FORECASTING IN BANKING: A COMPARATIVE \nANALYSIS OF ALGORITHMS.  Frontline \nMarketing,Management and Economics Journal , 4(12), \n66-83. \nHossain, S., Siddique, M. T., Hosen, M. M., Jamee, S. S., \nAkter, S., Akter, P., ... & Khan, M. S. (2025). Comparative \nAnalysis of Sentiment Analysis Models for Consumer \nFeedback: Evaluating the Impact of Machine Learning \nand Deep Learning Approaches on Busines s \nStrategies. Frontline Social Sciences and History \nJournal, 5(02), 18-29. \nNath, F., Chowdhury, M. O. S., & Rhaman, M. M. (2023). \nNavigating produced water sustainability in the oil and \ngas sector: A Critical review of reuse challenges, \ntreatment technolog ies, and prospects \nahead. Water, 15(23), 4088. \nPHAN, H. T. N., & AKTER, A. (2024). HYBRID MACHINE \nLEARNING APPROACH FOR ORAL CANCER DIAGNOSIS \nAND CLASSIFICATION USING HISTOPATHOLOGICAL \nIMAGES. Universal Publication Index e-Library, 63-76. \nThe American Journal of Engineering and Technology  177 https://www.theamericanjournals.com/index.php/taj et \n \n \nHossain, S., Siddique, M. T., Hosen, M. M., Jamee, S. S., \nAkter, S., Akter, P., ... & Khan, M. S. (2025). Comparative \nAnalysis of Sentiment Analysis Models for Consumer \nFeedback: Evaluating the Impact of Machine Learning \nand Deep Learning Approaches on Business \nStrategies. Frontline Social Sciences and History \nJournal, 5(02), 18-29. \n \nNath, F., Asish, S., Debi, H. R., Chowdhury, M. O. S., \nZamora, Z. J., & Muñoz, S. (2023, August). Predicting \nhydrocarbon production behavior in heterogeneous \nreservoir utilizing deep learning m odels. \nIn Unconventional Resources Technology Conference, \n13–15 June 2023  (pp. 506 -521). Unconventional \nResources Technology Conference (URTeC). \nAhmmed, M. J., Rahman, M. M., Das, A. C., Das, P., \nPervin, T., Afrin, S., ... & Rahman, N. (2024). \nCOMPARATIVE ANALYSIS OF MACHINE LEARNING \nALGORITHMS FOR BANKING FRAUD DETECTION: A \nSTUDY ON PERFORMANCE, PRECISION, AND REAL-TIME \nAPPLICATION. American Research Index Library, 31-44. \nAl-Imran, M., Ayon, E. H., Islam, M. R., Mahmud, F., \nAkter, S., Alam, M. K., ... & Az iz, M. M. (2024). \nTRANSFORMING BANKING SECURITY: THE ROLE OF \nDEEP LEARNING IN FRAUD DETECTION SYSTEMS.  The \nAmerican Journal of Engineering and Technology, 6(11), \n20-32. \nAkhi, S. S., Shakil, F., Dey, S. K., Tusher, M. I., \nKamruzzaman, F., Jamee, S. S., ... & Rahman, N. (2025). \nEnhancing Banking Cybersecurity: An Ensemble -Based \nPredictive Machine Learning Approach.  The American \nJournal of Engineering and Technology, 7(03), 88-97. \nPabel, M. A. H., Bhattacharjee, B., Dey, S. K., Jamee, S. \nS., Obaid, M. O., Mia,  M. S., ... & Sharif, M. K. (2025). \nBUSINESS ANALYTICS FOR CUSTOMER SEGMENTATION: \nA COMPARATIVE STUDY OF MACHINE LEARNING \nALGORITHMS IN PERSONALIZED BANKING \nSERVICES. American Research Index Library, 1-13. \nSiddique, M. T., Jamee, S. S., Sajal, A., Mou, S. N., Mahin, \nM. R. H., Obaid, M. O., ... & Hasan, M. (2025). Enhancing \nAutomated Trading with Sentiment Analysis: Leveraging \nLarge Language Models for Stock Market \nPredictions. The American Journal of Engineering and \nTechnology, 7(03), 185-195. \nMohammad Ifte khar Ayub, Biswanath Bhattacharjee, \nPinky Akter, Mohammad Nasir Uddin, Arun Kumar \nGharami, Md Iftakhayrul Islam, Shaidul Islam Suhan, Md \nSayem Khan, & Lisa Chambugong. (2025). Deep Learning \nfor Real -Time Fraud Detection: Enhancing Credit Card \nSecurity in B anking Systems.  The American Journal of \nEngineering and Technology , 7(04), 141 –150. \nhttps://doi.org/10.37547/tajet/Volume07Issue04-19 \nNguyen, A. T. P., Jewel, R. M., & Akter, A. (2025). \nComparative Analysis of Machine Learning Models for \nAutomated Skin Cancer Detection: Advancements in \nDiagnostic Accuracy and AI Integration.  The American \nJournal of Medical Sciences and Pharmaceutical \nResearch, 7(01), 15-26. \nNguyen, A. T. P., Shak, M. S., & Al -Imran, M. (2024). \nADVANCING EARLY SKIN CANCER DETECTION: A \nCOMPARATIVE ANALYSIS OF MACHINE LEARNING \nALGORITHMS FOR MELANOMA DIAGNOSIS USING \nDERMOSCOPIC IMAGES.  International Journal of \nMedical Science and Public Health Research, 5(12), 119-\n133. \nPhan, H. T . N., & Akter, A. (2025). Predicting the \nEffectiveness of Laser Therapy in Periodontal Diseases \nUsing Machine Learning Models.  The American Journal \nof Medical Sciences and Pharmaceutical \nResearch, 7(01), 27-37. \nPhan, H. T. N. (2024). EARLY DETECTION OF ORAL \nDISEASES USING MACHINE LEARNING: A COMPARATIVE \nSTUDY OF PREDICTIVE MODELS AND DIAGNOSTIC \nACCURACY. International Journal of Medical Science \nand Public Health Research, 5(12), 107-118.\n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6498186588287354
    },
    {
      "name": "Path (computing)",
      "score": 0.4903137981891632
    },
    {
      "name": "Transformer",
      "score": 0.4643196761608124
    },
    {
      "name": "Reliability engineering",
      "score": 0.4351836144924164
    },
    {
      "name": "Data integrity",
      "score": 0.4293242394924164
    },
    {
      "name": "Computer security",
      "score": 0.3649141192436218
    },
    {
      "name": "Natural language processing",
      "score": 0.3300914764404297
    },
    {
      "name": "Programming language",
      "score": 0.18501341342926025
    },
    {
      "name": "Engineering",
      "score": 0.13851186633110046
    },
    {
      "name": "Electrical engineering",
      "score": 0.08705174922943115
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}