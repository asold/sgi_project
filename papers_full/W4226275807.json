{
  "title": "Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models",
  "url": "https://openalex.org/W4226275807",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2168796573",
      "name": "Kazuki Miyazawa",
      "affiliations": [
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A3046462571",
      "name": "Yuta Kyuragi",
      "affiliations": [
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2103006713",
      "name": "Takayuki Nagai",
      "affiliations": [
        "Osaka University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2619383789",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W6769196770",
    "https://openalex.org/W2053101950",
    "https://openalex.org/W6767908396",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W3096690837",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W4249477935",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W3210279979",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W6748455135",
    "https://openalex.org/W6730357551",
    "https://openalex.org/W6775188310",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2963710346",
    "https://openalex.org/W3176404283",
    "https://openalex.org/W3152844947",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3037572520",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6752724743",
    "https://openalex.org/W3175199633",
    "https://openalex.org/W3126854045",
    "https://openalex.org/W6793736971",
    "https://openalex.org/W6797236868",
    "https://openalex.org/W6792417942",
    "https://openalex.org/W6791141078",
    "https://openalex.org/W3173948887",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3139848350",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Transformer-based models have garnered attention because of their success in natural language processing, and in several other fields, such as image and automatic speech recognition. In addition to them being trained on unimodal information, many transformer-based models have been proposed for multimodal information. In multimodal learning, a common problem encountered is the insufficiency of multimodal training data. In this study, to address this problem, a simple and effective method is proposed by using 1) unimodal pre-trained transformer models as encoders for each modal input and 2) a set of transformer layers to fuse their output representations. Further, the proposed method is evaluated by conducting several experiments on two common benchmarks: CMU multimodal opinion sentiment intensity dataset and multimodal internet movie database. The proposed model exhibits state-of-the-art performances on both benchmarks and is robust against the reduction in the amount of training data.",
  "full_text": "Received February 10, 2022, accepted February 23, 2022, date of publication March 14, 2022, date of current version March 22, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3159346\nSimple and Effective Multimodal Learning Based\non Pre-Trained Transformer Models\nKAZUKI MIYAZAWA\n 1, YUTA KYURAGI\n 1, AND TAKAYUKI NAGAI\n1,2, (Member, IEEE)\n1Graduate School of Engineering Science, Osaka University, Osaka 565-0871, Japan\n2Artiﬁcial Intelligence Exploration Research Center, The University of Electro-Communications, Tokyo 182-8585, Japan\nCorresponding author: Takayuki Nagai (nagai@sys.es.osaka-u.ac.jp)\nThis work was supported in part by the Japan Society for the Promotion of Science (JSPS) KAKENHI under Grant JP19J23364, and in part\nby the Grant-in-Aid for Scientiﬁc Research on Innovative Areas under Grant 20H05565.\nABSTRACT Transformer-based models have garnered attention because of their success in natural language\nprocessing, and in several other ﬁelds, such as image and automatic speech recognition. In addition to\nthem being trained on unimodal information, many transformer-based models have been proposed for\nmultimodal information. In multimodal learning, a common problem encountered is the insufﬁciency of\nmultimodal training data. In this study, to address this problem, a simple and effective method is proposed\nby using 1) unimodal pre-trained transformer models as encoders for each modal input and 2) a set\nof transformer layers to fuse their output representations. Further, the proposed method is evaluated by\nconducting several experiments on two common benchmarks: CMU multimodal opinion sentiment intensity\ndataset and multimodal internet movie database. The proposed model exhibits state-of-the-art performances\non both benchmarks and is robust against the reduction in the amount of training data.\nINDEX TERMS Multimodal machine learning, transformers, pre-trained models, emotion recognition.\nI. INTRODUCTION\nHumans live in a world with multimodal information. They\nsee, hear, smell, touch, and taste many things in their daily\nlives. In addition, they communicate and understand fel-\nlow humans through words, tones, and facial expressions.\nThus, learning the relations among different modalities and\ninterpreting their meaning are essential for creating artiﬁ-\ncial intelligence that can understand the world. To address\nthis problem, multimodal learning is employed for building\nmodels that can process and relate information from multiple\nmodalities [1].\nRecently, transformer-based [2] models have attracted sig-\nniﬁcant attention in various research ﬁelds. Although the\noriginal transformer [2] was designed for natural language\nprocessing (NLP) tasks, its mechanisms have been applied\nin various models using other modalities such as image\nand audio [3]–[6]. Because of the success of these modal-\nities, the interest in applying the transformer mechanism\nto multimodal learning has increased. However, training\nthese transformer-based models or any other neural-network-\nbased models requires a large amount of training data [1].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Joey Tianyi Zhou.\nSeveral large datasets for individual modalities are available;\nhowever, multimodal datasets are difﬁcult to create and ﬁnd\nbecause the collection of multimodal data is more difﬁcult\nthan the collection of unimodal data. Consequently, in multi-\nmodal learning, the problem of insufﬁcient multimodal train-\ning data is common. When the amount of training data is\ninsufﬁcient, a multimodal model is more likely to encounter\nunfamiliar input words or patterns in the test dataset. For\nexample, words that rarely or never appear in the training\ndataset can be encountered. To address this problem, exist-\ning pre-trained models trained on large datasets are used to\nextract better representations from each modal input. This\nenables the multimodal model to understand the meaning of\nsuch unknown inputs. Thus, in this study, a simple and effec-\ntive method is proposed for solving the aforementioned prob-\nlem by using 1) unimodal pre-trained transformer models as\nencoders for each modal input and 2) a set of transformer\nlayers to fuse their output representations.\nIn multimodal learning, the fusion of different modality\ninformation is considered important. Many types of multi-\nmodal fusion exist; however, the major approaches can be\nclassiﬁed into two types: early and late fusion [7]. The for-\nmer fuses low-level features, and the latter fuses prediction-\nlevel features. Thus, based on other multimodal transformer\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 29821\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nmodels [8]–[12], the proposed model was structured by using\ntwo pre-trained transformer models and a set of transformer\nlayers to fuse the outputs from the two pre-trained transformer\nmodels (Fig. 1). Although the outputs of the pre-trained trans-\nformer models were concatenated in a late-fusion manner,\ninstead of the [CLS] outputs, which can represent the entire\nsequence in a single token, being simply concatenated, simi-\nlar to a shallow-fusion model [10], the output representations\nwere fused using a few transformer layers. Moreover, the\nself-attention mechanism helped the model to extract the nec-\nessary information from both modality outputs, depending on\nthe original inputs.\nAn advantage of the proposed method is that the\npre-trained models can be easily alternated depending on the\navailability of a better pre-trained model or the input modal-\nities. This can be easily accomplished because ﬁne-tuning\nin advance with the pre-trained transformer models is not\nneeded, and thus, these pre-trained models can be directly\nused as encoders for each modality. Another pre-trained\ntransformer model can be easily added depending on the\nmodalities of the training data, because the fusion mecha-\nnism is simply a concatenation of the output representations\nof the pre-trained transformer models. Thus, the proposed\nmethod was assumed to be applicable to any combination of\nmodalities.\nThe effectiveness of the proposed method was veri-\nﬁed by evaluating it on two common benchmarks: the\nCMU multimodal opinion sentiment intensity dataset\n(CMU-MOSI) [13] and the multimodal internet movie\ndatabase (MM-IMDb) [14]. The results indicated that the pro-\nposed method outperformed the latest models and achieved\nstate-of-the-art performance in terms of most metrics on both\nthe datasets. The proposed method was also evaluated using\na reduced amount of training data for each dataset and was\nfound to outperform most of the latest models using only 25%\nto 50% of the original training dataset. This robustness to the\nreduction in the training data amount was attributed to the bet-\nter representations from the pre-trained transformer models\nand the structure required during their pre-training. Because\npre-trained transformer models are trained on a large amount\nof data within their modality, they provide better representa-\ntions of the data that never appear in the multimodal training\ndataset.\nII. RELATED WORK\nTill date, many transformer-based models have been pro-\nposed and used for single-modality and multimodal tasks.\nHerein, a few of the latest pre-trained transformer models\nare introduced, and subsequently, models focusing on mul-\ntimodal fusions are discussed.\nA. PRE-TRAINED TRANSFORMER MODELS\nSimilar to BERT [15], many transformer-based models are\npre-trained to learn general representations before they are\nﬁne-tuned to other downstream tasks. This pre-training phase\nis often performed with a large amount of unsupervised data\nfor an extended period of time, and subsequently, the models\nare expected to be ﬁne-tuned to a certain smaller downstream\ndataset for a short period of time. Recently, many pre-trained\ntransformer models have been proposed. In the NLP ﬁeld,\nmany improved models of the BERT [15] and transformer [2]\nhave been proposed, such as RoBERTa [16], XL-Net [17],\nLongformer [18], and GPT-2 [19]. Although the transformer\nwas originally designed for NLP tasks, the model has also\nbeen successful in other ﬁelds that involve the use of different\nmodality information. The vision transformer (ViT) [4] has\nbeen proposed for image recognition and classiﬁcation. The\nViT is often referred to as a state-of-the-art image classiﬁca-\ntion model, and it has been successful as an image encoder\nin other models, such as CLIP [20] and video transformer\nnetworks [21]. The video transformer network was designed\nfor video recognition, where a ViT was used to extract the\nfeatures for each frame. Further, models such as Wav2Vec [3],\nVQ-Wav2Vec [6], Wav2Vec 2.0 [5], and Conformer [22]\nhave been demonstrated to be successful in audio processing\nand speech recognition tasks. These pre-trained transformer\nmodels are composed of similar architectures using trans-\nformers [2]; however, they are trained on a large dataset of\ntheir target modality.\nB. MULTIMODAL FUSION\nIn multimodal learning, a model must combine the infor-\nmation of different modalities to improve the performance\nof multimodal tasks. Variational autoencoder (V AE)-based\nmodels, such as multimodal V AEs [23] and joint multimodal\nV AEs [24], have been proposed to learn a shared represen-\ntation across modalities. Since the proposal of the trans-\nformer [2], its performance and ability to visualize an entire\ninput sequence have attracted attention, and consequently,\nmany models have applied its attention mechanism to mul-\ntimodal learning using the models presented in the previous\nsection. The multimodal bitransformer [8], LXMERT [11],\nVilBERT [9], VisualBERT [12], and PixelBERT [25] extend\nthe BERT [15] architecture and change the inputs to text and\nimages. These models use text-pre-trained transformer mod-\nels as text encoders and other mechanisms for image encoders\nsuch as ResNet-152 [26] or trained their own transformer as\nan image encoder. In contrast, CLIP [20] uses the ViT [4] as\nits image encoder and text information as weakly supervising\ninformation. These models either use a pre-trained trans-\nformer model as an encoder for one of their modalities or sim-\nply use the structure of the transformer. The shallow fusion of\nself-supervised learning (SSL) models [10] uses two single-\nmodality-pre-trained transformer models, where the [CLS]\noutputs from the two pre-trained models are concatenated in a\nlate-fusion manner, and subsequently, the model is ﬁne-tuned\nfor multimodal emotion analysis tasks. Similar to the pro-\nposed model, this model relies on pre-trained transformer\nencoder models. However, the difference lies in the fact that\nthe proposed model uses the self-attention mechanism to\n29822 VOLUME 10, 2022\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nFIGURE 1. Model architecture. The upper part represents the structure of\nthe proposed model, and the lower part shows the pretrained models\ntrained on large amount of unimodal data. The proposed model\neffectively learns on relatively small amount of multimodal data by\nintegrating two pre-trained models with the transformer encoder. For the\npre-trained models,A and B, we used RoBERTa [16], ViT [4], and\nWav2Vec 2.0 [5] depending on the input modality.\ncombine the whole output of the pre-trained models, whereas\nin shallow fusion, a multi-layer perceptron (MLP) is used to\nmake predictions from the [CLS] outputs of the pre-trained\nmodels. Thus, because of the application of the transformer\nlayers over the entire output, the proposed model is assumed\nto be capable of determining a method to effectively extract\nthe necessary information from the outputs using the self-\nattention mechanism.\nIII. PROPOSED METHOD\nIn this section, the overall structure of the proposed model\nand the pre-trained models used are explained.\nA. PROPOSED MODEL\nThe overall architecture of the proposed model is shown\nin Fig. 1. The proposed model comprises three parts:\n1) a pre-trained transformer model for modality A,\n2) a pre-trained transformer model for modality B, and\n3) the top transformer encoder layers for multimodal fusion.\nFor the pre-trained models, RoBERTa [16], Wav2Vec 2.0 [5],\nand ViT [4] can be used depending on the modality of the\ninput. Further, RoBERTa and Wav2Vec 2.0 were employed\nas the two pre-trained models for the later experiment using\nCMU-MOSI, whereas RoBERTa and ViT were employed\nas the two pre-trained models for the subsequent experi-\nment using MM-IMDb. Although in principle, the proposed\nmethod can work with models other than these pre-trained\nmodels, it is validated in this study by using these pre-\ntrained models. For both experiments, transformer encoder\nlayers were used for multimodal fusion. Moreover, because\npositional embeddings were already added to the pre-trained\nmodels, they were not considered in the top transformer\nlayers.\nThe output hidden states from each pre-trained encoder\nmodel, oA and oB, with sizes oA ∈ Rs×dmodel and oB ∈\nRs×dmodel , respectively, were concatenated for use as the input\nfor the top transformer encoder layers as follows:\nh0 =concat(oA, oB), (1)\nwhere concat(a, b) represents the concatenation of a and b,\ns is the sequence length, and dmodel is the dimension size of\nthe model. In this study, we used a pre-trained model with the\nsame dmodel . If the dmodel is different for each modal, linear\nembedding can be used to make the dmodel the same for each\nmodal. Using the concatenated output, h0, as the input for\nthe top transformer layers, we calculated the hidden states in\neach layer, hℓ(ℓ∈{1, 2,3}), using a multi-head self-attention\nmechanism, which can be expressed as\nMultiHead (hℓ)=concat(m1,m2 ··· ,mh)W O\nℓ , (2)\nwhere\nQℓ =Kℓ =Vℓ =hℓ, (3)\nmi =attention\n(\nQℓW Q\nℓ,i, KℓW K\nℓ,i, VℓW V\nℓ,i\n)\n, (4)\ni =1,2,··· ,h\nattention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV . (5)\nQ, K, and V are the query, key, and value, respectively, and\ndk is the scaling factor that satisﬁes dk =dv =dmodel /h,\nwhere dmodel and h are the model dimensions and number\nof heads, respectively. W Q\nℓ,i, W K\nℓ,i, W V\nℓ,i, and W O\nℓ are the\nprojection parameters, whose dimensions are the following:\nW Q\nℓ,i ∈ Rdmodel ×dk , W K\nℓ,i ∈ Rdmodel ×dk , W V\nℓ,i ∈ Rdmodel ×dv ,\nand W O\nℓ ∈ Rhdv×dmodel . softmax(·) denotes the softmax\nfunction. Each transformer encoder layer has two sublay-\ners: a multi-head self-attention layer and a fully connected\nfeed-forward network layer FeedForward(·). Further, for each\nsublayer, a residual connection and layer-wise normalization\nLayerNorm(·) were employed. Therefore, the output of each\nlayer can be calculated as\nh′\nl−1 =LayerNorm (hl−1 +MultiHead(hl−1)), (6)\nhl =LayerNorm(h′\nl−1 +FeedForward(h′\nl−1)). (7)\nFor the purpose of classiﬁcation, the ﬁrst token embed-\nding in the last layer h3 was used, which corresponded to\nVOLUME 10, 2022 29823\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nthe [CLS] token in the output. The hyperparameters used in\nthe experiments are presented in Table 3.\nB. ROBERTA\nRoBERTa [16] is an extension model of BERT that removes\nthe next sentence prediction objective in BERT and pre-trains\nthe model longer with a larger amount of data. The archi-\ntecture of RoBERTa is similar to that of BERT, except\nfor some minor changes in its key parameters. With only\na few changes, RoBERTa has been shown to outperform\nBERT and XLNet in all nine tasks in GLUE [27], and many\nother studies have further conﬁrmed its usefulness in other\ntasks. Thus, RoBERTa was used as the encoder for the text\ninformation.\nFor the experiments using CMU-MOSI, roberta-base1 and\nroberta-large2 were used as the pre-trained model weights\nfor the text encoder. In addition, for the experiments using\nMM-IMDb, roberta-large was used as the pre-trained model\nweight for the text encoder.\nC. WAV2VEC 2.0\nWav2Vec 2.0 [5] is a model pre-trained in a self-supervised\nmanner similar to masked language modeling in BERT. It ran-\ndomly masks a certain proportion of time steps in the latent\nfeature encoder space and solves a contrastive task [28]. Dur-\ning this pre-training, wav2vec learns speech representation by\ncontrastive task, where the true latent is to be distinguished\nfrom its distractors. Following the pre-training phase, the\nmodel was ﬁne-tuned on the Librispeech-960 dataset [29]\nfor speech recognition tasks using connectionist temporal\nclassiﬁcation (CTC) loss [30].\nIn our experiment using CMU-MOSI, we use facebook/\nwav2vec2-base-960h3 and facebook/wav2vec2-large-960h4\nas the pretrained weights for our audio encoder.\nD. VISION TRANSFORMER\nThe ViT [4] is a model that facilitates image classiﬁcation\nusing transformer encoder models with a slight change in\nits encoder architecture. For a particular image input, the\nmodel splits the image into a sequence of ﬁxed-size non-\noverlapping patches, which are then linearly embedded and\nused as the input for the transformer layers. The ViT primarily\nrelies on transformer architectures and outperforms models\nusing convolutional architectures. Further, it is successful\nas an image encoder in other models, such as CLIP [20],\na weakly supervised image representation learning model that\nsucceeded in few-shot learning tasks.\nThus, in the experiment using MM-IMDb, google/vit-\nlarge-patch16-2245 were used as the pre-trained model\nweights for the image encoder.\n1https://huggingface.co/roberta-base\n2https://huggingface.co/roberta-large\n3https://huggingface.co/facebook/wav2vec2-base-960h\n4https://huggingface.co/facebook/wav2vec2-large-960h\n5https://huggingface.co/google/vit-large-patch16-224\nE. TRAINING\nThe proposed model can be trained in two ways:\n1) Full Training: The entire model, including the\npre-trained models and the top transformer layers, was\noptimized simultaneously.\n2) Frozen Pre-trained Weights: The top transformer layers\nwere optimized using multimodal training data, while\nthe pre-trained model weights were ﬁxed.\nIntuitively, it is expected that ‘‘full training results in a\nhigher performance.’’ Further, in the experiments described\nlater, these two learning methods were compared in terms\nof performance and the amount of training data. In addition,\n‘‘Full Training w/o Pre-trained Weights,’’ was compared\nas well, wherein the learning begins from scratch with-\nout using pre-trained weights while maintaining the model\nstructure.\nF. HOW DOES OUR PROPOSED MODEL WORK?\nIn this section, the effectiveness of the proposed model for\nmultimodal learning has been discussed considering two per-\nspectives.\nThe ﬁrst is the over-ﬁtting perspective. In general, a large\nmodel with many parameters, such as a transformer, should\nbe trained on a large amount of data, because if the amount\nof data is insufﬁcient, a high possibility of overﬁtting exists.\nOne solution to this problem involves setting good initial\nvalues. As the proposed model is a late-fusion-type multi-\nmodal learning model, wherein the layers close to the input\nsignal are independent embeddings for each modality and\nthese independent embeddings are achieved using pre-trained\nmodels which provide good initial values for training the\nmodel.\nThe other is the perspective of association between differ-\nent modalities. As mentioned earlier, the proposed method is\na multimodal learning method of late fusion type; that is, this\nmodel implies that each modality is embedded independently\nin the ﬁrst half, with the top transformer learning the associ-\nations between them. Therefore, if the pre-trained model has\nalready learned the embedding, the top-level transformer with\nrelatively fewer parameters only needs to learn the correspon-\ndence between modalities, implying that it can learn with less\ntraining data. Furthermore, because the pre-trained model is\nconsidered to represent a certain category in its embedding\nspace, the amount of data required to learn the associations\nbetween them is expected to be less than that required to\nlearn the correspondence at the sample level. Finally, ﬁne-\ntuning optimizes the entire system to be more adaptive to the\ntask.\nThese properties are very convenient for multimodal learn-\ning because collecting data on a single modality is relatively\nsimple with availability of many published pre-trained mod-\nels. However, collecting synchronized multimodal data or\npreparing a large amount of training data is both expensive\nand challenging.\n29824 VOLUME 10, 2022\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nTABLE 1. Dataset details.\nIV. EXPERIMENTS\nThe effectiveness of the proposed method was veriﬁed by\nevaluating it on two common benchmarks for the classiﬁ-\ncation tasks. First, the dataset details shown in Table 1 and\nevaluation settings are explained, and then, the experimental\nresults are presented.\nA. DATASET\n1) CMU-MOSI\nThe CMU-MOSI dataset [13] is a human multimodal senti-\nment analysis dataset comprising 2,199 monologue opinion\nvideo clips from YouTube movie reviews. Each clip contains\nthree modalities: acoustic, facial, and text.\nThe experiments in this study were conducted by\nemploying only acoustic and text information because no\nsingle-modality-pre-trained transformer model for facial\ninformation was found. In CMU-MOSI, each sample is\nlabeled by human annotators with a sentiment score in the\nrange of [−3, +3]. The objective was to predict the sentiment\nscore of each segment. There were 1,284, 229, and 686 seg-\nments in the training, validation, and test sets, respectively.\nFurther, the software development kit reported in [31] was\nused. Following [10], [32] 7-class accuracy, 2-class accuracy,\nmean average error (MAE), and correlation were adopted as\nthe evaluation metrics for the experiments.\n2) MM-IMDb\nThe MM-IMDb dataset [14] consists of 25,959 movie plot\noutlines and images of movie posters. The objective was to\nclassify the genres of each movie. This was a multi-label\nprediction problem, implying that one movie may belong to\nmultiple genres, as in the example in Table 2. There were\n15,552, 2,608, and 7,799 movies in the training, validation,\nand test sets, respectively. To perform the evaluation, the\nprocess outlined in [14], [32], [33] was followed, and the F1\nmacro, F1 micro, F1 samples, and F1 weighted were adopted\nas the evaluation metrics.\nB. BASELINE METHODS\nThe effectiveness of the proposed method was veriﬁed by\ncomparing it with the latest models on the aforementioned\ndatasets.\n• CMU-MOSI\nMulT\nMultimodal transformer (MulT) [34] is a\nmodel designed for unaligned multimodal lan-\nguage sequences, wherein a directional pair-\nwise cross-modal transformers are adopted\nto merge the multimodal information. The\nMulT architecture pairs all modalities with\ncross-modal transformers, followed by a trans-\nformer that performs predictions using the\nfused features.\nICCN\nInteraction canonical correlation network\n(ICCN) [35] extracts the interaction features of\na CNN in a deep canonical correlation analysis\n(DCCA)-based network, with the core idea of\nthe model being to learn the hidden correlations\nbetween the features extracted from the outer\nproduct of text and audioand text and videos.\nMAG-BERT/MAG-XLNet\nMAG-BERT and MAG-XLNet are models\nwith multimodal adaptation gate (MAG) [36]\napplied to a certain layer of BERT [15] and\nXLNet [17]. MAG allows BERT and XLNet\nto have multimodal nonverbal data during ﬁne-\ntuning. In contrast to the proposed model,\nwherein each modality is treated equally,\nMAG-BERT and MAG-XLNet use non-text\ninformation as complementary information to\ntext information.\nShallow-Fusion\nSimilar to the proposed model, the shal-\nlow fusion of SSL models [10] employs\nRoBERTa [16] as the language encoder and\nthe BERT-like transformer architecture trained\non discretized speech tokens based on VQ-\nWav2Vec [6]. However, in contrast to the pro-\nposed model, this model only concatenates the\n[CLS] output from each pre-trained model,\ninstead of considering all hidden states.\n• MM-IMDb\nBM-NAS\nBilevel multimodal neural architecture search\n(BM-NAS) [33] is a model designed for more\ngeneralized and ﬂexible DNNs for multimodal\nlearning, wherein a bilevel searching scheme\nthat learns the unimodal feature selection strat-\negy at the upper level and the multimodal\nfeature fusion strategy at the lower level is\nadopted.\nSMIL\nMultimodal learning with severely missing\nmodalities (SMIL) [32] is a Bayesian meta-\nlearning-based model proposed to solve the\nproblem of missing modalities in multimodal\ndatasets. This model aims to employ a feature\nreconstruction network, which generates an\napproximation of the missing modality, thereby\nenabling the model to obtain complete data in\nthe latent feature space.\nGMU\nGated multimodal units (GMUs) [14] are recur-\nrent units designed for multimodal fusion\nVOLUME 10, 2022 29825\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nTABLE 2. Example data for each dataset.\nTABLE 3. Hyperparameters of our model in each experiment. Epoch on the left shows those used in ‘‘Full Training’’ model, epoch in the middle shows the\nepochs used in ‘‘Frozen Pre-trained Weights’’ , and epoch on the right shows those used in ‘‘w/o Pre-trained Weights’’ models. ‘‘Lr,’’ , ‘‘Decay,’’ and\n‘‘Warmup’’ represent the learning rate, weight decay, and the warm up steps used in training. ‘‘#layers’’ is the number of layers used in our top\ntransformer layers.\nthat have one modality gate over the other.\nThis model resulted in the proposal of the\nMM-IMDb dataset [14].\nMMBT\nMultimodal bitransformers (MMBT) [8] use\nBERT [15] as their primary architecture and\nResNet-152 [26] to extract image features. Fur-\nther, the image features are concatenated with\nsentence tokens to form an input to the BERT\nstructure. Moreover, this functions on the idea\nof applying transformer architecture for multi-\nmodal fusion, similar to the proposed model.\nC. EXPERIMENTAL SETTINGS\nFor both datasets, the architecture shown in Fig. 1 was\nused. For CMU-MOSI, RoBERTa and Wav2Vec 2.0 were\nemployed as the two pre-trained models, whereas for MM-\nIMDb, RoBERTa and ViT were used. Hyperparameters used\nin both datasets are shown in Table 3. The transformer\nencoder were used as the top transformer layers to fuse\nthe output representations from the two pre-trained models.\nFurther, for both datasets, the model was applied under three\ndifferent conditions with different amounts of training data\n(10%,25%,50%,75%,90%,and100%) as follows:\n1) Full Training:\nFine-tuning the whole model including the pre-trained\nmodels\n2) Frozen Pre-trained Weights:\nFine-tuning the top transformer layers with the\npre-trained model weights frozen\n3) w/o Pre-trained Weights:\nWith same model size but directly ﬁne-tuning\nthe whole model without loading the pre-trained\nweights\nIn the experiment using the CMU-MOSI dataset, the\nmodel was evaluated using base and large models for the\npre-trained transformer models, wherein dmodel =768 and\n1024 were used for the base and large models, respectively.\nHowever, for the experiment using the MM-IMDb dataset,\nthe performance of the proposed model was compared with\ntwo other conditions: text-only and image-only. For each\nmodel, transformer encoder were added on top of each single-\nmodality-pre-trained model (RoBERTa and ViT) and subse-\nquently the model was ﬁne-tuned to the classiﬁcation task.\nHowever, similar experiments with the CMU-MOSI dataset\nwere not conducted because Wav2Vec 2.0, does not have\na [CLS] token, as in RoBERTa and ViT. We only used\nthe large models for the MM-IMDb dataset, because we\nfound that the large models achieved better performance\nthan base models in our ﬁrst experiment on the CMU-MOSI\ndataset. We used Hugging Face’s Transformers library [37]\nto build our models and load the pre-trained weights.\nWe used eight NVIDIA A100 (40GB) GPUs for each\nexperiment.\n29826 VOLUME 10, 2022\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nFIGURE 2. Results for training data reduction experiment using CMU-MOSI dataset ((a), (b)) and MM-IMDb dataset ((c), (d)). We compare our\nmodels on three different conditions: 1) fine-tuning the whole model including the pretrained models (Full Training), 2) fine-tuning the top\ntransformer layers with the pretrained model weights frozen (Frozen Pretrained Weights), and 3) same model size but directly fine-tuning the\nwhole model without loading the pretrained weights (w/o Pretrained Weights). The purple lines show the previous state-of-the-art model result\ntrained on full training data. For our results in all metrics, please see Tables 6, 7, and 8 in the Appendix.\nTABLE 4. Results for multimodal sentiment analysis on the CMU-MOSI\ndataset [13]. Performances of the other models are obtained from [32],\n[34], [35], [38], [10], [36].\nTABLE 5. Results for multimodal classification task on the MM-IMDb\ndataset [14]. Performances of the other models are taken\nfrom [8], [14], [32], [33].\nV. RESULTS AND DISCUSSION\nThis section presents the results and summarizes the\nobservations. The main results for the CMU-MOSI and\nFIGURE 3. Distribution of attention ratio for text data in MM-IMDb\nexperiment. The attention ratio is calculated for each attention head, and\nthe average value is used for this figure.\nMM-IMDb experiments are listed in Tables 4 and 5,\nrespectively.\nA. IMPROVEMENT IN PERFORMANCE\nDespite its simplicity, the proposed model achieved the state-\nof-the-art performance in terms of most metrics on both\ndatasets, as shown in Tables 4 and 5. A few other models\nalso performed as well as the proposed model, such as the\nshallow fusion of SSL models [10] and MMBTs [8], which\nshare similar ideas. For instance, the shallow fusion of SSL\nVOLUME 10, 2022 29827\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nFIGURE 4. Attention ratio for each label in MM-IMDb dataset [14]. Figure on the left shows the average attention ratio of text for each class in all\ntest data. Figures in the middle and the right show the number of examples that belong to each class in the top 100 text dominant examples and\nthe top 100 image dominant examples, respectively. These figures indicate that the model focuses more on each modality depending on the class\nthat the data belongs to.\nFIGURE 5. Top 10 Text Dominant Movies. Examples with an F1 score> 0.8 are selected. Missing labels are shown in red and mispredicted labels\nare shown with a strike-out line.\nmodels has two unimodal pre-trained transformer models and\nconcatenates their [CLS] output to make predictions, similar\nto the proposed model. However, MMBTs use transformer\nencoder layers to combine concatenated multimodal infor-\nmation, and this is an aspect of the proposed model. The\nresults obtained by using these models [8], [10] reveal that the\nuse of pre-trained transformer models as encoder components\nfor the entire model and the fusion mechanism with the\ntransformer encoder layers are effective in multimodal learn-\ning. We consider that the transformer encoder layers were\nable to successfully aggregate the information from the two\npre-trained models by applying the self-attention mechanism.\nHowever, we have not conducted comparative experiments on\nthe structure of this part and will investigate this in the future.\nB. ROBUSTNESS TO LESS TRAINING DATA\nIn Fig. 2, the results obtained using the proposed model for\nthree different conditions using different amounts of training\ndata are compared. The purple lines in Fig. 2 illustrate the pre-\nvious state-of-the-art performances, trained on 100% of the\noriginal training data, presented in Tables 4 and 5. The results\nindicate that the ‘‘Full Training’’ model outperformed those\nproposed in certain previous works [14], [32]–[34], [36], [38]\ndespite using only 25% to 50% of the original training data in\nboth datasets. This robustness to the reduction of the training\ndata amount is attributed to the use of pre-trained transformer\nmodel weights. Because the unimodal pre-trained models are\ntrained on a large dataset for their modality, they provide\nbetter embeddings for many words or patterns, including\nthose that never appear in the multimodal training dataset.\nMoreover, they are expected to have structured knowledge\nof the modality in the pre-trained model weights. To exam-\nine whether the improvement in the performance was not\nbecause of the number of parameters or the transformer struc-\nture but rather owing to the pre-trained model weights and\nstructured knowledge of the modality, the proposed model\nwas tested with pre-trained weights being frozen (‘‘Frozen\nPre-trained Weights’’) and without using the pre-trained\nmodel weights (‘‘w/o Pre-trained Weights’’). The results\nobtained using ‘‘w/o Pre-trained Weights’’ were inferior to\nthose with the ‘‘Full Training’’ model, regardless of the\namount of data, implying that pre-trained model weights were\nthe key to the improvements in the performance. Although\nthe ‘‘Frozen Pre-trained Weights’’ model performed bet-\nter than the ‘‘w/o Pre-trained Weights’’ model, its perfor-\nmance dropped more signiﬁcantly when compared with ‘Full\nTraining’’ and ‘‘w/o Pre-trained Weights.’’ Thus, the entire\nmodel, including the pre-trained models, should be trained\nto improve the performance and render it more robust to the\nreduction in the training data amount.\nC. COMPARISON WITH UNIMODAL MODELS\nTable 5 presents the results of our method using only uni-\nmodal information. The results indicate that the text-only\nmodel performs almost as well as the ‘‘Full Training’’ model,\nwhich was trained on information from both modalities. This\n29828 VOLUME 10, 2022\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nFIGURE 6. Top 10 Image Dominant Movies. Examples with F1 Score> 0.8 are selected. Missing labels are shown in red, and the mispredicted\nlabels are shown with a strike-out line.\nFIGURE 7. Attention in each head in the most text dominant example in MM-IMDb test dataset. Attention for the [CLS] token in the first layer of\nthe top transformer layers is shown. Purple represents the text tokens and the orange represents the image tokens.\nmay suggest that the full training model attracts attentions\nprimarily from the text encoder output; however, the results\nshown in Fig. 3 suggest that the top layer attracts more\nattention from the image encoder output. The attention is\ncaptured from the ﬁrst layer of the top transformer encoder\nlayers, and for text data αtext, it is calculated as\nαtext =\nσtext\nattention\nNtext\ntokens\nσtext\nattention\nNtext\ntokens\n+σimage\nattention\nNimage\ntokens\n, (8)\nwhere σtext\nattention and σimage\nattention represent the summation of\nattention over each modality and Ntext\ntokens and Nimage\ntokens are the\nnumber of tokens in each modality. This indicates that the\nproposed model uses the outputs from both the text and image\nencoder models to improve the performance.\nD. ATTENTION RATIO FOR EACH MODALITY\nFig. 4 shows the average attention ratio for each label in\nMM-IMDb dataset, which indicates that the modality to be\nfocused on is decided by the model depending on the data.\nVOLUME 10, 2022 29829\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nFIGURE 8. Attention in each head in the most image dominant example in MM-IMDb dataset. Attention for the [CLS] token in the first layer of\nthe top transformer layers is shown here. Purple shows the text tokens and the orange shows the image tokens.\nA clear correlation exists between the class and modality in\nwhich the model focuses. Fig. 4 (a) depicts that ‘‘Sport,’’\n‘‘Documentary,’’ and ‘‘Biography’’ movies are classiﬁed\nbased on text information rather than image information.\nIn contrast, ‘‘Horror,’’ ‘‘Western,’’ and ‘‘Film-Noir’’ movies\nare classiﬁed based on image information rather than text\ninformation. Fig. 4 (b) shows the number of examples that\nbelong to each class in the top 100 text dominant examples\n(top 100 data with the highest attention ratio for text), whereas\nFig. 4 (c) shows the same in the top 100 image dominant\nexamples. Thus, the model is more focused on text data for\nclasses such as ‘‘Comedy’’ and ‘‘Romance,’’ and on image\ndata for classes such as ‘‘Thriller,’’ ‘‘Horror,’’ and ‘‘Crime.’’\nFurther, compared with the text dominant examples shown\nin Fig. 5, the image dominant examples shown in Fig. 6 tend\nto have more terrifying-looking movie posters. Moreover, the\ndifference in the attention ratio among the labels suggests that\nthe model can decide which modality should be focused on\naccording to the inputs. In shallow fusion models [10], the\n[CLS] output from each pre-trained model are used, and an\nMLP layer is directly used for classiﬁcation. Figures 7 and 8\nshow the attention in each head for the ﬁrst [CLS] token in\nthe ﬁrst layer of the top transformer layers. The attention\nof the ﬁrst token for each modality is not the highest, and\neach head appears to gather different types of information\nfrom each modality. This indicates that the model efﬁciently\nextracts the necessary information from the entire output for\nclassiﬁcation tasks.\nE. PRE-TRAINED MODELS FOR OTHER MODALITIES\nThe proposed model fused multimodal information by simply\nconcatenating the outputs from the pre-trained transformer\nmodels; therefore, the number of modalities it can handle\ndepends on the number of existing pre-trained models. For\nexample, pre-trained transformer models exist for human\nposes [39], biological signals [40], videos [21], [41], [42],\nand robot dynamics [43]. Consequently, the model can be\neasily ﬁne-tuned to a multimodal task by combining these\npre-trained models and using the transformer layers on top.\nHowever, in cases where a pre-trained model for the modal-\nity in a multimodal dataset does not exist, a new trans-\nformer model can be pre-trained using a large unsupervised\ndataset of the modality, or a method reported in a recent\nwork [44] can be used, which showed that the pre-trained\n29830 VOLUME 10, 2022\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nTABLE 6. Results for all metrics in our training data reduction experiment\non the CMU-MOSI dataset. We compare ourbase models on three\ndifferent conditions: 1) fine-tuning the whole model including the\npre-trained models (Full Training), 2) fine-tuning the top transformer\nlayers with the pre-trained model weights frozen (Frozen Pre-trained\nWeights), and 3) using same model size but directly fine-tuning the whole\nmodel without loading the pre-trained weights (w/o Pre-trained Weights).\ntransformer of a modality can be transferred to different\nmodalities.\nF. LIMITATIONS OF THE PROPOSED METHOD\nIn theory, the proposed model can handle more modalities\nthan those used in our evaluations. However, in practice,\nit is dependent on the available computational resources.\nBecause the model has multiple pre-trained transformer mod-\nels, which can result in large size even by using only one\nof them, the proposed method has a limitation with respect\nto the model size. In addition to the pre-trained transformer\nmodels, a few transformer encoder layers were used for\nmultimodal fusion. In these layers, the self-attention mech-\nanism [2] was used instead of the cross-attention mechanism\nmentioned in [11] because the application of the former to\nthe concatenated outputs is expected to facilitate the model\nin extracting the necessary information from both modal-\nity outputs. However, the self-attention mechanism requires\nresults at greater computational costs than the cross-attention\nmechanism [11]. Thus, using the cross-attention or other\nmechanisms in the latest efﬁcient transformer models\ncan reduce the model size and increase the training\nspeed.\nVI. CONCLUSION\nIn this study, an architecture that is heavily reliant on existing\npre-trained transformer models was proposed. Unimodal pre-\ntrained transformer models, such as RoBERTa and Vision\nTransformer, were employed as encoders for each modality,\nand self-attention transformer layers were used to fuse their\noutput representations. The proposed model aimed to reduce\nthe amount of training data required to achieve state-of-the-\nart performance in multimodal learning. Further, the model\nwas evaluated on two common benchmarks for multimodal\nTABLE 7. Results for all metrics in our training data reduction experiment\non the CMU-MOSI dataset. We compare ourlarge models on three\ndifferent conditions: 1) fine-tuning the whole model including the\npre-trained models (Full Training), 2) fine-tuning the top transformer\nlayers with the pre-trained model weights frozen (Frozen Pre-trained\nWeights), and 3) using same model size but directly fine-tuning the whole\nmodel without loading the pre-trained weights (w/o Pre-trained Weights).\nTABLE 8. Results for all metrics in our training data reduction experiment\non the MM-IMDb dataset. We compare our models on three different\nconditions: 1) fine-tuning the whole model including the pre-trained\nmodels (Full Training), 2) fine-tuning the top transformer layers with the\npre-trained model weights frozen (Frozen Pre-trained Weights), and\n3) using same model size but directly fine-tuning the entire model\nwithout loading the pre-trained weights (w/o Pre-trained Weights).\nclassiﬁcation. State-of-the-art performance was achieved on\nboth benchmarks, and the robustness of the model to the\nreduction in the training data amount was demonstrated as the\nmodel outperformed the previous state-of-the-art models with\nonly 25% to 50% of the original training dataset. The results\nrevealed that the proposed method is simple and effective\nfor multimodal learning in both text-image and text-audio\ncombinations, thereby indicating that it can be effective for\nany other combination of modalities.\nIn the future, the multimodal learning method will be\nextended to generation tasks using multiple modalities.\nIn addition, as the proposed method has a limitation in\nterms of the computational costs involved, in future, the\nuse of more efﬁcient transformer models will be consid-\nered to reduce the model size and increase the training\nspeed.\nVOLUME 10, 2022 29831\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\nAPPENDIX\nA. RESULT DETAILS\nTables 6, 7, and 8 present the results for all metrics in the\ntraining-data reduction experiment conducted. Most of the\nmetrics exhibit patterns similar to the results shown in Fig. 2.\nThe training time for the ‘‘Full Training’’ model with 100%\ndata amount is approximately 40, 120, and 300 minutes for\nCMU-MOSI (base acc-7), CMU-MOSI (large acc-7), and\nMM-IMDb, respectively.\nREFERENCES\n[1] T. Baltrušaitis, C. Ahuja, and L.-P. Morency, ‘‘Multimodal machine learn-\ning: A survey and taxonomy,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 41, no. 2, pp. 423–443, Feb. 2019.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[3] S. Schneider, A. Baevski, R. Collobert, and M. Auli, ‘‘Wav2vec: Unsuper-\nvised pre-training for speech recognition,’’ in Proc. Interspeech, Sep. 2019,\npp. 3465–3469.\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. and Houlsby, ‘‘An image is worth 16 ×16 words: Transformers\nfor image recognition at scale,’’ in Proc. Int. Conf. Learn. Represent.\n(ICLR), 2021.\n[5] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, ‘‘Wav2vec 2.0: A frame-\nwork for self-supervised learning of speech representations,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2020, pp. 12449–12460.\n[6] A. Baevski, S. Schneider, and M. Auli, ‘‘Vq-wav2vec: Self-supervised\nlearning of discrete speech representations,’’ in Proc. Int. Conf. Learn.\nRepresent. (ICLR), 2020.\n[7] P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankanhalli, ‘‘Multi-\nmodal fusion for multimedia analysis: A survey,’’ Multimedia Syst., vol. 16,\nno. 6, pp. 345–379, Nov. 2010.\n[8] D. Kiela, S. Bhooshan, H. Firooz, and D. Testuggine, ‘‘Supervised mul-\ntimodal bitransformers for classifying images and text,’’ in Proc. Visually\nGrounded Interact. Lang. (ViGIL) NIPS Workshop, 2019.\n[9] J. Lu, D. Batra, D. Parikh, and S. Lee, ‘‘ViLBERT: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks,’’ in\nProc. Adv. Neural Inf. Process. Syst., vol. 2019, pp. 13–23.\n[10] S. Siriwardhana, A. Reis, R. Weerasekera, and S. Nanayakkara, ‘‘Jointly\nﬁne-tuning,’’ BERT-like self supervised models to improve multi-\nmodal speech emotion recognition,’’ in Proc. Interspeech, vol. 2020,\npp. 3755–3759.\n[11] H. Tan and M. Bansal, ‘‘LXMERT: Learning cross-modality encoder repre-\nsentations from transformers,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-\nIJCNLP), 2019, pp. 5100–5111.\n[12] L. Harold Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, ‘‘Visual-\nBERT: A simple and performant baseline for vision and language,’’ 2019,\narXiv:1908.03557.\n[13] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, ‘‘Multimodal sentiment\nintensity analysis in videos: Facial gestures and verbal messages,’’ IEEE\nIntell. Syst., vol. 31, no. 6, pp. 82–88, Nov. 2016.\n[14] J. E. A. Ovalle, T. Solorio, M. Montes-y-Gómez, and F. A. A. Gonz lez,\n‘‘Gated multimodal units for information fusion,’’ in Proc. Int. Conf. Learn.\nRepresent. (ICLR), 2017.\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nHum. Lang. Technol. Conf. North Am. Chapter Assoc. Comput. Linguistics\n(HLT-NAACL), 2019, pp. 4171–4186.\n[16] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[17] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language understand-\ning,’’ in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 5753–5763.\n[18] I. Beltagy, M. E. Peters, and A. Cohan, ‘‘Longformer: The long-document\ntransformer,’’ 2020, arXiv:2004.05150.\n[19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ‘‘Lan-\nguage models are unsupervised multitask learners,’’ OpenAI, Tech. Rep.,\n2019.\n[20] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\n‘‘Learning transferable visual models from natural language supervision,’’\nin Proc. Int. Conf. Mach. Learn. (ICML), vol. 2021, pp. 8748–8763.\n[21] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, ‘‘Video transformer net-\nwork,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops (ICCVW),\nOct. 2021, pp. 3163–3172.\n[22] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y . Wu, and R. Pang, ‘‘Conformer: Convolution-\naugmented transformer for speech recognition,’’ in Proc. Interspeech,\n2020, pp. 5036–5040.\n[23] M. Wu and N. Goodman, ‘‘Multimodal generative models for scalable\nweakly-supervised learning,’’ in Proc. Adv. Neural Inf. Process. Syst.,\n2018, pp. 5575–5585.\n[24] M. Suzuki, K. Nakayama, and Y . Matsuo, ‘‘Joint multimodal learning with\ndeep generative models,’’ in Proc. Int. Conf. Learn. Represent. (ICLR),\nWorkshop Track Proc., 2017.\n[25] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, ‘‘Pixel-BERT: Align-\ning image pixels with text by deep multi-modal transformers,’’ 2020,\narXiv:2004.00849.\n[26] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[27] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n‘‘GLUE: A multi-task benchmark and analysis platform for natural lan-\nguage understanding,’’ in Proc. Int. Conf. Learn. Represent. (ICLR),\n2019.\n[28] A. van den Oord, Y . Li, and O. Vinyals, ‘‘Representation learning with\ncontrastive predictive coding,’’ 2018, arXiv:1807.03748.\n[29] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‘‘Librispeech: An\nASR corpus based on public domain audio books,’’ in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process. (ICASSP), Apr. 2015, pp. 5206–5210.\n[30] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, ‘‘Connectionist\ntemporal classiﬁcation: Labelling unsegmented sequence data with recur-\nrent neural networks,’’ in Proc. Int. Conf. Mach. Learn. (ICML), 2006,\npp. 369–376.\n[31] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, and L.-P. Morency,\n‘‘Multi-attention recurrent network for human communication comprehen-\nsion,’’ in Proc. Conf. AAAI Artif. Intell., 2018, pp. 5642–5649.\n[32] M. Ma, J. Ren, L. Zhao, S. Tulyakov, C. Wu, and X. Peng, ‘‘SMIL:\nMultimodal learning with severely missing modality,’’ in Proc. Conf. AAAI\nArtif. Intell., 2021, pp. 2302–2310.\n[33] Y . Yin, S. Huang, and X. Zhang, ‘‘BM-NAS: Bilevel multimodal neural\narchitecture search,’’ 2021, arXiv:2104.09379.\n[34] Y .-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and\nR. Salakhutdinov, ‘‘Multimodal transformer for unaligned multimodal lan-\nguage sequences,’’ in Proc. Conf. Assoc. Comput. Linguist. Meet. (ACL),\nvol. 2019, pp. 6558–6569.\n[35] Z. Sun, P. Sarma, W. Sethares, and Y . Liang, ‘‘Learning relationships\nbetween text, audio, and video via deep canonical correlation for mul-\ntimodal language analysis,’’ in Proc. Conf. AAAI Artif. Intell., 2020,\npp. 8992–8999.\n[36] W. Rahman, M. K. Hasan, S. Lee, A. Bagher Zadeh, C. Mao,\nL.-P. Morency, and E. Hoque, ‘‘Integrating multimodal information in large\npretrained transformers,’’ in Proc. 58th Annu. Meeting Assoc. Comput.\nLinguistics, 2020, pp. 2359–2369.\n[37] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, and J. Davison, ‘‘Transformers: State-\nof-the-art natural language processing,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process. (EMNLP), 2020, pp. 38–45.\n[38] Y .-H. H. Tsai, P. P. Liang, A. Zadeh, L.-P. Morency, and R. Salakhutdinov,\n‘‘Learning factorized multimodal representations,’’ in Proc. Int. Conf.\nLearn. Represent. (ICLR), 2019.\n[39] K. Lin, L. Wang, and Z. Liu, ‘‘End-to-end human pose and mesh recon-\nstruction with transformers,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pat-\ntern Recognit. (CVPR), Jun. 2021, pp. 1954–1963.\n[40] J. J. Bird, M. Pritchard, A. Fratini, A. Ekart, and D. R. Faria, ‘‘Synthetic\nbiological signals machine-generated by GPT-2 improve the classiﬁcation\nof EEG and EMG through data augmentation,’’ IEEE Robot. Autom. Lett.,\nvol. 6, no. 2, pp. 3498–3504, Apr. 2021.\n29832 VOLUME 10, 2022\nK. Miyazawaet al.: Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models\n[41] H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y . Cui, and\nB. Gong, ‘‘V ATT: Transformers for multimodal self-supervised learning\nfrom raw video, audio and text,’’ in Proc. Adv. Neural Inf. Process. Syst.,\n2021.\n[42] H. Tan, J. Lei, T. Wolf, and M. Bansal, ‘‘VIMPAC: Video pre-\ntraining via masked token prediction and contrastive learning,’’ 2021,\narXiv:2106.11250.\n[43] D. Xing, J. Li, Y . Yang, and B. Xu, ‘‘General robot dynamics learning and\nGen2Real,’’ 2021, arXiv:2104.02402.\n[44] K. Lu, A. Grover, P. Abbeel, and I. Mordatch, ‘‘Pretrained transformers as\nuniversal computation engines,’’ 2021, arXiv:2103.05247.\nKAZUKI MIYAZAWA received the M.E. degree\nin engineering from The University of Electro-\nCommunications, Tokyo, Japan, in 2019. He is\ncurrently pursuing the Ph.D. degree with the\nDepartment of System Innovation, Graduate\nSchool of Engineering Science, Osaka University,\nOsaka, Japan. Since 2019, he has been a JSPS\nResearch Fellowship for Young Scientists. His\ncurrent research interests include multimodal data\nintegration, robot learning, concept formation,\nnatural language processing, and reinforcement learning.\nYUTA KYURAGI was born in Kariya, Aichi,\nJapan, in 1996. He received the B.S. degree\nin engineering science from Osaka University,\nOsaka, Japan, in 2020, where he is currently pur-\nsuing the M.S. degree in engineering science.\nTAKAYUKI NAGAI (Member, IEEE) received\nthe B.E., M.E., and Ph.D. degrees from the\nDepartment of Electrical Engineering, Keio Uni-\nversity, in 1993, 1995, and 1997, respectively.\nSince 1998, he has been with The University\nof Electro-Communications. From 2002 to 2003,\nhe was a Visiting Scholar at the Department of\nElectrical Computer Engineering, University of\nCalifornia at San Diego, San Diego, CA, USA.\nSince 2018, he has been a Professor with the Grad-\nuate School of Engineering Science, Osaka University. He currently works\nas a Specially-Appointed Professor at UEC AIX, a Visiting Researcher at the\nTamagawa University Brain Science Institute, and a Visiting Researcher at\nthe AIST AIRC. His research interests include intelligent robotics, cognitive\ndevelopmental robotics, and robot learning. He aims at realizing ﬂexible and\ngeneral intelligence like human by combining AI and robot technologies.\nHe has received the IROS Best Paper Award Finalist, the Advanced Robotics\nBest Paper Award, and the JSAI Best Paper Award.\nVOLUME 10, 2022 29833",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7669650316238403
    },
    {
      "name": "Transformer",
      "score": 0.7212988138198853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5590930581092834
    },
    {
      "name": "Encoder",
      "score": 0.5233854055404663
    },
    {
      "name": "The Internet",
      "score": 0.4435780644416809
    },
    {
      "name": "Machine learning",
      "score": 0.43591466546058655
    },
    {
      "name": "Speech recognition",
      "score": 0.35463976860046387
    },
    {
      "name": "Engineering",
      "score": 0.1257162094116211
    },
    {
      "name": "Voltage",
      "score": 0.11228248476982117
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}