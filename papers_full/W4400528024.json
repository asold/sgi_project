{
  "title": "Scaling Sequential Recommendation Models with Transformers",
  "url": "https://openalex.org/W4400528024",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Zivic, Pablo",
      "affiliations": [
        "MercadoLibre (Argentina)"
      ]
    },
    {
      "id": null,
      "name": "Vazquez, Hernan",
      "affiliations": [
        "MercadoLibre (Argentina)"
      ]
    },
    {
      "id": "https://openalex.org/A2314812539",
      "name": "SÃ¡nchez, Jorge",
      "affiliations": [
        "MercadoLibre (Argentina)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2072992969",
    "https://openalex.org/W2512971201",
    "https://openalex.org/W6733162650",
    "https://openalex.org/W4387849010",
    "https://openalex.org/W2990221466",
    "https://openalex.org/W2776933647",
    "https://openalex.org/W6823560597",
    "https://openalex.org/W4297971002",
    "https://openalex.org/W4367047145",
    "https://openalex.org/W3035287707",
    "https://openalex.org/W6600511658",
    "https://openalex.org/W6777295198",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6849071563",
    "https://openalex.org/W4285806565",
    "https://openalex.org/W2964044287",
    "https://openalex.org/W2009415795",
    "https://openalex.org/W6603510433",
    "https://openalex.org/W6600351811",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W3132147085",
    "https://openalex.org/W2390938437",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W6600639816",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W2469952266",
    "https://openalex.org/W2783272285",
    "https://openalex.org/W2745534091",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W4220894025",
    "https://openalex.org/W6600248585",
    "https://openalex.org/W6784376221",
    "https://openalex.org/W4384648324",
    "https://openalex.org/W6600388300",
    "https://openalex.org/W2966483207",
    "https://openalex.org/W3065542300",
    "https://openalex.org/W3100260481",
    "https://openalex.org/W91851626"
  ],
  "abstract": "Modeling user preferences has been mainly addressed by looking at users'\\ninteraction history with the different elements available in the system.\\nTailoring content to individual preferences based on historical data is the\\nmain goal of sequential recommendation.\\n The nature of the problem, as well as the good performance observed across\\nvarious domains, has motivated the use of the transformer architecture, which\\nhas proven effective in leveraging increasingly larger amounts of training data\\nwhen accompanied by an increase in the number of model parameters. This scaling\\nbehavior has brought a great deal of attention, as it provides valuable\\nguidance in the design and training of even larger models.\\n Taking inspiration from the scaling laws observed in training large language\\nmodels, we explore similar principles for sequential recommendation.\\n We use the full Amazon Product Data dataset, which has only been partially\\nexplored in other studies, and reveal scaling behaviors similar to those found\\nin language models. Compute-optimal training is possible but requires a careful\\nanalysis of the compute-performance trade-offs specific to the application.\\n We also show that performance scaling translates to downstream tasks by\\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\\napproach and findings provide a strategic roadmap for model training and\\ndeployment in real high-dimensional preference spaces, facilitating better\\ntraining and inference efficiency.\\n We hope this paper bridges the gap between the potential of transformers and\\nthe intrinsic complexities of high-dimensional sequential recommendation in\\nreal-world recommender systems.\\n Code and models can be found at https://github.com/mercadolibre/srt\\n",
  "full_text": "Scaling Sequential Recommendation Models with Transformers\nPablo Zivic\nMercado Libre Inc.\nBuenos Aires, Argentina\npablo.rzivic@mercadolibre.com\nHernan Vazquez\nMercado Libre Inc.\nBuenos Aires, Argentina\nhernan.vazquez@mercadolibre.com\nJorge SÃ¡nchez\nMercado Libre Inc.\nCÃ³rdoba, Argentina\njorge.sanchez@mercadolibre.com\nABSTRACT\nModeling user preferences has been mainly addressed by looking\nat usersâ€™ interaction history with the different elements available\nin the system. Tailoring content to individual preferences based on\nhistorical data is the main goal of sequential recommendation. The\nnature of the problem, as well as the good performance observed\nacross various domains, has motivated the use of the transformer\narchitecture, which has proven effective in leveraging increasingly\nlarger amounts of training data when accompanied by an increase in\nthe number of model parameters. This scaling behavior has brought\na great deal of attention, as it provides valuable guidance in the\ndesign and training of even larger models. Taking inspiration from\nthe scaling laws observed in training large language models, we ex-\nplore similar principles for sequential recommendation. Addressing\nscalability in this context requires special considerations as some\nparticularities of the problem depart from the language modeling\ncase. These particularities originate in the nature of the content\ncatalogs, which are significantly larger than the vocabularies used\nfor language and might change over time. In our case, we start\nfrom a well-known transformer-based model from the literature\nand make two crucial modifications. First, we pivot from the tradi-\ntional representation of catalog items as trainable embeddings to\nrepresentations computed with a trainable feature extractor, mak-\ning the parameter count independent of the number of items in the\ncatalog. Second, we propose a contrastive learning formulation that\nprovides us with a better representation of the catalog diversity.\nWe demonstrate that, under this setting, we can train our models\neffectively on increasingly larger datasets under a common experi-\nmental setup. We use the full Amazon Product Data dataset, which\nhas only been partially explored in other studies, and reveal scaling\nbehaviors similar to those found in language models. Compute-\noptimal training is possible but requires a careful analysis of the\ncompute-performance trade-offs specific to the application. We\nalso show that performance scaling translates to downstream tasks\nby fine-tuning larger pre-trained models on smaller task-specific\ndomains. Our approach and findings provide a strategic roadmap\nfor model training and deployment in real high-dimensional prefer-\nence spaces, facilitating better training and inference efficiency. We\nhope this paper bridges the gap between the potential of transform-\ners and the intrinsic complexities of high-dimensional sequential\nrecommendation in real-world recommender systems. Code and\nmodels can be found at https://github.com/mercadolibre/srt.\nThis work is licensed under a Creative Commons Attribution\nInternational 4.0 License.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657816\nCCS CONCEPTS\nâ€¢ Information systems â†’Personalization; Recommender\nsystems; Personalization;\nKEYWORDS\nSequential Recommendation, Scaling Laws, Transformers, Transfer\nLearning\nACM Reference Format:\nPablo Zivic, Hernan Vazquez, and Jorge SÃ¡nchez. 2024. Scaling Sequential\nRecommendation Models with Transformers. InProceedings of the 47th Inter-\nnational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™24), July 14â€“18, 2024, Washington, DC, USA. ACM, New\nYork, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657816\n1 INTRODUCTION\nA recommendation system aims to provide users with content that\nfits their preferences and interests. Classical methods have explored\nbuilding static models based on past user interactions to predict fu-\nture ones. Pattern mining and factorization-based methods are two\nclassical methodologies that stand as the most popular in the litera-\nture [10, 47]. These models seek to capture static preferences in the\ninteraction of users with items in a catalog. While the formulation\nlargely simplifies the modeling of otherwise complex interaction\npatterns observed in the real world, the main drawback of these\nmodels relies on the assumption of static behavior patterns. In real-\nity, user preferences are subject to a series of short- and long-term\nfactors that are very hard to disentangle [4, 41]. From a modeling\nperspective, user preference dynamics can be seen as latent factors\nthat govern the observed usersâ€™ behavior as they interact with the\nsystem. These interactions are diverse and depend on the nature\nof the actual system. For instance, the types of events that can be\nregistered in a music streaming platform differ from those observed\nin an e-commerce website. Despite the complexity of the task, the\ndriving hypothesis of modern recommendation systems is that such\nbehavioral patterns can be captured by models that can predict fu-\nture interactions from historical sequential records. The nature and\ncomplexity of such models have been influenced to a great extent by\nthe success of different machine-learning models in different fields,\nespecially those from the natural language literature. We can find\nsolutions based on simple Recurrent Neural Networks [ 16], con-\nvolutional architectures [51, 52], based on Attention mechanisms\n[29, 63] and, more recently, the Transformer [22, 48, 54]. Among\nthem, transformer-based solutions are the most promising. This\nis not only due to the success of this architecture in fields beyond\nlanguage modeling, such as computer vision [24], speech [27], and\ntime-series forecasting [32], but also to their flexibility and good\nscaling behavior. Another important factor of the transformer ar-\nchitecture that led to its adoption as the model of choice in many\napplications is the availability of a pre-fitted version that can be\narXiv:2412.07585v1  [cs.LG]  10 Dec 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Pablo Zivic, Hernan Vazquez, & Jorge SÃ¡nchez\nadapted easily to more specific tasks using a fraction of the data\nrequired to train a similar model from scratch. This pre-training\nand fine-tuning strategy has not yet been widely adopted in the se-\nquential recommendation literature. We believe that this obeys two\nmain reasons: first, the data used to train recommendation models\nis specific to each application domain, i.e. the nature of the catalog\nand type of events are problem-specific, making it challenging to\nleverage prefitted models on domains that might be closely related\nbut not the same as those on which the model has been trained (e.g.\na model pre-trained on Amazon data being fine-tuned to a different\ncatalog or e-commerce domain); second, there are particularities in\nthe sequential recommendation problem that constraint the design\nof solutions that scale.\nWhile predicting the next token in a sentence and the next item\nin an interaction sequence share structural similarities, the sequen-\ntial recommendation problem introduces some particularities that\nneed special attention. For instance, in language modeling, it is com-\nmon to cast the prediction task as a classification problem over a\nlarge set of tokens (subdivisions of words into finer sub-word units).\nAlthough large, the size of this vocabulary remains constrained\nto a manageable number (around 30K in most practical applica-\ntions) that does not change over time. On the contrary, most real\nrecommendation applications involve item sets (space of possible\nuser preferences) that expand to massive scales, often reaching into\nthe millions or even billions of different items [7]. Moreover, such\ncollections may change over time as items are constantly added\nand removed from the catalog. These characteristics impose design\nconstraints that must be satisfied if we are willing to take advan-\ntage of the flexibility and ease of adaptation observed by these\narchitectures in other domains.\nPerhaps the most intriguing characteristic of these models relies\non their ability to leverage increasingly large amounts of data by\nsimply growing the number of parameters accordingly. From a\nsystem design perspective, this poses new challenges around how\nto scale the amount of data and compute required by these models\nto leverage their full potential. From a practical perspective, this\nchoice is constrained not only by the desire to get the best possible\nperformance but also to achieve such performance within the limits\nof a given computational budget. In light of recent discoveries\nregarding scaling laws in the language [1, 23] and other domains\n[15, 43, 62], recent research has provided new insights into how\nmodel performance scales with the number of parameters, the size\nof the datasets used for training, and the required computational\nbudget [2, 17].\nIn this work, we explore the hypothesis that transformer-based\nsequential recommendation models exhibit scaling behaviors simi-\nlar to those observed in other domains. Under such a hypothesis, we\ninvestigate how, within a given computational budget, optimizing\nthe balance between model size and data size can yield improved\nresults. To do so, we propose a generic yet scalable model that takes\ninspiration from other transformer-based models from the litera-\nture but lets us experiment with problems and models of different\ncomplexity. We run experiments on the full version of the widely\nused Amazon Product Data (APD) dataset [37]. Our findings con-\nfirm our hypothesis, and we show how such scaling behavior can\nbe used in practice by training larger models that, when fine-tuned,\nachieve a performance that surpasses more complex approaches\nfrom the literature. Our main contributions are the following:\nâ€¢We propose a generic transformer-based architecture that is\nboth flexible and scalable.\nâ€¢We show scaling laws similar to those observed in language\nmodeling tasks.\nâ€¢We show that it is possible to pre-train recommendation\nmodels at scale and fine-tune them to particular downstream\ntasks, improving performance w.r.t to similar models trained\nfrom scratch.\nThe paper is organized as follows: Sec. 2 discusses some key\naspects of sequential recommendation models in the context of\nscalability, Sec. 3 proposes a formulation that makes the model\nindependent of the size of the catalog, Sec. 4 shows experimental\nresults. In Sec. 4.4 we derive analytical laws that relate the target\nmetric with the most relevant quantities of interest from a scaling\nperspective. In Sec. 4.5 we show we can use the pre-training and fine-\ntuning strategy for improving recommendations. Sec. 5 discusses\nrelated work. Finally, in Sec. 6 we draw some conclusions.\n2 SCALABILITY OF SEQUENTIAL\nRECOMMENDATION MODELS\nSequential recommendation models seek to capture user interac-\ntion patterns and a possibly large collection of available items in a\ncatalog. Users may perform many interaction types depending on\nthe nature of such elements: an e-commerce site, a music streaming\nservice, a social network, and others. For instance, users might play\na song, skip it, or add it to a playlist in a music recommendation\ncontext, while they can add an article to a shopping cart, buy it,\nadd it to a wish list, etc. Although such heterogeneity in the type of\ninteractions can be handled accordingly [21, 38, 58], for the purpose\nof this study, we subsume all domain-specific cases into a more\ngeneric \"user-item\" interaction (i.e. a user interacted with an item\nin some way). With this in mind, let ğ‘ˆ denote the user base of\na given platform and ğ¼ the collection of items they can interact\nwith. The behavior and preference dynamics of a user ğ‘¢ âˆˆğ‘ˆ can\nbe considered as embedded into the sequence of items they inter-\nacted with for a given period. Let ğ‘†ğ‘¢ = {ğ‘–ğ‘¢\n1 ,ğ‘–ğ‘¢\n2 ,...ğ‘– ğ‘¢ğ‘›}be such a\nsequence, where ğ‘–ğ‘¢\nğ‘˜ denotes the ğ‘˜-th item user ğ‘¢ interacted with.\nIn this context, a recommendation model can be thought of as a\nfunction ğ‘“ğœƒ, parameterized by ğœƒ, that takes as input the interaction\nhistory encoded by ğ‘†ğ‘¢ and seeks to predict the item or items that\nuser ğ‘¢will interact with in the future.\nGiven the sequential nature of the problem, we consider models\nbased on the transformer architecture [28]. These models, initially\nproposed in the context of language modeling tasks, have proven ef-\nfective in various domains. From a scaling perspective, and similarly\nto what happens in the natural language case, we have two clear\ndimensions that affect their scaling behavior: the number of model\nparameters, ğ‘, and the number of user-item interactions seen dur-\ning training. However, as we will see next, analyzing scalability in\nmost recommender systems proposed in the literature would also\nrequire considering the number of available items in the catalog,|ğ¼|.\nThis dependency originates in the way most transformer-based ap-\nproaches cast the sequential prediction task. In a direct translation\nScaling Sequential Recommendation Models with Transformers SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nof the next-word prediction used to train language models, recom-\nmendation transformers treat the elements of the input sequence\n(items from the catalog) as \"tokens\", i.e. atomic elements whose co-\noccurrence patterns we try to learn from data. As in the language\nmodeling case, the model is asked to learn the interaction patterns\nbetween sequences of such atoms and also a representation (embed-\nding) that encodes some intrinsic aspect of each such element. This\nimplies that, besides the actual number of parameters in the model\ndevoted to sequential prediction, we also need to store a number of\nvector embeddings equal to the catalog size. While in the case of\nlanguage the vocabulary size is relatively small, ranging between\nthe tens to a few hundred thousand elements, the size of the catalog\nfor a real-world recommendation model can grow dramatically, po-\ntentially reaching into the billions [7]. In such cases, the number of\nparameters associated with the matrix of trainable item embeddings\nquickly dominates the total parameter count. Such a dependency\nbetween model complexity and the size of the catalog is depicted in\nFig. 1. In the bottom panel of the figure, we show an estimate of the\n100 101 102 103 104 105 106 107\n# of users\n101\n104\n107\n# of items\n100 101 102 103 104 105 106 107\n# of interactions\n101\n104\n107\n# of items\n102 103 104 105 106 107 108\n# items\n104\n107\n1010\n# parameters\n33K 4M 33M 1B 19B\nFigure 1: Increase in the catalog size induced by an increase\nin the number of active users (top) and interactions (middle).\nIncreasing the number of items increases the total parameter\ncount (bottom) in models that use trainable item embeddings\nfor 64-dimensional embeddings. For smaller models (33K\nparameters), the catalog size dominates the total parameter\ncount, while for larger models (19B parameters), the total\nnumber of trainable parameters remains stable across a wide\nspectrum of catalog sizes.\ntotal number of parameters for models of different sizes (expressed\nin terms of the total number of parameters, ğ‘, as a function of the\nsize of the catalog, |ğ¼|. We observe that for models with a small\ncomplexity (33K and 4M parameters), the total parameter count\ngrows linearly with the size of the catalog after a relatively short\nnearly flat initial regime. For larger models ( > 1ğµ), on the other\nhand, the total parameter count remains stable across a wide range\nof |ğ¼|, growing only for extremely large values of |ğ¼|.\nThis phenomenon makes the analysis difficult since, by increas-\ning the number of training sequences, we would implicitly increase\nthe number of items interacted with by users, which would, in turn,\nincrease the number of parameters of the model. This growth is\nnot deterministic and depends on the diversity of items observed in\nthe pool of sequences used for training. This behavior is illustrated\nin the first two panels of Fig. 1, where we show the growth in the\nnumber of visited catalog items induced by an increase in the num-\nber of active users (top) and number of navigation data (middle).\nMoreover, building a solution based on learning item embeddings\nworsens the cold-start problem observed in real systems [11, 31].\nIn the next section, we reformulate the standard transformer-\nbased approach to break this dependency. In this way, the complex-\nity of the model becomes independent of |ğ¼|and we can analyze\nthe scaling behavior w.r.t the variables of interest (parameter count\nand training set size) concisely.\n3 A SCALABLE RECOMMENDATION\nFRAMEWORK\nWe take SASRec [22] as our reference model. SASRec is a transformer-\nbased architecture that has shown competitive performance on\nseveral sequential recommendation tasks [30] and is regarded as\na strong baseline in more recent evaluations [39]. In SASRec, the\nmodel takes as input a sequence of user-item interactions of length\nğ‘›and seeks to predict the item the user will interact with next. The\nsequence is fed into a transformer model of ğ¿layers to produce an\noutput embedding that matches a representation of the following\nitem in the sequence. The output might also include a classification\nlayer over the items in the catalog that induces additional complex-\nity [48]. Each item in the catalog is encoded as a trainable vector\nrepresentation of size ğ·, resulting in a total of |ğ¼|Ã— ğ· trainable\nparameters. As mentioned above, this dependency between the\nnumber of trainable parameters and the catalog size makes the\nanalysis of scaling behaviors difficult due to the interplay between\nğ‘ and |ğ¼|.\nFigure 2 (left) illustrates this scenario, where both the input and\noutput sequences correspond (to indices) to items in the catalog.\nFor large catalogs, the parameter count is dominated by the matrix\nof input embeddings, the output prediction layer(s), or both. Given\nthese observations, instead of considering the learning problem as\na classification one, we propose reformulating it as an embedding\nregression task [50], as follows. Given a user navigation sequence\nğ‘†ğ‘¢ = {ğ‘–ğ‘¢\n1 ,ğ‘–ğ‘¢\n2 ,...ğ‘– ğ‘¢ğ‘›}, we assume we can compute a ğ·-dimensional\nvector representation for each item in the sequence. To compute\nsuch representations, we rely on a parametric mappingğœ™ : ğ¼ â†’Rğ·.\nWe denote as ğœ™ğ‘¢\nğ‘˜ â‰¡ğœ™(ğ‘–ğ‘¢\nğ‘˜)the representation of ğ‘˜-th item user ğ‘¢\ninteracted with. These embeddings are user-independent in that\nthe representation for a given item is the same irrespective of how\nthe user might interact with it. We compute these representations\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Pablo Zivic, Hernan Vazquez, & Jorge SÃ¡nchez\ni4'\nTransformer encoder\nClassification layer\ni1 i2 i3 i4\ninput sequence\n{\ntarget\nEmbeddings LUT\noutput\n(item index)\n{\ni4'\nTransformer encoder\ni1 i2 i3 i4\ninput sequence\n{\nFeature encoder\nÏ•1 Ï•2 Ï•3 Ï•4\noutput\n(embedding)\ntarget\nÏ•( )\nÏ•'4\n{\nFigure 2: Traditional transformer-based recommendation\nmodels (left) learn vector embeddings for all items in the\ncatalog and access them via table lookups (LUT). The system\nmight also include a classification layer aimed at predicting\nthe index of the target item. On the contrary, we propose to\nuse a fixed (and task-agnostic) feature extractor to encode\nthe items in the catalog (right) and to predict item-to-item\nsimilarities using the output embeddings of the transformer\nmodel. We use the last element of the sequence as target\nand try to match its id (classification) or input embedding\n(regression).\non the fly and train the full model (including the feature extraction\nmodel ğœ™) to pick among (a subset of) them the one that corresponds\nto the target item for an input sequence ğ‘†ğ‘¢. This is illustrated in\nFigure 2 (right), where we have replaced the embedding layer with\na feature extractor that computes the item embeddings ( ğœ™ğ‘–,ğ‘– =\n1,.., 4) that feed the model. The output of this model (ğœ™â€²\n4) is used for\nprediction (and learning) by comparing similarities with the items\nin the catalog (their embeddings). This feature-based approach has\nshown good performance in the literature [40, 64]. In this case, the\nnumber of parameters associated with the computation of item\nembeddings is given by the number of trainable parameters in the\nfeature extraction module ğœ™, irrespective of the number of items in\nthe catalog.\nWe train our model autoregressively as follows. Given a training\nset of user navigation sequences of lengthğ‘›, we ask the model to pre-\ndict each element of any given sequence based on the (sub)sequence\nof previous interactions. We optimize the following loss:\nL(ğœƒ; S)=\nâˆ‘ï¸\nğ‘†âˆˆS\nâ„“(ğœƒ; ğ‘†). (1)\nHere, we omitted the superscript ğ‘¢ for the sake of clarity 1. Let\nSdenote the set of all subsequences with a length of at least 2\ninteractions. Let us denote byËœğ‘† = {ğ‘–1,...,ğ‘– ğ‘›âˆ’1}the partial sequence\ncontaining the first ğ‘›âˆ’1 items of ğ‘†. Our goal is to train a model\nthat, based on Ëœğ‘†, can rank the target ğ‘–ğ‘› as high as possible when\ncompared to other candidates from the catalog. We adopt sampled\n1Also, the information we are willing to capture relates to the preferences of users and\nthe way they interact with the system, and not on their particular identities.\nSoftmax [57] as our choice for â„“:\nâ„“(ğœƒ; ğ‘†)= âˆ’log\nexp\n\u0010\nğ‘“ğœƒ(Ëœğ‘†)ğ‘‡ğœ™ğ‘›/ğœ\n\u0011\nexp\n\u0010\nğ‘“ğœƒ(Ëœğ‘†)ğ‘‡ğœ™ğ‘›/ğœ\n\u0011\n+Ã\nğœ™âˆˆN( Ëœğ‘†)exp\n\u0010\nğ‘“ğœƒ(Ëœğ‘†)ğ‘‡ğœ™/ğœ\n\u0011 .\n(2)\nHere, ğ‘“ğœƒ denotes the model we are trying to fit, ğœ™ğ‘› the repre-\nsentation of the target item ğ‘–ğ‘› computed by the feature extraction\nmodule, ğœ is a temperature parameter that controls the softness of\nthe positive and negative interactions, and N(Ëœğ‘†)âŠ‚ ğ¼ is a set of neg-\natives whose cardinality is to be set. In practice, we apply the logQ\ncorrection proposed in [60] to the logits in Eq. (2) to correct for the\nbias introduced by the negative sampling distribution. In the rest\nof the paper, we refer to our transformer-based model and learning\nformulation as Scalable Recommendation Transformer (SRT) .\nThe framework introduced above is motivated by the need to\nset up a competitive yet simple baseline that scales well w.r.t the\nquantities we identified as the most relevant from a scaling perspec-\ntive, namely the number of trainable parameters and the number\nof samples (or interactions) observed during training.\nEq. (2) can be seen as an approximation to a cross-entropy loss\nover the items in the catalog, where we contrast against a subset of\nthe possible items. This corresponds to a generalization of the loss\nused in SASRec or BERT4Rec, whereN(Ëœğ‘†)is constrained to a single\nsample draw at random. Note that by drawing samples at random,\nwe take the risk of contrasting against uninformative samples that\nare easily distinguishable from the positive ones. On the other\nhand, if we choose an elaborate negative sampling methodology,\nwe might end up adding a non-negligible computation overhead\nto an otherwise simple model. Moreover, sampling hard negatives\nmight induce biases that correlate with the catalog size [33], adding\na degree of variability that is difficult to isolate. In our case, we opt\nto sample negatives from the item popularity distribution (i.e. items\nwith which users interacted the most are sampled more frequently).\nThis strategy is competitive and has a small footprint on the overall\ncomputations. From now on, we denote our models as SRT-X, where\nX is the number of negatives used to compute the loss in Eq. (2).\nBesides the advantages of the proposed formulation regarding\nscalability, an additional advantage of our model is the ability to\nwork with non-static catalogs. Adding and deleting items dynami-\ncally from a catalog (due to policy infringements, product stockout,\nnew trends, outdated information, etc) is commonplace in most\npractical applications. Building sequential recommendation models\nbased on fixed item sets brings many concerns regarding the us-\nability and maintainability of the system over time. These concerns\nmight hinder a wider adoption of these types of approaches.\nBefore delving deeper into scalability, which is the primary goal\nof our work, we first show that our formulation achieves compet-\nitive performance compared to other transformer-based formula-\ntions. Table 1 compares the performance of our model against other\npopular methods from the literature on the Beauty and Sports sub-\nsets of the Amazon Review Data [ 35] benchmark. Details of the\ndataset, metrics, and evaluation protocols are provided in Sec. 4.\nWe consider different versions of our model trained using 10, 100,\n300, and 1K negatives, respectively, and compare them against the\npopular SASRec [22] and BERT4Rec [48] models. For these models,\nwe show the metrics reported by Chen et al. [5] for compatibility\nScaling Sequential Recommendation Models with Transformers SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 1: Performance comparison between two reference\nmodels and our formulation under the NDCG@5 metric for\nthe Amazon Beauty and Sports datasets.\nBeauty Sports\nBERT4Rec [48] 0.0219 0.0143\nSASRec [22] 0.0241 0.0135\nSASRec-CE 0.0314 0.0170\nSRT-10 0.0235 0.0138\nSRT-100 0.0318 0.0171\nSRT-300 0.0340 0.0180\nSRT-1K 0.0366 0.0195\nof evaluation methodology. We also report a variation of SASRec\nin which we replaced the binary cross-entropy with a full cross-\nentropy loss over the items in the catalog. We name this variant as\nSASRec-CE. There are several observations to be made. First, we see\nthat our best model outperforms all other alternatives, even when,\nat its core, the underlying models are very similar. Second, the\nnegative sampling strategy is crucial in getting good performance.\nThis is interesting since we can see the multiclass cross-entropy\nloss as a measure that contrasts each positive against all negatives\n(full catalog) and, in that sense, can be seen as a limiting case for our\ncontrastive formulation. Increasing the number of samples beyond\nthis value becomes impractical as it involves computing embeddings\nfor additional ğ‘‹ğµ samples, with ğµ the size of the training batch.\nWe believe these results validate the overall formulation and set a\nstrong baseline model for scalability analysis.\n4 EXPERIMENTS\nThis section discusses our experimental setup in the context of\nstandard practices observed in the literature. We then show and\ndiscuss results on scalability and optimal compute allocation. Fi-\nnally, we show fine-tuning results that compete favorably with\nother methods from the literature.\n4.1 Evaluation Protocol\nWe ran experiments on the Amazon Product Data (APD) dataset\n[14, 35], a large dataset of product reviews crawled from Amazon\nbetween 1996 and 2014. The dataset consists of82.7 million reviews\nover 9.9 million different products written by more than 21 million\nusers. Reviews in this dataset correspond to a subset of all pur-\nchases made in the platform during the relevant time span. Due to\nits size, a common practice in the literature consists of using smaller\nsubsets of the data. For instance, \"Amazon Beauty\" corresponds to\nthe subset of samples where users bought (and reviewed) an item\nfrom the \"beauty\" category. To avoid issues related to cold-start\n[31], it is common to filter out users and products with less than five\npurchases. The remaining data is called a \"5-core\" dataset. These\ntwo procedures (per-category and 5-core filtering) distort or hide\nsome of the intrinsic characteristics of real-world recommenda-\ntion problems. For instance, if we consider the \"beauty\" category,\nonly 8.7% of the interactions originate from items with at least 5\npurchases/reviews. This is not only a matter of scale (with most\nTable 2: Statistics for the full and 5-core trimmed versions of\nAmazon Product Data and the Beauty and Sports categories.\nAPD Beauty Sports\nraw 5-core raw 5-core\n# interactions 82.7M 2.3M 198.5K 2.5M 296.3K\n# users 21.2M 22.4K 22.4K 35.6K 35.6K\n# items 9.9M 937.9K 12.1K 993.6K 18.4K\n# iter/item (avg) 8.4 2.4 16.4 2.5 16.1\n# iter/user (avg) 3.9 101.9 8.9 69.9 8.32\ndata being discarded) but a problem of deceiving evaluation, as\nresults reported on these datasets do not necessarily extrapolate\nto actual real systems. Table 2 provides dataset statistics for the\nentire dataset, two common subsets used in the literature, and their\n5-core trimmed versions. There are an order of magnitude fewer\ninteractions and items in the5-core version of the dataset compared\nto their full counterpart. Figure 3 shows the distribution of reviews\nper item for the beauty subset of APD for both the full and 5-core\nversions. As the figure shows, trimming the dataset reshapes the\noriginal problem into modeling a long-tail phenomenon, disregard-\ning the rich and more relevant aspects of real user interactions.\n100 101 102 103\nNumber of reviews\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Relative frequency\n5-core beauty\nFull beauty\nFigure 3: Distribution of the number of reviews per item in\nthe Amazon beauty dataset for the full and 5-core versions.\nSimilar behaviors are observed in Amazon sports.\nFrom the above and to analyze the scaling behavior of transformer-\nbased recommendation models, we propose a simple strategy to\ngenerate datasets of different sizes with the same distributional in-\nformation as the original (average sequence length, diversity, etc.).\nThe strategy consists of sampling users at random and recording\ntheir interactions into a single dataset. Note that by increasing the\nnumber of active users, we account for a more extensive set of items\nthey interact with (see Fig. 1). We follow standard practice and take\nthe last item from each sequence as the target for evaluation (test),\nthe item at position ğ‘›âˆ’1 for validation, and leave the first ğ‘›âˆ’2\nelements for training.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Pablo Zivic, Hernan Vazquez, & Jorge SÃ¡nchez\nTable 3: Architecture specification. Each column represents\na different model parameterized by the number of layers,\nğ‘›ğ¿, number of attention heads, ğ‘›ğ», and number of hidden\nembedding dimensions, ğ‘‘.\nğ‘›ğ¿ 24 16 8 8 8 4 2 4\nğ‘›ğ» 4 4 4 4 2 2 2 2\nğ‘‘ 256 256 256 128 128 128 128 64\nDue to the long-tail behavior of item categories, we expect larger\ndatasets to account also for larger catalogs. This poses a challenge\nfrom an evaluation perspective since computing retrieval metrics\nover the whole catalog becomes infeasible. We sample a set of\nrandom negatives for each positive, as in [22]. In our case, however,\nwe sample 10K negatives for each positive instead of only 100\nas in [22]. By doing so, we increase the probability of sampling\nnegatives, which are more challenging to discriminate w.r.t the\nground truth, while providing us with a closer approximation to\nthe full-catalog case. In our experiments, we chose the NDCG@5\nscore as a performance metric as it is one of the most common\nmetrics reported in the literature.\nFollowing [17, 23], we use the number of FLOPs as a proxy for\nthe amount of compute required to achieve a given performance for\ndifferent choices of model complexity, number of training samples,\nand the catalog size.\n4.2 Model Design and Training Algorithm\nWe adapt SASRec as outlined in Sec. 3, replacing the item embed-\nding matrix with a trainable feature encoder whose complexity\nis independent of the size of the catalog. Concretely, we take the\ntitle and brand of each product and tokenize them into a vocabu-\nlary of 30ğ‘˜ tokens with the SentencePiece tokenizer [26]. This way,\nwe replace the variable-sized item embedding matrix with a fixed\nmatrix of token embeddings. As shown in Table 1, these changes\nlead to comparable performance in standard benchmarks. Based on\nthis architecture, we consider different model complexities param-\neterized by the number of layers, ğ‘›ğ¿, number of attention heads\nper layer, ğ‘›ğ», and hidden embedding dimensionality, ğ‘‘. Table 3\ndetails the different combinations of these parameters we used in\nour experiments. To train our models, we use the Adam optimizer\nand a one-cycle learning rate policy consisting of a linear warm-up\nstage and a cosine decay after one-third of the total iterations. We\nset the base learning rate to 1ğ‘’ âˆ’4 and the number of epochs to\n50. We use gradient clipping (set to 1) and a weight decay factor of\n1ğ‘’âˆ’5. We base our implementations on the RecBole library [65].\n4.3 Scaling Model and Dataset Sizes for Optimal\nCompute\nIn this section, we explore the relationship between the target met-\nric and the compute resource requirements induced by different\ncombinations of model sizes and number of training interactions.\nOur evaluation differs from similar studies [6, 17, 23] in two main\naspects: first, we focus on a task-specific metric instead of a more\ngeneric loss; second, we train our models over multiple epochs,\nthus revisiting the same training sequences multiple times during\nthe training process. These differences originate from the particu-\nlarities of the sequential recommendation problem. We also focus\non a performance metric that is closely tied to the actual recom-\nmendation task (NDCG vs loss as in the language modeling case)\nand which is more informative from a practical standpoint.\nFigure 4 shows the target metric as a function of the number of\nFLOPs for different training runs obtained with different combina-\ntions in the number of model parameters and size of the training\nset. The left and right plots show the same runs but use a different\ncolor encoding to highlight different aspects of these runs. On the\nleft, the colors encode the number of interactions processed in each\ntraining subset. This value ranges from 80ğ¾ to 8.2ğ‘€. On the right,\nthe colors encode the number of non-embedding parameters in\neach model. We decided to plot this number instead of the total\nparameter count for the following reasons: first, the number of\ntoken embeddings is constant across the different runs; second,\nnon-embedding parameters (parameters of the transformer model)\nare responsible for capturing the sequential dependencies that are\nunique to our problem. The number of non-embedding parameters\nranges between 10.2ğ¾and 9.6ğ‘€. The figures show an improvement\nin the target metric as larger models or bigger training sets are used.\nIn smaller models, increasing the amount of training data reaches\na point where performance saturates. Such data regimens are only\nuseful if they come accompanied by an increase in the number of\nparameters in the model. From these runs, we also extracted the\nenvelope of maximal performance (i.e. the points that observe the\nbest NDCG score) among all the configurations that require the\nsame amount of FLOPs. This envelope is highlighted in blue in the\nleft panel of Figure 4. From these points, we build scaling plots in\nFigure 5. The plot on the left shows the number of seen interactions\nas a function of the number of FLOPs. The number of seen inter-\nactions is the number of interactions in the dataset used to train\nthe model, multiplied by the number of epochs required to reach\nthe point of maximal performance by the current configuration.\nWe chose this quantity since we work on a multi-epoch setting\nwhere the optimal point for each FLOP count results from a model\ntrained by a given number of epochs using a dataset with sequences\nof varying lengths. The panel on the right shows the number of\nnon-embedding parameters as a function of the number of FLOPs\nfor the points in the envelope. In both cases, color encodes the value\nof the NDCG@5 score.\nFrom these plots, we see a trend in that increasing the num-\nber of parameters in the model or the size of the dataset led to\nhigher performance scores. Smaller models do not exploit the more\nsignificant variability observed in larger datasets. Reaching good\nperformance by adding more data requires models with the flexibil-\nity to deal with this added complexity. In this case, however, it is not\neasy to disentangle the effect of increasing either of these factors.\nIf we look at the figure on the right, we observe that a subset of\nmodels achieves different degrees of performance according to the\nresources devoted to training them (larger datasets or more train-\ning iterations). We believe, however, that identifying the scaling\nbehavior brings valuable insights that allow us to extrapolate to\nnovel data and model complexity regimes.\nScaling Sequential Recommendation Models with Transformers SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n1012 1013 1014 1015 1016 1017\nFLOPs\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12NDCG @ 5\n107\n108\nD\n1012 1013 1014 1015 1016 1017\nFLOPs\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12NDCG @ 5\n107\nN\nFigure 4: NDCG@5 vs FLOPs for different runs with different training set sizes and model complexities. The colormap of each\nplot encodes the number of training interactions (left) and the number of non-embedding parameters (right).\n1010 1012 1014 1016 1018\nFLOPs\n105\n106\n107\n108\n109\n# seen interactions\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nNDCG@5\n1010 1012 1014 1016 1018\nFLOPs\n103\n104\n105\n106\n107\n# non-embedding parameters\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nNDCG@5\nFigure 5: Number of seen iterations (left) and number of non-embedding parameters as a function of the FLOP count for the\npoints of maximal performance. Color encodes the NDCG@5 score of each configuration. The dotted red lines show linear fit\ncurves of the corresponding point cloud in log-log space.\n4.4 Estimating Model Performance\nBased on the data obtained in our experiments, we present two\nformulations for estimating the expected performance in terms of\nthe target metric for recommendation. These models aim at asking\nthe following questions: a) for a fixed FLOP budget, is it possible\nto get an estimate of the maximum achievable performance? and\nb) for a given model and dataset size, is it possible to estimate the\nexpected maximum NDCG for that configuration? In the first case,\nwe assume there exists an \"oracle\" that selects the optimal model\nand dataset configuration.\n4.4.1 Estimating NDCG from a fixed FLOPs budget.In the experi-\nments, we recorded the maximum NDCG achieved for each FLOP\nbudget. Figure 6 shows such points together with linear and sig-\nmoidal fits. The figure shows a more complex relationship between\nNDCG and FLOPs in log space than the linear scaling behavior\nobserved in other studies. In our case, we observe the beginning\nof an asymptotic trend for the maximum achievable NDCG. This\nbehavior could be due to many factors, including the saturation\nof the target metric due to challenges intrinsic to the particular\nrecommendation problem (recommendation over broad item cate-\ngories, representation ambiguity in the item embeddings, etc). In\nthis case, a sigmoidal fit appears more appropriate, in which case it\ncorresponds to:\nNDCG(ğ¹ğ¿ğ‘‚ğ‘ƒğ‘ )â‰œ 0.396\n1 +ğ‘’âˆ’0.18(log(ğ¹ğ¿ğ‘‚ğ‘ƒğ‘  )âˆ’24.44) âˆ’0.247. (3)\nThis function reveals that as the FLOPs budget increases, the NDCG\napproaches an upper limit estimated at 0.149 (0.396-0.247), high-\nlighting the diminishing returns of increasing the computational\nbudget. We can identify the point where this diminishing return\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Pablo Zivic, Hernan Vazquez, & Jorge SÃ¡nchez\n1012 1013 1014 1015 1016\nFLOPS\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12NDCG@5\nNDCG envelope\nSigmoid fit\nLinear fit\nFigure 6: Relationship between the highest NDCG@5 values\nachieved at different logarithmically scaled FLOPs budgets.\nIt includes two fitted parametric functions: a linear function\nand a sigmoidal function, showcasing the asymptotic trend\nof NDCG@5 as computational resources increase.\nstarts at log(FLOPs)= 30.7, which corresponds to approximately\n2.15 Ã—10âˆ’13 FLOPs. At this point, performance reaches a maxi-\nmum estimated value of 0.0525 (0.155/2). Identifying the point of\ndiminishing returns is essential in optimizing resource allocation\nin real-world scenarios.\n4.4.2 Estimating NDCG for a given model and dataset size.Here,\nwe model the maximum achievable NDCG as a function of the\ntotal parameter count and the size of the dataset, as measured by\nthe number of seen interactions. The goal is to find a parametric\nfunction that captures the underlying relationship between the\nmodelâ€™s complexity, data size, and final task performance. This\ninvolves identifying key parameters that influence the expected\nrisk and then quantifying their impact on the modelâ€™s effectiveness,\nas measured by the NDCG score. We follow a risk decomposition\napproach and propose the following functional form similar to [17]:\nNDCG(ğ‘,ğ‘‡ )â‰œ ğ¸âˆ’ ğ´\nğ‘ğ›¼ âˆ’ ğµ\nğ‘‡ğ›½. (4)\nHere, ğ‘ denotes the total parameter count and ğ‘‡ number of user-\nitem interactions. We use a subtractive formulation to account for a\ntarget metric maximization law, instead of a loss minimization as in\n[17]. The chosen parametric form allows us to outline how changes\nin the number of parameters and the size of the dataset systemati-\ncally affect the modelâ€™s ability to rank items accurately. To fit the\nmodel, we use a non-linear least squares approach and constrain\nthe model coefficients to be non-negatives to avoid nonsensical\nsolutions. We obtain the following solution:\nNDCG(ğ‘,ğ‘‡ )â‰œ 0.163 âˆ’ 18.56\nğ‘0.376 âˆ’ 2.9\nğ‘‡0.364 . (5)\nFrom the above equation, we can interpret ğ¸ as the maximum\nexpected value for the NDCG@5 score, in which case reaches a\nvalue of 0.163. We also observe a similar value for the exponents\nfor both ğ‘ and ğ‘‡, suggesting that both data and parameters behave\nsimilarly from a scaling perspective. Figure 7 shows the predicted\nNDCG@5 score as a function of the number of model parameters,\nğ‘, and number of seen interactions.\n106 107\nN\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14NDCG@5\n80K\n408K\n813K\n4M\n8M\n# interactions\nFigure 7: A view of the resulting parametric function from\napplying risk decomposition to NDCG values achieved at\ndifferent model complexities (N) and size of the datasets as\ngiven by the number of user-item interactions.\n4.5 Transferability\nIn this section, we evaluate the transfer ability of some of our larger\npre-trained models by fine-tuning them in the Amazon beauty and\nsports subsets. This is a widely used strategy in the literature but\nhas seen lesser popularity in the context of sequential recommenda-\ntion. This is because, unlike the language and vision domains, the\ndata used to train such models are particular to each recommenda-\ntion domain (i.e. the nature of the catalog), and the type of events\nbeing recorded changes from case to case. Nevertheless, training\nmore generic models at scale and fine-tuning them to different\ndownstream tasks poses the same advantages observed in other\ndomains, such as improvements in the final performance, shorter\ndevelopment cycles, improvements in the backbone model translate\neffortlessly to improvements in downstream performance, etc. We\nshow that is it possible to pre-train and fine-tune recommendation\nmodels and that, by doing so, we obtain performance improvements\nthat could not be achieved by training similar models from scratch.\nOur adaptation strategy is as follows. Given a pre-trained model2,\nwe apply a progressive fine-tuning strategy that consisting of pro-\ngressively unfreezing layer by layer, tuning them for 10 epochs\nusing a learning rate of 1 Ã—10âˆ’4 and a one-cycle cosine sched-\nule. Once all transformer layers have been unfreeze, we unfreeze\nthe token embedding layer and train for an additional 50 epochs.\nTo avoid over-adaptation and catastrophic forgetting, we use the\nElastic Weight Consolidation (EWC) formulation of [25] which has\n2To avoid data leakage, we removed the beauty and sports subsets from our pre-training\ndatasets.\nScaling Sequential Recommendation Models with Transformers SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 4: Performance comparison of our models with other\nmethods from the literature.\nBeauty Sports\nNDCG@5 HIT@5 NDCG@5 HIT@5\nMINCE [40] 0.0378 0.0523 0.0196 0.0274\nICLRec [5] 0.0324 0.0493 0.0182 0.0283\nCoSeRec [33] 0.0361 0.0537 0.0196 0.0287\nCL4SRec [59] 0.0208 0.0396 0.0116 0.0219\nSRT-1K (from scratch) 0.0360 0.0611 0.0192 0.0313\nSRT-1K (fine-tuned) 0.0405 0.0645 0.0206 0.0344\nbeen proven to be effective in ranking contexts [34]. For EWC, we\nuse ğœ†= 100.\nTable 4 compares the results of the fine-tuned model, a similar\nmodel trained from scratch, and the following models from the\nliterature: MINCE [ 40], ICLRec [ 5], CoSeRec [ 33], and CL4SRec\n[59]. Results are reported on the beauty and sports subsets of APD.\nWe report NDCG@5 and HIT@5 scores. Table 4 shows that our\nfine-tuned variants outperform all alternatives by a margin. Inter-\nestingly, training from scratch on these datasets competes favorably\nwith more alternatives from the literature, showing that despite its\nsimplicity, our approach serves as a strong baseline for evaluations.\nIf we look at the NDCG@5 score, we see an improvement of more\nthan 12 and 7% for the Beauty and Sports subsets, respectively, for\nthe fine-tuned variant w.r.t to the models trained from scratch.\n5 RELATED WORK\nSequential recommendation is a branch of recommendation sys-\ntems, an area that recognizes the importance of sequential behavior\nin learning and discovering user preferences [ 56]. Initial models\nused the Markov Chain framework for anticipating user activi-\nties [12, 13, 42]. With advancements in deep learning, innovative\napproaches have emerged, such as employing Recurrent Neural\nNetworks (RNN) [ 16], attention mechanisms [ 49], and Memory\nnetworks [20]. The disruption introduced by transformer archi-\ntecture [54] led to significant progress, giving rise to well-known\napproaches like SASRec [22] and BERT4Rec [48].Despite their suc-\ncess, these methods face a substantial limitation in scaling. They\nrely on item IDs to represent the sequence of interactions, which\npresents several scalability issues [8]. First, the pure ID indexing\nof users and items is inherently discrete and fails to impart ad-\nequate semantic information to new items. Second, adding new\nitems requires modifications to the modelâ€™s vocabulary and param-\neters, causing transformer-based methods to scale poorly with an\nincrease in the item count, which is crucial for many real-world\nrecommendation systems.\nA viable solution to the constraints of ID-based recommender\nsystems is to integrate textual information such as item titles, de-\nscriptions, and user reviews. The UniSRec model exemplifies this\nby deriving adaptable representations from item descriptions [19].\nText-based Collaborative Filtering (TCF) with Large Language Mod-\nels like GPT-3 has demonstrated potential superiority over ID-based\nsystems. Nevertheless, the overreliance on text prompted the devel-\nopment of VQ-Rec, which utilizes vector-quantized representations\nto temper the influence of text [18]. Additionally, approaches like\nZSIR leverage Product Knowledge Graphs to augment item features\nwithout prior data [9], and ShopperBERT models user behavior via\npurchase histories [45]. IDA-SR advances this by using BERT to\ngenerate ID-agnostic representations from text [36]. On the con-\ntrary, MoRec illustrates that systems that combine IDs and text\ncan surpass those dependent solely on IDs [ 61]. However, these\nadvancements complicate existing architectures by adding compu-\ntational demand and complicating scalability.\nTo these intricate and parameter-intensive models, we must add\nthe challenge that data in real-world applications is often noisy and\nsparse. Various methods have adopted contrastive learning [53] in\nnew architectures, as seen with CoSeRec [33], ContraRec [55], and\nS3-Rec [66]. The success of these new contrastive learning-based\nmethods motivates further investigation into the effectiveness of\ncontrastive loss functions for item recommendation, particularly\nSampled Softmax [57]. Regrettably, these studies typically focus\non fixed item spaces and overlook the scaling issues of the func-\ntions. Scaling problems have been tackled through other methods.\nLSAN suggests aggressively compressing the original embedding\nmatrix [30], introducing the concept of compositional embeddings,\nwhere each item embedding is composed by combining a selection\nof base embedding vectors. Recently, the concept of infinite rec-\nommendation networks [44] introduced two complementary ideas:\nâˆâˆ’ğ´ğ¸, an infinite-width autoencoder to model recommendation\ndata, and DISTILL-CF, which creates high-fidelity data summaries\nof extensive datasets for subsequent model training.\nScaling issues are not unique to recommendation systems but are\ninherent in new transformer-based architectures. In the field of NLP,\nvarious studies have been carried out to discover scaling laws that\npredict the scaling of the model and inform decision-making [17].\nTo our knowledge, only two studies have attempted to find scaling\nlaws in recommendation systems, yet none in SR. The first study [3]\naimed to explore the scaling properties of recommendation models,\ncharacterizing scaling efficiency across three different axes (data,\ncompute, parameters) and four scaling schemes (embedding table\nscaling vertically and horizontally, MLP and top layer scaling) in\nthe context of CTR problems. Similarly, the second study [46] seeks\nto understand scaling laws in the pursuit of a general-purpose user\nrepresentation that can be assessed across a variety of downstream\ntasks.\n6 CONCLUSIONS\nIn this work, we studied the scaling behavior of the transformer\narchitecture applied to real-world sequential recommendation prob-\nlems. We introduced a simple and flexible architecture and learning\nformulation that allowed us to scale the recommendation problem\nand model complexity independently from each other.\nWe showed there exist scaling laws similar to those observed\nin other sequential prediction domains, offering insights into the\ndesign of larger and more capable models. We also show that by\npre-training larger recommendation transformers, we can fine-tune\nthem for downstream tasks with significantly lesser data and obtain\nperformance improvements compared to the same models trained\nfrom scratch.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Pablo Zivic, Hernan Vazquez, & Jorge SÃ¡nchez\nREFERENCES\n[1] Ibrahim M. Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revis-\niting Neural Scaling Laws in Language and Vision. (2022).\n[2] Ibrahim M. Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas\nBeyer. 2023. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model\nDesign. (2023).\n[3] Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and\nAdnan Aziz. 2022. Understanding Scaling Laws for Recommendation Models.\nCoRR (2022). arXiv:2208.08489\n[4] Pedro G. Campos, Fernando DÃ­ez, and IvÃ¡n Cantador. 2014. Time-aware recom-\nmender systems: a comprehensive survey and analysis of existing evaluation\nprotocols. User Model. User Adapt. Interact. 24, 1-2 (2014), 67â€“119.\n[5] Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. 2022.\nIntent Contrastive Learning for Sequential Recommendation. In WWW â€™22: The\nACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022,\nFrÃ©dÃ©rique Laforest, RaphaÃ«l Troncy, Elena Simperl, Deepak Agarwal, Aristides\nGionis, Ivan Herman, and Lionel MÃ©dini (Eds.). ACM, 2172â€“2182.\n[6] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Pa-\nganini, Jordan Hoffmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Se-\nbastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Henni-\ngan, Matthew J. Johnson, Albin Cassirer, Chris Jones, Elena Buchatskaya, David\nBudden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Marcâ€™Aurelio Ranzato,\nJack W. Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. 2022. Uni-\nfied Scaling Laws for Routed Language Models. In International Conference\non Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA\n(Proceedings of Machine Learning Research, Vol.162), Kamalika Chaudhuri, Ste-\nfanie Jegelka, Le Song, Csaba SzepesvÃ¡ri, Gang Niu, and Sivan Sabato (Eds.).\nPMLR, 4057â€“4086.\n[7] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks\nfor YouTube Recommendations. In Proceedings of the 10th ACM Conference on\nRecommender Systems, Boston, MA, USA, September 15-19, 2016, Shilad Sen,\nWerner Geyer, Jill Freyne, and Pablo Castells (Eds.). ACM, 191â€“198.\n[8] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang\nTang, and Qing Li. 2023. Recommender Systems in the Era of Large Language\nModels (LLMs). CoRR (2023). arXiv:2307.02046\n[9] Ziwei Fan, Zhiwei Liu, Shelby Heinecke, Jianguo Zhang, Huan Wang, Caiming\nXiong, and Philip S. Yu. 2023. Zero-shot Item-based Recommendation via Multi-\ntask Product Knowledge Graph Pre-Training. (2023), 483â€“493.\n[10] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learning\nfor Sequential Recommendation: Algorithms, Influential Factors, and Evaluations.\nACM Trans. Inf. Syst. 39, 1 (2020), 10:1â€“10:42.\n[11] Jyotirmoy Gope and Sanjay Kumar Jain. 2017. A survey on solving cold start prob-\nlem in recommender systems. In 2017 International Conference on Computing,\nCommunication and Automation (ICCCA). IEEE, 133â€“138.\n[12] Ruining He, Wang-Cheng Kang, and Julian J. McAuley. 2017. Translation-\nbased Recommendation. In Proceedings of the Eleventh ACM Conference on\nRecommender Systems, RecSys 2017, Como, Italy, August 27-31, 2017, Paolo Cre-\nmonesi, Francesco Ricci, Shlomo Berkovsky, and Alexander Tuzhilin (Eds.). ACM,\n161â€“169.\n[13] Ruining He and Julian J. McAuley. 2016. Fusing Similarity Models with Markov\nChains for Sparse Sequential Recommendation. In IEEE 16th International\nConference on Data Mining, ICDM 2016, December 12-15, 2016, Barcelona,\nSpain, Francesco Bonchi, Josep Domingo-Ferrer, Ricardo Baeza-Yates, Zhi-Hua\nZhou, and Xindong Wu (Eds.). IEEE Computer Society, 191â€“200.\n[14] Ruining He and Julian J. McAuley. 2016. Ups and Downs: Modeling the Vi-\nsual Evolution of Fashion Trends with One-Class Collaborative Filtering. In\nProceedings of the 25th International Conference on World Wide Web, WWW\n2016, Montreal, Canada, April 11 - 15, 2016, Jacqueline Bourdeau, Jim Hendler,\nRoger Nkambou, Ian Horrocks, and Ben Y. Zhao (Eds.). ACM, 507â€“517.\n[15] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob\nJackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy,\nBenjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler,\nJohn Schulman, Dario Amodei, and Sam McCandlish. 2020. Scaling Laws for\nAutoregressive Generative Modeling. CoRR (2020). arXiv:2010.14701\n[16] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016. Session-based Recommendations with Recurrent Neural Networks. (2016).\n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-\nOptimal Large Language Models. CoRR (2022). arXiv:2203.15556\n[18] Yupeng Hou, Zhankui He, Julian J. McAuley, and Wayne Xin Zhao. 2023. Learn-\ning Vector-Quantized Item Representation for Transferable Sequential Recom-\nmenders. In Proceedings of the ACM WebConference 2023, WWW 2023, Austin,\nTX, USA, 30 April 2023 - 4 May 2023, Ying Ding, Jie Tang, Juan F. Sequeda, Lora\nAroyo, Carlos Castillo, and Geert-Jan Houben (Eds.). ACM, 1162â€“1171.\n[19] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong\nWen. 2022. Towards Universal Sequence Representation Learning for Recom-\nmender Systems. InKDD â€™22: The 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, Aidong\nZhang and Huzefa Rangwala (Eds.). ACM, 585â€“593.\n[20] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y. Chang.\n2018. Improving Sequential Recommendation with Knowledge-Enhanced Mem-\nory Networks. In The 41st International ACM SIGIR Conference on Research\n& Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July\n08-12, 2018, Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun\nLiu, and Emine Yilmaz (Eds.). ACM, 505â€“514.\n[21] Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. 2020. Multi-\nbehavior Recommendation with Graph Convolutional Networks. In Proceedings\nof the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020,\nJimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong\nWen, and Yiqun Liu (Eds.). ACM, 659â€“668.\n[22] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-\nommendation. In IEEE International Conference on Data Mining, ICDM 2018,\nSingapore, November 17-20, 2018. IEEE Computer Society, 197â€“206.\n[23] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling Laws for Neural Language Models. CoRR (2020). arXiv:2001.08361\n[24] Salman H. Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-\nhad Shahbaz Khan, and Mubarak Shah. 2022. Transformers in Vision: A Survey.\nACM Comput. Surv. 54, 10s (2022), 200:1â€“200:41.\n[25] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and\nRaia Hadsell. 2016. Overcoming catastrophic forgetting in neural networks.\nCoRR (2016). arXiv:1612.00796\n[26] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for Neural Text Processing. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October\n31 - November 4, 2018, Eduardo Blanco and Wei Lu (Eds.). Association for Com-\nputational Linguistics, 66â€“71.\n[27] Siddique Latif, Aun Zaidi, Heriberto CuayÃ¡huitl, Fahad Shamshad, Moazzam\nShoukat, and Junaid Qadir. 2023. Transformers in Speech Processing: A Survey.\nCoRR (2023). arXiv:2303.11607\n[28] Sara Latifi, Dietmar Jannach, and AndrÃ©s Ferraro. 2022. Sequential recommenda-\ntion: A study on transformers, nearest neighbors and sampled metrics. Inf. Sci.\n609 (2022), 660â€“678.\n[29] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural Attentive Session-based Recommendation. In Proceedings of the 2017\nACM on Conference on Information and Knowledge Management, CIKM 2017,\nSingapore, November 06 - 10, 2017, Ee-Peng Lim, Marianne Winslett, Mark\nSanderson, Ada Wai-Chee Fu, Jimeng Sun, J. Shane Culpepper, Eric Lo, Joyce C.\nHo, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Aixin Sun,\nVincent S. Tseng, and Chenliang Li (Eds.). ACM, 1419â€“1428.\n[30] Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight\nSelf-Attentive Sequential Recommendation. In CIKM â€™21: The 30th ACM\nInternational Conference on Information and Knowledge Management, Virtual\nEvent, Queensland, Australia, November 1 - 5, 2021, Gianluca Demartini, Guido\nZuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 967â€“\n977.\n[31] Blerina Lika, Kostas Kolomvatsos, and Stathes Hadjiefthymiades. 2014. Facing\nthe cold start problem in recommender systems. Expert Syst. Appl. 41, 4 (2014),\n2065â€“2073.\n[32] Bryan Lim, Sercan Ã–mer Arik, Nicolas Loeff, and Tomas Pfister. 2019. Temporal\nFusion Transformers for Interpretable Multi-horizon Time Series Forecasting.\nCoRR (2019). arXiv:1912.09363\n[33] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S. Yu, Julian J. McAuley, and Caiming\nXiong. 2021. Contrastive Self-supervised Sequential Recommendation with\nRobust Augmentation. CoRR (2021). arXiv:2108.06479\n[34] JesÃºs LovÃ³n-Melgarejo, Laure Soulier, Karen Pinel-Sauvagnat, and Lynda\nTamine. 2021. Studying Catastrophic Forgetting in Neural Ranking Mod-\nels. In Advances in Information Retrieval - 43rd European Conference on IR\nResearch, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings,\nPart I (Lecture Notes in Computer Science, Vol. 12656), Djoerd Hiemstra, Marie-\nFrancine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio\nSebastiani (Eds.). Springer, 375â€“390.\n[35] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel.\n2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings\nof the 38th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Santiago, Chile, August 9-13, 2015, Ricardo Baeza-Yates,\nMounia Lalmas, Alistair Moffat, and Berthier A. Ribeiro-Neto (Eds.). ACM, 43â€“52.\nScaling Sequential Recommendation Models with Transformers SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n[36] Shanlei Mu, Yupeng Hou, Wayne Xin Zhao, Yaliang Li, and Bolin\nDing. 2022. ID-Agnostic User Behavior Pre-training for Sequential Rec-\nommendation. In Information Retrieval - 28th China Conference, CCIR\n2022, Chongqing, China, September 16-18, 2022, Revised Selected Papers\n(Lecture Notes in Computer Science, Vol. 13819), Yi Chang and Xiaofei Zhu\n(Eds.). Springer, 16â€“27.\n[37] Jianmo Ni, Jiacheng Li, and Julian J. McAuley. 2019. Justifying Recommenda-\ntions using Distantly-Labeled Reviews and Fine-Grained Aspects. InProceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui,\nJing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational\nLinguistics, 188â€“197.\n[38] Hao Peng, Renyu Yang, Zheng Wang, Jianxin Li, Lifang He, Philip S. Yu, Albert Y.\nZomaya, and Rajiv Ranjan. 2022. Lime: Low-Cost and Incremental Learning for\nDynamic Heterogeneous Information Networks. IEEE Trans. Computers 71, 3\n(2022), 628â€“642.\n[39] Aleksandr V. Petrov and Craig Macdonald. 2022. A Systematic Review and\nReplicability Study of BERT4Rec for Sequential Recommendation. In RecSys\nâ€™22: Sixteenth ACM Conference on Recommender Systems, Seattle, WA, USA,\nSeptember 18 - 23, 2022, Jennifer Golbeck, F. Maxwell Harper, Vanessa Murdock,\nMichael D. Ekstrand, Bracha Shapira, Justin Basilico, Keld T. Lundgaard, and\nEven Oldridge (Eds.). ACM, 436â€“447.\n[40] Ruihong Qiu, Zi Huang, and Hongzhi Yin. 2021. Memory Augmented Multi-\nInstance Contrastive Predictive Coding for Sequential Recommendation. In IEEE\nInternational Conference on Data Mining, ICDM 2021, Auckland, New Zealand,\nDecember 7-10, 2021, James Bailey, Pauli Miettinen, Yun Sing Koh, Dacheng Tao,\nand Xindong Wu (Eds.). IEEE, 519â€“528.\n[41] Dimitrios Rafailidis and Alexandros Nanopoulos. 2016. Modeling Users Pref-\nerence Dynamics and Side Information in Recommender Systems. IEEE Trans.\nSyst. Man Cybern. Syst. 46, 6 (2016), 782â€“792.\n[42] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Fac-\ntorizing personalized Markov chains for next-basket recommendation. In\nProceedings of the 19th International Conference on World Wide Web, WWW\n2010, Raleigh, North Carolina, USA, April 26-30, 2010, Michael Rappa, Paul Jones,\nJuliana Freire, and Soumen Chakrabarti (Eds.). ACM, 811â€“820.\n[43] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2020.\nA Constructive Prediction of the Generalization Error Across Scales. (2020).\n[44] Noveen Sachdeva, Mehak Preet Dhaliwal, Carole-Jean Wu, and Julian J. McAuley.\n2022. Infinite Recommendation Networks: A Data-Centric Approach. (2022).\n[45] Kyuyong Shin, Hanock Kwak, Kyung-Min Kim, Minkyu Kim, Young-Jin Park, Jisu\nJeong, and Seungjae Jung. 2021. One4all User Representation for Recommender\nSystems in E-commerce. CoRR (2021). arXiv:2106.00573\n[46] Kyuyong Shin, Hanock Kwak, Su Young Kim, Max NihlÃ©n RamstrÃ¶m, Jisu Jeong,\nJung-Woo Ha, and Kyung-Min Kim. 2023. Scaling Law for Recommendation\nModels: Towards General-Purpose User Representations. In Thirty-Seventh\nAAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference\non Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth\nSymposium on Educational Advances in Artificial Intelligence, EAAI 2023,\nWashington, DC, USA, February 7-14, 2023, Brian Williams, Yiling Chen, and\nJennifer Neville (Eds.). AAAI Press, 4596â€“4604.\n[47] Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A Survey of Collaborative Filtering\nTechniques. Adv. Artif. Intell. 2009 (2009), 421425:1â€“421425:19.\n[48] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Rep-\nresentations from Transformer. In Proceedings of the 28th ACM International\nConference on Information and Knowledge Management, CIKM 2019, Beijing,\nChina, November 3-7, 2019, Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui,\nElke A. Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu (Eds.). ACM,\n1441â€“1450.\n[49] Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and\nHongzhi Yin. 2020. Where to Go Next: Modeling Long- and Short-Term User\nPreferences for Point-of-Interest Recommendation. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020. AAAI Press, 214â€“221.\n[50] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved Recurrent Neural Net-\nworks for Session-based Recommendations. In Proceedings of the 1st Workshop\non Deep Learning for Recommender Systems, DLRS@RecSys 2016, Boston, MA,\nUSA, September 15, 2016, Alexandros Karatzoglou, BalÃ¡zs Hidasi, Domonkos\nTikk, Oren Sar Shalom, Haggai Roitman, Bracha Shapira, and Lior Rokach (Eds.).\nACM, 17â€“22.\n[51] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation\nvia Convolutional Sequence Embedding. In Proceedings of the Eleventh ACM\nInternational Conference on WebSearch and Data Mining, WSDM 2018, Marina\nDel Rey, CA, USA, February 5-9, 2018, Yi Chang, Chengxiang Zhai, Yan Liu, and\nYoelle Maarek (Eds.). ACM, 565â€“573.\n[52] Trinh Xuan Tuan and Tu Minh Phuong. 2017. 3D Convolutional Networks for\nSession-based Recommendation with Content Features. In Proceedings of the\nEleventh ACM Conference on Recommender Systems, RecSys 2017, Como, Italy,\nAugust 27-31, 2017, Paolo Cremonesi, Francesco Ricci, Shlomo Berkovsky, and\nAlexander Tuzhilin (Eds.). ACM, 138â€“146.\n[53] AÃ¤ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning\nwith Contrastive Predictive Coding. CoRR (2018). arXiv:1807.03748\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. (2017), 5998â€“6008.\n[55] Chenyang Wang, Weizhi Ma, Chong Chen, Min Zhang, Yiqun Liu, and Shaoping\nMa. 2023. Sequential Recommendation with Multiple Contrast Signals. ACM\nTrans. Inf. Syst. 41, 1 (2023), 11:1â€“11:27.\n[56] Shoujin Wang, Qi Zhang, Liang Hu, Xiuzhen Zhang, Yan Wang, and Charu\nAggarwal. 2022. Sequential/Session-based Recommendations: Challenges, Ap-\nproaches, Applications and Opportunities. In SIGIR â€™22: The 45th International\nACM SIGIR Conference on Research and Development in Information Retrieval,\nMadrid, Spain, July 11 - 15, 2022, Enrique AmigÃ³, Pablo Castells, Julio Gonzalo,\nBen Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 3425â€“3428.\n[57] Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,\nand Xiangnan He. 2022. On the Effectiveness of Sampled Softmax Loss for Item\nRecommendation. CoRR (2022). arXiv:2201.02327\n[58] Yiqing Wu, Ruobing Xie, Yongchun Zhu, Xiang Ao, Xin Chen, Xu Zhang, Fuzhen\nZhuang, Leyu Lin, and Qing He. 2022. Multi-view Multi-behavior Contrastive\nLearning in Recommendation. In Database Systems for Advanced Applications -\n27th International Conference, DASFAA 2022, Virtual Event, April 11-14, 2022,\nProceedings, Part II (Lecture Notes in Computer Science, Vol. 13246), Arnab\nBhattacharya, Janice Lee, Mong Li, Divyakant Agrawal, P. Krishna Reddy,\nMukesh K. Mohania, Anirban Mondal, Vikram Goyal, and Rage Uday Kiran\n(Eds.). Springer, 166â€“182.\n[59] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin\nDing, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation.\nIn 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala\nLumpur, Malaysia, May 9-12, 2022. IEEE, 1259â€“1273.\n[60] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee\nKumthekar, Zhe Zhao, Li Wei, and Ed H. Chi. 2019. Sampling-bias-corrected\nneural modeling for large corpus item recommendations. In Proceedings of the\n13th ACM Conference on Recommender Systems, RecSys 2019, Copenhagen,\nDenmark, September 16-20, 2019, Toine Bogers, Alan Said, Peter Brusilovsky,\nand Domonkos Tikk (Eds.). ACM, 269â€“277.\n[61] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan,\nand Yongxin Ni. 2023. Where to Go Next for Recommender Systems? ID- vs.\nModality-based Recommender Models Revisited. (2023), 2639â€“2649.\n[62] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022. Scaling\nVision Transformers. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 1204â€“\n1213.\n[63] Shuai Zhang, Yi Tay, Lina Yao, Aixin Sun, and Jake An. 2019. Next item recom-\nmendation with self-attentive metric learning. InThirty-Third AAAI Conference\non Artificial Intelligence, Vol. 9.\n[64] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie Xu, De-\nqing Wang, Guanfeng Liu, and Xiaofang Zhou. 2019. Feature-level Deeper\nSelf-Attention Network for Sequential Recommendation. In Proceedings of the\nTwenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI\n2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 4320â€“4326.\n[65] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu\nPan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, Yingqian Min, Zhichao\nFeng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang,\nand Ji-Rong Wen. 2021. RecBole: Towards a Unified, Comprehensive and Effi-\ncient Framework for Recommendation Algorithms. In CIKM â€™21: The 30th ACM\nInternational Conference on Information and Knowledge Management, Virtual\nEvent, Queensland, Australia, November 1 - 5, 2021, Gianluca Demartini, Guido\nZuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 4653â€“\n4664.\n[66] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,\nZhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for\nSequential Recommendation with Mutual Information Maximization. In CIKM\nâ€™20: The 29th ACM International Conference on Information and Knowledge\nManagement, Virtual Event, Ireland, October 19-23, 2020, Mathieu dâ€™Aquin, Ste-\nfan Dietze, Claudia Hauff, Edward Curry, and Philippe CudrÃ©-Mauroux (Eds.).\nACM, 1893â€“1902.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6766860485076904
    },
    {
      "name": "Scaling",
      "score": 0.590097963809967
    },
    {
      "name": "Transformer",
      "score": 0.532772421836853
    },
    {
      "name": "Engineering",
      "score": 0.12651345133781433
    },
    {
      "name": "Electrical engineering",
      "score": 0.12428751587867737
    },
    {
      "name": "Voltage",
      "score": 0.09385085105895996
    },
    {
      "name": "Mathematics",
      "score": 0.08183819055557251
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}