{
    "title": "ProGen: Language Modeling for Protein Generation",
    "url": "https://openalex.org/W3015531900",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4284322165",
            "name": "Madani, Ali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288384363",
            "name": "McCann, Bryan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202201585",
            "name": "Naik, Nikhil",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281371573",
            "name": "Keskar, Nitish Shirish",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282045001",
            "name": "Anand, Namrata",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288995176",
            "name": "Eguchi, Raphael R.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207706075",
            "name": "Huang Po‐Ssu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3176399732",
            "name": "Socher, Richard",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2074231493",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W2919834395",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2142678031",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2809879025",
        "https://openalex.org/W2947813521",
        "https://openalex.org/W2161062388",
        "https://openalex.org/W2980789587",
        "https://openalex.org/W2957874522",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3163435021",
        "https://openalex.org/W2103385859",
        "https://openalex.org/W2102461176",
        "https://openalex.org/W2141885858",
        "https://openalex.org/W2103017472",
        "https://openalex.org/W2991737195",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2751706698",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2143210482",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2917580301",
        "https://openalex.org/W2891185006",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2790952361",
        "https://openalex.org/W2161529001",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2774178591",
        "https://openalex.org/W2971227267",
        "https://openalex.org/W2194775991"
    ],
    "abstract": "Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ~280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",
    "full_text": "ProGen: Language Modeling for Protein Generation\nAli Madani 1 Bryan McCann 1 Nikhil Naik 1 Nitish Shirish Keskar 1 Namrata Anand 2 Raphael R. Eguchi 2\nPo-Ssu Huang 2 Richard Socher 1\nAbstract\nGenerative modeling for protein engineering is\nkey to solving fundamental problems in synthetic\nbiology, medicine, and material science. We pose\nprotein engineering as an unsupervised sequence\ngeneration problem in order to leverage the expo-\nnentially growing set of proteins that lack costly,\nstructural annotations. We train a 1.2B-parameter\nlanguage model, ProGen, on ∼280M protein se-\nquences conditioned on taxonomic and keyword\ntags such as molecular function and cellular com-\nponent. This provides ProGen with an unprece-\ndented range of evolutionary sequence diversity\nand allows it to generate with ﬁne-grained control\nas demonstrated by metrics based on primary se-\nquence similarity, secondary structure accuracy,\nand conformational energy.\n1. Introduction\nGenerating proteins with desired properties is one of the\nmost complex yet impactful problems in biology. Protein\nengineering research has grown over the past 50 years and\nyielded remarkable outcomes including the development\nof new enzymes, therapies, and sensors. However, leading\nexperimental techniques for protein engineering such as\ndirected evolution (Arnold, 1998) still rely on heuristics and\nrandom mutations to select initial sequences for rounds of\nevolution.\nThe raw amino acid sequence encodes a protein, and during\nsynthesis, this chain of amino acids folds in ways that exhibit\nlocal (secondary) and global (tertiary) structure. These struc-\ntural properties then directly determine a unique function,\nwhich is of ultimate interest to protein engineers. Unfortu-\nnately, obtaining three-dimensional structural information\nfor proteins is expensive and time consuming. Consequently,\nthere are three orders of magnitude more raw sequences than\nthere are sequences with structural annotations, and protein\n1Salesforce Research, Palo Alto, CA, USA 2Department of\nBioengineering, Stanford University, Stanford, CA, USA.\nCorrespondence to: Ali Madani <amadani@salesforce.com>.\nsequence data is growing at a near exponential rate.\nRecent research (Alley et al., 2019; Rives et al., 2019; Rao\net al., 2019) has begun to capitalize on the much larger set\nof raw protein sequences by adapting state-of-the-art rep-\nresentation learning techniques (Devlin et al., 2018) from\nnatural language processing (NLP) to classiﬁcation of pro-\ntein properties. However, there has been no attempt to adapt\nstate-of-the-art methods for artiﬁcial text generation (Rad-\nford et al., 2019), and in particular the kind of controllable\ngeneration (Keskar et al., 2019) that would be most useful\nfor protein engineering.\nWe introduce ProGen for controllable protein generation.\nProGen is a 1.2 billion parameter conditional language\nmodel trained on a dataset of 280 million protein sequences\ntogether with conditioning tags that encode a variety of\nannotation such as taxonomic, functional, and locational in-\nformation. By conditioning on these tags, ProGen provides\na new method for protein generation that can be tailored for\ndesired properties (Figure 1).\nAccording to NLP metrics, ProGen is a powerful language\nmodel, achieving comparable performance to similarly-\nsized models for English. This performance improves in\nsettings with larger amino acid contexts and when ProGen\nis provided a larger number of conditioning tags, which\nhighlights its potential for applications in providing viable,\nstarting sequences for directed evolution or de novo protein\ndesign (Huang et al., 2016). ProGen also performs well\nwhen used to model unseen protein families, but it is even\nmore effective when ﬁne-tuned for those unseen families as\nan alternative to training from random initialization. These\nresults inspire the use of ProGen to generate candidate se-\nquences in challenging, low-homology applications.\nProteins generated by ProGen satisfy desired structural, and\nby extension functional, properties when evaluated with\nmetrics for sequence similarity, secondary structure accu-\nracy, and conformational energy– from lower level structure\nto higher level structure. Generation performance is judged\nhigher quality by higher level metrics, which suggests that\nProGen has learned invariances to mutations at the sequence\nlevel that conserve structure and inferred function. At the\nhighest level, conformational energy analysis reveals that\ngenerated proteins exhibit energy levels near that of native\narXiv:2004.03497v1  [q-bio.BM]  8 Mar 2020\nProGen: Language Modeling for Protein Generation\nFigure 1.a) Protein sequence data is growing exponentially as compared to structural data. b) We utilize protein sequence data along with\ntaxonomic and keyword tags to develop a conditional language model: ProGen.\nproteins, providing our strongest evidence that these pro-\nteins satisfy the desired structure and inferred function. In\nour ﬁrst case study, we examine completion of a VEGFR2\nprotein, which is held-out in training. ProGen generates\ncandidate completions that conserve the structural elements\nmost important for determining function and exhibit con-\nformational energy near to native levels across a variety\nof generation lengths. In our second case study, we ob-\nserve that ProGen can select high ﬁtness antibody-binding\nGB1 proteins without supervised training or unsupervised\nﬁnetuning– indicating that ProGen has learned the underly-\ning distribution of functional proteins.\n2. Related Work\nProtein representation learning. Recent methods for con-\ntextualized representations (McCann et al., 2017; Peters\net al., 2018; Devlin et al., 2018) in natural language process-\ning have been demonstrated to work well for contextual pro-\ntein representation learning. Structural information about\na protein can be extracted from such representations using\nlinear methods, and the representations themselves can be\nadapted to improve performance on other tasks (Rives et al.,\n2019). Similarly, UniRep (Alley et al., 2019) demonstrated\nthat such representations could be used to predict stability\nof natural and de novo designed proteins as well as quanti-\ntative function of molecularly diverse mutants. TAPE (Rao\net al., 2019) is a new benchmark consisting of ﬁve tasks for\nassessing such protein embeddings. While this body of prior\nwork focuses on transferable representation learning using\nbidirectional models, our work demonstrates controllable\nprotein engineering with generative, unidirectional models.\nGenerative models for protein engineering. Recent gen-\nerative modeling work such as Ingraham et al. (2019) ex-\ntends the transformer to condition it on a graph-structured\nspeciﬁcation of a desired target. Anand & Huang (2018)\nutilizes generative adversarial networks to produce 2D pair-\nwise distance map for given protein structural fragments, es-\nsentially in-painting missing residues. The aforementioned\nwork, along with O’Connell et al. (2018), Boomsma &\nFrellsen (2017), and Greener et al. (2018), all utilize explicit\nstructural information for generative modeling, thereby are\nunable to fully capture the number and diversity of sequence-\nonly data available. Meanwhile sequence-only generative\nmodeling have been attempted recently through residual\ncausal dilated convolutional neural networks (Riesselman\net al., 2019) and variational autoencoders (Costello & Mar-\ntin, 2019). Unlike these prior works, our work on generative\nmodeling focuses on a high-capacity language models that\nscale well with sequence data and can be used for control-\nlable generation.\nLanguage Models and Controllable Generation. Large\nTransformer architectures (Vaswani et al., 2017) like GPT-\n2 (Radford et al., 2019) represent the state-of-the-art in\nunconditional language modeling and demonstrate impres-\nsive text generation capabilities (Zellers et al., 2019) af-\nter training on vast amounts of unsupervised English text.\nCTRL (Keskar et al., 2019) trained a similarly large Trans-\nformer architecture for language generation by conditioning\non properties of the text easily extracted at scale, e.g. do-\nmain, style, and even associated URL. We adapt this per-\nspective to protein engineering by training a conditional\ntransformer language model on amino acid sequences con-\nditioned on a set of protein properties referred to as condi-\ntioning tags. Notably different from Keskar et al. (2019),\nprotein engineering requires a ﬁner-grained, much larger,\nand more complex set of conditioning tags. Additionally,\na single protein can be paired with dozens of conditioning\ntags.\n3. Methods\nLet a = (a1,...,a na ) be a sequence of amino acids that\nconstitutes a protein. In the context of protein engineering,\nthere is typically also a set of desired protein properties\nsuch as function or afﬁliation with a particular organism.\nFollowing recent work on controllable, conditional language\nmodeling (Keskar et al., 2019), we refer to these properties\nProGen: Language Modeling for Protein Generation\ngenerally as ‘conditioning tags’ through which we would\nlike to control generation of amino acid sequences. Let\nc= (c1,...,c nc ) be a sequence of such conditioning tags,\nand let x = [ c; a] the sequence formed by prepending a\nconditioning tag sequence to an amino acid sequence. p(x)\nis then the probability over such combined sequences of\nlength n= na+nc. We can factorize this distribution using\nthe chain rule of probability (Bengio et al., 2003):\np(x) =\nn∏\ni=1\np(xi|x<i)\nThis decomposes the problem of conditional protein gen-\neration into next-token prediction, where a token xi can\neither be an amino acid or a conditioning tag. A neural net-\nwork with parameters θcan then be trained to minimize the\nnegative log-likelihood over a dataset D= {x1,...,x |D|}:\nL(D) = −\n|D|∑\nk=1\nlog pθ(xk\ni|xk\n<i)\nNote that p(a|c), the distribution over proteins condi-\ntioned on their corresponding conditioning tags, is just\none of the many conditional distributions that can be re-\ncovered from a model that learns p(x). A new protein ˜a\nof length ma with desired properties encoded by a con-\nditioning tag sequence ˜c of length mc can then be gen-\nerated by sequentially sampling its constituent symbols:\npθ(a0|˜c),pθ(a1|˜a0,˜c),...,p θ(ap|˜a<p,˜c).\nWe train a variant of the Transformer (Vaswani et al., 2017)\nto learn these conditional distributions over amino acids\nand conditioning tags. A sequence containing ntokens is\nembedded as a sequence of ncorresponding vectors in Rd.\nEach vector is the sum of a learned token embedding and\na sinusoidal positional embedding as in the original Trans-\nformer architecture. This sequence of vectors is stacked\ninto a matrix X0 ∈Rn×d so that it can be processed by l\nattention layers. The ith layer consists of two blocks, each\nof which preserves the model dimension d.\nThe core of the ﬁrst block is multi-head attention with k\nheads that uses a causal mask to preclude attending to future\ntokens:\nAttention(X,Y,Z ) = softmax\n(mask(XY⊤)√\nd\n)\nZ\nMultiHead(X,k) = [h1; ··· ; hk]Wo\nwhere hj = Attention(XW1\nj,XW 2\nj,XW 3\nj)\nThe core of the second block is a feedforward network with\nReLU activation that projects inputs to an inner dimension\nf, with parameters U ∈Rd×f and V ∈Rf×d:\nFF(X) = max(0,XU )V\nEach block precedes core functionality with layer normal-\nization (Ba et al., 2016; Child et al., 2019) and follows it\nwith a residual connection (He et al., 2016). Together, they\nyield Xi+1:\nBlock 1 Block 2\n¯Xi = LayerNorm(Xi) ¯Hi = LayerNorm(Hi)\nHi = MultiHead( ¯Xi) + ¯Xi Xi+1 = FF( ¯Hi) + ¯Hi\nScores are then computed from the output of the last layer:\nScores(X0) = LayerNorm(Xl)Wvocab\nDuring training, these scores are the inputs of a cross-\nentropy loss function. During generation, the scores corre-\nsponding to the ﬁnal token are normalized with a softmax,\nyielding a distribution for sampling a new token.\n3.1. Data\nWe utilize all protein sequences and associated tags\navailable in Uniparc (Leinonen et al., 2004), Unipro-\ntKB (Bairoch et al., 2005), SWISS-PROT (Bairoch et al.,\n2004), TrEMBL (Boeckmann et al., 2003), Pfam (Bate-\nman et al., 2004), and NCBI taxonomic information (Feder-\nhen, 2012). The aggregated dataset contains over 281M\nproteins—the most comprehensive, non-redundant, anno-\ntated database of proteins used to train a machine learning\nmodel. For the amino acid vocabulary, we use the standard\n25 amino acids designations in IUPAC (Pettit & Powell,\n2006). The conditioning tags are divided into 2 categories:\n(1) keyword tags and (2) taxonomic tags. Following the\ndeﬁnitions laid out in the UniprotKB controlled, hierarchi-\ncal vocabulary of keywords (many of which are derived\nfrom Gene Ontology (GO) terms) (Ashburner et al., 2000),\nthe conditioning keyword tags included 1100 terms ranging\nfrom cellular component, biological process, and molecular\nfunction terms. The taxonomic tags include 100k terms from\nthe NCBI taxonomy across the eight standard taxonomic\nranks. The aggregated dataset was split into a training set of\nsize 280M, a held-out protein family1 test set (OOD-test) of\nsize 100k, and a randomly sampled test set (ID-test) of size\n1M. OOD-test comprises of 20 protein families, as deﬁned\nin Pfam, that were excluded from the training data. Perfor-\nmance on OOD-test measures ability to model samples from\nunseen protein families, whereas performance on ID-test\nmeasures ability to model samples from a wider range of\nprotein families that more closely match the distribution of\nthe training set as described in section A.1\n1Protein families are groups of evolutionarily-related proteins\nthat have similar structure, function, and sequence similarity as\ndeﬁned by Pfam (Bateman et al., 2004)\nProGen: Language Modeling for Protein Generation\n3.2. Training Details\nFor training, we include each sequence and its reverse, as\nproteins are invariant to the temporal notion of sequence\ngeneration. We then prepend each sequence (and its re-\nverse) with a corresponding subset of conditioning tags.\nFor a given sequence, there can be multiple versions across\ndatabases, each with their own associated conditioning tags.\nIn training, we randomly sample which set of condition-\ning tags to utilize but bias toward SWISSPROT tags as\nthey are manually veriﬁed. We apply dropout to the con-\nditioning tags themselves at a rate of 0.4. We additionally\nalways include a sample with the sequence alone without\nconditioning tags so that ProGen can be used to complete\nproteins using only sequence data even when no protein\nproperties are known. We then truncate all sequences to\na maximum length of 512. Sequences of length less than\n512 were padded, but no loss was backpropagated through\nthe network for padding tokens. The model has dimension\nd= 1028, inner dimension f = 512, 36 layers, and 8 heads\nper layer. Dropout with probability 0.1 follows the residual\nconnections in each layer. Token embeddings were tied with\nthe embeddings of the ﬁnal output layer (Inan et al., 2016;\nPress & Wolf, 2016).\nOur model was implemented in TensorFlow (Abadi et al.,\n2016) and trained with a global batch size of 64 distributed\nacross 256 cores of a Cloud TPU v 3 Pod for 1M itera-\ntions. Training took approximately two weeks using Ada-\ngrad (Duchi et al., 2011) with linear warmup from 0 to\n1e−2 over 40k steps. Gradient norms were clipped to 0.25.\nTraining in early stages was improved and stabilized by ini-\ntializing with the pretrained weights of Keskar et al. (2019).\n3.3. Generation Details\nProGen generates proteins one amino acid at a time. For\none step of generation, ProGen takes a context sequence of\namino acids as input and outputs a probability distribution\nover amino acids. We sample from that distribution and\nthen update the context sequence with the sampled amino\nacid. This process repeats until a protein of desired length\nhas been generated. We compare different combinations\nof top-ksampling (Radford et al., 2019) with a repetition\npenalty designed for amino acid sequence generation. The\nrepetition penalty reduces the probability of amino acids\nthat have been generated within 4 tokens prior to the token\nto be predicted. Top-ksampling draws the next token from\nthe k most probable tokens in the distribution output by\nProGen. We report results for top- k values of k = 1 and\nk= 3 with repetition penalties of 0 and 1.2.\n3.4. Evaluation Details\nTo assess how well ProGen models the training and test\ndistributions, we rely on perplexity as the standard metric\nfor language models, a mean hard accuracy over each token\nto strictly assess each amino acid error, and a mean soft\naccuracy deﬁned by incorporating BLOSUM62 (Henikoff &\nHenikoff, 1992), a standard amino acid substitution matrix.\nPerplexity is the exponentiated cross-entropy loss computed\nover each token in a dataset. Thus, high quality language\nmodels are expected to have low perplexities. Mean per-\ntoken hard accuracy over the tokens in a sequence judges\na prediction incorrect for any amino acid that is not the\nground truth. Mean per-token soft accuracy relies on BLO-\nSUM62, a block substitution matrix that speciﬁes which\namino acid substitutions are more or less acceptable ac-\ncording to their frequency in known well-formed proteins.\nBLOSUM62 is widely used across adopted alignment soft-\nware (e.g., BLAST 2). Our mean per-token soft accuracy\nuses BLOSUM62 to penalize incorrect amino acid predic-\ntions according to the frequency of that substitution in the\nmatrix. In this way, if the substitution is likely in nature,\nsoft accuracy penalizes the model less.\nTo assess the quality of generation, we evaluate across three\nlevels of structure: (1) primary sequence similarity, (2) sec-\nondary structure accuracy, and (3) conformational energy\nanalysis.\nPrimary sequence similarity is deﬁned by a global, pairwise\nsequence alignment score computed with the Biopython\npackage3. This score is based on the Needleman-Wunsch\nalgorithm (Needleman & Wunsch, 1970) informed by the\nBLOSUM62 substitution matrix. We use a gap open penalty\nof −0.5 and gap continue penalty of −0.1. The resulting\nscore is then normalized by the length of the protein. Exper-\niments reporting sequence similarity are limited to test sam-\nples with a form of experimental evidence of X-ray/NMR\ncrystallography, mass spectrometry, or existence in cDNA\nor RT-PCR to indicate transcript existence. We refer the\nreader to UniprotKB existence scores with experimental\nevidence4 for further details.\nSecondary structure accuracy was computed per-residue for\npredicted secondary structures by PSIPRED5 with greater\nthan 0.5 conﬁdence. PSI-BLAST was performed on each\ngenerated sample to extract the Multiple Sequence Align-\nments (MSAs) with respect to the UniRef90 database (Suzek\net al., 2015). These MSAs were provided to PSIPRED for\nhigher quality secondary structure prediction. Experiments\nreporting secondary structure accuracy were limited to test\n2https://blast.ncbi.nlm.nih.gov/Blast.cgi\n3https://biopython.org/\n4https://www.uniprot.org/help/protein existence\n5http://bioinf.cs.ucl.ac.uk/psipred/\nProGen: Language Modeling for Protein Generation\nTable 1. ProGen outperforms uniform random and empirical base-\nlines on the full test set, which includes ID- and OOD-test. OOD-\ntest results reveal that ProGen also performs well on protein fam-\nilies unseen during training. Fine-tuning ProGen dramatically\nimproves performance over training from random initialization.\nMODEL PPL H ARD ACC.\nUNIFORM BASELINE 25 4\nEMPIRICAL BASELINE 18.14 6\nPROGEN 8.56 45\nID- TEST 8.17 45\nOOD- TEST 13.34 22\nOOD- TEST -20 ( RAND . INIT .) 17.78 9\nOOD- TEST -20 ( FINE -TUNED ) 7.45 50\nsamples with high UniprotKB existence scores as described\nin the previous paragraph.\nConformational energy uses the Rosetta-RelaxBB protocol6.\nRosetta-RelaxBB performs a Monte Carlo optimization of\nthe Rosetta energy function over the space of amino acid\ntypes and rotamers. The Rosetta energy is based on biophysi-\ncal laws and constraints. Between each design round, amino\nacid side-chains are replaced, while the carbon backbone\ntorsions are kept ﬁxed. Energy minimization/relaxation is\nperformed after threading the amino acid sequence through\nthe known structure. This allows the backbone to move,\npossibly into a lower energy state. A lower resulting Rosetta\nenergy correlates to a more relaxed-state and viable confor-\nmation for a given protein structure. Before applying the\nprocedure above, we relax the native template ﬁrst. Experi-\nments that report conformational energy are limited to test\nsamples from SWISSPROT with associated 3D structures\nin RCSB PDB 7.\nTo assess generative quality, we provide baselines for dif-\nferent levels of random mutation. For a given sequence, a\nproportion (25 −100%) of amino acids in the sequence is\nrandomly substituted within one of the 20 standard amino\nacids other than itself. For conformational energy, we also\ninclude an all-alanine baseline (i.e. a sequence with only the\namino acid alanine), as it is a non-bulky, chemically inert\namino acid that mimics the existing secondary structure well\nwhen substituted. These baselines provide a scale across\neach of the above metrics. A particular random mutation\nmay or may not have constructive or destructive effects on\nprotein structure or function. But viewed in aggregate, the\nperformance of the 100% mutation baseline for any met-\nric indicates failed generation. As performance approaches\n0%, generation statistically indicates a closer reﬂection to\ndesired structural and functional properties.\n6https://www.rosettacommons.org/\n7https://www.rcsb.org/\nFigure 2.Large model capacity is warranted as ProGen has yet\nto overﬁt. BLOSUM62-informed soft accuracy shows no gap\nbetween train and test performance, suggesting hard accuracy hides\nthe possibility that ProGen errors often correspond to amino acid\nsubstitutions found in nature. For metrics details see Section 3.4.\n4. Results and Analysis\n4.1. Evaluating ProGen as a language model\nIn this section, we demonstrate that ProGen is a high-quality\nlanguage model according to per-token metrics on the train-\ning and test sets.\nProGen generalizes to the full test set and achieves\nperplexities representative of a high-quality language\nmodel. Perplexities reported in Table 1 demonstrate that\nProGen dramatically improves over a Uniform Baseline, in\nwhich amino acids are sampled according to a uniform dis-\ntribution, and an Empirical Baseline, in which amino acids\nare sampled according to the empirical frequencies in the\ntraining set. As a point of reference, state-of-the-art unidi-\nrectional language models for English Wikipedia achieve\nperplexities that range from 10 to 17 depending on model\nsize (between 257M and 8.3B parameters) and whether\ntraining data was constrained to English Wikipedia (Rae\net al., 2019) or not (Shoeybi et al., 2019).\nProGen generalizes to unseen protein families. The sec-\nond section of Table 1 breaks this result into perplexities\nover the ID-test and OOD-test sets separately. Results on\nID-test conﬁrm that ProGen generalizes well to sequences\nthat belonged to protein families randomly sampled. As\nexpected, performance is worse on the sequences in the\nOOD-test set, but the model still outperforms the Empirical\nBaseline for those held out protein families.\nFine-tuning ProGen on unseen protein families im-\nproves over training from random initialization. We fur-\nProGen: Language Modeling for Protein Generation\nFigure 3.Full test set performance is better for later segments of\nsequences in keeping with intuition that additional context supports\nbetter predictions. We examined intervals up to 500 tokens to\nensure a minimum of 30k samples per interval.\n.\nther split OOD-test into OOD-test-80 and OOD-test-20,\nﬁne-tuned ProGen on OOD-test-80 until convergence (5\nepochs; Adam; linear learning rate warmup to 1k iterations),\nand retested on OOD-test-20. The third section of Table 1\nshows that ﬁne-tuning from ProGen improves over training\nthe same architecture with randomly initialized weights.\nProGen performance improves with increased amino\nacid and conditioning tag context. In Figure 3, we ex-\namine the mean perplexity and per-token hard accuracy over\ndifferent portions of proteins. Perplexity decreases and hard\naccuracy increases for later portions of a protein, in keeping\nwith the intuition that additional amino acid context narrows\ndown the possibilities for future tokens. The same trends\nhold when increasing the number of conditioning tags and\ntaking the mean over sequence lengths with the same of\ntags (in Figure 4). This indicates that conditioning tags also\nprovide signal that improves model predictions.\nTraining curves suggest that protein generation would\nbeneﬁt from even larger models and longer training.\nWith 1B parameters, ProGen is comparable in size to the\nlargest language models that have been publicly released\nfor any modality, and, to the best of our knowledge, it is the\nlargest model trained on amino acid sequences. Figure 2\nshows that despite its size and the amount of compute used\nto train, ProGen has yet to overﬁt the training data. This sug-\ngests that models for protein generation could still beneﬁt\nfrom even larger models and additional compute.\nBLOSUM62 soft accuracy reveals that ProGen predic-\ntion errors often follow natural amino acid substitutions\nthat likely conserve higher level structure. Though Pro-\nGen models proteins as pure sequences, protein function\nis more directly determined by the secondary and tertiary\nFigure 4.Full test set performance also improves as the number of\nconditioning tags associated with proteins increases. We examined\nproteins with up to 14 conditioning tags to ensure a minimum of\n3k samples per category.\nstructures that these sequences encode in three-dimensional\nspace. Model performance based on BLOSUM62 soft ac-\ncuracy (Section 3.4) is more than 20% higher than using\nhard accuracy, which indicates that when ProGen errors\nmay often be substitutions that are acceptable in nature be-\ncause they still reﬂect the proper higher-level properties.\nThis suggests that ProGen has learned how to work within\nfunction-preserving mutational invariances—we continue\nto validate this ﬁnding for primary, secondary, and confor-\nmational structure in Section 4.2.\n4.2. Generating with ProGen\nIn this section, we focus on assessing ProGen as a genera-\ntive model. Generation quality is directly correlated with\nevolutionary viability and functional qualities, which can be\ninferred through protein structure. For this reason, we assess\ngeneration quality by using metrics for primary sequence\nsimilarity, secondary structure accuracy, and conformational\nenergy (Section 3.4).\nWe also include several mutation baselines (Section 3.4)\nthat allow us to compare the similarity of generated proteins\nto a target, reference protein across all metrics. In reference\nto these mutation baselines, ProGen quality improves as\nwe move from primary sequence to full conformational\nstructure metrics, thereby suggesting the model has learned\nmutational invariances in structure which present as errors\nin lower-level metrics.\nProGen achieves higher sequence similarity scores with\nan amino acid repetition penalty. Figure 5 depicts the\nresults of experimenting with various combinations of top-\nk sampling and repetition penalties (see Section 3.4 for\ndetails). Over all context lengths, ProGen performs best\nProGen: Language Modeling for Protein Generation\nFigure 5.Across all context lengths, greedily sampling with a rep-\netition penalty provides the best results according to sequence\nsimilarity.\nwith k = 1 and the repetition penalty applied to recently\ngenerated amino acids. Consequently, we use these settings\nfor all following generation experiments. With this nearly\ngreedy sampling, ProGen manages to generate proteins with\nsequence similarity comparable to randomly mutating 50%\nof the amino acids that are not seen in the given context.\nSequence similarity suggests that ProGen merely ap-\nproaches the 25% mutation baseline, but secondary\nstructure accuracy suggests that ProGen surpasses it.In\nFigure 6, we analyze this sequence similarity across differ-\ning numbers of conditioning tags. Sequences associated\nwith at least 3 conditioning tags begin to exceed the 50%\nmutation baseline, and as amino acid context increases, se-\nquences with at least 8 conditioning tags approach the 25%\nmutation baseline. Notably, even in the best case, according\nto sequence similarity, ProGen doesn’t surpass the25% mu-\ntation baseline. By contrast, according to secondary struc-\nture accuracy, sequences with at least 8 conditioning tags\nsurpass the 25% mutation baseline (Figure 7). This discrep-\nancy between sequence similarity and secondary structure\naccuracy further corroborates our claim from Section 4: er-\nrors registered by lower-level metrics often correspond to\nacceptable substitutions according to higher-level metrics\nthat more directly correspond to functional viability.\nAfter threading and relaxation, samples generated by\nProGen are likely to exhibit desired structure and func-\ntion. As a measure of generation quality, we thread ProGen\nsequences through known structures and examine if they\nexhibit favorable, low energy states. Figure 8 shows the\ndifferences between the energy levels of native proteins,\nProGen samples, the native proteins with 50% and 100% of\namino acids randomly mutated, as well as the all-alanine\n0.5 0.6 0.7 0.8 0.9\nProportion of Sequence as Context\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0Sequence Similarity\nFigure 6.A greater number of conditioning tags enables higher\nquality generation. With at least 8 conditioning tags, generation\nquality approaches the 25% mutation baseline.\n[0,2] [3,7] [8,20]\nNumber of conditioning tags\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86Secondary structure accuracy\n25% mutation\nProGen\nFigure 7.ProGen generates sequences that conserve secondary\nstructure of the protein. Increasing the number of conditioning tags\nyields better secondary structure accuracy than the 25% mutation\nbaseline.\nbaseline. Proteins completed by ProGen are much closer\nto the energy levels of the native protein than all baselines.\nGenerated samples exhibit energy levels near or even below\ntheir associated relaxed native templates.\n4.3. Case Study: Completing VEGFR2 kinase domain\nVEGFR2 is responsible for fundamental cell processes such\nas cell proliferation, survival, migration, and differentiation.\nVEGFR2 was excluded from training as a subsequence be-\nlongs to a held out protein family in OOD-test. We study\nhow well ProGen generates in the context of a protein com-\nProGen: Language Modeling for Protein Generation\nFigure 8.Conformational energies for ProGen generated proteins\nsurpasses all baselines and adheres closely to the energy of the\nnative template.\nFigure 9.ProGen completion quality for VEGFR2 remains steadily\nnear native conformational energy levels across generation lengths.\npletion task. We consider the amino acid sequence begin-\nning at residue 806 and ending at residue 1168 of VEGFR2\n(PDB ID: 2XIR). For different generation lengths, we sam-\nple from ProGen to complete the sequence up to residue\n1168 with the remainder of the sequence provided as context.\nFigure 9 shows that the conformational energy calculated\nafter threading and relaxation of ProGen samples are lower\ncompared to all baselines, indicating better structural conser-\nvation. Generation quality remains near the native relaxed\nprotein independent of generation length.\nThe generated samples across Figure 9 exhibit a mean se-\nquence identity of 73.1% with the native sequence. This\ncorrelates to a lower sequence identity than the 25% muta-\ntion baseline (74% identity) but with better Rosetta energies.\nThis suggests meaningful deviation from the native protein\nwhile achieving the ultimate goal of preserving low energy.\n75% Mutation\nEnergy: -778.45\nProGen Generated\nEnergy: -900.98\n25% Mutation\nEnergy: -857.10\nFigure 10. ProGen makes fewer mistakes and prioritizes conserva-\ntion of secondary structure as compared to baselines. Blue is low\nenergy (stable) and red high (unstable).\nFigure 10 shows one sample from ProGen as well as one\nfrom each of the 25% and 75% mutation baselines. The\nProGen sample exhibits lower energy overall, and energy is\nhighest for amino acids that do not have secondary structure.\nThis suggests that ProGen learned to prioritize the most\nstructurally important segments of the protein.\n4.4. Case Study: Zero-shot ﬁtness selection for protein\nGB1\nThe ultimate goal of protein engineering is to engineer func-\ntional proteins. One promising avenue is via directed evolu-\ntion, which iterates through rounds of mutation and screen-\ning to converge on a high-ﬁtness (i.e. functioning) protein.\nMachine learning has shown initial promise to aid in the\nsubsequent rounds of directed evolution by in silico screen-\ning of proteins (Wu et al., 2019), but it still relies on random\nmutation in an exponentially large search space. Ideally,\na generative model, such as ProGen, that has learned the\ndistribution of evolutionarily-relevant proteins can directly\ngenerate high-ﬁtness proteins.\nWe examine the empirical ﬁtness landscape of protein G\ndomain B1 (GB1) binding to an antibody (Wu et al., 2016).\nProtein G is important for the puriﬁcation, immobilization,\nand detection of immunoglobulins (antibodies), proteins\nused by our immune system to neutralize pathogenic viruses\nand bacteria. Ideally, we would want the ability to generate\nGB1 proteins with high binding afﬁnity and stability. The\ndata includes 149,361 of a total 160,000 possible variants\nfrom NNK/NNS saturation mutagenesis at four positions\nknown to interact epistatically. Reported ﬁtness values cor-\nrespond to a measure of both stability (i.e. the fraction\nof folded proteins) and function (i.e. binding afﬁnity to\nIgG-Fc) by coupling mRNA display with next-generation\nsequencing. Protein sequences with high ﬁtness values are\ndesired.\nWithout supervised training of ProGen on the GB1 data\nor unsupervised ﬁne-tuning of ProGen on a subset of simi-\nlar immunoglobulin-binding proteins, we pass each variant\nProGen: Language Modeling for Protein Generation\nFigure 11.Without training on the Wu et al. (2016) dataset, Pro-\nGen can identify which protein variants exhibit high ﬁtness. The\ndataset reports ﬁtness values for protein variants of GB1 binding\nto an antibody. Each sample corresponds to mutating one of four\nhighlighted residues, in the above sequence, to a standard amino\nacid. At the left, the crystallized structure of GB1 is shown. At\nthe right, the ﬁtness value of samples selected through ProGen vs\nrandom selection are shown.\nthrough ProGen and select the top one hundred variants with\nthe lowest perplexity values. In Figure 11, we demonstrate\nProGen is effective in zero-shot selection of high-ﬁtness\nprotein sequences. In comparison, random mutation, which\nis the main technique used by directed evolution and ML-\nassisted directed evolution, statistically generates samples\nwith low or zero ﬁtness. With effective sampling techniques,\nProGen can be utilized to generate a spread of samples that\nare statistically high ﬁtness. These results imply that ProGen\nhas not only learned the distribution of structurally-relevant\nproteins, but also functionally-relevant proteins.\n5. Conclusion\nWe introduced ProGen, a controllable protein generation\nlanguage model trained on the full evolutionary diversity of\none of the largest sequence databases. The model generates\nproteins that exhibit near native structure energies which\nlikely implies functional viability. ProGen has the potential\nto play a new, complementary role alongside other state-\nof-the-art methods in protein engineering. For example, in\ndirected evolution, initial sequences may be sampled from\nProGen according to desired conditioning tags. In later\nrounds of evolution, protein completion with context for\nparticular residue spans, or hotspots, may provide higher\nﬁtness samples. In de novo protein design, using ProGen\nwith conditioning tags may allow for designing new proteins\nwith existing folding motifs in new protein families or host\norganisms. This same strategy may be used in conjunction\nwith threading and structure-based protein design. Because\nconditioning tags orient ProGen in sequence space, ProGen\nmay even be used as a model to sample from the distribution\nof evolutionarily viable proteins near one particular protein.\nThis may provide useful augmentations around data for non-\nhomologous domains where existing techniques, such as\nMSAs, fall short.\n6. Acknowledgements\nWe would like to thank Alex Chu for assistance in the thread-\ning and minimization experiments along with Jesse Vig for\nvisualizing the attention heads of ProGen.\nProGen: Language Modeling for Protein Generation\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensorﬂow: A system for large-scale machine learning.\nIn 12th {USENIX}Symposium on Operating Systems\nDesign and Implementation ({OSDI}16), pp. 265–283,\n2016.\nAlley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M.,\nand Church, G. M. Uniﬁed rational protein engineering\nwith sequence-based deep representation learning.Nature\nmethods, 16(12):1315–1322, 2019.\nAnand, N. and Huang, P. Generative modeling for protein\nstructures. In Advances in Neural Information Processing\nSystems, pp. 7494–7505, 2018.\nArnold, F. H. Design by directed evolution. Accounts of\nchemical research, 31(3):125–131, 1998.\nAshburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler,\nH., Cherry, J. M., Davis, A. P., Dolinski, K., Dwight, S. S.,\nEppig, J. T., et al. Gene ontology: tool for the uniﬁcation\nof biology. Nature genetics, 25(1):25–29, 2000.\nBa, J., Kiros, R., and Hinton, G. E. Layer normalization.\nCoRR, abs/1607.06450, 2016.\nBairoch, A., Boeckmann, B., Ferro, S., and Gasteiger, E.\nSwiss-prot: juggling between evolution and stability.\nBrieﬁngs in bioinformatics, 5(1):39–55, 2004.\nBairoch, A., Apweiler, R., Wu, C. H., Barker, W. C., Boeck-\nmann, B., Ferro, S., Gasteiger, E., Huang, H., Lopez,\nR., Magrane, M., et al. The universal protein resource\n(uniprot). Nucleic acids research , 33(suppl 1):D154–\nD159, 2005.\nBateman, A., Coin, L., Durbin, R., Finn, R. D., Hollich, V .,\nGrifﬁths-Jones, S., Khanna, A., Marshall, M., Moxon,\nS., Sonnhammer, E. L., et al. The pfam protein fami-\nlies database. Nucleic acids research, 32(suppl 1):D138–\nD141, 2004.\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research, 3(Feb):1137–1155, 2003.\nBoeckmann, B., Bairoch, A., Apweiler, R., Blatter, M.-C.,\nEstreicher, A., Gasteiger, E., Martin, M. J., Michoud,\nK., O’Donovan, C., Phan, I., et al. The swiss-prot pro-\ntein knowledgebase and its supplement trembl in 2003.\nNucleic acids research, 31(1):365–370, 2003.\nBoomsma, W. and Frellsen, J. Spherical convolutions and\ntheir application in molecular modelling. In Advances in\nNeural Information Processing Systems, pp. 3433–3443,\n2017.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nCostello, Z. and Martin, H. G. How to hallucinate functional\nproteins. arXiv preprint arXiv:1903.00458, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDuchi, J., Hazan, E., and Singer, Y . Adaptive subgradi-\nent methods for online learning and stochastic optimiza-\ntion. Journal of machine learning research, 12(Jul):2121–\n2159, 2011.\nFederhen, S. The ncbi taxonomy database. Nucleic acids\nresearch, 40(D1):D136–D143, 2012.\nGreener, J. G., Moffat, L., and Jones, D. T. Design of\nmetalloproteins and novel protein folds using variational\nautoencoders. Scientiﬁc reports, 8(1):1–12, 2018.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016.\nHenikoff, S. and Henikoff, J. G. Amino acid substitution\nmatrices from protein blocks.Proceedings of the National\nAcademy of Sciences, 89(22):10915–10919, 1992.\nHuang, P.-S., Boyken, S. E., and Baker, D. The coming\nof age of de novo protein design. Nature, 537(7620):\n320–327, 2016.\nInan, H., Khosravi, K., and Socher, R. Tying word vectors\nand word classiﬁers: A loss framework for language\nmodeling. arXiv preprint arXiv:1611.01462, 2016.\nIngraham, J., Garg, V ., Barzilay, R., and Jaakkola, T. Gener-\native models for graph-based protein design. In Advances\nin Neural Information Processing Systems , pp. 15794–\n15805, 2019.\nKeskar, N. S., McCann, B., Varshney, L. R., Xiong, C.,\nand Socher, R. Ctrl: A conditional transformer lan-\nguage model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019.\nLeinonen, R., Diez, F. G., Binns, D., Fleischmann, W.,\nLopez, R., and Apweiler, R. Uniprot archive. Bioinfor-\nmatics, 20(17):3236–3237, 2004.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors. In\nAdvances in Neural Information Processing Systems, pp.\n6294–6305, 2017.\nProGen: Language Modeling for Protein Generation\nNeedleman, S. B. and Wunsch, C. D. A general method\napplicable to the search for similarities in the amino acid\nsequence of two proteins. Journal of molecular biology,\n48(3):443–453, 1970.\nO’Connell, J., Li, Z., Hanson, J., Heffernan, R., Lyons, J.,\nPaliwal, K., Dehzangi, A., Yang, Y ., and Zhou, Y . Spin2:\nPredicting sequence proﬁles from protein structures using\ndeep neural networks. Proteins: Structure, Function, and\nBioinformatics, 86(6):629–633, 2018.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. arXiv preprint arXiv:1802.05365,\n2018.\nPettit, L. D. and Powell, K. The iupac stability constants\ndatabase. Chemistry international, 2006.\nPress, O. and Wolf, L. Using the output embedding to im-\nprove language models. arXiv preprint arXiv:1608.05859,\n2016.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. OpenAI Blog, 1(8):9, 2019.\nRae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap,\nT. P. Compressive transformers for long-range sequence\nmodelling. arXiv preprint arXiv:1911.05507, 2019.\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y ., Chen,\nP., Canny, J., Abbeel, P., and Song, Y . Evaluating pro-\ntein transfer learning with tape. In Advances in Neural\nInformation Processing Systems, pp. 9686–9698, 2019.\nRiesselman, A. J., Shin, J.-E., Kollasch, A. W., McMahon,\nC., Simon, E., Sander, C., Manglik, A., Kruse, A. C., and\nMarks, D. S. Accelerating protein design using autore-\ngressive generative models. bioRxiv, pp. 757252, 2019.\nRives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick,\nC. L., Ma, J., and Fergus, R. Biological structure and func-\ntion emerge from scaling unsupervised learning to 250\nmillion protein sequences. bioRxiv, pp. 622803, 2019.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,\nand Catanzaro, B. Megatron-lm: Training multi-billion\nparameter language models using gpu model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nSuzek, B. E., Wang, Y ., Huang, H., McGarvey, P. B., Wu,\nC. H., and Consortium, U. Uniref clusters: a comprehen-\nsive and scalable alternative for improving sequence sim-\nilarity searches. Bioinformatics, 31(6):926–932, 2015.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\ntion is all you need. In Guyon, I., Luxburg, U. V ., Bengio,\nS., Wallach, H., Fergus, R., Vishwanathan, S., and Gar-\nnett, R. (eds.), Advances in Neural Information Process-\ning Systems 30, pp. 5998–6008. Curran Associates, Inc.,\n2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf .\nVig, J. A multiscale visualization of attention in the trans-\nformer model. arXiv preprint arXiv:1906.05714, 2019.\nWu, N. C., Dai, L., Olson, C. A., Lloyd-Smith, J. O., and\nSun, R. Adaptation in protein ﬁtness landscapes is facili-\ntated by indirect paths. Elife, 5:e16965, 2016.\nWu, Z., Kan, S. J., Lewis, R. D., Wittmann, B. J., and\nArnold, F. H. Machine learning-assisted directed protein\nevolution with combinatorial libraries. Proceedings of\nthe National Academy of Sciences, 116(18):8852–8858,\n2019.\nZellers, R., Holtzman, A., Rashkin, H., Bisk, Y ., Farhadi,\nA., Roesner, F., and Choi, Y . Defending against neural\nfake news. arXiv preprint arXiv:1905.12616, 2019.\nZimmermann, L., Stephens, A., Nam, S.-Z., Rau, D., K¨ubler,\nJ., Lozajic, M., Gabler, F., S¨oding, J., Lupas, A. N., and\nAlva, V . A completely reimplemented mpi bioinformatics\ntoolkit with a new hhpred server at its core. Journal of\nmolecular biology, 430(15):2237–2243, 2018.\nProGen: Language Modeling for Protein Generation\nA. Appendix\nA.1. Measuring out-of-distribution\nThe objective of our work is to enable high-quality protein\ngeneration. To test the effectiveness of our trained model,\nwe had two test subsets: ID-Test and OOD-Test. ID-Test is\na random split of the non-redundant sample database and\ncan be viewed as a typical in-distribution test set of held-out\nsamples.\nIn contrast, OOD-Test represents an out-of-distribution set.\nOOD-Test consists samples that contained a matching sub-\nsequence residing in one of twenty Pfam protein families\nthat were held out of Train and ID-Test.\n3-GRAM SAE 5- GRAM SAE\nTRAIN AND ID-T EST 0.027 0.095\nTRAIN AND OOD-T EST 0.399 1.112\nID-T EST AND OOD-T EST 0.387 1.104\nTable 2.The training data and ID-Test data seem to be drawn from\na similar distribution, but OOD-Test is markedly different from the\nothers. SAE refers to the sum of absolute errors for normalized\n3-gram and 5-gram histograms. If two histograms were entirely\ndivergent, the SAE would yield a value of 2.\nTo quantify the out-of-distribution nature of OOD-Test, we\ncomputed a normalized histogram of 3-grams and 5-grams\nacross samples in the Train, ID-Test, and OOD-Test datasets.\nThe sum of absolute errors (SAE) was computed for a pair of\nhistograms as shown in Table 2. Two normalized histograms\nthat align perfectly would have an SAE of 0 and two normal-\nized histograms that are completely divergent would have\nan SAE of 2. The results imply that the OOD-Test is drawn\nfrom a signiﬁcantly different distribution.\nThe held-out protein families included PF18369,\nPF04680, PF17988, PF12325, PF03272,\nPF03938, PF17724, PF10696, PF11968,\nPF04153, PF06173, PF12378, PF04420,\nPF10841, PF06917, PF03492, PF06905,\nPF15340, PF17055, PF05318.\nA.2. Generation with only conditioning tags\nWe observe that ProGen can be used to generate proteins\nwith only conditioning tags and no initial amino acid context.\nFor the following example, we prompt ProGen to greedily\ngenerate a protein sequence with the tags Flavoprotein\nand FMN. As deﬁned by the UniprotKB keyword, the FMN\ntag refers to “a protein involved in ﬂavin adenine mononu-\ncleotide (FMN) synthesis or protein which contains at least\none FMN as prosthetic group/cofactor (ﬂavoproteins) or\ncosubstrate, such as many oxidation-reduction enzymes”.\nThe generated sequence of length 400 is then passed to the\nHHblits package by Zimmermann et al. (2018) to search\nfor a multiple sequence alignment (MSA). As shown in\nFigure 13, there are multiple sequences that align well with\nthe ProGen sequence. Figures 14-16 demonstrate the align-\nments have high E-values and have related properties. The\nlower the E-value, the lower the probability of a random\nmatch and the higher the probability that the alignment\nmatch is related to the searched sequence.\nA.3. Model visualizations\nProGen was trained from a randomly initialized embedding\nlayer with no prior knowledge of residue biochemical prop-\nerties. Through per-token training on millions of protein\nsequences, ProGen seems to have inherently learned the\nnatural clustering of amino acids that align with our under-\nstanding of biophysicochemical properties. In Figure 12,\nthe trained embedding weights for the standard amino acids\ntokens are reduced to three dimensions with principle com-\nponent analysis (PCA).\nFigure 12.Principle component analysis (PCA) of the ProGen’s\namino acid embeddings aligns with our intuition of amino acid\nproperties.\nUsing Vig (2019), we visualize the attention head patterns\nof ProGen. For both Figure 17 and Figure 18, we are visu-\nalizing the attention weight patterns in each head of ProGen\nfor α-actinin protein (PDB: 4D1E) residues 510 to 528,\nwhich exhibits an alpha helical structure. In Figure 17, we\nvisualize layers 1 to 3 and attention heads 1 to 12 of ProGen.\nThe attention mechanism exhibits well-differentiated local\nand global patterns which may indicate specialization of\neach head on different tasks.\nProGen: Language Modeling for Protein Generation\nFigure 13.There are multiple sequences that align well with the ProGen generated FMN sequence from only conditioning tags. Many of\nthe matching alignments have properties reﬂective of FMN proteins (e.g. oxidoreductases). A red color corresponds to a signiﬁcantly low\nE-value, implying a matching homolog. The MSA was directly taken using HHblits.\nProGen: Language Modeling for Protein Generation\nFigure 14.First alignment (ranked by E-value) of a ProGen generated FMN protein. An E-value less than 1e−4 and identity greater than\n40% is desired to consider the match as potentially homologous. The sequence labeled as Q is the ProGen protein and the sequence\nlabeled as T is the matched sequence.\nProGen: Language Modeling for Protein Generation\nFigure 15.Second alignment (ranked by E-value) of a ProGen generated FMN protein. An E-value less than 1e−4 and identity greater\nthan 40% is desired to consider the match as potentially homologous. The sequence labeled as Q is the ProGen protein and the sequence\nlabeled as T is the matched sequence.\nProGen: Language Modeling for Protein Generation\nFigure 16.Third alignment (ranked by E-value) of a ProGen generated FMN protein. An E-value less than 1e−4 and identity greater\nthan 40% is desired to consider the match as potentially homologous. The sequence labeled as Q is the ProGen protein and the sequence\nlabeled as T is the matched sequence.\nProGen: Language Modeling for Protein Generation\nFigure 17.Attention patterns of ProGen for a given sequence. Layers 1-3 (rows) and attention heads 1-12 (columns) are displayed. The\nattention mechanism exhibits well-differentiated local and global patterns which may indicate specialization of each head on different\ntasks. Two corresponding attention heads from this visualization are shown in Figure 18.\nFigure 18.Local attention pattern for two example attention heads. Lines indicate attention to previous tokens for a given predicted token."
}