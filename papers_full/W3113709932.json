{
  "title": "Rethinking the Value of Transformer Components",
  "url": "https://openalex.org/W3113709932",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2142863397",
      "name": "Wenxuan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126985900",
      "name": "Zhaopeng Tu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4300971654",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2752037867",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W2962822108",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2976872728",
    "https://openalex.org/W2990555152",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W2798761464",
    "https://openalex.org/W2125930537",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2951977278",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2963097630",
    "https://openalex.org/W2970810442",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2991018369",
    "https://openalex.org/W2952524847",
    "https://openalex.org/W2950248853",
    "https://openalex.org/W2963993763",
    "https://openalex.org/W2995816250",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W4294103325",
    "https://openalex.org/W2962966540",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W2952682849"
  ],
  "abstract": "Transformer becomes the state-of-the-art translation model, while it is not well studied how each intermediate component contributes to the model performance, which poses significant challenges for designing optimal architectures. In this work, we bridge this gap by evaluating the impact of individual component (sub-layer) in trained Transformer models from different perspectives. Experimental results across language pairs, training strategies, and model capacities show that certain components are consistently more important than the others. We also report a number of interesting findings that might help humans better analyze, understand and improve Transformer models. Based on these observations, we further propose a new training strategy that can improves translation performance by distinguishing the unimportant components in training.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6019–6029\nBarcelona, Spain (Online), December 8-13, 2020\n6019\nRethinking the Value of Transformer Components\nWenxuan Wang∗\nThe Chinese University of Hong Kong\njwxwang@gmail.com\nZhaopeng Tu†\nTencent AI Lab\nzptu@tencent.com\nAbstract\nTransformer becomes the state-of-the-art translation model, while it is not well studied how each\nintermediate component contributes to the model performance, which poses signiﬁcant chal-\nlenges for designing optimal architectures. In this work, we bridge this gap by evaluating the\nimpact of individual component (sub-layer) in trained Transformer models from different per-\nspectives. Experimental results across language pairs, training strategies, and model capacities\nshow that certain components are consistently more important than the others. We also report a\nnumber of interesting ﬁndings that might help humans better analyze, understand and improve\nTransformer models. Based on these observations, we further propose a new training strategy that\ncan improves translation performance by distinguishing the unimportant components in training.\n1 Introduction\nTransformer (Vaswani et al., 2017) has achieved the state-of-the-art performance on a variety of trans-\nlation tasks. It consists of different stacked components, including self-attention, encoder-attention, and\nfeed-forward layers. However, so far not much is known about the internal properties and functionalities\nit learns to achieve the performance, which poses signiﬁcant challenges for designing optimal architec-\ntures.\nIn this work, we bridge the gap by conducting a granular analysis of components on trained Trans-\nformer models. We attempt to understand how does each component contribute to the model outputs.\nSpeciﬁcally, we explore two metrics to evaluate the impact of a particular component on the model per-\nformance: 1) contribution in information ﬂow that manually masks individual component each time and\nevaluate the performance without that component; and 2)criticality in representation generalizationthat\ndepends on how much closer the weights can get for each component to the initial weights while still\nmaintaining performance. Those two metrics evaluate the component importance of a trained Trans-\nformer model from different perspectives. Empirical results on two benchmarking datasets reveal the\nfollowing observations (§3.1):\n•The decoder self-attention layers are least important, and the decoder feed-forward layers are most\nimportant.\n•The components that are closer to the model input and output (e.g., lower layers of encoder and\nhigher layers of decoder) are more important than components on other layers.\n•Upper encoder-attention layers in decoder are more important than lower encoder-attention layers.\nThe ﬁndings are consistent across different evaluation metrics, translation datasets, initialization seeds\nand model capacities, demonstrating their robustness.\n∗Work done when interning at Tencent AI Lab.\n†Zhaopeng Tu is the corresponding author.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n6020\nWe further analyze the underlying reason ( §3.2), and ﬁnd that lower dropout ratio and more training\ndata lead to less unimportant components. Unimportant components can be identiﬁed at early stage of\ntraining, which are not due to deﬁcient training. Finally, we show that unimportant components can\nbe rewound (Frankle and Carbin, 2019) to further improve the translation performance of Transformer\nmodels (§3.3).\n2 Methodology\nIn this section, we evaluate the importance of individual Transformer components via two different per-\nspective: contribution in information ﬂow and criticality in representation generalization.\n2.1 Contribution in Information Flow\n!(Hn)\n!Hn\nHn\nHn+1 !(Hn)\n!Hn\nHn\nHn+1⨁ ⨂ ⨁\ng\nFigure 1: Contribution of a\nsublayer F to the information\ntransformation.\nThe ultimate goal of machine translation is to fully transform the in-\nformation from the source side to the target side. It is essential to\nunderstand how information ﬂows from the input, across the encoder\nand the decoder, to the output. Figure 1 shows an example to illus-\ntrate how information ﬂows across a basic Transformer component\n(i.e., residual sub-layer).\nWe ﬁrst try to understand how each sub-layer contributes to the in-\nformation ﬂow from input to output. To understand the contribution\nof a particular component in the information ﬂow, we investigate the\neffect of masking that component. Intuitively, we followed Michel et al. (2019) to manually ablate each\ncomponent (i.e., replacing the output with zeros) from a trained Transformer, and evaluated the perfor-\nmance of the resulting masked Transformer. The component is important if the performance without that\ncomponent is signiﬁcantly worse than the full model’s, otherwise it is redundant given the rest of the\nmodel.\nFormally, we deﬁne the contribution score of n-th component as\nContrin =\nˆMn\n˜M\nwhere ˆMn =\n\n\n\n0 Mn <0\nMn 0 <Mn <C\nC M n >C\n, ˜M = max{ˆM1,..., ˆMN} (1)\nwhere Mn is the BLEU drop by ablating the n-th component. It is ﬁrst clipped to the range of [0,C]\nto avoid the minus importance value and exploding drop. Then it is normalized to [0,1] by dividing the\nmaximum drop ˜M. In this study, we set the constant number C as 10% of the BLEU score of baseline\nmodel.\n2.2 Criticality in Representation Generalization\nZhang et al. (2019) reported themodule criticality phenomenon, in which modules of the network present\ndifferent robustness characteristics to parameter perturbation (e.g. rewinding back to its initialization\nvalue). Notice that rewinding to initial value is a relaxation of setting to zeros, due to the initialization of\nTransformer model is Xavier Initialization with 0 means. The module is critical if rewinding its weights\nto the initialization harms the network performance, otherwise it is non-critical in full network.\nChatterji et al. (2020) theoretically formulated this phenomenon and revealed that the criticality metric\nis reﬂective of the network generalization. Speciﬁcally, they used a convex combination of the initial\nweights and the ﬁnal weights of a module to deﬁne an optimization path to traverse. They quantitatively\ndeﬁned the module criticality such that it depends on how much closer the weights can get to the initial\nweights on this path while still maintaining the performance. Figure 2 shows an example. It measures\nhow much the performance of a model rely on speciﬁc module.\nFormally, for then-th component, letθαn\nn = (1−αn)θ0\nn+αnθf\nn, αn ∈[0,1] be the convex combination\nbetween initial weights θ0\nn and the ﬁnal weights θf\nn. We deﬁne the criticality score of n-th component as\nCritin = minαn s.t. BLEU(Model with θf\nn) −BLEU(Model with θαn\nn ) <ϵ. (2)\n6021\n(a) Non-critical module\n (b) Critical module\nFigure 2: Loss values in the valleys that connect the initial weights θ0 to the ﬁnal weights θF of a non-\ncritical (left) and a critical (right) module. Given a ball with radius r (length of the red line), module\ncriticality can be deﬁned as how far one can push the ball in the valley towards initialization (length of\nthe white dashed line) divided by the radius r. Figure credit: Chatterji et al. (2020).\nIn other words, criticality score is the minimum α to maintain the performance drop within a thresh-\nold value ϵ. The criticality score of n-th component is small means we can move the weight of n-th\ncomponent a long way back to initialization without hurting model performance. In this study, we use\nϵ as 0.5 BLEU point, which generally indicates a signiﬁcant drop of translation performance on the\nbenchmarking datasets.\nAlthough both of the two metrics evaluate the component importance in terms of its effect on model\nperformance, there are considerable differences. The contribution score measures the effect of fully\nablating the component on model performance (i.e. hard metric), while the criticality score measures\nhow much the component can be rewound while maintaining the model performance (i.e. soft metric).\n3 Experiments\nData and Setup We conducted experiments on the benchmarking WMT2014 English-German (En-\nDe) and English-French (En-Fr) translation datasets, which consist of 4.6M and 35.5M sentence pairs\nrespectively. We employed BPE (Sennrich et al., 2016) with 32K merge operations for both language\npairs, and used case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as our evaluation metric.\nUnless otherwise stated, the Transformer model consists of 6-layer encoder and decoder. The layer\nsize is 512, the size of feed-forward sub-layer is 2048, and the number of attention heads is 8. We\nfollowed the settings in (Vaswani et al., 2017) to train the Transformer models on the En-De and En-Fr\ndatasets. We set the dropout as 0.1 and the initialization seed as 1 for all Transformer models .\n3.1 Observing Component Importance\nIn this section, we ﬁrst measure the component importance of trained Transformer models. Then we vary\nsome settings, which are threats to validity, to verify the consistency of our ﬁnding.\nSeveral observations on component importance. Figure 3 shows the importance of Transformer\ncomponents measured by two metrics. The two importance metrics agree well with each other, and\nreveal several observations in common:\n•In general, the decoder self-attention layers (“D:SA”) are least important, and the decoder feed-\nforward layers (“D:FF”) are most important.\n•Lower components in encoder (e.g. “E:SA” and “E:FF”) and higher components in decoder (e.g.\n“D:EA” and “D:FF”) are more important. This is intuitive, since these components are closer to the\ninput and output sequences, thus are more important for input understanding and output generation.\n6022\nStep 1ؙ\nEn-De\nEn-Fr\nSeed 1 Seed 66 Seed 99\n(a) En-De: Contribution\nStep 1ؙ\nEn-De\nEn-Fr\nSeed 1 Seed 66 Seed 99 (b) En-Fr: Contribution\n (c) En-De: Criticality\n (d) En-Fr: Criticality\nFigure 3: Importance of individual components measured by (a, b) contribution in information ﬂow, (c,\nd) criticality in representation generalization. Y-axis is the layer id and X-axis is the type of components.\n”E”, ”D”, ”SA”, ”EA” and ”FF” represent Encoder, Decoder, Self-attention, Encoder-attention and Feed-\nforward layer respectively. Darker cells denote more important components.\n•Higher encoder-attention (“D:EA”) layers in decoder are more important than lower encoder-\nattention layers. This is the same in V oita et al. (2019) which claims that lower part of decoder\nis more like a language model. For the other components, the bottom and top layers are more\nimportant than the intermediate layer.\nWe notice the main difference between the results of two metrics is on bottom feed-forward layers in\ndecoder. The contribution score is high but criticality score is low. It is because the performance are bad\nwhen α= 0and 1, but the performance are dramatically good when α≥3. So the contribution is high\nbut criticality is relatively low, according to the deﬁnition in Section 2.\nIn the following experiments, we discuss the threats to validity that could affect our ﬁnding. Unless\notherwise stated, we use contribution score as the default importance metric and report results on the\nEn-De dataset.\n(a) Seed 66\n (b) Seed 99\nStep 4ҁ12੶҂\n (c) Deep Tran.\n (d) Big Transformer\nFigure 4: Component importance for Transformer models with (a, b) different initialization seeds, and\n(c, d) different model capacities on the En-De dataset.\nConsistency across different initialization seeds and model capacities. As aforementioned, we eval-\nuate the component importance based on a trained NMT model, which can be inﬂuenced by various\nhyper-parameters. We identify two hyper-parameters that have been reported to signiﬁcantly inﬂuence\nthe model performance:\n•Initialization Seed: Recent works have shown that neural models are very sensitive to the initializa-\ntion seeds: even with the same hyper-parameter values, distinct random seeds can lead to substan-\ntially different results (Dodge et al., 2020).\n6023\n•Model Capacity: Depth and width are two key aspects in the design of a neural network architec-\nture. Lu et al. (2017) claimed that the depth of a network may determine the abstraction level, and\nthe width may inﬂuence the loss of information in the forwarding pass. Recent studies have also\ndemonstrated the signiﬁcant effect of varying depth (Wang et al., 2019) and width (Vaswani et al.,\n2017) on NMT models.\nFigure 4 shows the results of Transformer models with different initialization seeds and model capacities\non the En-De dataset. Speciﬁcally, we used two other different initialization seeds (i.e., “66” and “99”).\nFor the model capacity setting, we used deeper Transformer (i.e., 12 layer) and wider Transformer (i.e.,\nlayer size be 1024). Clearly, the above conclusions hold in all cases, demonstrating the robustness of our\nﬁndings. In the following experiments, we use Transformer-base with initialization seed 1 as the default\nmodel.\nFigure 5: Component Impor-\ntance for Transformer with\nLayerDrop.\nResults on Transformer trained with structured dropout. The\nTransformer model is trained without being aware of subsequent layer-\nwised ablating, which potentially affects the validity of our conclusions.\nIn response to this problem, we followed Fan et al. (2020) to explore\nLayerDrop, a form of structured dropout, which has a regularization ef-\nfect during training and allows for efﬁcient pruning at inference time.\nLayerDrop randomly drops entire components during training, which\nhas the advantage of making the network robust to subsequent pruning.\nFigure 5 depicts the component importance of Transformer trained with\nLayerDrop, which reconﬁrms our claim that different components at\ndifferent layers make distinct contributions to the model performance.\n3.2 Analyzing Unimportant Components\nIn this section, we delve into further analysis of unimportant components. We ﬁrst ﬁnd several factors that\ncan affect the number of unimportant components. Then we attempt to ﬁnd the reason of the existence of\nunimportant components by representation similarity analysis, learning dynamic analysis and Layerwise\nIsometry Check.\nDropout is the reason\nEn-De\nEn-Fr\nDropout 0\nDropout 0\nDropout 0.1\nDropout 0.1\nDropout 0.3\nDropout 0.3\n(a) Dropout: 0.0\nDropout is the reason\nEn-De\nEn-Fr\nDropout 0\nDropout 0\nDropout 0.1\nDropout 0.1\nDropout 0.3\nDropout 0.3 (b) Dropout: 0.1\nDropout is the reason\nEn-De\nEn-Fr\nDropout 0\nDropout 0\nDropout 0.1\nDropout 0.1\nDropout 0.3\nDropout 0.3 (c) Dropout: 0.3\n (d) Dropout: 0.5\nFigure 6: Effect of dropout ratio on component importance on the En-De dataset.\nLower dropout ratio and more training data lead to less unimportant components. The training\nprocedures of neural networks have rapidly evolved in recent years. In the experiments, we identify some\nfactors that wound affect the number of important components:\n•Dropout (Hinton et al., 2012): Dropout is a commonly used technology to avoid over-ﬁtting by ran-\ndomly dropping model weights with a speciﬁc probability. In order to maintain the functionality,\nthe model trained with dropout tends to have certain redundancy, which may explain our observa-\ntion that some components can be pruned without the degrade of performance (i.e., unimportant\ncomponents).\n6024\n•Training Data Size: Larger-scale training data generally contains more patterns, which may require\nmore components of the Transformer model to learn (i.e., important components).\nFigure 6 shows the effect of dropout ratio on component importance. We varied the dropout ratio in\n[0.0, 0.1, 0.3, 0.5] and trained different Transformer models with different dropout ratios from scratch\non the En-De dataset. The BLEU scores are 25.58, 27.56, 27.43 and 25.72 respectively. Generally,\nthe lower the dropout ratio, the fewer number of unimportant components the model has. One possible\nreason is that higher dropout ratio generally makes the trained model have more redundant components\nto accomplish the same functionality, thus more components can be pruned without degrading the model\nperformance (i.e., unimportant components).\nFigure 7 shows the effect of different training data sizes on component importance. We randomly\nsampled 5M, 10M, 15M, 20M examples from the En-Fr dataset, and trained different Transformer\nmodels on different subset. The BLEU scores are 39.67, 39.94, 40.44 and 40.71 respectively. As seen,\nthe more training data, the more important components are required, which conﬁrms our hypothesis.\nIn all cases, the lowermost E:SA and D:FF components, as well as the uppermost D:EA component\nare identiﬁed as important, which is consistent with the ﬁndings in Section 3.1.\n(a) Data: 5M\n (b) Data: 10M\n (c) Data: 20M\nStep 1ؙ\nEn-De\nEn-Fr\nSeed 1 Seed 66 Seed 99 (d) Data: 35M\nFigure 7: Effect of training data size on component importance on the En-Fr dataset.\nComponent En-De En-Fr\nImportant 0.54 0.55\nUnimportant 0.42 0.43\nTable 1: Representation similarity between\ncomponents output and model output.\nUnimportant components outputs are less similar to\nthe output layer representation. In order to under-\nstand why ablating unimportant layers doesn’t harm the\ntranslation performance, we used Projection Weighted\nCanonical Correlation Analysis (PWCCA) (Morcos et al.,\n2018) to analyze the similarity of layer-wise representa-\ntion between each component output and ﬁnal output. Ta-\nble 1 shows the similarity results. We averaged the similarity score of Top-7 most important layers (listed\nin Figure 9(b,c)) and Top-7 most unimportant layers. The representations of unimportant components\nare less similar to the output layer representation, comparing with important components’.\nUnimportant components can be identiﬁed at early stage of training. Recent studies have revealed\nthat unimportant weights in a dense model can be identiﬁed at early stage of training (You et al., 2020).\nLee et al. (2020) further claimed that the initialization value decides the unimportant weights. Inspired by\nthese ﬁndings, we try to answer the question: are unimportant components created to be unimportant?\nFigure 8 illustrates the learning dynamics of component importance on the En-De dataset. Although\nmost of the important components can be identiﬁed at early stage of training (e.g., epoch 3 or 4), they\ncannot be identiﬁed at initialization. The ﬁnding is also consistent with the similar important components\nof different initialization seeds (Figures 4 (a, b)).\nUnimportant components are not due to deﬁcient training. Some researchers may doubt that a\ncomponent fails to contribute to the model performance (i.e., “unimportant”) since it is not fully trained.\nLee et al. (2020) claimed when the initial weights are not chosen appropriately, the propagation of input\n6025\nFigure 8: Learning dynamics of component importance on the En-De dataset.\nLayer En-De En-Fr\nE:SA E:FF D:SA D:EA D:FF E:SA E:FF D:SA D:EA D:FF\n6 1.44 1.32 0.15 0.14 1.34 1.44 1.36 0.14 0.15 1.36\n5 1.44 1.34 0.17 0.14 1.35 1.44 1.37 0.18 0.15 1.36\n4 1.46 1.34 0.22 0.14 1.33 1.46 1.36 0.25 0.15 1.35\n3 1.45 1.34 0.29 0.14 1.32 1.46 1.36 0.25 0.15 1.35\n2 1.45 1.34 0.40 0.15 1.32 1.46 1.36 0.37 0.15 1.35\n1 1.71 1.34 0.93 0.15 1.32 1.69 1.36 0.98 0.15 1.35\nTable 2: Results of layerwise dynamical isometry check.\nsignals into layers can result in saturating error signals (i.e., gradients) under back propagation, leading\nto training deﬁciency. To dispel the doubt, we check whether a component can be efﬁciently trained by\nanalyzing the signal propagation properties.\nSaxe et al. (2014) introduced dynamical isometry to measure a faithful signal propagation, in which\nsignals propagate in a network isometrically with minimal ampliﬁcation or attenuation. Lee et al. (2020)\nshowed that a sufﬁcient condition to ensure faithful propagation is layerwise dynamical isometry, which\nis deﬁned as singular values of the layerwise Jacobians being concentrated around 1. This can guarantee\nthat the signal from layernis propagated to layern−1 (or vice versa) without ampliﬁcation or attenuation\nin any of its dimension, which leads to efﬁcient update of parameters of the corresponding component.\nTable 2 lists the results of layerwise dynamical isometry check. Each type of components in different\nlayers have similar layerwise isometry values, which cannot explain their different importance on the\nmodel performance. This indicates that the existence of unimportant components are not due to deﬁ-\ncient training (i.e. unfaithful signal propagation). The results on decoder self-attention components are\ndifferent because of the existence of masking.\n3.3 Distinguishing and Utilizing Group of Unimportant components\nIn our previous experiments, we observed the effect of ablating one sub-layer each time, without consid-\nering what would happen if we ablate more layers at the same time. In this section, we ﬁrst identify a\ngroup of unimportant components from a trained Transformer model, and then investigate how to exploit\nthem to improve translation performance.\nIdentify a group of unimportant components from a trained model. We ﬁrst followed Michel et al.\n(2019) to iteratively ablate multiple components from a trained Transformer model, and report the BLEU\nscore of ablated model (without retraining) in Figure 9(a). Although a few unimportant components\n6026\n(a) Ablating unimportant components\n (b) Ablated En-De Model\n (c) Ablated En-Fr Model\nFigure 9: (a) Translation performance of iteratively ablating unimportant components from trained Trans-\nformer models. (b, c) The resulting ablated models on the En-De and En-Fr datasets. Number denote\nablating order.\n(e.g., 3 or 4 components) can be ablated together without performance drop, ablating more components\nsigniﬁcantly harms the translation performance. These results reconﬁrm our analysis on the redundancy\nof components in Section 3.2. For example, suppose two componentsAand Bare considered redundant,\nindividually ablating one of them does not harm model performance. However, it does not necessarily\nmean ablating both of them would not harm the performance as well.\nFigures 9(b, c) list the identiﬁed group of unimportant components in Transformer models trained\non the En-De and En-Fr datasets. Speciﬁcally, we ablated 7 most unimportant components (i.e., 20%\ncomponents) that can harm the model performance most. In the following experiments, we utilize unim-\nportant components to improve translation performance with two strategies, namelycomponents pruning\nand components rewinding.\nModel En-De En-Fr\nPara. BLEU Para. BLEU\nStandard 98M 27.56 95M 40.75\nShallow 89M 27.31 86M 40.12\nPruned 89M 27.60 86M 40.40\nTable 3: Translation performance of pruning unim-\nportant components. “Shallow” denotes a 4-layer\ndecoder model, which has similar number of param-\neters with the pruned model. All models are trained\nfrom scratch with the same hyper-parameters.\nPrune unimportant components and retrain\nthe model. Since some of the layers are con-\nsistently unimportant, we built a model with-\nout those unimportant components and trained it\nfrom scratch (denoted as pruned model). Table 3\nlists the translation performance of the pruned\nmodel. Since the pruned unimportant compo-\nnents are all distributed in the decoder side, we\nalso implemented Transformer model with shal-\nlower decoder, which has the same number of\nparameters with the pruned model. As seen, the\npruned model achieves competitive performance\nwith the standard Transformer, and consistently outperforms the shallow model, demonstrating the rea-\nsonableness of the identiﬁed unimportant components.\nTraining En-De En-Fr\nStep BLEU Step BLEU\nStandard 100K 27.56 150K 40.75\n+ Continue +20K 27.53 +20K 40.63\n+ Rewind +20K 27.70 +20K 41.03\nTable 4: Translation performance of rewinding\nunimportant components. “Step” denotes training\nsteps.\nRewind unimportant components and ﬁne-\ntune the model. Recent studies have shown\nthat rejuvenating dead neurons can improve\nmodel performance by enhancing the resource\nutilization of neural networks to further realize\ntheir potentials (Qiao et al., 2019). Inspired by\nthis ﬁnding, we rewound the unimportant com-\nponent to the initialization values Frankle and\nCarbin (2019; Renda et al. (2020) and ﬁne-tune\n6027\nthem together with the other trained components\nfor a few more steps. For a fair comparison, we also ﬁne-tuned the trained Transformer model for the\nsame number of steps. As listed in Table 4, directly ﬁne-tuning the Transformer model (“Continue”)\ndoes not outperform the standard Transformer, while the rewind technique can further improve transla-\ntion performance.\n4 Related Work\nOur work is inspired by two lines of research: Interpreting transformer and network pruning.\nInterpreting Transformer Tranformer (Vaswani et al., 2017) has advanced the state of the art in var-\nious NLP tasks. Recently, there has been an increasing amount of work on interpreting speciﬁc com-\nponents of Transformer, such as encoder representations (Raganato and Tiedemann, 2018; Tang et al.,\n2019a; Yang et al., 2019), multi-head self-attention (Li et al., 2018; V oita et al., 2019; Michel et al., 2019;\nGeng et al., 2020), and encoder attention (Jain and Wallace, 2019; Li et al., 2019; Tang et al., 2019b).\nClosely related to our work, Domhan (2018) investigated how much each component of Transformer\nmatters. They revealed that self-attention is more important for the encoder side than the decoder side,\nand encoder attention and residual feed-forward components are key. The key difference between their\nwork and ours is that they evaluated the impact of individual component by retraining a model with other\ncomponents, while we investigate their contribution on a trained model. In addition, we conduct more\nsubtle analyses on components at different layers, and show other interesting ﬁndings, e.g. the lowest\nand uppermost layers are generally more important than intermediate layers.\nNetwork Pruning The state-of-the-art deep neural networks are usuallyover-parameterized: they have\nmuch more parameters than the number of training samples. (Denton et al., 2014). Recent study has\nshown that more than 90% of the parameters can be pruned without harming the performance of neural\nnetworks (Frankle and Carbin, 2019). In response to this problem, several researchers propose pruning\nto extract sub-networks from the over-parameterized network with no decrease of model performance.\nBased on the granularity level of pruning, network pruning methods can be divided into weight pruning\nand structured pruning. Weight pruning approaches prune the sparse weights distributed in different\ncomponents (Han et al., 2015; Han et al., 2016), while structured pruning removes coherent groups of\nweights to preserve the original structure of the network (Lin et al., 2017; Huang et al., 2018).\nIn the NLP community, recent studies have shown that Transformer is over-parameterized. For exam-\nple, V oita et al. (2019) and Michel et al. (2019) showed that most self-attention heads can be dropped.\nFan et al. (2020) reduced Transformer depth on demand with structured dropout. Along this direction,\nwe analyze the redundancy of Transformer on components level and reveal several interesting ﬁndings.\n5 Conclusion\nIn this work we investigate the impact of individual components in Transformer on model performance.\nExperimental results in a couple of settings show that different components are not equally important.\nThe decoder self-attention layers are least important, and the decoder feed-forward layers are most impor-\ntant. The components that are closer to the model input and output are consistently more important than\nthe others. Upper encoder-attention layers in decoder are more important than lower encoder-attention\nlayers. Further in-depth analyses reveal that the dropout and training data size can affect the number\nof unimportant components. We also ﬁnd that unimportant components can be identiﬁed at early stage\nof training and their existence is not because of deﬁcient training. Finally, we show that rewinding the\nunimportant components and then ﬁne-tuning the Transformer model for a few more steps can further\nimprove translation performance.\nFuture directions include designing better approaches to evaluate the impact of components (e.g.,\nfrom the perspective of information ﬂow), and validating our ﬁndings on other NMT architectures such\nas RNMT (Chen et al., 2018) and ConvS2S (Gehring et al., 2017).\n6028\nReferences\nNiladri Chatterji, Behnam Neyshabur, and Hanie Sedghi. 2020. The intriguing role of module criticality in the\ngeneralization of deep networks. In ICLR.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki\nParmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2018. The Best of Both Worlds:\nCombining Recent Advances in Neural Machine Translation. In ACL.\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. 2014. Exploiting linear structure\nwithin convolutional networks for efﬁcient evaluation. In NeurIPS.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-\nTuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. arXiv.\nTobias Domhan. 2018. How much attention do you need? a granular analysis of neural machine translation\narchitectures. In ACL.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing Transformer Depth on Demand with Structured\nDropout. ICLR.\nJonathan Frankle and Michael Carbin. 2019. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural\nNetworks. In ICLR.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence\nto sequence learning. In ICML.\nXinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, and Zhaopeng Tu. 2020. How Does Selective\nMechanism Improve Self-Attention Networks? In ACL.\nSong Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both weights and connections for efﬁcient\nneural networks. In NeurIPS.\nSong Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Compressing Deep Neural Networks with\nPruning, Trained Quantization and Huffman Coding. In ICLR.\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012.\nImproving neural networks by preventing co-adaptation of feature detectors. In arXiv.\nGao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2018. CondenseNet: An Efﬁcient\nDenseNet using Learned Group Convolutions. In CVPR.\nSarthak Jain and Byron C. Wallace. 2019. Attention is not explanation. In NAACL.\nNamhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip H. S. Torr. 2020. A signal propagation\nperspective for pruning neural networks at initialization. ArXiv, abs/1906.06307.\nJian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, and Tong Zhang. 2018. Multi-head attention with dis-\nagreement regularization. In EMNLP.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and Shuming Shi. 2019. On the word alignment from neural\nmachine translation. In ACL.\nJi Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. 2017. Runtime neural pruning. In NeurIPS.\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. The expressive power of neural\nnetworks: A view from the width. In NeurIPS.\nPaul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really Better than One? In NeurIPS.\nAri S. Morcos, Maithra Raghu, and Samy Bengio. 2018. Insights on representational similarity in neural networks\nwith canonical correlation. In NeurIPS.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\nof machine translation. In ACL.\nSiyuan Qiao, Zhe Lin, Jianming Zhang, and Alan Yuille. 2019. Neural rejuvenation: Improving deep network\ntraining by enhancing computational resource utilization. In CVPR.\n6029\nAlessandro Raganato and J¨org Tiedemann. 2018. An Analysis of Encoder Representations in Transformer-Based\nMachine Translation. In EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP.\nAlex Renda, Jonathan Frankle, and Michael Carbin. 2020. Comparing rewinding and ﬁne-tuning in neural network\npruning. In ICLR.\nAndrew Michael Saxe, James L McClelland, and Surya Ganguli. 2014. Exact solutions to the nonlinear dynamics\nof learning in deep linear neural networks. In ICLR.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with\nSubword Units. In ACL.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019a. Encoders help you disambiguate word senses in neural\nmachine translation. In EMNLP.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019b. Understanding neural machine translation by simpliﬁca-\ntion: The case of encoder-free models. In RANLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In NeurIPS.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL.\nXing Wang, Zhaopeng Tu, Longyue Wang, and Shuming Shi. 2019. Exploiting sentential context for neural\nmachine translation. In ACL.\nBaosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Zhaopeng Tu. 2019. Assessing the ability of\nself-attention networks to learn word order. In ACL.\nHaoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Yingyan Lin, Zhangyang Wang,\nand Richard Baraniuk. 2020. Drawing early-bird tickets: Towards more efﬁcient training of deep networks.\nICLR.\nChiyuan Zhang, Samy Bengio, and Yoram Singer. 2019. Are all layers created equal? ICML Workshop.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8048524856567383
    },
    {
      "name": "Computer science",
      "score": 0.6653959155082703
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.4418867826461792
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39680272340774536
    },
    {
      "name": "Machine learning",
      "score": 0.33212190866470337
    },
    {
      "name": "Engineering",
      "score": 0.2256530225276947
    },
    {
      "name": "Electrical engineering",
      "score": 0.1561301350593567
    },
    {
      "name": "Voltage",
      "score": 0.1358354389667511
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 32
}