{
  "title": "Comparitive performance of artificial intelligence-based large language models on the orthopedic in-training examination",
  "url": "https://openalex.org/W4408145305",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2551862756",
      "name": "Andrew Y. Xu",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2128164301",
      "name": "Manjot Singh",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A4222608624",
      "name": "Mariah Balmaceno-Criss",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A5102580182",
      "name": "Allison Oh",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2002175640",
      "name": "David Leigh",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2171791433",
      "name": "Mohammad Daher",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A3014145270",
      "name": "Daniel Alsoof",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2629223927",
      "name": "Christopher L. McDonald",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A248203812",
      "name": "Bassel G Diebo",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2097493005",
      "name": "Alan H. Daniels",
      "affiliations": [
        "Brown University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4220737633",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4360840406",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4368360859",
    "https://openalex.org/W3119050231",
    "https://openalex.org/W4377711218",
    "https://openalex.org/W6854475153",
    "https://openalex.org/W6811129797",
    "https://openalex.org/W4200099554",
    "https://openalex.org/W429083659",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W6846076640",
    "https://openalex.org/W4361295009",
    "https://openalex.org/W4214873977",
    "https://openalex.org/W4300817506",
    "https://openalex.org/W3110784362",
    "https://openalex.org/W3097023869",
    "https://openalex.org/W4323295362",
    "https://openalex.org/W4367312358"
  ],
  "abstract": "Background Large language models (LLMs) have many clinical applications. However, the comparative performance of different LLMs on orthopedic board style questions remains largely unknown. Methods Three LLMs, OpenAIâ€™s GPT-4 and GPT-3.5, and Google Bard, were tested on 189 official 2022 Orthopedic In-Training Examination (OITE) questions. Comparative analyses were conducted to assess their performance against orthopedic resident scores and on higher-order, image-associated, and subject category-specific questions. Results GPT-4 surpassed the passing threshold for the 2022 OITE, performing at the level of PGY-3 to PGY-5 ( p = .149, p = .502, and p = .818, respectively) and outperforming GPT-3.5 and Bard ( p &lt; .001 and p = .001, respectively). While GPT-3.5 and Bard did not meet the passing threshold for the exam, GPT-3.5 performed at the level of PGY-1 to PGY-2 ( p = .368 and p = .019, respectively) and Bard performed at the level of PGY-1 to PGY-3 ( p = .440, .498, and 0.036, respectively). GPT-4 outperformed both Bard and GPT-3.5 on image-associated ( p = .003 and p &lt; .001, respectively) and higher-order questions ( p &lt; .001). Among the 11 subject categories, all models performed similarly regardless of the subject matter. When individual LLM performance on higher-order questions was assessed, no significant differences were found compared to performance on first order questions (GPT-4 p = .139, GPT-3.5 p = .124, Bard p = .319). Finally, when individual model performance was assessed on image-associated questions, only GPT-3.5 performed significantly worse compared to performance on non-image-associated questions ( p = .045). Conclusion The AI-based LLM GPT-4, exhibits a robust ability to correctly answer a diverse range of OITE questions, exceeding the minimum score for the 2022 OITE, and outperforming predecessor GPT-3.5 and Google Bard.",
  "full_text": null,
  "topic": "Orthopedic surgery",
  "concepts": [
    {
      "name": "Orthopedic surgery",
      "score": 0.7119326591491699
    },
    {
      "name": "Medicine",
      "score": 0.6261742115020752
    },
    {
      "name": "Subject (documents)",
      "score": 0.5277764797210693
    },
    {
      "name": "Subject matter",
      "score": 0.4742240905761719
    },
    {
      "name": "Order (exchange)",
      "score": 0.4445143938064575
    },
    {
      "name": "Internal medicine",
      "score": 0.4015331268310547
    },
    {
      "name": "Surgery",
      "score": 0.35887643694877625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3285856246948242
    },
    {
      "name": "Psychology",
      "score": 0.17236316204071045
    },
    {
      "name": "Library science",
      "score": 0.15365532040596008
    },
    {
      "name": "Computer science",
      "score": 0.13684684038162231
    },
    {
      "name": "Curriculum",
      "score": 0.11929634213447571
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27804330",
      "name": "Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    }
  ]
}