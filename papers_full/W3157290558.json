{
  "title": "Handwritten Mathematical Expression Recognition with Bidirectionally Trained Transformer",
  "url": "https://openalex.org/W3157290558",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5039730001",
      "name": "Wenqi Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004474713",
      "name": "Liangcai Gao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047456533",
      "name": "Zuoyu Yan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061781593",
      "name": "Shuai Peng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025131129",
      "name": "Lin Du",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101552414",
      "name": "Ziyin Zhang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2895924502",
    "https://openalex.org/W2886776719",
    "https://openalex.org/W2806199895",
    "https://openalex.org/W2521665229",
    "https://openalex.org/W2528252828",
    "https://openalex.org/W2623860192",
    "https://openalex.org/W2409722449",
    "https://openalex.org/W2037121285",
    "https://openalex.org/W3004196288",
    "https://openalex.org/W2466062786",
    "https://openalex.org/W2005079280",
    "https://openalex.org/W3002575754",
    "https://openalex.org/W1998768285",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1978799108",
    "https://openalex.org/W2113029741",
    "https://openalex.org/W2750938222",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2072449446",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3031548983",
    "https://openalex.org/W3176268775",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W1810641085",
    "https://openalex.org/W3159162875",
    "https://openalex.org/W2935811960",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3034644216",
    "https://openalex.org/W3110178865",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2133656308",
    "https://openalex.org/W2785966045",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W3208345164",
    "https://openalex.org/W2963637207",
    "https://openalex.org/W2996840337",
    "https://openalex.org/W2905413777",
    "https://openalex.org/W2031071334",
    "https://openalex.org/W3130079480",
    "https://openalex.org/W3034799230"
  ],
  "abstract": "Encoder-decoder models have made great progress on handwritten mathematical expression recognition recently. However, it is still a challenge for existing methods to assign attention to image features accurately. Moreover, those encoder-decoder models usually adopt RNN-based models in their decoder part, which makes them inefficient in processing long $\\LaTeX{}$ sequences. In this paper, a transformer-based decoder is employed to replace RNN-based ones, which makes the whole model architecture very concise. Furthermore, a novel training strategy is introduced to fully exploit the potential of the transformer in bidirectional language modeling. Compared to several methods that do not use data augmentation, experiments demonstrate that our model improves the ExpRate of current state-of-the-art methods on CROHME 2014 by 2.23%. Similarly, on CROHME 2016 and CROHME 2019, we improve the ExpRate by 1.92% and 2.28% respectively.",
  "full_text": "Handwritten Mathematical Expression\nRecognition with Bidirectionally Trained\nTransformer\nWenqi Zhao1[0000‚àí0002‚àí2952‚àí0531], Liangcai Gao1(\u0000 ), Zuoyu Yan1, Shuai\nPeng1, Lin Du2, and Ziyin Zhang 2\n1 Wangxuan Institute of Computer Technology, Peking University, Beijing, China\n1027572886a@gmail.com\n{gaoliangcai,yanzuoyu,pengshuaipku}@pku.edu.cn\n2 Huawei AI Application Research Center\n{dulin09,zhangziyin1}@huawei.com\nAbstract. Encoder-decoder models have made great progress on hand-\nwritten mathematical expression recognition recently. However, it is still\na challenge for existing methods to assign attention to image features\naccurately. Moreover, those encoder-decoder models usually adopt RNN-\nbased models in their decoder part, which makes them ineÔ¨Écient in pro-\ncessing long LATEX sequences. In this paper, a transformer-based de-\ncoder is employed to replace RNN-based ones, which makes the whole\nmodel architecture very concise. Furthermore, a novel training strategy\nis introduced to fully exploit the potential of the transformer in bidirec-\ntional language modeling. Compared to several methods that do not use\ndata augmentation, experiments demonstrate that our model improves\nthe ExpRate of current state-of-the-art methods on CROHME 2014 by\n2.23%. Similarly, on CROHME 2016 and CROHME 2019, we improve\nthe ExpRate by 1.92% and 2.28% respectively.\nKeywords: handwritten mathematical expression recognition ¬∑ trans-\nformer ¬∑ bidirection ¬∑ encoder-decoder model\n1 Introduction\nThe encoder-decoder models have shown quite eÔ¨Äective performance on various\ntasks such as scene text recognition [6] and image captioning [25]. Handwrit-\nten Mathematical Expression Recognition (HMER) aims to generate the math\nexpression LATEX sequence according to the handwritten math expression im-\nage. Since HMER is also an image to text modeling task, many encoder-decoder\nmodels [34,36] have been proposed for it in recent years.\nHowever, existing methods suÔ¨Äer from the lack of coverage problem [36] to\nvarying degrees. This problem refers to two possible manifestations: over-parsing\nand under-parsing. Over-parsing means that some regions of HME image are\nredundantly translated multiple times, while under-parsing denotes that some\nregions remain untranslated.\narXiv:2105.02412v3  [cs.CV]  16 May 2021\n2 W. Zhao et al.\nMost encoder-decoder models are RNN-based models, which have diÔ¨Éculty\nmodeling the relationship between two symbols that are far apart. Previous\nstudy [2] had noted this long-term dependency problem caused by gradient van-\nishing. This problem is exposed more obviously in HMER task. Compared to\ntraditional natural language processing, L ATEX is a markup language designed\nby human, and thus has a clearer and more distinct syntactic structure, e.g., ‚Äú{‚Äù\nand ‚Äú}‚Äù are bound to appear in pairs. When dealing with long LATEX sequences,\nit is diÔ¨Écult for RNN-based models to capture the relationship between two\ndistant ‚Äú{‚Äù and ‚Äú }‚Äù symbol, resulting in lack of awareness of the L ATEX syntax\nspeciÔ¨Åcation.\nTraditional autoregressive models [34,36] use left-to-right (L2R) direction to\npredict symbols one by one in the inference phase. Such approaches may gen-\nerate unbalanced outputs [15], which preÔ¨Åxes are usually more accurate than\nsuÔ¨Éxes. To overcome this problem, existing study [15] employ two independent\ndecoders, trained for left-to-right and right-to-left directions, respectively. This\nusually leads to more parameters and longer training time. Therefore, an intu-\nitive attempt is to adapt a single decoder for bi-directional language modeling.\nIn this paper, we employ the transformer [22] decoder into HMER task, alle-\nviating the lack of coverage problem [36] by using positional encodings. Besides,\na novel bidirectional training strategy is proposed to obtain a Bidirectionally\nTrained TRansformer (BTTR) model. The strategy enables single transformer\ndecoder to perform both L2R and R2L decoding. We further show that our\nBTTR model outperforms RNN-based ones in terms of both training paralleliza-\ntion and inferencing accuracy. The main contributions of our work are summa-\nrized as follows:\n‚Ä¢To the best of our knowledge, it is the Ô¨Årst attempt to use end-to-end trained\ntransformer decoder for solving HMER task.\n‚Ä¢The combination of image and word positional encodings enable each time\nstep to accurately assign attention to diÔ¨Äerent regions of input image, alle-\nviating the lack of coverage problem.\n‚Ä¢A novel bidirectional training strategy is proposed to perform bidirectional\nlanguage modeling in a single transformer decoder.\n‚Ä¢Compared to several methods that do not use data augmentation, exper-\niments demonstrate that our method obtains new SOTA performance on\nvarious dataset, including an ExpRate of 57.91%, 54.49%, and 56.88% on\nthe CROHME 2014 [18], CROHME 2016 [19], and CROHME 2019 [17] test\nsets, respectively.\n‚Ä¢We make our code available on the GitHub. 3\n2 Related Work\n2.1 HMER Methods\nIn the last decades, many approaches [4,13,27‚Äì29,32,37] related to HMER have\nbeen proposed. These approaches can be divided into two categories: grammar-\n3 https://github.com/Green-Wood/BTTR\nHMER with Bidirectionally Trained Transformer 3\nbased and encoder-decoder based. In this section, we will brieÔ¨Çy review the\nrelated work in both categories.\nGrammar Based These methods usually consist of three parts: symbol seg-\nmentation, symbol recognition, and structural analysis. Researchers have pro-\nposed a variety of predeÔ¨Åned grammars to solve HMER task, such as stochastic\ncontext-free grammars [1], relational grammars [16], and deÔ¨Ånite clause gram-\nmars [3,5]. None of these grammar rules are data-driven, but are hand-designed,\nwhich could not beneÔ¨Åt from large dataset.\nEncoder-Decoder Based In recent years, a series of encoder-decoder models\nhave been widely used in various tasks [26, 30, 31]. In HMER tasks, Zhang et\nal. [36] observed the lack of coverage problem and proposed WAP model to solve\nthe HMER task. In the subsequent studies, DenseWAP [34] replaced VGG en-\ncoder in the WAP with DenseNet [11] encoder, and improved the performance.\nFurther, DenseWAP-TD [35] enhanced the model‚Äôs ability to handle complex\nformulas by substituting string decoder with a tree decoder. Wu et al. [24] used\nstroke information and formulated the HMER as a graph-to-graph(G2G) model-\ning task. Such encoder-decoder based models have achieved outstanding results\nin several CROHME competitions [17‚Äì19].\n2.2 Transformer\nTransformer [22] is a neural network architecture based solely on attention mech-\nanisms. Its internal self-attention mechanism makes transformer a breakthrough\ncompared to RNN in two aspects. Firstly, transformer does not need to depend\non the state of the previous step as RNN does. Well-designed parallelization\nallows transformer to save a lot of time in the training phase. Secondly, to-\nkens in the same sequence establish direct one-to-one connections through the\nself-attention mechanism. Such a mechanism fundamentally solves the gradient\nvanishing problem of RNN [2], making transformer more suitable than RNN on\nlong sequences. In recent years, RNN is replaced by transformer in various tasks\nin computer vision and natural language processing [8,20].\nRecently, transformer has been used in oÔ¨Ñine handwritten text recognition.\nKang et al. [14] Ô¨Årst adopted transformer networks for the handwritten text\nrecognition task and achieved state-of-the-art performance. For the task of math-\nematical expression, ‚ÄúUniv. Linz‚Äù method in CROHME 2019 [17] used a Faster\nR-CNN detector and a transformer decoder. Two subsystems were trained sep-\narately.\n2.3 Right-to-Left Language Modeling\nTo solve the problem that traditional autoregressive models can only perform\nleft-to-right(L2R) language modeling, many studies have attempted right-to-\nleft(R2L) language modeling. Liu et al. [15] trained a R2L model separately.\n4 W. Zhao et al.\nDuring the inference phase, hypotheses from L2R and R2L models are re-ranked\nto produce the best candidate. Furthermore, Zhang et al. [38] used R2L model to\nregularize L2R model in the training phase to obtain a better L2R model. Zhou\net al. [39] proposed SB-NMT that utilized a single decoder to generate sequences\nbidirectionally. However, all of these methods increase model complexity. On the\nother hand, our approach achieves bidirectional language modeling on a single\ndecoder while keeping the model concise.\n3 Methodology\nIn this section, we will detailed introduce the proposed BTTR model archi-\ntecture, as illustrated in Fig. 1. In section 3.1, we will brieÔ¨Çy describe the\nDenseNet [11] model used in the encoder part. In section 3.2 and 3.3, the posi-\ntional encodings and transformer model used in the encoder and decoder part\nwill be described in detail. Finally in section 3.4, we will introduce the proposed\nnovel bidirectional training strategy, which allows to perform bidirectional lan-\nguage modeling in a single transformer decoder.\nCNN\nAdd & NormFeed ForwardAdd & NormMulti-HeadAttention\nAdd & NormMaskedMulti-HeadAttention\nImage Positional Encoding\nWordPositionalEncodingEmbeddingOutputs ‚Éóùë¶;‚Éñùë¶(shifted right)\nLinear\nSoftmax\nOutputs ‚Éóùë¶;‚Éñùë¶Probabilities\n+\n+\n√óN\nInput Image\nFig. 1.The architecture of BTTR model. L2R and R2L sequences [‚àí ‚Üíy ; ‚Üê ‚àíy ] are concate-\nnated through the batch dimension as the input to decoder part.\nHMER with Bidirectionally Trained Transformer 5\n3.1 CNN Encoder\nIn the encoder part, DenseNet is used as the feature extractor for HME images.\nThe main idea of DenseNet is to increase information Ô¨Çow between layers by\nintroducing direct connections between each layer and all its subsequent layers.\nIn this way, given output features x0,x1,..., xl‚àí1 from 0th to (l‚àí1)th layer, the\noutput feature of lth layer can be computed by:\nx‚Ñì = H‚Ñì ([x0; x1; ... ; x‚Ñì‚àí1]) (1)\nwhere [x0; x1; ... ; x‚Ñì‚àí1] denotes the concatenation operation of all the output\nfeatures, and H‚Ñì(¬∑) denotes a composite function of three consecutive layers: a\nbatch normalization (BN) [12] layer, followed by a ReLU [9] layer and a 3 √ó3\nconvolution (Conv) layer.\nThrough concatenation operation in the channel dimension, DenseNet en-\nables better propagation of gradient. The paper [11] states that by this dense\nconnection, DenseNet can achieve better performance with fewer parameters\ncompared to ResNet [10].\nIn addition to DenseNet, we also add a 1 √ó1 convolution layer in the encoder\npart to adjust the image feature dimension to the size of embedding dimension\ndmodel for subsequent processing.\n3.2 Positional Encoding\nThe positional information of image features and word vectors can eÔ¨Äectively\nhelp the model to identify regions that need to attend. In the previous studies [34,\n36], although the RNN-based model inherently takes the order of word vectors\ninto account, it neglects the positional information of image features.\nIn this paper, since the transformer model itself doesn‚Äôt have any sense of\nposition for each input vector, we use two types of positional encodings to address\nthis information. In detail, we use image positional encodings and word positional\nencodings to represent the image feature position and word vector position,\nrespectively.\nWe refer to image features and word vector features as content-based, and\nthe two types of positional encodings as position-based. As illustrated in Fig. 1,\ncontent-based and position-based features are summed up, as the input to trans-\nformer decoder.\nWord Positional EncodingWord positional encoding is basically the same\nas sinusoidal positional encoding proposed in the original transformer work [22].\nGiven position posand dimension das input, the word positional encoding vector\npW\npos,d is deÔ¨Åned as:\npW\npos,d[2i] = sin(pos/100002i/d) (2)\npW\npos,d[2i+ 1] = cos(pos/100002i/d) (3)\nwhere i is the index in dimension.\n6 W. Zhao et al.\nImage Positional Encoding A 2-D normalized positional encoding is used\nto represent the image position features. We Ô¨Årst compute sinusoidal positional\nencoding pW\npos,d/2 in each of the two dimensions and then concatenate them\ntogether. Given a 2-D position tuple (x,y) and the same dimension das the word\npositional encoding, the image positional encoding vector pI\nx,y,d is represented\nas:\n¬Øx= x\nH, ¬Øy= y\nW (4)\npI\nx,y,d = [pW\n¬Øx,d/2; pW\n¬Øy,d/2] (5)\nwhere H and W are height and width of input images.\n3.3 Transformer Decoder\nFor the decoder part, we use the standard transformer model [22]. Each basic\ntransformer decoder layer module consists of four essential parts. In the follow-\ning, we will describe the implementation details of these components.\nScaled Dot-Product AttentionThis attention mechanism is essentially us-\ning the query to obtain the value from key-value pairs, based on the similarity\nbetween the query and key. The output matrix can be computed in parallel by\nthe query Q, key K, value V, and dimension dk.\nAttention(Q,K,V) = softmax(QKT\n‚àödk\n)V (6)\nMulti-Head Attention With multi-head mechanism, the scaled dot-product\nattention module can attend to feature-map from multiple representation sub-\nspaces jointly. With projection parameter matrices WQ\ni ‚ààRdmodel √ódk ,WK\ni ‚àà\nRdmodel √ódk ,WV\ni ‚ààRdmodel √ódv , we Ô¨Årst project the query Q, key K, and value\nV into a subspace to compute the head Hi.\nHi = Attention\n(\nQWQ\ni ,KWK\ni ,VWV\ni\n)\n(7)\nThen all the heads are concatenated and projected with a parameter matrix\nWO ‚ààRhdv√ódmodel and the number of heads h.\nMultiHead(Q,K,V) = [H1; ... ; Hh] WO (8)\nMasked Multi-Head AttentionIn the decoder part, due to the autoregres-\nsive property, the next symbol is predicted based on the input image and previ-\nously generated symbols. In the training phase, a lower triangle mask matrix is\nused to enable the self-attention module to restrict the attention region for each\ntime step. Due to masked multi-head attention mechanism, the whole training\nprocess requires only one forward computation.\nHMER with Bidirectionally Trained Transformer 7\nPosition-wise Feed-Forward NetworkFeed-Forward Network(FNN) con-\nsists of three operations: a linear transformation, a ReLU activation function\nand another linear transformation.\nFFN(x) = max (0,xW1 + b1) W2 + b2 (9)\nAfter multi-head attention, the information between positions has been fully\nexchanged. FFN enables each position to integrate its own internal information\nseparately.\n3.4 Bidirectional Training Strategy\nFirst, two special symbols ‚Äú‚ü®SOS‚ü©‚Äù and ‚Äú‚ü®EOS‚ü©‚Äù are introduced in the dictionary\nto denote the start and end of the sequence. For the target L ATEX sequence\ny = {y1,...,y T }, we denote the target sequence from left to right (L2R) as‚àí ‚Üíy = {‚ü®SOS‚ü©,y1,...,y T ,‚ü®EOS‚ü©}, and the right-to-left (R2L) target sequence as‚Üê ‚àíy = {‚ü®EOS‚ü©,yT ,...,y 1,‚ü®SOS‚ü©}.\nConditioned on image x and model parameter Œ∏, the traditional autoregres-\nsive model need to compute the probability distribution:\np(‚àí ‚Üíyj |‚àí ‚Üíy<j,x,Œ∏ ) (10)\nwhere j is the index in target sequence.\nIn this paper, since the transformer model itself does not actually care about\nthe order of input symbols, we can use a single transformer decoder for bi-\ndirectional language modeling. Modeling both Eq. (10) and Eq. (11) at the same\ntime.\np(‚Üê ‚àíyj |‚Üê ‚àíy<j,x,Œ∏ ) (11)\nTo achieve this goal, a simple yet eÔ¨Äective bidirectional training strategy is pro-\nposed, in which for each training sample, we generate two target sequences, L2R\nand R2L, from the target LATEX sequence, and compute the training loss in the\nsame batch (details in Section 4.2). Compared with unidirectional language mod-\neling, our approach trains a model to perform bidirectional language modeling\nwithout sacriÔ¨Åcing model conciseness. Experiment results in Section 5.3 verify\nthe eÔ¨Äectiveness of our bidirectional training strategy.\n4 Implementation Details\n4.1 Networks\nIn the encoder part, to make a fair comparison with the previous state-of-the-\nart method, we use the same DenseNet feature extractor as the DenseWAP\nmodel [34]. SpeciÔ¨Åcally, three bottleneck layers are used in the backbone network\nand transition layers are added in between to reduce the number of feature-maps.\nIn each bottleneck layer, we set the growth rate to k = 24, the depth of each\n8 W. Zhao et al.\nblock to D= 16, and the compression hyperpatameter of the transition layer to\nŒ∏= 0.5.\nIn the decoder part, we use the standard transformer model. We set the\nembedded dimension and model dimension to dmodel = 256, the number of heads\nin the multi-head attention module to H = 8, the dimension of intermediate\nlayers in the FFN to dff = 1024, and the number of transformer decoder layer\nto N = 3. The 0.3 dropout rate is used to prevent overÔ¨Åtting.\n4.2 Training\nOur training objective is to maximize the predicted probability of the ground\ntruth symbols in Eq. (10) and Eq. (11), so we use the standard cross-entropy\nloss function to calculate the loss between the predicted probabilities w.r.t. the\nground truth at each decoding position. Given the training sample\n{\nx(z),y(z)}Z\nz=1,\nthe objective function for optimization is shown as follows:\n‚àí ‚ÜíL(z)\nj (Œ∏) = ‚àílog p(‚àí ‚Üíy(z)\nj |‚àí ‚Üíy(z)\n<j ,x(z),Œ∏) (12)\n‚Üê ‚àíL(z)\nj (Œ∏) = ‚àílog p(‚Üê ‚àíy(z)\nj |‚Üê ‚àíy(z)\n<j ,x(z),Œ∏) (13)\nL(Œ∏) = 1\n2ZL\nZ‚àë\nz=1\nL‚àë\nj=1\n(‚àí ‚ÜíL(z)\nj (Œ∏) + ‚Üê ‚àíL(z)\nj (Œ∏)\n)\n(14)\nThe model is trained from scratch using the Adadelta algorithm [33] with a\nweight decay of 10 ‚àí4 , œÅ = 0.9, and œµ = 10‚àí6. PyTorch framework is used to\nimplement our model. The model is trained on four NVIDIA 1080Ti GPUs with\n11 √ó4 GB memory.\n4.3 Inferencing\nIn the inference phase, we aim to generate the most likely L ATEX sequence con-\nditioned on the input image. Which can be fomulated as follows:\nÀÜy = argmax\ny\np(y |x,Œ∏) (15)\nwhere x denotes the input image and Œ∏ denotes the model parameter.\nUnlike the training phase where a lower triangular mask matrix is used to\ngenerate the prediction for all time steps simultaneously. Since we have no ground\ntruth of the previously predicted symbol, we can only predict symbols one by one\nuntil the ‚ÄúEnd‚Äù symbol appears or the predeÔ¨Åned maximum length is reached.\nObviously, we cannot search for all possible sequences, thus a heuristic beam\nsearch is proposed to balance the computational cost with the quality of decod-\ning. Further, taking advantage of the fact that our decoder is capable of bidirec-\ntional language modeling, approximate joint search [15] is used to improve the\nperformance. The basic idea consists of three steps: (1) Firstly, a beam search\nHMER with Bidirectionally Trained Transformer 9\nis performed on L2R and R2L directions to obtain two k-best hypotheses.(2)\nThen, we reverse L2R hypotheses to R2L direction and R2L hypotheses to L2R\ndirection and treat these hypotheses as ground truth to compute the loss values\nfor each of them as in the training phase.(3) Finally, those loss values are added\nto their original hypothesis scores to obtain the Ô¨Ånal scores, which is then used\nto Ô¨Ånd the best candidate. In practice, we set beam size k = 10, the maximum\nlength to 200, and length penalty Œ±= 1.0.\n5 Experiments\n5.1 Datasets\nWe use the Competition on Recognition of Online Handwritten Mathematical\nExpressions (CROHME) benchmark, which currently is the largest dataset for\nhandwritten mathematical expression to validate the proposed model. We use\nthe same training dataset but diÔ¨Äerent test datasets. The training set contains\n8836 handwritten mathematical expressions, while the test sets of CROHME\n2014/2016/2019 contain 986/1147/1199 expressions respectively.\nIn the CROHME dataset, each handwritten math expression is saved in a\nInkML Ô¨Åle, which contains handwritten stroke trajectory information and ground\ntruth in both MathML and LATEX formats. We transform the handwritten stroke\ntrajectory information in the InkML Ô¨Åles to oÔ¨Ñine images in bitmap format for\ntraining and testing. With oÔ¨Écial evaluation tools provided by the CROHME\n2019 [17] organizers and the ground truth in symLG format, we convert the\npredicted LATEX sequences to symLG format and evaluate the performance.\n5.2 Compare with state-of-the-art results\nThe results of some models on the CROHME 2014/2016/2019 datasets are shown\nin Table 1. To ensure the fairness of performance comparison, the methods we\nshow all use only the oÔ¨Écially provided 8836 training samples. Neither we nor\nthe methods we compare use data augmentation.\nWe Ô¨Årst provide results of three traditional handwritten mathematical for-\nmula recognition methods based on tree grammar in CROHME 2014 as the base-\nline, denoted as I, VI, and VII. For CROHME 2016, we provide the best perform-\ning ‚ÄúTOKYO‚Äù method as the baseline, which only used oÔ¨Écial training samples.\nFor CROHME 2019 oÔ¨Écial methods, we provide ‚ÄúUniv. Linz‚Äù [17] method as\nthe baseline. For Image-to-LATEX methods, we use the previous state-of-the-art\n‚ÄúWYGIWYS‚Äù [7], ‚ÄúPAL-v2‚Äù [23], ‚ÄúWAP‚Äù [36], ‚ÄúWeakly supervised WAP‚Äù(WS\nWAP) [21], ‚ÄúDenseWAP‚Äù [34] as well as the tree decoder-based ‚ÄúDenseWAP-\nTD‚Äù [35] method. The ‚ÄúOurs-Uni‚Äù and ‚ÄúOurs-Bi‚Äù methods denote the model\ntrained using the vanilla transformer decoder and the model trained with the\nbidirectional training strategy on top of it.\nIn Table 1, by comparing ‚ÄúDenseWAP‚Äù with ‚ÄúOurs-Uni‚Äù, both are unidirec-\ntional string decoder based models, we can obtain 4.29% average performance\n10 W. Zhao et al.\nimprovement in ExpRate by simply replacing the RNN-based decoder with the\nvanilla transformer decoder.\nCompared with other methods, our proposed BTTR model outperforms the\nprevious state-of-the-art methods in nearly all metrics and is about 3.4% ahead\nof the ‚ÄúDenseWAP-TD‚Äù method in ExpRate, which explicitly encodes our prior\nknowledge about the LATEX grammar through a tree decoder.\nTable 1.Performance comparison of single models on the CROHME 2014/2016/2019\ntest sets(in %), where ‚ÄúExpRate‚Äù, ‚Äú ‚â§1 error‚Äù and ‚Äú ‚â§2 error‚Äù columns mean expres-\nsion recognition rate when zero to two structural or symbol errors can be tolerated.\n‚ÄúStruRate‚Äù column means structure recognition rate.\nDataset Model ExpRate ‚â§1 error ‚â§2 error StruRate\nCROHME14\nI 37.22 44.22 47.26 -\nVI 25.66 33.16 35.90 -\nVII 26.06 33.87 38.54 -\nWYGIWYS 36.4 - - -\nWAP 40.4 56.1 59.9 -\nDenseWAP 43.0 57.8 61.9 63.2\nPAL-v2 48.88 64.50 69.78 -\nDenseWAP-TD 49.1 64.2 67.8 68.6\nWS WAP 53.65 - - -\nOurs-Uni 48.17 59.63 63.29 65.01\nOurs-Bi 53.96 66.02 70.28 71.40\nCROHME16\nTOKYO 43.94 50.91 53.70 61.6\nWAP 37.1 - - -\nDenseWAP 40.1 54.3 57.8 59.2\nPAL-v2 49.61 64.08 70.27 -\nDenseWAP-TD 48.5 62.3 65.3 65.9\nWS WAP 51.96 64.34 70.10 -\nOurs-Uni 44.55 55.88 60.59 61.55\nOurs-Bi 52.31 63.90 68.61 69.40\nCROHME19\nUniv. Linz 41.49 54.13 58.88 60.02\nDenseWAP 41.7 55.5 59.3 60.7\nDenseWAP-TD 51.4 66.1 69.1 69.8\nOurs-Uni 44.95 56.13 60.47 60.63\nOurs-Bi 52.96 65.97 69.14 70.06\n5.3 Ablation Study\nIn Table 2, the Ô¨Årst ‚ÄúIPE‚Äù column denotes whether to use image positional\nencoding or not. Secondly, the ‚ÄúBi-Trained‚Äù column shows whether the bidirec-\nHMER with Bidirectionally Trained Transformer 11\ntional training strategy is used. On the ‚ÄúAJS‚Äù column, \u0013 indicates the use of\napproximate joint search [15], while \u0017 represents L2R search. The last ‚ÄúEnsem-\nble‚Äù column denotes whether the ensemble method is used.\nFirst, we can see that whether to use image positional encoding or not makes\nhuge diÔ¨Äerence on the CROHME 2019 test set. This shows that image positional\nencoding improves the generalization ability of our model in diÔ¨Äerent scales.\nComparing the 2 nd and the 3 rd rows in each dataset, we can see that the\nmodel trained using the bidirectional training strategy still outperforms the uni-\ndirectionally trained model by about 2.52% in ExpRate, though both of them\nusing L2R search. This shows that while training a bidirectional language model,\nthe bidirectional training strategy also helps the whole model to extract infor-\nmation from the images more comprehensively.\nFurther, using the properties of the bidirectional language model, we evaluate\nthe decoding results of both L2R and R2L directions using approximate joint\nsearch, resulting in an improvement of about 4.68% in ExpRate.\nFinally we report the results using the ensemble method, showing that this\ncan signiÔ¨Åcantly improve the overall recognition performance by about 3.35%\nin ExpRate. SpeciÔ¨Åcally, We train Ô¨Åve models initialized with diÔ¨Äerent random\nseeds and average their prediction probabilities at each decoding step.\nTable 2.Ablation study on the CROHME 2014/2016/2019 test sets(in %)\nDataset IPE Bi-Trained AJS Ensemble ExpRate\nCROHME14\n\u0017 \u0017 \u0017 \u0017 45.13\n\u0013 \u0017 \u0017 \u0017 48.17\n\u0013 \u0013 \u0017 \u0017 49.49\n\u0013 \u0013 \u0013 \u0017 53.96\n\u0013 \u0013 \u0013 \u0013 57.91\nCROHME16\n\u0017 \u0017 \u0017 \u0017 43.33\n\u0013 \u0017 \u0017 \u0017 44.55\n\u0013 \u0013 \u0017 \u0017 46.90\n\u0013 \u0013 \u0013 \u0017 52.31\n\u0013 \u0013 \u0013 \u0013 54.49\nCROHME19\n\u0017 \u0017 \u0017 \u0017 20.77\n\u0013 \u0017 \u0017 \u0017 44.95\n\u0013 \u0013 \u0017 \u0017 48.79\n\u0013 \u0013 \u0013 \u0017 52.96\n\u0013 \u0013 \u0013 \u0013 56.88\n12 W. Zhao et al.\n5.4 Case Study\nAs can be seen in Fig. 2, we give several case studies for the ‚ÄúDenseWAP‚Äù,\n‚ÄúOurs-Uni‚Äù and ‚ÄúOurs-Bi‚Äù models. These three models use the same DenseNet\nencoder. The diÔ¨Äerence between these three models is that ‚ÄúDenseWAP‚Äù uses\nan RNN-based decoder, ‚ÄúOurs-Uni‚Äù adapts a vanilla transformer decoder, and\n‚ÄúOurs-Bi‚Äù uses the bidirectional training strategy to train transformer decoder.\nFirstly, comparing the prediction results between ‚ÄúDenseWAP‚Äù and ‚ÄúOurs-\nUni‚Äù, we can see that for input images with complex structure, the ‚ÄúDenseWAP‚Äù\nmodel cannot predict all the symbols completely. Moreover, for input image with\ndiscontinuous structure, the right half is unnecessarily predicted twice. Problems\nmentioned above reÔ¨Çecting under-parsing and over-parsing phenomenon. How-\never, the ‚ÄúOurs-Uni‚Äù and ‚ÄúOurs-Bi‚Äù models employed with positional encodings\nare able to identify all the symbols in these images accurately.\nSecondly, by comparing the prediction results of ‚ÄúOurs-Uni‚Äù and ‚ÄúOurs-Bi‚Äù,\nwe Ô¨Ånd that ‚ÄúOurs-Bi‚Äù gives more accurate predictions. Owing to the re-score\nmechanism in approximate joint search procedure, ‚ÄúOurs-Bi‚Äù avoids the asym-\nmetry of ‚Äú{‚Äù and ‚Äú}‚Äù in the prediction results given by ‚ÄúOurs-Uni‚Äù.\nImageDenseWAPOurs-UniOurs-Bib ^ { -1 -1 }= b ^ { -1 } a ^ { -1 } <EOS> b ^ { -1 } c ^ { -1 } = b ^ { -1 } a ^ { -1 } <EOS>b ^ { -1 } c ^ { -1 } = b ^ { -1 } a ^ { -1 } <EOS>\n\\tan \\alpha _ { i} = \\alpha _ { in } <EOS> \\tan \\alpha _ { i} <EOS> \\tan \\alpha _ { i} <EOS>\n\\frac { 4 x ^ { 2 } -9 } { 4x+1 2 x + 9 } <EOS>\\frac { 4 x ^ { 2 } -9 } { 4 x ^ { 2 } + 1 2 x + 9<EOS>\\frac { 4 x ^ { 2 } -9 } { 4 x ^ { 2 } + 1 2 x + 9 } <EOS>\n\\frac { 1 } { \\sqrt { 2 } } ( \\frac { b } { \\sqrt { z} -0 })<EOS>\n\\frac { 1 } { \\sqrt { 2 } } ( \\frac { b } { \\sqrt { 2 } } }-0 ) <EOS>\n\\frac { 1 } { \\sqrt { 2 } } ( \\frac { b } { \\sqrt { 2 } } -0 ) <EOS>\nFig. 2.Case studies for the ‚ÄúDenseWAP‚Äù [34], ‚ÄúOurs-Uni‚Äù and ‚ÄúOurs-Bi‚Äù models. The\nred symbols represent incorrect predictions, while the green symbols represent correct\npredictions.\nHMER with Bidirectionally Trained Transformer 13\n6 Conclusion\nIn this paper, a novel bidirectionally trained trans former model is proposed for\nhandwritten mathematical expression recognition task. Compared to previous\napproaches, our proposed BTTR model has the following three advantages: (1)\nThrough image positional encoding, the model could capture the location in-\nformation of the feature-map to guide itself to reasonably assign attention and\nalleviate the lack of coverage problem. (2) We take the advantage of the trans-\nformer model‚Äôs permutation-invariant property to train a decoder with bidirec-\ntional language modeling capability. The bidirectional training strategy enables\nBTTR model to make predictions in both L2R and R2L directions while ensur-\ning model simplicity. (3) The RNN-based decoder is replaced by the transformer\ndecoder to improve the parallelization of training process. Experiments demon-\nstrate the eÔ¨Äectiveness of our proposed BTTR model. Concretely speaking, our\nmodel gets ExpRate scores of 57.91%, 54.49%, and 56.88% on the CROHME\n2014, CROHME 2016, and CROHME 2019 respectively.\nAcknowledgments\nThis work is supported by the projects of National Key R&D Program of China\n(2019YFB1406303) and National Natural Science Foundation of China (No.\n61876003), which is also a research achievement of Key Laboratory of Science,\nTechnology and Standard in Press Industry (Key Laboratory of Intelligent Press\nMedia Technology).\nReferences\n1. Alvaro, F., S¬¥ anchez, J.A., Bened¬¥ ƒ±, J.M.: Recognition of on-line handwritten mathe-\nmatical expressions using 2d stochastic context-free grammars and hidden markov\nmodels. Pattern Recognition Letters 35, 58‚Äì67 (2014)\n2. Bengio, Y., Frasconi, P., Simard, P.: The problem of learning long-term dependen-\ncies in recurrent networks. In: IEEE international conference on neural networks.\npp. 1183‚Äì1188. IEEE (1993)\n3. Chan, K.F., Yeung, D.Y.: An eÔ¨Écient syntactic approach to structural analysis of\non-line handwritten mathematical expressions. Pattern recognition 33(3), 375‚Äì384\n(2000)\n4. Chan, K.F., Yeung, D.Y.: Mathematical expression recognition: a survey. Interna-\ntional Journal on Document Analysis and Recognition 3(1), 3‚Äì15 (2000)\n5. Chan, K.F., Yeung, D.Y.: Error detection, error correction and performance eval-\nuation in on-line mathematical expression recognition. Pattern Recognition 34(8),\n1671‚Äì1684 (2001)\n6. Cheng, Z., Bai, F., Xu, Y., Zheng, G., Pu, S., Zhou, S.: Focusing attention: To-\nwards accurate text recognition in natural images. In: Proceedings of the IEEE\ninternational conference on computer vision. pp. 5076‚Äì5084 (2017)\n7. Deng, Y., Kanervisto, A., Rush, A.M.: What you get is what you see: A visual\nmarkup decompiler. arXiv preprint arXiv:1609.04938 10, 32‚Äì37 (2016)\n14 W. Zhao et al.\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n9. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiÔ¨Åer neural networks. In: Pro-\nceedings of the fourteenth international conference on artiÔ¨Åcial intelligence and\nstatistics. pp. 315‚Äì323 (2011)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770‚Äì778 (2016)\n11. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected\nconvolutional networks. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. pp. 4700‚Äì4708 (2017)\n12. IoÔ¨Äe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\n13. Jiang, Z., Gao, L., Yuan, K., Gao, Z., Tang, Z., Liu, X.: Mathematics content\nunderstanding for cyberlearning via formula evolution map. In: Proceedings of the\n27th ACM International Conference on Information and Knowledge Management.\npp. 37‚Äì46 (2018)\n14. Kang, L., Riba, P., RusiÀú nol, M., Forn¬¥ es, A., Villegas, M.: Pay attention to\nwhat you read: Non-recurrent handwritten text-line recognition. arXiv preprint\narXiv:2005.13044 (2020)\n15. Liu, L., Utiyama, M., Finch, A., Sumita, E.: Agreement on target-bidirectional\nneural machine translation. In: Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies. pp. 411‚Äì416 (2016)\n16. MacLean, S., Labahn, G.: A new approach for recognizing handwritten mathemat-\nics using relational grammars and fuzzy sets. International Journal on Document\nAnalysis and Recognition (IJDAR) 16(2), 139‚Äì163 (2013)\n17. Mahdavi, M., Zanibbi, R., Mouchere, H., Viard-Gaudin, C., Garain, U.: Icdar 2019\ncrohme+ tfd: Competition on recognition of handwritten mathematical expressions\nand typeset formula detection. In: 2019 International Conference on Document\nAnalysis and Recognition (ICDAR). pp. 1533‚Äì1538. IEEE (2019)\n18. Mouchere, H., Viard-Gaudin, C., Zanibbi, R., Garain, U.: Icfhr 2014 competition\non recognition of on-line handwritten mathematical expressions (crohme 2014). In:\n2014 14th International Conference on Frontiers in Handwriting Recognition. pp.\n791‚Äì796. IEEE (2014)\n19. Mouch` ere, H., Viard-Gaudin, C., Zanibbi, R., Garain, U.: Icfhr2016 crohme: Com-\npetition on recognition of online handwritten mathematical expressions. In: 2016\n15th International Conference on Frontiers in Handwriting Recognition (ICFHR).\npp. 607‚Äì612. IEEE (2016)\n20. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: International Conference on Machine Learning. pp. 4055‚Äì\n4064. PMLR (2018)\n21. Truong, T.N., Nguyen, C.T., Phan, K.M., Nakagawa, M.: Improvement of end-\nto-end oÔ¨Ñine handwritten mathematical expression recognition by weakly super-\nvised learning. In: 2020 17th International Conference on Frontiers in Handwriting\nRecognition (ICFHR). pp. 181‚Äì186. IEEE (2020)\n22. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30, 5998‚Äì6008 (2017)\nHMER with Bidirectionally Trained Transformer 15\n23. Wu, J.W., Yin, F., Zhang, Y.M., Zhang, X.Y., Liu, C.L.: Handwritten mathemati-\ncal expression recognition via paired adversarial learning. International Journal of\nComputer Vision pp. 1‚Äì16 (2020)\n24. Wu, J.W., Yin, F., Zhang, Y., Zhang, X.Y., Liu, C.L.: Graph-to-graph: Towards ac-\ncurate and interpretable online handwritten mathematical expression recognition.\nAAAI 2021 (2021)\n25. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,\nBengio, Y.: Show, attend and tell: Neural image caption generation with visual\nattention. In: International conference on machine learning. pp. 2048‚Äì2057 (2015)\n26. Yan, Z., Ma, T., Gao, L., Tang, Z., Chen, C.: Persistence homology for link pre-\ndiction: An interactive view. arXiv preprint arXiv:2102.10255 (2021)\n27. Yan, Z., Zhang, X., Gao, L., Yuan, K., Tang, Z.: Convmath: A convolutional se-\nquence network for mathematical expression recognition. In: 2020 25th Interna-\ntional Conference on Pattern Recognition (ICPR). pp. 4566‚Äì4572. IEEE (2021)\n28. Yuan, K., Gao, L., Jiang, Z., Tang, Z.: Formula ranking within an article. In:\nProceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries. pp.\n123‚Äì126 (2018)\n29. Yuan, K., Gao, L., Wang, Y., Yi, X., Tang, Z.: A mathematical information re-\ntrieval system based on rankboost. In: Proceedings of the 16th ACM/IEEE-CS on\nJoint Conference on Digital Libraries. pp. 259‚Äì260 (2016)\n30. Yuan, K., He, D., Jiang, Z., Gao, L., Tang, Z., Giles, C.L.: Automatic generation\nof headlines for online math questions. In: Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence. vol. 34, pp. 9490‚Äì9497 (2020)\n31. Yuan, K., He, D., Yang, X., Tang, Z., Kifer, D., Giles, C.L.: Follow the curve:\nArbitrarily oriented scene text detection using key points spotting and curve pre-\ndiction. In: 2020 IEEE International Conference on Multimedia and Expo (ICME).\npp. 1‚Äì6. IEEE (2020)\n32. Zanibbi, R., Blostein, D.: Recognition and retrieval of mathematical expressions.\nInternational Journal on Document Analysis and Recognition (IJDAR)15(4), 331‚Äì\n357 (2012)\n33. Zeiler, M.D.: Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701 (2012)\n34. Zhang, J., Du, J., Dai, L.: Multi-scale attention with dense encoder for handwritten\nmathematical expression recognition. In: 2018 24th international conference on\npattern recognition (ICPR). pp. 2245‚Äì2250. IEEE (2018)\n35. Zhang, J., Du, J., Yang, Y., Song, Y.Z., Wei, S., Dai, L.: A tree-structured decoder\nfor image-to-markup generation. In: ICML. p. In Press (2020)\n36. Zhang, J., Du, J., Zhang, S., Liu, D., Hu, Y., Hu, J., Wei, S., Dai, L.: Watch,\nattend and parse: An end-to-end neural network based approach to handwritten\nmathematical expression recognition. Pattern Recognition 71, 196‚Äì206 (2017)\n37. Zhang, X., Gao, L., Yuan, K., Liu, R., Jiang, Z., Tang, Z.: A symbol dominance\nbased formulae recognition approach for pdf documents. In: 2017 14th IAPR In-\nternational Conference on Document Analysis and Recognition (ICDAR). vol. 1,\npp. 1144‚Äì1149. IEEE (2017)\n38. Zhang, Z., Wu, S., Liu, S., Li, M., Zhou, M., Xu, T.: Regularizing neural ma-\nchine translation by target-bidirectional agreement. In: Proceedings of the AAAI\nConference on ArtiÔ¨Åcial Intelligence. vol. 33, pp. 443‚Äì450 (2019)\n39. Zhou, L., Zhang, J., Zong, C.: Synchronous bidirectional neural machine trans-\nlation. Transactions of the Association for Computational Linguistics 7, 91‚Äì105\n(2019)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.782932460308075
    },
    {
      "name": "Transformer",
      "score": 0.7650375366210938
    },
    {
      "name": "Encoder",
      "score": 0.7438967227935791
    },
    {
      "name": "Exploit",
      "score": 0.5738363265991211
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5507272481918335
    },
    {
      "name": "Language model",
      "score": 0.42852553725242615
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42803260684013367
    },
    {
      "name": "Speech recognition",
      "score": 0.39342281222343445
    },
    {
      "name": "Machine learning",
      "score": 0.3292655646800995
    },
    {
      "name": "Engineering",
      "score": 0.1117570698261261
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": []
}