{
  "title": "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection",
  "url": "https://openalex.org/W2963824106",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2611597884",
      "name": "Xinru Yan",
      "affiliations": [
        "University of Minnesota, Duluth"
      ]
    },
    {
      "id": "https://openalex.org/A2099107867",
      "name": "Ted Pedersen",
      "affiliations": [
        "University of Minnesota, Duluth"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979401726",
    "https://openalex.org/W2251092808",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2001465333",
    "https://openalex.org/W2015089823",
    "https://openalex.org/W2126263492",
    "https://openalex.org/W2033175753",
    "https://openalex.org/W2007780422",
    "https://openalex.org/W1844374543",
    "https://openalex.org/W2753059774",
    "https://openalex.org/W2252089152",
    "https://openalex.org/W2055496638"
  ],
  "abstract": "This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.",
  "full_text": "Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 385–389,\nVancouver, Canada, August 3 - 4, 2017.c⃝2017 Association for Computational Linguistics\nDuluth at SemEval-2017 T ask 6: Language Models in Humor Detection\nXinru Y an & T ed Pedersen\nDepartment of Computer Science\nUniversity of Minnesota Duluth\nDuluth, MN, 55812 USA\n{yanxx418,tpederse}@d.umn.edu\nAbstract\nThis paper describes the Duluth system\nthat participated in SemEval-2017 T ask 6\n#HashtagW ars: Learning a Sense of Hu-\nmor. The system participated in Subtasks\nA and B using N-gram language models,\nranking highly in the task evaluation. This\npaper discusses the results of our system\nin the development and evaluation stages\nand from two post-evaluation runs.\n1 Introduction\nHumor is an expression of human uniqueness\nand intelligence and has drawn attention in di-\nverse areas such as linguistics, psychology , phi-\nlosophy and computer science. Computational\nhumor draws from all of these ﬁelds and is\na relatively new area of study . There is\nsome history of systems that are able to gener-\nate humor (e.g., (\nStock and Strapparava , 2003),\n( ¨Ozbal and Strapparava , 2012)). However, hu-\nmor detection remains a less explored and chal-\nlenging problem (e.g., ( Mihalcea and Strapparava ,\n2006), ( Zhang and Liu , 2014), ( Shahaf et al. ,\n2015), ( Miller and Gurevych , 2015)).\nSemEval-2017 T ask 6 ( Potash et al. , 2017) also\nfocuses on humor detection by asking participants\nto develop systems that learn a sense of humor\nfrom the Comedy Central TV show , @midnight\nwith Chris Hardwick . Our system ranks tweets ac-\ncording to how funny they are by training N-gram\nlanguage models on two different corpora. One\nconsisting of funny tweets provided by the task\norganizers, and the other on a freely available re-\nsearch corpus of news data. The funny tweet data\nis made up of tweets that are intended to be hu-\nmorous responses to a hashtag given by host Chris\nHardwick during the program.\n2 Background\nTraining Language Models (LMs) is a straight-\nforward way to collect a set of rules by utilizing\nthe fact that words do not appear in an arbitrary\norder; we in fact can gain useful information about\na word by knowing the company it keeps (\nFirth,\n1968). A statistical language model estimates the\nprobability of a sequence of words or an upcom-\ning word. An N-gram is a contiguous sequence of\nN words: a unigram is a single word, a bigram is a\ntwo-word sequence, and a trigram is a three-word\nsequence. For example, in the tweet\ntears in Ramen #SingleLifeIn3W ords\n“tears”, “in”, “Ramen” and “#Sin-\ngleLifeIn3W ords” are unigrams; “tears in”,\n“in Ramen” and “Ramen #SingleLifeIn3W ords”\nare bigrams and “tears in Ramen” and “in Ramen\n#SingleLifeIn3W ords” are trigrams.\nAn N-gram model can predict the next word\nfrom a sequence of N-1 previous words. A tri-\ngram Language Model (LM) predicts the condi-\ntional probability of the next word using the fol-\nlowing approximation:\nP (wn|wn−1\n1 ) ≈ P (wn|wn−2, wn−1) (1)\nThe assumption that the probability of a word\ndepends only on a small number of previous words\nis called a Markov assumption (\nMarkov, 2006).\nGiven this assumption the probability of a sen-\ntence can be estimated as follows:\nP (wn\n1 ) ≈\nn∏\nk=1\nP (wk|wk−2, wk−1) (2)\nIn a study on how phrasing affects memorabil-\nity , (\nDanescu-Niculescu-Mizil et al. , 2012) take a\nlanguage model approach to measure the distinc-\ntiveness of memorable movie quotes. They do this\n385\nby evaluating a quote with respect to a “common\nlanguage” model built from the newswire sec-\ntions of the Brown corpus (\nKucera and Francis ,\n1967). They ﬁnd that movie quotes which are less\nlike “common language” are more distinctive and\ntherefore more memorable. The intuition behind\nour approach is that humor should in some way be\nmemorable or distinct, and so tweets that diverge\nfrom a “common language” model would be ex-\npected to be funnier.\nIn order to evaluate how funny a tweet is, we\ntrain language models on two datasets: the tweet\ndata and the news data. T weets that are more prob-\nable according to the tweet data language model\nare ranked as being funnier. However, tweets that\nhave a lower probability according to the news lan-\nguage model are considered the funnier since they\nare the least like the (unfunny) news corpus. W e\nrelied on both bigrams and trigrams when training\nour models.\nW e use KenLM (\nHeaﬁeld et al. , 2013) as our\nlanguage modeling tool. Language models are\nestimated using modiﬁed Kneser-Ney smoothing\nwithout pruning. KenLM also implements a back-\noff technique so if an N-gram is not found, KenLM\napplies the lower order N-gram’s probability along\nwith its back-off weights.\n3 Method\nOur system\n1 estimated tweet probability using N-\ngram LMs. Speciﬁcally , it solved the comparison\n(Subtask A) and semi-ranking (Subtask B) sub-\ntasks in four steps:\n1. Corpus preparation and pre-processing: Col-\nlected all training data into a single ﬁle. Pre-\nprocessing included ﬁltering and tokeniza-\ntion.\n2. Language model training: Built N-gram lan-\nguage models using KenLM.\n3. T weet scoring: Computed log probability for\neach tweet based on trained N-gram language\nmodel.\n4. T weet prediction: Based on the log probabil-\nity scores.\n• Subtask A – Given two tweets, compare\nand predict which one is funnier.\n1 https://xinru1414.github.io/HumorDetection-\nSemEval2017-T ask6/\n• Subtask B – Given a set of tweets associ-\nated with one hashtag, rank tweets from\nthe funniest to the least funny .\n3.1 Corpus Preparation and Pre-processing\nThe tweet data was provided by the task orga-\nnizers. It consists of 106 hashtag ﬁles made up\nof about 21,000 tokens. The hashtag ﬁles were\nfurther divided into a development set trial\ndir\nof 6 hashtags and a training set of 100 hashtags\ntrain\ndir. W e also obtained 6.2 GB of English\nnews data with about two million tokens from the\nNews Commentary Corpus and the News Crawl\nCorpus from 2008, 2010 and 2011\n2 . Each tweet\nand each sentence from the news data is found on\na single line in their respective ﬁles.\n3.1.1 Preparation\nDuring the development of our system we trained\nour language models solely on the 100 hashtag\nﬁles from train\ndir and then evaluated our per-\nformance on the 6 hashtag ﬁles found in trial dir.\nThat data was formatted such that each tweet was\nfound on a single line.\n3.1.2 Pre-processing\nPre-processing consists of two steps: ﬁltering and\ntokenization. The ﬁltering step was only for the\ntweet training corpus. W e experimented with vari-\nous ﬁltering and tokenziation combinations during\nthe development stage to determine the best set-\nting.\n• Filtering removes the following elements\nfrom the tweets: URLs, tokens starting with\nthe “@” symbol (T witter user names), and to-\nkens starting with the “#” symbol (Hashtags).\n• T okenization: T ext in all training data was\nsplit on white space and punctuation\n3.2 Language Model T raining\nOnce we had the corpora ready , we used the\nKenLM T oolkit to train the N-gram language mod-\nels on each corpus. W e trained using both bigrams\nand trigrams on the tweet and news data. Our lan-\nguage models accounted for unknown words and\nwere built both with and without considering sen-\ntence or tweet boundaries.\n2 http://www .statmt.org/wmt11/featured-translation-\ntask.html\n386\n3.3 T weet Scoring\nAfter training the N-gram language models, the\nnext step was scoring. For each hashtag ﬁle that\nneeded to be evaluated, the logarithm of the proba-\nbility was assigned to each tweet in the hashtag ﬁle\nbased on the trained language model. The larger\nthe probability , the more likely that tweet was ac-\ncording to the language model. T able 1 shows an\nexample of two scored tweets from hashtag ﬁle\nBad\nJob In 5 W ords.tsv based on the tweet data\ntrigram language model. Note that KenLM reports\nthe log of the probability of the N-grams rather\nthan the actual probabilities so the value closer to\n0 (-19) has the higher probability and is associated\nwith the tweet judged to be funnier.\n3.4 T weet Prediction\nThe system sorts all the tweets for each hashtag\nand orders them based on their log probability\nscore, where the funniest tweet should be listed\nﬁrst. If the scores are based on the tweet lan-\nguage model then they are sorted in ascending or-\nder since the log probability value closest to 0 indi-\ncates the tweet that is most like the (funny) tweets\nmodel. However, if the log probability scores are\nbased on the news data then they are sorted in de-\nscending order since the largest value will have the\nsmallest probability associated with it and is there-\nfore least like the (unfunny) news model.\nFor Subtask A, the system goes through the\nsorted list of tweets in a hashtag ﬁle and com-\npares each pair of tweets. For each pair, if the\nﬁrst tweet was funnier than the second, the system\nwould output the tweet\nids for the pair followed\nby a “1”. If the second tweet is funnier it outputs\nthe tweet\nids followed by a “0”. For Subtask B,\nthe system outputs all the tweet ids for a hashtag\nﬁle starting from the funniest.\n4 Experiments and Results\nIn this section we present the results from our de-\nvelopment stage ( T able 2 ), the evaluation stage\n(T able 3 ), and two post-evaluation results ( T a-\nble 3 ). Since we implemented both bigram and\ntrigam language models during the development\nstage but only results from trigram language mod-\nels were submitted to the task, we evaluated\nbigram language models in the post-evaluation\nstage. Note that the accuracy and distance mea-\nsurements listed in T able 2 and T able 3 are deﬁned\nby the task organizers (\nPotash et al. , 2017).\nT able 2 shows results from the development\nstage. These results show that for the tweet data\nthe best setting is to keep the # and @, omit sen-\ntence boundaries, be case sensitive, and ignore to-\nkenization. While using these settings the trigram\nlanguage model performed better on Subtask B\n(.887) and the bigram language model performed\nbetter on Subtask A (.548). W e decided to rely\non trigram language models for the task evalua-\ntion since the advantage of bigrams on Subtask A\nwas very slight (.548 versus .543). For the news\ndata, we found that the best setting was to per-\nform tokenization, omit sentence boundaries, and\nto be case sensitive. Given that trigrams performed\nmost effectively in the development stage, we de-\ncided to use those during the evaluation.\nT able 3 shows the results of our system dur-\ning the task evaluation. W e submitted two runs,\none with a trigram language model trained on the\ntweet data, and another with a trigram language\nmodel trained on the news data. In addition, after\nthe evaluation was concluded we also decided to\nrun the bigram language models as well. Contrary\nto what we observed in the development data, the\nbigram language model actually performed some-\nwhat better than the trigram language model. In\naddition, and also contrary to what we observed\nwith the development data, the news data proved\ngenerally more effective in the post–evaluation\nruns than the tweet data.\n5 Discussion and Future W ork\nW e relied on bigram and trigram language mod-\nels because tweets are short and concise, and often\nonly consist of just a few words.\nThe performance of our system was not con-\nsistent when comparing the development to the\nevaluation results. During development language\nmodels trained on the tweet data performed bet-\nter. However during the evaluation and post-\nevaluation stage, language models trained on the\nnews data were signiﬁcantly more effective. W e\nalso observed that bigram language models per-\nformed slightly better than trigram models on the\nevaluation data. This suggests that going forward\nwe should also consider both the use of unigram\nand character–level language models.\nThese results suggest that there are only slight\ndifferences between bigram and trigram models,\nand that the type and quantity of corpora used to\ntrain the models is what really determines the re-\n387\nThe hashtag: #BadJobIn5W ords\ntweet id tweet score\n705511149970726912 The host of Singled Out #Bad-\nJobIn5W ords @midnight\n-19.923433303833008\n705538894415003648 Donut receipt maker and sorter\n#BadJobIn5W ords @midnight\n-27.67446517944336\nT able 1: Scored tweets according to the trigram LM. The log probability scor es computed based on the\ntrigram LM are shown in the third column.\nDataSet N-gram # and\n@ re-\nmoved\nSentence\nBound-\naries\nLowercase T okenization Subtask A\nAccuracy\nSubtask B\nDistance\ntweets trigram False False False False 0.543 0.887\ntweets bigram False False False False 0.548 0.900\ntweets trigram False True True False 0.522 0.900\ntweets bigram False True True False 0.534 0.887\nnews trigram NA False False T rue 0.539 0.923\nnews bigram NA False False True 0.524 0.924\nnews trigram NA False False False 0.460 0.923\nnews bigram NA False False False 0.470 0.900\nT able 2: Development results based on trial dir data. The settings we chose to train LMs are in bold.\nDataSet N-gram # and\n@ re-\nmoved\nSentence\nBound-\naries\nLowercase T okenization Subtask A\nAccuracy\nSubtask B\nDistance\ntweets trigram False False False False 0.397 0.967\ntweets bigram False False False False 0.406 0.944\nnews trigram NA False False T rue 0.627 0.872\nnews bigram NA False False True 0.624 0.853\nT able 3: Evaluation results (bold) and post-evaluation results based on evaluation dir data. The trigram\nLM trained on the news data ranked 4th place on Subtask A and 1st place on Subtask B.\nsults.\nThe task description paper ( Potash et al. , 2017)\nreported system by system results for each hash-\ntag. W e were surprised to ﬁnd that our perfor-\nmance on the hashtag ﬁle #BreakUpIn5W ords in\nthe evaluation stage was signiﬁcantly better than\nany other system on both Subtask A (with accu-\nracy of 0.913) and Subtask B (with distance score\nof 0.636). While we still do not fully understand\nthe cause of these results, there is clearly some-\nthing about the language used in this hashtag that\nis distinct from the other hashtags, and is some-\nhow better represented or captured by a language\nmodel. Reaching a better understanding of this re-\nsult is a high priority for future work.\nThe tweet data was signiﬁcantly smaller than\nthe news data, and so certainly we believe that this\nwas a factor in the performance during the evalu-\nation stage, where the models built from the news\ndata were signiﬁcantly more effective. Going for-\nward we plan to collect more tweet data, particu-\nlarly those that participate in #HashtagW ars. W e\nalso intend to do some experiments where we cut\nthe amount of news data and then build models to\nsee how those compare.\nWhile our language models performed well,\nthere is some evidence that neural network models\ncan outperform standard back-off N-gram models\n(\nMikolov et al. , 2011). W e would like to experi-\nment with deep learning methods such as recurrent\nneural networks, since these networks are capable\nof forming short term memory and may be better\nsuited for dealing with sequence data.\n388\nReferences\nCristian Danescu-Niculescu-Mizil, Justin Cheng, Jon\nKleinberg, and Lillian Lee. 2012. Y ou had me at\nhello: How phrasing affects memorability . In Pro-\nceedings of the 50th Annual Meeting of the Associ-\nation for Computational Linguistics: Long P apers\n- V olume 1 . Association for Computational Linguis-\ntics, Stroudsburg, P A, USA, ACL ’12, pages 892–\n901.\nJ. Firth. 1968. A synopsis of linguistic theory 1930-\n1955. In F . Palmer, editor, Selected P apers of J. R.\nFirth, Longman.\nKenneth Heaﬁeld, Ivan Pouzyrevsky , Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modi-\nﬁed Kneser-Ney language model estimation. In Pro-\nceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics . Soﬁa, Bulgaria,\npages 690–696.\nHenry Kucera and W . Nelson Francis. 1967. Compu-\ntational Analysis of Present-day American English .\nBrown University Press, Providence, RI, USA.\nA. A. Markov . 2006. An example of statistical inves-\ntigation of the text Eugene Onegin concerning the\nconnection of samples in chains. Science in Context\n19(4):591–600.\nRada Mihalcea and Carlo Strapparava. 2006. Learn-\ning to laugh (automatically): Computational models\nfor humor recognition. Computational Intelligence\n22(2):126–142.\nT om´aˇs Mikolov , Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock `y, and Sanjeev Khudanpur. 2011. Ex-\ntensions of recurrent neural network language\nmodel. In Acoustics, Speech and Signal Processing\n(ICASSP), 2011 IEEE International Conference on .\nIEEE, pages 5528–5531.\nTristan Miller and Iryna Gurevych. 2015. Automatic\ndisambiguation of English puns. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(V olume 1: Long P apers) . Association for Computa-\ntional Linguistics, Beijing, China, pages 719–729.\nG ¨ozde ¨Ozbal and Carlo Strapparava. 2012. A com-\nputational approach to the automation of creative\nnaming. In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Long P apers-V olume 1 . Association for Com-\nputational Linguistics, pages 703–711.\nPeter Potash, Alexey Romanov , and Anna Rumshisky .\n2017. SemEval-2017 T ask 6: #HashtagW ars: learn-\ning a sense of humor. In Proceedings of the\n11th International W orkshop on Semantic Evalua-\ntion (SemEval-2017) . V ancouver, BC.\nDafna Shahaf, Eric Horvitz, and Robert Mankoff.\n2015. Inside jokes: Identifying humorous cartoon\ncaptions. In Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery\nand Data Mining . ACM, New Y ork, NY , USA, KDD\n’15, pages 1065–1074.\nOliviero Stock and Carlo Strapparava. 2003. Getting\nserious about the development of computational hu-\nmor. In Proceedings of the Eighteenth Interna-\ntional Joint Conference on Artiﬁcial Intelligence .\nAcapulco, pages 59–64.\nRenxian Zhang and Naishi Liu. 2014. Recognizing hu-\nmor on T witter. In Proceedings of the 23rd ACM In-\nternational Conference on Conference on Informa-\ntion and Knowledge Management . ACM, New Y ork,\nNY , USA, CIKM ’14, pages 889–898.\n389",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.9663546085357666
    },
    {
      "name": "Computer science",
      "score": 0.8161084651947021
    },
    {
      "name": "Task (project management)",
      "score": 0.7942261695861816
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.7272535562515259
    },
    {
      "name": "Natural language processing",
      "score": 0.6252378821372986
    },
    {
      "name": "Sense of humor",
      "score": 0.5831294059753418
    },
    {
      "name": "Language model",
      "score": 0.5355318188667297
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5352441072463989
    },
    {
      "name": "Psychology",
      "score": 0.10808765888214111
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210115145",
      "name": "University of Minnesota, Duluth",
      "country": "US"
    }
  ],
  "cited_by": 9
}