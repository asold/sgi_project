{
  "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
  "url": "https://openalex.org/W3207493267",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2646628646",
      "name": "Woojeong Jin",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2125847941",
      "name": "Yu Cheng",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2166559730",
      "name": "Yelong Shen",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Wei‐Zhu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies",
        "Microsoft (Finland)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2904565150",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3085177480",
    "https://openalex.org/W3035370595",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3118641406",
    "https://openalex.org/W2950104027",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3177174258",
    "https://openalex.org/W3199693760",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3104279398",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3126464137",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4287113019"
  ],
  "abstract": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks.Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github.com/woojeongjin/FewVLM",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2763 - 2775\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nA Good Prompt Is Worth Millions of Parameters:\nLow-resource Prompt-based Learning for Vision-Language Models\nWoojeong Jin1∗ Yu Cheng2 Yelong Shen2 Weizhu Chen2 Xiang Ren1\n1University of Southern California 2Microsoft Corporation\n{woojeong.jin,xiangren}@usc.edu {yu.cheng,yelong.shen,wzchen}@microsoft.com\nAbstract\nLarge pre-trained vision-language (VL) mod-\nels can learn a new task with a handful of\nexamples and generalize to a new task with-\nout ﬁne-tuning. However, these VL mod-\nels are hard to deploy for real-world applica-\ntions due to their impractically huge sizes and\nslow inference speed. To solve this limitation,\nwe study prompt-based low-resource learn-\ning of VL tasks with our proposed method,\nFEWVLM, relatively smaller than recent few-\nshot learners. For F EWVLM, we pre-train a\nsequence-to-sequence transformer model with\npreﬁx language modeling (PreﬁxLM) and\nmasked language modeling (MaskedLM). Fur-\nthermore, we analyze the effect of diverse\nprompts for few-shot tasks. Experimental\nresults on VQA show that F EWVLM with\nprompt-based learning outperforms Frozen\n(Tsimpoukelli et al., 2021) which is 31×larger\nthan F EWVLM by 18.2% point and achieves\ncomparable results to a 246 ×larger model,\nPICa (Yang et al., 2021). In our analysis,\nwe observe that (1) prompts signiﬁcantly af-\nfect zero-shot performance but marginally af-\nfect few-shot performance, (2) models with\nnoisy prompts learn as quickly as hand-crafted\nprompts given larger training data, and (3)\nMaskedLM helps VQA tasks while PreﬁxLM\nboosts captioning performance. Our code\nis publicly available at https://github.\ncom/woojeongjin/FewVLM\n1 Introduction\nFine-tuning large pre-trained language models\n(PLMs) have led to strong results in various do-\nmains including vision-language tasks (Devlin\net al., 2019; Raffel et al., 2020; Brown et al., 2020;\nRadford et al., 2021). Such large PLMs can learn\na new task with a few examples or generalize\nto a new task without ﬁne-tuning on any train-\ning examples, i.e., few-shot and zero-shot learn-\n∗ Work was mainly done while interning at Microsoft\nAzure AI.\nquestion: What position is this man playing? answer: <text_1> \nan image ofa small black dog standing over a plate of food.\nVQA\nCaptioning\nInput imageInput text\n<text_1> pitcherTarget text\nInput text\nTarget text\nInput image\nFigure 1: Examples of VQA and Captioning tasks.\nIn our setup, we convert the tasks into generative tasks\nin which models need to generate target text given input\ntext and an image.\ning (Brown et al., 2020; Radford et al., 2021; Tsim-\npoukelli et al., 2021). Few-shot learning overcomes\nthe challenges of data-hungry supervised learning,\nwhere collecting human-labeled data is costly and\nslow. However, recent few-shot models such as\nGPT3 (Brown et al., 2020), Frozen (Tsimpoukelli\net al., 2021), and PICa (Yang et al., 2021) are too\nlarge to deploy in small or moderate computing\nmachines due to their gigantic model sizes\nIn this paper, we study low-resource learning of\nVL tasks with our proposed method, FEWVLM, a\nmoderate-sized vision-language model, in which\nwe ﬁne-tune the model with no or a handful of\ntraining examples. For FEWVLM , we pre-train\na sequence-to-sequence transformer model (Cho\net al., 2021; Raffel et al., 2020) with preﬁx lan-\nguage modeling (PreﬁxLM) and masked language\nmodeling (MaskedLM). This setup is more practi-\ncal in that training and inference can be run eco-\nnomically using standard computing hardware and\n2763\nTransformer EncoderTransformer DecoderWhat position is this man playing? \n <s><text_1>pitcher\n<text_1>pitcher</s>question: [Q] answer: <text_1> \n[Q] [Q] <text_1>Prompts\nFaster R-CNN\nquestion:What position is this man playing? answer:<text_1>\nFigure 2: Illustration of F EWVLM. This shows inference of F EWVLM with prompt-based learning. Given a\nprompt template, we convert the question text into input text. The prompt helps the model generate correct answers.\nit is expensive to obtain a large number of qual-\nity training examples in the real world. In such\na few-shot setting, task-speciﬁc prompts or task\ndescriptions are important and have shown effec-\ntiveness in few-shot NLP tasks (Gao et al., 2021;\nRadford et al., 2021; Schick and Schütze, 2021a,b;\nBrown et al., 2020).\nTo extend the success to VL tasks, we aim to\nanswer the following questions for prompt-based\nlow-resource VL learning. Q1) How does prompt\ndesign affect zero/few-shot learning on new tasks?\nQ2) Does prompt design still matter given larger\ntraining? Q3) How do different pre-training ob-\njectives affect zero/few-shot learning? To answer\nthese questions, we explore various prompt for-\nmats including hand-crafted and noisy prompts on\nzero/few-shot VL learning datasets. In addition, we\nstudy pre-training objectives on few-shot tasks in-\nspired by Raffel et al. (2020): preﬁx language mod-\neling (PreﬁxLM) inspired by Raffel et al. (2020)\nand masked language modeling (MaskedLM). To\nthis end, we investigate the model’s performance\non few-shot VL tasks including visual question an-\nswering (Goyal et al., 2017; Marino et al., 2019;\nHudson and Manning, 2019), captioning (Agrawal\net al., 2019; Young et al., 2014) (Fig. 1), and mini-\nImageNet (Vinyals et al., 2016).\nIn our empirical analysis, our FEWVLM with\nprompt-based learning outperforms Frozen (Tsim-\npoukelli et al., 2021) which is 31 ×larger than\nFEWVLM by 18.2% point on zero-shot VQAv2\nand achieves comparable results to a 246×larger\nmodel, PICa (Yang et al., 2021). Furthermore,\nwe observe that (1) prompts signiﬁcantly affect\nzero-shot performance but marginally affect few-\nshot performance on new tasks (§6.2 and §6.3),\n(2) models with noisy prompts learn as quickly\nas hand-crafted prompts given larger training data\n(§6.5), and (3) MaskedLM helps few-shot VQA\ntasks while PreﬁxLM boosts captioning perfor-\nmance (§6.6).\n2 Related Work\nVision-language few-shot learning. Recently,\nseveral few-shot learners on vision-language tasks\nwere proposed including GPT (Radford et al.,\n2019; Brown et al., 2020), Frozen (Tsimpoukelli\net al., 2021), PICa (Yang et al., 2021), and\nSimVLM (Wang et al., 2021). Frozen (Tsim-\npoukelli et al., 2021) is a large language model\nbased on GPT-2 (Radford et al., 2019), and is trans-\nformed into a multimodal few-shot learner by ex-\ntending the soft prompting to incorporate a set of\nimages and text. Their approach shows the few-\nshot capability on visual question answering and\nimage classiﬁcation tasks. Similarly, PICa (Yang\net al., 2021) uses GPT-3 (Brown et al., 2020) to\nsolve VQA tasks in a few-shot manner by provid-\ning a few in-context VQA examples. It converts\nimages into textual descriptions so that GPT-3 can\nunderstand the images. SimVLM (Wang et al.,\n2021) is trained with preﬁx language modeling on\nweakly-supervised datasets. It demonstrates its ef-\nfectiveness on a zero-shot captioning task. While\nthese models achieve improvement on few-shot\ntasks, they are impractical to use in real-world ap-\nplications due to their model sizes.\nLanguage model prompting. Providing prompts\nor task descriptions play an vital role in improving\npre-trained language models in many tasks (Gao\net al., 2021; Radford et al., 2021; Schick and\nSchütze, 2021a,b; Brown et al., 2020). Among\nthem, GPT models (Radford et al., 2019; Brown\net al., 2020) achieved great success in prompting\n2764\na lady walking next to a bicyclePrefix LMcarryingan umbrella\nMasked LMa lady walking next to a <text_1> carryingan <text_2><text_1> bicycle <text_2> umbrella\nInput image\nTarget textInput text\nFigure 3: Pre-training objectives. We pre-\ntrain F EWVLM with masked language modeling\n(MaskedLM) and preﬁx language modeling (Pre-\nﬁxLM).\nor task demonstrations in NLP tasks. In light of\nthis direction, prompt-based approaches improve\nsmall pre-trained models in few-shot text classiﬁ-\ncation tasks (Gao et al., 2021; Schick and Schütze,\n2021a,b). CLIP (Radford et al., 2021) also ex-\nplores prompt templates for image classiﬁcation\nwhich affect zero-shot performance. We follow\nthese core ideas so we aim to improve zero-shot\nand few-shot performance using prompts in vision-\nlanguage tasks.\n3 Analysis Setup\nIn this work, we study the zero-shot and few-shot\nperformance of vision-language models L. We\nintroduce our analysis setup: problem formulation,\nanalysis questions, downstream tasks and datasets,\nevaluation metrics, and baselines.\n3.1 Problem Formulation\nFor zero-shot tasks, a pre-trained VL modelLhave\nno access to training set Dtrain and development\nset Ddev, and directly makes inference on the test\ninstances Dtest. For few-shot tasks, we compose\na dev set Ddev from training data and ensure that\n|Dtrain|= |Ddev|following Perez et al. (2021);\nGao et al. (2021) to tune the hyper-parameters and\nselect the model. We limit the sizes of training and\ndevelopment sets to meet the goal of learning from\nlimited data. The size of Dtrain and Ddev are small\n— i.e., we set the size of both to 16 in our study.\n3.2 Analysis Questions\nWe aim to answer the following questions in this\nstudy through experiments on multiple VL datasets.\nQ1) How does prompt design affect zero/few-\nshot learning on new tasks? Providing a pre-\ntrained language model with task-speciﬁc prompts\nor signiﬁcantly improves zero-shot and few-shot\nperformance on NLP domains (Gao et al., 2021;\nSchick and Schütze, 2021a,b; Brown et al., 2020).\nFor this question, we test several ad-hoc prompts\non vision-language tasks and analyze how large\nzero-shot and few-shot performance is affected by\ndifferent prompts, hand-crafted and noisy prompts,\nin Sec. 6.5.\nQ2) Does prompt design still matter given\nlarger training data? As we will see in our ex-\nperiments, prompts affect the zero/few-shot per-\nformance. However, prompts may have different\neffects when models are given different sizes of\ntraining data. To answer this question, we train\nmodels with different sizes of training data and\nvarious prompts, and compare the performance be-\ntween different prompts.\nQ3) How do different pre-training objectives af-\nfect zero/few-shot performance? We study two\ndifferent pre-training objectives on few-shot per-\nformance: preﬁx language modeling (PreﬁxLM)\ninspired by Raffel et al. (2020) and masked lan-\nguage modeling (MaskedLM). In this setup, we\npre-train our model with different objectives and\ntest the model on zero-shot and few-shot tasks in\nSec. 6.6.\n3.3 Downstream Tasks and Datasets\nIn this work, we mainly focus on three tasks: vi-\nsual question answering, captioning, and categor-\nical learning. The visual question answering task\nrequires models to answer a question to a given\ncontext image. We convert the visual question\nanswering task into a generation task so that the\nmodel can generate answers in the zero-shot setting.\nThe captioning task requires a model to generate\ndescriptions for a given context image. The cat-\negorical learning requires a model to choose the\ncorrect category or class. We evaluate our model in\nan open-ended fashion to quantify fast learning of\ncategories, in which it must generate correct labels\nunlike other classiﬁcation methods.\nWe include VQAv2 (Goyal et al., 2017), OK-\nVQA (Marino et al., 2019), and GQA (Hudson\n2765\nTable 1: Hand-crafted prompts. We study hand-crafted prompts on zero-shot and few-shot tasks. [Q] and [A]\nrefer to question text and answer text, respectively. <text_1> is a sentinel token. We append image features to\ninput text. Target prompts are “ [A]” and “<text_1> [A]” in VQA. We use caption text as a target prompt in\ncaptioning.\nTask ID Input prompt Example\nVQA\nP1 [Q] <text_1> input:What position is this man playing?<text_1>output:<text_1>pitcher\nP2 question:[Q]answer: input:question: What position is this man playing? answer:output:<text_1>pitcher\nP3 question:[Q]answer:\n<text_1>\ninput:question: What position is this man playing? answer:<text_1>output:\n<text_1>pitcher\nCaptioning\nQ1 a picture of input:a picture ofoutput:a small black dog standing over a plate of food.\nQ2 a photo of input:a photo ofoutput:a small black dog standing over a plate of food.\nQ3 an image of input:an image ofoutput:a small black dog standing over a plate of food.\nand Manning, 2019) for visual question answer-\ning tasks, and NoCaps (Agrawal et al., 2019), and\nFlickr30k (Young et al., 2014) for image caption-\ning.1 We use Karpathy split (Karpathy and Li,\n2015) for Flickr30k, which re-splits train and val\nimages into 29,000 / 1,014 / 1,000 for train / vali-\ndation / test. For categorical learning, we include\nminiImageNet (Vinyals et al., 2016), a meta learn-\ning dataset. Following (Tsimpoukelli et al., 2021),\nwe use only meta test data to evaluate FEWVLM\nin a few-shot manner and test on 5-way k-shot\nsetup, where 5 classes and k examples per class\nare given.2\n3.4 Evaluation Metrics\nTo evaluate few-shot performance, we randomly\nsample 5 different training and dev splits and mea-\nsure average performance on the 5 splits. We ﬁne-\ntune the vision-language models with 200 epochs\nfor the few-shot setup and choose the best check-\npoint on the dev set. For NoCaps task, it does not\nhave training data. Thus we use the training data\nfrom COCO captioning in the experiments follow-\ning Wang et al. (2021). We evaluate on the VQAv2\nvalidation set, GQA test-dev, OK-VQA test set, test\nset of Karpathy split for Flickr30k captioning, and\nNoCaps validation set. We adopt accuracy for VQA\ndatasets and miniImageNet, and CIDEr (Vedantam\net al., 2015) and SPICE (Anderson et al., 2016) as\nevaluation metrics for captioning.\n3.5 Baselines\nWe evaluate strong zero/few-shot vision-language\nlearners for comparison: Frozen (Tsimpoukelli\net al., 2021), PICa (Yang et al., 2021) for VQA\n1We include COCO captioning results on Sec. B of Ap-\npendix.\n2For VQA and captioning, we include k samples in total,\nnot per class.\ndatasets and SimVLM (Wang et al., 2021) for cap-\ntioning datasets. We include Uniﬁed VLP (Zhou\net al., 2020) for few-shot VQAv2 and Flickr30k.\nAlso, we compare them with fully ﬁne-tuned mod-\nels Lfull as upper bounds of few-shot models for\neach task; these models are ﬁne-tuned on the entire\ndatasets while few-shot models can access a small\namount of data. For fully ﬁne-tuned models Lfull,\nwe borrow numbers from Uniterlarge (Chen et al.,\n2019) for VQAv2, Oscar (Li et al., 2020b) for GQA,\nSimVLM (Wang et al., 2021) and VinVL (Zhang\net al., 2021) for NoCaps CIDER and SPICE re-\nspectively, and Uniﬁed VLP (Zhou et al., 2020)\nfor Flickr30k captioning. We include VL-T5no-vqa\nas a baseline which is pre-trained without visual\nquestion answering datasets (Cho et al., 2021). For\nminiImageNet, we include Frozen and AFHN (Li\net al., 2020a). Frozen is designed for few-shot\nlearning while AFHN is for meta learning, which\nis smaller and faster.\n4 Method\nBefore diving into the analysis, we introduce our\nmodel, FEWVLM , to do zero/few-shot learning\non VL tasks and answer the analysis questions we\nraised. We introduce FEWVLM architecture and\npre-training objectives.\n4.1 Encoder-decoder Vision-language Model\nWe adopt an encoder-decoder architecture (Cho\net al., 2021; Vaswani et al., 2017), to encode visual\nand text inputs and generate target text. We repre-\nsent an input image with 36 object regions from a\nFaster R-CNN (Ren et al., 2015) trained on Visual\nGenome (Krishna et al., 2017). The sets of region\nrepresentations are fed into the encoder by append-\ning them to the text Cho et al. (2021). We train\nthe model parameters θby minimizing the negative\n2766\nlog-likelihood of target text y tokens given input\ntext xand image v:\nLθ = −\n|y|∑\ni=1\nlog Pθ(yi|y<i,x,v ). (1)\nThe model is not task-speciﬁc, so it is a good option\nfor zero/few-shot settings.\n4.2 Pre-training Objectives\nWe pre-train the models with both preﬁx language\nmodeling (PreﬁxLM) and masked language mod-\neling (MaskedLM). Fig. 3 illustrates the PreﬁxLM\nand MaskedLM.\nPreﬁx language modeling. We include preﬁx lan-\nguage modeling (PreﬁxLM) following Raffel et al.\n(2020). Given an image and a span of text, this\nobjective randomly splits the text into two separate\ncomponents; the former component with the given\nimage is used as inputs to the encoder and the latter\ncomponent is used as target text to be generated by\nthe decoder.\nMasked language modeling. We follow Cho et al.\n(2021) to do masked language modeling. This\nobjective is to replace random spans with num-\nbered sentinel tokens, e.g., <text_1>, and then\nthe masked text is fed into the encoder. Then the\ndecoder generates the masked spans as target text.\nWe randomly mask 15% of input text tokens and\nreplace them with sentinel tokens.\nPre-training data. To pre-train FEWVLM , we\ncollect image-caption data from MS COCO (Lin\net al., 2014; Chen et al., 2015) and Visual Genome\n(VG) (Krishna et al., 2017). The pre-training\ndatasets contains 9.18M image-text pairs and 180K\ndistinct images.\n5 Low-resource Adaptation\nIn downstream tasks, we train our model with\nfew-shot examples. Fig. 2 shows an illustration\nof FEWVLM in inference time. Given a prompt\ntemplate P, we ﬁrst get input text and target text\nusing the template x,y = P(input,label). Then\nwe train model parameters by minimizing the nega-\ntive log-likelihood in Eq. (1). In inference, we use\nthe same prompt and the model generates the label\ntext. Here we obtain the ﬁnal label by removing\nthe target prompt template.\n5.1 Prompt Design\nPrompts affect the performance of the vision-\nlanguage model (Cho et al., 2021); we study the\neffect of different prompts on the zero-shot and few-\nshot performance on downstream tasks. Tables 1\nand 11 show prompts we used in our experiments.\n5.1.1 Visual Question Answering\nThe visual question answering tasks (VQA, OK-\nVQA, and GQA) require models to answer a\nquestion to a given context image. Recent ap-\nproaches (Chen et al., 2019; Tan and Bansal, 2019;\nSu et al., 2020; Li et al., 2019, 2020b) tackle visual\nquestion answering tasks as multi-label classiﬁca-\ntion over a predeﬁned set of answer candidates.\nInstead, we approach the visual question answer-\ning tasks as a generation task so that the model\ncan produce the answers without introducing any\ntask-speciﬁc heads. In this setup, prompts act as\nconstraints to guide the models to generate proper\nformats of answers; models might generate a sen-\ntence for VQA, which is not the correct format,\nwithout prompts.\nTherefore, we study several prompts for input\nand output as shown in Tables 1 and 11; we explore\nhand-crafted prompts (Table 1) and noisy prompts\nfor ablation study (Table 11).\nHand-crafted prompts. For input prompts, we\nexplore three different templates: “question: [Q]\nanswer:” and with the <text_1> sentinel token\nat the end. Similarly to masked language model-\ning, we expect models to generate words thanks to\nthe sentinel token. For target prompts, we explore\ntwo different templates: “ [A]” (an answer) and\n“<text_1> [A]” (an answer with a sentinel to-\nken). Here, we aim to mimic MaskedLM’s target\ntext format, so the similar format helps the model\nquickly adapt to the new task. We call each prompt\nID as in Table 1.\nNoisy prompts. To understand the effect of noisy\nprompts in zero/few-shot learning, we include irrel-\nevant prompts, noisy tokens, and random sentences\nas in Table 11. Irrelevant prompts are random ques-\ntions or instructions that mislead models to answer\nwrong questions or follow irrelevant instructions.\nNoisy tokens are randomly selected from T5’s vo-\ncabulary, so we test how robust our model is to ran-\ndom tokens. Finally, random sentences are captions\nfrom MS COCO and this gives false information\nto models.\n5.1.2 Captioning\nIn NoCaps and Flickr30k, we explore three hand-\ncrafted input prompts: “a picture of”, “a photo of”,\nand “an image of”. We study the effect of different\n2767\nTable 2: Zero-shot VQA results. We test models with-\nout any training examples. VL-T5 no-vqa is pre-trained\nwithout VQA datasets. Compared to larger models,\nFrozen and PICa-Full, our models outperform them or\nshow the comparable results.\nModel Model\nsize VQAv2 OK-\nVQA GQA\nUniﬁed VLP 122M 0.0 - -\nVL-T5no-vqa 224M 13.5 5.8 6.3\nFrozen 7B 29.5 5.9 -\nPICa 175B - 17.5 -\nFEWVLMbase 224M 43.4 11.6 27.0\nFEWVLMlarge 740M 47.7 16.5 29.3\nTable 3: Few-shot VQA results. We report average\nperformance over 5 different splits. The size of training\nand validation sets are 16 for our F EWVLM and VL-\nT5no-vqa, and Frozen and PICa use 4 and 16 in-context\ntraining examples, respectively. For the fair compari-\nson to Frozen, we include F EWVLM∗\nbase with 4 train-\ning and validation examples.\nModel Model\nsize VQAv2 OK-\nVQA GQA\nUniﬁed VLP 122M 24.3 - -\nVL-T5no-vqa 224M 31.8 12.7 19.6\nFrozen 7B 38.2 12.6 -\nPICa 175B 54.3 43.3 -\nFEWVLM∗base 224M 45.1 14.5 26.9\nFEWVLMbase 224M 48.2 15.0 32.2\nFEWVLMlarge 740M 51.1 23.1 35.7\nFine-tunedLfull - 72.6 - 61.5\nTable 4: Zero-shot captioning results. We use the\nCIDEr and SPICE metrics for evaluation.\nModel Model size NoCaps Flickr30k\nCIDEr SPICE CIDEr SPICE\nUniﬁed VLP 122M - - 24.9 7.2\nVL-T5no-vqa 224M 4.4 5.3 2.6 2.0\nSimVLMhuge - 101.4 - - -\nFEWVLMbase 224M 42.2 8.5 31.0 10.0\nFEWVLMlarge 740M 47.7 9.1 36.5 10.7\nTable 5: Few-shot captioning results. We report av-\nerage performance over 5 different splits. We use the\nCIDEr and SPICE metrics for evaluation.\nModel Model size NoCaps Flickr30k\nCIDEr SPICE CIDEr SPICE\nUniﬁed VLP 122M - - 28.8 9.4\nVL-T5no-vqa 224M 22.0 6.8 12.8 8.3\nFEWVLMbase 224M 48.6 10.0 32.6 12.8\nFEWVLMlarge 740M 53.1 10.4 37.0 13.5\nFine-tunedLfull - 112.2 13.1 67.4 17.0\nword choices in this captioning task. While the\nthree different words have similar meanings, they\nshow different performance in zero-shot and few-\nshot tasks as we will see in our experiments.. For\ntarget prompts, we just train the model with the\noriginal caption without any additional prompts.\n5.1.3 MiniImageNet\nIn miniImageNet, we train our model with a hand-\ncrafted input prompt, “ This is <text_1>,” and\ntarget prompt, “ <text_1> [A].” We compare\nour model with and without prompts in this dataset\nto study whether prompts are helpful in categorical\nlearning.\n6 Results and Discussion\nIn this section, we ﬁrst discuss our main results on\nzero-shot and few-shot tasks and then answer the\nquestions we raised: does prompt design matter in\nzero/few-shot learning?\n6.1 Experiment Details\nFor pre-training, we set batch size 1,280 and\n800 for FEWVLMbase and FEWVLMlarge, respec-\ntively and pre-train them with 30 epochs. We\nuse learning rate 1e-4 with 5% linear warmup.\nFor few-shot learning, we train models with\n200 epochs, learning rate 5e-5 and 5% linear\nwarmup and choose the best checkpoint on the\ndev set. For FEWVLM , we use “question: [Q]\nanswer <text_1>” (P3) as an input prompt and\n“<text_1> [A]” as a target prompt for visual\nquestion answering, and “an image of” (Q3) as an\ninput prompt for captioning, which show the best\nperformance. We will study the effect of different\nprompts in Sec. 6.5. The sizes of of Dtrain and\nDdev are 16 on VQA and captioning tasks. For\nminiImageNet, we use ‘This is<text_1>,” and\n“<text_1> [A]” as input and target prompts. In\nthis data, we test with {1, 3, 5}-shots per class.\n6.2 Performance on Zero-shot Learning\nWe evaluate the existing models in a zero-shot\nmanner, in which models do not have access to\nany training data. Tables 2 and 4 show the re-\nsults on VQA and captioning datasets, respec-\ntively. First, FEWVLM with the hand-crafted\nprompt (P3) achieves better performance than other\nbaselines on VQA datasets. In particular, our\nFEWVLM base signiﬁcantly outperforms Frozen\n2768\nTable 6: 5-way miniImageNet results. We evaluate\nFEWVLM in a generative manner. The shot represents\nthe number of training examples per class.\nModel Model\nsize 1 shot 3 shots 5 shots\nFrozen 7B 14.5 34.7 33.8\nFEWVLMbase(no\nprompt) 224M 48.0 75.0 82.6\nFEWVLMbase 224M 57.0 78.0 84.2\nFEWVLMlarge 740M 57.1 78.3 84.4\nAFHN - 62.3 - 78.1\nwhich is about 31×larger than ours. Also, PICa\nbased on GPT3 (Brown et al., 2020) shows the best\nperformance on OK-VQA. It is noticeable that our\nFEWVLMlarge, the 246×smaller model, achieves\nthe comparable result to PICa. Compared to VL-\nT5no-vqa which is the same architecture as ours,\nFEWVLM base improves VQAv2 performance by\nabout 30% point. As we will see in the later section,\nour pre-training objectives and the prompts boost\nthe VQA performance. On NoCaps, SimVLMhuge\nshows the best performance. Our FEWVLM base\nsigniﬁcantly improves the performance compared\nto VL-T5no-vqa. As we will see in the later section,\nour pre-training objectives and the prompts boost\nthe VQA and captioning performance.\n6.3 Performance on Few-shot Learning\nTables 3 and 5 show the few-shot performance\non VQA and captioning datasets. Sizes of train-\ning and validation sets are 16 for FEWVLM , VL-\nT5no-vqa, and Uniﬁed VLP; and Frozen and PICa\nuse 4 and 16 in-context demonstration examples,\nrespectively.\nOn VQAv2 and OK-VQA, PICa shows the best\nperformance while our FEWVLMlarge achieves the\ncomparable result on VQAv2. OK-VQA requires\nexternal knowledge to answer unlike other VQA\ndatasets, so larger models and large pre-training\ndata (prior knowledge) are necessary to improve.\nInterestingly, FEWVLM∗\nbase, which is trained with\n4 training examples, outperforms Frozen. On cap-\ntioning data, FEWVLM base notably outperforms\nVL-T5no-vqa by 31.1% point on NoCaps CIDEr.\nUniﬁed VLP slightly underperforms FEWVLM\non Flickr30k captioning task. We conjecture that\ntheir architecture is based on a encoder-decoder\ntransfomer and it is pre-trained with a captioning\ntask (Zhou et al., 2020).\nTable 7: Zero-shot results of hand-crafted prompts.\nWe test different input prompts in zero-shot predictions.\nWe use a CIDEr metric for Flickr30k. Note that zero-\nshot setting does not require target prompts.\nno prompt P1 P2 P3\nVQAv2 3.7 9.9 19.0 43.4\nno prompt Q1 Q2 Q3\nFlickr30k 9.6 15.2 25.6 31.0\n6.4 MiniImageNet\nTable 6 shows results on miniImageNet, where\nmodels must choose the correct class for each\nimage. We train and evaluate FEWVLM in an\ngenerative manner; the model must generate cor-\nrect label text to get the credit. FEWVLM signiﬁ-\ncantly outperforms Frozen in all shots. Note that\nwe train FEWVLM with a few training samples\nwhile Frozen uses them as in-context demonstra-\ntion. Interestingly, FEWVLM with a hand-crafted\nprompt improves performance a lot on the 1-shot\ncase, while it marginally improves on the 5-shot\ncase.\n6.5 Study of Prompt Design\nHere we examine the effect of different prompts on\nFEWVLMbase in Table 7 and Figs. 6, 5, and 4. We\ntest the model on VQAv2 and Flickr30k datasets.\n6.5.1 Zero-shot Predictions\nTable 7 shows the zero-shot performance on\nVQAv2 and Flickr30k. We observe that zero-shot\nresults are remarkably affected by input prompts\non both datasets. For input prompts, <text_1>\nin P1 and P3 helps the zero-shot predictions sig-\nniﬁcantly compared to “no prompt” and P2. We\nconjecture that <text_1> guides the model to\npredict masked spans similarly to MaskedLM, so\nit improves the performance.\nOn Flickr30k, we examine different word\nchoices of prompts: “a picture of” (Q1), “a photo\nof” (Q2), and “an image of” (Q3). For instance,\nusing “an image of” outperforms using no prompt\nby 21.4 point. It is noticeable that different word\nchoices signiﬁcantly affect the zero-shot results.\n6.5.2 Few-shot Predictions\nWe study various input prompts including irrele-\nvant prompts, noisy tokens, and random sentences\non VQAv2 (Fig. 4). First, noisy prompts and no\nprompt achieve near 0 accuracy on the zero-shot\nsetting. In few-shot predictions, FEWVLM with\n2769\n0 10 20 30 50 100 200\nTraining size\n0\n10\n20\n30\n40\n50ACC on VQAv2\nhand-crafted\nno prompt\nirrelevant prompts\nnoisy tokens\nrandom sentences\nFigure 4: VQAv2 results on noisy prompts. We in-\nvestigate different prompts on various training sizes.\nFEWVLM is trained with our best hand-crafted prompt\n(P3), irrelevant prompts, noisy tokens and random sen-\ntences. We list the prompt templates in Table 11 of\nappendix. We use “ <text_1> [A]” as our target\nprompt.\n10 20 30 50 100 200 300\nTraining size\n35.0\n37.5\n40.0\n42.5\n45.0CIDEr on Flickr30k\n hand-crafted\nno prompt\nFigure 5: Flickr30k results on hand-crafted prompts.\nWe investigate different hand-crafted prompts (Q1, Q2,\nand Q3) on various training sizes.\nnoisy prompts learns as quickly as hand-crafted\nprompts given larger data. For example, our model\nwith noisy prompts achieves comparable results\nto the best hand-crafted prompt. Among all dif-\nferent types of noisy prompts, random sentences\ndeteriorate performance the most. This is because\nthe random sentences come from captions in MS\nCOCO, so the model might choose the answer from\nwrong captions not from images. Interestingly,\nno prompt outperforms the other noisy prompts\nand even shows similar to or better than the hand-\ncrafted prompt with larger training data. We also\nobserve a similar phenomenon on Flickr30k; no\nprompt performs similar to hand-crafted prompts\nin Fig. 5.\n10 20 30 50 100 200 300\nTraining size\n30\n35\n40\n45\n50ACC on VQAv2\n<text_1> [A]\n[A]\nFigure 6: VQAv2 results on different target prompts.\nWe investigate different target prompts with hand-\ncrafted input prompts on various training sizes.\nTable 8: Results on different pre-training objectives.\nWe test our pre-training objectives to investigate how it\naffects zero-shot and few-shot performance. We train\nFEWVLMbase with 16 training and validation exam-\nples.\nObjective VQAv2 GQA Flickr30k\nCIDEr\nZero-shot\nMaskedLM 42.4 25.1 4.6\nPreﬁxLM 11.9 6.7 26.8\nMaskedLM + PreﬁxLM43.4 27.0 31.0\nFew-shot\nMaskedLM 46.0 31.4 18.5\nPreﬁxLM 40.8 27.6 31.8\nMaskedLM + PreﬁxLM48.2 32.2 32.6\nIn addition, we explore two different target\nprompts, “<text_1> [A]” and “[A].” We try\nto mimic the MaskedLM’s target text format, so\nwe add “<text_1>” to target prompt on VQA.\nThis might help the model’s fast adaptation to a\nnew task since they share the same target prompt.\nIn Fig. 6, we notice an interesting phenomenon; the\ntarget prompt “[A]” shows a larger variance than\nthe other suggesting that introducing “<text_1>”\nhelps the model quickly adapt to a new task. How-\never, both prompts show similar results given larger\ntraining data, e.g., 300.\n6.6 Pre-training Objectives\nWe investigate how pre-training objectives affect\ndifferent tasks. We pre-train FEWVLM with dif-\nferent pre-training objectives: masked language\nmodeling (MaskedLM) and preﬁx language model-\ning (PreﬁxLM).\nIn Table 8, we observe that MaskedLM helps\n2770\nVQA tasks while PreﬁxLM helps captioning tasks\nin zero-shot and few-shot settings. We conjecture\nthat MaskedLM is to predict spans, which is anal-\nogous to predict correct answers to questions, and\nPreﬁxLM is to generate the rest of the given pre-\nﬁx, which is similar to captioning tasks. In other\nwords, if the pre-training task is similar to the down-\nstream tasks, then it will help performance further.\nWhen pre-training with both objectives, they cre-\nate a synergetic effect and thus improve cross-task\ngeneralization.\n7 Conclusion\nIn this work, we present FEWVLM , a few-shot\nprompt-based learner on vision-language tasks. On\ndiverse datasets, FEWVLM outperforms baselines\nand shows comparable results to PICa which is\n246×larger than ours. We observe that prompts\nare vital in zero-shot and few-shot tasks and each\npre-training objective helps different few-shot tasks.\nAlso, we ﬁnd out that models with larger training\ndata are not signiﬁcantly affected by noisy prompts.\nFuture work includes exploring automatic prompt\ngeneration and diverse formats of few-shot tasks\nsuch as multiple-choice VQA. Finding optimal\nprompts require exhaustive engineering to achieve\nthe best performance and leads to impressive re-\nsults. We leave the exploration of these directions\nto future investigations.\nReferences\nHarsh Agrawal, Peter Anderson, Karan Desai, Yufei\nWang, Xinlei Chen, Rishabh Jain, Mark Johnson,\nDhruv Batra, Devi Parikh, and Stefan Lee. 2019.\nnocaps: novel object captioning at scale. In 2019\nIEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October\n27 - November 2, 2019, pages 8947–8956. IEEE.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic proposi-\ntional image caption evaluation. InEuropean confer-\nence on computer vision, pages 382–398. Springer.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft coco cap-\ntions: Data collection and evaluation server. ArXiv\npreprint, abs/1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. Uniter: Learning universal\nimage-text representations.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In Proceedings of the 38th International Con-\nference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 ofProceedings\nof Machine Learning Research , pages 1931–1942.\nPMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nV in VQA matter: Elevating the role of image un-\nderstanding in visual question answering. In 2017\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July\n21-26, 2017, pages 6325–6334. IEEE Computer So-\nciety.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2019, Long Beach, CA, USA, June 16-\n20, 2019, pages 6700–6709. Computer Vision Foun-\ndation / IEEE.\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\n2771\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\nSociety.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123(1):32–\n73.\nKai Li, Yulun Zhang, Kunpeng Li, and Yun Fu. 2020a.\nAdversarial feature hallucination networks for few-\nshot learning. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020 , pages\n13467–13476. IEEE.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. ArXiv preprint, abs/1908.03557.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020b. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA, June 16-20, 2019 , pages 3195–\n3204. Computer Vision Foundation / IEEE.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models.ArXiv\npreprint, abs/2105.11447.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research , pages\n8748–8763. PMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and\nJian Sun. 2015. Faster R-CNN: towards real-time\nobject detection with region proposal networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 91–99.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2339–2352, Online. As-\nsociation for Computational Linguistics.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. ArXiv preprint, abs/2106.13884.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nRamakrishna Vedantam, C. Lawrence Zitnick, and\nDevi Parikh. 2015. Cider: Consensus-based image\ndescription evaluation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2015,\nBoston, MA, USA, June 7-12, 2015 , pages 4566–\n4575. IEEE Computer Society.\n2772\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. 2016. Matching\nnetworks for one shot learning. In Advances in\nNeural Information Processing Systems 29: Annual\nConference on Neural Information Processing Sys-\ntems 2016, December 5-10, 2016, Barcelona, Spain,\npages 3630–3638.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. ArXiv preprint, abs/2108.10904.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xi-\naowei Hu, Yumao Lu, Zicheng Liu, and Lijuan\nWang. 2021. An empirical study of gpt-3 for\nfew-shot knowledge-based vqa. ArXiv preprint ,\nabs/2109.05014.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5579–5588.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason J. Corso, and Jianfeng Gao. 2020. Uni-\nﬁed vision-language pre-training for image caption-\ning and VQA. In The Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artiﬁcial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artiﬁcial In-\ntelligence, EAAI 2020, New York, NY, USA, Febru-\nary 7-12, 2020, pages 13041–13049. AAAI Press.\n2773\nTable 9: Model architectures.\nHyperparameter FEWVLMbase FEWVLMlarge\n# Layers 12+12 24+24\nHidden dimension 768 1,024\nFF hidden size 3,072 4,096\n# Attention head 12 16\nAttention head size 64 64\nTable 10: COCO captioning results. We use the\nCIDEr and SPICE metrics for evaluation.\nModel Model size Zero-shot Few-shot\nCIDEr SPICE CIDEr SPICE\nVL-T5no-vqa 224M 4.9 2.0 43.0 10.8\nSimVLMhuge - 102.3 22.1 - -\nFEWVLMbase 224M 84.5 16.2 98.7 18.9\nFEWVLMlarge 740M 92.1 17.3 100.4 19.1\nUniﬁed VLP\n(fully supervised)122M - - 117.7 21.3\n0 10 20 30 50 100 200 300\nTraining size\n10\n20\n30\n40\n50ACC on VQAv2\nno prompt\nP1\nP2\nP3\nFigure 7: VQAv2 results on hand-crafted prompts\nand the target prompt “<text_1> [A]”.\nA Model Architectures\nTable 9 shows model parameters in our model,\nFEWVLM . FEWVLM base and FEWVLM large is\nbased on VL-T5 (Cho et al., 2021) and T5 (Raffel\net al., 2020), respectively.\nB COCO Captioning\nWe evaluate our model with COCO captioning data.\nWe use Karpathy split (Karpathy and Li, 2015) for\nMS COCO captioning, which re-splits train and\nval images into 113,287 / 5000 / 5000 for train\n/ validation / test. Table 10 shows the results on\nCOCO.\nC Prompt Study\nTables 7, 8, and 9 show the results of each prompt\non VQAv2 and Flickr30k with various training\n0 10 20 30 50 100 200\nTraining size\n10\n20\n30\n40\n50ACC on VQAv2\nno prompt\nP1\nP2\nP3\nFigure 8: VQAv2 results on hand-crafted prompts\nand the target prompt “[A]”\n0 10 20 30 50 100 200 300\nTraining size\n10\n20\n30\n40CIDEr on Flickr30k\nno prompt\nQ1\nQ2\nQ3\nFigure 9: Flickr30k results on hand-crafted\nprompts.\nsizes.\nD Effect of Pre-training Data\nWe pre-train our model with different datasets: MS\nCOCO and Visual Genome (VG), and Conceptual\nCaptions (CC). We investigate which pre-training\ndataset helps the downstream tasks in a few-shot\nmanner. In Table 12, we observe that MS COCO\nand VG datasets are more helpful to the down-\nstream tasks than CC.\n2774\nTable 11: Prompt templates. We test different input prompts on VQAv2. [Q] refers to input question text. We\nuse <text_1> [A] as target text. We append image features to input text.\nInput prompt template Category\nFill in the blank in the below sentence:[Q] irrelevant prompts\nQuestion:[Q]True or False? irrelevant prompts\n[Q]What color is the ﬂoor? irrelevant prompts\nParaphrase this into a different question?[Q] irrelevant prompts\n[Q]How many are they? irrelevant prompts\nnezg publice passed Dream[Q] noisy tokens\nbeneﬁc video starting garbagetap Talent summary[Q] noisy tokens\ngestion Bun dates youngest batteriesfeder organisationoyez[Q] noisy tokens\n[Q]chefernt,iei geekutilisées plantingasta Pest principiiMF saddle véritable noisy tokens\n[Q]composant emergency laissé Klägereiniger swipe concentrateOSS/18 rewardprepaid noisy tokens\n[Q]A black dog is sitting on a couch. random sentences\n[Q]A man working at a kitchen counter in a room illuminated by sunlight. random sentences\nA brown purse is sitting on a green bench.[Q] random sentences\nA television that is sitting next to signs.[Q] random sentences\n[Q]A woman is wearing white pants. random sentences\nTable 12: Few-shot results on different pre-training\ndatasets. We examine different pre-training datasets\non each downstream tasks.\nDataset VQAv2 GQA Flickr30k\nMS COCO, VG 48.2 32.2 32.6\nConceptual Captions36.7 25.9 22.3\n2775",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.862533688545227
    },
    {
      "name": "Language model",
      "score": 0.6879515051841736
    },
    {
      "name": "Closed captioning",
      "score": 0.6800158023834229
    },
    {
      "name": "Task (project management)",
      "score": 0.5884554386138916
    },
    {
      "name": "Inference",
      "score": 0.5518791675567627
    },
    {
      "name": "Transformer",
      "score": 0.5497860312461853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5494097471237183
    },
    {
      "name": "Machine learning",
      "score": 0.5153281092643738
    },
    {
      "name": "Prefix",
      "score": 0.4842776656150818
    },
    {
      "name": "Code (set theory)",
      "score": 0.44282591342926025
    },
    {
      "name": "Point (geometry)",
      "score": 0.41996294260025024
    },
    {
      "name": "Natural language processing",
      "score": 0.3998800814151764
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.1400718092918396
    },
    {
      "name": "Image (mathematics)",
      "score": 0.1049531102180481
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210105678",
      "name": "Microsoft (Finland)",
      "country": "FI"
    }
  ],
  "cited_by": 86
}