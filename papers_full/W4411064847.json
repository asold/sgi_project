{
  "title": "Co-Intelligent Design of Catalysis Research with Large Language Models: Hype or Reality?",
  "url": "https://openalex.org/W4411064847",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5044914316",
      "name": "David Balcells",
      "affiliations": [
        "University of Oslo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4411064847",
    "https://openalex.org/W4406779522",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4386554921",
    "https://openalex.org/W4401183144",
    "https://openalex.org/W4380686968",
    "https://openalex.org/W4403703015",
    "https://openalex.org/W4320854981",
    "https://openalex.org/W4407152690"
  ],
  "abstract": "Large language models are making a strong impact on broad fields of science, pushing the efforts to minimize hallucinations and to support specialized language. Co-intelligence, the synergy between human and artificial intelligences, can have a strong potential in research through the use of large language models. This Viewpoint focused on the question: Can large language models collaborate with chemists in the design of catalysis research? In a conversation based on a few prompts giving minimal guidance, ChatGPT-4o proved its co-design abilities at different levels of detail, making a major contribution to the definition of a research project concept and its workflows. Relevant flaws were also observed, highlighting the importance of involving critical thinking from highly-educated researchers. The assessment of the results by ChatGPT-4o itself and other large language models, including DeepSeek and Gemini, was also explored.",
  "full_text": "Co-Intelligent Design of Catalysis Research\nwith Large Language Models:\nHype or Reality?\nDavid Balcells∗\nHylleraas Centre for Quantum Molecular Sciences, Department of Chemistry, University of\nOslo, P.O. Box 1033, Blindern, 0315 Oslo, Norway\nE-mail: david.balcells@kjemi.uio.no\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAbstract\nLarge language models are making a strong impact on broad fields of science, push-\ning the efforts to minimize hallucinations and to support specialized language. Co-\nintelligence, the synergy between human and artificial intelligences, can have a strong\npotential in research through the use of large language models. This Viewpoint focused\non the question: Can large language models collaborate with chemists in the design of\ncatalysis research?In a conversation based on a few prompts giving minimal guidance,\nChatGPT-4o proved its co-design abilities at different levels of detail, making a major\ncontribution to the definition of a research project concept and its workflows. Relevant\nflaws were also observed, highlighting the importance of involving critical thinking from\nhighly-educated researchers. The assessment of the results by ChatGPT-4o itself and\nother large language models, including DeepSeek and Gemini, was also explored.\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nIntroduction\nCo-intelligence (CoI) is the collaborative intelligence that results from synergies among many\nindividuals sharing diverse knowledge. Experimental-theoretical collaborations in chemistry1\nand other fields can be naturally regarded as a form of CoI, and human-robot interactions\nhave also been discussed in this context. 2 The present Viewpoint on CoI focused on a re-\nsearch design task synergizing human and artificial intelligences (HI and AI, respectively), 3\nin which the AI was a large language model (LLM).\nLLMs implement and leverage natural language processing with generative AI methods 4–7\nthat enable computers to engage in conversations with HIs, replying prompted questions in\na humanly manner. 8 LLMs have proven their ability to perform optimization tasks 9 and to\nlearn and deliver reasoning 10,11 that, in some cases, can be related to scientific understand-\ning.12 LLMs are also subject to several concerns, including threatening dual-use scenarios, 13\nthe “hallucination” of made-up contents, 14,15 and the faulty processing of specialized lan-\nguage, like that of chemistry. 16 Ongoing research is focusing on mitigating these flaws, 17,18\nwhich still persist to a significant extent.\nLLMs have recently entered the field of chemistry,19–24 including automated workflows show-\ning a high degree of autonomy. 25,26 Their use as research assistants has been explored for\nboth trivial27 and challenging28,29 tasks, in a wide range of applications including catalysis,30\ndata-mining,31,32 molecule33,34 and materials35 design, chemical space exploration,36 organic\nsynthesis,37,38 property optimization,39 and chemical education,40,41 including critical think-\ning.42 As in other fields, chemistry LLMs are often fine-tuned towards specialized areas like,\nfor example, MOFs43 and peptides,44 which requires collecting domain-specific data.45,46 The\ndevelopment of LLMs for chemistry is an active research area, with new directions including\nthe synergies between evolutionary and deep learning. 47,48\n3\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nFigure 1: Co-intelligent (CoI) design of the concept and workflows of a research project\nsynergizing human intelligence (HI) with the artificial intelligence (AI) of large language\nmodels (LLMs).\nA potential consequence of applying LLMs to chemistry research is a boost in productivity,\nan effect that has been observed and quantified in other fields of knowledge and labor. 49,50\nTo date, LLMs have been mostly applied to the low-level implementation of projects that\nwere pre-designed by researchers, putting the focus on the automated (and autonomous)\nexecution of concrete, technical tasks. The present Viewpoint shifts the focus up to the\nhigh-level design of a research project, including its defining concept and workflows (Figure\n1). In this setting, the collaboration between the researcher HI and the LLM AI can be\nespecially valuable, and yet remains largely unexplored.\nThe project co-designed with an LLM was framed on the topic of computational cataly-\nsis with machine learning (ML). The main reasons for selecting this topic were: 1) it is\nhighly-specialized, requiring advanced knowledge at the postgraduate level;51 2) it allows for\nobserving how an AI (LLM) contributes to the development of other AIs (ML models); 52\nand 3) the author has expertise on it 48,53–57 and could thus assess the quality of the results.\n4\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nThe co-design task was done under the constraints that the LLM should be 1) general-\npurpose, without any fine-tuned domain specialization, 2) freely available to users through\nthe Internet, and 3) used with minimal prompt engineering. It should be noted that point\n3, which could be regarded as a bad practice in this field, was hereby adopted with the sole\naim of challenging the LLM. The task was based on two conversations with the 4o model of\nChatGPT: one on high-level design, yielding the project outline, and one on intermediate-\nlevel design, yielding more detailed information like, for example, workflows (Figure 1). The\nproject designs were evaluated by both the author and ChatGPT, exploring also the use of an\n“AI forum” in which two other state-of-the-art LLMs (DeepSeek6 and Gemini7) contributed\nto the assessment under the same constraints.\nOverall, the LLMs proved their capacity to develop and assess the project, playing a major\nrole in both the design of the research concept and the derivation of the workflows, though\nsignificant flaws were also observed.\nResults\nFont convention:\n• Sans serif bold : Researcher (author) prompts.\n• Type writer: LLM replies.\nProject design: High-level\nIn the first prompt, I indicated to ChatGPT my scientific area of expertise (“ researcher in\ncomputational chemistry”) and our joint task (“ design a researcher project together ”).\nThe reply opened with an enthusiastic “That sounds like an exciting endeavor!”, which\ncould be seen as a commercial product feature satisfying costumers and a potential distrac-\n5\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\ntion from the critical reading of outputs that can be uncertain without notice, though some\nusers may also appreciate this optimistic tone. 8 The reply continued with “ several key\naspects” that “we should consider”.\nAmong the “key aspects”, which all made sense and, overall, were project-holistic, I picked\nthose that defined the start of the research by prompting the focus (“homogeneous catalysis\nwith transition metals ”) and objective (“ to develop machine learning models predict-\ning energy barriers ”). The reply kept the enthusiasm (“ Great!”) and was positive about\nthe suggested focus and objective (“ well-defined and impactful”). ChatGPT continued\nwith a general sketch of the project that could be already seen as part of the design process,\nreferring to “ literature review” and “ ML model development”, and suggesting options\nand strategies like “publish and collaborate”.\nI then prompted the model to narrow the scope of the project: “ I want to start by defin-\ning the chemical system of interest. Perhaps you can help me? ”. ChatGPT defined\nthis as a “ critical step”, providing me with multiple choices on the chemistry (metals,\nligands, reactions) framed in a context of “ scientific relevance”, “challenges”, “data\navailability”, and “ existing research”, and closing with relevant examples, together\nwith strategies for making my choice.\nAfter reflecting as advised, I chose “ PdL2 complexes for catalytic cross-coupling ”, then\nprompting “How do we start the project? ”. The ChatGPT reply was rich and correct but\nsomehow excessive since it started with a “ literature review”, an continued all the way\nto the “documentation and publication” of the research, including details like the use of\nDFT calculations. Reacting to the intial request, “ Identify key mechanistic insights\nespecially related to catalytic cycles”, I prompted this question: “ focus on pre-\ndicting the barrier of a single step of the catalytic cycle, which step shall we choose? ”.\n6\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nText Box 1 (written by ChatGPT).High-level outline of the co-designed research concept.\nPredictive Modeling of Oxidative Addition Barriers in PdL2 Complexes Using GNNs\nGoal: The primary aim of this research project is to develop a machine learning model using graph\nneural networks (GNNs) to predict the activation energy barriers of the oxidative addition (OA) step\nin PdL2-catalyzed cross-coupling reactions. The project emphasizes leveraging the model’s insights\nto guide ligand design and explore the chemical space for innovative catalytic applications.\nMotivation: Understanding and predicting OA barriers in Pd-catalyzed reactions is crucial for de-\nsigning more efficient catalysts, which are pivotal in industries such as pharmaceuticals and materials\nscience. Transition metal catalysis with Pd complexes is well-recognized for its versatility and effi-\ncacy. By optimizing ligand design to reduce energy barriers, significant improvements can be made\nin catalytic performance and sustainability.\nKey Ideas: Ligand Influence. Focus on elucidating how different ligand environments impact\nthe OA step, providing insights for rational catalyst design. Explainability. Develop explainable\nGNN models that not only predict energy barriers but also offer interpretative insights into molecular\nfeatures influencing these barriers. Open Science. Emphasize transparency and reproducibility\nby making datasets, code, and findings open-access.\nMethods: Data Collection and Structure. Construct a comprehensive dataset of PdL 2 com-\nplexes (over 5050 variants possible with 100 unique ligands), with data preserved in graph format\nto utilize relational information. Graph Neural Networks. Implement existing GNN architec-\ntures (e.g., GCN, GAT) to learn patterns within molecular graphs representing PdL 2 systems. Use\nexplainability methods like GNNExplainer to identify dominant structural features affecting OA.\nModel Training and Validation. Split data for training and testing, using MAE and r 2 as key\nmetrics for model validation. Employ cross-validation techniques to ensure model robustness. Ex-\nploratory Tools. Develop visualization tools for chemical space exploration, aiding chemists in\nidentifying promising ligands for experimental follow-up.\nStructure: Research Design. Define goals, select ligand sets, and set up computational pipelines\nfor data generation and management. Model Development. Construct and train GNN models to\npredict OA barriers, incorporating features from molecular graphs.Validation and Interpretabil-\nity. Validate models using performance metrics and explainability techniques, critically analyzing\nthe impact of ligand features on model predictions. Application and Dissemination. Apply\nmodels to explore chemical spaces for novel ligand ideas, openly sharing results and methodologies\nwith the scientific community.\n7\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nFollowing an overview on the cross-coupling mechanism, ChatGPT recommended to select\nthe oxidative addition (OA) step since it “ is frequently the rate-determining step”,\nthough it correctly noted that other steps like transmetallation and reductive elimination can\nalso play a relevant role in the kinetics. I agreed on selecting the OA reaction and, among\nthe next project design steps suggested, I decided to focus on defining the “ parameters\naffecting OA, including ligand electronic and steric properties” by prompting\n“How many ligands do you think we need to predict OA barriers for PdL 2 complexes?”,\nalso informing ChatGPT about my interest on using deep learning methods and my access\nto high-performance computing (HPC) resources.\nBefore giving a number of ligands, ChatGPT noted the relevance of chemical diversity,\nquantitative structure activity relationships (QSAR), data quality, and applicability. It then\nsuggested using 100-200 ligands that could thereafter be extended to 300-500. I then ignored\nthe next steps proposed since I was curious to ask “ With 100 ligands, how many different\nPdL2 complexes will we have? ” as I wondered Can it do the combinatorics math?The re-\nply made the necessary distinction between homoleptic [Pd(L)2] and heteroleptic [Pd(L)(L’)]\ncomplexes, giving a total of 5050 PdL 2 complexes. This is correct only if the bidentate L\nligands are symmetric (equivalent metal anchors) and the complex is square planar, both\nbeing reasonable assumptions, though they were not prompted by the user nor explicitly\nnoted by the model.\nMy next request was “ design a protocol to automate the computation of the OA tran-\nsition states” (TSs). Here ChatGPT proposed this stepwise approach: “ pre-computation\nsetup”, “ automated TS search”, “ post-computation analysis”, “ data management”,\nand “ML preparation”, referring to diverse items like software selection, scripting, saddle-\npoint optimization algorithms, parallel execution in HPC, error handling, and metadata\nannotation, making also a final note on feature selection.\n8\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nFollowing these suggestions, I prompted “ What are the options to define the features\nand the ML models? ”. For the features, ChatGPT mentioned both traditional approaches,\nlike QSAR with chemical descriptors, and more advanced ML options, including molecular\ngraphs, which were then linked to graph neural network (GNN) methods. Related con-\nsiderations on hyperparameter tuning, cross-validation, and explainability were also given.\nSince I was interested on both graphs and chemical insight, which were not connected in\nthe ChatGPT reply, I prompted “ Let’s use graphs with graph neural networks. Can we\nhave explainability with them? ” The answer provided several approaches to this problem,\nincluding “subgraph analysis”, “attention mechanisms”, and “post hoc techniques”\nlike the “ GNNExplainer”. In the final considerations, the model advocated for the human-\nin-the-loop approach58 stating “collaborate with chemists to validate explanations\nand ensure they align with chemical intuition”.\nAfter giving ChatGPT a final chance to add “anything else we need to define at this high\nlevel”, I prompted it to “ summarize the research project concept co-designed in this\nconversation”, including the “ goal, motivation, key ideas, methods, and structure. ”.\nThe resulting outline is shown in Text Box 1.\nProject design: Intermediate-level\nAs intended, the project outline in Text Box 1 was very general. Besides the additional\ninformation that ChatGPT already provided during the high-level design conversation, I\ndecided to dig deeper into intermediate-level details by asking more concrete questions about\nthe methodology. Text Boxes 2 and 3 provide the prompts and one reply example (full\nconversations are available in the Supporting Information).\n9\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nStructure assembly: For this task, which is more difficult for metal complexes than for\norganic molecules, ChatGPT designed a systematic workflow, making a distinction between\nhomoleptic and heteroleptic species, and including non-trivial tasks like conformational sam-\npling. On the software, it missed libraries tailored for metal complexes, like molSimplify. In\na follow-up question on how to detect the metal-anchoring atoms of the ligands, it extended\nthe association between “ correct isomer” and “lowest energy” with additional, valid cri-\nteria like “ substrate-accessible binding site”, and “ known experimental binding\npatterns” from the literature, recommending “manual review or expert consultation”.\nComputational method: ChatGPT recommended hybrid functionals, like B3LYP and\nM06, combined with double-ζ basis sets including relativistic effective core potentials (ECPs)\nfor palladium. The methods proposed for TS search and validation were in line with this\nDFT approach. In the follow-up prompt, I was curious to see if it could generate input\nfiles59 to run the calculations, which it did correctly for the Gaussian software, and including\nsolvent effects, as requested. It explained the meaning of the keywords, thus promoting and\nfacilitating an informed use of the files. However, there was one important flaw: despite\nreferring to relativistic effects in palladium, it missed the associated keyword requesting the\nuse of ECPs in a way that may go unnoticed, since the input file would run anyway, though\nits output will reflect this missing element.\nData preparation for the GNNs : Building molecular graphs from raw data produced\nby chemistry software is not obvious. ChatGPT made a protocol in which the graph con-\nnectivity was derived from the geometries. The attribution of the graph nodes and edges\nwent beyond trivial features, like atomic number and bond type, adding additional ones like\npartial charges and bond lengths, respectively. The resulting graphs would thus be both\nquantum- and geometry-informed, which is known to boost downstream ML tasks. 55,60,61\nOther important aspects, like formatting, labeling, and validation were also addressed. The\nfollow-up prompt on resolving the metal center connectivity was replied with the sugges-\n10\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\ntion of using quantum mechanical approaches like QTAIM. The question on the origin of\nthis problem was also addressed, using arguments like the presence of “ non-standard bond\ntypes such as π-backbonding”.\nExplainability with GNNs: The multi-step workflow proposed by ChatGPT included the\noptimization of the GNN model, installation of the PyTorch Geometric library to implement\nthe GNNExplainer method, definition of the predictions to be explained, and visualizations\nbased on the molecular graphs. It also recommended ways of leveraging explainability like\ndoing “ modifications in ligand structures to enhance catalytic activity” after\nidentifying which “ subgraphs predominantly influence model predictions”. At the\nvalidation level, “collaborate with domain experts” checking that explanations “ align\nwith chemical understanding and intuition”. Asking for help on such intuition, I\nasked which subgraphs (ligands) would correlate with the lower OA barriers. Here ChatGPT\nshowed its abilities to use chemistry language, correctly pointing at both the electronic and\nsteric properties of the ligands, which need to be non-bulky, strong donors to promote OA,\nalso referring to nuances like secondary van der Waals interactions.\n11\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nText Box 2 (written by the author).Prompts used in the intermediate-level design conversation.\nStructure assembly\nPrompt: We will now take this co-design effort to a lower level closer to project implementation.\nDesign a workflow for the assembly of the structures of the 5050 metal complexes.\nFollow-up prompt: If a ligand has more than one heteroatom that can bind to palladium, how\ncan I adapt this workflow to identify the correct isomer?\nComputational method prompt\nPrompt: Which computational method do you recommend for the calculation of the oxidative\naddition transition state?\nFollow-up prompt: Write a Gaussian input file for transition state optimization. How can I\ninclude solvent effects?\nData preparation for the GNNs prompt\nPrompt: How do I transform the raw data from the Gaussian calculations into graphs for the GNN\nmodels? Make a protocol, with detail.\nFollow-up prompt: It is difficult to define the connectivity of the metal center. Do you know\nwhy? Is there any measure you can suggest to address this problem?\nExplainability with GNNs:\nPrompt: Regarding the explainability with GNNExplainer: Design a workflow for its implementa-\ntion and evaluation.\nFollow-up prompt: Trying to build some intuition: From a molecular graph perspective, which\nsubgraphs would you expect to correlate with the lower oxidative addition barriers?\n12\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nText Box 3 (written by ChatGPT).Reply to low-level design prompt on data preparation for\nGNNs.\nTo transform raw data from Gaussian calculations into graphs suitable for Graph Neural Network\n(GNN) models, you’ll need to process the data from structural and energetic outputs into a format\nthat captures the molecular graph structure. Here’s a detailed protocol to guide you through this\ntransformation:\nStep 1: Geometry Data. Extract optimized Cartesian coordinates from the Gaussian output\nfile. You can use parsing tools like cclib, a Python library that reads and processes computational\nchemistry files. Connectivity Information. If not explicitly available, infer connectivity using\ndistance criteria or using software that can determine bonds based on atomic distances [...]\nStep 2: Define the Molecular Graph Structure. Nodes (Atoms): Each atom in the molecule\nbecomes a node. Assign features to nodes, such as: element type (e.g., one-hot encoding of elements),\natomic number, hybridization state, partial charges or electronegativity. Edges (Bonds): Each bond\nbetween atoms becomes an edge. Define features for edges, like: Bond type (single, double, triple,\naromatic), length, angle (for improved connectivity representation) [...]\nStep 3: Creating Graph Data Structures for GNNs. Graph Data Formatting: Use a graph\nlibrary such as DGL (Deep Graph Library) or PyTorch Geometric for handling graph data effi-\nciently and creating the input data structures required by GNNs. Convert the molecular graph\ndata into these frameworks, ensuring that node and edge are appropriately packed. Data Storage\nand Organization. Organize your graph data into datasets, with clear partitioning for training,\nvalidation, and testing. Use consistent file formats for data retrieval, such as JSON or HDF5 for\nstructured data storage [...]\nStep 4: Integrate Auxiliary Information. Additional Features: Include calculated properties\nfrom Gaussian, like transition state energy or thermodynamic parameters, as additional node, edge,\nor graph-level features as needed. Graph Labeling. Assign labels to the graphs if supervised\nlearning is involved, such as relative barrier heights [...]\nStep 5: Quality Control and Testing. Validation: Validate the graph representations by visual-\nizing a subset using graph visualization tools to ensure structural integrity and semantic meaning\nInitial Model Testing: Conduct initial tests with the GNN model on this dataset to verify data\nquality and preprocessing adequacy before full-scale model training [...]\n13\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nDiscussion and conclusions\nAfter reading the co-designed outline (Text Box 1), I found the project goal to be clear and\nI noticed the reference to the chemical space concept, which is relevant in this field and not\nreferred to in the prompts. The motivation was formulated by making a connection to the\nconcept of sustainability that was not prompted either. From this perspective, it can be\ncriticized that other metals, like nickel, might be more convenient, though ChatGPT men-\ntioned this metal as a possible choice, also in relation to other catalytic reactions that could\nbe of interest (polymerization and cross-electrophile coupling). In the key ideas, there were\nthree highlights, all proposed by the model: 1) impact of ligand nature on the OA step, 2)\nexplainability from graphs, and 3) open science, with the latter strongly aligning with the\nresearch culture of the ML community in chemistry. The literature shows that the methods\nsuggested are state-of-the-art, covering the entire data→ representations → models pipeline,\nas well as its validation. The outline referred to the development of visualization tools in\na somehow vague and isolated manner, and it did not mention the computational origin of\nthe training data, which is a characteristic of the project that might be too relevant to be\nomitted at this level.\nOverall, the research project concept was coherent and capable of passing the Turing test. 62\nAfter forming my opinion, I wanted to have an assessment from ChatGPT itself. In a sep-\narate chat, I prompted: “ I recently outlined the research project given below. First,\nmake a critical assessment of it and, second, tell me if a human can detect that it\nwas co-designed with an AI. ”. The reply was interesting: novelty, relevance, impact, focus\non explainability, and commitment to open science were all identified as strengths, whereas\ndata costs (energy barriers are computationally expensive), model complexity (GNNs are),\nnarrow focus (on OA), and difficult explainability (with GNNs) were listed as weaknesses\nand presented as challenges. On the point of detecting AI, the model said that structure,\nclarity, synthesis of concepts, and emphasis on open science could suggest AI involvement,\n14\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nthough this was difficult to determine definitely. We both agreed that the content appeared\nprofessional and scholarly, and I would also add that it could be understood in detail by any\nspecialist in the field and, in general, by any chemist with a minimal education on AI.\nI extended this critical assessment of the ChatGPT design by using the DeepSeek and Gemini\nLLMs. DeepSeek gave a concrete and elaborate analysis on the weaknesses remarking that,\nfor example, 5,050 data points may be insufficient to train a GNN, and that GNNExplainer\nmay highlight non-causal features. The DeepSeek feedback was insightful and useful from a\npractical point of view, but the model seemed to ignore that this was not a detailed project\ndescription but an outline, as mentioned in the prompt. Similarly, Gemini noted the lack\nof detail, and, interestingly, it disagreed with DeepSeek by stating that the data should be\nlarge enough for training GNN models. Both models are to some extent correct: 5,050 data\npoints should be enough to start a GNN model but more data can make it more accurate\nand transferable. A valuable contribution from Gemini was the suggestion of formulating a\nbaseline for model evaluation.\nRegarding the involvement of AI in the design of the project, DeepSeek concluded that a\nco-intelligence origin was plausible, giving a somehow off-the-prompt critical and opinion-\nated reply, using expressions like “ reliance on ML buzzwords”. Gemini’s reply was more\naligned with the prompt question, stating that it was very unlikely that a human could\ndefinitively detect AI involvement. It also mentioned the lack of style and factual errors that\nare common in older, less advanced LLMs. Overall, the use of other LLMs in the assessment\nprovided valuable information from a critical perspective. The appearance of discrepancies\ncould be an efficient way of identifying elements requiring further examination.\n15\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAt the intermediate-level design, ChatGPT delivered the deeper level of detail that was ex-\npected. It was useful to have many choices and informed support to select one of them.\nHowever, the number of choices and level of detail appeared to be excessive in some of the\nreplies, and it is at this point that more elaborate prompt engineering can become useful.\nIt was clear that the model can manage chemistry knowledge at these three levels: general-\ntextbook, computational-technical, and ML-chemoinformatics, being intelligent in the sense\nof making non-trivial connections like, for example, relating difficulties in defining the molec-\nular graph connectivity to the complex bond patterns that arise from electronic structure\ntheory applied to transition metals.\nThe present Viewpoint showed that the co-intelligent design of research projects with LLMs\nis already possible without any fine-tuned specialization or sophisticate prompt engineering.\nThe use of specialized chemistry terms was not problematic and hallucinations, in the sense\nof made-up content, were not observed, though the generation of citations, a task often af-\nfected by this issue, was not requested. Further, there were important flaws, like missing\nrelativistic effects for palladium, which cannot be tolerated in frontier research of excellence.\nFrom this perspective, de-skilling in the context of AI 63 is a concern, since domain experts\nare needed to assess LLM outputs. However, one can also take this perspective: Assume this\nis the worst AI you will ever use.3 While this work was being done, two new models were\nreleased for research: Deep Research (OpenAI) and co-scientist (Google). Both may take\nco-intelligent research to the next level. Another relevant issue were the assumptions made\nby model like, for example, computing the combinatorics of the chemical space assuming\nsquare planar geometries with symmetric ligands. Even if chemically sound, non-prompted\nassumptions should be noted to the user for avoiding misunderstandings similar to those\noccurring in communication among humans.\n16\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nFinally, and regarding the question Hype or reality?, this Viewpoint showed that LLMs can\nalready contribute to the co-intelligent design of research. As recommended by the models\nthemselves, co-intelligence should follow a human-in-the-loop approach focused on collabora-\ntion and involving supervision by highly-educated researchers. From this perspective, LLMs\ncan be seen as a powerful tool that should be used in a careful, informed manner.\nMethods\nThese three LLM models were used:\n• ChatGPT-4o,4,5 for both the high- and intermediate-level conversations on project\ndesign. The LLM model version used was the 4 Omni.\n• DeepSeek,6 for the critical assessment of the project outline co-designed with Chat-\nGPT. The LLM model version used was DeepThink (R1), with the Search option\nturned off.\n• Gemini,7 for the critical assessment of the project outline co-designed with ChatGPT.\nThe LLM model version used was Flash 2.0 Thinking Experimental.\nDue to the intrinsic nature of LLMs, the prompts used in this study can yield similar but not\nexactly equal replies when re-inputted into the same models. The scarcity and simplicity of\nthe prompts intentionally aimed at challenging the models. Prompt engineering strategies,\nlike chain-of-thought conversations with domain experts providing reasoning, are expected\nto provide results of higher quality. 64 As many other AIs, LLMs are dependent on, and\ninfluenced by, the data used to train them. Further, factors like content filtering, adaption\nto conversation history, and server load can influence the LLM outputs. Finally, though\nall these models support multimodal sessions, only text prompts and generated text output\nwere used for this Viewpoint.\n17\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nSupporting information\nFull transcripts of all conversations.\nConflicts of interest\nThere are no conflicts of interest to declare.\nAcknowledgments\nThe RCN FRIPRO (catLEGOS; project number 325003) and SFF (Hylleraas Centre; project\nnumber 262695) programs for financial support. Ainara Nova for reading the manuscript and\nproviding valuable feedback.\nReferences\n(1) Br¨ ase, S. Digital chemistry: navigating the confluence of computation and experimen-\ntation - definition, status quo, and future perspective. Digital Discovery2024, 3, 1923–\n1932.\n(2) Goldberg, K. Robots and the return to collaborative intelligence. Nat. Mach. Intell.\n2019, 1, 2–4.\n(3) Mollick, E. Co-Intelligence: Living and Working with AI. Book 2024, 1st Edition,\nPenguin.\n(4) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.;\nKaiser, L.; Polosukhin, I. Attention Is All You Need. arXiv 2017-2023, pre-print,\nDOI: 10.48550/arXiv.1706.03762.\n(5) Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.;\nLee, Y. T.; Li, Y.; Lundberg, S.; Nori, H.; Palangi, H.; Ribeiro, M. T.; Zhang, Y.\n18\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nSparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv 2023,\npre-print, DOI: 10.48550/arXiv.2303.12712.\n(6) DeepSeek, A. I.; Guo, D. et al. DeepSeek-R1: Incentivizing Reasoning Ca-\npability in LLMs via Reinforcement Learning. arXiv 2025, pre-print, DOI:\n10.48550/arXiv.2501.12948.\n(7) Anil et al. Gemini: A Family of Highly Capable Multimodal Models. arXiv 2023-2024,\npre-print, DOI: 10.48550/arXiv.2312.11805.\n(8) Salles, A.; Evers, K.; Farisco, M. Anthropomorphism in AI. AJOB Neuroscience2020,\n11, 88–95.\n(9) Yang, C.; Wang, X.; Lu, Y.; Liu, H.; Le, Q. V.; Zhou, D.; Chen, X. Large Language\nModels as Optimizers. arXiv 2023-2024, pre-print, DOI: 10.48550/arXiv.2309.03409.\n(10) Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E. H.; Le, Q. V.;\nZhou, D. Chain-of-thought prompting elicits reasoning in large language models. NIPS\n2022, Conference Proceedings, 24824–2483.\n(11) Strong, E.; DiGiammarino, A.; Weng, Y.; Kumar, A.; Hosamani, P.; Hom, J.;\nChen, J. H. Chatbot vs Medical Student Performance on Free-Response Clinical Rea-\nsoning Examinations. JAMA Intern Med.2023, 183, 1028–1030.\n(12) Krenn, M.; Pollice, R.; Guo, S. Y.; Aldeghi, M.; Cervera-Lierta, A.; Friederich, P.;\nGomes, G. d. P.; H¨ ase, F.; Jinich, A.; Nigam, A.; Yao, Z.; Aspuru-Guzik, A. On\nscientific understanding with artificial intelligence. Nat. Rev. Phys.2022, 4, 761–769.\n(13) Urbina, F.; Lentzos, F.; Invernizzi, C.; Ekins, S. Dual use of artificial-intelligence-\npowered drug discovery. Nat. Mach. Intell.2022, 4, 189–191.\n(14) Walters, W. H.; Wilder, E. I. Fabrication and errors in the bibliographic citations\ngenerated by ChatGPT. Sci. Rep.2023, 13, 14045.\n19\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(15) Chen, A.; Chen, D. O. Accuracy of Chatbots in Citing Journal Articles. JAMA Netw.\nOpen 2023, 6, e2327647.\n(16) Nascimento, C. M. C.; Pimentel, A. S. Do Large Language Models Understand Chem-\nistry? A Conversation with ChatGPT. J. Chem. Inf. Model.2023, 63, 1649–1655.\n(17) Chelli, M.; Descamps, J.; Lavou´ e, V.; Trojani, C.; Azar, M.; Deckert, M.; Raynier, J. L.;\nClowez, G.; Boileau, P.; Ruetsch-Chelli, C. Hallucination Rates and Reference Accuracy\nof ChatGPT and Bard for Systematic Reviews: Comparative Analysis. J. Med. Internet\nRes. 2024, 26, e53164.\n(18) Zhang, W.; Guy, M. A.; Yang, J.; Hao, L.; Liu, J.; Hawkins, J. M.; Mustakis, J.;\nMonfette, S.; Hein, J. E. Leveraging GPT-4 to transform chemistry from paper to\npractice. Digital Discovery 2024, 3, 2367–2376.\n(19) White, A. D. The future of chemistry is language. Nat. Rev. Chem.2023, 7, 457–458.\n(20) Ross, J.; Belgodere, B.; Chenthamarakshan, V.; Padhi, I.; Mroueh, Y.; Das, P. Large-\nscale chemical language representations capture molecular structure and properties.\nNat. Mach. Intell.2022, 4, 1256–1264.\n(21) Herck, J. V. et al. Assessment of fine-tuned large language models for real-world chem-\nistry and material science applications. Chem. Sci. 2025, 16, 670–684.\n(22) Ramos, M. C.; Collison, C. J.; White, A. D. A review of large language models and\nautonomous agents in chemistry. Chem. Sci. 2025, 16, 2514–2572.\n(23) Jablonka, K. M. et al. 14 examples of how LLMs can transform materials science and\nchemistry: a reflection on a large language model hackathon. Digital Discovery 2023,\n2, 1233–1250.\n(24) Wellawatte, G. P.; Schwaller, P. Human interpretable structure-property relationships\n20\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nin chemistry using explainable machine learning and large language models. Commun.\nChem. 2025, 8, 11.\n(25) Boiko, D. A.; MacKnight, R.; Kline, B.; Gomes, G. Autonomous chemical research with\nlarge language models. Nature 2023, 624, 570–578.\n(26) Bran, A. M.; Cox, S.; Schilter, O.; Baldassari, C.; White, A. D.; Schwaller, P. Augment-\ning large language models with chemistry tools. Nat. Mach. Intell.2024, 6, 525–535.\n(27) Gibb, B. C. Eric the AI research assistant for chemists. Nat. Chem.2025, 17, 157–158.\n(28) F´ ebba, D.; Egbo, K.; Callahan, W. A.; Zakutayev, A. From text to test: AI-generated\ncontrol software for materials science instruments. Digital Discovery 2025, 4, 35–45.\n(29) Zheng, Z. L.; Zhang, O. F.; Borgs, C.; Chayes, J. T.; Yaghi, O. M. ChatGPT Chemistry\nAssistant for Text Mining and the Prediction of MOF Synthesis. J. Am. Chem. Soc.\n2023, 145, 18048–18062.\n(30) Mor´ an-Gonz´ alez, L.; Burnage, A. L.; Nova, A.; Balcells, D. AI Approaches to Homo-\ngeneous Catalysis with Transition Metal Complexes. ACS Catal.2025, 15, 9089–9105.\n(31) Leong, S. X.; Pablo-Garc´ ıa, S.; Zhang, Z.; Aspuru-Guzik, A. Automated electrosyn-\nthesis reaction mining with multimodal large language models. Chem. Sci. 2024, 15,\n17881–17891.\n(32) Ai, Q. X.; Meng, F. W.; Shi, J.; Pelkie, B.; Coley, C. W. Extracting structured data from\norganic synthesis procedures using a fine-tuned large language model. Digital Discovery\n2024, 3, 1822–1831.\n(33) Ye, G. De novo drug design as GPT language modeling: large chemistry models with\nsupervised and reinforcement learning. J. Comput. Aided Mol.2024, 38, 20.\n(34) Bhattacharya, D.; Cassady, H. J.; Hickner, M. A.; Reinhart, W. F. Large Language\nModels as Molecular Design Engines. J. Chem. Inf. Model.2024, 64, 7086–7096.\n21\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(35) Antunes, L. M.; Butler, K. T.; Grau-Crespo, R. Crystal structure generation with\nautoregressive large language modeling. Nat. Commun. 2024, 15, 10570.\n(36) Skinnider, M. A.; Stacey, R. G.; Wishart, D. S.; Foster, L. J. Chemical language models\nenable navigation in sparsely populated chemical space. Nat. Mach. Intell. 2021, 3,\n759–770.\n(37) Ruan, Y.; Lu, C.; Xu, N.; He, Y.; Chen, Y.; Zhang, J.; Xuan, J.; Pan, J.; Fang, Q.;\nGao, H.; Shen, X.; Ye, N.; Zhang, Q.; Mo, Y. An automatic end-to-end chemical syn-\nthesis development platform powered by large language models. Nat. Commun. 2024,\n15, 10160.\n(38) Zhang, C.; Lin, Q.; Zhu, B.; Yang, H.; Lian, X.; Deng, H.; Zheng, J.; Liao, K. SynAsk:\nunleashing the power of large language models in organic synthesis. Chem. Sci. 2025,\n16, 43–56.\n(39) Wu, Z.; Zhang, O.; Wang, X.; Fu, L.; Zhao, H.; Wang, J.; Du, H.; Jiang, D.; Deng, Y.;\nCao, D.; Hsieh, C. Y.; Hou, T. Leveraging language model for advanced multiproperty\nmolecular optimization via prompt engineering. Nat. Mach. Intell.2024, 6, 1359–1369.\n(40) Hare, P. M. Coding with AI in the Physical Chemistry Laboratory. J. Chem. Educ.\n2024, 101, 3869–3874.\n(41) Alasadi, E. A.; Baiz, C. R. Multimodal Generative Artificial Intelligence Tackles Visual\nProblems in Chemistry. J. Chem. Educ.2024, 101, 2716–2729.\n(42) Guo, Y.; Lee, D. Leveraging ChatGPT for Enhancing Critical Thinking Skills. J. Chem.\nEduc. 2023, 100, 4876–4883.\n(43) Zheng, Z.; Rampal, N.; Inizan, T. J.; Borgs, C.; Chayes, J. T.; Yaghi, O. M. Large lan-\nguage models for reticular chemistry.Nat. Rev. Mater.2025, 10, DOI: 10.1038/s41578–\n025–00772–8.\n22\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(44) Guan, C.; Fernandes, F. C.; Franco, O. L.; de la Fuente-Nunez, C. Leveraging large\nlanguage models for peptide antibiotic design. Cell Rep. Phys. Sci..2025, 6, 102359.\n(45) Ma, K. AI agents in chemical research: GVIM – an intelligent research assistant system.\nDigital Discovery 2025, 4, 355–375.\n(46) Schilling-Wilhelmi, M.; R´ ıos-Garc´ ıa, M.; Shabih, S.; Gil, M. V.; Miret, S.; Koch, C. T.;\nM´ arquez, J. A.; Jablonka, K. M. From text to insight: large language models for\nchemical data extraction. Chem. Soc. Rev.2025, 54, 1125–1150.\n(47) Lu, J.; Song, Z.; Zhao, Q.; Du, Y.; Cao, Y.; Jia, H.; Duan, C. Generative Design\nof Functional Metal Complexes Utilizing the Internal Knowledge of Large Language\nModels. ChemRxiv 2024, pre-print, DOI: 10.26434/chemrxiv–2024–z29m3.\n(48) Kneiding, H.; Balcells, D. Augmenting genetic algorithms with machine learning for\ninverse molecular design. Chem. Sci. 2024, 15, 15522–15539.\n(49) Peng, S.; Kalliamvakou, E.; Cihon, P.; Demirer, M. The Impact of AI on Devel-\noper Productivity: Evidence from GitHub Copilot. arXiv 2023, pre-print, DOI:\n10.48550/arXiv.2302.06590.\n(50) Noy, S.; Zhang, W. Experimental evidence on the productivity effects of generative\nartificial intelligence. Science 2023, 381, 187–192.\n(51) von Lilienfeld, O. A.; M¨ uller, K. R.; Tkatchenko, A. Exploring chemical compound\nspace with quantum-based machine learning. Nat. Rev. Chem.2020, 4, 347–358.\n(52) Krenn, M.; Buffoni, L.; Coutinho, B.; Eppel, S.; Foster, J. G.; Gritsevskiy, A.; Lee, H.;\nLu, Y.; Moutinho, J. P.; Sanjabi, R., N Sonthalia; Tran, N. M.; Valente, F.; Xie, Y.;\nYu, R.; Kopp, M. Forecasting the future of artificial intelligence with machine learning-\nbased link prediction in an exponentially growing knowledge network.Nat. Mach. Intell.\n2023, 5, 1326–1335.\n23\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(53) Friederich, P.; dos Passos Gomes, G.; De Bin, R.; Aspuru-Guzik, A.; Balcells, D. Ma-\nchine learning dihydrogen activation in the chemical space surrounding Vaska’s com-\nplex. Chem. Sci. 2020, 11, 4584–4601.\n(54) Chen, S.; Nielson, T.; Zalit, E.; Skjelstad, B. B.; Borough, B.; Hirschi, W. J.; Yu, S.;\nBalcells, D.; Ess, D. H. Automated Construction and Optimization Combined with Ma-\nchine Learning to Generate Pt(II) Methane C–H Activation Transition States Original\nPaper. Top. Catal.2022, 65, 312–324.\n(55) Kneiding, H.; Lukin, R.; Lang, L.; Reine, S.; Pedersen, T. B.; De Bin, R.; Balcells, D.\nDeep learning metal complex properties with natural quantum graphs.Digital Discovery\n2023, 2, 618–633.\n(56) Mor´ an-Gonz´ alez, L.; Betten, J. E.; Kneiding, H.; Balcells, D. AABBA Graph Kernel:\nAtom-Atom, Bond-Bond, and Bond-Atom Autocorrelations for Machine Learning. J.\nChem. Inf. Model.2024, 64, 8756–8769.\n(57) Kneiding, H.; Nova, A.; Balcells, D. Directional multiobjective optimization of metal\ncomplexes at the billion-system scale. Nat. Comput. Sci.2024, 4, 263–273.\n(58) Mosqueira-Rey, E.; Hern´ andez-Pereira, E.; Alonso-R´ ıos, D.; Bobes-Bascar´ an, J.;\nFern´ andez-Leal, A. Human-in-the-loop machine learning: a state of the art.Artif. Intell.\nRev. 2023, 56, 3005–3054.\n(59) Jacobs, P. F.; Pollice, R. Developing large language models for quantum chemistry\nsimulation input generation. Digital Discovery 2025, 4, DOI: 10.1039/D4DD00366G.\n(60) Guan, Y.; Coley, C. W.; Wu, H.; Ranasinghe, D.; Heid, E.; Struble, T. J.; Pattanaik, L.;\nGreen, W. H.; Jensen, K. F. Regio-selectivity prediction with a machine-learned reac-\ntion representation and on-the-fly quantum mechanical descriptors. Chem. Sci. 2021,\n12, 2198–2208.\n24\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(61) Atz, K.; Grisoni, F.; Schneider, G. Geometric deep learning on molecular representa-\ntions. Nat. Mach. Intell.2021, 3, 1023–1032.\n(62) Turing, A. M. Computing Machinery and Intelligence. Mind 1950, 49, 433–460.\n(63) Crowston, K.; Bolici, F. Deskilling and upskilling with AI systems. iConference 2025,\nproceedings, https://crowston.syr.edu/sites/crowston.syr.edu/files/GAI and skills.pdf.\n(64) Luo, F.; Zhang, J.; Wang, Q.; Yang, C. Leveraging Prompt Engineering in Large Lan-\nguage Models for Accelerating Chemical Research. ACS Cent. Sci.2025, 11, 511–519.\n25\nhttps://doi.org/10.26434/chemrxiv-2025-5d76r-v2 ORCID: https://orcid.org/0000-0002-3389-0543 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.44809722900390625
    },
    {
      "name": "Catalysis",
      "score": 0.41391411423683167
    },
    {
      "name": "Knowledge management",
      "score": 0.3390439450740814
    },
    {
      "name": "Process management",
      "score": 0.3263680934906006
    },
    {
      "name": "Engineering",
      "score": 0.2923569083213806
    },
    {
      "name": "Chemistry",
      "score": 0.24972909688949585
    },
    {
      "name": "Organic chemistry",
      "score": 0.08286690711975098
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I184942183",
      "name": "University of Oslo",
      "country": "NO"
    }
  ],
  "cited_by": 1
}