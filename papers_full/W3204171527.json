{
  "title": "Localizing Objects with Self-Supervised Transformers and no Labels",
  "url": "https://openalex.org/W3204171527",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227577663",
      "name": "Siméoni, Oriane",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746806239",
      "name": "Puy, Gilles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287298078",
      "name": "Vo, Huy V.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288815277",
      "name": "Roburin, Simon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223092418",
      "name": "Gidaris, Spyros",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222899354",
      "name": "Bursuc, Andrei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753705823",
      "name": "Pérez, Patrick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162490956",
      "name": "Marlet, Renaud",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224480118",
      "name": "Ponce, Jean",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W3008503415",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2962877362",
    "https://openalex.org/W3102696055",
    "https://openalex.org/W1586079445",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2167828171",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2086412200",
    "https://openalex.org/W3101821705",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W3035725370",
    "https://openalex.org/W2098493135",
    "https://openalex.org/W3185290741",
    "https://openalex.org/W2994971263",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2119474464",
    "https://openalex.org/W2112301665",
    "https://openalex.org/W3167700966",
    "https://openalex.org/W2772804149",
    "https://openalex.org/W2088049833",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3170360335",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2105628432",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963996492",
    "https://openalex.org/W3095121901",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2964028976",
    "https://openalex.org/W2951270658",
    "https://openalex.org/W2995181141",
    "https://openalex.org/W2991448611",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W2963603913",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2911448865",
    "https://openalex.org/W2990500698",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2952809485",
    "https://openalex.org/W2919112510",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3106670560",
    "https://openalex.org/W3120055985",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W3175492513",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2981842371",
    "https://openalex.org/W2103658758",
    "https://openalex.org/W3123746945",
    "https://openalex.org/W2926779152",
    "https://openalex.org/W2210113682",
    "https://openalex.org/W3070936185",
    "https://openalex.org/W3116298410",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3158775450",
    "https://openalex.org/W1919709169"
  ],
  "abstract": "Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. The code to reproduce our results can be found at https://github.com/valeoai/LOST.",
  "full_text": "Localizing Objects with Self-Supervised Transformers\nand no Labels\nOriane Sim´eoni1, Gilles Puy1, Huy V . V o1,2, Simon Roburin1,3, Spyros Gidaris1, Andrei\nBursuc1, Patrick P´erez1, Renaud Marlet1,3 and Jean Ponce2,4\n1Valeo.ai, Paris, France\n2Inria and DIENS (ENS-PSL, CNRS, Inria), Paris, France\n3LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vall´ee, France\n4Center for Data Science, New York University, New York, USA\nAbstract\nLocalizing objects in image collections without supervision can help to avoid expensive annotation\ncampaigns. We propose a simple approach to this problem, that leverages the activation features of a\nvision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any\nexternal object proposal nor any exploration of the image collection; it operates on a single image. Yet,\nwe outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC\n2012. We also show that training a class-agnostic detector on the discovered objects boosts results by\nanother 7 points. Moreover, we show promising results on the unsupervised object discovery task. The\ncode to reproduce our results can be found at https://github.com/valeoai/LOST.\nFigure 1: Three applications of LOST to unsupervised single-object discovery (left), multi-object discovery (middle)\nand object detection (right). In the latter case, objects discovered by LOST are clustered into categories, and cluster\nlabels are used to train a classical object detector. Although large image collections are used to train the underlying\nimage representation [13] and the detector [51], no annotationis ever used in the pipeline. See Figure 3 and Tables\n1, 3 for more experiments.\n1 Introduction\nObject detectors are now part of critical systems, such as autonomous vehicles. However, to reach a high\nlevel of performance, they are trained on a vast amount of costly annotated data. Various approaches have\nbeen proposed to reduce these costs, such as semi-supervision [41], weak supervision [52], active-learning\n[3] and self-supervision [25] with task ﬁne-tuning.\nWe consider here the extreme case of localizing objects in images without any annotation. Early\nworks investigate regions proposals based on saliency [ 84] or intra-image similarity [ 65], i.e., only\nbetween patches within the considered image (not across the image collection). However, these proposals\n1\narXiv:2109.14279v1  [cs.CV]  29 Sep 2021\nhave low precision and are produced in large quantities only to reduce the search space in other tasks,\nsuch as supervised [28, 27] or weakly-supervised [9, 60] object detection. Often using region proposals\nas input, unsupervised object discovery leverages information from the entire image collection and\nexplores inter-image similarities to localize objects in an unsupervised fashion, e.g., with probabilistic\nmatching [15], principal component analysis [ 72], optimization [67, 68] and ranking [69]. However,\nbecause of the quadratic complexity of region comparison among images, together with the high number\nof region proposals for a single image, these methods hardly scale to large datasets. Other approaches do\nnot require annotations but exploit extra modalities, e.g., audio [1] or LiDAR [61].\nWe propose here a simple approach to localize objects in an image, that we then apply tounsupervised\nobject discovery. Our localization method stays at the level of a single image, rather than exploring\ninter-image similarity, which makes it linear w.r.t. the number of images and thus highly scalable. For\nthis, we leverage high-quality features obtained from a visual transformer pre-trained with DINO self-\nsupervision [13]. Concretely, we divide the image of interest into equal-sized patches and feed it to the\nDINO model. Instead of focusing on the CLS token, we propose to use the key component of the last\nattention layer for computing the similarities between the different patches. In doing so, we are able to\nlocalize a part of an object by selecting the patch with the least number of similar patches, here called\nthe seed. The justiﬁcation for this seed selection criterion is based on the empirical observation that\npatches of foreground objects are less correlated than patches corresponding to background. We add\nto this initial seed other patches that are highly correlated to it and thus likely to be part of the same\nobject, a process which we call seed expansion. Finally, we construct a binary object segmentation mask\nby computing the similarities of each image patch to the selected seed patches and infer the bounding\nbox of an object as the box that tightly encloses the largest connected component in this mask that\ncontains the initial seed. In following this simple method, we not only outperform methods for region\nproposals but also those for single-object discovery. Even more, by training an off-the-self class-agnostic\nobject detector using our localized boxes as ground-truth boxes, we are able to derive a much more\naccurate object localization model that is actually able to detect multiple objects in an image. We call this\ntask unsupervised class-agnostic object detection(which may resort to self-supervision despite being\ncalled unsupervised). Finally, by using clustering techniques to group the localized objects into visual\nconsistent classes, we are able to train class-aware object detectors without any human supervision, but\nusing instead the predicted object locations and their cluster ids as ground-truth annotations. We call\nthis task unsupervised (class-aware) object detection. We show that the predictions of our unsupervised\ndetection model for certain clusters correlate very well with labelled semantic classes in the dataset and\nreach for them detection results competitive to object detectors trained with weak supervision [9, 60].\nOur main contributions are as follows: (1) we show how to extract relevant features from a self-\nsupervised pre-trained vision transformer and use the patch correlations within an image to propose a\nsimple single-object localization method with linear complexity w.r.t. to dataset size; (2) we leverage\nit to train both class-agnostic and class-aware unsupervised object detectors able to accurately localize\nmultiple object per image and, in the class-aware case, group them to semantically-coherent classes;\n(3) we outperform the state of the art in unsupervised object discovery with a signiﬁcant margin.\n2 Related work\nObject detection with limited supervision.Region proposal methods [4, 65, 84] generate in an unsu-\npervised way numerous class-agnostic bounding boxes with high recall but low precision, to speed-up\nsliding window search. From supervised pre-trained networks, objects can emerge by masking the\ninput [7], interpreting neurons [ 81] or from saliency maps [ 54]. Weakly-supervised object detection\n(WSOD) uses image-level labels without bounding boxes [9, 60] to learn to detect objects. The different\ninstances of WSOD (each with speciﬁc assumptions on the availability and amount of image-level\n2\nand box-level annotations) are often addressed as semi-supervised learning [23, 59] and leverage self-\ntraining [49, 37]. Recent work replaces manual annotations with automatic supervision from a different\nmodality, e.g., LiDAR [61] or audio [1]. In contrast, we do not use any annotations or other modalities at\nany stage: we extract object candidates from the activations of a self-supervised pre-trained network,\ncompute pseudo-labels and then train an object detector.\nObject discovery. Given a collection of images, object discovery groups images depicting similar\nobjects, and then localizes objects within these images. Early works [29, 53, 56, 58, 71] focus mostly on\nthe ﬁrst task and to, a lesser extent, on localization [53, 56, 78, 82]. On the contrary, [15, 38, 67, 68, 69]\nshift focus on the second task and achieve good object localization on image collections in the wild.\nHowever, casting object discovery as the selection of recurring visual patterns across an image collection\ninvolves expensive computation and only [69] is able to scale to large datasets. Our work also discovers\nobject locations but does not consider inter-image similarity. Instead, we rely on the power of self-\nsupervised transformer features [13] and only consider intra-image similarity. Consequently, our method\ncan localize objects in a single image with little computation. Close to ours, [80] is also able to localize\nobjects from a single image by exploiting scale-invariant features. Finally, some works [10, 20, 30, 43, 46]\non object discovery attempt to simultaneously learn an image representation and to decompose images\ninto object masks. These works, however, are only evaluated on image collections of very simple\ngeometric objects.\nTransformers. In this work, we leverage transformer representations to address object discovery. Self-\nattention layers have been previously integrated into CNNs [ 35, 70, 11], yet transformers for vision\nare very recent [ 50, 16, 14, 18] and still in an incipient stage. Findings on training heuristics [62, 77]\nand architecture design [ 42, 63, 76] are released at high pace. Early adaptations of transformers to\ndifferent tasks (e.g., image classiﬁcation [18], retrieval [19], object detection [11, 83, 42] and semantic\nsegmentation [42, 57, 74]) have demonstrated their utility and potential for vision. Meanwhile, several\nworks attempt to better understand this new family of models from various perspectives [13, 47, 64, 8, 45].\nInterestingly, transformers have been shown to be less biased towards textures than CNNs [ 64, 47],\nhinting that their features encapsulate more object-aware representations. These ﬁndings motivate us to\nstudy manners of localizing objects from transformer features.\nSelf-supervised learning (SSL)is a powerful training scheme to learn useful representations without\nhuman annotations. It does so via a pretext learning task for which the supervision signal comes from the\ndata itself [48, 26, 79]. SSL pre-trained networks have been shown to outperform ImageNet pre-trained\nnetworks on several computer vision tasks, in particular object detection [ 24, 32, 12, 31, 25]. For\ntransformers, SSL methods also work well [13, 75], bringing a few interesting side-effects. In particular,\nDINO [13] feature activations appear to contain explicit information about the semantic segmentation\nof objects in an image. In the same spirit, we extract another kind of transformer features to build our\nobject localization.\n3 Proposed approach\nOur method exploits image representations extracted by a vision transformer. In this section, we ﬁrst\nrecall how such representations are obtained, then present our method.\n3.1 Transformers for Vision\nInput. Vision transformers operate on a sequence of patches of ﬁxed size P×P. For a color image I of\nspatial size H×W, we have N = HW/P2 patches of size 3P2 (we assume for simplicity that H and\nW are multiples of P). Each patch is ﬁrst embedded in a d-dimensional latent space via a trained linear\n3\nFigure 2: Initial seed, patch similarities and patch degrees.Top: images from Pascal VOC2007. Middle: initial\nseed p∗(in red) and patches similar to p∗(in grey), i.e., such that f\n⊤\np fq ≥0 hence ap∗q = 1. Bottom: map of\ninverse degrees 1/dp of all patches p (yellow to blue, for low to high degrees). The initial seed p∗is the patch with\nthe lowest degree. Figure is best viewed in color.\nprojection layer. An additional, learned vector called the “class token”, CLS, is adjoined to the patch\nembeddings, yielding a transformer input in R(N+1)×d.\nSelf-attention. Transformers consist of a sequence of multi-head self-attention layers and multi-layer\nperceptrons (MLPs) [ 66, 18]. Three different learned linear transformations are applied to an input\nX ∈R(N+1)×d of a self-attention layer to produce a query Q, a key K and a value V, all in R(N+1)×d.\nThe output of the self-attention layer is Y = softmax\n(\nd−1/2 QK⊤)\nV ∈R(N+1)×d, where softmax is\napplied row-wise. For simplicity, we describe here the case of a single-head attention layer, but attention\nlayers usually contain multiple heads. In this work, we concatenate the keys (or queries, or values) from\nall heads in the last self-attention layer to obtain our feature representations.\nFeatures for object localization.We use transformers trained in a self-supervised manner using DINO\n[13]. Caron et al. [13] show that sensible object segmentations can be obtained from the self-attention of\nthe CLS query produced by the last attention layer. We adapt this strategy in section 4 to perform object\nlocalization, providing a baseline (‘DINO-seg’) that produces fair results. However, we found that its\ndoes not fully exploit the potential of the self-supervised transformer features. We propose a novel and\neffective strategy for localizing objects using another way to extract and use features. Our method, called\nLOST, is constructed by computing similarities between patches of a single image, using this time patch\nkeys kp ∈Rd, p= 1,...,N , extracted at the last layer of a transformer.\n3.2 Finding objects with LOST\nOur method takes as input d-dimensional image features F ∈RN×d extracted from a single image via a\nneural network; N denotes the spatial dimension (number of patches) of the image features F, while\nfp ∈Rd is the feature vector of the patch at spatial position p∈{1,...,N }. We assume that there is at\nleast one object in the image and LOST tries to localize one of them given the input features. To that end,\nit relies on a selection of patches that are likely to belong to an object. We call these patches “seeds”.\n4\nFigure 3: Object localizations on VOC07. The red square represents the seed p∗, the yellow box is the box\nobtained using only the seed p∗, and the purple box is the box obtained using all the seeds S.\nInitial seed selection.Our seed selection strategy is based on the assumptions that (a) regions/patches\nwithin objects correlate more with each other than with background patches and vice versa, and (b) an\nindividual object covers less area than the background. Consequently, a patch with little correlation in\nthe image has higher chances to belong to an object.\nTo compute the patch correlations, we rely on the distinctiveness of self-supervised transformer\nfeatures, which is particularly noticeable when using transformer’s keys. We empirically observe that\nusing these tranformer features as patch representation meets assumption (a) in practice: patches in an\nobject correlate positively with each other but negatively with patches in the background. Therefore,\nbased on assumption (b), we select the ﬁrst seed p∗by picking the patch with the smallest number of\npositive correlations with other patches.\nConcretely, we build a patch similarity graph Gper image, represented by the binary symmetric\nadjacency matrix A= (apq)1≤p,q≤N ∈{0,1}N×N such that\napq =\n{ 1 if f\n⊤\np fq ≥0,\n0 otherwise. (1)\nIn other words, two nodes p,q are connected by an undirected edge if their features fp,fq are positively\ncorrelated. Then, we select the initial seed p∗as a patch with the lowest degree dp:\np∗= arg min\np∈{1,...,N}\ndp where dp =\nN∑\nq=1\napq. (2)\nWe show in Figure 2 examples of seeds p∗selected in four different images. A representation of the\ndegree map for each of these images is also presented. We remark that the patches with lowest degrees\nare the most likely to fall in an object. Finally, we also observe in this ﬁgure that the few patches that\ncorrelate positively with p∗are also likely to belong to an object.\nSeed expansion. Once the initial seed is selected, the second step consists in selecting patches correlated\nwith the seed that are also likely to fall in the object. Again, we achieve this step relying on the\nempirical observations that pixels within an object tend to be positively correlated and to have a small\ndegree in G. We select the next best seeds after p∗as the pixels that are positively correlated with fp∗:\nS= {q|q∈Dk and f\n⊤\nq fp∗ ≥0}within Dk, the kpatches with the lowest degree. (In case of patches\nwith equal degrees, we break ties arbitrarily to ensure that |Dk|= k.) Note that p∗∈Dk and a typical\nvalue for kis 100.\nBox extraction. The last step consists in computing a mask m ∈ {0,1}N by comparing the seed\nfeatures in Swith all the image features. The qth entry of the mask m satisﬁes\nmq =\n{\n1 if ∑\ns∈Sf\n⊤\nq fs ≥0,\n0 otherwise. (3)\n5\nIn other words, a patch q is considered as part of an object if, on average, its feature fq positively\ncorrelates with the features of the patches in S. To remove the last spurious correlated patches, we ﬁnally\nselect the connected component in m that contains the initial seed and use the bounding box of this\ncomponent as the detected object. An illustration of the detected boxes before and after seed expansion\nis provided in Figure 3.\n3.3 Towards unsupervised object detection\nWe exploit the accurate single-object localization of LOST for training object detection models without\nany human supervision. Starting from a set of unlabeled images, each one assumed to contain at least\none prominent object, we extract one bounding box per image using LOST. Then, we train off-the-shelf\nobject detectors using these pseudo-annotated boxes. We explore two scenarios: class-agnostic and\n(pseudo) class-aware training of object detectors.\nClass-agnostic detection (CAD).A class-agnostic detection model localizes salient objects in an image\nwithout predicting nor caring about their semantic category. We train such a detector by assigning\nthe same “foreground” category to all the boxes produced by LOST, which we call “pseudo-boxes”\nafterwards, as they are obtained with no supervision. Unlike LOST, the trained detector can localize\nmultiple objects per image, even if it was trained on a dataset containing only one pseudo-box annotation\nper image. The experiments conﬁrm that the trained detector can output multiple detections and the\nquantitative results (Table 1) show that this trained detector is in fact even better than LOST in terms of\nlocalization accuracy.\nClass-aware detection (OD).We now consider a typical detector that both localizes objects and recog-\nnizes their semantic category. To train such a detector, apart from LOST’s pseudo-boxes, we also need a\nclass label for each of these boxes. In order to remain fully-unsupervised, we discover visually-consistent\nobject categories using K-means clustering. For each image, we crop the object detected by LOST, resize\nthe cropped image to 224 ×224, feed this image in the DINO pre-trained transformer, and extract the\nCLS token at the last layer. The set of CLS tokens are clustered using K-means and the cluster index is\nused as a pseudo-label for training the detector. At evaluation time, we match these pseudo-labels to the\nground-truth class labels using the Hungarian algorithm [39], which give names to pseudo-labels.\n4 Experiments\nWe explore in this section three variants of the object localization problem, in order of increasing\ncomplexity: (1) localizing one salient object in each image (single-object discovery) in§4.2, (2) using the\ncorresponding bounding boxes as ground-truth to train a binary classiﬁer for foreground object detection\n(unsupervised class-agnostic object detection), and (3) using clustering to capture an unsupervised notion\nof object categories, and detect the corresponding instances (unsupervised object detection). Both are\ndiscussed in §4.3. None of the building blocks of this pipeline uses any annotation, just a large number\nof unlabelled images to sequentially train, in a self-supervised way, the DINO transformer, the class\nagnostic foreground/background classiﬁer, and ﬁnally the classiﬁer using the cluster identiﬁer as labels.\nAlso, we provide more qualitative results in supplementary.\n4.1 Experimental setup\nBackbone networks. Unless otherwise speciﬁed, we use the ViT-S model introduced in [ 13], which\nfollows the architecture of DEiT-S [62]. It is trained using DINO [13], with a patch size of P = 16and\nthe keys K (without the entry corresponding to the CLS token) of the last layer as input features F, with\nwhich we achieve the best results. Results obtained alternatively with the attention, the queries and values\n6\nMethod VOC07 trainval VOC12 trainval COCO 20k\nSelective Search [65] 18.8 20.9 16.0\nEdgeBoxes [84] 31.1 31.6 28.8\nKim et al. [38] 43.9 46.4 35.1\nZhang et al. [80] 46.2 50.5 34.8\nDDT+ [72] 50.2 53.1 38.2\nrOSD [68] 54.5 55.3 48.5\nLOD [69] 53.6 55.1 48.5\nDINO-seg (w. ViT-S/16) 45.8 46.2 42.1\nLOST (ours) 61.9 64.0 50.7\nrOSD [68] + CAD 58.3 62.3 53.0\nLOD [69] + CAD 56.3 61.6 52.7\nLOST (ours) + CAD 65.7 70.4 57.5\nTable 1: Single-object discovery.CorLoc performance on VOC07 trainval, VOC12 trainval and COCO 20k. We\ncompare LOST to state-of-the-art object discovery methods [38, 68, 69, 72, 80], as well as to two object proposal\nmethods [65, 84]. We also compare to the segmentation method proposed in DINO [ 13], denoted by DINO-seg.\nAdditionally, we train a class-agnostic dectector (+ CAD) using as ground-truth either our pseudo-boxes or the boxes\nof rOSD [68] or LOD [69].\nare presented and discussed in the supplementary material. For comparison, we also present results using\nthe base version of ViT (ViT-B), ViT-S with a patch size ofP = 8, as well as with features of the last\nconvolutional layer of a dilated ResNet-50 [34] and of a VGG16 [55] pre-trained either following DINO,\nor in a supervised fashion on Imagenet [17].\nDatasets. We evaluate the performance of our approach on the three variants of object localization\non VOC07 [22] trainval+test, VOC12 [21] trainval and COCO 20K [40, 68]. VOC07 and VOC12 are\ncommonly used benchmarks for object detection [ 28, 27]. COCO 20k is a subset of the COCO2014\ntrainval dataset [40], consisting of 19817 randomly chosen images, used as a benchmark in [68]. When\nevaluating results on the unsupervised object discovery task, we follow a common practice and evaluate\nscores on the trainval set of the different datasets. Such an evaluation is possible as the task is fully\nunsupervised. We follow the same principle for the unsupervised class-agnostic task: we generate boxes\non VOC07 trainval, VOC12 trainval and COCO 20k, use them to train a class-agnostic detector, and then\nevaluate again on these datasets (against ground-truth boxes this time). For unsupervised class-aware\nobject detection, we generate boxes and train the detector on VOC07 trainval and/or VOC12 trainval,\nbut evaluate the detector on the VOC07 test set to facilitate comparisons to weakly-supervised object\ndetection methods. Note that for unsupervised object discovery, some previous works [67, 68, 69, 72]\nevaluate on subsets of VOC07 trainval and VOC12 trainval. For completeness, we present the object\ndiscovery performance of our method on these reduced datasets in the supplemental material.\n4.2 Application to unsupervised object discovery\nSimilar to methods for unsupervised single-object discovery, LOST produces one box for each image.\nIt therefore can be directly evaluated for this task. Following [ 15, 67, 68, 69, 80], we use the Correct\nLocalization (CorLoc) metric, i.e., the percentage of correct boxes, where a predicted box is considered\ncorrect if it has an intersection over union(IoU) score superior to 0.5 with one of the labeled object\nbounding boxes.\nComparison to prior work.In Table 1, we present the CorLoc of our method, in comparison to\nstate-of-the-art object discovery methods [38, 68, 69, 72, 80] and region proposals [65, 84].\n7\nBackbone pre-training VOC07 trainval VOC12 trainval COCO 20k\nVGG16 supervised 42.0 47.2 30.2\nResNet50 supervised 33.5 39.1 25.5\nResNet50 DINO 36.8 42.7 26.5\nViT-S/8 DINO 55.5 57.0 49.5\nViT-S/16 DINO 61.9 64.0 50.7\nViT-B/16 DINO 60.1 63.3 50.0\nTable 2: Impact of the backbone.We evaluate LOST on features originating from different backbones: ViT [18]\nsmall (ViT-S) and base (ViT-B) with patch sizeP = 8or 16, ResNet50 [34] pre-trained following DINO [13], and\nVGG16 [55] and ResNet50 trained in a fully-supervised fashion on Imagenet [17].\nMethodSupervis.aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmeanWSDDN [9]weak39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.934.8PCL [60]weak54.4 9.0 39.3 19.2 15.7 62.9 64.4 30.0 25.1 52.5 44.4 19.6 39.3 67.7 17.8 22.9 46.6 57.5 58.6 63.043.5rOSD [68] + ODnone38.8 44.7 25.2 15.8 0.0 52.9 45.4 38.9 0.0 16.6 24.4 43.3 57.2 51.6 8.2 0.7 0.0 9.1 65.8 9.427.4LOST pseudo-boxesnone42.8 0.0 16.4 3.9 0.0 32.4 17.1 26.2 0.0 14.2 11.3 28.1 43.9 15.8 2.2 0.0 0.1 5.6 39.9 2.315.1LOST + ODnone57.4 0.0 40.0 19.3 0.0 53.4 41.2 72.2 0.2 24.0 28.1 55.0 57.2 25.0 8.3 1.1 0.9 21.0 61.4 5.628.6LOST + OD† none62.0 38.5 49.3 23.1 4.2 57.0 41.9 70.4 0.0 3.6 18.9 30.8 52.8 45.5 12.5 0.6 9.1 9.0 67.2 0.829.9\nTable 3: Object detection.Results (in AP@0.5 %) on VOC07 test. LOST+ OD and rOSD [68] + OD are trained\non VOC07 trainval. LOST + OD†is trained on the union of VOC07 and VOC12 trainval sets.\nDespite its simplicity, we see that LOST outperforms the other methods by large margins. We also\ncompare against an adapted version of the segmentation method proposed in [13]. Concretely, we extract\nthe self-attention of the CLS query at the last layer of the transformer, create a binary mask where the\n0.6 N largest entries of this self-attention are set to 1, retrieve the largest spatially-connected component\nfrom this binary mask, and use the bounding box of this component as the detected object. This method\nreturns one box per self-attention head and we report results obtained with the best performing head over\nthe entire dataset, noted as DINO-seg. LOST improves over DINO-seg by 8 to 17 of CorLoc points,\ndemonstrating the efﬁcacy of our approach for object localization based on self-supervised pre-trained\ntransformer features.\nFinally, we also evaluate our unsupervised class-agnostic detector (denoted by ‘+ CAD’) for single-\nobject discovery. To this end, we return for each image the box that the detector assigns the highest score.\nIt can be seen that training a class-agnostic detector on LOST’s outputs further improves the performance\nby 4 to 7 CorLoc points. In total, our method surpasses the prior state of the art by at least 10 CorLoc\npoints on each evaluated dataset.\nImpact of the backbone architecture.Table 2 studies the effect of the backbone on LOST. We see\nthat transformer representations are better suited for our method (best results with ViT-S/16). In contrast,\nour performance using the DINO-pre-trained ResNet-50 is signiﬁcantly lower. It indicates that the\nperformance of our method is not only due to the contributions of self-supervision but also to the property\nand quality of the speciﬁc features we extract.\n4.3 Unsupervised object detection\nHere we explore the application of LOST in unsupervised object detection. To that end, we use LOST’s\npseudo-boxes to train a Faster R-CNN model [51] on the datasets. We measure detection performance\nusing the Average Precision at IoU 0.5metric (AP@0.5), which is commonly used in the PASCAL\ndetection benchmark. As Faster R-CNN backbone, we use a ResNet50 pre-trained with DINO self-\nsupervision, thus making our training pipeline fully-unsupervised. We trained the Faster R-CNN models\nusing the detectron2 [73] implementation (more details in the supplementary material).\n8\nVOC07 VOC12 COCO20k\nTraining set (when applicable) trainval trainval trainval\nEvaluation set trainval test trainval trainval\nEdgeBoxes [84] 3.6 4.4 4.8 1.8\nSelective Search [65] 2.9 3.6 4.2 1.6\nrOSD [68] + CAD 24.2 25.2 29.0 8.4\nLOD [69] + CAD 22.7 23.7 28.4 8.8\nLOST + CAD 29.0 29.0 33.5 9.9\nTable 4: Class-agnostic unsupervised object detection results(in AP@0.5 %). Trainings, corresponding to\n‘method + CAD’, are performed on the bare images and rely only on the fully-unsupervised methods rOSD [68],\nLOD [69] and LOST (ours). Evaluation of unsupervised object detection may thus be performed on the same images\nas those used for unsupervised training (without manual annotations). The classic methods EdgeBoxes [ 84] and\nSelective Search [65] do not involve any training.\nPseudo-labels. To generate pseudo-labels for the class-aware detectors, we apply K-means clustering\non DINO-ViT-S tokens using as many clusters as the number of different classes in the dataset. Since the\ncluster-based pseudo-labels are “anonymous”, to evaluate the detection results we must map the clusters\nto the ground-truth classes. Following prior work in image clustering [ 5, 6, 36], we use Hungarian\nmatching [39] for that. We stress that this matching is only for reporting evaluation results; we do not\nuse any human labels during training.\nUnsupervised class-aware detection. Table 3 provides results of unsupervised class-aware object\ndetectors trained with LOST (entry ‘LOST + OD’). We are not aware of any prior work that addresses\nunsupervised object detection on real-world images of complex scenes, as those in PASCAL, that does\nnot use extra modalities. We could not compare to [2, 61] as we focus on image-only benchmarks.\nWe see that, although fully-unsupervised, our method learns to accurately detect several object\nclasses. For example, detection performance for classes “aeroplane”, “bus”, “dog”, “horse” and “train” is\nmore than 50.0%, and for “cat” it reaches 72.2%. Even more so, for some classes our method achieves\nbetter AP than the weakly-supervised methods WSDDN [9] and PCL [60], which require image-wise\nhuman labels. Although the results are not entirely comparable due to backbone differences between our\nmethod and the weakly-supervised ones (self-supervised ResNet50 vs. supervised VGG16), they still\ndemonstrate the efﬁcacy of our method in unsupervised object detection, which is an extremely hard and\nill-posed task.\nWe also evaluate the AP of our pseudo-boxes (with their assigned cluster id as pseudo-labels) when\ngenerated for VOC07 test (entry ‘LOST pseudo-boxes’). Evidently, training the detector on pseudo-boxes\nleads to a signiﬁcantly higher AP than the initial pseudo-boxes.\nFinally, switching our pseudo-boxes with those of rOSD [68] for the detector training (adding pseudo-\nlabels to rOSD pseudo-boxes by clustering DINO features in exactly the same way as in our method)\nleads to performance degradation (entry ‘rOSD + OD’).\nUnsupervised class-agnostic detection.In Table 4, we report class-agnostic detection results obtained\nusing pseudo-boxes from our method (‘LOST + CAD’) as well as from rOSD [68] (‘rOSD + CAD’) and\nLOD [69] (‘LOD + CAD’). As we see, our method leads to a signiﬁcantly better detection performance.\nWe also report detection results using the Selective Search [65] and EdgeBox[84] proposal algorithms,\nwhich perform worse than our method.\n4.4 Limitations and future work\nDespite the good performance of LOST, it exhibits some limitations.\n9\nLOST, as it stands, can separate same-class instances that do not overlap (as it only keeps the\nconnected component of the initial seed to create a box), but it is not designed to separate instances\nwhen overlapping. This is actually a challenging problem, related to the difference between supervised\nsemantic [44] and instance [33] segmentation methods, which, as far as we know, is an open problem\nin the absence of any supervision. A potential lead could be to use a matching algorithm such as\nProbabilistic Hough Matching to separate instances within image regions found in multiple images.\nAnother issue is when an object covers most of the image. It violates our second assumption for\nthe initial seed selection (expressed in subsection 3.2) that an individual object covers less area than\nthe background, thus possibly causing the seed to fall in the background instead of a foreground object.\nIdeally, we would like to ﬁlter out such failure cases, e.g., by using the attention maps of the CLS token.\nWe leave this as future work.\n5 Conclusion\nWe have presented LOST, a simple, yet effective method for localizing objects in images without any\nlabels, by leveraging self-supervised pre-trained transformer features [13]. Despite its simplicity, LOST\noutperforms state-of-the-art methods in object discovery by large margins. Having high precision, the\nboxes found by LOST can be used as pseudo ground truth for training a class-agnostic detector which\nfurther improves the object discovery performance. LOST boxes can also be used to train an unsupervised\nobject detector that yields competitive results compared to weakly-supervised counterparts for several\nclasses.\nFuture work will be dedicated to investigate other applications of LOST boxes, e.g., high-quality\nregion proposals for object detection tasks, and the power of self-supervised transformer features for\nunsupervised object segmentation.\n6 Acknowledgments and Disclosure of Funding\nThis work was supported in part by the Inria/NYU collaboration, the Louis Vuitton/ENS chair on artiﬁcial\nintelligence and the French government under management of Agence Nationale de la Recherche as part\nof the “Investissements d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). Huy\nV . V o was supported in part by a Valeo/Prairie CIFRE PhD Fellowship.\n10\nReferences\n[1] Triantafyllos Afouras, Yuki M Asano, Francois Fagan, Andrea Vedaldi, and Florian Metze. Self-\nsupervised object detection from audio-visual correspondence. In arXiv, 2021.\n[2] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised\nlearning of audio-visual objects from video. In ECCV, 2020.\n[3] Hamed H. Aghdam, Abel Gonzalez-Garcia, Joost van de Weijer, and Antonio M. L´opez. Active\nlearning for deep detection neural networks. In ICCV, 2019.\n[4] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. Measuring the objectness of image windows.\nTPAMI, 34, 2012.\n[5] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous\nclustering and representation learning. In arXiv, 2019.\n[6] Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Sutter, and Bj ¨orn Ommer. Cliquecnn: Deep\nunsupervised exemplar learning. In arXiv, 2016.\n[7] A Bergamo, L Bazzani, D Anguelov, and L Torresani. Self-taught object localization with deep\nnetworks. In WACV, 2016.\n[8] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and\nAndreas Veit. Understanding robustness of transformers for image classiﬁcation. In arXiv, 2021.\n[9] Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In CVPR, 2016.\n[10] Christopher Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick,\nand Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. In arXiv,\n2019.\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[12] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n[13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In arXiv, 2021.\n[14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020.\n[15] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce. Unsupervised object discovery and\nlocalization in the wild: Part-based matching with bottom-up region proposals. In CVPR, 2015.\n[16] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In ICLR, 2020.\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale\nHierarchical Image Database. In CVPR, 2009.\n11\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\nIn ICLR, 2021.\n[19] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herve Jegou. Training vision transformers\nfor image retrieval. In arXiv, 2021.\n[20] Martin Engelcke, Adam Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative\nscene inference and sampling with object-centric latent representations. In ICLR, 2020.\n[21] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman.\nThe PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.\n[22] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.\nThe PASCAL visual object classes challenge 2007 (VOC2007) results, 2007.\n[23] Jiyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, and Ram Nevatia. Note-rcnn: Noise tolerant\nensemble rcnn for semi-supervised object detection. In ICCV, 2019.\n[24] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P´erez, and Matthieu Cord. Learning\nrepresentations by predicting bags of visual words. In CVPR, 2020.\n[25] Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick P´erez.\nLearning representations by predicting bags of visual words. In CVPR, 2021.\n[26] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\npredicting image rotations. In ICLR, 2018.\n[27] Ross Girshick. Fast R-CNN. In ICCV, 2015.\n[28] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for\naccurate object detection and semantic segmentation. In CVPR, 2014.\n[29] Kristen Grauman and Trevor Darrell. Unsupervised learning of categories from sets of partially\nmatching image features. In CVPR, 2006.\n[30] Klaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel\nZoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation\nlearning with iterative variational inference. In ICML, 2019.\n[31] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS,\n2020.\n[32] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\n[33] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017.\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016.\n[35] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018.\n12\n[36] Xu Ji, Jo˜ao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised\nimage classiﬁcation and segmentation. In ICCV, 2019.\n[37] Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, and Wei Liu. Deep self-taught learning for\nweakly supervised object localization. In CVPR, 2017.\n[38] Gunhee Kim and Antonio Torralba. Unsupervised detection of regions of interest using iterative\nlink analysis. In NeurIPS, 2009.\n[39] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\nquarterly, 2:83–97, 1955.\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, 2014.\n[41] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu,\nZsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. In ICLR, 2021.\n[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In arXiv, 2021.\n[43] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention.\nIn NeurIPS, 2020.\n[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 3431–3440, 2015.\n[45] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby,\nDustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In arXiv,\n2021.\n[46] Tom Monnier, Elliot Vincent, Jean Ponce, and Mathieu Aubry. Unsupervised layered image\ndecomposition into object prototypes. In arXiv, 2021.\n[47] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan,\nand Ming-Hsuan Yang. Intriguing properties of vision transformers. In arXiv, 2021.\n[48] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving\njigsaw puzzles. In ECCV, 2016.\n[49] Ilija Radosavovic, Piotr Doll´ar, Ross Girshick, Georgia Gkioxari, and Kaiming He. Data distillation:\nTowards omni-supervised learning. In CVPR, 2017.\n[50] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. In NeurIPS, 2019.\n[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In NeurIPS, 2015.\n[52] Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee, Alexander G. Schwing,\nand Jan Kautz. Instance-aware, context-focused, and memory-efﬁcient weakly supervised object\ndetection. In CVPR, 2020.\n13\n[53] Bryan Russell, William Freeman, Alexei Efros, Josef Sivic, and Andrew Zisserman. Using multiple\nsegmentations to discover objects and their extent in image collections. In CVPR, 2006.\n[54] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-\ntion. In ICCV, 2017.\n[55] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. In ICLR, 2015.\n[56] Josef Sivic, Bryan Russell, Alexei Efros, Andrew Zisserman, and William Freeman. Discovering\nobjects and their location in images. In ICCV, 2005.\n[57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for\nsemantic segmentation. In arXiv, 2021.\n[58] Jiayu Tang and Paul H Lewis. Non-negative matrix factorisation for object class discovery and\nimage auto-annotation. In CIVR, 2008.\n[59] Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, and Caiming Xiong. Proposal learning for\nsemi-supervised object detection. In WACV, 2021.\n[60] Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, and Alan Yuille. Pcl:\nProposal cluster learning for weakly supervised object detection. TPAMI, 42, 2018.\n[61] Hao Tian, Yuntao Chen, Jifeng Dai, Zhaoxiang Zhang, and Xizhou Zhu. Unsupervised object\ndetection with lidar clues. In CVPR, 2021.\n[62] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J´egou. Training data-efﬁcient image transformers & distillation through attention. In arXiv,\n2020.\n[63] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv´e J´egou. Going\ndeeper with image transformers. In arXiv, 2021.\n[64] Shikhar Tuli, Ishita Dasgupta, Erin Grant, and Thomas L Grifﬁths. Are convolutional neural\nnetworks or transformers more like human vision? In CogSci, 2021.\n[65] Jasper Uijlings, Karin van de Sande, Theo Gevers, and Arnold Smeulders. Selective search for\nobject recognition. IJCV, 2013.\n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[67] Huy V . V o, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick P ´erez, and Jean Ponce.\nUnsupervised image matching and object discovery as optimization. In CVPR, 2019.\n[68] Huy V . V o, Patrick P´erez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-\nscale image collections. In ECCV, 2020.\n[69] Huy V . V o, Elena Sizikova, Cordelia Schmid, Patrick P´erez, and Jean Ponce. Large-scale unsuper-\nvised object discovery. In arXiv, 2021.\n[70] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\n14\n[71] Markus Weber, Max Welling, and Pietro Perona. Towards automatic discovery of object categories.\nIn CVPR, 2000.\n[72] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua Shen, and Zhi-Hua Zhou. Unsupervised\nobject discovery and co-localization by deep descriptor transforming. PR, 2019.\n[73] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.\nhttps://github.com/facebookresearch/detectron2, 2019.\n[74] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-\nformer: Simple and efﬁcient design for semantic segmentation with transformers. In arXiv, 2021.\n[75] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-\nsupervised learning with swin transformers. In arXiv, 2021.\n[76] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on\nimagenet. In arXiv, 2021.\n[77] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\nIn arXiv, 2021.\n[78] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Mining and-or graphs for graph matching\nand object discovery. In ICCV, 2015.\n[79] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n[80] Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian Zhang, Qingji Guan, Qi Zou, and Haibin\nLing. Object discovery from a single unlabeled image by mining frequent itemsets with multi-scale\nfeatures. TIP, 29, 2020.\n[81] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors\nemerge in deep scene cnns. In ICLR, 2015.\n[82] Jun-Yan Zhu, Jiajun Wu, Yan Xu, Eric Chang, and Zhuowen Tu. Unsupervised object class\ndiscovery via saliency-guided multiple class learning. In CVPR, 2012.\n[83] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}:\nDeformable transformers for end-to-end object detection. In ICLR, 2021.\n[84] Lawrence Zitnick and Piotr Doll´ar. Edge boxes: Locating object proposals from edges. In ECCV,\n2014.\n15\nSeed selection Expansion & Box extrac. k CorLoc\napq with fp,q = qp,q in Eq. (1) fp = qp in Sand mq 100 30.8\napq with fp,q = vp,q in Eq. (1) fp = vp in Sand mq 100 50.5\napq with fp,q = kp,q in Eq. (1) fp = kp in Sand mq 100 61.9\n˜apq deﬁned in 4 fp = qp in Sand mq 100 30.8\n˜apq deﬁned in 4 fp = vp in Sand mq 100 29.9\n˜apq deﬁned in 4 fp = kp in Sand mq 100 30.7\n˜apq deﬁned in 4 using ˜Sand ˜mq 100 30.8\napq with fp,q = kp,q in Eq. (1) fp = kp in Sand mq 1 38.3\napq with fp,q = kp,q in Eq. (1) fp = kp in Sand mq 50 58.8\napq with fp,q = kp,q in Eq. (1) fp = kp in Sand mq 150 61.8\napq with fp,q = kp,q in Eq. (1) fp = kp in Sand mq 200 61.2\nTable 5: Ablation study.CorLoc performance on VOC2007 for different choices of transformer features in the\nseed selection, expansion and box extraction steps, as well as inﬂuence on the results of the parameter k (maximum\nnumber of patches with the lowest degree, in Dk, for seed expansion).\nA Ablation Study\nA.1 Which transformer features to choose?\nAs explained in subsection 3.2 of the main paper, we chose to use the keys kp of the last attention layer\nas patch features fp in LOST. As we will see here, this choice provides the best localization performance\namong other alternatives. Speciﬁcally, in the ﬁrst section of Table 5, we report the performance of LOST\nwhen using as patch features fp either the keys kp, the queries qp, or the values vp of the attention layer.\nWe see that, when using the queries qp or the values vp, LOST’s performance deteriorates by at least 11\nCorLoc points compared to using the keys kp.\nAnother way to measure the similarity between two patches in a transformer architecture is to use the\nscalar product between the queries and the keys. We thus test substituting\n˜apq =\n{ 1 if q\n⊤\np kq + k\n⊤\np qq ≥0,\n0 otherwise, (4)\nfor apq in Eq. (1) in the main paper, when selecting the ﬁrst, initial seed. Note that this choice of ˜apq\nensures the symmetry of the adjacency matrix. We test this new choice of similarity matrix when using\nthe queries, keys or values in the seed expansion step, i.e., in S, and in the box extraction steps, i.e., in m\nas deﬁned in Eq. (3) in the main paper.\nFinally, we also test another alternative by changing the deﬁnition ofSto ˜S= {q|q∈Dk and q\n⊤\nq kp∗+\nk\n⊤\nq qp∗ ≥0}and changing the deﬁnition of mq in Eq. (3) to\n˜mq =\n{\n1 if ∑\ns∈S\n(\nk\n⊤\nq qs + q\n⊤\nq ks\n)\n≥0,\n0 otherwise.\n(5)\nResults in Table 5 show that all these alternatives using queries and keys yield results that are not as good\nas when using the keys as patch features.\nA.2 Importance of the seed expansion step\nWe analyse here the importance of the seed expansion step that is controlled by k. The seed expansion\nstep allows us to enlarge the region of interest so as to include all the parts of an object and not only the\n16\nMethod VOC07 trainval VOC12 trainval COCO20k\nDINO-seg [head 0] 25.9 24.6 30.1\nDINO-seg [head 1] 36.2 35.9 35.8\nDINO-seg [head 2] 32.1 33.2 31.6\nDINO-seg [head 3] 21.6 20.0 26.3\nDINO-seg [head 4] 45.8 46.2 42.1\nDINO-seg [head 5] 35.5 42.1 26.5\nDINO-seg BCC 38.8 45.2 28.8\nDINO-seg HAIoU 46.1 47.6 40.8\nLOST (ours) 61.9 64.0 50.7\nTable 6: DINO-seg ablation study.We compare here CorLoc results on datasets VOC07 trainval, VOC12 trainval\nand COCO20k when applying the DINO-seg method to create a box from the different heads of the attention layer.\nAlso, DINO-seg BCC selects the box/head that produces the biggest connected component, and DINO-seg HAIoU\nselects the box/head that has the highest average IoU with the other 5 boxes. We additionally report results with our\nmethod LOST for comparison.\npart localized from the ﬁrst, initial seed.\nTable 5 presents the impact of the parameter k, which corresponds to the maximum number of\npatches that can be used to construct the mask m. We notice that, without seed expansion (i.e., k= 1),\nthere is a drastic drop in the localization performance. The performance then improves when increasing\nkto 100-150 with a slight decrease at 200.\nVisualizations of results with k= 1and k= 100are presented in the Figure 3 of the main paper and\nFigure 4 here. We see that the boxes in yellow obtained with k= 1are small and localized on probably\nwhat is the most discriminative part of the objects. Increasing kpermits us to increase the size of the box\nand localize the object better. We also present in Figure 5 cases of failures where the seed expansion step\nis either insufﬁcient to localize the whole object or yields a box containing multiple objects.\nA.3 Analysis of DINO-seg\nIn this section, we investigate alternative setups of the baseline DINO-seg which is based on the work of\nCaron et al. [13]. They are presented in Table 6.\nFirst, instead of using the best attention head over the entire dataset (as we did in the main paper), we\nevaluate the localization accuracy of DINO-seg for each one of the 6 available heads. We ﬁnd out that\none head in particular, namely head 4, captures objects well, whilst results with other heads are much\nlower. Due to its superior performance, in the main paper we report DINO-seg using head 4.\nWe also explore dynamically selecting one box per image among boxes corresponding to the different\nheads using some heuristics. We report the two variants that gave the best results. In the ﬁrst variant, we\nconsider selecting the box corresponding to the head with the biggest connected component (‘DINO-seg\nBCC’). However, it yields worse results than with head 4. We also try selecting, over the 6 boxes of the\ndifferent heads, the box that has the highest average IoU overlap with the remaining 5 boxes (‘DINO-seg\nHAIoU’). It improves over DINO-seg [head 4] by 1 point on both VOC07 and VOC12. However, as\nshown in Table 6, it still performs signiﬁcantly worse than LOST in this single-object discovery task.\nA.4 Impact of the number of clusters on class-aware detection training\nFor the unsupervised class-aware detection experiments of the main paper, we assume that we know\nthe exact number of object classes present in the used dataset, i.e., 20 in the VOC dataset, and use the\n17\nNumber K of clusters 20 25 30 40\nMean AP (%) 29.9 29.4 34.0 32.2\nTable 7: Impact of number of clusters in object detection.Results, using the mean AP@0.5 (%) across all the\nclasses, on VOC07 test. All models are trained using LOST’s pseudo-boxes (i.e., LOST + OD) on the VOC07 and\nVOC12 trainval sets. The number of classes in VOC is 20.\nMethod VOC07 noh VOC12 noh\nOSD [67] 40.7 -\nDDT+ [72] 43.4 46.3\nrOSD [68] 49.3 51.2\nLOD [69] 48.0 50.5\nLOST 54.9 57.5\nTable 8: CorLoc results on the VOC07 noh and VOC12 noh datasets.\nsame number of K-means clusters. Here we only assume that we have a rough estimate of the number of\nclasses and study the impact of the requested number of clusters on the performances of the unsupervised\ndetector.\nTo that end, in Table 7, we provide the mean AP across all the20 VOC classes when using 20, 25,\n30 and 40 clusters. For the case when we use more clusters than the 20 classes of the VOC dataset,\nHungarian matching, which is used for reporting the AP results, will map to the VOC classes only the 20\nmost ﬁtted available clusters. Thus, when reporting the per-class AP results, we ignore the detections in\nthese unmatched clusters (since they have not been mapped to any ground-truth class).\nIn Table 7, we observe that our unsupervised detector achieves good results for all the numbers of\nclusters. Interestingly, for 30 and 40 clusters there is a noticeable performance improvement. Similar\nﬁndings have been observed on prior clustering work [36, 61, 1].\nA.5 Impact of the non-determinism of the K-means clustering\nWe investigate the impact of the randomness in the K-means clustering on the results of the object\ndetector. To that end, we repeat 4 times, using different random seeds, the unsupervised class-aware\nobject detection experiment with LOST + OD†(using the model trained on the union of VOC07 and\nVOC12 trainval sets, cf. Table 3 in subsection 4.3 of the main paper). We obtain a standard deviation\nof 0.8 for the AP@0.5 %, which shows that the method is fairly insensitive to the randomness of the\nclustering method.\nB More quantitative results and comparisons\nB.1 Results on more datasets used in previous work\nFor completeness, we present in Table 8 results on the datasets used in previous object discovery works\n[67, 72, 68, 69]. In particular, we evaluate our method on the datasets VOC07 noh and VOC12 noh\ndatasets (also named VOC all in literature). They are subsets of the trainval set of the well-known\nPASCAL VOC 2007 and PASCAL VOC 2012 datasets containing 3550 and 7838 images respectively.\nThese subsets exclude all images containing only objects annotated as “hard” or “truncated” and all\nboxes annotated as “hard” or “truncated”.\n18\nMethod odAP50 odAP@[50-95]\nVOC07trainval VOC12trainval COCO20ktrainval VOC07trainval VOC12trainval COCO20ktrainval\nKimet al. [38] 9.5 11.8 3.93 2.49 3.11 0.96DDT+ [72] 8.7 11.1 2.41 3.0 4.1 0.73rOSD [68] 13.1 15.4 5.18 4.29 5.27 1.62LOD [69] 13.9 16.1 6.63 4.47 5.34 1.98LOD [69] + CAD 15.8 20.9 7.26 5.03 7.07 2.28LOST + CAD19.8 24.9 7.93 6.71 8.85 2.51\nTable 9: Multi-object discovery performance in odAP (Average Precision for object discovery) of our method and\nthe baselines [38, 72, 68, 69].\nB.2 Multi-object discovery results\nWe compare in Table 9 the object discovery performance of different methods in the setting where\nmultiple regions are returned per image. This setting has been explored in [68] and [69].\nFollowing [69], instead of considering the object recall (detection rate) for a given number of\npredicted regions per image, as in [68], we consider as a metric a form of Average Precision adapted to\nthe task, that we name here “odAP”. It is the average of the AP of predicted objects for each number of\npredicted regions, from one to the maximum number of ground-truth objects in an image in the dataset.\nThis odAP metric thus does not depend on the number of detections per image and remains related to AP,\nwhich is a standard metric for object detection. [69] actually uses two variants of this metric: odAP50,\nwhere a prediction is correct if its intersection-over-union (IoU) with one of the ground-truth boxes is at\nleast 0.5, and odAP@[50-95], the average odAP value at 10 equally-spaced values of the IoU threshold\nbetween 0.5 and 0.95.\nAs LOST only returns one region per image, we only consider here LOST + CAD, which is the\noutput of a class-agnostic detector (CAD) trained with LOST boxes, and we compare it to other existing\napproaches. It can be seen that LOST + CAD outperforms signiﬁcantly all the previous methods,\nincluding the class-agnostic detector trained with LOD [69] boxes (LOD [69] + CAD).\nB.3 Image nearest neighbor retrieval\nFollowing LOD [69], we use LOST box descriptors to ﬁnd images that are similar to each other (image\nneighbors) in the image collection.\nTo this end, each image is represented by the CLS descriptors of its LOST box and the cosine\nsimilarity between these descriptors is used to deﬁne a similarity between the images. Then, for each\nimage, the top τ images with the highest similarity are chosen as its neighbors. Similar to LOD [69], we\nchoose τ = 10and use CorRet [15] as the evaluation metric, deﬁned as the average percentage of the\nretrieved image neighbors that are actual neighbors (i.e., that contain objects of the same category) in the\nground-truth image graph over all images.\nWe compare the performance of our method in this task with rOSD [68] and LOD [69] in Table 10.\nWe see that LOST boxes, when represented by DINO [ 13] features, yield the better CorRet score\ncompared to [68, 69]. When VGG16 [55] features are used, LOST is behind LOD [69] but better than\nrOSD [68].\nB.4 Using DINO features\nWe are aware that, in Table 1 of the main paper, we compare our method using a transformer backbone to\nmethods based on a VGG16 pre-trained on ImageNet models. For a fair comparison, we investigate here\nthe state-of-the-art LOD [69] method when adapted to use the transformers features.\n19\nMethod Features CorRet (%)\nrOSD [68] VGG16 [55] 64\nLOD [69] VGG16 [55] 70\nLOST (ours) VGG16 [55] 68\nLOST (ours) DINO [13] 72\nTable 10: Image neighbor retrieval\nperformance (CorRet) of different\nmethods.\nMethod Features CorLoc (%)\nVOC07trainval VOC12trainval COCO20ktrainval\nLOD [69] VGG16 [55] 53.6 55.1 48.5\nLOD [69] DINO [13] 43.2 45.9 33.7\nLOST (ours) VGG16 [55] 42.0 47.2 30.2\nLOST (ours) DINO [13] 61.9 64.0 50.7\nTable 11: Single-object discovery performance in CorLoc of LOD [ 69]\nand LOST with different types of features.\nLOD [69] uses the algorithm from rOSD [ 68] to generate region proposals from CNN features\nfor their pipeline, but we observe that this algorithm does not yield good proposals with transformer\nfeatures. We therefore run LOD with edgeboxes [ 84] and use DINO [ 13] features, extracted with\nROIPool [27], to represent these proposals. We present the results on VOC07 trainval, VOC12 trainval\nand COCO20k trainval dataset in Table 11.\nOur results in Table 2 of the main paper show that a direct adaption of LOST, designed by analysing\nthe properties of transformers features, to CNN features yields worse performance. Conversely, as we\nsee in Table 11 here, adapting algorithms developed using properties of CNN features to transformer\nfeatures is also not direct. Nevertheless, the number of design choices to adapt these algorithms to new\ntypes of features is vast and we do not exclude that some design choices might improve the results even\nfurther, e.g., by exploiting together CNN and transformer features.\nB.5 Using supervised pre-training.\nWe test LOST but this time using a transformer pre-trained under full supervision on ImageNet. We use\nthe model provided by DeiT [62].\nWith this model, LOST achieves a CorLoc of 16.9% which is signiﬁcantly worse than the results\nobtained with the DINO self-supervised pre-trained model. We remark that a similar observation was\nmade for DINO [13], where the segmentation performance obtained with the model trained under full\nsupervision yields signiﬁcantly worse results than when using DINO’s model. It is unclear, however, if\nthis difference of performance can be attributed to the properties of the self-supervision loss or to the\nmore aggressive data augmentation used during DINO pre-training.\nC More visualizations (single- and multi-object discovery)\nWe present in Figures 4-9 additional qualitative results of our method.\nFigure 4 and Figure 5 are discussed in the subsection A.2.\nFigure 6 and Figure 7 show successful examples of LOST + CAD in VOC07trainval and COCO20k trainval\ndatasets. It can be seen that it is able to localize multiple objects in the same image.\nFigure 8 and Figure 9 present results obtained with LOST + OD on the VOC07 and COCO datasets\nrespectively. They show the localization predictions with their predicted pseudo-classes. Each pseudo-\nclass is assigned a different color. In Figure 9, the “person” objects are assigned three different pseudo-\nclasses; those failures show the difﬁculty to assign the same class to “person” in very different positions.\nD Training details of the Faster R-CNN detection models\nIn the main paper, we explore the application of LOST in unsupervised object detection by using its\npseudo-boxes as ground truth for training Faster R-CNN detection models.\n20\nFor the implementation of the Faster R-CNN detector, we use the R50-C4 model of Detectron2 [73]\nthat relies on a ResNet-50 [34] backbone. In our experiments, this ResNet-50 backbone is pre-trained\nwith DINO self-supervision. Then, to train the Faster R-CNN model on the considered dataset, we use\nthe protocol and most hyper-parameters from He et al. [32].\nIn details, we train with mini-batches of size 16 across 8 GPUs using SyncBatchNorm to ﬁnetune\nBatchNorm parameters, as well as adding an extra BatchNorm layer for the RoI head after conv5, i.e.,\nRes5ROIHeadsExtraNorm layer in Detectron2. During training, the learning rate is ﬁrst warmed-up\nfor 100 steps to 0.02 and then reduced by a factor of 10 after 18K and 22K training steps. We use\nin total 24K training steps for all the experiments, except when training class-agnostic detectors on\nthe pseudo-boxes of the VOC07 trainval set, in which case we use 10K steps. For all experiments,\nduring training, we freeze the ﬁrst two convolutional blocks of ResNet-50, i.e., conv1 and conv2 in\nDetectron2.\n21\nFigure 4: Object localization on VOC07.The red square represents the seed p∗, the yellow bos is the\nbox obtained using only the seed p∗, and the purple box is the box obtained using all the seeds Swith\nk= 100.\n22\nFigure 5: Cases of localization failure on VOC07.The red square represents the seed p∗, the yellow\nbox is the box obtained using only the seed p∗, and the purple box is the box obtained using all the seeds\nSwith k= 100.\nFigure 6: Multi-object discovery on VOC07 (LOST + CAD).Predictions performed by the class-\nagnostic detector on VOC07.\n23\nFigure 7: Multi-object discovery on COCO (LOST + CAD).Predictions performed by the class-\nagnostic detector on COCO.\nFigure 8: Multi-object discovery on VOC07 (LOST + OD).Predictions performed by the class-aware\ndetector on VOC07 (a different color per class).\n24\nFigure 9: Multi-object discovery on COCO (LOST + OD).Predictions performed by the class-aware\ndetector on COCO (a different color per class). The actual “person” class is assigned three different\npseudo-classes, illustrating the difﬁculty to see a single category for a “person” in very different positions.\n25",
  "topic": "Pascal (unit)",
  "concepts": [
    {
      "name": "Pascal (unit)",
      "score": 0.8576080799102783
    },
    {
      "name": "Computer science",
      "score": 0.7837139368057251
    },
    {
      "name": "Artificial intelligence",
      "score": 0.658200204372406
    },
    {
      "name": "Annotation",
      "score": 0.6208308339118958
    },
    {
      "name": "Transformer",
      "score": 0.6086710691452026
    },
    {
      "name": "Code (set theory)",
      "score": 0.4820290803909302
    },
    {
      "name": "Computer vision",
      "score": 0.4721377491950989
    },
    {
      "name": "Detector",
      "score": 0.46428361535072327
    },
    {
      "name": "Source code",
      "score": 0.4263388514518738
    },
    {
      "name": "Object detection",
      "score": 0.41492199897766113
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3755543828010559
    },
    {
      "name": "Programming language",
      "score": 0.07847476005554199
    },
    {
      "name": "Engineering",
      "score": 0.05508407950401306
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}