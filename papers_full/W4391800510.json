{
  "title": "NavFormer: A Transformer Architecture for Robot Target-Driven Navigation in Unknown and Dynamic Environments",
  "url": "https://openalex.org/W4391800510",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2348061483",
      "name": "Wang Haitong",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A4299752815",
      "name": "Tan, Aaron Hao",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2745548365",
      "name": "Nejat, Goldie",
      "affiliations": [
        "University of Toronto"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202938653",
    "https://openalex.org/W4366668996",
    "https://openalex.org/W1507418669",
    "https://openalex.org/W3120006980",
    "https://openalex.org/W2002440441",
    "https://openalex.org/W2950989657",
    "https://openalex.org/W2962887844",
    "https://openalex.org/W2883574738",
    "https://openalex.org/W3029795912",
    "https://openalex.org/W6754725917",
    "https://openalex.org/W3140673083",
    "https://openalex.org/W3095029720",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6692846177",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6748638692",
    "https://openalex.org/W2142943472",
    "https://openalex.org/W178419168",
    "https://openalex.org/W2963821308",
    "https://openalex.org/W2971384749",
    "https://openalex.org/W4220817121",
    "https://openalex.org/W2964319688",
    "https://openalex.org/W2892258706",
    "https://openalex.org/W6741002519",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W6838931476",
    "https://openalex.org/W4285216306",
    "https://openalex.org/W2117211893",
    "https://openalex.org/W2338007649",
    "https://openalex.org/W6796289742",
    "https://openalex.org/W6731334075",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W4214717370",
    "https://openalex.org/W2107667896",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6753516098",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6851416138"
  ],
  "abstract": "In unknown cluttered and dynamic environments such as disaster scenes, mobile\\nrobots need to perform target-driven navigation in order to find people or\\nobjects of interest, while being solely guided by images of the targets. In\\nthis paper, we introduce NavFormer, a novel end-to-end transformer architecture\\ndeveloped for robot target-driven navigation in unknown and dynamic\\nenvironments. NavFormer leverages the strengths of both 1) transformers for\\nsequential data processing and 2) self-supervised learning (SSL) for visual\\nrepresentation to reason about spatial layouts and to perform\\ncollision-avoidance in dynamic settings. The architecture uniquely combines\\ndual-visual encoders consisting of a static encoder for extracting invariant\\nenvironment features for spatial reasoning, and a general encoder for dynamic\\nobstacle avoidance. The primary robot navigation task is decomposed into two\\nsub-tasks for training: single robot exploration and multi-robot collision\\navoidance. We perform cross-task training to enable the transfer of learned\\nskills to the complex primary navigation task without the need for\\ntask-specific fine-tuning. Simulated experiments demonstrate that NavFormer can\\neffectively navigate a mobile robot in diverse unknown environments,\\noutperforming existing state-of-the-art methods in terms of success rate and\\nsuccess weighted by (normalized inverse) path length. Furthermore, a\\ncomprehensive ablation study is performed to evaluate the impact of the main\\ndesign choices of the structure and training of NavFormer, further validating\\ntheir effectiveness in the overall system.\\n",
  "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2024 1  NavFormer: A Transformer Architecture for Robot Target-Driven Navigation in Unknown and Dynamic Environments   Haitong Wang, Aaron Hao Tan, Student Member, IEEE and Goldie Nejat, Member, IEEE  Abstractâ€”In unknown cluttered and dynamic environments such as disaster scenes, mobile robots need to perform target-driven navigation in order to find people or objects of interest, where the only information provided about these targets are images of the individual targets. In this paper, we introduce NavFormer, a novel end-to-end transformer architecture developed for robot target-driven navigation in unknown and dynamic environments. NavFormer leverages the strengths of both 1) transformers for sequential data processing and 2) self-supervised learning (SSL) for visual representation to reason about spatial layouts and to perform collision-avoidance in dynamic settings. The architecture uniquely combines dual-visual encoders consisting of a static encoder for extracting invariant environment features for spatial reasoning, and a general encoder for dynamic obstacle avoidance. The primary robot navigation task is decomposed into two sub-tasks for training: single robot exploration and multi-robot collision avoidance. We perform cross-task training to enable the transfer of learned skills to the complex primary navigation task. Simulated experiments demonstrate that NavFormer can effectively navigate a mobile robot in diverse unknown environments, outperforming existing state-of-the-art methods. A comprehensive ablation study is performed to evaluate the impact of the main design choices of NavFormer. Furthermore, real-world experiments validate the generalizability of NavFormer.  Index Termsâ€”Dynamic and unknown environments, image-guided search, target-driven robot navigation. I. INTRODUCTION obile robots can be used to search for potential victims in unknown environments including in urban disaster environments [1], [2], in buildings engulfed by fire [3], and/or unstructured outdoor environments [4]. Images of potential victims can be provided to the robots for them to search a disaster environment for these specific individuals, while avoiding collisions with rescue workers, victims, and other robots. In this paper, we address the problem of robot target-driven navigation (TDN) in unknown and dynamic environments. This problem requires a mobile robot to navigate an unknown and dynamic environment using only an onboard \nRGB camera to search for a static target given its image.  TDN in unknown and dynamic environments is a challenging problem as: 1) there are no global maps of the environment available, therefore, a robot needs to reason about the spatial layout of the environment based on its own partial observations to prevent deadlocks and redundant coverage [5], and 2) the presence of dynamic obstacles needs to be detected for collision avoidance [6] and for spatial reasoning of the environment [7]. In this paper, we assume that for each robot, the dynamic obstacles are other moving robots.  To-date, existing robot TDN methods for unknown environments have mainly used: 1) deep reinforcement learning (DRL) [5], [8]-[12] or 2) imitation learning (IL) [13]. Images of targets have consisted of either indoor scenes (e.g., kitchen, bedroom) [8] or household objects (e.g., chair, microwave) [10]. These methods take RGB images as observations and the target image as input into a convolutional neural network (CNN) to extract features (e.g., geometry, patterns) and encode them into a latent vector. Namely, for DRL methods, navigation actions are generated using either fully connected layers (FCL) [8], long short term memory (LSTM) [10], or attention-based memory retrieval [5] approaches. For IL methods [13], the robot action is generated by predicting the next expected observation (NEO) using an inverse dynamics model. These aforementioned TDN methods have mainly been applied to static environments. However, in dynamic environments, they may result in degraded performance due to the presence of moving obstacles that are treated as static. This may lead to misinterpretation of the spatial layout of the environment [7], which in turn can result in ineffective navigation decisions.  In this paper, we propose NavFormer, a novel end-to-end DL architecture consisting of a dual-visual encoder module and a transformer-based navigation network to address for the first time the problem of TDN in unknown and dynamic environments. NavFormer utilizes a decoder-only transformer [14] to make navigation decisions conditioned on both the target image and robot trajectory history. To obtain high-quality datasets for navigation policy learning, we decompose the task of TDN in dynamic environments into two well-studied sub-tasks in the literature: single-robot exploration, and multi-robot collision avoidance. Our main contributions in this paper include: 1) the development of the first end-to-end DL approach for robot target-driven navigation in unknown dynamic environments; 2) the incorporation of a dual-visual encoder system to extract static and general (i.e., static and dynamic) features for reasoning the spatial layouts of environments and collision avoidance, which is trained using self-supervised learning (SSL); and 3) the development of a cross-task training \nM \nManuscript received February 9, 2024; Revised May 8, 2024; Accepted June 3, 2024. This paper was recommended for publication by Editor Pascal Vasseur upon evaluation of the Associate Editor and Reviewersâ€™ comments. This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), and in part by the Canada Research Chairs program (CRC). (Corresponding author: Haitong Wang.) The authors are with the Autonomous Systems and Biomechatronics Laboratory (ASBLab), Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON M5S 3G8, Canada (e-mail: haitong.wang@mail.utoronto.ca;aaronhao.tan@utoronto.ca; nejat@mie.utoronto.ca).  Digital Object Identifier (DOI): see top of this page. \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\n2  IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2024 \nstrategy to train NavFormer on the two subtasks of single robot exploration and multi-robot collision avoidance.  II. RELATED WORKS Herein, we discuss the pertinent literature on: 1) robot target-driven navigation in static unknown environments, 2) robot navigation in dynamic environments, and 3) cross-task training. A. Robot Target-Driven Navigation in Static Environments Previous work has mainly used: 1) DRL [5], [8]-[12] or 2) IL [13] to address TDN tasks. These methods used Siamese CNN such as ResNet [15] to generate a joint embedding from robot image observations and the target image, and then they used this joint embedding to generate navigation actions. In [8], the Asynchronous Advantage Actor Critic (A3C) [16] was used to train scene-specific FCLs to generate navigation actions using joint embedding. Each environment required training on a set of scene-specific layers. In [9], object recognition was used to generate bounding boxes of the target object via two FCLs, and then used by an action prediction module to generate robot navigation actions. In [10], the Importance Weighted Actor-Learner Architecture (IMPALA) [18] was used to train a navigation network with LSTM [17] to account for historical observations for robot navigation in 3D mazes. In [5], a memory buffer and an attention module were used to further improve a robotâ€™s temporal reasoning via long-term memory for indoor navigation.  In [11], a DRL architecture combined with semantic scene priors was presented. It was constructed Using a knowledge graph to represent semantic relationships between objects in the scene.  An actor-critic network was used to generate actions. In [12], hierarchical policy learning with Intrinsic-Extrinsic Modeling (HIEM) was developed by using a high-level policy that generated sub-goals to guide target search and a low-level policy to generate navigation actions.  In [13], a generative IL module was used to predict the NEO, which was then used by an action prediction module to predict the robotâ€™s action. B. Robot Navigation in Dynamic Environments Robot navigation methods in dynamic environments can be categorized as: 1) classical methods [6], [19], [20], or 2) learning-based methods [21]-[25]. Learning-based methods include: 1) DRL methods [21]-[23], and 2) hybrid methods [24], [25] using both DRL and IL.  1) Classical Methods: In [6], the Velocity Obstacle (VO) method was used for robot collision avoidance in 2D dynamic environments by generating a potential collision area based on the velocities, positions and sizes of a robot and its nearby moving obstacle (another robot), and then selecting a robot velocity that avoided this area. Reciprocal VO (RVO) [19] address issues in movement oscillation of VO and the Non-Holonomic Optimal Reciprocal Collision Avoidance (NH-ORCA) approach [20] extend VO methods for robots with non-holonomic constraints. 2) Learning-based Methods: In [21], a multi-robot collision avoidance architecture was developed by using a CNN to directly map laser scans to robot velocities. In [22], a Gated Recurrent Unit (GRU) [27] was incorporated in [21] to account for historical observations to improve temporal reasoning in \nunknown environments. In [23], RL-RVO used a set of sequential VO and RVO vectors representing the states of nearby obstacles and a Bidirectional GRU network to generate navigation velocity.  In [24], a Hybrid CPU/GPU A3C for Collision Avoidance with DRL (GA3C-CADRL) method used a LSTM to encode spatial information of nearby obstacles and was trained using IL and then DRL. In [25], the Pathfinding via Reinforcement and Imitation Multi-Agent Learning (PRIMAL) was developed for multi-robot navigation by using a CNN to encode the local 2D map and a LSTM to generate robot actions. During training, PRIMAL randomly switched between DRL and IL to learn a navigation policy that improved navigation performance. C. Cross-task Training Cross-task training considers training an agent on multiple tasks ranging from video games [18],[28] to robotic applications such as tracking [29] and navigation [10]. For example, in [18], an off-policy actor-critic architecture, IMPALA, was developed to train an RL agent to complete multiple tasks with decoupled acting and learning and off-policy correction. In [29], an End-to-End Visual Active Tracking (E-VAT) method divided the VAT task into two sequential sub-tasks: exploration and tracking. The sub-tasks were trained concurrently using an asymmetric actor-critic architecture and IMPALA. In [10], the TDN task in static environments was addressed. Training was split into localization and navigation phases. The localization network was trained by self-supervised learning for target object localization, and the navigation policy was trained via IMPALA for target navigation. In [28], a Multi-Game Decision Transformer was utilized to train a single transformer model to play multiple video games by learning from a diverse offline dataset with both expert and non-expert data. D. Summary of Limitations The aforementioned TDN methods [5], [8]-[13] address the problem of a single robot navigating to a target location in an unknown static environment. However, these methods do not consider dynamic obstacles (i.e., other robots) in their environments, leading to inaccuracies in spatial reasoning and degraded navigation performance [7]. Robot navigation in dynamic environments has been achieved using classical methods [6], [19], [20], or learning-based methods [21]-[25]. These methods consist of only local planning schemes without considering global spatial layouts, resulting in robots that become trapped in local minima (e.g., dead ends) [31]. Furthermore, they cannot find a target provided in an RGB image without a given location. To address these limitations, we have developed NavFormer, the first DL method for robot TDN in unknown and dynamic environments. Our method utilizes cross-task training on a decision transformer architecture [32] developed for the TDN task, encompassing both exploration and multi-robot collision avoidance tasks. III. TARGET-DRIVEN NAVIGATION PROBLEM IN UNKNOWN AND DYNAMIC ENVIRONMENTS  A. Problem Definition Robot target-driven navigation in unknown and dynamic environments describes the following problem: a mobile robot \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\nWANG et al.: NAVFORMER: ROBOT TARGET-DRIVEN NAVIGATION IN UNKNOWN AND DYNAMIC ENVIRONMENTS 3   \nğ“‡ needs to navigate to a target ğ¼\tutilizing only an RGB image of the target ğ‘” and visual observations ğ‘œâˆˆÎ©\t(i.e., RGB images) of the environment obtained from an onboard camera. The dynamic obstacles (i.e., other robots) in the environment are represented by a set ğ‘€. There are no a priori global maps of the environment available and the 2D location of the static target object ğ‘™! is unknown. The target object is defined by a 3D geometric shape ğ‘âˆˆğµ and color ğ‘âˆˆğ¶. The objective of the robot ğ“‡ is to minimize the expected travel distance ğ‘‘ between the robotâ€™s start location ğ‘™\"\tand target location ğ‘™!:  minE[ğ‘‘(ğ‘™\",ğ‘™!)]. (1) B. GC-POMDP for Robot Target-Driven Navigation We model the robot TDN problem as a goal-conditioned partially observable Markov decision process (GC-POMDP). GC-POMDP is described as a tuple (ğ’®,â„Š,ğ’œ,ğ’«,â„›,Î©,ğ’ª), where ğ’® denotes the state space, and â„Š is the set of target RGB images. Robot actions, ğ‘âˆˆğ’œ, are represented by a 2D vector of linear and angular velocity. ğ’«\tis the state transition function ğ’«(ğ‘ ,ğ‘,ğ‘ â€²)=ğ‘(ğ‘ â€²|ğ‘ ,ğ‘). â„› is the reward function, ğ‘Ÿ=â„›(ğ‘ ,ğ‘).\t Î© is the observation space and ğ’ª is the observation probability function ğ’ª(ğ‘ #,ğ‘,ğ‘œ)=ğ‘(ğ‘œ|ğ‘ #,ğ‘). At each time step, the robot observes the environment, takes an action, then transitions to the next state and receives a reward.  The objective is to learn a policy ğœ‹$(ğ‘|ğ‘”,ğœ) that is conditioned on the target image ğ‘” and robot historical trajectory ğœ to maximize the expected return:\tğ¸[âˆ‘ğ‘Ÿ%&%'(]. The robot historical trajectory ğœ consists of returns-to-go ğ‘…J, observations\tğ‘œ, and actions ğ‘:  Ï„%=Lğ‘…J(,ğ‘œ(,ğ‘(,ğ‘…J),ğ‘œ),ğ‘),â€¦,ğ‘…J%,ğ‘œ%N, (2) where the return-to-go, ğ‘…J%, is defined as the desired total sum of rewards to achieve from the current timestep\tğ‘¡ to the terminal timestep ğ‘‡ of the episode [32]:   ğ‘…J%=âˆ‘ğ‘Ÿ*&*'%. (3) Return-to-go is used to generate actions conditioned on desired returns rather than past rewards. The return-to-go at the first timestep, ğ‘…J(, is a user specified desired total rewards.  â…£. NAVFORMER ARCHITECTURE The proposed NavFormer TDN architecture, Fig. 1(a), consists of three subsystems: 1) Dataset collection: to obtain \ndatasets containing robot trajectories for policy learning, and RGB image pairs for representation learning; 2) Training: to train both the NavFormer model using offline reinforcement learning and the dual-visual encoders (DVE) using self-supervised learning, and 3) Inference: which uses the trained NavFormer model for the TDN task in unknown dynamic environments. In this section, we will discuss the development of the NavFormer structure, Fig. 1(b). NavFormer contains: 1) a Multi-modal Input Sequence containing the target image ğ‘” and robot trajectory ğœ, 2) a Dual-visual encoder module that separately extracts static and general (i.e., static and dynamic) features from visual observations, and 3) a transformer-based Navigation Network (NavN) that is conditioned on the multi-modal input embeddings to generate navigation actions. A. Multi-Modal Input Sequence The multi-modal input sequence ğ“ˆğ“‰ consists of the RGB image of the target ğ‘” and robot trajectory ğœ%: The sequence is converted to embeddings of the same dimension ğ‘‘,= 128. We use a linear layer to project ğ‘…J- and ğ‘- to embeddings ğ‘’./,-\tand ğ‘’1,-, respectively. These embeddings are combined with the embeddings output by the DVE and provided to the NavN. B. Dual-Visual Encoders Our architecture utilizes DVE for effective TDN: 1) the static encoder ğ‘“\" extracts spatial features for environmental layout reasoning, which is essential for exploring the unknown environment, and 2) the general encoder ğ‘“2 extracts obstacle-specific features from nearby static and dynamic obstacles, facilitating collision avoidance during navigation. Two separate encoders are used to explicitly extract features specific to the two sub-tasks. Task-specific feature extraction has been shown to enhance policy learning in robot navigation [33].  Each encoder consists of a CNN with three convolutional layers with a kernel size, stride, and output channel of (8, 4, 32), (4, 2, 64), (3, 2, 64) [34]. All images (i.e.,\tğ‘” and ğ‘œ) are in the dimension of (84, 84, 3). Given an input sequence ğ“ˆğ“‰, ğ‘” is used by ğ‘“2 to generate a target embedding ğ‘’2. Furthermore, each observation ğ‘œ- is used by ğ‘“\" and ğ‘“2 to generate a static \n (a) (b) Fig. 1. (a) NavFormer TDN architecture consisting of Dataset Collection, Training and Inference subsystems. (b) NavFormer structure consisting of multi-modal input sequence, dual-visual encoders, and navigation network. \n ğ“ˆğ“‰=Lğ‘”,ğ‘…J(,ğ‘œ(,ğ‘(,ğ‘…J),ğ‘œ),ğ‘),â€¦,ğ‘…J%,ğ‘œ%N . (4) \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\n4  IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2024 \nembedding ğ‘’3,-\" and a general embedding ğ‘’3,-41, respectively. ğ‘’3,-\" and ğ‘’3,-41 are then combined with the embedding of returns-to-go ğ‘’./,- and actions ğ‘’1,- from Section â…£.A. Thus, ğ“ˆğ“‰ is converted into a sequence of embeddings ğ¸ğ“ˆ,6 which is then used by the NavN to generate navigation actions:  ğ¸ğ“ˆ,6=Lğ‘’2,ğ‘’.Ë†,(,ğ‘’3,(\",ğ‘’3,(41,ğ‘’1,(,â€¦,ğ‘’.Ë†,%,ğ‘’3,%\",ğ‘’3,%41N. (5) 1) Training Loss: In order to train the two visual encoders, the Bootstrap Your Own Latent (BYOL) [35] self-supervised learning method is used. For each encoder, BYOL uses two neural networks, Fig. 2: 1) an online network parameterized by ğœ™ that consists of an encoder (ğ‘“\",â€ˆ9,\tğ‘“2,â€ˆ9), a projector (ğ‘”\",9, ğ‘”2,9), and a predictor (ğ‘\",9, ğ‘2,9); and 2) a target network parameterized by ğœ‰ that consists of an encoder (ğ‘“\",â€ˆ:,\tğ‘“2,â€ˆ:), and a projector (ğ‘”\",:, ğ‘”2,:). Herein, we represent these parameters in general as (ğ‘“9, ğ‘”9, ğ‘9, ğ‘“:, ğ‘”:) as the training of ğ‘“\" and ğ‘“2 follow the same procedure. The online network and the target network share the same architecture for the encoder and the projector. The projector and the predictor are represented by a multiple-layer perceptron (MLP). The weights of the target network are an exponential moving average of the online network weights.  Two distributions of image augmentations including random resizing, cropping, color jittering were applied to obtain two augmented views ğ‘£ and ğ‘£# from the input observation image ğ‘œ. The online network takes ğ‘£ as input, and outputs representation ğ‘¦9=ğ‘“9(ğ‘£), projection ğ‘§9=ğ‘”9Lğ‘¦9N, and a prediction ğ‘9Lğ‘§9N. The target network takes as input ğ‘£#, and outputs representation ğ‘¦:#=ğ‘“:(ğ‘£#) and projection ğ‘§:#=ğ‘”:Lğ‘¦:#N. The loss function is defined as the mean squared error (MSE) between the normalized online prediction and target projection:   â„’9,:=âˆ¥;!<=!>âˆ¥;!<=!>âˆ¥\"âˆ’=#$âˆ¥=#$âˆ¥\"âˆ¥)). (6) BYOL uses a symmetrical loss by swapping ğ‘£ and ğ‘£# as input to the online and target networks again to compute â„’]9,:: \t â„’9,:@ABC=â„’9,:+â„’]9,:.\t (7) The final BYOL loss is the summation of the loss for ğ‘“\",  â„’\",D,E@ABC and the loss for ğ‘“2, â„’2,D,E@ABC: \t â„’@ABC=â„’\",D,E@ABC+â„’2,D,E@ABC.\t (8) To ensure that the static encoder learns only the static features from the environment, we modified the standard BYOL procedure [35] by generating the two augmented views \n(ğ‘£, ğ‘£#) from two different images (ğ‘œ\",ğ‘œF) of the same view of the scene. ğ‘œF denotes an RGB image that contains dynamic obstacles (i.e., other robots) while ğ‘œ\" denotes an image that contains only static obstacles. Therefore, the static encoder learns only the common static obstacle features shared between the images (ğ‘œ\",ğ‘œF). For the general encoder, we follow the standard BYOL procedure [42] and generate two different augmented views from the same image ğ‘œF.  C. Navigation Network The NavN is used to generate the robotâ€™s action ğ‘%G( given the sequence of embeddings ğ¸ğ“ˆ,6. Our NavN uses a causal transformer (CT) model based on the Generative Pre-trained Transformer 2 (GPT-2) [14]. We use CT as its self-attention mechanism can effectively utilize sequential memory of past static embeddings ğ‘’3\", enabling the network to reason about the spatial layout of the environment based on the robot trajectory ğœ%. The CT can also implicitly infer the motion of dynamic obstacles by dynamically adjusting the weight of each general embedding eHIJ from consecutive image observations via self-attention layers.  The structure of our NavN consists of three stacked transformer blocks, and a linear layer. To understand the temporal and multi-modal nature of the input data, positional encodings are introduced as learnable embeddings for each timestep and modality, which are added to ğ¸ğ“ˆ,6.  Dropout is applied to the ğ¸ğ“ˆ to prevent overfitting [36]. ğ¸ğ“ˆ is then sent into ğ‘= 3 stacked transformer blocks, each consisting of a masked multi-head attention (MMHA) submodule and a MLP submodule. The context length of each transformer block is 1024. In the MMHA submodule, the embeddings ğ¸ğ“ˆ are linearly projected into â„= 2 heads of keys ğ¾, queries ğ‘„ and values ğ‘‰ in the dimension of ğ‘‘*=\t64 for self-attention. The MMHA submodule is connected to a MLP that has a hidden layer and residual connection. The last transformer block outputs a sequence of hidden states ğ»ğ“ˆ:  ğ»ğ“ˆ=Lâ„2,â„./,(,â„3,(\",â„3,(41,â€¦,â„./,%,â„3,%\",â„3,%41N. (9) We concatenate the target hidden state â„2 with the observation hidden state â„3,-41 and provide fâ„2;â„3,%41h into a linear layer to predict the next navigation action ğ‘i-. This hidden state concatenation helps the CT to implicitly detect the target; thereby, improving the navigation policy.  1) Training Loss: We utilize the Decision Transformer [32] offline RL method for training the NavN. The loss of the NavN is computed using the MSE between the predicted actions ğ‘i- and ground-truth actions ğ‘-:  â„’K&=(&âˆ‘âˆ¥ğ‘%âˆ’ğ‘i%âˆ¥)&%'( . (10) â…¤. DATASET COLLECTION We collected three datasets in 3D simulated environments from mobile robots to train NavFormer, Fig. 1(a): 1) Robot Exploration Dataset, ğ’Ÿâ„¯, 2) Collision Avoidance Dataset, ğ’Ÿğ’¸ğ’¶, and 3) Representation Learning Dataset, ğ’Ÿğ“ˆğ“ˆâ„“. The target objects include various geometric shapes (sphere, box) and colors (green, red, white, yellow, blue). Exploration Dataset ğ““ğ“®: This dataset comprises 3,496 robot trajectories and the corresponding target RGB images, totaling 342,954 timesteps for a single robot exploration task in 3D \nFig. 2. BYOL procedure for the static and general encoder.  \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\nWANG et al.: NAVFORMER: ROBOT TARGET-DRIVEN NAVIGATION IN UNKNOWN AND DYNAMIC ENVIRONMENTS 5   \nenvironment with only static obstacles (i.e., walls). Each robot trajectory was obtained in a different random environment with size ranging from 5m Ã— 5m to 17.5m Ã— 17.5m. The trajectory ğœ consists of returns-to-go ğ‘…J, observations ğ‘œ, and actions ğ‘. To generate this dataset, the Frontier Exploration method in [38] was used to select the nearest frontier location for the robot. The robot planned a global path using A* and followed this path using DWA [30] as the local planner. ğ’Ÿâ„¯ is used to train NavFormer to learn the skill of exploration. Collision Avoidance Dataset ğ““ğ“¬ğ“ª: This dataset comprises 7,467 robot trajectories collected using NH-ORCA [20] and the corresponding target images, totaling 469,378 timesteps for the task of multi-robot collision avoidance in dynamic environments. Each trajectory was obtained in a 12.5 m Ã— 12.5 m environment with the number of dynamic obstacles ranging from 2 to 8. ğ’Ÿğ’¸ğ’¶ was used to learn collision avoidance. Representation Learning Dataset ğ““ğ“¼ğ“¼ğ“µ: This dataset, consisting of 98,000 image pairs taken in 3D mazes with dynamic obstacles, was collected across 1,000 randomly generated environments, varying in size from 5 m Ã— 5 m to 17.5 m Ã— 17.5 m and containing 4 to 25 dynamic obstacles. Each image pair contained two RGB images Lğ‘œ-\",ğ‘œ-FN taken by the robotâ€™s onboard camera. To collect this dataset: 1) we randomly placed all dynamic obstacles in different locations, and recorded their observations (i.e., ğ‘œ-F) and locations; and 2) a single robot was placed at all previous locations and its own observations were captured (i.e., ğ‘œ-\"). We aligned each ğ‘œ-\" with its corresponding ğ‘œ-F based on the locations where ğ‘œ-\" and ğ‘œ-F were taken. ğ’Ÿğ“ˆğ“ˆâ„“ was used for the representation learning. â…¥. TRAINING We used a cross-task training approach for the policy learning of NavFormer. Namely, we decomposed the robot TDN task into the two sub-tasks of: 1) single robot exploration, and 2) multi-robot collision avoidance. NavFormer was trained on ğ’Ÿâ„¯ and ğ’Ÿğ’¸ğ’¶ to learn exploration and collision avoidance skills. To compute â„’K&(10), we alternated between ğ’Ÿâ„¯\tand\tğ’Ÿğ’¸ğ’¶ during each training iteration to sample trajectories. To train the DVE, we use ğ’Ÿğ“ˆğ“ˆâ„“ to sample image pairs to compute â„’@ABC, (8). The final loss is:  â„’=â„’@ABC+â„’UV. (11) Training was performed using an RTX3070 GPU, an AMD Ryzen Threadripper 3960X CPU and 128GB of memory. The training of the DVE utilized a batch size of 512. The Adam optimizer [39] used 0.0004 for the learning rate and weight decay. NavFormer was trained with a batch size of 25 and 150 for trajectory sampling from ğ’Ÿâ„¯\tand ğ’Ÿğ’¸ğ’¶, respectively. We used different batch sizes for trajectory sampling to balance the total \nnumber of frames in each batch as trajectories in ğ’Ÿâ„¯ were longer than trajectories in ğ’Ÿğ’¸ğ’¶. Dropout of 0.1 is applied to the CT. The AdamW optimizer [40] was used to train the NavFormer with a learning rate of 0.0002 and weight decay of 0.0001, over a span of 10,000 iterations. â…¦. SIMULATED EXPERIMENTS We conducted two sets of experiments to evaluate the overall performance of our NavFormer architecture: 1) a comparison study with our approach and state-of-the-art (SOTA) learning methods to compare TDN strategies on: i) unseen 3D maze environments, ii) unseen photorealistic 3D environments, and iii) unseen dynamic obstacles; and 2) an ablation study to investigate the contributions of the design choices of NavFormer. A. Comparison Study in Unseen Maze Environments We used three performance metrics for these experiments: 1) mean success rate (SR) of robots reaching target objects, 2) Success weighted by normalized inverse Path Length (SPL) which measures the efficiency of the navigation method [41]:  SPL=(W%âˆ‘ğ‘†- â„“ğ’¾XJY(ğ“…ğ’¾,â„“ğ’¾)W%-'( , (12) where ğ‘] is the number of robot trials, ğ‘†- is a binary indicator of success in trial ğ‘–. â„“ğ’¾ is the shortest path length from the start location of the robot to the target location, and ğ“…ğ’¾ is the actual robot path length, and 3) mean collision rate (CR), CR=(W%âˆ‘W',)W*,)W%-'(, whereğ‘4,- is the number of collisions in trial ğ‘–, ğ‘,,- denotes the total number of timesteps in trial ğ‘–. We randomly generated a total of 54 new 3D environments in Gazebo consisting of static obstacles, target objects, and dynamic obstacles (robots). For all the methods, the test environments were unseen during training. The sizes of these environments were 7.5 m Ã— 7.5 m, 12.5 m Ã— 12.5 m, and 17.5 m Ã— 17.5 m, Fig. 3. Each robot had a different target object to navigate to. The target objects were randomly generated with different geometric shapes and colors, and locations. We deployed 2, 4 and 6 Jackal robots in each environment.  1) Comparison Methods: We benchmarked our NavFormer method against the following comparison methods: Velocity Random Walk (VRW): The velocity random walk policy randomly samples robot actions from a uniform distribution. VRW is chosen as a lower bound approach. Deep Siamese Actor-Critic (DSAC) [8]: The Deep Siamese Actor-Critic model consists of Siamese CNN layers and scene-specific FCLs. We use the procedure in [13] to adapt DSAC to use a single set of scene-specific layers across all training environments in order to generalize to unseen environments. \nTABLE I  COMPARISON BETWEEN NAVFORMER AND SOTA METHODS FOR UNSEEN MAZE ENVIRONMENTS Setup VRW  DSAC MA-TDN DT FCA NavFormer (ours) Env size (m) # of robots SR SPL CR SR SPL CR SR SPL CR SR SPL CR SR SPL CR SR SPL CR 7.5 Ã— 7.5 2 39.17% 0.287 0.691 53.33% 0.377 0.316 61.67% 0.315 0.314 57.50% 0.419 0.405 94.17% 0.653 0.044 80.83% 0.562 0.228 4 33.33% 0.220 0.694 47.08% 0.353 0.395 46.67% 0.248 0.542 47.92% 0.395 0.529 80.83% 0.598 0.142 66.25% 0.465 0.325 6 32.78% 0.187 0.711 42.78% 0.337 0.493 43.06% 0.234 0.559 40.28% 0.332 0.608 76.11% 0.580 0.101 54.44% 0.435 0.430 12.5 Ã— 12.5 2 16.67% 0.100 0.678 24.17% 0.179 0.324 30.00% 0.159 0.387 39.17% 0.258 0.372 85.00% 0.599 0.070 53.33% 0.374 0.251 4 15.83% 0.104 0.680 19.17% 0.133 0.460 23.33% 0.137 0.394 26.25% 0.201 0.430 73.33% 0.482 0.096 48.89% 0.378 0.223 6 13.61% 0.101 0.716 23.89% 0.18 0.550 23.06% 0.118 0.488 31.94% 0.231 0.530 60.28% 0.454 0.071 48.75% 0.359 0.355 17.5 Ã— 17.5 2 5.83% 0.053 0.666 10.00% 0.084 0.371 20.83% 0.117 0.342 34.17% 0.222 0.352 57.50% 0.395 0.068 44.17% 0.316 0.279 4 5.00% 0.032 0.622 10.00% 0.085 0.472 13.75% 0.078 0.494 22.08% 0.151 0.449 47.50% 0.331 0.100 38.33% 0.279 0.204 6 5.83% 0.032 0.585 10.56% 0.092 0.538 16.39% 0.098 0.554 22.50% 0.169 0.546 40.83% 0.296 0.081 35.56% 0.272 0.350 \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\n6  IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2024 \nMemory-Augmented Target-Driven Navigation (MA-TDN) [5]: MA-TDN uses an attention module to retrieve the memory of past robot observations to generate navigation actions.  Decision Transformer (DT) [32]: The decision transformer uses only one visual encoder for feature extraction from visual observations. The navigation action is generated using the hidden state of the observation embedding.  Frontier + NH-ORCA (FCA): FCA uses frontier exploration as a high-level planner to explore the environment and uses NH-ORCA as local controller to avoid collisions with other obstacles. This method is chosen as an upper bound approach as it uses extra information unavailable to NavFormer and other SOTA learning methods, including point clouds and map, other robotsâ€™ states (i.e., velocities, positions, sizes), and specific target locations. It should be noted that as FCA utilizes the above additional information, it does not directly address the TDN problem in dynamic environments using only visual observations as defined in this paper. For both DSAC and MA-TDN, we convert their discrete action space to continuous space by outputting a mean and variance to define a Gaussian distribution for action sampling. Training of DSAC and MA-TDN were achieved with PPO [26] using the same 3D maze training environments and number of robots as NavFormer. DT was trained on ğ’Ÿâ„¯ and ğ’Ÿğ’¸ğ’¶ using Eq. (10). For DT and NavFormer, ğ‘…J( is initialized to be 1. The robot only receives a positive reward of 1 when it successfully navigates to the target object. 2) Experimental Procedure: At the beginning of each trial, robots were randomly located in the environment and assigned an RGB image of a target ğ‘”. A robot found a target when the distance between the target and robot was within 1.5m. A trial terminated when either all robots found their corresponding targets or the total timesteps exceeded 500. Each timestep is 0.2s. A new environment was randomly generated every 10 trials. 60 trials were conducted for each experiment setup. 3) Results: The SR, SPL, and CR results for NavFormer and the comparison methods across the different environment sizes and number of robots are presented in Table I. In general, NavFormer consistently outperformed VRW, DSAC, MA-TDN and DT. As the environment size increased, SR and SPL decreased and CR increased for all methods due to the: 1) increased level of difficulty introduced by more static obstacles in the larger environments, and 2) longer travel distance between start and target location with the allocated time. NavFormer had higher SR and SPL and lower CR than VRW, \nDSAC, MA-TDN, DT across all the three environments and number of robots. NavFormer was able to achieve accurate spatial layout representation from invariant features extracted by the static encoder, minimizing the need for redundant coverage of an environment. Target hidden state concatenation enabled NavFormer to achieve improved implicit target detection during navigation by directly conditioning on the target hidden states. This aids the robot to recognize and reach the target object when it is within robotâ€™s camera view, thereby facilitating task completion. FCA had higher performance than NavFormer across all environments as it used additional information such as the map of the environment to select frontiers to explore, ground-truth state information of other robots to avoid collisions.  B. Comparison Study in Unseen Photorealistic Environments We conducted experiments in three 3D photorealistic environments which included a bookstore (14.3 m Ã— 15.6 m), a two-room cafe (9.3 m Ã— 23.1 m), and a single-floor multi-room house (19.0 m Ã— 11.0 m), as shown in Fig. 4. These environments represent unseen out-of-distribution environments with different configurations and appearances from our 3D maze environments used during training. The same target objects were used as in Section â…¦.A. Three Jackal robots were deployed in each environment. Both robot start and target object locations were randomly chosen. We performed 30 trials in each environment type for all methods. 1) Results: The results are presented in Table II. NavFormer achieved lower performance in unseen out-of-distribution photorealistic environments compared to its performance in unseen 3D mazes (Table I) as expected. This is similar to other visual tracking methods such as E-VAT [29]. However, NavFormer still outperformed VRW, DSAC, MA-TDN and DT  in all three metrics. This is due to the data augmentation techniques utilized by NavFormer during the training of DVE, which allow for visual feature extraction to be robust to environmental changes. In turn, NavFormer has better generalization capabilities than these methods. As FCA uses additional sensory information about the environment and target, it is less sensitive to environmental variability. C. Comparison Study with Unseen Dynamic Obstacles We conducted simulated experiments with unseen dynamic obstacles to compare the robustness of the learned policy between NavFormer and other SOTA methods. During the dataset collection and training phases, only the Jackal robots were used as dynamic obstacles. To consider unseen dynamic obstacles during testing, we included mobile TurtleBot3 robots. The experiments were conducted in 3D mazes (12.5 m Ã— 12.5 m), with a total of four robots deployed per trial: one Jackal robot, and three dynamic obstacles. Namely, we explored two \n    (a) (b) (c) (d) Fig. 3. Robot environments: (a) 7.5 m Ã— 7.5 m with 2 robots, (b) 12.5 m Ã— 12.5 m with 4 robots, (c) 17.5 m Ã— 17.5 m with 6 robots, and (d) a mobile robot and a target object.  \nTABLE II  COMPARISON BETWEEN NAVFORMER AND SOTA METHODS FOR UNSEEN PHOTOREALISTIC ENVIRONMENTS  VRW DSAC MA-TDN DT FCA NavFormer SR 8.89% 11.11% 17.78% 22.22% 82.22% 36.67% SPL 0.062 0.081 0.122 0.148 0.612 0.269 CR 0.612 0.620 0.590 0.510 0.092 0.412 \n   (a)  (b) (c) Fig. 4. Photorealistic environments with three jackal robots and eight target objects: (a) a bookstore, (b) a two-room cafe, and (c) a single floor house with multiple rooms. \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\nWANG et al.: NAVFORMER: ROBOT TARGET-DRIVEN NAVIGATION IN UNKNOWN AND DYNAMIC ENVIRONMENTS 7   \nscenarios, where: 1) all dynamic obstacles are Jackal robots, and 2) all dynamic obstacles are TurtleBot3 robots. Sixty trials were conducted for each scenario, with a new maze randomly generated every 10 trials. As in Section VII.A, we used the same target objects, however, both robot starting and target object locations were randomly chosen. 1) Results: The results are shown in Table III. As expected, NavFormerâ€™s performance decreased slightly in the presence of unseen dynamic obstacles. However, NavFormer still outperformed the other SOTA methods, showcasing the robustness of its learned policy against other TDN SOTA methods for unseen dynamic obstacles. This is due to the CT network in NavFormer using the self-attention layers to dynamically adjust the weights of input robot historical trajectory that contain consecutive frames of visual observations. Thus, enabling the network to implicitly identify and adjust to the movement patterns of dynamic obstacles even when they have not been seen during training. FCA explicitly utilized ground-truth state information of other robots, therefore it achieved better performance than NavFormer. D. Ablation Study We conducted an ablation study to investigate the impact of the design choices on the training methods and architecture design of NavFormer. Namely, we considered:   (1) NavFormer without SSL: It was trained using only the loss of â„’UV, Eq. (10), (2) NavFormer without Training on ğ‰ğ’†: When computing â„’UV, Eq. (10), robot exploration trajectories ğœ, from ğ’Ÿâ„¯ were not included, (3) NavFormer without Training on ğ›•ğ’„ğ’‚: When computing â„’UV, Eq. (10), robot collision avoidance trajectories Ï„41 from ğ’Ÿğ’¸ğ’¶ were not included, (4) NavFormer without Target Hidden States Concatenation (THSC): The feed-forward layer of NavN includes input only from the hidden state of the observation â„3,%41 to generate ğ‘i%, (5) NavFormer without Static Encoder ğ’‡ğ’”: We removed the static encoder during training, (6) NavFormer without General Encoder ğ’‡ğ’ˆ: We removed the dynamic encoder during training, (7) NavFormer with ResNet18: We replaced the DVE with a single pre-trained ResNet18 [15]. \nWe conducted 60 trials in 6 maze environments of 12.5 m Ã— 12.5 m with 4 Jackal robots for each variant. A new environment is randomly generated every 10 trials. 1) Results: The results of the ablation study are presented in Table IV. NavFormer achieved the highest SR and SPL and the lowest CR among all variants. We found that NavFormer w/o ğ‘“\" achieved overall the second highest performance when compared to NavFormer, showing the significant role of the general encoder in extracting obstacle-specific features that are important for navigation. NavFormer with ResNet18 achieved the lowest performance among all variants due to the domain gap between the test environments (i.e., simulated 3D mazes) and ImageNet (i.e., web-scale object-centric images) [42]. The pretrained visual representations do not generalize to domain-specific robot navigation task, leading to the decrease in performance [43]. We also found that NavFormer w/o Ï„41 achieved better performance than NavFormer w/o Ï„,. We postulate that this is due to the skill of exploration being more important for the completion of the TDN task. NavFormer inherently acquires the skill of collision avoidance with static obstacles during the learning of exploration skills, diminishing the incremental benefit of learning from Ï„41. VIII. REAL-WORLD EXPERIMENTS We conducted real-world experiments in a 9.0 m by 10.0 m office environment, Fig. 5. Two Jackal robots were deployed with each having a ZED 2 camera for obtaining image observations. Eight target objects with varying geometric shapes (i.e., box, ball) and colors (i.e., blue, green, orange, yellow) were used. The same three datasets as described in Section V were collected: with 1) 50 single robot trajectories for exploration, 2) 100 two-robot trajectories for collision avoidance, and 3) 500 image pairs containing one image with dynamic obstacles and one image with only static obstacles. We finetuned NavFormer on these real-world datasets using the same set of hyperparameters as in Section VI and trained for 2,000 iterations. For testing, 15 trials were conducted with randomly placed target objects and robot starting locations.  NavFormer achieved an SR of 63.33%, SPL of 0.406, and CR of 0.229, respectively. This is comparable to the results in Table I for the small-mid-size environments. A video of our NavFormer method addressing the TDN problem in both the simulated and real-world environments is provided on our YouTube channel at https://youtu.be/PSVsLM1eGXo. IX. CONCLUSION In this paper, we present the development of a novel end-to-end DL model, NavFormer, to address the challenging problem of robot target-driven navigation in unknown and dynamic environments. Our approach uniquely combines a Dual-Visual \nTABLE III  COMPARISON STUDY WITH SEEN AND UNSEEN DYNAMIC OBSTACLES  Metrics VRW DSAC MA-TDN DT FCA NavFormer Seen Dynamic Obstacles SR 15.83% 19.17% 23.33% 26.25% 73.33% 48.89% SPL 0.104 0.133 0.137 0.201 0.482 0.378 CR 0.680 0.460 0.394 0.430 0.096 0.223 Unseen Dynamic Obstacles SR 13.33% 16.67% 21.67% 21.67% 75.00% 41.67% SPL 0.101 0.131 0.157 0.190 0.442 0.322 CR 0.591 0.606 0.552 0.501 0.086 0.301 \n Fig. 5.  Top view of the real-world office environment with two mobile robots and eight target objects with zoomed in views of the different regions. \nTABLE IV   ABLATION STUDY Methods SR SPL CR NavFormer 48.89% 0.378 0.223 NavFormer w/o SSL 39.58% 0.272 0.376 NavFormer w/o THSC 32.08% 0.261 0.500 NavFormer w/o Ï„! 12.08% 0.109 0.280 NavFormer w/o Ï„\"# 40.83% 0.256 0.237 NavFormer w/o ğ‘“$ 41.25% 0.289 0.230 NavFormer w/o ğ‘“% 38.33% 0.273 0.396 NavFormer w ResNet18 7.92% 0.080 0.765 \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.\n8  IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2024 \nEncoder system with a transformer-based Navigation Network. The former is trained via self-supervised learning, while the latter uses offline reinforcement learning. Extensive simulated experiments were conducted with varying environment sizes and different number of dynamic obstacles. The results show that NavFormer outperformed state-of-the-art learning-based methods in successfully navigating to targets. Both photorealistic simulated experiments and real-world experiments validated the generalizability of NavFormer to effectively solve the TDN problem in dynamic unknown complex environments. An ablation study further validated our design choices of NavFormer. Future work will expand our architecture by incorporating object recognition backbones for common objects and different dynamic obstacles in everyday environments. REFERENCES [1] A. H. Tan, F. P. Bejarano, Y. Zhu, R. Ren, and G. Nejat, â€œDeep Reinforcement Learning for Decentralized Multi-Robot Exploration With Macro Actions,â€ IEEE Robot. Autom. Lett., vol. 8, no. 1, pp. 272â€“279, Jan. 2023. [2] A. Fung, B. Benhabib, and G. Nejat, â€œRobots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations,â€ IEEE Robot. Autom. Lett., vol. 8, no. 6, pp. 3550â€“3557, Jun. 2023. [3] R. R. Murphy, â€œActivities of the rescue robots at the World Trade Center from 11-21 september 2001,â€ IEEE Robot. Autom. Mag., vol. 11, no. 3, pp. 50â€“61, Sep. 2004. [4] A. H. Tan, S. Narasimhan, and G. Nejat, â€œ4CNet: A Confidence-Aware, Contrastive, Conditional, Consistency Model for Robot Map Prediction in Multi-Robot Environments,â€ arXiv:2402.17904, 2024. [5] L. Mezghan et al., â€œMemory-Augmented Reinforcement Learning for Image-Goal Navigation,â€ in Proc. IEEE Int. Conf. Intell Robots Syst, 2022, pp. 3316â€“3323. [6] P. Fiorini and Z. Shiller, â€œMotion planning in dynamic environments using velocity obstacles,â€ Int. J. Robot. Res., vol. 17, no. 7, pp. 760â€“772, Jul. 1998. [7] C. Yu et al., â€œDS-SLAM: A Semantic Visual SLAM towards Dynamic Environments,â€ in Proc. IEEE Int. Conf. Intell. Robot Syst., 2018, pp. 1168â€“1174. [8] Y. Zhu et al., â€œTarget-driven visual navigation in indoor scenes using deep reinforcement learning,â€ in Proc. IEEE Int. Conf. Robot. Aumomat., 2017, pp. 3357â€“3364. [9] X. Ye, Z. Lin, H. Li, S. Zheng, and Y. Yang, â€œActive Object Perceiver: Recognition-guided Policy Learning for Object Searching on Mobile Robots,â€ in Proc. IEEE Int. Conf. Intell. Robots Syst., 2018, pp. 6857-6863. [10] A. Devo, G. Mezzetti, G. Costante, M. L. Fravolini, and P. Valigi, â€œTowards Generalization in Target-Driven Visual Navigation by Using Deep Reinforcement Learning,â€ IEEE Trans. Robot., vol. 36, no. 5, pp. 1546â€“1561, Oct. 2020. [11] W. Yang, X. Wang, A. Farhadi, A. Gupta, and R. Mottaghi, â€œVisual Semantic Navigation using Scene Priors,â€ arXiv:1810.06543, 2018. [12] X. Ye and Y. Yang, â€œEfficient Robotic Object Search Via HIEM: Hierarchical Policy Learning With Intrinsic-Extrinsic Modeling,â€ IEEE Robot. Autom. Lett., vol. 6, no. 3, pp. 4425â€“4432, Jul. 2021. [13] Q. Wu, X. Gong, K. Xu, D. Manocha, J. Dong, and J. Wang, â€œTowards Target-Driven Visual Navigation in Indoor Scenes via Generative Imitation Learning,â€ IEEE Robot. Autom. Lett., vol. 6, no. 1, pp. 175â€“182, Jan. 2021. [14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, â€œLanguage Models are Unsupervised Multitask Learners,â€ OpenAI blog, 2019. [15] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep Residual Learning for Image Recognition,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770â€“778. [16] V. Mnih et al., â€œAsynchronous Methods for Deep Reinforcement Learning,â€ in Proc. Int. Conf. Mach. Learn., 2016, vol. 48, pp. 1928â€“1937. \n[17] S. Hochreiter and J. Schmidhuber, â€œLong Short-Term Memory,â€ Neural Comput., vol. 9, no. 8, pp. 1735â€“1780, Nov. 1997. [18] L. Espeholt et al., â€œIMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,â€ in Proc. Int. Conf. Mach. Learn., Jul. 2018, pp. 1407â€“1416. [19] J. D. Van Berg, M. Lin, and D. Manocha, â€œReciprocal velocity obstacles for real-time multi-agent navigation,â€ in Proc. IEEE Int. Conf. Robot. Automat., 2008, pp. 1928â€“1935. [20] J. Alonso-Mora, A. Breitenmoser, M. Rufli, P. Beardsley, and R. Siegwart, â€œOptimal reciprocal collision avoidance for multiple non-holonomic robots,â€ in Distrib. Auton. Robot. Syst., Springer, 2013, pp. 203â€“216. [21] P. Long, T. Fanl, X. Liao, W. Liu, H. Zhang, and J. Pan, â€œTowards optimally decentralized multi-robot collision avoidance via deep reinforcement learning,â€ in Proc. IEEE Int. Conf. Robot. and Automat., 2018, pp. 6252â€“6259. [22] J. Zeng, R. Ju, L. Qin, Y. Hu, Q. Yin, and C. Hu, â€œNavigation in Unknown Dynamic Environments Based on Deep Reinforcement Learning,â€ Sensors, vol. 19, no. 18, Art. no. 18, Jan. 2019. [23] R. Han et al., â€œReinforcement Learned Distributed Multi-Robot Navigation With Reciprocal Velocity Obstacle Shaped Rewards,â€ IEEE Robot. Autom. Lett., vol. 7, no. 3, pp. 5896â€“5903, Jul. 2022. [24] M. Everett, Y. F. Chen, and J. P. How, â€œMotion Planning among Dynamic, Decision-Making Agents with Deep Reinforcement Learning,â€ in Proc. IEEE Int. Conf. Intell. Robots Syst., Dec. 2018, pp. 3052â€“3059. [25] G. Sartoretti et al., â€œPRIMAL: Pathfinding via Reinforcement and Imitation Multi-Agent Learning,â€ IEEE Robot. Autom. Lett., vol. 4, no. 3, pp. 2378â€“2385, 2019. [26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ pp. 1â€“12, 2017. [27] K. Cho et al., â€œLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,â€ 2014, arXiv:1406.1078. [28] K.-H. Lee et al., â€œMulti-Game Decision Transformers,â€ Adv. Neural Inf. Process. Syst., pp. 27921-27936, 2022. [29] A. Dionigi, A. Devo, L. Guiducci, and G. Costante, â€œE-VAT: An Asymmetric End-to-End Approach to Visual Active Exploration and Tracking,â€ IEEE Robot. Autom. Lett., vol. 7, no. 2, pp. 4259â€“4266, Apr. 2022. [30] D. Fox, W. Burgard, and S. Thrun, â€œThe dynamic window approach to collision avoidance,â€ IEEE Robot. Autom. Mag., vol. 4, no. 1, pp. 23â€“33, 1997. [31] W. Khaksar, S. Vivekananthen, K. S. M. Saharia, M. Yousefi, and F. B. Ismail, â€œA review on mobile robots motion path planning in unknown environments,â€ Proc. IEEE Int. Symp. Robot. Intell. Sens., pp. 295â€“300, Apr. 2016. [32] L. Chen et al., â€œDecision Transformer: Reinforcement Learning via Sequence Modeling,â€ 2021, arXiv:2106.01345. [33] P. Mirowski et al., â€œLearning to navigate in complex environments,â€ Proc. Int. Conf. Learn. Representations, 2017. [34] V. Mnih et al., â€œHuman-level control through deep reinforcement learning,â€ Nature, vol. 518, no. 7540, pp. 529â€“533, 2015. [35] J.-B. Grill et al., â€œBootstrap Your Own Latent - A New Approach to Self-Supervised Learning,â€ in Adv. Neural Inf. Process. Syst, pp. 21271â€“21284, 2020. [36] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, â€œDropout: A Simple Way to Prevent Neural Networks from Overï¬tting,â€ J. Mach. Learn. Res., pp. 1929â€“1958, 2014. [37] R. S. Sutton and A. G. Barto, Reinforcement learning: an introduction, Second edition. in Adaptive Comput. Mach. Learn. Ser., Cambridge, Massachusetts: The MIT Press, 2018. [38] B. Yamauchi, â€œFrontier-based approach for autonomous exploration,â€ in Proc. IEEE Int. Symp. Compt. Intell. Robot. Automat., 1997. [39] D. P. Kingma and J. Lei, â€œAdam: A Method for Stochastic Optimization,â€ 2015, arXiv:1412.6980. [40] I. Loshchilov and F. Hutter, â€œDecoupled Weight Decay Regularization,â€ 2019, arXiv:1711.05101. [41] P. Anderson et al., â€œOn Evaluation of Embodied Navigation Agents,â€ 2018, arXiv:1807.06757. [42] J. Deng, W. Dong, R. Socher, L.-J. Li, Kai Li, and Li Fei-Fei, â€œImageNet: A large-scale hierarchical image database,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 248â€“255. [43] A. Majumdar et al., â€œWhere are we in the search for an Artificial Visual Cortex for Embodied Intelligence?â€ arXiv:2303.18240, 2024.  \nThis article has been accepted for publication in IEEE Robotics and Automation Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LRA.2024.3412638\nÂ© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\r\nSee https://www.ieee.org/publications/rights/index.html for more information.",
  "topic": "Architecture",
  "concepts": [
    {
      "name": "Architecture",
      "score": 0.7223126292228699
    },
    {
      "name": "Transformer",
      "score": 0.6116844415664673
    },
    {
      "name": "Robot",
      "score": 0.5682262778282166
    },
    {
      "name": "Computer science",
      "score": 0.5612244606018066
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36867713928222656
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.325401246547699
    },
    {
      "name": "Engineering",
      "score": 0.28147369623184204
    },
    {
      "name": "Geography",
      "score": 0.172378271818161
    },
    {
      "name": "Electrical engineering",
      "score": 0.1333460807800293
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}