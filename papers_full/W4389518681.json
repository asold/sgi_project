{
  "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation",
  "url": "https://openalex.org/W4389518681",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100388124",
      "name": "Yuanyuan Liang",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2103809590",
      "name": "Jianing Wang",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2943066805",
      "name": "Hanlun Zhu",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A120759433",
      "name": "Lei Wang",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2233383541",
      "name": "Qian Weining",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2955016064",
      "name": "Yunshi Lan",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287891464",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W3156366114",
    "https://openalex.org/W3163840908",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W4205479723",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4287815536",
    "https://openalex.org/W4296960510",
    "https://openalex.org/W2964588180",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W3163842339",
    "https://openalex.org/W4283790835",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W4297578259",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4226174273",
    "https://openalex.org/W4386566681",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2887938737",
    "https://openalex.org/W2945978556",
    "https://openalex.org/W2964236999",
    "https://openalex.org/W3176175717",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W4375959300",
    "https://openalex.org/W4287888135",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W3025611493",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385573367",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4385570025",
    "https://openalex.org/W4293567942",
    "https://openalex.org/W4321276803",
    "https://openalex.org/W2980345007",
    "https://openalex.org/W1977556410",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W3197277089",
    "https://openalex.org/W4389519058"
  ],
  "abstract": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4329–4343\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPrompting Large Language Models with Chain-of-Thought for Few-Shot\nKnowledge Base Question Generation\nYuanyuan Liang1, Jianing Wang1, Hanlun Zhu1, Lei Wang2\nWeining Qian1, Yunshi Lan1∗\n1 East China Normal University, 2 Singapore Management University\nleonyuany@stu.ecnu.edu.cn, {lygwjn, timberflowing}@gmail.com\nlei.wang.2019@phdcs.smu.edu.sg, {wnqian, yslan}@dase.ecnu.edu.cn\nAbstract\nThe task of Question Generation over Knowl-\nedge Bases (KBQG) aims to convert a logi-\ncal form into a natural language question. For\nthe sake of expensive cost of large-scale ques-\ntion annotation, the methods of KBQG under\nlow-resource scenarios urgently need to be de-\nveloped. However, current methods heavily\nrely on annotated data for fine-tuning, which\nis not well-suited for few-shot question genera-\ntion. The emergence of Large Language Mod-\nels (LLMs) has shown their impressive gen-\neralization ability in few-shot tasks. Inspired\nby Chain-of-Thought (CoT) prompting, which\nis an in-context learning strategy for reason-\ning, we formulate KBQG task as a reasoning\nproblem, where the generation of a complete\nquestion is split into a series of sub-question\ngeneration. Our proposed prompting method\nKQG-CoT first selects supportive logical forms\nfrom the unlabeled data pool taking account of\nthe characteristics of the logical form. Then, we\nconstruct a task-specific prompt to guide LLMs\nto generate complicated questions based on se-\nlective logic forms. To further ensure prompt\nquality, we extend KQG-CoT into KQG-CoT+\nvia sorting the logical forms by their com-\nplexity. We conduct extensive experiments\nover three public KBQG datasets. The results\ndemonstrate that our prompting method consis-\ntently outperforms other prompting baselines\non the evaluated datasets. Remarkably, our\nKQG-CoT+ method could surpass existing few-\nshot SoTA results of the PathQuestions dataset\nby 18.25, 10.72, and 10.18 absolute points on\nBLEU-4, METEOR, and ROUGE-L, respec-\ntively.\n1 Introduction\nQuestion generation task requires a system to pro-\nduce natural language questions based on the given\ncontext. KBQG (Guo et al., 2022) is one of the im-\nperative question generation tasks when the given\n∗*Corresponding author\nFigure 1: Overview of KQG-CoT framework.\ncontext derived from Knowledge Bases (KBs) is in\nthe form of logical. KBQG has attracted increas-\ning interests from both the industry and academia\ndue to its potential for data augmentation in QA\nsystems (Xiong et al., 2022; Chen et al., 2023) and\nits ability to assist dialogue systems in creating\ncoherent questions (Lee et al., 2018).\nExisting studies (Kumar et al., 2019; Ke et al.,\n2021; Fei et al., 2022; Guo et al., 2022; Chen et al.,\n2023) for KBQG tasks have predominantly uti-\nlized neural network-based approaches and demon-\nstrated impressive performance by conducting fine-\ntuning on extensive training datasets. However,\nas the collection of KBQG data is labor-intensive,\nresearchers start paying attention to the few-shot\nKBQG tasks (Xiong et al., 2022), where a great\nchallenge is posed for suppliers with limited re-\nsources: 1) A great deal of annotated data is de-\nmanded to allow the existing fine-tuned models to\ngeneralize well over different logical forms. How-\never, due to the limitations of low-resource avail-\nability, training conventional models by fine-tuning\non the full data becomes unrealistic. 2) A logical\nform is composed of entities, relations, and query\ngrammar. Having logical forms with various com-\nbinations of these basic components is crucial to\n4329\nuphold the model’s capability for compositional\ngeneralization. The lack of data leads to a com-\npositional challenge to the KBQG tasks (Gu et al.,\n2021). 3) Certain logical forms can become com-\nplex when operations such as aggregation, superla-\ntives, and comparisons are involved. Representing\nthese logical forms presents additional challenges.\nMoreover, developing a KBQG method that incor-\nporates diverse and elaborate expressions becomes\nparticularly difficult in such low-resource scenar-\nios (Xiong et al., 2022; Guo et al., 2022).\nRecently, LLMs such as GPT-3 and Codex (Gao\net al., 2022; Suzgun et al., 2022; Wei et al., 2022;\nWang et al., 2023a) have proven their strong gen-\neralizability on a wide range of few-shot and zero-\nshot tasks with CoT, including text interpretation,\ncomputer vision, planning and reasoning. Mean-\nwhile, a line of work (Kasner et al., 2022; Moiseev\net al., 2022; Andrus et al., 2022; Trajanoska et al.,\n2023; Xie et al., 2023) validates that LLMs have\nthe strong capability to accurately capture the se-\nmantics of relations between values in the data,\nenabling to transform the structured instructions\nto narrative text. The above studies inspire us to\nexplore few-shot KBQG tasks by prompting LLMs\nwith CoT.\nHowever, how to apply LLMs to KBQG with\nCoT is still unclear. On one hand, KBQG differs\nfrom tasks like code generation or question an-\nswering, as it involves incorporating KB-specific\nitems into the input instead of self-contained narra-\ntives. Therefore, formatting the input in an easily\nunderstandable manner while considering the KB\nschema is crucial. On the other hand, the challenge\nlies in designing effective CoT prompts (Wei et al.,\n2022) that can enhance the performance of LLMs\nin the context of few-shot KBQG.\nIn this work, we propose KQG-CoT framework,\nwhich is the first attempt for training-free few-shot\nKBQG with LLMs. As shown in Figure 1, our\nframework consists of two main steps, the objects\nof which are supportive logical forms selection\nfrom an unlabeled data pool and prompt construc-\ntion. To acquire coherent logical forms, we employ\na clustering technique to carefully choose multiple\nlogical forms that serve as representatives, consid-\nering both their syntactic and semantic characteris-\ntics. To construct prompt, inspired by the principle\nof CoT (Wei et al., 2022), we take the selected logi-\ncal forms as exemplars and write rationales to split\nthe generation of a complete question into mul-\ntiple steps. We concatenate the above rationales\nwith the queried logical form to form a prompt,\nwhich guides a LLM to outcome a reasoning pro-\ncess of generating a complex question aligning with\nthe logical form. We further improve KQG-CoT\nto KQG-CoT+ via sorting the supportive logical\nforms by complexity.\nAs previous methods rely heavily on the train-\ning instances to fine-tune a KBQG model. KQG-\nCoT does not need numerous logical form ques-\ntion pairs to train the models. We test the perfor-\nmance of our prompting methods under few-shot\nsetting on three public datasets, namely WebQues-\ntions (Kumar et al., 2019), PathQuestions (Zhou\net al., 2018), and GrailQA (Gu et al., 2021). We\nconduct a comprehensive comparison with a range\nof commonly used CoT baseline methods including\nAuto-CoT (Zhang et al., 2023c), Active-CoT (Diao\net al., 2023), Random-CoT (Brown et al., 2020)\nand so on. The experimental results show that we\ncan outperform all of them with an observable mar-\ngin. Besides, we also compare with a set of SoTA\nsystems trained with full data or few data. Our\nfew-shot method could achieve competitive results\nto the full training methods. Remarkably, our few-\nshot method could surpass existing few-shot SoTA\nresults of PathQuestions dataset by 18.25, 10.72\nand 10.18 absolute points on BLEU-4, METEOR\nand ROUGE-L, respectively.\nKQG-CoT provides a simple but effective solu-\ntion to few-shot KBQG problem, we expect it could\nserve as an important baseline for future investiga-\ntion to KBQG tasks under low-resource scenarios.\nOur main contributions are summarized as fol-\nlows:\n• By encoding and clustering the skeletons of\nlogical forms, we successfully retrieved sup-\nportive logical forms that are particularly suit-\nable for constructing effective prompts.\n• We reorganized the sequence of examples and\nutilized the CoT method to construct prompts\nthat are highly effective for large language\nmodels.\n• The experimental results indicate that our\nmethod surpasses the baseline by a signifi-\ncant margin and achieves performance levels\nthat are comparable to fine-tuned methods.\n4330\n2 Related Work\nKnowledge Base Question Generation.The early\napproaches for KBQG tasks are template-based\nmethods. Berant et al. (2013 and Talmor and Be-\nrant (2018a) utilized search engines and manual an-\nnotation to construct the natural language questions\nbased on logical forms. However, template-based\nmethods rely on manual intervention, which is hard\nto be scaled up. With the advancement of deep neu-\nral networks, neural network-based methods have\nemerged as a prominent and widely adopted ap-\nproach. Kumar et al. (2019) and Chen et al. (2023)\nproposed end-to-end models based on Transformer\nand Graph2seq models, which are capable of gen-\nerating complex, multi-hop questions based on a\nsubgraph. Follow-up studies (Fei et al., 2022; Guo\net al., 2022) developed more complicated models\nfor KBQG, which ensure the relevance between\nthe generated questions and subgraphs. Xiong\net al. (2022) proposed a method for low-resource\nKBQG, where an auto-prompter is developed to\nparaphrase a logical form into a description, so\nthat a pre-trained language model can be fine-tuned\nwith the augmented data. Our work is different\nfrom this one as our method focuses on solving\nfew-shot KBQG challenge with frozen LLMs.\nFew-shot Learning for Text Generation.In re-\ncent years, significant progress has been made in\nthe field of few-shot learning for text generation.\nOne line of work develops meta-learning frame-\nworks for text generation (Mi et al., 2019; Madotto\net al., 2019; Zeng et al., 2021; Hospedales et al.,\n2022), which aims to acquire an optimal initial-\nization that enables accurate and rapid adaptation\nto a new task, even when limited data is available.\nOther line of work proposes different augmentation\nalgorithms to synthesize the data for training (Song\net al., 2019; Zhao et al., 2022), so that conven-\ntional text generation models can be applied to the\naugmented data. Most recently, LLMs are lever-\naged to solve few-shot text generation tasks such as\ntext summarization (Yang et al., 2023; Zhang et al.,\n2023b; Liu et al., 2023), machine translation (Wang\net al., 2023b; Hendy et al., 2023), dialogue gener-\nation (Zhang et al., 2023a; Valvoda et al., 2022;\nKang et al., 2022) and so on. There is no existing\nstudy applying LLMs to few-shot KBQG tasks.\nIn-Context Learning with LLMs.Without gradi-\nent updates, In-Context Learning (ICL) effectively\ntackles a wide range of NLP tasks by incorporating\na small number of prompted examples as part of\nthe input (Ruis et al., 2023) to help LLMs under-\nstand the tasks. Multiple studies (Su et al., 2022;\nRubin et al., 2022) explored the selection of ex-\namples that are similar to the query during prompt\nconstruction. Recent researches (Lu et al., 2022a;\nLiu et al., 2022; Diao et al., 2023; Wang et al.,\n2023c) highlight that the order of these examples\nin the prompt has a substantial influence. CoT is\na prompting strategy decomposing complex tasks\ninto sub-tasks, helping the model to derive the cor-\nrect answers progressively (Wei et al., 2022; Zhou\net al., 2023). It has been widely used in mathemati-\ncal word problem solving, common-sense reason-\ning, and symbolic reasoning. Our work incorpo-\nrates CoT strategy into KBQG tasks, where itera-\ntive process enables LLMs to ultimately obtain a\ncomplex question aligning with the logical form.\n3 Methodology\n3.1 Problem Formulation\nA KB consists of a set of triples. A logical form\nis a structural expression of a subgraph in the KB,\nwhich may consist of complex operations (e.g., ag-\ngregation, comparative and superlative) and can\nbe utilized to execute against a KB. The task of\nKBQG requires a system to generate a natural lan-\nguage question when given a logical form and the\ncorresponding KBs with consistent semantics.\n3.2 Method Overview\nRecently, the LLM has shown its impressive in-\ncontext few-shot learning capabilities. Instead of\nfine-tuning a pre-trained model to adapt it to a\ndownstream task, we can simply apply it to a new\ntask with a few examples as prompt during infer-\nence (Yang et al., 2022; Li et al., 2023). For the\nKBQG task, we adopt a two-stage method to de-\nsign CoT prompts, which effectively enable the\nLLM to comprehend complex logical forms and\ngenerate questions. Concretely, the first stage Sup-\nportive Logical Forms Selectionfocuses on iden-\ntifying supportive examples that represent various\nsyntax patterns of logical forms. To accomplish\nthis, we encode the structure of logical forms, per-\nform clustering, and employ sampling techniques\nto select top- k supportive logical forms. Once\nthese supportive examples are selected, we leverage\nLLMs with CoT prompts to generate natural lan-\nguage questions. This leads us to the second stage,\nPrompt Construction, which involves producing\nsub-questions as rationales. Through this process,\n4331\nwe can ultimately formulate a complex question\nthat adequately captures the semantic of the logi-\ncal form. A schematic diagram of our method is\ndisplayed in Figure 2.\n3.3 Supportive Logical Forms Selection\nZhang et al. (2023c) has shown that when con-\nstructing demonstrations, we need to mitigate the\neffect of few-shot CoT errors by differentiating the\ndesign of demonstrations. In KBQG tasks, support-\nive logical forms are those that can cover diverse\nlogical rules, so as to offer more syntax informa-\ntion for LLMs to generate questions. Unlike the\nnarrative inputs, the logical form is a combination\nof program structures and schema items (i.e., en-\ntities and relations). Therefore, it is essential to\ntake both aspects into consideration when select-\ning supportive logical forms. In our approach, we\nutilize Structure Encoding and Clustering, fol-\nlowed by a Logical Form Samplingprocess to\nselect supportive logical forms.\nStructure Encoding and Clustering. To ensure\nthe logical forms can be drafted for unseen ques-\ntions, we extract their structures by converting the\nschema items into symbolic variables. Specifi-\ncally, we keep the grammars in the logical form\nunchangeable. Then, we replace the relation with\nsymbol “ r” and we replace the entity with “ e”.\nThis structure is also known as a abstract query\ngraph (Chen et al., 2021), which reflects the topol-\nogy and the component classes of logical forms.\nFor instance, the raw logical form is:\n(AND medicine.routed_drug\n(JOIN medicine.routed_drug.marketed_formulations\nm.0hqs1x)).\nIt becomes the following structure after conversion:\n(AND r (JOIN r e)).\nOnce we have obtained the structure of the log-\nical forms, which filters out the semantic mean-\ning of the logical forms. We encode the structure\nrepresentation into a fix-length embedding. In de-\ntail, we view the structure as a sequence of to-\nkens. We encode the contexts of the sequence with\nSentence-Transformers (Reimers and Gurevych,\n2019), which is an advanced model for text em-\nbedding. The encoded vectors are well-suited for\ncalculating the similarity between sentences. We\nextract the final hidden state of as the vectorized\nrepresentation of the sentence. After that, we utilize\nthe K-means (Hartigan and Wong, 1979) clustering\nalgorithm to group the encoded structure into k\nclusters based on their syntactic similarity.\nLogical Form Sampling. Each cluster contains a\ngroup of logical forms with the similar structure,\nwe randomly pick up a structure from each group\nand obtain k representative structures. As each\nstructure may correspond to multiple logical forms.\nWe further identify k logical forms with distinct se-\nmantics deriving from the k selected structures. To\nthis end, we iteratively sample logical forms hold-\ning the maximum diversity of semantics. Specifi-\ncally, for the first logical form, we randomly pick\nup one from the candidates. Then we search logical\nforms for another structure. We greedily pick up\na candidate with least semantic similarity to the\nselected logical forms, where the similarity is mea-\nsured by the encoding of the original logical forms.\nWe repeat the process until we have gone through\nk structures as shown in Figure 2.\nTo help the LLMs fully understand the logical\nforms, we substitute the entities in the original log-\nical forms with their surface names in the KB. In\nthis way, we obtain k supportive logical forms.\n3.4 Prompt Construction\nSince some logical forms have complicated se-\nmantics and even nested syntactic structures are\nincluded. Following the CoT method, we construct\na reasoning chain prompt based on the supportive\nlogical forms retrieved above. For each example,\nwe need to generate a reasoning chain based on log-\nical forms to elicit LLMs generate questions from\nsimple to complicated. To this end, we hold two\ncriteria when constructing reasoning chains:\n(i) The templates should break up the generation\nof a complicated question into a step by step\nprocess.\n(ii) The templates should clearly identify the sub-\ncomponent in a logical form that requires\nLLMs to focus on for each step.\nTherefore, we first break down a logical form\nin a nested manner, where the follow-up logi-\ncal forms include the preceding logical forms.\nSpecifically, the first step usually generates a\nsimple question querying one-hop relation from\nthe topic entity. The second step usually gener-\nates a question querying two-hop relation chain\ninvolving the above one-hop relation. As we\ncan see from Figure 2, the first step of prompt\n4332\nFigure 2: KQG-CoT framework. The supportive logical forms are selected from an unlabeled data pool by extracting\nthe structures, clustering the structures and sampling the most representative ones. A total of k demonstrations are\nautomatically constructed using reasoning chains. The tested logical form is appended to the demonstrations to\nform the complete prompt, which can elicit the LLM to generate a series of subquestions sequentially from simple\nto complicated. Finally, the last subquestion can be extracted as the final prediction.\nparses the entire logical form into one-hop relation\nsubgraph1 “(AND sports.sport.team_coaches\nJohn Russo )” which leads to a simple subques-\ntion1 “sport team coach john russo ”. The second\nstep includes the parsed logical form appended\nto the previous step as a component and gener-\nates question “Which sport does john russo coach?”\nbased on the subgraph2 and subquestion1. As a\nresult, we continuously expand the logical form\nuntil a complete question is formed. This step-by-\nstep process ensures that the generated question is\nsemantically coherent and grammatically accurate.\nDuring inference, we concatenate all the demon-\nstrations and queried logical form as the final\nprompt. Based on the example in Figure 2, the\nprompt includes “ Input: (AND ... Input: (JOIN\n... Input: (COUNT ... S.A. ”. After receiving the\nprompt, LLMs outcome the predictions that clari-\nfies the intermediate generation steps of subques-\ntion1, subquestion2, and subquestion3. And the\nlast subquestion will be our final predicted ques-\ntion, which is “What is the number of aircraft man-\nufacturer in the legal structure of s.a. ?”.\nDataset #Q #R #E #T\nWQ 22,989 672 25,703 2/99/5.8\nPQ 9,731 378 7,250 2/3/2.7\nGQ 64,331 3,720 32,585 1/4/1.4\nTable 1: Statistics of the evaluated datasets. #Q de-\nnotes the number of questions. #R and #E denote\nthe total number of relations and entities, respectively.\n#T denotes the minimum/maximum/average number of\ntriplets involved in each question.\n4 Experiment\nIn this section, we first introduce the KBQG\ndatasets used to evaluate the performance of our\nproposed method and the comparable baseline\nmethods. Next, we present the implementation\ndetails and demonstrate the experimental results.\n4.1 Data and Metrics\nWe evaluate our prompting method on the follow-\ning three public datasets:\nWebQuestions (WQ) (Kumar et al., 2019) 1 is\na KBQG dataset combining instances from We-\nbQuestionsSP (Serban et al., 2016) and Com-\n1https://github.com/liyuanfang/mhqg\n4333\nMethod WQ PQ GQ\nB M R B M R B M R\nStandard Prompt 24.86 29.01 52.74 55.87 42.24 76.83 29.17 33.52 42.95\nRandom-CoT 25.02 29.37 53.16 56.42 42.61 77.03 29.81 33.75 43.31\nManual-CoT 28.44 30.24 54.30 60.37 42.88 77.48 30.18 33.61 44.89\nActive-CoT 26.02 29.55 54.01 58.78 43.86 76.78 30.27 33.71 44.07\nAuto-CoT 28.42 29.65 53.47 59.59 43.16 77.13 30.17 34.22 44.47\nKQG-CoT (Ours) 28.89 30.41 54.38 60.81 43.54 77.35 30.51 34.26 44.91\nKQG-CoT+ (Ours) 29.73 31.08 55.14 61.71 44.27 78.41 31.24 34.94 45.36\nTable 2: Few-shot evaluation of existing prompting methods with Frozen LLMs on three KBQG datasets. The best\nand second best results are boldfaced and underlined respectively.\nplexWebQuestions (Talmor and Berant, 2018b). It\nprovides questions, answers, and annotated sub-\ngraphs. This dataset is commonly evaluated in\nexisting work (Guo et al., 2022).\nPathQuestions (PQ)(Zhou et al., 2018)2 is a com-\nmonly used KBQG dataset constructed from a\nKBQA dataset. It contains questions inquiring a\nchain of relations, wherein the path between the\ntopic entities and answer entities is 2-hop or 3-hop.\nGrailQA (GQ)(Gu et al., 2021)3 is a large-scale\nKBQA dataset built on Freebase, which covers\n86 domains. It covers complex questions which\nrequire counting, ranking and even superlative\ninquiry. Each question is associated with a s-\nexpression, which can be viewed as a logic form.\nWe collect the annotated the logic form from\nthe training set as the data pool and leave the orig-\ninal questions untouched. The questions in the\nvalidation or test set are sampled to evaluate our\nmethod. Statistics of evaluated datasets are shown\nin Table 1.\nFollowing previous KBQG studies, we rely on\na set of well-established metrics as for KBQG\nevaluation: BLEU-4 (Papineni et al., 2002), ME-\nTEOR (Banerjee and Lavie, 2005) and ROUGE-\nL (Lin, 2004). BLEU-4 and ROUGE-L can be\nviewed as precision and recall for text generation\ntasks, respectively. METEOR is a comprehensive\nmetric beyond exact matches, which also accounts\nfor partial matches and variations in word order.\nWe denote them as B, M and R, respectively.\n4.2 Comparable Methods\nWe denote our prompting method as KQG-CoT.\nPrevious studies (Lu et al., 2022b) have proven\nthat the order of the exemplars is significant to the\nprompt results, we implement an improved version\n2https://github.com/zmtkeke/IRN\n3https://dki-lab.github.io/GrailQA/\nby sorting the demonstrations from short to long\nafter sampling. We denote this method as KQG-\nCoT+.\nAs there is no existing attempt for few-shot\nKBQG tasks with LLMs, we adopt five general\nprompting methods under few-shot scenarios as\nour baselines.\nStandard Prompt(Brown et al., 2020) is a stan-\ndard prompting method of in-context learning,\nwhere k random logical forms and questions are\nconcatenated to form the prompt. The prediction is\none-step generation.\nRandom-CoT is an intuitive CoT prompting base-\nline where k logical forms are randomly selected\nfrom the data pool and we follow the original\nwork (Brown et al., 2020) to describe the sub-task\nin a narrative.\nManual-CoT (Wei et al., 2022) is a CoT prompting\nwith k human-written exemplars as demonstrations\nand the sub-task is presented in narratives.\nActive-CoT (Diao et al., 2023) is an ensemble\nframework for CoT prompting. The multiple logi-\ncal forms are randomly selected as a validation set.\nThen multiple measurements (e.g., disagreement,\nvariance) are leveraged as the uncertainty value for\neach logical form to produce the final question.\nAuto-CoT (Zhang et al., 2023c) automatically con-\nstructs prompt by selecting k demonstrations with\na cluster-based algorithm and the sub-task is pre-\nsented in narratives. We simply adopt the prompt-\ning method to KBQG tasks by encoding all logical\nform in a textual way.\n4.3 Implementation Details\nFor encoding of logical forms, we utilize all-\nMiniLM-L6-v24 checkpoint from the Sentence-\nTransformers library in Huggingface for effective\n4https://huggingface.co/sentence-transformers/\nall-MiniLM-L6-v2\n4334\nencoding. As this is a few-shot scenario, we manu-\nally write the rationales for the k demonstrations in\nthe chain prompt. We utilize text-davinci-003\nfrom OpenAI API5 to generate questions and set\nthe number of clusters as k = 126.\n4.4 Main Results\nMethod WQ\nB M R\nFull Training\nL2A (Du et al., 2017) 6.01 26.95 25.24\nTransformer (Vaswani et al., 2017) 8.94 13.79 32.63\nMHQG (Kumar et al., 2019) 11.57 29.69 35.53\nBiGraph2Seq (Chen et al., 2023) 29.45 30.96 55.45\nT5-Large (Raffel et al., 2020) 28.78 30.55 55.12\nJointGT (Ke et al., 2021) 30.02 32.05 55.60\nIGND (Fei et al., 2021) 30.62 31.41 55.82\nLFKQG (Fei et al., 2022) 31.66 32.69 56.75\nDSM (Guo et al., 2022) 28.62 - 64.25\nFew-shot Evaluation\nKQG-CoT 28.89 30.41 54.87\nKQG-CoT+ 29.73 31.08 55.46\nTable 3: Comparison between few-shot evaluation of\nKQG-CoT/KQG-CoT+ and full-trained evaluation of\nother systems on WQ.\nMethod PQ\nB M R\nFull Training\nL2A (Du et al., 2017) 17.00 50.38 19.72\nTransformer (Vaswani et al., 2017)56.43 43.45 73.64\nMHQG (Kumar et al., 2019) 25.99 33.16 58.94\nBiGraph2Seq (Chen et al., 2023) 61.48 44.57 77.72\nAutoQGS (Xiong et al., 2022) 65.13 47.50 76.80\nT5-Large (Raffel et al., 2020) 58.95 44.72 76.58\nIGND (Fei et al., 2021) 61.69 45.11 77.28\nLFKQG (Fei et al., 2022) 63.92 46.91 78.40\nJointGT (Ke et al., 2021) 65.89 48.25 78.87\nDSM (Guo et al., 2022) 61.03 - 86.06\nFew-shot Evaluation\nBiGraph2Seq (Chen et al., 2023) 1.01 4.99 12.07\nJointGT (Ke et al., 2021) 43.15 35.91 69.57\nAutoQGS (Xiong et al., 2022) 43.46 33.55 68.23\nKQG-CoT 60.81 43.54 77.35\nKQG-CoT+ 61.71 44.27 78.41\nTable 4: Comparison between few-shot evaluation of\nKQG-CoT/KQG-CoT+ and few-shot/full-trained evalu-\nation of other systems on PQ.\nComparison with Baselines.Table 2 showcases\nthe experimental results of our methods and base-\nline approaches. We have the following observa-\ntions based on it:\n1) Comparing all CoT prompting methods, in\nthe few-shot setting, our KQG-CoT+ prompting\n5https://openai.com/blog/openai-codex/\n6Detailed prompt design of KQG-CoT+ is presented in\nAppendix A.3.\nconsistently outperforms other method across all\nKBQG datasets by a remarkable margin. Specif-\nically, KQG-CoT+ improves the performance of\nthe competitive Auto-CoT by 0.72 to 2.12 absolute\nvalues for all datasets. Meanwhile, KQG-CoT also\noutperforms existing CoT prompting methods on\nBLEU-4 of all the datasets.\n2) Comparing CoT methods with standard\nprompting, we notice that all the CoT prompt-\ning methods outperform the standard prompting\nmethod, which indicates that, to generate questions\nwith complex logic and long dependency, splitting\nthe entire generation task into sub-tasks are crucial\nfor maintaining the coherence and accuracy of the\nquestions.\n3) Comparing Auto-CoT, KQG-CoT and KQG-\nCoT+, even though all these methods adapt clus-\ntering to select k demonstrations, KQG-CoT and\nKQG-CoT+ are more effective as we elaborately\ndesign encoding algorithm and prompt templates\nfor KBQG tasks, which makes it fit more into the\nquestion generation from the logical forms.\nComparison with Other Systems. We further\ncompare our prompting methods with other KBQG\nsystems on the WQ and PQ datasets. According\nto our knowledge, we are the first to work on the\nKBQG task using the GQ dataset, so there are no\nexisting methods available for comparison.\nIn Table 3, we can see that with 12 demonstra-\ntions, our method can outperform majority of full-\ntrained systems on WQ dataset, where all training\ndata is leveraged to train a model. KQG-CoT+\nprompting method can achieve 29.73%, 31.08%\nand 55.46% for BLEU-4, ROUGE-L and ME-\nTEOR respectively, which are close to the SoTA\nresults.\nIn Table 4, we can see that for PQ dataset, our\nmethod can still achieve better results than most\nof existing full-trained KBQG models. Compared\nwith existing methods under few-shot settings, our\nmethods can significantly improve the BLEU-4\nover AutoQGS by around 20 absolute points. It\nis worth noting that AutoQGS takes 0.1% training\ninstances for training and we simply leverage12 in-\nstances for inference, which highlights superiority\nof our methods.\n4.5 More Analysis\nHuman Evaluation. We further conduct human\nevaluation by randomly sampling 300 examples\nfrom the test set of WQ dataset. The generated\n4335\nInput: (AND military.military_conflict (JOIN military.military_conflict.force_strengths (JOIN\n(R military.military_resource.conflicts) Bendix AN/FPS-20)))\nManual-CoT: Which military conflict involves the bendix an/fps-20 and what are its force strengths?\nActive-CoT: What military conflict has force strengths using bendix an/fps-20?\nAuto-CoT: What are the force strengths in the bendix an/fps-20 military conflict?\nKQG-COT+: Which military conflict has force strengths with bendix an/fps-20?\nGround Truth: Which military conflict has force strengths with conflicts bendix an/fps-20?\nInput:(AND measurement_unit.measurement_system (JOIN measurement_unit.measurement_system.\nheat_capacity_units Joule per kelvin))\nManual-CoT: What is the measurement system that uses joules per kelvin for heat capacity units?\nActive-CoT: What is the measurement system for heat capacity units of joule per kelvin?\nAuto-CoT: Which measurement system uses joule per kelvin as its heat capacity unit?\nKQG-COT+: What measurement system uses joule per kelvin as a units to measure heat capacity?\nGround Truth: What system uses joule per kelvin as the unit to measure heat capacity?\nTable 5: Illustrative examples from KQG-CoT+ and baseline methods on GQ.\nModel Synt. Comp. Relev.\nGround Truth 4.88 4.92 4.91\nStandard Prompt 3.67 3.76 3.99\nRandom-CoT 4.05 4.21 4.12\nManual-CoT 4.60 4.54 4.72\nActive-CoT 4.56 4.71 4.75\nAuto-CoT 4.38 4.77 4.55\nKQG-CoT+ 4.63 4.80 4.78\nTable 6: Results of human evaluations on WQ. Synt.,\nComp. and Relev. denote syntactic correctness, com-\nplexity and relevance, respectively.\nMethod GQ\nB M R\nKQG-CoT+ 31.24 34.94 45.36\n(a) w/o CoT 30.11 33.58 43.88\n(b) K-means →Random 29.81 33.75 43.31\n(c) w/o structure encoding 30.03 33.41 43.76\nTable 7: Ablation study of our KQG-CoT+ method on\nGQ.\nquestions are rated on a scale of 1 to 5 considering\nthe aspects of syntactic correctness, complexity,\nand relevance to the given logical forms. We ask\nthree annotators to score the generated questions\nwith 1-point being poor and 5-point being perfect.\nThe score of each question is averaged over all\nannotators. We present the results in Table 6, where\nwe can observe a similar trend between human and\nautomatic evaluation. Our approach outperforms\nall comparable methods, the evaluated scores of\nwhich are close to the ground truth.\nAblation Study.We conduct ablation study to as-\nsess the effectiveness of components of our model\nand display the results in Table 7. We first exclude\nthe CoT reasoning chain, and observe a perfor-\nmance drop of the evaluate metrics. This indicates\nthat CoT plays an important role in generating com-\nplicated questions. Then we remove the K-means\nalgorithm and randomly select supportive logical\nforms. The decrease of the results indicates that our\nclustering algorithm could provide more diverse\nlogical forms as our demonstrations. We further\nencode the entire logical forms without extracting\ntheir structures. The results decrease which indi-\ncate that the structure is a significant indicator to\nobtain the clusters7.\nFigure 3: The BLEU-4 and ROUGE-L scores of our\nmethod and Random-CoT with increasing number of\nshots on GQ.\nEffect ofk. We investigate the effect of k in Fig-\nure 3. As observed, with an increase of the number\nof demonstrations, both our methods and Random-\nCoT show increasing BLEU-4 and ROUGE-L\nscores. This indicates that the number of demon-\nstrations is significant in activate the potentials of\nLLMs. Compared with Random-CoT, our method\nshows a larger gain when the value of k becomes\nlarge, this indicates that our methods indeed pick up\nthe most representative logical form as the demon-\nstrations.\nCase Study.To provide a comprehensive compar-\n7The ablation study on WQ and PQ is presented in Ap-\npendix A.1.\n4336\nison between KQG-CoT+ method and the base-\nline models on GQ dataset, we present multiple\nexample cases in Table 5. Our method elicits the\nintermediate generation steps and provides more\nguidance to LLMs so that our KQG-CoT+ gener-\nates questions that are grammatically correct and\nsemantically close to the given logical form. In con-\ntrast, baseline methods may encounter issues such\nas inconsistency in the logical form, misplaced\nmodifiers, or unsmooth expressions.\nEffectiveness of Structured Encoding and Clus-\ntering. To demonstrate the effectiveness of the\nproposed Structured Encoding and Clustering in\nselecting diverse structures, we conducted a quan-\ntitative assessment of the average semantic simi-\nlarity between the logical forms extracted using\nour method and the baseline method at K=8 on the\nGrailQA dataset. The results are presented in Table\n8. The data from the initial segment, shown in the\ntable below, reveals that the logical forms chosen\nby our method exhibit a lower average semantic\nsimilarity. When viewed collectively, these find-\nings offer strong evidence for the efficacy of our\nproposed approach.\nMethod Average_similarity\nRandom 0.285\nActive-CoT 0.274\nAuto-CoT 0.265\nKQG-CoT 0.252\nTable 8: The average semantic similarity between the\nlogical forms of different methods.\nImpact of Sorted Order.To assess the impact\nof the sorted order of demonstrations in KQG-\nCoT+, we compared the performance of Auto-CoT\nand Active-CoT using the same sorted order of\ndemonstrations in KQG-CoT+ (i.e., Auto-CoT+\nand Active-CoT+) and conducted experiments on\nthe GrailQA dataset . The Table 9 shows that, com-\npared to the Active-CoT+ and Auto-CoT+ methods,\nour proposed KQG-CoT+ method still exhibits sig-\nnificant improvements.\nMethod B M R\nActive-CoT+ 30.40 34.04 44.22\nAuto-CoT+ 30.52 34.59 44.77\nKQG-CoT+ 31.24 34.94 45.36\nTable 9: The result data for Auto-CoT+, Active-CoT+,\nand KQG-CoT+ on the GrailQA dataset.\nKQG-CoT Improve KBQA Task.To confirm\nthe efficacy of our approach in enhancing the\nperformance of KBQA methods, we initiated a\ndata augmentation procedure for the WebQuestions\ndataset. It’s important to highlight that the aug-\nmented dataset was merely half the size of the orig-\ninal dataset. Next, we trained the KBQA method\nRnG-KBQA (Ye et al., 2022) by combining the\naugmented and original datasets, resulting in the\nimproved version called RnG-KBQA+. The results,\nas outlined in Table 10, demonstrate that we con-\nducted a relatively straightforward augmentation\non a limited dataset subset. Nevertheless, the F1\nscore of the original KBQA method witnessed a\nnotable increase of 2.8%. This demonstrates that\nour proposed KBQG method provides significant\nassistance to downstream KBQA tasks8.\nMethod F1-Score\nRnG-KBQA 75.6\nRnG-KBQA+ 78.4\nTable 10: The result of our approach in improving the\nperformance of KBQA methods.\n5 Conclusion\nIn this paper, we presented the KQG-CoT approach\nto tackle few-shot KBQG tasks. KQG-CoT re-\ntrieves relevant logical forms from unlabeled data\nand incorporates their characteristics. It then gener-\nates explicit prompt to showcase the reasoning pro-\ncess for complex question generation based on the\nselected examples. Experimental results demon-\nstrate that our approach achieves state-of-the-art\nperformance compared to baselines and even shows\ncompetitive results to full-training methods.\nLimitations\nOur proposed prompting method, KQG-CoT, par-\ntially relies on handcrafted prompts when writing\nthe subquestions. However, handcrafted prompts\nare usually based on the personal knowledge and\nexperience of the exports, which can introduce sub-\njective biases.\nAcknowledgements\nThis work was supported by Natural Science Foun-\ndation of China (Project No. 62206097) and\nShanghai Pujiang Talent Program (Project No.\n22PJ1403000). We sincerely thank the anonymous\nreviewers for their valuable comments and feed-\nback.\n8Further analysis will be presented in Appendix A.4.\n4337\nReferences\nBerkeley R Andrus, Yeganeh Nasiri, Shilong Cui, Ben-\njamin Cullen, and Nancy Fulda. 2022. Enhanced\nstory comprehension for large language models\nthrough dynamic document-based knowledge graphs.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 36:10436–10444.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nYongrui Chen, Huiying Li, Yuncheng Hua, and Guilin\nQi. 2021. Formal query building with query struc-\nture prediction for complex question answering over\nknowledge base. arXiv pr arXiv:2109.03614.\nYu Chen, Lingfei Wu, and Mohammed Zaki. 2023.\nToward subgraph-guided knowledge graph question\ngeneration with graph neural networks. IEEE trans-\nactions on neural networks and learning systems, PP.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1342–1352,\nVancouver, Canada. Association for Computational\nLinguistics.\nZichu Fei, Qi Zhang, and Yaqian Zhou. 2021. Itera-\ntive gnn-based decoder for question generation. In\nProceedings of the 2021 conference on empirical\nmethods in natural language processing, pages 2573–\n2582.\nZichu Fei, Xin Zhou, Tao Gui, Qi Zhang, and Xuan-\njing Huang. 2022. LFKQG: A controlled generation\nframework with local fine-tuning for question gener-\nation over knowledge bases. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 6575–6585. International Committee\non Computational Linguistics.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv pr arXiv:2211.10435.\nYu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy\nLiang, Xifeng Yan, and Yu Su. 2021. Beyond i.i.d.:\nThree levels of generalization for question answering\non knowledge bases. In Proceedings of the Web\nConference 2021, page 3477–3488.\nShasha Guo, Jing Zhang, Yanling Wang, Qianyi Zhang,\nCuiping Li, and Hong Chen. 2022. DSM: Question\ngeneration over knowledge base via modeling diverse\nsubgraphs with meta-learner. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4194–4207. Association\nfor Computational Linguistics.\nJohn A Hartigan and Manchek A Wong. 1979. Algo-\nrithm as 136: A k-means clustering algorithm. Jour-\nnal of the royal statistical society. series c (applied\nstatistics), 28(1):100–108.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation. arXiv\npr arXiv:2302.09210.\nT. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey.\n2022. Meta-learning in neural networks: A survey.\nIEEE Transactions on Pattern Analysis Machine In-\ntelligence, 44(09):5149–5169.\nMinki Kang, Jin Myung Kwak, Jinheon Baek, and\nSung Ju Hwang. 2022. Knowledge-consistent di-\nalogue generation with knowledge graphs. In ICML\n2022 Workshop on Knowledge Retrieval and Lan-\nguage Models.\nZdenˇek Kasner, Ioannis Konstas, and Ond ˇrej Dušek.\n2022. Mind the labels: Describing relations in\nknowledge graphs with pretrained models. arXiv\npr arXiv:2210.07373.\nPei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Lin-\nfeng Song, Xiaoyan Zhu, and Minlie Huang. 2021.\nJointGT: Graph-text joint representation learning for\ntext generation from knowledge graphs. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 2526–2538, Online.\nAssociation for Computational Linguistics.\n4338\nVishwajeet Kumar, Yuncheng Hua, Ganesh Ramakrish-\nnan, Guilin Qi, Lianli Gao, and Yuan-Fang Li. 2019.\nDifficulty-controllable multi-hop question generation\nfrom knowledge graphs. In The Semantic Web–ISWC\n2019: 18th International Semantic Web Conference,\npages 382–398. Springer.\nChe-Hao Lee, Tzu-Yu Chen, Liang-Pu Chen, Ping-Che\nYang, and Richard Tzong-Han Tsai. 2018. Auto-\nmatic question generation from children’s stories for\ncompanion chatbot. In 2018 IEEE International Con-\nference on Information Reuse and Integration (IRI),\npages 491–494.\nTianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su,\nand Wenhu Chen. 2023. Few-shot in-context learning\nfor knowledge base question answering. In The 61st\nAnnual Meeting of the Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv pr arXiv:2303.16634.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022a. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022b. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and\nPascale Fung. 2019. Personalizing dialogue agents\nvia meta-learning. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5454–5459, Florence, Italy. Asso-\nciation for Computational Linguistics.\nFei Mi, Minlie Huang, Jiyong Zhang, and Boi Faltings.\n2019. Meta-learning for low-resource natural lan-\nguage generation in task-oriented dialogue systems.\nIn Proceedings of the Twenty-Eighth International\nJoint Conference on Artificial Intelligence, IJCAI-19,\npages 3151–3157. International Joint Conferences on\nArtificial Intelligence Organization.\nFedor Moiseev, Zhe Dong, Enrique Alfonseca, and Mar-\ntin Jaggi. 2022. SKILL: Structured knowledge infu-\nsion for large language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1581–1588,\nSeattle, United States. Association for Computational\nLinguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nLaura Eline Ruis, Akbir Khan, Stella Biderman, Sara\nHooker, Tim Rocktäschel, and Edward Grefenstette.\n2023. Large language models are not zero-shot com-\nmunicators.\nIulian Vlad Serban, Alberto García-Durán, Caglar\nGulcehre, Sungjin Ahn, Sarath Chandar, Aaron\nCourville, and Yoshua Bengio. 2016. Generating fac-\ntoid questions with recurrent neural networks: The\n30M factoid question-answer corpus. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 588–598, Berlin, Germany. Association for\nComputational Linguistics.\nYiping Song, Zequn Liu, Wei Bi, Rui Yan, and Ming\nZhang. 2019. Learning to customize model struc-\n4339\ntures for few-shot dialogue generation tasks. In An-\nnual Meeting of the Association for Computational\nLinguistics.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. 2022. Selec-\ntive annotation makes language models better few-\nshot learners. arXiv pr arXiv:2209.01975.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, , and Jason Wei. 2022. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\narXiv preprint arXiv:2210.09261.\nAlon Talmor and Jonathan Berant. 2018a. The web\nas a knowledge-base for answering complex ques-\ntions. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 641–651, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nAlon Talmor and Jonathan Berant. 2018b. The web\nas a knowledge-base for answering complex ques-\ntions. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 641–651, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nMilena Trajanoska, Riste Stojanov, and Dimitar Tra-\njanov. 2023. Enhancing knowledge graph construc-\ntion using large language models. arXiv.\nJosef Valvoda, Yimai Fang, and David Vandyke. 2022.\nPrompting for a conversation: How to control a dia-\nlog model? In Proceedings of the Second Workshop\non When Creative AI Meets Conversational AI, pages\n1–8, Gyeongju, Republic of Korea. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems. Curran Associates, Inc.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023a. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels. arXiv pr arXiv:2305.04091.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and Zhaopeng Tu. 2023b.\nDocument-level machine translation with large lan-\nguage models.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023c. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nTingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu\nLiu, and Hongwei Wang. 2023. Empirical study of\nzero-shot ner with chatgpt.\nGuanming Xiong, Junwei Bao, Wen Zhao, Youzheng\nWu, and Xiaodong He. 2022. Autoqgs: Auto-prompt\nfor low-resource knowledge-based question genera-\ntion from sparql. In Proceedings of the 31st ACM\nInternational Conference on Information Knowledge\nManagement, page 2250–2259. Association for Com-\nputing Machinery.\nXianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and\nWei Cheng. 2023. Exploring the limits of chatgpt for\nquery or aspect-based text summarization. arXiv pr\narXiv:2302.08081.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In The 36th AAAI Conference on Artificial\nIntelligence (AAAI).\nXi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou,\nand Caiming Xiong. 2022. RNG-KBQA: Genera-\ntion augmented iterative ranking for knowledge base\nquestion answering. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nJiali Zeng, Yongjing Yin, Yang Liu, Yubin Ge, and\nJinsong Su. 2021. Domain adaptive meta-learning for\ndialogue state tracking. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 29:2493–\n2501.\nBiao Zhang, Barry Haddow, and Alexandra Birch.\n2023a. Prompting large language model for\nmachine translation: A case study. arXiv pr\narXiv:2301.07069.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2023b. Benchmarking large language models for\nnews summarization.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023c. Automatic chain of thought prompt-\ning in large language models. In The Eleventh Inter-\nnational Conference on Learning Representations.\nYingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng,\nDongkyu Lee, Yiping Song, Jian Sun, and Nevin\nZhang. 2022. Improving meta-learning for low-\nresource text classification and generation via mem-\nory imitation. In Proceedings of the 60th Annual\n4340\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 583–595,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\nMantong Zhou, Minlie Huang, and Xiaoyan Zhu. 2018.\nAn interpretable reasoning network for multi-relation\nquestion answering. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 2010–2022, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nA Appendix\nA.1 Ablation Study on More Datasets\nWe display Table 12 to show more ablation studies\non WQ and PQ datasets. We can also recognize the\nsignificance of our CoT reasoning chain, K-means\nalgorithm, and structure encoding.\nA.2 Illustrative Examples of KQG-CoT+\nPrompt\nWe present a selection of illustrative examples\nshowcasing our proposed prompts and predictions\non WQ, GQ, and PQ in Table 13, Table 14 and\nTable 15, respectively. As\nA.3 Detailed Prompt Design of KQG-CoT+\nTo enhance the guidance provided to LLM in ques-\ntion generation, we have included a descriptive sen-\ntence in the demonstrations, which states: “ Let’s\nengage in a step-by-step exercise of generating\nquestions from logical forms. We have provided\nseveral examples, each comprising an ’Input’ logi-\ncal form and a corresponding ’Subquestion’ that\nwe aim to generate. By deconstructing the input\nlogical form into basic components, we can gen-\nerate questions iteratively until we get the final\nquestion. For each ’Subgraph’, we can construct a\nrelevant ’Subquestion’ phrase to assist in generat-\ning the subsequent question in the sequence. ”.\nA.4 Effect of Demonstration Order\nDuring the experiment, we made a noteworthy ob-\nservation regarding the impact of demonstration\norder on the performance of our method. We con-\nducted a comprehensive exploration of various sort-\ning techniques, including uncertainty-based sort-\ning (Diao et al., 2023), random sorting, and sort-\ning based on the number of logical form jumps.\nThe detailed experimental results are presented in\nTable 11. It becomes evident that arranging the\ndemonstrations in ascending order of the number\nof logical form jumps leads to the most favorable\noutcomes. This finding highlights the structural\ncomplexity of logical forms when organizing the\ndemonstrations.\nMethod GQ\nB M R\nKQG-CoT+ 31.24 34.94 45.36\n(a) -Uncertainty 30.36 33.91 45.05\n(b) -Similarity 31.20 34.63 45.28\n(c) -Random 30.81 34.26 44.91\n(d) -L2s 30.52 33.66 44.83\nTable 11: The results of using different sorting methods\nfor demonstrations on the GQ dataset are as follows:\nOur KQG-CoT+ method is sorted in ascending order of\nthe number of logical form jumps. Random sorting is\ndone randomly. L2S sorting is performed in ascending\norder of length. Uncertainty sorting is based on de-\nscending order of uncertainty values. Lastly, similarity\nsorting is based on descending order of similarity values\nbetween the logical forms of demonstrations and tests.\n4341\nMethod WQ PQ\nB M R B M R\nKQG-CoT+ 29.73 31.08 55.14 61.71 44.27 78.41\n(a) w/o CoT 28.75 30.12 54.24 60.83 44.06 77.88\n(b) K-means →Random 25.02 29.37 53.16 56.42 42.61 77.03\n(c) w/o structure encoding 28.52 29.73 54.28 60.34 43.26 77.59\nTable 12: Ablation study of our KQG-CoT+ method on WQ and PQ.\nDemonstrations\nInput: (JOIN (R location.country.official_language) (JOIN location.country.languages_spoken romansh language))\nSubgraph1: (JOIN location.country.languages_spoken romansh language)\nSubgraph2: (JOIN (R location.country.official_language) Subgraph1)\nSubquestion1: country languages spoken romansh language\nSubquestion2: What is the main language spoken in the country that romansh language is used ?\n...\nInput: (AND (JOIN people.cause_of_death.parent_cause_of_death drug) (JOIN (R people.deceased_person.cause_of_death)\n(JOIN film.actor.film (JOIN film.performance.character julia biggs))))\nSubgraph1: (JOIN people.cause_of_death.parent_cause_of_death drug)\nSubgraph2: (JOIN film.performance.character julia biggs)\nSubgraph3: (JOIN film.actor.film Subgraph2)\nSubgraph4: (JOIN (R people.deceased_person.cause_of_death) Subgraph3)\nSubgraph5: (AND Subgraph1 Subgraph4)\nSubquestion1: parent cause of death drug\nSubquestion2: performance character julia biggs\nSubquestion3: film actor who performance julia biggs\nSubquestion4: cause of death of film actor who performance julia biggs\nSubquestion5: Which drugs caused the death of the actor who played julia biggs ?\nInput: (JOIN (R location.country.currency_used) (JOIN location.country.national_anthem (JOIN\ngovernment.national_anthem_of_a_country.anthem aruba dushi tera)))\nPrediction\nInput: (JOIN (R film.performance.actor) (AND (JOIN film.performance.character simon birch) (JOIN film.film.starring (JOIN\nfilm.performance.actor ian michael smith))))\nSubgraph1: (JOIN film.performance.character simon birch)\nSubgraph2: (JOIN film.performance.actor ian michael smith)\nSubgraph3: (JOIN film.film.starring Subgraph2)\nSubgraph4: (AND Subgraph1 Subgraph3)\nSubgraph5: (JOIN (R film.performance.actor) Subgraph4)\nSubquestion1: performance character simon birch\nSubquestion2: performance actor ian michael smith\nSubquestion3: performance actor ian michael smith star in\nSubquestion4: performance character simon birch the film that ian michael smith star in\nSubquestion5: Who plays simon birch in the movie that ian michael smith acted in ?\nTable 13: Prompt with demonstrations and prediction on WQ, the preceding section displays the prompt, and\nfollowed section displays the outputs generated by LLMs.\n4342\nDemonstrations\nInput: (ARGMIN base.exoplanetology.exoplanet astronomy.astronomical_discovery.discovery_date)\nSubgraph1: (ARGMIN base.exoplanetology.exoplanet astronomy.astronomical_discovery.discovery_date)\nSubquestion1: Which exoplanet was first to be found ?\n...\nInput: (AND digicams.digital_camera (AND (lt digicams.digital_camera.weight 250.0^^http://www.w3.org/2001/XMLSchema\n#float)(JOIN (R digicams.camera_viewfinder_type.digital_cameras) (JOIN digicams.camera_viewfinder_type.digital\n_cameras Sony Alpha 700))))\nSubgraph1: (lt digicams.digital_camera.weight 250.0^^http://www.w3.org/2001/XMLSchema#float)\nSubgraph2: (JOIN digicams.camera_viewfinder_type.digital_cameras Sony Alpha 700 )\nSubgraph3: (JOIN (R digicams.camera_viewfinder_type.digital_cameras) Subgraph2)\nSubgraph4: (AND Subgraph1 Subgraph3)\nSubgraph5: (AND digicams.digital_camera Subgraph4)\nSubquestion1: digital cameras that weight less than 250.0\nSubquestion2: viewfinder type digital cameras sony alpha 700\nSubquestion3: digital cameras the same viewfinder type as the sony alpha 700\nSubquestion4: digital cameras the same viewfinder type as the sony alpha 700 and weight less than 250.0\nSubquestion5: Are there any digital cameras that use the same viewfinder as the sony alpha 700 that weight less than 250.0?\nInput: (AND music.genre (JOIN (R music.genre.parent_genre) (JOIN music.genre.albums confessions tour)))\nPrediction\nSubgraph1: (JOIN music.genre.albums confessions tour)\nSubgraph2: (JOIN (R music.genre.parent_genre) Subgraph1)\nSubgraph3: (AND music.genre Subgraph2)\nSubquestion1: the music genre albums confessions tour\nSubquestion2: the albums confessions tour is part of what parent genre\nSubquestion3: The albums confessions tour is part of what parent genre of a musical genre?\nTable 14: Prompt and prediction on GQ, the preceding section is the prompt, and the blue text following it represents\nthe prediction.\nDemonstrations\nInput: (JOIN (R people.person.gender) (JOIN (R people.person.parents) sviatoslav ii of kiev))\nSubgraph1: (JOIN (R people.person.parents) sviatoslav ii of kiev)\nSubgraph2: (JOIN (R people.person.gender) Subgraph1)\nSubquestion1: sviatoslav ii of kiev ’s parents\nSubquestion2: What is the gender of sviatoslav ii of kiev ’s dad ?\n...\nInput: (JOIN (R people.deceased_person.place_of_death) (JOIN (R people.person.children) (JOIN (R people.person.children) p\nj kennedy)))\nSubgraph1: (JOIN (R people.person.children) p j kennedy)\nSubgraph2: (JOIN (R people.person.children) Subgraph1)\nSubgraph3: (JOIN (R people.deceased_person.place_of_death) Subgraph2)\nSubquestion1: p j kennedy ’s children\nSubquestion2: children of p j kennedy ’s children\nSubquestion3: What is the place of death of kid of p j kennedy ’s son ?\nInput: (JOIN (R music.recording.releases) (JOIN (R music.recording.tracks) o holy night))\nPrediction\nSubgraph1: (JOIN (R music.recording.tracks) o holy night)\nSubgraph2: (JOIN (R music.recording.releases) Subgraph1)\nSubquestion1: o holy night ’s tracks\nSubquestion2: What is the releases of recording of o holy night ’s tracks ?\nTable 15: Prompt and prediction on PQ, the preceding section is the prompt, and the blue text following it represents\nthe prediction.\n4343",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7747145891189575
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6173569560050964
    },
    {
      "name": "Generalization",
      "score": 0.6113830804824829
    },
    {
      "name": "Task (project management)",
      "score": 0.6087398529052734
    },
    {
      "name": "Artificial intelligence",
      "score": 0.588770866394043
    },
    {
      "name": "Natural language processing",
      "score": 0.5165611505508423
    },
    {
      "name": "Natural language understanding",
      "score": 0.47100889682769775
    },
    {
      "name": "Natural language",
      "score": 0.4642210006713867
    },
    {
      "name": "Knowledge base",
      "score": 0.43503665924072266
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4341656267642975
    },
    {
      "name": "Machine learning",
      "score": 0.3402644395828247
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}