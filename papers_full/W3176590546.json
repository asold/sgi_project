{
    "title": "Two-Stream Convolution Augmented Transformer for Human Activity Recognition",
    "url": "https://openalex.org/W3176590546",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2004947742",
            "name": "Bing Li",
            "affiliations": [
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A1970957467",
            "name": "Wei Cui",
            "affiliations": [
                "Institute for Infocomm Research",
                "Agency for Science, Technology and Research"
            ]
        },
        {
            "id": "https://openalex.org/A2009336559",
            "name": "Wei Wang",
            "affiliations": [
                "Dongguan University of Technology",
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2104362018",
            "name": "Le Zhang",
            "affiliations": [
                "Agency for Science, Technology and Research",
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2105162505",
            "name": "Zhenghua Chen",
            "affiliations": [
                "Agency for Science, Technology and Research",
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2118727056",
            "name": "Min Wu",
            "affiliations": [
                "Agency for Science, Technology and Research",
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2004947742",
            "name": "Bing Li",
            "affiliations": [
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A1970957467",
            "name": "Wei Cui",
            "affiliations": [
                "Institute for Infocomm Research",
                "Agency for Science, Technology and Research"
            ]
        },
        {
            "id": "https://openalex.org/A2009336559",
            "name": "Wei Wang",
            "affiliations": [
                "Dongguan University of Technology",
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2104362018",
            "name": "Le Zhang",
            "affiliations": [
                "Institute for Infocomm Research",
                "Agency for Science, Technology and Research"
            ]
        },
        {
            "id": "https://openalex.org/A2105162505",
            "name": "Zhenghua Chen",
            "affiliations": [
                "Agency for Science, Technology and Research",
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2118727056",
            "name": "Min Wu",
            "affiliations": [
                "Agency for Science, Technology and Research",
                "Institute for Infocomm Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6652820343",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W2899145720",
        "https://openalex.org/W2802660937",
        "https://openalex.org/W2743415265",
        "https://openalex.org/W2089677013",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W6767497195",
        "https://openalex.org/W2141336889",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W2958506234",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2340862004",
        "https://openalex.org/W2095396347",
        "https://openalex.org/W2594230123",
        "https://openalex.org/W2338892592",
        "https://openalex.org/W2981892815",
        "https://openalex.org/W2763219399",
        "https://openalex.org/W2762142193",
        "https://openalex.org/W2102205543",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2469690627",
        "https://openalex.org/W1985006367",
        "https://openalex.org/W2010882865",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W1983705368",
        "https://openalex.org/W2972317743"
    ],
    "abstract": "Recognition of human activities is an important task due to its far-reaching applications such as healthcare system, context-aware applications, and security monitoring. Recently, WiFi based human activity recognition (HAR) is becoming ubiquitous due to its non-invasiveness. Existing WiFi-based HAR methods regard WiFi signals as a temporal sequence of channel state information (CSI), and employ deep sequential models (e.g., RNN, LSTM) to automatically capture channel-over-time features. Although being remarkably effective, they suffer from two major drawbacks. Firstly, the granularity of a single temporal point is blindly elementary for representing meaningful CSI patterns. Secondly, the time-over-channel features are also important, and could be a natural data augmentation. To address the drawbacks, we propose a novel Two-stream Convolution Augmented Human Activity Transformer (THAT) model. Our model proposes to utilize a two-stream structure to capture both time-over-channel and channel-over-time features, and use the multi-scale convolution augmented transformer to capture range-based patterns. Extensive experiments on four real experiment datasets demonstrate that our model outperforms state-of-the-art models in terms of both effectiveness and efficiency.",
    "full_text": "Two-Stream Convolution Augmented Transformer for\nHuman Activity Recognition\nBing Li1, Wei Cui2\u0003, Wei Wang3,1\u0003, Le Zhang2, Zhenghua Chen2, Min Wu2\n1School of Computer Science and Engineering, University of New South Wales, Australia\n2Institute for Infocomm Research, Agency for Science, Technology and Research (ASTAR), Singapore\n3Dongguan University of Technology, China\n{bing.li, weiw}@unsw.edu.au, {cuiw, zhang_le, chen_zhenghua, wumin}@i2r.a-star.edu.sg\nAbstract\nRecognition of human activities is an important task due\nto its far-reaching applications such as healthcare system,\ncontext-aware applications, and security monitoring. Recent-\nly, WiFi based human activity recognition (HAR) is becom-\ning ubiquitous due to its non-invasiveness. Existing WiFi-\nbased HAR methods regard WiFi signals as a temporal se-\nquence of channel state information (CSI), and employ deep\nsequential models (e.g., RNN, LSTM) to automatically cap-\nture channel-over-time features. Although being remarkably\neffective, they suffer from two major drawbacks. Firstly, the\ngranularity of a single temporal point is blindly elementary\nfor representing meaningful CSI patterns. Secondly, the time-\nover-channel features are also important, and could be a natu-\nral data augmentation. To address the drawbacks, we propose\na novel Two-stream Convolution Augmented Human Activi-\nty Transformer (THAT) model. Our model proposes to uti-\nlize a two-stream structure to capture both time-over-channel\nand channel-over-time features, and use the multi-scale con-\nvolution augmented transformer to capture range-based pat-\nterns. Extensive experiments on four real experiment datasets\ndemonstrate that our model outperforms state-of-the-art mod-\nels in terms of both effectiveness and efﬁciency 1.\nIntroduction\nHuman activity recognition (HAR) aims to recognize vari-\nous human activities such as walking, sitting, falling down,\netc. It is widely used in many ﬁelds such as healthcare\nsystem (Zheng et al. 2017; Jobanputra, Bavishi, and Doshi\n2019), smart building solutions (Pu et al. 2013), and Internet\nof Things (IoT) applications. For examples, elderly people\nmonitoring (Jobanputra, Bavishi, and Doshi 2019), energy-\nefﬁciency system for smart buildings (Pu et al. 2013), daily\nactivity recognition for robot-assisted living (Zhu and Sheng\n2011). Other applications include fall detection (Wang, Wu,\nand Ni 2017), security monitoring (Zheng et al. 2017), res-\ncue systems (Grzonka et al. 2010), etc.\nDue to its importance, numerous research efforts are de-\nvoted to HAR in recent years, mostly rely on various sensors\nor dedicated devices, such as cameras (Aggarwal and Ryoo\n\u0003W. Wang and W. Cui are the corresponding authors.\nCopyright c\r 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1Code: https://github.com/windofshadow/THAT\nFigure 1: A human with different activities brings unique\nmulti-path reﬂections and refractions in wireless signals\nfrom a transmitter (TX) to a receiver (RX).\n2011), wearable sensors (Ertin et al. 2011), and radars (Lien\net al. 2016). However, these methods commonly require lim-\nited working conditions and special treatments on deploy-\nment and commissioning. E.g., cameras have limited view-\ning angles, illumination requirement, and needs line-of-sight\n(LOS) condition (no fog, smoke, or other obstacles). The re-\nliance on dedicated devices and limited working conditions\ngenerally leads to high costs yet low efﬁcacy, which hinders\ntheir applications from daily-life scenes.\nRecently, the research community has sought using off-\nthe-shelf devices to recognize human activities in a “device-\nfree” manner, primarily based on the WiFi, an ubiquitously\navailable device. WiFi-based solutions build upon the prin-\nciple that human actions between WiFi transmitters and re-\nceivers (as shown in Fig. 1) will incur subtle yet unique vari-\nations (a.k.a. multi-path and the fading effect) on WiFi sig-\nnals (Wang et al. 2015). There are two kinds of common-\nly used WiFi signal data: received signal strength (RSS)\nand channel state information (CSI). Due to being ﬁner-\ngrained and containing abundant environment information\nthat would beneﬁt HAR task, CSI becomes the de-facto s-\ntandard. However, different from RSS where the patterns are\nsimple and explicit, in CSI, the patterns are often implicit\nbecause they underneath the large amount of raw data (90\ntimes more than RSS data), leading to a high noise-feature\nratio. Thus, it is hard to obtain satisfactory results solely re-\nlying on extracting simple yet effective patterns.\nObserving this, pioneer research efforts have sought\nfor various deep learning (DL) techniques, such as CN-\nN (Chowdhury 2018), RNN (Youseﬁ et al. 2017), or LST-\nM (Chen et al. 2018), to automatically extract informative\nfeatures. Despite being effective, existing DL-based solu-\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n286\ntions share some common drawbacks, which hinder them\nfrom further performance improvement, as listed below:\ni) Order-unawareness. Spatial models such as CNN sim-\nply treat CSI data as an image and use two-dimensional ker-\nnels to extract spatial patterns. However, different from im-\nages where semantics of both dimensions are identical (i.e.,\nrefers to spatial positions), for CSI data, the meanings of the\ntwo dimensions are completely different (i.e., refers to tem-\nporal time-stamp and channel state, respectively). Simply\ntreating them equally will yield inferior features. Moreover,\nCNN is hard to effectively distinguishing order-sensitive ac-\ntivities such as sit-down and stand up.\nii) Neglect the temporal features extracted along the chan-\nnel dimension (time-over-channel). Sequential models such\nas RNN and LSTM regard CSI data as a temporal sequence.\nThey are aware of orders and able to capture channel fea-\ntures extracted along the temporal dimension (channel-over-\ntime). Unfortunately, they neglect the time-over-channel fea-\ntures, which are shown to be effective in distinguishing ac-\ntions with similar motions but different body postures, such\nas sit down and lie down, since the time-over-channel fea-\ntures are more sensitive to density changes as different chan-\nnel frequency has different penetration on human bodies.\niii) Granularity is too small. Existing models take every\npoint as the meaningful unit to generate features. The point-\nlevel granularity is too small to see the whole picture, since\na meaningful pattern usually appears in a continuous range\nrather than a single point due to the continuity2 of CSI data.\nTo address the above issues, we propose a sophisticat-\ned yet highly effective Two-stream Convolution Augment-\ned Human Activity Transformer (THAT) model to exert\nthe advances of deep learning techniques on HAR tasks.\nOur model presents a two-tower structure, each tower is in\ncharge of the channel stream or the temporal stream to ex-\ntract both time-over-channel and channel-over-time features.\nThe core component of our model is the Multi-scale Con-\nvolution Augmented Transformer (MCAT), which adopts a\nresidual-connected multi-head self-attention to effectively\ngenerate hidden representations and a multi-scale convo-\nlution block to capture range-based patterns. To make the\nmodel be order-sensitive, we propose a Gaussian range en-\ncoding to preserve the positional information of the CSI da-\nta. Besides, owing to the parallel nature of self-attention and\nconvolution, our model is also very time-efﬁcient. The ex-\nperimental results show that the proposed model not only\nachieves a very high accuracy (above 98.7%), but also is\n1.83\u00183.37\u0002faster than state-of-the-art sequential models.\nRelated Work\nCSI based human sensing methods mainly rely on data-\ndriven approaches to learn the complex relationship between\nwireless signals and human action (Wang et al. 2019). Tra-\nditional schemes for CSI-based activity recognition ﬁrstly\nextract discriminative and representative features from mul-\ntiple domains and then feed the extracted features into ma-\nchine learning models to predict certain activities as a multi-\n2Not only the temporal dimension, but the channel dimension\nis also continuous due to the continuity of channel frequency.\nMulti-head\nSelf-Attention\nAverage pooling\nRaw CSI data\nAdd & LayerNorm\nMulti-scale CNN\nwith Adaptive Scale \nAttention\nAdd & LayerNorm\nCNN with max-pooling\nMulti-head\nSelf-Attention\nAdd & LayerNorm\nMulti-scale CNN\nwith Adaptive Scale \nAttention\nAdd & LayerNorm\nTemporal \nStream\nChannel \nStream\nH\nCNN with max-pooling\nSoftmax\nPrediction\nLinear\nTemporal Module Channel Module\nAggregation Layer\nPrediction Layer\nAverage pooling\nTiming alignment Input and Pre-processing \nLayer\nMCAT Layer\nGaussian Range \nEncoding\nConcatenation\nFigure 2: Architecture of THAT Model.\nclass classiﬁcation problem. Some of the popular techniques\nare support vector machines (SVMs) (Wang et al. 2016),\nhidden markov model (HMM) (Wang et al. 2015), and k-\nnearest neighbor algorithm (kNN) (Ali et al. 2015).\nHowever, handcrafted solutions in previous works re-\nquire export-knowledge and are labor-intensive and time-\nconsuming. Recently, deep architectures contribute to the\nsigniﬁcant advancement in the ﬁelds of computer vision,\nnatural language processing, data analysis and so on. Some\nresearchers propose to use deep learning approaches, which\nare able to save that labor-intensive and time-consuming fea-\nture exploration procedure. Youseﬁ et al.proposed a deep\nlearning approach, i.e., LSTM, which could hold tempo-\nral state information of activities and help in extraction of\nimplicit features for similar activities (Youseﬁ et al. 2017).\nChen et al.argued that the conventional LSTM can only pro-\ncess the CSI measurements in the forward direction, which\nmay lead to some informative features loss (Chen et al.\n2018). Based on that, they proposed an attention based bidi-\nrectional long short-term memory (ABLSTM) approach for\nhuman activity recognition using WiFi CSI. In (Shi et al.\n2019), discriminative features for different human activi-\nties were extracted by LSTM with RNN and then were in-\nputted to a softmax classiﬁer for activity recognition. Gao\n287\net al.developed a CSI-based sparse auto-encoder (SAE) net-\nwork for device free wireless localization and activity recog-\nnition (Gao et al. 2017). Maet al.(Ma et al. 2018) proposed a\ndeep learning framework based on convolutional neural net-\nwork (CNN) for sign gesture recognition using CSI, which\nachieved higher accuracy and efﬁciency than the non-deep-\nlearning baselines.\nModel\nModel Overview\nThe architecture of our THAT model is summarized in Fig-\nure 2. From bottom to top, the ﬁrst layer inputs the raw CSI\ndata and pre-processes it into temporal and channel streams.\nThe MCAT layer extracts discriminative features from the\ntwo-stream. Finally, the two-stream features are aggregated\nand fed it into the prediction layer for the ﬁnal output.\nInput and Pre-processing Layer receives the raw CSI da-\nta as inputs and resizes it into temporal stream and channel\nstream to be fed as the input of MCAT layer. The raw CSI\ndata is a set of records fr1;r2;:::;r ng. Each record ri is a\ntwo-dimensional matrix, in which a cell vtc 2ri is a real\nvalue indicating the state value of channel c at time t. To\nfacilitate batch-processing, we perform a timing alignmen-\nt3 to make the size of each record identical, i.e., ri has the\nsame dimensionality as T\u0002C. To be less memory-intensive,\nwe shrink the size of temporal dimension T by preforming\na mean-pooling on adjacent time slots. The temporal dimen-\nsion ﬁrst (with dimensionality T\u0002C) data is used as tempo-\nral stream. The channel dimension ﬁrst (with dimensionality\nC\u0002T) data is used as channel stream, which could be read-\nily fetched by a simple transpose operation.\nMCAT Layer extracts discriminative features from the t-\nwo stream data. This layer manifests a two-tower struc-\nture. Each tower is a stack of H-layer (for temporal stream,\nand N-layer for channel stream) multi-scale convolution\naugmented transformers. The two towers separately extract\nchannel-over-time features and time-over-channel features.\nNotably, if H = 0or N = 0, the model is degenerated to a\nsingle-stream model.\nAggregation Layer receives features of the temporal stream\nand channel stream and aggregates them into ﬁxed-length\nvectors via two separate Convolutional blocks (CNN). Then,\ntemporal and channel vectors are concatenated to be fed as\nthe input of the prediction layer.\nPrediction Layer is a linear layer with a softmax operation\nto compute the probabilities over activity categories.\nMulti-scale Convolution Augmented Transformer\n(MCAT)\nMCAT consists of two sequentially stacked sub-layers: (1)\na multi-head self-attention, and (2) a multi-scale CNN with\nan adaptive scale. Each of the two sub-layers is encompassed\nby a residual connection (He et al. 2016) (Add) and a layer\nnormalization (Ba, Kiros, and Hinton 2016) (LayerNorm).\n3We evenly split a time range into time slots and map each row\ninto their slot according to their time-stamps.\n...\n...\n...\n...\n...\n...\n...\n...\n...\nKernel size =1\ni\nKernel size =3\nKernel size =7\nFeed-forward \nNeural Network\nMulti-scale features \nof position i\nSoftmax\nEnsembled features\n...\n...\n...\n...\nInput feature matrix\nFigure 3: An example of generating ensembled features for\nposition ivia multi-scale convolutional blocks. The top part\nshows convolutions with kernel size 1, 3, and 7, and the bot-\ntom part shows an adaptive scale attention.\nMulti-head Self-attention Module We employ a multi-\nhead self-attention mechanism (Vaswani et al. 2017) to\ngenerate hidden representations from inputs. Self-attention\ncould directly integrate the information within the entire se-\nquence; thus could fully overcome the long-range dependen-\ncy problem of RNN and LSTM.\nThe self-attention outputs a weighted sum of the values,\nwhere weight assigned to each value is computed by the dot-\nproduct of the query with the corresponding key. Formally,\nit is deﬁned as:\nAttention(Q;K;V ) = softmax(QKT\npdk\n)V; (1)\nwhere dk is the queries and keys dimension, and pdk is a\nscaling factor that enables smoother gradients. Q2RL\u0002dk ,\nK 2RL\u0002dk , and V 2RL\u0002dv is queries, keys, and values,\nrespectively. They are generated by letting the input X 2\nRL\u0002din go through three linear projections, namely,\nQ= XWQ; K= XWK; V= XWV (2)\nwhere WQ 2 Rdin\u0002dk , WK 2 Rdin\u0002dk , and WV 2\nRdin\u0002dv are projection parameters.\nIn order to jointly attend to information from different\nrepresentation subspaces, the queries, keys and values are\nindependently projected htimes (called h-head) with differ-\nent projection parameters, and the outputs of projections are\nconcatenated and projected again to result the ﬁnal output:\nMultiHead(Q;K;V ) = [head1; :::; headh]WO; (3)\nwhere headi = Attention( XWi\nQ;XWi\nK;XWi\nV), and\nWO 2Rhdv\u0002do is the ﬁnal projection matrix.\nTo facilitate residual connections, all sub-layers in the M-\nCAT, as well as the input layer, share identical dimension,\ni.e., din = h\u0003dv = do.\n288\nMulti-scale Convolutional Block with Adaptive Scale At-\ntention For HAR tasks, a single temporal point is too s-\nmall to be informative. To capture features in a lager scope,\nwe propose multi-scale convolutional blocks that use differ-\nent scale kernel sizes and adaptively adjust between different\nscale by an adaptive scale attention mechanism.\nGiven a set of kernel sizes J = j1;j2;:::;j jJj, each ker-\nnel size is related to a speciﬁc scale, e.g., a 30 kernel size\nenables the model to capture the pattern within a temporal\nrange of 30 ms. The output of multiple convolutional blocks\nis P = P1;P2;:::;P jJj, where Pi 2RL\u0002do is the feature\nmatrix regarding the i-th kernel size ji. Pi is deﬁned as:\nPi = ReLU(Dropout(BN(Conv(Wji; X)))) (4)\nEq. 4 consists of four cascaded operations: a convolu-\ntion (Conv( \u0001)), a batch normalization (BN( \u0001)) (Ioffe and\nSzegedy 2015), a Dropout operation (Dropout( \u0001)) (Srivas-\ntava et al. 2014), and a ReLU unit. The core operation is\nConv(Wji; X), which receives the output features X of\nthe multi-head self-attention module along with learnable\nweights Wji 2Rdo\u0002ji\u0002do. The Wji consists of do ﬁlters,\nand each ﬁlter has the size of ji\u0002do, convolving ji adjacent\npositions. Notice that we use zero-padding to the two sides\nof Xby b(ji\u00001)=2cto make the resulting feature map have\nthe same size as L\u0002do. Fig. 3 shows an example of us-\ning scales 1;3;7 to capture pattern within different ranges to\nyield three features vectors.\nNotably, it is unwise to use all those features since some\nof them are either redundant or less informative. In order to\nadaptively ensemble features at different scales, we propose\nan adaptive scale attention mechanism, as shown in Fig. 3.\nFormally, the ensembled features Pens are deﬁned as:\nPens = \u000bP =\nX\ni\n\u000biPi; (5)\nwhere \u000bi is an attention score of Pi at scale ji. \u000b =<\n\u000b1;\u000b2;:::;\u000b jJj > is the vector of attention scores, which\ncan be computed by:\n\u000b = softmax(FFN(P)); (6)\nwhere FFN(\u0001) is a two-layer fully connected feed-forward\nneural network:\nFFN(Pi) = ReLU(PiWT\n1 + b1)WT\n2 + b2; (7)\nwhere W1 2dh \u0002do and W2 21 \u0002dh are parameter ma-\ntrices with a hidden dimension dh, and b1 and b2 are biases.\nPreserve Order Information by Gaussian Range En-\ncoding The order information is vitally important in i-\ndentifying reverse actions such as sit down and stand\nup.Unfortunately, existing positional encodings methods,\ne.g., absolute encodings (Vaswani et al. 2017) or relative\nencodings (Shaw, Uszkoreit, and Vaswani 2018) are all de-\nﬁned on a single point: they assign a unique and highly-\ndiscriminative encoding for each single point. As we argued,\na meaningful unit should be a range rather than a single\npoint. Simply encoding each point will turn out to be a kind\nof noise, and lead to generally worse results. Thus, we wish\nto use range-based encodings instead.\nPosition 140\nPDF vector\nK univariate Gaussian distributions\n0.17 ...0.09 0.01 0.00 0.62 ...0.33 0.03 0.00\n...\n...\n...\n...\n...\n...\n...\n...\n...\nK learnable range embeddings\nGaussian Range Encoding\n...\n...\nNormalized weight vector\nFigure 4: An example of generating Gaussian range encod-\ning for one of positions. The ﬁnal Gaussian range encoding\nis the multiplication of the K learnable range embeddings\nand normalized PDF vector of KGaussian distributions.\nTo make the encoding be ﬂexible to different start/end\nblank periods or subtle action speed changes, we propose\na Gaussian range encoding that allows a position belonging\nto multiple ranges at the same time, and dynamically adjust\nproportions of different ranges during the training.\nWithout loss of generality, we assume there are K differ-\nent ranges, and use random variable zij to denote the oc-\ncurrence of position i belonging to the j-th range. We as-\nsume zij is drawn from a Gaussian distribution N(\u0016j;\u001bj)\n(i.e., zij \u0018 N(\u0016j;\u001bj)) with probability pj(i). As such,\na position i is a mixture of all K ranges, and the dis-\ntribution over ranges are pi =< p1(i)\n\u0010 ;p2(i)\n\u0010 ;:::; pK(i)\n\u0010 >\nwhere \u0010 is a normalization factor. Given the values vector\nv=<v1;v2;:::;v K >, vj is the value of the j-th range. The\nexpectation of position iis pivT.\nAll \u0016;\u001b, and vare unobserved hidden variables, and thus\nthey cannot be directly estimated through maximum like-\nlihood estimation (MLE). Fortunately, we can implicitly\nperform an expectation-maximization by neural networks’\nback-propagation. We set \u0016;\u001b, and v as randomly initial-\nized free parameters, and dynamically update them with the\ntraining of the whole THAT model.\nFormally, let \f 2RL\u0002K be the normalized weights over\nKGaussian distributions, which is deﬁned as\n\f = softmax(B); (8)\nwhere B 2RL\u0002K is a weight matrix, in which each cell bij\nindicates the weight of the j-th Gaussian distributions for\nposition i. bij is formally deﬁned as:\nbij = \u0000(i\u0000\u0016(j))2\n2\u001b(j)2 \u0000log(\u001b(j)); (9)\n289\nwhere \u0016(j) and \u001b(j) are learnable parameters that indicate\nthe mean and standard deviation for j-th Gaussian distribu-\ntions, respectively.\nFinally, the range-biased stream X0 is generated by\nadding the range encodings to the original stream X:\nX0= X+ \fE; (10)\nwhere E 2RK\u0002din is a set of learnable range encodings.\nWe can see that E corresponds to v. \f corresponds to pi,\nwhich can be proved that it is equivalent to computing L1-\nnormalized Kunivariate Gaussian probability density func-\ntions (PDFs).\nFig. 4 shows an illustrative examples of the range encod-\ning. For position 140, we can get the normalized PDF vector\nfor K different Gaussian distributions, and each element in\nthe vector indicates the proportion the corresponding range.\nThe ﬁnal encoding is the multiplication of the range embed-\ndings and their corresponding proportions.\nAggregation Layer\nAssume that the features extracted from temporal and chan-\nnel streams have the size ofT\u0002dt and C\u0002dc, respectively.\nIn order to be amenable for the prediction layer, it is neces-\nsary to aggregate the features into a small ﬁxed length vector.\nThe aggregation layer aggregates each feature matrix using\na CNN with a max-pooling operation:\nuX = v(WX;X)\n= ReLU(Dropout(Pooling(Conv(WX; X))));\n(11)\nwhere v(\u0001) consists of four cascaded operations: a convo-\nlution, a 1-max-over-time pooling operation (Kim 2014), a\nDropout operation, and a ReLU unit. The convolutional lay-\ner has wkernel sizes and lﬁlters, and each ﬁlter has the size\nof h\u0002d(hdenotes the kernel size). Further, we use a1-max-\npooling operation to select the largest value over the feature\nmap of a particular kernel to capture the most important fea-\nture, and it also helps to shrink the feature size to the kernel\nnumber. The output vector uX has a ﬁxed size 1 \u0002wl.\nThe ﬁnal feature vector U 2R1\u0002(wt+wc) is generated by\nconcatenating the two output vectors:\nU = [uT; uC]: (12)\nThe ﬁnal feature vector U is fed to the prediction layer to\nidentify different activities.\nLoss Function\nThe loss function Lis a standard cross-entropy loss (CE-\nloss), which is deﬁned as:\nL= \u0000\njAjX\ni\nyilog(f(\u0012; <XT; XC >)); (13)\nwhere yi is the ground-truth label, and f(\u0001) denotes the pre-\ndicted distribution of the ﬁnal prediction layer of THAT\nmodel. XT and XC are input temporal and channel streams.\njAjis the number of different activity categories.\nDatasets # Rec. # Activ. Freq. TX-RX Dist.\nOfﬁce Rm 140 7 1K Hz 3m\nActivity Rm 600 6 500 Hz 4.5m\nMeeting Rm 600 6 500 Hz 2m\nActiv.+Meet. 1200 6 500 Hz 4.5m/2m\nTable 1: Statistics of the four evaluation datasets.\nMethods Ofﬁce Activity Meeting Activ.+Meet.\nS-RF 75.3 80 84.7 82.6\nS-HMM 79.7 75 83.4 80.5\nLSTM 91.4 89.7 90.6 90.1\nCNN 96.4 94.3 96.2 95.4\nABLSTM 97.1 95.6 96.8 95.9\nTHAT 98.2 98.4 99.0 98.6\nTable 2: The recognition accuracy (%) comparison of THAT\nand baselines on the four evaluation datasets.\nExperimental Evaluation\nIn this section, we evaluate the performance of the pro-\nposed THAT model on human activity recognition tasks, and\ndemonstrate its superiority over state-of-the-art models.\nEvaluation Datasets\nWe used four datasets for evaluation, i.e., Ofﬁce Room 4,\nActivity Room, Meeting Room, and Activity+Meeting. The\nﬁrst dataset is publicly available, and the last three are col-\nlected by our prototype system. Each dataset is a set of CSI\nmatrices along with their corresponding ground-truth labels.\nTheir brief statistics are summarized in Table 1.\nExperimental Settings and Evaluation Metrics\nThe strides of average pooling were set to 4 for temporal\nstream and 3 for channel stream. The number of Gaussian\ndistributions was K = 10. The \u0016s were evenly distributed\namong temporal dimension, namely, beginning from 25 and\nending with 475 with the step 50. All \u001bs were set to 8. The\nnumber of stacks for temporal module wasH = 5and chan-\nnel module was N = 1; The dimensionalitydin = dk = do\nwas set to 90 and 500, the number of heads h were set\nto 9 and 200, and dv = do=h. The dropout rate was 0:1.\nFor temporal module and channel module, the kernel sizes\nwere f1;3;5gand f1;2;3g, the hidden dimensions dh were\n360 and 4000. For temporal and channel module, the ker-\nnel numbers wwere set to 128 and 16, and the kernel sizes\nl were set to f10;40gand f2;4g. The dropout rate for this\nlayer was set to 0:5.\nThe model was implemented using Pytorch 1.4 with\nPython 3.6, and trained on a Nvidia 1080Ti GPU. For op-\ntimization, we used Adam (Kingma and Ba 2014) with an\ninitial learning rate 0:001. All weight parameters were ini-\ntialized using Xavier (Glorot and Bengio 2010). All datasets\nadopt 8:1:1 train/dev/test split. The batch size was 16. We\n4https://github.com/ermongroup/Wiﬁ_Activity_Recognition\n290\nEnvironment Methods Lie down Fall Pick up Run Sit down Stand up Walk Average\nOfﬁce Room\nS-RF 65.1 82.2 85.5 83.8 59.8 60.7 89.8 75.3\nS-HMM 62.9 86.7 89.2 94.2 73.4 59.9 92.0 79.8\nLSTM 95.0 92.8 98.1 96.9 82.4 82.2 93.4 91.6\nCNN 97.4 97.5 98.0 98.5 90.2 93.8 99.0 96.3\nABLSTM 96.4 98.9 97.6 97.8 94.9 96.5 96.6 97.0\nTHAT 96.4 99.0 98.9 98.7 97.4 99.9 98.4 98.4\nEnvironment Methods Jump Bow Run Sit down Wave hand Walk Average\nActivity Room\nS-RF 66.1 71.2 88.3 78.2 87.1 88.8 80.0\nS-HMM 36.8 47.9 92.7 90.8 89.8 91.7 75.0\nLSTM 88.1 81.8 90.6 90.9 94.7 91.9 89.7\nCNN 94.1 90.4 93.8 96.5 97.2 93.8 94.3\nABLSTM 93.4 93.9 95.5 96.6 96.5 97.9 95.6\nTHAT 98.8 98.7 96.7 98.7 98.7 98.9 98.4\nMeeting Room\nS-RF 84.6 88.7 81.1 82.1 81.4 90.5 84.7\nS-HMM 67.1 77.2 89.1 82.8 90.6 93.6 83.4\nLSTM 89.6 87.2 90.5 91.6 93.7 90.7 90.6\nCNN 96.2 92.2 95.8 98.4 98.3 96.2 96.2\nABLSTM 96.5 98.2 97.1 98.5 92.8 97.6 96.8\nTHAT 99.3 99.8 97.3 99.7 99.2 98.4 99.0\nActivity+Meeting\nS-RF 70.9 82.8 88.7 85.6 81.7 85.9 82.6\nS-HMM 50.9 66.8 88.7 88.8 89.7 97.8 80.5\nLSTM 78.8 91.9 96.9 92.1 89.2 91.7 90.1\nCNN 93.1 95.2 97.3 95.9 97.1 93.6 95.4\nABLSTM 93.6 94.5 97.1 97.4 95.8 97.1 95.9\nTHAT 99.0 99.2 97.4 99.6 97.6 98.7 98.6\nTable 3: Categorical performance comparison on Ofﬁce Room, Activity Room, Meeting Room, and Activity+Meeting datasets.\nran the model for a maximum of 50 epochs and selected the\nbest on validation set for testing.\nEvaluation Metrics Following the common practices, we\nadopt recognition accuracy (i.e., the proportion of correctly\nrecognized activities among all predictions) as the metric.\nBaselines\nWe compared with ﬁve state-of-the-art models: two feature-\nbased models S-RF (Youseﬁ et al. 2017) and S-HMM (Wang\net al. 2017), and three DL-based models CNN, LSTM (Y-\nouseﬁ et al. 2017), and ABLSTM (Chen et al. 2018).\nMain Results\nTable 2 lists the recognition accuracy of the proposed\nTHAT model compared with baselines on the four evalua-\ntion datasets. From Table 2, we can see that:\ni) Our model THAT excels all baselines signiﬁcantly,\nachieving new state-of-the-art results on these datasets. Our\nmodel outperforms the best previous model ABLSTM by\n1:1 pts (percentage points), 2:8 pts, 2:2 pts, and 2:7 pts on\nthe four datasets, respectively. This demonstrates our convo-\nlution augmented transformer and the two-stream structure\ndo truly achieve a better performance on HAR tasks.\nii) The performance gaps between feature-based mod-\nels (S-RF and S-HMM), and deep-neural models (LST-\nM, CNN, ABLSTM, and THAT) are huge. Among all the\nmodels, S-HMM has the worst performance (on average\n79:65% accuracy), and S-RF is only slightly better than S-\nHMM (80:65%). For DL-baseed models, LSTM performs\nthe worst since it suffers greatly from the long-range de-\npendency problem. Being able to adaptively integrate hid-\nden states by a attention mechanism, ABLSTM achieved a\nbetter results (around 6 pts) than traditional LSTM model\nand became the best among baselines. All DL-based models\nachieved at least 10 pts average improvements over feature-\nbased models. Compared with the best feature-based model,\nour model achieved 18:6 pts, 18:4 pts, 14:3 pts, and 16 pt-\ns improvements. This is because deep-neural models are of\nhigh expressiveness, and can automatically and adaptively\nextract useful features from raw data.\niii) Our model performed equally well on different work-\ning scenarios. On the ﬁrst three single-scenario datasets, the\ndifference is only 0:6 pt. For hybrid-scenario dataset Activ-\nity+Meeting, the performance of THAT model is commen-\nsurately good compared to that under single-scenario and\nexcels other models signiﬁcantly. This demonstrates the ef-\nfectiveness of THAT on hybrid-scenario. Another observa-\ntion is that other DL-based models (i.e., LSTM, CNN, and\nABLSTM) perform slightly worse on Activity Room than\nother two. We believe it mainly caused by the TX-RX dis-\ntance, which is larger than others in Activity Room (4.5m\nv.s. 2m and 3m). It may weaken the received strength and\nentail more noise. Thus, our model is of high robustness a-\ngainst different working scenarios.\nCategorical Performance Comparison\nTable 3 shows the categorical performance comparison be-\ntween THAT and baselines on the four evaluation datasets.\nWe can see that our model outperformed almost all base-\n291\nModel Ofﬁce Room Activity Room Meeting Room Activity+Meeting\nAccuracy \u0001 Accuracy \u0001 Accuracy \u0001 Accuracy \u0001\nTHAT 98.2 - 98.4 - 99.0 - 98.6 -\n- Gaussian Range Encoding 97.9 -0.4 97.9 -0.5 98.5 -0.5 98.2 -0.4\n- Gaussian Range Encoding (+ PE) 91.1 -7.2 84.3 -14.1 90.3 -8.7 87.8 -10.8\n- Multi-scale CNN 95.3 -3.0 97.4 -1.0 97.0 -2.0 97.3 -1.3\n- Multi-scale CNN (+ PFFN) 97.2 -1.1 97.2 -1.2 98.1 -0.9 97.7 -0.9\n- Temporal Module 92.0 -6.3 95.2 -3.2 97.5 -1.5 95.9 -2.7\n- Channel Module 93.8 -4.5 97.7 -0.7 98.3 -0.7 98.0 -0.6\nTable 4: Ablation study results compared with the full THAT model. The accuracy is shown in percentage (%).\nlines in identifying any activity on all the four datasets. This\ndemonstrates our model is effective in capturing general pat-\nterns rather than just taking advantages of the “biases” of\nsome speciﬁc activities.\nAnother observation is that, LSTM and CNN suffer more\ndifﬁculties in recognizing reverse activities sit down and s-\ntand up because they need position or order information.\nOwing to involving an attention mechanism, ABLSTM has\nan improvement over LSTM. Our model performs signif-\nicantly better than both scope-based model CNN (5.5 pts)\nwhich does not preserve positional information and sequen-\ntial model ABLSTM (1.5 pts) that focuses on a ﬁxed scale.\nThis demonstrates our proposed Gaussian range encoding\nand multi-scale CNN block could preserve order informa-\ntion and are able to capture multi-scale features.\nAblation Test\nWe conducted an ablation study to evaluate the contributions\nof the proposed Gaussian range encoding, multi-scale CNN,\nand the two-stream structure. Table 4 represents the results.\nIn each test, we ablated a speciﬁc component from the ful-\nl model. Notably, we made two further testes by replacing\nthe ablated component with an alternative component: the\nﬁrst used positional encoding proposed in (Vaswani et al.\n2017) to replace our Gaussian range encoding (i.e., - Gaus-\nsian Range Encoding (+ PE)), the second used a position-\nwise feed-forward neural network to replace our multi-scale\nCNN (i.e., - Multi-scale CNN (+ PFFN)).\nFrom Table 4, we can see that position/order informa-\ntion does help a better recognition. The ablation on Gaus-\nsian range encoding incurs roughly 0.5 pt accuracy de-\ncline. Interestingly, the results of using positional encoding\nPE (Vaswani et al. 2017) are even worse than without any\npositional encoding. This is caused by the over-positionality\nthat the encodings assigned to each single position turn out\nto be an interference.\nThe ablation on multi-scale CNN incurs a 1.83 pts accu-\nracy decline, compared with using PFFN, still 1 pt perfor-\nmance gap. This demonstrates the multi-scale pattern cap-\ntured by the CNN is critical, especially on datasets (e.g., Of-\nﬁce Room) that have a long time range.\nAnother observation is, compared with single-stream\nmodel, the two-stream structure could effectively improve\nthe model performance. This indicates the two-stream could\nbe complementary to improve the recognition accuracy. A-\nmong the two streams, the ablation on temporal module in-\nModels Training (Sec) Testing (Sec) Recs/Sec\nS-RF 6.09 0.016 31250\nS-HMM 0.029 0.22 2272.72\nLSTM 5168.86 4.39 113.9\nCNN 1474.76 1.12 446.43\nABLSTM 11352.82 6.77 73.86\nTHAT 2996.75 1.55 332.58\nTable 5: Empirical execution time on Activity Rm dataset.\ncurs a bigger decline, which is in accord with common sens-\nes that temporal features are more intuitive and important.\nFrom ablation test, we can conclude that all these compo-\nnents contribute signiﬁcantly to the model performance.\nEmpirical Efﬁciency\nTo validate the time-efﬁciency of THAT, we show the em-\npirical execution time on Activity Room dataset in Table 5.\nFrom Table 5, we can see that all DL-based models\n(i.e., LSTM, CNN, ABLSTM, and THAT) are more time-\nconsuming than traditional machine learning models (i.e.,\nS-RF, S-HMM). Our THAT model (and CNN) has a better\ntime-efﬁciency than sequential models LSTM and ABLST-\nM, since most of the computations (e.g., multi-head self-\nattention module, all convolution modules) can be comput-\ned in parallel. The high time costs in training phase would\nnot be a big issue since it can be computed ofﬂine. In test-\ning phase, our model is 3.37\u0002faster than ABLSTM, 1.83\u0002\nfaster than LSTM, and competitive to CNN. The through-\nput rate of THAT is 332.58 Recs/Sec, indicating each record\ncan be inferred within 4ms. Considering the window size of\neach record is 4 seconds, the time cost of our model could\nbe fully ignored. This demonstrates our model is not only\nof high effectiveness, but also efﬁcient enough for real-time\nWiFi-based human activity recognition.\nConclusion\nIn this paper, we propose a novel neural network for device-\nfree HAR tasks. Our model extracts both time-over-channel\nand channel-over-time features, and uses Gaussian range\nencoding and a multi-scale convolution block to capture\nrange patterns. The experimental results show that, com-\npared with the best previous model ABLSTM, our model\ndelivers an average 2.2 pts accuracy improvement, while be-\ning 1.83\u00183.37\u0002faster.\n292\nAcknowledgments\nWei Wang is supported by ARC DPs 170103710 and\n180103411, and D2DCRC DC25002 and DC25003. Wei\nCui is supported by NSFC Grant 6190323.\nReferences\nAggarwal, J. K.; and Ryoo, M. S. 2011. Human activity anal-\nysis: A review. ACM Computing Surveys (CSUR) 43(3): 16.\nAli, K.; Liu, A. X.; Wang, W.; and Shahzad, M. 2015.\nKeystroke recognition using wiﬁ signals. In Proceedings of\nthe 21st Annual International Conference on Mobile Comput-\ning and Networking, 90–102.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer normal-\nization. NIPS .\nChen, Z.; Zhang, L.; Jiang, C.; Cao, Z.; and Cui, W. 2018.\nWiFi CSI based passive human activity recognition using at-\ntention based BLSTM. IEEE Transactions on Mobile Com-\nputing 18(11): 2714–2724.\nChowdhury, T. Z. 2018. Using Wi-Fi channel state informa-\ntion (CSI) for human activity recognition and fall detection.\nPh.D. thesis, University of British Columbia.\nErtin, E.; Stohs, N.; Kumar, S.; Raij, A.; Al’Absi, M.; and\nShah, S. 2011. AutoSense: unobtrusively wearable sensor\nsuite for inferring the onset, causality, and consequences of\nstress in the ﬁeld. In Proceedings of the 9th ACM Conference\non Embedded Networked Sensor Systems, 274–287. ACM.\nGao, Q.; Wang, J.; Ma, X.; Feng, X.; and Wang, H. 2017. CSI-\nbased device-free wireless localization and activity recogni-\ntion using radio image features. IEEE Transactions on Vehic-\nular Technology 66(11): 10346–10356.\nGlorot, X.; and Bengio, Y . 2010. Understanding the difﬁculty\nof training deep feedforward neural networks. InProceedings\nof the thirteenth international conference on artiﬁcial intelli-\ngence and statistics, 249–256.\nGrzonka, S.; Dijoux, F.; Karwath, A.; and Burgard, W. 2010.\nMapping indoor environments based on human activity. In\n2010 IEEE International Conference on Robotics and Au-\ntomation, 476–481. IEEE.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 770–\n778.\nIoffe, S.; and Szegedy, C. 2015. Batch Normalization: Accel-\nerating Deep Network Training by Reducing Internal Covari-\nate Shift. In International Conference on Machine Learning,\n448–456.\nJobanputra, C.; Bavishi, J.; and Doshi, N. 2019. Human ac-\ntivity recognition: a survey. Procedia Computer Science 155:\n698–703.\nKim, Y . 2014. Convolutional neural networks for sentence\nclassiﬁcation. arXiv preprint arXiv:1408.5882 .\nKingma, D. P.; and Ba, J. 2014. Adam: A method for stochas-\ntic optimization. arXiv preprint arXiv:1412.6980 .\nLien, J.; Gillian, N.; Karagozler, M. E.; Amihood, P.; Schwe-\nsig, C.; Olson, E.; Raja, H.; and Poupyrev, I. 2016. Soli: U-\nbiquitous gesture sensing with millimeter wave radar. ACM\nTransactions on Graphics (TOG) 35(4): 142.\nMa, Y .; Zhou, G.; Wang, S.; Zhao, H.; and Jung, W. 2018.\nSignﬁ: Sign language recognition using wiﬁ. Proceedings of\nthe ACM on Interactive, Mobile, Wearable and Ubiquitous\nTechnologies 2(1): 1–21.\nPu, Q.; Gupta, S.; Gollakota, S.; and Patel, S. 2013. Whole-\nhome gesture recognition using wireless signals. In Proceed-\nings of the 19th annual international conference on Mobile\ncomputing & networking, 27–38. ACM.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-Attention\nwith Relative Position Representations. In NAACL-HLT.\nShi, Z.; Zhang, J. A.; Xu, R.; and Cheng, Q. 2019. Deep learn-\ning networks for human activity recognition with csi correla-\ntion feature extraction. In ICC 2019-2019 IEEE International\nConference on Communications (ICC), 1–6. IEEE.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to preven-\nt neural networks from overﬁtting. The journal of machine\nlearning research 15(1): 1929–1958.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998–6008.\nWang, H.; Zhang, D.; Wang, Y .; Ma, J.; Wang, Y .; and Li,\nS. 2016. RT-Fall: A real-time and contactless fall detection\nsystem with commodity WiFi devices. IEEE Transactions on\nMobile Computing 16(2): 511–526.\nWang, W.; Liu, A. X.; Shahzad, M.; Ling, K.; and Lu, S. 2015.\nUnderstanding and modeling of wiﬁ signal based human ac-\ntivity recognition. In Proceedings of the 21st annual interna-\ntional conference on mobile computing and networking, 65–\n76.\nWang, W.; Liu, A. X.; Shahzad, M.; Ling, K.; and Lu, S.\n2017. Device-free human activity recognition using commer-\ncial WiFi devices. IEEE Journal on Selected Areas in Com-\nmunications 35(5): 1118–1131.\nWang, Y .; Wu, K.; and Ni, L. M. 2017. Wifall: Device-free\nfall detection by wireless networks. IEEE Transactions on\nMobile Computing 16(2): 581–594.\nWang, Z.; Jiang, K.; Hou, Y .; Dou, W.; Zhang, C.; Huang, Z.;\nand Guo, Y . 2019. A Survey on Human Behavior Recognition\nUsing Channel State Information. IEEE Access 7: 155986–\n156024.\nYouseﬁ, S.; Narui, H.; Dayal, S.; Ermon, S.; and Valaee, S.\n2017. A survey on behavior recognition using wiﬁ channel\nstate information. IEEE Communications Magazine 55(10):\n98–104.\nZheng, X.; Wang, J.; Shangguan, L.; Zhou, Z.; and Liu, Y .\n2017. Design and implementation of a CSI-based ubiquitous\nsmoking detection system. IEEE/ACM Transactions on Net-\nworking .\nZhu, C.; and Sheng, W. 2011. Wearable sensor-based hand\ngesture and daily activity recognition for robot-assisted liv-\ning. IEEE Transactions on Systems, Man, and Cybernetics-\nPart A: Systems and Humans 41(3): 569–573.\n293"
}