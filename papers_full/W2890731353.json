{
    "title": "Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder",
    "url": "https://openalex.org/W2890731353",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2095728525",
            "name": "Yunsu Kim",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A2310811917",
            "name": "Jiahui Geng",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A2090846850",
            "name": "Hermann Ney",
            "affiliations": [
                "RWTH Aachen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2740514420",
        "https://openalex.org/W4298393544",
        "https://openalex.org/W2594021297",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W2740132093",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2741986357",
        "https://openalex.org/W2140406733",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963602293",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2741602058",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2778814079",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2169153112",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2172267041",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W4299579390",
        "https://openalex.org/W2121745180",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963804993",
        "https://openalex.org/W2153653739",
        "https://openalex.org/W2963118869",
        "https://openalex.org/W3082674894",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W4241645538",
        "https://openalex.org/W2952190837",
        "https://openalex.org/W2963506925"
    ],
    "abstract": "Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper, we propose simple yet effective methods to improve word-by-word translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance, which provides better understanding of learning the cross-lingual word embedding and its usage in translation.",
    "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 862–868\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n862\nImproving Unsupervised Word-by-Word Translation\nwith Language Model and Denoising Autoencoder\nYunsu Kim Jiahui Geng Hermann Ney\nHuman Language Technology and Pattern Recognition Group\nRWTH Aachen University\nAachen, Germany\n{surname}@cs.rwth-aachen.de\nAbstract\nUnsupervised learning of cross-lingual word\nembedding offers elegant matching of words\nacross languages, but has fundamental limi-\ntations in translating sentences. In this pa-\nper, we propose simple yet effective methods\nto improve word-by-word translation of cross-\nlingual embeddings, using only monolingual\ncorpora but without any back-translation. We\nintegrate a language model for context-aware\nsearch, and use a novel denoising autoencoder\nto handle reordering. Our system surpasses\nstate-of-the-art unsupervised neural transla-\ntion systems without costly iterative training.\nWe also analyze the effect of vocabulary size\nand denoising type on the translation perfor-\nmance, which provides better understanding\nof learning the cross-lingual word embedding\nand its usage in translation.\n1 Introduction\nBuilding a machine translation (MT) system re-\nquires lots of bilingual data. Neural MT mod-\nels (Bahdanau et al., 2015), which become the\ncurrent standard, are even more difﬁcult to train\nwithout huge bilingual supervision (Koehn and\nKnowles, 2017). However, bilingual resources\nare still limited to some of the selected language\npairs—mostly from or to English.\nA workaround for zero-resource language pairs\nis translating via an intermediate (pivot) language.\nTo do so, we need to collect parallel data and train\nMT models for source-to-pivot and pivot-to-target\nindividually; it takes a double effort and the de-\ncoding is twice as slow.\nUnsupervised learning is another alternative,\nwhere we can train an MT system with only mono-\nlingual corpora. Decipherment methods (Ravi and\nKnight, 2011; Nuhn et al., 2013) are the ﬁrst work\nin this direction, but they often suffer from a huge\nlatent hypothesis space (Kim et al., 2017).\nRecent work by Artetxe et al. (2018) and Lam-\nple et al. (2018) train sequence-to-sequence MT\nmodels of both translation directions together in an\nunsupervised way. They do back-translation (Sen-\nnrich et al., 2016a) back and forth for every itera-\ntion or batch, which needs an immensely long time\nand careful tuning of hyperparameters for massive\nmonolingual data.\nHere we suggest rather simple methods to build\nan unsupervised MT system quickly, based on\nword translation using cross-lingual word embed-\ndings. The contributions of this paper are:\n•We formulate a straightforward way to com-\nbine a language model with cross-lingual\nword similarities, effectively considering\ncontext in lexical choices.\n•We develop a postprocessing method for\nword-by-word translation outputs using a de-\nnoising autoencoder, handling local reorder-\ning and multi-aligned words.\n•We analyze the effect of different artiﬁcial\nnoises for the denoising model and propose\na novel noise type.\n•We verify that cross-lingual embedding on\nsubword units performs poorly in translation.\n•We empirically show that cross-lingual map-\nping can be learned using a small vocabulary\nwithout losing the translation performance.\nThe proposed models can be efﬁciently trained\nwith off-the-shelf softwares with little or no\nchanges in the implementation, using only mono-\nlingual data. The provided analyses help for bet-\nter learning of cross-lingual word embeddings for\ntranslation purpose. Altogether, our unsupervised\nMT system outperforms the sequence-to-sequence\nneural models even without training signals from\nthe opposite translation direction, i.e. via back-\ntranslation.\n863\n2 Cross-lingual Word Embedding\nAs a basic step for unsupervised MT, we learn a\nword translation model from monolingual corpora\nof each language. In this work, we exploit cross-\nlingual word embedding for word-by-word trans-\nlation, which is state-of-the-art in terms of type\ntranslation quality (Artetxe et al., 2017; Conneau\net al., 2018).\nCross-lingual word embedding is a continu-\nous representation of words whose vector space\nis shared across multiple languages. This en-\nables distance calculation between word embed-\ndings across languages, which is actually ﬁnding\ntranslation candidates.\nWe train cross-lingual word embedding in a\nfully unsupervised manner:\n1. Learn monolingual source and target embed-\ndings independently. For this, we run skip-\ngram algorithm augmented with character n-\ngram (Bojanowski et al., 2017).\n2. Find a linear mapping from source embed-\nding space to target embedding space by\nadversarial training (Conneau et al., 2018).\nWe do not pre-train the discriminator with\na seed dictionary, and consider only the top\nVcross-train words of each language as input to\nthe discriminator.\nOnce we have the cross-lingual mapping, we\ncan transform the embedding of a given source\nword and ﬁnd a target word with the closest em-\nbedding, i.e. nearest neighbor search. Here, we\napply cross-domain similarity local scaling (Con-\nneau et al., 2018) to penalize the word similarities\nin dense areas of the embedding distribution.\nWe further reﬁne the mapping obtained from\nStep 2 as follows (Artetxe et al., 2017):\n3. Build a synthetic dictionary by ﬁnding mu-\ntual nearest neighbors for both translation di-\nrections in vocabularies of Vcross-train words.\n4. Run a Procrustes problem solver with the dic-\ntionary from Step 3 to re-train the mapping\n(Smith et al., 2017).\n5. Repeat Step 3 and 4 for a ﬁxed number of\niterations to update the mapping further.\n3 Sentence Translation\nIn translating sentences, cross-lingual word em-\nbedding has several drawbacks. We describe each\nof them and our corresponding solutions.\n3.1 Context-aware Beam Search\nThe word translation using nearest neighbor\nsearch does not consider context around the cur-\nrent word. In many cases, the correct translation is\nnot the nearest target word but other close words\nwith morphological variations or synonyms, de-\npending on the context.\nThe reasons are in two-fold: 1) Word embed-\nding is trained to place semantically related words\nnearby, even though they have opposite meanings.\n2) A hubness problem of high-dimensional em-\nbedding space hinders a correct search, where lots\nof different words happen to be close to each other\n(Radovanovi´c et al., 2010).\nIn this paper, we integrate context information\ninto word-by-word translation by combining a lan-\nguage model (LM) with cross-lingual word em-\nbedding. Let f be a source word in the current\nposition and ea possible target word. Given a his-\ntory hof target words beforee, the score of eto be\nthe translation of f would be:\nL(e; f,h) =λemb log q(f,e) +λLM log p(e|h)\nHere, q(f,e) is a lexical score deﬁned as:\nq(f,e) =d(f,e) + 1\n2\nwhere d(f,e) ∈[−1,1] is a cosine similarity be-\ntween f and e. It is transformed to the range [0,1]\nto make it similar in scale with the LM probability.\nIn our experiments, we found that this simple lin-\near scaling is better than sigmoid or softmax func-\ntions in the ﬁnal translation performance.\nAccumulating the scores per position, we per-\nform a beam search to allow only reasonable trans-\nlation hypotheses.\n3.2 Denoising\nEven when we have correctly translated words for\neach position, the output is still far from an ac-\nceptable translation. We adopt sequence denois-\ning autoencoder (Hill et al., 2016) to improve the\ntranslation output of Section 3.1. The main idea\nis to train a sequence-to-sequence neural network\nmodel that takes a noisy sentence as input and pro-\nduces a (denoised) clean sentence as output, both\nof which are of the same (target) language. The\nmodel was originally proposed to learn sentence\nembeddings, but here we use it directly to actually\nremove noise in a sentence.\nTraining label sequences for the denoising net-\nwork would be target monolingual sentences, but\n864\nwe do not have their noisy versions at hand. Given\na clean target sentence, the noisy input should\nbe ideally word-by-word translation of the corre-\nsponding source sentence. However, such bilin-\ngual sentence alignment is not available in our un-\nsupervised setup.\nInstead, we inject artiﬁcial noise into a clean\nsentence to simulate the noise of word-by-word\ntranslation. We design different noise types after\nthe following aspects of word-by-word translation.\n3.2.1 Insertion\nWord-by-word translation always outputs a target\nword for every position. However, there are a\nplenty of cases that multiple source words should\nbe translated to a single target word, or that some\nsource words are rather not translated to any word\nto make a ﬂuent output. For example, a German\nsentence “Ich h¨ore zu.” would be translated to\n“I’m listening to.” by a word-by-word transla-\ntor, but “I’m listening.” is more natural in English\n(Figure 1).\nIch höre zu\nI’m listening to\nI’m listening\nWord-by-word\nDenoising\nFigure 1: Example of denoising an insertion noise.\nWe pretend to have extra target words which\nmight be translation of redundant source words, by\ninserting random target words to a clean sentence:\n1. For each position i, sample a probabilitypi ∼\nUniform(0,1).\n2. If pi < pins, sample a word efrom the most\nfrequent Vins target words and insert it before\nposition i.\nWe limit the inserted words by Vins because tar-\nget insertion occurs mostly with common words,\ne.g. prepositions or articles, as the example above.\nWe insert words only before—not after—a posi-\ntion, since an extra word after the ending word\n(usually a punctuation) is not probable.\n3.2.2 Deletion\nSimilarly, word-by-word translation cannot handle\nthe contrary case: when a source word should be\ntranslated into more than one target words, or a\ntarget word should be generated from no source\nwords for ﬂuency. For example, a German word\n“im” must be “in the” in English, but word transla-\ntion generates only one of the two English words.\nAnother example is shown in Figure 2.\neine der besten\none the best\none of the best\nWord-by-word\nDenoising\nFigure 2: Example of denoising a deletion noise.\nTo simulate such situations, we drop some\nwords randomly from a clean target sentence (Hill\net al., 2016):\n1. For each position i, sample a probabilitypi ∼\nUniform(0,1).\n2. If pi <pdel, drop the word in the position i.\n3.2.3 Reordering\nAlso, translations generated word-by-word are not\nin an order of the target language. In our beam\nsearch, LM only assists in choosing the right word\nin context but does not modify the word order. A\ncommon reordering problem of German→English\nis illustrated in Figure 3.\nwas ich gesagt habe\nwhat I said have\nwhat I have said\nWord-by-word\nDenoising\nFigure 3: Example of denoising the reordering noise.\nFrom a clean target sentence, we corrupt its\nword order by random permutations. We limit the\nmaximum distance between an original position\nand its new position like Lample et al. (2018):\n1. For each position i, sample an integerδi from\n[0,dper].\n2. Add δi to index i and sort the incremented\nindices i+ δi in an increasing order.\n3. Rearrange the words to be in the new po-\nsitions, to which their original indices have\nmoved by Step 2.\n865\nde-en en-de fr-en en-fr\nSystem B LEU [%] B LEU [%] B LEU [%] B LEU [%]\nWord-by-Word 11.1 6.7 10.6 7.8\n+ LM 14.5 9.9 13.6 10.9\n+ Denoising 17.2 11.0 16.5 13.9\n(Lample et al., 2018) 13.3 9.6 14.3 15.1\n(Artetxe et al., 2018) - - 15.6 15.1\nTable 1: Translation results on German ↔English newstest2016 and French ↔English newstest2014.\nBeam size is 10 and top 100 words are considered in the nearest neighbor search.\nThis is a generalized version of swapping two\nneighboring words (Hill et al., 2016). Reordering\nis highly dependent of each language, but we\nfound that this noise is generally close to word-\nby-word translation outputs.\nInsertion, deletion, and reordering noises were ap-\nplied to each mini-batch with different random\nseeds, allowing the model to see various noisy ver-\nsions of the same clean sentence over the epochs.\nNote that the deletion and permutation noises\nare integrated in the neural MT training of Artetxe\net al. (2018) and Lample et al. (2018) as additional\ntraining objectives. Whereas we optimize an inde-\npendent model solely for denoising without archi-\ntecture change. It allows us to easily train a larger\nnetwork with a larger data. Insertion noise is of\nour original design, which we found to be the most\neffective (Section 4.1).\n4 Experiments\nWe applied the proposed methods on WMT\n2016 German ↔English task and WMT 2014\nFrench↔English task. For German/English, we\ntrained word embeddings with 100M sentences\nsampled from News Crawl 2014-2017 monolin-\ngual corpora. For French, we used News Crawl\n2007-2014 (around 42M sentences). The data was\nlowercased and ﬁltered to have a maximum sen-\ntence length 100. German compound words were\nsplitted beforehand. Numbers were replaced with\ncategory labels and recovered back after decoding\nby looking at the source sentence. Also, frequent\ncasing was applied to the translation output.\nfasttext (Bojanowski et al., 2017) was used to\nlearn monolingual embeddings for only the words\nwith minimum count 10. MUSE (Conneau et al.,\n2018) was used for cross-lingual mappings with\nVcross-train = 100k and 10 reﬁnement iterations\n(Step 3-5 in Section 2). Other parameters follow\nthe values in Conneau et al. (2018). With the same\ndata, we trained 5-gram count-based LMs using\nKenLM (Heaﬁeld, 2011) with its default setting.\nDenoising autoencoders were trained using\nSockeye (Hieber et al., 2017) on News Crawl\n2016 for German/English and News Crawl 2014\nfor French. We considered only top 50k frequent\nwords for each language and mapped other words\nto <unk>. The unknowns in the denoised output\nwere replaced with missing words from the noisy\ninput by a simple line search.\nWe used 6-layer Transformer encoder/decoder\n(Vaswani et al., 2017) for denoisers, with embed-\nding/hidden layer size 512, feedforward sublayer\nsize 2048 and 8 attention heads.\nAs a validation set for the denoiser training, we\nused newstest2015 (German ↔English) or\nnewstest2013 (French ↔English), where the\ninput/output sides both have the same clean target\nsentences, encouraging a denoiser to keep at least\nclean part of word-by-word translations. Here, the\nnoisy input showed a slight degradation of per-\nformance; the model seemed to overﬁt to speciﬁc\nnoises in the small validation set.\nOptimization of the denoising models was done\nwith Adam (Kingma and Ba, 2015): initial learn-\ning rate 0.0001, checkpoint frequency 4000, no\nlearning rate warmup, multiplying 0.7 to the learn-\ning rate when the perplexity on the validation set\ndid not improve for 3 checkpoints. We stopped the\ntraining if it was not improved for 8 checkpoints.\nTable 1 shows the results. LM improves word-\nby-word baselines consistently in all four tasks,\ngiving at least +3% B LEU . When our denoising\nmodel is applied on top of it, we have additional\ngain around +3% B LEU . Note that our meth-\nods do not involve any decoding steps to gener-\nate pseudo-parallel training data, but still perform\n866\nbetter than unsupervised MT systems that rely on\nrepetitive back-translations (Artetxe et al., 2018;\nLample et al., 2018) by up to +3.9% B LEU . The\ntotal training time of our method is only 1-2 days\nwith a single GPU.\n4.1 Ablation Study: Denoising\ndper pdel Vins BLEU [%]\n2 14.7\n3 14.9\n5 14.9\n3 0.1 15.7\n0.3 15.1\n3 0.1\n10 16.8\n50 17.2\n500 16.8\n5000 16.5\nTable 2: Translation results with different values of de-\nnoising parameters for German→English.\nTo examine the effect of each noise type in de-\nnoising autoencoder, we tuned each parameter of\nthe noise and combined them incrementally (Ta-\nble 2). Firstly, for permutations, a signiﬁcant im-\nprovement is achieved fromdper = 3, since a local\nreordering usually involves a sequence of 3 to 4\nwords. With dper > 5, it shufﬂes too many con-\nsecutive words together, yielding no further im-\nprovement. This noise cannot handle long-range\nreordering, which is usually a swap of words that\nare far from each other, keeping the words in the\nmiddle as they are.\nSecondly, we applied the deletion noise with\ndifferent values of pdel. 0.1 gives +0.8% B LEU ,\nbut we immediately see a degradation with a larger\nvalue; it is hard to observe one-to-many transla-\ntions more than once in each sentence pair.\nFinally, we optimized Vins for the insertion\nnoise, ﬁxing pins = 0.1. Increasing Vins is gener-\nally not beneﬁcial, since it provides too much vari-\nations in the inserted word; it might not be related\nto its neighboring words. Overall, we observe the\nbest result (+1.5% BLEU ) with Vins = 50.\n4.2 Ablation Study: Vocabulary\nWe also examined how the translation perfor-\nmance varies with different vocabularies of cross-\nlingual word embedding in Table 3. The ﬁrst three\nrows show that BPE embeddings performs worse\nV ocabulary B LEU [%]\nMerges\nBPE\n20k 10.4\n50k 12.5\n100k 13.0\nVcross-train\nWord\n20k 14.4\n50k 14.4\n100k 14.5\n200k 14.4\nTable 3: Translation results with different vocabularies\nfor German→English.\nthan word embeddings, especially with smaller\nvocabulary size. For small BPE tokens (1-3 char-\nacters), the context they meet during the embed-\nding training is much more various than a com-\nplete word, and a direct translation of such small\ntoken to a BPE token of another language would\nbe very ambiguous.\nFor word level embeddings, we compared dif-\nferent vocabulary sizes used for training the\ncross-lingual mapping (the second step in Section\n2). Surprisingly, cross-lingual word embedding\nlearned only on top 20k words is comparable to\nthat of 200k words in the translation quality. We\nalso increased the search vocabulary to more than\n200k but the performance only degrades. This\nmeans that word-by-word translation with cross-\nlingual embedding depends highly on the frequent\nword mappings, and learning the mapping be-\ntween rare words does not have a positive effect.\n5 Conclusion\nIn this paper, we proposed a simple pipeline\nto greatly improve sentence translation based on\ncross-lingual word embedding. We achieved\ncontext-aware lexical choices using beam search\nwith LM, and solved insertion/deletion/reordering\nproblems using denoising autoencoder. Our novel\ninsertion noise shows a promising performance\neven combined with other noise types. Our meth-\nods do not need back-translation steps but still out-\nperforms costly unsupervised neural MT systems.\nIn addition, we proved that for general translation\npurpose, an effective cross-lingual mapping can be\nlearned using only a small set of frequent words,\nnot on subword units.\n867\nAcknowledgments\nThis work has received\nfunding from the Euro-\npean Research Council\n(ERC) under the European\nUnion’s Horizon 2020\nresearch and innovation\nprogramme, grant agreement No. 694537 (SEQ-\nCLAS). The GPU computing cluster was partially\nfunded by Deutsche Forschungsgemeinschaft\n(DFG) under grant INST 222/1168-1 FUGG. The\nwork reﬂects only the authors’ views and neither\nERC nor DFG is responsible for any use that may\nbe made of the information it contains.\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language mod-\neling. In 15th Conference of the European Chap-\nter of the Association for Computational Linguistics\n(EACL 2017), pages 937–947, Valencia, Spain.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In 55th Annual Meeting of\nthe Association for Computational Linguistics (ACL\n2017), pages 451–462, Vancouver, Canada.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised neural ma-\nchine translation. In 6th International Conference\non Learning Representations (ICLR 2018), Vancou-\nver, Canada.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations\n(ICLR 2015), San Diego, CA, USA.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herve Jegou. 2018.\nWord translation without parallel data. In 6th Inter-\nnational Conference on Learning Representations\n(ICLR 2018), Vancouver, Canada.\nAria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,\nand Dan Klein. 2008. Learning bilingual lexicons\nfrom monolingual corpora. In 46th Annual Meet-\ning of the Association of Computational Linguistics:\nHuman Language Technologies (ACL-HLT 2008),\npages 771–779, Columbus, OH, USA.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In EMNLP 6th Workshop\non Statistical Machine Translation (WMT 2011),\npages 187–197, Edinburgh, Scotland.\nFelix Hieber, Tobias Domhan, Michael Denkowski,\nDavid Vilar, Artem Sokolov, Ann Clifton, and Matt\nPost. 2017. Sockeye: A toolkit for neural machine\ntranslation. arXiv Preprint. 1712.05690.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\n2016. Learning distributed representations of sen-\ntences from unlabelled data. In 15th Annual Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT 2016), pages\n1367–1377, San Diego, CA, USA.\nYunsu Kim, Julian Schamper, and Hermann Ney. 2017.\nUnsupervised training for large vocabulary transla-\ntion using sparse lexicon and word classes. In 15th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics (EACL 2017),\npages 650–656, Valencia, Spain.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations\n(ICLR 2015), San Diego, CA, USA.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In ACL 2017\n1st Workshop on Neural Machine Translation (NMT\n2017), pages 28–39, Vancouver, Canada.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu.\n2003. Statistical phrase-based translation. In\n2003 Conference of the North American Chapter\nof the Association for Computational Linguistics on\nHuman Language Technology (NAACL-HLT 2003),\npages 48–54, Edmonton, Canada.\nGuillaume Lample, Ludovic Denoyer, and\nMarc’Aurelio Ranzato. 2018. Unsupervised\nmachine translation using monolingual corpora\nonly. In 6th International Conference on Learning\nRepresentations (ICLR 2018), Vancouver, Canada.\nMalte Nuhn, Julian Schamper, and Hermann Ney.\n2013. Beam search for solving substitution ci-\nphers. In 51th Annual Meeting of the Association\nfor Computational Linguistics (ACL 2013), pages\n1569–1576, Soﬁa, Bulgaria.\nMiloˇs Radovanovi´c, Alexandros Nanopoulos, and Mir-\njana Ivanovi´c. 2010. On the existence of obstinate\nresults in vector space models. In 33rd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (SIGIR 2010), pages\n186–193, Geneva, Switzerland.\nSujith Ravi and Kevin Knight. 2011. Deciphering for-\neign language. In 49th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL 2011),\npages 12–21, Portland, OR, USA.\n868\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In 54th Annual Meet-\ning of the Association of Computational Linguistics\n(ACL 2016), pages 86–96, Berlin, Germany.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In 54th Annual Meeting of\nthe Association of Computational Linguistics (ACL\n2016), pages 1715–1725, Berlin, Germany.\nSamuel L. Smith, David H.P. Turban, Steven Hamblin,\nand Nils Y . Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In 5rd International Conference on Learn-\ning Representations (ICLR 2017), Toulon, France.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30 (NIPS 2017), pages 5998–6008,\nLong Beach, CA, USA.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Adversarial training for unsupervised\nbilingual lexicon induction. In 55th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2017), pages 1959–1970, Vancouver, Canada."
}