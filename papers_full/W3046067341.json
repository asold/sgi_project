{
  "title": "Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media",
  "url": "https://openalex.org/W3046067341",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5067022874",
      "name": "Xiangjue Dong",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A5057862674",
      "name": "Changmao Li",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A5101829031",
      "name": "Jinho D. Choi",
      "affiliations": [
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2529281176",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2758985501",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2251920663",
    "https://openalex.org/W2263859238",
    "https://openalex.org/W2294058101",
    "https://openalex.org/W2963133695",
    "https://openalex.org/W2963635943",
    "https://openalex.org/W2607623312",
    "https://openalex.org/W2888643222",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964066928",
    "https://openalex.org/W2250480277",
    "https://openalex.org/W2252381721",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2996428491"
  ],
  "abstract": "We present a transformer-based sarcasm detection model that accounts for the context from the entire conversation thread for more robust predictions. Our model uses deep transformer layers to perform multi-head attentions among the target utterance and the relevant context in the thread. The context-aware models are evaluated on two datasets from social media, Twitter and Reddit, and show 3.1% and 7.0% improvements over their baselines. Our best models give the F1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively, becoming one of the highest performing systems among 36 participants in this shared task.",
  "full_text": "Proceedings of the Second Workshop on Figurative Language Processing, pages 276–280\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n276\nTransformer-based Context-aware Sarcasm Detection in\nConversation Threads from Social Media\nXiangjue Dong\nComputer Science\nEmory University\nAltanta, GA, USA\nxiangjue.dong@emory.edu\nChangmao Li\nComputer Science\nEmory University\nAltanta, GA, USA\nchangmao.li@emory.edu\nJinho D. Choi\nComputer Science\nEmory University\nAltanta, GA, USA\njinho.choi@emory.edu\nAbstract\nWe present a transformer-based sarcasm detec-\ntion model that accounts for the context from\nthe entire conversation thread for more robust\npredictions. Our model uses deep transformer\nlayers to perform multi-head attentions among\nthe target utterance and the relevant context in\nthe thread. The context-aware models are eval-\nuated on two datasets from social media, Twit-\nter and Reddit, and show 3.1% and 7.0% im-\nprovements over their baselines. Our best mod-\nels give the F1-scores of 79.0% and 75.0% for\nthe Twitter and Reddit datasets respectively,\nbecoming one of the highest performing sys-\ntems among 36 participants in this shared task.\n1 Introduction\nSarcasm is a form of ﬁgurative language that im-\nplies a negative sentiment while displaying a pos-\nitive sentiment on the surface (Joshi et al., 2017).\nBecause of its conﬂicting nature and subtlety in lan-\nguage, sarcasm detection has been considered one\nof the most challenging tasks in natural language\nprocessing. Furthermore, when sarcasm is used in\nsocial media platforms such as Twitter or Reddit to\nexpress users’ nuanced intents, the language is of-\nten full of spelling errors, acronyms, slangs, emojis,\nand special characters, which adds another level of\ndifﬁculty in this task.\nDespite of its challenges, sarcasm detection has\nrecently gained substantial attention because it can\nbring the last gist to deep contextual understanding\nfor various applications such as author proﬁling,\nharassment detection, and irony detection (Van Hee\net al., 2018). Many computational approaches have\nbeen proposed to detect sarcasm in conversations\n(Ghosh et al., 2015; Joshi et al., 2015, 2016). How-\never, most of the previous studies use the utterances\nin isolation, which makes it hard even for human to\ndetect sarcasm without the contexts. Thus, it’s es-\nsential to interpret the target utterances along with\ncontextual information comprising textual features\nfrom the conversation thread, metadata about the\nconversation from external sources, or visual con-\ntext (Bamman and Smith, 2015; Ghosh et al., 2017;\nGhosh and Veale, 2017; Ghosh et al., 2018).\nThis paper presents a transformer-based sarcasm\ndetection model that takes both the target utterance\nand its context and predicts if the target utterance\ninvolves sarcasm. Our model uses a transformer\nencoder to coherently generate the embedding rep-\nresentation for the target utterance and the context\nby performing multi-head attentions (Section 4).\nThis approach is evaluated on two types of datasets\ncollected from Twitter and Reddit (Section 3), and\ndepicts signiﬁcant improvement over the baseline\nusing only the target utterance as input (Section 5).\nOur error analysis illustrates that the context-aware\nmodel can catch subtle nuance that cannot be cap-\ntured by the target-oriented model (Section 6).\n2 Related Work\nJust as most other types of ﬁgurative languages are,\nsarcasm is not necessarily complicated to express\nbut requires comprehensive understanding in con-\ntext as well as commonsense knowledge rather than\nits literal sense (Van Hee et al., 2018). Various ap-\nproaches have been presented for this task.\nMost earlier works had taken the target utterance\nwithout context as input. Both explicit and implicit\nincongruity features were explored in these works\n(Joshi et al., 2015). To detect whether certain words\nin the target utterance involve sarcasm, several ap-\nproaches based on distributional semantics were\nproposed (Ghosh et al., 2015). Additionally, word\nembedding-based features like distance-weighted\nsimilarities were also adapted to capture the subtle\nforms of context incongruity (Joshi et al., 2016).\nNonetheless, it is difﬁcult to detect sarcasm by con-\nsidering only the target utterances in isolation.\n277\nNon-textual features such as the properties of the\nauthor, audience and environment were also taken\ninto account (Bamman and Smith, 2015). Both the\nlinguistic and context features were used to distin-\nguish between information-seeking and rhetorical\nquestions in forums and tweets (Oraby et al., 2017).\nTraditional machine learning methods such as Sup-\nport Vector Machines were used to model sarcasm\ndetection as a sequential classiﬁcation task over\nthe target utterance and its surrounding utterances\n(Wang et al., 2015). Recently, deep learning meth-\nods using LSTM were introduced, considering the\nprior turns (Ghosh et al., 2017) as well as the suc-\nceeding turns (Ghosh et al., 2018).\n3 Data Description\nGiven a conversation thread, either from Twitter or\nReddit, a target utterance is the turn to be predicted,\nwhether or not it involves sarcasm, and the context\nis an ordered list of other utterances in the thread.\nTable 1 shows the examples of conversation threads\nwhere the target utterances involve sarcasm.1\nUtterance\nC1\nThis feels apt this morning but I don’t feel ﬁne ...\n<URL>\nC2\n@USER it is what’s going round in the heads of\nmany I know ...\nT\n@USER @USER I remember a few months back\nwe were saying the Americans shouldn’t tell us\nhow to vote on brexit\n(a) Sarcasm example from Twitter.\nUtterance\nC1 Promotional images for some guy’s Facebook page\nC2 I wouldn’t let that robot near me\nT Sounds like you don’t like science, you theist sheep\n(b) Sarcasm example from Reddit.\nTable 1: Examples of the conversation threads where\nthe target utterances involve sarcasm.Ci: i’th utterance\nin the context, T: the target utterance.\nThe Twitter data is collected by using the hashtags\n#sarcasm and #sarcastic. The Reddit data\nis a subset of the Self-Annotated Reddit Corpus that\nconsists of 1.3 million sarcastic and non-sarcastic\nposts (Khodak et al., 2017). Every target utterance\nis annotated with one of the two labels, SARCASM\nand NOT_SARCASM. Table 2 shows the statistics\nof the two datasets provided by this shared task.\n1Note that the target utterance can appear at any position of\nthe context although its exact position is not provided in this\nyear’s shared task data.\nNotice the huge variances in the utterance lengths\nfor both the Twitter and the Reddit datasets. For the\nReddit dataset, the average lengths of conversations\nas well as utterances are signiﬁcantly larger in the\ntest set than the training set that potentially makes\nthe model development more challenging.\nNC AU AT\nTRN 5,000 4.9 (±3.2) 140.4 (±112.8)\nTST 1,800 4.2 (±1.9) 128.5 (±78.8)\n(a) Twitter dataset statistics.\nNC AU AT\nTRN 4,400 3.5 (±0.8) 45.8 (±17.3)\nTST 1,800 5.3 (±2.0) 93.6 (±57.8)\n(b) Reddit dataset statistics.\nTable 2: Statistics of the two datasets provided by the\nshared task. TRN: training set, TST: test set, NC: # of\nconversations, AU: Avg # of utterances per conversation\n(including the target utterances) and its stdev, AT: Avg\n# of tokens per utterance and its stdev.\n4 Approach\nTwo types of transformer-based sarcasm detection\nmodels are used for our experiments:\na) The target-oriented model takes only the tar-\nget utterance as input (Section 4.1).\nb) The context-aware model takes both the target\nutterance and the context utterances as input\n(Section 4.2).\nThese two models are coupled with the latest trans-\nformer encoders e.g., BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2020), and ALBERT (Lan\net al., 2019), and compared to evaluate how much\nimpact the context makes to predict whether or not\nthe target utterance involves sarcasm.\n4.1 Target-oriented Model\nFigure 1a shows the overview of the target-oriented\nmodel. Let W = {w1, . . . , wn}be the input target\nutterance, where wi is the i’th token inW and n is\nthe max-number of tokens in any target utterance.\nW is ﬁrst prepended by the special token c repre-\nsenting the entire target utterance, which creates\nthe input sequence Ito = {c}⊕ W. Ito is then\nfed into the transformer encoder, which generates\nthe sequence of embeddings {ec}⊕ Ew, where\nEw = {ew\n1 , . . . , ew\nn }is the embedding list for W\nand (ec, ew\ni ) are the embeddings of (c, wi) respec-\ntively. Finally, ec is fed into the linear decoder to\ngenerate the output vectoroto that makes the binary\ndecision of whether or not W involves sarcasm.\n278\nTransformer Encoder (TE)\new\n1 ew\nn/uni22EFec\noto\n/uni22EFc w1 wnw2\new\n2\nLinear Decoder (LD)\nv1\nTransformer Encoder (TE)\new\n1 ew\nn/uni22EFec\noca\nc w1 wn\nLinear Decoder (LD)\n/uni22EFs /uni22EFvm\nes ev\n1 /uni22EFev\nm\n(a) Target-oriented model (Section 4.1)\nTransformer Encoder (TE)\new\n1 ew\nn/uni22EFec\noto\n/uni22EFc w1 wnw2\new\n2\nLinear Decoder (LD)\nv1\nTransformer Encoder (TE)\new\n1 ew\nn/uni22EFec\noca\nc w1 wn\nLinear Decoder (LD)\n/uni22EFs /uni22EFvm\nes ev\n1 /uni22EFev\nm (b) Context-aware model (Section 4.2)\nFigure 1: The overview of our transformer-based target-oriented and context-aware models.\n4.2 Context-aware Model\nFigure 1b shows the overview of the context-aware\nmodel. Let Li be the i’th utterance in the context.\nThen, V = L1 ⊕···⊕ Lk = {v1, . . . , vm}is the\nconcatenated list of tokens in all context utterances,\nwhere k is the number of utterances in the context,\nv1 is the ﬁrst token in L1 and vm is the last token\nin Lk. The input sequence Ito from Section 4.1 is\nappended by the special token s representing the\nseparator between the target utterance and the con-\ntext, and also V , which creates the input sequence\nIca = Ito ⊕{s}⊕V . Then, Ica gets fed into the\ntransformer encoder, which generates a sequence\nof embeddings {ec}⊕ Ew ⊕{es}⊕ Ev, where\nEv = {ev\n1, . . . , ev\nm}is the embedding list for V ,\nand (es, ev\ni ) are the embeddings of (s, vi) respec-\ntively. Finally, ec is fed into the linear decoder to\ngenerate the output vector oca that makes the same\nbinary decision to detect sarcasm.\n5 Experiments\n5.1 Data Split\nFor all our experiments, a mixture of the Twitter\nand the Reddit datasets is used. The Twitter train-\ning set provided by the shared task consists of 5,000\ntweets, where the labels are equally balanced be-\ntween SARCASM and NOT_SARCASM (Table 2).\nWe ﬁnd, however, 4.82% of them are duplicates,\nwhich are removed before data splitting. As a re-\nsult, 4,759 tweets are used for our experiments.\nLabels in the Reddit training set are also equally\nbalanced and no duplicate is found in this dataset.\nTwitter Reddit\nTRN DEV TRN DEV\nSARCASM 2,020 239 1,973 227\nNOT_SARCASM 2,263 237 1,987 213\nTable 3: Statistics of the data split used for our experi-\nments, where 10% of each dataset is randomly selected\nto create the development set.\n5.2 Models\nThree types of transformers are used for our exper-\niments, that are BERT-Large (Devlin et al., 2019),\nRoBERTa-Large (Liu et al., 2020), and ALBERT-\nxxLarge (Lan et al., 2019), to compare the perfor-\nmance among the current state-of-the-art encoders.\nEvery model is run three times and their average\nscores as well as standard deviations are reported.\nAll models are trained on the combined Twitter +\nReddit training set and evaluated on the combined\ndevelopment set (Table 3).\n5.3 Experimental Setup\nAfter an extensive hyper-parameter search, we set\nthe learning rate to 3e-5, the number of epochs to\n30, and use different seed values, 21, 42, 63, for the\nthree runs. Additionally, based on the statistics of\neach dataset, we set the maximum sequence length\nto 128 for the target-oriented models while it is set\nto 256 for the context-aware models by considering\nthe different lengths of the input sequences required\nby those approaches.\n5.4 Results\nThe baseline scores are provided by the organizers,\nthat are 60.0% for Reddit and 67.0% for Twitter us-\ning the single layer LSTM attention model (Ghosh\net al., 2018). Table 4 shows the results achieved by\nour target-oriented (Section 4.1) and the context-\naware (Section 4.2) models on the combined devel-\nopment set. The RoBERTa-Large model gives the\nhighest F1-scores for both the target-oriented and\ncontext-aware models. The context-aware model\nusing RoBERTa-Large show an improvement of\n1.1% over its counterpart baseline so that this model\nis used for our ﬁnal submission to the shared task.\nNote that it may be possible to achieve higher per-\nformance by ﬁne-tuning hyperparameters for the\nTwitter and Reddit datasets separately, which we\nwill explore in the future.\n279\nP R F1\nB-L 77.3 (±0.6) 79.9 (±0.8) 78.6 (±0.1)\nR-L 73.4 (±0.6) 88.5 (±1.4) 80.2 (±0.5)\nA-XXL 76.1 (±1.4) 83.3 (±2.3) 79.5 (±0.2)\n(a) Results from the target-oriented models (Section 4.1).\nP R F1\nB-L 76.3 (±1.0) 82.7 (±1.6) 79.4 (±0.5)\nR-L 77.3 (±3.8) 86.1 (±4.0) 81.3 (±0.2)\nA-XXL 76.5 (±3.3) 82.7 (±3.1) 79.4 (±2.2)\n(b) Results from the context-aware models (Section 4.2).\nTable 4: Results on the combined Twitter+Reddit devel-\nopment set. B-L: BERT-Large,R-L: RoBERTa-Large,\nA-XXL: ALBERT-xxLarge.\nTable 5 shows the results by the RoBERTa-Large\nmodels on the test sets. The scores are retrieved by\nsubmitting the system outputs to the shared task’s\nCodaLab page.2 The context-aware models sig-\nniﬁcantly outperform the target-oriented models\non the test sets, showing improvements of 3.1%\nand 7.0% on the F1 scores for the Twitter and the\nReddit datasets, respectively. The improvement on\nReddit is particularly substantial due to the much\ngreater lengths of the conversation threads and ut-\nterances in the test set compared to the ones in\nthe training set (Table 2). As the ﬁnal results, we\nachieve 79.0% and 75.0% for the Twitter and Red-\ndit datasets respectively that mark the 2nd places\nfor both datasets at the time of the submission.\nP R F1\nTwitter 75.5 (±0.7) 76.4 (±0.6) 75.2 (±0.8)\nReddit 67.9 (±0.5) 69.2 (±0.7) 67.4 (±0.5)\n(a) Results from the target-oriented RoBERTa-Large models.\nP R F1\nTwitter 78.4 (±0.6) 78.9 (±0.3) 78.3 (±0.7)\nReddit 74.5 (±0.6) 74.9 (±0.5) 74.4 (±0.7)\n(b) Results from the context-aware RoBERTa-Large models.\nTable 5: Results on the test sets from CodaLab.\n6 Analysis\nFor a better understanding in our ﬁnal model, errors\nfrom the following three situations are analyzed\n(TO: target-oriented, CA: context-aware):\n•TwCc: TO is wrong and CA is correct.\n•TcCw: TO is correct and CA is wrong.\n•TwCw: Both TO and CA are wrong.\n2https://competitions.codalab.org/\ncompetitions/22247\nTable 6 shows examples for every error situation.\nFor TwCc, TO predicts it to be NOT_SARCASM. In\nthis example, it is difﬁcult to tell if the target utter-\nance involves sarcasm without having the context.\nFor TcCw, CA predicts it to be NOT_SARCASM. It\nappears that the target utterance is long enough to\nprovide enough features for TO to make the correct\nprediction, whereas considering the extra context\nmay increase noise for CA to make the incorrect\ndecision. For TwCw, both TO and CA predict it to\nbe NOT_SARCASM. This example seems to require\ndeeper reasoning to make the correct prediction.\nUtterance\nC1 who has ever cared about y * utube r * wind .\nC2\n@USER Back when YouTube was beginning it was a\ncool giveback to the community to do a super polished\nhigh production value video with YT talent . Not the\nsame now . The better move for them would be to do like\n5-6 of them in several categories to give that shine .\nT @USER @USER I look forward to the eventual annual\nTubies Awards livestream .\n(a) Example when TO is wrong and CA is correct.\nUtterance\nC1\nI am asking the chairs of the House and Senate committees\nto investigate top secret intelligence shared with NBC\nprior to me seeing it.\nC2\n@USER Good for you, sweetie! But using the legislative\nbranch of the US Government to ﬁx your media grudges\nseems a bit much.\nT @USER @USER @USER you look triggered after someone\ncriticizes me, are conservatives skeptic of ppl in power?\n(b) Example when TO is correct and CA is wrong.\nUtterance\nC1\nIf I could start my #Brand over, this is what I would\nemulate my #Site to look like .. And I might, once my\nanual contract with #WordPress is up . Even tho I don’t\nthink is very; I can’t help but to ﬁnd ... <URL> <URL>\nC2 @USER There is no design on it except for links ?\nT\n@USER It’s the of what #Works in this current #Mindset\nof #MassConsumption; wannabe fast due to caused by, and\nbeing just another and. is the light, bringing color back\nto this sad world of and.\n(c) Example when both TO and CA are wrong.\nTable 6: Examples of the three error situations. Ci: i’th\nutterance in the context, T: the target utterance.\n7 Conclusion\nThis paper explores the beneﬁt of considering rel-\nevant contexts for the task of sarcasm detection.\nThree types of state-of-the-art transformer encoders\nare adapted to establish the strong baseline for\nthe target-oriented models, which are compared\nto the context-aware models that show signiﬁcant\nimprovements for both Twitter and Reddit datasets\nand become one of the highest performing models\nin this shared task.\n280\nAll our resources are publicly available at Emory\nNLP’s open source repository: https://github.\ncom/emorynlp/figlang-shared-task-2020\nAcknowledgments\nWe gratefully acknowledge the support of the AWS\nMachine Learning Research Awards (MLRA). Any\ncontents in this material are those of the authors\nand do not necessarily reﬂect the views of AWS.\nReferences\nDavid Bamman and Noah Smith. 2015. Contextual-\nized Sarcasm Detection on Twitter. In International\nAAAI Conference on Web and Social Media, pages\n574–577.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4171–4186.\nAniruddha Ghosh and Tony Veale. 2017. Magnets for\nSarcasm: Making Sarcasm Detection Timely, Con-\ntextual and Very Personal. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 482–491, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nDebanjan Ghosh, Alexander Richard Fabbri, and\nSmaranda Muresan. 2017. The Role of Conversa-\ntion Context for Sarcasm Detection in Online Inter-\nactions. Proceedings of the 18th Annual SIGdial\nMeeting on Discourse and Dialogue, pages 186–\n196.\nDebanjan Ghosh, Alexander Richard Fabbri, and\nSmaranda Muresan. 2018. Sarcasm Analysis us-\ning Conversation Context. Comput. Linguist. ,\n44(4):755–792.\nDebanjan Ghosh, Weiwei Guo, and Smaranda Mure-\nsan. 2015. Sarcastic or Not: Word Embeddings to\nPredict the Literal or Sarcastic Meaning of Words.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1003–1012, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nAditya Joshi, Pushpak Bhattacharyya, and Mark J. Car-\nman. 2017. Automatic Sarcasm Detection: A Sur-\nvey. ACM Computing Surveys, 50(5):1–22.\nAditya Joshi, Vinita Sharma, and Pushpak Bhat-\ntacharyya. 2015. Harnessing Context Incongruity\nfor Sarcasm Detection. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers) , pages 757–762, Beijing,\nChina. Association for Computational Linguistics.\nAditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak\nBhattacharyya, and Mark Carman. 2016. Are Word\nEmbedding-based Features Useful for Sarcasm De-\ntection? In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1006–1011, Austin, Texas. Association\nfor Computational Linguistics.\nMikhail Khodak, Nikunj Saunshi, and Kiran V odrahalli.\n2017. A Large Self-Annotated Corpus for Sarcasm.\nProceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), abs/1704.05579.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv,\n11942(1909).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. In Proceedings of the International\nConference on Learning Representations.\nShereen Oraby, Vrindavan Harrison, Amita Misra,\nEllen Riloff, and Marilyn Walker. 2017. Are you\nserious?: Rhetorical Questions and Sarcasm in So-\ncial Media Dialog. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 310–319, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. SemEval-2018 Task 3: Irony Detection in En-\nglish Tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nZelin Wang, Zhijian Wu, Ruimin Wang, and Yafeng\nRen. 2015. Twitter Sarcasm Detection Exploiting a\nContext-Based Model. In WISE.",
  "topic": "Sarcasm",
  "concepts": [
    {
      "name": "Sarcasm",
      "score": 0.953977108001709
    },
    {
      "name": "Conversation",
      "score": 0.7826257944107056
    },
    {
      "name": "Thread (computing)",
      "score": 0.7762904167175293
    },
    {
      "name": "Computer science",
      "score": 0.7559520602226257
    },
    {
      "name": "Transformer",
      "score": 0.7410674691200256
    },
    {
      "name": "Utterance",
      "score": 0.6619254350662231
    },
    {
      "name": "Social media",
      "score": 0.6590876579284668
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5124202370643616
    },
    {
      "name": "Natural language processing",
      "score": 0.4374629855155945
    },
    {
      "name": "Machine learning",
      "score": 0.39024049043655396
    },
    {
      "name": "Speech recognition",
      "score": 0.34596967697143555
    },
    {
      "name": "World Wide Web",
      "score": 0.24878382682800293
    },
    {
      "name": "Psychology",
      "score": 0.14014384150505066
    },
    {
      "name": "Communication",
      "score": 0.12944990396499634
    },
    {
      "name": "Linguistics",
      "score": 0.12563499808311462
    },
    {
      "name": "Engineering",
      "score": 0.08602157235145569
    },
    {
      "name": "Irony",
      "score": 0.07888150215148926
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    }
  ],
  "cited_by": 21
}