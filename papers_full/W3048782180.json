{
  "title": "Fine-Grained Visual Textual Alignment for Cross-Modal Retrieval Using Transformer Encoders",
  "url": "https://openalex.org/W3048782180",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2169874835",
      "name": "Nicola Messina",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A2102836602",
      "name": "Giuseppe Amato",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A97551770",
      "name": "Andrea Esuli",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A2032466494",
      "name": "Fabrizio Falchi",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A2161798937",
      "name": "Claudio Gennaro",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A4209757484",
      "name": "StÃ©phane Marchand-Maillet",
      "affiliations": [
        "University of Geneva"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2467557055",
    "https://openalex.org/W3035454331",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2901988662",
    "https://openalex.org/W2963040148",
    "https://openalex.org/W2963467339",
    "https://openalex.org/W3048588868",
    "https://openalex.org/W2963224792",
    "https://openalex.org/W2900953995",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W2979304729",
    "https://openalex.org/W2552579943",
    "https://openalex.org/W2964120214",
    "https://openalex.org/W2903529742",
    "https://openalex.org/W3008356057",
    "https://openalex.org/W2990571534",
    "https://openalex.org/W3017098848",
    "https://openalex.org/W2962716332",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W1957706851",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2988823324",
    "https://openalex.org/W2913618459",
    "https://openalex.org/W2810482788",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2346425926",
    "https://openalex.org/W2982078236",
    "https://openalex.org/W3035605030",
    "https://openalex.org/W2778100917",
    "https://openalex.org/W3021244424",
    "https://openalex.org/W6767769535",
    "https://openalex.org/W3092820619",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W2985951359",
    "https://openalex.org/W2962779575",
    "https://openalex.org/W2896798564",
    "https://openalex.org/W2965848243",
    "https://openalex.org/W3028452400",
    "https://openalex.org/W3035588244",
    "https://openalex.org/W2981586349",
    "https://openalex.org/W3005971801",
    "https://openalex.org/W2886970679",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963499204",
    "https://openalex.org/W3105817677",
    "https://openalex.org/W3104180728",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3001555892",
    "https://openalex.org/W2984814708",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2989176720",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3160179442",
    "https://openalex.org/W2973978812",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2963899908",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2624614404",
    "https://openalex.org/W2942614241",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972895044",
    "https://openalex.org/W2953106684"
  ],
  "abstract": "Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences (i.e., image regions and words, respectively) to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task. Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way toward the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN .",
  "full_text": "Fine-grained Visual Textual Alignment for Cross-Modal\nRetrieval using Transformer Encoders\nNICOLA MESSINA, ISTI - CNR, Italy\nGIUSEPPE AMATO, ISTI - CNR, Italy\nANDREA ESULI, ISTI - CNR, Italy\nFABRIZIO FALCHI,ISTI - CNR, Italy\nCLAUDIO GENNARO, ISTI - CNR, Italy\nSTÃ‰PHANE MARCHAND-MAILLET, VIPER Group - University of Geneva, Switzerland\nDespite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching\nremains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence\nmatching based on word-region alignments, using supervision only at the global image-sentence level.\nSpecifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network\n(TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences,\ni.e., image regions and words, respectively, in order to preserve the informative richness of both modalities.\nTERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets.\nMoreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task.\nFocusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual\ndata pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and\ntextual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In\nthis respect, TERAN merges the information from the two domains only during the final alignment phase,\nimmediately before the loss computation. We argue that the fine-grained alignments produced by TERAN\npave the way towards the research for effective and efficient methods for large-scale cross-modal information\nretrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the\nMS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence\nretrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at\nhttps://github.com/mesnico/TERAN.\nCCS Concepts: â€¢ Computing methodologiesâ†’Semi-supervised learning settings; Neural networks; â€¢\nInformation systemsâ†’Multimedia and multimodal retrieval.\nAdditional Key Words and Phrases: deep learning, cross-modal retrieval, multi-modal matching, computer\nvision, natural language processing\nACM Reference Format:\nNicola Messina, Giuseppe Amato, Andrea Esuli, Fabrizio Falchi, Claudio Gennaro, and StÃ©phane Marchand-\nMaillet. 2020. Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders.\nACM Trans. Multimedia Comput. Commun. Appl.0, 0, Article 0 ( 2020), 23 pages. https://doi.org/XX.XXXX/\nXXXXXXX.XXXXXXX\nAuthorsâ€™ addresses: Nicola Messina, ISTI - CNR, Pisa, Italy, nicola.messina@isti.cnr.it; Giuseppe Amato, ISTI - CNR,\nPisa, Italy, giuseppe.amato@isti.cnr.it; Andrea Esuli, ISTI - CNR, Pisa, Italy, andrea.esuli@isti.cnr.it; Fabrizio Falchi, ISTI -\nCNR, Pisa, Italy, fabrizio.falchi@isti.cnr.it; Claudio Gennaro, ISTI - CNR, Pisa, Italy, claudio.gennaro@isti.cnr.it; StÃ©phane\nMarchand-Maillet, VIPER Group - University of Geneva, Geneva, Switzerland, stephane.marchand-maillet@unige.ch.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ© 2020 Association for Computing Machinery.\n1551-6857/2020/0-ART0 $15.00\nhttps://doi.org/XX.XXXX/XXXXXXX.XXXXXXX\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\narXiv:2008.05231v2  [cs.CV]  2 Mar 2021\n0:2 Messina et al.\n1 INTRODUCTION\nSince 2012, deep learning has obtained impressive results in several vision and language tasks.\nRecently, various attempts have been made to merge the two worlds, and state-of-the-art results\nhave been obtained in many of these tasks, including visual question answering [2, 12, 52], image\ncaptioning [6, 14, 48, 65], and image-text matching [5, 9, 28, 39]. In this work, we deal with the\ncross-modal retrieval task, with a focus on the visual and textual modalities. The task consists\nin finding the top-relevant images representing a natural language sentence given as a query\n(image-retrieval), or, vice versa, in finding a set of sentences that best describe an image given as a\nquery (sentence-retrieval).\nThis cross-modal retrieval task is closely related to image-sentence matching, which consists in\nassigning a score to a pair composed of an image and a sentence. The score is high if the sentence\nadequately describes the image, and low if the input sentence is unrelated to the corresponding\nimage. The score function learned by solving the matching problem can be then used for deciding\nwhich are the top-relevant images and sentences in the two image- and sentence- retrieval scenarios.\nThe matching problem is often very difficult since a deep high-level understanding of images and\nsentences is needed for succeeding in this task.\nVisuals and texts are used by humans to fully understand the real world. Although they are\nof equal importance, the information hidden in these two modalities has a very different nature.\nThe text is already a well-structured description developed by humans in hundreds of years, while\nimages are nothing but raw matrices of pixels hiding very high-level concepts and structures.\nImages and texts do not describe only static entities. In fact, they can easily portray relationships\nbetween the objects of interest, e.g.: \"The kid kicks the ball\". Therefore, it would be helpful to also\nunderstand spatial and even abstract relationships linking them together.\nVision and language matching has been extensively studied [3, 9, 24, 28, 39]. Many works employ\nstandard architectures for processing images and texts, such as CNNs-based models for image\nprocessing and recurrent networks for language. Usually, in this scenario, the image embeddings\nare extracted from standard image classification networks, such as ResNet or VGG, by employing\nthe network activations before the classification head. Usually, descriptions extracted from CNN\nnetworks trained on classification tasks can only capture global summarized features of the image,\nignoring important localized details. For this reason, recent works make extensive use of attention\nmechanisms, which are able to relate each visual object, extracted from the spatial locations of a\nfeature map or an object detector to the most interesting parts of the sentence, and/or vice-versa.\nMany of these works, such as ViLBERT[39], ImageBERT[45], VL-BERT[51], IMRAM[4], try to\nlearn a complex scoring function ğ‘  = ğœ™(ğ¼,ğ¶)that measures the affinity between an image and a\ncaption, where ğ¼ is an image, ğ¶ is the caption and ğ‘  is a normalized score in the range [0,1]. These\nare very effective models for tackling the matching task, and they reach state-of-the-art results.\nHowever, they remain very inefficient for large-scale image or sentence retrieval: the problem with\nthese approaches is that it is not possible to extract visual and textual descriptions separately, as\nthe pipelines are strongly entangled through cross-attention or memory layers. Thus, if we want to\nretrieve images related to a given query text, we have to compute all the similarities using theğœ™\nfunction and then sort the resulting scores in descending order. This is unfeasible if we want to\nretrieve images or sentences from a large database in a few milliseconds.\nIn our previous work, we introduced the Transformer Encoder Reasoning Network (TERN)\narchitecture [43], which is a transformer-based model able to independently process images and\nsentences to match them into the same common space. TERN is a useful architecture for producing\ncompact yet informative features that could be used in cross-modal retrieval setups for efficient\nindexing using metric-space or text-based approaches. TERN processes visual and textual elements\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:3\nusing transformer encoder layers, exploring and reasoning on the relationships among image\nregions and sentence words. However, its main objective is to match images and sentences as\natomic, global entities, by learning a global representation of them inside special tokens (I-CLS and\nT-CLS) processed by the transformer encoder. This usually leads to performance loss and possibly\npoor generalization since fine-grained information useful for effective matching is lost during the\nprojection to a fixed-sized common space.\nFor this reason, in this work, we propose TERAN (Transformer Encoder Reasoning and Alignment\nNetwork) in which we force a fine-grained word-region alignment. Fine-grained matching deals\nwith the accurate understanding of the local correspondences between image regions and words,\nas opposed to coarse-grained matching, where only a summarized global descriptions of the two\nmodalities is considered. In fact, differently from TERN, the objective function is directly defined on\nthe set of regions and words in output from the architecture, and not on a potentially lossy global\nrepresentation. Using this objective, TERAN tries to individually align the regions and the words\ncontained in images and sentences respectively, instead of directly matching images and sentences\nas a whole. The information available to TERAN during training is still coarse-grained, as we do\nnot inject any information about word-region correspondences. The fine-grained alignment is thus\nobtained in a semi-supervised setup, where no explicit word-region correspondences are given to\nthe network.\nOur TERAN proposal shares most of the previous TERN building blocks and interconnections:\nthe visual and textual pipelines are forwarded separately and they are fused only during the loss\ncomputation, in the very last stage of the architecture, making scalable cross-modal information\nretrieval possible. At the same time, this novel architecture employs state-of-the-art self-attentive\nmodules, based on the transformer encoder architecture [53], able to spot out hidden relationships\nin both modalities for a very effective fine-grained alignment.\nTherefore, TERAN is able to produce independent visual and textual features usable in efficient\nretrieval scenarios implementing two simple visual and textual pipelines built of modern self-\nattentive mechanisms. In spite of its overall simplicity, TERAN is able to reach state-of-the-art\nresults in the image and sentence retrieval task, even when compared with complex entangled\nvisual-textual matching models. Experiments show that TERAN can generalize better with respect\nto the previous TERN approach.\nIn the evaluation of the proposed matching procedure, we used a typical information retrieval\nsetup using the Recall@K metrics (with ğ¾ = {1,5,10}.) However, in common search engines where\nthe user is searching for related images and not necessarily exact matches, the Recall@K evaluation\ncould be too rigid, especially when ğ¾ = 1. For this reason, as in our previous work [43], in addition\nto the strict Recall@K metric, we propose to measure the retrieval abilities of the system with a\nnormalized discounted cumulative gain metric (NDCG) with relevance computed exploiting caption\nsimilarities.\nSummarizing, the contributions of this paper are the following:\nâ€¢we introduce the Transformer Encoder Reasoning and Alignment Network (TERAN), able to\nproduce fine-grained region-word alignments for efficient cross-modal information retrieval.\nâ€¢we show that TERAN can reach state-of-the-art results on the cross-modal visual-textual\nretrieval task, both in terms of Recall@K and NDCG, while producing visually-pleasant region-\nwords alignments without using supervision at the region-word level. Retrieval results are\nmeasured both on MS-COCO and Flickr30k datasets.\nâ€¢we quantitatively compare TERAN with our previous work [43], and we perform an extensive\nstudy on several variants of our novel model, including weight sharing in the last transformer\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:4 Messina et al.\nlayers, stop-words removal during training, different pooling protocols for the matching loss\nfunction, and the usage of different language models.\n2 RELATED WORK\nIn this section, we review some of the previous works related to image-text joint processing for\ncross-modal retrieval and alignment, and high-level relational reasoning, on which this work lays\nits foundations. Also, we briefly summarize the evaluation metrics available in the literature for the\ncross-modal retrieval task.\nImage-Text Processing for Cross-Modal Retrieval\nImage-text matching is often cast to the problem of inferring a similarity score among an image\nand a sentence. Usually, one of the common approaches for computing this cross-domain similarity\nis to project images and texts into a common representation space on which some kind of simi-\nlarity measure can be defined (e.g.: cosine or dot-product similarities). Images and sentences are\npreprocessed by specialized architectures before being merged at some point in the pipeline.\nConcerning image processing, the standard approach consists in using Convolutional Neural\nNetworks (CNNs), usually pre-trained on image classification tasks. In particular, [8, 16, 26, 35, 54]\nuse VGGs, while [9, 10, 17, 38] use ResNets. Concerning sentence processing, many works [ 9, 17,\n24, 28, 30] employ GRU or LSTM recurrent networks to process natural language, often considering\nthe final hidden state as the only feature representing the whole sentence. The problem with these\nkinds of methodologies is that they usually extract extremely summarized global descriptions of\nimages and sentences. Therefore, a lot of useful fine-grained information needed to reconstruct\ninter-object relationships for precise image-text alignment is permanently lost.\nFor these reasons, many works try to employ region-level information, together with word-level\ndescriptions provided by recurrent networks, to understand fine-grained alignments between words\nand localized patches in the image. Recent works [4, 13, 15, 28, 36, 37, 56] exploit the availability\nof pre-computed region-level features extracted from the Faster-RCNN [47] object detector. An\nalternative consists in using the features maps in output from ResNets, without aggregating them,\nfor computing fine-grained attentions over the sentences [11, 18, 22, 55, 57, 60].\nRecently, the transformer architecture [53] achieved state-of-the-art results in many natural\nlanguage processing tasks, such as next sentence prediction or sentence classification. The results\nachieved by the BERT model [7] are a demonstration of the power of the attention mechanism\nto produce accurate context-aware word descriptions. For this reason, some works in image-text\nmatching use BERT to extract contextualized word embeddings for representing sentences [46, 50,\n58, 59]. Drawing inspiration from the powerful contextualization capabilities of the transformer\nencoder architecture, some works use BERT-like processing on both visual and textual modalities,\nsuch as ViLBERT [39], ImageBERT [45], Pixel-BERT [19], VL-BERT [51].\nThese latest works achieve state-of-the-art results in sentence and image retrieval, as well as\nexcellent results on the downstream word-region alignment task [5]. However, they cannot produce\nseparate image and caption descriptions; this is an important requirement in real-world search\nengines, where usually, at query time, only the query element is forwarded through the network,\nwhile all the elements of the database have already been processed by means of an offline feature\nextraction process.\nSome architectures have been designed so that they are natively able to extract disentangled\nvisual and textual features. In particular, in [9] the authors introduce the VSE++ architecture. They\nuse VGG and ResNets visual features extractors, together with an LSTM for sentence processing, and\nthey match images and captions exploiting hard-negatives during the loss computation. With their\nVSRN architecture [30], the authors introduce a visual reasoning pipeline built of Graph Convolution\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:5\nNetworks (GCNs) and a GRU to sequentially reason on the different image regions. Furthermore,\nthey impose a sentence reconstruction loss to regularize the training process. The authors in [18]\nuse a similar objective, but employing a pre-trained multi-label CNN to find semantically relevant\nimage patches and their vectorial descriptions. Differently, in [50] an adversarial learning method\nis proposed, where a discriminator is used to learn modality-invariant representations. The authors\nin [11] use a contextual attention-based LSTM-RNN which can selectively attend to salient regions\nof an image at each time step, and they employ a recurrent canonical correlation analysis to find\nhidden semantic relationships between regions and words.\nThe works closer to our setup are SAEM [59] and CAMERA [46]. In [59] the authors use triplet\nand angular loss to project the image and sentence features into the same common space. The\nvisual and textual features are obtained through transformer encoder modules. Differently from our\nwork, they do not enforce fine-grained alignments and they pool the final representations to obtain\na single-vector representation. Instead, in [46] the authors use BERT as language model and an\nadaptive gating self-attention module to obtain context-enhanced visual features, projecting them\ninto the same common space using cosine similarity. Unlike our work, they specifically focus on\nmulti-view summarization, as multiple sentences can describe the same images in many different\nbut complementary ways.\nThe loss used in our work is inspired by the matching loss introduced by the MRNN architecture\n[24], which seems able to produce very good region-word alignments by supervising only the\nglobal image-sentence level.\nHigh-Level Reasoning\nAnother branch of research from which this work draws inspiration is focused on the study of\nrelational reasoning models for high-level understanding. The work in [49] proposes an architecture\nthat separates perception from reasoning. They tackle the problem of Visual Question Answering\nby introducing a particular layer called Relation Network (RN), which is specialized in comparing\npairs of objects. Object representations are learned using a four-layer CNN, and the question\nembedding is generated through an LSTM. The authors in [41, 42] extend the RN for producing\ncompact features for relation-aware image retrieval. However, they do not explore the multi-modal\nretrieval setup.\nOther solutions try to stick more to a symbolic-like way of reasoning. In particular, [ 12, 23]\nintroduce compositional approaches able to explicitly model the reasoning process by dynamically\nbuilding a reasoning graph that states which operations must be carried out and in which order to\nobtain the right answer.\nRecent works employ Graph Convolution Networks (GCNs) to reason about the interconnections\nbetween concepts. The authors in [31, 62, 63] use GCNs to reason on the image regions for image\ncaptioning, while [32, 61] use GCN with attention mechanisms to produce the scene graph from\nplain images.\nCross-Modal Retrieval Evaluation Metrics\nAll the works involved with image-caption matching evaluate their results by measuring how good\nthe system is at retrieving relevant images given a query caption (image-retrieval) and vice-versa\n(caption-retrieval).\nUsually the Recall@K metric is used [ 9, 29, 30, 39, 45], where typically ğ¾ = {1,5,10}. On the\nother hand, [3] introduced a novel metric able to capture non-exact results by weighting the ranked\ndocuments using a caption-based similarity measure.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:6 Messina et al.\nAdd & Norm\nFeed-Forward\nNetwork\nMulti-Head\nAttention\ns input vectors\nAdd & Norm\ns output vectors\nQ K V\nquery \nFFN\nkey\nFFN\nvalues\nFFN\ns â¨‰ s attention matrix\nQ K V\nAtt(Q, K, V)\nsoftmax\n(per row)\nFig. 1. A high-level view of the transformer encoder layer. Every arrow carries ğ‘  fixed-sized vectors.\nWe extend the metric introduced in [3], giving rise to a powerful evaluation protocol that handles\nnon-exact yet relevant matches. Relaxing the constraints of exact-match similarity search is an\nimportant step towards an effective evaluation of real search engines.\n3 REVIEW OF TRANSFORMER ENCODERS\nOur proposed architecture is based on the well established Transformer Encoder (TE) architecture,\nwhich heavily relies on the concept of self-attention. The self-attention mechanism tries to weight\nevery vector of the sequence using a scalar value normalized in the range [0,1]computed as a\nfunction of the input vectors themselves. In particular, the attention is computed by using a query\nvector ğ‘„ and a set of key-value (ğ¾, ğ‘‰) pairs derived from data using simple feed-forward networks,\nand processed as shown in Equation 1. More in detail, the attention-aware vector in output from\nthe attention module is computed for every input element as a weighted sum of the values, where\nthe weight assigned to each value is computed as a similarity score (scaled dot-product) between\nthe query with the corresponding key:\nAtt(ğ‘„,ğ¾,ğ‘‰ )= softmax\n\u0012ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n\u0013\nğ‘‰. (1)\nğ‘„,ğ¾,ğ‘‰ are the query, the key, and the value respectively, while the factorâˆšğ‘‘ğ‘˜ is used to mitigate\nthe vanishing gradient problem of the softmax function in case the inner product assumes too\nlarge values. In real implementations, a multi-head attention is used: the input vectors are chunked,\nand every chunk is processed independently using a different instantiation of the above-described\nmechanism. This helps in capturing the relationships between the different portions of every input\nvector.\nFinally, the output from the TE is computed through a simple feed-forward layer applied to\nthe ğ´ğ‘¡ğ‘¡(ğ‘„,ğ¾,ğ‘‰ )vectors, with a ReLU activation function. This simple feed-forward layer casts\nin output a set of features having the same dimensionality of the input sequence. Two residual\nconnections followed by layer normalization are also present around the self-attention and the\nfeed-forward sub-modules. An overview of the transformer encoder architecture is shown in\nFigure 1.\nAlthough the TE was initially developed to work on sequences, there are no architectural\nconstraints that prevent its usage on sets of vectors instead of sequences. In fact, the TE module has\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:7\nnot any built-in sequential prior which considers every vector in a precise position in the sequence.\nThis makes the TE suitable for processing visual features coming from an object detector.\nWe argue that the transformer encoder self-attention mechanism can drive a simple but powerful\nreasoning mechanism able to spot hidden relationships between the vector entities, whatever\nnature they have (visual or textual). Also, the encoder is designed in a way that multiple instances\nof it could be stacked in sequence. Using multiple levels of attention helps in producing a deeper\nand more powerful reasoning pipeline.\n4 TRANSFORMER ENCODER REASONING AND ALIGNMENT NETWORK (TERAN)\nOur Transformer Encoder Reasoning and Alignment Network (TERAN) leverages our previous work\n[43] that introduced the TERN architecture. TERAN modifies the learning objective of our previous\nwork by forcing a fine-grained alignment between the region and word features in output from the\nlast transformer encoder (TE) layers so that meaningful fine-grained concepts are produced.\nAs TERN, our TERAN reasoning engine is built using a stack of TE layers, both for the visual\nand the textual data pipelines. The TE takes as input sequences or sets of entities, and it can reason\nupon these entities disregarding their intrinsic nature. In particular, we consider the salient regions\nin an image as visual entities, and the words present in the caption as textual entities.\nMore formally, the input to our reasoning pipeline is a set ğ¼ = {ğ‘Ÿ0,ğ‘Ÿ1,...,ğ‘Ÿ ğ‘›}of ğ‘›image regions\n(visual entities) representing an image ğ¼ and a sequence ğ¶ = {ğ‘¤0,ğ‘¤1,...,ğ‘¤ ğ‘š }of ğ‘šwords (textual\nentities) representing the corresponding caption ğ¶. Thus, the reasoning module continuously\noperates on sets and sequences of ğ‘›and ğ‘šobjects respectively for images and captions.\nThe TERN architecture in [43] produces summarized representations of both images and words\nby employing special I-CLS and T-CLS tokens that are forwarded towards the layers of the TEs.\nIn the end, the processed I-CLS and T-CLS tokens gather important global knowledge from both\nmodalities. Contrarily, TERAN does not produce aggregated fixed-sized representations for images\nand sentences. For this reason, it does not employ the global features constructed inside the I-CLS\nand T-CLS tokens. Instead, it tries to impose a global matching loss defined on the variable-length\nsets in output from the last TE layers that is able, as a side effect, to produce also good and\ninterpretable region-word alignments.\nThe overall architecture is shown in Figure 2. We left in the scheme the I-CLS and T-CLS tokens\nconnections for comparison with the TERN architecture presented in [43]. These tokens are still\nused for a targeted experiment that exploits the combination of the TERN and TERAN losses (more\ndetails in Section 7). However, they are not used in the main TERAN experiments.\nThe visual features extracted from Faster-RCNN are conditioned with the information related to\nthe geometry of the bounding-boxes. This is done through a simple fully-connected stack in the\nearly visual pipeline before the reasoning steps. The two linear projection layers within the TE\nmodules are used to project the visual and textual concepts in spaces having the same dimensionality.\nThen, the latest TE layers perform further processing before outputting the final features that are\nused to compute the final matching loss.\nDifferently from TERN, we initially do not share the weights of the last TE layers. We will discuss\nthe effect of weight sharing in our ablation study, in Section 8.1.\nIn our novel TERAN architecture, the features in output from the last TE layers are used to\ncompute a region-word alignment matrix ğ´âˆˆR|ğ‘”ğ‘˜ |Ã—|ğ‘”ğ‘™ |, where ğ‘”ğ‘˜ is the set of indexes of the region\nfeatures from the ğ‘˜-th image and ğ‘”ğ‘™ is the set of indexes of the words from the ğ‘™-th sentence. We\nuse cosine similarity for measuring affinity between the ğ‘–-th region and the ğ‘—-th word. If {ğ’—ğ‘– }and\n{ğ’” ğ‘— }are the sets of contextualized region and word vectors in output from the network for the k-th\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:8 Messina et al.\nA tennis player serving \na ball on the court BERT\n(pretrained)\nTEs\nTEs\nTEs\nTEs\nReasoning\nT-CLS\n......\n...... ......\n......\n......\nPooling\nI-T Similarity\nPairwise R-W Similarities\nLinear projection \nlayers\n{vi}\n{sj}\nI-CLS\nFC\nFaster-RCNN\nRegion \nFeatures\nFC\nFC\nFC\nRegion \nBounding \nBoxes\nx1, y1, \nx2, y2, \nArea\nFig. 2. The proposed TERAN architecture. TEs stands for Transformer Encoders, and it indicates a stack of\nTE layers whose internals are recalled in Section 3 and explained in detail in [53]. Region and word features\nare extracted through a bottom-up attention model based on Faster-RCNN and BERT, respectively. The final\nimage-text (I-T) similarity score is obtained by pooling a region-word (R-W) similarity matrix. Note that the\nspecial I-CLS and T-CLS are not used in the basic formulation of TERAN.\nimage and the l-th sentence respectively, then ğ´is constructed as:\nğ´ğ‘– ğ‘—= ğ’—ğ‘‡\nğ‘– ğ’” ğ‘—\nâˆ¥ğ’—ğ‘– âˆ¥âˆ¥ğ’” ğ‘— âˆ¥ ğ‘– âˆˆğ‘”ğ‘˜,ğ‘— âˆˆğ‘”ğ‘™ (2)\nAt this point, the global similarity ğ‘†ğ‘˜ğ‘™ between the ğ‘˜-th image and the ğ‘™-th sentence is computed\nby pooling this similarity matrix through an appropriate pooling function. Inspired by [24] and\n[28], we employ the max-sum pooling, which consists in computing the max over the rows of ğ´\nand then summing or, equivalently, max-over-regions sum-over-words(ğ‘€ğ‘Ÿ ğ‘†ğ‘¤) pooling. We explore\nalso the dual version, as in [28], by computing the max over the columns and then summing, or\nmax-over-words sum-over-regions(ğ‘€ğ‘¤ğ‘†ğ‘Ÿ ) pooling:\nğ‘†ğ‘€ğ‘Ÿ ğ‘†ğ‘¤\nğ‘˜ğ‘™ =\nâˆ‘ï¸\nğ‘— âˆˆğ‘”ğ‘™\nmax\nğ‘– âˆˆğ‘”ğ‘˜\nğ´ğ‘– ğ‘— ğ‘œğ‘Ÿ ğ‘† ğ‘€ğ‘¤ğ‘†ğ‘Ÿ\nğ‘˜ğ‘™ =\nâˆ‘ï¸\nğ‘– âˆˆğ‘”ğ‘˜\nmax\nğ‘— âˆˆğ‘”ğ‘™\nğ´ğ‘– ğ‘— (3)\nSince both these similarity functions are not symmetric due to the diverse outcomes we obtain by\ninverting the order of the sum and max operations, we introduce also the symmetric form, obtained\nby summing the two:\nğ‘†Symm\nğ‘˜ğ‘™ = ğ‘†ğ‘€ğ‘Ÿ ğ‘†ğ‘¤\nğ‘˜ğ‘™ +ğ‘†ğ‘€ğ‘¤ğ‘†ğ‘Ÿ\nğ‘˜ğ‘™ (4)\n4.1 Learning Objective\nGiven the global image-sentence similarities ğ‘†ğ‘˜ğ‘™ computed through alignments pooling, we can\nproceed as in previous works [9, 30] using a contrastive learning method: we use a hinge-based\ntriplet ranking loss, focusing the attention on hard negatives, as introduced by [9]. Therefore, we\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:9\nused the following loss function:\nğ¿ğ‘˜ğ‘™ = max\nğ‘™â€²\n[ğ›¼+ğ‘†ğ‘˜ğ‘™â€² âˆ’ğ‘†ğ‘˜ğ‘™ ]++\nmax\nğ‘˜â€²\n[ğ›¼+ğ‘†ğ‘˜â€²ğ‘™ âˆ’ğ‘†ğ‘˜ğ‘™ ]+\n(5)\nwhere [ğ‘¥]+â‰¡ğ‘šğ‘ğ‘¥(0,ğ‘¥)and ğ›¼ is a margin that defines the minimum separation that should hold\nbetween the truly matching word-region embeddings and the negative pairs. The hard negativesğ‘˜â€²\nand ğ‘™â€²are computed as follows:\nğ‘˜â€²= arg max\nğ‘—â‰ ğ‘˜\nğ‘†(ğ‘—,ğ‘™)\nğ‘™â€²= arg max\nğ‘‘â‰ ğ‘™\nğ‘†(ğ‘˜,ğ‘‘) (6)\nwhere (ğ‘˜,ğ‘™)is a positive pair. As in [9], the hard negatives are sampled from the mini-batch and\nnot globally, for performance reasons.\n4.2 Region and Word Features Extraction\nThe ğ¼ = {ğ‘Ÿ0,ğ‘Ÿ1,...,ğ‘Ÿ ğ‘›}and ğ¶ = {ğ‘¤0,ğ‘¤1,...,ğ‘¤ ğ‘š }initial descriptions for images and captions\ncome from state-of-the-art visual and textual pre-trained networks, Faster-RCNN with Bottom-Up\nattention and BERT respectively.\nFaster-RCNN [47] is a state-of-the-art object detector. It has been used in many downstream tasks\nrequiring salient object regions extracted from images. Therefore, Faster-RCNN is one of the main\narchitectures implementing human-like visual perception. The work in [2] introduces bottom-up\nvisual features by training Faster-RCNN with a Resnet-101 backbone on the Visual Genome dataset\n[27]. Using these features, they can reach remarkable results on the two downstream tasks of image\ncaptioning and visual question answering.\nConcerning text processing, we use BERT [7] for extracting word embeddings. BERT already\nuses a multi-layer transformer encoder to process words in sentences and capture their functional\nrelationships through the same powerful self-attention mechanism. BERT embeddings are trained\non some general natural language processing tasks such as sentence prediction or sentence classifi-\ncation and demonstrated state-of-the-art results in many downstream natural language tasks. BERT\nembeddings, unlike word2vec [44], capture the context in which each word appears. Therefore,\nevery word embedding carries information about the surrounding context, that could be different\nfrom caption to caption.\nSince the transformer encoder architecture does not embed any sequential prior in its architecture,\nwords are given a sequential order by mixing some positional information into the learned input\nembeddings. For this reason, the authors in [53] add sine and cosine functions of different frequencies\nto the input embeddings. This is a simple yet effective way to transform a set into a sequence.\n5 COMPUTATIONAL EFFICIENCY OF TERAN\nA principled objective of our work is efficient feature extraction for cross-modal retrieval applica-\ntions. In these scenarios, it is mandatory to have a separable network that can produce visual and\ntextual features by independently forwarding the two disentangled visual and textual pipelines.\nFurthermore, the similarity function should be simple, so that it is efficient to compute.\nTERAN, as well as TERN [43] and other works in literature [30, 46, 59] adhere to this principle.\nIn fact, if ğ¾ is the number of images and ğ¿the number of sentences in the database, these methods\nhave a feature space complexity, as well as a feature extraction time complexity, ofğ‘‚(ğ¾)+ğ‘‚(ğ¿).\nOther works that entangle the visual and textual pipelines, such as [4, 56, 60] require a feature\nspace, and a number of network evaluations, scaling with ğ‘‚(ğ¾ğ¿). These methods are impractical\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:10 Messina et al.\nto deploy to real-world scalable search engines. Some of these works partially solve this issue\nby keeping the two representations separated up until a certain point in the network, so that\nthese intermediate representations can be cached, as proposed in [ 40]. In all these cases, a new\nincoming query to the system needsğ‘‚(ğ¾)or ğ‘‚(ğ¿)re-evaluations of the whole network (depending\non whether we are considering image or sentence retrieval); in the best case, we need to re-evaluate\nthe last attention layers, which could be similarly expensive.\nRegarding the similarity computation, TERN uses simple dot products that enable quick and\nefficient document rankings in modern search engines. TERAN implements also a very simple\nsimilarity function, built of simple dot products and summations without including complex layers of\nmemories or attentions. This possibly enables an implementation that uses metric space approaches\nto prune the search space and obtain very efficient image or sentence rankings for a given query.\nHowever, the implementation of the TERAN similarity function in real-world search engines is left\nfor future research.\n6 NDCG METRIC FOR CROSS-MODAL RETRIEVAL\nAs of now, many works in the computer vision literature treating image-text matching measure\nthe retrieval abilities of the proposed methods by employing the well known Recall@K metric. The\nRecall@K measures the percentage of queries able to retrieve the correct item among the first K\nresults. This is a metric perfectly suitable for scenarios where the query is very specific and thus\nwe expect to find the elements that match perfectly among the first search results. However, in\ncommon search engines, the users are not asked to input a very detailed query, and they are often\nnot searching for an exact match. They expect to find in the first retrieved positions some relevant\nresults, with relevance defined using some pre-defined and often subjective criterion.\nFor this reason, inspired by the work in [ 3] and following the novel ideas introduced by our\nprevious work on TERN [43], we employ a common metric often used in information retrieval\napplications, the Normalized Discounted Cumulative Gain (NDCG). The NDCG is able to evaluate\nthe quality of the ranking produced by a certain query by looking at the first ğ‘ positions of the\nranked elements list. The premise of NDCG is that highly relevant items appearing lower in a\nsearch result list should be penalized as the graded relevance value is reduced proportionally to the\nposition of the result.\nThe NDCG until position ğ‘ is defined as follows:\nNDCGğ‘ = DCGğ‘\nIDCGğ‘\n, where ğ·ğ¶ğºğ‘ =\nğ‘âˆ‘ï¸\nğ‘–=1\nrelğ‘–\nlog2 (ğ‘–+1); (7)\nrelğ‘– is a positive number encoding the affinity that the ğ‘–-th element of the retrieved list has\nwith the query element, and IDCGğ‘ is the DCGğ‘ of the best possible ranking. Thanks to this\nnormalization, NDCGğ‘ acquires values in the range [0,1].\nThe relğ‘– values can be computed using well-established sentence similarity scores between a\nsentence and the sentences associated with a certain image. More formally, we could think of\ncomputing relğ‘– = ğœ(Â¯ğ¶ğ‘–,ğ¶ğ‘— ), where Â¯ğ¶ğ‘– is the set of all captions associated to the image ğ¼ğ‘– , and\nğœ : SÃ—S â†’[0,1]is a similarity function defined over a pair of sentences returning their normalized\nsimilarity score. With this simple expedient, we could efficiently compute quite large relevance\nmatrices using similarities defined over captions, which are in general computationally much\ncheaper than similarities computed between images and sentences directly.\nWe thus compute the ğ‘Ÿğ‘’ğ‘™ğ‘– value in the following ways:\nâ€¢relğ‘– = ğœ(Â¯ğ¶ğ‘–,ğ¶ğ‘— )in case of image retrieval, where ğ¶ğ‘— is the query caption\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:11\nâ€¢relğ‘– = ğœ(Â¯ğ¶ğ‘—,ğ¶ğ‘– )in case of caption retrieval, where Â¯ğ¶ğ‘— is the set of captions associated to the\nquery image ğ¼ğ‘— .\nIn our work, we use ROUGE-L[33] and SPICE[1] as sentence similarity functions ğœ for computing\ncaption similarities. These two scoring functions capture different aspects of the sentences. In\nparticular, ROUGE-L operates on the longest common sub-sequences, while SPICE exploits graphs\nassociated with the syntactic parse trees, and has a certain degree of robustness against synonyms.\nIn this way, SPICE is more sensitive to high-level features of the text and semantic dependencies\nbetween words and concepts rather than to pure syntactic constructions.\n7 EXPERIMENTS\nWe trained the TERAN architecture and we measured its performance on the MS-COCO [34] and\nFlickr30k datasets [64], computing the effectiveness of our approach on the image retrieval and\nsentence retrieval tasks. We compared our results against state-of-the-art approaches on the same\ndatasets, using the introduced NDCG and the already-in-use Recall@K metrics.\nThe MS-COCO dataset comes with a total of 123,287 images. Every image has associated a set of 5\nhuman-written captions describing the image. We follow the splits introduced by [24] and followed\nby the subsequent works in this field [ 9, 10, 30]. In particular, 113,287 images are reserved for\ntraining, 5,000 for validating, and 5,000 for testing. Differently, Flickr30k consists of 31,000 images\nand 158,915 English texts. Like MS-COCO, each image is annotated with 5 captions. Following the\nsplits by [24], we use 29,000 images for training, 1,000 images for validation, and the remaining\n1,000 images for testing. For MS-COCO, at test time the results for both 5k and 1k test-sets are\nreported. In the case of 1k images, the results are computed by performing 5-fold cross-validation\non the 5k test split and averaging the outcomes.\nWe computed caption-caption relevance scores for the NDCG metric using ROUGE-L[33] and\nSPICE[1], as explained in Section 6, and we set the NDCG parameter ğ‘ = 25 as in [ 3] in our\nexperiments. We employed the NDCG metrics measured during the validation phase for choosing\nthe best performing model to be used during the test phase.\nFor a better comparison with our previous TERN approach, we included three more targeted\nexperiments. In the first two, called TERN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ Test and TERN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ Test we used the best-\nperforming TERN model, trained as explained in [43], testing it using theğ‘€ğ‘¤ğ‘†ğ‘Ÿ and ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ alignments\ncriteria respectively. TERN is effectively able to output features for every image region or word;\nhowever, it is never constrained to produce meaningful descriptions out of these sets of features;\nhence, this trial is aimed at checking the quality of the alignment of the concepts in output from\nthe previous TERN architecture. In the third experiment, calledTERN w. Align, we tried to integrate\nthe objectives of both TERN and TERAN during training, by combining their losses using the\nuncertainty weighting method proposed in [25], and testing the model using the TERN inference\nprotocol. Thus, in this experiment, we effectively reuse the I-CLS and T-CLS tokens as global\ndescriptions for images and sentences, as described in [43]. This experiment aimed to evaluate if the\nTERAN alignment objective can help TERN learn better fixed-sized global vectorial descriptions.\n7.1 Implementation Details\nWe employ the BERT model pre-trained on the masked language task on English sentences, using\nthe PyTorch implementation by HuggingFace1. These pre-trained BERT embeddings are 768-D.\nFor the visual pipeline, we extracted the bottom-up features from the work by [2], using the code\nand pre-extracted features provided by the authors 2. Specifically, for MS-COCO we used the\n1https://github.com/huggingface/transformers\n2https://github.com/peteanderson80/bottom-up-attention\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:12 Messina et al.\nalready-extracted bottom-up features, while we extracted from scratch the features for Flickr30k\nusing the available pre-trained model.\nIn the experiments, we used the bottom-up features containing the top 36 most confident\ndetections, although our pipeline already handles variable-length sets of regions for each image by\nappropriately masking the attention weights in the TE layers.\nConcerning the reasoning steps, we used a stack of 4 TE layers for visual reasoning. We found the\nbest results when fine-tuning the BERT pre-trained model, so we did not add further reasoning TE\nlayers for the textual pipeline. The final common space, as in [9], is 1024-dimensional. We linearly\nprojected the visual and textual features to a 1024-d space and then we processed the resulting\nfeatures using 2 final TEs before computing the alignment matrix.\nAll the TEs feed-forward layers are 2048-dimensional and the dropout is set to 0.1. We trained\nfor 30 epochs using Adam optimizer with a batch size of 40 and a learning rate of 1ğ‘’âˆ’5 for the\nfirst 20 epochs and 1ğ‘’âˆ’6 for the remaining 10 epochs. The ğ›¼ parameter of the hinge-based triplet\nranking loss is set to 0.2, as in [9, 30].\n7.2 Results\nWe compare our TERAN method against the following baselines: JGCAR [55], SAN [21], VSE++ [9],\nSMAN [22], M3A-Net [20], AAMEL [57], MRNN [24], SCAN [28], SAEM [59], CASC [60], MMCA\n[58], VSRN [30], PFAN [56], Full-IMRAM [4], and CAMERA [ 46]. We clustered these methods\nbased on the visual feature extractor they use: VGG, ResNet, or Region CNN (e.g., Faster-RCNN).\nTo have a better comparison with our method, we also annotated in the tables whenever they use\nBERT as the textual model, or if they use disentangled visual-textual pipelines for efficient feature\ncomputation, as explained in Section 5. Also note that many of the listed methods report the results\nusing an ensemble of two models having different training initialization parameters, where the\nfinal similarity is obtained by averaging the scores in output from each model. Hence, we reported\nalso our ensemble results, for a better comparison with these baselines. In the tables, we indicate\nensemble methods postponing \"(ens.)\" to the method name.\nWe used the original implementations from their respective GitHub repositories to compute the\nNDCG metrics for the baselines, where possible. In the case of missing pre-trained models, we were\nnot able to produce consistent results with the original papers. In this case, we do not report the\nNDCG metrics (\"-\").\nOn both the 1K and 5K test sets, our novel TERAN approach reaches state-of-the-art results\non almost all the metrics. Concerning the results reported in Table 1 regarding 1K test set, the\nbest performing TERAN model is the one implementing the max-over-regions sum-over-words\n(ğ‘€ğ‘Ÿ ğ‘†ğ‘¤) pooling method, although the model using the symmetric loss reaches comparable results.\nWe chose the same TERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ model to evaluate the ensemble, reaching an improvement of\n5.7% and 3.5% on the Recall@1 metric on image and sentence retrieval respectively, with respect to\nthe best baseline using ensemble methods, which is CAMERA [46]. Notice, however, that even the\nbasic TERAN model without ensemble is able to surpass CAMERA in many metrics. This confirms\nthe power of the TERAN model despite its overall simplicity.\nTable 2 reports the results for the 5K test set, which confirm the superiority of TERANğ‘€ğ‘Ÿ ğ‘†ğ‘¤ over\nall the baselines also on the full test set. In this scenario, we increase the Recall@1 performance\nby 11.3% and 7.6% on image and sentence retrieval with respect to the CAMERA approach. On\nthe other hand, the max-over-words sum-over-regions (ğ‘€ğ‘¤ğ‘†ğ‘Ÿ ) method loses around 10% on the\nRecall@1 metrics with respect to the best performing TERAN non-ensemble model. In this case,\nthe Recall@K metric does not improve over top results obtained by the current state-of-the-art\napproaches. Nevertheless, this model loses only about 1.5% during image-retrieval and about\n3.5% during sentence-retrieval as far as the SPICE NDCG metric is concerned, reaching perfectly\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:13\nTable 1. Results on the MS-COCO dataset, on the 1K test set.\nImage Retrieval Sentence Retrieval\nRecall@K NDCG Recall@K NDCG\nModel K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE\n(VGG)\nJGCAR [55] 40.2 74.8 85.7 - - 52.7 82.6 90.5 - -\nSAN [21] 60.8 90.3 95.7 - - 74.9 94.9 98.2 - -\n(ResNet)\nVSE++ [9] â€  52.0 84.3 92.0 0.712 0.617 64.6 90.0 95.7 0.705 0.658\nSMAN [22] 58.8 87.4 93.5 - - 68.4 91.3 96.6 - -\nM3A-Net [20] 58.4 87.1 94.0 - - 70.4 91.7 96.8 - -\nAAMEL [57] 59.9 89.0 95.1 - - 74.3 95.4 98.2 - -\n(Region CNN)\nMRNN [24] 27.4 60.2 74.8 - - 38.4 69.9 80.5 - -\nSCAN (ens.) [28] 58.8 88.4 94.8 - - 72.7 94.8 98.4 - -\nSAEM (ens.)[59] Â§â€  57.8 88.6 94.9 - - 71.2 94.1 97.7 - -\nCASC [60] 58.9 89.8 96.0 - - 72.3 96.0 99.0 - -\nMMCA [58] Â§ 61.6 89.8 95.2 - - 74.8 95.6 97.7 - -\nVSRN [30] â€  60.8 88.4 94.1 0.723 0.621 74.0 94.3 97.8 0.737 0.690\nVSRN (ens.) [30] â€  62.8 89.7 95.1 0.732 0.637 76.2 94.8 98.2 0.748 0.704\nPFAN (ens.) [56] 61.6 89.6 95.2 - - 76.5 96.3 99.0 - -\nFull-IMRAM [4] 61.7 89.1 95.0 - - 76.7 95.6 98.5 - -\nCAMERA [46] Â§â€  62.3 90.1 95.2 - - 75.9 95.5 98.6 - -\nCAMERA (ens.) [46] Â§â€  63.4 90.9 95.8 - - 77.5 96.3 98.8 - -\nTERN [43] 51.9 85.6 93.6 0.725 0.653 63.7 90.5 96.2 0.716 0.674\nTERN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤Test 51.5 84.9 93.1 0.722 0.642 26.6 70.3 86.3 0.568 0.530\nTERN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ Test 51.2 84.6 92.9 0.722 0.643 61.9 88.9 95.7 0.713 0.666\nTERN w. Align 54.5 86.9 94.2 0.724 0.643 65.5 91.0 96.5 0.720 0.675\nTERAN Symm. 63.5 91.1 96.3 0.739 0.666 76.3 95.3 98.4 0.741 0.701\nTERAN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ 57.5 88.4 94.9 0.730 0.658 70.8 93.5 97.3 0.725 0.681\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ 65.0 91.2 96.4 0.741 0.668 77.7 95.9 98.6 0.746 0.707\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ (ens.) 67.0 92.2 96.9 0.747 0.680 80.2 96.6 99.0 0.756 0.720\nÂ§ Uses BERT as language model\nâ€  Uses disentangled visual-textual pipelines\ncomparable results with our state-of-the-art method. In light of these results, we deduce that the\nğ‘€ğ‘¤ğ‘†ğ‘Ÿ model is not so effective in retrieving the perfect-matching elements; however, it is still very\ngood at retrieving the relevant ones.\nAs far as image retrieval is concerned, in the TERN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ Test and TERN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ Test experiments\nwe can see that the TERN architecture trained as in [43] performs fairly good when the similarity\nis computed as in the novel TERAN architecture, using the region and words outputs and not the\nI-CLS and T-CLS global descriptions. In particular, the use of max-over-words sum-over-regions\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:14 Messina et al.\nTable 2. Results on the MS-COCO dataset, on the 5K test set.\nImage Retrieval Sentence Retrieval\nRecall@K NDCG Recall@K NDCG\nModel K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE\n(ResNet)\nVSE++ [9] â€  30.3 59.4 72.4 0.656 0.577 41.3 71.1 81.2 0.597 0.551\nM3A-Net [20] 38.3 65.7 76.9 - - 48.9 75.2 84.4 - -\nAAMEL [57] 39.9 71.3 81.7 - - 51.9 84.2 91.2 - -\n(Region CNN)\nMRNN [24] 10.7 29.6 42.2 - - 16.5 39.2 52.0 - -\nSCAN (ens.) [28] 38.6 69.3 80.4 - - 50.4 82.2 90.0 - -\nVSRN [30] â€  37.9 68.5 79.4 0.676 0.596 50.3 79.6 87.9 0.639 0.598\nVSRN (ens.) [30] â€  40.5 70.6 81.1 0.684 0.609 53.0 81.1 89.4 0.652 0.612\nFull-IMRAM [4] 39.7 69.1 79.8 - - 53.7 83.2 91.0 - -\nMMCA [58] Â§ 38.7 69.7 80.8 - - 54.0 82.5 90.7 - -\nCAMERA [46] Â§â€  39.0 70.5 81.5 - - 53.1 81.3 89.8 - -\nCAMERA (ens.) [46] Â§â€  40.5 71.7 82.5 - - 55.1 82.9 91.2 - -\nTERN [43] 28.7 59.7 72.7 0.665 0.599 38.4 69.5 81.3 0.601 0.556\nTERN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤Test 28.3 59.1 72.2 0.663 0.592 6.8 28.4 46.7 0.406 0.372\nTERN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ Test 28.1 58.6 71.8 0.663 0.592 35.5 67.5 78.9 0.600 0.551\nTERN w. Align 31.4 62.5 75.3 0.667 0.597 40.2 71.1 81.9 0.606 0.561\nTERAN Symm. 41.0 71.6 82.3 0.680 0.607 54.8 82.7 90.9 0.641 0.601\nTERAN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ 34.1 65.7 77.8 0.669 0.596 45.3 76.3 86.2 0.611 0.564\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ 42.6 72.5 82.9 0.682 0.610 55.6 83.9 91.6 0.643 0.606\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤(ens.) 45.1 74.6 84.4 0.689 0.622 59.3 85.8 92.4 0.658 0.624\nÂ§ Uses BERT as language model\nâ€  Uses disentangled visual-textual pipelines\nsimilarity still works quite well compared to the similarity computed through I-CLS and T-CLS\nglobal visual and textual features as it is in TERN.\nNotice instead that on the sentence retrieval task, theTERN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ Test experiment obtains a very\nlow performance. This is the consequence of the fact that TERN is trained to produce global-scale\nimage-sentence matchings, while it is never forced to produce meaningful fine-grained aligned\nconcepts. This is further supported by the evidence that if we visualize the region-words alignments\nas explained in the following Section 8.5 we obtain random word groundings on the image, meaning\nthat the concepts in output from TERN are not sufficiently informative.\nIn order to better compare TERAN with our previous TERN approach, in Figure 3 we report\nthe validation curves for both NDCG and Recall@1 metrics, for both methods. We can notice how\nthe NDCG metric overfits in our previous TERN model, especially when using the SPICE metric,\nwhile the Recall@ keeps increasing. On the other hand, TERAN demonstrates better generalization\nabilities on both metrics. This is a clear indication that TERAN is better able to retrieve relevant\nitems in the first positions, as well as exact matching elements. Instead, TERN is more prone to\noverfitting to the SPICE metric, meaning that at a certain point in training, the network still searches\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:15\n0 100 k\n 200 k\n 300 k\n 400 k\nIteration\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74NDCG\nTERAN - ROUGE-L\nTERAN - SPICE\nTERN - ROUGE-L\nTERN - SPICE\n0 100 k\n 200 k\n 300 k\n 400 k\nIteration\n30\n40\n50\n60R@1\nTERAN\nTERN\nFig. 3. Validation metrics on sentence-to-image retrieval, measured during the training phase, for the average-\nover-sentences scenario. TERN overfits on the NDCG metrics, while Recall@1 still improves. TERAN instead\ngeneralizes better on both metrics.\nfor the top matching element, but with a tendency to push away possible relevant results compared\nto the novel TERAN approach.\nHowever, looking at the results from the TERN w. Alignexperiment, we can notice that by\naugmenting the TERN objective with the TERAN alignment loss, we can slightly increase the TERN\noverall performance. This confirms that a more precise and meaningful region-word alignment has\na visible effect also on the quality of the fixed-sized global embeddings produced by TERN.\nIn Table 3 we report the results on the Flickr30k dataset. Our single-model TERANğ‘€ğ‘Ÿ ğ‘†ğ‘¤ method\noutperforms the best baseline (CAMERA) on the image retrieval task while approaching the single-\nmodel CAMERA performance on the sentence retrieval task. Nevertheless, even on Flickr30k our\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ method with model ensemble obtains state-of-the-art results with respect to all\nthe baselines on all the metrics, gaining 4.6% and 1.5% on the Recall@1 metric on the image and\nsentence retrieval tasks respectively.\nOn the MS-COCO dataset, our system powered by a single GTX 1080Ti can compute a single\nimage-to-sentence query in âˆ¼0.12ğ‘  on 5k sentences of the test split; in the sentence-to-image\nscenario, it can produce scores and rank the 1K images in âˆ¼0.02ğ‘ . These timings allow the TERAN\nscores to be effectively used, for example, in a re-ranking phase, where the first 1k images - 5k\nsentences have been previously retrieved using a faster descriptor (e.g., the one from TERN).\n7.3 Qualitative Analysis for Image Retrieval\nThe visualization of image retrieval results is a good way to qualitatively appreciate the retrieval\nabilities of the proposed TERAN model. Figures 4 and 5 show examples of images retrieved given\na textual caption as a query, with scores computed using the max-over-regions sum-over-words\nmethod. In particular, Figure 4 shows image retrieval results for a couple of flexible query captions.\nThe red-marked images represent the exact-matching elements from the ground-truth. We can\ntherefore conclude that the retrieved images in these examples are incorrect results for the Recall@1\nmetric (and for the first query even for Recall@5). However, in the very first positions, we find\nnon-matching yet relevant images, due to the ambiguity of the query caption. These are common\nexamples where NDCG succeeds over the Recall@K metric since we need a flexible evaluation for\nnot-too-strict query captions.\nFigure 5 reports instead image retrieval results for a couple of very specific query captions. For\nthe first two queries, the network succeeds in positioning the only really relevant image in the\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:16 Messina et al.\nTable 3. Results on the Flickr30k dataset.\nImage Retrieval Sentence Retrieval\nRecall@K NDCG Recall@K NDCG\nModel K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE\n(VGG)\nJGCAR [55] 35.2 62.0 72.4 - - 44.9 75.3 82.7 - -\nSAN [21] 51.4 77.2 85.2 - - 67.0 88.0 94.6 - -\n(ResNet)\nVSE++ [9] â€  39.6 70.1 79.5 0.631 0.494 52.9 80.5 87.2 0.601 0.514\nTIMAM [50] Â§â€  42.6 71.6 81.9 - - 53.1 78.8 87.6 - -\nSMAN [22] 43.4 73.7 83.4 - - 57.3 85.3 92.2 - -\nM3A-Net [20] 44.7 72.4 81.1 - - 58.1 82.8 90.1 - -\nAAMEL [57] 49.7 79.2 86.4 - - 68.5 91.2 95.9 - -\n(Region CNN)\nMRNN [24] 15.2 37.7 50.5 - - 22.2 48.2 61.4 - -\nSCAN (ens.) [28] 48.6 77.7 85.2 - - 67.4 90.3 95.8 - -\nPFAN (ens.) [56] 50.4 78.7 86.1 - - 70.0 91.8 95.0 - -\nSAEM (ens.)[59] Â§â€  52.4 81.1 88.1 - - 69.1 91.0 95.1 - -\nVSRN [30] â€  53.0 77.9 85.7 0.673 0.545 70.4 89.2 93.7 0.676 0.592\nVSRN (ens.) [30] â€  54.7 81.8 88.2 0.680 0.556 71.3 90.6 96.0 0.688 0.606\nFull-IMRAM [4] 53.9 79.4 87.2 - - 74.1 93.0 96.6 - -\nMMCA [58] Â§ 54.8 81.4 87.8 - - 74.2 92.8 96.4 - -\nCASC [60] 60.2 78.3 86.3 - - 68.5 90.6 95.9 - -\nCAMERA [46] Â§â€  58.9 84.7 90.2 - - 76.5 95.1 97.2 - -\nCAMERA (ens.) [46] Â§â€  60.3 85.9 91.7 - - 78.0 95.1 97.9 - -\nTERN [43] 41.1 71.9 81.2 0.647 0.512 53.2 79.4 86.0 0.624 0.529\nTERAN Symm. 55.7 83.1 89.3 0.678 0.555 71.8 90.5 94.7 0.676 0.603\nTERAN ğ‘€ğ‘¤ğ‘†ğ‘Ÿ 49.4 78.3 85.9 0.664 0.536 60.5 85.1 92.2 0.651 0.558\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ 59.5 84.9 90.6 0.686 0.564 75.8 93.2 96.7 0.687 0.614\nTERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ (ens.) 63.1 87.3 92.6 0.695 0.577 79.2 94.4 96.8 0.707 0.636\nÂ§ Uses BERT as language model\nâ€  Uses disentangled visual-textual pipelines\nfirst position (a dog sitting on a bench on the upper query, and Pennsylvania Avenue, uniquely\nidentifiable by the street sign, on the lower query). In this case, the Recall@1 metric also succeeds,\ngiven that the query captions are very selective. The third example, instead, evidences a failure\ncase where the model cannot deal with very subtle details. The (only) correct result is ranked 6th in\nthis case; in the first ranking positions, the model can find images with a vase used as a centerpiece,\nbut the table is not often visible, and when it is visible, it is not in the corner of the room.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:17\nQuery: A large jetliner sitting on top of an airport runway.\nQuery: An eating area with a table and a few chairs.\nFig. 4. Example of image retrieval results for a couple of flexible query captions. These are common examples\nwhere NDCG succeeds over the Recall@K metric. The ground-truth matching image is not among the very\nfirst positions; however, the top-ranked images are also visually very relevant.\nQuery: A large white dog is sitting on a bench beside an elderly man.\nQuery: An old black and white photo of Pennsylvania Avenue.\nQuery: Table situated in corner of room with a vase for a center piece.\nFig. 5. Example of image retrieval results for a couple of very specific query captions.\n8 ABLATION STUDY\n8.1 The Effect of Weight Sharing\nWe tried to apply weight sharing for the last 2 layers of the TERAN architecture, those after the\nlinear projection to the 1024-d space. Weight sharing is used to reduce the size of the network\nand enforce a structure able to perform common reasoning on the high-level concepts, possibly\nreducing the overfitting and increasing the stability of the whole network. We experimented with\nthe effects of weight sharing on the MS-COCO dataset with 1K test set, for both the max-over-words\nsum-over-regions and the max-over-regions sum-over-words scenarios.\nResults are shown in the 2-nd and 6-th rows of Table 4. It can be noticed that the values are\nperfectly comparable with the TERAN results reported in Table 1, suggesting that at this point in\nthe network the abstraction is high enough that concepts coming from images and sentences can\nbe processed in the exact same way. This result shows that vectors at this stage have been freed\nfrom any modality bias and they are fully comparable in the same representation space.\nAlso, in the max-over-words sum-over-regions scenario (6-th row), there is a small gain both in\nterms of Recall@K and NDCG. This confirms the slight regularization effect of the weight sharing\napproach.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:18 Messina et al.\n0 100 k\n 200 k\n 300 k\n 400 k\nIteration\n0.60\n0.65\n0.70NDCG\nROUGE-L\nSPICE\n0 100 k\n 200 k\n 300 k\n 400 k\nIteration\n40\n50\n60R@1\nFig. 6. Validation metrics measured during the training phase, for the average-over-sentences scenario. This\nmodel overfits on the NDCG metrics on the image-retrieval task, while Recall@1 still improves.\n8.2 Averaging Versus Summing\nWe tried to do average instead of sum during the last pooling phase of the alignment matrix. We\nconsider only the case in which we average-over-sentences; in fact, since in our experiments the\nnumber of visual concepts is always fixed to the 36 more influent ones during the object detection\nstage, average-over-regions and sum-over-regions do not differ substantially.\nThus, we considered the case of max-over-regions average-over-words (ğ‘€ğ‘Ÿ Avgğ‘¤):\nğ‘†ğ‘˜ğ‘™ =\nÃ\nğ‘— âˆˆğ‘”ğ‘™ maxğ‘– âˆˆğ‘”ğ‘˜ ğ´ğ‘– ğ‘—\n|ğ‘”ğ‘™ | . (8)\nIf we compute the average instead of the sum in the max-over-regions sum-over-words scenario,\nthe final similarity score between the image and the sentence is no more dependent on the number\nof concepts from the textual pipeline: the similarities are averaged and not accumulated.\nIn the 3-rd row of Table 4 we can notice that by averaging we lose an important amount of\ninformation with respect to the max-over-regions sum-over-words scenario (1-st row). This insight\nsuggests that the complexity of the query is beneficial for achieving high-quality matching.\nAnother side effect of using average instead of the max is the premature clear overfitting on the\nNDCG metrics as far as image-retrieval is concerned. The effect is shown in Figure 6. The clear\noverfitting of the NDCG metrics resembles the training curve trajectories of TERN (Figure 3). This\nresult demonstrates that although this model can correctly perform exact matching, it is pulling\naway relevant results from the head of the ranked list of images, during the validation phase.\n8.3 Removing Stop-Words During Alignment\nSome words may carry no substantial meaning by themselves, such as articles or prepositions.\nThese words with a high and diffuse frequency of use are typically calledstop-words and are usually\nremoved in classical text analysis processes. In this context, removing stop-words may help the\narchitecture to focus only on the important concepts. Doing so, the training process is simplified as\nthe noise introduced by possibly irrelevant words is removed. Results are reported in the 4-th row\nof Table 4. The overall performance, both in terms of Recall@ and NDCG is comparable, yet with\na small decrease, with the one obtained without stop-words removal (1-st row of the table). This\nsuggests that in this context stop-words are linguistic elements that bring some useful information\nto distinguish ambiguous scenes. Prepositions and adverbs often indicate the spatial arrangement\nof objects, thus \"chair near the table\" is not the same as \"chair over the table\"; distinguishing these\nfine-grained differences is beneficial for obtaining a precise image-text matching.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:19\nTable 4. Results for the ablation study experiments. We organize the methods in the table clustering them by\nthe pooling method, for an easier comparison (max-over-regions methods in the upper part and max-over-\nwords methods on the lower part). In the first row of both sections we report the TERAN results from Table 1.\nExperiments are computed on the MS-COCO dataset, 1K test set.\nImage Retrieval Sentence Retrieval\nRecall@K NDCG Recall@K NDCG\nModel K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE\nğ‘€ğ‘Ÿ ğ‘†ğ‘¤ (from Table 1) 65.0 91.2 96.4 0.741 0.668 77.7 95.9 98.6 0.746 0.707\nğ‘€ğ‘Ÿ ğ‘†ğ‘¤ Shared-W 64.5 91.3 96.3 0.740 0.667 77.3 95.9 98.4 0.746 0.706\nğ‘€ğ‘Ÿ Avgğ‘¤ 57.2 87.6 93.6 0.705 0.587 68.6 92.4 96.7 0.721 0.671\nğ‘€ğ‘Ÿ ğ‘†ğ‘¤ StopWordsFilter 64.2 91.1 96.3 0.737 0.658 76.8 95.9 98.6 0.745 0.705\nğ‘€ğ‘Ÿ ğ‘†ğ‘¤ Bi-LSTM 55.6 86.9 93.9 0.734 0.666 67.4 92.5 96.9 0.717 0.677\nğ‘€ğ‘Ÿ ğ‘†ğ‘¤ Bi-GRU 56.3 87.1 94.0 0.735 0.666 69.1 93.4 97.1 0.720 0.678\nğ‘€ğ‘¤ğ‘†ğ‘Ÿ (from Table 1) 57.5 88.4 94.9 0.730 0.658 70.8 93.5 97.3 0.725 0.681\nğ‘€ğ‘¤ğ‘†ğ‘Ÿ Shared-W 58.1 88.4 95.0 0.730 0.657 71.1 93.1 97.7 0.728 0.683\n8.4 Using Different Language Models\nDespite the power of BERT [ 7] for obtaining contextualized representations for the words in a\nsentence, many works use recurrent bidirectional networks instead, such as Bi-GRU or Bi-LSTMs.\nIn the 5-th and 6-th row of Table 4 we report the results for the TERAN model with Bi-GRU and\nBi-LSTM in substitution of the BERT model for language processing. We used 300-dimensional\nword embeddings, and a hidden size of 512 so that the final bi-directional sentence feature is a\n1024-dimensional description; we used the same training protocol and hyper-parameters used for\nthe main experiments. The results suggest that BERT is an essential ingredient for reaching top\nresults on the Recall@K metrics, especially when K={1, 5}. In particular, Bi-LSTM and Bi-GRU lose\naround 14% on image retrieval and 12% on sentence retrieval on the Recall@1 metric compared to\nthe TERAN ğ‘€ğ‘Ÿ ğ‘†ğ‘¤ single-model method. However, we can notice that TERAN with these recurrent\nlanguage models still maintains a comparable performance with respect to the NDCG metric,\nespecially on the image retrieval task.\n8.5 Visualizing the Visual-Word Alignments\nInspired by the work in [24], we try to visualize the region-word alignments learned by TERAN on\nsome images from the test set of MS-COCO dataset. We recall that no supervision was used at the\nregion-word level during the training phase.\nIn Figure 7, we report some figures where every sentence word has been associated with the top-\nrelevant image region. The affinity between visual concepts (region features) and textual concepts\n(word features) has been measured through cosine similarity, just as during the training phase.\nWe can see that the words have overall plausible groundings on the image they describe. Some\nwords are really difficult to ground, such as articles, verbs, or adjectives. However, we can notice that\nphrases describing a visual entity and composed of nouns with the related articles and adjectives\n(e.g. \"a green tie\", or \"a wooden table\") are often grounded to the same region. This further confirms\nthat the TERAN architecture can produce meaningful concepts, and it is also able to cluster them\nunder the form of complete reasonable phrases.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:20 Messina et al.\nFig. 7. Visualization of the word-region alignments. Near each word, we report the cosine similarity computed\nbetween that word and the top-relevant image region associated with it. We slightly offset the overlapping\nbounding-boxes for a better visualization.\nWe can notice some wrong word groundings in the images, such as the phrase \"eyes closed\"\nthat is associated with the region depicting the closed mouth. In this case, the error seems to lie\non some localized misunderstanding of the scene (in this case the noun \"eyes\" has probably been\nmisunderstood since the mouth and the eyes are both closed). Overall, however, complex scenes\nare correctly broken down into their salient elements, and only the key regions are attended.\n9 CONCLUSIONS\nIn this work, we introduced the Transformer Encoder Reasoning and Alignment Network (TERAN).\nTERAN is a relationship-aware architecture based on the Transformer Encoder (TE) architecture,\nexploiting self-attention mechanisms, able to reason about the spatial and abstract relationships\nbetween elements in the image and in the text separately.\nDifferently from TERN [43], TERAN forces a fine-grained alignment among the region and word\nfeatures without any supervision at this level. We demonstrated that by enforcing this fine-grained\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:21\nword-region alignment at training time we can obtain state-of-the-art results on the popular MS-\nCOCO and Flickr30K datasets. Besides, thanks to the overall simplicity of the proposed model, we\ncan obtain effective visual and textual features for use in scalable retrieval setups.\nWe measured the performance of our TERAN architecture in the context of cross-modal retrieval\nusing both the already-in-use Recall@K metric and the newly introduced NDCG with the ROUGE-L\nand SPICE textual relevance measures. In spite of its simplicity, TERAN can outperform current\nstate-of-the-art models on these two retrieval metrics, competing with the currently very effective\nentangled visual-textual matching models, which on the contrary are not able to produce features for\nscalable retrieval. Furthermore, we showed that TERAN can successfully output visually-pleasant\nword-region alignments. We also observed that a further reduction of the network complexity can\nbe obtained by sharing the weights of the last TE layers. This has important benefits also on the\nstability and in the generalization abilities of the whole architecture.\nIn the end, we think that this work proposes an interesting path towards efficient and effective\ncross-modal information retrieval.\nACKNOWLEDGMENTS\nThis work was partially supported by â€œIntelligenza Artificiale per il Monitoraggio Visuale dei Siti\nCulturali\" (AI4CHSites) CNR4C program, CUP B15J19001040004, by the AI4EU project, funded by\nthe EC (H2020 - Contract n. 825619), and AI4Media under GA 951911.\nREFERENCES\n[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image\ncaption evaluation. In European Conference on Computer Vision. Springer, 382â€“398.\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018.\nBottom-up and top-down attention for image captioning and visual question answering. In Proc. of the IEEE conference\non computer vision and pattern recognition. 6077â€“6086.\n[3] Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio Falchi, and Alejandro Moreo. 2018. Picture it in your mind:\nGenerating high level visual representations from textual descriptions. Information Retrieval J.21, 2-3 (2018), 208â€“229.\n[4] Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, and Jungong Han. 2020. IMRAM: Iterative Matching with\nRecurrent Attention Memory for Cross-Modal Image-Text Retrieval. In Proc. of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 12655â€“12663.\n[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019.\nUniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740(2019).\n[6] Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2019. Show, control and tell: A framework for generating\ncontrollable and grounded captions. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. 8307â€“8316.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. InNAACL-HLT 2019. Association for Computational Linguistics, 4171â€“4186.\n[8] Aviv Eisenschtat and Lior Wolf. 2017. Linking image and text with 2-way nets. In Proc. of the IEEE conference on\ncomputer vision and pattern recognition. 4601â€“4611.\n[9] Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. VSE++: Improving Visual-Semantic Embeddings\nwith Hard Negatives. In BMVC 2018. BMVA Press, 12.\n[10] Jiuxiang Gu, Jianfei Cai, Shafiq R Joty, Li Niu, and Gang Wang. 2018. Look, imagine and match: Improving textual-visual\ncross-modal retrieval with generative models. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition.\n7181â€“7189.\n[11] Yawen Guo, Hui Yuan, and Kun Zhang. 2020. Associating Images with Sentences Using Recurrent Canonical Correlation\nAnalysis. Applied Sciences10, 16 (2020), 5516.\n[12] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to reason: End-to-end\nmodule networks for visual question answering. In Proc. of the IEEE International Conference on Computer Vision.\n804â€“813.\n[13] Feiran Huang, Xiaoming Zhang, Zhonghua Zhao, and Zhoujun Li. 2018. Bi-directional spatial-semantic attention\nnetworks for image-text matching. IEEE Transactions on Image Processing28, 4 (2018), 2008â€“2020.\n[14] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. 2019. Attention on attention for image captioning. InProc.\nof the IEEE International Conference on Computer Vision. 4634â€“4643.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\n0:22 Messina et al.\n[15] Yan Huang and Liang Wang. 2019. Acmm: Aligned cross-modal memory for few-shot image and sentence matching.\nIn Proc. of the IEEE International Conference on Computer Vision. 5774â€“5783.\n[16] Yan Huang, Wei Wang, and Liang Wang. 2017. Instance-aware image and sentence matching with selective multimodal\nlstm. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition. 2310â€“2318.\n[17] Yan Huang, Qi Wu, Chunfeng Song, and Liang Wang. 2018. Learning semantic concepts and order for image and\nsentence matching. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition. 6163â€“6171.\n[18] Yan Huang, Qi Wu, Wei Wang, and Liang Wang. 2018. Image and sentence matching via semantic concepts and order\nlearning. IEEE transactions on pattern analysis and machine intelligence(2018).\n[19] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-BERT: Aligning Image Pixels\nwith Text by Deep Multi-Modal Transformers. arXiv preprint arXiv:2004.00849(2020).\n[20] Zhong Ji, Zhigang Lin, Haoran Wang, and Yuqing He. 2020. Multi-Modal Memory Enhancement Attention Network\nfor Image-Text Matching. IEEE Access8 (2020), 38438â€“38447.\n[21] Zhong Ji, Haoran Wang, Jungong Han, and Yanwei Pang. 2019. Saliency-guided attention network for image-sentence\nmatching. In Proc. of the IEEE International Conference on Computer Vision. 5754â€“5763.\n[22] Zhong Ji, Haoran Wang, Jungong Han, and Yanwei Pang. 2020. SMAN: Stacked Multimodal Attention Network for\nCross-Modal Image-Text Retrieval. IEEE Transactions on Cybernetics(2020).\n[23] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross\nGirshick. 2017. Inferring and executing programs for visual reasoning. In Proc. of the IEEE International Conference on\nComputer Vision. 2989â€“2998.\n[24] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In Proc. of\nthe IEEE conference on computer vision and pattern recognition. 3128â€“3137.\n[25] Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task learning using uncertainty to weigh losses for scene\ngeometry and semantics. In Proc. of the IEEE conference on computer vision and pattern recognition. 7482â€“7491.\n[26] Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. 2015. Associating neural word embeddings with deep image\nrepresentations using fisher vectors. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition.\n4437â€“4446.\n[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis,\nLi-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision123, 1 (2017), 32â€“73.\n[28] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention for image-text\nmatching. In Proc. of the European Conference on Computer Vision (ECCV). 201â€“216.\n[29] Kuang-Huei Lee, Hamid Palangi, Xi Chen, Houdong Hu, and Jianfeng Gao. 2019. Learning visual relation priors for\nimage-text matching and image captioning with neural scene graph generators. arXiv preprint arXiv:1909.09953(2019).\n[30] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. 2019. Visual semantic reasoning for image-text matching.\nIn Proc. of the IEEE International Conference on Computer Vision. 4654â€“4662.\n[31] Xiangyang Li and Shuqiang Jiang. 2019. Know more say less: Image captioning based on scene graphs.IEEE Transactions\non Multimedia21, 8 (2019), 2117â€“2130.\n[32] Yikang Li, Wanli Ouyang, Bolei Zhou, Jianping Shi, Chao Zhang, and Xiaogang Wang. 2018. Factorizable net: an\nefficient subgraph-based framework for scene graph generation. InProc. of the European Conference on Computer Vision\n(ECCV). 335â€“351.\n[33] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\n74â€“81.\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence\nZitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer,\n740â€“755.\n[35] Xiao Lin and Devi Parikh. 2016. Leveraging visual question answering for image-caption ranking. In European\nConference on Computer Vision. Springer, 261â€“277.\n[36] Chunxiao Liu, Zhendong Mao, An-An Liu, Tianzhu Zhang, Bin Wang, and Yongdong Zhang. 2019. Focus your attention:\nA bidirectional focal attention network for image-text matching. In Proc. of the 27th ACM International Conference on\nMultimedia. 3â€“11.\n[37] Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang, and Yongdong Zhang. 2020. Graph Structured\nNetwork for Image-Text Matching. InProc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n10921â€“10930.\n[38] Yu Liu, Yanming Guo, Erwin M Bakker, and Michael S Lew. 2017. Learning a recurrent residual fusion network for\nmultimodal matching. In Proc. of the IEEE International Conference on Computer Vision. 4107â€“4116.\n[39] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. In Advances in Neural Information Processing Systems. 13â€“23.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.\nFine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders 0:23\n[40] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder.\n2020. Efficient Document Re-Ranking for Transformers by Precomputing Term Representations. arXiv preprint\narXiv:2004.14255 (2020).\n[41] Nicola Messina, Giuseppe Amato, Fabio Carrara, Fabrizio Falchi, and Claudio Gennaro. 2018. Learning relationship-\naware visual features. In Proc. of the European Conference on Computer Vision (ECCV). 0â€“0.\n[42] Nicola Messina, Giuseppe Amato, Fabio Carrara, Fabrizio Falchi, and Claudio Gennaro. 2019. Learning visual features\nfor relational CBIR. International Journal of Multimedia Information Retrieval(2019), 1â€“12.\n[43] Nicola Messina, Fabrizio Falchi, Andrea Esuli, and Giuseppe Amato. 2020. Transformer Reasoning Network for\nImage-Text Matching and Retrieval. InInternational Conference on Pattern Recognition (ICPR) 2020 (Accepted).\n[44] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in\nVector Space. In 1st International Conference on Learning Representations, ICLR 2013.\n[45] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. 2020. Imagebert: Cross-modal pre-training with\nlarge-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966(2020).\n[46] Leigang Qu, Meng Liu, Da Cao, Liqiang Nie, and Qi Tian. 2020. Context-Aware Multi-View Summarization Network\nfor Image-Text Matching. In Proc. of the 28th ACM International Conference on Multimedia. 1047â€“1055.\n[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with\nregion proposal networks. In Advances in neural information processing systems. 91â€“99.\n[48] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence\ntraining for image captioning. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition. 7008â€“7024.\n[49] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy\nLillicrap. 2017. A simple neural network module for relational reasoning. (2017), 4967â€“4976.\n[50] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris. 2019. Adversarial representation learning for text-to-image\nmatching. In Proc. of the IEEE International Conference on Computer Vision. 5814â€“5824.\n[51] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic\nVisual-Linguistic Representations. In International Conference on Learning Representations.\n[52] Damien Teney, Lingqiao Liu, and Anton van Den Hengel. 2017. Graph-structured representations for visual question\nanswering. In Proc. of the IEEE conference on computer vision and pattern recognition. 1â€“9.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998â€“6008.\n[54] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2016. Order-Embeddings of Images and Language. In 4th\nInternational Conference on Learning Representations, ICLR.\n[55] Shuhui Wang, Yangyu Chen, Junbao Zhuo, Qingming Huang, and Qi Tian. 2018. Joint global and co-attentive\nrepresentation learning for image-sentence retrieval. In Proc. of the 26th ACM international conference on Multimedia.\n1398â€“1406.\n[56] Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li, and Xin Fan. 2019. Position focused attention\nnetwork for image-text matching. arXiv preprint arXiv:1907.09748(2019).\n[57] Kaimin Wei and Zhibo Zhou. 2020. Adversarial Attentive Multi-modal Embedding Learning for Image-Text Matching.\nIEEE Access(2020).\n[58] Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng Wu. 2020. Multi-Modality Cross Attention Network\nfor Image and Sentence Matching. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n10941â€“10950.\n[59] Yiling Wu, Shuhui Wang, Guoli Song, and Qingming Huang. 2019. Learning fragment self-attention embeddings for\nimage-text matching. In Proc. of the 27th ACM International Conference on Multimedia. 2088â€“2096.\n[60] Xing Xu, Tan Wang, Yang Yang, Lin Zuo, Fumin Shen, and Heng Tao Shen. 2020. Cross-modal attention with semantic\nconsistence for image-text matching. IEEE Transactions on Neural Networks and Learning Systems(2020).\n[61] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. 2018. Graph r-cnn for scene graph generation. In\nProc. of the European conference on computer vision (ECCV). 670â€“685.\n[62] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. 2019. Auto-encoding scene graphs for image captioning. In\nProc. of the IEEE Conference on Computer Vision and Pattern Recognition. 10685â€“10694.\n[63] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring visual relationship for image captioning. In Proc. of the\nEuropean conference on computer vision (ECCV). 684â€“699.\n[64] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event descriptions.Transactions of the Association for Computational\nLinguistics 2 (2014), 67â€“78.\n[65] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. 2020. Unified Vision-Language\nPre-Training for Image Captioning and VQA.. InAAAI. 13041â€“13049.\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 0, No. 0, Article 0. Publication date: 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7369614839553833
    },
    {
      "name": "Sentence",
      "score": 0.639061450958252
    },
    {
      "name": "Question answering",
      "score": 0.5548989772796631
    },
    {
      "name": "Modal",
      "score": 0.5212400555610657
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49948644638061523
    },
    {
      "name": "Search engine indexing",
      "score": 0.49628549814224243
    },
    {
      "name": "Natural language processing",
      "score": 0.48211002349853516
    },
    {
      "name": "Transformer",
      "score": 0.47293806076049805
    },
    {
      "name": "Encoder",
      "score": 0.46587061882019043
    },
    {
      "name": "Information retrieval",
      "score": 0.382485568523407
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}