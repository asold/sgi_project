{
  "title": "LEVA: Using Large Language Models to Enhance Visual Analytics",
  "url": "https://openalex.org/W4392405493",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1977847877",
      "name": "Zhao Yuheng",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2351396891",
      "name": "Zhang Yixing",
      "affiliations": [
        "Fudan University",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2102355683",
      "name": "Zhang Yu",
      "affiliations": [
        "Fudan University",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2037600270",
      "name": "Zhao Xin-yi",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1489005433",
      "name": "Wang, Junjie",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A4330337859",
      "name": "Shao, Zekai",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2743487855",
      "name": "Turkay, Cagatay",
      "affiliations": [
        "University of Warwick"
      ]
    },
    {
      "id": "https://openalex.org/A2354002489",
      "name": "Chen, Siming",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3114823909",
    "https://openalex.org/W2073800769",
    "https://openalex.org/W2145154883",
    "https://openalex.org/W4225086367",
    "https://openalex.org/W3024342691",
    "https://openalex.org/W2488113179",
    "https://openalex.org/W2948894190",
    "https://openalex.org/W2906284509",
    "https://openalex.org/W2625126745",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4298326528",
    "https://openalex.org/W4288762736",
    "https://openalex.org/W4385565383",
    "https://openalex.org/W2946535156",
    "https://openalex.org/W2122441390",
    "https://openalex.org/W2013996050",
    "https://openalex.org/W6843135089",
    "https://openalex.org/W2059216172",
    "https://openalex.org/W2886887279",
    "https://openalex.org/W6859894131",
    "https://openalex.org/W2141115795",
    "https://openalex.org/W2065302863",
    "https://openalex.org/W2398042895",
    "https://openalex.org/W3031494326",
    "https://openalex.org/W2107139603",
    "https://openalex.org/W6852467873",
    "https://openalex.org/W4323651262",
    "https://openalex.org/W4324314555",
    "https://openalex.org/W4297094736",
    "https://openalex.org/W4323022566",
    "https://openalex.org/W6856050420",
    "https://openalex.org/W3022905935",
    "https://openalex.org/W2982550137",
    "https://openalex.org/W4384345667",
    "https://openalex.org/W3213578841",
    "https://openalex.org/W4382498938",
    "https://openalex.org/W2888611489",
    "https://openalex.org/W3081277912",
    "https://openalex.org/W4288762839",
    "https://openalex.org/W3168843540",
    "https://openalex.org/W6855330075",
    "https://openalex.org/W3190752453",
    "https://openalex.org/W3198767185",
    "https://openalex.org/W3173728943",
    "https://openalex.org/W4304481137",
    "https://openalex.org/W1512287614",
    "https://openalex.org/W4220952091",
    "https://openalex.org/W6812734964",
    "https://openalex.org/W2107026277",
    "https://openalex.org/W6789971762",
    "https://openalex.org/W2545930194",
    "https://openalex.org/W6635668259",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W2969478830",
    "https://openalex.org/W4298326929",
    "https://openalex.org/W6733968838",
    "https://openalex.org/W6857285574",
    "https://openalex.org/W6854516261",
    "https://openalex.org/W6853844158",
    "https://openalex.org/W3199328824",
    "https://openalex.org/W4366328164",
    "https://openalex.org/W4384816576",
    "https://openalex.org/W4385474529",
    "https://openalex.org/W4386114085",
    "https://openalex.org/W4387323413",
    "https://openalex.org/W1594453896",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W2589373666",
    "https://openalex.org/W3031170804",
    "https://openalex.org/W4380559178",
    "https://openalex.org/W4390722655",
    "https://openalex.org/W4297435087"
  ],
  "abstract": "Visual analytics supports data analysis tasks within complex domain problems. However, due to the richness of data types, visual designs, and interaction designs, users need to recall and process a significant amount of information when they visually analyze data. These challenges emphasize the need for more intelligent visual analytics methods. Large language models have demonstrated the ability to interpret various forms of textual data, offering the potential to facilitate intelligent support for visual analytics. We propose LEVA, a framework that uses large language models to enhance users' VA workflows at multiple stages: onboarding, exploration, and summarization. To support onboarding, we use large language models to interpret visualization designs and view relationships based on system specifications. For exploration, we use large language models to recommend insights based on the analysis of system status and data to facilitate mixed-initiative exploration. For summarization, we present a selective reporting strategy to retrace analysis history through a stream visualization and generate insight reports with the help of large language models. We demonstrate how LEVA can be integrated into existing visual analytics systems. Two usage scenarios and a user study suggest that LEVA effectively aids users in conducting visual analytics.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nLEVA: Using Large Language Models to\nEnhance Visual Analytics\nYuheng Zhao, Yixing Zhang, Yu Zhang, Xinyi Zhao, Junjie Wang,\nZekai Shao, Cagatay Turkay, Siming Chen\nAbstract—Visual analytics supports data analysis tasks within complex domain problems. However, due to the richness of data types,\nvisual designs, and interaction designs, users need to recall and process a significant amount of information when they visually analyze\ndata. These challenges emphasize the need for more intelligent visual analytics methods. Large language models have demonstrated\nthe ability to interpret various forms of textual data, offering the potential to facilitate intelligent support for visual analytics. We propose\nLEVA, a framework that uses large language models to enhance users’ VA workflows at multiple stages: onboarding, exploration, and\nsummarization. To support onboarding, we use large language models to interpret visualization designs and view relationships based on\nsystem specifications. For exploration, we use large language models to recommend insights based on the analysis of system status and\ndata to facilitate mixed-initiative exploration. For summarization, we present a selective reporting strategy to retrace analysis history\nthrough a stream visualization and generate insight reports with the help of large language models. We demonstrate how LEVA can be\nintegrated into existing visual analytics systems. Two usage scenarios and a user study suggest that LEVA effectively aids users in\nconducting visual analytics.\nIndex Terms—Insight recommendation, mixed-initiative, interface agent, large language models, visual analytics\n✦\n1 I NTRODUCTION\nVisual analytics (V A) combines data analysis techniques with\nvisualizations for effective understanding, reasoning and decision-\nmaking on the basis of large and complex datasets [21], [22], [55].\nHowever, the V A processes often require significant effort on the\nside of the user, resulting in a less efficient data interpretation and\nanalysis process [58]. Several challenges underpin this inefficiency:\na steep learning curve with V A systems’ onboarding [50], a\ntendency to lose direction in data exploration [29], and the difficulty\nof summarizing final insights [8]. Such issues emphasize the need\nfor a more intelligent approach to V A.\nTo specify the challenges further: firstly, the challenge of\nonboarding stems from unfamiliarity with a V A system, leading to\nreduced efficiency in grasping visual mappings and interactions [5],\n[41]. Secondly, during data exploration, the numerous avenues to\nanalyze data and the lack of structured guidance and direction\nmake it difficult to uncover insights [6]. Thirdly, keeping track of\nand aggregating key findings is often difficult and summarizing\ninsights is a time-consuming task that demands significant effort\nand attention to detail [3], [33]. These challenges highlight the\nneed for an intelligent framework to support V A processes and\nfoster efficacy across these pivotal stages.\nTo address these problems, researchers focus on enhancing V A\nthrough visualization onboarding [49], interaction recommendation\nand guidance [4], [29], [47], and result summarization [8], [44].\n• Yuheng Zhao, Yixing Zhang, Xinyi Zhao, Zekai Shao, Siming Chen are\nwith School of Data Science, Fudan University. E-mail: {yuhengzhao,\nxinyizhao19, zkshao19, simingchen }@fudan.edu.cn, {yixingzhang23,\nwangjj23}@m.fudan.edu.cn.\n• Yu Zhang is with Department of Computer Science, University of Oxford.\nE-mail: yuzhang94@outlook.com.\n• Cagatay Turkay is with University of Warwick. E-mail: Ca-\ngatay.Turkay@warwick.ac.uk.\n• Siming Chen and Yu Zhang are the corresponding authors.\nManuscript received April 19, 2005; revised August 26, 2015.\nHowever, these methods often do not take advantage of advance-\nments in intelligent algorithms and are not easily adaptable to\nvarious V A systems. There is a lack of generalizable intelligent\napproaches that can work across different V A systems.\nThe Large Language Models (LLMs) exhibit broader knowl-\nedge and problem-solving abilities, making it possible to address\nthe above challenges in the three stages of V A. Firstly, as LLMs\ncan interpret visualizations’ declarative grammar [13], [35], we\ncan try to propose the grammar for V A systems and use it as\ninput for LLMs to generate onboarding tutorials. Additionally,\nLLMs can process various data types [39], making it possible\nto recommend insights by analyzing both the system status and\nunderlying data, assisting users’ exploration. Finally, LLMs exhibit\npowerful summarization capabilities [17], which may help to\nsummarize the exploration process and generate a report with\nrich forms.\nIn this paper, we propose a framework named LEV A (LLM-\nEnhanced Visual Analytics) that uses LLMs to enhance visual\nanalytics in three stages of the workflow. In the onboarding phase,\nwe provide a solution that enables LLMs to interpret visualizations\nin each of the views and these views’ relationships based on a\nspecification of the V A system. This enables the flexible creation\nof tutorials for various V A systems. In the exploration phase, we\ndesign an insight recommendation strategy that guides LLMs in\nrecommending insights based on the understanding of the system,\nanalytical task, user’s interaction, and data to facilitate mixed-\ninitiative exploration [18]. The methodology incorporates a two-\nstep process, including the selection of insight types and assessment\nof executed insights. Additionally, we integrate insights in the\noriginal V A system as annotations instead of textual descriptions,\nmaking communication between the LLM and end-users more\nintuitive. In the summarization phase, LEV A facilitates the user to\nretrace the analytical history via an interactive stream visualization,\nallowing the selection of an analytical path for report generation.\narXiv:2403.05816v1  [cs.HC]  9 Mar 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nFig. 1. The LEVA framework proposes strategies for leveraging LLMs to enhance visual analytics workflows, starting from onboarding and exploration\nto summarization. The architecture (middle) connects analysts (top) and LLMs (bottom) to achieve mixed-initiative exploration through interactive\ninterfaces and guidance strategies of LLMs.\nOur strategy involves a report generation method where the LLMs\nsynthesize visualization images and explanations to produce a\nreport using LaTeX code. We have developed two integrated\nsystems combining our framework with two original V A systems.\nThe interface includes a chat view, original system view, interaction\nstream view, and report view. To the best of our knowledge, LEV A\nis the first attempt to embed LLMs in complex visual analytics\nworkflows to support users in various analysis stages. Our main\ncontributions are as follows:\n• We propose a framework, LEV A, for using LLMs to enhance\nmixed-initiative exploration in three stages of users’ V A\nworkflow.\n• We demonstrate how LEV A can be implemented in existing V A\nsystems to facilitate visual analytics by enabling a connection\nbetween users, interface and LLMs.\n• We report observations and learnings from two usage scenarios\nand a user study to demonstrate the effectiveness of our\nframework.\n2 R ELATED WORK\nThis section reviews the literature that aims to enhance V A in\ndifferent stages of users’ workflow and the literature using LLMs\nfor V A to examine its abilities.\n2.1 Onboarding in Visual Analytics\nVisualization onboarding is the process of supporting users in\nreading, interpreting, and extracting information from visual\nrepresentations of data [50]. Stoiber et al. [49] found that a V A\nsystem may lack low-level information about the data, such as\nunderstanding principles of the specific data format, data types, or\ndata structure, which limits data selection and manipulation [22].\nVaishali et al. [12] found that non-expert users often lack visualiza-\ntion literacy to interpret the data and understand the interactions\nwith and between visualizations in a dashboard. They pointed out\nthat it is necessary to bridge the knowledge gap between the system\nand the user’s background before exploring the data. Previous\nresearch conducted various onboarding strategies for V A systems.\nThe onboarding form mainly includes textual descriptions, video-\nbased, and step-by-step tours with tooltips and overlays [59]. Kwon\net al. [23] conducted a user study to compare these methods and\nfound that an interactive tour is better than others with a more\nengaging experience. Previous studies have shown that onboarding\ntools can help users better understand a V A system. However,\nthese tools are limited in their ability to be easily integrated into\ndifferent systems. To address this, we proposed an intelligent\ninteractive onboarding method to generate tutorials based on the\nunified grammar of V A systems.\n2.2 Insights Recommendation in Visual Analytics\nInsight recommendations within visualization methodologies have\ngarnered attention in recent V A research, serving as effective\ninstruments to aid users in their analytical tasks. These methods\ncan broadly be stratified into recommendations encompassing\nannotations or captions, interactions, and direct visualizations.\nA corpus of research has been proposed on generating single\nvisualization [19], [37], [42] or multiple-view visualizations [11].\nTypically, these studies deal with data table queries, outputting\nstatic visualizations. In contrast, our approach adapts to a V A sys-\ntem, producing an enhanced V A system complete with interactive\ninsight annotations. A separate thread of research, exemplified by\nLai et al. [24] and Liu et al. [32], aims at appending annotations to\nvisualizations. However, their emphasis is limited to single-view\nvisualizations without an overarching framework for comprehensive\nV A systems.\nRecognizing the complexity of V A systems, some guidance\ntheories have been proposed [7], [47]. Ceneda et al. [6] char-\nacterized interaction guidance along the knowledge gap of the\nuser, the input and output of the guidance generation process, and\nthe degree of guidance that is provided to users. A commonly\nused implementation method is modeling interaction sequences\nto predict the next interaction object [29]. While this strategy\noffers a diverse set of suggestions, it needs to collect large\namounts of interaction data, grapple with issues of explainability,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nand be applied to specific V A systems. Instead of relying on\ninteraction data, we mine insights directly from data. We let\nLLMs understand V A systems and dispatch tasks for computation\nfunctions. This approach not only ensures explainability but also\nfosters adaptability to diverse V A systems.\n2.3 Insights Summarization in Visual Analytics\nIn data analysis, the task of translating insights into comprehensive\nreports necessitates substantial effort from the user for documen-\ntation and summarization. Prior research such as DataShot [57]\nintroduced methods for automatic report generation, enabling the\nderivation of insight-driven reports directly from data along with\nvisuals attached. Li et al. [27] refined this concept by encapsulating\ndata insights within automatically generated notebooks. However,\nthese methods do not support summarization in V A systems. Chen\net al. [8] innovated a storytelling approach, incorporating recording\nand editing utilities in V A. VisInReport [44] offered a tool that\nfosters manual discourse transcript analysis, and curates reports\ncontingent on user interactions. However, these methods do not\nautomate the visual examination of exploration routes; obtaining\ninsights often requires manual review and refinement. Liu et\nal. [33] designed an analytical graph depicting the journey of\nexploratory data analysis. Drawing inspiration from this, we fuse\na stream visualization to endorse real-time retrospectives. Our\nnovel methodology harnesses LLMs to discern pertinent records,\nfacilitating automatic summation and report creation.\n2.4 Large Language Models for Visualization\nThere has been a rising interest in employing language models for\nvarious downstream applications. Categorizing based on the format\nof inputs and outputs, LLMs can process code, declarative syntax,\nnatural language, and structured data.\nRelated work in the visualization domain focuses on under-\nstanding or generating visualizations. A typical application is\nusing natural language to generate data visualizations through\nan interface [45]. Moreover, codes [36] or declarative grammar for\nvisualizations [13], [35], [38] is often used as input or output as\na simplified form of code. This type of work demonstrates code\ncomprehension of language models and basic knowledge of visual-\nizations. Inspired by this, we propose to use declarative grammar\nto represent V A systems, which are essentially combinations of\nmultiple visualizations.\nIn addition to visualization tasks, Language models can also\nanalyze data [61] or act as agents to use tools with APIs [43]. Thus,\nwe propose that in addition to using LLMs to process data, some\ncomplex computation methods can be used as additional tools,\nenabling support for a wider range of V A analysis scenarios. The\nlanguage model is proficient in generating text with format [39].\nFor example, Liu et al. [34] adopt LLMs for generating semantic\ninput texts based on GUI context. We leverage this prowess to\ngenerate and integrate tutorials, insights, and reports to enhance\nuser experience in using V A. Finally, the language model supports\ndialogue mode, which allows users to ask and answer questions\nfreely, as well as decompose long tasks to support more complex\nanalysis tasks.\nTo conclude, the development of LLMs has opened up op-\nportunities to enhance V A. Our work is the first trial towards a\ncomprehensive integration of LLMs for intelligent V A.\n3 M OTIVATION\nInterpreting and analyzing data with V A often requires significant\nuser effort [58]. Various challenges lead to such inefficient analysis,\nsuch as the steep learning curve associated with onboarding V A\nsystems [50], getting sidetracked during data exploration [29],\nand difficulty summarizing the final insights [8]. These challenges\nunderscore the necessity for a more intelligent approach to V A.\nCombining a review of previous studies, we summarized the fol-\nlowing design considerations and propose the design requirements\naimed at enhancing the user experience in V A.\n3.1 Design Considerations\nFrom our analysis of the V A literature, a recurring concern emerges:\ndespite the advances, users still have to expend excessive effort in\nvarious stages of analysis. In each of these stages, we identify and\ndissect specific challenges that intensify the effort users must exert:\nC1 System onboarding: Before users can extract value from\nV A, they must first grasp the nuances of the system’s design.\nHowever, two main challenges hinder this. Firstly, although V A\nsystems are often designed with a target user group in mind,\nthe data and visual encoding displayed in visualization might\nnot always align with the users’ background. Ambiguities\nin data information, such as the meaning of data, structure,\ntypes, and transformation, may lead to misinterpretations [22].\nSecondly, a lack of visualization literacy among users can\ncomplicate the interpretation of visual data, making the initial\nstage of data analysis more laborious than it should be [49].\nThis emphasizes the need for a more efficient onboarding\nprocess that not only caters to users but also provides clear\nand intuitive information to smoothen the transition and usage.\nC2 Insights exploration: When embarking on the journey of\ninsight discovery, users are often bogged down by interactively\nanalyzing complex data and visualization, which is time-\nconsuming [16]. On top of this, the multi-faceted nature\nof insights means forming hypotheses requires finding parts\nof many insights that are relevant to the task and then\nvalidating them by interacting to find insights in new states,\nmaking the exploration stage more challenging [2]. Given\nthese complexities, there emerges a need for V A systems to\nautomatically extract and evaluate insights with users’ tasks in\nmind, ensuring more focused and efficient data analysis.\nC3 Results summarization: Summarizing and reporting findings\nis a final, yet essential step in V A systems. However, it’s\na process that is often unduly laborious. Users typically\nnavigate through multiple iterative explorations to validate\nhypotheses [8]. Yet, not every exploration results in valuable\nknowledge [44]. As a result, analysts frequently spend signifi-\ncant time and effort sifting through, distilling, and manually\ndrafting reports from their interactive exploration outcomes.\nThis highlights the pressing need for automated capturing\nof findings and allows users to trace back. Furthermore,\nsuch automation should provide users with comprehensive\nillustrated reports [27], sparing them the chore of crafting\nthem themselves.\n3.2 Design Requirements\nDrawing insights from these design considerations, we have\ndeveloped a set of targeted design requirements. These requirements\nare pivotal for enhancing V A, aiming to address the challenges\nidentified earlier.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nR1 Visual encoding: To address the barriers of system compre-\nhension and misinterpretations (C1), it’s vital that we offer\nan onboarding tutorial to help users understand how data and\nvisualization are mapped. This could include providing data\ndefinitions and encodings.\nR2 Interaction and coordination: Considering the challenges\nusers face in discerning the usage of visualizations in a V A\nsystem (C1), we should provide an interactive step-by-step\ntutorial of each view. The content needs to clarify interactions\nand the relationship between views.\nR3 Insight discovery: In light of the repetitive analytical tasks\nusers undergo and the depth of domain knowledge required\n(C2), our framework should provide automated data analysis\nthat helps users discover diversified insights. This may include\nanalysis for different data types.\nR4 Hypothesis formulation and validation: Given that explo-\nration often occurs in an iterative process (C2), our framework\nshould enable the discovery of insights in constantly updated\nstatuses to facilitate hypothesis formulation and validation.\nR5 Summarization of exploration results: Considering the labor-\nintensive nature of the summarization process (C3), our\nframework should prioritize aiding users in filtering and\nsummarizing their exploration outcomes. This entails the\nmanagement and visualization of historical insights as well as\nthe capability to generate comprehensive reports.\n3.3 LLM-Enhanced User Workflow in Visual Analytics\nOur objective is to employ intelligent V A to address the challenges\nfaced by analysts when using V A systems. However, creating\ndistinct models for each exploration stage, workflow, chart type,\ndata type, and domain is labor-intensive and difficult to scale,\nrequiring general frameworks and models. Large language models\nhave emerged for various analytical tasks, offering the potential for\ncontinuous assistance throughout the workflow. Thus, we propose\na framework that leverages LLMs to enhance visual analytics in\nthree stages of the workflow (Fig. 2). Based on the analysis of\nconsiderations and requirements, we summarize the integration into\nthree stages: onboarding, exploration, insight recommendation in\nexploration, and summarization for selective reporting.\nFig. 2. The LLM-enhanced visual analytics workflow shows how LLMs\ncontribute to the progress of the analysis. The LLMs support visualiza-\ntion understanding, mixed-initiative guidance, and automatic summary\nwhile users experience onboarding, exploration, and summarization.\nOnboarding refers to data understanding, visualization, and interaction\nperception. Exploration refers to insight discovery, hypothesis formulation,\nand validation. Summarization refers to selective reporting.\nTo further detail how we intend to tackle these challenges,\nwe outline the roles and purposes of the LLM-enhanced system.\nThe objective of the LLM-enhanced system is to alleviate the\nchallenges users confront in the three stages. The end-users of this\nenhanced system are analysts or individuals who frequently work\nwith V A systems. On the other hand, the LEV A implementation\nitself is intended for design and development professionals who\naim to create or augment V A systems. The primary users of LEV A\nare system developers and designers who wish to leverage large\nlanguage models to enhance the capabilities of their V A tools.\n4 LEVA F RAMEWORK\nIn this section, we will introduce our framework (Fig. 1). To\nenhance the V A workflow, we carefully identify the important\nsteps of users in using visual analytics and propose LEV A to\nboost the analysis efficiency and enrich the insight exploration\n(Sec. 3.3). The architecture connects analysts and LLMs to achieve\nmixed-initiative exploration. In our framework, the original system\nis a visual analysis system, while our LEV A-enhanced system\nis added with natural language dialogue, interactive onboarding\ntutorials, recommended insights, analysis history visualization, and\ngenerated reports. The pipeline of our framework goes through\nthree stages: (1) Onboarding: uses LLMs to interpret visualization\nand views’ relationships based on a system specification for\nusers; (2) Exploration: guides LLMs in recommending insights to\nfacilitate user’s exploration through the analysis of the system’s\nunderlying data. (3) Summarization: allows users to retrace and\nselect their analysis history through visualization and use LLMs to\ngenerate insight reports.\n4.1 VA System Onboarding\nWhen faced with complex V A systems that contain multiple views,\nmany users experience a steep learning curve. Therefore, we\npropose an onboarding tutorial generation method, which aims\nto help users understand two kinds of knowledge in a V A system:\nvisual style and coordination.\nThe onboarding can follow two types of approaches: one is\nbottom-up, i.e., introducing the details first, and the other is top-\ndown, which involves providing an overview initially. Following\nTanahashi et al., who demonstrated that top-down is more effective\nin introducing visualizations [54], we take a top-down approach.\nThe tutorial will start from the system-level introduction to the\nview level. To that end, we leverage LLMs’ ability to comprehend\nthe system’s information and generate tutorials. Previous studies\ndemonstrate that declarative grammar of visualization is readily un-\nderstood by language models [35], [36]. To encapsulate automatic\ntutorial generation, we need a unified specification for different V A\nsystems. To this aim, we propose a specification of the V A system\ndesigned in line with the top-down approach, which is used as\ninput for LLMs.\nThe specification starts from the system-level information\nSystemSpec. We provide the systemInfo attribute to detail the\nsystem’s name and the total number of views it comprises and the\nviewsInfo to describe each view’s style and coordination. Examples\nare available in Appendix A.\n1 class SystemSpec {\n2 /** Information about the overall system */\n3 systemInfo: {\n4 systemName: string\n5 viewNumber: number\n6 }\n7 /** Style and coordination between views */\n8 viewsInfo: {\n9 viewStyleInfo: ViewStyleInfo[]\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\n10 viewCoordinationInfo: ViewCoordinationInfo[]\n11 }\n12 }\nFor the view-level information, we introduced the\nviewStyleInfo to delve into the design of each individual view.\nThis information captures the essence of the V A views through\nthe viewName attribute, representing its identity, while the layers\nattribute describes its visual components. Each layer details the\nmark type (e.g., area), and the encoding attributes that provide\ninsights into how data fields are visually mapped onto axes, color\nscales, and size scales. Additionally, an optional tooltip specifica-\ntion can be employed to enrich the user’s interactive experience by\ndisplaying supplementary information upon interaction.\n1 class EncodingInfo {\n2 field: string;\n3 type: string;\n4 description: string;\n5 }\n6 class ViewStyleInfo {\n7 viewName: string\n8 /** Information about styles of multiple encodings */\n9 layers: {\n10 markType: string\n11 encoding: {\n12 x?: EncodingInfo[];\n13 y?: EncodingInfo[];\n14 color?: EncodingInfo[];\n15 size?: EncodingInfo[];\n16 /** Detail level data field */\n17 lod?: EncodingInfo[];\n18 }\n19 /** Display prompt information for mouse hover.*/\n20 tooltip?: EncodingInfo[];\n21 }[]\n22 }\nUnderstanding the coordination between different views is\npivotal for a coherent exploration experience. Thus, we introduced\nthe ViewCoordinationInfo into the specification. This information\nbridges the interactions between the source and target views through\nthe sourceViewName and targetViewName attributes respectively.\nFurthermore, the coordinationType attribute delineates the nature of\ninteractions (e.g., filter, brush), and the interaction array provides a\ngranular breakdown of user-triggered events and their subsequent\nramifications on the target views.\n1 class ViewCoordinationInfo {\n2 sourceViewName: string\n3 targetViewName: string | string[]\n4 /** Coordination type */\n5 coordinationType: string\n6 interaction: {\n7 /** Type of interaction */\n8 type: string\n9 /** Interaction’s effect on target */\n10 effect?: {\n11 /** Action type */\n12 action: string\n13 targetViewName: string\n14 /** Data category for action */\n15 category: string\n16 /** Control the result */\n17 changeby: string\n18 }\n19 }[]\n20 }\nOur specification utilizes a key-value pair approach to deter-\nmine the choice of fields and their format. This approach could\noffer flexibility and can be adapted for a variety of visualization\ntypes, not just the common visualizations. For example, users may\ndefine a customized card using a “title” and “context” to represent\nthe encoding instead of using “x” and “y”. Therefore, for special\nexamples, the definition of key-value pairs is flexible, as long as it\nmakes the LLM understand. Moreover, the specification not only\nhelps LLMs generate intuitive onboarding tutorials but also aids in\nunderstanding the system to assist subsequent stages.\n4.2 Insight Recommendation in Exploration\nVisual exploration can be a challenging task that requires significant\neffort and expertise, particularly when there are no clear focal points\nto guide the process. Here, we introduce a strategy that channels\nLLMs towards enhancing insight recommendations. The approach\nincludes three steps: selecting insight types, computing insights,\nand scoring insights.\n4.2.1 Defining Insight\nIn our framework, insights are considered the basic units in visual\nexploration. Following previous studies [14], [57], we define insight\nusing four attributes:\ninsight := ⟨type , parameters,sub ject,score⟩ (1)\nInsight type: We first identify the 15 insight types based on\nprevious work [14], [25], [48] including finding extreme, outlier,\nchange point, trend, etc. To cover more complex data types, such\nas text and graphs, we have added three common types to the list,\ni.e., text summary and key nodes or key links. More details of the\ndefinition of these types are available in the Appendix E. Moreover,\nsome insight types may require the analysis of coordinated views\nwithin a V A system. Therefore, from the perspective of complexity,\nwe define an insight to be either aligned with a single view or across\nmultiple views. For example, the trend of “sales” within the “sales\nview”, or identifying positive correlations between “sales” and\n“profit” in both “sales view” and “profit view”. This distinction will\nhelp LLMs understand how to calculate insights in V A systems.\nInsight parameters: For each insight type, we use parameters\nto describe the characteristics of an insight, such as the direction\nof correlation and the location or time of a summarized event.\nInsight subject: The subject in our framework defines the data\nscope to derive an insight, which includes four attributes:\nsub ject := ⟨subspace,dimension,measure,context⟩ (2)\nEach subject corresponds to a subset of data in a view. The\nvalues of the dimension can be mapped to the x-axis, and the\nmeasure can be mapped to the y-axis. The subspace is a filter of a\ndataset. For example, if a line chart has two products’ “sales”, the\ndimension is the time, measure can be “sales” or “profit”, and a\nsubspace can be a selected time range or a selected product. For\ntext data analysis, we can use measure to refer to the text data\ncolumn in a dataset. In more complex graph data, the dimension\ncan refer to the types of entities and relationships within the graph.\nThe measure can signify quantifiable properties or attributes related\nto nodes or edges.\nIn addition to the data set in the original view, some additional\ndata may also serve as an important data source. For example,\nderived insights can be reserved to hint at subsequent analysis.\nSecond, intricate tasks such as finding important nodes associated\nwith particular events surpass the bounds of a conventional\nsubject due to the demand for event summaries. For these certain\nrequirements, we define the additional data source as context.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nInsight score: Different insights are not equally attractive to\nusers. Previous research defines the importance score including\nimpact and significance scores. However, to achieve insight\nrecommendations in the V A system, it is also important to assess\nwhat insight types could be used to solve the task. Thus, we\nconsider relevance, impact, and significance scores together to\ncalculate the insight score.\nThe significance calculation method is adapted from QuickIn-\nsight [14]. Specifically, this score is calculated based on hypothesis\ntesting, which takes a value within the range [0,1]. Take finding\nthe outstanding number one item in a group as an example. The\nhypothesis is that the data obeys the null hypothesis, i.e., long-\ntail distribution. The p-value is used to indicate whether the data\nexcludes the maximum that will go against the null hypothesis.\nThus, we get the significance score as 1 − p (p is the p-value).\nOther examples of calculation methods are introduced in this\nspecification 1. While our analysis utilizes p-values to assess the\nsignificance of insights, we recognize the concerns around the use\nof p-values. First, the p-value is volatile and does not convey the\nmagnitude of an effect. Second, the p-value is not suitable for\nall insight types. To address these issues, our scoring mechanism\nfor insights is designed to be adaptable. We suggest incorporating\nalternative measures such as effect sizes, Cohen’s d, Odds Ratio, or\nthe coefficient of determination [51] for broader applicability. For\ntext analysis, metrics like BLEU and ROUGE or LLMs themselves\ncould serve as more suitable substitutes [10].\nTo assess the impact score, there are two kinds of aspects that\nneed to be considered. One is the coverage of the data subspace\nover the entire dataset, and another is the semantics of the data\nsubspace. Previous work defines the impact as the coverage of the\ndata subspace over the entire dataset [57]. Take the social media\nevent analysis (used in Sec. 6.1) as an example, “microblog” and\n“call center” are two types of messages that represent two distinct\nsubspaces. If the amount of “microblog” messages surpasses that\nof “call center”, the insights derived from the “microblog” data are\nconsidered to have a greater impact due to their more extensive\ncoverage. However, from the semantic aspect, the message from\nthe “call center” is more impactful because it is more reliable\nand timely. We assess such impact using LLMs based on their\nunderstanding of the dataset and their broad common knowledge\nof data analysis in different scenarios.\nAs users typically engage with a V A system with an analytical\ntask in mind, we use a relevance score to quantitatively measure\nthe congruence between the emergent insight and the overarching\nanalytical task. We let LLMs evaluate the insights by considering\nhow closely the insights align with the task. For instance, in event\nanalysis, the task is to detect risk events and regions in the city.\nTherefore, analyzing messages to summarize events would score\nhighly on relevance due to its direct connection with the task.\nConsidering three scores jointly, we compute the insight\nscore as score = ∑k∈{signi f icance,impact,relevance} wk ·scorek. Here, wk\nrepresents the weights for the significance score, impact score, and\nrelevance score, respectively. Since our purpose of getting data\ninsight is to uncover its patterns, the significance score should be\ngiven more weight than the others. Therefore, we empirically set\nthe weights to be 0.5, 0.2, and 0.3, respectively. The weights are\nadjustable to fit different preferences.\n1. https://www.microsoft.com/en-us/research/uploads/prod/2016/12/\nInsight-Types-Specification.pdf\n4.2.2 Recommendation Strategy\nThe insights recommendation strategy should be intuitive, adaptive,\nand relevant to the user’s tasks. We address this with the following\ntwo-step strategy:\nStep 1 (Insight type selection): Considering the multitude of\npossible insight types, we prioritize selecting those that are most\nrelevant to the task. When the user makes a selection on a specific\nview, we use LLMs to find relevant insight types according to\nthe user’s selection and task and give a relevance score. The high-\nrelevance score insight types will be used for further calculation. To\nautomatically execute these insight types, LLMs need to schedule\nthe data that compute insight needs (e.g., data tables, column\nnames). The output includes the selected insight types, relevance\nscores, and the data information for insight calculation.\nStep 2 (Insight assessment): This step aims to conduct a\ncomprehensive assessment of insights. First, the last step’s selected\ninsights types will be executed to generate the insights. We use the\nLLMs to translate the calculated results into complete sentences.\nSecond, we calculate the combined score of insights from three\naspects. As we defined before, the significance score is calculated\nby statistical methods, and the impact and relevance scores are\nassessed by LLMs. The final output of this step is the insights\nranked by the combined score and the corresponding data points in\nthe views for annotation.\nAfter the two steps, the insights should be annotated to the\nrelevant views in the original V A system. Users can constantly\ninteract with the view to gain new insights from LLMs.\n4.3 Summarization of Exploration Results\nFollowing exploration interactions with LLMs, users may wish to\nembark on a new exploration round, thereby establishing a human-\nin-the-loop analysis process for the continuous acquisition of novel\nknowledge. Given that not every interaction stage will yield the\ndesired insights, we advocate for an interactive strategy to filter\nexploration results and create comprehensive reports, capitalizing\non LLM’ text summarization abilities.\nRecord preservation: Throughout the analytical process, it is\nimperative to maintain a detailed record of the user’s exploration\njourney, including interactions and insights. We store these elements\ntogether because insights and interactions provide key findings that\ncontribute valuable context for report creation. To elaborate, user\ninteractions act as filters that modify the state of the analysis,\nwhich defines the subspace of insights. For instance, consider a\nscenario where a user selects a specific time period. This action\nprompts the LLM to summarize relevant events in this time period.\nWhen presenting the summary of events, it is crucial to provide\nthe time period. This approach ensures that a comprehensive and\ncoherent insight is captured, enhancing the overall understanding\nand relevance of the insights generated. Each recorded data set is\ndefined to span m analysis rounds. Within each round, there are\nn distinct steps, representing the user’s adopted insights from the\nLLM or their self-motivated interactions. We can represent the\nexploration journey as a matrix M of dimensions m×n, where each\nelement Mi, j denotes the jth step in the ith round. For each step\nMi, j, the following details are preserved:\n• focused view: a descriptor of which visualization or data view\nwas in focus.\n• insights: the specific insights generated by LLM. If the user\ninteracts by themselves, we record the interaction object.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\n• screenshots: a saved screenshot of the focused view crucial for\nsubsequent report illustration.\nSelective reporting: To provide users with a cohesive un-\nderstanding of their analytical journey, our system presents an\ninteractive mechanism. This mechanism graphically illustrates the\nmatrix M, allowing users to retrace the insights for reporting. Once\na user selects a round of data, the curated sequence from M serves\nas input for the LLM. Using this structured input, the LLM crafts a\nreport aggregating both the analytical insights and the visualization,\ncapturing the user’s exploration journey.\n5 LEVA I MPLEMENTATION\nThe implementation requirement of our framework includes two\nparts: the extensions of the original V A system and the LLM-\npowered components (Fig. 3). The original V A system requires\nconfiguration files and data handlers to process user selections and\nLLM outputs. The LLM-powered components consist of prompt\nhandlers and presentation modules for showcasing outputs across\nthe original system view and three new views.\nFig. 3. The integration overview for augmenting a VA system with\nLLM capabilities involves both existing extensions and LLM-powered\ncomponents in LEVA’s implementation. The enhancement brings new\ncapabilities for end-users at three stages.\nIn the following, we will introduce the efforts to develop the\nextensions of the original system and the LLM-powered compo-\nnents LEV A provided, including onboarding tutorial generation,\ninsights recommendation, report generation, and the final integrated\ninterface. For prompt templates used, we provide examples in\nAppendix B.\n5.1 Extensions of Original System for Integration\nTo integrate our framework within a V A system, we need to prepare\nthe configuration files and data handlers for the original system.\nSpecification: The specification acts as a blueprint for LLMs,\nallowing them to decode and understand the design and usage\nof the original V A system. By comprehending the specifications,\nLLMs can craft detailed and appropriate tutorials tailored to the\nV A system. Furthermore, as the specification contains the data and\ncoordination information in each view, it can be used to help LLMs\nexplore data by knowing the system state and determine which\nview and data column to calculate insights from.\nAnalysis task: Before recommending insights, it is necessary\nto propose a user task to describe in natural language sentences. If\nit is not specified, we can also use LLMs to propose the task that\nis used to select the relevant insight types.\nInsight functions: We propose a list of functions to calculate\ninsights in different types, e.g., get the outstanding top one and find\nan outlier. These functions correspond to the insight type defined in\nSec. 4.2.1. The common insight types we give may not be sufficient\nin some special cases. Thus, developers can add functions according\nto their domain-specific analysis requirements. Previous taxonomies\nsummarize complex tasks that might be helpful for proposing the\ncomputing functions, such as taxonomies for graph analysis [25],\nspatial and temporal analysis [1], social media analysis [9].\nInsight function APIs: We let LLMs choose the functions from\nthe APIs list to calculate the insight based on their understanding of\nthe system. To achieve this goal, we need to give the definition of\na function. It is better to have two attributes to explain the purpose\nand outcome of the function: ⟨name,description⟩. For example, to\ncalculate the outstanding number one item in a group of data, the\nname could be “get outstanding top1”, and the description could\nbe “Calculate the leading value is significantly higher than all the\nremaining values”. Providing the APIs of insight functions enables\nLLMs to choose the suitable analytical method and execute them\nby completing the parameters.\nData handlers: To support the connection between LLMs and\nthe original system, we need to implement these handlers to capture\nthe changes in the system and add the new features from LLMs.\n• Add tutorial: Receive the tutorials from LLMs and employ a\ntour guide tool to display the tutorial.\n• Get selection data: Upon a user interacting with a specific view,\nthe system needs to get the selection data, including the filter\nand the updated data on each view.\n• Add annotation: The original V A system needs to highlight the\ninsights generated by LLMs on target views with annotations.\nThis function should enable the selection of visual elements,\nchanging their style, and adding annotations around them.\n• Apply annotation filter: Allow the selection of the annotation\nacross the views, such as filtering and deleting.\n5.2 LLM-powered Components\nIn addition to the above extensions, the enhancement also needs\nLLM-powered components to support onboarding, exploration, and\nsummarization within the enhanced V A interface.\n5.2.1 Onboarding Tutorial Generation\nIn the onboarding stage, we use LLMs to generate tutorials by\ninputting the specifications of a V A system. To design the output\nof LLMs, considering the text form of the tutorial will increase the\nuser’s reading time, we designed the tutorial as an interactive tour\nguide. Therefore, the output of LLMs is a list of steps in the tour.\nEach step within the tour includes two attributes, including title and\ndescription. The value of the title is viewName. The description\nincludes visualization Type, Encoding, and Coordination. We let\nLLMs output description as HTML format to paraphrase the long\ntext to fewer lines and set font styles for clear observation. When\nusers select to start the onboarding tour, the tutorial will be triggered\nand added to the original system to introduce each view. The prompt\nfor generating an onboarding tutorial is shown below:\nPrompt template for onboarding:\nHere are the specifications of a visual analytics system.\n{ specification data }\nThe specification includes the system-level, view-level, and\nviews’ coordination information. You need to introduce each\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nview’s style (data meaning, visual mapping) and the relationship\nbetween views. Please give your answer in the following format:\n{ format requirements }\n5.2.2 Insights Recommendation\nWe implement the interactive recommendation to undergo two steps\nof conversation with LLMs. The first step is to select appropriate\ninsight types based on the selection data and analytical task, and\nthe second step is to execute and assess the insight to select the\nfinal results.\nIn the first round, the inputs consist of four components,\nincluding specification, the current interaction, analysis task and\ninsight function list. We added specifications including view\nstyleinfo and viewscoordinationinfo to the input. The views\ncoordinationinfo allows LLMs to know the target views after the\nuser’s selection. The viewstyleinfo contains the data information\nin the view that will be used as parameters to compose the data for\ninsight calculation. The current interaction is represented as a triplet:\n⟨viewName,dimName,value⟩. If the user selects non-consecutive\nelements, such as two locations, California and New York, on\nthe map (Sec. 6.2), the current selection will be two triples in\nan array. Moreover, if the selection from the previous analysis\nstep is not canceled, it will remain together to provide context for\nfurther analysis. Then, LLMs determine the types of insight that\ncan be analyzed based on the insightfunctionAPIs and analytical\ntask and assess with a relevance score. In order to execute these\ninsight functions, we define the output of LLMs as a quadruple:\n⟨functionName,viewName,variableName,dimName⟩. The prompt\ntemplate for insight type selection is shown below:\nPrompt template for insight type selection:\nWhen the user makes an action, the system changes. You should\nanalyze data types of connected views based on the coordination\ninformation between views.\n{ current selection }\n{ view style info }\n{ views coordination info }\nAccording to the data info in each view and the analytical task,\nyou should select all suitable analytical functions related to the\nuser’s task. You also need to give a relevance score to assess\nhow closely related the insight is to the task.\n{ analytical task }\n{ insight function APIs }\nPlease give your answer in the following format:\n{ format requirements }\nIn the second round, the selected insight functions are executed\nto get insights and significance scores, which are the insight\ncalculationresults to be the input for further assessment. Then,\nthe calculated results are organized into natural language sentences\nto describe insights by LLMs. After generating insights and obtain-\ning the significance score, the next step is to assess impact scores.\nWe let LLMs assign an impact score based on the nature of each\ninsight, e.g., potential consequences, urgency and timeliness, and\ninfluence on decision-making. The developer can further modify\nthe definition of impact score to fit specific analysis scenarios. The\ninsight will correspond to a triplet: ⟨viewName,dimName,value⟩\nto locate the insight with highlight effect or annotations on the\ncorresponding Original system view. The prompt template for\ninsight assessment is shown below:\nPrompt template for insight assessment:\nThe selected insights are implemented, and the result is returned,\nincluding the value and significance score. You also need to give\nan impact score. You can consider combining your data analysis\nexperience to evaluate from the following aspects: potential\nconsequences, urgency and timeliness, and influence on decision-\nmaking.\n{ insight calculation results }\nPlease give your answer in the following format:\n{ format requirements }\nAfter obtaining the generated insight with the structured\nformat, the Add annotation function within the original V A system\nwill be executed to link the data objects from the insights to\nelements within the views. For instance, if the LLM returns insight\n{‘viewsName’:‘Sales|ByState’,‘fieldName’:‘State/Province’,\n‘value’:[‘California’,‘NewYork’],‘final_score’:0.5} , the V A\nsystem needs to be able to locate the two points on the map,\nchange the style of the target element (e.g., stroke color), and add\nan annotation at that location. As we described in Sec. 4.2.1, each\ninsight type can correspond to a unique view or multiple views.\nThus, if an insight is cross-view, annotations will be added on\nmultiple views as well. Considering that it is possible that not all\ninsight types are what the user would like to see, we prefer to use\nsingle-view insights to analyze step by step.\nWhen using LLMs for data analysis directly, it’s crucial to\nconsider the strengths and limitations of the language model. We\ntested the LLMs’ data analysis performance on tabular datasets.\nThe results indicated that their accuracy for basic tasks was\nrelatively low, as detailed in Appendix E. Therefore, for tabular\ndata analysis, we use rule-based methods to guarantee accurate\nresults and use LLMs to invoke these functions based on the\nunderstanding of underlying data and the user’s task. However,\nLLMs demonstrate exceptional competence when analyzing textual\ndata, as evidenced by recent studies [17], [31]. Thus, we use LLMs\nto analyze these textual data tasks directly. Furthermore, if the\nrecommended insights are incomplete or inaccurate, we provide\nan open-question answering function. Users can type follow-up\nquestions to understand the insights in detail. Users can type follow-\nup questions to understand the insights in detail. The LLMs will\nanalyze the underlying data in the current state and return insights\nand a highlight of source data, improving the explainability of how\nthe insights are derived.\nWe adopt two strategies to address the response speed prob-\nlem of LLMs. One way is to control the output for efficiency.\nConsidering that some computational tasks, like text summaries,\ncan produce lengthy outputs, it’s essential to define the output\nlength in the prompt template to ensure it remains concise.\nAnother way is interactive questions and answers. Before executing\ninsight functions, we adopt an interactive approach where LLMs\nrecommend questions based on the assessment of the relevant\ninsight type derived from the insight selection stage. Users can\nselect a desired question to obtain insights. This alternative strategy\ncan reduce the waiting time to calculate all insights.\nIn summary, the limitation of input and output influences both\nthe calculation method and the interaction paradigm. Depending on\nthe specific analysis scenario, the above strategy can be fine-tuned\nto strike the right balance. We also discuss these challenges and\nfuture directions in Sec. 8.\n5.2.3 Report Generation\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nBased on the strategy introduced in Sec. 4, we implement the\nmethods in two steps. First, the insights’s annotations are saved, and\nthe data is a 5-tuple: ( insight, type, value, viewName, imageName).\nThe combination of analysis round m, n and viewName is the\nimage name , which will be used in LaTeX code. In general,\neach step obtains the corresponding screenshot according to\nviewName. Considering the first and last step preferably needs\nto give an overview of the entire report, we can set up to capture\nall views. Then, for a round of historicalanalysisdata, we let\nLLMs generate reports in a textual form. The prompt template for\nsummarizing reports is shown below:\nPrompt template for report summarization:\nHere is a historical analysis of the system data. The data contains\ninsights that need to be reported:\n{ historical analysis data }\nYour task is to write an insight report to present these findings.\nThe amount of insight should be equal to the number of steps\nfor given data. Ensure you include both a cover(report title) and\na conclusion.\n{ other requirements }\nSecond, given the exceptional performance of LLMs in code\ngeneration, the textual report can be transformed into a LaTeX-\nformatted presentation report. We can add requirements in the\noutput format to set the report styles. The final generated reports\nare presented to the user through an interactive visualization that\nsupports intuitive reading and markup. The prompt template for\nLaTeX code generation is shown below:\nPrompt template for LaTeX code generation:\nTransform the summarized report into LaTeX slides. For each\nslide, if an insight exhibits a clear hierarchy, segment it using\nbullet points. Accompany each insight with a screenshot from\nthe system. The filenames for these screenshots can be found\nin the historical analysis data. Please integrate the following\ncommands for style configuration.\n{ setting requirements }\n5.2.4 LEVA Interface\nTo support the whole framework, we design an interface to bridge\nusers, LLMs, and the underlying data. The interface comprised\nof four main components: Chat view , Original system view ,\nInteraction stream view, and Report view, as shown in Fig. 4.\nChat view: To receive feedback and control the entire workflow\nin our framework, this view serves as an interactive interface\nwhere users receive feedback and control other views (Fig. 4A).\nDuring the onboarding stage, the Chat view initiates a tour guide to\nintroduce the original system. When users start on exploration, this\nview will showcase questions proposed by LLMs. Users can make\nselections, and the selected questions and insights are recorded and\nvisualized in the Interaction stream view (Fig. 4C) and to support\nreport generation in Report view (Fig. 4D). Moreover, the view\nallows for open-question answering, letting users engage in fluid\nconversations with LLMs to clarify doubts or derive new insights.\nDue to possible wrong formats generated by LLMs, we allow a\nfeedback mechanism to display error messages in the Chat view\nand make users aware of failed issues.\nOriginal system view: The original V A system is combined in\nthis view. To augment the system, LEV A introduces annotations\nas arguments. These annotations, serving as guiding markers, help\nusers identify interesting data patterns. To efficiently manage these\nannotations, a dedicated control panel in Chat view has been\nintroduced (Fig. 4b2). It offers filtering capabilities and options to\nclear all annotations on the view. Users can interact with the views,\nprompting LLMs to propose questions based on their selections.\nAfter recommending a cross-view insight, if users select an area\nwith annotations in the source view, the system will show the other\npart of the insights in the target view.\nInteraction stream view & Report view: The Interaction\nStream View stands as a historical ledger, cataloging analytical\ninsights the user obtained from LLMs (Fig. 4C). Users can hover\nover the node to retrace details in each step. Each step’s analytical\ninsights are automatically saved. Considering that the user may\nwant to stop the current round and start a new one, we provide\nan end button to enable users to end their current analysis round,\nsignaling the system to start a new round of analysis. Moreover,\nwe set the Interaction stream view as hidden by default. Users can\nopen the interaction view in the menu at the top of the Chat view\nwhen they need to trace back. Upon selecting an interaction path,\nthe Report view (Fig. 4D) is triggered, presenting a comprehensive\nreport for that round.\n6 U SAGE SCENARIOS\nTo evaluate our framework, we demonstrate the LEV A-enhanced\nV A system in two usage scenarios: one is analyzing multi-facet\nevent data, and the other is analyzing tabular data. We use the\nOpenAI GPT-4 model in our work.\n6.1 Analyzing Multi-faceted Event Data\nTo illustrate how LEV A aids users throughout the V A workflow, we\nopted to reproduce a V A system: the recipient of the IEEE V AST\nChallenge 2021 Mini-Challenge 3 Award [40]. Our motivation\nfor this choice stems from several compelling reasons. Firstly,\nthis system exemplifies the intricate, human-in-the-loop decision-\nmaking tasks inherent to visual analytics. Secondly, it incorporates a\nrepresentative blend of data types and corresponding visualizations,\nencompassing text, graph, spatial, and temporal data, which is a\ntypical V A system. Lastly, its recognition as an award winner lends\ncredibility and affirms its representativeness.\nThe challenge’s task is to detect and evaluate public risks in\nAbila City during the evening of January 23, 2014. The provided\ndata include microblog records and emergency dispatch records\nfrom a call center. Thus, the system centers around a comprehensive\ntimeline that serves as the main interaction point to detect event\nevolution. A message view presents messages from specific time\nperiods. A keyword view displays messages within a selected\nperiod, allowing users to hone in on particular topics. A graph\nview reflects occurrence relations between various entities, such as\npersons and locations. Finally, the system provides a map view of\nAbila City to show the message distribution and the risk levels.\nIn this dataset, we mainly follow the guidelines outlined in\nSec. 5. However, to provide a multi-faceted analysis of the event, we\nhave made some minor adjustments and additional considerations.\n• insight functions: We propose some insight types for event\nanalysis. The single-view insight type includes summarizing\nthe events of a certain period in high-risk areas, summarizing\nthe events of a keyword, finding the nodes, messages, and\nkeywords associated with the events, and retrieving values.\nThese functions could be related and combined to address a\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nFig. 4. An implementation of LEVA comprises of four components. Users can communicate with LLMs and control the insight annotations in (a) Chat\nview; the recommended insights for next step analysis from LLMs are updated in (b) Original system view; Users can retrace the interaction history in\n(d) Interaction stream view; Once a historical analysis path is selected in (d), the generated insight report will display in (e) Report view.\nmore complex task. For example, summarizing the events could\nbe recommended as the first and then finding the relevant nodes\nin the graph. The events serve as a context in addition to the\ngraph data, as we defined in Sec. 4.2.1. We also define a cross-\nview insight to analyze multiple views at once, which is to\nsummarize events with temporal and spatial information. The\nannotations of this insight type will show on both the timeline\nview and map view.\n• propose questions: Considering that textual data analysis may\ntake a long response time, here we let the LLM propose the\nquestions first, and then the user chooses one question on the\nChat view to execute the insight functions, as the consideration\nwe described in Sec. 5.2.2.\nAt the beginning, we click the onboarding button to start a tour\nguide (Fig. 5a). The tutorial introduces the visualization type, visual\nencoding, and the coordination between views. This guidance leads\nus to know the meanings of data analyzed in the timeline view (Fig.\n5b). We can also obtain that hexagons on the map denote risk levels\nof the region (Fig. 5d). The system also tells us that all the other\nviews link to the timeline, which renders based on message type\nand keywords selected from the keyword view (Fig. 5c). Without\nthis guidance, one needs to take more time to explore the system\nand might build an inaccurate understanding of the system.\nIn the next exploration stage, we first look at the timeline as it\nfilters other views. We selected 19:34 to 19:43 since they contain\nthe highest peak. The assistant proposes a question: “What are the\nmain events in high-risk regions?” (Fig. 4b), which is related to our\nanalysis task. Then, the system points out two high-risk regions,\nFig. 5. An onboarding tour example of the VAST challenge system. (a)\nInitiation via the onboarding button, (b) Introductions to data meanings\nof “mbdata” and “ccdata”, (c) The coordination of keyword view and\ntimeline view based on selected keywords, and (d) The visual encoding\nof hexagon colors representing risk levels in specific regions.\nsummarizes two events (Fig. 4b1), and figures out the most relevant\nnodes (Fig. 4d1) and messages (Fig. 4c1). By briefly scanning\nthe summaries, we know one high-risk incident is a bicyclist who\ngets hit but is helped out by people from Brew’ve Been Served.\nThe other is a black van and police shootout in the store. The\nannotations on the Graph view indicate that the key player in\n“Gelatogalore Shooting Incident” is the “van guys”, which happens\nin “Gelatogalore”, which is a location colored in green. To further\nknow if the black van exits in other time periods, we select ”van”\nfrom the keyword view (Fig. 4e1). The assistant catches our action\nand proposes to analyze the related events with “van”. We clicked\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nFig. 6. An example of implementing our framework for tabular data analysis in the exploration stage, which demonstrates the analysis results of each\nstep in a single round (a-d) and the result of multiple rounds (e-g). The annotations represent the insights and potential impact of the LLM output. The\nanalytical process starts with selecting Consumer on Segment view and four exploration steps with LLMs recommendations (a-d). With the help of\nLLMs, we found that manufacturers with high sales in different regions have a significant distribution of preferences (e-g).\nit and four events are summarized along the timeline: “Hit and\nrun”, “Pulled over”, “Hostage situation”, and “Standoff” (Fig. 4e2).\nAfter brushing each event on the timeline view, the annotations\nshow up on the map view. Reading the details of each event, we\nknow the whole story of the black van.\nIn the process of exploration, valuable insights are recorded\nand can be traced back in the interaction stream graph to avoid\nforgetting important insights. As incidents related to the black van\nraise the most public safety concerns, we choose this round to\ngenerate the report (Fig. 4f). The generated report explains each\ninsight and retains the pictures of the exploration process, and\nsummarizes the appropriate title and conclusion page (Fig. 4g).\n6.2 Analyzing Tabular Data\nTo demonstrate the generality of the proposed framework, we apply\nit to tabular data analysis 6. According to our method design in\nSec. 4, text data analysis is a task that LLMs are good at, but table\ndata are more suitable for analysis by statistical methods, ensuring\naccuracy and efficiency. Nonetheless, LLMs remain instrumental,\nespecially in distributing tasks and assessing insights. Thus, this\nstudy mainly introduces the exploration stage of tabular data.\nFor illustrative purposes, we select a dashboard from Tableau\nthat displays superstore sales data from 2022 2. This dashboard\nfeatures nine distinct views: a choropleth map indicating state-\nwise sales and five bar charts delineating sales across segments,\ncategories, sub-categories, top 10 manufacturers, and top 10\n2. https://public.tableau.com/app/profile/p.padham/viz/\nSuperstoreDashboard 16709573699130/SuperstoreDashboard\ncustomers. Additionally, there are four line charts that trace the\ntrajectories of sales, profits, orders, and customer metrics.\nBefore exploration, we prepare specifications, tasks, insight\nfunctions, and data handlers and adjust insights content based\non the analytics scenario, following implementation guidelines\noutlined in Sec. 5.1. Some considerations described as below:\n• specification: As Tableau dashboards’ source files (with file\nextension .twb) are structured in XML format and contain all\nthe system information that can be extracted and converted\nto the values in specifications, which allows us to extract\nspecifications automatically.\n• analysis task: While the dashboards display a variety of\ndata, including “sales”, “profit”, “orders” and “customers”,\nwe initially set a task to focus on analyzing the “sales” situation\nfrom multiple perspectives.\n• insight functions: We employ fundamental insight functions\nas referenced in Sec.4.2.1, such as outstanding number one,\nchange point, trend, and correlation. Given the breadth of\ninsights possible with tabular data, it is easy for users to\noverlook connections to prior findings. To address this, we also\nset to compare current insights with those from the previous\nstep, using the earlier results as context for computation, as\ndefined in Sec.4.2.1.\n• data handlers: We integrated the Tableau dashboard into a\nwebsite using its embedding API [52]. This API permits “get\nselection data” (Sec. 5.1). However, due to API limitations,\ntutorials and insights are shown in the chat view, and analytical\ninsights are stored accordingly.\n• propose explanations: Considering that in sales analysis, users\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE 1\nThe questionnaire and the corresponding types, including objective questions and subjective questions.\nType Specific questions\nR1: Perceptual visual encoding\nO1:What do the visual encoding and corresponding data mean for the timeline view?\nS1: Do you know what each data variaiables means?\nS2: Do you understand the meaning of the visual elements?\nR2: Interaction and Coordination\nO2: How the timeline view coordinated with other views?\nS3: Are you clear on how to interact in each view?\nS4: Are you clear on how the views are related?\nR3: Data Pattern Discovery O3: What high-risk level events occurred in the peak period?\nS5: Is it easy to get data findings (such as events, key nodes) in these views?\nR4: Hypothesis Formulation and Validation\nO4: What are the key player and location of the summarized event?\nS6: Are you clear about the next step analysis for validation?\nS7: Do you have easy access to rich hypotheses?\nR5: Summarization of Exploration Results\nO5: Discover related events of the keyplayer and summarize them into a report.\nS8: Is it easy to write an analysis report on the interaction results?\nS9: Are you satisfied with the quality of the report you wrote?\nmay need some hint for reason analysis behind the insights.\nWe let the LLM give additional explanations in the final output\nbased on its board knowledge [26].\nIn the beginning, we see that the Consumer on sales by segment\nview was highlighted as the highest value. After selection, the other\nfour bar charts and the map are filtered, and some insights are\nrecommended. The first recommended insight is “a significant\nchange point in March 2022” in the sales line chart (Fig. 6a).\nThe LLMs also suggest potential reasons for the change point\ncould be a successful market campaign or business activities of\nthe superstore. Then, a recommended insight is a strong positive\ncorrelation between profit and sales (Fig. 6b). This finding makes\nus realize that the strategy launched in February succeeds in turning\nsales into profits, which can continue to be used in the future.\nBeyond time series insights, the recommendations also high-\nlight extreme values, specifically, the top two rankings in states\nor manufacturers (Fig. 6c, d). We notice that the two states with\noutstanding sales are “California” and “New York”. Consequently,\nour subsequent analytical focus pivots to these states. Within\nthese jurisdictions, the top two manufacturers identify as “Other”\nand “GBC”, the latter being a renowned office supply brand (Fig.\n6e). The below explanation suggests a distinct market inclination\ntowards smaller manufacturers in these regions.\nAn intriguing insight emerges when comparing the top manu-\nfacturers at the state and national levels. While “Other” and “Canon”\ndominate sales across most states, “Canon” does not maintain this\nlead in “California” and “New York” (Fig. 6f). This variation\naccentuates the nuanced manufacturing preferences specific to\nregions. Motivated by this finding, we move to analyze the\ndistribution of “Canon” nationwide. Upon deselecting “California”\nand “New York”, the recommended insight reveals Washington and\nDelaware as the leading states and reiterates heightened popularity\nof “Canon” in these regions (Fig. 6g). These findings may help\npotential adjustments to cater to regional sales predilections.\n7 U SER STUDY\nWe conducted a user study to evaluate the effectiveness of LEV A for\nenhancing V A. Specifically, we wanted to verify if LEV A improves\nusers’ understanding of the original system, and aids in insights\ndiscovery and summarization more efficiently.\n7.1 Study Setup\nHere, we introduce the user study by discussing participants and\napparatus, questionnaire, procedure, and results analysis.\nParticipants and Apparatus: We recruited 20 participants\nwith backgrounds ranging from computer science, data science,\nand mathematics to business analysis, ages from 19 to 25 ( µ =\n22.37,σ = 1.79), denoted as P1-P20. Among them, 4 participants\nare novices in using V A systems. Participants were randomly\nassigned to two groups, of which 10 participants used the V A\nsystem from the V AST challenge system without LEV A’s assistance\nas the control group and 10 with assistance as the treatment group.\nThe studies were all conducted using a monitor with a resolution\nof 2560 × 1440, along with a mouse and keyboard.\nProcedure: The study was composed of three sessions, begin-\nning with a 10-minute introduction to our framework, the usage\nof our system, and the original system. Participants can follow\nthe experimenter to use the system and familiarize themselves\nwith system functions and workflow. The formal assessment was\nconducted using a questionnaire including objective and subjective\nquestions. Each participant in the two groups was first asked to\nanswer objective questions, followed by subjective questions. We\nalso conducted a short interview to collect detailed feedback from\nthe participants.\nQuestionnaire and Measurements: Based on the five require-\nments (R1-R5) outlined in Section 3.2, we propose five objective\nquestions (O1-O5) and nine subjective questions (S1-S9) to evaluate\nthe effectiveness of our framework in meeting these requirements\n(Table. 1). Each requirement was tested with at least one objective\nquestion and one subjective question. For objective questions, the\nO1-O4 assessed users’ understanding of data and visual mapping,\nas well as their ability to discover insights from the V A system. The\nO5 required participants to find insights and write a report. The S5-\nS9 is to verify the effectiveness of LEV A in insight recommendation\nand summarization. Answer times and correct rates were recorded\nfor each question to gauge the influence of LEV A in terms of\nboth efficiency and effectiveness. The correctness assessment for\neach question is not a strictly binary 0-1 variable but allows for\na 0.5 score when answering half of it correctly. For example,\nfor the first question (O1) “ What do the visual encoding and\ncorresponding data mean for the timeline view?”, if the user can\ncomprehend the visual encoding but is uncertain about the data\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n0\n0.2\n0.4\n0.6\n0.8\n1\nControl GroupTreatment Group\nCorrect Rate\nR1 R2 R3 R4 R51 \n2 \n3 \n4 \n5 \n6 \n7 \nControl GroupTreatment Group\nScore\n*** *** ** *** *** \nO1 O2 O3 O4 O5\n0 \n100 \n200 \n300 \n400 \nControl GroupTreatment Group\nTime Cost (s)\n* *** * *** \nFig. 7. User Study Results. On the left is the correct rate of five objective questions. In the middle are self-rated scores for subjective questions given\nby participants. On the right are the answer time for the four timed questions. The number of asterisks (*) in the upper part of the figure indicates the\nsignificance level of the test (* : p < 0.05; ** : p < 0.01; *** : p < 0.001). The results suggests our method can improve performance from these three\nperspectives.\nmeaning, a score of 0.5 is recorded. Finally, the average score of all\nparticipants is calculated as the correct rate for this question. For\nsubjective questions, we included a 7-point Likert scale to allow\nusers to evaluate their level of understanding of the system and\nwhether they encountered any difficulties in gaining data insights\nand writing analysis reports. The control group was asked, “Do\nyou understand the meaning of the visual elements of each view?\nRate your understanding from 1 to 7.” For the experimental group,\nthe question was modified with the prefix “With LEV A’s assistance”\nto gauge the impact of LEV A on understanding. In table 1, we\npresent only the core questions, omitting prefixes and suffixes for\nbrevity. Among them, S1-S4 are used to verify whether LEV A\nhelps onboarding. Thus, we need to test their understanding of the\nUI components of the original V A system. The S5-S9 is to verify\nthe effectiveness of LEV A in insight discovery and summarization.\n7.2 Results and Analysis\nTo compare the answer time and subjective scores in two groups,\nwe first conducted the Shapiro-Wilk test in the user study to verify\nthe assumption of normality, ensuring the validity of subsequent\nt-tests. The result of the Shapiro-Wilk test confirmed the normal\ndistribution of users’ answer times and scores in our samples. After\nestablishing normality, we applied the independent t-test to assess\ndifferences in answer times and accuracy between the treatment and\ncontrol groups, ensuring the reliability of our results. We reported\nour results of the correct rate, subjective scores and time cost. The\ndetailed result analysis of the user study is presented below.\nAccuracy: We reported the results of objective and subjective\nmeasures to assess users’ understanding of V A. The correct rates\nof five objective questions are shown in Fig. 7 (left). For each\nquestion, the treatment group had a correct rate of over 85%, while\nthe control group’s correct rate ranged from 50% to 65%. Most\nparticipants can distinguish encoding, but as the legend is not\nspecific, they were unclear about the meaning of “mbdata” and\n“ccdata” represented by the different colors in the Timeline view\n(O1). Furthermore, in O2, most participants in the control group\nfound it challenging to know the influence of the keyword view on\nthe timeline, while in the treatment group, there were explanations\nof the interactions between various views. In O3, all participants\nin the treatment group answered correctly, while the control group\nwas only 60% correct because the two events with higher risk\nlevels were automatically highlighted in the system exploration\nin the treatment group. However, the participants in the control\ngroup had to switch between the map and message view and read\nthe text repeatedly. The O4 yielded a 50% correctness rate in the\ncontrol group. This lower accuracy can be attributed to the fact\nthat a significant portion of participants could only identify the key\nplayer involved in the event while struggling to pinpoint the event’s\nlocation within the intricate graph view. In contrast, the treatment\ngroup benefited from an automated annotated system that provided\nclear event location information. For insight summarization (O5),\nonly a few participants in the control group were able to assemble a\nmore coherent understanding of the event (P12, P15, P16), and they\nhad prior experience in social media V A and cost a considerable\namount of time (over 5 minutes). In contrast, in the treatment group,\nonly one participant (P4) failed due to their insensitivity to the\nlocation name and not noticing the legend.\nScore: As shown in Fig. 7 (middle), the treatment group\nobtained higher scores than the control group, and the average\nscore is improved by approximately 49.21%. This indicates that\nparticipants perceived LEV A as an improvement over the original\nV A system in various aspects. For R1 and R2, while most\nparticipants understood brushing the timeline would filter other\nviews, it was easy to overlook the filtering from the keyword\nview to the timeline view. Thus, they mostly got half of the\nscore in accuracy but gave a lower subjective score due to the\nconfusion in detailed interactions. This suggests that there was a\nlack of clear explanations of interactions and coordination in the\nsystem, underscoring the significance of effective onboarding. The\nperformance in insight discovery varies among participants; only a\nfew people can find some data patterns and form new hypotheses\n(R3, R4). For report generation (R5), the treatment group scored\nsignificantly higher than the control group in the users’ own scores\non the ease (S8) and the quality (S9) of generating reports (p\n<0.001). Participants found it difficult to locate previously explored\ndata due to the large amount of information (P18, P19). The results\nimply that users typically encounter greater difficulty and produce\nlower-quality insights when tasked with self-directed exploration\nand manual report generation compared to the generation of reports\nthrough an automated process.\nTime Cost: We reported the time costs of the five objective\nproblems in Fig. 7 (right). The results indicate that, with the\nexception of the first question (O1), the treatment group exhibited\nshorter answer time compared to the control group ( p < 0.05). For\nthe last four tasks, the average time is reduced by approximately\n39.26% in the treatment group. For insight recommendations (O3,\nO4), the average time is reduced by approximately 41.98% and\n31.27%. For the response time of LLMs, summarization events\n(O3) need more time to return results, about 25s for the long input\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\nand output data of O3, but it is still the one with the most time\nsavings. P2 suggested that “Even though event summarization took\na bit longer than others, it was acceptable given that it saved\nus even more time than if we had to read and summarize the\ninformation ourselves”. The LLMs’s response time of finding key\nnodes (O4) is faster than O3, only requiring around 5-6s, improving\nthe average 31.27% of time. For the report generation (O5), using\nthe LLM to summarize can save an average of 42.27% of time\ncompared with no assistance. These findings underscore that the\nutilization of LEV A can substantially diminish the time investment\nrequired by users when employing the V A system. In addition, we\nobserved that LLMs have the probability that the text summary\nis not comprehensive enough. In the study for P7, the LLM did\nnot clearly summarize who the key player was. However, through\nthe free ask in the chat view, P7 got the answer, which took one\nminute longer than the average time. This shows that when relying\non LLMs for data analysis, it is necessary to provide free questions\nand answers to ensure the acquisition of detailed information.\nFeedback: In the final interview, participants were asked about\ntheir opinions on the system. On one side, participants in the\ncontrol group offered feedback pertaining to the V A system itself,\nhighlighting issues such as unclear visual elements and legend\n(P11, P13), confusing color schemes (P20), excessive textual data\n(P15, P17, P19), and complex interaction between views (P12),\nno idea where to click and how to explore (P20). On the other\nside, participants in the treatment group contributed constructive\nsuggestions for improvement:\nFor onboarding tutorial generation, many participants agreed\nthat onboarding guidance is required, especially for beginners\n(P9). To improve the tutorial, P4 suggested that “ The system can\nfurther highlight some important keywords in the tutorial ”. P7\nand P8 recommended incorporating animations, such as arrows, to\nexplain the interactions and relationships between views: “ Some\nanimations like arrows could be used to explain the interactions\nand associations between views .” This inspires us that the more\nintuitive tutorial generation could be a future research direction.\nFor insight exploration, participants had varied feedback on\ninsight exploration. Some found that the LLM effectively guided\ntheir analysis (P4), quickly leading them to valuable insights (P3).\nOthers mentioned that building on the LLM’s analysis, they were\ninspired to think further and pose new questions by free ask (P7).\nHowever, there were also comments about the improvement of more\nanalytical methods. P5 provided a suggestion for improvement:\n“In addition to following the original workflow exploration of the\nsystem, like the event analysis, some other tasks, such as starting\nfrom a person or spreading relationships, could be considered.” The\ncomment points out the need for more types of insight. Therefore,\nfurther study could focus on how to bring more domain knowledge\nand analytical methods to LLMs.\nFor selective report generation, most participants appreciated\nthe reports. P6 mentioned that “not only the comprehensive content\nwith images and texts but also the good formatting .” To improve\nthe report, P10 pointed out that “ If some steps in a stream view\ncould be removed or merged, the generated report would be more\nuseful.” The comments demonstrate that they considered our report\ngeneration as a convenient and useful function, but further need to\nimprove the log organization and screening. Additionally, we also\ncollected the comments for scalability. P3 is interested in using\nLEV A’s components as plug-ins for other V A systems. This is a\npractical suggestion to provide a powerful tool to enhance more\nV A scenarios we plan to study further. We discuss these valuable\nsuggestions from the user study in the Sec. 8\n8 D ISCUSSION\nIn this section, we discuss the generalizability and the performance\nof LLMs we observed and highlight the lessons learned from the\nresearch and future directions.\nGeneralizability: Our framework is generalizable in four\naspects. First, the system specifications we formulate can be\nexpanded and comprehended by LLMs for tutorial generation.\nSecond, our strategy for recommending insights is adaptable,\nallowing the LLMs to distribute computational tasks and assess\ninsights in V A systems. Third, our interactive reports generation\nstrategy can be extended to other systems by preserving analysis\nrecords. During practical implementations, users can fine-tune these\nstrategies based on the V A tasks and the performance nuances of\nLLMs. Finally, LEV A remains independent of LLMs. Currently,\nwe integrate LLMs’s capabilities to support the exploration of\nV A workflows. Although the future appearance of models with\nother modal inputs saves efforts on engineering implementations of\nbasic information processing, LEV A’s strategy for guiding human\nintelligence model communication remains unchanged.\nThe performance of LLMs: Despite their immense strengths,\nLLMs still have several limitations, and there is a lively and\nongoing debate on their merits [53]. The first problem is the\naccuracy. While one might anticipate that LLMs-enhanced systems\nwill improve as more users interact with them, there are potential\nrisks that LLMs could assist analysts in ways that might not be\nentirely accurate. Recent research focuses on using fine-tuned\nLLMs for tool using [43], transforming natural language into\ncode [62], and employing advanced prompting strategies such as\nself-instruction to compute step-by-step [56]. To enable correct\nparsing output, we could add an example of constraining the\noutput format of LLMs [60]. Moreover, we could also provide an\nerror reporting strategy to make users aware of the unsuccessful\nresponse. We argue that addressing and communicating potential\nerrors remains an exciting and open research challenge and calls\nfor further exploration. The second problem is the response time.\nOur current approach is to control the output length of the LLMs or\ninvoke alternative computation functions that are faster than LLMs.\nHowever, controlling the output length may sacrifice the level\nof detail and depth in the generated output. One future direction\ncould be exploring acceleration strategies from the perspective of\ncache mechanisms [15], and predictive analytics might offer speed\nimprovements without compromising the quality of the insights\ngenerated.\nHuman-LLM collaboration in open-ended exploration: In\nthe context of open-ended exploration, the interplay between LLM\nassistance and human judgment presents a nuanced dynamic. Our\nuser study reveals that the timely questions and insights proposed\nby LLMs could facilitate efficiency and even prompt users to follow\nup with their own new questions. However, it is also essential to\nrecognize that users could be over-reliant on LLM guidance. They\ncan thus be steered toward particular directions while missing\nothers. This interplay between guided exploration and autonomous\ndiscovery is critical to the design of LLM-supported analytical\nsystems. It warrants careful consideration to balance the benefits\nof guidance with the freedom of exploration – a challenge also\nrecognized in the visual analytics guidance literature [6].\nDomain knowledge integration for LLMs in specific tasks:\nWhile ensuring accuracy through the LLM-based insight function\ninvocation mechanisms, there is a need to enhance problem-solving\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\nflexibility in specific domain scenarios. The user study indicates\nthat some users with domain analysis backgrounds will have\nmore profound analytical ideas in open-question answering. LLMs\nneed better insight into the calculation and understanding of the\nanalytical task. This capability needs more domain knowledge.\nThere might be two methods: one is tailoring insight types and\nfunctions that calculate insights for a particular problem, and\nanother is to employ a fine-tuned LLM, specifically optimized\nto serve the needs of the domain. [20]. Both of which are open\nchallenges for further research.\nInsight recommendation vs. Interaction recommendation:\nOur current insight recommendation focuses on extracting essential\ninsights from the underlying data, capitalizing on the inherent\npatterns and relationships present within the dataset. Another\nstrategy is interaction recommendation, which derives insights\nfrom many user interaction data, learning and predicting the next\ninteraction object [29]. Such an approach recommended insights\nwith a more substantial contextual relevance. Looking forward,\nthere’s potential to integrate the historical interaction data. This\nmerger can pave the way for more intelligent and contextually\naligned insight recommendations.\nInterpretation of user interaction for report generation:\nIn report generation, both user selections and LLM-generated\ninsights are pivotal. User selections offer a crucial context for the\ninsights generated by the LLM, making it essential to preserve\nthese selections for a complete exploration record. Currently, our\napproach can describe the actions taken by the user but falls short in\ninterpreting the underlying motivations of these actions, impacting\nour capacity to provide comprehensive context in the exploration\nnarrative. Typically, these selections are driven by insights users\nfrom their observations of data and combining personal knowledge.\nFuture directions could include mining the related data patterns\nand combining more domain knowledge to generate more coherent\nexploration reports.\nNarrative-driven report generation: Current methodologies\nin report generation within our framework primarily focus on\ncompiling logs of exploratory logs into a step-wise report. Looking\nahead, future research could pivot towards employing narrative\nstructures and strategies for the automatic summarization of these\nlogs. One direction could be using LLMs for automatic summation.\nThe enhancements would come in two folds. Firstly, refining the\nnarrative by identifying and reorganizing story pieces based on data\nrelationships like temporal and spatial transitions [28], and then\ncrafting coherent explanations [63]. Secondly, enhancing data pre-\nsentation and narrative flow by considering the transitions between\nnarrative segments [46] with multi-modality expression. These\nnarrative techniques could provide a more intuitive understanding\nof the explored results.\nLLM-based enhancement vs. Rule-based enhancement: The\nadvantage of using LLMs is that it enhances flexibility and\nscalability in aiding various V A scenarios, offering capabilities\nbeyond what a few lines of traditional coding can achieve. LLMs\nexcel in natural language understanding and generate insights\nacross a broad knowledge domain. While traditional coding is\nprecise, it often becomes cumbersome and inflexible when faced\nwith diverse scenarios and evolving user needs. Specifically, it\nwould require creating an extensive set of detailed rules to break\ndown various query requests, matching computational modules\nwith data, binding views to different types of insights, utilizing\nnumerous manual templates to introduce the system, describing\ncomputation results, and guiding the exploration process.\nComponentization and plug-in: During our user study, we\nfound out that users expect using our framework to assist them\nin exploring more V A systems with different tasks. Therefore,\nit is better to offer a toolkit and divide the functionality of\nLEV A and the views included in the current implementation\ninto components. This toolkit will include the LLM-powered\ncomponents and programmatic interfaces. For developers of the\noriginal V A system to integrate with this toolkit, the minimum\ndeveloping cost is to provide the extensions and APIs to receive\nthe LLM’s output and modify the prompts to customize the output\nformat of tutorials or annotations. By doing so, we could allow\nLEV A to be integrated as plugins in different V A systems. Further,\nwe plan to provide more templates to allow users to customize\nthe information (e.g., specification) entered into the LLM and the\ndesired tutorials, reports, and insight recommendation from the\nLLM according to their needs.\nSpecification for V A systems:In the extraction of the declar-\native grammar required in LEV A, we refer to previous work\ndescribing basic charts [30] and adding descriptions of the data\ntable, user interaction, and coordination based on the goal of\nunderstanding data, view and insight recommendation. An abstract-\nlevel description of data and functionality for V A specifications\ncan further benefit various downstream tasks. Such abstractions\ncan greatly facilitate endeavors like the automatic generation of\nvisualization systems and automated storytelling. As we found the\ninteraction between views could be introduced with more intuitive\nannotations and animations in the tutorial, the future work can\nresearch on how to improve the specification and guide LLMs to\ngenerate such tutorial.\n9 C ONCLUSION\nIn this study, we introduced LEV A, a framework that integrates\nLLMs into V A workflows to achieve intelligent V A. LEV A\nenhances visual analytics through three pivotal stages: onboarding,\nexploration, and summarization. During the onboarding stage,\nit interprets visualizations and their relationships, fostering the\ncreation of dynamic tutorials. In the exploration stage, our insight\nrecommendation strategy harnesses LLMs recommend analytical\ninsights based on the interpretation of the system’s status and data,\nenriching visual analysis via annotations. In the summarization\nstage, LEV A allows users to revisit and select analytical history,\nstreamlining the report generation process. Our integration of LEV A\nwith a V A system led to the development of an interactive interface\nthat fosters a dialogue between users and LLMs. We conducted two\nusage scenarios and a user study to demonstrate the effectiveness\nof our framework.\nACKNOWLEDGMENTS\nThe authors want to thank the reviewers for their suggestions.\nThis work is supported by Natural Science Foundation of China\n(NSFC No.62202105) and Shanghai Municipal Science and Tech-\nnology Major Project (2021SHZDZX0103), General Program (No.\n21ZR1403300), Sailing Program (No. 21YF1402900) and ZJLab.\nSUPPLEMENTAL MATERIAL\nAppendix A introduces the two specifications of the V AST\nchallenge system and the Tableau system. Appendix B describes the\nprompt examples we used to generate tutorials, insights, and reports.\nAppendix E describes the evaluation of LLMs’ data analysis\ncapability.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16\nREFERENCES\n[1] N. Andrienko, G. Andrienko, S. Miksch, H. Schumann, and S. Wrobel.\nA theoretical model for pattern discovery in visual analytics. Visual\nInformatics, 5(1):23–42, 2021.\n[2] M. Brehmer and T. Munzner. A multi-level typology of abstract\nvisualization tasks. IEEE Transactions on Visualization and Computer\nGraphics, 19(12):2376–2385, 2013.\n[3] S. P. Callahan, J. Freire, E. Santos, C. E. Scheidegger, C. T. Silva, and\nH. T. V o. VisTrails: Visualization meets data management. InProceedings\nof the ACM SIGMOD International Conference on Management of Data ,\npages 745–747, 2006.\n[4] Y . Cao, X. Li, J. Pan, and W. Lin. VisGuide: User-oriented recommenda-\ntions for data event extraction. In Proceedings of SIGCHI Conference on\nHuman Factors in Computing Systems, pages 1–13, 2022.\n[5] D. Ceneda, N. Andrienko, G. Andrienko, T. Gschwandtner, S. Miksch,\nN. Piccolotto, T. Schreck, M. Streit, J. Suschnigg, and C. Tominski. Guide\nme in analysis: A framework for guidance designers. Computer Graphics\nForum, 39(6):269–288, 2020.\n[6] D. Ceneda, T. Gschwandtner, T. May, S. Miksch, H.-J. Schulz, M. Streit,\nand C. Tominski. Characterizing guidance in visual analytics. IEEE\nTransactions on Visualization and Computer Graphics, 23(1):111–120,\n2017.\n[7] D. Ceneda, T. Gschwandtner, and S. Miksch. A review of guidance\napproaches in visual data analysis: A multifocal perspective. Computer\nGraphics Forum, 38(3):861–879, 2019.\n[8] S. Chen, J. Li, G. Andrienko, N. Andrienko, Y . Wang, P. H. Nguyen,\nand C. Turkay. Supporting Story Synthesis: Bridging the gap between\nvisual analytics and storytelling. IEEE Transactions on Visualization and\nComputer Graphics, 26(7):2499–2516, 2018.\n[9] S. Chen, L. Lin, and X. Yuan. Social media visual analytics. Computer\nGraphics Forum, 36(3):563–587, 2017.\n[10] C. Chenghan and H. Lee. Can large language models be an alternative to\nhuman evaluations? arxiv.2305.01937, 2023.\n[11] D. Deng, A. Wu, H. Qu, and Y . Wu. DashBot: Insight-driven dashboard\ngeneration based on deep reinforcement learning. IEEE Transactions on\nVisualization and Computer Graphics, 29(1):690–700, 2023.\n[12] V . Dhanoa, C. Walchshofer, A. Hinterreiter, H. Stitz, E. Groeller, and\nM. Streit. A process model for dashboard onboarding. Computer Graphics\nForum, 41(3):501–513, 2022.\n[13] V . Dibia. LIDA: a tool for automatic generation of grammar-agnostic visu-\nalizations and infographics using large language models. In Proceedings\nof the Annual Meeting of the Association for Computational Linguistics:\nSystem Demonstrations, pages 113–126, 2023.\n[14] R. Ding, S. Han, Y . Xu, H. Zhang, and D. Zhang. QuickInsights: Quick\nand automatic discovery of insights from multi-dimensional data. In\nProceedings of International Conference on Management of Data, pages\n317–332, 2019.\n[15] P. R. Doshi, E. A. Rundensteiner, and M. O. Ward. Prefetching for visual\ndata exploration. In Proceedings of the Eighth International Conference\non Database Systems for Advanced Applications , pages 195–202, 2003.\n[16] D. Gotz and M. X. Zhou. Characterizing users’ visual analytic activity\nfor insight provenance. Information Visualization, 8(1):42–55, 2009.\n[17] T. Goyal, J. J. Li, and G. Durrett. News summarization and evaluation in\nthe era of gpt-3. arXiv.2209.12356, 2023.\n[18] E. Horvitz. Principles of mixed-initiative user interfaces. In Proceedings\nof the SIGCHI Conference on Human Factors in Computing Systems ,\npages 159–166, 1999.\n[19] K. Hu, M. A. Bakker, S. Li, T. Kraska, and C. Hidalgo. VizML: A machine\nlearning approach to visualization recommendation. In Proceedings of\nthe SIGCHI Conferenceon Human Factors in Computing Systems, pages\n1–12, 2019.\n[20] C. Jeong. Fine-tuning and utilization methods of domain-specific llms.\narxiv.2401.02981, 2024.\n[21] D. Keim, G. Andrienko, J.-D. Fekete, C. G ¨org, J. Kohlhammer, and\nG. Melan c ¸on. Visual Analytics: Definition, process, and challenges .\nSpringer, 2008.\n[22] J. Kohlhammer, D. Keim, M. Pohl, G. Santucci, and G. Andrienko.\nSolving problems with visual analytics. Procedia Computer Science ,\n7:117–120, 2011.\n[23] B. C. Kwon and B. Lee. A comparative evaluation on online learning\napproaches using parallel coordinate visualization. In Proceedings of\nSIGCHI Conference on Human Factors in Computing Systems , pages\n993–997, 2016.\n[24] C. Lai, Z. Lin, R. Jiang, Y . Han, C. Liu, and X. Yuan. Automatic annotation\nsynchronizing with textual description for visualization. In Proceedings of\nthe 2020 SIGCHI Conference on Human Factors in Computing Systems ,\npages 1–13, 2020.\n[25] B. Lee, C. Plaisant, C. S. Parr, J.-D. Fekete, and N. Henry. Task taxonomy\nfor graph visualization. In Proceedings of the 2006 AVI Workshop on\nBEyond Time and Errors: Novel Evaluation Methods for Information\nVisualization, pages 1–5, 2006.\n[26] H. Li, Y . Wang, Q. V . Liao, and H. Qu. Why is AI not a panacea for data\nworkers? an interview study on human-ai collaboration in data storytelling.\narXiv.2304.08366, 2023.\n[27] H. Li, L. Ying, H. Zhang, Y . Wu, H. Qu, and Y . Wang. Notable: On-the-fly\nassistant for data storytelling in computational notebooks. In Proceedings\nof SIGCHI Conference on Human Factors in Computing Systems, 2023.\n[28] W. Li, Z. Wang, Y . Wang, D. Weng, L. Xie, S. Chen, H. Zhang, and\nH. Qu. GeoCamera: Telling stories in geographic visualizations with\ncamera movements. In Proceedings of the 2023 CHI Conference on\nHuman Factors in Computing Systems, 2023.\n[29] Y . Li, Y . Qi, Y . Shi, Q. Chen, N. Cao, and S. Chen. Diverse interaction\nrecommendation for public users exploring multi-view visualization\nusing deep learning. IEEE Transactions on Visualization and Computer\nGraphics, 29(1):95–105, 2023.\n[30] Y . Lin, H. Li, A. Wu, Y . Wang, and H. Qu. DMiner: Dashboard design\nmining and recommendation. IEEE Transactions on Visualization and\nComputer Graphics, pages 1–15, 2023.\n[31] C. Liu and B. Wu. Evaluating large language models on graphs:\nPerformance insights and comparative analysis. arXiv.2308.11224, 2023.\n[32] C. Liu, L. Xie, Y . Han, D. Wei, and X. Yuan. AutoCaption: An approach\nto generate natural language description from visualization automatically.\nIn Proceedings of IEEE Pacific Visualization Symposium, pages 191–195,\n2020.\n[33] Y . Liu, T. Althoff, and J. Heer. Paths Explored, Paths Omitted, Paths\nObscured: Decision points & selective reporting in end-to-end data\nanalysis. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems, pages 1–14, 2020.\n[34] Z. Liu, C. Chen, J. Wang, X. Che, Y . Huang, J. Hu, and Q. Wang. Fill\nin the Blank: Context-aware automated text input generation for mobile\ngui testing. In Proceedings of International Conference on Software\nEngineering, pages 1355–1367, 2023.\n[35] Y . Luo, N. Tang, G. Li, J. Tang, C. Chai, and X. Qin. Natural language\nto visualization by neural machine translation. IEEE Transactions on\nVisualization and Computer Graphics, 28(1):217–226, 2022.\n[36] P. Maddigan and T. Susnjak. Chat2VIS: Generating data visualizations via\nnatural language using chatgpt, codex and gpt-3 large language models.\nIEEE Access, 11:45181–45193, 2023.\n[37] D. Moritz, C. Wang, G. L. Nelson, H. Lin, A. M. Smith, B. Howe, and\nJ. Heer. Formalizing visualization design knowledge as constraints:\nActionable and extensible models in draco. IEEE Transactions on\nVisualization and Computer Graphics, 25(1):438–448, 2019.\n[38] A. Narechania, A. Srinivasan, and J. Stasko. NL4DV: A toolkit for\ngenerating analytic specifications for data visualization from natural\nlanguage queries. IEEE Transactions on Visualization and Computer\nGraphics, 27(2):369–379, 2021.\n[39] OpenAI. GPT-4 technical report. arxiv.2303.08774, 2023.\n[40] L. Peng, Y . Zhao, Y . Hou, Q. Wang, S. Shen, X. Lai, J. Gao, J. Dong,\nZ. Lin, and S. Chen. Mixed-initiative visual exploration of social media\ntext and events. In Proceedings of the IEEE Conference on Visualization\nand Visual Analytics, 2021.\n[41] I. P ´erez-Messina, D. Ceneda, M. El-Assady, S. Miksch, and F. Sper-\nrle. A typology of guidance tasks in mixed-initiative visual analytics\nenvironments. Computer Graphics Forum, 41(3):465–476, 2022.\n[42] X. Qian, R. A. Rossi, F. Du, S. Kim, E. Koh, S. Malik, T. Y . Lee, and\nJ. Chan. Learning to recommend visualizations from data. In Proceedings\nof the ACM SIGKDD Conference on Knowledge Discovery & Data Mining,\npages 1359–1369, 2021.\n[43] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\nB. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and\nM. Sun. ToolLLM: Facilitating large language models to master 16000+\nreal-world apis. arxiv.2307.16789, 2023.\n[44] R. Sevastjanova, M. El-Assady, A. Bradley, C. Collins, M. Butt, and\nD. Keim. VisInReport: Complementing visual discourse analytics through\npersonalized insight reports. IEEE Transactions on Visualization and\nComputer Graphics, 28(12):4757–4769, 2022.\n[45] L. Shen, E. Shen, Y . Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, and J. Wang.\nTowards natural language interfaces for data visualization: A survey.IEEE\nTransactions on Visualization and Computer Graphics, 29(6):3121–3144,\n2022.\n[46] D. Shi, F. Sun, X. Xu, X. Lan, D. Gotz, and N. Cao. AutoClips: An\nautomatic approach to video generation from data facts. In Computer\nGraphics Forum, volume 40, pages 495–505. Wiley Online Library, 2021.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17\n[47] F. Sperrle, D. Ceneda, and M. El-Assady. Lotse: A practical framework\nfor guidance in visual analytics. IEEE Transactions on Visualization and\nComputer Graphics, 29(1):1124–1134, 2022.\n[48] J. Stasko, R. Amar, and J. Eagan. Low-level components of analytic\nactivity in information visualization. In Proceedings of the 2005 IEEE\nSymposium on Information Visualization, page 15, 2005.\n[49] C. Stoiber, D. Ceneda, M. Wagner, V . Schetinger, T. Gschwandtner,\nM. Streit, S. Miksch, and W. Aigner. Perspectives of visualization\nonboarding and guidance in va. Visual Informatics, 6(1):68–83, 2022.\n[50] C. Stoiber, F. Stoiber, M. Pohl, H. Stitz, M. Streit, and W. Aigner.\nVisualization onboarding: Learning how to read and use visualizations. In\nProceedings of VisComm Workshop at IEEE VIS Conference, 2019.\n[51] G. M. Sullivan and R. Feinn. Using effect size—or why the P value is not\nenough. Journal of graduate medical education, 4(3):279–282, 2012.\n[52] Tableau. Tableau embedding api. https://help.tableau.com/current/api/\nembedding api/en-us/index.html, 2003.\n[53] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli. Understanding the\ncapabilities, limitations, and societal impact of large language models.\narxiv.2102.02503, 2021.\n[54] Y . Tanahashi, N. Leaf, and K.-L. Ma. A study on designing effective\nintroductory materials for information visualization. Computer Graphics\nForum, 35(7):117–126, 2016.\n[55] J. J. Thomas and K. A. Cook. Illuminating the Path: An R&D Agenda for\nVisual Analytics, pages 69–104. IEEE Press, 2005.\n[56] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and\nH. Hajishirzi. Self-Instruct: Aligning language model with self generated\ninstructions. 2212.10560, 2022.\n[57] Y . Wang, Z. Sun, H. Zhang, W. Cui, K. Xu, X. Ma, and D. Zhang.\nDataShot: Automatic generation of fact sheets from tabular data. IEEE\nTransactions on Visualization and Computer Graphics, 26(1):895–905,\n2020.\n[58] A. Wu, D. Deng, F. Cheng, Y . Wu, S. Liu, and H. Qu. In defence of visual\nanalytics systems: Replies to critics. IEEE Transactions on Visualization\nand Computer Graphics, 29(1):1026–1036, 2023.\n[59] M. A. Yalcin. A systematic and minimalist approach to lower barriers\nin visual data exploration. PhD thesis, University of Maryland, College\nPark, 2016.\n[60] S. Yousefi, L. Betthauser, H. Hasanbeig, A. Saran, R. Milli `ere, and\nI. Momennejad. In-Context learning in large language models: A\nneuroscience-inspired analysis of representations. arxiv.2310.00313, 2023.\n[61] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su, X. Li,\nA. Su, T. Zhang, C. Zhou, K. Shou, M. Wang, W. Zhu, G. Lu, C. Ye,\nY . Ye, W. Ye, Y . Zhang, X. Deng, J. Xu, H. Wang, G. Chen, and J. Zhao.\nTableGPT: Towards unifying tables, nature language and commands into\none GPT. arxiv.2307.08674, 2023.\n[62] W. Zhang, Y . Shen, W. Lu, and Y . Zhuang. Data-Copilot: Bridging billions\nof data and humans with autonomous workflow. arxiv.2306.07209, 2023.\n[63] J. Zhao, S. Xu, S. Chandrasegaran, C. Bryan, F. Du, A. Mishra, X. Qian,\nY . Li, and K.-L. Ma. Chartstory: Automated partitioning, layout, and\ncaptioning of charts into comic-style narratives. IEEE Transactions on\nVisualization and Computer Graphics, 29(2):1384–1399, 2021.\nAPPENDIX\nThe following section outlines the detailed specifications for the\ntwo visual analytics systems we used. These specifications could\nbe examples for LLMs to generate tutorials for a new V A system.\n.1 VAST Challenge System\nWe provide the specification of the V AST challenge system we\nused in Sec.6.1.\n1 {\n2 \"systemInfo\": {\n3 \"systemName\": \"Social media event analysis system\",\n4 \"viewNumber\": 5\n5 },\n6 \"viewInfo\": {\n7 \"viewStyleInfo\": [\n8 {\n9 \"viewName\": \"Timeline\",\n10 \"layers\": [\n11 {\n12 \"markType\": \"area\",\n13 \"encoding\": {\n14 \"x\": {\"field\": \"datetime\", \"type\": \"temporal\n\"},\n15 \"y\": [\n16 {\"field\": \"message count\", \"type\": \"\nquantitative\", \"description\": \"\nmicroblog data\"},\n17 {\"field\": \"ccdata count\", \"type\": \"\nquantitative\", \"description\": \"call\ncenter data\"}\n18 ],\n19 \"color\": {\"field\": \"message type\", \"type\": \"\nnominal\"}\n20 }\n21 }\n22 ]\n23 },\n24 {\n25 \"viewName\": \"Risk Map\",\n26 \"layers\": [\n27 {\n28 \"markType\": \"hexagon\",\n29 \"encoding\": {\n30 \"x\": {\"field\": \"Latitude\", \"type\": \"\nquantitative\"},\n31 \"y\": {\"field\": \"Longitude\", \"type\": \"\nquantitative\"},\n32 \"lod\": {\"field\": \"Risk Level\", \"type\": \"\nquantitative\", \"description\": \"Risk\nLevel is positively correlated with the\nnumber of messages and negative\nemotions.\"\n33 \"color\": {\"field\": \"Risk Level\", \"type\": \"\nquantitative\"},\n34 }\n35 },\n36 \"tooltip\": [\n37 {\"field\": \"Risk Level\", \"type\": \"\nquantitative\"},\n38 {\"field\": \"Messages\", \"type\": \"text\"}\n39 ]\n40 },\n41 {\n42 \"markType\": \"square\",\n43 \"encoding\": {\n44 \"x\": {\"field\": \"Latitude\", \"type\": \"\nquantitative\"},\n45 \"y\": {\"field\": \"Longitude\", \"type\": \"\nquantitative\"},\n46 \"color\": {\"field\": \"Importance Level\", \"type\n\": \"quantitative\"}\n47 },\n48 \"tooltip\": [\n49 {\"field\": \"Risk Level\", \"type\": \"\nquantitative\"},\n50 {\"field\": \"Important Level\", \"type\": \"\nquantitative\"},\n51 {\"field\": \"Messages\", \"type\": \"text\"},\n52 ]\n53 },\n54 {\n55 \"markType\": \"point\",\n56 \"encoding\": {\n57 \"x\": [ {\"field\": \"Latitude\", \"type\": \"\nquantitative\"},{\"field\": \"Latitude\", \"\ntype\": \"quantitative\"}],\n58 \"y\": [ {\"field\": \"Longitude\", \"type\": \"\nquantitative\"},{\"field\": \"Longitude\", \"\ntype\": \"quantitative\"}],\n59 \"color\": {\"field\": \"message type\", \"type\": \"\nnominal\"}\n60 }\n61 },\n62 ]\n63 },\n64 {\n65 \"viewName\": \"Graph\",\n66 \"layers\": [\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18\n67 {\n68 \"markType\": \"circle\",\n69 \"encoding\": {\n70 \"nodes\": {\"field\": \"node ID\", \"type\": \"\nnominal\"},\n71 \"color\": {\"field\": \"group\", \"type\": \"nominal\n\"},\n72 \"size\": {\"field\": \"degree\", \"type\": \"\nquantitative\"}\n73 }\n74 },\n75 {\n76 \"markType\": \"line\",\n77 \"encoding\": {\n78 \"source\": {\"field\": \"node ID\", \"type\": \"\nnominal\"},\n79 \"target\": {\"field\": \"node ID\", \"type\": \"\nnominal\"},\n80 \"size\": {\"field\": \"co-occurrence\", \"type\": \"\nquantitative\"}\n81 }\n82 }\n83 ]\n84 },\n85 {\n86 \"viewName\": \"Message\",\n87 \"layers\": [\n88 {\n89 \"markType\": \"card\",\n90 \"encoding\": {\n91 \"title\": {\"field\": \"time\", \"type\": \"temporal\n\"},\n92 \"content\": {\"field\": \"message\", \"type\": \"\ntext\"}\n93 }\n94 }\n95 ]\n96 },\n97 {\n98 \"viewName\": \"Keyword\",\n99 \"layers\": [\n100 {\n101 \"markType\": \"text\",\n102 \"encoding\": {\n103 \"size\": {\"field\": \"frequency\", \"type\": \"\nquantitative\"},\n104 \"color\": {\"field\": \"frequency\", \"type\": \"\nquantitative\"}\n105 }\n106 }\n107 ]\n108 }\n109 ]\n110 },\n111 \"viewCoordinationInfo\": [\n112 {\n113 \"sourceViewName\": \"Timeline\",\n114 \"targetViewName\": [\"Risk Map\", \"Graph\", \"Messages\",\n\"Keyword\"],\n115 \"coordinationType\": \"filter\",\n116 \"interaction\": {\"type\": \"click\"}\n117 },\n118 {\n119 \"sourceViewName\": \"Keyword\",\n120 \"targetViewName\": [\"Timeline\"],\n121 \"coordinationType\": \"filter\",\n122 \"interaction\": {\n123 \"type\": \"click\",\n124 \"effect\": {\n125 \"action\": \"addCategory\",\n126 \"targetView\": \"Timeline\",\n127 \"category\": \"event\",\n128 \"changeby\": \"dropdown\"\n129 }\n130 }\n131 }\n132 ]\n133 }\n.2 Tableau Superstore System\nWe also provide the specification of the tableau’s system we used\nin Sec.6.2.\n1 {\n2 \"systemInfo\": {\n3 \"systemName\": \"Superstore system\",\n4 \"viewNumber\": 9\n5 },\n6 \"viewsInfo\": {\n7 \"viewStyleInfo\": [\n8 {\n9 \"viewName\": \"Sales | By Category\",\n10 \"layers\": [\n11 {\n12 \"markType\": \"Bar\",\n13 \"encoding\": {\n14 \"x\": [\n15 {\"field\": \"AGG(Metric Swap Calc (CY))\", \"\ntype\": \"quantitative\"},\n16 {\"field\": \"AGG(Metric Swap Calc (PY))\", \"\ntype\": \"quantitative\"}\n17 ],\n18 \"y\": {\"field\": \"Category\", \"type\": \"nominal\"\n}\n19 }\n20 }\n21 ]\n22 },\n23 {\n24 \"viewName\": \"Sales | By Manufacturer\",\n25 \"layers\": [\n26 {\n27 \"markType\": \"Bar\",\n28 \"encoding\": {\n29 \"x\": {\"field\": \"Manufacturer\", \"type\": \"\nnominal\"},\n30 \"y\": {\"field\": \"AGG(Metric Swap Calc (CY))\",\n\"type\": \"quantitative\"}\n31 }\n32 }\n33 ]\n34 },\n35 {\n36 \"viewName\": \"Sales | By Segment\",\n37 \"layers\": [\n38 {\n39 \"markType\": \"Bar\",\n40 \"encoding\": {\n41 \"x\": [\n42 {\"field\": \"AGG(Metric Swap Calc (CY))\", \"\ntype\": \"quantitative\"},\n43 {\"field\": \"AGG(Metric Swap Calc (PY))\", \"\ntype\": \"quantitative\"}\n44 ],\n45 \"y\": {\"field\": \"Segment\", \"type\": \"nominal\"}\n46 }\n47 }\n48 ]\n49 },\n50 {\n51 \"viewName\": \"Sales | By State\",\n52 \"layers\": [\n53 {\n54 \"markType\": \"circle\",\n55 \"encoding\": {\n56 \"x\": {\"field\": \"AGG(Metric Swap Calc (CY))\",\n\"type\": \"quantitative\"},\n57 \"y\": {\"field\": \"State/Province\", \"type\": \"\nnominal\"},\n58 \"size\": {\"field\": \"AGG(Metric Swap Calc (CY)\n)\", \"type\": \"quantitative\"}\n59 }\n60 }\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19\n61 ]\n62 },\n63 {\n64 \"viewName\": \"Sales | By Sub-Category\",\n65 \"layers\": [\n66 {\n67 \"markType\": \"Bar\",\n68 \"encoding\": {\n69 \"x\": {\"field\": \"Sub-Category\", \"type\": \"\nnominal\"},\n70 \"y\": [\n71 {\"field\": \"AGG(Metric Swap Calc (CY))\", \"\ntype\": \"quantitative\"},\n72 {\"field\": \"AGG(Metric Swap Calc (PY))\", \"\ntype\": \"quantitative\"}\n73 ]\n74 }\n75 }\n76 ]\n77 },\n78 {\n79 \"viewName\": \"Profit KPI (Line)\",\n80 \"layers\": [\n81 {\n82 \"markType\": \"Line\",\n83 \"encoding\": {\n84 \"x\": {\"field\": \"Profit\", \"type\": \"\nquantitative\"},\n85 \"y\": {\"field\": \"MONTH(Order Date)\", \"type\":\n\"temporal\"}\n86 }\n87 }\n88 ]\n89 },\n90 {\n91 \"viewName\": \"Sales KPI (Line)\",\n92 \"layers\": [\n93 {\n94 \"markType\": \"Line\",\n95 \"encoding\": {\n96 \"x\": {\"field\": \"Sales\", \"type\": \"\nquantitative\"},\n97 \"y\": {\"field\": \"MONTH(Order Date)\", \"type\":\n\"temporal\"}\n98 }\n99 }\n100 ]\n101 },\n102 {\n103 \"viewName\": \"Total Orders KPI (Area)\",\n104 \"layers\": [\n105 {\n106 \"markType\": \"Automatic\",\n107 \"encoding\": {\n108 \"x\": {\"field\": \"CNTD(Order ID)\", \"type\": \"\nquantitative\"},\n109 \"y\": {\"field\": \"MONTH(Order Date)\", \"type\":\n\"temporal\"}\n110 }\n111 }\n112 ]\n113 },\n114 {\n115 \"viewName\": \"Total Customers KPI (Area)\",\n116 \"layers\": [\n117 {\n118 \"markType\": \"Automatic\",\n119 \"encoding\": {\n120 \"x\": {\"field\": \"CNTD(Customer Name)\", \"type\"\n: \"quantitative\"},\n121 \"y\": {\"field\": \"MONTH(Order Date)\", \"type\":\n\"temporal\"}\n122 }\n123 }\n124 ]\n125 }\n126 ]\n127 },\n128 \"viewCoordinationInfo\": [\n129 {\n130 \"sourceViewName\": \"Sales | By State\",\n131 \"targetViewName\": [\n132 \"Sales | By Manufacturer\",\n133 \"Sales | By Segment\",\n134 \"Sales | By Sub-Category\",\n135 \"Sales | By Category\",\n136 \"Profit KPI (Line)\",\n137 \"Sales KPI (Line)\",\n138 \"Total Orders KPI (Area)\",\n139 \"Total Customers KPI (Area)\"\n140 ],\n141 \"coordinationType\": \"filter\",\n142 \"interaction\": {\"type\": \"click\"}\n143 },\n144 {\n145 \"sourceViewName\": \"Sales | By Segment\",\n146 \"targetViewName\": [\n147 \"Sales | By Manufacturer\",\n148 \"Sales | By Sub-Category\",\n149 \"Sales | By Category\",\n150 \"Sales | By State\",\n151 \"Profit KPI (Line)\",\n152 \"Sales KPI (Line)\",\n153 \"Total Orders KPI (Area)\",\n154 \"Total Customers KPI (Area)\"\n155 ],\n156 \"coordinationType\": \"filter\",\n157 \"interaction\": {\"type\": \"click\"}\n158 },\n159 {\n160 \"sourceViewName\": \"Sales | By Category\",\n161 \"targetViewName\": [\n162 \"Sales | By Manufacturer\",\n163 \"Sales | By Segment\",\n164 \"Sales | By Category\",\n165 \"Sales | By State\",\n166 \"Profit KPI (Line)\",\n167 \"Sales KPI (Line)\",\n168 \"Total Orders KPI (Area)\",\n169 \"Total Customers KPI (Area)\"\n170 ],\n171 \"coordinationType\": \"filter\",\n172 \"interaction\": {\"type\": \"click\"}\n173 },\n174 {\n175 \"sourceViewName\": \"Sales | By Sub-Category\",\n176 \"targetViewName\": [\n177 \"Sales | By Manufacturer\",\n178 \"Sales | By Segment\",\n179 \"Sales | By Category\",\n180 \"Sales | By State\",\n181 \"Profit KPI (Line)\",\n182 \"Sales KPI (Line)\",\n183 \"Total Orders KPI (Area)\",\n184 \"Total Customers KPI (Area)\"\n185 ],\n186 \"coordinationType\": \"filter\",\n187 \"interaction\": {\"type\": \"click\"}\n188 },\n189 {\n190 \"sourceViewName\": \"Sales | By Manufacturer\",\n191 \"targetViewName\": [\n192 \"Sales | By Segment\",\n193 \"Sales | By Sub-Category\",\n194 \"Sales | By Category\",\n195 \"Sales | By State\",\n196 \"Profit KPI (Line)\",\n197 \"Sales KPI (Line)\",\n198 \"Total Orders KPI (Area)\",\n199 \"Total Customers KPI (Area)\"\n200 ],\n201 \"coordinationType\": \"filter\",\n202 \"interaction\": {\"type\": \"click\"}\n203 }\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20\n204 ]\n205 }\nWithin this appendix, we showcase several exemplary prompts\nencompassing various functionalities of our framework. These\nprompts elucidate how the framework can be utilized across\ndifferent stages of the visual analytics process.\n.3 Onbaording\nThe following prompt aims at generating an onboarding tutorial\nbased on the specifications of a given system:\nPrompt example for onboarding:\nHere are the specifications of a visual analytics system.\n(Add a specification)\nThe specification includes the system-level, view-level and views’\ncoordination information. You need to introduce each view\nwith style (data meaning, visual mapping) and the relationship\nbetween views. Please give your answer in the following JSON\nformat:\n1 {\"viewName\": \"\",\n2 \"content\":\n3 - <b>Style</b>: \"\"<br>\n4 - <b>Coordination<b>: \"\"<br>\n5 }\nAn output example of a tutorial:\n1 [\n2 {\n3 \"viewName\": \"Sales | By Category\",\n4 \"content\": {\n5 \"viewName\": \"Sales | By Category\",\n6 \"content\":\n7 \"<b>Style</b>: Displays sales data by category,\nusing bar marks. The X-axis represents both\nthe current year and previous year metrics,\nwhile the Y-axis represents the categories.<\nbr>\",\n8 \"<b>Coordination</b>: When a bar (representing a\ncategory) is clicked, it filters data in the\nviews: ’Sales | By Manufacturer’, ’Sales | By\nSegment’, ’Sales | By Sub-Category’, ’Sales\n| By State’, ’Profit KPI (Line)’, ’Sales KPI\n(Line)’, ’Total Orders KPI (Area)’, ’Total\nCustomers KPI (Area)’.<br>\"\n9 },\n10 }, ...\n11 ]\n.4 Recommending Insights\nIn this section, we outline the prompt examples that serve as a\nflexible way to instruct insight recommendation tasks.\nThe first template focuses on recommending insights types\nbased on the user’s selections and choosing functions to calculate\ninsights. Here is an example prompt:\nPrompt example for insight calculation:\nWhen the user makes an action on a view, the system changes.\n1 Current selections:\n2 [{\"dimName\": \"Segment\", \"value\": \"Consumer\",\"viewName\":\n\"Sales | By Segment\"},\n3 {\"dimName\": \"State/Province\", \"value\": \"California\",\"\nviewName\": \"Sales | By State\"},\n4 {\"dimName\": \"State/Province\", \"value\": \"New York\",\"\nviewName\": \"Sales | By State\"}]\n5 }\n1 viewStyleInfo\n2 {\n3 \"viewName\": \"Sales | By Category\",\n4 \"markType\": \"Bar\",\n5 \"encoding\": {\n6 \"x\": [\n7 \"Metric Swap Calc (% Chg)(Label)\",\n8 \"AGG(Metric Swap Calc (CY))\",\n9 \"AGG(Metric Swap Calc (PY))\"\n10 ],\n11 \"y\": [\n12 \"Category\"\n13 ]\n14 }, ...\n15 Coordination information\n16 {\n17 \"sourceViewName\": \"Sales | By Segment\",\n18 \"targetViewName\": [\n19 \"Sales | By Manufacturer\",\n20 \"Sales | By Sub-Category\",\n21 \"Sales | By Category\",\n22 \"Sales | By State\",\n23 \"Profit KPI (Line)\",\n24 \"Sales KPI (Line)\",\n25 \"Total Orders KPI (Area)\",\n26 \"Total Customers KPI (Area)\"\n27 ],\n28 \"coordinationType\": \"filter\",\n29 \"interaction\": {\n30 \"type\": \"click\"\n31 }\n32 },\nAccording to the data info in each view and the analytical\ntask, you should select all suitable analytical functions related to\nthe user’s task. If a view contains more than one measure, all\nmeasures need to be considered to find appropriate functions.\nCross-view insights, such as correlation, are allowed. You also\nneed to give a relevance score to assess how closely related the\ninsight type is to the task.\n1 {\"user task\": \"Analyze the sales of the superstore from\ndifferent perspectives.\"}\n2 \"functions\": [\n3 {\n4 \"name\": \"get_change_point\",\n5 \"description\": \"Get the change point in a time\nseries dataset\"\n6 },{\n7 \"name\": \"get_seasonality\",\n8 \"description\": \"Get the seasonality in a time\nseries dataset\"\n9 },{\n10 \"name\": \"get_trend\",\n11 \"description\": \"Get the trend in a time series\ndataset\"\n12 },{\n13 \"name\": \"get_outlier\",\n14 \"description\": \"Get outliers in a dataset\",\n15 },{\n16 \"name\": \"get_correlation\",\n17 \"description\": \"Get the correlation of two time\nseries datasets\"\n18 },{\n19 \"name\": \"get_outstanding_top1\",\n20 \"description\": \"Get the first n most values that\nare significantly larger than other values in\nthe data column\",\n21 },{\n22 \"name\": \"get_outstanding_last\",\n23 \"description\": \"Get the last item in a data column\nthat is significantly smaller than the other\nvalues\"\n24 }\n25 ]\nFor all possible insight types, you need to output a JSON list\ncontaining view name, function name, variables name, and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 21\nrelevanceScore. The viewName and variableName can be listed\nif they have two variables on two views, such as calculating\ncorrelation. Your output should be like this:\n1 [{\"viewName\":\"\", \"functionName\":\"\", \"variableName\":\"\",\n\"relevanceScore\":}].\nAfter the functions return the results of insights with the\nsignificance, the next step is to evaluate the impact of these insights.\nThis next template assists in converting the raw data results into\nmore interpretable sentences and quantifies the potential impact of\nthe insights on the user’s tasks. Here’s an example:\nPrompt example for insight assessment:\nThe selected insights are implemented and return the result,\nincluding the value and significance score. The data is sorted\nby significance score. The dimName is the x-axis, value is the\nanswer’s x-axis value, variableName is the answer’s column\nname (generally y-axis), result is the answer, and parameters\nused to describe the answer (e.g., ’Same’ means two series have\nthe same direction in correlation calculation.\nI need you to translate the answer into a sentence. The answer\nshould contain more info to explain in detail. You also need to\ngive an impact score. The impact score quantifies the potential\nbusiness impact of the insight. Please give your answer in the\nfollowing format:\n1 [{\"impactScore\":, \"answer\":\"\"}]\nLastly, the following template is designed for situations where\nthere are multiple rounds of insight generation. Its purpose is to\ncompare and identify the similarities or differences between the\ndatasets of insights obtained in different rounds or conditions.\nHere’s an example:\nPrompt example for comparing insights:\nFrom two rounds, identify the similarity or difference of the same\ntype of insights in two insight datasets. If we do not select a\ndata element, then the insights are aggregated values for all data,\nbut if we select one or two elements, the insights are aggregated\nfor those two elements. Please use a sentence to describe the\nsimilarity or difference. And do not use round. You should use\nthe data name to represent the round. Your output format should\nbe like this:\n1 [{\"answer\":\"\", \"viewName\":\"\", \"dimName\":\"\", \"value\n\":\"\"}]\nAn output example of insights:\n1 [{’question’: ’’, ’viewName’: ’Sales | By State’, ’\ndimName’: ’State/Province’, ’value’: [’California’,\n’New York’], ’functionName’: ’\nget_outstanding_top2’, ’variableName’: ’AGG(Metric\nSwap Calc (CY))’, ’result’: [57682.45549999996,\n36906.00399999999], ’significance_score’: 1.0, ’\nparameters’: None, ’relevance_score’: 0.9, ’\nimpact_score’: 0.9, ’answer’: ’The states with the\nhighest sales in the current year are California\nand New York with sales amounting to 57682.46 and\n36906.00 respectively.’, ’final_score’: 0.5},\n2 {’question’: ’’, ’viewName’: ’Sales KPI (Line)’, ’\ndimName’: ’MONTH(Order Date)’, ’value’:\n’2022-03-01 00:00:00’, ’functionName’: ’\nget_change_point’, ’variableName’: ’SUM(Sales)’, ’\nresult’: 526.9643807018581, ’significance_score’:\n0.9999999999285639, ’parameters’: None, ’\nrelevance_score’: 0.9, ’impact_score’: 0.9, ’\nanswer’: ’There was a significant change in sales\nin March 2022, with the sales amounting to\n526.96.’, ’final_score’: 0.4999999999996},\n3 {’question’: ’’, ’viewName’: ’Sales KPI (Line)’, ’\ndimName’: None, ’value’: None, ’functionName’: ’\nget_trend’, ’variableName’: ’SUM(Sales)’, ’result’:\n’increase’, ’significance_score’:\n0.9999999794746961, ’parameters’: None, ’\nrelevance_score’: 0.9, ’impact_score’: 0.9, ’\nanswer’: ’The sales has been increasing.’, ’\nfinal_score’: 0.4999999989737348},\n4 {’question’: ’’, ’viewName’: ’Sales | By Manufacturer’,\n’dimName’: ’Manufacturer’, ’value’: [’Other’, ’\nCanon’], ’functionName’: ’get_outstanding_top2’, ’\nvariableName’: ’AGG(Metric Swap Calc (CY))’, ’\nresult’: [36727.54599999997, 27459.864], ’\nsignificance_score’: 0.9920215895038219, ’\nparameters’: None, ’relevance_score’: 0.9, ’\nimpact_score’: 0.9, ’answer’: ’The manufacturers\nwith the highest sales in the current year are\nOther and Canon with sales amounting to 36727.55\nand 27459.86 respectively.’, ’final_score’:\n0.4996010794751911},\n5 ...]\n.5 Generate Report\nIn this section, we outline the prompt examples to generate reports.\nThe first template is designed to help users create insightful\nreports based on historical analysis data of the system. Here, for\nthe first step, the user’s interaction, we set the type “click” as type\nto let LLMs know this piece of data, while for the second step, we\nset the type using the function description for better clarification\nof this step’s insight. The addition of type makes the LLMs more\naware of the source of insight.\nPrompt template for report summarization:\nHere is a historical analysis of the system data. The data includes\nuser interactions and insights at each step of the analysis:\n1 [{\"step\": \"step 1\", \"insights\": {\"type\": \"click\", \"\nvalue\": [\"19:35\", \"19:43\"], \"viewName\": \"Timeline\n\"}, \"image_name\": \"Timeline_1_0.png\"},\n2 {\"step\": \"step 2\", \"insights\": {\"type\": \"The events in\ntop 2 risk-level regions during a given time\nperiod\", \"value\": [{\"title\": \"Gelatogalore\nShooting Incident\", \"label\": \"Shooting incident\ninvolving a black van and police at Gelatogalore\nleading to a cop’s death.\"}, {\"title\": \"Biker\nHenri Incident\", \"label\": \"Biker Henri was hit and\nconsoled with coffee at a local shop.\"}], \"\nviewName\": \"Risk Map\"}, \"imageName\": \"RiskMap_1_1.\npng\"},...]\nYour task is to write an insight report to present these findings.\nThe amount of insight should be equal to the number of steps\nfor given data. Ensure you include both a cover(report title) and\na conclusion.\nThe following template aids users in converting their summa-\nrized reports into LaTeX-formatted slides.\nPrompt template for LaTeX code generation:\nTransform the summarized report into LaTeX slides. For each\nslide, if an insight exhibits a clear hierarchy, segment it using\nbullet points. Accompany each insight with a screenshot from\nthe system. The image name for these screenshots can be found\nin the historical analysis data. Please integrate the following\ncommands for style configuration.\n1 \\documentclass{beamer}\n2 \\usetheme{Madrid}\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 22\nTABLE 2\nThe table presents commonly used insight types in visual analytics.\nInsight type Description\nOutstanding No.1 The leading value is significantly higher than all the remaining values.\nOutstanding Top 2 The leading two values are significantly higher than all the remaining values.\nOutstanding Last The value is remarkably lower than all the remaining values.\nAttribution The leading value dominates (accounting for ≥ 50%) the group.\nChange Point A specific point in time where there is a significant change or shift in the underlying data-generating process.\nOutlier An observation or data point that significantly deviates from the rest of the data.\nSeasonality A regular and predictable pattern of fluctuations or variations that occur at specific intervals of time.\nTrend A time series has an obvious trend (increase or decrease) with a certain turbulence level (steadily/ with turbulence).\nCorrelation The statistical relationships between random variables, multidimensional data or time series.\nDifference The similarity or difference between two or more datasets\nAggregation The descriptive statistical indicators (e.g., average, sum, count, etc.) based on the data attributes.\nValue The the exact value of data attribute(s) under specific criteria.\nText summary The core ideas of a text dataset. The summary might have spatial or temporal features.\nImportant nodes or links The important nodes or links in a graph under specific criteria.\nImportant text or keywords The important original texts or keywords under specific criteria.\n3 \\usecolortheme{seahorse}\n4 \\includegraphics[height=0.6\\textheight,keepaspectratio\n]{../image/image_name.png}.\nThe image name is provided in the historical analysis data.\nAn output example of a report:\n1 \\documentclass{beamer}\n2 \\usetheme{Madrid}\n3 \\usecolortheme{seahorse}\n4\n5 \\begin{document}\n6 \\title{Risk Event Analysis for Abila City}\n7 \\date{January 23, 2014}\n8 \\frame{\\titlepage}\n9\n10 \\begin{frame}{Specific Time Period of Interest}\n11 \\begin{itemize}\n12 \\item During the evening, a specific time period\nbetween 19:34 to 19:43 surfaced as particularly\nsignificant.\n13 \\end{itemize}\n14 \\centering\n15 \\includegraphics[width=0.8\\textwidth]{../image/\nTimeline_1_0.png}\n16 \\end{frame}\n17\n18 \\begin{frame}{Major Risk Events}\n19 \\begin{itemize}\n20 \\item Shooting incident at Gelatogalore\n21 \\begin{itemize}\n22 \\item Confrontation between a black van and the\npolice.\n23 \\item Resulted in a police officer’s death.\n24 \\end{itemize}\n25 \\item Vehicular incident involving Biker Henri\n26 \\begin{itemize}\n27 \\item Biker Henri was hit.\n28 \\item Later consoled with a coffee at a local\nshop.\n29 \\end{itemize}\n30 \\end{itemize}\n31 \\centering\n32 \\includegraphics[width=0.6\\textwidth]{../image/\nRiskMap_1_1.png}\n33 \\end{frame}\n34\n35 \\begin{frame}{Messages Analysis}\n36 \\begin{itemize}\n37 \\item Real-time reactions from the public on the\nincidents.\n38 \\item Considerable sympathy and concern for Biker\nHenri.\n39 \\item Chaos and fear during the Gelatogalore\nShooting Incident.\n40 \\end{itemize}\n41 \\centering\n42 \\includegraphics[width=0.3\\textwidth]{../image/\nMessage_1_2.png}\n43 \\end{frame}\n44\n45 \\begin{frame}{Graph-based Representation}\n46 \\begin{itemize}\n47 \\item Nodes associated with two primary incidents.\n48 \\item Hashtags and entities linked to each event.\n49 \\end{itemize}\n50 \\centering\n51 \\includegraphics[width=0.8\\textwidth]{../image/\nGraph_1_3.png}\n52 \\end{frame}\n53\n54 \\begin{frame}{Keyword Analysis}\n55 \\begin{itemize}\n56 \\item Keywords associated with Gelatogalore\nShooting Incident:\n57 \\begin{itemize}\n58 \\item ’shot’, ’police’, ’gelato’, ’gelatogalore’,\n’van’, ’cop’\n59 \\end{itemize}\n60 \\item Keywords linked with Biker Henri Incident:\n61 \\begin{itemize}\n62 \\item ’hit’, ’run’, ’coffee’, ’park’, ’driver’\n63 \\end{itemize}\n64 \\end{itemize}\n65 \\centering\n66 \\includegraphics[width=0.5\\textwidth]{../image/\nKeyword_1_4.png}\n67 \\end{frame}\n68\n69 \\begin{frame}{Significance of ’van’}\n70 \\begin{itemize}\n71 \\item The keyword ’van’ was isolated as significant.\n72 \\end{itemize}\n73 \\centering\n74 \\includegraphics[width=0.5\\textwidth]{../image/\nKeyword_1_5.png}\n75 \\end{frame}\n76\n77 \\begin{frame}{Timeline Analysis of ’van’}\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23\n78 \\begin{itemize}\n79 \\item Sequence of events linked to the keyword ’van\n’.\n80 \\item Highlights the pivotal role of a black van in\nthe evening’s events.\n81 \\end{itemize}\n82 \\centering\n83 \\includegraphics[width=0.8\\textwidth]{../image/\nTimeline_1_6.png}\n84 \\end{frame}\n85\n86 \\begin{frame}{Conclusion}\n87 \\begin{itemize}\n88 \\item The evening was fraught with tension,\ncharacterized by two major risk events.\n89 \\item Data-driven insights derived from analyses\nhighlight the sequence, nature, and public\nsentiment of the events.\n90 \\item Provides a foundation for future\ninvestigations and response strategies.\n91 \\end{itemize}\n92 \\end{frame}\n93\n94 \\end{document}\nIn this section, we test the ability of gpt-3.5-turbo to calculate\nanswers to the common insight tasks. The insight types are in\nTable 2. This test mainly considers the following two factors. First,\nwhether GPT can calculate the accurate answer of a given task\n(accuracy). Second, the number of data rows LLMs can handle.\nDue to the first factor, we tested 50 times, each using different\ndata for an insight task. The data used each time is randomly\ngenerated within a certain range according to the same format\n(same number of data rows). For the second factor, in an insights\ntask, tests are performed on different numbers of data rows (each 50\ntimes). For example, for a single point insight finding the maximum\nvalue, the data format for each test is an array, and the length of\nthe array is a variable parameter. For each fixed array length, test\n50 times using randomly generated data. Besides, to facilitate the\nautomation of the program to extract the answers to the GPT’s\nresponses, we let GPT output in a required format like “Use {} to\ninclude the result”. The results of our tests are in Fig. 8.\nFig. 8. A test to assess the data analysis ability of LLMs. The y-axis\nrepresents the number of correct trials out of 50 trials. The x-axis shows\nthe varying data rows, which include [20, 50, 80, 100, 120, 150, 180,\n200]. The accuracy of most types of insight analysis tends to decrease\nas the rows of data increase.\nBased on the test results, the accuracy of most types of insight\nanalysis tends to decrease as the volume of data increases. Besides,\nthere are notable differences in the performance of GPT across\nvarious analytical tasks.\nIn certain singular shape insight analysis tasks, like identifying\nthe “outstanding last” or “outstanding No. 1,” GPT exhibits more\nfavorable performance, and the decline in accuracy is less apparent\nwith increasing data volume. It is worth highlighting that with\na large volume of data, the accuracy of the top 2 is notably\nlower than that of the No.1. In the false results produced by\nGPT, it is not uncommon for one of the top 2 predictions to be\naccurate while the other is incorrect. For example, in the 47th test\n(numbero f datarows= 200) of the “outstanding top 2 values” task,\nGPT output, “The outstanding top 2 values are 998, 993.” The\ncorrect top value is indeed 998, while the second largest number is\n997, which differs from GPT’s answer of 993.\nNonetheless, in time series-related tasks, such as “trend”\nand “seasonality” analysis, GPT performs inadequately. When\ndetermining the trend of a time series, GPT often makes mistakes\nwhen assessing data that lacks a distinct increasing or decreasing\ntrend and instead exhibits fluctuations. For example, in the 20th test\n(numbero f datarows= 200) of the “trend” task, GPT output, “The\ntrend of the data in the time series over time is decreased. {-1}.”\nHowever, there is no ascending or descending trend in this data. In\nFig. 8, it can be observed that their accuracy rate is consistently\naround 50%, implying that the performance is nearly equivalent to\nrandom selection.\nAn example of a prompt during testing is as follows:\nPrompt example for finding outstanding No.1 value:\nI have a table that shows the values for each individual, each\nbelonging to one category, for a total of three categories.\n1 Data:\n2 category,individual_index,value\n3 \"category2\",0,402\n4 \"category1\",1,525\n5 \"category1\",2,188\n6 \"category1\",3,570\n7 \"category3\",4,781\n8 \"category2\",5,421\n9 \"category2\",6,698\n10 \"category1\",7,188\n11 \"category3\",8,83\n12 \"category3\",9,739\nMy question is: What is the outstanding No.1 value for an\nindividual? Use {} to include the result. For example, if the No.1\nvalue is 970, please output {970} at the end.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8513720631599426
    },
    {
      "name": "Visual analytics",
      "score": 0.8281739354133606
    },
    {
      "name": "Interactive visual analysis",
      "score": 0.7097532749176025
    },
    {
      "name": "Automatic summarization",
      "score": 0.6313658952713013
    },
    {
      "name": "Visualization",
      "score": 0.5978924632072449
    },
    {
      "name": "Data science",
      "score": 0.5761791467666626
    },
    {
      "name": "Analytics",
      "score": 0.5750884413719177
    },
    {
      "name": "Data visualization",
      "score": 0.5737448930740356
    },
    {
      "name": "Workflow",
      "score": 0.5394436120986938
    },
    {
      "name": "Onboarding",
      "score": 0.5308103561401367
    },
    {
      "name": "Visual language",
      "score": 0.5263332724571228
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5191985368728638
    },
    {
      "name": "Cultural analytics",
      "score": 0.5100424885749817
    },
    {
      "name": "Semantic analytics",
      "score": 0.25904780626296997
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2549050748348236
    },
    {
      "name": "World Wide Web",
      "score": 0.2534483075141907
    },
    {
      "name": "Database",
      "score": 0.1188710629940033
    },
    {
      "name": "The Internet",
      "score": 0.08843812346458435
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Web modeling",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    }
  ],
  "cited_by": 27
}