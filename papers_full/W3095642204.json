{
  "title": "Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models",
  "url": "https://openalex.org/W3095642204",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2104885275",
      "name": "Xi Yang",
      "affiliations": [
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A1972599956",
      "name": "Xing He",
      "affiliations": [
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2322467324",
      "name": "Hansi Zhang",
      "affiliations": [
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2962731227",
      "name": "Yinghan Ma",
      "affiliations": [
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A1985857611",
      "name": "Jiang Bian",
      "affiliations": [
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2101355887",
      "name": "Yonghui Wu",
      "affiliations": [
        "University of Florida Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W256669079",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2972297345",
    "https://openalex.org/W2800055384",
    "https://openalex.org/W2171313960",
    "https://openalex.org/W2151870335",
    "https://openalex.org/W2180017908",
    "https://openalex.org/W2251427843",
    "https://openalex.org/W115293911",
    "https://openalex.org/W2270234620",
    "https://openalex.org/W13343750",
    "https://openalex.org/W2125674401",
    "https://openalex.org/W2770160079",
    "https://openalex.org/W2542948012",
    "https://openalex.org/W1541954861",
    "https://openalex.org/W2616920365",
    "https://openalex.org/W2735784619",
    "https://openalex.org/W3023154087",
    "https://openalex.org/W3020931369",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W2888285200",
    "https://openalex.org/W3094834348",
    "https://openalex.org/W2498672755",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W3009459039",
    "https://openalex.org/W4247236552",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W3103145119",
    "https://openalex.org/W1503398984",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3101613385"
  ],
  "abstract": "Background Semantic textual similarity (STS) is one of the fundamental tasks in natural language processing (NLP). Many shared tasks and corpora for STS have been organized and curated in the general English domain; however, such resources are limited in the biomedical domain. In 2019, the National NLP Clinical Challenges (n2c2) challenge developed a comprehensive clinical STS dataset and organized a community effort to solicit state-of-the-art solutions for clinical STS. Objective This study presents our transformer-based clinical STS models developed during this challenge as well as new models we explored after the challenge. This project is part of the 2019 n2c2/Open Health NLP shared task on clinical STS. Methods In this study, we explored 3 transformer-based models for clinical STS: Bidirectional Encoder Representations from Transformers (BERT), XLNet, and Robustly optimized BERT approach (RoBERTa). We examined transformer models pretrained using both general English text and clinical text. We also explored using a general English STS dataset as a supplementary corpus in addition to the clinical training set developed in this challenge. Furthermore, we investigated various ensemble methods to combine different transformer models. Results Our best submission based on the XLNet model achieved the third-best performance (Pearson correlation of 0.8864) in this challenge. After the challenge, we further explored other transformer models and improved the performance to 0.9065 using a RoBERTa model, which outperformed the best-performing system developed in this challenge (Pearson correlation of 0.9010). Conclusions This study demonstrated the efficiency of utilizing transformer-based models to measure semantic similarity for clinical text. Our models can be applied to clinical applications such as clinical text deduplication and summarization.",
  "full_text": "Original Paper\nMeasurement of Semantic Textual Similarity in Clinical Texts:\nComparison of Transformer-Based Models\nXi Yang, PhD; Xing He, MSc; Hansi Zhang, MSc; Yinghan Ma, BSc; Jiang Bian, PhD; Yonghui Wu, PhD\nDepartment of Health Outcomes and Biomedical Informatics, University of Florida, Gainesville, FL, United States\nCorresponding Author:\nYonghui Wu, PhD\nDepartment of Health Outcomes and Biomedical Informatics\nUniversity of Florida\n2004 Mowry Road\nGainesville, FL, 32610\nUnited States\nPhone: 1 352 294 8436\nEmail: yonghui.wu@ufl.edu\nAbstract\nBackground: Semantic textual similarity (STS) is one of the fundamental tasks in natural language processing (NLP). Many\nshared tasks and corpora for STS have been organized and curated in the general English domain; however, such resources are\nlimited in the biomedical domain. In 2019, the National NLP Clinical Challenges (n2c2) challenge developed a comprehensive\nclinical STS dataset and organized a community effort to solicit state-of-the-art solutions for clinical STS.\nObjective: This study presents our transformer-based clinical STS models developed during this challenge as well as new\nmodels we explored after the challenge. This project is part of the 2019 n2c2/Open Health NLP shared task on clinical STS.\nMethods: In this study, we explored 3 transformer-based models for clinical STS: Bidirectional Encoder Representations from\nTransformers (BERT), XLNet, and Robustly optimized BERT approach (RoBERTa). We examined transformer models pretrained\nusing both general English text and clinical text. We also explored using a general English STS dataset as a supplementary corpus\nin addition to the clinical training set developed in this challenge. Furthermore, we investigated various ensemble methods to\ncombine different transformer models.\nResults: Our best submission based on the XLNet model achieved the third-best performance (Pearson correlation of 0.8864)\nin this challenge. After the challenge, we further explored other transformer models and improved the performance to 0.9065\nusing a RoBERTa model, which outperformed the best-performing system developed in this challenge (Pearson correlation of\n0.9010).\nConclusions: This study demonstrated the efficiency of utilizing transformer-based models to measure semantic similarity for\nclinical text. Our models can be applied to clinical applications such as clinical text deduplication and summarization.\n(JMIR Med Inform 2020;8(11):e19735) doi: 10.2196/19735\nKEYWORDS\nclinical semantic textual similarity; deep learning; natural language processing; transformers\nIntroduction\nSemantic textual similarity (STS) is a natural language\nprocessing (NLP) task to quantitatively assess the semantic\nsimilarity between two text snippets. STS is usually approached\nas a regression task where a real-value score is used to quantify\nthe similarity between two text snippets. STS is a fundamental\nNLP task for many text-related applications, including text\ndeduplication, paraphrasing detection, semantic searching, and\nquestion answering. In the general English domain, semantic\nevaluation (SemEval) STS shared tasks have been organized\nannually from 2012 to 2017 [1-6], and STS benchmark datasets\nwere developed for evaluation [6]. Previous work on STS often\nused machine learning models [7-9] such as support vector\nmachine [10], random forest [11], convolutional neural networks\n[12], and recurrent neural networks [13] and topic modeling\ntechniques [8] such as latent semantic analysis [14] and latent\nDirichlet allocation [15]. Recently, deep learning models based\non transformer architectures such as Bidirectional Encoder\nRepresentations from Transformers (BERT) [16], XLNet [17],\nand Robustly optimized BERT approach (RoBERTa) [18] have\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 1http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\ndemonstrated state-of-the-art performances on the STS\nbenchmark dataset [19] and remarkably outperformed the\nprevious models. More recently, the Text-to-Text Transfer\nTransformer model [20] and the StructBERT model [21] have\nfurther improved the performance on the STS benchmark. These\nstudies demonstrated the efficiency of transformer-based models\nfor STS tasks.\nRapid adoption of electronic health record (EHR) systems has\nmade longitudinal health information of patients available\nelectronically [22,23]. EHRs consist of structured, coded data\nand clinical narratives. The structured EHR data are typically\nstored as predefined medical codes (eg, International\nClassification of Diseases, 9th/10th Revision, codes for\ndiagnoses) in relational databases. Various common data models\nwere used to standardize EHR data to facilitate downstream\nresearch and clinical studies [24]. However, clinical narratives\nare often documented in a free-text format, which contains many\ntypes of detailed patient information, such as family history,\nadverse drug events, and medical imaging result interpretations,\nthat are not well captured in the structured medical codes [25].\nAs free text, the clinical notes may contain a considerable\namount of duplication, error, and incompleteness for various\nreasons (eg, copy-and-paste or using templates and inconsistent\nmodifications) [26,27]. STS can be applied to assess the quality\nof the clinical notes and reduce redundancy to support\ndownstream NLP tasks [28]. However, up until now, only a few\nstudies [29-31] have explored STS in the clinical domain due\nto the limited data resources for developing and benchmarking\nclinical STS tasks. Recently, a team at the Mayo Clinic\ndeveloped a clinical STS dataset, MedSTS [32], which consists\nof more than 1000 annotated sentence pairs extracted from\nclinical notes. Based on the MedSTS dataset, the 2018\nBioCreative/Open Health NLP (OHNLP) challenge [33] was\norganized as the first shared task examining advanced NLP\nmethods for STS in the clinical domain. In this challenge, two\ndifferent teams explored various machine learning approaches,\nincluding several deep learning models [30,31]. Later, more\nteams competed in the 2019 National NLP Clinical Challenges\n(n2c2)/OHNLP STS challenge with a larger clinical STS dataset\n[34]. During this challenge, many new emerging NLP\ntechniques, such as transformer-based models, were explored.\nThis study presents our machine learning models developed for\nthe 2019 n2c2/OHNLP STS challenge. We explored\nstate-of-the-art transformer-based models (BERT, XLNet, and\nRoBERTa) for clinical STS. We systematically examined\ntransformer models pretrained using general English corpora\nand compared them with clinical transformer models pretrained\nusing clinical corpora. We also proposed a representation fusion\nmethod to ensemble the transformer-based models. In this\nchallenge, our clinical STS system based on the XLNet model\nachieved a Pearson correlation score of 0.8864, ranked as the\nthird-best performance among all participants. After the\nchallenge, we further explored a new transformer-based model,\nRoBERTa, which improved the performance to 0.9065 and\noutperformed the best performance (0.9010) reported in this\nchallenge. This study demonstrated the efficiency of\ntransformer-based models for STS in the clinical domain.\nMethods\nDataset\nThe 2019 n2c2 organizers developed a corpus of 2054 sentence\npairs derived from over 300 million deidentified clinical notes\nfrom the Mayo Clinic’s EHR data warehouse. The sentence\npairs were divided into a training set of 1642 sentence pairs for\nmodel development and a test set of 412 sentence pairs for\nevaluation. Similar to the annotation scheme in the general\nEnglish domain, the challenge corpus was annotated by\nassigning a similarity score for each sentence pair as a number\non a scale from 0.0 to 5.0, where 0.0 indicates that the semantics\nof the two sentences are entirely independent (ie, no overlap in\ntheir meanings), and 5.0 signifies that two sentences are\nsemantically equivalent. Annotators used arbitrary similarity\nscores between 0.0 and 5.0, such as 2.5 or 3.5, to reflect different\nlevels of equality. Table 1 presents the descriptive statistics of\nthe datasets. The distribution of similarity scores is quite\ndifferent between the training and test datasets. In the training\nset, the range with the most cases (509/1642, 31.0%) was (3.0,\n4.0], whereas in the test set, most scores (238/412, 57.8%) were\ndistributed in the range (0.0, 1.0]. In this study, we denoted this\nchallenge dataset as STS-Clinic. In addition to the STS-Clinic,\nwe also used a general English domain STS benchmark dataset\nfrom the SemEval 2017 [6] as an external source. We merged\nthe original training and development datasets to create a unique\ndataset of 7249 annotated sentence pairs. We denoted this\ncombined general English domain dataset as STS-General and\nused it as a complementary training set for model development\nin this study. Compared to the STS-Clinic, the similarity scores\nin STS-General were more evenly distributed in different ranges\n(Table 1).\nTable 1. Descriptive statistics of the datasets.\nAnnotation distribution, n (%)Sentence pairs, nDataset\n(4.0, 5.0](3.0, 4.0](2.0, 3.0](1.0, 2.0][0.0, 1.0]\n273 (16.6)509 (31.0)394 (24.0)154 (9.4)312 (19.0)1642STS-Clinica Training\n34 (8.3)62 (15.0)32 (7.8)46 (11.2)238 (57.8)412STS-Clinic Test\n1962 (27.1)1260 (17.4)1413 (19.5)1122 (15.5)1492 (20.6)7249STS-General Training\naSTS: semantic textual similarity.\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 2http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nPreprocessing of Sentence Pairs\nWe developed a preprocessing pipeline to normalize each\nsentence pair, including (1) converting all words to lower case;\n(2) inserting white spaces to separate words from punctuation\n(eg, “[ab/cd]” → “[ ab / cd ]”; “abc,def” → “abc , def”); and\n(3) replacing two or more spaces or tabs (“\\t”) with a single\nspace. We did not remove any stop-words from the sentences\nand kept the original formats of the numbers without any\nconversion. Since different transformer models adopted different\ntokenization strategies (eg, WordPiece for BERT, byte pair\nencoding for RoBERTa, and SentencePiece for XLNet), our\npreprocessing automatically picked the appropriate tokenizer\naccording to the transformer model in use.\nTransformer Model-Based STS System\nIn this study, we investigated three transformer models (BERT,\nXLNet, and RoBERTa) for clinical STS. BERT is a bidirectional\ntransformer-based encoder model pretrained with a combination\nof masked language modeling (MLM) and next sentence\nprediction. RoBERTa has the same architecture as BERT but\npretrained with a robust optimizing strategy. The RoBERTa\npretraining procedure used dynamic MLM but removed the\nnext sentence prediction task. XLNet is a transformer-based\nmodel pretrained with the bidirectional autoregressive language\nmodeling method. Unlike the MLM used by BERT and\nRoBERTa, the autoregressive language model uses data\npermutation instead of data corruption and reconstruction. All\nthree transformer models provided two different settings: a\n“BASE” setting and a “LARGE” setting. The main difference\nbetween the BASE model and the LARGE model is the number\nof layers. For example, the BERT-base model features 12 layers\nof transformer encoder layers, 768 hidden units in each layer,\nand 12 attention heads, while the BERT-large consists of 24\ntransformer blocks with a hidden size of 1024 and 16 attention\nheads. The total number of parameters for the BERT-large\nmodel is approximately 340 million, which is about 3 times\nmore than the BERT-base model. In this study, we explored\ngeneral transformers (pretrained using general English corpora)\nusing both the BASE model and the LARGE model. We also\nexamined clinical transformers pretrained using clinical notes\nfrom the MIMIC-III database. For clinical transformers, we\nadopted the BASE settings as we did not observe additional\nbenefits from using the LARGE setting.\nAs shown in Figure 1, our STS system has two modules: (1) a\ntransformer model–based feature learning module and (2) a\nregression-based similarity score learning module. In the feature\nlearning module, transformer-based models were applied to\nlearn distributed sentence-level representations from sentence\npairs. In the similarity score learning module, we adopted a\nlinear regression layer to calculate a similarity score between\n0.0 and 5.0 according to the distributed representations derived\nfrom the transformers. We explored both single-model and\nensemble solutions. Figure 1A shows the single-model solution\nwhere only one transformer-based model was used for feature\nrepresentation learning. Figure 1B shows the ensemble solution\nwhere different transformer models were integrated. Ensemble\nlearning is an efficient approach to aggregate different machine\nlearning models to achieve better performance [35]. In this\nwork, we tried different strategies to combine the distributed\nrepresentations from two or three transformers as a new input\nlayer for the similarity score learning module. We explored\nseveral methods to combine the distributed representations from\ndifferent transformers, including (1) simple head-to-tail\nconcatenation, (2) pooling, and (3) convolution.\nFigure 1. An overview of our single-model and ensemble solutions for clinical STS. STS: semantic textual similarity.\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 3http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTraining Strategy\nAs shown in Figure 2, we adopted a two-phase procedure to\ntrain our clinical STS models. In the first phase, an intermediate\nSTS model was fine-tuned using the STS-General corpus.\nSubsequently, the intermediate model was further fine-tuned\nusing the STS-Clinic corpus in phase 2. The fine-tuned model\nfrom the second phase was used for final testing. We used 5-fold\ncross-validation for hyperparameter optimization in both phase\n1 and phase 2 training. We optimized the epoch number, batch\nsize, and learning rate according to the cross-validation results.\nFigure 2. The two-stage procedure for clinical STS model development. STS: semantic textual similarity.\nExperiments and Evaluations\nIn this study, we implemented our STS system using the\nTransformers library developed by the HuggingFace team [36].\nWe also used the PyTorch-based general transformer models\ntrained using general English corpora maintained by the\nHuggingFace team. The clinical transformer models were\nderived by further pretraining these general transformer models\nwith clinical notes from the MIMIC-III database [37]. Table 2\nshows the hyperparameters used for each transformer model.\nFor evaluation, the results were calculated as the Pearson\ncorrelation scores using the official evaluation script provided\nby the 2019 n2c2/OHNLP challenge organizers. To report the\nP value for each Pearson correlation score, we adopted the SciPy\npackage [38].\nTable 2. Hyperparameters for transformer models.\nLearning rateaBatch sizeNumber of epochsModel\n1.00E-0584BERT-baseb\n1.00E-0583BERT-mimic\n1.00E-0583BERT-large\n1.00E-0543XLNet-base\n1.00E-0543XLNet-mimic\n1.00E-0544XLNet-large\n1.00E-0543RoBERTa-basec\n1.00E-0543RoBERTa-mimic\n1.00E-0543RoBERTa-large\n1.00E-0584BERT-large + XLNet-large\n1.00E-0543BERT-large + RoBERTa-large\n1.00E-0544RoBERTa-large + XLNet-large\n1.00E-0523BERT-large + XLNet-large + RoBERTa-large\naThe learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of\na loss function [39].\nbBERT: Bidirectional Encoder Representations from Transformers.\ncRoBERTa: Robustly optimized BERT approach.\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 4http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nResults\nTable 3 compares the performance of the different transformer\nmodels on the test dataset. The RoBERTa-large model achieved\nthe best Pearson correlation of 0.9065 among all models, which\noutperformed the two models we developed and submitted\nduring the challenge, including the XLNet-large (a Pearson\ncorrelation score of 0.8864) and the BERT-large models (a\nPearson correlation score of 0.8549). For RoBERTa and XLNet,\nthe models developed using the LARGE setting pretrained using\ngeneral English corpora achieved better performances than their\nBASE settings (0.9065 vs 0.8778 for RoBERTa; 0.8864 vs\n0.8470 for XLNet, respectively), whereas the BERT-base\nachieved a Pearson correlation score of 0.8615 that outperformed\nthe BERT-large model’s score of 0.8549. For all transformers,\nthe models pretrained using general English corpora (in both\nLARGE settings and BASE settings) outperformed their\ncorresponding clinical models pretrained using clinical notes\nfrom the MIMIC-III database. Among the ensemble models,\nthe BERT-large + RoBERTa-large model achieved the best\nPearson correlation score of 0.8914, which is remarkably lower\nthan the best model, RoBERTa-large. We also observed that\nthe performances of ensemble models were often in between\nthe two individual models (eg, BERT-large + RoBERTa-large\nachieved 0.8914, which is between the BERT-large score of\n0.8549 and RoBERTa-large score of 0.9065). The ensemble\nmodel of all three transformers achieved a Pearson correlation\nof 0.8452, which was even worse.\nTable 3. Performances of the Pearson correlation on the test set.\nP valuePearson correlation on test setModel\n<.0010.8615BERT-basea\n<.0010.8521BERT-mimic\n<.0010.8549BERT-largeb\n<.0010.8470XLNet-base\n<.0010.8286XLNet-mimic\n<.0010.8864XLNet-largeb,c\n<.0010.8778RoBERTa-based\n<.0010.8705RoBERTa-mimic\n<.0010.9065RoBERTa-large\n<.0010.8764BERT-large + XLNet-largeb\n<.0010.8914BERT-large + RoBERTa-large\n<.0010.8854RoBERTa-large + XLNet-large\n<.0010.8452BERT-large + XLNet-large + RoBERTa-large\naBERT: Bidirectional Encoder Representations from Transformers.\nbThe challenge submissions.\ncThe best challenge submission (ranked 3rd).\ndRoBERTa: Robustly optimized BERT approach.\nDiscussion\nPrincipal Results\nClinical STS is a fundamental task in biomedical NLP. The\n2019 n2c2/OHNLP shared task was organized to solicit\nstate-of-the-art STS algorithms in the clinical domain. We\nparticipated in this challenge and developed a deep\nlearning–based system using transformer-based models. Our\nbest submission (XLNet-large) achieved the third-best\nperformance (a Pearson correlation score of 0.8864) among the\n33 teams. Based on our participation, we further explored\nRoBERTa models and improved the performance to 0.9065\n(RoBERTa-large), demonstrating the efficiency of transformer\nmodels for clinical STS. We also further explored three different\nensemble strategies to develop ensembled models using\ntransformers. Our experimental results show that the ensemble\nmethods did not outperform the unified individual models.\nAnother interesting finding is that the transformers pretrained\nusing the clinical notes from the MIMIC-III database did not\noutperform the general transformers pretrained using general\nEnglish corpora on clinical STS. One possible reason might be\nthat the clinical corpora we used for training are relatively small\ncompared with the general English corpus. Further investigation\nexamining these findings is warranted.\nExperiment Findings\nAlthough previous studies [40-44] have shown that pretraining\ntransformer models with domain-specific corpora could enhance\ntheir performances in domain-related downstream tasks (such\nas clinical concept extraction), our results in this study indicated\nthat this strategy might not be helpful for clinical STS. For all\nthree types of transformers explored in this study, the models\npretrained using general English text consistently obtained\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 5http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nhigher scores than the corresponding models pretrained using\nclinical text. For example, the Pearson correlation score achieved\nby the RoBERTa-mimic was 0.8705; however, the\nRoBERTa-base yielded a higher performance of 0.8778. Tawfik\net al [45] have similarly observed that the PubMed pretrained\nBioBERT did not outperform the corresponding general BERT\nmodel pretrained using English text on clinical STS.\nIn the clinical STS task, using STS-General (an STS corpus\nannotated in the general English domain) as an extra training\nset in addition to STS-Clinic could efficiently improve\nperformances for transformer-based models. Taking the\nRoBERTa model as an example, the RoBERTa-large fine-tuned\nusing only the clinical text (ie, STS-Clinic) achieved a Pearson\ncorrelation score of 0.8720; however, the same model fine-tuned\nwith both the general English text (ie, STS-General) and clinical\ntext (ie, STS-Clinic) achieved a score of 0.9065 (approximately\n0.035 higher). We observed similar results for BERT and\nXLNet. Without Phase 1 (Figure 2), the BERT-large and\nXLNet-large models achieved Pearson correlation scores of\n0.8413 and 0.8626, respectively, which are lower than the results\nwe submitted (0.8549 and 0.8864) using two-phase training.\nWe looked into the training datasets for possible reasons.\nAlthough the STS-General and STS-Clinic were extracted from\ndifferent domains, there are common contents shared between\nthem. First, the annotation guidelines between the two datasets\nwere highly aligned. For both datasets, the annotation scale is\nfrom 0.0 to 5.0, and each score reflects the same similarity level.\nSince the two STS datasets were annotated by different\nannotators, subjective annotation bias might be introduced (eg,\nthe judgement and agreement of semantic similarity among\nannotators might be different in the two datasets). However,\nour experiment results showed that training with both datasets\nimproved the performance despite the potential annotation bias.\nSecond, a considerable portion of STS-Clinic sentence pairs\nare common descriptions that do not require comprehensive\nclinical knowledge to interpret the semantics. Typical examples\ninclude sentences extracted from Consultation Note or Discharge\nSummary as follows:\nPlan: the patient stated an understanding of the\nprogram, and agrees to continue independently with\na home management program.\nThank you for choosing the name M.D. care team for\nyour health care needs!\nOn the other hand, there are many sentences in the STS-General\nassociated with healthcare. An example is exhibited below:\nAlthough obesity can increase the risk of health\nproblems, skeptics argue, so do smoking and high\ncholesterol.\nTang et al [30] have demonstrated that combining\nrepresentations derived from different models is an efficient\nstrategy in clinical STS. We explored similar strategies to\ncombine sentence-level distributed representations, including\nvector concatenation, average pooling, max pooling, and\nconvolution. Surprisingly, our results showed that such ensemble\nstrategies did not help transformer-based STS systems. For\nexample, for the ensemble model derived from the BERT-large\nand the XLNet-large models (ie, BERT-large + XLNet-large),\nthe achieved Pearson correlation scores for vector concatenation,\naverage pooling, max pooling, and convolution were 0.8764,\n0.8760, 0.8799, and 0.8803, respectively. All the results were\napproximately 0.01 lower than that for XLNet-large (0.8864).\nWe also observed that ensemble models’ performances were\nconsistently in between the two individual models (0.8549 for\nBERT-large and 0.8864 for XLNet-large). Future studies should\nexamine this finding.\nTo examine the statistical significance among different models’\nresults, we used a 1-tailed parametric test based on the Fisher\nZ-transformation [46], adopted in the previous SemEval STS\nshared tasks [2-4]. Our best model (ie, RoBERTa-large)\nachieved a statistically significant higher performance than most\nof our other solutions (see Multimedia Appendix 1) but was not\nsignificantly better than the models XLNet-large (P=.07),\nBERT-large + RoBERTa-large (P=.13), and RoBERTa-large\n+ XLNet-large (P=.06). The significance analysis indicated that\nthese four models performed very similarly to each other.\nError Analysis\nWe compared the system prediction from our best model (ie,\nRoBERTa-large) with the gold standards and identified sentence\npairs with the largest discrepancy in terms of the similarity\nscore. Among the top 50 sentence pairs, 26 of them had labeled\nscores in the range of 0.0 to 1.0, and only 6 sentence pairs had\ngold standard STS scores over 3.0. We further split the testing\nresults into two subsets using a threshold score of 2.5 on gold\nstandards and calculated the mean and median of the differences\nbetween the gold standards and predictions. For the subgroup\nconsisting of sentence pairs with gold standard scores over 2.5,\nthe mean and median of difference were 0.46 and 0.37. For the\nother subset (difference≤2.5), the mean and median of difference\nwere 0.69 and 0.66. Therefore, it was more challenging for the\nsystem to predict appropriate STS scores for sentence pairs with\nlow similarity (gold standard score≤2.5) than for those with\nhigh similarity.\nWe also observed that sentence pairs with high similarity scores\nusually have a similar sentence structure where many words\noccur in both sentences. Therefore, we hypothesized that the\nSTS models will assign higher scores to sentence pairs that\nshare a large portion of their lexicons and similar syntax. To\ntest our hypothesis, we adopted the BertViz package [47] to\nprofile the attention pattern of the RoBERTa-large model (ie,\nour best STS model). BertViz can generate the attention pattern\nbetween two sentences by linking words via lines, where the\nline weights reflect the attention weights; higher line weights\nindicate higher attention weights between the two words. Table\n4 and Figure 3 show an example for two sentence pairs on a\nsimilar topic from the training and test sets. In the first example\nfrom the training set, the attention pattern has three dominant\nattention weights (eg, “questions-questions”) and the similarity\nscore for this sentence pair is labeled as 5.0. However, the\nattention pattern for the sentence pair from the test set also has\nsimilar dominant attention weights (such as\n“questions-questions”) but was labeled with a similarity score\nof 0.0.\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 6http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 4. Transformer model attention visualization on two examples from STS-Clinic.\nPredictionGold standardSentence pairCategory\nN/Ab5Training • S1a: advised to contact us with questions or concerns.\n• S2: please do not hesitate to contact me with any further questions.\n2.50Test • S1: patient discharged ambulatory without further questions or concerns noted.\n• S2: please contact location at phone number with any questions or concerns regarding\nthis patient.\naS: sentence.\nbN/A: not applicable.\nFigure 3. Transformer model attention visualization on two examples from STS-Clinic. STS: semantic textual similarity.\nLimitations\nThis study has limitations. First, it is worth exploring methods\nto effectively integrate clinical resources with general English\nresources in transformer-based models. In this study, we\nexplored an approach by pretraining transformer-based models\nwith a clinical corpus (ie, MIMIC-III corpus). However, our\nresults showed that this approach was not efficient. Therefore,\nnew strategies to better integrate medical resources are needed.\nSecond, our clinical STS systems performed better for sentence\npairs with high similarity scores (ie, similarity score≥3 in gold\nstandard) whereas, for the sentence pairs with low similarity\nscores (ie, similarity score<2 in gold standard), our systems still\nneed to be improved. How to address this issue is one of our\nfuture focuses.\nConclusions\nIn this study, we demonstrated transformer-based models for\nmeasuring clinical STS and developed a system that can use\nvarious transformer algorithms. Our experiment results show\nthat the RoBERTa model achieved the best performance\ncompared to other transformer models. Our study demonstrated\nthe efficiency of transformer-based models for assessing the\nsemantic similarity for clinical text. Our models and system\ncould be applied to various downstream clinical NLP\napplications. The source code, system, and pretrained models\ncan be accessed on GitHub [48].\nAcknowledgments\nResearch reported in this publication was supported by (1) the University of Florida Clinical and Translational Science Institute,\nwhich is supported in part by the National Institutes of Health (NIH) National Center for Advancing Translational Sciences under\naward number UL1TR001427; (2) the Patient-Centered Outcomes Research Institute under award number ME-2018C3-14754;\n(3) the Centers for Disease Control and Prevention under award number U18DP006512; (4) the NIH National Cancer Institute\nunder award number R01CA246418; and (5) the NIH National Institute on Aging under award number R21AG061431-02S1.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes\nof Health and Patient-Centered Outcomes Research Institute.\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 7http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nWe would like to thank the n2c2 organizers for providing the annotated corpus and the guidance for this challenge. We gratefully\nacknowledge the support of NVIDIA Corporation for the donation of the graphics processing units used for this research.\nAuthors' Contributions\nXY, JB, and YW were responsible for the overall design, development, and evaluation of this study. YM, HZ, and XH were\ninvolved in conducting experiments and result analysis. XY, JB, and YW wrote and edited this manuscript. All authors reviewed\nthe manuscript critically for scientific content, and all authors gave final approval of the manuscript for publication.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nThe 1-tailed parametric test results based on Fisher Z-transformation.\n[XLSX File (Microsoft Excel File), 10 KB-Multimedia Appendix 1]\nReferences\n1. Agirre E, Cer D, Diab M, Gonzalez-Agirre A. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. : Association\nfor Computational Linguistics; 2012 Presented at: *SEM 2012: The First Joint Conference on Lexical and Computational\nSemantics; June 7-8, 2012; Montréal, Canada p. 385-393 URL: https://www.aclweb.org/anthology/S12-1051\n2. Agirre E, Cer D, Diab M, Gonzalez-Agirre A, Guo W. *SEM 2013 shared taskmantic Textual Similarity. : Association for\nComputational Linguistics; 2013 Presented at: *SEM 2013: The Second Joint Conference on Lexical and Computational\nSemantics; June 13-14, 2013; Atlanta, USA p. 32-43 URL: https://www.aclweb.org/anthology/S13-1004\n3. Agirre E, Banea C, Cardie C, Cer D, Diab M, Gonzalez-Agirre A, et al. SemEval-2014 Task 10: Multilingual Semantic\nTextual Similarity. 2014 Presented at: The 8th International Workshop on Semantic Evaluation (SemEval 2014); Aug\n23-24, 2014; Dublin, Ireland p. 81-91 URL: https://www.aclweb.org/anthology/S14-2010 [doi: 10.3115/v1/s14-2010]\n4. Agirre E, Banea C, Cardie C, Cer D, Diab M, Gonzalez-Agirre A, et al. SemEval-2015 Task 2: Semantic Textual Similarity,\nEnglish, Spanish and Pilot on Interpretability. 2015 Presented at: The 9th International Workshop on Semantic Evaluation\n(SemEval 2015); June 4-5, 2015; Denver, USA p. 252-263 URL: https://www.aclweb.org/anthology/S15-2045/ [doi:\n10.18653/v1/s15-2045]\n5. Agirre E, Banea C, Cer D, Diab M, Gonzalez-Agirre A, Mihalcea R, et al. SemEval-2016 Task 1: Semantic Textual\nSimilarity, Monolingual and Cross-Lingual Evaluation. 2016 Presented at: The 10th International Workshop on Semantic\nEvaluation (SemEval-2016); June 16-17, 2016; San Diego, USA p. 497-511 URL: https://www.aclweb.org/anthology/\nS16-1081/ [doi: 10.18653/v1/S16-1081]\n6. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and\nCrosslingual Focused Evaluation. 2017 Presented at: The 11th International Workshop on Semantic Evaluation\n(SemEval-2017); Aug 3-4, 2017; Vancouver, Canada p. 1-14 URL: https://www.aclweb.org/anthology/S17-2001/ [doi:\n10.18653/v1/s17-2001]\n7. Farouk M. Measuring Sentences Similarity: A Survey. ArXiv 2019 Oct 06 [FREE Full text] [doi:\n10.17485/ijst/2019/v12i25/143977]\n8. Ramaprabha J, Das S, Mukerjee P. Survey on Sentence Similarity Evaluation using Deep Learning. In: J. Phys.: Conf. Ser.\n2018 Apr 25 Presented at: National Conference on Mathematical Techniques and its Applications (NCMTA 18); Jan 5–6,\n2018; Kattankulathur, India URL: https://iopscience.iop.org/article/10.1088/1742-6596/1000/1/012070 [doi:\n10.1088/1742-6596/1000/1/012070]\n9. Gomaa WH, Fahmy AA. A Survey of Text Similarity Approaches. IJCA 2013 Apr 18;68(13):13-18. [doi:\n10.5120/11638-7118]\n10. Béchara H, Costa H, Taslimipoor S, Gupta R, Orasan C, Corpas PG, et al. MiniExperts: An SVM Approach for Measuring\nSemantic Textual Similarity. Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)\nDenver, Colorado: Association for Computational Linguistics; 2015 Presented at: The 9th International Workshop on\nSemantic Evaluation (SemEval 2015); Jun 4-5, 2015; Denver, USA p. 96-101 URL: https://www.aclweb.org/anthology/\nS15-2017/ [doi: 10.18653/v1/S15-2017]\n11. Buscaldi D, Flores J, Ruiz I, Rodriguez I. SOPA: Random Forests Regression for the Semantic Textual Similarity task.\n2015 Presented at: The 9th International Workshop on Semantic Evaluation (SemEval 2015); Jun 4-5, 2015; Denver, USA\np. 132-137 URL: https://www.aclweb.org/anthology/S15-2024/ [doi: 10.18653/v1/s15-2024]\n12. He H, Gimpel K, Lin J. Multi-perspective sentence similarity modeling with convolutional neural networks. 2015 Presented\nat: The 2015 Conference on Empirical Methods in Natural Language Processing; Sept 19 – 21, 2015; Lisbon, Portugal p.\n1576-1586 URL: https://www.aclweb.org/anthology/D15-1181/ [doi: 10.18653/v1/d15-1181]\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 8http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n13. Mueller J, Thyagarajan A. Siamese recurrent architectures for learning sentence similarity. : AAAI Press; 2016 Presented\nat: The Thirtieth AAAI Conference on Artificial Intelligence; Feb 12–17, 2016; Phoenix, USA p. 2786-2792 URL: https:/\n/dl.acm.org/doi/10.5555/3016100.3016291 [doi: 10.5555/3016100.3016291]\n14. Kashyap A, Han L, Yus R, Sleeman J, Satyapanich T, Gandhi S, et al. Robust semantic text similarity using LSA, machine\nlearning, and linguistic resources. Lang Resources & Evaluation 2015 Oct 30;50(1):125-161. [doi:\n10.1007/s10579-015-9319-2]\n15. Niraula N, Banjade R, Ştefănescu D, Rus V. Experiments with Semantic Similarity Measures Based on LDA and LSA.\n2013 Presented at: The First International Conference on Statistical Language and Speech Processing (SLSP 2013); July\n29-31, 2013; Tarragona, Spain p. 188-199 URL: https://link.springer.com/chapter/10.1007/978-3-642-39593-2_17 [doi:\n10.1007/978-3-642-39593-2_17]\n16. Devlin J, Chang M, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding.\narXiv 2018 [FREE Full text]\n17. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le Q. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. arXiv 2019 [FREE Full text]\n18. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv\n2019 [FREE Full text]\n19. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S. GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding. arXiv 2019 [FREE Full text]\n20. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer. arXiv 2019 [FREE Full text]\n21. Wang W, Bi B, Yan M, Wu C, Bao Z, Xia J, et al. StructBERT: Incorporating Language Structures into Pre-training for\nDeep Language Understanding. arXiv 2019 [FREE Full text]\n22. Birkhead GS, Klompas M, Shah NR. Uses of electronic health records for public health surveillance to advance public\nhealth. Annu Rev Public Health 2015 Mar 18;36:345-359. [doi: 10.1146/annurev-publhealth-031914-122747] [Medline:\n25581157]\n23. Obeid JS, Beskow LM, Rape M, Gouripeddi R, Black RA, Cimino JJ, et al. A survey of practices for the use of electronic\nhealth records to support research recruitment. J Clin Transl Sci 2017 Aug;1(4):246-252 [FREE Full text] [doi:\n10.1017/cts.2017.301] [Medline: 29657859]\n24. Garza M, Del Fiol G, Tenenbaum J, Walden A, Zozus MN. Evaluating common data models for use with a longitudinal\ncommunity registry. J Biomed Inform 2016 Dec;64:333-341 [FREE Full text] [doi: 10.1016/j.jbi.2016.10.016] [Medline:\n27989817]\n25. Rosenbloom ST, Denny JC, Xu H, Lorenzi N, Stead WW, Johnson KB. Data from clinical notes: a perspective on the\ntension between structure and flexible documentation. J Am Med Inform Assoc 2011;18(2):181-186 [FREE Full text] [doi:\n10.1136/jamia.2010.007237] [Medline: 21233086]\n26. Zhang R, Pakhomov S, McInnes BT, Melton GB. Evaluating measures of redundancy in clinical texts. AMIA Annu Symp\nProc 2011;2011:1612-1620 [FREE Full text] [Medline: 22195227]\n27. Wang MD, Khanna R, Najafi N. Characterizing the Source of Text in Electronic Health Record Progress Notes. JAMA\nIntern Med 2017 Aug 01;177(8):1212-1213 [FREE Full text] [doi: 10.1001/jamainternmed.2017.1548] [Medline: 28558106]\n28. Botsis T, Hartvigsen G, Chen F, Weng C. Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\nAMIA Jt Summits Transl Sci Proc 2010 Mar 01;2010:1-5 [FREE Full text] [Medline: 21347133]\n29. Sogancioglu G, Öztürk H, Özgür A. BIOSSES: a semantic sentence similarity estimation system for the biomedical domain.\nBioinformatics 2017 Jul 15;33(14):i49-i58 [FREE Full text] [doi: 10.1093/bioinformatics/btx238] [Medline: 28881973]\n30. Xiong Y, Chen S, Qin H, Cao H, Shen Y, Wang X, et al. Distributed representation and one-hot representation fusion with\ngated network for clinical semantic textual similarity. BMC Med Inform Decis Mak 2020 Apr 30;20(Suppl 1):72 [FREE\nFull text] [doi: 10.1186/s12911-020-1045-z] [Medline: 32349764]\n31. Chen Q, Du J, Kim S, Wilbur WJ, Lu Z. Deep learning with sentence embeddings pre-trained on biomedical corpora\nimproves the performance of finding similar sentences in electronic medical records. BMC Med Inform Decis Mak 2020\nApr 30;20(Suppl 1):73 [FREE Full text] [doi: 10.1186/s12911-020-1044-0] [Medline: 32349758]\n32. Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, et al. MedSTS: a resource for clinical semantic textual\nsimilarity. Lang Resources & Evaluation 2018 Oct 24;54(1):57-72. [doi: 10.1007/s10579-018-9431-1]\n33. Rastegar-Mojarad M, Liu S, Wang Y, Afzal N, Wang L, Shen F, et al. BioCreative/OHNLP Challenge 2018. 2018 Presented\nat: The 9th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB); Aug 29 -\nSept 1, 2018; Washington DC, USA p. 575 URL: https://doi.org/10.1145/3233547.3233672[doi: 10.1145/3233547.3233672]\n34. Wang Y, Fu S, Shen F, Henry S, Uzuner O, Liu H. Overview of the 2019 n2c2/OHNLP Track on Clinical Semantic Textual\nSimilarity. JMIR Medical Informatics 2020 [FREE Full text] [doi: 10.2196/23375]\n35. Zhang C, Ma Y, editors. Ensemble Learning. In: Ensemble machine learning: methods and applications. New York, USA:\nSpringer; 2012.\n36. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. HuggingFace's Transformers: State-of-the-art Natural\nLanguage Processing. arXiv 2019 [FREE Full text]\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 9http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n37. Johnson AEW, Pollard TJ, Shen L, Lehman LH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible critical care\ndatabase. Sci Data 2016;3:160035 [FREE Full text] [doi: 10.1038/sdata.2016.35] [Medline: 27219127]\n38. Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, SciPy 1.0 Contributors. SciPy 1.0: fundamental\nalgorithms for scientific computing in Python. Nat Methods 2020 Mar;17(3):261-272 [FREE Full text] [doi:\n10.1038/s41592-019-0686-2] [Medline: 32015543]\n39. Murphy K. Machine Learning: A Probabilistic Perspective. Cambridge, USA: MIT Press; 2012.\n40. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining. Bioinformatics 2020 Feb 15;36(4):1234-1240. [doi: 10.1093/bioinformatics/btz682] [Medline:\n31501885]\n41. Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. arXiv\n2019 [FREE Full text]\n42. Alsentzer E, Murphy J, Boag W, Weng W, Jin D, Naumann T, et al. Publicly Available Clinical BERT Embeddings. arXiv\n2019 [FREE Full text]\n43. Peng Y, Yan S, Lu Z. Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo\non Ten Benchmarking Datasets. 2019 Presented at: The 18th BioNLP Workshop and Shared Task (BioBLP 2019); Aug 1,\n2019; Florence, Italy p. 58-65. [doi: 10.18653/v1/w19-5006]\n44. Si Y, Wang J, Xu H, Roberts K. Enhancing clinical concept extraction with contextual embeddings. J Am Med Inform\nAssoc 2019 Nov 01;26(11):1297-1304. [doi: 10.1093/jamia/ocz096] [Medline: 31265066]\n45. Tawfik NS, Spruit MR. Evaluating sentence representations for biomedical text: Methods and experimental results. J\nBiomed Inform 2020 Apr;104:103396. [doi: 10.1016/j.jbi.2020.103396] [Medline: 32147441]\n46. Fisher RA. Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large\nPopulation. Biometrika 1915 May;10(4):507. [doi: 10.2307/2331838]\n47. Vig J. A Multiscale Visualization of Attention in the Transformer Model. arXiv 2019 Jun 12 [FREE Full text] [doi:\n10.18653/v1/p19-3007]\n48. 2019 N2C2 Track-1 Clinical Semantic Textual Similarity. GitHub. URL: https://github.com/uf-hobi-informatics-lab/\n2019_N2C2_Track1_ClinicalSTS.git [accessed 2020-11-02]\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers\nMIMIC-III: Medical Information Mart for Intensive Care\nMLM: masked language modeling\nn2c2: National Natural Language Processing Clinical Challenges\nNIH: National Institutes of Health\nNLP: natural language processing\nOHNLP: Open Health Natural Language Processing\nRoBERTa: Robustly optimized BERT approach\nSemEval: semantic evaluation\nSTS: semantic textual similarity\nEdited by Y Wang; submitted 27.07.20; peer-reviewed by F Li, J Lei, A Mavragani; comments to author 06.10.20; revised version\nreceived 19.10.20; accepted 26.10.20; published 23.11.20\nPlease cite as:\nYang X, He X, Zhang H, Ma Y, Bian J, Wu Y\nMeasurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models\nJMIR Med Inform 2020;8(11):e19735\nURL: http://medinform.jmir.org/2020/11/e19735/\ndoi: 10.2196/19735\nPMID: 33226350\n©Xi Yang, Xing He, Hansi Zhang, Yinghan Ma, Jiang Bian, Yonghui Wu. Originally published in JMIR Medical Informatics\n(http://medinform.jmir.org), 23.11.2020. This is an open-access article distributed under the terms of the Creative Commons\nAttribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete\nbibliographic information, a link to the original publication on http://medinform.jmir.org/, as well as this copyright and license\ninformation must be included.\nJMIR Med Inform 2020 | vol. 8 | iss. 11 | e19735 | p. 10http://medinform.jmir.org/2020/11/e19735/\n(page number not for citation purposes)\nYang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7730587720870972
    },
    {
      "name": "Natural language processing",
      "score": 0.761509895324707
    },
    {
      "name": "Semantic similarity",
      "score": 0.5915294289588928
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5703558921813965
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.550399899482727
    },
    {
      "name": "Transformer",
      "score": 0.505516529083252
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.49644047021865845
    },
    {
      "name": "Information retrieval",
      "score": 0.44096195697784424
    },
    {
      "name": "Biomedical text mining",
      "score": 0.44057855010032654
    },
    {
      "name": "Text mining",
      "score": 0.23782354593276978
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800717037",
      "name": "University of Florida Health",
      "country": "US"
    }
  ]
}