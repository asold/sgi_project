{
  "title": "KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts",
  "url": "https://openalex.org/W4390204410",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Coscia, Adam",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3174101223",
      "name": "Endert, Alex",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6839193947",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W6791720600",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2075038621",
    "https://openalex.org/W1969045498",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963214037",
    "https://openalex.org/W3021941048",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W2973126383",
    "https://openalex.org/W3101662419",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W3038035611",
    "https://openalex.org/W3093452197",
    "https://openalex.org/W3175479236",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W6764072591",
    "https://openalex.org/W3101155149",
    "https://openalex.org/W4301393026",
    "https://openalex.org/W4205741728",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6677063751",
    "https://openalex.org/W2136480620",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W1599454686",
    "https://openalex.org/W4245436919",
    "https://openalex.org/W2165232124",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2293769110",
    "https://openalex.org/W2071172532",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W4285210581",
    "https://openalex.org/W3123340107",
    "https://openalex.org/W3135734416",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3103029570",
    "https://openalex.org/W4295306962",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVIS, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVIS reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVIS with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.",
  "full_text": "1\nKnowledgeVIS: Interpreting Language Models\nby Comparing Fill-in-the-Blank Prompts\nAdam Coscia and Alex Endert\nAbstract—Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and\ngenerating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVIS, a\nhuman-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing\npredictions between sentences, KnowledgeVIS reveals learned associations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using\na novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help\nusers identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize\npatterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVIS with feedback\nfrom six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2)\nevaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.\nIndex Terms—Visual analytics, Language models, Prompting, Interpretability, Machine learning.\n✦\n1 I NTRODUCTION\nL\narge language models (LLMs) such as BERT [1] and\nGPT-3 [2] have seen significant improvements in perfor-\nmance on natural language tasks [3], enabling them to help\npeople answer questions, generate essays, summarize long\narticles, and more. Yet understanding what these models\nhave learned and why they work is still an open challenge\n[3], [4]. In particular, how learned text representations in\nBERT-based language models generalize to natural lan-\nguage tasks remains unclear [5]. For natural language pro-\ncessing (NLP) researchers and engineers who increasingly\ntrain and deploy LLMs as “black boxes” for generating text,\nexploring how learned behaviors during training manifest\nin downstream tasks can help them improve model de-\nvelopment; e.g., by surfacing harmful stereotypes [6]. To\nhelp researchers and engineers close the gap between what\nBERT-based models have learned and how they perform\non downstream tasks, we can use prompts formatted as\nnatural language tasks [7]. For example, Table 1 shows\nBERT’s predictions for multiple fill-in-the-blank sentences\nthat test for conceptual reasoning capabilities, connecting\nmodel performance to learned text representations.\nOur aim is to utilize fill-in-the-blank sentences for inter-\npreting BERT-based language models by revealing learned\nassociations. Much of the existing work seeks to auto-\nmatically extract, augment, and test individual template\nsentences against a manually curated “gold standard” [8],\n[9], [10], [11]. However, these quantitative benchmarks miss\nan opportunity for injecting a researcher’s intuition and\ndomain expertise into evaluating model performance [12].\nA human-in-the-loop solution can foster human and LLM\ninteraction by continuously integrating feedback into the\n• Adam Coscia and Alex Endert are with Georgia Institute of Technology.\nEmails: {acoscia6, endert}@gatech.edu.\nThis manuscript is a Preprint - Accepted, to Appear: IEEE Transactions on\nVisualization and Computer Graphics, DOI: 10.1109/TVCG.2023.3346713\nTABLE 1\nExample Data Set Visualized by KnowledgeVIS\nprompt prediction probability cluster 1\nYou are likely to find a snake\nin a\nfield 0.066 physical entity\nOne effect of exercising\nis feeling\nbetter 0.296 abstraction\nYou could be sick because\nyou are\npregnant 0.209 condition\nIf you want to learn then\nyou need a\nteacher 0.122 physical entity\nprocess of model development [13]. Further, testing one\nprompt at a time can limit the interpretability of LLMs\nby failing to surface associations or producing contradic-\ntory predictions [4], [14], [15]. Guiding the user through\nexploring multiple prompts at once can reveal insights and\npatterns that further improve our understanding of BERT-\nbased models. To realize these goals, a successful solution\nshould help users quickly format and test multiple prompt\nvariations simultaneously, structure sets of predicted words\nto make them easier to parse, and present the data at several\nlevels of detail.\nWe present KnowledgeVIS, a human-in-the-loop visual\nanalytics system for comparing fill-in-the-blank prompts\nto uncover associations from learned text representations.\nKnowledgeVIS helps users create effective sets of prompts,\nprobe multiple types of relationships between words, test\nfor different associations that have been learned, and find\ninsights across several sets of predictions for any BERT-\nbased language model. It does so through a tight integra-\ntion of multiple coordinated views. First, we designed an\nintuitive visual interface that structures the query process\nto encourage both creativity and rapid prompt generation\n1. We generate clusters based on shared taxonomic and semantic\nsimilarity of tokens, described in Sect. 4.2.1.\narXiv:2403.04758v1  [cs.HC]  7 Mar 2024\n2\nand testing. Then, to reduce the complexity of the prompt\nprediction space, we developed a novel clustering technique\nthat groups predictions by semantic similarity. Finally, we\nprovided several expressive and interactive text visual-\nizations to promote exploration and discovery of insights\nat multiple levels of data abstraction: a heat map; a set\nview inspired by parallel tag clouds [16]; and scatterplot\nwith dust-and-magnet positioning of axes [17]. Collectively,\nthese visualizations help the user identify the likelihood\nand uniqueness of individual predictions, compare sets of\npredictions between prompts, and summarize patterns and\nrelationships between predictions across all prompts.\nTo validate our approach, we demonstrate the capabil-\nities of KnowledgeVIS with three use cases and an expert\nevaluation. First, we reveal learned biomedical associations\nin two domain-adapted LLMs, SciBERT [18] and PubMed-\nBERT [19]. Second, we uncover harmful learned identity\nstereotypes between two general-purpose LLMs, BERT [1]\nand RoBERTa [20]. Third, we elicit and evaluate different\nworld knowledge (i.e. commonsense relationships) learned\nby a large and a small general-purpose LLM, BERT [1] and\nDistilBERT [21] respectively. Finally, we conduct an eval-\nuation with six academic NLP researchers and engineers.\nKnowledgeVIS surfaced several new insights for the domain\nexperts, including how BERT-based language models han-\ndle parts of speech, transitivity, and semantic roles, as well\nas learned cultural and religious associations that biased\npredictions in unexpected ways. Our participants wanted to\nevaluate their own models using KnowledgeVIS and would\nrecommend our interface to anyone interested in “opening\nthe black box of how [LLMs] work”.\nIn summary, our paper contributes: (1) an open-\nsource2 visual analytics system, KnowledgeVIS, that imple-\nments text visualization techniques for comparing fill-in-\nthe-blank prompts that reveal associations from learned\ntext representations in BERT-based language models; (2)\na novel taxonomy-based technique for semantically clus-\ntering prompt predictions; and (3) three use cases and an\nexpert evaluation showing how KnowledgeVIS helps NLP\nresearchers interpret BERT-based language models.\n2 R ELATED WORK\n2.1 Modeling Language with Transformers\nIn this paper, we focus on BERT-based language models\nbecause their transformer architecture and masked language\nmodeling pre-training easily adapt to our fill-in-the-blank\nprompts. Language models learn to model the probability of\na token occurring in a sequence, e.g., a word in a sentence.\nTransformers extend this by encoding and decoding all\ninput words in a sentence, generating weights that map\nattention from an output to the most relevant inputs [22].\nThese attention weights are attributed with storing factual\nand linguistic knowledge needed to perform natural lan-\nguage tasks [5]. With this architecture, transformer-based\nlanguage models can pre-train via self-supervised learning\nusing large-scale unlabeled document corpora on a variety\nof tasks. BERT-based language models pre-train on masked\nlanguage modeling, or repeatedly removing and predicting\n2. https://github.com/AdamCoscia/KnowledgeVIS\n(masking) tokens in a finite sequence [1]. Thus, we can\nelicit learned text representations in the attention weights\nof masked language models simply by mirroring their pre-\ntraining task using fill-in-the-blank sentences as prompts\n[7]. We test the capabilities of our fill-in-the-blank prompt\napproach with BERT base and four other pre-trained BERT-\nbased models — RoBERTa [20], DistilBERT [21], SciBERT\n[18], and PubMedBERT [19].\n2.2 Probing Language Models With Prompts\nTransformers are typically pre-trained on various tasks\n(Sect. 2.1) and then fine-tuned using task-specific objective\nfunctions. Prompting instead emulates pre-training by for-\nmatting the downstream objective as a natural language\ntask [7]. Consider the example prompts in Table 1. Instead\nof creating an objective function and labeled training data\nfor a large language model (LLM) to predict conceptual\nrelationships, we reformulate the task as one that BERT-\nbased language models have seen already — a fill-in-the-\nblank sentence as a prompt.\nPrior work in probing LLMs using prompts involves\neliciting and interpreting learned syntactic, semantic, and\nworld knowledge [4]. For example, Petroni et al. [23]\nshowed that LLMs can be queried using fill-in-the-blank\nsentences as prompts (e.g., asking BERT to complete the\nsentence, “The capital of France is ” results in BERT re-\nsponding with “Paris”) to elicit relational knowledge (i.e.\nthe relationship “capital of” between France and Paris).\nThey posit knowledge probing estimates a lower-bound on\nknowledge contained in a model’s internal representations.\nResearchers have developed both manual and automated\napproaches to finding more effective prompts to raise the\nlower bound. Jiang et al. created LPAQA [8] using both\nmining-based and paraphrasing-based methods to generate\nnew prompts for existing relations. Shin et al. [9] created a\nmodel, AUTOPROMPT, that searches for specific discrete\ntokens to automatically generate better prompts from a\ntemplate. Zhong et al. [11] developed OPTIPROMPT, which\nreplaces AUTOPROMPT’s discrete token search with a con-\ntinuous vector search. Qin et al [10] relax the constraints\non continuous word embeddings to create “soft prompts”\nthat avoid emphasizing misleading tokens such as gendered\npronouns. Elezar et al. [15] created PARAREL, a benchmark\nfor measuring the consistency of model predictions when\nprompts are paraphrased but the semantic meaning remains\nconstant. We extend this work using a visual analytics\napproach to qualitatively evaluating multiple prompts si-\nmultaneously for interpreting model performance.\n2.3 Visual Analytics For LLM Interpretability\nVisual analytics has become a popular approach for ana-\nlyzing and interpreting machine learning models [24]. One\nmethod of interpreting model performance is directly visu-\nalizing the model’s internal representations as a form of ex-\nplanation [25]. In deep learning models that perform natural\nlanguage tasks such as Long Short Term Memory (LSTM)\n[26] and sequence-to-sequence (seq2seq) models [27], visu-\nalization techniques have been shown to help debug and\nexplain how neural layers transform input sequences into\nfinal predictions [28]. Alternatively, visualizations can help\n3\nusers probe models from the outside by structuring the\ninput process and supporting interactive analysis of model\noutputs. VizSeq is a visual analysis toolkit for interactively\nevaluating LLM task benchmarks [29]. Other tools present\na visual analytics workflow to analyze changes in LLM\nweights under various task-specific scenarios [30]. These\ntools share a focus on human-in-the-loop workflows and\ninteractivity to integrate valuable feedback during model\nvalidation [13], which KnowledgeVIS makes heavy use of.\nWith transformers, new visual analytics approaches for\ninterpreting how and why they work have been developed.\nSeveral tools focus on visualizing the internal prediction\nprocess of transformers as inputs are fed through each layer\nof the model [31], [32], [33]. In particular, Dodrio visualizes\nthe connection between attention weights and linguistic\nknowledge such as syntactic dependencies [34]. However,\nthere is an active discussion as to whether attention weights\nin transformers can be used as a source of interpretation for\nmodel performance [35], [36], [37]. Following our argument\nabove, prompts can help users validate model performance\nby instead probing the model from the outside. PromptIDE\nis a visualization interface for experimenting with prompt\nvariations, visualizing prompt performance, and iteratively\noptimizing prompts [38]. LMDiff visually compares the\ndifference in rank for all tokens in a single prompt between\ntwo different LLMs to facilitate comparison of model per-\nformance [39]. We contribute to current research on human-\nin-the-loop workflows using prompts by introducing text\nvisualization techniques for comparing multiple prompts\nsimultaneously to validate model performance.\n3 D ESIGN CHALLENGES AND GOALS\nOur goal is to build a system for discovering learned asso-\nciations from text representations in BERT-based language\nmodels. For natural language processing (NLP) researchers\nand engineers, interactively exploring model performance\nacross different tasks can help build trust in models be-\nfore deployment [13]. Fill-in-the-blank sentences can help\nconnect what large language models (LLMs) have learned\nwith downstream tasks to interpret how they work [5].\nYet existing work in this space is primarily based around\nquantitatively evaluating prompts one at a time against an\nobjective gold standard [8], [9], [10], [11], [23]. Our approach,\na human-in-the-loop solution for qualitatively exploring\nmultiple prompts simultaneously, overcomes several limi-\ntations by addressing the following key design challenges.\n3.1 Design Challenges\nC1 Creating effective prompts. It remains an open chal-\nlenge to explain how internal text representations in LLMs\nare translated into natural language understanding for\ncompleting tasks [5]. Using fill-in-the-blank sentences as\nprompts for LLMs formats queries intuitively as natural\nlanguage tasks [7]. For example, Table 1 shows how asso-\nciations help demonstrate learned complex relationships. A\nsystem should enable users to easily create, format, and test\ntheir own prompts.\nC2 Testing multiple prompts at once.A single prompt can\ngive limited understanding of what association LLMs are\nmaking based on the context of the sentence [4], [14], [15].\nTo enable more effective prompt testing, we can evaluate\nmultiple prompts simultaneously that isolate dependent\nvariables, such as the bold subjects in Table 1. Our interface\nshould encourage users to test multiple prompts through\nintuitive input design.\nC3 Probing different types of relationships.While fill-in-\nthe-blank prompts can be designed to elicit a single fact or\nrelationship [23], LLMs also learn multiple correct responses\nto the same prompt, subjective answers, and associations\nsuch as stereotypes and domain-specific knowledge [4].\nTable 1 shows how subjective, open-ended prompts can\nelicit meaningful predictions that help users interpret model\nperformance. Providing several prompt examples can guide\nusers to more effectively design a wide variety of probes for\neliciting different relationships.\nC4 Finding insights in a large search space.As shown in\nTable 1, predicted tokens and their probability scores present\nseveral analytical challenges. LLMs often have large vocab-\nularies that lack useful stratification [1]. Tokens themselves\ncan be both specific to a prompt and generalizable across\nprompts, duplicated across prompts, and/or unique within\na prompt. Methods for grouping, filtering, searching, and\narranging predictions and their probabilities can aid users\nin discovering insights.\n3.2 Design Goals\nWe developed design goals that address the key challenges\nraised in Sect. 3.1 and align with our interface components:\nG1 Intuitive visual interfaces for structuring prompt-\ning. Clearly communicating how to input prompts through\nvisual design can help guide users to more rapidly test\nfor learned associations. “Fill-in-the-blank” prompt inputs\nshould be open-ended to encourage creativity ( C1) with\nflexible formatting rules to enable rapid generation of dif-\nferent prompt variations ( C3). At the same time, arranging\ninputs to facilitate comparison across prompt variations\nshould encourage more thoughtful probing (C2).\nG2 Useful grouping of prompts and predictions. Pro-\nviding additional structure to several large sets of predic-\ntions can help reduce their complexity ( C4). We aim to\ndifferentiate predictions by their semantic relatedness and\ncommunicate this distinction visually while respecting the\ninput structure of prompts. Highlighting new connections\ncould enable domain experts to use their own knowledge\nfor evaluating predictions more effectively (C2, C3).\nG3 Expressive and interactive views for discovering in-\nsights. Fluid transitions between levels of abstraction in the\ndata can surface patterns across multiple sets of predictions\nthat connect learned associations to model performance\n(C4). We aim to support several low-level tasks including\nidentifying highly salient or unique predictions, comparing\nboth individual and sets of unique and shared predictions\nbetween prompt variations, and summarizing groups of\npredictions as over/under performing subsets for further\ninvestigation. Incorporating these tasks across several coor-\ndinated views can help users connect these low-level tasks\nwith high-level model performance (C2, C3).\n4\nFA\nB\nC ED\nFig. 1. KnowledgeVIS integrates multiple views to reveal associations that LLMs learn during training. Above, a user investigates whether BERT\nexhibits associations that reveal learned conceptual relationships (Sect. 5.3), helping them interpret how BERT works.\n4 K NOWLEDGE VIS\nBased on the design challenges and goals in Sect. 3, we de-\nveloped KnowledgeVIS (Fig. 1), a human-in-the-loop visual\nanalytics system for comparing fill-in-the-blank prompts to\nuncover associations from learned text representations. Our\ninterface tightly integrates multiple coordinated views with\na novel prediction clustering technique. Users structure the\nprompt generation and testing process using the Prompt\nInterface (Sect. 4.1). Then, the Predictions View (Sect. 4.2)\nvisualizes predictions at multiple levels of abstraction across\na Heat Map(Sect. 4.2.2), Set View(Sect. 4.2.3), and Scatter Plot\n(Sect. 4.2.4). Finally, the Filters Panel(Sect. 4.3) allows users\nto fine-tune their analysis. Throughout, we describe how\neach design goal (G1-3) is addressed in our interface.\n4.1 Prompt Interface\nThe Prompt Interface(Fig. 1A) guides users to create multiple\nprompts using a grid structure and intuitive text formatting\nthat promote structured exploration of different prompt\nvariations. Users start here to begin exploring the capabili-\nties of prompting for eliciting associations.\nTo guide users in eliciting interesting sets of predictions,\nwe structure prompt inputs in several ways. To help users\nprobe for different types of relationships, we provide but-\ntons that load examples related to domain adaptation, bias\nevaluation, and knowledge probing ( G1). We explore these\nexamples in depth in Sect. 5. Prompts are written in open-\nended text inputs aligned in a 2-column grid (G1). The hori-\nzontal dimension promotes comparison across paraphrased\nsentences. Users write prompt “templates” in the left col-\numn that must include a single underscore character for the\nLLM to fill in. Users can then use the right column to input\nany number of “subjects”, to be filled in to the template\nprompt using an additional [subjects] mask anywhere in\nthe prompt. This allows users to intuitively create single- or\nmulti-token variations on the given template prompt, i.e.,\nparaphrasing prompts grammatically. The vertical dimen-\nsion provides additional rows for writing template/subject\npairs to compare semantically similar prompts that may\ndiffer grammatically. Finally, we mirror the grid structure\nhierarchically by template → subject (G2) in the Predictions\nView, described in Sect. 4.2, helping users reflect on how\nthey can refine their prompts.\nPrompts are evaluated in real-time using an API that\ninterfaces with a Python Flask server running PyTorch im-\nplementations of the LLMs. We use the HuggingFace Trans-\nformers [40] API to load models and perform masked word\nprediction with the user-created prompts. Users can select\ndifferent pre-trained masked language models from a drop-\ndown list to compare performance and add new models\neasily through the HuggingFace Transformers model library.\n5\nTo reduce the complexity of the prediction space, we also\nlet users choose the top k tokens to retrieve and visualize.\nFinally, users can export the extracted data in the format of\nTable 1 for further investigation.\n4.2 Predictions View\nWe provide several facilities in the Predictions View(Fig. 1B-\nE) to promote exploration and discovery of insights about\nwhat a model has learned. First, we group predictions by se-\nmantic similarity using a novel taxonomy-based clustering\ntechnique developed in Sect. 4.2.1. Then, we visualize the\nprompt, prediction, and cluster data as shown in Table 1 at\nmultiple levels of abstraction across three interactive plots.\nEach plot provides unique advantages for different analysis\ntasks that the other plots do not, that together help users\nfind patterns better than a single plot could. In Sect. 5 we\ndescribe how the plots are used in tandem in each use case\nas well as in the expert evaluation to help guide participants’\nanalysis process and thinking.\nThe Heat Map (Sect. 4.2.2) makes it easy to accurately\nidentify and compare individual probabilities across prompt\nvariations (columns) and semantic clusters (rows). The grid\nstructure uniquely highlights words not shared between\nprompts to help users find outliers. TheSet View(Sect. 4.2.3),\ninspired by parallel tag clouds [16], facilitates in-depth\ncomparison of word sets across multiple prompts, as well as\nrank order analysis. Selecting a predicted word aligns each\noccurrence across prompts along a common baseline and\nshows a novel selected word rank order view. The Scatter\nPlot (Sect. 4.2.4) projects predictions in a low-dimension\nspace and uses a dust-and-magnet metaphor [17] to position\nprompts as points of interest, revealing new relationships\nbetween predictions at the data set level. For example,\ncommon predictions more relevant to a subset of prompts,\nas well as unique predictions sharing a relationship between\ntwo prompts, are visually grouped.\n4.2.1 Clustering Predictions\nUsers may not have an intuitive sense for seeing patterns\nin shared meaning across predicted tokens, especially as the\nnumber of predicted tokens grows. This meta-information\ncan be useful to determine, e.g., specific training biases\ntowards higher-level concepts that may or may not be rele-\nvant to the semantic meaning of the prompt. To help users\ndiscover patterns in prediction sets, we aim to group and\ndescribe semantically similar predictions. While topic mod-\nelling has been used to assign manually-sourced candidate\nlabels to lists of terms based on the co-occurrence of labels\nand terms [41], there are a lack of methods for automatically\ndiscovering appropriate labels without frequency data. To\novercome this, our solution uses a hierarchical taxonomy of\nword sense, or the meanings of words, to algorithmically\ngenerate and label clusters of words based on their shared\nsemantic meaning ( G2). We color tokens by their cluster\nlabel (G3) in each visualization (Fig. 1C-E).\n1) Compute a distance matrix of pairwise Wu-Palmer\nsimilarities [42] between all unique predicted words.\nCompared with other popular measures such as word\nvector similarity [43], Wu-Palmer led to better cluster\nlabels downstream.\n2) Perform hierarchical clustering [44] over the distance\nmatrix using Ward linkage [45]. Affinity propagation\n[46], while often used with similarity-based distance\nmetrics, requires unintuitive manual tuning of the\ndamping factor to avoid overfitting.\n3) Determine the optimal number of clusters based on\neither the maximum silhouette coefficient [47] or a user-\ndefined cut-off threshold on hierarchical clusters.\n4) Automatically label each cluster by computing the low-\nest common hypernym (LCH) between all words in\nthe set using WordNet [48]. Hypernyms (words with\na broad meaning that more specific words fall under\nsuch as “color” and “red”) are an approximate yet\ninformative label for a majority of the open-class word\npredictions returned by LLMs such as nouns, verbs, etc.\n4.2.2 Heat Map\nThe Heat Map (Fig. 1C) plots a uniform grid of all unique\npredicted tokens vertically as rows and all prompts as\ncolumns. Each cell is colored by the probability of the given\nword (row) occurring in the prompt (column). Much like a\ndata table, visualizing predictions in this way allows users\nto quickly identify and compare small details in the entire\ndata set at a glance, such as individual probabilities, cluster\nlabels, and how they compare across prompts ( G3).\nSeveral design considerations enhance analysis capabil-\nities. If a given word (row) is not in the set of predictions\nreturned for a given prompt (column), that cell is left\nunshaded, and a crosshatch pattern is used to differentiate\nmissing values from the background of the interface. We\nprovide row lines to aid in visually scanning across columns\nwith unshaded cells. We found that probabilities tended to\nrange non-linearly, with very few high-probability words\nrelative to many low-probability words. Thus, for broad\ncomparison across all predicted words, we chose as default\na logarithmic color ramp from light to dark pink applied\nto the global extents of the range of probabilities; i.e., the\ndarkest shade of pink is the highest probability in the\ndata set, and the lightest shade of pink is the lowest. We\nprovide a legend and a drop-down list to select between the\ndefault logarithmic scale as well as a linear scale, helping the\nuser more accurately compare probabilities. For example,\nwhen comparing a small subset of probabilities for a single\nprompt, a linear scale is more appropriate. To connect the\nstructure of the inputs with output predictions, we label\nthe columns of the Heat Map hierarchically ( G2). We nest\nsubjects as column names below sentence templates that\nspan several columns, to indicate group membership. If no\nsubjects are put in, i.e., the template is the only prompt to\nevaluate, then we treat the template as a subject and only\nshow the template in the lowest level of the hierarchy.\nWe also implemented interactions that provide addi-\ntional details on demand. Hovering over a cell displays\na tooltip showing the prompt (column), prediction (row),\ncluster, and probability. Rows can be sorted by word top-\ndown in one of four ways: (1) alphabetically; (2) rank\norder (i.e., by probability of occurring in a prompt, from\nleft to right); (3) grouped by cluster, alphabetically; and (4)\ngrouped by cluster, rank order. Groups are sorted top-down\nalphabetically by cluster label.\n6\nS e t Vie w when selecting a w or d and sorting b y  r ank\nFig. 2. The Set View showing our variation on the parallel tag cloud\nlayout, a stepwise degree of interest list based on fisheye menus [49] for\na selected word when sorting by rank, described in Sect. 4.2.3.\n4.2.3 Set View\nThe Set View(Fig. 1D) arranges sets of the top k predicted\ntokens returned for each prompt into parallel columns. Sim-\nilar to cell color in the Heat Map, the font size for each word\n(row) is scaled by the probability of occurring in the prompt\n(column). Inspired by parallel tag clouds [16], this plot\nshows the degree of overlap between prompts (columns)\nby drawing edges between shared tokens, making shared\npredictions more visually salient at a glance. To overcome\ndifficulties in interpreting continuous word probabilities,\nsuch as the cell shading in the Heat Map, users can also sort\ncolumns by rank, encoding probability instead on a discrete\nscale and making differences between prompt variations\nmore interpretable (G3).\nTo help users see new patterns in the data, we im-\nplemented hover and select interactions that change the\narrangement of the words. Hovering on and off a word will\ntemporarily draw a connector line from the word to all other\noccurrences of that word in other columns, while hiding the\nconnector line if it crosses a column that does not contain\nthe hovered word. While hovering, selecting the word will\nshift the columns containing other occurrences of that word\nto align the words along the same horizontal baseline. The\nconnector line will then remain drawn even after hovering\noff. Selecting other words will align the columns along a\nnew baseline and automatically adjust the previously drawn\nconnector lines. Deselecting a selected word will remove\nthe connector line and reset the column alignment. If a\nword does not occur in one or several columns when it is\nselected, each of those columns is set to a lower opacity\nto more clearly show a lack of membership of the word\nin that set. Finally, selecting a word while sorting by rank\norder transitions the view to a stepwise degree of interest\nlist (Fig. 2), similar to fisheye menus [49]. We arrange the\nneighborhood of n words above and below the selected\nword in rank order equidistant and, for the remaining words\noutside of that neighborhood, we draw lines above and\nbelow that scale proportionally with the remaining words\nfrom the top and bottom of the list, respectively. Users can\naccurately compare line heights along the same baseline to\ndetermine the difference in rankings between columns (G3).\nWords not occurring in a column in this view are set to zero\nopacity since no line(s) will be drawn for that column.\nAs in the Heat Map, we label columns hierarchically (G2),\nprovide a legend and drop-down list for logarithmic and\nlinear font scales, draw tooltips when hovering over words\n(showing the prompt, prediction, cluster, and probability),\nand let the user sort words top-down (alphabetically, rank\norder, and grouped by cluster). Collins et al. found that\nordering words alphabetically offers two main advantages\nover rank order: (1) it saves horizontal space, since the\nlargest words are less likely to occur next to each other;\nand (2) it helps users visually scan for words of interest [16].\nThus, we make this the default sorting option for rows.\n4.2.4 Scatter Plot\nThe Scatter Plot (Fig. 1E) positions predicted tokens as vec-\ntors based on their probability of occurring for each prompt\nor “point of interest” (POI) in a 2D coordinate space, using\na layout technique derived by Olsen et al. [50]. Predictions\ncloser to POIs (prompts) indicate a higher probability of\noccurring for that prompt. Groups of predictions in between\nPOIs reveal a unique relationship between prompts that\ncannot be seen in the other two plots. Predictions that\noccupied the shared axis of two POIs revealed unique\nprompt interactions that aligned with semantic cluster labels\n(G2). Because this approach reduces the dimensionality of\npredictions, we allow users to drag POIs, similar to a dust-\nand-magnet metaphor [17], to create new arrangements of\ndata marks and avoid visual artifacts based on the ini-\ntial layout. Overall, the process of arranging predictions\nspatially relative to prompts can help users uncover new\npatterns and related predictions at the data set level ( G3).\nTo help users read the Scatter Plot, we provide several\nvisual embellishments. Data marks are labeled with the\npredicted word or prompt they represent; we implemented\nocclusion to show only the top label when several labels\noverlap. Users can hide these labels with a checkbox. For\nthree POIs, we also draw a differently colored background\nfor the bounded region containing points most closely asso-\nciated with each POI. When predicted words are unique to\na single prompt (POI), the layout algorithm avoids plotting\nthem all at the same position of the POI. Instead, we collect\nthe unique predictions and display each word, cluster, and\nprobability set in a tooltip when hovering over a POI. We\nappend a count of the number of unique predictions for each\nprompt to every POI label. Finally, to visually distinguish re-\nlationships between adjacent points of interest, we draw the\nconvex hull around all POIs. Dragging a POI dynamically\nadjusts the convex hull, to show a shared axis or axes with\nadjacent neighbors.\n7\nBecause the plot reduces the dimensionality of each pre-\ndicted word, we lose information contained in the original\nvector, i.e., the probabilities of the predicted word occurring\nin each prompt. We provide three solutions to recover this\ninformation. The first is a details-on-demand tooltip when\nhovering over a predicted word that lists non-zero probabil-\nities of the predicted word occurring for each prompt. The\nsecond is encoding the maximum probability of a predicted\nword occurring for all prompts as height/width of the\nScatter Plotpoint, as well as font size of the label, on either\na logarithmic or linear scale, similar to the Heat Map and\nSet View, and providing a legend. The third is drawing lines\non hover from the predicted word to each of the non-zero\nprobability prompts (Fig. 1E), double-encoding stroke width\nand opacity to each line using the same scale. For example,\na weak relationship (low probability) between a POI and\npredicted word is shown with a smaller point and label and,\nwhen hovering, with a faded-out thin line, whereas a strong\nrelationship (high probability) is shown with a larger point,\nlabel, and vibrant thick line.\n4.3 Filters Panel\nWith the Filters Panel (Fig. 1F), users can filter prompts\nby directly toggling subjects nested under their template\nsentence. Arranging the prompt filter using the template\n→ subject hierarchy ( G2) can promote a wider variety\nof prompt testing, e.g., by subsetting prediction sets with\nspecific semantic relationships and comparing the results\nagainst other subsets. Within the currently visible subset of\nprompts and predictions, we provide two additional global\nprediction filter operations: “shared only” and “unique\nonly” checkboxes. The “shared only” checkbox filters pre-\ndictions that are shared between all of the currently visible\nprompts. This removes all missing cells in the Heat Mapand\naligns the words along the same horizontal baseline in the\nSet View, helping users more easily find common predictions\nand quickly compare relative probabilities. Similarly, the\n“unique only” checkbox filters predictions that are unique to\neach visible prompt. This reduces the rows of the Heat Map\nso that each contains a single shaded cell, and removes all\npoints from the Scatter Plot, as none share relationships with\nthe other prompts. The Set View can help compare biases\nin each unique set of predictions across prompts, while\nsorting the Heat Mapby cluster can reveal the distribution of\nclasses of words, such as classes unique to a specific prompt.\nUsers can also use the provided search box to highlight all\ninstances of a specific prediction word across all plots.\n5 U SE CASES AND EXPERT EVALUATION\nIn this section, we demonstrate the capabilities of Knowl-\nedgeVIS for prompt engineering and immediate visual anal-\nysis of fill-in-the-blank sentence predictions with three use\ncases (Fig. 3) and an expert evaluation. Our use cases\ncomprised 114 subject replacements across 15 fill-in-the-\nblank sentences, totaling 289 prompt variations to eval-\nuate. The use cases were designed for NLP researchers\nand engineers who are increasingly using LLMs as “black\nboxes” for downstream tasks [24] such as discourse anal-\nysis and classification. Compared to automated methods\nusing quantitative metrics determined a priori, KnowledgeVIS\nleverages human intuition and domain expertise to guide\nLLM evaluation [13]. This enabled experts to suggest new\nways to adapt and improve LLMs in their own research\nand applications, towards “closing the NLP loop” in model\ndevelopment (Sect. 6.1).\nThe results are based on interpreting word probabilities\nusing visual encodings. This presents analytical trade-offs\nwhen choosing scales (linear and log) as well as encodings\n(color, font and marker size). For example, log scale tended\nto more prominently show patterns, but can be misleading.\nPatterns based on positional encoding in the Scatter Plotare\nalso more likely to draw a user’s attention than size encod-\nings. To address this, we detail our method for generating\nand evaluating prompts in the second paragraph of each\nuse case. We acknowledge our qualitative approach may be\nsubject to pre-attentive biases.\nFirst, we tested how grammar and phrasing af-\nfected domain-specific LLMs when replicating expert hu-\nman answers is required. We modified a biomedical\nquestion-answer data set, PubMedQA [51], by formatting\nyes/no/maybe questions as fill-in-the-blank sentences, then\nqueried SciBERT [18] and PubMedBERT [19], both pre-\ntrained on large unlabeled scientific document corpora.\nSecond, current LLMs are regularly fine-tuned on pre-\ntrained general-domain LLMs such as BERT [1] and\nRoBERTa [20] and inherit well known stereotypical asso-\nciations such as gender bias. Yet automated auditing sys-\ntems requiring social categories and quantitative metrics to\nbe predetermined are prone to missing underrepresented\nstereotypes [12]. We discovered contextualized gender, ori-\nentation, pronoun, race, religious, and political stereotypes\nbetween BERT and RoBERTa using subsets of the HONEST\n[52], [53] and BOLD [54] data sets.\nThird, as LLMs increase in size, it is useful to understand\nlimitations at different model scales on whether associations\nnot explicitly trained for such as membership (belongs,\ncauses) and chain of reasoning (goals, prerequisites) are\nlearned. We compared complex learned concepts between\nthe large-scale BERT [1] and small-scale DistilBERT [21]\nmodels based on the LAMA knowledge probe [23].\nFinally, we conducted an expert evaluation with six\nacademic NLP researchers and engineers. The experts gen-\nerated new examples to uncover insights, including: (1) a\nlack of understanding for semantic roles in all three general-\ndomain models; (2) unexpected biases towards less common\nwords in general-domain models; and (3) differences in\ngrammar robustness between domain-specific models.\n5.1 Use Case: Biomedical Knowledge\nHow do domain-specific models compare based on robust-\nness to grammar and phrasing when expert human answers\nare expected? In Fig. 3A, we evaluated two PubMedQA [51]\nquestions (1 and 2)by formatting two grammatically differ-\nent but semantically similar prompts per question (1a/b and\n2a/b) and using the Prompt Interfaceto replace key phrases.\nWe mostly replaced subjects in multiple locations within\na sentence with a single word (e.g., “short” replaced by\n“long”) in the Prompt Interface. The Heat Map was critical\nfor finding predictions not shared between prompts. The\n8\nI den tity  S t er eo typesB Kno wledg e P r obingC\na\n1\nb\nQ uestion * A nsw er\nS tudies ha v e sho wn significan t \nc omplica tion r a t es; it is no t sa f e.\nT her apeutic an tic oagula tion in \nthe tr auma pa tien t: is it sa f e ?\n8 subject r eplac emen ts | 16 pr omp t v aria tions t o tal\n21 subject r eplac emen ts | 84 pr omp t v aria tions t o tal\n25 subject r eplac emen ts | 50 pr omp t v aria tions t o tal\n29 subject r eplac emen ts | 58 pr omp t v aria tions t o tal\n19 subject r eplac emen ts | 57 pr omp t v aria tions t o tal\nT h e r a p e u t i c  a n t i c o a g u l a t i o n  i s  _  f o r    t o  r e c e i v e . t r a u m a p a t i e n t s\nI t  i s  _  f o r    t o  r e c e i v e  . t r a u m a p a t i e n t s t h e r a p e u t i c  a n t i c o a g u l a t i o n\npa tien ts\ntr au m a\nt h er apeuti c  an ti c oa g u l a tion an tic oagula tion ther ap y\nmale /  f emale /  middle ag ed\nhumans\n*  J in e t al. 2019 “P ub M ed Q A ”  | **  N ossa e t al. 2021 “HONE ST ” ,  N ossa e t al. 2022 “H arm ful S en t enc e C omple tion ”  | ***  D hamala e t al. 2021 “BOLD”  | ****  P e tr oni e t al. 2019 \"L anguag e models as k no wledg e bases ? \"\n12 subject r eplac emen ts | 24 pr omp t v aria tions t o tal\na\n2\nb\nQ uestion * A nsw er\nS tudies ha v e c on firmed \ntheir  e ff ectiv eness.\nA  short sta y  in a g ener al childr en ' s \nhospital: ar e the y  e ff ectiv e ?\nA    i n  a     i s  _  t o  . s h o r t s t a y g e n e r a l c h i l d r e n ' s h o s p i t a l t a k e\nI t  i s  _  t o   a    i n  a    . t a k e s h o r t s t a y g e n e r a l c h i l d r e n ' s h o s p i t a l\ng ener a l\nsta y\ns h ort long\nw ar d\nacademic ta k e\nh osp i ta l medical c en t er\nha v e\nchild r en ’ s adult ’ s\nB io m edi c al Kno wledg eA\nT esting di ff er enc es in g r a mm ar and p h r as i n g\nS ci BER T  ( 2019 )  v s P u b M e d BER T  ( 2021 ) BER T  ( 2018 )  v s R o BER T a ( 2019 )\nB i nar y\nL GB T Q A I+\nw oman daugh t er f emale man son male ,  ,  ,  ,  ,  ,  ,  \n,  ,  \nstr aigh t\nhe t er ose x ual she he\nper son non - binar y tr ans g ender q ueer,  ,  ,  ,  ,  \n,  ,  ,  ,  ,  \nga y\nlesbian bise x ual homose x ual the y x e z e\n[ s u b j e c t ]  l i k e s  t o  _ .  /  [ s u b j e c t ]  i s  m e a n t  t o  b e  _ .2\n[ s u b j e c t ]  i s  a  g r e a t  _ .  /  [ s u b j e c t ]  i s  k n o w n  a s  a  _ .1\nG en d er ,  ,  an d   **O r i en ta tion P r onouns\nR a c e\nP o li ti c s\nM a n y  e v e n  a t t r i b u t e  [ s u b j e c t ]  f o r  b e i n g  _ .\nA sian A mericans ,  A frican A mericans ,  E ur opean \nA mericans ,  H ispanic and L a tino A mericans\nC ommunism ,  S ocialism ,  D emocr ac y ,  L iber alism ,  \nC onserv a tism ,  N a tionalism ,  A nar chism ,  F ascism\nR e ligi on J udaism ,  C hristianit y ,  I slam ,  H induism ,  B uddhism ,  \nS i k hism ,  A theism\n3\nO n e  f u n d a m e n t a l  m o r a l  q u a l i t y  i n / o f  [ s u b j e c t ]  i s  _ .4\nT h e  p e o p l e  o f  [ s u b j e c t ]  a r e  o f t e n  k n o w n  f o r  _ .5\nR a c e  an d  /  I d eo l o gi es ***R e ligi ous P o li ti c a l\nD isc o v ering un d err epr esen t e d  biases C omparing model si z e f or  learning c on c ep ts\n1\n2\nM e m b er s hi p ( B e l on g s / C auses )  ****\nY o u  a r e   t o   a  t h i n g  i n  a  _ . l i k e l y f i n d\nO n e   o f  d o i n g  i s   _ .e f f e c t f e e l i n g\nfi n d\nlik e ly unli k el y\nsee ,  loca t e\nd o i n g\nt hi n g sna k e ,  ca t ,  k eepsa k e ,  \nheirloom ,  idea ,  str a t eg y\nsuc c eeding ,  f ailing ,  \ne x er cising ,  sleeping ,  \nthin k ing ,  w orr y ing\nf ee li n g g e tti ng ,  bec oming\ne ff e c t r esult ,  c onse q uenc e\n3\n4\nC h a i n o f  R eason i n g  ( P r er e q u i s i t es / G oa l s )  ****\nY o u   b e  t h i s  b e c a u s e  y o u   _ .c o u l d a r e\nI f  y o u   d o  t h e n  y o u   a  _ .w a n t  t o n e e d\nar e\nc ou ld should ,  w ould\nw an t ,  will ,  migh t\nnee d\nw an t t o should ,  must\nw an t ,  li k e ,  disli k e\nd o\nt hi s happ y ,  sad ,  righ t ,  wr ong ,  \nhealth y ,  sic k\ndriv e ,  fl y ,  suc c eed ,  f ail ,  \ndisc o v er ,  learn ,  cr ea t e\nBER T  ( 2018 )  v s D i sti l BER T  ( 2019 )\nFig. 3. Data sets and prompts used in our use cases (Sect. 5) to show howKnowledgeVIS can help NLP researchers and engineers interpret LLMs.\nSet Viewwas also useful for understanding ranking patterns\nthat the Heat Map cannot represent, such as when different\nprompts exhibit similar prediction sets but differently or-\ndered. The logarithmic scale was mostly used, as the models\ngenerally returned few highly probable predictions.\nFor PubMedBERT, key phrases and synonyms changed\nrecommendations regardless of the context of the sentence.\nFor example, the Heat Mapshowed missing entries between\ntherapeutic anticoagulation (“ideal”, “significant”, “imper-\native”, “standard”) and anticoagulation therapy (“costly”,\n“expensive”, “complex”), while the Set Viewshowed some\npositive associations for “patients” and not for “humans”.\nPubMedBERT also associated “long” with sets of words in-\ncluding “expensive”, “dangerous” and “bad”, while “short”\nwas “safe”, “convenient”, “simple”. This association persists\neven when other phrases change, raising concerns that\nthe model is not recognizing important syntax and only\nfocusing on “short”/“long”, which may be a consequence\nof its training on PubMed articles. Thus, PubMedBERT\ncan be good for general-purpose recommendations where\ngrammar is consistent with the training data. Where key\nphrases change or are important to consider, it may be hard\nfor PubMedBERT to unlearn certain strong associations.\nFor SciBERT, key phrase changes are generally ignored\nwhile the context and grammar of the sentence more heavily\nchange predictions than with PubMedBERT. For example,\nwhere SciBERT is consistent (i.e. most rows filled in the\nHeat Map) across subject replacements for 1a, it is sim-\nilarly consistent yet opposite for 1b (“recommended” vs\n“not”). Similarly, SciBERT finishes the sentence for 2a with\n“difficult”, “easy”, “hard”, and “simple” across almost all\nvariations, not making any recommendation, while using\n“take” in 2b results in recommendations like “able”, “possi-\nble”, “required” and “likely”. Sentence phrasing affects how\nmuch SciBERT recommends something. This could make\nSciBERT good for learning associations on key phrases, but\nharder to adapt to new grammars and phrasings.\nOverall, while both models are susceptible to grammar\nand phrasing issues, PubMedBERT tends to associate rec-\nommendations with certain key phrases, whereas SciBERT\nbases it on the word order.\n5.2 Use Case: Identity Stereotypes\nHow can important yet underrepresented identity stereo-\ntypes be discovered in general-purpose LLMs? In Fig. 3B,\nwe modified prompts from the HONEST [52], [53] data set\n(1 and 2) for measuring gender, orientation, and pronoun\nbiases between binary and LGBTQIA+ communities. We\nalso modified prompts from the BOLD [54] data set (3, 4,\nand 5) to further investigate United States racial, religious,\nand political identity stereotypes. Importantly, this use case\nonly highlights a subset of identities and doesn’t account for\nintersectionality [55] (i.e., identifying with multiple groups);\nfuture work should investigate intersectional identity com-\npletions in masked language models.\nIn contrast to the previous use case, we primarily re-\nplaced subjects in a single location within a sentence with\nmultiple words (e.g., “woman”, “daughter”, etc.) and used\nrank-order views and set membership in the Set View and\nScatter Plot. Using a linear scale, we saw few predictions\nmore likely than others and high probability, making rank-\ning an important metric to distinguish behavior. We used the\nSet Viewand Scatter Plotto surface shared sets of predictions\nand find unique predictions that stand out, e.g., specific\npredictions unique to a single set, or a prediction that is\nhigher for a prompt than the rest. The shared/unique filters\nwere critical for reducing the prediction space in the Set\nView, such as when few/many predictions are shared and\nunique predictions reveal shared prompt groups.\nGender, orientation, and pronouns. For BERT, as ex-\npected, binary labels were more often associated with gen-\nder norms and positivity compared with LGBTQIA+ labels\nbeing misclassified with stereotypical and negative excep-\ntions (e.g., “beautiful” and “admired”, versus “different”\nand “temporary”). For RoBERTa, we found bias in associ-\nations with morality and gender norms to be less frequent\nand isolated overall. Yet one unexpected and concerning\nassociation in both models (i.e. high ranking and shared\nrelationships in the Set Viewand Scatter Plot) is “lesbians”,\n“women”, and “female” with many LGBTQIA+ labels.\nCould more LGBTQIA+ content be associated with or writ-\nten for women, or is this a more ingrained misclassification\nbias? Why is this association not debiased in RoBERTa given\n9\nits performance with other labels? Further, the association\nof LGBTQIA+ labels and morality, particularly themes ob-\nserved such as “evil” and “sin”, is then more likely to\nrefer to women than men. It is unclear in what direction\nthe association between woman, LGBTQIA+ labels, and\nmorality is targeted. Another unexpected association was\nmade between men, sports and sexuality in RoBERTa. In-\nterestingly, the inclusion of heterosexual/homosexual labels\nreveals associations with “athletes”, “coaches”, “players”,\nand “leaders”. This could reveal a mechanism of the training\nsurfacing to debias identity labels missing a particular asso-\nciation. Finally, we observed difficulties in both models with\nunderstanding LGBTQIA+ pronouns at all, which could\npoint to an unintentional bias with little training data to\nsupport masked language modeling.\nRace, religion, and politics. For BERT, given its poor\nperformance on gender, orientation, and pronoun labels, we\nwere surprised to find very few negative associations with\nrace, religion, and politics overall. Yet we found Hispanic\nand Latino Americans had very few unique associations (i.e.\nlow probability and variability in the Set View) compared\nwith other labels; it is likely that Hispanic and Latino\nAmericans are underrepresented in BERT’s training. For\nRoBERTa, we found a higher rate of bias and negative\nassociations (i.e. high probability and variability in the Set\nView) across underrepresented groups in the United States,\nsuch as Asian Americans with “bullying”, “discrimination”;\nHispanic and Latino Americans with “gangs”, “homeless-\nness”; and African Americans with “slavery”, “hardship”.\nRoBERTa also exhibited strong biases in attributes, moral\nqualities, and affiliations between two different groups of\nreligions. In the Scatter Plot, Islam/Hinduism/Christianity\nshared many points associated with marriage and morality\ncompared to Judaism/Buddhism/Sikhism with peace and\nservice (“polygamy”/“patriarchy”/“evil”/“oppressive” vs\n“tolerant”/“strong”/“good”/“compassion”). It is surpris-\ning that RoBERTa has such ingrained stereotypes given the\ndebiasing shown in juxtaposition with otherwise stereo-\ntypical predictions from BERT. Interestingly, moral quali-\nties of political ideologies were divided in RoBERTa into\nshared qualities between groups. Using the Scatter Plot,\nwe identified associations along shared edges (Anarchism,\nFacism and “violence”; Facism, Communism and “weak”,\n“evil”; Fascism, Conservatism and “arrogance”; National-\nism, Conservatism and “loyalty”; Conservatism, Liberalism\nand “tolerance”, “moderate”). RoBERTa also frequently sug-\ngested Communism is “Jewish” (i.e. most rows filled in the\nHeat Map), relating two identities and suggesting learned\nintersectional biases may exist, though overlapping identity\nassociations were otherwise rarely seen in both models.\n5.3 Use Case: Knowledge Probing\nHow well do LLMs learn complex relationships at different\nmodel scales? In Fig. 3C, we created templates to elicit\nassociations with open-ended conceptual prompts and var-\nious subject replacements that test reasoning capabilities for\nmembership (1 - belongs and 2 - causes) and chain of\nreasoning (3 - prerequisites and 4 - goals), based on the\nLAMA [23] knowledge probe.\nWe replaced subjects in multiple locations within a sen-\ntence with multiple words (e.g., “effect” and “feeling” in\nthe same sentence) and used the Scatter Plot to observe\nhigher-level patterns across prompts, helping us identify\nclusters of labels. Some labels sit on the line between two\nsubjects, which was very interesting. Prediction clusters also\nprovided evidence of learned semantic understanding of\nknowledge where subject associations matched their pre-\ndiction hypernym labels (e.g., snake/cat produced mostly\n“physical entity” predictions, while strategy/idea produced\nmostly “abstraction” predictions). We also used the Set View\nselected word rank view (Fig. 2) to see how predictions\ncompare across prompts, when some are higher/lower, lists\nthat do not match, lists that are offset slightly, etc.\nFor BERT, associations are mostly unique and relevant\nto the subject replacements across all prompts. Membership\nis highly dependent on the subject (i.e. the unique filter\nshows most rows in both the Heat Map and Set View).\nFor example, we saw differences between where you find,\nlocate and see things (e.g., “drawer” vs “building” vs\n“dream”, respectively), or when feeling, getting or becom-\ning (e.g., “satisfied” vs “older” vs “greater”, respectively).\nRelationships can also be positive or negative – consequence\nproduces negative associations while result/effect share\ncommon positive associations (e.g., “powerless”/“bad” and\n“good”/“desired”, respectively). BERT also understands\nconceptual pairs (i.e. groups of predictions sharing an edge\nin the Scatter Plot). Snake/cat are animals found in a “park”\nor “garden”; heirloom/keepsake are objects in a “museum”\nor “collection”; a strategy/idea are found in a “story” or\n“job”. The predictions followed their subject hypernym clus-\nters (e.g., snake/cat produced mostly “physical entity” pre-\ndictions, strategy/idea produced mostly “abstraction” pre-\ndictions, while keepsake/heirloom were mixed). Chain of\nreasoning prompts produced similar results. BERT showed\nunique and relevant prerequisites (i.e. few shared connector\nlines in the Set View) for healthy/sick such as “hungry”,\n“tired” or “pregnant”, happy/sad such as “alone” or “here”\nand right/wrong such as “stubborn”, “smart” or “blind”.\nTop predictions for goals such as drive and fly are cor-\nrect (e.g., “car”/“map” and “pilot”/“wings”, respectively).\nSucceed and fail are opposite (“plan”/“strategy” vs “dis-\ntraction”/“reason”), while discover, learn, and create are all\nassociated with “teachers”, “lessons”, and “partners”. BERT\ndemonstrates a strong understanding of complex reasoning\nacross both membership and chain of reasoning examples.\nFor DistilBERT, we found strong performance similar to\nBERT in making associations for belongs and goals, such\nas similar pair associations between snake/cat, drive/fly\nand discover/learn/create in the Scatter Plot. We noticed\nassociations were mostly noun-based and did not follow our\nscheme of separating membership and chain-of-reasoning.\nHowever, DistilBERT failed to make interesting or useful as-\nsociations (i.e. the shared filter shows most rows in both the\nHeat Map and Set View) for different causes or prerequisite\nprompts, which are generally verb-based associations. This\nsuggests that DistilBERT, and potentially other small-scale\nmodels, may only capture and represent noun relationships\nwhile struggling to capture verb relationships.\nWe also noticed biases, such as BERT exhibiting learned\nassociations in both chain of reasoning prompts with female\n10\ngender labels. This appeared in numerous associations of\n“women” being wrong more than right, in “pregnancy”\nbeing a common prediction across all prerequisite prompts,\nand in goal prompts where to do something you need a\n“woman”, “mother”, “wife” or “girl”. This is highly con-\ncerning for how prevalent this association is despite the\nnumerous subject replacements and prompt variations we\ntested. Interestingly, DistilBERT does not exhibit these same\nbiases, despite strong noun-based subject predictions.\n5.4 Expert Evaluation\nWe recruited six academic NLP researchers and engineers\n(P1 − 6) to verify whether KnowledgeVIS was helpful, in-\ntuitive, and insightful for interpreting BERT-based language\nmodels. The experts’ work spanned linguistics and language\nmodeling, cluster and discourse analysis, text classification\nand regression, and domain applications such as learning\nsciences and medical data. They all had familiarity with\neither (1) training new transformers or (2) adapting existing\ntransformers for downstream tasks. Participants received\na brief demonstration of how KnowledgeVIS works before\nfreely exploring the interface. Before and after exploration,\nwe conducted semi-structured interviews to understand\neach participant’s background and their findings when us-\ning the interface. We indicate when participant feedback is\nfrom examining a use case using the example buttons in\nthe Prompt Interfaceand when feedback is from an example\nthey created. We structured feedback around (1) insights\nthat experts gained; (2) the usefulness of the visualizations;\nand (3) future applications of KnowledgeVIS.\nInsights. The experts described several interesting and\ninsightful findings from using KnowledgeVIS. P5 tested the\nsensitivity of the general-domain models (BERT, RoBERTa,\nand DistilBERT) to grammar and rephrasing when test-\ning different subjects with the prompt “The [subject] ate\nthe/several .”. They learned that the models mostly re-\nspected both parts of speech and transitivity, e.g., correctly\npredicting singular and plural foods; however, they also\nfound the same models struggled with semantic roles,\ne.g., predicting that both cows and wolves eat meat: “The\nmodel isn’t really looking at the syntax. It’s just looking\nat the words.’ P 4 discovered potentially biased religious\nassociations when testing BERT and DistilBERT for bias\nwith the adage, “A [subject] a day keeps the away.”\nThey found using more common Western fruits as subjects\n(e.g., apples and bananas) led to predicted words with\npositive associations to the fruit such as “gospel”, “wine”,\nand “god” while less common fruits (e.g., durians) led to\nnegative associations such as “demons”, “plagues”, and\n“apocalypse”. Because both models returned similar results,\nP4 surmised that a lack of training examples for subjects\nlike durian, and not the training mechanism, was the issue,\nsuggesting this as a way to fix this error. P 2 used the\nPrompt Interface to isolate and replace keywords, such “IV\ntubes” and “hospital patients”, for testing the robustness\nof SciBERT and PubMedBERT in the biomedical knowledge\nuse case. They found that SciBERT, which uses a custom\nwordpiece vocabulary (scivocab), was more consistent and\naccurate in recommendations than PubMedBERT for key-\nwords related to the vocabulary. Given the examples came\nfrom PubMedQA, they said, “I would expect PubMedBERT\nto be more reliable based on its training.” P 3 investigated\nthe biomedical knowledge use case as well and made two\nimportant interpretations about how these models are work-\ning: (1) common grammar mistakes are prevalent, such as\nthose made by second language English speakers; and (2)\nnegative associations are rare in general.\nVisualizations. The experts highlighted the usefulness of\nthe Prompt Interface for making connections between the\nsubject and blank in the sentence more obvious and insight-\nful. P 3 found the clustering of predictions working better\nthan expected in the knowledge probing use case; inspired,\nthey iteratively added more nouns, verbs, and relationships\nto get increasingly larger and more diverse semantic group-\nings. While it took some time to learn, the complexity and\ndiversity of information in the visualizations allowed par-\nticipants to answer different questions with each plot. For\nexample, P 6 both complemented and critiqued the Scatter\nPlot for making interesting yet potentially spurious correla-\ntions more salient, suggesting that a minimum number of\nprompts may be needed for higher confidence. P 1 praised\nthe “logical progression” of the plots, from the least complex\nHeat Mapon the left to the most complex Scatter Ploton the\nright, for helping them intuitively unpack the complexity of\nthe data in increasing amounts of detail.\nApplications. After uncovering insights, several experts\nwanted to use KnowledgeVIS as a “launch pad” for exploring\nmodel differences in their own work. P 3 wanted to “chal-\nlenge the best performing models on HuggingFace” against\ntheir own work analyzing large data sets for language\nacquisition, using KnowledgeVIS to immediately visualize\nconcepts and rapidly test which models perform better out-\nof-the-box. P2 uses BERT-based models for natural language\nunderstanding (NLU) tasks within discourse analysis, such\nas identifying individual speakers by classification. After\ninvestigating the effects of keywords on model predictions\nin the biomedical knowledge use case, they suggested using\nKnowledgeVIS for testing domain-specific prompts such as\n“Force equals mass times .” with LLMs trained on speech\ntranscriptions in physics lectures and textbooks. All partic-\nipants felt KnowledgeVIS was most useful to anyone inter-\nested in understanding LLMs by “opening the black box of\nhow they work”, especially for rapid qualitative evaluation.\nP3 suggested that NLP teachers could use KnowledgeVIS\nto demonstrate vocabulary and grammar structures. P 1\nhighlighted how KnowledgeVIS shows dense information\non a single page “very succinctly”, which can help model\nengineers easily investigate ethical performance factors such\nas harmful biases before deploying their models.\n6 D ISCUSSION\n6.1 Closing the NLP Loop\nTwo of the biggest challenges in human-in-the-loop NLP are\n(1) how to create and group effective prompts and (2) what\nto do once insights are discovered during model evaluation.\nOur use cases and expert evaluation suggest several ways\nKnowledgeVIS could help mitigate uncovered biases and\nerrors in the future, towards closing the loop.\n11\nOne solution is to create prompts as test cases to aug-\nment training data. Using the Prompt Interface, users can\nsystematically test for limitations on what has been learned,\neither when training data is scarce or a particular concept\nis not well represented in the corpus. For example, in our\nuse cases we found a lack of Hispanic and Latino Amer-\nican representation as well as difficulties in recognizing\nLGBTQIA+ pronouns by creating and grouping prompts\nthat varied identity phrases as subjects. Researchers can also\ntest for sensitivity to common text dimensions such as parts\nof speech, transitivity, and semantic roles. Based on P 3’s\ninterpretations of the biomedical use case, more examples\nof both negative recommendations and diverse grammar\npatterns in the training data would likely make both models\nmore robust. To overcome “cold start” difficulties in gen-\nerating prompts, we provide several example buttons that\ndemonstrate a variety of test cases. We discuss future work\nin automatically generating prompts in Sect. 6.3.\nAnother is to help researchers narrow the initial se-\nlection of models. Our evaluation of differences between\nmodels led to important findings. We found SciBERT was\nmore sensitive to changes in context and grammar, while\nPubMedBERT was more sensitive to subject replacements.\nComparing BERT against DistilBERT for learning complex\nreasoning, we realized DistilBERT had trouble with verb re-\nplacements, but could handle noun replacements well, even\nat its smaller model size. By comparing prompt variations\nbetween models using a drop-down list, researchers can\nquickly gain an understanding of the trade-offs of different\npre-trained models and when their use case may be a better\nfit for what a given model is already good at.\nFinally, discovering unexpected yet important concepts\nand patterns can suggest future training ideas. The Set\nView and Scatter Plot were effective for revealing uncom-\nmon associations by grouping predictions, such as be-\ntween women, LGBTQIA+ labels, and morality, or be-\ntween the groups of Islam/Hinduism/Christianity and Ju-\ndaism/Buddhism/Sikhism. P2 suggested that for domain-\nadapted models, KnowledgeVIS can help rapidly generate\na variety of test cases for specific learned concepts; e.g., a\nphysics model testing concepts such as “Force times mass\nequals .” Our human in the loop approach enables experts\nto identify patterns quantitative metrics can miss and re-\nevaluate the same models before deploying them.\n6.2 Improving Human-LM Interaction\nAs new advancements in machine learning for NLP emerge,\nthere is an opportunity for visual analytics tools to support\nhuman-in-the-loop evaluation of LLM performance [13]\nwhere quantitative benchmarks fall short [12].KnowledgeVIS\nmakes LLMs more interpretable by guiding the user to\nexplore the space of predictions in different ways. Typically\nmachine learning models are quantitatively benchmarked\nagainst gold standards for precision and recall. This brutally\nobjective methodology misses an opportunity for injecting\nhuman intuition and domain expertise into the iterative pro-\ncess of training and validating model performance. Knowl-\nedgeVIS does this by presenting patterns of model output\nat a higher level, to gain insight through repeated measures\nrather than one-shot explanations, and using natural lan-\nguage tasks to show how the model performs. We believe\nthis is a more human-centered approach to interpreting\nLLMs in an area dominated by tools that focus on showing\nmore features, making the model more complex.\nIn general, there is a challenge of making deep learning\nmodels in machine learning interpretable. KnowledgeVIS ad-\ndresses what is being visualized (model predictions), for who\n(model users), and why (interpretability) to overcome limi-\ntations around prerequisite background knowledge in deep\nlearning needed. For example, Hohman et al. [24] use a table\nof terminology in their visual analytics for deep learning\nsurvey to preface what is being visualized for interpreting\nhow deep learning models work. They explicitly describe\nmodel usersthat develop and train domain-specific, smaller-\nscale models and applications and “often download pre-\ntrained model weights online to use as a starting point” [24].\nConsider, for example, an expert in linguistics developing a\ntransformer-based approach to sentiment classification for\nanalyzing historical documents. Because model users are\nnot always experts in deep learning and yet are increasingly\nusing LLMs for downstream tasks, there can be severe\nconsequences [6] without accessible tools that overcome bar-\nriers to communicating what LLMs have learned. We seek to\nbridge the gap needed to interpret deep learning terminol-\nogy and how transformer-based language models work by\nshowing how the model performs on the downstream tasks\nof interest to model users. KnowledgeVIS empowers users to\nexplore their data in the way they think about it.\n6.3 Limitations and Future Work\nWhile in this paper we focus on eliciting and evaluat-\ning semantic and commonsense knowledge, other types of\nknowledge exist (e.g., syntactic, linguistic) [4]. We could\nextend our approach by representing parts of speech (POS)\nor semantic roles in various ways; e.g., by visualizing the\nsyntactic tree structure of the prompt or by labeling words\nby POS and/or role, in addition to their semantic cluster.\nUsers could also directly annotate visualizations; e.g., man-\nually grouping predicted tokens and assigning context in\none plot that can be viewed in another plot. This could\nextend to new visualizations that help users directly com-\npare probabilities for a few words at a time, and between\ngroups of words. Additionally, to overcome “cold start”\nissues in creating and grouping new prompts, we could\nuse generative LLMs to provide related concepts for existing\nprompts by seeding sentences with topic keywords. How-\never, while we focus on prompting as the method of eliciting\nknowledge and visualizations for evaluating prompts, more\nwork is needed to understand the limitations of prompts as\nsources of interpretability [3]. For example, it is unclear what\nthe effects of grammar are on model performance. Finally,\nwe designed our visualizations to reasonably support up\nto 10 prompts at a time and found it sufficient to return\nbetween k = 30 and k = 200 top predicted tokens at one\ntime. The vocabulary of some LLMs, however, can extend\nbeyond hundreds of thousands of tokens [2]. It is unclear\nhow scaling our approach by visualizing more prompts and\npredictions at the same time might affect the exploration\nand discovery process, positively or negatively.\n12\n7 C ONCLUSION\nAs transformer-based large language models (LLMs) con-\ntinue to improve in their ability to help people answer ques-\ntions, generate essays, and more, it is critical for researchers\nto interpret how and why they work, in order to build better\nmodels that reduce bias, mitigate stereotypes, learn domain-\nspecific knowledge, etc. In this work, we presented Knowl-\nedgeVIS, a visual analytics system for discovering learned\nassociations in LLMs. By intuitively formatting multiple\nsentences as prompts and visualizing predictions across\nseveral coordinated views, KnowledgeVIS reveals learned\nassociations for different types of relationships between\npredictions that help researchers interpret how the LLM is\nperforming. KnowledgeVIS demonstrates several capabili-\nties such as eliciting sensitive medical domain knowledge,\nuncovering harmful stereotypes, and probing complex rea-\nsoning capabilities. Combined with expert feedback that\nvalidates the effectiveness and usability of the system, we\nbelieve KnowledgeVIS contributes to a growing area of visu-\nalization for LLM interpretability.\nACKNOWLEDGMENTS\nSupport provided by NSF IIS-1750474 and DRL-2247790.\nREFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers). Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019,\npp. 4171–4186.\n[2] T. Brown et al., “Language models are few-shot learners,” in\nAdvances in Neural Information Processing Systems, H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.\nCurran Associates, Inc., 2020, pp. 1877–1901.\n[3] A. Srivastava et al., “Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models,” Transactions on\nMachine Learning Research, 2023.\n[4] A. Rogers, O. Kovaleva, and A. Rumshisky, “A primer in BERTol-\nogy: What we know about how BERT works,” Transactions of the\nAssociation for Computational Linguistics, vol. 8, pp. 842–866, 2020.\n[5] A. Roberts, C. Raffel, and N. Shazeer, “How much knowledge can\nyou pack into the parameters of a language model?” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP). Online: Association for Computational\nLinguistics, Nov. 2020, pp. 5418–5426.\n[6] A. Abid, M. Farooqi, and J. Zou, “Persistent anti-muslim bias\nin large language models,” in Proceedings of the 2021 AAAI/ACM\nConference on AI, Ethics, and Society, ser. AIES ’21. New York, NY,\nUSA: Association for Computing Machinery, 2021, p. 298–306.\n[7] P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\ntrain, prompt, and predict: A systematic survey of prompting\nmethods in natural language processing,” ACM Comput. Surv.,\nvol. 55, no. 9, jan 2023.\n[8] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How Can We Know\nWhat Language Models Know?” Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 423–438, 07 2020.\n[9] T. Shin, Y. Razeghi, R. L. Logan IV , E. Wallace, and S. Singh,\n“AutoPrompt: Eliciting Knowledge from Language Models with\nAutomatically Generated Prompts,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing\n(EMNLP). Online: Association for Computational Linguistics,\nNov. 2020, pp. 4222–4235.\n[10] G. Qin and J. Eisner, “Learning how to ask: Querying LMs with\nmixtures of soft prompts,” in Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. Online: Association for\nComputational Linguistics, Jun. 2021, pp. 5203–5212.\n[11] Z. Zhong, D. Friedman, and D. Chen, “Factual probing is [MASK]:\nLearning vs. learning to recall,” inProceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. Online: Association for\nComputational Linguistics, Jun. 2021, pp. 5017–5033.\n[12] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,\n“On the dangers of stochastic parrots: Can language models be\ntoo big?” in Proceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, ser. FAccT ’21. New York, NY,\nUSA: Association for Computing Machinery, 2021, p. 610–623.\n[13] Z. J. Wang, D. Choi, S. Xu, and D. Yang, “Putting humans in the\nnatural language processing loop: A survey,” in Proceedings of the\nFirst Workshop on Bridging Human–Computer Interaction and Natu-\nral Language Processing. Online: Association for Computational\nLinguistics, Apr. 2021, pp. 47–52.\n[14] A. Warstadt et al., “Investigating BERT’s knowledge of language:\nFive analysis methods with NPIs,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). Hong Kong, China: Association for Computa-\ntional Linguistics, Nov. 2019, pp. 2877–2887.\n[15] Y. Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy,\nH. Sch ¨utze, and Y. Goldberg, “Measuring and Improving Con-\nsistency in Pretrained Language Models,” Transactions of the Asso-\nciation for Computational Linguistics, vol. 9, pp. 1012–1031, 12 2021.\n[16] C. Collins, F. B. Viegas, and M. Wattenberg, “Parallel tag clouds to\nexplore and analyze faceted text corpora,” in2009 IEEE Symposium\non Visual Analytics Science and Technology, 2009, pp. 91–98.\n[17] J. S. Yi, R. Melton, J. Stasko, and J. A. Jacko, “Dust & magnet:\nMultivariate information visualization using a magnet metaphor,”\nInformation Visualization, vol. 4, no. 4, pp. 239–256, 2005.\n[18] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language\nmodel for scientific text,” arXiv preprint arXiv:1903.10676, 2019.\n[19] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Nau-\nmann, J. Gao, and H. Poon, “Domain-specific language model\npretraining for biomedical natural language processing,” ACM\nTrans. Comput. Healthcare, vol. 3, no. 1, oct 2021.\n[20] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[21] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” arXiv preprint\narXiv:1910.01108, 2019.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.\n[23] F. Petroni, T. Rockt ¨aschel, S. Riedel, P . Lewis, A. Bakhtin, Y. Wu,\nand A. Miller, “Language models as knowledge bases?” in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP). Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 2463–\n2473.\n[24] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau, “Visual analytics\nin deep learning: An interrogative survey for the next frontiers,”\nIEEE Transactions on Visualization and Computer Graphics, vol. 25,\nno. 8, pp. 2674–2693, 2019.\n[25] Z. J. Wang, R. Turko, O. Shaikh, H. Park, N. Das, F. Hohman,\nM. Kahng, and D. H. Polo Chau, “Cnn explainer: Learning con-\nvolutional neural networks with interactive visualization,” IEEE\nTransactions on Visualization and Computer Graphics, vol. 27, no. 2,\npp. 1396–1406, 2021.\n[26] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[27] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[28] H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer, H. Pfister, and\nA. M. Rush, “Seq2seq-vis: A visual debugging tool for sequence-\nto-sequence models,” IEEE Transactions on Visualization and Com-\nputer Graphics, vol. 25, no. 1, pp. 353–363, 2019.\n[29] C. Wang, A. Jain, D. Chen, and J. Gu, “VizSeq: a visual anal-\nysis toolkit for text generation tasks,” in Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\n13\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP): System Demonstrations. Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 253–\n258.\n[30] I. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen,\nS. Gehrmann, E. Jiang, M. Pushkarna, C. Radebaugh, E. Reif, and\nA. Yuan, “The language interpretability tool: Extensible, interac-\ntive visualizations and analysis for NLP models,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations. Online: Association for Com-\nputational Linguistics, Oct. 2020, pp. 107–118.\n[31] J. Vig, “A multiscale visualization of attention in the transformer\nmodel,” in Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations. Florence,\nItaly: Association for Computational Linguistics, Jul. 2019, pp. 37–\n42.\n[32] B. Hoover, H. Strobelt, and S. Gehrmann, “exBERT: A Visual\nAnalysis Tool to Explore Learned Representations in Transformer\nModels,” in Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics: System Demonstrations. Online:\nAssociation for Computational Linguistics, Jul. 2020, pp. 187–196.\n[33] J. F. DeRose, J. Wang, and M. Berger, “Attention flows: Analyzing\nand comparing attention mechanisms in language models,” IEEE\nTransactions on Visualization and Computer Graphics, vol. 27, no. 2,\npp. 1160–1170, 2021.\n[34] Z. J. Wang, R. Turko, and D. H. Chau, “Dodrio: Exploring Trans-\nformer Models with Interactive Visualization,” in Proceedings of\nthe Joint Conference of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference\non Natural Language Processing: System Demonstrations. Online:\nAssociation for Computational Linguistics, 2021, pp. 132–141.\n[35] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning, “What does\nBERT look at? an analysis of BERT’s attention,” inProceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP. Florence, Italy: Association for Computational\nLinguistics, Aug. 2019, pp. 276–286.\n[36] S. Jain and B. C. Wallace, “Attention is not Explanation,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers). Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019,\npp. 3543–3556.\n[37] P . Atanasova, J. G. Simonsen, C. Lioma, and I. Augenstein, “A\ndiagnostic study of explainability techniques for text classifica-\ntion,” in Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP). Online: Association for\nComputational Linguistics, Nov. 2020, pp. 3256–3274.\n[38] H. Strobelt, A. Webson, V . Sanh, B. Hoover, J. Beyer, H. Pfister, and\nA. M. Rush, “Interactive and visual prompt engineering for ad-hoc\ntask adaptation with large language models,” IEEE Transactions on\nVisualization and Computer Graphics, vol. 29, no. 1, pp. 1146–1156,\n2023.\n[39] H. Strobelt, B. Hoover, A. Satyanaryan, and S. Gehrmann, “LMdiff:\nA visual diff tool to compare language models,” in Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations . Online and Punta Cana,\nDominican Republic: Association for Computational Linguistics,\nNov. 2021, pp. 96–105.\n[40] T. Wolf et al., “Transformers: State-of-the-art natural language pro-\ncessing,” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations. Online:\nAssociation for Computational Linguistics, Oct. 2020, pp. 38–45.\n[41] J. H. Lau, K. Grieser, D. Newman, and T. Baldwin, “Automatic\nlabelling of topic models,” in Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language\nTechnologies - Volume 1, ser. HLT ’11. USA: Association for\nComputational Linguistics, 2011, p. 1536–1545.\n[42] Z. Wu and M. Palmer, “Verbs semantics and lexical selection,” in\nProceedings of the 32nd Annual Meeting on Association for Computa-\ntional Linguistics, ser. ACL ’94. USA: Association for Computa-\ntional Linguistics, 1994, p. 133–138.\n[43] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[44] D. M ¨ullner, “Modern hierarchical, agglomerative clustering algo-\nrithms,” arXiv preprint arXiv:1109.2378, 2011.\n[45] J. H. W. Jr., “Hierarchical grouping to optimize an objective func-\ntion,” Journal of the American Statistical Association, vol. 58, no. 301,\npp. 236–244, 1963.\n[46] B. J. Frey and D. Dueck, “Clustering by passing messages between\ndata points,” Science, vol. 315, no. 5814, pp. 972–976, 2007.\n[47] P . J. Rousseeuw, “Silhouettes: A graphical aid to the interpretation\nand validation of cluster analysis,” Journal of Computational and\nApplied Mathematics, vol. 20, pp. 53–65, 1987.\n[48] G. A. Miller, “Wordnet: A lexical database for english,” Commun.\nACM, vol. 38, no. 11, p. 39–41, nov 1995.\n[49] B. B. Bederson, “Fisheye menus,” in Proceedings of the 13th annual\nACM symposium on User interface software and technology, 2000, pp.\n217–225.\n[50] K. A. Olsen, R. R. Korfhage, K. M. Sochats, M. B. Spring, and\nJ. G. Williams, “Visualization of a document collection: The vibe\nsystem,” Information Processing & Management, vol. 29, no. 1, pp.\n69–81, 1993.\n[51] Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu, “PubMedQA:\nA dataset for biomedical research question answering,” in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP). Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 2567–\n2577.\n[52] D. Nozza, F. Bianchi, and D. Hovy, “”HONEST: Measuring hurtful\nsentence completion in language models”,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. Online:\nAssociation for Computational Linguistics, Jun. 2021, pp. 2398–\n2406.\n[53] D. Nozza, F. Bianchi, A. Lauscher, and D. Hovy, “Measuring\nharmful sentence completion in language models for LGBTQIA+\nindividuals,” in Proceedings of the Second Workshop on Language\nTechnology for Equality, Diversity and Inclusion. Dublin, Ireland:\nAssociation for Computational Linguistics, May 2022, pp. 26–34.\n[54] J. Dhamala, T. Sun, V . Kumar, S. Krishna, Y. Pruksachatkun, K.-W.\nChang, and R. Gupta, “Bold: Dataset and metrics for measuring\nbiases in open-ended language generation,” in Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency,\nser. FAccT ’21. New York, NY, USA: Association for Computing\nMachinery, 2021, p. 862–872.\n[55] K. Crenshaw, “Demarginalizing the intersection of race and sex:\nA black feminist critique of antidiscrimination doctrine, feminist\ntheory and antiracist politics,” The University of Chicago Legal\nForum, vol. 1989, no. 1, pp. 139–167, 1989.\nAdam Coscia is a PhD student at Geor-\ngia Tech’s School of Interactive Computing\nand a member of the Visual Analytics Lab.\nHis research interests include Visual Analytics,\nHuman-Computer Interaction, and Explainable\nArtificial Intelligence (AI) with Large Language\nModels and Knowledge Graphs. He received his\nB.S. in Physics. He won the President’s Fellow-\nship for top incoming PhD students.\nAlex Endert is an associate professor at the\nSchool of Interactive Computing, Georgia Tech.\nHe directs the Visual Analytics Lab, which ex-\nplores novel user interaction techniques for vi-\nsual analytics.\n14\nAPPENDIX A\nPREDICTIONS CLUSTERING ALGORITHM DESIGN\nTo help users discover patterns in predictions sets, we aim\nto automatically find and label sets of predictions based\non shared semantic similarity. Our solution algorithmically\ngenerates clusters and allows the user to investigate them\nacross each of the plots in the Predictions View. Here, we\ndescribe our design process and rationale.\nFirst, we seek a method of labeling clusters to more\neffectively communicate what the semantic similarity of\nwords in that clusters is. To determine these labels, we use\na taxonomy-based method implemented in WordNet [48].\nConsider that a majority of the predictions returned by\nthe language model are open-class words such as nouns,\nverbs, etc. WordNet categorizes the senses of words as\nsets of synonyms (synsets) taxonomically according to their\nhypernyms — words with a broad meaning that more\nspecific words fall under. For example, color is a hypernym\nof red. Thus, an informative label for a set of words can\nbe automatically determined by finding the lowest common\nhypernym (LCH) of the set using WordNet.\nNext, we seek a similarity measure to compare words\nbased on their shared hypernyms. We chose Wu-Palmer\nsimilarity [42], as it is a measure based on the taxonomic\ndepth of two senses (synsets) and that of their least common\nsubsumer (i.e. most specific ancestor node, or hypernym).\nWe also experimented with measuring similarity by gener-\nating word vectors [43] but found that Wu-Palmer similarity\ntended to perform better for clustering smaller sets of words\nthat produced a more unique and descriptive LCH.\nFinally, we seek a method for clustering words based\non their similarity measure. Consider that our aim is to\nautomatically find and label sets of semantically related\nwords. This means we need an unsupervised method where\nthe number of clusters is unknown a priori. Thus, we can\nperform hierarchical clustering[44] using Wu-Palmer sim-\nilarity as the distance measure between observations, i.e.,\npredicted words, and an appropriate linkage method, e.g.,\nWard’s method [45]. To determine an optimal number of\nclusters, we can find the maximum silhouette coefficient [47]\nbased on mean intra-cluster and nearest-cluster distance of\nall w words for any number c of clusters 2 ≤ c ≤ w. Affin-\nity propagation [46] is a comparable unsupervised clus-\ntering technique specifically for similarity-based distance\nmeasures that produces similar results for small numbers\nof unique predictions. However, as w grows, we found it\ntended to produce too many clusters with less descriptive\nlabels and required manual tuning of the damping factor\nto keep the number of clusters useful. This can instead be\nsolved with hierarchical clustering by setting a user-defined\nthreshold u on the optimal number of clusters 2 ≤ c ≤ u\nwhile searching for a maximum silhouette coefficient.\nAPPENDIX B\nSET VIEW RANKED SELECT LAYOUT ALGORITHM\nWe designed a novel variation on the parallel tag cloud [16]\nlayout in the Set View(Fig. 4). Selecting a word while sorting\nby rank order transitions the view to a step-wise degree\nof interest list, similar to fisheye menus [49]. We arrange\nk=16\nn=5\n1\n5\n6\n8\n10\n11\n12\n13\n14\n16\n15\n9\n7\n4\n3\n2h t\nh b\nS e t Vie w when selecting a w or d and sorting b y  r ank\nr=6r=9 r=11 r=5\nϕ t = 3/10,  ϕ b = 2/10 ϕ t = 5/10ϕ b = 5/10 ϕ b = 6/10\nFig. 4. The Set View showing our variation on the parallel tag cloud\nlayout, a step-wise degree of interest list based on fisheye menus [49] for\na selected word when sorting by rank, described in Fig. 4.2.3. We query\nthe top k = 16predictions and select “cook” from the resulting view. Our\nuser can quickly see “cook” is ranked lower for the “man”/“boy” subjects\nthan for “woman”/“girl”, showing a slight gender bias in the probability of\nthe occupation occurring, even though it appears for all subjects.\nthe neighborhood of n words above and below the selected\nword in rank order equidistant and, for the remaining words\noutside of that neighborhood, we draw lines above and\nbelow that scale proportionally with the remaining words\nfrom the top and bottom of the list, respectively.\nFor the top k tokens, n neighborhood words, and rank r\nof the selected word in each column, our layout algorithm\nperforms the following operations:\n1) position each occurrence of the selected word along the\nsame horizontal baseline in the center of the plot, set-\nting columns without the selected word to zero opacity;\nand\n2) arrange the neighborhood of ranked words 1 ≤ r±n ≤\nk above and below the selected word top-down in rank-\norder, uniformly spaced.\n3) If r > n+ 1:\na) compute the percentage of remaining words in the\nlist above the selected word ϕt = r−n−1\nk−n−1 ;\nb) compute the remaining height ht from the top of the\nr − n word to the top of the plot; and\nc) draw a line upwards from the top of the r − n word\nof length ht · ϕt.\n4) If r < k− n:\na) compute the percentage of remaining words in the\nlist below the selected word ϕb = k−n−r\nk−n−1 ;\n15\nb) compute the remaining height hb from the bottom of\nthe r + n word to the bottom of the plot; and\nc) draw a line downwards from the bottom of the r + n\nword of length hb · ϕb.\nAn example for k = 16 tokens is shown in Fig. 4. We\nfound that n = 5 neighborhood words revealed enough\ndetails at once while ensuring the line had a reasonable\namount of remaining space to be informative. If the rankr of\nthe selected word is less thann from the top or bottom of the\ncolumn, we arrange the remainingr−1 or k−r words above\nor below the selected word and do not draw lines, as there\nare no remaining words in the list. Additionally, if a word\ndoes not occur in one or several columns, those columns\nare now set to have zero opacity, as they cannot be drawn\nto scale with the new layout. Neighborhood words can be\nselected in this layout, and the algorithm will accordingly\nshift the neighborhood of words up or down as well as\nlengthen and shorten the lines.\nAPPENDIX C\nSCATTER PLOT INITIAL LAYOUT ALGORITHM\nThe Scatter Plotpositions predicted tokens as vectors based\non their probability of occurring for each prompt or “point\nof interest” (POI) in a 2D coordinate space, using a layout\ntechinque derived by Olsen et al. [50].\nThe initial layout of m prompts and all predicted words\nis as follows:\n1) position m POIs at the vertices of an m-sided reg-\nular polygon, calculate the display position pi =\n(x, y) for each POI, and create a POI position vector\nP[p1, p2, ..., pm].\n2) For each unique predicted word D[d1, d2, ..., dm],\nwhere di is the probability of the predicted word oc-\ncurring in prompt pi:\na) combine the two vectors P and D into the set S =\n{(d1, p1), (d2, p2), ...,(dm, pm)}.\nb) If the probability of the predicted word is non-zero\nfor only one prompt pj (∀x where x ̸= j | dx = 0),\nthe final position of D would be on top of pj; do not\nplot.\nc) Otherwise, remove two elements from S, (da, pa) and\n(db, pb), calculate a new score ds = da + db and\nposition ps = ((1−t)·pa,x+t·pb,x, (1−t)·pa,y+t·pb,y),\nwhere t = db\nds\nis the ratio of the distance from pa to\nps, and put the new score/position pair (ds, ps) back\nin S.\nd) If S contains more than one element, repeat (c).\nOtherwise, plot the predicted word D at the final\nremaining ps.\nAPPENDIX D\nSUPPORTING FIGURES FOR USE CASES\nWe demonstrate the capabilities ofKnowledgeVIS for prompt\nengineering and immediate visual analysis of fill-in-the-\nblank sentence predictions with three use cases. Importantly,\nthe results are based on interpreting word probabilities\nusing visual encodings, which presents analytical trade-offs\nwhen choosing scales (linear and log) as well as encodings\n(color, font and marker size). We acknowledge our qualita-\ntive approach may be subject to pre-attentive biases. Here,\nwe present supplemental figures demonstrating the insights\nwe discovered. Please see the full paper for an in-depth\ndiscussion of the implications of our findings.\nFirst, we test grammar and phrasing by formatting\nyes/no/maybe questions from a biomedical question-\nanswer data set, PubMedQA [51], as fill-in-the-blank sen-\ntences and querying SciBERT [18] and PubMedBERT [19].\nSecond, we test for contextualized gender, orientation, pro-\nnoun, race, religious, and political stereotypes between\nBERT [1] and RoBERTa [20] using subsets of the HONEST\n[52], [53] and BOLD [54] data sets. Third, we test whether\ncomplex learned concepts based on the LAMA knowledge\nprobe [23], such as membership (belongs, causes) and chain\nof reasoning (goals, prerequisites), are learned between\nlarge-scale BERT and small-scale DistilBERT [21].\nBiomedical knowledge. How do domain-specific models\ncompare based on robustness to grammar and phrasing\nwhen expert human answers are expected?\nFor PubMedBERT (Fig. 5), the Heat Map shows miss-\ning entries between therapeutic anticoagulation (“ideal”,\n“significant”, “imperative”, “standard”) and anticoagula-\ntion therapy (“costly”, “expensive”, “complex”), as well\nas some positive associations for “patients” and not for\n“humans”. PubMedBERT also associated “long” with sets\nof words including “expensive”, “dangerous” and “bad”,\nwhile “short” was “safe”, “convenient”, “simple”. This\nassociation persists even when other phrases change. For\nSciBERT (Fig. 6), key phrase changes are generally ignored\nwhile the context and grammar of the sentence more heavily\nchange predictions than with PubMedBERT. For example,\nwhere SciBERT is consistent (i.e. most rows filled in the\nHeat Map) across subject replacements for 1a, it is similarly\nconsistent yet opposite for 1b (“recommended”/“required”\nvs “not”). Similarly, SciBERT finishes the sentence for 2a\nwith “difficult”, “easy”, “hard”, and “simple” across almost\nall variations, not making any recommendation, while using\n“take” in 2b results in recommendations like “able”, “possi-\nble”, “required” and “likely”.\nIdentity stereotypes. How can important yet underrep-\nresented identity stereotypes be discovered in general-\npurpose language models?\nFor BERT (Fig. 7), as expected, binary labels were\nmore often associated with gender norms and positivity\ncompared with LGBTQIA+ labels being misclassified with\nstereotypical and negative exceptions (e.g., “beautiful”\nand “admired”, versus “different” and “temporary”).\nDespite its poor performance on gender, orientation,\nand pronoun labels, we were surprised to find very few\nnegative associations with race, religion, and politics\noverall. However, Hispanic and Latino Americans had\nvery few unique associations compared with other labels.\nFor RoBERTa (Fig. 8), we found bias in associations\nwith morality and gender norms to be less frequent\nand isolated overall. Yet we found a higher rate of\nbias and negative associations across underrepresented\ngroups in the United States, such as Asian Americans\n16\nB iomedical Kno wledg e\nP ub M ed BER T (2021)\nT her apeutic an tic oagula tion\nan tic oagula tion ther ap y\npa tien ts\nhumans\nis _ f or  tr auma t o r ec eiv e.\n◆\n• ♥\n■\n◆\n◆\n•\n• •\n♥\n♥\n♥\n■\n■\n•\n■\n♥\n♥\n■ ■\n■\n■\n■\n•\n• •\n•\nA short\nlong\nchildr en ’ s\nadult’ s\ng ener al\nacademic\nsta y  in a is _ t o tak e.•■\nFig. 5. Two Heat Mapsshowing how grammar and phrasing affect PubMedBERT. The glyphs highlight predictions mentioned in the body text.\nwith “bullying”, “discrimination”; Hispanic and Latino\nAmericans with “gangs”, “homelessness”; and African\nAmericans with “slavery”, “hardship”. RoBERTa also\nexhibited strong biases in attributes, moral qualities, and\naffiliations between two different groups of religions. In\nthe Scatter Plot, Islam/Hinduism/Christianity shared many\npoints associated with marriage and morality compared\nto Judaism/Buddhism/Sikhism with peace and service\n(“polygamy”/“patriarchy”/“false”/“evil”/“oppressive” vs\n“tolerant”/“strong”/“good”/“compassion”/“excellence”).\nInterestingly, moral qualities of political ideologies were\ndivided in RoBERTa into shared qualities between groups.\nUsing the Scatter Plot, we identified associations along\nshared edges (Anarchism, Facism and “violence”; Facism,\nCommunism and “weak”, “evil”; Fascism, Conservatism\nand “arrogance”; Nationalism, Conservatism and “loyalty”;\nConservatism, Liberalism and “tolerance”, “moderate”).\nRoBERTa also frequently suggested Communism is\n“Jewish” (i.e. most rows filled in the Heat Map), relating two\ndifferent identities and suggesting learned intersectional\nbiases may exist, though overlapping identity associations\nwere otherwise rarely seen in both models. For both BERT\nand RoBERTa (Fig. 9), we observed an association between\n“lesbians”, “women”, and “female” and many LGBTQIA+\nlabels. Another unexpected association was made between\nmen, sports and sexuality in RoBERTa. The inclusion of\nheterosexual/homosexual labels reveals associations with\n“athletes”, “coaches”, “players”, and “leaders”.\nKnowledge probing. How well do LLMs learn complex\nrelationships at different model scales?\nFor BERT (Fig. 10), associations are mostly unique and\nrelevant to the subject replacements across all prompts.\nWe saw differences between where you find, locate and\nsee things (e.g., “drawer” vs “building” vs “dream”, re-\nspectively), or when feeling, getting or becoming (e.g.,\n“satisfied” vs “older” vs “greater”, respectively). Rela-\ntionships can also be positive or negative – consequence\nproduces negative associations while result/effect share\ncommon positive associations (e.g., “powerless”/“bad” and\n“good”/“desired”, respectively). BERT also understands\nconceptual pairs (i.e. groups of predictions sharing an edge\nin the Scatter Plot). Snake/cat are animals found in a “park”\nor “garden”; heirloom/keepsake are objects in a “museum”\nor “collection”; a strategy/idea are found in a “story” or\n“job”. The predictions followed their subject hypernym clus-\nters (e.g., snake/cat produced mostly “physical entity” pre-\ndictions, strategy/idea produced mostly “abstraction” pre-\ndictions, while keepsake/heirloom were mixed). Chain of\n17\nA short\nlong\nsta y  in a g ener al childr en ’ s hospital is _ t o tak e.\nsta y  in a g ener al childr en ’ s hospital.short\nlong\nI t is _ t o tak e a\nT her apeutic an tic oagula tion\nan tic oagula tion ther ap y\npa tien ts\nhumans\nis _ f or  tr auma t o r ec eiv e.\nther apeutic an tic oagula tion.\nan tic oagula tion ther ap y .\npa tien ts\nhumans\nI t is _ f or  tr auma t o r ec eiv e\nB iomedical Kno wledg e\nS ciBER T (2019)\nFig. 6. A Heat Mapand a Set Viewshowing how grammar and phrasing affect SciBERT.\nreasoning prompts produced similar results. BERT showed\nunique and relevant prerequisites (i.e. few shared connector\nlines in the Set View) for healthy/sick such as “hungry”,\n“tired” or “pregnant”, happy/sad such as “alone” or “here”\nand right/wrong such as “stubborn”, “smart” or “blind”.\nTop predictions for goals such as drive and fly are correct\n(e.g., “car”/“map” and “pilot”/“wings”, respectively). Suc-\nceed and fail are opposite (“plan”/“strategy” vs “distrac-\ntion”/“reason”), while discover, learn, and create are all\nassociated with “teachers”, “lessons”, and “partners”. For\nDistilBERT (Fig. 10), we found strong performance similar to\nBERT in making associations for belongs and goals, such as\nsimilar pair associations between snake/cat, strategy/idea,\ndry/fly and discover/learn/create in the Scatter Plot. We\nnoticed these kinds of associations were mostly noun-based\nand didn’t follow our scheme of separating membership\nand chain-of-reasoning. However, DistilBERT failed to make\ninteresting or useful associations (i.e. the shared filter shows\nmost rows in both the Heat Map and Set View) for different\ncauses or prerequisite prompts, which are generally verb-\nbased associations. We also noticed biases, such as BERT\nexhibiting learned associations in both chain of reasoning\nprompts with female gender labels. This appeared in nu-\nmerous associations of “women” being wrong more than\nright, in “pregnancy” being a common predictions across\nall prerequisite prompts, and in goal prompts where to do\nsomething you need a “woman”, “mother”, “wife” or “girl”.\nInterestingly, DistilBERT does not exhibit these same biases,\ndespite strong noun-based subject predictions.\n18\nI den tity  S t er eo types\n[ subject] is mean t t o be _. the w oman / the daugh t er\nthe man / the son / the per son\nthe tr ans g ender  / the queer\nM an y  e v en a ttribut e [ subject] f or  being _.\nO ne fundamen tal mor al quality  o f  [ subject] is _.\nT he people o f  [ subject] ar e o ft en kno wn f or  _.\nAsian A mericans / A frican A mericans\nE ur opean A mericans / H ispanic and La tino A mericans\nBER T (2018)\nFig. 7. Two Set Viewsshowing how identity stereotypes are perpetuated in BERT.\n19\nI den tity  S t er eo types\nR oBER Ta (2019)\nT he people o f  [ subject] ar e o ft en kno wn f or  _. T he people o f  [ subject] ar e o ft en kno wn f or  _.M an y  e v en a ttribut e [ subject] f or  being _.\nAsian A mericans / A frican A mericans\nE ur opean A mericans / H ispanic and La tino A mericans\nChristianity  / I slam / H induism\nS ikhism  / B uddhism / J udaism / A theism\nC ommunism / S ocialism / D emocr acy  / Liber alism\nC onserv a tism / A nar chism / N a tionalism / F acism\nFig. 8. A Set Viewand two Scatter Plotsshowing how identity stereotypes are perpetuated in RoBERTa.\nI den tity  S t er eo types\nBER T (2018) and R oBER Ta (2019)\n[ subject] is a gr ea t _. the man / the he t er ose xual / the homose xual\n[ subject] is kno wn as a _.\nthe tr ans g ender  / the queer  / the he t er ose xual / the ga y\nthe lesbian / the bise xual / the homose xual\nFig. 9. Two Set Viewsand a Scatter Plotshowing how identity stereotypes are perpetuated in both BERT and RoBERTa.\n20\nKno wledg e P r obing\nD istil BER T (2019)\nY ou ar e lik ely  t o find a [ subject] in a _.\nsnak e / ca t / k eepsak e / heirloom\nidea / str a t egy\nO ne e ff ect o f  [ subject] is f eeling _.\nsuc c eeding / f ailing / e x er cising\nsleeping / thinking / w orrying\nY ou c ould be [ subject] because y ou ar e _.\nhapp y  / sad / righ t / wr ong\nhealth y  / sick\nI f  y ou w an t t o [ subject] then y ou need a _.\ndriv e / fly  / suc c eed / f ail\ndisc o v er  / learn / cr ea t e\nBER T (2018)\nY ou ar e lik ely  t o find a [ subject] in a _.\nsnak e / ca t / k eepsak e / heirloom\nidea / str a t egy\nO ne e ff ect o f  [ subject] is f eeling _.\nsuc c eeding / f ailing / e x er cising\nsleeping / thinking / w orrying\nY ou c ould be [ subject] because y ou ar e _.\nhapp y  / sad / righ t / wr ong\nhealth y  / sick\nI f  y ou w an t t o [ subject] then y ou need a _.\ndriv e / fly  / suc c eed / f ail\ndisc o v er  / learn / cr ea t e\nFig. 10. Eight Scatter Plotsshowing how well complex relationships are learned in BERT versus DistilBERT.",
  "topic": "Blank",
  "concepts": [
    {
      "name": "Blank",
      "score": 0.8286795616149902
    },
    {
      "name": "Computer science",
      "score": 0.8274818062782288
    },
    {
      "name": "Natural language processing",
      "score": 0.4822988510131836
    },
    {
      "name": "Data visualization",
      "score": 0.45760631561279297
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.4280746281147003
    },
    {
      "name": "Visualization",
      "score": 0.38440629839897156
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34763509035110474
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33524608612060547
    },
    {
      "name": "Programming language",
      "score": 0.3341362774372101
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}