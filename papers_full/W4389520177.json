{
  "title": "Towards large language model-based personal agents in the enterprise: Current trends and open problems",
  "url": "https://openalex.org/W4389520177",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2020765609",
      "name": "Vinod Muthusamy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115087023",
      "name": "Yara Rizk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231207944",
      "name": "Kiran Kate",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2798283922",
      "name": "Praveen Venkateswaran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2969065834",
      "name": "Vatche Isahagian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2052311398",
      "name": "Ashu Gulati",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161096669",
      "name": "Parijat Dube",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4307478220",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4377130745",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W4378474282",
    "https://openalex.org/W4223510772",
    "https://openalex.org/W4388660746",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4297683418",
    "https://openalex.org/W4388626886",
    "https://openalex.org/W2963641152",
    "https://openalex.org/W4378945636",
    "https://openalex.org/W3175367361",
    "https://openalex.org/W3098800734",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4378509386",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W3092778060",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4389518771",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4366591012",
    "https://openalex.org/W4394828156",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4385570140",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4378470753",
    "https://openalex.org/W1997760292",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W2997771882",
    "https://openalex.org/W4353113046",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4323354733",
    "https://openalex.org/W2623293810",
    "https://openalex.org/W4388720459",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4380551927",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2604698497",
    "https://openalex.org/W4321345515"
  ],
  "abstract": "There is an emerging trend to use large language models (LLMs) to reason about complex goals and orchestrate a set of pluggable tools or APIs to accomplish a goal. This functionality could, among other use cases, be used to build personal assistants for knowledge workers. While there are impressive demos of LLMs being used as autonomous agents or for tool composition, these solutions are not ready mission-critical enterprise settings. For example, they are brittle to input changes, and can produce inconsistent results for the same inputs. These use cases have many open problems in an exciting area of NLP research, such as trust and explainability, consistency and reproducibility, adherence to guardrails and policies, best practices for composable tool design, and the need for new metrics and benchmarks. This vision paper illustrates some examples of LLM-based autonomous agents that reason and compose tools, highlights cases where they fail, surveys some of the recent efforts in this space, and lays out the research challenges to make these solutions viable for enterprises.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6909–6921\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards large language model-based personal agents in the enterprise:\nCurrent trends and open problems\nVinod\nMuthusamy\nIBM Research\nYara Rizk\nIBM Research\nKiran Kate\nIBM Research\nPraveen\nVenkateswaran\nIBM Research\nVatche Isahagian\nIBM Research\nAshu Gulati\nPersistent Systems\nParijat Dube\nIBM Research\nAbstract\nThere is an emerging trend to use large lan-\nguage models (LLMs) to reason about complex\ngoals and orchestrate a set of pluggable tools\nor APIs to accomplish a goal. This function-\nality could, among other use cases, be used to\nbuild personal assistants for knowledge work-\ners. While there are impressive demos of LLMs\nbeing used as autonomous agents or for tool\ncomposition, these solutions are not ready for\nmission-critical enterprise settings. For exam-\nple, they are brittle to input changes, and can\nget stuck in reasoning loops. These use cases\nraise challenging problems opening up excit-\ning areas of NLP research, such as trust and\nexplainability, consistency and reproducibility,\nadherence to guardrails and policies, best prac-\ntices for composable tool design, and the need\nfor new metrics and benchmarks. This vision\npaper illustrates some examples of LLM-based\nautonomous agents that reason and compose\ntools, highlights cases where they fail, surveys\nsome of the recent efforts in this space, and\nlays out the research challenges to make these\nsolutions viable for enterprises.\n1 Introduction\nThe emergence of ChatGPT has put large language\nmodels (LLMs) (Bommasani et al., 2021) under\nthe microscope to determine what they can and\ncannot do (Hu, 2023). With LLMs outperform-\ning state-of-the-art approaches for traditional ar-\neas of natural language processing (NLP) research\n(Yang et al., 2023a), the focus shifted to other ways\nthat LLMs can be applied. The last few months,\nfor example, have brought on a flurry of activity\naround LLMs as an orchestrator or brain of au-\ntonomous agents tasked with completing high level\ngoals (Park et al., 2023; Wu et al., 2023a; Vemprala\net al., 2023; Singh et al., 2022).\nAutonomous agents perceive the world they op-\nerate in, reason about the events within it and their\ngoal, decompose this goal into sub-problems, and\nTask list LLM-based agentExternal tools\nKnowledge and contextHigh level goal\n(1) User goal added to task list(2,4) Task prioritization: Based on goals, tasks, knowledge, context, and available tools\n(3) Reasoning on tasks: Decompose tasks to sub-tasks(5) Tool selection and execution: Invoke tools(6) Tool observation: Interprets tool output\n(7) Reasoning on output: Update tasks based on tool output\nFigure 1: An LLM-based agent autonomously reasons\nabout tasks and composes external tools to complete\ntasks, and ultimately achieve the user’s goal.\nperform a sequence of actions (equivalent to com-\nposing external tools and APIs to carry out tasks)\nto achieve a new state in this world (Russell, 2010).\nFigure 1 depicts the iterative pattern that some\nLLM-based autonomous agents are applying (Yao\net al., 2022; Nakajima, 2023; Wang et al., 2023b;\nBrohan et al., 2023).\nPhilosophically, it is perhaps not surprising that\nlanguage models are being used for reasoning and\ncarrying out complex goals. For example, the\nSapir-Whorf Hypothesis (Kay and Kempton, 1984)\nsuggests that language influences thought. Bet-\nter language models then should be capable of\nmore sophisticated thinking, including reasoning\nand planning. Another connection is to the modes\nof thinking in humans: System 1 for instinctive\nthought processes, and System 2 for more delibera-\ntive thought (Kahneman, 2011). One can consider\nNLP tasks such as entity recognition as belonging\nto the System 1 bucket, whereas tasks where LLMs\nare used to reason and plan belong to the System 2\nbucket. As language models become more power-\nful, perhaps, they acquire System 2 abilities.\nIn enterprise settings, autonomous agents of-\nten take the form of personal assistants capable\nof conversing in natural language and executing\nactions that alter the state of the world (Kephart,\n6909\n(a) Call a restaurant to order a\npizza. (@AI-Growth-Startups, 2023)\n(b) Perform research to prepare for a pod-\ncast. (@jamesbbaker4 , James Baker)\n(c) Invent a receipt for an upcoming\nevent. (Sharma, 2023)\nFigure 2: Examples of autonomous agents that use LLMs to reason about a high level goal and compose tools such\nas search engines, phone services, and text to speech models, to achieve the goal.\n2021; Rizk et al., 2020; Chakraborti et al., 2022).\nAmong other functionality, these assistants carry\nout task-oriented dialog, further coupling language\nand reasoning. In addition to performing their\nfunctions accurately, enterprise-ready technology\nmust be explainable and reliable. They must also\nadhere to strict regulations and governance, espe-\ncially around concerns such as privacy, bias, and\nauditability. We notice, however, that beyond im-\npressive demos, these LLM orchestrators are brittle,\nfail in unpredictable ways, and are not consistent\nwhich is not ideal for enterprise applications.\nIn this vision paper, we discuss the readiness of\nLLM-based autonomous agents for enterprise ap-\nplications. First, we survey both anecdotal and aca-\ndemic works that have implemented LLM-based\nagents. Then, we show empirical evidence of\nLLMs as tool composers in an enterprise travel\nuse case. We rely on both analyses to define a\nset of challenges that must be addressed to make\nLLMs resilient enough to use as an orchestrator for\nmission-critical enterprise use cases.\n2 Anecdotal uses of LLM-based agents\nChatGPT created a lot of excitement around build-\ning LLM-based autonomous agents, where re-\nsearchers and non-researchers alike were inspired\nto create demos, applications, and services using\nLLMs. In this section, we review anecdotal ac-\ncounts of LLM-based autonomous agents and tool\ncomposers. They help stretch the imagination of\nwhat is possible, but do not perform experiments\nto stress test their solutions and understand the\nstrengths and limitations of LLMs in this role.\n2.1 Examples\nAn agent, inspired by Auto-GPT (Richards, 2023),\nwas able to successfully order a pizza over the\nphone (@AI-Growth-Startups, 2023; @rogerhamil-\nton , Roger James Hamilton); GPT-4 reasoned\nabout the goal, asked clarifying questions such as\nthe type of pizza, used tools to find a nearby restau-\nrant and look up the phone number, made a phone\ncall, and conversed with a real human (who did\nnot realize they were conversing with a bot). Fig-\nure 2a shows some of the internal reasoning by this\nagent. While this was an impressive demo, even\nthe author admitted that it took a few attempts to\nsuccessfully place an order, illustrating the brittle\nnature of these agents. It was also a custom so-\nlution with a known task, and a fixed set of tools\nincluding speech-to-text, text-to-speech, and the\nTwilio API for phone calls. It is not clear how\nthis would extend to support arbitrary goals and\ndynamically pluggable tools.\nAnother agent based on Auto-GPT and\nBabyAGI (Nakajima, 2023) concepts was given\nthe task of preparing relevant topics for a pod-\ncast (@jamesbbaker4 , James Baker). Given the\nnames and expertise of the hosts, it performed a se-\nries of web searches for current events related to the\nhosts’ expertise, reasoned about whether the topics\nwere relevant and covered the interests of all the\nhosts, and performed further searches to improve\non the topics. Figure 2b depicts a partial output\nfrom this agent. It was restricted to web search as\na tool and hence avoided issues with tools that may\nhave undesirable side effects.\nAn agent was used to “browse the web to dis-\ncover the next upcoming event and invent a unique\n6910\nand original recipe that would suit it” (Sharma,\n2023). It used GPT-4 to devise a five-step plan,\nincluding performing web searches to find an up-\ncoming event, generating a relevant recipe, and\nwriting the output to a file. Figure 2c shows the\nrecipe this agent wrote to a file.\nThere are other examples of LLM-based\nagents performing sales prospecting (@ompemi\n, Omar Pera), autonomously carrying out tasks in a\ntask list (Scott, 2023), and managing social media\naccounts (Sharma, 2023).\n2.2 Limitations\nA common observation on the LLM-based au-\ntonomous agent examples is that they tend to be\none-off demos. Applying more scrutiny reveals\nissues around robustness, practicality, and trustwor-\nthiness. For example, an agent can get into loops\nwhere it continuously tries to decompose a task\ninto smaller ones or generates new tasks that are\nduplicates of ones already in the task list or have\nbeen completed. This leads to agents performing\nwork but not making progress towards the higher\nlevel goal (Xiao, 2023; Molony, 2023).\nThese agents can be brittle (e.g., sensitive to\nsmall changes in how the goals are expressed) and\nsometimes even producing different outputs for\nthe same input (Brandt, 2023). The authors have\nalso observed such brittleness when ChatGPT Plu-\ngins (OpenAI, 2023) was used to compose tools,\nwith inconsistent behavior for the same or similar\ninputs (Patil et al., 2023). The agents are also sen-\nsitive to the LLM that is used, making it difficult to\npredict their behavior with different models or even\nnewer versions of the same models. This lack of\nrobustness is a challenge in enterprise applications.\nAnother observation is that these can be slow\nand expensive. They can make hundreds of calls\nto LLMs as they reason about the goals and make\nobservations on the tool outputs. For example, even\nrelatively simple goals can take 50 calls to GPT-4\ncosting about $14 (Xiao, 2023). This is not only\nexpensive monetarily, but these iterative LLM calls\ncan take several minutes or longer to accomplish a\ngoal making them impractical for interactive apps.\nAllowing agents to decide how to compose tools\nthat have side-effects can be dangerous. Some use\ncases only compose tools such as web search with\nlittle harmful effects, while others utilize tools to\nsend emails, write to a file system, or manipulate\ncalendars. The agents and LLMs have little or no\nActions\n(Gupta et al.’22)(Ren et al. ’18)\nNo side-effectsNotificationsReversible actionsIrreversible actions\nDomain\nAgnostic\nRobotics\nService\nHealth\nToolformer (Schick et al. ’23)WikiChat (Semnani et al. ’23)WebGPT (Nakano et al. ’21)Pearl (Sun et al. ’23)\nReACT (Yao et al. ’22)Chameleon (Lu et al. ’23)TaskMatrix.AI (Liang et al. ’23)MM-ReACT (Yang et al. ’23)API-bank (Li et al. ’23)\nBloombergGPT (Wu et al. ’23)\n(Jo et al. ’23)\n(Park et al. ’23)(Huang et al. ’22)(Vemprala et al. ’23)(Singh et al. ’22)\nChat-CoT (Chen et al. ’23)\n(Au et al. ’23)\nAutoTAMP (Chen et al. ’23)SayCan (Brohan et al. ’23)Text2Motion (Lin et al. ’23)TidyBot (Wu et al. ’23)\nFigure 3: Spectrum of actions per domain\nunderstanding of the risks of actions taken with a\ntool and may autonomously perform actions such\nas installing certificates as a super user (Molony,\n2023). These risks are somewhat mitigated by hav-\ning agents prompt the user at every step before\nacting, but this diminishes the value of these agents\nautonomously carrying out tedious work for users.\nThere are also studies that present the limitations\nof the reasoning abilities of LLMs (Borji, 2023).\nSince reasoning is such a critical part of how agents\nuse LLMs, these limitations will have a direct effect\non the performance of these agents.\nThese anecdotal observations of current limita-\ntions of LLM-based agents present roadblocks to\nadoption for enterprise use cases, where risk, cost,\nand robustness are critical issues. We formulate a\nset of research challenges in Section 5 to address\nthese and other roadblocks.\n3 A review of LLM-based agents\nIn the previous section, we provided evidence of\nthe feasibility of LLM-based agents from non-\nacademic sources such as blogs, Twitter posts,\nGitHub repositories, and demos. While these gen-\nerated excitement around LLMs outside of the re-\nsearch community, this evidence does not allow\nus to assess the true capabilities and limitations of\nLLMs. Next, we survey published literature that\nmore systematically analyzes LLMs in domains-\nagnostic and domain-specific settings. These pa-\npers consider a spectrum of actions from no side-\neffects to irreversible as illustrated in Figure 3.\nDomain-agnostic autonomous agents Recent\npapers extend the use of LLMs beyond simple\ntasks such as summarization or entity extraction to\nhandle complex reasoning and question answering.\nThis includes augmenting LLMs with the ability\nto query the internet (Nakano et al., 2021; Sem-\nnani et al., 2023) or to use of different types of\n6911\ntools (e.g. calculators) or access external informa-\ntion (e.g. Google, or Wikipedia search) (Schick\net al., 2023; Chen et al., 2023c). When (Sun\net al., 2023) leveraged LLMs to decompose com-\nplex tasks into sequence of actions that have no\nside effects, TaskMatrix.AI and ReACT present a\nvision that utilizes GPT-4 to connect any APIs to\ncomplete tasks (Liang et al., 2023; Yao et al., 2022;\nYang et al., 2023b) (Lu et al., 2023) focused on plug\nand play when leveraging LLMs for tool composi-\ntion. Finally, (Li et al., 2023) created a benchmark\nwith 500+ APIs to evaluate the effectiveness of\nLLMs as tool composers.\nAs depicted in Figure 1, an evolution of these\nagents adds long term memory and task prioritiza-\ntion capabilities. An LLM decomposes high level\ngoals to smaller tasks, a vector store is typically\nused for long term memory to recall outputs from\nlong task lists, and an LLM is used to iteratively\nreprioritize existing tasks and create new tasks\nbased on the progress towards the goal (Richards,\n2023; Nakajima, 2023; Wang et al., 2023b).\nRobotic agents Due to their emergent behav-\niors (Bommasani et al., 2021), LLMs have been\nused to make goal-driven decisions. These ap-\nproaches mainly rely on LLMs to accomplish tasks\nin the physical or virtual world using Internet-of-\nThings devices and robots (Huang et al., 2022;\nSingh et al., 2022; Vemprala et al., 2023). (Huang\net al., 2022) evaluate whether LLMs contain infor-\nmation necessary to accomplish goals without ad-\nditional training or grounding. Followup efforts on\ngrounding primitive actions include Text2Motion\n(Lin et al., 2023a), AutoTAMP (Chen et al., 2023b)\nand SayCan (Brohan et al., 2023) to guide LLM-\nbased task and motion planning. TidyBot (Wu et al.,\n2023a) uses LLMs to infer generalized user pref-\nerences that are applicable to future interactions.\nLLM-based interactive agents mimicked human\nbehavior in (Park et al., 2023).\nOther domains Dialog systems with the goals\nof performing tasks such as finding restaurants or\nreserving hotels have been addressed in the NLP\ncommunity (Gupta et al., 2022; Ren et al., 2018).\nFor the banking sector, (Wu et al., 2023b) trained\na finance specific LLM. In the medical domain,\n(Au Yeung et al., 2023) tested ChatGPT for clin-\nical question-answering. (Jo et al., 2023) created\na public health intervention system that can chat\nwith patients and notify health care professionals\nif intervention was necessary. In the area of online\ngames, V oyager (Wang et al., 2023a) uses GPT-4 to\nbuild an agent to continuously explore a Minecraft\nworld with the goal of making novel discoveries. It\nuses a catalog of skills or tools, and can also learn\nnew skills.\n4 Use case: chatbot for enterprise travel\nThe integration of APIs with LLMs (Liang et al.,\n2023) has raised the question of whether LLMs\ncan act as task-oriented chatbots, particularly on\ncomplex tasks that require the execution of a se-\nquence of APIs. Automating tasks in enterprise\nsettings is particularly difficult since each company\nhas custom processes in place and often little data\nto train or fine-tune an LLM to their peculiarities.\n4.1 Travel example\nLet’s consider a travel example where LLM users\nmay want to book a trip which includes making\nflight, hotel, and car rental arrangements. For en-\nterprises, additional steps are necessary such as es-\ntimating travel costs and submitting a pre-approval\nwith adequate justification (e.g., client event or con-\nference travel), as shown in Table 1. Simply classi-\nfying these utterances into intent classes may not be\nenough as some of the information to distinguish\nbetween these clusters is not found in the natural\nlanguage utterances provided by the users.\n4.2 Methodology\nHistorically, process knowledge was embedded\ninto task-oriented dialog systems by system de-\nvelopers. For example, a chatbot may ask the user\n(based on their underlying dialog tree) if their travel\nis a client event or a conference in cases where the\nprocess requires this information to determine the\nnext step. By placing LLMs at the center of such\ndialog systems, how can we infuse such process\nknowledge into LLMs’ decision making? In this\ncase study, we focus on doing so through prompt-\ning but fine-tuning and possible pre-training mod-\nels with appropriate data could also be considered.\nThe knowledge to make good decisions may\ncome from multiple sources (Figure 4). For ex-\nample, business process descriptions or standard\noperating procedures may include the necessary\nsteps for business travel (e.g., submit a pre-approval\nfirst). Past process traces showing the execution of\nsuch processes can reveal information about best\npractices (e.g., most travel requests to conferences\n6912\nUser Utterance Cluster Expected Sequence of APIs\nBOOK MY TRIP FROM BOSTON TO\nNYC FOR MAY 2\nBusiness, Day trip Slot filling –> Car rental\nRESERVE MY TRIP TO PARIS FOR THE\nACME WORKSHOP FROM MAR.1-4\nBusiness, Multi-day Travel Estimation –> Travel Pre-approval\nBOOK MY TRIP FROM BOS TO SFO\nFOR MAY 5-10\nPersonal Slot filling –> Airline booking\nI’M ATTENDING EMNLP 2023 Conference, Multi-day Publication database –> Travel estimate API\n–> Travel pre-approval\nTable 1: Travel Example\nFigure 4: Sources of information to feed into an LLM\ninclude listing papers accepted at the conference).\n4.3 Experimental setup\nModel and prompt To circumvent the need for\na multi-modal foundation model, we represent all\nthe sources of information in natural language, as\nin Figure 5. We experiment with a few variations\nof the prompt by paraphrasing some of its sections.\nMultiple versions of the prompts are fed into an\ninstruction-tuned transformer model (FLAN-T5)\n(Longpre et al., 2023).\nFigure 5: A prompt template for the travel use case\nDataset We created natural language utterances\nto sequence of API pairs (similar to Table 1)\nfrom two publicly available travel datasets (ap-\nprox. 700 from Google employee travel (Emp,\n2022) and 1200 from Frames (Fra, 2018). We man-\nually labelled twelve phrases and then clustered the\nphrases to weakly label them.\nEvaluation metrics To compare the various\nprompts, we adopted the BLEU metric (Papineni\net al., 2002) and calculated precision, recall and F1-\nscore as follows. Precision is the total number of\ncommon APIs between the ground truth sequence\nand the model’s output divided by the total number\nof APIs in the model’s output sequence. Recall\nis the total number of common APIs between the\nground truth sequence and the model’s output di-\nvided by the total number of APIs in the ground\ntruth sequence. We calculated them at the set and\nsequence levels. Set level metrics compare the set\nof APIs in the ground truth and the output sequence,\nwhereas sequence level metrics are based on the\nlongest common subsequence between the ground\ntruth and output sequence.\n4.4 Experimental results\nTables 2-5 summarize our results. A baseline\nprompt (v1) does not include any process knowl-\nedge or past process traces (i.e., only the blue sec-\ntions of Figure 5), whereas the remaining versions\n(v2-v5) include all sections in Figure 5 but para-\nphrased to express the same information in differ-\nent ways (see Appendix for an example). We can\nsee from the tables that including process informa-\ntion, not just users’ natural language phrases and\nconversation context, outperformed the baseline\nprompt (in both set level and sequence level evalu-\nations). Furthermore, the way that this information\nis included in the prompt matters (i.e., v4 and v5\nperform better than other versions).\nTo evaluate LLMs’ consistency, we prompted\nthe model 30 times with a fixed input using a tem-\nperature of 0.5. Prompt v5 was used as the template\nand we compared the output of the model to the\noutput when the temperature is set to 0. We did\n6913\nPrompt Precision Recall F1-score BLEU\nv1 0.32 0.39 0.35 0.38\nv2 0.38 0.48 0.43 0.19\nv3 0.61 0.76 0.68 0.55\nv4 0.61 0.78 0.68 0.57\nv5 0.65 0.79 0.71 0.62\nTable 2: Comparing prompts on Google Employee\nTravel Dataset (averaged over instances). Precision,\nrecall, and F-1 are calculated at set level.\nPrompt Precision Recall F1-score BLEU\nv1 0.54 0.55 0.54 0.42\nv2 0.57 0.55 0.56 0.40\nv3 0.55 0.61 0.58 0.40\nv4 0.68 0.60 0.64 0.56\nv5 0.63 0.55 0.59 0.52\nTable 3: Comparing prompts on Frames Dataset (av-\neraged over instances). Precision, recall, and F-1 are\ncalculated at set level.\nPrompt Precision Recall F1-score\nv1 0.28 0.34 0.31\nv2 0.33 0.44 0.38\nv3 0.56 0.69 0.62\nv4 0.55 0.70 0.62\nv5 0.61 0.73 0.69\nTable 4: Comparing prompts on Google Employee\nTravel Dataset (averaged over instances). Precision,\nrecall, and F-1 are calculated at sequence level.\nPrompt Precision Recall F1-score\nv1 0.52 0.53 0.53\nv2 0.54 0.52 0.53\nv3 0.53 0.59 0.56\nv4 0.65 0.57 0.61\nv5 0.61 0.53 0.57\nTable 5: Comparing prompts on Frames Dataset (av-\neraged over instances). Precision, recall, and F-1 are\ncalculated at sequence level.\nPhrase BLEU Precision Recall F1-\nscore\n1 0.56 0.46 1.00 0.63\n2 0.86 0.92 0.89 0.91\n3 0.95 0.93 0.97 0.95\n4 0.48 0.51 1.00 0.68\n5 0.70 0.75 0.74 0.74\n6 0.74 0.90 0.78 0.84\n7 0.81 0.84 0.88 0.86\n8 0.97 0.97 1.00 0.99\n9 0.68 0.72 0.72 0.72\n10 0.77 0.96 0.82 0.89\n11 0.71 0.82 0.93 0.87\n12 0.91 0.85 0.97 0.91\nAverage ±\nStandard\nDeviation\n0.76 ±\n0.15\n0.80±\n0.17\n0.89±\n0.10\n0.83±\n0.11\nTable 6: Repeatability on Manually Labelled Phrases\nthis for the 12 manually labeled phrases from the\ndataset, as shown in Table 6. Ideally, we want the\nmetrics to be close to 1 to have a consistent LLM.\nIn this case, we see that the results are not very\nrepeatable with an average BLEU score of 0.76.\n5 Research challenges\nWe now draw on the limitations observed from\nanecdotal applications of LLM-based agents (Sec-\ntion 2), experimental evaluation of an enterprise\ntravel case study (Section 4), and our own experi-\nences developing enterprise application platforms1\nto define a set of research challenges that need to\nbe solved before LLM-based agents can be applied\nto mission-critical enterprise use cases.\n5.1 Metrics and benchmarks\nThe evaluation of personal agents depends on the\nproblem formulation. As a task-oriented dialog, it\nis commonly evaluated for accuracy of intent de-\ntection and accuracy of slot filling, i.e., how well\nthe values of parameters detected from the natu-\nral language utterance match the ground truth API\ncall. AST (abstract syntax tree) sub-tree match-\ning is another metric to measure the correctness of\nAPI calls. The natural language utterance can be\nsingle-intent or multi-intent. This is the simplest\nform of personal agents and has existing bench-\nmarks (Hemphill et al., 1990; Coucke et al., 2018;\nQin et al., 2020; Patil et al., 2023). Most of these\nbenchmarks contain a limited number (dozens) of\nAPIs or tools with a limited number of slots per\nAPI and some of them are synthetically generated.\nFor a multi-turn dialog, it is also important to\nevaluate the number of turns required to achieve\nthe goal. If the task requires executing a sequence\nof dependent tools, then the order of the tools be-\ncomes important. There are a few benchmarks\navailable for this setting (Rastogi et al., 2020; Li\net al., 2023; Budzianowski et al., 2018) with only\none of them being a fully human annotated cor-\npus (Budzianowski et al., 2018).\nFurthermore, most of these datasets are simpli-\nfied and do not represent enterprise scenarios. For\nexample, (Li et al., 2023) has well-documented\nhandcrafted APIs with a handful of parameters per\nAPI. Real tools on the other hand have many con-\nfiguration options and are rarely well documented.\nFor a more complex setting of autonomous\nagents interacting with each other, the evaluation\n1Citations omitted for double-blind review.\n6914\nis challenging. Human evaluation is a critical part\nfor building such agents (Park et al., 2023). If the\ntasks change the state of the world, then comparing\nthe state after each action is a potential metric.\nThe development of representative benchmarks\nhas accelerated the field of NLP. We believe this\nis an important area to focus on for enterprise per-\nsonal agents, especially ones that factor in business\nprocess information and other modalities to fully\ncapture the domain knowledge.\n5.2 Data for fine-tuning\nMost of the LLM training efforts on tasks in the\nnatural language domain have large amounts and\nvariety of relevant and (un)labeled training data\nthat has been collected and open-sourced by the\nlarger research community. In most enterprise set-\ntings, there is either a lack of sufficient labeled\nopen-source real-world data to fine-tune a model\ndue to their inherent proprietary nature, or lack of\nnecessary infrastructure or training expertise that\nenables the fine tuning of such LLMs. One way\nto overcome this is by further advancements in\nprompt engineering (Wei et al., 2022) but also to\nprovide a framework that efficiently guides devel-\nopers to structure their prompts, and effectively\nprompt the model to generate more accurate results\nand enhance the overall performance without the\nneed to fine-tune the models.\n5.3 Composing tools\nLLMs are not typically explicitly trained with the\ngoal of composing tools and acting as the reason-\ning engine of an autonmous agent. Likewise, tools\nsuch as web search APIs, databases, and file sys-\ntem primitives aren’t designed to called by a dy-\nnamic agent and LLM. We see some evidence of\nthis impedance mismatch with ChatGPT Plugins\nrequiring plugin developers to author manifest files\nfor their existing tools, as well as extensive guid-\nance on how to describe the tools, so they can be\ncomposed by ChatGPT (OpenAI, 2023).\nMore work is needed on improving the capabili-\nties of LLMs to compose tools that take structured\ninputs and outputs, not just natural language. Fur-\nthermore, agents need an understanding of the risks\nand side-effects of performing actions with tools;\na tool to search the web should be treated differ-\nently from one that performs financial transactions.\nThis might require better interfaces or program-\nming models to make these tools more consumable\nby agents and LLMs. A related aspect is these\nefforts should improve the predictability and de-\nbuggability of these systems. As touched on in\nSection 2, current agents are brittle and hard to test.\n5.4 Pluggability\nAn enterprise user needs to perform many tasks on\na typical day. With an ever-evolving landscape of\nsoftware tools, this set is large and anything but\nconstant. Hence, personal assistants should allow\nfor easy plugging in of new tools. Current prompt-\nbased approaches such as ChatGPT function call-\ning (Eleti et al., 2023) are limited by the context\nlength of the model to add more APIs. Approaches\nsuch as (Schick et al., 2023) can potentially scale\nto a large number of APIs but new APIs cannot be\nadded dynamically without re-training the model.\nEnsuring pluggability will also make it difficult to\npurely rely on fine-tuning models since the rate\nof adding and removing plugins and data scarcity\ncould make fine-tuning prohibitive.\n5.5 Reproducibility and reliability\nThe sensitive and business critical nature of enter-\nprise tools and systems requires consistency and\nreliability from their tools and compositions. How-\never, LLMs are capable of hallucinating predictions\n(Jiang et al., 2020), and could generate inconsis-\ntent and invalid tools and compositions, thereby\nhaving harmful consequences. Additionally, these\nmodels are brittle, where small variations in the\nprompt could result in different predictions, re-\nsulting in users’ having very different experiences\nwith the same model. Approaches like instruction-\ntuning (Ouyang et al., 2022) and chain-of-thought\nprompting (Wei et al., 2022) alleviate some of these\nproblems by breaking down prompts into stages to\nimprove prediction consistency. However, these\napproaches do not provide any guarantees on the\nreliability and reproducibility of the predictions.\nAn increasingly popular approach to enable re-\nliable LLM predictions is constrained decoding\n(Hokamp and Liu, 2017), which enforces the model\nto only consider certain outputs for predictions\nby modifying their log-probabilities based on the\ngiven constraints. This would enable enterprise\nsystems to prevent hallucinated outputs. Addition-\nally, enterprises could also represent their policies\nas constraints to the model, to enforce compliance.\nHowever, as the number of constraints increases\nand in multi-modal settings, representing these con-\nstraints and policies in a format that the model can\nconsume presents a significant challenge.\n6915\n5.6 Confidence and failing gracefully\nData sources used to train LLMs are often restricted\nto positive knowledge and do not provide sufficient\nnegative examples, including appropriate responses\nin case of failure or lack of knowledge, resulting\nin LLMs confidently producing incorrect answers\n(Chen et al., 2023a; Jiang et al., 2020). In enter-\nprise settings, it is critical for LLM based systems\nto be able to recognize their limitations and fail\ngracefully. This is challenging to achieve, since\nthe notion of uncertainty is dependent on the ap-\nplication domain, the knowledge sources used to\ntrain the model, and an evaluation of the model’s\nresponse. While there has been some initial efforts\nto develop methods to measure the confidence or\nuncertainty of black-box models (Lin et al., 2023b;\nKuhn et al., 2023), they do not translate to many\nenterprise use-cases such as those described in this\npaper, necessitating specialized approaches.\n5.7 Error handling and failure semantics\nAs indicated previously in Section 2.2, LLM agents\ncan get into loops trying to decompose a task or\nperform actions that can have lasting side effects\non their environment. Handling such situations will\nbe necessary to provide a reliable and consistent\noutput in an enterprise setting. Furthermore, as\nLLM’s access to tools increases, it becomes critical\nfor them to overcome errors such as page not found,\nnon-responsive server, or unauthorized access, that\ncan get them stuck and hinder their progress.\n5.8 Multi-modality\nMany enterprise applications have decades worth\nof information saved in various modalities such as\nspreadsheets, scanned documents in image format,\nunstructured emails, business process diagrams and\nothers. One way to address this diversity is to bring\neverything into the language space (per section 4).\nPrior work has shown that better performance can\nbe achieved by learning a unified representation\nacross modalities (Bao et al., 2022) as opposed to\ntreating these modalities independently (Jia et al.,\n2021) or converting into one modality where infor-\nmation will be inevitably lost (Xiang et al., 2023).\nCreating a new class of foundation models that\ninclude LLMs along with models for enterprise\nspecific modalities like business processes is worth\ninvestigating (Rizk et al., 2022).\n5.9 Generalizability and scalability\nMost popular LLMs have a large number of pa-\nrameters, making them prohibitively expensive to\nfine-tune for different tasks. Given the varied ob-\njectives of enterprise use-cases, ensuring the gener-\nalizability of models to new tasks and domains, and\nunforeseen tools is essential. Approaches like few-\nshot prompting and prompt tuning have become\nmore popular (Lester et al., 2021; Gu et al., 2022),\nleveraging specific and well-crafted examples to\nimprove model generalizability. The growing size\nalso inherently increases inference time and infras-\ntructure costs, and enterprise services that depend\non these models often have latency constraints. Ad-\ndressing the scalability and costs of LLM-driven\nenterprise applications is another significant chal-\nlenge that requires attention.\n5.10 General purpose or specialized models\nIn Figure 1, a single LLM is used for multiple\ntasks including task reasoning and composing tools.\nThere is no reason to presuppose that a single\nmodel is well-suited to every step in that flow. Thus,\nthought should be given to whether specialized\nmodels for each task perform better and how to\nchain them to complete the overall objective.\n6 Final thoughts and next steps\nEnterprises are on a continuous quest to opti-\nmize repetitive and tedious work. Advancements\nin LLMs have broadened imaginations on what\nknowledge work can be automated, and LLM-\nbased autonomous agents can be the vehicle to\ndeliver incredible productivity gains.\nAttempts in the community to build agents us-\ning even state-of-the-art LLMs, as well as our\ncase study using LLMs for an enterprise travel use\ncase, reveal how inadequate these solutions are for\nmission-critical enterprise use cases. We outline\nseveral research challenges that the NLP and other\nresearch communities can investigate.\nWhile we describe individual challenges, such\nas robustness, multi-modality, and tool composi-\ntion, the community must take a holistic view of\nthe problem, addressing not just LLM advances,\nbut the entire software lifecycle including tool au-\nthoring, and debugging. Defining clear use cases\nand benchmarks will be important steps.\n6916\nLimitations\nOn the survey and challenges contributions of this\nwork: our coverage of the literature is incomplete\ndue to the extraordinary fast pace and sheer volume\nof work posted on arxiv, blogs, etc. Furthermore,\ndue to this rapid pace, we do cite a large number\nof non-peer-reviewed work. Our discussion of ex-\nisting challenges and how to resolve them is also\ncolored by our experience in industry and may not\ninclude some more theoretical challenges that the\ncommunity should also solve in conjunction with\nthe practical challenges.\nOn the experimental contributions of this work:\nour datasets are small in size compared to what\nmay be commonly used, their labeling is noisy due\nto the pseudo-labeling approach we adopted and\nmay not be very realistic due to the additional pro-\ncessing we performed to get them to the format we\nrequired. Furthermore, the metrics we calculated\ndo not measure all characteristics that we may want\nto evaluate. Also, since we only used one ground\ntruth to calculate the metrics, this may result in less\naccurate values (e.g., BLEU becomes more accu-\nrate as more references are used). Finally, the ex-\nperimental analysis is not comprehensive, missing\nsome ablation studies and other experiments that\ncould help answer additional questions on the per-\nformance on LLMs on task-oriented dialog tasks.\nEthics statement\nOur work discusses how to enable LLMs to per-\nform actions (or compose tools) whose purpose is\nto change the state of the real world. Given the\nemergent property of LLMs that may result in un-\npredictable behavior, allowing these LLMs to alter\nthe state of the world by performing these actions\ncould lead to irrevocable changes that could have\nnegative impact (e.g., autonomous agents capable\nof stealing money from people’s banks).\nReferences\n2018. Frames Dataset. https://www.\nmicrosoft.com/en-us/research/project/\nframes-dataset/download/. Accessed on June\n23, 2023.\n2022. About - open data - city of\ngreater sudbury. https://opendata.\ngreatersudbury.ca/documents/\n9dafb3c069754aea83dd6e3aa8c9873e/about.\nAccessed on June 21, 2023.\n@AI-Growth-Startups. 2023. My ChatGPT - GPT-4\nAgent ordered me pizza BY PHONE!\nJoshua Au Yeung, Zeljko Kraljevic, Akish Luintel, Al-\nfred Balston, Esther Idowu, Richard J Dobson, and\nJames T Teo. 2023. Ai chatbots not yet ready for\nclinical use. Frontiers in Digital Health, 5:60.\nHangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,\nOwais Khan Mohammed, Kriti Aggarwal, Subho-\njit Som, Songhao Piao, and Furu Wei. 2022. Vlmo:\nUnified vision-language pre-training with mixture-of-\nmodality-experts. Advances in Neural Information\nProcessing Systems, 35:32897–32912.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nAli Borji. 2023. A categorical archive of chatgpt fail-\nures.\nCraig Brandt. 2023. Ansible and ChatGPT: Putting it\nto the test.\nAnthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian\nIbarz, Alex Irpan, Eric Jang, Ryan Julian, et al. 2023.\nDo as i can, not as i say: Grounding language in\nrobotic affordances. In Conference on Robot Learn-\ning, pages 287–318. PMLR.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Ultes Stefan, Ramadan Os-\nman, and Milica Gaši ´c. 2018. Multiwoz - a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nTathagata Chakraborti, Yara Rizk, Vatche Isahagian,\nBurak Aksar, and Francesco Fuggitti. 2022. From\nnatural language to workflows: Towards emergent in-\ntelligence in robotic process automation. In Business\nProcess Management: Blockchain, Robotic Process\nAutomation, and Central and Eastern Europe Forum:\nBPM 2022 Blockchain, RPA, and CEE Forum, Mün-\nster, Germany, September 11–16, 2022, Proceedings,\npages 123–137. Springer.\nJiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei\nLi, and Yanghua Xiao. 2023a. Say what you mean!\nlarge language models speak too positively about\nnegative commonsense knowledge. arXiv preprint\narXiv:2305.05976.\nYongchao Chen, Jacob Arkin, Yang Zhang, Nicholas\nRoy, and Chuchu Fan. 2023b. Autotamp: Autoregres-\nsive task and motion planning with llms as translators\nand checkers. arXiv preprint arXiv:2306.06531.\n6917\nZhipeng Chen, Kun Zhou, Beichen Zhang, Zheng\nGong, Wayne Xin Zhao, and Ji-Rong Wen. 2023c.\nChatcot: Tool-augmented chain-of-thought reason-\ning on\\\\chat-based large language models. arXiv\npreprint arXiv:2305.14323.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nLayla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie\nZumer, Justin D. Harris, Emery Fine, Rahul Mehro-\ntra, and Kaheer Suleman. 2017. Frames: A corpus\nfor adding memory to goal-oriented dialogue systems.\nIn Proceedings of the 18th Annual SIGdial Meeting\non Discourse and Dialogue, pages 207–219.\nAtty Eleti, Jeff Harris, and Logan Kilpatrick. 2023.\nFunction calling and other api updates. Accessed\non June 23, 2023.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2022. Ppt: Pre-trained prompt tuning for few-shot\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8410–8423.\nRaghav Gupta, Harrison Lee, Jeffrey Zhao, Yuan Cao,\nAbhinav Rastogi, and Yonghui Wu. 2022. Show,\ndon’t tell: Demonstrations outperform descriptions\nfor schema-guided task-oriented dialogue. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4541–4549.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. arXiv preprint arXiv:1704.07138.\nKrystal Hu. 2023. Chatgpt sets record\nfastest growing user base: Analyst note.\nhttps://www.reuters.com/technology/chatgpt-\nsets-record-fastest-growing-user-base-analyst-note-\n2023-02-01. Accessed on June 21, 2023.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In International Conference on Ma-\nchine Learning, pages 9118–9147. PMLR.\n@jamesbbaker4 (James Baker). https:\n//twitter.com/jamesbbaker4/status/\n1645898646762782735. Accessed on June 23,\n2023.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nEunkyung Jo, Daniel A Epstein, Hyunhoon Jung, and\nYoung-Ho Kim. 2023. Understanding the benefits\nand challenges of deploying conversational ai lever-\naging large language models for public health inter-\nvention. In Proceedings of the 2023 CHI Conference\non Human Factors in Computing Systems, pages 1–\n16.\nDaniel Kahneman. 2011. Thinking, fast and slow .\nmacmillan.\nPaul Kay and Willett Kempton. 1984. What is the\nsapir-whorf hypothesis? American anthropologist,\n86(1):65–79.\nJeffrey O Kephart. 2021. Multi-modal agents for busi-\nness intelligence. In Proceedings of the 20th Interna-\ntional Conference on Autonomous Agents and Multi-\nAgent Systems, pages 17–22.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\narXiv preprint arXiv:2302.09664.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms. arXiv\ne-prints, pages arXiv–2304.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. 2023. Taskmatrix. ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. arXiv preprint arXiv:2303.16434.\nKevin Lin, Christopher Agia, Toki Migimatsu, Marco\nPavone, and Jeannette Bohg. 2023a. Text2motion:\nFrom natural language instructions to feasible plans.\narXiv preprint arXiv:2303.12153.\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023b.\nGenerating with confidence: Uncertainty quantifi-\ncation for black-box large language models. arXiv\npreprint arXiv:2305.19187.\n6918\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nRick Molony. 2023. Auto-GPT – Promise versus Real-\nity.\nYohei Nakajima. 2023. Task-driven autonomous agent\nutilizing gpt-4, pinecone, and langchain for diverse\napplications.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\n@ompemi (Omar Pera). 2023. An ai agent that au-\ntonomously does sales prospecting on its own with\ngpt-4. powered by babyagi from @yoheinakajima &\nrun on @replit. imagine once you integrate it with\n@langchainai tools like @hubspot or apollo.\nOpenAI. 2023. ChatGPT plugins.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai,\nMeredith Ringel Morris, Percy Liang, and Michael S.\nBernstein. 2023. Generative agents: Interactive sim-\nulacra of human behavior.\nShishir G Patil, Tianjun Zhang, Xin Wang, and\nJoseph E Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nLibo Qin, Xiao Xu, Wanxiang Che, and Ting Liu. 2020.\nAgif: An adaptive graph-interactive framework for\njoint multiple intent detection and slot filling. arXiv\npreprint arXiv:2004.10087.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghav Gupta, and Pranav Khaitan. 2020. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8689–8696.\nLiliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. To-\nwards universal dialogue state tracking. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2780–2786.\nToran Bruce Richards. 2023. Auto-GPT: An Au-\ntonomous GPT-4 Experiment. https://github.\ncom/Significant-Gravitas/Auto-GPT.\nYara Rizk, Vatche Isahagian, Scott Boag, Yasaman\nKhazaeni, Merve Unuvar, Vinod Muthusamy, and\nRania Khalaf. 2020. A conversational digital assis-\ntant for intelligent process automation. In Business\nProcess Management: Blockchain and Robotic Pro-\ncess Automation Forum: BPM 2020 Blockchain and\nRPA Forum, Seville, Spain, September 13–18, 2020,\nProceedings 18, pages 85–100. Springer.\nYara Rizk, Praveen Venkateswaran, Vatche Isahagian,\nand Vinod Muthusamy. 2022. A case for business\nprocess-specific foundation models. arXiv preprint\narXiv:2210.14739.\n@rogerhamilton (Roger James Hamilton). 2023. This\nguy built a gpt4 ai that followed a prompt to find\nand call a pizza co and order a pizza without them\nrealizing it was a bot. #ai is doubling its power\nevery 3 months. scary to think where we will be\n6 months from now... #gpt4 #chatgpt full video:\nhttps://youtu.be/8jgfq2qqq2q.\nStuart J Russell. 2010. Artificial intelligence a modern\napproach. Pearson Education, Inc.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nGarrett Scott. 2023. Do Anything Machine.\nSina J Semnani, Violet Z Yao, Heidi C Zhang, and Mon-\nica S Lam. 2023. Wikichat: A few-shot llm-based\nchatbot grounded with wikipedia. arXiv preprint\narXiv:2305.14292.\nNitin Sharma. 2023. AutoGPT: The New Kid on the AI\nBlock That’s Changing Everything!\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. 2022.\nProgprompt: Generating situated robot task plans\nusing large language models. arXiv preprint\narXiv:2209.11302.\nSimeng Sun, Yang Liu, Shuohang Wang, Chenguang\nZhu, and Mohit Iyyer. 2023. Pearl: Prompting large\nlanguage models to plan and execute actions over\nlong documents. arXiv preprint arXiv:2305.14564.\n6919\nSai Vemprala, Rogerio Bonatti, Arthur Bucker, and\nAshish Kapoor. 2023. Chatgpt for robotics: Design\nprinciples and model abilities. Microsoft Auton. Syst.\nRobot. Res, 2:20.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. V oyager: An open-ended\nembodied agent with large language models.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023b. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJimmy Wu, Rika Antonova, Adam Kan, Marion Lep-\nert, Andy Zeng, Shuran Song, Jeannette Bohg, Szy-\nmon Rusinkiewicz, and Thomas Funkhouser. 2023a.\nTidybot: Personalized robot assistance with large\nlanguage models. arXiv preprint arXiv:2305.05658.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023b.\nBloombergGPT: A large language model for finance.\narXiv preprint arXiv:2303.17564.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui\nWang, Zichao Yang, and Zhiting Hu. 2023. Lan-\nguage models meet world models: Embodied expe-\nriences enhance language models. arXiv preprint\narXiv:2305.10626.\nHan Xiao. 2023. Auto-GPT Unmasked: The Hype and\nHard Truths of Its Production Pitfalls.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023a. Harnessing the power of llms in\npractice: A survey on chatgpt and beyond. arXiv\npreprint arXiv:2304.13712.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023b.\nMm-react: Prompting chatgpt for multimodal rea-\nsoning and action. arXiv preprint arXiv:2303.11381.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran,\nKarthik R Narasimhan, and Yuan Cao. 2022. React:\nSynergizing reasoning and acting in language models.\nIn NeurIPS Foundation Models for Decision Making\nWorkshop.\nAppendix\nA Prompts\nCheck supplemental material for complete prompts.\nFigure 6 shows an example of the difference in how\nthe rules are phrased between v2 and v4.\nFigure 6: Rephrasing of rules in prompt v2 and v4\nB Datasets\nEmployee Travel/Golden dataset We have used\nfreely available Employee Travel 2022 dataset from\nGoogle Dataset Search. This dataset contains infor-\nmation about the employee travel expenses for the\nyear 2022. Details are provided on the employee\n(name, title, department), the travel (dates, location,\npurpose) and the cost (expenses, recoveries). Count\nof rows = 178.\nFor the purpose of generating NL utterances, we\nused four columns (termed as entities) - dates (start\nand end), location and purpose of travel. Using\nthese entities, we divide our NL utterance genera-\ntion into four categories:\n• random_sample_4: with all the four entities\npresent (start date, end date, purpose of travel\nand location of travel)\n• random_sample_3: with any three random\nentities present\n• random_sample_2: with any two random enti-\nties present\n• random_sample_1: with any one random en-\ntity present\nUsing OpenAI’s text-davinci-003 model, we engi-\nneered a prompt after preprocessing the dataset, to\ngenerate five distinct user utterances with the help\nof the entities, in first person. The same process\nwas repeated for all the defined categories above.\nThis approach generated an overall count of 3560\nutterances from the initial set of 178 rows.\nPast Travel Requests We created a template, us-\ning seven entities, to craft travel requests made by\nemployees in the past. These past travel requests\nare a part of our prompt.\n6920\nTemplate: The travel request by <name> (<ti-\ntle>) was approved for <amount>; his trip to <desti-\nnation> from <date> to <date> was for the purpose\nof <purpose>\nExample: ’The travel request by Adair, Brendan\n(Director of Transit) was approved for $1230.3; his\ntrip to Toronto, ON from 2022-07-17 to 2022-07-\n20 was for the purpose of Ontario Transportation\nExpo- OTE’\nFrames Dataset Frames is a dialogues dataset\nwhich was collected in a Wizard-of-Oz fashion by\n(El Asri et al., 2017). Two humans talked to each\nother via a chat interface. One was playing the\nrole of the user and the other one was playing the\nrole of the conversational agent called wizard. The\nwizards had access to a database of 250+ packages,\neach composed of a hotel and round-trip flights.\nFrames is composed of 1369 human-human di-\nalogues with an average of 15 turns per dialogue.\nThis corpus contains goal-oriented dialogues be-\ntween users who were given some constraints to\nbook a trip and assistants who search a database to\nfind appropriate trips.\nThe Frames dialogues are in JSON format. Each\ndialogue has five main fields: user_id, wizard_id,\nid, userSurveyRating and turns. For our purpose,\nwe extracted the first occurrence from every user-\nwizard dialogue or id. On postprocessing, we were\nable to create a dataset of about 1200 user utter-\nances. The postprocessing was done using HDB-\nSCAN to group similar sentences into clusters and\nthen discarding those clusters which contained only\ngreetings (such as Hi, Hello there) without any re-\nquest for travel booking.\nC Metrics\nWe will calculate our set level and sequence level\nmetrics for an example utterance. Consider the\nfollowing utterance with the corresponding ground\ntruth and model output sequence.\nutterance: I’m planning a ski trip to Banff, AB in\nFebruary. Can you help me plan it out?\noutput_sequence:{Information Gathering API,\nBook Hotel API, Book Flight API, Book Train\nAPI}\ngound_truth:{Information Gathering API, Book\nFlight API, Book Hotel API, Rent Car API}\nFor set level metrics we will look at the intersec-\ntion of the set of APIs in output_sequence and\nground_truth. We have:\nset(output_sequence) ⋂ set(ground_truth) =\n{Information Gathering API, Book Hotel API,\nBook Flight API}\nThen, set level precision is:\n|set(output_sequence)\n⋂\nset(ground_truth)|\n|output_sequence| = 3\n4 ,\nand set level recall is:\n|set(output_sequence)\n⋂\nset(ground_truth)|\n|ground_truth| = 3\n5 .\nFor sequence level metrics, we will look at\nthe longest common subsequence (LCS) between\nthe sequence of APIs in output_sequence and\nground_truth. :\nLCS(output_sequence, ground_truth) = {Infor-\nmation Gathering API, Book Hotel API}.\nThen, the sequence level precision is:\n|LCS(output_sequence,ground_truth)|\n|output_sequence| = 2\n4,\nand the sequence level recall is:\n|LCS(output_sequence,ground_truth)|\n|ground_truth| = 2\n5.\nObserve that another common subsequence is {In-\nformation Gathering API, Book Flight API} which\nis also of length 2. If we use this instead, we will\nget the same sequence level precision and recall.\n6921",
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.7305760383605957
    },
    {
      "name": "Computer science",
      "score": 0.7014269232749939
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5850865840911865
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5057675838470459
    },
    {
      "name": "Data science",
      "score": 0.4397214949131012
    },
    {
      "name": "Knowledge management",
      "score": 0.37694090604782104
    },
    {
      "name": "Software engineering",
      "score": 0.3588169813156128
    },
    {
      "name": "Artificial intelligence",
      "score": 0.24790722131729126
    },
    {
      "name": "Programming language",
      "score": 0.11134219169616699
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}