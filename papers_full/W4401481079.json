{
  "title": "CSWin-UNet: Transformer UNet with cross-shaped windows for medical image segmentation",
  "url": "https://openalex.org/W4401481079",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1989526235",
      "name": "Liu Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109793426",
      "name": "Gao Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109255866",
      "name": "Yu Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1927837453",
      "name": "Wang Fei",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Yuan, Ru-Yue",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035665735",
    "https://openalex.org/W4283515839",
    "https://openalex.org/W4206693420",
    "https://openalex.org/W4303649515",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W6753182481",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W3118439879",
    "https://openalex.org/W4321240015",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W4308737504",
    "https://openalex.org/W6796814978",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W4214654781",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W6635487051",
    "https://openalex.org/W6799370480",
    "https://openalex.org/W6795435739",
    "https://openalex.org/W2987322772",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W4393150367",
    "https://openalex.org/W4393156082",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W3010749409",
    "https://openalex.org/W2943931219",
    "https://openalex.org/W3014974815",
    "https://openalex.org/W3182906273",
    "https://openalex.org/W4283796551",
    "https://openalex.org/W6791406852",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W6803821885",
    "https://openalex.org/W4319300975",
    "https://openalex.org/W4393158677",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6777983745",
    "https://openalex.org/W2804047627",
    "https://openalex.org/W6718240422",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W4255499827",
    "https://openalex.org/W3138634820",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2604292070",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W2963840672"
  ],
  "abstract": null,
  "full_text": "CSWin-UNet: Transformer UNet with Cross-Shaped\nWindows for Medical Image Segmentation\nXiao Liu\n , Peng Gao\n , Tao Yu\n , Fei Wang\n , Ru-Yue Yuan\nQufu Normal University\n Harbin Institute of Technology Shenzhen\nAbstract\nDeep learning, especially convolutional neural networks (CNNs) and Transformer\narchitectures, have become the focus of extensive research in medical image seg-\nmentation, achieving impressive results. However, CNNs come with inductive\nbiases that limit their effectiveness in more complex, varied segmentation scenarios.\nConversely, while Transformer-based methods excel at capturing global and long-\nrange semantic details, they suffer from high computational demands. In this study,\nwe propose CSWin-UNet, a novel U-shaped segmentation method that incorporates\nthe CSWin self-attention mechanism into the UNet to facilitate horizontal and ver-\ntical stripes self-attention. This method significantly enhances both computational\nefficiency and receptive field interactions. Additionally, our innovative decoder\nutilizes a content-aware reassembly operator that strategically reassembles features,\nguided by predicted kernels, for precise image resolution restoration. Our exten-\nsive empirical evaluations on diverse datasets, including synapse multi-organ CT,\ncardiac MRI, and skin lesions, demonstrate that CSWin-UNet maintains low model\ncomplexity while delivering high segmentation accuracy. Codes are available at\nhttps://github.com/eatbeanss/CSWin-UNet.\n1 Introduction\nMedical image segmentation is an essential research topic of medical image computing and computer\nassisted intervention, primarily by processing images to obtain helpful information, such as the shape,\nsize, and structure of diseased organs or tissues, providing more accurate and detailed diagnostic and\ntreatment recommendations[1, 2].\nDeep learning-based medical image segmentation methods can directly classify the entire image at\nthe pixel level and have been widely applied in multiple medical fields[3, 4], including lung computed\ntomography (CT) image segmentation, brain magnetic resonance image (MRI) segmentation, and\ncardiac ultrasound image segmentation. These methods not only improve segmentation accuracy\nbut also further advance the field of medical imaging. Convolutional neural network (CNN) is\none of the most widely used deep learning technologies in the field of computer vision. The fully\nconvolutional network (FCN)[5], an extension of CNN, promotes the development of the field of\nmedical image segmentation. Existing studies have proposed extended convolution and context\nlearning methods to address the limited receptive field of conventional convolutional operations[6, 7,\n8]. Moreover, UNet[9], with its innovative U-shaped encoder-decoder design and skip connections,\nand merges feature maps from the encoder and decoder to preserve critical spatial details from shallow\nlayers. This architecture has become a staple in image segmentation. Enhanced derivatives of UNet,\nsuch as UNet++ [10], AttentionUNet[11], and ResUNet[12], have further refined the segmentation\ncapabilities, offering improved performance across a spectrum of imaging modalities.\nDespite the successes of CNN-based methods in medical image segmentation, they are challenged\nby their limited capacity to capture global and long-range semantic information and inherent induc-\ntive biases[13, 14, 15]. Inspired by the transformative impact of the Transformer architecture in\narXiv:2407.18070v3  [eess.IV]  19 Sep 2024\nnatural language processing (NLP)[ 16], researchers have begun to apply this technology to com-\nputer vision tasks, aiming to mitigate some limitations of CNNs[ 17, 18, 19]. At the core of the\nTransformer architecture is the self-attention mechanism, which processes embedded information\nfrom all positions within the input sequence in parallel, rather than sequentially. This mechanism\nallows the Transformer to adeptly manage long-range information dependencies and adapt to varying\ninput sequence lengths. A specific adaptation for image processing, the Vision Transformer[ 20],\nexemplifies this by segmenting the input image into a series of fixed patches, each converted into a\nvector that is then processed by a Transformer encoder. Through the encoding stage, self-attention\nestablishes inter-patch relationships, capturing comprehensive contextual information. The resultant\nencoded features are subsequently utilized in tasks such as object detection and image segmentation,\nutilizing decoders or classifiers. The introduction of the Vision Transformer has not only infused fresh\nperspectives into image processing but also achieved results that rival or surpass those of traditional\nCNNs[21, 22, 23, 24]. Although the Transformer architecture excels in processing global and long-\nrange semantic information, its computational efficiency is often compromised due to the extensive\nnature of its self-attention mechanisms. Addressing this inefficiency, the Swin Transformer[ 25]\ninnovated with a window self-attention mechanism that limits attention to discrete windows within\nthe image, drastically reducing computational complexity. However, this approach somewhat restricts\nthe interaction among receptive fields. To overcome this, the CSWin Transformer[ 26] proposed\nthe cross-shaped window (CSWin) self-attention, which can compute self-attention horizontally\nand vertically in parallel, achieving better results at a lower computational cost. Additionally, the\nCSWin Transformer introduced local-enhanced positional encoding (LePE), which imposes positional\ninformation on each Transformer block. Unlike previous positional encoding methods[27, 28], LePE\ndirectly manipulates the results of attention weights rather than being added to the computation of\nattention. LePE makes the CSWin Transformer more effective for object detection and image seg-\nmentation. As the development of Transformers progresses, many studies have combined CNNs with\nTransformer blocks. TransUNet[13] and LeViT-UNet[29] integrated UNet with Transformers and\nachieved competitive results on abdominal multi-organ and cardiac segmentation datasets. In addition,\nsome researchers have developed segmentation models using pure Transformers. Swin-UNet[30]\nemployed Swin Transformer blocks to construct encoders and decoders in an UNet-like architecture,\nshowing improved performance compared to TransUNet[31]. However, this segmentation method\nbased on Swin Transformer still has limitations in receptive field interaction, and the computational\ncost is also relatively high.\nMedical images usually have high resolution and contain many interrelated delicate structures. One\nof our primary concerns is how to better handle long-range dependencies in medical images with\nless computational resource consumption. Furthermore, accurate boundary segmentation in medical\nimages is crucial for diagnosis and treatment compared to semantic segmentation. Therefore, another\nfocus of our study is how to retain more detailed information and provide more explicit boundaries\nduring the segmentation process. Inspired by the innovative CSWin Transformer[26], we introduce\na novel Transformer-based method, dubbed CSWin-UNet, for medical image segmentation. This\nmethod is designed to reduce computational costs while simultaneously enhancing segmentation\naccuracy. Unlike TransUNet[13], a CNN-Transformer hybrid architecture, CSWin-UNet, similar\nto Swin-UNet[30], is a pure Transformer-based U-shaped architecture. The critical difference be-\ntween CSWin-UNet and Swin-UNet is that the former equipped CSWin Transformer blocks in both\nthe encoder and decoder and designed different numbers of blocks according to different scales.\nMoreover, we introduced the CARAFE (Content-Aware ReAssembly of FEatures) layer[ 31] for\nupsampling in the decoder. Initially, input medical images are transformed into convolutional token\nembeddings, which are then processed by the encoder to extract contextual features. These features\nare subsequently upsampled by the CARAFE layer, which enables precise feature reassembly. Ad-\nditionally, skip connections are employed to fuse high-level semantic information with low-level\nspatial details continuously. The process culminates in the transformation of feature embeddings\ninto a segmentation mask that matches the original input sizes. Through cross-shaped window\nself-attention, our method can maintain the efficient feature extraction capability for medical images\nwhile reducing computational complexity. Additionally, by combining the classic architecture of\nUNet, it can effectively integrate features of different scales in the encoder and decoder, thereby\nimproving segmentation accuracy. Finally, the introduction of the CARAFE layer for upsampling can\nmore effectively preserve the edges and detailed features of the segmentation objects. Experimental\nevaluations of our CSWin-UNet method reveal superior segmentation accuracy and robust gener-\nalization capabilities compared to other existing methods. Furthermore, it showcases considerable\n2\nh1\nhN\n(a) Full attention\nhN\n(c) Criss-cross \nh1\nhN\nh1\n(b) Shifted local\nBlock\nNext\nhN\nh1\nhN\nh1\n(d) Sequential axial\nhN\nh1\nBlock\nNext\nFigure 1: Illustration of different self-attention mechanisms. hi denotes the i-th attention head.\nadvantages in reducing computational complexity for medical image segmentation tasks. The key\ncontributions of this study are detailed below:\n• We developed a novel U-shaped encoder-decoder network architecture, CSWin-UNet, utiliz-\ning CSWin Transformer blocks specifically tailored for medical image segmentation.\n• The CSWin self-attention mechanism was incorporated to implement horizontal and vertical\nstripes self-attention learning. This enhancement significantly broadens the focus area for\neach token, facilitating more comprehensive analysis and contextual integration.\n• In the decoder, the CARAFE layer was employed as an alternative to the traditional trans-\nposed convolution or interpolation strategies for upsampling. This choice allows for more\naccurate pixel-level segmentation masks.\n• Comprehensive experimental results validate that CSWin-UNet is not only lightweight but\nalso demonstrates efficient performance, surpassing existing methods in both computational\nefficiency and segmentation accuracy.\nThe structure of this paper is organized as follows. Section 2 reviews recent works and developments\nin the field of medical image segmentation, setting the context for the innovations introduced in this\nstudy. Section 3 describes in detail the methodology of the newly proposed CSWin-UNet, highlighting\nthe novel aspects of the architecture and its components. Section 4 presents the experimental\nresults, demonstrating the effectiveness and efficiency of CSWin-UNet compared to existing methods.\nSection 5 concludes the paper.\n2 Related works\n2.1 Self-attention mechanisms in image segmentation\nThe application of self-attention mechanisms in image segmentation has been widely studied. Re-\nsearch in [32, 33, 34] demonstrates that designing different self-attention mechanisms for suitable\nscenarios can significantly improve segmentation performance. Medical image segmentation tasks\noften involve subtle but critical structures, and self-attention mechanisms can better capture the\nrelationships between these complex structures, making the design of effective and appropriate\nself-attention mechanisms particularly important. However, many existing vision Transformers still\nuse global attention mechanisms with high computational complexity, as shown in Fig.1(a). To\naddress this issue, Swin Transformer[25] adopts a shifted version of the local self-attention mecha-\nnism, as depicted in Fig.1(b), achieving interactions between different windows through a sliding\n3\nwindow mechanism. Additionally, axial self-attention[ 35] and criss-cross attention[ 36] calculate\nattention within stripes along horizontal and vertical directions, as shown in Fig.1(c) and (d), re-\nspectively. Nevertheless, axial self-attention is limited by the sequential mechanism and window\nsize, while criss-cross attention performs poorly in specific applications due to overlapping windows.\nCSWin Transformer[26] introduces cross-shaped window (CSWin) self-attention, which can compute\nself-attention for horizontal and vertical stripe regions in parallel. Compared to previous attention\nmechanisms, this attention mechanism is more general and effective in handling image processing\ntasks.\n2.2 CNN-based medical image segmentation\nIn medical image segmentation, CNNs are predominantly employed, with several vital architectures\ndriving advancements in the field. Among these, FCN[5] stands out as an end-to-end architecture that\nclassifies pixels directly, converting fully connected layers to convolutional ones to accommodate\nimages of any size. The UNet[ 9] model, featuring a symmetric U-shaped encoder-decoder archi-\ntecture, excels in delivering accurate segmentation of medical images. Building on the foundation\nlaid by FCN and UNet, several enhanced methods have been proposed. For instance, SegNet[ 37]\nincorporates ideas from both FCN and UNet, utilizing max-pooling operators to refine the accuracy of\nsegmentation masks, and has been effectively applied in diverse medical segmentation tasks[38, 39].\nUNet++[10] extends the original UNet design by integrating densely nested skip connections, which\nminimize information loss between the encoder and decoder, thereby improving segmentation perfor-\nmance. AttentionUNet[11] augments the UNet architecture with attention mechanisms to increase\nboth accuracy and robustness. Lastly, nnU-Net[ 40] introduces an adaptive approach to network\narchitecture selection, automatically optimizing model configurations based on specific task require-\nments and dataset characteristics, thus enhancing adaptability across various segmentation challenges.\nAdditionally, MRNet[41] proposed a multi-rater agreement model to calibrate segmentation results,\nand Pan et al.[42] designed a hybrid-supervised learning strategy to address the issue of scarce\nmedical image labels.\n2.3 Transformer-based medical image segmentation\nGiven the high resolution and complexity of medical images, which encompass a vast number of\npixels and intricate local features, traditional CNN-based medical image segmentation methods, while\neffective at capturing detailed image information, often fall short in accessing global and long-range\nsemantic contexts. In contrast, with their global contextual modeling capabilities, Transformers play a\npivotal role in boosting segmentation performance by effectively encoding larger receptive fields and\nlearning relationships between distant pixels. This advantage has spurred researchers to incorporate\nTransformers into medical image segmentation frameworks. For example, TransUNet[13] employs\na Transformer as the encoder to derive contextual representations from medical images, coupled\nwith a UNet-based decoder for precise pixel-level segmentation. This combination illustrates the\nenhanced ability of Transformers to capture global contextual information, leading to improved\nsegmentation accuracy. Similarly, TransFuse[43] integrates CNN and Transformer branches within a\nsingle framework, using a specialized module to merge outputs from both pathways to produce final\nsegmentation masks. Further, UNetR[44] utilizes a Transformer to encode input 3D images, paired\nwith a CNN decoder to complete the segmentation process, while MT-UNet[45] introduces a hybrid\nTransformer architecture that learns both intra- and inter-sample relationships. HiFormer[46] presents\nanother hybrid model, combining two CNNs with a Swin Transformer module and a dual-level\nfusion module to integrate and transfer multi-scale feature information to the decoder. Among purely\nTransformer-based methods, Swin-UNet[30] uses a Swin Transformer[25] as the encoder to capture\nglobal contextual embeddings, which are then progressively upsampled by a UNet decoder, leveraging\nskip connections to enhance detail preservation. Additionally, DFQ [47] introduced decoupled feature\nqueries within a Vision Transformer (ViT) framework, enabling segmentation models to adapt more\nbroadly to different tasks.\nInspired by the advancements in multi-head self-attention mechanisms, specifically the CSWin\nTransformer[26], we developed CSWin-UNet, a medical image segmentation method based on CSWin\nself-attention. This model further conserves computational resources while elevating segmentation\naccuracy, representing a significant stride forward in the application of Transformers to medical\nimage segmentation.\n4\nConvolutional Token \nEmbedding\n(7×7, stride 4)\nCSWin Transformer \nBlock ×1\nConvolution Layer\nCSWin Transformer \nBlock ×2\nConvolution Layer\nCSWin Transformer \nBlock ×9\nConvolution Layer\nCSWin Transformer \nBlock ×1\nCSWin Transformer \nBlock ×1\nCarafe Layer\nCSWin Transformer \nBlock ×2\nCarafe Layer\nCSWin Transformer \nBlock ×9\nCarafe Layer\nCSWin Transformer \nBlock ×1\nLinear Projection\nCarafe Layer (4×)\nSkip Connection\nSkip Connection\nSkip Connection\nFigure 2: Overview of the proposed CSWin-UNet. The decoder and encoder are symmetrical and\neach consists of four stages.\n3 Methodology\nThe overall architecture of CSWin-UNet is illustrated in Fig.2, which consists of an encoder, a\ndecoder, and skip connections, with the basic unit being the CSWin Transformer block. For medical\nimages with an input dimension of H × W × 3, similar to CvT[34], we utilize convolutional token\nembedding (with a 7 ×7 kernel and a stride of 4) to obtainH/4 ×W/4 patch tokens with C channels.\nBoth the encoder and decoder are consisted of four stages. Like UNet[5], skip connections are\nemployed to merge features at each stage of the encoder and decoder to retain contextual information\nbetter. In the encoder, convolutional layers (with a 3 × 3 kernel and a stride of 2) are used for\ndownsampling, reducing the resolution to half of its input size while doubling the channel count.\nUpsampling in the decoder is performed through the CARAFE Layer, increasing the resolution to\ntwice its input size while halving the channel count. Finally, a 4× CARAFE upsampling operation\nis performed to restore the resolution to the input resolution H × W, and a linear layer is used to\nconvert the feature map into a segmentation mask.\n3.1 CSWin Transformer block\nWith its self-attention mechanism, the traditional Transformer architecture excels in establishing\nglobal semantic dependencies by processing all pixel positions, which, however, leads to high\ncomputational costs in high-resolution medical imaging. The Swin Transformer[25] mitigates these\ncosts with a shifted window attention mechanism that divides the image into distinct, non-overlapping\nwindows, allowing for localized self-attention. This adaptation helps manage the high resolution\nof images while controlling computational complexity. Yet, the effectiveness of this approach\ndepends on the window size; smaller windows might miss some global information, while larger\nones could unnecessarily raise computational demands and storage. Contrasting with the shifted\nwindow attention mechanism, CSWin self-attention organizes attention into horizontal and vertical\nstripes, enhancing parallel computation capabilities. This structure not only conserves computational\nresources but also broadens the interaction within receptive fields. As depicted in Fig.3, the CSWin\n5\nLN\nCross-Shaped\nWindow Self-Attention\nLN\nMLP\nFigure 3: Pipeline of the CSWin Transformer Block.\nsw\nsw\nSplit Head\nConcat\nh1\nhN/2\nhN/2+1\nhN\nFigure 4: Illustration of the CSWin self-attention mechanism. First, split the multiple heads\n{h1, h2, . . . , hN } into two groups {h1, h2, . . . , hN/2} and {hN/2+1, hN/2+2, . . . , hN }, perform-\ning self-attention in parallel on the horizontal and vertical stripes, respectively, and concatenate the\noutputs. Next, the width of the stripe sw can be adjusted to achieve optimal performance. Generally,\nchoose a smaller sw for higher resolutions and a larger sw for lower resolutions.\nTransformer block, built on this innovative self-attention design, includes a CSWin self-attention\nmodule, a LayerNorm (LN) layer, a multi-layer perceptron (MLP), and skip connections. This\nconfiguration optimally balances local and global information processing, significantly improving\nefficiency and effectiveness in complex medical image segmentation tasks.\nIn the multi-head self-attention mechanism, the input feature X ∈ RH×W×C undergoes an initial\ntransformation where it is linearly mapped across N heads, with N typically chosen as an even\nnumber. Distinct from conventional self-attention and the shifted window-based multi-head self-\nattention, the CSWin self-attention uniquely facilitates local self-attention learning within divided\nhorizontal or vertical stripes, as depicted in Fig.4. This configuration allows each head to compute\nself-attention within its designated stripe either horizontally or vertically. These operations are\nperformed in parallel, effectively broadening the scope of the attention computation area while\nsimultaneously reducing the overall computational complexity.\nIn the horizontal stripes self-attention configuration of the CSWin Transformer, the input featureX is\nsystematically divided into M non-overlapping horizontal stripes, represented as [X1\nh, X2\nh, . . . , XM\nh ],\nwhere each stripe has a width sw, and M is determined by the ratio H/sw. The parameter sw is\nadjustable and critical for balancing computational complexity with the model’s learning capability.\nSpecifically, a larger sw enhances the model’s ability to explore long-range pixel correlations within\neach stripe, potentially capturing more extensive contextual information. Consider the computation\nwithin a specific head, denoted as the n-th head. In this scenario, the dimensions of the queries (Q),\n6\nkeys (K), and values (V) are each dn = C/N, where C is the number of channels, and N is the total\nnumber of heads. The self-attention output Y i\nn for the n-th head within the i-th horizontal stripe is\ncalculated as follows:\nQi\nn = WQ\nn Xi\nh\nKi\nn = WK\nn Xi\nh\nV i\nn = WV\nn Xi\nh\nY i\nn = Softmax\n\u0012Qi\nn(Ki\nn)T\n√dn\n\u0013\nV i\nn\n(1)\nwhere Xi\nh ∈ Rsw×W×C is the feature map of the i-th horizontal stripe; WQ\nn ∈ RC×dn, WK\nn ∈\nRC×dn, and WV\nn ∈ RC×dn represent the weight matrix of Q, K and V of the n-th head. This\noperation is performed separately and in parallel for each stripe to allow self-attention within that\nspecific horizontal stripe. The self-attention from M horizontal stripes are concatenated to construct\nthe horizontal self-attention H-Attentionn for the n-th head:\nX = [X1\nh, X2\nh, . . . , XM\nh ]\nH-Attentionn(X) = [Y 1\nn , Y2\nn , . . . , YM\nn ]\n(2)\nSimilar to horizontal stripes self-attention, the input feature X ∈ RH×W×C is evenly divided into S\nnon-overlapping vertical stripes [X1\nv , X2\nv , . . . , XS\nv ] for vertical self-attention, where the stripe width\nis also sw, and S = W/sw. Considering the n-th attention head as an example, where the dimensions\nof Q, K, and V are dn = C/N. The self-attention output Y i\nn for the n-th head within the i-th vertical\nstripe can be computed as follows:\nQi\nn = WQ\nn Xi\nv\nKi\nn = WK\nn Xi\nv\nV i\nn = WV\nn Xi\nv\nY i\nn = Softmax\n\u0012Qi\nn(Ki\nn)T\n√dn\n\u0013\nV i\nn\n(3)\nwhere Xi\nv ∈ RH×sw×C is the feature map of the i-th vertical stripe. The self-attention from S\nvertical stripes are concatenated to construct the vertical self-attentionV-Attentionn for the n-th head:\nX = [X1\nv , X2\nv , . . . , XS\nv ]\nV-Attentionn(X) = [Y 1\nn , Y2\nn , . . . , YS\nn ]\n(4)\nWe split the N heads into two groups, each containing N/2 heads. Each head within these groups\ngenerates its self-attention output. The first group is tasked with learning horizontal stripes self-\nattention, while the second group learns vertical stripes self-attention. After computing the self-\nattention separately, the outputs from these two groups are concatenated. This concatenation is\nperformed along the channel dimension as:\nhn =\n\u001aH-Attentionn(X), n = 1, 2, . . . , N/2\nV-Attentionn(X), n = N/2 + 1, N/2 + 2, . . . , N\nCSWin-Attention(X) = concat(h1, h2, . . . ,hN )Wo\n(5)\nwhere hn denotes the n-th attention head; Wo ∈ RC×C is a weight matrix used to linearly transform\nthe concatenated output of the multi-head self-attention mechanism to produce the final attention\noutput, this linear transformation can help to learn the relationship between different heads and fuse\nthe attention information. The concatenated output effectively incorporates horizontal and vertical\ncontextual information, comprehensively learning the spatial relationships within the input image.\nBased on the above self-attention mechanism, the CSWin Transformer block can be defined as:\nˆXl = CSWin-Attention\n\u0000\nLN(Xl−1)\n\u0001\n+ Xl−1\nXl = MLP\n\u0010\nLN( ˆXl)\n\u0011\n+ ˆXl (6)\nwhere Xl represents the output of the l-th CSWin Transformer block or the precedent convolutional\nlayer of each stage.\n7\n3.2 Encoder\nIn the encoder, the input image dimensions are H/4 × W/4 × C, which then enter four stages for\nfeature extraction. Downsampling operations accompany the first three stages. The number of CSWin\nTransformer blocks varies across the four stages, and details on block count settings will be discussed\nlater. The downsampling layer is implemented by a convolutional layer with a kernel size of3 × 3\nand a stride of 2, which reduces the resolution to half of its input size while doubling the number of\nchannels. The stripe width sw changes according in different stages. As the resolution continuously\ndecreases and the channel number increases, a smaller sw is chosen in stages with larger resolutions\nand a larger sw in stages with smaller resolutions, effectively enlarging the attention region of each\ntoken in stages with smaller resolutions. Additionally, the input image resolution is 224 × 224. To\nensure the mediate feature map size of the input image can be divided by sw, we set sw for the four\nstages are 1, 2, 7, and 7.\n3.3 Decoder\nCorresponding to the encoder, the decoder also has four stages. Image resolution and channel number\nincreases are achieved through the CARAFE layer in the last three stages. The number of CSWin\nTransformer blocks and the stripe width sw for attention learning in the four stages are consistent\nwith those set in the encoder. Commonly used upsampling methods include linear interpolation and\ntransposed convolution. Bilinear interpolation only considers adjacent pixels and may blur the edges\nof the image, leading to unclear boundaries in the segmentation results, whereas the receptive field of\ntransposed convolution is usually constrained by the kernel size and stride, not only limiting its ability\nto represent local variations but also requiring the learning of the weights and biases of the transposed\nconvolution kernels. Unlike these methods, we use the CARAFE[31] to achieve upsampling.\nThe CARAFE layer is an advanced upsampling mechanism comprising two principal components: a\nkernel prediction module and a content-aware reassembly module. The kernel prediction module\ninitiates with a convolution layer tasked with predicting the reassembly kernels from the encoded\nfeatures. It includes three sub-modules: channel compressor, context encoder, and kernel normalizer.\nThe channel compressor reduces the dimensionality of the channel space in the input feature map\nX ∈ RH×W×C, thereby lowering computational complexity and focusing on essential feature\ninformation. Following channel compression, the context encoder processes the reduced feature\nmap to encode contextual information, which is instrumental in generating the reassembly kernels.\nEach predicted reassembly kernel undergoes normalization through a Softmax function in the kernel\nnormalizer to ensure that the output distribution of the weights is probabilistic and sums to one,\nenhancing the stability and performance of the upsampling process. With an upsampling ratio σ\n(where σ is an integer), CARAFE aims to generate an expanded feature map X′ ∈ RσH×σW ×C.\nFor each pixel l′ = (i′, j′) in X′, it corresponds to a specific pixel l = (i, j) in X, determined by\ni = ⌊i′/d⌋ and j = ⌊j′/d⌋. The kernel prediction moduleψ predicts a unique reassembly kernel Wl′\nfor each pixel l′ based on the neighborhood N(Xl, k), which is a k × k region centered around pixel\nl on X. This neighborhood extracts localized features, which the predicted kernel uses to effectively\nreassemble and upsample the feature map.\nWl′ = ψ (N(Xl, kencoder)) (7)\nwhere kencoder denotes the receptive filed of the content encoder.\nThe second step is content-aware reassembly, where the input features are reassembled using a\nconvolutional layer, and the content-aware reassembly module ϕ reassembles N(Xl, k) with the\nreassembly kernel Wl′ :\nX′\nl′ = ϕ (N(Xl, kup), Wl′ ) (8)\nwhere kup is the size of reassembly kernel. For each reassembly kernel Wl′ , the content-aware\nreassembly module reassembles the features within the local square region. The module ϕ performs a\nweighted summation. For a pixel positionl and its centered neighborhoodN(Xl, kup), the reassembly\nprocess is as follows:\nX′\nl′ =\nrX\nn=−r\nrX\nm=−r\nWl′(n,m) · X(i+n,j+m) (9)\nwhere r = ⌊kup/2⌋.\n8\nEach pixel within N(Xl, kup) contributes differently to the upsampled pixel l′. The reassembled\nfeature map can enhance the focus on relevant information within the local area, providing more\nrobust semantic information than the original feature map. Additionally, similar to UNet[ 9], we\nuse skip connections to merge the feature maps outputted from the encoder and decoder, thereby\nproviding more prosperous and more accurate spatial information that helps recover image detail.\nSubsequently, a 1×1 convolution kernel is used to reduce the number of channels after concatenation,\nensuring consistency with the number of feature channels in the upsampling process.\n4 Experiments\n4.1 Implementation details\nCSWin-UNet is implemented using Python and the PyTorch framework. The model training and\nevaluation are conducted on an NVIDIA ® GeForce RTXTM 3090 GPU with 24GB VRAM. We\ninitialize the CSWin Transformer blocks with pretrained weights from ImageNet[ 48] to leverage\nprior knowledge and accelerate the convergence process. For data augmentation, schemes such as\nflipping and rotation are employed to enhance the diversity of the training dataset, thereby helping\nthe model generalize better to unseen data. During the training phase, the batch size is set to 24,\nand we use a learning rate of 0.05. Optimization is performed using the stochastic gradient descent\n(SGD) method with a momentum of 0.9 and a weight decay factor of 10−4. This setup is chosen to\noptimize the balance between rapid learning and convergence stability. Furthermore, to effectively\ntrain CSWin-UNet, we employ a combined loss function that integrates Dice and cross-entropy losses,\ndefined as follows:\nLoss = αLossDice + βLosscross (10)\nwhere α and β are two hyperparameters used to balance the impact of LossDice and Losscross\non the final loss, respectively. This combined loss targets both pixel-level accuracy and holistic\nsegmentation quality, ensuring robust learning and improved performance across varied medical\nimage segmentation tasks.\n4.2 Datasets and metrics\n4.2.1 Synapse dataset\nThe synapse multi-organ segmentation dataset includes 30 CT scans from the MICCAI 2015 Multi-\nAtlas Abdominal Organ Segmentation Challenging, encompassing a total of 3779 abdominal CT\nimages. Each CT scan consists of 85 to 198 slices with 512 × 512 pixels each, and each voxel\nmeasures ([0.54, 0.54] × [0.98, 0.98] × [2.5, 5.0])mm3. Following the settings in literature[49, 13],\n18 sets are selected for training and 12 for evaluation. The segmentation performance on eight types\nof abdominal organs (aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen, stomach) is\nevaluated using the mean Dice-similarity coefficient (DSC) and the mean Hausdorff Distance (HD)\nas metrics.\n4.2.2 ACDC dataset\nThe Automated Cardiac Diagnosis Challenge (ACDC) dataset was released during the 2017 ACDC\nchallenge and raised a multi-category cardiac 3D MRI dataset that includes 100 sets of short-axis\nMR cardiac images obtained through cine MR 1.5T and 3T scanners. Medical experts provided\nannotations for three cardiac structures: the right ventricle (RV), myocardium (MYO), and left\nventricle (LV) [50]. We randomly selected 70 sets of MR images for training, 10 for validation,\nand 20 for evaluation. The ACDC dataset uses the mean DSC as an evaluation metric to assess the\nsegmentation results of the three cardiac structures.\n4.2.3 Skin lesion segmentation datasets\nWe conducted experiments on the ISIC2017[51], ISIC2018[52], and PH2[53] datasets. The ISIC\ndatasets include a large number of dermoscopic images, covering various skin lesions. Following\nthe settings in HiFormer[46], we used 1400 images for training, 200 images for validation, and 400\nimages for testing in the ISIC2017 dataset; 1815 images for training, 259 images for validation, and\n520 images for testing in the ISIC2018 dataset; and 80 images for training, 20 images for validation,\n9\nTable 1: Detailed comparisons with recent medical image segmentation methods in terms of DSC\nand HD on the Synapse dataset. The first and second best values are highlighted in red and blue\nfonts, respectively.\nMethods Backbone DSC↑ HD↓ Aorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\nV-Net[54] CNN 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98DARR[49] CNN 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96UNet[9] CNN 76.85 39.7089.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58Att-UNet[11] CNN 77.77 36.0289.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75HiFormer-B[46]CNN+Transformer80.39 14.7086.21 65.69 85.23 79.77 94.61 59.52 90.99 81.08MT-UNet[45]CNN+Transformer78.59 26.5987.92 64.99 81.47 77.29 93.06 59.46 87.75 76.81TransUNet[13]CNN+Transformer77.48 31.6987.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62Swin-UNet[30]Transformer79.13 21.5585.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60CSWin-UNet Transformer81.12 18.8687.13 67.85 83.51 78.53 95.23 65.94 89.05 81.74\nDSC HD\nAorta\nGallbladderLeft KidneyRight Kidney\nLiver\nPancreas Spleen Stomach\n0\n20\n40\n60\n80\n100 Mean ± 95% CI\nFigure 5: Error bars (95% confidence interval) of mean DSC, mean HD, and DSC for each organ on\nthe Synapse dataset.\nand 100 images for testing in the PH2 dataset. We evaluated the skin lesion segmentation task using\nmean DSC, sensitivity (SE), specificity (SP), and accuracy (ACC) as metrics.\n4.3 Results on Synapse dataset\nAs shown in Table 1, our proposed method on the Synapse dataset has improved the mean DSC and\nHD. Meanwhile, we represent error bars (95% confidence interval) of mean DSC, mean HD, and\nDSC for each organ in Fig.5. Compared to TransUNet[13] and Swin-UNet[30], our mean DSC has\nincreased by 3.64% and 1.99%, respectively, and the mean HD has improved by 12.83% and 2.69%.\nNotably, in the segmentation of the pancreas, the DSC of CSWin-UNet is significantly higher than\nthat of other segmentation methods. Unlike other organs, the pancreas has blurry boundaries and\nvariability, and our method achieved more precise segmentation results for the pancreas, indicating\nthat our CSWin-UNet provides higher segmentation accuracy in complex segmentation environments.\nTo more intuitively evaluate the proposed method, we conducted a visual analysis of the segmentation\nresults. Fig.6 displays the comparative results on the Synapse dataset. The first row shows that\nSwin-UNet and HiFormer-B exhibit significant errors in segmenting small organs like the gallbladder\n(green label), with Swin-UNet[30] failing to delineate boundaries accurately and HiFormer-B[46]\nmistakenly identifying other areas as the gallbladder. The second row indicates that Swin-UNet,\nTransUNet[13], HiFormer-B, and UNet[9] all fail to segment the stomach (orange label) completely.\nThe third row reveals that Swin-UNet and HiFormer-B incorrectly label large areas of other organs as\nthe pancreas (yellow label). Considering the quantitative metrics and visual results, our proposed\nCSWin-UNet achieves accurate segmentation of refined and complex organs, produces more accurate\nsegmentation results, demonstrates greater robustness against complex backgrounds, and performs\nsuperior edge structure handling.\n10\n/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b\n/uni00000026/uni00000036/uni0000003a/uni0000004c/uni00000051/uni00000010/uni00000038/uni00000031/uni00000048/uni00000057/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\n/uni00000036/uni0000005a/uni0000004c/uni00000051/uni00000010/uni00000038/uni00000031/uni00000048/uni00000057\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000038/uni00000031/uni00000048/uni00000057\n/uni0000002b/uni0000004c/uni00000029/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni00000038/uni00000010/uni00000031/uni00000048/uni00000057\n/uni00000024/uni00000052/uni00000055/uni00000057/uni00000044\n/uni0000002a/uni00000044/uni0000004f/uni0000004f/uni00000045/uni0000004f/uni00000044/uni00000047/uni00000047/uni00000048/uni00000055\n/uni0000002f/uni00000048/uni00000049/uni00000057/uni00000003/uni0000002e/uni0000004c/uni00000047/uni00000051/uni00000048/uni0000005c\n/uni00000035/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003/uni0000002e/uni0000004c/uni00000047/uni00000051/uni00000048/uni0000005c\n/uni0000002f/uni0000004c/uni00000059/uni00000048/uni00000055\n/uni00000033/uni00000044/uni00000051/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056\n/uni00000036/uni00000053/uni0000004f/uni00000048/uni00000048/uni00000051\n/uni00000036/uni00000057/uni00000052/uni00000050/uni00000044/uni00000046/uni0000004b\nFigure 6: Segmentation results of eight abdominal organs (aorta, gallbladder, left kidney, right kidney,\nliver, pancreas, spleen, stomach) in the Synapse dataset using different methods.\nTable 2: Comparison results of top-tier medical image segmentation methods on the ACDC dataset.\nThe first and second best values are highlighted in red and blue fonts, respectively.\nMethods Backbone DSC ↑ RV MYO LV\nUNet[9] CNN 87.55 87.10 80.63 94.92\nAtt-UNet[11] CNN 86.75 87.58 79.20 93.47\nnnUNet[40] CNN 90.91 89.21 90.20 93.35\nUNetR[44] CNN+Transformer 88.61 85.29 86.52 94.02\nTransUNet[13] CNN+Transformer 89.71 88.86 84.53 95.73\nSwin-UNet[30] Transformer 90.00 88.55 85.62 95.83\nCSWin-UNet Transformer 91.46 89.68 88.94 95.76\nDSC RV Myo LV\n70\n75\n80\n85\n90\n95\n100\nMean ± 95% CI\nFigure 7: Error bars (95% confidence interval) of mean DSC and DSC for each cardiac structure on\nthe ACDC dataset.\n4.4 Results on ACDC dataset\nTable 2 presents the experimental results of our proposed CSWin-UNet on the ACDC dataset and\ncompares them with other advanced methods. Fig.7 represents the error bars (95% confidence\ninterval) of mean DSC and DSC for each cardiac structure. In the table, RV represents the right\n11\nTable 3: Comparative results of the proposed CSWin-UNet with other state-of-the-art methods on\nthree skin lesion segmentation datasets. The first and second best values are highlighted in red and\nblue fonts, respectively.\nMethod Backbone ISIC2017[51] ISIC2018[52] PH2[53]DSC↑ SE↑ SP↑ ACC↑ DSC↑ SE↑ SP↑ ACC↑ DSC↑ SE↑ SP↑ ACC↑UNet[9] CNN 81.59 81.72 96.80 91.6485.45 88.00 96.97 94.0489.36 91.25 95.88 92.33Att-UNet[11] CNN 80.82 79.98 97.76 91.4585.66 86.7498.63 93.76 90.03 92.05 96.40 92.76TransUNet[13]CNN+Transformer81.23 82.63 95.77 92.0784.99 85.78 96.53 94.5288.40 90.63 94.27 92.00HiFormer-B[46]CNN+Transformer92.53 91.55 98.40 97.0291.02 91.1997.55 96.21 94.60 94.20 97.7296.61Swin-UNet[30]Transformer91.83 91.42 97.98 97.0189.46 90.5697.98 96.45 94.49 94.10 95.6496.78CSWin-UNet Transformer91.47 93.79 98.56 97.2691.11 92.3197.88 95.25 94.29 95.63 97.82 96.82\nISIC2017 ISIC2018 PH²\n70\n75\n80\n85\n90\n95\n100\nDSC SE SP ACC Mean ± 95% CI\nFigure 8: Error bars (95% confidence interval) of DSC, SE, SP, and ACC on ISIC2017, ISIC2018,\nand PH2 datasets.\nTable 4: Computational efficiency of different medical image segmentation methods on the Synapse\ndataset. The first and second best values are highlighted in red and blue fonts, respectively.\nModel Backbone #Params (M) FLOPs (G) DSC↑ HD↓\nTransUNet[13] CNN+Transformer 96.07 88.91 77.48 31.69\nUNetR[44] CNN+Transformer 86.00 54.70 79.56 22.97\nHiFormer-B[46] CNN+Transformer 25.51 8.05 80.39 14.70\nSwin-UNet[30] Transformer 27.17 6.16 79.13 21.55\nCSWin-UNet Transformer 23.57 4.72 81.12 18.86\nventricle, MYO represents the myocardium, and LV represents the left ventricle. The results indicate\nthat the proposed CSWin-UNet achieves better identification and segmentation of these organs, with\nan accuracy rate of 91.40%, demonstrating good generalization capabilities and robustness.\n4.5 Results on skin lesion segmentation datasets\nTable 3 shows the experimental results, and Fig.8 shows the error bars (95% confidence interval) of\nDSC, SE, SP, and ACC on three skin lesion segmentation datasets. The experimental results indicate\nthat the proposed CSWin-UNet outperforms other methods in most evaluation metrics. Notably,\nCSWin-UNet achieves better performance than Swin-UNet[ 30] in most metrics, demonstrating\nsatisfactory generalization capability. We also visualized the skin lesion segmentation results in Fig.9.\nCompared to Swin-UNet[30], our CSWin-UNet has certain advantages in preserving the edges and\ndetailed features of the segmented objects. However, in cases of low contrast or occlusion, as shown\nin Fig.9(d), the segmentation produces significant errors.\n4.6 Comparison of computational efficiency\nAn essential objective for neural network model design is to reduce the parameter count and compu-\ntational complexity as much as possible while maintaining its performance. This reduction is crucial\n12\n/uni00000044/uni00000011\n/uni00000045/uni00000011\n/uni00000046/uni00000011\n/uni0000002c/uni00000051/uni00000053/uni00000058/uni00000057/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048\n/uni00000047/uni00000011\n/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b\n/uni00000026/uni00000036/uni0000003a/uni0000004c/uni00000051/uni00000010/uni00000038/uni00000031/uni00000048/uni00000057/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\n/uni00000036/uni0000005a/uni0000004c/uni00000051/uni00000010/uni00000038/uni00000031/uni00000048/uni00000057\nFigure 9: Visual comparison of segmentation results of CSWin-UNet and Swin-UNet on the ISIC2017\ndataset. The ground truth and the predicted segmentation boundaries are shown in red and blue,\nrespectively.\nTable 5: Ablation study of different upsampling strategies on the Synapse dataset. The first and\nsecond best values are highlighted in red and blue fonts, respectively.\nStrategies DSC↑ Aorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach#Params(M) FLOPs(G)\nBilinearinterpolation78.82 85.32 57.09 82.68 75.14 94.35 65.12 88.86 81.98 23.39 4.38\nTransposedconvolution80.31 86.06 65.10 85.77 79.79 94.58 61.73 89.68 79.79 26.03 8.88\nCARAFE 81.12 87.13 67.85 83.51 78.53 95.23 65.94 89.05 81.74 23.57 4.72\nfor enabling more efficient model training and deployment in devices with limited computational\nresources. Therefore, in evaluating a model, it is necessary to consider not only its accuracy and\ngeneralizability but also its parameter count and computational complexity. Here, we use FLOPs and\nparameters (in millions, M) to measure computational complexity. The performance comparison on\nthe Synapse dataset is shown in Table 4. The results demonstrate that the proposed CSWin-UNet\nachieves excellent segmentation performance under conditions of the lowest complexity.\n4.7 Ablation studies\nIn this section, we conducted an ablation study on the performance of CSWin-UNet on the Synapse\ndataset. Specifically, we explored the effects of different upsampling strategies in the decoder, the\nnumber of skip connections, different network architecture, and different hyperparameters in the\ncombined loss function on the performance.\n13\nTable 6: Ablation study of different numbers of skip connection on the Synapse dataset. The first and\nsecond best values are highlighted in red and blue fonts, respectively.\n# Skip connectionDSC↑ Aorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\n0 65.41 62.22 49.25 69.89 66.60 90.21 39.10 77.57 68.42\n1 76.19 84.53 65.51 77.18 71.50 93.75 54.32 86.75 75.98\n2 79.85 86.21 64.98 81.60 76.10 94.27 62.59 90.66 82.44\n3 81.12 87.13 67.85 83.51 78.53 95.23 65.94 89.05 81.74\nTable 7: Ablation study of different numbers of CSWin Transformer blocks of each stage on the\nSynapse dataset. The first and second best values are highlighted in red and blue fonts, respectively.\n# Blocks # Params (M) FLOPs (G) DSC↑ HD↓\n[1, 2, 6, 1] 18.82 3.79 79.19 24.88\n[1, 2, 9, 1] 23.57 4.72 81.12 18.86\n[1, 2, 12, 1] 28.32 5.65 80.15 20.43\n4.7.1 Upsampling strategy\nIn the encoder, downsampling is performed using a convolutional layer with a stride of 2, and\ncorrespondingly, upsampling is needed in the decoder to upsample the feature maps, thereby pre-\nserving more information. In this paper, we introduce the CARAFE layer to achieve upsampling\nand increase feature channels, which uses the content of the input features themselves to guide the\nupsampling process for more accurate and efficient feature reassembly. To verify the effectiveness of\nthe CAREFE layer, we conducted experiments on the Synapse dataset with bilinear interpolation,\ntransposed convolution, and CARAFE layer in CSWin-UNet, as shown in Table 5. Upsampling with\nthe CAREFE layer achieved the highest segmentation accuracy. Moreover, compared to transposed\nconvolution, CAREFE introduces very little computational overhead. Experimental results indicate\nthat the CSWin-UNet combined with the CARAFE layer can achieve optimal performance.\n4.7.2 Skip connection\nSimilar to the UNet, we also introduced skip connections to enhance finer segmentation details\nby restoring low-level spatial information. In CSWin-UNet, skip connections are positioned at the\nresolution scales of 1/4, 1/8, and 1/16. We sequentially reduced the skip connections at the scales of\n1/16, 1/8, and 1/4, setting the number of skip connections to 3, 2, 1, and 0 to explore the impact of\nvarying numbers of skip connections on segmentation accuracy. As shown in Table 6, segmentation\naccuracy generally improves with an increase in the number of skip connections. Notably, CSWin-\nUNet achieved more significant improvements in segmentation accuracy on smaller organs (such as\nthe aorta, gallbladder, kidneys, and pancreas) than on larger organs (such as the liver, spleen, and\nstomach). Therefore, to achieve optimal performance, we set the number of skip connections to 3.\n4.7.3 Network architecture\nNeural networks with too few layers can result in insufficiently rich and accurate feature represen-\ntations, making it difficult to understand the image context, thereby leading to poor segmentation\nperformance. Conversely, having too many layers increases the computational burden and makes\nit difficult for the network to converge. Therefore, when designing the network architecture, a\nbalance was struck between network depth and model performance, enabling the model to achieve\nhigh segmentation accuracy with limited computational resources. Additionally, to prevent non-\nconvergence issues due to excessive depth[55], the block count in the final stage is set to one. By\ncomparing the number of parameters and computational costs of other Transformer-based medical\nimage segmentation methods, we set the block numbers in the four stages to [1, 2, 6, 1], [1, 2, 9, 1],\nand [1, 2, 12, 1], with encoder and decoder blocks symmetrically arranged. As shown in Table 7, the\nnetwork architecture with block settings of [1, 2, 9, 1] achieved the best performance.\n14\nTable 8: Ablation study of different hyperparameters of the combined loss function on the Synapse\ndataset. The first and second best values are highlighted in red and blue fonts, respectively.\nα β DSC↑ HD↓\n1 0 76.18 33.97\n0 1 80.28 29.19\n0.5 0.5 80.79 25.65\n0.4 0.6 81.12 18.86\n0.6 0.4 80.27 27.55\n4.7.4 Combined loss function\nWe explored the impact of different hyperparameters of the combined loss function on segmentation\naccuracy. Here, we set α and β in Eq.10 to [1, 0], [0, 1], [0.5, 0.5], [0.4, 0.6], and [0.3, 0.7]. We\nconducted an ablation study on the Synapse dataset, and the experimental results showed that using\nthe combined loss function resulted in higher segmentation accuracy than using either Dice or cross-\nentropy losses alone, especially in the case of using only Dice loss without cross-entropy loss. Table\n8 shows that the segmentation performance was optimal when α and β were set to [0.4, 0.6].\n4.8 Discussions\nOur comprehensive experimental results on three different types of medical image segmentation\ndatasets, including CT, MRI, and skin lesion images, demonstrate that the proposed CSwin-UNet\nis more advanced and suitable for medical images of various modalities than other state-of-the-art\nmedical image segmentation methods. However, our method shows some deficiencies in some\nchallenging cases, such as significant differences in segmentation accuracy for different samples\nof the gallbladder and kidney regions in the Synapse dataset, as shown in Fig.6. According to the\nvisualization results in Fig.9, there is still much room for improvement in segmentation performance\nwhen dealing with low-contrast images in the skin lesion segmentation datasets. Additionally, the\npre-training of the model dramatically impacts its performance. In our study, we initialized the\nencoder and decoder with the weights trained by the CSwin Transformer[ 26] on ImageNet[ 48].\nTherefore, exploring end-to-end medical image segmentation methods is one of the research topics\nwe are striving to pursue in the future.\n5 Conclusion\nIn this paper, we address the limitations of receptive field interactions in previous Transformer-based\nmedical image segmentation models by introducing an efficient and lightweight method, CSWin-\nUNet. Utilizing the CSWin self-attention mechanism from the CSWin Transformer, we incorporate\nthis technology into a U-shaped encoder-decoder architecture. This integration not only reduces\ncomputational costs but also improves receptive field interactions and segmentation accuracy. In the\ndecoder, the CARAFE layer is employed for upsampling, which helps retain intricate details and\nenhances the precision of organ edge segmentation. Comprehensive evaluations on three large-scale\nmedical image segmentation datasets illustrate that CSWin-UNet surpasses other state-of-the-art\nmethods in segmentation accuracy. Furthermore, CSWin-UNet is more lightweight regarding model\nparameters and computational load, suggesting significant potential for further optimizations and\nenhancements in deep learning applications for complex medical image segmentation tasks.\nReferences\n[1] S. Asgari Taghanaki, K. Abhishek, J. P. Cohen, J. Cohen-Adad, G. Hamarneh, Deep semantic\nsegmentation of natural and medical images: a review, Artificial Intelligence Review 54 (2021)\n137–178.\n[2] G. Zhao, Y . Zhang, M. Ge, M. Yu, Bilateral u-net semantic segmentation with spatial attention\nmechanism, CAAI Transactions on Intelligence Technology 8 (2) (2023) 297–307.\n[3] R. Wang, T. Lei, R. Cui, B. Zhang, H. Meng, A. K. Nandi, Medical image segmentation using\ndeep learning: A survey, IET Image Processing 16 (5) (2022) 1243–1267.\n15\n[4] I. Qureshi, J. Yan, Q. Abbas, K. Shaheed, A. B. Riaz, A. Wahid, M. W. J. Khan, P. Szczuko,\nMedical image segmentation using deep semantic-based methods: A review of techniques,\napplications and emerging trends, Information Fusion 90 (2023) 316–352.\n[5] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic segmentation, in:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2015,\npp. 3431–3440.\n[6] F. Yu, V . Koltun, Multi-scale context aggregation by dilated convolutions, arXiv preprint\narXiv:1511.07122.\n[7] H. Zhao, J. Shi, X. Qi, X. Wang, J. Jia, Pyramid scene parsing network, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2017, pp. 2881–2890.\n[8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille, Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolution, and fully connected crfs, IEEE\ntransactions on pattern analysis and machine intelligence 40 (4) (2017) 834–848.\n[9] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedical image\nsegmentation, in: Proceedings of the International Conference on Medical Image Computing\nand Computer Assisted Intervention, 2015, pp. 234–241.\n[10] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, J. Liang, Unet++: A nested u-net architecture\nfor medical image segmentation, in: Proceedings of the International Conference on Medi-\ncal Image Computing and Computer Assisted Intervention, International Workshop of Deep\nLearning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support,\nSpringer, 2018, pp. 3–11.\n[11] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y . Hammerla, B. Kainz, et al., Attention u-net: Learning where to look for the pancreas,\narXiv preprint arXiv:1804.03999.\n[12] F. I. Diakogiannis, F. Waldner, P. Caccetta, C. Wu, Resunet-a: A deep learning framework for\nsemantic segmentation of remotely sensed data, ISPRS Journal of Photogrammetry and Remote\nSensing 162 (2020) 94–114.\n[13] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, Y . Zhou, Tran-\nsunet: Transformers make strong encoders for medical image segmentation, arXiv preprint\narXiv:2102.04306.\n[14] R. Azad, A. R. Fayjie, C. Kauffmann, I. Ben Ayed, M. Pedersoli, J. Dolz, On the texture\nbias for few-shot cnn segmentation, in: Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2021, pp. 2674–2683.\n[15] M. Wen, Q. Zhou, B. Tao, P. Shcherbakov, Y . Xu, X. Zhang, Short-term and long-term memory\nself-attention network for segmentation of tumours in 3d medical images, CAAI Transactions\non Intelligence Technology 8 (4) (2023) 1524–1537.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin,\nAttention is all you need, Advances in Neural Information Processing Systems 30.\n[17] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko, End-to-end object\ndetection with transformers, in: European Conference on Computer Vision, Springer, 2020, pp.\n213–229.\n[18] L. Ye, M. Rochan, Z. Liu, Y . Wang, Cross-modal self-attention network for referring image\nsegmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 10502–10511.\n[19] X. Yu, J. Wang, Y . Zhao, Y . Gao, Mix-vit: Mixing attentive vision transformer for ultra-fine-\ngrained visual categorization, Pattern Recognition 135 (2023) 109131.\n[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16×16 words: Transformers for\nimage recognition at scale, arXiv preprint arXiv:2010.11929.\n[21] Y . Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, W. Liu, You only look at one sequence:\nRethinking transformer in vision through object detection, Advances in Neural Information\nProcessing Systems 34 (2021) 26183–26197.\n16\n[22] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable detr: Deformable transformers for\nend-to-end object detection, arXiv preprint arXiv:2010.04159.\n[23] R. Strudel, R. Garcia, I. Laptev, C. Schmid, Segmenter: Transformer for semantic segmentation,\nin: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp.\n7262–7272.\n[24] R. Guo, D. Niu, L. Qu, Z. Li, Sotr: Segmenting objects with transformers, in: Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2021, pp. 7157–7166.\n[25] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer: Hierarchical\nvision transformer using shifted windows, in: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 10012–10022.\n[26] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, B. Guo, Cswin transformer:\nA general vision transformer backbone with cross-shaped windows, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12124–12134.\n[27] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, C. Shen, Conditional positional encodings\nfor vision transformers, arXiv preprint arXiv:2102.10882.\n[28] P. Shaw, J. Uszkoreit, A. Vaswani, Self-attention with relative position representations, arXiv\npreprint arXiv:1803.02155.\n[29] G. Xu, X. Zhang, X. He, X. Wu, Levit-unet: Make faster encoders with transformer for medical\nimage segmentation, in: Proceedings of the Chinese Conference on Pattern Recognition and\nComputer Vision, Springer, 2023, pp. 42–53.\n[30] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, M. Wang, Swin-unet: Unet-like pure\ntransformer for medical image segmentation, in: European Conference on Computer Vision,\nSpringer, 2022, pp. 205–218.\n[31] J. Wang, K. Chen, R. Xu, Z. Liu, C. C. Loy, D. Lin, Carafe: Content-aware reassembly of\nfeatures, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019,\npp. 3007–3016.\n[32] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, R. Girdhar, Masked-attention mask transformer\nfor universal image segmentation, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2022, pp. 1290–1299.\n[33] Q. Bi, S. You, T. Gevers, Learning content-enhanced mask transformer for domain generalized\nurban-scene segmentation, in: Proceedings of the AAAI Conference on Artificial Intelligence,\nV ol. 38, 2024, pp. 819–827.\n[34] Q. Bi, S. You, T. Gevers, Learning generalized segmentation for foggy-scenes by bi-directional\nwavelet guidance, in: Proceedings of the AAAI Conference on Artificial Intelligence, V ol. 38,\n2024, pp. 801–809.\n[35] J. Ho, N. Kalchbrenner, D. Weissenborn, T. Salimans, Axial attention in multidimensional\ntransformers, arXiv preprint arXiv:1912.12180.\n[36] Z. Huang, X. Wang, Y . Wei, L. Huang, H. Shi, W. Liu, T. S. Huang, Ccnet: Criss-cross attention\nfor semantic segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence\n45 (6) (2020) 6896–6908.\n[37] V . Badrinarayanan, A. Kendall, R. Cipolla, Segnet: A deep convolutional encoder-decoder\narchitecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine\nIntelligence 39 (12) (2017) 2481–2495.\n[38] S. Almotairi, G. Kareem, M. Aouf, B. Almutairi, M. A.-M. Salem, Liver tumor segmentation in\nct scans using modified segnet, Sensors 20 (5) (2020) 1516.\n[39] S. Alqazzaz, X. Sun, X. Yang, L. Nokes, Automated brain tumor segmentation on multi-modal\nmr image using segnet, Computational visual media 5 (2019) 209–219.\n[40] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, K. H. Maier-Hein, nnu-net: a self-configuring\nmethod for deep learning-based biomedical image segmentation, Nature Methods 18 (2) (2021)\n203–211.\n[41] W. Ji, S. Yu, J. Wu, K. Ma, C. Bian, Q. Bi, J. Li, H. Liu, L. Cheng, Y . Zheng, Learning\ncalibrated medical image segmentation via multi-rater agreement modeling, in: Proceedings\n17\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 12341–\n12351.\n[42] J. Pan, Q. Bi, Y . Yang, P. Zhu, C. Bian, Label-efficient hybrid-supervised learning for medical\nimage segmentation, in: Proceedings of the AAAI Conference on Artificial Intelligence, V ol. 36,\n2022, pp. 2026–2034.\n[43] Y . Zhang, H. Liu, Q. Hu, Transfuse: Fusing transformers and cnns for medical image seg-\nmentation, in: Proceedings of the International Conference on Medical Image Computing and\nComputer Assisted Intervention, Springer, 2021, pp. 14–24.\n[44] A. Hatamizadeh, Y . Tang, V . Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, D. Xu,\nUnetr: Transformers for 3d medical image segmentation, in: Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 2022, pp. 574–584.\n[45] H. Wang, S. Xie, L. Lin, Y . Iwamoto, X.-H. Han, Y .-W. Chen, R. Tong, Mixed transformer\nu-net for medical image segmentation, in: Proceedings of the IEEE international conference on\nacoustics, speech and signal processing, IEEE, 2022, pp. 2390–2394.\n[46] M. Heidari, A. Kazerouni, M. Soltany, R. Azad, E. K. Aghdam, J. Cohen-Adad, D. Merhof,\nHiformer: Hierarchical multi-scale representations using transformers for medical image seg-\nmentation, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, 2023, pp. 6202–6212.\n[47] Q. Bi, J. Yi, H. Zheng, W. Ji, Y . Huang, Y . Li, Y . Zheng, Learning generalized medical image\nsegmentation from decoupled feature queries, in: Proceedings of the AAAI Conference on\nArtificial Intelligence, V ol. 38, 2024, pp. 810–818.\n[48] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al., Imagenet large scale visual recognition challenge, International\nJournal of Computer Vision 115 (2015) 211–252.\n[49] S. Fu, Y . Lu, Y . Wang, Y . Zhou, W. Shen, E. Fishman, A. Yuille, Domain adaptive relational\nreasoning for 3d multi-organ segmentation, in: Proceedings of the International Conference on\nMedical Image Computing and Computer Assisted Intervention, Springer, 2020, pp. 656–666.\n[50] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al., Deep learning techniques for automatic mri cardiac\nmulti-structures segmentation and diagnosis: is the problem solved?, IEEE transactions on\nmedical imaging 37 (11) (2018) 2514–2525.\n[51] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo,\nK. Liopyris, N. Mishra, H. Kittler, et al., Skin lesion analysis toward melanoma detection: A\nchallenge at the 2017 international symposium on biomedical imaging, in: IEEE International\nSymposium on Biomedical Imaging, IEEE, 2018, pp. 168–172.\n[52] N. Codella, V . Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman, B. Helba, A. Kalloo,\nK. Liopyris, M. Marchetti, et al., Skin lesion analysis toward melanoma detection 2018: A chal-\nlenge hosted by the international skin imaging collaboration, arXiv preprint arXiv:1902.03368.\n[53] T. Mendonça, P. M. Ferreira, J. S. Marques, A. R. Marcal, J. Rozeira, Ph 2-a dermoscopic\nimage database for research and benchmarking, in: Annual International Conference of the\nIEEE Engineering in Medicine and Biology Society, IEEE, 2013, pp. 5437–5440.\n[54] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural networks for volumetric\nmedical image segmentation, in: Proceedings of the International Conference on 3D Vision,\nIeee, 2016, pp. 565–571.\n[55] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, H. Jégou, Going deeper with image\ntransformers, in: Proceedings of the IEEE/CVF International Conference on Computer Vision,\n2021, pp. 32–42.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8306573033332825
    },
    {
      "name": "Segmentation",
      "score": 0.7634274363517761
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6251089572906494
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6008520722389221
    },
    {
      "name": "Transformer",
      "score": 0.5614423751831055
    },
    {
      "name": "Deep learning",
      "score": 0.557929277420044
    },
    {
      "name": "Image segmentation",
      "score": 0.5093201398849487
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3829219937324524
    },
    {
      "name": "Computer vision",
      "score": 0.35883498191833496
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}