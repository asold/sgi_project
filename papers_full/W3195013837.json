{
  "title": "Are the Multilingual Models Better? Improving Czech Sentiment with Transformers",
  "url": "https://openalex.org/W3195013837",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5069604241",
      "name": "Pavel Přibáň",
      "affiliations": [
        "University of West Bohemia"
      ]
    },
    {
      "id": "https://openalex.org/A5025269577",
      "name": "Josef Steinberger",
      "affiliations": [
        "University of West Bohemia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2968689265",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W3087975121",
    "https://openalex.org/W2982468288",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2891844856",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W2572303474",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963119602",
    "https://openalex.org/W2963123788",
    "https://openalex.org/W2159335442",
    "https://openalex.org/W2973071945",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W3046368065",
    "https://openalex.org/W2950856799",
    "https://openalex.org/W1573926077",
    "https://openalex.org/W3023071679",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2250805461",
    "https://openalex.org/W3045210825",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963908579",
    "https://openalex.org/W2201092681",
    "https://openalex.org/W2108646579",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3034724424",
    "https://openalex.org/W2250941225",
    "https://openalex.org/W2750747353",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W3102046836",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1866452853",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3166583983"
  ],
  "abstract": "In this paper, we aim at improving Czech sentiment with transformer-based models and their multilingual versions. More concretely, we study the task of polarity detection for the Czech language on three sentiment polarity datasets. We fine-tune and perform experiments with five multilingual and three monolingual models. We compare the monolingual and multilingual models' performance, including comparison with the older approach based on recurrent neural networks. Furthermore, we test the multilingual models and their ability to transfer knowledge from English to Czech (and vice versa) with zero-shot cross-lingual classification. Our experiments show that the huge multilingual models can overcome the performance of the monolingual models. They are also able to detect polarity in another language without any training data, with performance not worse than 4.4 % compared to state-of-the-art monolingual trained models. Moreover, we achieved new state-of-the-art results on all three datasets.",
  "full_text": "Are the Multilingual Models Better?\nImproving Czech Sentiment with Transformers\nPavel Pˇrib´aˇn1,2and Josef Steinberger1,2\nUniversity of West Bohemia, Faculty of Applied Sciences, Czech Republic\n1NTIS – New Technologies for the Information Society,\n2Department of Computer Science and Engineering,\n{pribanp,jstein}@kiv.zcu.cz\nhttp://nlp.kiv.zcu.cz\nAbstract\nIn this paper, we aim at improving Czech senti-\nment with transformer-based models and their\nmultilingual versions. More concretely, we\nstudy the task of polarity detection for the\nCzech language on three sentiment polarity\ndatasets. We ﬁne-tune and perform experi-\nments with ﬁve multilingual and three mono-\nlingual models. We compare the monolingual\nand multilingual models’ performance, includ-\ning comparison with the older approach based\non recurrent neural networks. Furthermore, we\ntest the multilingual models and their ability\nto transfer knowledge from English to Czech\n(and vice versa) with zero-shot cross-lingual\nclassiﬁcation. Our experiments show that the\nhuge multilingual models can overcome the\nperformance of the monolingual models. They\nare also able to detect polarity in another lan-\nguage without any training data, with perfor-\nmance not worse than 4.4 % compared to state-\nof-the-art monolingual trained models. More-\nover, we achieved new state-of-the-art results\non all three datasets.\n1 Introduction\nIn recent years, BERT-like models (Devlin et al.,\n2019) based on the Transformer architecture\n(Vaswani et al., 2017) and generalized language\nmodels brought a signiﬁcant improvement in per-\nformance in almost any NLP task (Raffel et al.,\n2020a), especially in English. Despite this fact,\nnot much work has been recently done in senti-\nment analysis for the Czech language with the lat-\nest Transformer models. We partly ﬁll this gap by\nfocusing on the Sentiment Classiﬁcation task, also\nknown as Polarity Detection.\nPolarity detection is a classiﬁcation task where\nthe goal is to assign a sentiment polarity to a given\ntext. The positive, negative and neutral classes are\nusually used as the polarity labels. The polarity can\nalso be deﬁned with a different number of labels,\ni.e., ﬁne-grained sentiment analysis (Liu, 2012).\nThe models based on BERT were almost ex-\nclusively trained for English, limiting their us-\nage to other languages. Recently, however, their\ncross-lingual adaptions like mBERT (Devlin et al.,\n2019), mT5 (Xue et al., 2020), XLM (Conneau and\nLample, 2019) or XLM-R (Conneau et al., 2020)\nemerged along with other non-English monolingual\nversions, for example, Czech (Sido et al., 2021),\nFrench (Martin et al., 2020; Le et al., 2019), Ara-\nbic (Safaya et al., 2020), Romanian (Dumitrescu\net al., 2020), Dutch (Vries et al., 2019) or Finnish\n(Virtanen et al., 2019).\nOur motivation is to reveal the performance lim-\nits of the current SotA transformer-based models on\nthe Czech polarity detection task, check the ability\nof the multilingual models to transfer knowledge\nbetween languages and unify the procedure and\ndata that enable the correct future evaluation of this\ntask.\nIn this paper, we focus on the task of polarity de-\ntection applied on Czech text by comparing the per-\nformance of seven pre-trained transformer-based\nmodels (both monolingual and multilingual) on\nthree Czech datasets. We ﬁne-tune each model\non each dataset and we provide a comprehensive\nsurvey of their performance. Our experiments\nshow the effectiveness of the Transformer models\nthat signiﬁcantly outperform the older approaches\nbased on recurrent neural networks. We observe\nthat the monolingual models can be notably out-\nperformed by the multilingual models, but only\nby those with much more parameters. Moreover,\nwe achieve new state-of-the-art results on all three\nevaluated datasets.\nWe are also interested in the ability of the multi-\nlingual models to transfer knowledge between lan-\nguages and its usability for polarity detection. Thus,\nwe perform zero-shot cross-lingual classiﬁcation,\narXiv:2108.10640v1  [cs.CL]  24 Aug 2021\nﬁne-tune four cross-lingual transformer-based mod-\nels on the English dataset and then test the models\non Czech data. We also perform the same experi-\nment in the reverse direction, i.e., from Czech to En-\nglish. The results reveal that the XLM-R-Large\nmodel (ﬁne-tuned solely on English) can achieve\nvery competitive results that are only about 4 %\nworse than the SotA model ﬁne-tuned by us on\nCzech data. To the best of our knowledge, this is\nthe ﬁrst paper that performs zero-shot cross-lingual\npolarity detection for the Czech language.\nWe also noticed that the comparison with the\nprevious works is rather problematic and thus, we\nprovide a split for all Czech datasets that allows\ncomparing future works much easier. Our code and\npre-trained models are publicly available1.\nOur main contributions are the following: 1) We\nprovide the comprehensive performance compar-\nison of the currently available transformer-based\nmodels for the Czech language on the polarity de-\ntection task along with the models’ optimal settings.\n2) We test the ability of the multilingual models\nto transfer knowledge between Czech and English.\n3) We release all the ﬁne-tuned models and code\nfreely for research purposes and we provide a data\nsplit that allows future comparison and evaluation.\nFurthermore, we achieved new state-of-the-art re-\nsults for all three evaluated datasets.\n2 Related Work\nThe previous approaches (Kim, 2014; Johnson and\nZhang, 2016; Cliche, 2017; Baziotis et al., 2017;\nGray et al., 2017; Conneau et al., 2017) for English\npolarity detection and other related tasks mostly\nrelied on transfer learning and pre-trained word em-\nbeddings such as word2vec (Mikolov et al., 2013)\nand fastText (Bojanowski et al., 2017) in combina-\ntions with Convolutional Neural Networks (CNN)\nor Long Short-Term Memory (LSTM) (Hochreiter\nand Schmidhuber, 1997), eventually in conjunc-\ntion with the modiﬁed attention mechanism (Bah-\ndanau et al., 2015; Rockt¨aschel et al., 2015; Raffel\nand Ellis, 2015). Furthermore, the new contex-\ntualized word representations such as CoVe (Mc-\nCann et al., 2017) or ELMo (Peters et al., 2018)\nand pre-trained language model ULMFiT (Howard\nand Ruder, 2018) were successfully applied to the\npolarity detection. Finally, the latest transformer-\nbased models like BERT (Devlin et al., 2019), GPT\n1https://github.com/pauli31/\nimproving-czech-sentiment-transformers\n(Radford et al., 2018), RoBERTa (Liu et al., 2019)\nor T5 (Raffel et al., 2020b) that are all in general\ntrained on language modeling tasks proved their\nperformance superiority for English over all previ-\nous approaches, for example in (Sun et al., 2019).\nThese models are pre-trained on a modiﬁed lan-\nguage modeling tasks with a huge amount of un-\nlabeled data. In the end, they are ﬁne-tuned for a\nspeciﬁc downstream task.\nThe initial works on Czech polarity detection\nand sentiment analysis usually used lexical fea-\ntures (Steinberger et al., 2011; Veselovsk ´a et al.,\n2012) or Bag-of-Words text representations along\nwith the Naive Bayes or logistic regression classi-\nﬁers (Habernal et al., 2013) or a combination of\nsupervised and unsupervised approach (Brychc´ın\nand Habernal, 2013). Lenc and Hercig (2016) ap-\nplied CNN using the architecture from (Kim, 2014)\nand the LSTM neural network to all three datasets\nthat we use in this paper. Another usage of LSTM\nneural network with the self-attention mechanism\n(Humphreys and Sui, 2016) can be found in (Li-\nbovick`y et al., 2018). Similarly, Sido and Konop´ık\n(2019) tried to use curriculum learning with CNN\nand LSTM.\nLeheˇcka et al. (2020) pre-trained a BERT-based\nmodel for polarity detection with an improved pool-\ning layer and distillation of knowledge technique.\nThe most recent application of the Transformer\nmodel is in (Sido et al., 2021). The authors created\na BERT model for Czech and, as one of the evalua-\ntion tasks, they performed polarity detection on the\nFB and CSFD datasets.\nTo the best of our knowledge, there are no previ-\nous works that focus on the zero-shot cross-lingual\npolarity detection task in the Czech language. The\nrecent related work can be found in (Eriguchi et al.,\n2018), where the authors use the neural machine\ntranslation encoder-based model and English data\nto perform zero-shot cross-lingual sentiment clas-\nsiﬁcation on French. In (Eriguchi et al., 2018) the\nauthors performed the zero-shot classiﬁcation from\nSlovene to Croatian. Another related work can be\nfound in (Wang and Banko, 2021; Qin et al., 2020).\n3 Data\nTo the best of our knowledge, there are three Czech\npublicly available datasets for the polarity detection\ntask: (1) movie review dataset (CSFD), (2) Face-\nbook dataset (FB) and (3) product review dataset\n(Mallcz), all of them come from (Habernal et al.,\n2013) and each text sample is annotated with one\nof three2 labels, i.e., positive, neutral and negative,\nsee Table 1 for the class distribution. For the cross-\nlingual experiments we use the two-class English\nmovie review dataset (IMDB) (Maas et al., 2011).\nDataset Part Positive Negative Neutral Total\nCSFD\ntrain 22 117 21 441 22 235 65 793\ndev 2 456 2 399 2 456 7 311\ntest 6 324 5 876 6 077 18 277\ntotal 30 897 29 716 30 768 91 381\nFB\ntrain 1 605 1 227 3 311 6 143\ndev 171 151 361 683\ntest 811 613 1 502 2 926\ntotal 2 587 1 991 5 174 9 752\nMallcz\ntrain 74 100 7 498 23 022 104 620\ndev 8 253 848 2 524 11 625\ntest 20 624 2 041 6 397 29 062\ntotal 102 977 10 387 31 943 145 307\nIMDB\ntrain 12 500 12 500 - 25 000\ntest 12 500 12 500 - 25 000\ntotal 25 000 25 000 - 50 000\nTable 1: Datasets statistics.\nThe FB dataset contains 10k random posts from\nnine different Facebook pages that were manually\nannotated by two annotators. The CSFD dataset is\ncreated from 90k Czech movie reviews from the\nCzech movie database3 that were downloaded and\nannotated according to their star rating (0–2 stars\nas negative, 3–4 stars as neutral, 5–6 stars as pos-\nitive). The Mallcz dataset consists of 145k users’\nreviews of products from Czech e-shop4, the labels\nare assigned according to the review star rating on\nthe scale 0-5, where the reviews with 0-3 stars are\nlabeled as negative, 4 stars as neutral and 5 stars as\npositive. The English IMDB dataset includes 50k\nmovie reviews scraped from the Internet Movie\nDatabase5 with positive and negative classes split\ninto training and testing parts of equal size.\nSince there is no ofﬁcial partitioning for the\nCzech datasets, we split them into training, de-\nvelopment and testing parts with the same class\ndistribution for each part as it is in the original\ndataset, see Table 1. For the Mallcz and CSFD\ndatasets, we use the following ratio: 80 % for train-\ning data, 20 % for testing data, for the FB dataset,\nit is 70 % and 30 %, respectively and 10 % from\nthe training data (for all datasets) is used as the\n2The FB dataset also contains 248 samples with a fourth\nclass called bipolar, but we ignore this one.\n3https://www.csfd.cz\n4https://www.mall.cz\n5https://www.imdb.com\ndevelopment data. We used different split ratio for\nthe FB dataset because it is approximately ten and\nsixteen times smaller than the CSFD and Mallcz\ndatasets, respectively and we did not want to reduce\nthe size of the testing data too much.\n4 Models Description\nWe performed exhaustive experiments with\ntransformed-based models and in order to com-\npare them with the previous works, we also im-\nplemented the older models (baseline models) that\ninclude the logistic regression classiﬁer and the\nBiLSTM neural network.\n4.1 Baseline Models\nWe re-implemented the best models from (Haber-\nnal et al., 2013), i.e., logistic regression classiﬁer\n(lrc) with character n-grams (in a range from 3-\ngrams to 6-grams), word uni-grams and bi-grams\nfeatures. The second baseline model is the LSTM\nmodel partly inspired by (Baziotis et al., 2017). Its\ninput is a sequence of t tokens represented as a\nmatrix M ∈Rt×d, where d= 300is a dimension\nof the Czech pre-trained fastText word embeddings\n(Bojanowski et al., 2017)6. The maximal size of\nthe input vocabulary is set to 300 000. The input is\npassed into the trainable embedding layer that is fol-\nlowed by two BiLSTM (Graves and Schmidhuber,\n2005) layers and after each, the dropout (Srivastava\net al., 2014) is applied. After the two BiLSTM lay-\ners, the self-attention mechanism is applied. The\noutput is then passed to a fully-connected softmax\nlayer. An output of the softmax layer is a probabil-\nity distribution over the possible classes. We use\nthe Adam (Kingma and Ba, 2014) optimizer with\ndefault parameters ( β1 = 0.9,β2 = 0.999) and\nwith weight decay modiﬁcation (Loshchilov and\nHutter, 2017) and the cross-entropy loss function.\nWe replace numbers, emails and links with generic\ntokens, we tokenize input text with the TokTok\ntokenizer7 and we use a customized stemmer8.\nWe use different hyper-parameters for each\ndataset, see Appendix A.1 for the complete set-\ntings.\n6Available at https://fasttext.cc/docs/en/\ncrawl-vectors.html\n7https://github.com/jonsafari/tok-tok\n8https://github.com/UFAL-DSG/alex/\nblob/master/alex/utils/czech_stemmer.py\n4.2 Transformer Models\nIn total, we use eight different transformer-based\nmodels (ﬁve of them are multilingual). All of them\nare based on the original BERT model. They use\nonly the encoder part of the original Transformer\n(Vaswani et al., 2017), although their pre-training\nprocedure may differ. There are also text-to-text\nmodels like T5 (Raffel et al., 2020b) and BART\n(Lewis et al., 2019) and their multilingual versions\nmT5 (Xue et al., 2020) and mBART (Liu et al.,\n2020; Tang et al., 2020). The main difference from\nBERT-like models is that they use the full encoder-\ndecoder architecture of the Transformer. They are\nmainly intended for text generation tasks (e.g., ab-\nstractive summarization). We decided to use only\nthe BERT-like models with the same architecture\nbecause they ﬁt more for the classiﬁcation task.\nAll the models are pre-trained on a modiﬁed\nlanguage modeling task, for example, Masked Lan-\nguage Modeling (MLM) and eventually on some\nclassiﬁcation task like Next Sentence Prediction\n(NSP) or Sentence Ordering Prediction (SOP), see\n(Devlin et al., 2019; Lan et al., 2020) for details.\nThe evaluated models differ in the number of pa-\nrameters (see Table 2) and thus, their performance\nis also very different, see Section 5.\nModel #Params V ocab #Langs\nCzert-B 110M 30k 1\nCzert-A 12M 30k 1\nRandomALBERT 12M 30k 1\nmBERT 177M 120k 104\nSlavicBERT 177M 120k 4\nXLM 570M 200k 100\nXLM-R-Base 270M 250k 100\nXLM-R-Large 559M 250k 100\nTable 2: Models statistics with a number of parameters,\nvocabulary size and a number of supported languages.\nCzert-B is Czech version of the of the original\nBERTBASE model (Devlin et al., 2019). The only\ndifference is that during the pre-training, the au-\nthors increased the batch size to 2048 and they\nslightly modiﬁed the NSP prediction task (Sido\net al., 2021).\nCzert-A is the Czech version of the ALBERT\nmodel (Lan et al., 2020), also with the same mod-\niﬁcation as Czert-B, i.e., batch size was set to\n2048 and the modiﬁed NSP prediction task is used\ninstead of the SOP task (Sido et al., 2021).\nRandomALBERT we follow the evaluation in\n(Sido et al., 2021) and we also test randomly initial-\nized ALBERT model without any pre-training to\nshow the importance of pre-training of such mod-\nels and its performance inﬂuence on the polarity\ndetection task.\nmBERT (Devlin et al., 2019) is a multilingual\nversion of the original BERTBASE, jointly trained\non 104 languages.\nSlavicBERT (Arkhipov et al., 2019) is initialized\nfrom the mBERT checkpoint and further pre-trained\nwith a modiﬁed vocabulary only for four Slavic\nlanguages (Bulgarian, Czech, Polish and Russian).\nXLM (Conneau and Lample, 2019) utilizes the\ntraining procedure of the original BERT model\nfor multilingual settings mainly by using the Byte-\nPair Encoding (BPE) and increasing the shared\nvocabulary between languages.\nXLM-R-Base (Conneau et al., 2020) is a multi-\nlingual version of the RoBERTa (Liu et al., 2019)\nspeciﬁcally optimized and pre-trained for 100 lan-\nguages.\nXLM-R-Large (Conneau et al., 2020) is the\nsame model as the XLM-R-Base, but it is larger\n(it has more parameters).\n4.3 Transformers Fine-Tuning\nTo utilize the models for text classiﬁcation, we\nfollow the default approaches mentioned in the cor-\nresponding models’ papers and we ﬁne-tune all pa-\nrameters of the models. In all models except XLM,\nwe use the ﬁnal hidden vector h ∈RH of the spe-\ncial classiﬁcation token [CLS] or <s> taken from\nthe pooling layer9 of BERT or RoBERTa models,\nrespectively. The vector h represents the entire en-\ncoded sequence input, where H is the hidden size\nof the corresponding model. We add a task-speciﬁc\nlinear layer (with a dropout set to 0.1) represented\nby a matrix W ∈R|C|×H, where C is a set of\nclasses. We compute the classiﬁcation output, i.e.,\nthe input sample being classiﬁed as class c∈Cas\nc= argmax (hWT ).\nIn the case of the XLM model, we take the last\nhidden state (without any pooling layer) of the ﬁrst\ninput token and we apply the same linear layer\n(W ∈R|C|×H) and approach to obtain the clas-\nsiﬁcation output. For learning, we use the Adam\noptimizer with default parameters and with weight\ndecay (same as for the LSTM model), and the cross-\n9The pooling layer is a fully-connected layer of size H\nwith a hyperbolic tangent used as the activation function.\nentropy loss function. See Section 5.1 and Ap-\npendix A.2 for the hyper-parameters we used.\n5 Experiments & Results\nWe perform two types of experiments, i.e.,mono-\nlingual and cross-lingual. In monolingual experi-\nments, we ﬁne-tune and evaluate the Transformer\nmodels for each dataset separately on three-class\n(positive, negative and neutral) and two-class (pos-\nitive and negative) sentiment analysis. We also im-\nplemented the logistic regression (lrc) and LSTM\nbaseline models and we compare the results with\nthe existing works.\nIn cross-lingual experiments, we test the ability\nof four multilingual transformer-based models to\ntransfer knowledge between English and Czech.\nWe run the multilingual models only on the two-\nclass datasets (positive and negative). We ﬁne-tune\neither on English (IMDB) or Czech (CSFD), and\nthen we evaluate on the other language. Thus we\nperform the zero-shot cross-lingual classiﬁcation.\nWe decided to use the IMDB and CSFD dataset\nbecause they are from the same domain i.e., movie\nreviews.\nEach experiment10 was performed at least ﬁve\ntimes and we report the results using the macro F1\nscore.\n5.1 Monolingual Experiments\nThe goal of the monolingual experiments is to re-\nveal the current state-of-the-art performance on the\nCzech polarity datasets, namely CSFD, FB and\nMallcz (see Section 3) and provide a comparison\nbetween the available models and their settings.\nAs we already mentioned, we split the datasets\ninto training, development and testing parts. There\nis no ofﬁcial split for the datasets and we found\nout that all the available works usually use either\n10-fold cross-validation or they split11 the datasets\non their own, the †and * symbols in Table 3, re-\nspectively causing the comparison to be difﬁcult.\nWe ﬁne-tune all models on training data and\nwe measure the results on the development data.\nWe select the model with the best performance on\nthe development data and we ﬁne-tune the model\non combined training and development data. We\nreport the results in Table 3 on the testing data with\n95% conﬁdence intervals.\n10Except for the experiments with the lrc model.\n11The authors do not provide any recipe to reproduce the\nresults.\nFirstly, we re-implemented the logistic regres-\nsion classiﬁer (lrc) with the best feature combi-\nnation from (Habernal et al., 2013) and we report\nthe results on our data split. We can see that we\nobtained very similar results to the ones stated in\n(Habernal et al., 2013). We also tried to improve\nthis baseline with Tf-idf weighting, but it did not\nlead to any signiﬁcant improvements, so we de-\ncided to keep the settings the same as in (Habernal\net al., 2013), so the results are comparable.\nFor the LSTM model, we tried different com-\nbinations of hyper-parameters (learning rate, op-\ntimizer, dropout, etc.). We report the used hyper-\nparameters for the results from Table 3 in Appendix\nA.2. Our implementation is only about 1 % worse\nthan LSTM with the self-attention model from (Li-\nbovick`y et al., 2018), but they used a different data\nsplit. For the Mallcz dataset, we were not able to\noutperform the lrc baseline with the LSTM model.\nWe ﬁne-tune all parameters of the seven pre-\ntrained BERT-based models and one randomly ini-\ntialized ALBERT model. In our experiments, we\nuse constant learning rate and also linear learning\nrate decay (without learning rate warm-up) with\nthe following initial learning rates: 2e-6, 2e-5 and\n2.5e-5. We got inspired by the ones used in (Sun\net al., 2019). Based on the average number of to-\nkens for each dataset and models’ tokenizer (see\nTable 4 and Figures 1, 2, 3) 12, we use a max se-\nquence length of 64 and a batch size of 32 for the\nFB dataset. We restrict the max sequence length\nfor the CSFD and Mallcz datasets to 512 and use a\nbatch size of 32. All other hyper-parameters of the\nmodels are set to the pre-trained models’ defaults.\nSee Table 7 in Appendix A.2 for the reported re-\nsults’ hyper-parameters.\nWe repeated all the basic experiments with the\npolarity detection task from (Sido et al., 2021) with\nthe new data split. Our results do not signiﬁcantly\ndiffer, as shown in Table 8 and in Appendix A.2.\nIf we compare the BERT model from (Lehe ˇcka\net al., 2020) with the Czert-B, mBERT and\nSlavicBERT models13, we can see that on the\nbinary task, they also perform very similarly, i.e.,\naround 93 %, but again they used different test data\n(the entire CSFD dataset 14). The obvious obser-\nvation is that the XLM-R-Large model is supe-\n12The distributions of the other models were similar to those\nshown in the mentioned Figures.\n13All of them should have the same or almost the same\narchitecture and a similar number of parameters.\n14The examples with positive and negative classes.\nModel 3 Classes 2 Classes\nCSFD FB Mallcz CSFD FB Mallcz\nlrc (ours) 79.63 67.86 76.71 91.42 88.12 88.98\nLSTM (ours) 79.88 ±0.18 72.89±0.49 73.43±0.12 91.82 ±0.09 90.13±0.17 88.02±0.24\nCzert-A 79.89±0.60 73.06±0.59 76.79±0.38 91 .84±0.84 91.28±0.18 91.20±0.26\nCzert-B 84.80 ±0.10 76.90±0.38 79.35±0.24 94.42 ±0.15 93.97±0.30 92.87±0.15\nmBERT 82.74 ±0.16 71.61±0.13 70.79±5.74 93.11 ±0.29 88.76±0.42 72.79±3.09\nSlavicBERT 82.59 ±0.12 73.93±0.53 75.34±2.54 93.47 ±0.33 89.84±0.43 90.99±0.15\nRandomALBERT 75.79 ±0.18 62.53±0.46 64.81±0.25 89.99 ±0.21 81.71±0.56 85.38±0.10\nXLM-R-Base 84.82 ±0.10 77.81±0.50 75.43±0.07 94.32 ±0.34 93.26±0.74 92.56±0.07\nXLM-R-Large 87.08±0.11 81.70±0.64 79.81±0.21 96.00 ±0.02 96.05±0.01 94.37±0.02\nXLM 83.67 ±0.11 71.46±1.58 77.56±0.08 93.86 ±0.18 89.94±0.27 91.97±0.22\n(Habernal et al., 2013)† 79.00 69.00 75.00 - 90.00 -\n(Brychc´ın and Habernal, 2013)† 81.53±0.30 - - - - -\n(Libovick`y et al., 2018)* 80.80 ±0.10 - - - - -\n(Leheˇcka et al., 2020)* - - - 93.80 - -\nTable 3: The ﬁnal monolingual results as macro F1 score for all three Czech polarity datasets on two and three\nclasses. For experiments with neural networks performed by us, we present the results with a 95% conﬁdence\ninterval. The models from papers marked with †were evaluated with 10-fold cross-validation and the ones marked\nwith * were evaluated on custom data split.\nModel CSFD FB Mallcz\nAvg. Max. Avg. Max. Avg. Max.\nCzert-B 84.5 1000 20.3 64 34.3 1471\nmBERT 111.6 1206 25.6 66 46.6 2038\nSlavicBERT 83.6 983 20.7 62 34.3 1412\nXLM 100.5 1058 22.6 64 41.0 1812\nCzert-A 81.7 993 19.7 62 32.6 1435RandomALBERT\nXLM-R-Base 93.9 952 20.4 53 37.5 1670XLM-R-Large\nTable 4: The average and maximum number of sub-\nword tokens for each model’s tokenizer and dataset.\nrior to all others by a signiﬁcant margin for any\ndataset. Only for the three-class Mallcz dataset, the\nCzert-B model is competitive (the conﬁdence\nintervals almost overlap). From the results for\nthe RandomALBERT model, we can see how im-\nportant is the pre-training phase for Transformers,\nsince the model is even worse than the logistic re-\ngression classiﬁer15.\n5.2 Cross-lingual Experiments\nThe cross-lingual experiments were performed\nwith the multilingual models that support English\nand Czech. For these experiments, we use linear\nlearning rate decay with an initial learning rate of\n2e-6.\nFirstly, we ﬁne-tuned the models on the English\nIMDB dataset and we evaluated them on the test\npart of the Czech binary CSFD dataset (i.e., zero-\n15The model was trained for a maximum of 15 epochs and it\nwould probably get better with a higher number of epochs, but\nthe other models were trained for the same or lower number\nof epochs.\nshot cross-lingual classiﬁcation). We randomly\nselected 5k examples from the IMDB dataset as the\ndevelopment data. The rest of the 45k examples\nis used as training data. We select the models that\nperform best on the English development data 16\nand we report the results in Table 5. The test (cs)\ncolumn refers to results obtained on the CSFD test-\ning part. For easier comparison, we also include\nthe Monoling. (cs) column that contains the results\n(same as in Table 3) for models trained on Czech\ndata. The XLM-R-Large was able to achieve re-\nsults only about 4.4 % worse than the same model\nthat was ﬁne-tuned on Czech data. It is a great\nresult if we consider that the model has never seen\nany labeled Czech data. The XLM and mBERT mod-\nels perform much worse.\nModel EN→CS Monoling. (cs)\ndev (en) test (cs)\nXLM-R-Base 94.52±0.12 88.01±0.28 94.32 ±0.34\nXLM-R-Large 95.86±0.06 91.61±0.06 96.00±0.02\nXLM 92.76 ±0.34 75.37±0.29 93.86 ±0.18\nmBERT 93.07 ±0.03 76.32±1.13 93.11 ±0.29\nTable 5: Macro F1 score for cross-lingual experiments\nfrom English to Czech.\nThe second type of cross-lingual experiment was\nperformed in a reverse direction, i.e., from Czech\nto English. We use the Czech CSFD training and\ntesting data for ﬁne-tuning and we evaluate the\nmodel on the English IMDB test data. We report\nthe results in Table 6 using the accuracy because\n16The dev (en) column in Table 5.\nthe current state-of-the-art works (Thongtan and\nPhienthrakul, 2019; Sun et al., 2019) use this met-\nric. Similarly to the previous case, we selected the\nmodel that performs best on Czech CSFD devel-\nopment data. For these experiments, the mBERT\ndid not converge. As in the previous experiment,\nthe XLM-R-Large performs best and it achieves\nalmost 94 % accuracy that is only 3.4 % below\nthe current SotA result from (Thongtan and Phien-\nthrakul, 2019).\nModel CS→EN\ndev (cs) test (en)\nXLM-R-Base 94.22 ±0.01 89.53±0.15\nXLM-R-Large 95.65 ±0.17 93.98±0.10\nXLM 93.66 ±0.13 78.24±0.46\n(Thongtan and Phienthrakul, 2019) - 97.42\n(Sun et al., 2019) - 95.79\nTable 6: Accuracy results for cross-lingual experiments\nfrom Czech to English.\nBased on the results, we can conclude that the\nXLM-R-Large model is very capable of transfer-\nring knowledge between English and Czech (and\nprobably between other languages as well). It is\nalso important to note that Czech and English are\nlanguages from a different language family with\na high number of differences both in syntax and\ngrammar.\n5.3 Discussion & Remarks\nWe can see from the results that the recent pre-\ntrained transformer-based models beat the older\napproaches (lrc and LSTM) by a large margin.\nThe monolingual Czert-B model is in general\noutperformed only by the XLM-R-Large and\nXLM-R-Base models, but these models have ﬁve\ntimes/three times more parameters, and eight times\nlarger vocabulary. Taking into account these facts,\nthe Czert-B model is still very competitive. It\nmay be beneﬁcial in certain situations to use a\nsmaller model like this that does not need such\ncomputational resources as the ones that are re-\nquired by the XLM-R-Large.\nDuring the ﬁne-tuning, we observed that in most\ncases, the lower learning rate 2e-6 (see Table 7 in\nAppendix A.2) leads to better results. Thus we\nrecommend using the same one or similar order.\nThe higher learning rates tend to provide worse\nresults and the model does not converge.\nAccording to the generally higher conﬁdence\ninterval, the ﬁne-tuning of a smaller dataset like\nFB that has only about 6k training examples is, in\ngeneral, less stable and more prone to overﬁtting\nthan training a model on datasets with tens of thou-\nsands of examples. We also noticed that ﬁne-tuning\nof the mBERT and SlavicBERT on the Mallcz\ndataset is very unstable (see the conﬁdence interval\nin Table 3). Unfortunately, we did not ﬁnd out the\nreason. A more detailed error analysis could reveal\nthe reason.\n6 Conclusion\nIn this work, we evaluated the performance of\navailable transformer-based models for the Czech\nlanguage on the task of polarity detection. We\ncompared the performance of the monolingual and\nmultilingual models and we showed that the large\nXLM-R-Large model can outperform the mono-\nlingual Czert-B model. The older approach\nbased on recurrent neural networks is surpassed\nby a very large margin by the Transformers. More-\nover, we achieved new state-of-the-art results on\nall three Czech polarity detection datasets.\nWe performed zero-shot cross-lingual polarity\ndetection from English to Czech (and vice versa)\nwith four multilingual models. We showed that\nthe XLM-R-Large is able to detect polarity in\nanother language without any labeled data. The\nmodel performs no worse than 4.4 % in comparison\nto our new state-of-the-art monolingual model. To\nthe best of our knowledge, this is the ﬁrst work that\naims at cross-lingual polarity detection in Czech.\nOur code and pre-trained models are publicly avail-\nable.\nIn the future work, we intend to perform a deep\nerror analysis to ﬁnd in which cases the current\nmodels fail and compare approaches that use the\nlinear cross-lingual transformations (Artetxe et al.,\n2018; Brychc´ın, 2020) that explicitly map semantic\nspaces into one shared space. The second step in\nthe cross-lingual settings is to employ more than\ntwo languages and utilize the models for different\ndomains.\nAcknowledgments\nThis work has been partly supported by ERDF ”Re-\nsearch and Development of Intelligent Components\nof Advanced Technologies for the Pilsen Metropoli-\ntan Area (InteCom)” (no.: CZ.02.1.01/0.0/0.0/17\n048/0007267); and by Grant No. SGS-2019-018\nProcessing of heterogeneous data and its special-\nized applications. Computational resources were\nsupplied by the project ”e-Infrastruktura CZ” (e-\nINFRA LM2018140) provided within the program\nProjects of Large Research, Development and In-\nnovations Infrastructures.\nReferences\nMikhail Arkhipov, Maria Troﬁmova, Yuri Kuratov, and\nAlexey Sorokin. 2019. Tuning multilingual trans-\nformers for language-speciﬁc named entity recogni-\ntion. In Proceedings of the 7th Workshop on Balto-\nSlavic Natural Language Processing , pages 89–93,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. InPro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 789–798, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nChristos Baziotis, Nikos Pelekis, and Christos Doulk-\neridis. 2017. DataStories at SemEval-2017 task 4:\nDeep LSTM with attention for message-level and\ntopic-based sentiment analysis. In Proceedings of\nthe 11th International Workshop on Semantic Eval-\nuation (SemEval-2017), pages 747–754, Vancouver,\nCanada. Association for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTom´aˇs Brychc ´ın. 2020. Linear transformations\nfor cross-lingual semantic textual similarity.\nKnowledge-Based Systems, 187:104819.\nTom´aˇs Brychc´ın and Ivan Habernal. 2013. Unsuper-\nvised improving of sentiment analysis using global\ntarget context. In Proceedings of the International\nConference Recent Advances in Natural Language\nProcessing RANLP 2013 , pages 122–128, Hissar,\nBulgaria. INCOMA Ltd. Shoumen, BULGARIA.\nMathieu Cliche. 2017. BB twtr at SemEval-2017\ntask 4: Twitter sentiment analysis with CNNs and\nLSTMs. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017) ,\npages 573–580, Vancouver, Canada. Association for\nComputational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nAlexis Conneau, Holger Schwenk, Lo ¨ıc Barrault, and\nYann Lecun. 2017. Very deep convolutional net-\nworks for text classiﬁcation. In Proceedings of the\n15th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Volume 1,\nLong Papers, pages 1107–1116, Valencia, Spain. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nStefan Dumitrescu, Andrei-Marius Avram, and Sampo\nPyysalo. 2020. The birth of Romanian BERT. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings,\npages 4324–4328, Online. Association for Computa-\ntional Linguistics.\nAkiko Eriguchi, Melvin Johnson, Orhan Firat, Hideto\nKazawa, and Wolfgang Macherey. 2018. Zero-\nshot cross-lingual classiﬁcation using multilin-\ngual neural machine translation. arXiv preprint\narXiv:1809.04686.\nAlex Graves and J ¨urgen Schmidhuber. 2005. Frame-\nwise phoneme classiﬁcation with bidirectional lstm\nand other neural network architectures. Neural Net-\nworks, 18(5-6):602–610.\nScott Gray, Alec Radford, and Diederik P Kingma.\n2017. Gpu kernels for block-sparse weights. arXiv\npreprint arXiv:1711.09224, 3.\nIvan Habernal, Tom ´aˇs Pt ´aˇcek, and Josef Steinberger.\n2013. Sentiment analysis in Czech social media us-\ning supervised machine learning. In Proceedings\nof the 4th Workshop on Computational Approaches\nto Subjectivity, Sentiment and Social Media Analy-\nsis, pages 65–74, Atlanta, Georgia. Association for\nComputational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput. ,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nGlyn W Humphreys and Jie Sui. 2016. Attentional\ncontrol and the self: the self-attention network (san).\nCognitive neuroscience, 7(1-4):5–17.\nRie Johnson and Tong Zhang. 2016. Supervised and\nsemi-supervised text categorization using lstm for\nregion embeddings. In Proceedings of the 33rd In-\nternational Conference on International Conference\non Machine Learning - Volume 48 , ICML’16, page\n526–534. JMLR.org.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Beno ˆıt Crabb´e, Laurent Besacier, and Di-\ndier Schwab. 2019. Flaubert: Unsupervised lan-\nguage model pre-training for french. arXiv preprint\narXiv:1912.05372.\nJan Leheˇcka, Jan ˇSvec, Pavel Ircing, and Luboˇs ˇSm´ıdl.\n2020. Bert-based sentiment analysis using distilla-\ntion. In Statistical Language and Speech Processing,\npages 58–70, Cham. Springer International Publish-\ning.\nLadislav Lenc and Tom ´as Hercig. 2016. Neural net-\nworks for sentiment analysis in czech. In Proceed-\nings of the 16th ITAT: Slovensko ˇcesk´y NLP work-\nshop (SloNLP 2016) , volume 1649 of CEUR Work-\nshop Proceedings , pages 48–55, Bratislava, Slo-\nvakia. Comenius University in Bratislava, Faculty of\nMathematics, Physics and Informatics, CreateSpace\nIndependent Publishing Platform.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, trans-\nlation, and comprehension. arXiv preprint\narXiv:1910.13461.\nJindrich Libovick `y, Rudolf Rosa, Jindrich Helcl, and\nMartin Popel. 2018. Solving three czech nlp tasks\nwith end-to-end neural models. In ITAT, pages 138–\n143.\nBing Liu. 2012. Sentiment analysis and opinion min-\ning. Synthesis lectures on human language technolo-\ngies, 5(1):1–167.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems , volume 30. Curran\nAssociates, Inc.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxiang\nChe. 2020. Cosda-ml: Multi-lingual code-switching\ndata augmentation for zero-shot cross-lingual NLP.\nCoRR, abs/2006.06402.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nColin Raffel and Daniel PW Ellis. 2015. Feed-\nforward networks with attention can solve some\nlong-term memory problems. arXiv preprint\narXiv:1512.08756.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020a. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020b. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nTim Rockt ¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2015.\nReasoning about entailment with neural attention.\narXiv preprint arXiv:1509.06664.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. Kuisail at semeval-2020 task 12: Bert-cnn for\noffensive speech identiﬁcation in social media.\nJakub Sido and Miloslav Konop ´ık. 2019. Curriculum\nlearning in sentiment analysis. In Speech and Com-\nputer, pages 444–450, Cham. Springer International\nPublishing.\nJakub Sido, Ond ˇrej Pra ˇz´ak, Pavel P ˇrib´aˇn, Jan Pa ˇsek,\nMichal Sej´ak, and Miloslav Konop´ık. 2021. Czert–\nczech bert-like model for language representation.\narXiv preprint arXiv:2103.13031.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958.\nJosef Steinberger, Polina Lenkova, Mohamed Ebrahim,\nMaud Ehrmann, Ali Hurriyetoglu, Mijail Kabadjov,\nRalf Steinberger, Hristo Tanev, Vanni Zavarella, and\nSilvia V ´azquez. 2011. Creating sentiment dictio-\nnaries via triangulation. In Proceedings of the 2nd\nWorkshop on Computational Approaches to Subjec-\ntivity and Sentiment Analysis (WASSA 2.011), pages\n28–36, Portland, Oregon. Association for Computa-\ntional Linguistics.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune BERT for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and ﬁnetuning. arXiv\npreprint arXiv:2008.00401.\nTan Thongtan and Tanasanee Phienthrakul. 2019. Sen-\ntiment classiﬁcation using document embeddings\ntrained with cosine similarity. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 407–414, Florence, Italy. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nKaterina Veselovsk´a, Jan Hajic, and Jana Sindlerov ´a.\n2012. Creating annotated resources for polarity clas-\nsiﬁcation in czech. In KONVENS, pages 296–304.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for ﬁnnish.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. arXiv:1912.09582 [cs].\nCindy Wang and Michele Banko. 2021. Practical\ntransformer-based multilingual text classiﬁcation.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies:\nIndustry Papers, pages 121–129, Online. Associa-\ntion for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mT5: A massively\nmultilingual pre-trained text-to-text transformer.\nA Appendix\nA.1 LSTM Hyper-parameters\nWe use cross-entropy as the loss function and the\nAdam (Kingma and Ba, 2014) optimizer with de-\nfault parameters (β1 = 0.9,β2 = 0.999) and with\na modiﬁcation from (Loshchilov and Hutter, 2017)\nfor the FB dataset. The embedding layer is train-\nable with a maximum size of 300k. The max se-\nquence length for the inputttokens is 64 for the FB\ndataset and 512 for the CSFD and Mallcz dataset\nwith weight decay in the optimizer set to 0. We\nuse Czech pre-trained fastText (Bojanowski et al.,\n2017) embeddings17. For the Mallcz and CSFD\ndatasets, we use 128 units in the BiLSTM layers\nand a batch size of 128. For the FB dataset, we use\n256 units in the BiLSTM layers and a batch size of\n256 with weight decay in the optimizer set to 1e-4.\nFor all datasets, we use 10 % of total steps (batch\nupdates) to warm up the learning rate, which means\nthat during the training, the linear rate is ﬁrstly\nlinearly increasing to the initial learning rate before\nbeing decayed with the corresponding learning rate\nscheduler. The dropout after the BiLSTM layers\nis set to 0.2. We use cosine (the * symbol in Table\n7) and the exponential learning rate scheduler (the\n‡symbol in Table 7) with a decay rate set to 0.05.\nTable 7 contains the initial learning rate and the\nnumber of epochs for the LSTM model for each\ndataset.\nA.2 Transformer Hyper-parameters\nFor ﬁne-tuning of the transformer-based models,\nwe use the same modiﬁcation (Loshchilov and Hut-\nter, 2017) of the Adam (Kingma and Ba, 2014)\noptimizer with default weight decay set to 1e-2.\nWe use different learning rates and a number of\nepochs for each combination of the models and\ndatasets, see Table 7. We use either constant linear\nrate or linear learning rate decay without learning\nrate warm-up. We use default values of all other\nhyper-parameters.\nFor the cross-lingual experiments, we use only\nthe linear learning rate decay scheduler with the\ninitial learning rate set to 2e-6 without learning\nrate warm-up. For the cross-lingual experiments\nfrom English to Czech, the numbers of epochs\nused for the ﬁne-tuning are 5, 2, 4 and 10 for\nthe XLM-R-Base, XLM-R-Large, XLM and\nmBERT models, respectively. For the cross-lingual\n17Available at https://fasttext.cc/docs/en/\ncrawl-vectors.html\n0 100 200 300 400 500 600 700 800 900 1000\nSubword Token Counts\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000Frequency\n(a) CSFD – Czert-B\n0 100 200 300 400 500 600 700 800 900 1000\nSubword Token Counts\n0\n20000\n40000\n60000\n80000Frequency\n(b) Mallcz – Czert-B\nFigure 1: Subword token histograms for the CSFD and\nMallcz datasets for the Czert-B model.\nexperiments from Czech to English, the numbers\nof epochs used for the ﬁne-tuning are 25, 5 and\n9 for the XLM-R-Base, XLM-R-Large and\nXLM models18, respectively.\nA.3 Computational Cluster\nFor ﬁne-tuning of the Transformers models we use\nthe Czech national cluster Metacentrum19. We use\ntwo NVIDIA A100 GPUs each with 40GB mem-\nory.\n18The mBERT model did not converge for this experiment\n19See https://wiki.metacentrum.cz/wiki/\nUsage_rules/Acknowledgement\nModel 3 Classes 2 ClassesCSFD FB Mallcz CSFD FB Mallcz\nLog. reg. (ours) 79.63 67.86 76.71 91.42 88.12 88.98LSTM (ours) 79.88±0.18(5e-4 / 2)*72.89±0.49(5e-4 / 5)*73.43±0.12(5e-4 / 10)‡ 91.82±0.09(5e-4 / 2)*90.13±0.17(5e-4 / 5)*88.02±0.24(5e-4 / 2)‡\nCzert-A 79.89±0.60(2e-6 / 8)73.06±0.59(2e-5 / 8)76.79±0.38(2e-5 / 12)91.84±0.84(2e-5 / 8)91.28±0.18(2e-5 / 15)† 91.20±0.26(2e-5 / 14)\nCzert-B 84.80 ±0.10(2e-5 / 12)76.90±0.38(2e-6 / 5)† 79.35±0.24(2e-5 / 15)94.42±0.15(2e-5 / 15)93.97±0.30(2e-5 / 2)92.87±0.15(2e-5 / 15)\nmBERT 82.74 ±0.16(2e-6 / 13)71.61±0.13(2e-6 / 13)† 70.79±5.74(2e-5 / 10)93.11±0.29(2e-6 / 14)† 88.76±0.42(2e-5 / 8)72.79±3.09(2e-5 / 1)\nSlavicBERT 82.59±0.12(2e-6 / 12)73.93±0.53(2e-5 / 4)75.34±2.54(2e-5 / 10)93.47±0.33(2e-6 / 15)† 89.84±0.43(2e-5 / 9)† 90.99±0.15(2e-6 / 14)†\nRandomALBERT 75.79±0.18(2e-6 / 14)62.53±0.46(2e-6 / 14)† 64.81±0.25(2e-6 / 15)† 89.99±0.21(2e-6 / 14)† 81.71±0.56(2e-6 / 15)† 85.38±0.10(2e-6 / 14)†\nXLM-R-Base 84.82±0.10(2e-6 / 15)† 77.81±0.50(2e-6 / 7)† 75.43±0.07(2e-6 / 15)† 94.32±0.34(2e-6 /14)† 93.26±0.74(2e-6 / 5)† 92.56±0.07(2e-6 / 12)†\nXLM-R-Large87.08±0.11(2e-6 / 11 )81.70±0.64(2e-6 / 5)† 79.81±0.21(2e-6 / 24)† 96.00±0.02(2e-6 / 143)† 96.05±0.01(2e-6 / 15)94.37±0.02(2e-6 / 15)†\nXLM 83.67 ±0.11(2e-5 / 11)71.46±1.58(2e-6 / 9)† 77.56±0.08(2e-6 / 14)† 93.86±0.18(2e-5 / 5)89.94±0.27(2e-6 / 15)† 91.97±0.22(2e-6 / 16)†\nTable 7: The ﬁnal monolingual results as macroF1 score and hyper-parameters for all three Czech polarity datasets\non two and three classes. For experiments with neural networks performed by us, we present the results with a 95%\nconﬁdence interval. For each result, we state the used learning rate and the number of epochs used for the training.\nThe †symbol denotes that the result was obtained with constant learning rate, ∗denotes the cosine learning rate\ndecay, ‡denotes exponential learning rate decay, otherwise the linear learning rate decay was used.\nModel CSFD FB\n(Sido et al., 2021) Ours (Sido et al., 2021) Ours\nmBERT 82.80 ± 0.14 (2e-6 / 13) 82.74±0.16(2e-6 / 13) 71.72 ± 0.91(2e-5 / 6) 71.61±0.13(2e-6 / 13)\nSlavicBERT 82.51 ± 0.14 (2e-6 / 12) 82.59±0.12(2e-6 / 12) 73.87 ± 0.50(2e-5 / 3) 73.93±0.53(2e-5 / 4)\nRandomALBERT 75.40 ± 0.18(2e-6 / 13) 75.79±0.18(2e-6 / 14 ) 59.50 ± 0.47(2e-6 / 14) 62.53±0.46(2e-6 / 14)\nCzert-A 79.58 ± 0.46 (2e-6 / 8) 79.89±0.60(2e-6 / 8) 72.47 ± 0.72(2e-5 / 8) 73.06±0.59(2e-5 / 8)\nCzert-B 84.79 ± 0.26 (2e-5 / 12) 84.80±0.10(2e-5 / 12 ) 76.55 ± 0.14(2e-6 / 12) 76.90±0.38(2e-6 / 5)\nTable 8: Comparison of results from (Sido et al., 2021) with results obtained by us.\n0 100 200 300 400 500 600 700 800 900 1000\nSubword Token Counts\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000Frequency\n(a) CSFD – XLM-R-Base and XLM-R-Large\n0 100 200 300 400 500 600 700 800 900 1000\nSubword Token Counts\n0\n20000\n40000\n60000\n80000Frequency\n(b) Mallcz – XLM-R-Base and XLM-R-Large\nFigure 2: Subword token histograms for the CSFD\nand Mallcz datasets for the XLM-R-Base and\nXLM-R-Large models.\n0 100 200 300 400 500 600 700 800 900 1000\nSubword Token Counts\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500Frequency\n(a) CSFD – mBERT\n0 100 200 300 400 500 600 700 800 900 1000\nSubword Token Counts\n0\n20000\n40000\n60000\n80000Frequency\n(b) Mallcz – mBERT\nFigure 3: Subword token histograms for the CSFD and\nMallcz datasets for the mBERT model.",
  "topic": "Czech",
  "concepts": [
    {
      "name": "Czech",
      "score": 0.9277292490005493
    },
    {
      "name": "Computer science",
      "score": 0.8095437288284302
    },
    {
      "name": "Transformer",
      "score": 0.7063356041908264
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5961108207702637
    },
    {
      "name": "Polarity (international relations)",
      "score": 0.5714597702026367
    },
    {
      "name": "Natural language processing",
      "score": 0.5241755843162537
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5056345462799072
    },
    {
      "name": "Language model",
      "score": 0.47425830364227295
    },
    {
      "name": "Task (project management)",
      "score": 0.4416106939315796
    },
    {
      "name": "Linguistics",
      "score": 0.12641820311546326
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Cell",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}