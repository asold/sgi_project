{
  "title": "Audio Transformers",
  "url": "https://openalex.org/W3158129762",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4284385573",
      "name": "Verma, Prateek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2671173797",
      "name": "Berger, Jonathan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3010387158",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2516301810",
    "https://openalex.org/W2936975153",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W3103145119",
    "https://openalex.org/W2782490852",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2795935131",
    "https://openalex.org/W3105661801",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W3090388844",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3093494400",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2916113431",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3021164770",
    "https://openalex.org/W1495590748",
    "https://openalex.org/W3030163527"
  ],
  "abstract": "Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean aver-age precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional net-work designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant band-width filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation.",
  "full_text": "2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 17-20, 2021, New Paltz, NY\nAUDIO TRANSFORMERS\nPrateek Verma and Jonathan Berger\nStanford University\n450 Jane Stanford Way, Stanford CA, 94305,\nprateekv, brg@stanford.edu\nFigure 1: An overview of the proposed Audio Transformer architecture using front end fully connected encoder with Transformer layers and\npooling layers. It takes 1s of input, and divides it into patches of size 25ms, followed by learning a front end, to feed it to Transformer.\nABSTRACT\nOver the past two decades, CNN architectures have produced com-\npelling models of sound perception and cognition, learning hierar-\nchical organizations of features. Analogous to successes in com-\nputer vision, audio feature classification can be optimized for a par-\nticular task of interest, over a wide variety of datasets and labels.\nIn fact similar architectures designed for image understanding have\nproven effective for acoustic scene analysis. Here we propose ap-\nplying Transformer based architectures without convolutional lay-\ners to raw audio signals. On a standard dataset of Free Sound 50K,\ncomprising of 200 categories, our model outperforms convolutional\nmodels to produce state of the art results. This is significant as\nunlike in natural language processing and computer vision, we do\nnot perform unsupervised pre-training for outperforming convolu-\ntional architectures. On the same training set, with respect mean\naverage precision benchmarks, we show a significant improvement.\nWe further improve the performance of Transformer architectures\nby using techniques such as pooling inspired from convolutional\nnetwork designed in the past few years. In addition, we also show\nhow multi-rate signal processing ideas inspired from wavelets, can\nbe applied to the Transformer embeddings to improve the results.\nWe also show how our models learns a non-linear, non-constant\nbandwidth filter-bank, which shows an adaptable time frequency\nfront end representation for the task of audio understanding, differ-\nent from other tasks e.g. pitch estimation. 1\n1*Although not the first instance of an acoustic scene understanding\nmodel without convolutions, this is, to our knowledge, the first end-to-end\none. At the same time as viT, [1] showed how in a two-step process, one can\nachieve the same.\nIndex Terms— Transformers, audio understanding, wavelets\n1. INTRODUCTION AND RELATED WORK\nAcoustic scene analysis is a classical signal processing and machine\nlearning problem whose goal is to predict the contents of an input\nsignal within a brief duration, typically one second. In addition\nto modeling perception, computer simulation of hearing combined\nwith models of other sensory systems will help bridge the gap be-\ntween humans and computers. For the past decade, CNNs have\nbecome a de-facto architecture in learning mappings from fixed di-\nmensional inputs to fixed dimensional outputs [2, 3]. CNN archi-\ntectures inspired from vision [2], adapted for acoustic scene under-\nstanding, achieve similar performance gains for audio also.\nThe core backbone of this work is Transformer architecture\nwhich recently have recently produced state of the art results in\na variety of domains, including protein sequences [4], text [5, 6],\nsymbolic music [7], video[8, 9] and image understanding [10, 11].\nBy learning transformers on the latent representations, and condi-\ntioning a wavenet generator, they were able to achieve compelling\nresults in music generation [12] and style transfer, which was im-\npossible without the guidance of meta-data and convolutional ar-\nchitectures [13]. They have also been used in learning latent audio\nrepresentations such as [14, 15] for solving pseudo-tasks such as in-\nfilling to learn time-dependent representations [1, 6]. As opposed\nto learning latent representations, the reduced time-scales of Trans-\nformers can advantageously model input representations. A major\ndrawback of convolutional architecture is the fixed filter across the\nentire input. Furthermore, Transformers take advantage of attention\narXiv:2105.00335v2  [cs.SD]  11 May 2025\n2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 17-20, 2021, New Paltz, NY\nmechanism with the output at a location dependent upon the input\nat some other location.\nThe core idea of this work is to replace traditional convolutional\nbased architectures [3], combined convolutional and Transformer\narchitectures [16, 17], and recurrent architectures [18, 19] with a\npurely Transformer based architecture. Our work is distinct from\nthe method proposed in [1] which was not an end-to-end approach\nand which required a two step approach (specifically, learning a dic-\ntionary of latent codes, and using the discrete latent codes as an\ninput to transformer architectures). Similar approaches were suc-\ncessfully used in areas such as speech recognition [16] to mimic\nBERT [6]. All these state of the art performances were possible due\nto the architectures’ ability to model long term dependency inputs\nand the attention mechanism present in them enabling focus only\non the part of the input that is important [20].\nThe organization of the paper is as follows: Section I introduces\nthe problem and the literature survey followed by the dataset we\nused to benchmark the results in section II. The next section details\nthe methodology followed by results and discussion in Section IV .\nWe conclude the paper in Section V followed by our thoughts of\nfuture work and references.\n2. DATASET\nWe train and evaluate our architectures with FSD50K [21], an open\ndataset of over 51k audio files comprising over 100 hours of man-\nually labeled audio using 200 classes drawn from the AudioSet [3]\nontology. FSD50K is freely available under the creative commons\nlicense, contains many more high quality audio annotations, and\ntwice number of training examples in the balanced set-up than Au-\ndioSet. We used the already provided training and the validation\nsplits to tune the model and tested them on the evaluation setup pro-\nvided. In total there are about 51,197 clips available ranging from\n0.3-30s. We downsample all the clips to 16kHz sampling rate using\n[22]. We follow the same setup for reporting the results as done\nin [21]. All the training was carried on 1s audio chunks with the\nlabels inherited for all the chunks in clips greater than 1s. For sam-\nples less than 1s the audio clip is repeated to fill 1s, resulting in a\nsingle training example for that clip. On an average, the duration\nper clip is 7.6s, with 1.22 average labels per clip, uploaded by 7225\nuser ids, thus encompassing a diverse range of sources, acoustic en-\nvironments, microphones, and locations, to name a few.\n3. METHODOLOGY\n3.1. Baseline Transformer Architectures\nThis section describes the Transformer architecture as described in\n[20] that we used to train the system as shown in Figure 1. A de-\ntailed explanation is given in [23], but for the sake of clarity and\ncompleteness we describe it here. As a black-box, which we would\ndescribe in more detail in this section, it takes as an input a sequence\nof a fixed length T, and produces the same length but with a chosen\ndimension, which we call E, which denotes the size of the latent\nspace. More specifically, it maps a sequence x = (x1, x2, ....xT )\nto a sequence of same length T, namely z : ( z1, z2, ....zT ) ,\nwhere each of the dimensions of(z1, z2, ....zT ) is the chosen hyper-\nparameter E, which in our case is 64, the size of the embedding.\nFor the sake of brevity, we would explain only one Transformer\nEncoder, and for a model with layers L, each of the stack is super-\nimposed on the other one.\nEach Transformer module consists of a attention block and\na feed-forward block. The output of each of them is passed\nthrough a layer norm and a residual layer. So after both the at-\ntention block and the feed-forward block, if the input to a sub-\nblock (attentionFa or feed-forwardFff block) is a sequence xb, in-\nstead of passing the output directly to the next module/sub-block,\nwe pass along the block layer norm and the residual output xbo as\nxbo = LayerNorm (xb + Fa/ff (xb)) This follows the notion that\nlayer-norm/skip connections help in better convergence/improved\nperformance. We now describe each of the two sub-blocks that are\npart of the transformer block namely, i) multi-headed causal atten-\ntion ii) feed-forward architecture\n3.1.1. Multi-Headed Causal Attention\nA multi-headed causal attention function can be described as a\nweighting function that decides how to get the output of each step.\nIt learns a probabilistic score of how important each of the embed-\nding, is while predicting the output. A multi-headed attention con-\nsists of first learning a probabilistic score. It is then multiplied with\neach of the inputs to determine how important each of the input\nis for prediction of the embedding for a position pos belonging to\n1, 2, 3....T. We use scaled-dot product attention as the type of at-\ntention mechanism. A query, key and a value vector is learned for\neach of the position for each of the inputs. This is done by implicitly\nlearning matrices, WQ, WK and WV to produce a query vector q,\nkey vector k and value vectorv for each of the inputs for a single at-\ntention head. We take the dot product of query and key vectors, the\nresult is multiplied by a normalization factor (the inverse of square\nroot of size of the vector as done in [20]), before taking a soft-max\nacross all the inputs. Each of the value vector is multiplied by this\nscore to get the output of the attention module. Mathematically, for\na query matrix Q, key matrix K, and a value matrix V , it is de-\nfined as, Attention(Q, K, V) = softmax( QKT\n√\ndk\n). We can also\nlearn multiple such attention maps forh attention heads, defined as,\nMutliHeadAttention (Q, K, V) = Concat(h1, h2, ...hh)Wo,\nwhere each of the attention heads hi is defined as\nAttention(Qi, Ki, Vi) =softmax( QiKT\ni√dk\n)\nand Wo is a matrix learned during training.\n3.1.2. Feed Forward Architecture & Positional Information\nWe weigh the saliency of each of the input signal via multi-headed\nattention for passing at a position pos. Each of the input at posi-\ntion pos is passed through a feed-forward architecture. We have the\noutput of the feed-forward layers xbo for an input xb, for the di-\nmension of feed-forward layers dff , in case of 2-layer network is,\nFF (xb) = max(0, xbW1 + b1)W2 + b2.. We apply this function\nidentically at each of the inputs. As described in [20], to each of the\ninputs, positional encoding are added. As the input is passed on as\na list, the model does not take into account the relative position, and\nthus the positional encoding are needed. For any position pos for\nthe dimension i of the latent space, we use sinusoidal function, i.e.\nto each position pos and embedding dimension i in E, we add,\nPEpos,2i/2i+1 = sin/cos(pos/10000(2i/E))\nThis adds positional information for each point in time, of input\nwith dimension E , before passing thorough self-attention layers.\n2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 17-20, 2021, New Paltz, NY\n3.2. Adapting Transformer Architecture for raw waveforms\nWe adapt Transformer architectures using ideas from traditional sig-\nnal processing. Since the Transformer has O(n2) complexity w.r.t\nmemory and computation requirements, we choose to follow the\ntraditional route of windowing the signal. For all the experiments,\nas discussed before we work with 1s of audio input sampled at\n16kHz yielding 16,000 samples.\nFigure 2: Core idea of wavelets utilizing multi-scale learning on\n(left) from [24], and using them to create a layer that operates on\nintermediate Transformer embeddings at various scales. We show a\ndemo signal and we retain half of them, and modify the other half\nusing variable sized windows.\nThe window length is chosen to be 25ms, choosing a non-\noverlapping rectangular window. The rectangular window provides\nthe network an optimal windowing function which, as we will see\nin a few of the learned filters, adapts itself to a shape of han-\nning/hamming window. We fix the front end to be a dense layer\nof size 2048 neurons followed by another layer of size 64, primar-\nily to adapt to the size of the embedding layer of Transformer. A\nsingle dense layer of size 2048 successfully learned a filter-back\nto learn a neural-time frequency representation as shown in [25].\nThis design was chosen as it produced state of the art results for\nan equally difficult problem of pitch estimation in polyphonic audio\n[25], with feed forward layers. Since Transformer layers consist\nonly of attention + feed-forward blocks, we achieve an end-to-end\narchitecture that does not have any convolutional operators. This\nyields a front end representation of 40 time steps each of size 64\ndimensions, (64 being a hyper-parameter). We choose 6 layers of\nTransformer module with the size of latent code being 64 for each of\nthe layers, and 8 attention head, with 128 dim 3-layer dense layer to\nconvert to the desired feature space. For comparing with a smaller\nmodel, we choose 3-layers of Transformers with similar setup. The\nlast layer of Transformer is reduced to a smaller dimension using\naverage pooling across time. The output of the last dense layer of\ndimension 200, chosen same as number of output labels.\n3.3. Transformer Architectures Inspired From CNNs: Pooling\nWe explored further performance enhancements to the baseline\nTransformer architecture proposed in the previous section. For this\nwe draw inspiration of convolutional architectures used for the past\ndecade to understand images [26] and audio [3]. The traditional\nmodels e.g. Resnet-50 [2], consists of using a combination of con-\nvolutional layers followed by pooling. The use of pooling layers\nhas two advantages. It reduces the number of computations by re-\nducing the size of inputs in the higher layers. More importantly\nit allows the higher-layer neurons to have much broader receptive\nfield sizes, and allows the network to learn hierarchically ordered\nfeatures, which is important for a particular problem. Pooled Trans-\nformers outperform the baseline Transformer architecture, while re-\ntaining the same number of parameters. In our experiments average\npooling performed significantly better than max-pooling, as it re-\ntains more information on the signal. As described in Figure 1, we\nuse pooling across time after every two layers of Transformers, with\nstride 1, to reduce the dimensionality of the input by a factor of 2,\nand shows significant performance gain as compared to the original\nTransformer architecture without pooling.\n3.4. Learning multi-scale embeddings\nIn this adaptation, we draw inspiration from wavelet decomposition\nand success of pooling layers. We explored if we can decompose the\nintermediate embeddings out of the Transformer, at multiple scale\nsimilar to idea of wavelet decomposition. In order to achieve it, we\nfix up our kernel to be average operation acrossall windows chosen\nat a particular level. Notice that we choose different widow sizes at\ndifferent dimensions of embedding along the time axis. The man-\nner of implementation is again a design choice and there are several\ninteresting ideas possible in future, including the choice of kernel.\nWe draw inspiration from the work carried out in [24], as seen in\nFigure 2. We adapt the window size, in factors of 1,2,4,8 and so\non, following a geometric progression. The value is assigned to all\nof the elements as opposed to reducing the size, as done in pooling\nthus retaining the same size. This operation in fully differentiable,\nand can be trained in end-to-end architectures. This is different than\nwork carried out on spectral filtering [27], as we choose to operate\nfirstly with variable window size as opposed to fixed windows, and\nsecondly do not take explicit hand-crafted bands of filters. Addi-\ntionally, we choose to model the space of embeddings-time hierar-\nchically with only a few large windows, and large number of smaller\nwindow, most of them being 1 to retain the embeddings at their orig-\ninal scale. This retains the original transformer embeddings, with\nhalf of the embeddings unchanged, and tinkers with the other half.\nThis combination has been at the core of wavelet transforms.\n4. RESULTS & DISCUSSION\nFor all of the architectures, we only tuned learning rate to be con-\nsistent with the results shown in [21]. All of the Transformers have\n6 layers (3 for small transformers) with 64 dim embeddings, and\n3-Layer 128 neuron feed forward layers, and 8 attention head. The\nfront end consists of 1024/2048 dimensional layer followed by a 64\ndimensional dense layer for small and large transformers. We com-\npared the same Transformer architectures with that of using i) pool-\ning layers ii) multi-scale filters. We observed that even the small-\nest of the Transformer architectures outperform traditional convo-\nlutional architectures. This is quite significant, unlike problems in\nvision [10], where the margin was not as significant. Another ob-\nservation is also that the performance keeps improving with more\ndepth. All the models were trained using Tensorflow framework\n[28], with Huber Loss as the error criteria between the predictions\nand the ground truth, using Adam optimizer [29]. We see that the\nmulti-scale approach outperforms the same transformer architecture\nwithout the intermediate layers, which is encouraging. However, is\nnot able to beat the pooling layers. This may perhaps be due to\nability of Transformers to better model smaller latent embeddings\nacross time.\n2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 17-20, 2021, New Paltz, NY\nFigure 3: Sorted filters, learned by the front end, learns a problem specific non linear, non constant bandwidth filter-bank. This is shown by\ncomparing it to that learned by the same front end for polyphonic pitch estimation as shown in [25].\nTable 1: Comparison of various proposed architecture as shown\nin the table below for mean average precision (mAP) metric. We\nsee how even baseline Transformer architectures without using any\nconvolutional layers can outperform widely used CNN architectures\nfor acoustic scene understanding by significant margins. [21]\nNeural Model Architecture mAP # Param\nCRNN [21] 0.417 0.96M\nVGG-like [21] 0.434 0.27M\nResNet-18 [21] 0.373 11.3M\nDenseNet-121 [21] 0.425 12.5M\nSmall Transformer 0.469 0.9M\nLarge 6- Layer Transformer 0.525 2.3M\nLarge Transformer with multi-scale filters 0.541 2.3M\nLarge 6- Layer Transformer with Pooling 0.537 2.3M\n4.1. What the front end learns\nWe follow a strategy similar to that described in [25] to understand\nwhat the filters learn. We deploy the same front end in our work\nwhich is again a feed-forward layer consisting of 2048 neuron, fol-\nlowed by a 64-dim dense layer. This is similar to the analogy of\ngetting a mel-like representation which is learnable end-to-end. Af-\nter training, we take the first layer and sort the filter according to\nthe peaks of their Fourier representation. We see that it manages\nto learn a non-linear, non-constant bandwidth filterbank as seen in\nFigure 3. We also see that with using the same front end for two dif-\nferent applications, namely for pitch estimation and acoustic scene\nunderstanding, the shape and the resolution of the learned filter-\nbank is different. In addition, we can also see a step-wise pattern,\nwhich shows multiple filters assigned to the same frequency bin\nto account for the phase variations of the input signals. Figure 4\ndepicts a few chosen filters for the sake of discussion here. We\nobserve a variety of ideas that can be interpreted from signal pro-\ncessing perspective, and also to take into account the characteristics\nof the input signal i.e. frequency, timbre, and energy. We can see,\nin center-top row, that a filter learns a pure sinusoidal basis of a cer-\ntain frequency. Furthermore, it also manages to learn a windowing\nfunction that closely resembles hanning/hamming window. The fil-\nters in the left column present at the top-bottom are characteristic\nof an onset detector, which can, respectively, be a slow/rapid onset.\nFurther, the filter present in the second row, third column shows a\nslowly moving signal, which may be latching onto the overall en-\nergy envelop of a signal for certain characteristic sounds. These\ncorrelations to traditional signal processing ideas present in these\nfilters are compelling.\nFigure 4: Filters learned from the first layer of front end show strong\ncorrelations to signal processing, particularly learning sinusoidal\nsignals, onset detectors, energy envelops, and windowing functions\n5. CONCLUSION & FUTURE WORK\nWe have shown here how a Transformer architecture without the\nuse of convolutional filters can be adapted for large scale audio un-\nderstanding. The work is promising, outperforming other convo-\nlutional architectures by a significant margin. We show our model\ncan learn a time frequency front end that is adaptable to the partic-\nular problem of interest, in this case, large scale audio understand-\ning. There are several possible research directions ahead. With\nthe advancements in Transformer architectures such as switch trans-\nformers [30], and sparse transformers [31], these results would fur-\nther improve. Additionally, with the success of unsupervised repre-\nsentation learning architectures for audio [1], it will be interesting\nto do large scale pre-training for making robust audio representa-\ntions. It will be also be useful to explore a wider search over hyper-\nparameters to increase the reported precision scores, which most\nlikely will go up even further.\n6. REFERENCES\n[1] P. Verma and J. Smith, “A framework for contrastive and\ngenerative learning of audio representations,” arXiv preprint\narXiv:2010.11459, 2020.\n[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” inProceedings of the IEEE conference\non computer vision and pattern recognition , 2016, pp. 770–\n778.\n2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 17-20, 2021, New Paltz, NY\n[3] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio\nset: An ontology and human-labeled dataset for audio events,”\nin 2017 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2017, pp. 776–780.\n[4] A. Madani, B. McCann, N. Naik, N. S. Keskar, N. Anand,\nR. R. Eguchi, P.-S. Huang, and R. Socher, “Progen: Lan-\nguage modeling for protein generation,” arXiv preprint\narXiv:2004.03497, 2020.\n[5] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. , “Language models are few-shot learners,”\narXiv preprint arXiv:2005.14165, 2020.\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[7] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Si-\nmon, C. Hawthorne, A. M. Dai, M. D. Hoffman, M. Din-\nculescu, and D. Eck, “Music transformer,” arXiv preprint\narXiv:1809.04281, 2018.\n[8] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid,\n“Videobert: A joint model for video and language representa-\ntion learning,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 7464–7473.\n[9] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video\naction transformer network,” inProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n2019, pp. 244–253.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al., “An image is worth 16x16 words:\nTransformers for image recognition at scale,” arXiv preprint\narXiv:2010.11929, 2020.\n[11] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,\nA. Ku, and D. Tran, “Image transformer,” in International\nConference on Machine Learning. PMLR, 2018, pp. 4055–\n4064.\n[12] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and\nI. Sutskever, “Jukebox: A generative model for music,” arXiv\npreprint arXiv:2005.00341, 2020.\n[13] P. Verma and J. O. Smith, “Neural style transfer for audio\nspectograms,” arXiv preprint arXiv:1801.01589, 2018.\n[14] P. Verma, C. Chafe, and J. Berger, “Neuralogram: A deep\nneural network based representation for audio signals,” arXiv\npreprint arXiv:1904.05073, 2019.\n[15] A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu, “Neu-\nral discrete representation learning,” arXiv preprint\narXiv:1711.00937, 2017.\n[16] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec\n2.0: A framework for self-supervised learning of speech rep-\nresentations,” arXiv preprint arXiv:2006.11477, 2020.\n[17] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-\nsupervised learning of discrete speech representations,” arXiv\npreprint arXiv:1910.05453, 2019.\n[18] A. Haque, M. Guo, and P. Verma, “Conditional end-to-end\naudio transforms,” arXiv preprint arXiv:1804.00047, 2018.\n[19] A. Haque, M. Guo, P. Verma, and L. Fei-Fei, “Audio-linguistic\nembeddings for spoken sentences,” in ICASSP 2019-2019\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2019, pp. 7355–7359.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” arXiv preprint arXiv:1706.03762, 2017.\n[21] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra,\n“Fsd50k: an open dataset of human-labeled sound events,”\narXiv preprint arXiv:2010.00475, 2020.\n[22] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haber-\nland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson,\nW. Weckesser, J. Bright, et al., “Scipy 1.0: fundamental al-\ngorithms for scientific computing in python,”Nature methods,\nvol. 17, no. 3, pp. 261–272, 2020.\n[23] G. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M.\nRush, “Opennmt: Open-source toolkit for neural machine\ntranslation,” in Proc. ACL , 2017. [Online]. Available:\nhttps://doi.org/10.18653/v1/P17-4012\n[24] J. Berger, R. R. Coifman, and M. J. Goldberg, “Removing\nnoise from music using local trigonometric bases and wavelet\npackets,” Journal of the Audio Engineering Society , vol. 42,\nno. 10, pp. 808–818, 1994.\n[25] P. Verma and R. W. Schafer, “Frequency estimation from\nwaveforms using multi-layered neural networks.” in INTER-\nSPEECH, 2016, pp. 2165–2169.\n[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei, “Imagenet: A large-scale hierarchical image database,” in\n2009 IEEE conference on computer vision and pattern recog-\nnition. Ieee, 2009, pp. 248–255.\n[27] A. Tamkin, D. Jurafsky, and N. Goodman, “Language through\na prism: A spectral approach for multiscale language repre-\nsentations,” Advances in Neural Information Processing Sys-\ntems, vol. 33, 2020.\n[28] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isard, et al. , “Ten-\nsorflow: A system for large-scale machine learning,” in 12th\n{USENIX} symposium on operating systems design and im-\nplementation ({OSDI} 16), 2016, pp. 265–283.\n[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic op-\ntimization,” arXiv preprint arXiv:1412.6980, 2014.\n[30] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers:\nScaling to trillion parameter models with simple and\nefficient sparsity,” Jan 2021. [Online]. Available: https:\n//arxiv.org/abs/2101.03961\n[31] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generat-\ning long sequences with sparse transformers,” arXiv preprint\narXiv:1904.10509, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8209096789360046
    },
    {
      "name": "Transformer",
      "score": 0.6986718773841858
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5481765270233154
    },
    {
      "name": "Speech recognition",
      "score": 0.5247573852539062
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4780450463294983
    },
    {
      "name": "Audio signal",
      "score": 0.46135827898979187
    },
    {
      "name": "Pooling",
      "score": 0.44908565282821655
    },
    {
      "name": "Filter bank",
      "score": 0.44127795100212097
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4217613637447357
    },
    {
      "name": "Machine learning",
      "score": 0.36460456252098083
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.3109530210494995
    },
    {
      "name": "Computer vision",
      "score": 0.16078868508338928
    },
    {
      "name": "Speech coding",
      "score": 0.12622234225273132
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 33
}