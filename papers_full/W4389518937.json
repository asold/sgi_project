{
  "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues",
  "url": "https://openalex.org/W4389518937",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098605877",
      "name": "Hongru Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132471230",
      "name": "Minda Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098551404",
      "name": "Yang Deng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2036086788",
      "name": "Rui Wang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2127241586",
      "name": "Fei Mi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109577764",
      "name": "Wang Weichao",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2111728759",
      "name": "Ya-sheng Wang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A4316578536",
      "name": "Wai Chung Kwan",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2121363826",
      "name": "Irwin King",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224426291",
      "name": "Kam-Fai Wong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3206455169",
    "https://openalex.org/W3106007100",
    "https://openalex.org/W3035044096",
    "https://openalex.org/W4285213083",
    "https://openalex.org/W4402683869",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W3098641803",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3015322406",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2962989446",
    "https://openalex.org/W4224243241",
    "https://openalex.org/W3092288641",
    "https://openalex.org/W4226487007",
    "https://openalex.org/W4306316892",
    "https://openalex.org/W4285290591",
    "https://openalex.org/W4225425545",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W4285300946",
    "https://openalex.org/W3176908654",
    "https://openalex.org/W3035148359",
    "https://openalex.org/W4385574047",
    "https://openalex.org/W3212092284",
    "https://openalex.org/W4361193179",
    "https://openalex.org/W4308198680",
    "https://openalex.org/W3200814992",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4382202551",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4389520185",
    "https://openalex.org/W4385570169",
    "https://openalex.org/W4387210575",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4221103197",
    "https://openalex.org/W3186804217",
    "https://openalex.org/W4377371656",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4385572598",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4378464746",
    "https://openalex.org/W4287816019",
    "https://openalex.org/W2945978556",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4225824663",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4385572203"
  ],
  "abstract": "Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset Knowledge Behind Persona (KBP), which is the first to consider the dependency between persona and implicit knowledge. Experimental results on the KBP dataset demonstrate that the SAFARI framework can effectively produce persona-consistent and knowledge-enhanced responses.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9556–9569\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models as Source Planner for Personalized\nKnowledge-grounded Dialogues\nHongru Wang♡, Minda Hu♣, Yang Deng♠∗, Rui Wang♢, Fei Mi♢, Weichao Wang♢,\nYasheng Wang♢, Wai-Chung Kwan♡, Irwin King♣, Kam-Fai Wong♡∗\n♡Department of Systems Engineering and Engineering Management,\n♣Department of Computer Science and Engineering,\nThe Chinese University of Hong Kong\n♠National University of Singapore, ♢Huawei Noah’s Ark Lab\n{hrwang, kfwong}@se.cuhk.edu.hk ydeng@nus.edu.sg\nAbstract\nOpen-domain dialogue system usually requires\ndifferent sources of knowledge to generate\nmore informative and evidential responses.\nHowever, existing knowledge-grounded dia-\nlogue systems either focus on a single knowl-\nedge source or overlook the dependency be-\ntween multiple sources of knowledge, which\nmay result in generating inconsistent or even\nparadoxical responses. To incorporate multi-\nple knowledge sources and dependencies be-\ntween them, we propose SAFARI, a novel\nframework that leverages the exceptional ca-\npabilities of large language models (LLMs) in\nplanning, understanding, and incorporating un-\nder both supervised and unsupervised settings.\nSpecifically, SAFARI decouples the knowledge\ngrounding into multiple knowledge sources se-\nlection and response generation, which allows\neasy extension to various knowledge sources in-\ncluding the possibility of not using any sources.\nTo study the problem, we construct a person-\nalized knowledge-grounded dialogue dataset\nKnowledge Behind Persona (KBP), which is\nthe first to consider the dependency between\npersona and implicit knowledge. Experimental\nresults on the KBP dataset demonstrate that the\nSAFARI framework can effectively produce\npersona-consistent and knowledge-enhanced\nresponses.\n1 Introduction\nKnowledge enhancement techniques (Yu et al.,\n2022) have significantly empowered machines\nto deepen their understanding of the underlying\nknowledge in open-domain dialogues (Huang et al.,\n2020), surpassing what can be solely acquired from\nconversational corpora. Recent years have wit-\nnessed various open-domain dialogue systems rely-\ning on different types of knowledge sources, such\nas topic (Wang et al., 2022; Zhu et al., 2023), ex-\nternal documents ( e.g., Wikipedia) (Dinan et al.,\n∗Co-Corresponding Author.\n2019; Zhou et al., 2020; Wang et al., 2023c; Deng\net al., 2023c), persona (Zhang et al., 2018; Liu et al.,\n2022a; Wang et al., 2023b; Deng et al., 2023b), user\nmemory (Xu et al., 2022c,b; Park et al., 2023; Deng\net al., 2022), and more. Realizing the limitations of\nusing single-source knowledge, some recent stud-\nies further develop dialogue systems with access to\nmulti-source knowledge (Wu et al., 2021, 2022b;\nJang et al., 2022).\nDespite their effectiveness of enriching the dia-\nlogue responses with multi-source knowledge, ex-\nisting methods typically design models to incor-\nporate all sources indiscriminately, resulting in a\ncumbersome process that struggles to handle cases\ndependent on the interaction between some specific\nsources instead of all (Wu et al., 2022b; Fu et al.,\n2022). Moreover, the importance of comprehend-\ning the potential dependency between knowledge\nsources is overlooked in previous works, which\nmay result in generating paradoxical responses\n(Majumder et al., 2020; Jang et al., 2022). For ex-\nample, humans often express their persona with the\nassistance of external knowledge. As shown in Fig-\nure 1(a), for responding to the question \"Hi, what\ndo you like to eat?\", it is inadequate to only incor-\nporate single-source knowledge from user persona,\ne.g., \"I am a vegetarian\", since relevant informa-\ntion from external documents is also required, e.g.,\nVegetarian(Vegetarians like to eat fruits and vegeta-\nbles). However, being unaware of the dependency\nbetween these two different sources of knowledge\n(persona and documents), dialogue systems may\nselect the document implying inconsistent personas\n(e.g., \"Food contains meat, fruits, ...\"), leading to\nresponses conflicting with defined personas (e.g.,\n\"I like to eat meat \"). Therefore, it attaches great\nimportance in modeling the interaction and depen-\ndency of different sources1 in building knowledge-\ngrounded dialogue systems.\nThe absence of problem definitions and well-\n1More dependency cases can be found in the Appendix A.\n9556\n你好呀，我来自安徽，你是哪里人呀？Hello,IamfromAnhuiprovince,whichprovinceareufrom?Sources\nPersonaDocuments\n非常高兴认识你，我是广东人。Nicetomeetyou,IamfromGuangdongprovince.\nMemory\nPersonaConfigLocalKnowledgeUserMemory\nMiddleResults\n我是深圳人深圳是广东省的一座城ShenzhenisoneofcitiesinGuangdong.IliveinShenzhen.\n… ……\nRetrieveTop-N\nIliketoeatfruits andvegetable.\nPersona\nNULLAsanAIlanguagemodel,Idonothave……\nNULLIamavegetarian\nIliketoeatmeat.\nDocumentswodependency Documentswdependency Foodcontainsmeat,fruits,…\nIliketoeatstones,…\nPersona-ConsistentPersona-Inconsistent\nHi,whatisyourfavorite food?\nVegetarian liketoeatfruits andvegetable \nPersona-Inconsistent\n(a)Dependency betweenMultipleSources(b)SAFARIFramework\nNULL\n你是南方人还是北方人？\n南方人，我来自华南地区。\n你平时攀岩吗？不敢去，我恐高\n那你会不会唱歌？太难了，我只会听\nAreyoufromthesouthornorthofChina?\nSouthofChina.\nDo you usually rock climb?\nNo,Iamtooscaredofheightsofclimb.\nCanusingthesongs?\nUsedPersonaUsedKnowledgeP1 P1-K1\nIt’ stoodifficult,Icannot.\nUsedPersonaUsedKnowledgeP3 P3-K1\nUsedPersonaUsedKnowledgeN N\nFoshanbelongstotheregionofSouthChina.\n我是佛山人\n我有恐高症\nIcomesfromFoshan.\nI have acrophobia\nPERSONA\nDOCUMENTS{P1:K1,…K5,…,P3:K1,…}\n有恐高症的人无法爬山攀岩，无法做飞机看窗外Peoplewhohaveacrophobiacannotclimbmountainsandrockclimbing.\nP1\nP3\nK1 ……佛山的所属地区是中国华南地区\nK1\n……\n……\nP1\nP3\nPersonaConfig\n(c)AnexampleofKBPdataset\n……\nFigure 1: (a) An example of dependency of two sources involved in the persona-consistent dialogue system\n(PERSONA and DOCUMENTS); (b) our proposed SAFARI framework to plan, retrieve, and incorporate multiple sources\nof knowledge: PERSONA, DOCUMENTS, and so on. Planning, Retrieval and Assembling steps are divided by dashed\nlines; (c) A sample from the KBP dataset. There are three situations of responses in our datasets: 1) response\nwithout the need for any sources (NULL), 2) response using only personae description (from PERSONA source), and 3)\nresponse using both persona and knowledge (from PERSONA, DOCUMENTS sources). The example here presents the\nfirst and third situations. We highlight the response and used knowledge with the same color.\nestablished benchmarks greatly impedes the\nprogress of building dialogue systems that can\ncapture the knowledge dependency between dif-\nferent sources. To this end, we construct a per-\nsonalized knowledge-grounded dialogue dataset,\nnamed Knowledge Behind Persona (KBP), to\nmimic the scenario where the understanding of\npersona-knowledge dependency is required to pro-\nduce consistent responses. KBP aims to build better\npersona-consistent dialogue systems with the help\nof utilizing underlying knowledge behind different\npersona descriptions comprehensively.\nIn order to address the interaction and depen-\ndency issue between specific sources, we pro-\npose a novel framework, named Source plAnner\nFor personAlized knowledge-gRounded dIalogues\n(SAFARI). Seeing the potential of large language\nmodels (LLMs) in planning the use of external in-\nformation (Schick et al., 2023; Shen et al., 2023;\nGao et al., 2023), we explore LLMs’ capability of\nconnecting different sources in the context of the\npersonalized knowledge-grounded dialogue system.\nAs illustrated in Figure 1(b), the whole response\ngeneration can be modeled into three steps in SA-\nFARI: 1) Planning to make a series of decisions of\nwhether to use a specific knowledge source given\nrelationship descriptions between different sources;\n2) Retrieval to retrieve top-n results from external\ndatabases according to the decisions; 3) Assem-\nbling to incorporate all retrieved knowledge into\nthe final response generation. Benefiting from de-\ncoupling source selection and response generation,\nour framework is more flexible and scalable, allow-\ning independent modification of each component.\nAdditionally, our framework can easily accommo-\ndate scenarios where multiple or no sources are\nrequired. To sum up, our contributions are listed\nbelow:\n• We propose the SAFARI framework to augment\nthe dialogue system to plan and incorporate mul-\ntiple sources of knowledge into responses ((e.g.,\ndecide whether or not require knowledge, which\nsource to call, and when to call)), and further ad-\ndress the knowledge dependency issue between\nsources in both supervised and unsupervised\nmanners by leveraging LLMs.\n• We build a personalized knowledge-grounded di-\nalogue dataset, KBP, where the responses are\nconditioned on multiple sources of knowledge,\nleading to more user-engaged dialogues with in-\nformative and persona-consistent knowledge.\n• We conduct exhaustive experiments to validate\nthe effectiveness of our proposed framework to\nincorporate multiple sources and capture the de-\npendency between them2.\n2 Related Works\n2.1 Personalized Dialogue System\nTo build a personalized dialog agent, Zhang et al.\n(2018) extensively investigated this task with a\nnew dataset Persona-Chat, where a pre-defined per-\nsona set is a form of multiple sentences of tex-\ntual description. Lots of works follow this setting\nand have taken mutual persona perception (Liu\n2The code can be found in https://github.com/\nruleGreen/SAFARI/.\n9557\net al., 2020; Kim et al., 2020; Xu et al., 2022a),\npersona-sparse scenario (Song et al., 2021; Welch\net al., 2022), long-term persona memory (Xu et al.,\n2022c), persona extending (Liu et al., 2022b) and\npersona order bias (Chen et al., 2023) into con-\nsideration. Although some of them complement\nthe insufficient semantics in short persona descrip-\ntions by further utilizing an external commonsense\nknowledge base to extend existing persona sets\n(Majumder et al., 2020; Liu et al., 2022b), they\nstill fall into the conventional framework coupling\nthe knowledge selection with the response gener-\nation (Wu et al., 2022b), rendering it infeasible to\nhandle various sources of knowledge. There have\nalso been works showing that the combination of\ndifferent knowledge sources such as persona de-\nscriptions and Wikipedia can further improve the\noverall performance (Jang et al., 2022; Wu et al.,\n2021, 2022a). However, they still fail to capture\npossible dependency between knowledge sources.\nIn their framework, knowledge is not used as the\nrole to assist persona-consistent response genera-\ntion, but as an additional resource to generate a\nmore informative response (Dinan et al., 2019; Xue\net al., 2023) or select a suitable persona (Jang et al.,\n2022; Fu et al., 2022).\n2.2 LLMs for Planning\nLarge Language Models (LLMs) show remarkable\ncapabilities in planning the use of various exter-\nnal resources, such as tools (Schick et al., 2023),\nmodels (Shen et al., 2023), and APIs (Li et al.,\n2023), to solve various NLP tasks and suit differ-\nent applications in practice. Alternatively, different\ntypes of knowledge can be retrieved from external\nsources, as illustrated in WebGPT (Nakano et al.,\n2022) and ReAct (Yao et al., 2023). Integrating\nvarious knowledge sources to improve the quality\nof LLM generation becomes increasingly challeng-\ning due to the need for strategic planning, sequen-\ntial decision-making, and complex reasoning. Pre-\nvious research primarily focuses on either earlier\ndecision-making stages (Nakano et al., 2022; Shen\net al., 2023) or the subsequent response generation\n(Sun et al., 2023; Schick et al., 2023; Deng et al.,\n2023a), instead of establishing a complete frame-\nwork for planning the use of multiple knowledge\nsources to generate appropriate responses. There is\na latest work named TPE which regards different\nknowledge sources as conceptual tools and pro-\nposes a multi-persona collaboration framework to\nmodel the decision-making process of the call or-\nder for multiple knowledge sources (Wang et al.,\n2023a). We differ in exploring the planning capa-\nbility of LLMs to decide whether or not require\nknowledge, which source to call, and when to call\nin both the supervised and unsupervised manner.\n3 Data Collection\nIn this section, we detailedly introduce the process\nof data collection and statistics of the collected\ndata. The data collection process can be divided\ninto two steps: Step 1. Persona and Knowledge\nAcquisition and Step 2. Dialog Collection.\n3.1 Persona and Knowledge Acquisition\nSeeds Preparation. To reduce annotation\ncost, we take advantage of the currently available\npersona dialogue dataset: DuLemon (Xu et al.,\n2022c) and two widely-adopted Chinese knowl-\nedge bases: Baike3 and Ownthink4 to produce seed\ndata. Specifically, we first cluster all persona sen-\ntences from DuLeMon into 10 topics. After re-\nmoving duplicate, rare, and similar personas, we\ncarefully choose around 20 personas for each left\ntopic as seed personas5. In addition, we manually\nadd some personas for the existing topics and new\ntopics. The final personas consist of age, nation,\npersonality, career, movie, music, sport, book, con-\nstellation, locality, gender, others. For retrieving\npersona-related knowledge, we simply combine\ntwo aforementioned knowledge bases with similar\nfiltering operations and store them as (head entity,\nattribute, tail entity) tuples.\nPersona and Knowledge Matching. For each\npersona sentence, we segment it into a sequence\nof words with a Chinese word segmentation tool\njieba6. If any words exactly match the head en-\ntity or tail entity of a certain knowledge tuple, we\ntransform the tuple into a sentence according to pre-\ndefined templates and then save it as one knowl-\nedge for this persona sentence. In this way, we\ncan obtain various knowledge for each persona sen-\ntence. Consistent with previous works (Zhang et al.,\n2018; Majumder et al., 2020), we randomly sample\n3 persona sentences along with 5 knowledge sen-\ntences per persona to form a persona description\n3http://www.openkg.cn\n4http://github.com/ownthink/KnowledgeGraphData\n5Some topics are removed if they contain less than 20\npersonas. We don’t pick hundreds of personas because one\npersona sentence has a vast amount of knowledge behind it.\n6https://github.com/fxsjy/jieba\n9558\nof the system for each dialog. More details and\ntemplates can be found in Appendix B.\n3.2 Dialogue Collection\nCollection Setting. Following the setting of Jang\net al. (2022), annotators are instructed to make a\ndialogue by considering persona and correspond-\ning knowledge under the single-person setup. In\nthis way, one person can better understand what\npersona to ask as the human and what knowledge\nto use as the system, in comparison with two in-\ndependent persons playing two roles separately.\nDuring the collection, each annotator first selects\na suitable persona and then optionally identifies\nrelevant knowledge, giving a knowledge-enhanced\nand persona-consistent response at last.\nTraining and Pilot Annotation. All annotators\nare first required to take a training tutorial to learn\nthe annotation procedure, requirements, and exam-\nples of annotated dialogues. Afterward, they are\ngiven 30 personas to make 10 dialogues. We pro-\nvide corresponding feedback to help them adjust\ntheir annotation criteria. To establish the necessity\nof persona-knowledge dependency, we consider\nthe situation where the response will be persona-\ninconsistent without the assistance of knowledge.\nTo this end, annotators are requested to ask ques-\ntions centered on the implications based on knowl-\nedge and persona. For example, the annotator is\nsupposed to ask \"Which province are you from?\"\ninstead of \"Which province does Shenzhen belong\nto?\", given the persona \"I live in Shenzhen\" and\ncorresponding knowledge \"Shenzhen belongs to\nGuangdong province\".\nBatch Collection. After pilot annotation, we\nconduct dialogue collection batch by batch and reg-\nularly coach the quality of collected data 7. For\neach batch, we sample personas different from pre-\nviously annotated dialogues to increase its diversity\nin the whole dataset. The addition, deletion, and re-\nvision of persona and knowledge are also accepted\nand updated at the batch level8.\nGrounding Annotation. We also gather the labels\nof grounding knowledge sources for the system’s\nresponses by asking the annotators to specify the\nsources they draw from while providing responses,\nsuch as PERSONA or DOCUMENTS. For instance, gen-\n7We write a python script to check typos (e.g. the labels\nof used knowledge is not exist in given knowledge bases) and\nprovided feedback after each batch.\n8The annotators must check for persona conflicts and re-\nfrain from relying too much on single knowledge.\nKBP Train Valid Test\n# dialogues 1,981 248 248\n# samples 9,821 1,227 1,229\n# avg turns 4.96 4.93 4.96\n# utterances 19,642 2,454 2,458\n# avg length 17.6 17.3 17.5\n# resp w/ persona 86.1% 84.4% 85.3%\n# resp w/ p_and_k 76.3% 74.2% 75.1%\nTable 1: Statistics of KBP dataset.\nerating a response may rely on persona alone or\nboth persona and knowledge. With the labels of\nthese grounded sources, the planning abilities of the\ndialogue systems can be quantitatively measured.\n3.3 Statistical Analysis\nWe organize the collected personas ( PERSONA\nsource), persona-related knowledge 9 (DOCUMENTS\nsource), and dialogues in the form shown in Fig-\nure 1(c). We finally collect 2,477 dialogues and\n24,554 utterances with 5 turns per dialogue on av-\nerage. We then split the collected data into train,\nvalidation, and test sets using the 8:1:1 ratio. The\ndataset statistics are summarized in Table 1, includ-\ning the number of dialogues, utterances, average\nlength, as well as data sources used. The aver-\nage length per utterance reaches 17.6, hinting at\nthe informativity and depth of the conversation.\nIt is shown that over 86% of responses used per-\nsona (i.e., resp w/ persona) and 76% used both per-\nsona and knowledge (i.e., resp w/ p_and_k), which\nshows that KBP is capable as a benchmark to eval-\nuate the different grounding abilities of models.\n4 Method\n4.1 Task Definition\nWe first provide a general definition of a dia-\nlogue system that requires multiple sources and\nthen we instantiate the definition in the con-\ntext of personalized knowledge-grounded dia-\nlogue. For each dialogue, the dialogue context\nc = {u1, s1, u2, s2, ..., ut} and different knowl-\nedge sources K = {K1, K2, ..., Ki} are provided,\nwhere Ki = {k1\ni , k2\ni , ..., kj\ni } indicates the ith\nsource’s name of K. kj\ni denotes the jth knowl-\nedge in natural language from Ki. If K2 is reliant\non K1, knowledge should be retrieved from K2\nbased on the selected knowledge in K1. Such re-\nliance should also be embodied in the construction\n9The knowledge related to the same persona forms a docu-\nment, and different documents form DOCUMENTS source.\n9559\nof K2, in a way such as K2 = {k1\n1 : {k1\n2, k2\n2}, k2\n1 :\n{k3\n2, k4\n2}, ...}. The goal of the system is to gen-\nerate a response st conditioned on c and a set of\nknowledge {kj\ni , ..., km\nn } retrieved from K if re-\nquired10. Specifically in the context of person-\nalized knowledge-grounded dialogue, we regard\n{PERSONA, DOCUMENTS} as {K1, K2} respectively.\nThere is a potential dependency between these two\nsources, and the goal is to generate a response\nst conditioned on a set of knowledge {pj\ni , ...kn\nm},\nwhere pj\ni is retrieved from PERSONA and kn\nm is re-\ntrieved from DOCUMENTS. The response can also be\ngenerated conditioned on a set of knowledge from\na single source PERSONA or without any sources.\n4.2 Supervised SAFARI\nThere are three different steps in our proposed SA-\nFARI framework: Planning, Retrieval, and As-\nsembling as shown in Figure 1(b). We will intro-\nduce them step-by-step under both supervised and\nunsupervised settings.\nPlanning The goal of the planning step is to\nmake a series of decisions to decide whether or not\nthe corresponding source of knowledge is required\nand determine their call order if needed. Since\nthe dependency relationship is previously known,\nwe only need to make sure that a certain knowl-\nedge source is called after the sources it depends\non. Thus, we formulate this task as sequence-to-\nsequence generation by directly outputting either\nrequired sources in execution order or NULL if the\nresponse does not need any knowledge as follows:\nM : c → Ki, Kj, ..., Kn or NULL, (1)\nwhere M is parameterized by LLMs. We add\nKi, ..., Kn and NULL into the vocabulary of LLMs\nas special tokens. The key idea here is similar to\nthe recent ToolkenGPT (Hao et al., 2023), which\nregards different tools as special tokens in the vo-\ncabulary. Besides that, we add other special to-\nkens to indicate the different parts of the input, i.e.,\n[SOURCE] and [EOS] to indicate the start and end\npositions of sources. In this way, LLM can model\nthe dependency between different sources and learn\nwhen and how to call certain sources.\nRetrieval According to the output of the last step,\nthere are two cases in this step: (1) the response\ndoes not need any external sources of knowledge,\n10Unlike most of the previous works, we also consider the\nresponse that does not need any knowledge in our setting.\nLargeLanguageModels（LLMs)\nDialogHistory\nSelectedSources:PERSONADOCUMENTS\nRetrievedResultsPERSONADOCUMENTS\nSystemResponse\nStep1:Planning Step3:Assembling\nPERSONADOCUMENTS\nRetrievedResults\nStep2:Retrieval\nRetriever\nFigure 2: The supervised framework of SAFARI for\npersonalized knowledge-grounded dialogues. We use\ndifferent colors to indicate different steps. The black\narrow denotes the flow of data without the the involve-\nment of LLM.\nand the agent can skip this step; (2) the response\nneeds multiple sources of knowledge, and the agent\nstrictly follows the output source order to retrieve\ntop-n related knowledge k∗\ni for the ith knowledge\nsource according to the dialogue context c, and\nif there is a dependency here, it will use preced-\ning retrieved results k∗\nj in the planned execution\norder as a filter. Specifically, assuming the output\norder is PERSONA, DOCUMENTS in the planning step\nfor a persona-consistent dialogue system, we first\nretrieve top-1 result p∗from PERSONA, and then we\nretrieve k∗from DOCUMENTS according to c and p∗.\nHere the utilization of p∗depends on the system-\natic design. For example, if there is a designated\nsource K∗for each p∗(a.k.a dependency), we can\nsimply retrieve k∗from K∗. And there is another\ncase where all knowledge is stored together and we\ncan concatenate c and p∗as a query in the retriever.\nR : Ki, Kj, ..., Kn → kj\ni , ..., km\nn (2)\nAssembling We concatenate all preceding results\ntogether with the dialogue context c to generate the\nresponse:\nM : Inp → st, (3)\nwhere Inp = {c [SOURCE]Ki, ..., Kn[EOS]\n[MIDDLE]kj\ni , ..., km\nn [EOM]}. [MIDDLE] and [EOM]\nrepresent the start and end positions of retrieved\nresults. Forming the input in this way has two ad-\nvantages. Firstly, the name of the sources indicates\nthe type of results retrieved, which provides more\nsignals to the LLMs. Secondly, it allows us to train\nthe language model in a multi-task manner using\nteacher forcing. The loss is only calculated on to-\nkens related to the planning and the response as\nshown in Figure 2. We first predict the planning\n9560\nand then generate the response according to the\npreceding results when inference.\n4.3 Unsupervised SAFARI\nInspired by the recent progress using LLMs as a\ncontroller to plan a call order of different mod-\nels (Shen et al., 2023), we adopt a similar way\nhere by providing detailed prompts to leverage the\nLLMs’ capability to address the dependency be-\ntween different knowledge sources. We consider\ntwo settings: zero-shot and in-context learning for\nplanning and assembling steps here since the re-\ntrieval step is the same as above.\nThere are different knowledge bases storing relevant infor-\nmation:\nK_1: {K_1_DESC}\nK_2: {K_2_DESC}\n......\nThere exists a dependency between these knowledge bases.\n{DEPENDENCY_DESC}\nHere is the dialogue between the user and the system: {DI-\nALOGUE}\nBased on the user’s last question, please determine if it\nrequires invoking the corresponding knowledge base. If the\ninvocation is necessary, output the names of the knowledge\nbases in the order they should be invoked. If no invocation\nis needed, output NULL.\nTable 2: The zero-shot prompt of unsupervised SAFARI\nat planning step (translated from Chinese to English).\nThe dialogue is as follows:\n{DIALOGUE}\nThe following knowledge is retrieved from different\nsources of knowledge bases:\n{MIDDLE_RESULTS}\nPlease play the role of the system and generate a reply ac-\ncording to the context of the dialogue and given knowledge.\nPlease make sure your reply is consistent with the given\nknowledge. If the provided knowledge is NULL, generate\na response solely based on the dialogue context.\nSystem:\nTable 3: The zero-shot prompt of unsupervised SAFARI\nat assembling step (translated from Chinese to English).\nPlanning. Instead of directly providing supervision\nsignals, we provide a description for each source\nof knowledge, accompanied by the corresponding\ndependency between the sources. The prompts are\nshown in Table 2.\nAssembling. We feed the dialogue content and the\nretrieved knowledge into the prompt as organized\nin Table 3, adapting LLMs to generate responses\naccording to dialogue context and retrieved results.\nThe full prompts of the unsupervised planning\nstep can be found in Appendix D. For few-shot in-\ncontext learning, we prepend three corresponding\ndemonstrations from the train set to the zero-shot\nprompts during evaluation.\n5 Experiments\n5.1 Experimental Setups\nImplementation Details. We mainly choose\nBELLE-LLAMA-7B-2M (Ji et al., 2023) and\nChatGLM-6B (Du et al., 2022; Zeng et al., 2023)\nas two backbone models for supervised setting\nsince they are two popular open-source Chinese\nmodels. And we additionally add ChatGPT (gpt-\n3.5-turbo-0301)11 for the unsupervised setting. For\ntraining, we set the batch size as 8, train models\nwith 3 epochs and save the checkpoint with the\nlowest validation loss. For other hyper-parameter\nsettings, we mainly follow the corresponding\nofficial code12. Due to the computation limit, we\nconduct training with LoRA (Hu et al., 2021)\nat one single 3090 GPU, and it cost about 4-6\nhours. For the unsupervised setting, we set both\nthe temperature and top p as 0.1 to reduce the\nrandomness of LLMs. Besides that, we use three\ntypes of retrievers including both sparse and dense\nretrieval: BM25 (Robertson and Zaragoza, 2009),\nDPR (Karpukhin et al., 2020), and RocketQAv2\n(Ren et al., 2021). We only retrieve the top-ranked\nresult from each source in the experiments13.\nEvaluation Metrics. During the evaluation, we\nuse different metrics at different steps. We use F1\nfor planning, Recall@1 for retrieval, and BLEU\n(Papineni et al., 2002), Rouge-L (Lin, 2004). For\nassembling, Knowledge Consistency (K.C) and Per-\nsona Consistency (P.C) are calculated using our\nfinetuned NLI models (Madotto et al., 2019). More\ndetails including retrieval models, NLI models, and\nNLI metrics can be found in Appendix C.\n5.2 Performance of Planning\nThere are three types of decisions representing\ndifferent sources required in the next step: NULL,\nPERSONA, and Both (selecting both PERSONA and\nDOCUMENTS). Table 4 demonstrates the F1 of plan-\nning under different settings. Under supervised\nsettings, despite LLMs achieving high F1 scores at\nBoth, the performance at NULL and Persona is still\nunsatisfactory, since there are fewer training sam-\nples in these two cases. On the other hand, under\n11https://openai.com/blog/chatgpt\n12https://github.com/THUDM/ChatGLM-6B and https:\n//github.com/LianjiaTech/BELLE\n13The effects of different choices is analyzed at Section 6.\n9561\nModel NULL Persona Both\nSupervised\nBELLE-LLAMA-7B-2M 42.67 (194) 14.08 (17) 83.77 (1018)\nCHATGLM-6B 47.10(129) 31.96(69) 86.59(1031)\nUnsupervised\nZero-shot\nBELLE-LLAMA-7B-2M 28.55(940) 8.94 (54) 32.47 (235)\nCHATGLM-6B 25.60 (1225) 0.0 (0) 0.43 (4)\nCHATGPT 11.45 (116)20.67(233) 74.88(880)\nIn-context\nBELLE-LLAMA-7B-2M 9.22 (36) 18.21 (1193) 0.0 (0)\nCHATGLM-6B 25.67 (1190) 1.49 (9) 4.62 (30)\nCHATGPT 27.95(699) 23.14(238) 41.98(292)\nTable 4: The F1 of different decisions in Planning of\ndifferent LLMs under supervised/unsupervised settings.\nWe also report the frequency of different decisions in\nthe bracket. There are 181 NULL, 125 PERSONA and 923\nPERSONA, and DOCUMENTS in the ground planning.\nunsupervised settings, the LLMs are over-confident\nin their decisions to use NULL, and they misunder-\nstand the dependency between different sources\n(sometimes deciding to only use DOCUMENTS with-\nout PERSONA)14. This result reveals the LLMs’ low\naccuracy in expressing uncertainty and fetching un-\nknown knowledge. Furthermore, in-context learn-\ning cannot improve this situation, which is similar\nto the observation in Amayuelas et al. (2023).\n5.3 Performance of Retrieval\nWith the ground-truth planning labels (except\nNULL), we examine three types of retrievers, includ-\ning BM25, RocketQAv2, and DPR, to evaluate\nthe retrieval performance. Table 5 presents the Re-\ncall@1 (R@1) of the different retrievers. We found\nthat the DPR and RocketQAv2 can achieve over\n80% R@1 when retrieving from PERSONA source\nwhile only about 50% from DOCUMENTS and the\nR@1 at DOCUMENTS†further decreases after remov-\ning the dependency. First, the semantics between\ndifferent knowledge from DOCUMENTS with the de-\npendency are similar to the same underlying per-\nsona p∗, making them more difficult to be distin-\nguished. In addition, noisy knowledge sentences\nare introduced since there exists no dependency.\nMoreover, we observe that DPR performs the best\nout of these three retrievers in all sources of knowl-\nedge while BM25 performs worst15, revealing the\nimportance of dense retrieval models in this task.\n14We assign the case that LLMs predict DOCUMENTS only as\nNULL since this case does not exist in KBP.\n15RocketQAv2 is generally not competitive with DPR be-\ncause of the pre-trained weights in the RocketQAv2, since it\nis pre-trained using QA datasets and the length of the question\nis much shorter than dialogue context.\nModel Persona Both\nPERSONA DOCUMENTS DOCUMENTS†\nBM25 36.80 48.97 15.05 11.37\nRocketQAv2 80.00 92.31 50.49 35.75\nDPR 83.20 93.07 51.67 39.33\nTable 5: The performance ofRetrieval of different types\nof retrievers. There are 125 examples that only require\nPERSONA and 923 require both PERSONA and KNOWLEDGE.\nWe also report the Recall@1 of DOCUMENTS without\ndependency (DOCUMENTS†).\nTherefore, we set DPR as the retriever in our ex-\nperiments afterward.\n5.4 Performance of Assembling\nTable 6 demonstrates the performance of response\ngeneration under both supervised and unsupervised\nsettings. Referring to Table 4, the performance of\nthe planning step largely affects the results in the\nassembling step, when the retriever is the same.\nMostly, better planning leads to better responses in\nall metrics. The supervised models are much better\nthan unsupervised models since their planning re-\nsults are much better, while ChatGPT performs best\nunder unsupervised settings due to a similar rea-\nson. We found that BELLE achieves higher BLEU1\nand Rouge-L, K.C but lower P.C than ChatGLM\nsince the planning gap between them mainly comes\nfrom PERSONA. In addition, due to poor retrieval\nperformance at DOCUMENTS (Table 5), the consis-\ntency score K.C is also much lower than P.C. With\ndemonstrations in the prompt, we observe generally\nbetter performance on most metrics, since LLMs\ntend to accept the personalized role, rather than gen-\nerating responses like “As an AI language model,\nI do not have persona ....”. Overall, we conclude\nthat the grounding ability of supervised models is\nmuch better than unsupervised ones, and ChatGPT\nperforms best under the unsupervised setting.\n6 Analysis\nIn this section, we analyze the effects of differ-\nent components and the choice of the number of\nretrieved results, based on ChatGLM under the su-\npervised setting. In addition, we conduct human\nevaluations to verify the quality of automatic eval-\nuations.\n6.1 Impacts of Different Steps\nWe investigate the effects of individual steps by\nproviding the model ground-truth labels from each\nstep to generate the response, enabling us to an-\n9562\nModel BLEU1 Rouge-L P.C K.C\nSupervised Setting\nBELLE-LLAMA-7B-2M 30.48 34.61 75.34 46.62\nCHATGLM-6B 23.81 26.70 76.99 42.39\nUnsupervised Setting\nZero-shot\nBELLE-LLAMA-7B-2M 11.84 19.24 30.59 27.34\nCHATGLM-6B 6.18 14.50 14.73 24.73\nCHATGPT 12.06 24.44 73.47 38.00\nIn-context\nBELLE-LLAMA-7B-2M 19.51 22.25 72.98 24.89\nCHATGLM-6B 13.74 19.69 16.92 24.89\nCHATGPT 16.03 25.62 46.38 35.56\nTable 6: The performance of Assembling under super-\nvised/unsupervised settings.\nalyze and understand the specific effects of each\nstep in a clear and systematic way. Table 7 presents\nthe results. First, we note that the inclusion of\nground-truth planning labels or knowledge casts\na positive impact on performance. Planning pri-\nmarily enhances P.C and K.C, while grounding\nknowledge contributes to BLEU1 and Rouge-L\nscores. The best results are obtained when both\nsignals are combined. Secondly, we also conduct\nan ablation study by removing some modules: 1)\nremoving dependency information (-Dependency);\n2) removing DOCUMENTS and only using PERSONA (-\nDocuments); and 3) removing the planning step by\nalways selecting PERSONA (-Planning∗) or always\nselecting PERSONA and DOCUMENTS (-Planning∗∗)\nfor each turn. It can be found at the bottom of Ta-\nble 7 that all metrics are dropped differently after\nremoving different components except for Rouge-\nL when always selecting two knowledge sources.\nTo conclude, SAFARI can effectively incorporate\nmultiple sources (compared with -Documents) and\nfurther address dependency issues (compared with -\nDependency). Moreover, SAFARI demonstrates its\nversatility by effectively handling multiple sources\nand efficiently selecting relevant ground knowl-\nedge. Notably, SAFARI outperforms existing\nmethods that indiscriminately utilize all available\nsources (compared with -Planning∗∗).\n6.2 Different Numbers of Retrieved Results\nThe number of retrieved results plays a key role\nin the response generation. There is a trade-off\nbetween accuracy and recall, while a small number\nof retrieved results may not cover enough seman-\ntics but a large number may introduce additional\nnoises. Table 8 presents the results of the different\nnumbers of retrieved results. We observe that the\nModel BLEU1 RougeL P.C K.C\nCHATGLM-6B 23.81 26.70 76.99 42.39\n+ Ground Planning 24.29 27.01 86.16 57.12\n+ Ground Retrieval25.86 29.15 79.52 53.95\n+ Ground P & R 25.71 29.43 90.56 72.99\n- Dependency 23.32 25.53 75.67 38.49\n- Documents 23.06 25.34 75.91 36.53\n- Planning∗ 23.51 25.98 72.90 24.89\n- Planning∗∗ 23.69 26.81 71.60 34.91\nTable 7: Ablation study on the impact of different steps\nand modules in SAFARI.\nNumber Assembling\nBLEU1 RougeL P.C K.C\n1 23.81 26.70 76.99 42.39\n2 22.70 25.57 71.03 29.45\n3 20.69 24.05 69.73 27.91\nTable 8: The performance of Assembling of different\nnumber of retrieved results.\nperformance of response generation decreases with\nthe number, which indicates that noisy knowledge\nwill harm the quality of the generated responses.\n6.3 Human Evaluation\nHuman evaluation is conducted to evaluate the qual-\nity of generated response in terms of three met-\nrics: coherence score (Coh.), persona consistency\nscore (Per.Cs), and knowledge consistency score\n(Know.Cs). We randomly sample 100 responses\nwith grounding information for each model and\nask three annotators to indicate its coherence score\n(1-5) and whether the response is consistent with\nthe given persona (1/0), and knowledge (1/0). Ta-\nble 9 shows the result. We observe that supervised\nmethods achieve higher performance than unsu-\npervised ones, which corroborates the findings of\nthe automatic evaluation results presented in Ta-\nble 6. Besides that, we found BELLE achieves the\nhighest performance across all metrics and outper-\nforms ChatGLM since the effects of planning are\nnot considered during human evaluation. More-\nover, we also found that in-context learning brings\na lower rejection rate and more human-like re-\nsponses. Specifically, the rejection rate of BELLE\nunder the setting of zero-shot learning is about\n32%, while the number is reduced to 12% under\nin-context learning16.\n7 Conclusion\nIn this paper, we propose a novel framework SA-\nFARI to incorporate multiple sources of knowledge\n16More analysis can be found in Appendix E\n9563\nModel Coh. Per.Cs (%) Know.Cs (%)\nSupervised Setting\nBELLE-LLAMA-7B-2M 4.38 72.0 63.8\nCHATGLM-6B 4.06 68.0 59.1\nUnsupervised Setting\nZero-shot\nBELLE-LLAMA-7B-2M 2.84 24.7 19.5\nCHATGLM-6B 2.58 17.0 14.8\nCHATGPT 4.00 63.4 33.3\nIn-context\nBELLE-LLAMA-7B-2M 3.36 40.0 21.7\nCHATGLM-6B 2.88 32.0 28.9\nCHATGPT 4.03 54.0 48.8\nTable 9: The results of human evaluation. The inter-\nagreement is about 86%.\nbases and further address the dependency issue be-\ntween them. Unlike previous works, SAFARI can\nbe extended to multiple sources easily and it can\nhandle cases that do not require any sources or re-\nquire some instead of all sources between them. We\nbuild the first personalized knowledge-grounded\ndialogue (KBP) dataset, and experimental results\nprove the effectiveness and robustness of SAFARI.\nLimitations\nIn this paper, we propose the SAFARI framework\nto incorporate multiple sources and address the de-\npendency issue between them by leveraging the\nexceptional capability of LLMs. However, we ac-\nknowledge two major limitations of the paper:\nSAFARI There is an error propagation issue\nwhen decoupling the source selection from the re-\nsponse generation. The cascaded design may prop-\nagate the error in the intermediate step. Once the\nplanning sources are wrong, the retriever can not\nretrieve correct results from the knowledge bases.\nKBP Although the constructed dataset has al-\nready considered three different cases in which\nresponses require 0, 1, and 2 knowledge sources,\nthere are other useful sources in the knowledge-\ngrounded dialogue system such as the users’ mem-\nory source or other sources in different applications.\nBesides that, the world knowledge may become\noutdated as time goes by.\nEthics Statement\nIn this paper, there are two ethical issues about the\nLLMs and dataset respectively.\nUsages of LLMs We strictly follow the license\nand policy of released LLMs, and we do not guar-\nantee the content generated content by LLMs is\nsafe and harmless. We note that LLMs may in-\nherit hallucination issues as shown in the planning\nanalysis, and it will plan not to use corresponding\nsources due to poor performance to express uncer-\ntainty. The calls of the OpenAI API in this paper\nwere carried out by Dr. Yang Deng, one of the cor-\nresponding authors from the National University of\nSingapore.\nHuman Annotation The human inspection and\nannotation were conducted by a reputable data an-\nnotation company, and the annotators are compen-\nsated fairly based on the market price without re-\nvealing any personal information. Besides that,\nour dataset may contain biased opinions due to the\nsubjectivity of manual annotation.\nAcknowledgement\nWe would like to express our heartfelt gratitude\nto all anonymous reviewers for their insightful\ncomments and suggestions. The work described\nhere was partially supported by grants from the\nRGC General Research Funding Scheme (GRF)\n14222922 (CUHK 2151185).\nReferences\nAlfonso Amayuelas, Liangming Pan, Wenhu Chen, and\nWilliam Wang. 2023. Knowledge of knowledge: Ex-\nploring known-unknowns uncertainty with large lan-\nguage models.\nYu Cao, Wei Bi, Meng Fang, Shuming Shi, and Dacheng\nTao. 2022. A model-agnostic data manipulation\nmethod for persona-based dialogue generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 7984–8002, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nLiang Chen, Hongru Wang, Yang Deng, Wai Chung\nKwan, Zezhong Wang, and Kam-Fai Wong. 2023.\nTowards robust personalized dialogue generation via\norder-insensitive representation regularization. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 7337–7345, Toronto,\nCanada. Association for Computational Linguistics.\nYang Deng, Wenqiang Lei, Lizi Liao, and Tat-Seng\nChua. 2023a. Prompting and evaluating large lan-\nguage models for proactive dialogues: Clarifica-\ntion, target-guided, and non-collaboration. CoRR,\nabs/2305.13626.\nYang Deng, Yaliang Li, Wenxuan Zhang, Bolin Ding,\nand Wai Lam. 2022. Toward personalized answer\ngeneration in e-commerce via multi-perspective pref-\nerence modeling. ACM Trans. Inf. Syst., 40(4):87:1–\n87:28.\n9564\nYang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang\nLei, Tat-Seng Chua, and Wai Lam. 2023b. A unified\nmulti-task learning framework for multi-goal con-\nversational recommender systems. ACM Trans. Inf.\nSyst., 41(3):77:1–77:25.\nYang Deng, Wenxuan Zhang, Yifei Yuan, and Wai Lam.\n2023c. Knowledge-enhanced mixed-initiative dia-\nlogue system for emotional support conversations. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, pages 4079–4095.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nTingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong\nWen, and Rui Yan. 2022. There are a thousand\nhamlets in a thousand people’s eyes: Enhancing\nknowledge-grounded dialogue with personal mem-\nory. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 3901–3913, Dublin,\nIreland. Association for Computational Linguistics.\nYunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong,\nHaofen Wang, and Jiawei Zhang. 2023. Chat-rec:\nTowards interactive and explainable llms-augmented\nrecommender system.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.\n2023. Toolkengpt: Augmenting frozen language\nmodels with massive tools via tool embeddings.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020.\nChallenges in building intelligent open-domain di-\nalog systems. ACM Trans. Inf. Syst. , 38(3):21:1–\n21:32.\nYoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh,\nSuhyune Son, Yeonsoo Lee, Donghoon Shin, Se-\nungryong Kim, and Heuiseok Lim. 2022. Call for\ncustomized conversation: Customized conversation\ngrounding persona and knowledge. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 36, pages 10803–10812.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023. Exploring the impact of instruction data scal-\ning on large language models: An empirical study on\nreal-world use cases. CoRR, abs/2303.14742.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nHyunwoo Kim, Byeongchang Kim, and Gunhee Kim.\n2020. Will I sound like me? improving persona\nconsistency in dialogues through pragmatic self-\nconsciousness. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 904–916, Online. Asso-\nciation for Computational Linguistics.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nQian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou,\nZixuan Chen, Bin Zhou, and Dongmei Zhang. 2020.\nYou impress me: Dialogue generation via mutual per-\nsona perception. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1417–1427, Online. Association for\nComputational Linguistics.\nYifan Liu, Wei Wei, Jiayi Liu, Xianling Mao, Rui Fang,\nand Dangyang Chen. 2022a. Improving personality\nconsistency in conversation by persona extending.\nIn Proceedings of the 31st ACM International Con-\nference on Information & Knowledge Management,\npages 1350–1359.\nYifan Liu, Wei Wei, Jiayi Liu, Xianling Mao, Rui Fang,\nand Dangyang Chen. 2022b. Improving personality\nconsistency in conversation by persona extending.\nIn Proceedings of the 31st ACM International Con-\nference on Information & Knowledge Management.\nACM.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and\nPascale Fung. 2019. Personalizing dialogue agents\nvia meta-learning. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5454–5459, Florence, Italy. Asso-\nciation for Computational Linguistics.\nBodhisattwa Prasad Majumder, Harsh Jhamtani, Tay-\nlor Berg-Kirkpatrick, and Julian McAuley. 2020.\nLike hiking? you probably enjoy nature: Persona-\ngrounded dialog with commonsense expansions. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9194–9206, Online. Association for Computa-\ntional Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\n9565\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai,\nMeredith Ringel Morris, Percy Liang, and Michael S.\nBernstein. 2023. Generative agents: Interactive sim-\nulacra of human behavior.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A joint training method\nfor dense passage retrieval and passage re-ranking.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2825–2835, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nStephen Robertson and Hugo Zaragoza. 2009. The prob-\nabilistic relevance framework: Bm25 and beyond.\nFoundations and Trends® in Information Retrieval,\n3(4):333–389.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nHaoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan\nZhang, and Ting Liu. 2021. BoB: BERT over BERT\nfor training persona-based dialogue models from lim-\nited personalized data. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 167–177, Online. Association\nfor Computational Linguistics.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2023. Recitation-augmented language\nmodels.\nHongru Wang, Mingyu Cui, Zimo Zhou, and Kam-\nFai Wong. 2022. TopicRefine: Joint topic predic-\ntion and dialogue response generation for multi-turn\nend-to-end dialogue system. In Proceedings of the\n5th International Conference on Natural Language\nand Speech Processing (ICNLSP 2022), pages 19–29,\nTrento, Italy. Association for Computational Linguis-\ntics.\nHongru Wang, Huimin Wang, Lingzhi Wang, Minda\nHu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi,\nand Kam-Fai Wong. 2023a. Tpe: Towards better\ncompositional reasoning over conceptual tools with\nmulti-persona collaboration.\nHongru Wang, Rui Wang, Fei Mi, Zezhong Wang,\nRuifeng Xu, and Kam-Fai Wong. 2023b. Chain-\nof-thought prompting for responding to in-depth dia-\nlogue questions with llm.\nRui Wang, Jianzhu Bao, Fei Mi, Yi Chen, Hongru Wang,\nYasheng Wang, Yitong Li, Lifeng Shang, Kam-Fai\nWong, and Ruifeng Xu. 2023c. Retrieval-free knowl-\nedge injection through multi-document traversal for\ndialogue models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6608–6619,\nToronto, Canada. Association for Computational Lin-\nguistics.\nCharles Welch, Chenxi Gu, Jonathan K. Kummerfeld,\nVeronica Perez-Rosas, and Rada Mihalcea. 2022.\nLeveraging similar users for personalized language\nmodeling with limited data. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1742–1752, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nSean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. 2019. Dialogue natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3731–3741, Florence, Italy. Association for\nComputational Linguistics.\nSixing Wu, Ying Li, Minghui Wang, Dawei Zhang,\nYang Zhou, and Zhonghai Wu. 2021. More is bet-\nter: Enhancing open-domain dialogue generation via\nmulti-source heterogeneous knowledge. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 2286–2300,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nSixing Wu, Ying Li, Ping Xue, Dawei Zhang, and\nZhonghai Wu. 2022a. Section-aware commonsense\nknowledge-grounded dialogue generation with pre-\ntrained language model. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics, pages 521–531, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguis-\ntics.\nSixing Wu, Ying Li, Dawei Zhang, and Zhonghai Wu.\n2022b. KSAM: Infusing multi-source knowledge\ninto dialogue generation via knowledge source aware\nmulti-head decoding. In Findings of the Association\nfor Computational Linguistics: ACL 2022, pages 353–\n363, Dublin, Ireland. Association for Computational\nLinguistics.\nChen Xu, Piji Li, Wei Wang, Haoran Yang, Siyun Wang,\nand Chuangbai Xiao. 2022a. COSPLAY. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval. ACM.\n9566\nJing Xu, Arthur Szlam, and Jason Weston. 2022b. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5180–5197, Dublin,\nIreland. Association for Computational Linguistics.\nXinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu,\nHua Wu, Haifeng Wang, and Shihang Wang. 2022c.\nLong time no see! open-domain conversation with\nlong-term persona memory. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2639–2650, Dublin, Ireland. Association for\nComputational Linguistics.\nBoyang Xue, Weichao Wang, Hongru Wang, Fei Mi,\nRui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang,\nQun Liu, and Kam-Fai Wong. 2023. Improving fac-\ntual consistency for knowledge-grounded dialogue\nsystems via knowledge enhancement and alignment.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels.\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022. A\nsurvey of knowledge-enhanced text generation. ACM\nComput. Surv., 54(11s):227:1–227:38.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning\nRepresentations (ICLR).\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204–2213,\nMelbourne, Australia. Association for Computational\nLinguistics.\nChujie Zheng, Jinfeng Zhou, Yinhe Zheng, Libiao Peng,\nZhen Guo, Wenquan Wu, Zheng-Yu Niu, Hua Wu,\nand Minlie Huang. 2022. CDConv: A benchmark\nfor contradiction detection in Chinese conversations.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 18–\n29, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nHao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang,\nand Xiaoyan Zhu. 2020. KdConv: A Chinese\nmulti-domain dialogue dataset towards multi-turn\nknowledge-driven conversation. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 7098–7108, Online. As-\nsociation for Computational Linguistics.\nQi Zhu, Fei Mi, Zheng Zhang, Yasheng Wang, Yitong\nLi, Xin Jiang, Qun Liu, Xiaoyan Zhu, and Minlie\nHuang. 2023. KPT: keyword-guided pre-training for\ngrounded dialog generation. In Thirty-Seventh AAAI\nConference on Artificial Intelligence, AAAI 2023,\nThirty-Fifth Conference on Innovative Applications\nof Artificial Intelligence, IAAI 2023, Thirteenth Sym-\nposium on Educational Advances in Artificial Intelli-\ngence, EAAI 2023, Washington, DC, USA, February\n7-14, 2023, pages 14065–14073. AAAI Press.\n9567\nA Dialogues Requiring Knowledge\nDependency\nTable 10 shows another example of knowledge de-\npendency. In this example, without considering the\npersona-knowledge reliance, the entity (proverb\n瓦合), which is irrelevant to the band 草东没有\n派对, is retrieved. Although the retrieved knowl-\nedge is unrelated to the ground persona, ChatGPT\nstill tries to combine these two disconnected pieces\nof information and generate an inconsistent re-\nsponse (Response w/o. Dependency).\nGround Persona: \"I really like 草东没有派对 (No\nParty for Cao Dong)\"\nRetrieved Knowledge w/o. dependency: \"瓦合 (The\nClod) is a Chinese proverb that metaphorically represents\na motley crowd.\"\nGround Knowledge w. dependency: \"瓦合 is a new\nalbum from the band 草东没有派对\"\n......\nUser: Do you want to listen to 瓦合(The Clod)?\nResponse w/o. Dependency: I absolutely love 草东\n没有派对! 瓦合 is a proverb that describes a motley\ncrowd, representing a group of people without organiza-\ntion or coordination. So regarding 瓦合, I can say that\nit represents a group of individuals lacking organization\nor coordination. Do you have any other questions I can\nassist you with?\nResponse w. Dependency: You bet! I absolutely love\n草东没有派对! 瓦合is their newly released album, and\nI’m really looking forward to listening to the songs on it.\nWould you like to share with me which song you enjoy\nthe most in 瓦合.\n......\nTable 10: An example dialogue with ChatGPT showing\npersona-knowledge dependency (translated from Chi-\nnese to English)\nB Data Preparation\nSeed Preparation In the original persona dataset,\nDuLeMon, most of the persona does not contain\nmuch knowledge. For example, about 30% to 40%\nof persona is about the name such as \"my name\nis XXX\". After clustering, we removed topics that\ncontain less than 20 personas. There are 5 topics\nleft: career, book, music, movie, personality. Then\nwe add some basic persona such as gender, age,\nlocality, nation, sport, and others.\nPersona and Knowledge Matching For each per-\nsona description after segmentation, we first re-\nmove stop-words 17 and frequently used words.\n17https://github.com/goto456/stopwords/blob/\nmaster/cn_stopwords.txt\nAttributes Templates\nnick_name, birth_place, ... {head entity}’s {attribute} is {tail entity}rewards, members, ... The {attribute} of {head entity} contains {tail entity}\nTable 11: Two major templates to convert triples into\nnatural language descriptions (translated from Chinese\nto English).\nAnd then we filter duplicated words to reduce un-\nnecessary computation. For the left words, we map\nthem to the corresponding knowledge base one by\none. We discarded some useless attributes such as\npenmanship and pronunciation, and then translate\nthe matched triples to natural language sentences\naccording to pre-defined templates as shown in\nTable 11. If we do not find any matched triples,\nwe just discard the persona. At last, we manually\nchecked each persona description to make sure (1)\nthere is no repetitive knowledge statement, and (2)\neach persona contains at least 5 knowledge state-\nments 18.\nC Implementation Details\nWe first illustrate the details of finetuning and then\nintroduce our definition of NLI metrics: P.C and\nK.C.\nRetrieval Models. We finetune RocketQAv2 and\nDPR using our own KBP dataset by regarding\n(context, used_persona / document) as the positive\nand (context, unrelated_persona / document) as the\nnegative. We set epochs as 5 and max sequence\nlength as 512, and mainly follow the scripts:\nhttps://github.com/PaddlePaddle/RocketQA\nand https://github.com/Alibaba-NLP/\nMulti-CPR/tree/main/retrieval respectively\nfor other parameters. For RocketQAv2, we load the\nweights of pre-trained model zh_dureader_de_v2\nas introduced in the official homepage, which is\ntrained on the largest Chinese QA dataset, and\nwe use 12-layer bert-base-chinese with 110M\nparameters as backbone model for DPR.\nNLI Models. Following previous work (Kim\net al., 2020; Cao et al., 2022), we finetune an\nNLI model (Welleck et al., 2019) using our own\ndataset by regarding (ground_persona / document,\nresponse) as the positive and randomly sampled\n(unrelated_persona / document, response) as the\nnegative. We also use bert-base-chinese as the\nbackbone model. We concatenate and encode the\nground persona/document k and response r in the\nform of [CLS]k[SEP]r[SEP], and we train the\n18We will manually add it if there are less than 5 knowledge\nstatements.\n9568\nC(r, g) =\n\n\n\nNLI(r, g), if there is grounding g for r, and planning also uses g′.\n0, if there is grounding g for r, and planning does not use g′.\n0, if there is no grounding g for r, and planning uses g′.\n1, if there is no grounding g for r, and planning does not use g′.\n(4)\nmodel to predict whether responses are consistent\nwith corresponding personas or documents. The\nbatch size for fine-tuning is 8. The maximal train-\ning epoch is 5, and the max sequence length of\nthe encoder is 512. In the experiments, we use\nthe AdamW optimizer with a learning rate of 2e-\n5 and an epsilon of 1e-6. We evaluate the NLI\nmodel on the KBP test set every 500 iterations dur-\ning training, and we save the checkpoint with the\nhighest performance on the KBP test set. The fine-\ntuned model achieves > 95% accuracy for both\npersona (95.94%) and knowledge (96.95%) on the\nKBP test dataset. We then calculate the persona\nconsistency (P.C) and knowledge consistency (K.C)\naccording to the output of the NLI model. Com-\npared with many previous works (Madotto et al.,\n2019; Cao et al., 2022) that calculate the NLI score\nonly when ground-truth knowledge includes per-\nsonas or documents, our framework is more sophis-\nticated and introduces the planning step, which con-\nsiders the situation where responses do not require\nany ground knowledge. Thus, if we only calculate\nthe consistency score in the occasion where there\nexists ground truth personas/documents, it is un-\nfair and inaccurate for our framework, since wrong\nground knowledge could also hurt the quality of\nresponses and the system is not penalized when it\ngives wrong planning (e.g. always calling external\nsources). We design a new calibrated metric for\ndialogue consistency as described in Eq 4,5,6,7:\nNLI(r, g) =\n{\n1, if r entails g\n0, if r does not entail g (5)\nP.C =\n∑m\ni C(ri, pi)\nm (6)\nK.C =\n∑m\ni C(ri, ki)\nm (7)\nThere are four cases during experiments as\nshown in Eq 4: 1). there exists ground-truth ground-\ning g for the response r in the test set and the plan-\nning also decides to retrieve ground knowledge\ng′from corresponding sources (either PERSONA or\nDOCUMENTS); 2). there is g for r, but the planning\ndecides not to use g′; 3). there is no grounding g\nfor r, while the planning decides to useg′; 4). there\nexists no g, and the planning step decides not to\nuse g′either. We only calculate the NLI score of\n(g, r) using finetuned NLI model in the first case of\nEq 4. With this definition, we can get the calibrated\nscore P.C and K.C for the KBP test set.\nD Prompts Templates under\nUnsupervised Setting\nTable 12 demonstrates the zero-shot prompt of the\nSAFARI planning step on the KBP dataset, and\nthe zero-shot prompt for the assembling step is the\nsame as Table 3.\nThere are two knowledge bases storing relevant informa-\ntion:\nPERSONA: This knowledge base stores information re-\nlated to system personas, such as gender, age, place of\norigin, hobbies, personality traits, and other relevant data.\nDOCUMENTS: This knowledge base stores domain-\nspecific knowledge related to system personas, such as\nthe domain knowledge about the place of origin.\nThere exists a dependency between these knowledge bases.\nThe invocation of DOCUMENTS relies on the results from\nPERSONA. Please ensure the correct order of invoking\nthem.\nHere is the dialogue between the user and the system: {di-\nalogue_history}\nBased on the user’s last question, please determine if it\nrequires invoking the corresponding knowledge base. If the\ninvocation is necessary, output the names of the knowledge\nbases in the order they should be invoked. If no invocation\nis needed, output NULL.\nTable 12: The prompts of unsupervised SAFARI on\nKBP dataset (translated from Chinese to English)\nE Human Evaluation\nWe find that human is more likely to find persona-\ninconsistent cases. There are some responses that\nhave intra-sentence contradictions (Zheng et al.,\n2022), for example, “ My zodiac signs are Aries\nand Taurus”. In addition, there are other responses\nrelated to the persona description that are incon-\nsistent. Both these types of responses are easy to\nidentify by humans but hard for the NLI model\nto detect, resulting in lower Per.Cs during human\nevaluation.\n9569",
  "topic": "Persona",
  "concepts": [
    {
      "name": "Persona",
      "score": 0.7906744480133057
    },
    {
      "name": "Computer science",
      "score": 0.7401211261749268
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6446928977966309
    },
    {
      "name": "Construct (python library)",
      "score": 0.6024889945983887
    },
    {
      "name": "Domain knowledge",
      "score": 0.517853319644928
    },
    {
      "name": "Natural language processing",
      "score": 0.5116172432899475
    },
    {
      "name": "Knowledge base",
      "score": 0.4931771159172058
    },
    {
      "name": "Grounded theory",
      "score": 0.4847854971885681
    },
    {
      "name": "Focus (optics)",
      "score": 0.4528045952320099
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4513106942176819
    },
    {
      "name": "Planner",
      "score": 0.4328393340110779
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4283286929130554
    },
    {
      "name": "Knowledge-based systems",
      "score": 0.4258239269256592
    },
    {
      "name": "Knowledge management",
      "score": 0.39329013228416443
    },
    {
      "name": "Data science",
      "score": 0.3928016722202301
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3807200491428375
    },
    {
      "name": "Qualitative research",
      "score": 0.10580199956893921
    },
    {
      "name": "Programming language",
      "score": 0.08550083637237549
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    }
  ]
}