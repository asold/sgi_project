{
    "title": "Multilingual Question Answering for Malaysia History with Transformer-based Language Model",
    "url": "https://openalex.org/W4395005770",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5092965440",
            "name": "Qi Zhi Lim",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A5086127398",
            "name": "Chin Poo Lee",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A5017532046",
            "name": "Kian Ming Lim",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A5104175677",
            "name": "Jing Xiang Ng",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A5092965441",
            "name": "Eric Khang Heng Ooi",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A5108866929",
            "name": "Nicole Kai Ning Loh",
            "affiliations": [
                "Multimedia University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2197667787",
        "https://openalex.org/W1604291600",
        "https://openalex.org/W2949021450",
        "https://openalex.org/W2793292424",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4316660378",
        "https://openalex.org/W4319432032",
        "https://openalex.org/W4312226881",
        "https://openalex.org/W4385714618",
        "https://openalex.org/W4387951109",
        "https://openalex.org/W3174995573",
        "https://openalex.org/W4309639857",
        "https://openalex.org/W4285814425",
        "https://openalex.org/W4381887815",
        "https://openalex.org/W4309236643",
        "https://openalex.org/W3011920570",
        "https://openalex.org/W3109435212",
        "https://openalex.org/W1519301183",
        "https://openalex.org/W3186157409",
        "https://openalex.org/W6781533629",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "In natural language processing (NLP), a Question Answering System (QAS) refers to a system or model that is designed to understand and respond to user queries in natural language. As we navigate through the recent advancements in QAS, it can be observed that there is a paradigm shift of the methods used from traditional machine learning and deep learning approaches towards transformer-based language models. While significant progress has been made, the utilization of these models for historical QAS and the development of QAS for Malay language remain largely unexplored. This research aims to bridge the gaps, focusing on developing a Multilingual QAS for history of Malaysia by utilizing a transformer-based language model. The system development process encompasses various stages, including data collection, knowledge representation, data loading and pre-processing, document indexing and storing, and the establishment of a querying pipeline with the retriever and reader. A dataset with a collection of 100 articles, including web blogs related to the history of Malaysia, has been constructed, serving as the knowledge base for the proposed QAS. A significant aspect of this research is the use of the translated dataset in English instead of the raw dataset in Malay. This decision was made to leverage the effectiveness of well-established retriever and reader models that were trained on English data. Moreover, an evaluation dataset comprising 100 question-answer pairs has been created to evaluate the performance of the models. A comparative analysis of six different transformer-based language models, namely DeBERTaV3, BERT, ALBERT, ELECTRA, MiniLM, and RoBERTa, has been conducted, where the effectiveness of the models was examined through a series of experiments to determine the best reader model for the proposed QAS. The experimental results reveal that the proposed QAS achieved the best performance when employing RoBERTa as the reader model. Finally, the proposed QAS was deployed on Discord and equipped with multilingual support through the incorporation of language detection and translation modules, enabling it to handle queries in both Malay and English. Doi: 10.28991/ESJ-2024-08-02-019 Full Text: PDF",
    "full_text": " Available online at www.ijournalse.org \nEmerging Science Journal \n(ISSN: 2610-9182) \nVol. 8, No. 2, April, 2024 \n \n \nPage | 675 \n \nMultilingual Question Answering for Malaysia History with \nTransformer-based Language Model \n \nQi Zhi Lim 1, Chin Poo Lee 1* , Kian Ming Lim 1 , Jing Xiang Ng 1,                            \nEric Khang Heng Ooi 1, Nicole Kai Ning Loh 1 \n1 Faculty of Information Science and Technology, Multimedia University, Melaka, 75450 Malaysia. \n \n \nAbstract \nIn natural language processing (NLP), a Question Answering System (QAS) refers to a system or \nmodel that is designed to understand and respond to user queries in natural language. As we navigate \nthrough the recent advancements in QAS, it can be observed th at there is a paradigm shift of the \nmethods used from traditional machine learning and deep learning approaches towards transformer-\nbased language models. While significant progress has been made, the utilization of these models \nfor historical QAS and the development of QAS for Malay language remain largely unexplored. \nThis research aims to bridge the gaps, focusing on developing a Multilingual QAS for history of \nMalaysia by utilizing a transformer -based language model. The system development process \nencompasses various stages, including data collection, knowledge representation, data loading and \npre-processing, document indexing and storing, and the establishment of a querying pipeline with \nthe retriever and reader. A dataset with a collection of 100 articl es, including web blogs related to \nthe history of Malaysia, has been constructed, serving as the knowledge base for the proposed QAS. \nA significant aspect of this research is the use of the translated dataset in English instead of the raw \ndataset in Malay. This decision was made to leverage the effectiveness of well-established retriever \nand reader models that were trained on English data. Moreover, an evaluation dataset comprising \n100 question -answer pairs has been created to evaluate the performance of th e models. A \ncomparative analysis of six different transformer -based language models, namely DeBERTaV3, \nBERT, ALBERT, ELECTRA, MiniLM, and RoBERTa, has been conducted, where the \neffectiveness of the models was examined through a series of experiments to det ermine the best \nreader model for the proposed QAS. The experimental results reveal that the proposed QAS achieved \nthe best performance when employing RoBERTa as the reader model. Finally, the proposed QAS \nwas deployed on Discord and equipped with multiling ual support through the incorporation of \nlanguage detection and translation modules, enabling it to handle queries in both Malay and English. \nKeywords:  \nQuestion Answering; \nHistorical Knowledge; \nNatural Language Processing; \nDeBERTaV3; \nBERT; \nALBERT; \nELECTRA; \nMiniLM; \nRoBERTa. \n \n \n \n \nArticle History: \nReceived: 23 November 2023 \nRevised: 24 February 2024 \nAccepted: 08 March 2024 \nPublished: 01 April 2024  \n \n \n1- Introduction \nHistory is referred to as the study and interpretation of past events, human activities, and societies. It encompasses \nthe documentation, analysis, and understanding of the chronological sequence of events, causal relationships, and the \nevolution of cultures, civilizations, and institutions over time. The study of history serves multiple purposes. It provides \ninsights into the achievements, struggles, and aspirations of past societies and offers valuable lessons and perspectives \nfor contemporary societies. History allows us to understand the factors that have influenced the development of social, \npolitical, economic, and cultural systems. It sheds light on the interconnections and interdependencies between different \nregions and civilizations, helping people to recognize the shared heritage and global influences that have shaped this \n                                                           \n* CONTACT: cplee@mmu.edu.my \nDOI: http://dx.doi.org/10.28991/ESJ-2024-08-02-019 \n¬© 2024 by the authors. Licensee ESJ, Italy. This is an open access article under the terms and conditions of the Creative \nCommons Attribution (CC-BY) license (https://creativecommons.org/licenses/by/4.0/). \n\nEmerging Science Journal | Vol. 8, No. 2 \nPage | 676 \ninterconnected world. Additionally, history plays a crucial role in shaping collective memory and identity. It helps \nindividuals and communities understand their roots, heritage, and cultural identity. By examining the past, a deeper \nunderstanding of origins can be acquired, the diversity of human experiences can be appreciated, and a sense of empathy \nand tolerance for different perspectives can be fostered. Unfortunately, from the  surveys shown, students have low \nachievement in the history subject [1]. When students want to understand more about historical knowledge, various \nsources of data will be shown to them. It might cause confusion for the students, as some information might be wrong \nor appropriate [1]. Therefore, there is a need to create a question-answering system tailored to historical knowledge. \nQuestion Answering System (QAS) is an application or technology that targets providing answers to user queries or \nquestions in a human-like manner. It leverages the power of natural language processing (NLP) and information retrieval \ntechniques to understand the meaning and intent of user questions. Later, the relevant information will be retrieved from \na given knowledge base. Gene rally, a QAS is developed by following a three -step process: question understanding, \ninformation retrieval, and answer generation. For question understanding, the system will analyze the query to \ncomprehend its meaning, intent, and context. This involves p arsing the question, identifying keywords, extracting \nrelevant information, and determining the type of answer expected. For information retrieval, the system will search for \nthe relevant information within a knowledge base or a collection of documents. Va rious techniques, such as keyword \nmatching, semantic search, and advanced information retrieval models, are employed to locate and rank relevant \npassages or documents. For answer generation, the system will process and synthesize the most appropriate answe r to \nthe user's question based on the retrieved information. This could involve extracting specific sentences or passages, \nsummarizing relevant information, or generating a concise response using natural language generation techniques.  \nBased on the review of existing literature, it is evident that the methodology of QAS has evolved from traditional \nmachine learning and deep learning approaches to predominantly utilizing transformer -based language models. This \nparadigm shift underscores the increasing signif icance and effectiveness of transformer architectures in enhancing the \ncapabilities and performance of QAS. Despite the notable progress made in the field of QAS, there is still a noticeable \nresearch gap in which the ability of the models to address histor ical questions remains largely unexplored. Therefore, \nthis research aims to bridge the gap by exploring the use of transformer-based language models for historical QAS. From \nanother perspective, it can be found that only a limited number of existing works have explored the application of QAS \nin the Malay language. Hence, this research also emphasizes the development of a multilingual QAS with support for \nboth Malay and English. More specifically, this study aims to develop a multilingual QAS specifically ta ilored to \nhistorical knowledge, with a particular focus narrowed down to the history of Malaysia.  \nThe system development process involves multiple stages, including data collection, knowledge representation, data \nloading and pre -processing, document index ing and storing, and the establishment of the querying pipeline with the \nretriever and reader. In this study, a dataset comprising 100 articles, including web blogs related to the history of \nMalaysia, has been constructed, serving as the knowledge base for the proposed QAS. It is worth noting that the original \ndataset in Malay was translated into English to leverage the effectiveness of well-established retriever and reader models \ntrained on English data. Additionally, an evaluation dataset consisting of 100 question-answer pairs has been created for \nthe evaluation of the model's performance. This study explored six different transformer-based language models, namely \nDeBERTaV3, BERT, ALBERT, ELECTRA, MiniLM, and RoBERTa. A series of experiments have been con ducted to \ndetermine the best reader model for the question-answering process. While the proposed QAS utilizes an English-based \nmodel and operates on an English dataset, it can provide answers in Malay for user presentation by incorporating \nlanguage detection and translation modules. Finally, the scope of this research extends to the deployment of the proposed \nQAS on Discord. \nThe main contributions of this study are: \n‚Ä¢ Proposed a novel Multilingual QAS for Malaysian History by utilizing a transformer -based language model. \n‚Ä¢ Conducted a comparative analysis of six different transformer -based language models through extensive \nexperiments using the self-collected evaluation dataset. \n‚Ä¢ Deployed the proposed QAS on Discord with multilingual support (Malay and English) through the incorporation \nof language detection and translation modules. \nThe rest of the paper is organized as follows: Section 2 presents a comprehensive literature review of QAS. Section \n3 explains the methodology used to develop the proposed QAS in depth. Section 4 covers the experiment details and the \nexperimental results. Section 5 describes the deployment of the proposed QAS. Finally, the conclusion and future work \nfor this research are included in Section 6. \n2- Literature Review \nEarlier QAS, such as LUNAR [2], were merely a natural language front -end for structured database query systems. \nThese systems utilized natural language processing (NLP) techniques to analyze questions posed by users and transform \nthem into a canonical form . This canonical form was then used to construct a standard database query that could be \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 677 \nunderstood by the underlying database system. Androutsopoulos et al. [3] introduced the Natural Language Interface to \nDatabases (NLITB), which enables users to query databases using natural language instead of formal query language. It \nuses a domain-specific semantic grammar that maps natural language phrases to the corresponding database queries. The \ngrammar used consists of production rules that define the syntactic a nd semantic structures of natural language \nexpressions. Other than that, it also deploys conceptual graphs to represent the meaning of queries and database contents. \nCompared to LUNAR, NLTIB is more user-friendly as it lessens the use of formal query languages. \nAs time goes on, QAS has grown with the purpose of detecting intended question requirements in their natural form \nthrough the linguistic analysis of proposed questions. MASQUE [4] is a QAS that focuses on the linguistic analysis of \nthe question asked. It represents queries in a logical form, typically using formal logic or logical representation language, \nto capture the intended meaning and requirements of the question. Once the question is represented in logical form, \nMASQUE then converts it into a d atabase query for information retrieval. Zheng [5] presented an open -domain QAS, \nAnswerBus, based on sentence -level web information retrieval. In this research work, the system accepts queries from \nusers using six languages, which are English, German, Fren ch, Spanish, Italian, and Portuguese, and provides answers \nto users using English. To answer the users‚Äô questions, five search engines and directories have been utilized to retrieve \nthe relevant Web pages and extract sentences that are determined to contain answers. \nWhile navigating through the latest advancements in QAS, it is evident that the development of QAS has been \nrevolutionized by transformer-based language models. With the introduction of transformer architecture with attention \nmechanisms [6], researchers have explored novel approaches to QAS based on transformers. Ou et al. [7] proposed an \nautomatic multimedia-based question-answer pair generation in the computer -assisted healthy education system using \nMandarin. The system proposed is divided into  three parts: the text generation module, the answer extraction module, \nand the BART -based question generation module. In this research, manually labeled question and answer pairs have \nbeen improved for the subsequent use of retrieval based QAS. From anoth er perspective, Zhang [8] studied the \napplication of similarity algorithms in designing an intelligent English QAS. In this study, the WordNet semantic \ndictionary is employed to assess the semantic information of a sentence to identify the longest word mat ched, then \ndetermine answers for the questions using the WordNet sentence similarity algorithm.  \nDas and Nirmala [9] enhanced healthcare QAS by adopting the BioBERT framework to identify suitable answers for \nthe questions. The proposed healthcare QAS can be utilized for question generation and task-specific data that are related \nto the healthcare domain. Likewise, Gupta [10] studied the application of QAS in the biomedical domain and conducted \na comparative analysis of various pretrained language models. Ma ximum Inner Product Search (MIPS) was utilized in \nthe research to retrieve the top 10 passages for question answering. Pudasaini and Shakya [11] introduced a question -\nanswering dataset for the biomedical domain and adopted transfer learning on the pretrained large language models. The \nresults demonstrated the importance of domain -specific finetuning for the application of automated tasks in the \nbiomedical field. Furthermore, Alzubi et al. [12] proposed COBERT, a dual algorithmic retriever -reader system for \nanswering complex queries related to the Corona virus. The retriever employs the TF -IDF vectorizer while the reader \nutilizes BERT transformers, and the proposed DistilBERT model showcased outstanding performance across other pre -\ntrained models in the specific question-answering task. \nArcharya et al. [13] proposed a simple and smart QAS using Named Entity Recognition (NER) and BERT. In this \nwork, NER is utilized to extract predefined keywords from the context, representing the most important part -of-speech \nof the data source. Following that, the BERT model is used to predict the answer to a question based on the predefined \ndata. Similarly, Yin [14] researched QAS based on the BERT framework. BERT embeddings and a hierarchical attention \nmodel that comprises co -attention and self -attention mechanisms are employed to identify the consecutive paragraph \nrange and generate answers to given questions.  Yang et al. [15] proposed a knowledge graph question -answering \n(KGQA) model for bridge inspection. The proposed metho d enhanced the contextual representation through the \ncombination of BERT and static domain dictionaries and resolved the problem of semantic matching by implementing \nthe hierarchical cross-attention network. Moreover, Tian et al. [16] introduced an intelligent question-answering system \nfor safety hazard knowledge based on deep semantic mining. In this study, BERT, Bidirectional Gated Recurrent Unit \n(BiGRU), and self -attention mechanisms have been integrated for effective feature extraction, and a Siamese ne ural \nnetwork is implemented for answer selection. \nLiu & Huang [17] developed a Chinese QAS based on GPT. In this research work, the sentences are not divided into \nwords, but the whole sentence is utilized as input. This paper has contributed by replacing  the language model module \nso that the Chinese context can be completely utilized by the Transformer. The researchers suggested that the QAS can \nbe further enhanced by adding clustering to divide the problem and answer into different categories, which can boost the \nprecision of the system and reduce computation complexity. Noraset et al. [18] proposed a novel Thai QAS, WabiQA, \nwhich was implemented using a BM25F-based document retriever and a bi-directional LSTM document reader. WabiQA \nutilized articles from Wikipedia as a knowledge source to perform question -answering in Thai language. The findings \nof this research work have proved that reading a small but relevant piece of text will benefit the overall question -\nanswering performance. \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 678 \nAlthough numerous rese arch studies have been conducted on QAS, there are only a limited number of works that \nexplore the application of QAS in Malay language. Ainon et al. [19] proposed a Malay QAS named SIGMA, which \ncomprises three main components: Parser, Analyzer, and Respon se Generator. The Parser transforms Bahasa Malaysia \nqueries into semantic representations, while the Response Generator searches a domain-specific database for appropriate \nresponses. These components utilize semantic grammar, transition trees, and ellipsis  handling to process input requests \nefficiently and generate meaningful replies. Puteh et al. [20] explored Malay QAS on the Quran and emphasized the \nimportance of question classification. The research presents a machine learning -based answer type classification model \nto identify the answer type, thereby assisting the QAS in retrieving the correct answers to the questions. Furthermore, \nLim et al. [21] introduced an enhanced framework that combines translation models and convolutional neural networks \n(CNN) for effective question classification, improving the performance of Malay -English mixed-language QAS. \n3- Methods \nThe development process of the proposed Question Answering System (QAS) in this study involves multiple key \nstages. The initial stage of the entire  process is the data collection from the Internet and knowledge representation in \nseparate text files. Next, the proposed QAS employs an indexing pipeline with a file converter and a preprocessor to \nload and pre-process the knowledge data. The documents are then indexed and stored into a Document Store, serving as \nthe knowledge base for question -answering. Additionally, a querying pipeline is implemented to retrieve answers for \nuser queries. Specifically, the querying pipeline consists of a retriever that r etrieves relevant documents based on the \nquery and a reader that extracts the most appropriate answers to the query.  \nAs the main objective of this study is to build a QAS for historical knowledge, the impact of adopting different \ntransformer-based languag e models as the reader model on question -answering performance has been investigated. \nSpecifically, a total of six models are selected to be examined: DeBERTaV3, BERT, ALBERT, ELECTRA, MiniLM, \nand RoBERTa. It is worth noting that the implementation of the proposed system is done by utilizing the open -source \nHaystack framework [22]. Figure 1 depicts the workflow of the proposed QAS, highlighting the research methodology \nof this study. In the following subsections, each stage in developing the proposed QAS wi ll be explained in detail to \nfacilitate a comprehensive understanding. \n \nFigure 1. Question Answering System (QAS) Workflow \n3-1- Data Collection and Knowledge Representation \nIn order to develop a QAS specifically tailored to historical knowledge, it is crucial to construct a dataset with diverse \nand representative historical content. This study primarily focuses on the history of Malaysia, aiming to serve as a proof \nof concept to demonstrate the effectiveness of the transformer-based language models in answering historical questions. \nAs a result, a dataset comprising 100 articles, including web blogs that are written in Malay and related to the history of \nMalaysia, was collected from the Internet. During this phase, elements such as headers, footers, and references within \nthe articles are removed to eliminate noise and streamline the content. The removal process was conducted manually by \nhuman annotators, ensuring the utmost accuracy in re taining only the relevant and informative content for further \nanalysis and utilization. The contents of all the collected documents are then stored in separate text files, where each text \nfile represents a single document. \nUpon completing this stage, a collection of text files with contents related to Malaysia‚Äôs history in Malay is produced. \nHowever, since the transformer-based language models selected for this study are extensively trained using English data, \nthey specifically learn the inner representation of the English language that can be utilized in various downstream tasks. \nHence, all the Malay text files are translated into English to create an English-version dataset to be used in the subsequent \nstages. This decision was made in order to align the l anguage of the dataset with the language of the selected models, \nensuring that the results accurately reflect the question -answering performance without being affected by the language \nconstraints. In this study, the translator module utilized is the small variant of the BigBird transformer model [23], which \nis available through the Malaya library. \n\nEmerging Science Journal | Vol. 8, No. 2 \nPage | 679 \n3-2- Indexing Pipeline \nThe indexing pipeline of the proposed QAS consists of two main components, which are a text converter and a \npreprocessor. With the knowledge rep resentation in text files, the text converter in the indexing pipeline functions to \nload and convert the text files into Haystack document objects. Subsequently, the preprocessor in the indexing pipeline \nwill pre-process the document objects in the system.  The preprocessor follows several steps, starting with normalizing \nthree or more consecutive empty lines to just two empty lines. Then, it removes leading and trailing whitespace from \neach line in the text. Additionally, it splits the documents into smalle r segments, each containing a maximum of 200 \nwords. This segmentation is crucial as it enhances the reader's scanning and extraction speed when processing the \nretrieved text and identifying the top answer candidates. In this work, the preprocessor is confi gured to split the \ndocuments in such a way that two adjacent documents overlap by 20 words, ensuring that the document boundaries do \nnot fall in the middle of sentences.  \nBy completing the pre-processing steps, a database, which is known as DocumentStore i n Haystack, is initialized to \nindex and store all the pre-processed document objects. In this particular work, the InMemoryDocumentStore is utilized, \nwhich is a simple document store that requires no external setup. \n3-3- Querying Pipeline \nUpon the completion of the indexing pipeline, the next stage in the proposed QAS is the querying pipeline. The \nquerying pipeline is responsible for understanding and interpreting input queries, then retrieving the most relevant \ninformation from the indexed  document store. The querying pipeline consists of two main components, which are the \nretriever and the reader. By utilizing the modules supported by the Haystack framework, the retriever and reader selected \nfor the proposed QAS are the BM25 Retriever and FARM Reader.  \nBest Match 25 (BM25) Retriever is an information retrieval model that is implanted based on the BM25 ranking \nalgorithm. The BM25 is a variation of Term Frequency -Inverse Document Frequency (TF-IDF), which outperforms its \npredecessor in two ke y aspects. The BM25 algorithm considers the term frequency, document length, and average \ndocument length in the collection to compute the relevance score of a document for a given query. On top of that, the \nranking process of the BM25 is fine -tuned by inco rporating factors such as term saturation and document length \nnormalization. The BM25 Retriever in Haystack utilizes the BM25 scoring formula to rank the documents in the corpus \nbased on their relevance to the query. The top -ranked documents are then passe d to the reader of the querying pipeline \nfor further processing.  \nFramework for Adapting Representation Models (FARM) Reader is a reader module supported by the Haystack \nframework, which is also developed by Deepset AI. The FARM Reader is specifically designed to extract answers for a \nquery from a given context passage following a two-step process. First, a query-encoding approach will be used to embed \nboth the query and the context passage into vector representations, which can be processed by machine learning models. \nNext, a prediction head will be applied on top of the encoded representations to generate answers for the specific query. \nThe prediction head functions to predict the start and end positions of the answer span within the context passage. By \nidentifying the boundaries of the answer span, the FARM Reader can extract the relevant text as the answer for a given \nquery. The FARM Reader leverages powerful pre-trained transformer-based language models that have been trained on \nlarge-scale datasets and have a strong understanding of language semantics, enabling it to provide accurate and \ncontextually relevant answers.  \nBy utilizing the BM25 Retriever and FARM Reader, the complete querying pipeline for the proposed QAS is formed. \nWhen an input query is re ceived during runtime, it will be passed to the BM25 Retriever to retrieve the top -ranked \ndocuments from the document store based on the relevance scores. By performing this, the scope of the search has been \nnarrowed down, and the retrieved documents will be forwarded to the FARM Reader. Then, the reader will comprehend \nand analyze the content of the documents by applying the language models. The reader will search for relevant passages \nand assign confidence scores to each potential answer for the given query. Finally, the reader will select the answer with \nthe highest confidence score and return it as the final result for the user presentation. With this, the entire workflow of \nthe proposed QAS is uncovered. \n3-3-1- Transformer-based Language Models for FARM Reader \nIn this study, a comparative analysis of six widely used transformer -based language models has been conducted to \ndetermine the best model to be employed as the reader for the proposed QAS. The models under consideration are \nDeBERTaV3, BERT, ALBERT, ELECTRA , MiniLM, and RoBERTa. These models were selected for their advanced \ntransformer architecture, which enables them to comprehend complex language contexts and capture intricate patterns. \nAdditionally, their pre -training on extensive English data ensures eff ective comprehension and precise responses in \nvarious NLP tasks, including question-answering.  \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 680 \nIt is worth noting that all the selected models are off -the-shelf models that have been fine -tuned for question -\nanswering and are publicly available on Hugging Face. They were fine -tuned on a popular reading comprehension \ndataset, Stanford Question Answering Dataset 2.0 (SQuAD2.0) [24]. SQuAD2.0 contains a total of 100,000 question -\nanswer pairs and over 50,000 unanswerable questions. In order to facilitate a bett er understanding, the selected models \nwill be introduced and explained accordingly in the following paragraphs. \nDeBERTaV3 [25] is an enhanced version of the Decoding-enhanced BERT with Disentangled Attention (DeBERTa) \nmodel [26], which improves the BERT an d RoBERTa models by using the disentangled attention mechanism and an \nenhanced mask decoder. It introduces a new pre-training task known as Replaced Token Detection (RTD) to replace the \nconventional Mask Language Modeling (MLM) task. The research revealed that the sharing of embeddings in the \nELECTRA model may cause conflicting directions in the token embeddings between the discriminator and the generator, \nleading to undesirable dynamics. To address the issue, a novel Gradient -Disentangled Embedding Sharing  method is \nproposed in DeBERTaV3 to effectively avoid the conflicting dynamics, thereby improving the training efficiency and \nquality of the pre-trained model. \nBidirectional Encoder Representations from Transformers (BERT) [27] is a transformer model pre-trained on a large \ncorpus of English data in a self -supervised manner. It incorporates two key objectives during the pre -training process, \nwhich are Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). MLM involves randomly masking \nwords in a sentence and training a BERT model to predict the masked words, enabling it to learn bidirectional \nrepresentations. On the other hand, NSP focuses on predicting if two masked sentences are sequentially connected or \nnot, enhancing the understanding of the BERT model in sentence relationships. By leveraging these objectives, the BERT \nmodel acquires a comprehensive understanding of the English language, allowing it to extract features and train \nclassifiers for a wide range of downstream tasks.  \nA Lite BERT (ALBERT) [28] is an innovative language model that has made significant contributions to NLP. The \nmain contributions of ALBERT include three key components, which are factorized embedding parameterization, cross-\nlayer parameter sharing, and inter-sentence coherence loss. Factorized embedding parameterization reduces the memory \nrequirements of the model by breaking down the large vocabulary embeddings into smaller matrices. Cross -layer \nparameter sharing enables the model to share parameters across layers, signi ficantly reducing the model size without \ncompromising performance. Lastly, the inter -sentence coherence loss introduced during pre -training enhances the \nmodel‚Äôs ability to understand relationships between sentences in a document. These contributions not on ly made \nALBERT more memory -efficient and computationally scalable but also improved its language understanding \ncapabilities, leading to a revolution in the development of efficient and effective language models.  \nEfficiently Learning an Encoder that Classif ies Token Replacements Accurately (ELECTRA) [29] is an advanced \ntechnique used for pre -training in NLP. Unlike conventional methods that primarily use masked language modeling, \nELECTRA employs a unique framework consisting of a generator and a discriminator. The generator is trained to replace \ninput tokens with plausible alternatives, while the role of the discriminator is to differentiate between the original and \nreplaced tokens. This approach motivates the generator to generate realistic and contextually fitting replacements, \nleading to effective and efficient pre -training. With this, ELECTRA has showcased remarkable performance across \nvarious downstream tasks in the field of NLP. \nMiniLM [30] is a novel language model developed by Microsoft Research Asia t o address the challenges posed by \nthe large parameter sizes of pre-trained models such as BERT in real-world NLP applications. It introduces a simple and \neffective approach to compress the large transformer models, named deep self -attention distillation. It involves training \na smaller model, referred to as the student, to closely emulate the crucial self -attention module of the larger model, \nknown as the teacher. Additionally, MiniLM enhances the distillation process by introducing the scaled dot -product \nbetween values in the self-attention module as new deep self-attention knowledge, along with the attention distributions. \nWith these advancements, MiniLM overcomes the challenges associated with size and computational requirements, \nenabling more practical and efficient deployment in real-world NLP applications. \nRobustly optimized BERT approach (RoBERTa) [31] is an advanced transformer language model that builds upon \nthe BERT architecture. It enhances the architecture of the BERT model by integrating extensive training data, extending \nthe training time, and employing refined training methods. It eliminates the next -sentence prediction task, prioritizing \nmasked language modeling and employing dynamic masking. These improvements enable RoBERTa to grasp a greater \nunderstanding of context and vastly enhance its language representation abilities, resulting in outstanding performance \nacross diverse NLP tasks and establishing it as a cutting-edge model in the field. \n4- Experiments \nTo develop a promising Question Answering  System (QAS) for historical knowledge, a series of experiments have \nbeen conducted in this study to find out the optimal reader model for determining the best answers to user queries based \non the retrieved context. Specifically, six experiments have been carried out to study the effectiveness of employing \ndifferent transformer-based language models as the reader model for the proposed QAS, namely DeBERTaV3, BERT, \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 681 \nALBERT, ELECTRA, MiniLM, and RoBERTa. Through a comparative analysis of the models‚Äô performanc e, the aim \nis to determine the best model that yields the most promising results in answering historical questions. The dataset \nconstructed for model evaluation is introduced in Section 4-1; the evaluation metrics employed to assess the performance \nof the models are covered in Section 4-2; and the experimental results are presented in Section 4-3. \n4-1- Evaluation Dataset \nTo evaluate the performance of the selected models, a question-answering dataset consisting of 100 question-answer \npairs is constructed. This e valuation dataset was created by analyzing the articles collected from online resources \n(described in Section 3.2) thoroughly to identify important pieces of information that could be used to formulate \nquestions. As a result, a total of 100 question -answer pairs were generated, covering different aspects of the historical \ncontents of the entire corpus, including people, places, dates, and facts. In this stage, human evaluators play an important \nrole in verifying the question -answer pairs, ensuring that the answers to the questions are correct and can be directly \nextracted from the original source. \nIt is crucial to note that the evaluation dataset is generated from the translated corpus, which is in English instead of \nMalay. As mentioned in the previous section, the reason for constructing the evaluation dataset using the English corpus \nis because the models adopted in this study are primarily based on English. Meanwhile, the aim of this study is to verify \nthe ability of the models to effectively extract the c orrect answers from the entire corpus. Therefore, by aligning the \nlanguage of the evaluation dataset with the language of the models, it can be ensured that the evaluation results accurately \nreflect the performance of the models in the question-answering task. \n4-2- Evaluation Metrics \nIn this study, four evaluation metrics have been applied to effectively evaluate the performance of the transformer -\nbased language models, which are F1 -score, Jaccard similarity, cosine similarity, and semantic textual similarity. The \ncalculations of all the metrics are done for each sample, then averaged by the total number of samples in the evaluation \ndataset. Each of these metrics provides different perspectives on examining the models‚Äô performance in question -\nanswering tasks. By  considering these metrics, a comprehensive understanding of the model‚Äôs effectiveness can be \ngained, providing guidance to select the best reader model for the proposed QAS.  \nF1-score is a metric that is used to measure the overall effectiveness of a model  in terms of precision and recall. To \ncalculate the F1-score, the ground truth label (gold answer) and the predicted answer will be taken as input. Before the \ncalculation, the ground truth label and the predicted answer will be converted into a sequence of  tokens through word \nsplitting. Next, the common tokens are identified by the intersection between them, representing the correct answer \nextracted by the model. Then, the true positives (TP), false positives (FP), and false negatives (FN) are computed base d \non the token counts. After that, the precision and recall are calculated, indicating the ratio of correctly identified answers \nto the total number of answers identified and the ratio of correctly identified answers to the total number of correct \nanswers in the ground truth label. Finally, the F1 -score for the model is calculated as the harmonic mean of precision \nand recall, giving equal weight to both metrics. The calculation of the F1-score can be written as: \nùêπ1 =\n2√óùëùùëüùëíùëêùëñùë†ùëñùëúùëõ√óùëüùëíùëêùëéùëôùëô\nùëùùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëüùëíùëêùëéùëôùëô   (1) \nJaccard similarity is also known as the Jaccard index or Jaccard coefficient, where the similarity between two sets \nwill be measured. To calculate the Jaccard similarity, the ground truth label and the predicted answer will fi rst be \nconverted into sets of individual tokens. This is done to focus on the unique elements and ignore the order of the words. \nThe Jaccard similarity is then calculated as the size of the intersection of the ground truth label set and predicted answer \nset divided by the size of their union. Mathematically, it can be represented as:  \nùêΩùëéùëêùëêùëéùëüùëë(ùê¥,ùêµ) =\nùê¥‚à©ùêµ\nùê¥‚à™ùêµ  (2) \nwhere A denotes the ground truth label set and B denotes the predicted answer set. The resulting Jaccard similarity value \nranges from 0 to 1, where 0 indicates no overlap between the sets and 1 indicates a perfect match. It is worth noting that \nJaccard similarity solely focuses on the shared elements between the ground truth and predicted answer sets and does \nnot consider the exact word-by-word matching or the semantic understanding. \nCosine similarity is a common metric used in natural language processing (NLP) tasks to measure the similarity \nbetween two vectors representing textual information, which is also applied to question answering. To calculate the \ncosine similarity, the ground truth label and predicted answer are converted into vector form using the Bag -of-Words \n(BoW) model. Next, the dot product of the two vectors is computed. After that, the Euclidean magnitudes of both vectors \nare calculated, which is the square root of the sum of the squares of all its elements. Finally, the cosine similarity score \nis calculated by dividing the dot product of the two vectors by the product of the magnitudes of the two vectors. The \nformula for cosine similarity is as follows: \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 682 \nùê∂ùëúùë†ùëñùëõùëí(ùëã,ùëå) =\nùëã‚àôùëå\n‚Äñùëã‚Äñ√ó‚Äñùëå‚Äñ  (3) \nwhere X denotes the ground truth label vector and Y denotes the predicted answer vector. The resulting cosine similarity \nscore ranges from -1 to 1, where -1 indicates that the two vectors are di ametrically opposed, 0 signifies that the vectors \nare orthogonal, and 1 means they are identical. In the context of question -answering, a higher cosine similarity score \nindicates greater similarity between the ground truth label and the predicted answer.  \nSemantic similarity is the measurement of the degree of similarity or relatedness between two or more texts based on \ntheir underlying meaning or semantics. To compute the semantic similarity between the ground truth label and the \npredicted answer, the sentence transformer is utilized. It is used to transform sentences into dense vector representations \nor embeddings in semantic space. For the evaluation in this study, a sentence transformer model named ‚Äúall -MiniLM-\nL6-v2‚Äù is being used for the encoding task. To get the score for semantic similarity, the cosine similarity for the \nembeddings of the ground truth label and predicted answer is calculated. The resulting semantic similarity score ranges \nfrom 0 to 1, where 0 indicates completely dissimilar and 1 indic ates completely similar. By quantifying the similarity \nbetween texts based on their underlying meaning, semantic similarity provides a more nuanced understanding of text \nrelationships when evaluating the performance of a model in question-answering tasks. \n4-3- Experimental Results \nIn this stage, extensive experiments were conducted using the evaluation dataset to assess the performance of different \ntransformer-based language models. It is worth noting that the top-k parameter for the retriever is set to 3 to retrieve the \nthree most relevant documents for the reader throughout all the experiments. On the other hand, the top -k parameter for \nthe reader is set to 1, which means that it will only return the most appropriate answers to the questions. Table 1 \nsummarizes the experimental results for all the models presented in this study, namely DeBERTaV3, BERT, ALBERT, \nELECTRA, MiniLM, and RoBERTa. \nTable 1. Summary of Experimental Results \nModel Average F1-score Average Jaccard Similarity Average Cosine Similarity Average Semantic Similarity \nDeBERTaV3 0.74 0.69 0.75 0.83 \nBERT 0.77 0.72 0.78 0.85 \nALBERT 0.78 0.72 0.79 0.86 \nELECTRA 0.78 0.72 0.79 0.86 \nMiniLM 0.80 0.74 0.81 0.88 \nRoBERTa 0.92 0.88 0.93 0.96 \nThe experimental results presented in Table 1 clearly demonstrate the substantial impact of employing different reader \nmodels on the performance of the proposed QAS. From Table 1, it can be noticed that RoBERTa shows the most \npromising result by achieving an average F1 -score of 0.92, an average Jaccard similarity of 0.88, an average cosine \nsimilarity of 0.93, and an average semantic similarity of 0.96. The outstanding performance of the RoBERTa model can \nbe attributed to its deep contextual understanding, e xtensive pre-training, and effective handling of complex language \nstructures, enabling it to better process and comprehend the textual data in the question -answering task. \nIn contrast, the performance of DeBERTaV3 as the reader model in the querying pipeli ne is the worst among all the \nmodels presented in the experiments conducted. The model only achieves 0.74, 0.69, 0.75, and 0.83 for the average F1 -\nscore, average Jaccard similarity, average cosine similarity, and average semantic similarity, respectively. The poor \nperformance of the DeBERTaV3 model might be due to its limitations on contextual understanding as well as the \nintricate model architecture that does not align well with the question -answering task presented in this study. In the \nmeantime, the BERT model performs slightly better than the DeBERTaV3 model, scoring at 0.77, 0.72, 0.78, and 0.85 \nfor the evaluation metrics used in this study. \nOn the other hand, the experimental results show that ALBERT and ELECTRA have similar efficiency in identifying \nthe best answer candidate from the retrieved documents. Both models have achieved an average F1 -score of 0.78, an \naverage Jaccard similarity of 0.72, an average cosine similarity of 0.79, and an average semantic similarity of 0.86. \nMoreover, while not as so phisticated as the RoBERTa model, MiniLM displayed decent efficiency in the question -\nanswering task in this study, attaining 0.80, 0.72, 0.81, and 0.88 for the average F1 -score, average Jaccard similarity, \naverage cosine similarity, and average semantic similarity, respectively. \nTo sum up, the reader initialized with the RoBERTa model stands out as the best performer of all the other models \npresented in this study. To provide a more intuitive insight into the model‚Äôs performance in answering historical \nquestions, several samples from the evaluation dataset with the gold (correct) answer and the predicted answer are \npresented in Table 2. The samples are grouped into people, places, dates, and facts, showing that the RoBERTa model \ncan accurately provide answers for different types of historical questions. \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 683 \nTable 2. Samples with Gold Answer and Predicted Answer (RoBERTa) \nTypes Question Gold Answer Predicted Answer \nPeople \nWho made the decision for Singapore‚Äôs separation from Malaysia? Tunku Abdul Rahman Putra Al-Haj Tunku Abdul Rahman Putra Al-Haj \nWho introduced the policy of applying Islamic values in the administration \nof Malaysia? Mahathir Mohamad Mahathir Mohamad \nWho signed the Federation of Malaya Agreement on behalf of King George \nVI? Sir Edward Gent Edward James Gent \nWho ruled Sarawak between 1841 and 1946? The Brooke family The Brooke family \nWho led the Naning residents during the war? Dato ‚ÄòDol Said Dato ‚ÄòDol Said \nPlaces \nWhere did King Bagindo hail from? Minangkabau Minangkabau \nWhere was Tan Cheng Lock born? Heeren Road, Melaka Heeren Road, Melaka \nWhich school did Sambanthan attend in Kuala Kangsar, Perak? Clifford School Clifford School \nIn which regions did the puppet appear in the Malay world, as mentioned by \nDato‚Äô A. Aziz Deraman? Kelantan and Java Island Kelantan and Java Island \nWhat region is the Maya civilization located in? Southern Mexico and northern \nCentral America \nSouthern Mexico and northern Central \nAmerica \nDates \nWhen was Tun Dr. Mahathir bin Mohamad born? July 10, 1925 July 10, 1925 \nWhen was the Federation of Malaya Agreement signed? January 21, 1948 January 21, 1948 \nWhen did the Malaysian Constitution come into effect? August 31, 1957 August 31, 1957 \nWhen was ASEAN established? August 8, 1967 August 8, 1967 \nWhen was Rukun Negara formed? 31 August 1970 31 August 1970 \nFacts \nWhat was the predecessor of the Federation of Malaya? Malayan Union Malayan Union \nWhat was the Great Revolution also known as? The Revolution of 1688 The Revolution of 1688 \nWhat are the three parts into which the power is divided in the Malaysian \ngovernment? Legislature, Justice, and Executive Legislature, Justice, and Executive \nWhat are the three major races in Malaysia? Malays, Chinese and Indians Malays, Chinese and Indians \nWhat was the name of the agreement that combined North Borneo, Sarawak, \nand Singapore with the existing states in the Federation of Malaya? Malaysia Agreement Malaysia Agreement \nBy examining the samples provided in Table 2, it is more evident that the RoBERTa model has demonstrated its \nproficiency in effectively addressing historical questions in this study. Therefore, it is adopted as the reader model of the \nquerying pipeline in developing the proposed QAS. \n5- Deployment \nBased on the experimental results, RoBERTa  has been selected as the reader model for the proposed Question \nAnswering System (QAS). The deployment of the proposed QAS is carried out on Discord, an online platform that \nprovides a well-established channel for research and practical applications. Discord was chosen as the preferred platform \nto deploy the proposed QAS due to its user -friendly interface and large user base. By incorporating the language \ndetection and translation modules, the proposed QAS can provide answers to questions in multiple langu ages, \nspecifically English and Malay. Figure 2 shows a flowchart illustrating the process flow of the deployed QAS in Discord, \nproviding a clearer insight into how the system works. \n \nFigure 2. Flowchart of the Deployed QAS in Discord \n\nEmerging Science Journal | Vol. 8, No. 2 \nPage | 684 \nThe process begins when the user inputs a query into the system, which subsequently detects the language of the \nquery. The proposed system is designed to accommodate both Malay and English. If the language detected is Malay, \ntranslation becomes necessary as the data used is in English. Conversely, if the language detected is English, translation \nis not required. The query is then forwarded to the querying pipeline, which functions to retrieve relevant context and \nextract the best answer to the query. It is important to note that the answer returned will be converted back to the language \nof the original query if needed. Finally, the output answer is presented to the user.  \nFigure 3 displays a screenshot of the proposed Multilingual QAS for Malaysia History  deployed in Discord, \nshowcasing its capability in effectively answering questions related to the history of Malaysia. The examples provided \nin the screenshot also demonstrate that the system is equipped with multilingual support, enabling it to answer questions \nfrom users in both English and Malay. \n \nFigure 3. Screenshot of the Multilingual QAS for Malaysia History \n6- Conclusion \nIn conclusion, this study has successfully developed a Multilingual Question Answering System (QAS) for Malaysia \nHistory by utilizing a transformer-based language model. It serves as a proof of concept, demonstrating the effectiveness \nof transformer -based language models in answering historical questions. The system development process involved \nseveral major steps, including data collection from the Internet, knowledge representation in separate text files, data \ntranslation from Malay to English, document l oading and pre -processing, document indexing and storing into \nDocumentStore, and the initialization of the querying pipeline with retriever and reader modules to retrieve relevant \ncontext and extract answers for user queries. \nIn order to determine the opti mal reader model, a comparative analysis of six different transformer -based language \nmodels, including DeBERTaV3, BERT, ALBERT, ELECTRA, MiniLM, and RoBERTa, has been conducted. The \nperformance of the models was evaluated using a dataset comprising 100 question-answer pairs related to the history of \nMalaysia, which was meticulously created for this study. Four evaluation metrics, namely average F1 -score, average \nJaccard Similarity, average cosine similarity, and average semantic similarity, were employed to assess the performance \nof the proposed QAS. As a result, the RoBERTa model stands out as the best performer, demonstrating the best overall \nperformance on the question-answering task. \nBased on the experimental results, RoBERTa was employed as the reader model for the proposed QAS to effectively \nextract answers for user queries. Lastly, the proposed QAS was deployed on Discord, which is a widely used \ncommunication platform. By incorporating language detector and translator modules, the proposed system is capable of \nhandling multilingual queries from users. In this case, users can ask questions in either Malay or English, and they will \nreceive responses in the original language they used. \nIn the future development of multilingual QAS, it is worthwhile to inve stigate the utilization of multilingual models \nfor more seamless and enhanced multilingual support. Additionally, fine -tuning of the transformer -based language \nmodels can be adopted to further improve their performance for specific use cases. Furthermore, exploring the potential \nof generative QAS is also worth considering. Instead of solely highlighting the specific span of text that answers a query, \na generative QAS can generate a novel textual response for the user. This opens up possibilities for generat ing more \ncreative and diverse answers for the users in the question-answering process. \n\nEmerging Science Journal | Vol. 8, No. 2 \nPage | 685 \n7- Declarations  \n7-1- Author Contributions \nConceptualization, Q.Z.L., J.X.N., E.K.H.O., N.K.N.L., C.P.L. and K.M.L.; methodology, Q.Z.L., J.X.N., E.K.H.O., \nN.K.N.L., C.P.L. and K.M.L.; software, Q.Z.L., J.X.N., E.K.H.O.  and N.K.N.L.; validation, Q.Z.L., J.X.N., E.K.H.O., \nN.K.N.L., C.P.L.  and K.M.L.; formal analysis, C.P.L. and K.M.L.; investigation, Q.Z.L., J.X.N., E.K.H.O.  and \nN.K.N.L.; resources, Q.Z.L. and J.X.N.; data cu ration, Q.Z.L., J.X.N., E.K.H.O., N.K.N.L., C.P.L.  and K.M.L.; \nwriting‚Äîoriginal draft preparation, Q.Z.L., J.X.N., E.K.H.O., N.K.N.L., C.P.L. , and K.M.L.; writing ‚Äîreview and \nediting, Q.Z.L., J.X.N., E.K.H.O., N.K.N.L., C.P.L. and K.M.L.; visualization, Q.Z.L., J.X.N., E.K.H.O. and N.K.N.L.; \nsupervision, C.P.L. and K.M.L.; project administration, C.P.L. and K.M.L.; funding acquisition, C.P.L. and K.M.L. All \nauthors have read and agreed to the published version of the manuscript. \n7-2- Data Availability Statement \nThe data presented in this study are available on request from the corresponding author . \n7-3- Funding \nThis research was funded by Telekom Malaysia R&D under grant number RDTC/221064 and RDTC/231075 . \n7-4- Institutional Review Board Statement \nNot applicable. \n7-5- Informed Consent Statement \nNot applicable. \n7-6- Conflicts of Interest \nThe authors declare that there is no conflict of interest regarding the publication of this manuscript. In addition, the \nethical issues, including plagiarism, informed consent, misconduct, data fabrication and/or falsification, double \npublication and/or submission, and redundancies have been completely observed by the authors. \n8- References  \n[1] Chee-Huay, C., & Kee -Jiar, Y. ( 2016). Why Students Fail in History: A Minor Case Study in Malaysia and Solutions from \nCognitive Psychology Perspective. Mediterranean Journal of Social Sciences. doi:10.5901/mjss.2016.v7n1p517. \n[2] Woods, W., Kaplan, R. M., & Nash -Webber, B. L. (1972). The Lunar Science Natural Language Information System: Final \nReport. BBN Report No. 11501, Contract No. NAS9-1115 NASA Manned Spacecraft Center, Houston, Texas, United States. \n[3] Androutsopoulos, I., Ritchie, G. D., & Thanisch, P. (1996). A Framework for Natural Language Interfaces to Temporal Databases. \nProceedings of the 20th Australasian Computer Science Conference, 5‚Äì7 February, 1997, Sydney, Australia. \n[4] Ojokoh, B., & Adebisi, E. (2019). A review of question answering systems. Journal of Web Engineering, 17(8), 717 ‚Äì758. \ndoi:10.13052/jwe1540-9589.1785. \n[5] Zheng, Z. (2002). AnswerBus question answering system. Proceedings of the Second International Conference on Huma n \nLanguage Technology Research, 399-404. doi:10.3115/1289189.1289238. \n[6] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is all \nyou need. 31st Conference on Neural Information Proces sing Systems (NIPS 2017), 4-9 December, 2017, Long Beach, United \nStates. \n[7] Ou, Y.-Y., Chuang, S.-W., Wang, W.-C., & Wang, J.-F. (2022). Automatic Multimedia-based Question-Answer Pairs Generation \nin Computer Assisted Healthy Education System. 2022 10th International Conference on Orange Technology (ICOT), Shanghai, \nChina. doi:10.1109/icot56925.2022.10008119. \n[8] Zhang, J. (2022). Application Research of Similarity Algorithm in the Design of English Intelligent Question Answering System. \n2022 IEEE 2nd Intern ational Conference on Mobile Networks and Wireless Communications (ICMNWC) , Karnataka, India. \ndoi:10.1109/icmnwc56175.2022.10031708. \n[9] Das, B., & Nirmala, S. J. (2022). Improving Healthcare Question Answering System by Identifying Suitable Answers. 2022 IEEE \n2nd Mysore Sub Section International Conference (MysuruCon), Mysuru, India. doi:10.1109/mysurucon55714.2022.9972435. \n[10] Gupta, S. (2023). Top K Relevant Passage Retrieval for Biomedical Question Answering. arXiv preprint arXiv:2308.04028 . \ndoi:10.48550/arXiv.2308.04028. \nEmerging Science Journal | Vol. 8, No. 2 \nPage | 686 \n[11] Pudasaini, S., & Shakya, S. (2023). Question Answering on Biomedical Research Papers using Transfer Learning on BERT -\nBase Models. 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Kirtipur, \nNepal. doi:10.1109/i-smac58438.2023.10290240. \n[12] Alzubi, J. A., Jain, R., Singh, A., Parwekar, P., & Gupta, M. (2023). COBERT: COVID -19 Question Answering System Using \nBERT. Arabian Journal for Science and Engineering, 48(8), 11003‚Äì11013. doi:10.1007/s13369-021-05810-5. \n[13] Acharya, S., Sornalakshmi, K., Paul, B., & Singh, A. (2022). Question Answering System using NLP and BERT. 3rd International \nConference on Smart Electronics and Communication (ICOSEC), Trichy, India. doi:10.1109/icosec54921.2022.9952050. \n[14] Yin, J. (2022). Research on Question Answering System Based on BERT Model. 2022 3rd International Conference on Computer \nVision, Image and Deep Learning &amp; International Conference on Computer Engineering and Applic ations (CVIDL & \nICCEA), Changchun, China. doi:10.1109/cvidliccea56201.2022.9824408. \n[15] Yang, J., Yang, X., Li, R., Luo, M., Jiang, S., Zhang, Y., & Wang, D. (2023). BERT and hierarchical cross attention -based \nquestion answering over bridge inspection knowledg e graph. Expert Systems with Applications, 233, 120896. \ndoi:10.1016/j.eswa.2023.120896. \n[16] Tian, D., Li, M., Ren, Q., Zhang, X., Han, S., & Shen, Y. (2023). Intelligent question answering method for construction safety \nhazard knowledge based on deep semantic mining. Automation in Construction, 145, 104670. doi:10.1016/j.autcon.2022.104670. \n[17] Liu, S., & Huang, X. (2019). A Chinese Question Answe ring System based on GPT. 2019 IEEE 10th International Conference \non Software Engineering and Service Science (ICSESS), Beijing, China. doi:10.1109/icsess47205.2019.9040807. \n[18] Noraset, T., Lowphansirikul, L., & Tuarob, S. (2021). WabiQA : A Wikipedia -Based Thai Question -Answering System. \nInformation Processing & Management, 58(1), 102431. doi:10.1016/j.ipm.2020.102431. \n[19] Ainon, R. N., Salim, S. S., & Noor, N. E. M. (1989). A question-answering system in Bahasa Malaysia. Fourth IEEE Region 10 \nInternational Conference TENCON, Bombay, India. doi:10.1109/tencon.1989.176892. \n[20] Puteh, N., Husin, M. Z., Tahir, H. M., & Hussain, A. (2019). Building a question classification model for a Malay question \nanswering system. International Journal of Innovative Technology and Exploring Engineering, 8(5s), 184‚Äì190. \n[21] Lim, H. T., Huspi, S. H., & Ibrahim, R. (2021). A Conceptual Framework for Malay-English Mixed -language Question \nAnswering System. 2021 International Congress of Advanced Technology and Engin eering (ICOTEN) , Taiz, Yemen . \ndoi:10.1109/icoten52080.2021.9493503. \n[22] Pietsch, M., M√∂ller, T., Kostic, B., Risch, J., Pippi, M., Jobanputra, M., Zanzottera, S., Cerza, S., Blagojevic, V., Stadelm ann, \nT., Soni, T., & Lee, S. (2019). Haystack: the end -to-end N LP framework for pragmatic builders.  Available online:  \nhttps://github.com/deepset-ai/haystack (accessed on March 2024). \n[23] Pietsch, M., M√∂ller, T., Kostic, B., Risch, J., Pippi, M., Jobanputra, M., ... &  Lee, S. (2019). Haystack: the end -to-end NLP \nframework for pragmatic builders. \n[24] Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., & \nAhmed, A. (2020). Big bird: Transformers for longer se quences. 34th Conference on Neural Information Processing Systems \n(NeurIPS 2020), 6-12 December, 2020, Vancouver, Canada. \n[25] Rajpurkar, P., Jia, R., & Liang, P. (2018). Know what you don't know: Unanswerable questions for SQuAD. arXiv  preprint \narXiv:1806.03822. doi:10.18653/v1/p18-2124. \n[26] He, P., Gao, J., & Chen, W. (2021). Debertav3: Improving deberta using electra -style pre-training with gradient -disentangled \nembedding sharing. arXiv preprint arXiv:2111.09543. \n[27] He, P., Liu, X., Gao, J.,  & Chen, W. (2020). Deberta: Decoding -enhanced BERT with disentangled attention. arXiv preprint \narXiv:2006.03654. doi:10.48550/arXiv.2111.09543. \n[28] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformer s for language \nunderstanding. arXiv preprint arXiv:1810.04805. doi:10.48550/arXiv.1810.04805. \n[29] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Albert: A lite BERT for self-supervised learning \nof language representations. arXiv preprint arXiv:1909.11942. doi:10.48550/arViv.1909.11942. \n[30] Clark, K., Luong, M. T., Le, Q. V., & Manning, C. D. (2020). Electra: Pre -training text encoders as discriminators rather than \ngenerators. arXiv preprint arXiv:2003.10555. doi:10.48550/arXiv.2003/10555. \n[31] Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., & Zhou, M. (2020). MINILM: Deep self-attention distillation for task-agnostic \ncompression of pre -trained transformers. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), 6 -12 \nDecember, 2020, Vancouver, Canada. \n[32] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized BERT \npretraining approach. arXiv preprint arXiv:1907.11692. doi:10.48550/arXiv.1907.11692. "
}