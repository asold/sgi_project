{
    "title": "Faster Depth-Adaptive Transformers",
    "url": "https://openalex.org/W3112673818",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2105151920",
            "name": "Yijin Liu",
            "affiliations": [
                "Beijing Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2133392087",
            "name": "Fandong Meng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2093278426",
            "name": "Jie Zhou",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2097442149",
            "name": "Yufeng Chen",
            "affiliations": [
                "Beijing Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2169141889",
            "name": "Jinan Xu",
            "affiliations": [
                "Beijing Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2105151920",
            "name": "Yijin Liu",
            "affiliations": [
                "Beijing Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2133392087",
            "name": "Fandong Meng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2093278426",
            "name": "Jie Zhou",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2097442149",
            "name": "Yufeng Chen",
            "affiliations": [
                "Beijing Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2169141889",
            "name": "Jinan Xu",
            "affiliations": [
                "Beijing Jiaotong University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6769227307",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2562731582",
        "https://openalex.org/W6759826913",
        "https://openalex.org/W2740721704",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W6757641445",
        "https://openalex.org/W6668254845",
        "https://openalex.org/W2608568997",
        "https://openalex.org/W6676984168",
        "https://openalex.org/W2740711318",
        "https://openalex.org/W2949380545",
        "https://openalex.org/W2952186591",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2154053567",
        "https://openalex.org/W6748511785",
        "https://openalex.org/W2803763037",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3015298864",
        "https://openalex.org/W6754894847",
        "https://openalex.org/W2798754355",
        "https://openalex.org/W2769858569",
        "https://openalex.org/W2962853356",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W6637433842",
        "https://openalex.org/W2997945091",
        "https://openalex.org/W2915716523",
        "https://openalex.org/W2890832667",
        "https://openalex.org/W2786464815",
        "https://openalex.org/W2964301648",
        "https://openalex.org/W2964329882",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2962897020",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W1681397005",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2114524997",
        "https://openalex.org/W2963349685",
        "https://openalex.org/W2952164680",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2070246124",
        "https://openalex.org/W2981757109",
        "https://openalex.org/W2786959368",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W2950014519",
        "https://openalex.org/W2963393494",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W2163455955",
        "https://openalex.org/W2906310736",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4294027320",
        "https://openalex.org/W3101731278",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W4302343710"
    ],
    "abstract": "Depth-adaptive neural networks can dynamically adjust depths according to the hardness of input words, and thus improve efficiency. The main challenge is how to measure such hardness and decide the required depths (i.e., layers) to conduct. Previous works generally build a halting unit to decide whether the computation should continue or stop at each layer. As there is no specific supervision of depth selection, the halting unit may be under-optimized and inaccurate, which results in suboptimal and unstable performance when modeling sentences. In this paper, we get rid of the halting unit and estimate the required depths in advance, which yields a faster depth-adaptive model. Specifically, two approaches are proposed to explicitly measure the hardness of input words and estimate corresponding adaptive depth, namely 1) mutual information (MI) based estimation and 2) reconstruction loss based estimation. We conduct experiments on the text classification task with 24 datasets in various sizes and domains. Results confirm that our approaches can speed up the vanilla Transformer (up to 7x) while preserving high accuracy. Moreover, efficiency and robustness are significantly improved when compared with other depth-adaptive approaches.",
    "full_text": "Faster Depth-Adaptive Transformers\nYijin Liu, 1 Fandong Meng,2 Jie Zhou,2 Yufeng Chen1 and Jinan Xu1\n1Beijing Jiaotong University, China\n2Pattern Recognition Center, WeChat AI, Tencent Inc, China\nfyijinliu, fandongmeng, withtomzhoug@tencent.com\nfchenyf, jaxug@bjtu.edu.cn\nAbstract\nDepth-adaptive neural networks can dynamically adjust\ndepths according to the hardness of input words, and thus\nimprove efﬁciency. The main challenge is how to measure\nsuch hardness and decide the required depths (i.e., layers)\nto conduct. Previous works generally build a halting unit to\ndecide whether the computation should continue or stop at\neach layer. As there is no speciﬁc supervision of depth se-\nlection, the halting unit may be under-optimized and inaccu-\nrate, which results in suboptimal and unstable performance\nwhen modeling sentences. In this paper, we get rid of the\nhalting unit and estimate the required depths in advance,\nwhich yields a faster depth-adaptive model. Speciﬁcally, two\napproaches are proposed to explicitly measure the hardness\nof input words and estimate corresponding adaptive depth,\nnamely 1) mutual information (MI) based estimation and\n2) reconstruction loss based estimation. We conduct experi-\nments on the text classiﬁcation task with 24 datasets in var-\nious sizes and domains. Results conﬁrm that our approaches\ncan speed up the vanilla Transformer (up to 7x) while pre-\nserving high accuracy. Moreover, efﬁciency and robustness\nare signiﬁcantly improved when compared with other depth-\nadaptive approaches.\nIntroduction\nIn the NLP literature, neural networks generally conduct a\nﬁxed number of computations over all words in a sentence,\nregardless of whether they are easy or difﬁcult. In terms of\nboth efﬁciency and ease of learning, it is preferable to dy-\nnamically vary the numbers of computations according to\nthe hardness of input words (Dehghani et al. 2019).\nGraves (2016) ﬁrstly proposes adaptive computation time\n(ACT) to improve efﬁciency of neural networks. Speciﬁ-\ncally, ACT employs a halting unit upon each word when\nprocessing a sentence, then this halting unit determines a\nprobability that computation should continue or stop layer-\nby-layer. Its application to sequence processing is attractive\nand promising. For instance, ACT has been extended to re-\nduce computations either by exiting early or by skipping lay-\ners for the ResNet (Figurnov et al. 2017), the vanilla Trans-\nformer (Elbayad et al. 2020), and the Universal Transformer\n(Dehghani et al. 2019).\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nHowever, there is no explicit supervision to directly train\nthe halting unit of ACT, and thus how to measure the hard-\nness of input words and decide required depths is the key\npoint. Given a task, previous works generally treat the loss\nfrom different layers as a measure to implicitly estimate\nthe required depths, e.g., gradient estimation in ACT, or re-\ninforcement rewards in SkipNet (Wang et al. 2018). Un-\nfortunately, these approaches may lead to inaccurate depth\nselections with high variances, and thus unstable perfor-\nmance. More recently, the depth-adaptive Transformer (El-\nbayad et al. 2020) directly trains the halting unit with the\nsupervision of ‘pseudo-labels’, which is generated by com-\nparing task-speciﬁc losses over all layers. Despite its suc-\ncess, the depth-adaptive Transformer still relays on a halting\nunit, which brings additional computing costs for depth pre-\ndictions, hindering its potential performance.\nIn this paper, we get rid of a halting unit when building our\nmodel, and thus no additional computing costs need to esti-\nmate depth. Instead, we propose two approaches to explicitly\nestimate the required depths in advance, which yield a faster\ndepth-adaptive Transformer. Speciﬁcally, the MI-based ap-\nproach calculates the mutual dependence between a word\nand all categorical labels. The larger the MI value of the\nword is, the more information of labels is obtained through\nobserving this word, thus fewer depths are needed to learn an\nadequate representation for this word, and vice versa. Due\nto the MI-based approach is purely conducted in the data\npreprocessing stage, the computing cost is ignorable when\ncompared with training a neural model in the depth-adaptive\nTransformer. The reconstruction loss based approach mea-\nsures the hardness of learning a word by reconstructing it\nwith its contexts in a sentence. The less reconstruction loss\nof the word is, the more easily its representation is learned.\nTherefore the index of the layer with minimum reconstruc-\ntion loss can be regarded as an approximation for required\ndepths. As a by-product, the reconstruction loss based ap-\nproach is easy to apply to unsupervised scenarios, as it needs\nno task-speciﬁc labels. Both of the above approaches aim to\nﬁnd a suitable depth estimation. Afterward, the estimated\ndepths are directly used to guide our model to conduct cor-\nresponding depth for both training and testing.\nWithout loss of generality, we base our model on the\nTransformer encoder. Extensive experiments are conducted\non the text classiﬁcation task (24 datasets in various sizes\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n13424\nand domains). Results show that our proposed approaches\ncan accelerate the vanilla Transformer up to 7x, while pre-\nserving high accuracy. Furthermore, we improve the efﬁ-\nciency and robustness of previous depth-adaptive models.\nOur main contributions are as follows1:\n• We are the ﬁrst to estimate the adaptive depths in advance\nand do not rely on a halting unit to predict depths.\n• We propose two effective approaches to explicitly esti-\nmate the required computational depths for input words.\nSpeciﬁcally, the MI-based approach is computing efﬁ-\ncient and the reconstruction loss based one is also appli-\ncable in unsupervised scenarios.\n• Both of our approaches can accelerate the vanilla Trans-\nformer up to 7x, while preserving high accuracy. Fur-\nthermore, we improve previous depth-adaptive models in\nterms of accuracy, efﬁciency, and robustness.\n• We provide thorough analyses to offer more insights and\nelucidate properties of our approaches.\nModel\nDepth Estimation\nIn this section, we introduce how to quantify the hardness\nof learning representations for input words and obtain corre-\nsponding estimated depths.\nMutual Information Based Estimation. Mutual Infor-\nmation (MI) is a general concept in information theory. It\nmeasures the mutual dependence between two random vari-\nables X and Y. Formally, the MI value is calculated as:\nMI(X; Y) =\nX\ny2Y\nX\nx2X\np(X;Y ) \u0001log( p(X;Y )(x;y)\npX(x) \u0001pY (y)) (1)\nwhere p(X;Y ) is the joint probability of X and Y, and pX\nand pY are the probability functions of X and Y respec-\ntively.\nMI has been widely used for feature selection in the statis-\ntic machine learning literature (Peng, Long, and Ding 2005).\nIn our case of text classiﬁcation, X is the set of all words,\nand Y is the set of predeﬁned labels. Given a word x 2X\nand a labely2Y, the value ofMI(x;y) measures the degree\nof dependence between them. The larger MI(x;y) is, the\ngreater certainty between this wordxand label yis, and thus\nfewer computations are needed to learn an adequate repre-\nsentation for xto predict y. For example, the word ‘terrible’\ncan decide a ‘negative’ label with high conﬁdence in senti-\nment analysis tasks, and thus it is unnecessary to conduct a\nvery deep transformation when processing words with high\nMI values, and vice versa. Namely, we force our models not\nto merely focus on a few ‘important’ words and pay more\nattention to overview contexts when learning the representa-\ntion of a sentence. In this way, our models avoid overﬁlling\n1Codes will appear at https://github.com/Adaxry/Adaptive-\nTransformer\nlimited ‘important’ words, which also takes an effect of reg-\nularization, and thus improve generalization and robustness.\nBased on the above assumptions, it is intuitive and suitable\nto choose MI to quantify the difﬁculty of learning a word.\nFormally, given a dataset with vocab X and label set Y,\nthe MI value MI(x) for word xis calculated as:\nMI(x) =\nX\ny2fY g\nX\nix2f0;1g\nX\niy2f0;1g\nP(ix;iy)\n\u0001log\n\u0012 P(ix;iy)\nP(ix) \u0001P(iy)\n\u0013 (2)\nwhere ix is a boolean indicator whether word xexists in a\nsentence. Similarly, iy indicates the existence of label y. In\npractice, the probability formulas P(\u0001) in Equation (2) are\ncalculated by frequencies of words, labels, or their combi-\nnations. A smooth factor (0.1 in our experiments) is intro-\nduced to avoid zero division. To avoid injecting information\nof golden labels when testing, we only use the training set to\ncalculate MI values,\nOnce the MI value MI(x) for each word is obtained, we\nproceed to generate the pesudo-label of depth distribution\nd(x) accordingly. As the histogram of MI values shown\nin Figure 1 (upper part), there is an obvious long tail phe-\nnomenon, which manifests that the distribution is extremely\nimbalanced. To alleviate this issue, we perform a logarithmic\nscaling for the original MI(x) as:\nMIlog(x) =\u0000log (MI(x)) (3)\nNext, according to the scaledMIlog(x), we uniformly divide\nall words into N bins 2 with ﬁxed-width margin, where N\ndenotes a predeﬁned number of bins (i.e., maximum depth).\nConsequently, the estimated depth value d(x) for word xis\nthe index of corresponding bins.\nThe MI-based approach is purely calculated at the data\npreprocessing stage, thus it is highly efﬁcient in computation\nand does not rely on additional trainable parameters.\nReconstruction Loss Based Estimation. Generally in a\nsentence, several words may bring redundant information\nthat has been included by their contexts. Thus if we mask\nout these trivial words, it would be easier to reconstruct\nthem than others. Namely, The less reconstruction loss of a\nword is, the more easily its representation is learned. Based\non the above principle, we utilize this property of recon-\nstruction loss to quantify the hardness of learning the rep-\nresentation for input words and then estimate their required\ndepths. Firstly, we ﬁnetune BERT (Devlin et al. 2019) with\na masked language model task (MLM) on datasets of down-\nstream tasks. Note that we modify BERT to make predic-\ntions at any layers with a shared classiﬁer, which is also\nknown as anytime prediction (Huang et al. 2017; Elbayad\net al. 2020). The losses from all layers are summed up 3 to\n2We set N to 12 for the compatibility of BERT.\n3We experimented with different weights (e.g.,random sample,\nor linearly decaying with the number of layers) for different layers,\nand ﬁnally choose the simple equal weights.\n13425\nMI (bit)\nbad\ngreat\nboring\nbest\ndull\nhorrible\nworst\nexcellent\nhighly\nupset\nwaste\nwell\nlove\nstupid\nperfect\ndumb\nunless\nsilly\nhoping\nmaybe\nnoble\nkept\ntoday\nlegendary\nminutes\nclassic\nwrong\nbrat\nhocker\nearning\nprecious\none\n1\n2\n3\n4\nEstimated Depth\n5\n5\n4\n3\n2\n1\nFigure 1: The histogram of MI values of partial words from\nIMDB ( upper part), and corresponding depths of these\nwords by using the reconstruction loss based estimation (the\nbottom part).\nthe ﬁnal loss. After ﬁnetuning the MLM, given an input sen-\ntence x with jxjwords, we sequentially replace each word\nxt (t 2[1;jxj]) with a special symbol <MASK>, and then\nfeed the sentence with a <MASK> into the MLM. Finally,\nthe index of a layer with the minimum loss is selected as the\nestimated depth value d(xt):\nd(xt) = arg min\nn\n(lossn \u0000\u0015n) (4)\nwhere n 2 N is the index of layer, lossn is the loss of\n<MASK> in the n-th layer, and\u0015nis the penalty factor to en-\ncourage a lower selection4. Speciﬁcally, we train MLMs fol-\nlowing the experimental setup of BERT (Devlin et al. 2019)\nwith two major differences: 1) We make predictions at ev-\nery layer with a shared classiﬁer instead of only at the ﬁnal\nlayer in BERT; 2) We remove the next sentence prediction\ntask following RoBERTa (Liu et al. 2019).\nComparisons Between the Two Approaches. Although\nthe above approaches perform differently, they both serve as\na measure to estimate required depths for input words from\nthe perspective of learning their representations. We proceed\nto make a detailed comparison between the two approaches.\nIn the term of computational cost, the MI-based ap-\nproach calculates MI values, and then stores the word-depth\npairs that resemble word embeddings. The above procedures\n4We elaborate the effect and choice of \u0015in the following ana-\nlytical Section.\nx1 x2 ··· xt\nCalculated block Copied block\n···\nFigure 2: The overview of our depth-adaptive Transformer.\nOnce a word xt achieve its own depth d(xt), it will simply\ncopy states to upper layers.\nmerely happen at the stage of data preprocessing, which re-\nquires trivial computational cost and does not rely on addi-\ntional trainable parameters, and thus the MI-based approach\nis highly efﬁcient in computation. In contrast, the recon-\nstruction loss based approach needs to train several MLMs\nwith anytime prediction, which yields extra computational\ncosts. Considering the MLMs are dependent on the main\nmodel, the calculation of depths can be conducted in ad-\nvance in a piplined manner.\nAs the histogram shown in Figure 1, we observe differ-\nent preferences between the two estimations. Firstly, the MI-\nbased approach (upper part) tends to assign higher MI val-\nues to label-relevant words (e.g.,opinion words ‘perfect’ and\n‘horrible’ in IMDB). After the scaling function described\nby Equation (3), these opinion words are assigned a lower\nnumber of depths, namely fewer computational steps. Such\noperations make our models not only focus on a few ‘im-\nportant’ words, but also pay more attention to the overview\ncontexts, which takes an effect of regularization, and thus\nimprove generalization and robustness.\nUnlike the bias for label-related words in the MI-based\napproach, the reconstruction based approach (bottom part in\nFigure 1) purely relies on the unsupervised context to mea-\nsures the hardness of learning, which is good at recognizing\ncommon words (e.g., ‘today’, ‘one’ and ‘me’), and assigns\na smaller number of computations, and vice versa. As a by-\nproduct, the reconstruction loss based approach is applicable\nto unsupervised scenarios, as it needs no task-speciﬁc labels.\nDepth-Adaptive Mechanism\nAs the overview shown in Figure 2, we stackN layers of the\nTransformer encoder to model a sentence. The Transformer\nencoder consists of two sub-layers in each layer. The ﬁrst\nsub-layer is a multi-head dot-product self-attention and the\nsecond one is a position-wise fully connected feed-forward\nnetwork. We refer readers to the original paper (Vaswani\net al. 2017) for more details.\nTo make sure all hidden states of the same layer are avail-\n13426\nDataset Classes T\nype Average\nLenghts\nMax\nLengths\nTrain\nSample\nTest\nSample\nTREC (Li\nand Roth 2002) 6 Question\n12 39 5,952 500\nAG’s News (Zhang, Zhao, and LeCun 2015) 4 T\nopic 44 221 120,000 7,600\nDBPedia (Zhang, Zhao, and LeCun 2015) 14 T\nopic 67 3,841 560,000 70,000\nSubj (Pang and Lee 2004) 2 Sentiment\n26 122 10,000 CV\nMR (Pang and Lee 2005) 2 Sentiment\n23 61 10,622 CV\nAmazon-16 (Liu, Qiu, and Huang 2017) 2 Sentiment\n133 5,942 31,880 6,400\nIMDB (Maas et al. 2011) 2 Sentiment\n230 2,472 25,000 25,000\nYelp Polarity (Zhang, Zhao, and LeCun 2015) 2 Sentiment\n177 2,066 560,000 38,000\nYelp Full (Zhang, Zhao, and LeCun 2015) 5 Sentiment\n179 2,342 650,000 50,000\nTable 1: Dataset statistics. ‘CV’ refers to 5-fold cross-validation. There are 16 subsets in Amazon-16.\nable to compute self-attention, once a word xt reaches its\nown maximal layerd(xt), it will stop computation, and sim-\nply copy its states to the next layer until all words stop or the\nmaximal layer N is reached. Formally, at the n-th layer, for\nthe word xt, its hidden state hn\ni are updated as follows:\nhn\nt =\n\u001ahn\u00001\nt if n>d (xt)\nTransformer(hn\u00001\nt ) else (5)\nwhere n2[1;N] refers to the index of the layer. Especially,\nh0\nt is initialized by the BERT embedding.\nTask-speciﬁc Settings\nAfter dynamic steps of computation for each word position,\nwe make task-speciﬁc predictions upon the maximal stop\nlayer nmax 2[1;N] among all word positions. The feature\nvector v consists of mean and max pooling of output hidden\nstates hnmax , and is activated by ReLU. Finally, a softmax\nclassiﬁer are built on v. Formally, the above-mentioned pro-\ncedures are computed as follows:\nv = ReLU([max(hnmax ); mean(hnmax )])\nP(eyjv) = softmax(W v+ b) (6)\nwhere W 2Rdmodel\u0002jSj and b 2RjSj are parameters of\nthe classiﬁer, jSjis the size of the label set, and P(eyjv) is\nthe probability distribution. At the training stage, we use the\ncross-entropy loss computed as:\nLoss= \u0000\njSjX\ni=1\nyilog(Pi(eyjv)) (7)\nwhere yi is the golden label. For testing, the most proba-\nble label ^yis chosen from above probability distribution de-\nscribed by Equation (6):\n^y= arg maxP(eyjv) (8)\nExperiments\nTask and Datasets\nText classiﬁcation aims to assign a predeﬁned label to text\n(Zhang, Zhao, and LeCun 2015), which is a classic task for\nnatural language processing and is generally evaluated by\naccuracy score. Generally, The number of labels may range\nfrom two to more, which corresponds to binary and ﬁne-\ngrained classiﬁcation. We conduct extensive experiments on\nthe 24 popular benchmarks collected from diverse domains\n(e.g., topic,sentiment) ranging from modestly sized to large-\nscaled. The statistics of these datasets are listed in Table 1.\nImplementation Details\nFor the MI-based estimation approach, we calculate word-\ndepth pairs on the training set in advance and then calcu-\nlate depths for words in the test set. For the reconstruction\nbased approach, we calculate word-depth pairs for both train\nand test set without using label information. The penalty\nfactor \u0015 in the reconstruction loss based approach is set to\n0.1. Dropout (Srivastava et al. 2014) is applied to word em-\nbeddings, residual connection , and attention scores with a\nrate of 0.1. Models are optimized by the Adam optimizer\n(Kingma and Ba 2014) with gradient clipping of 5 (Pascanu,\nMikolov, and Bengio 2013). BERTbase is used to initialize\nthe Transformer encoder. Long sentences exceed 512 words\nare clipped.\nMain Results\nResults on Amazon-16. Amazon-16 consists of consumer\ncomments from 16 different domains (e.g., Apparel). We\ncompare our approaches with different baseline models in\nTable 2. The Multi-Scale Transformer (Guo et al. 2019b) is\ndesigned to capture features from different scales, and the\nStar-Transformer (Guo et al. 2019a) is a lightweight Trans-\nformer with a star-shaped topology. Due to the absence of\na powerful contextual model (e.g., BERT), their results un-\nderperform others by a margin. The Transformer model is\nﬁnetuned on BERT and conducts ﬁxed 12 layers for every\ninstance, which yields a strong baseline model. Following\nthe setup of depth-adaptive Transformer, we add a halting\nunit on the bottom layer of the Transformer encoder, and\ngenerate a ‘pesudo-label’ for the halting unit by classiﬁca-\ntion accuracy. Our approaches (the last two columns in Ta-\nble 2) achieve better or comparable performance over these\nstrong baseline models. The MI-based approach also takes a\nregularization effect, and thus it achieves better performance\nthan the reconstruction counterpart.\n13427\nData /\nModel Multi-Scale\nTransformer\nStar\n-\nTransformer Transformer Transformer\nw/ halting unit\nTransformer\nw/ MI estimation\nTransformer\nw/ reconstruction\nApparel 86.5 88.7 91.9 91.6 91.4 91.8\nBaby 86.3 88.0 88.8 88.1 90.6 88.4\nBooks 87.8 86.9 89.5 88.3 89.6 89.5\nCamera 89.5 91.8 91.8 92.7 93.8 92.9\nDvd 86.5 87.4 88.3 91.7 91.4 91.5\nElectronics 84.3 87.2 90.8 89.3 90.6 90.2\nHealth 86.8 89.1 91.7 91.3 91.6 88.4\nImdb 85.0 85.0 88.3 89.8 89.5 90.6\nKitchen 85.8 86.0 87.6 88.1 89.2 86.8\nMag\nazines 91.8 91.8 94.2 94.8 94.6 94.7\nMr 78.3 79.0 83.7 81.6 82.3 82.5\nMusic 81.5 84.7 89.9 89.3 89.5 87.2\nSoftw\nare 87.3 90.9 91.2 92.9 92.3 93.8\nSports 85.5 86.8 87.1 89.2 88.4 89.8\nT\noys 87.8 85.5 89.7 90.3 90.9 89.7\nV\nideo 88.4 89.3 93.4 94.3 93.1 93.5\nAvg 86.2 87.4 89.9 90.2 90.5 90.1\nTable 2: Accuracy scores (%) on the Amazon-16 datasets. Best results on each dataset are bold. The results of Multi-Scale\nTransformer (Guo et al. 2019b) is cited from the original paper, and other results are our implementations with several recent\nadvanced techniques (e.g., BERT initialization) under the uniﬁed setting.\nModels /\nDataset TREC MR\nSubj IMDB AG. DBP. Yelp P. Yelp F. Avg.\nRCRN (T\nay, Tuan, and Hui 2018) 96.20 –\n– 92.80 – – – – –\nCov\ne (McCann et al. 2017) 95.80 –\n– 91.80 – – – – –\nTe\nxt-CNN (Kim 2014) 93.60 81.50\n93.40 – – – – – –\nMulti-QT (Logesw\naran and Lee 2018) 92.80 82.40\n94.80 – – – – – –\nAdaSent (Zhao,\nLu, and Poupart 2015) 92.40 83.10\n95.50 – – – – – –\nCNN-MCFA\n(Amplayo et al. 2018) 94.20 81.80\n94.40 – – – – – –\nCapsule-B (Y\nang et al. 2018) 92.80 82.30\n93.80 – 92.60 – – – –\nDNC+CUW (Le,\nTran, and Venkatesh 2019) – –\n– – 93.90 – 96.40 65.60 –\nRegion-Emb\n(Qiao et al. 2018) – –\n– – 92.80 98.90 96.40 64.90 –\nChar-CNN\n(Zhang, Zhao, and LeCun 2015) – –\n– – 90.49 98.45 95.12 62.05 –\nDPCNN (Johnson\nand Zhang 2017) – –\n– – 93.13 99.12 97.36 69.42 –\nDRNN (W\nang 2018) – –\n– – 94.47 99.19 97.27 69.15 –\nSWEM-concat (Shen\net al. 2018) 92.20 78.20\n93.00 – 92.66 98.57 95.81 63.79 –\nStar-T\nransformer (Guo et al. 2019a) y 93.00 79.76\n93.40 94.52 92.50 98.62 94.20 63.21 88.65\nBERT\n(Devlin et al. 2019) – –\n– 95.49 – 99.36 98.11 70.68 –\nXLNet (Y\nang et al. 2019) – –\n– 96.80 95.55 99.40 98.63 72.95 –\nTransformer\n(Vaswani et al. 2017) y 96.00 83.75\n96.00 95.58 95.13 99.22 98.09 69.80 91.69\nw/ Halting\nunit (Elbayad et al. 2020) y 95.80 83.23\n96.00 95.80 95.50 99.30 98.25 69.75 91.70\nw/ MI\nestimation (ours) y 96.50 84.20 96.00\n96.72 95.90 99.32 98.10 72.98 92.46\nw/ Reconstruction\nestimation (ours) y 96.20 83.90 96.30 96.60\n95.65 99.25 98.00 69.58 91.93\nTable 3: Accuracy scores (%) on modestly sized and large-scaled datasets. ‘AG.’, ‘DBP.’, ‘Yelp P.’ and ‘Yelp F.’ are the abbre-\nviations of ‘AG’s News‘, ‘DBPedia’, ‘Yelp Polarity’ and ‘Yelp Full’, respectively.yis our implementations with several recent\nadvanced techniques and analogous parameter sizes. ‘Transformer’ is initialized byBERTbase with 12 ﬁxed layers.\nResults on Larger Benchmarks. Although the Amazon-\n16 benchmark is challenging, its small data size makes the\nresults prone to be unstable, therefore we conduct experi-\nments on larger benchmarks for a more convincing conclu-\nsion. In this paragraph, we only focus on the classiﬁcation\naccuracy listed in Table 3, and more detailed results about\ncomputing speed and model robustness will be discussed in\nthe next section.\nThe upper part of Table 3 lists several high-performance\nbaseline models. Their detailed descriptions are omitted\nhere. In terms of accuracy, our approaches achieve compa-\nrable performance with these state-of-the-art models. At the\nbottom part of Table 3, we ﬁnetune BERT as our strong base-\nline model. Results show that this baseline model performs\non par with the state-of-the-art XLNet (Yang et al. 2019).\nThen we build a halting unit at the bottom of the baseline\nmodel under the same setup with the depth-adaptive Trans-\nformer. Results show that applying a halting unit has no\n13428\nFigure 3: Accuracy scores (a) and speed (b) for each model on IMDB when N 2[1;12]. The solid line indicates the mean\nperformance, and the size of the colored area indicates variance (used to measure robustness). ‘speed’ is the number of samples\ncalculated in ten-second on one Tesla P40 GPU with the batch size of 1.\nobvious impact on accuracy. The last two rows list results\nof our estimation approaches, where the MI-based approach\nbrings in consistent improvements over the baseline and the\nTransformer w/ a halting unit, by +0.77% and +0.76% on av-\nerage, respectively. We speculate the improvements mainly\ncome from the additional deep supervision and regulariza-\ntion effect of the MI-based approach. In contrast, the recon-\nstruction based approach only show improvements over the\nbaseline model (+0.24%) and the Transformer w/ a halting\nunit (+0.23%) by a small margin.\nAnalysis\nWe conduct analytical experiments on the modestly sized\nIMDB to offer more insights and elucidate the properties of\nour approaches.\nEffect of the Maximum Number of Layers\nFirstly, we train several ﬁxed-layer Transformers with N\nranging from one to twelve, and then build a halting unit\non the above Transformers to dynamically adjust the actual\nnumber of layers to conduct. Meanwhile, we respectively\nutilize our two approaches on the ﬁxed-layer Transformer\nto activate dynamic layers. Note that each model is trained\nwith different random initialization three times and we re-\nport the mean and variance. Here, we take the variance value\nto measure the robustness against the random initialization\nand different depth selections. As drawn in Figure 3, solid\nlines are the mean performance, and the size of the colored\nareas indicate variances.\nAccuracy and Robustness. Results of accuracy and ro-\nbustness are drawn in Figure 3 (a). In the lower layers\n(N 2[1;6]), as the searching space for depth selection is\nsmall, the depth-adaptive models perform worse than the\nTransformer baseline. In contrast, when N 2 [6;12], the\nFigure 4: Speed for each model on IMDB whenbatchsize2\n[1;15]. The solid line indicates the mean performance, and\nthe size of the colored area indicates variance (used to mea-\nsure robustness). ‘speed’ is the number of samples calcu-\nlated in ten seconds on one Tesla P40 GPU.\ndepth-adaptive models come up with the baseline. Due to the\nadditional depth supervision and the regularization effect,\nthe application of our approaches can further signiﬁcantly\nimprove accuracy and robustness over both the Transformer\nand w/ a halting unit. (green and blue lines vs. purple line in\nFigure 3 (a))\nSpeed and Robustness. Figure 3 (b) shows the speed and\nrobustness of each model. The speed of vanilla Transformer\nalmost linearly decays with the growth of the number of\n13429\n2\n4\n6\ndepth\nI have anticipatedthe variousSci-ﬁ and thriller\nMI based estimation Reconstruction based\nestimation\nFigure 5: The histogram of the depth distribution of a case\nfrom IMDB, which is estimated by our approaches.\nlayers. As N 2[1;3], due to the additional prediction for\ndepths, models w/ halting unit runs a bit slower than the\nbaseline. However, the superiority of adaptive depths be-\ncomes apparent with the growth of the number of layers. In\nparticular, as N = 12, the model w/ halting run 3.4x faster\nthan the ﬁxed-layer baseline (pure lines vs. red line in Fig-\nure 3 (b)). As our approaches free the dependency for depth\nprediction and can further speed up the model, both of our\napproaches run about 7x faster than the ﬁxed-layer baseline\n(green and blue lines vs. red line in Figure 3 (b)). In ad-\ndition, our approaches perform more robust in the term of\nspeed gains than the Transformer w/ a halting unit.\nSpeed on Different Batch Sizes\nThe depth-adaptive models conduct dynamic computations\nat each word position, and thus the actually activated depths\nare decided by the maximal depth value. As a result, when\nthe batch size gets larger, the ﬁnal activated depth may po-\ntentially become larger as well, which may hurt the effec-\ntiveness of depth-adaptive models. In this section, we ﬁx the\nmaximal number of layer N to 12, and then compare the\nspeed of each model. As shown in Figure 4, the speed gain\nof the depth-adaptive models (green, blue and purple lines in\nFigure 4) grows slower than the vanilla Transformer (red line\nin Figure 4). However, the absolute speed of depth-adaptive\nmodels is still much faster than the vanilla Transformer. We\nleave the further improvement of depth-adaptive models on\nlarger batch sizes to future works.\nEffect of Penalty Factor \u0015\nIf no constraints are applied on the depth selection, the re-\nconstruction loss based approach tends to choose a layer\nas deep as possible, and thus an extra penalty factor \u0015 is\nnecessary to encourage a lower choice. We simply search\n\u0015 2[0;0:2], and ﬁnally set it to 0.1 for a good accuracy-\nspeed trade-off. The detailed results are list in Table 4.\nCase Study\nWe choose a random sentence from the IMDB dataset, and\nshow the estimated depths outputted by both approaches in\nFigure 5 (upper part). We observe that the MI-based estima-\ntion tends to assign a smaller number of depths for opin-\n\u0015 0 0.05\n0.10 0.15 0.20\naccuracy 96.54 96.31\n96.55 96.27 96.29\nspeed 23 33\n48 54 58\naverage depth 9.5 6.3\n4.5 3.9 3.6\nTable 4: Effect of penalty factor\u0015. The deﬁnition of ‘speed’\nis same as that in Figure 3. ‘average depth’ is the average\npredicted depth of words in test set.\nion words, e.g., ‘anticipated’ and ‘thriller’. While the re-\nconstruction loss based estimation is prone to omit common\nwords, e.g., ‘and’.\nRelated Work\nOur work is mainly inspired by ACT (Graves 2016), and\nwe further explicitly train the halting union with the super-\nvision of estimated depths. Unlike Universal Transformer\n(Dehghani et al. 2019) iteratively applies ACT on the same\nlayer, we dynamically adjust the amount of both computa-\ntion and model capacity.\nA closely related work named ‘Depth-Adaptive Trans-\nformer’ (Elbayad et al. 2020) uses task-speciﬁc loss as an\nestimation of depth selection. Our approaches are different\nfrom it in three major aspects: 1) We get rid of the halting\nunit and remove the additional computing cost for depths,\nthus yield a faster depth-adaptive Transformer; 2) our MI-\nbased estimation does not need to train an extra module, and\nis highly efﬁcient in computation; 3) our reconstruction loss\nbased estimation is unsupervised, and can be easily applied\non general unlabeled texts. Another group of works also\naims to improve efﬁciency of neural network through re-\nducing the entire layers, e.g., DynaBERT (Hou et al. 2020),\nLayerDrop (Fan, Grave, and Joulin 2019) and MobileBERT\n(Sun et al. 2020). In contrast, our approaches perform adap-\ntive depths in the ﬁne-grained word level.\nConclusion\nWe get rid of the halting unit and remove the additional\ncomputing cost for depths, thus yield a faster depth-adaptive\nTransformer. Speciﬁcally, we propose two effective ap-\nproaches 1) mutual information based estimation and 2)\nreconstruction loss based estimation. Experimental results\nconﬁrm that our approaches can speed up the vanilla Trans-\nformer (up to 7x) while preserving high accuracy. Moreover,\nwe signiﬁcantly improve previous depth-adaptive models in\nterms of accuracy, efﬁciency, and robustness. We will fur-\nther explore the potential improvement of the depth-adaptive\nTransformer when facing larger batch size in future work.\nAcknowledgments\nThe research work described in this paper has been sup-\nported by the National Key R&D Program of China\n(2020AAA0108001) and the National Nature Science Foun-\ndation of China (No. 61976015, 61976016, 61876198 and\n61370130). This work was done when Yijin Liu was intern-\ning at Pattern Recognition Center, WeChat AI, Tencent Inc,\nChina. Jinan Xu is the corresponding author of the paper.\n13430\nReferences\nAmplayo, R. K.; Lee, K.; Yeo, J.; and Hwang, S.-w. 2018.\nTranslations as additional contexts for sentence classiﬁca-\ntion. In Proceedings of IJCAI.\nDehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and\nŁukasz Kaiser. 2019. Universal Transformers. In Proceed-\nings of ICLR.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of NAACL.\nElbayad, M.; Gu, J.; Grave, E.; and Auli, M. 2020. Depth-\nAdaptive Transformer. In ICLR.\nFan, A.; Grave, E.; and Joulin, A. 2019. Reducing Trans-\nformer Depth on Demand with Structured Dropout. In In-\nternational Conference on Learning Representations.\nFigurnov, M.; Collins, M. D.; Zhu, Y .; Zhang, L.; Huang, J.;\nVetrov, D.; and Salakhutdinov, R. 2017. Spatially Adaptive\nComputation Time for Residual Networks. In Proceedings\nof CVPR.\nGraves, A. 2016. Adaptive computation time for recurrent\nneural networks. arXiv .\nGuo, Q.; Qiu, X.; Liu, P.; Shao, Y .; Xue, X.; and Zhang, Z.\n2019a. Star-transformer. In Proceedings of NAACL.\nGuo, Q.; Qiu, X.; Liu, P.; Xue, X.; and Zhang, Z. 2019b.\nMulti-Scale Self-Attention for Text Classiﬁcation. arXiv\npreprint arXiv:1912.00544.\nHou, L.; Shang, L.; Jiang, X.; and Liu, Q. 2020. DynaBERT:\nDynamic BERT with Adaptive Width and Depth. arXiv\npreprint arXiv:2004.04037.\nHuang, G.; Chen, D.; Li, T.; Wu, F.; van der Maaten, L.;\nand Weinberger, K. Q. 2017. Multi-scale dense networks\nfor resource efﬁcient image classiﬁcation. arXiv preprint\narXiv:1703.09844 .\nJohnson, R.; and Zhang, T. 2017. Deep Pyramid Convolu-\ntional Neural Networks for Text Categorization. InProceed-\nings of ACL.\nKim, Y . 2014. Convolutional neural networks for sentence\nclassiﬁcation. In Proceedings of EMNLP.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv .\nLe, H.; Tran, T.; and Venkatesh, S. 2019. Learning to Re-\nmember More with Less Memorization. In Proceedings of\nICLR.\nLi, X.; and Roth, D. 2002. Learning question classiﬁers. In\nProceedings of COLING, 1–7.\nLiu, P.; Qiu, X.; and Huang, X. 2017. Adversarial Multi-task\nLearning for Text Classiﬁcation. In Proceedings of ACL.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLogeswaran, L.; and Lee, H. 2018. An efﬁcient framework\nfor learning sentence representations. arXiv .\nMaas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y .;\nand Potts, C. 2011. Learning word vectors for sentiment\nanalysis. In Proceedings of ACL, 142–150.\nMcCann, B.; Bradbury, J.; Xiong, C.; and Socher, R. 2017.\nLearned in Translation: Contextualized Word Vectors. In\nProceedings of NeurIPS.\nPang, B.; and Lee, L. 2004. A Sentimental Education: Senti-\nment Analysis Using Subjectivity Summarization Based on\nMinimum Cuts. In Proceedings of ACL.\nPang, B.; and Lee, L. 2005. Seeing Stars: Exploiting Class\nRelationships for Sentiment Categorization with Respect to\nRating Scales. In Proceedings of ACL.\nPascanu, R.; Mikolov, T.; and Bengio, Y . 2013. On the difﬁ-\nculty of training recurrent neural networks. In Proceedings\nof ICML.\nPeng, H.; Long, F.; and Ding, C. 2005. Feature selection\nbased on mutual information criteria of max-dependency,\nmax-relevance, and min-redundancy. IEEE Transactions\non pattern analysis and machine intelligence27(8): 1226–\n1238.\nQiao, C.; Huang, B.; Niu, G.; Li, D.; Dong, D.; He, W.; Yu,\nD.; and Wu, H. 2018. A New Method of Region Embedding\nfor Text Classiﬁcation. In Proceedings of ICLR.\nShen, D.; Wang, G.; Wang, W.; Min, M. R.; Su, Q.; Zhang,\nY .; Li, C.; Henao, R.; and Carin, L. 2018. Baseline needs\nmore love: On simple word-embedding-based models and\nassociated pooling mechanisms. In Proceedings of ACL.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: A Simple Way to Prevent\nNeural Networks from Overﬁtting. The journal of machine\nlearning research.\nSun, Z.; Yu, H.; Song, X.; Liu, R.; Yang, Y .; and Zhou, D.\n2020. MobileBERT: a Compact Task-Agnostic BERT for\nResource-Limited Devices. In ACL.\nTay, Y .; Tuan, L. A.; and Hui, S. C. 2018. Recurrently Con-\ntrolled Recurrent Networks. In Proceedings of NeurIPS.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS.\nWang, B. 2018. Disconnected Recurrent Neural Networks\nfor Text Categorization. In Proceedings of ACL.\nWang, X.; Yu, F.; Dou, Z.-Y .; Darrell, T.; and Gonzalez, J. E.\n2018. SkipNet: Learning Dynamic Routing in Convolutional\nNetworks. In The European Conference on Computer Vision\n(ECCV).\nYang, M.; Zhao, W.; Ye, J.; Lei, Z.; Zhao, Z.; and Zhang, S.\n2018. Investigating Capsule Networks with Dynamic Rout-\ning for Text Classiﬁcation. InProceedings of EMNLP. Brus-\nsels, Belgium.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. XLNet: Generalized Autoregres-\nsive Pretraining for Language Understanding. In NIPS.\n13431\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level\nconvolutional networks for text classiﬁcation. In Advances\nin neural information processing systems, 649–657.\nZhao, H.; Lu, Z.; and Poupart, P. 2015. Self-adaptive hierar-\nchical sentence model. In Proceedings of IJCAI.\n13432"
}