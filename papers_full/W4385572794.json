{
  "title": "Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset",
  "url": "https://openalex.org/W4385572794",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2329025544",
      "name": "Anton Eklund",
      "affiliations": [
        "Umeå University"
      ]
    },
    {
      "id": "https://openalex.org/A2100283520",
      "name": "Mona Forsman",
      "affiliations": [
        "Abdelmalek Essaâdi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W3144750446",
    "https://openalex.org/W151377110",
    "https://openalex.org/W2803437449",
    "https://openalex.org/W3036644138",
    "https://openalex.org/W3100651519",
    "https://openalex.org/W4221142221",
    "https://openalex.org/W2250533720",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4287100616",
    "https://openalex.org/W2072644219",
    "https://openalex.org/W2159426623",
    "https://openalex.org/W4224315235",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2171343266",
    "https://openalex.org/W3155457426"
  ],
  "abstract": "Topic models are powerful tools to get an overview of large collections of text data, a situation that is prevalent in industry applications. A rising trend within topic modeling is to directly cluster dimension-reduced embeddings created with pretrained language models. It is difficult to evaluate these models because there is no ground truth and automatic measurements may not mimic human judgment. To address this problem, we created a tool called STELLAR for interactive topic browsing which we used for human evaluation of topics created from a real-world dataset used in industry. Embeddings created with BERT were used together with UMAP and HDBSCAN to model the topics. The human evaluation found that our topic model creates coherent topics. The following discussion revolves around the requirements of industry and what research is needed for production-ready systems.",
  "full_text": "Proceedings of EMNLP 2022 Industry Track, pages 645–653\nDecember 9–11, 2020. ©2022 Association for Computational Linguistics\n645\nTopic Modeling by Clustering Language Model Embeddings: Human\nValidation on an Industry Dataset\nAnton Eklund\nUmeå University\nAdlede AB\nUmeå, Sweden\nanton.eklund@cs.umu.se\nMona Forsman\nAdlede AB\nUmeå, Sweden\nmona.forsman@adlede.com\nAbstract\nTopic models are powerful tools to get an\noverview of large collections of text data, a\nsituation that is prevalent in industry applica-\ntions. A rising trend within topic modeling is\nto directly cluster dimension-reduced embed-\ndings created with pretrained language models.\nIt is difficult to evaluate these models because\nthere is no ground truth and automatic measure-\nments may not mimic human judgment. To\naddress this problem, we created a tool called\nSTELLAR for interactive topic browsing which\nwe used for human evaluation of topics cre-\nated from a real-world dataset used in industry.\nEmbeddings created with BERT were used to-\ngether with UMAP and HDBSCAN to model\nthe topics. The human evaluation found that\nour topic model creates coherent topics. The\nfollowing discussion revolves around the re-\nquirements of industry and what research is\nneeded for production-ready systems.\n1 Introduction\nContextual advertising is a rising solution for ad\nplacement on the Internet, which avoids the need\nfor user data and cookies. However, to find good\ncontexts for a placement, the content of a page\nneeds to be known and classified as a useful ad-\nvertising context. News media are dependent on\nadvertising for funding their work and is therefore\nan important market for contextual advertising. The\nnews is constantly changing, which makes it diffi-\ncult to create classifiers that can catch and catego-\nrize new articles. A possible way to solve this is to\nuse unsupervised topic models, which are powerful\ntools to structure large collections of text data.\nTraditional approaches to do topic modeling are\nstochastic, the most well-known one being Latent\nDirichlet Allocation (LDA) (Blei et al., 2003). The\nproblem with stochastic approaches is that they are\nslow and getting increasingly more difficult to in-\ntegrate with modern language models (Zhao et al.,\n2021; Vayansky and Kumar, 2020). To tackle this,\nNeural Topic Models (NTMs), which leverage the\npower of neural networks to create topic models,\nare becoming increasingly popular. The techniques\nof our particular interest are Neural Topic Model-\ning by Clustering Embeddings (NTM-CE). We de-\nfine NTM-CE as models that use a distance-based\nclustering algorithm on the document embeddings\ncreated with a Pretrained Language Model (PLM).\nPerforming topic modeling by directly clustering\nembeddings has been shown to perform compara-\nbly to LDA (Sia et al., 2020), or better (Meng et al.,\n2022), and is claimed to create more coherent top-\nics than other types of NTMs (Zhang et al., 2022).\nThese results make the models attractive for use\nwithin industry as analytical tools, and hopefully as\npart of an automatic classification pipeline. Here,\nwe evaluate a modified BERTopic (Grootendorst,\n2022) on an industry dataset consisting of unstruc-\ntured news articles from a brief period of time.\nThe evaluation of topic models is not trivial\nsince the lack of annotated datasets makes meth-\nods like the F1 score not applicable. It also is\ncounterintuitive to our purpose of having flexible\ntopic models if they are evaluated through static\ndataset categories. Instead, the field has gravitated\nto automatic measurements which do not require\na ground truth like the Normalized Pointwise Mu-\ntual Information (NPMI) (Bouma, 2009). NPMI\nis a popular way to evaluate topic models, as it is\nclaimed to emulate human judgment of topic co-\nherence (Lau et al., 2014). However, an alarming\nstudy by Hoyle et al. (2021) argues that automatic\nevaluation with NPMI cannot emulate human judg-\nment, a fact which topic modeling papers usually\nrely on to make their claims. From an industry\nperspective, it is important to be able to trust and\nvalidate topic models before they can be used in\na production system. Additionally, to the best of\nour knowledge, there are no studies that use hu-\nman evaluation for NTM-CE. Therefore, this paper\npresents a human expert evaluation using our novel\n646\ntool Systematic Topic Evaluation Leveraging Lists\nof ARticles (STELLAR), which is described fur-\nther in Section 4. The human expert evaluation is\ndescribed in Section 5.\nA problem that remains for topic models, includ-\ning NTM-CE, is the interpretability of the resulting\ntopics. This is usually addressed by selecting a\nset of keywords deemed to be the most descrip-\ntive of the topic. The words closest to the centroid\nof a cluster can be used as descriptors as seen in\nBianchi et al. (2021). Another solution by Grooten-\ndorst (2022) is to use a class-based term weighting\nto extract keywords. The question remains if, and\nto what extent, human evaluators would find the\nkeywords descriptive enough for the overall topic.\nHence, we add an assessment of the topic descrip-\ntion using a simple four-point scale.\nIn this paper, we demonstrate NTM-CE in the\nindustry setting of contextual advertisement and\ndo a human expert evaluation of the topic model\nusing our new STELLAR tool. The NTM-CE is\nan implementation of BERTopic, described in Sec-\ntion 3, that has been applied to a news data set,\ndescribed in Section 2. The STELLAR tool for\ntopic evaluation is described in Section 4, with fur-\nther explanation of the human expert evaluation in\nSection 5. The results of the human evaluation are\npresented in Section 6, with further discussions of\nthe process, the results, and future improvements\nin Section 7.\n2 Data\nThe dataset used for this study is a unique col-\nlection of publicly available English online news\narticles. The collection consists of 10000 articles\nfrom 58 publishers collected between 2022-05-29\nand 2022-06-22. The lengths of the articles range\nfrom 501 to 99000 characters with a mean length\nof 3052. 9753 articles are shorter than 10000 char-\nacters. Except for removing articles shorter than\n500 characters, no other filtering of the articles was\napplied. This makes the dataset have the same char-\nacteristics as the news from the sampled weeks,\nwith topics such as the Queen’s jubilee, Cancelled\nflights, and Formula 1 racing taking up a dispro-\nportionally large part of the content. These are\nexamples of large but brief news topics that will be\nirrelevant in a few weeks, illustrating the dynamic\nnature of the news cycle and why static classifiers\nare of limited use.\n3 Topic Modeling Pipeline\nOur NTM-CE approach adopts the pipeline of\nBERTopic (Grootendorst, 2022) and CETopic\n(Zhang et al., 2022) by using the sequence pre-\nsented in Figure 1. The class-based TF-IDF of\nGrootendorst (2022) is used to create keywords for\nthe topics. The components are described in more\ndetail in the following section.\nFigure 1: The topic modeling pipeline starts with vec-\ntorization using BERT, followed by dimension reduc-\ntion using UMAP, and ends with clustering using HBD-\nSCAN.\nVectorization was performed with Transformer-\nbased (Vaswani et al., 2017) model BERT (Devlin\net al., 2019). It was chosen as it has been widely\nused in neural topic models and shown to per-\nform well (Grootendorst, 2022; Zhang et al., 2022;\nBianchi et al., 2021). Those models used Sentence-\nBERT from Reimers and Gurevych (2019). How-\never, to have the results less tied to a specific BERT\nmodel, the model in this project used the Hugging-\nFace base model 1 (768D, 12A, 12L) which was\nfine-tuned with Masked Language Modeling for\nthe task. As the final document embedding we used\nthe averaged token embeddings of hidden_state 1.2\nDimension reduction of high dimensional vec-\ntors is used to, among other things, reveal patterns\nin the data and reduce vector space noise. Tech-\nniques for dimension reduction come in two main\ncategories: dimensionality reduction based on ma-\ntrix factorization and based on neighbor graphs.\nIn this study, we used the neighbor graph method\nUMAP (McInnes et al., 2018) because it was re-\nported to be both faster and have better clustering\nquality than the popular t-SNE (Maaten and Hinton,\n2008) in the original article.\nClustering has a plethora of techniques but we\nsettled for HDBSCAN (Campello et al., 2013;\n1https://huggingface.co/docs/\ntransformers/model_doc/bert\n2Using the unconventional hidden_state 1 as the embed-\ndings was due to a bug in the code which was found after\nthe human evaluation was completed. However, the vector\nspace is similar to the embeddings from the more conventional\nlast_hidden_state. Therefore, for showcasing STELLAR, and\nexploring NTM-CE, we deemed using the embeddings from\nhidden_state 1 sufficient as the topic model still follows our\ndefinition of NTM-CE.\n647\nMcInnes and Healy, 2017) because of its successful\nuse in Grootendorst (2022) and its ability to dynam-\nically choose the number of topics and their size.\nWe used soft clustering for HDBSCAN, meaning\nthat all points in the vector space get assigned to\na cluster, which in turn means that no points were\nconsidered outliers.\n4 The STELLAR Topic Browser\nSystematic Topic Evaluation Leveraging Lists of\nARticles (STELLAR) is a tool developed to sim-\nplify the in-depth exploration of a topic model into\nwhat constitutes the topics rather than just consid-\nering the top keywords. The user wants to: 1) get a\nvisual overview of what topics exist and how they\nare related to each other, 2) be able to quickly iden-\ntify articles that do not fit into the topic, and 3) go\nbeyond keywords to validate a topic. To solve 1),\nthere is a list of topics with their description along-\nside a 3D vector space visualization. For 2) and 3),\nthe proposed solution is to allow the user to read\nthe title and keywords of the article and, if needed,\nto read the text body. The tool needs to be dynamic\nand interactive, as the user needs the flexibility to\nstudy topics freely and investigate different aspects\nwithout recreating the topic model and plots.\nSTELLAR, shown in Figure 2, was created as\nan application that can run directly in an Internet\nbrowser. Its purpose was to aid the user to perform\nactivities 1–3 as described above. It was imple-\nmented using the Python Flask3 library. The core\nfunctionalities are a topic list, an article list from\nthe chosen topic, a box for the article text body,\nand a 3D visualization made in Plotly4. For each\ntopic, the articles can be marked as not belonging\nto the topic and thus assist with the evaluation of\nthe topics. The 3D visualization shows the individ-\nual articles as points in the vector space reduced\nto three dimensions, which are color-coded to the\ncluster to which they belong. Hovering over the ar-\nticles shows the title and the keywords of the article\nwhich helps the user to get a better understanding\nof the cluster and search in different sections of\na cluster. The repository 5 for STELLAR was re-\nleased.\n3https://flask.palletsprojects.com/en/2.1.x/\n4https://plotly.com/python/\n5https://github.com/antoneklund/STELLAR\n5 Human Expert Evaluation\nThe first human evaluation of the topic model and\nSTELLAR was made by three experts (including\nthe first author) in the field of news space analysis.\nFrom now we call them evaluators. An evaluator\nis distinguished from an annotator in this work, by\nhaving the more complex task of contextualizing\na set of articles, finding patterns, and drawing the\nline for what is considered a topic by excluding ar-\nticles. In contrast, an annotator selects topics from\na list of choices and makes decisions for individual\narticles. We deem the evaluation task too complex\nto easily be crowd-sourced. We acknowledge that\nthree evaluators may be too few to make strong\nclaims about NTM-CE in general. However, for\nour purpose of demonstrating one NTM-CE model\non an industry dataset, as well as collecting sug-\ngested improvements for STELLAR, we deemed\nthe small expert group adequate.\nWe make a distinction between the termscluster,\ntopic, and focus topic. A cluster is a set of points\nthat are grouped by the clustering algorithm rep-\nresenting a group of article embeddings. A topic\nis a cluster of article embeddings combined with\nthe descriptive topic keywords. This is the output\nof the topic model. A focus topic is the topic of a\ncluster that the evaluators decide that most of the\narticle supports.\nEach evaluator received an individual dataset\nwith 20 randomly sampled articles per topic. Five\nof the articles per topic overlapped between the\nevaluators to calculate inter-rater agreement. The\ntask given was to systematically analyze each topic\nby reading the article titles and keywords, and read-\ning the article body if needed. Then, record their\nevaluation by 1) deciding on a focus topic with the\nhelp of suggestions 6, 2) record the id of articles\nthat did not belong to the focus topic, and 3) give\na score between 1 and 4 on how well they thought\nthe keywords given by the topic model reflected\nthe focus topic. The scores correspond to: 1=very\nbad, 2=bad, 3=good, and 4=very good. We chose\na four-point scale to force the evaluators to decide\nif the keywords are good or bad. The instructions\ngiven to the evaluators are specified in Appendix A.\nInter-rater agreement AG was used to assess the\n6The list of suggested topics was compiled by the first\nauthor which will introduce biases. However, evaluators were\nencouraged to record their custom focus topic if none of the\nitems on the list was satisfactory.\n648\nFigure 2: The user interface of STELLAR. The four core components are the 3D visualization, a list of topics with\nkeywords, a list of articles from chosen topics with keywords, and the article text of a chosen topic.\nreliability of the evaluators. It was calculated as:\nAG = nagree\nN (1)\nwhere nagree is the number of the agreeing deci-\nsions, and N is the number of possible decisions.\nTwo different types of decisions are aggregated.\nThe first type of decision is the focus topic of the\nclusters, called Agreement focus topic. The second\ntype of decision is for each overlapping article. The\nevaluator decides whether they belong to the focus\ntopic or not, called Agreement overlapping articles.\nTo assess whether the topic model produced\ncoherent topics. Our definition of evaluator-\ndetermined coherence score (Coh) is:\nCoh = 1− nmisplaced\nnarticles\n(2)\nwhere narticles is the number of articles evaluated\nin the topic and nmisplaced is the number of arti-\ncles that the evaluators found was misplaced into\nthat topic. We call a topic coherent if Coh ≥ 0.8.\nThis threshold at 80% is where we consider a topic\ncoherent enough for being useful in an industrial\napplication. Further, a topic that has at least one\nevaluator labeling it Incoherent will be considered\nincoherent, regardless of the opinions of other eval-\nuators.\nWe are aware that the judgment of how coherent\na topic is will depend on the individual experiences\nNr topics found 63\nNr coherent topics 52\nAverage Coh for coherent topics 96%\nArticles in coherent topics 57%\nAgreement focus topic 87%\n(including incoherent topics)\nAgreement focus topic for coherent topics 98%\nAgreement overlapping articles 95%\nKeywords describing topic 2.8\nTable 1: Statistics of the topic modeling and the human\nevaluation.\nand interests of every evaluator. However, the pur-\npose of the evaluation is not to find a ground truth\nof what is the focus topic, but rather to determine if\nthe articles presented by the model form a coherent\ntopic. If the evaluators draw the line on what consti-\ntutes a topic differently, we see that as a limitation\nof the model, and the reduction in coherence score\nis justified.\n6 Results\nThe topic modeling pipeline resulted in 63 clus-\nters with varying sizes as seen in Figure 3. The\nlargest one contains 3347 articles, and the smallest\nones are around 20. In total, 2367 of the 10000\narticles were manually analyzed. The evaluators\nagreed on 95% of their decision on overlapping\n649\nFigure 3: A pie chart illustrating the sizes and coherence of all topics. Light blue means that the topic is incoherent.\nDifferent shades of blue indicate how strong the coherence is based on the human evaluation. In the north-west part\nof the graph, the mixed wedges are Formula 1, Gambling, and Gardening.\narticles. The focus topics identified by the evalua-\ntors were [Court/Legal, Crime/Violence, Entertain-\nment, Epidemic, Food/Drink, Football, Formula 1,\nGambling, Gardening, Health, Incoherent, Money,\nPolitics, Royal, Science, Sports, Technology, Travel,\nUkraine War, Weather], a total of 20 focus topics.\nA focus topic can be assigned to multiple clusters.\nBroader focus topics such as Entertainment con-\ntain articles about gossip, celebrities, movies, and\nTV-series. The Sports topic is all sports excluding\nFootball and Formula 1. It is made up of tennis,\ncombat sports, golf, cricket, and rugby among the\nlarger ones.\nThe data from the human evaluation in Table 1\nshows that 52 out of 63 topics were over 80% co-\nherent. Topics that were determined coherent had\nan average coherence score of 96%. Those topics\ncontain 5653 of the articles, which is 57% of the\ndataset. A little less than half of the articles ended\nup in incoherent topics. The largest Incoherent\ncluster (see Figure 3) consists of shorter articles,\nwith an average length of 1500 characters. The fea-\ntures of Incoherent clusters will be further explored\nin Section 7.\nThe evaluators agree on the focus topic for 87%\nof topics. In topics where coherence is high, the\nevaluators agree on the focus topic for 98% of the\ntopics, that is, all topics except one. The disagree-\nments between the evaluators usually came from\nwhen one evaluator had chosen Incoherent while\nthe others had specified a topic.\nAnother common disagreement, important for\nunderstanding the difficulty of the topic modeling\nproblem is shown in Table 2. The focus topic was\nabout Weather, but one can find that the topic con-\nsisted of two subtopics, which we can call Fore-\ncasts and Hurricanes. One evaluator decided the\nfocus topic to be Forecastsand then continued to\nmark the articles about Hurricanes as misplaced.\nHowever, the other evaluators considered the focus\ntopic as Weather and thus fully coherent. Cases\nlike these, where one evaluator creates a more nar-\n650\nrow focus topic than the others, make up for much\nof the reduction in the total coherence score.\n7 Discussion\nThe topic model found 63 topics in which 20 focus\ntopics were identified. We interpret it as a good\npartitioning of the corpus, except for the fact that\nthe largest topic was labeled as Incoherent. The\noptimal number of topics found may vary greatly\nbetween corpora and the aim should not be to find\none cluster per focus topic from the focus topic\nlist, e.g. finding 20 clusters for this dataset. How-\never, for an analytical application in the industry,\nit would be advantageous to have a way to collect\nclusters with a similar focus topic into a larger col-\nlection. Whether that should be done with the vec-\ntor space distance or with other methods remains\nto be studied.\nThe human evaluation determined 52 out of 63\ntopics to be coherent, with an average coherency of\n96%. However, only 57% of the dataset ended up\nin coherent topics. One reason for this is the strict\nrequirement that all evaluators should agree on the\nfocus topic for a topic to be considered coherent.\nHowever, the foremost reason is that the largest\ntopic, which contained 3347 articles, was labeled\nIncoherent.\nA deeper inspection of the clusters of incoherent\ntopics gave interesting insights. The largest cluster\ncontained short articles with half the average length\nas the rest of the dataset. We assume they have\nbeen padded before the BERT vectorization and are\nclustered on the padding artifacts. An informal test\nto re-cluster this particular cluster was done to see\nif dividing it into smaller partitions would create\ncoherent clusters. However, these new clusters did\nnot create coherent topics either. Since we found\nlarge coherent clusters such as Football, as well\nas small Incoherent clusters, we believe that the\nclusters should not necessarily be as small nor as\nbalanced as possible, balancing being something\nMeng et al. (2022) emphasized. Rather, we think\nthat the dynamic properties of HDBSCAN could\nsuffice if guided by inputs from suitable vector\nspace statistics, and applied to a well-formed vector\nspace.\nFurther analysis of the Incoherent topics re-\nvealed patterns among the articles but not enough\nto make them coherent. Some of the topics con-\ntain articles on multiple focus topics. An example\nis a topic with the combination Real estate, Home\nstyling, and Tourist attractionsthat all describe nice\nenvironments but not in one coherent focus topic.\nOne topic has locally anchored articles, but about\ndifferent focus topics and locations. One of the top-\nics is dominated by first-person stories, however,\nthe focus topics differ, and hence it was incoherent\nin the evaluation. The same can be said for a topic\nwith very emotional content. These topics deserve\na deeper examination and understanding, as they\nhave themes that have a stylistic or emotional char-\nacter. Studying this remains future work since it\nwas not included in the evaluator instructions.\nAccording to the keyword evaluation, the key-\nwords describing the topics were on average posi-\ntive (> 2.5), yet not good (2.8). The overall posi-\ntive assessment was still slightly unexpected as the\nperception before the study was that the keywords\ndid not describe the focus topics well. One factor\naffecting the results might be that the evaluators\nhad a better understanding of what the keywords\nmean after reading the topic articles and therefore\nthought the keywords described the articles well.\nA more focused study on keywords for topic de-\nscriptions needs to be done to investigate this. Nev-\nertheless, since the description was not close to\nvery good (4), ways forward might be to find better\nkeyword extractors or other methods to describe\nthe topics. A preferred scenario for our industry\npurposes is a topic model where we trust that all\ntopics have Coh ≥ 80% and that the topic descrip-\ntions are clear enough for a human to determine\nthe focus topic without looking deeply into what\narticles are in the topic. An ideal scenario would be\nthat we can trust a system to automatically decide\nthe focus topic similar to human judgment.\nThe evaluators agreed on the focus topic for87%\nof the topics and also had an agreement of 95% for\noverlapping articles. The agreement on the focus\ntopic for coherent topics was almost perfect at98%,\nwhich means that it was almost always recogniz-\nable. However, as shown in Table 2, there are\ndifficulties even for humans to determine where\nto limit a focus topic. Then, can we expect topic\nmodels to do that for us? We expect topic models\nto be able to divide the articles into topics with a\nfocus reasonably well. However, for the contex-\ntual advertisement vital finesse of correctly finding\nnarrow or trendy focus topics, a human will still\nbe needed. An important addition for managing\ncontextual campaigns would be the possibility to\nanalyze topics over time in the style of Blei and\n651\nTopic 0 [spells, sunny, hurricane, forecast, cloudy, highs, rain, temperatures, weather,...]\nid: 777 1st of 2022, Hurricane Agatha heads for Mexico tourist towns\n[landfall, millimeters, mazunte, mexico, kph, storm, puerto, oaxaca, center, ...]\nid: 2510 Hurricane Agatha is first named storm of Atlantic season after hitting Mexico...\n[maximum, hurricanes, noaa, atlantic, storm, inches, southern, mexico, hurricane, ...]\nid: 2197 Met Office gives Scotland weather update for Queen’s Platinum Jubilee weekend\n[forecast, places, warmer, rain, unsettled, dry, drier, weather, spells, showers]\nid: 4899 The wait is over, Met Office has revealed Platinum Jubilee weather forecast\n[northern, drier, umbrella, western, dry, sunny, spells, showers, weather, sunshine]\nid: 5770 London weather: Exactly when temperatures in capital are expected to soar to 23C\n[20c, june, 13c, gentle, 11c, sunny, intervals, lows, highs, breeze]\nTable 2: Example of when the evaluators disagreed on the focus topic. One evaluator decided the focus topic to\nbe Forecastsand then continued to mark the articles about Hurricanes (on the top) as errors. However, the other\nevaluators decided that the whole topic was coherent as Weather.\nLafferty (2006) or Wang and McCallum (2006).\nThis is something that Grootendorst (2022) has\nbeen working on with BERTopic. Another obser-\nvation was that it would be beneficial if the time-\nconsuming analysis was only required to be done\nonce and then have systems detecting new topics\nemerging and disappearing.\nThe tool STELLAR, presented in this paper, was\ncreated to allow an evaluator to systematically eval-\nuate a topic model by reading the articles that make\nup a topic. The main purpose was to be able to\napply a credible coherence score on a topic model\nwhile using as little evaluator time as possible. We\nbelieve that STELLAR aids this purpose reason-\nably well. Since this is the first evaluation with\nSTELLAR, naturally, there are improvements to be\nmade both to the evaluation process and the tool\nitself. When doing human evaluation of topic mod-\nels, usually the concept of an intrusion task is used\nto identify how coherent a topic is (Chang et al.,\n2009; Hoyle et al., 2021). This task is not fully\ntransferable to our concept of coherence. However,\nwe believe the incorporation of ideas around the\nintrusion task would make STELLAR better.\nFinally, we believe that using the PLM not only\nallows for the topic models to stay relevant when\nnew language models are released but also creates\na more interpretable vector space for analysis since\none can observe what topics are related to each\nother with visual inspection. This human expert\nevaluation of NTM-CE concludes that the tech-\nnique is viable and has many attractive benefits.\nHowever, it has some limitations that need to be\naddressed before being used to its full potential as\nan automatic classifier.\n8 Conclusions\nIn this study, we applied Neural Topic Modeling\nby Clustering Embeddings (NTM-CE) made with\nBERT on an industry dataset of news articles. Our\nhuman evaluation of NTM-CE, done with our novel\nSTELLAR tool, agrees with previous studies of\nthe technique: coherent topics can be created by\nclustering embeddings from a pretrained language\nmodel. However, only 57% of the articles ended up\nin coherent topics. Inspection of incoherent topics\nrevealed them to consist of multiple focus topics,\nor have some other emotional or stylistic character-\nistic. Unraveling the workings of incoherent topics\nto increase the number of articles in coherent topics\nshows great opportunity for industry application.\nWith the STELLAR tool, we hope to keep improv-\ning on NTM-CE as a promising technique for the\nfuture.\nEthical Considerations\nThe dataset was scraped from public news sites\nof established publishers. No personal blogs were\nused. The names of the people who are written in\nthe articles are mentioned as public persons. We\ndo not view this as a privacy infringement. The\narticles are not redistributed.\nWe identified that our personal biases have an\nimpact on the outcome of the results. Examples\nare choosing the list of focus topics or determining\nwhen to limit a topic. In practice, those choices\nin turn might have an effect on what type of topic\nmodel is deployed in the end. A consideration\ncould be to include a more diverse group of ex-\npert evaluators. A model might be too generalizing\nand fail to identify topics that are associated with\nmarginalized groups or cultures, leading to technol-\nogy being catered to a homogeneous majority.\n652\nAcknowledgements\nWe thank Adrian Andreasson and Dusan Mitic for\nbeing expert evaluators, Frank Drewes and anony-\nmous reviewers for their insightful input, and the\nrest of the university and Adlede teams. This Ph.D.\nstudent is funded by the Swedish Foundation for\nStrategic Research, project id ID19-0055.\nReferences\nFederico Bianchi, Silvia Terragni, Dirk Hovy, Debora\nNozza, and Elisabetta Fersini. 2021. Cross-lingual\ncontextualized topic models with zero-shot learning.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1676–1683, Online.\nAssociation for Computational Linguistics.\nDavid M. Blei and John D. Lafferty. 2006. Dynamic\ntopic models. In ICML ’06: Proceedings of the 23rd\ninternational conference on Machine learning, pages\n113–120.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993–1022.\nGerlof Bouma. 2009. Normalized (pointwise) mutual\ninformation in collocation extraction. Proceedings\nof the Biennial GSCL Conference.\nRicardo J. G. B. Campello, Davoud Moulavi, and Jo-\nerg Sander. 2013. Density-based clustering based\non hierarchical density estimates. In Advances in\nKnowledge Discovery and Data Mining, pages 160–\n172, Berlin, Heidelberg. Springer Berlin Heidelberg.\nJonathan Chang, Sean Gerrish, Chong Wang, Jordan\nBoyd-Graber, and David Blei. 2009. Reading tea\nleaves: How humans interpret topic models. Ad-\nvances in neural information processing systems, 22.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794.\nAlexander Hoyle, Pranav Goel, Andrew Hian-Cheong,\nDenis Peskov, Jordan Boyd-Graber, and Philip\nResnik. 2021. Is automated topic model evalua-\ntion broken? the incoherence of coherence. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 34, pages 2018–2033. Curran Associates,\nInc.\nJey Han Lau, David Newman, and Timothy Baldwin.\n2014. Machine reading tea leaves: Automatically\nevaluating topic coherence and topic model quality.\nIn Proceedings of the 14th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 530–539, Gothenburg, Sweden.\nAssociation for Computational Linguistics.\nLaurens Van Der Maaten and Geoffrey Hinton. 2008.\nVisualizing high-dimensional data using t-SNE. Jour-\nnal of Machine Learning Research, 9:2579–2605.\nLeland McInnes and John Healy. 2017. Accelerated\nhierarchical density based clustering. In 2017 IEEE\nInternational Conference on Data Mining Workshops\n(ICDMW). IEEE.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas\nGroßberger. 2018. Umap: Uniform manifold ap-\nproximation and projection. Journal of Open Source\nSoftware, 3(29):861.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, and\nJiawei Han. 2022. Topic discovery via latent space\nclustering of pretrained language model representa-\ntions. In Proceedings of the ACM Web Conference\n2022, WWW ’22, page 3143–3152, New York, NY ,\nUSA. Association for Computing Machinery.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nSuzanna Sia, Ayush Dalmia, and Sabrina J. Mielke.\n2020. Tired of topic models? clusters of pretrained\nword embeddings make for fast and good topics too!\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1728–1736, Online. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nIke Vayansky and Sathish Kumar. 2020. A review\nof topic modeling methods. Information Systems,\n94:101582.\nXuerui Wang and Andrew McCallum. 2006. Topics\nover time: A non-markov continuous-time model\nof topical trends. In Proceedings of the 12th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining , KDD ’06, page\n424–433, New York, NY , USA. Association for Com-\nputing Machinery.\n653\nZihan Zhang, Meng Fang, Ling Chen, and Moham-\nmad Reza Namazi Rad. 2022. Is neural topic mod-\nelling better than clustering? an empirical study on\nclustering with contextual embeddings for topics. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3886–3893, Seattle, United States. Association\nfor Computational Linguistics.\nHe Zhao, Dinh Phung, Viet Huynh, Yuan Jin, Lan Du,\nand Wray Buntine. 2021. Topic modelling meets\ndeep neural networks: A survey. arXiv preprint\narXiv:2103.00498.\nA Human Evaluation Protocol\nInstructions for filling in Table\nFor each Topic in Topics List:\n1. Click on the Topic in the Topic list.\n2. Notice the keywords describing the Topic.\n3. Read the article titles and keywords.\n4. Click on the article to read the body if not\nclear what the topic is about.\n5. Choose the main topic from the list. A topic\nin the list can be chosen multiple times.\n(a) If you can’t find a topic that includes 50%\nof the articles, then choose “no topic”7.\n(b) If you don’t agree with any of the topics\nin the list, write “custom” and then in the\nnotes write your custom topic. Please\nmake suggestions so that it is not only\nmy biases determining the categories.\n6. Write down the article id for articles that do\nnot belong to the topic.\n7. Write on a scale (1-4) if you think the key-\nwords are a good representation of the topic.\n1=bad, 2=sort of bad, 3=sort of good, 4=good.\nWhile doing the task. Write notes of interesting\nthings that you reflect over. Also, make general\nnotes about what improvements that can be made\nto the tool.\n7We have translated ’No topic’ to ’Incoherent’ when writ-\ning the article.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8022050261497498
    },
    {
      "name": "Cluster analysis",
      "score": 0.6864626407623291
    },
    {
      "name": "Dimension (graph theory)",
      "score": 0.6292743682861328
    },
    {
      "name": "Language model",
      "score": 0.6091822385787964
    },
    {
      "name": "Topic model",
      "score": 0.5746637582778931
    },
    {
      "name": "Ground truth",
      "score": 0.566411554813385
    },
    {
      "name": "Data science",
      "score": 0.49851393699645996
    },
    {
      "name": "Data modeling",
      "score": 0.42095568776130676
    },
    {
      "name": "Human Dimension",
      "score": 0.4116343855857849
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38466066122055054
    },
    {
      "name": "Information retrieval",
      "score": 0.3701271414756775
    },
    {
      "name": "Natural language processing",
      "score": 0.3620823621749878
    },
    {
      "name": "Database",
      "score": 0.1857069730758667
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Human rights",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}