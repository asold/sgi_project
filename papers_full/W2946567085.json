{
  "title": "Adaptive Attention Span in Transformers",
  "url": "https://openalex.org/W2946567085",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2035043562",
      "name": "Sainbayar Sukhbaatar",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2114720862",
      "name": "Edouard Grave",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1882694979",
      "name": "Piotr Bojanowski",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2512114774",
      "name": "Armand Joulin",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2740984755",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2556046966"
  ],
  "abstract": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n331\nAdaptive Attention Span in Transformers\nSainbayar Sukhbaatar Edouard Grave Piotr Bojanowski Armand Joulin\nFacebook AI Research\n{sainbar,egrave,bojanowski,ajoulin}@fb.com\nAbstract\nWe propose a novel self-attention mechanism\nthat can learn its optimal attention span. This\nallows us to extend signiﬁcantly the maximum\ncontext size used in Transformer, while main-\ntaining control over their memory footprint\nand computational time. We show the effec-\ntiveness of our approach on the task of charac-\nter level language modeling, where we achieve\nstate-of-the-art performances on text8 and\nenwiki8 by using a maximum context of 8k\ncharacters.\n1 Introduction\nLanguage models are at the core of many NLP\napplications, like machine translation or dialogue.\nRecently, much progress has been made by a new\nneural network called Transformer (Vaswani et al.,\n2017). Part of its success is due to its ability to\ncapture long term dependencies. This is achieved\nby taking long sequences as inputs and explicitly\ncompute the relations between every token via a\nmechanism called the “self-attention” layer (Al-\nRfou et al., 2019).\nWhile this layer allows for information to prop-\nagate across long distances, it has a computa-\ntional and memory cost that scales quadratically\nwith the size of the input sequence. As a con-\nsequence, Transformers hardly scale to sequences\nof more than a thousand tokens. This is partic-\nularly problematic in the case of character level\nlanguage modeling where dependencies are often\nspread over a few thousands time steps.\nIn this work, we propose an alternative to the\nself-attention layer to reduce the computational\nburden of a Transformer. Our layer learns its op-\ntimal context size, resulting in a network where\neach attention layer gathers information on their\nown context. In practice, we observe that this\nleads to Transformer with small context in the low-\nlevel layers and very large ones for the last lay-\ners. With this modiﬁcation, we are able to scale\ninput sequences to more than 8k tokens with no\nloss of performance, nor additional computational\nor memory cost. We validate our approach on the\ntask of character level language modeling where\nwe reach state-of-the-art performances while re-\nducing the number of FLOPS. The code to repro-\nduce our results is publicly available1.\n2 Approach\n2.1 Sequential transformer network\nLanguage modeling is the problem of assigning a\nprobability to a sequence of tokens (w1,...,w T ):\nP(w1,...,w T ) =\nT∏\nt=1\nP(wt |wt−1,...,w 1).\nRecent progress was made with a new auto-\nregressive model called Sequential Trans-\nformer (Vaswani et al., 2017). A Transformer is\nmade of a sequence of layers that are composed of\na block of parallel self-attention layers followed\nby a feedforward network. We refer to Vaswani\net al. (2017) for the details on the structure. In this\npaper, we make a couple of modiﬁcations to the\nTransformer model: we use the relative position\nembeddings of Shaw et al. (2018) and the caching\nmechanism of Dai et al. (2019) to speed up the\ntrain and test time.\nSelf-attention layer. A core mechanism of a\ntransformer network is the self-attention layer,\nwhich consists of multiple attention heads work-\ning in parallel. Each attention head applies the at-\ntention mechanism of Bahdanau et al. (2015) to its\nown input. Given a token tin a sequence, the head\n1https://github.com/facebookresearch/\nadaptive-span\n332\n−100 −80 −60 −40 −20\nContext\n0.000\n0.002\n0.004\nAttention\nHead A\nHead B\nFigure 1: Attention patterns of two different heads of\na standard Transformer. The two patterns are quali-\ntatively different: Head A utilizes recent steps, while\nHead B has uniform attention over the context.\nﬁrst computes similarities with its past, i.e., any\ntoken rin the span [t−S,t):\nstr = x⊤\nt W⊤\nq (Wkxr + pt−r) , (1)\nwhere Wk and Wq are the “key” and “query” ma-\ntrices, and pt−r is the relative position embedding.\nThe attention weights are then obtained by apply-\ning a softmax function on these similarities:\natr = exp (str)∑t−1\nq=t−S exp (stq)\n, (2)\nFinally, the head outputs a vector yt by taking the\naverage of the past representations weighted by\ntheir attention weights:\nyt =\nt−1∑\nr=t−S\natrWvxr, (3)\nwhere Wv is called the “value” matrix. Out-\nputs from different heads are then concatenated\ntogether and multiplied by an output matrix Wo\nbefore feeding to the next layer.\nSimilar to the memory access mechanisms\nof Sukhbaatar et al. (2015), it pulls information\nfrom the past to update the current token repre-\nsentation. Repeating this mechanism in consecu-\ntive layers allows for information to ﬂow over long\ndistances. However, for each input token, each at-\ntention head scales linearly in memory and time in\nthe context size, or attention span. There are typ-\nically 12 layers with 8 heads each that processes\n512 tokens simultaneously. This drastically limits\nthe maximum attention span used in Transformers.\n2.2 Adaptive attention span\nEach attention head of a Transformer shares the\nsame attention span S. This assumes that every\nhead requires the same span to form its represen-\ntation. As shown in Figure 1, this assumption does\nnot hold in the context of character level language\nx\nmz(x)\n1\nz z+ R\nFigure 2: The soft mask as a function of the distance.\nmodeling: some heads (e.g., Head A) focus on the\nrecent history, while others take information from\nthe whole available context (e.g., Head B). In this\nsection, we propose to learn the attention span of\neach head independently to reduce their computa-\ntional and memory cost.\nFor each head, we add a masking function to\ncontrol for the span of the attention. A masking\nfunction is a non-increasing function that maps a\ndistance to a value in [0,1]. We take the following\nsoft masking function mz parametrized by a real\nvalue zin [0,S]:\nmz(x) = min\n[\nmax\n[1\nR(R+ z−x) ,0\n]\n,1\n]\n,\nwhere Ris a hyper-parameter that controls its soft-\nness. This soft masking function is inspired by Jer-\nnite et al. (2017). In Figure 2, we show the shape\nof this piecewise function as a function of the dis-\ntance. The attention weights from Eq. 2 are then\ncomputed on the masked span, i.e.,\natr = mz(t−r) exp (str)\nt−1∑\nq=t−S\nmz(t−q) exp (stq)\n.\nWe add a ℓ1 penalization on the parameters zi for\neach attention head iof the model to the loss func-\ntion:\nL= −log P(w1,...,w T ) + λ\nM\n∑\ni\nzi,\nwhere λ > 0 is the regularization hyper-\nparameter, and M is the number of heads in each\nlayer. Our formulation is differentiable in the pa-\nrameters zi and we learn them jointly with the rest\nof the model.\nDynamic attention span. As an exten-\nsion, we consider a dynamic computation\napproach (Graves, 2016) where the attention\nspan dynamically change based on the current\ninput (Luong et al., 2015; Shu and Nakayama,\n2017). At a time step t, the span parameter zt of\n333\nan attention head is then a function of the input\nparametrized by a vector v and a scalar b, i.e.,\nzt = Sσ(vT xt + b). We penalize zt in the same\nway as before and learn the parametersv, bjointly\nwith the rest of the parameters.\n3 Experiments\nIn this section, we evaluate the impact of our adap-\ntive attention mechanism in the experimental set-\nting of Al-Rfou et al. (2019) for character level\nlanguage modeling.\nDataset. We use the text8 and enwik8\ndatasets of Mahoney (2011). The both dataset\nhave 100M tokens. We report bit per character\n(bpc) on dev and test set.\nImplementation details. We experiment with\ntwo sizes of models. Our small models have\n12 layers and a hidden size of dh = 512, ex-\ncept for the feedforward ReLU layers, which have\n2048 units. The large models have 24 layers with\na hidden size of dh = 768, and a ReLU size\nof 4096. All models have 8 attention heads in\neach layer. Token and position embedding pa-\nrameters are initialized from N(0,1), and the pro-\njection matrices W{q,k,v,o} are initialized from\nU(−1/√dh,1/√dh). A single set of position em-\nbeddings pt is shared across all the heads.\nIn adaptive-span models, we reprameterized the\nspan parameter z by z = Sz′, where z′ ∈[0,1]\nis initialized to 0. In dynamic-span models, the\nbias term bis initialized −4 to make initial spans\nsmall. We set the hyperparameters λ= 2×10−6\nand R= 32for the both type of models, except λ\nis reduced to 0.5 ×10−6 when S = 8192because\nzwas not growing longer than 4000.\nWe use Adagrad with a batch size of 64 and\nﬁxed learning rate of 0.07 and 32k warm-up steps.\nOur warm-up strategy differs from Vaswani et al.\n(2017): we linearly increase learning rate from\nzero to the ﬁnal learning rate. Gradients of each\nmodule are clipped at 0.03 for better stability. At\ntrain time, we use a block of512 consecutive char-\nacters and compute the loss and gradient for each\nof those 512 characters.\nIn small models, we apply dropout with a rate\nof 0.3 to the attention and the feedforward ReLU\nactivations. We train small models for600Ksteps\n(900K steps when S = 8192), which takes about\n2 ∼3 days on 8 V100 GPUs depending on the at-\ntention span limit. Large models are trained with\na dropout rate of 0.4 until the validation perfor-\nmance stopped improving (250Ksteps for text8\nand 150K steps for enwik8), and then further\ntrained for 20K steps with a learning rate divided\nby 10.\nResults. In Table 1, we compare our sequential\nTransformer with the adaptive spans (“Adaptive-\nSpan”) of Sec. 2.2 to models of Al-Rfou et al.\n(2019) and Dai et al. (2019). For small models,\nour model outperforms the other Transformers by\n0.07 bcp while signiﬁcantly reducing the mem-\nory usage for large attention span. Interestingly,\neven with a limit on span sets to 8192, the av-\nerage span is only 314. Similar results are ob-\ntained on enwik8 as shown in Table 2, where the\nadaptive-span model outperformed similar sized\nmodels with a signiﬁcantly smaller average span.\nOur large models achieved state-of-the-art perfor-\nmances on both datasets with fewer parameters\nand FLOPS.\nIn Figure 3, we compare the ﬁxed and adaptive\nspan small Transformers as we increase the atten-\ntion span limitS. The performance of both models\nimprove as the limit increase (see Figure 3(left)),\nbut the adaptive-span model beneﬁts more from\nlonger span. As shown on the Figure 3(center),\na Transformer with adaptive spans controls its av-\nerage spans, leading to reduction of up to 70% in\nthe number of FLOPS for the inference with large\nspans (see Figure 3(right)).\nImpact on the attention span.In Figure 4, we\nshow the ﬁnal attention spans of every attention\nheads of our small adaptive-span model with S =\n4096. Even though all the span sizes are initial-\nized to the same value, we see large varieties in\ntheir ﬁnal values. We can see that the lowest 5\nlayers have the smallest possible attention span,\nwhich is R = 32 of the masking function. This\nindicates that lower layers in a Transformer model\ndo not really require a long attention span in this\nparticular task. In contrast, few attention heads\nin the higher layers have very long spans, exceed-\ning several thousand. Although there is a general\ntendency of higher layers having longer attention\nspans, it is not a simple monotonic function of the\nlayer height.\nImpact on the number of FLOPS. Having a\nsmaller attention span has a direct impact on the\ntotal number of FLOPS necessary for comput-\ning one-step prediction. In a standard ﬁxed-span\n334\nModel #layers Avg. span #Params #FLOPS dev test\nSmall models\nT12 (Al-Rfou et al., 2019) 12 512 44M 22G - 1.18\nAdaptive-Span (S = 8192) 12 314 38M 42M 1.05 1.11\nLarge models\nT64 (Al-Rfou et al., 2019) 64 512 235M 120G 1.06 1.13\nT-XL (Dai et al., 2019) 24 3800 277M 438M - 1.08\nAdaptive-Span (S = 8192) 24 245 209M 179M 1.01 1.07\nTable 1: Character level language modeling on text8. We report bpc for the dev and test sets, as well as, the\nnumber of parameters, the average attention spans and total number of FLOPS (an estimate of the number of\nFLOPS necessary for computing one step prediction).\n256 1024 4096\nSpan limit (S)\n1.06\n1.08\n1.10\nDev. (bpc)\nFixed\nAdaptive\n256 1024 4096\nSpan limit (S)\n0\n2500\n5000\n7500Average span\n256 1024 4096\nSpan limit (S)\n0.0\n0.5\n1.0\n1.5\nFLOPS\n×108\nFigure 3: Left: validation performances improve as the attention span limit S increase (we did not train a ﬁxed-\nspan model withS = 8192due to memory limitation). Center: average attention span of trained models. Learning\nattention spans signiﬁcantly reduces the average attention span. Right: the number of FLOPS during inference\ntime grows almost linearly with Sfor the ﬁxed span models. The adaptive-span models do not have this growth in\n#FLOPS because they have a very small attention span on average.\nModel #layers #Params #FLOPS dev / test\nSmall models\nT12 12 44M 22G - / 1.11\nT-XL 12 41M 64M - / 1.06\nAdaptive 12 39M 41M 1.04 / 1.02\nLarge models\nT64 64 235M 120G - / 1.06\nT-XL 18 88M 329M - / 1.03\nT-XL 24 277M 438M - / 0.99\nAdaptive 24 209M 181M 1.00 / 0.98\nTable 2: Results on enwik8. The span limit is S =\n8192 for the adaptive-span models.\nmodel, the total number of FLOPS is mostly con-\ntrolled by the feed-forward layer (accounting for\n62% of FLOPS when S = 256). However, as the\nspan increase, the attention layer dominates the\ncomputation (82% of FLOPS when S = 8192),\nmaking it hard to scale to longer sequences. In\ncontrast, the learning of an attention span keeps\ncomputation at a relatively constant level even as\n1 2 3 4 5 6 7 8 9 10 11 12\nLayers\n101\n102\n103\nAttention span\nFigure 4: Adaptive spans (in log-scale) of every atten-\ntion heads in a 12-layer model with span limit S =\n4096. Few attention heads require long attention spans.\nSincrease as shown in Figure 3(right).\nThe memory usage is also dominated by the at-\ntention layer as the attention span increase. Thus,\nreducing the average span will also reduce the\nmemory usage. However, because all heads in a\nsingle layer attend to common state vectors, the\nmaximum span within each layer will determine\nthe memory usage. The same is true for the num-\nber of FLOPS if all heads of a layer are computed\ntogether, as often done for better efﬁciency.\nIn practice, the largest ﬁxed-span model that can\nﬁt in memory for training had a span of S = 2048\n(batches had to be split when S = 4096), and\n335\nov e r l ook s t he pa r k and i t s nume r ou s\n100\n200Average span\nFigure 5: Example of average dynamic attention span\nas a function of the input sequence. The span is aver-\naged over the layers and heads.\nModel Avg. span dev\nAdaptive (S = 1024) 123 1.08\nDynamic (S = 1024) 149 1.08\nTable 3: Comparison between adaptive and dynamic\nattention span on text8.\nit took about 550ms per batch. In contrast, an\nadaptive-span model with a 4 times longer span\nof S = 8192ﬁt in memory and took about similar\ntime per batch.\nDynamic span. In Table 3, we show the adap-\ntive and dynamic spans achieved the same perfor-\nmance with comparable average spans ontext8.\nFigure 5 shows how the average dynamic span\nadapts to the input sequence. The span increases at\nthe beginning of words and in the middle of com-\nposed words, e.g., to predict the “l” in “overlook”.\n4 Conclusion\nIn this work, we present a novel self-attention\nlayer with an adaptive span. This mechanism al-\nlows for models with longer context, and thus with\nthe capability to catch longer dependencies. We\nhave shown the importantce of this feature in the\ncontext of character level modeling where infor-\nmation is spread over great distances.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2019. Character-level lan-\nguage modeling with deeper self-attention. In Pro-\nceedings of the 33rd AAAI Conference on Artiﬁcial\nIntelligence.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\nCarbonell, Quoc V . Le, and Ruslan Salakhutdi-\nnov. 2019. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. CoRR,\nabs/1901.02860.\nAlex Graves. 2016. Adaptive computation time for re-\ncurrent neural networks. CoRR, abs/1603.08983.\nYacine Jernite, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Variable computation in re-\ncurrent neural networks. In 5th International Con-\nference on Learning Representations, ICLR.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP.\nMatt Mahoney. 2011. Large text compression bench-\nmark. URL: http://www. mattmahoney. net/text/text.\nhtml.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT.\nRaphael Shu and Hideki Nakayama. 2017. An empiri-\ncal study of adequate vision span for attention-based\nneural machine translation. In Proceedings of the\nFirst Workshop on Neural Machine Translation.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In Advances in Neural Information Process-\ning Systems 28.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7698715925216675
    },
    {
      "name": "Transformer",
      "score": 0.7503045201301575
    },
    {
      "name": "Memory footprint",
      "score": 0.5229700207710266
    },
    {
      "name": "Context model",
      "score": 0.5025129318237305
    },
    {
      "name": "Span (engineering)",
      "score": 0.49720504879951477
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43114182353019714
    },
    {
      "name": "Voltage",
      "score": 0.17665895819664001
    },
    {
      "name": "Engineering",
      "score": 0.12441131472587585
    },
    {
      "name": "Electrical engineering",
      "score": 0.0803365707397461
    },
    {
      "name": "Programming language",
      "score": 0.07910120487213135
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ],
  "cited_by": 279
}