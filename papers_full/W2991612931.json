{
  "title": "Integrating Graph Contextualized Knowledge into Pre-trained Language Models",
  "url": "https://openalex.org/W2991612931",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1976599103",
      "name": "He Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2008572887",
      "name": "Zhou Di",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111694291",
      "name": "Xiao Jinghui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2030371270",
      "name": "Jiang Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1952241286",
      "name": "Liu Qun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748068597",
      "name": "Yuan, Nicholas Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107651531",
      "name": "Xu Tong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2889583850",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2804220263",
    "https://openalex.org/W2774837955",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2052217781",
    "https://openalex.org/W2973154071",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W2951105272",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W2964116313"
  ],
  "abstract": "Complex node interactions are common in knowledge graphs, and these interactions also contain rich knowledge information. However, traditional methods usually treat a triple as a training unit during the knowledge representation learning (KRL) procedure, neglecting contextualized information of the nodes in knowledge graphs (KGs). We generalize the modeling object to a very general form, which theoretically supports any subgraph extracted from the knowledge graph, and these subgraphs are fed into a novel transformer-based model to learn the knowledge embeddings. To broaden usage scenarios of knowledge, pre-trained language models are utilized to build a model that incorporates the learned knowledge representations. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and improvement above TransE indicates that our KRL method captures the graph contextualized information effectively.",
  "full_text": "Integrating Graph Contextualized Knowledge into Pre-trained Language Models\nBin He1, Di Zhou1, Jinghui Xiao1, Xin Jiang1, Qun Liu1, Nicholas Jing Yuan2, Tong Xu3\n1Huawei Noah’s Ark Lab\n2Huawei Cloud & AI\n3School of Computer Science, University of Science and Technology of China\n{hebin.nlp, zhoudi7, xiaojinghui4, jiang.xin, qun.liu, nicholas.yuan}@huawei.com, tongxu@ustc.edu.cn\nAbstract\nComplex node interactions are common in knowledge graphs,\nand these interactions also contain rich knowledge informa-\ntion. However, traditional methods usually treat a triple as\na training unit during the knowledge representation learn-\ning (KRL) procedure, neglecting contextualized information\nof the nodes in knowledge graphs (KGs). We generalize the\nmodeling object to a very general form, which theoretically\nsupports any subgraph extracted from the knowledge graph,\nand these subgraphs are fed into a novel transformer-based\nmodel to learn the knowledge embeddings. To broaden us-\nage scenarios of knowledge, pre-trained language models are\nutilized to build a model that incorporates the learned knowl-\nedge representations. Experimental results demonstrate that\nour model achieves the state-of-the-art performance on sev-\neral medical NLP tasks, and improvement above TransE indi-\ncates that our KRL method captures the graph contextualized\ninformation effectively.\nIntroduction\nPre-trained language models learn contextualized word rep-\nresentations on large-scale text corpus through an self-\nsupervised learning method, which are ﬁnetuned on down-\nstream tasks and can obtain the state-of-the-art (SOTA) per-\nformance (Peters et al. 2018; Radford et al. 2018; Devlin\net al. 2019). This gradually becomes a new paradigm for\nnatural language processing research. Recently, knowledge\ninformation has been integrated into pre-trained language\nmodels to enhance the language representations, such as\nERNIE-Tsinghua (Zhang et al. 2019) and ERNIE-Baidu\n(Sun et al. 2019). Zhang et al. (2019) makes a prelimi-\nnary attempt to utilize knowledge information learned from\na knowledge graph to improve the performance of some\nknowledge-driven tasks. Intuitively, this model can be di-\nrectly applied to the medical domain, where large-scale cor-\npora and knowledge graphs are both available.\nIn ERNIE-Tsinghua, entity embeddings are learned by\nTransE (Bordes et al. 2013), which is a popular transition-\nbased method for knowledge representation learning (KRL).\nTransE simply considers each triple in the knowledge graph\nas a training instance, which may be insufﬁcient to model\nWork in progress.\nFigure 1: A subgraph extracted from a medical knowledge\ngraph. The rectangles represent medical entities and directed\narrows denote the relations between these entities. Only part\nof the neighbor nodes are listed for clarity.\ncomplex information transmission between nodes in the\nknowledge graph. In the medical knowledge graph, some en-\ntities have a large number of related neighbors, and TransE\ncan not model all the neighbors for the corresponding en-\ntity simultaneously. Figure 1 shows a subgraph of a med-\nical knowledge graph containing several medical entities.\nIn this ﬁgure, four incoming and four outgoing neighbor-\ning nodes (hereinafter called “in-entity” and “out-entity”) of\nnode “Bacterial pneumonia ” are listed with various types\nof relation between the nodes. Therefore, in order to learn\nmore comprehensive node embeddings, it is necessary to in-\ncorporate more interactive information between nodes (we\ncall this “graph contextualized information”).\nGraph attention networks (GATs) (Veliˇckovi´c et al. 2018)\nupdate the entity embedding by its 1-hop neighbors, which\npays more attention to the node information interactions.\nBased on GATs, multi-hop neighbors of a given entity are in-\ntegrated to capture further information (Nathani et al. 2019).\nInspired by previous work, we extend the information inter-\naction between nodes to a more general approach, which can\ntreat arbitrary subgraph from the knowledge graph as a train-\ning example. Speciﬁcally, the subgraph is transformed into\na sequence of nodes, and the KRL procedure is performed\nin a way similar to training a language model. In this man-\nner, more comprehensive contextualized information of each\nnode can be incorporated into the learned knowledge rep-\narXiv:1912.00147v3  [cs.CL]  30 Sep 2021\nresentations. Besides, we believe that entities and relations\nshould be mutually inﬂuential during the KRL process, so\nthe relations are regarded as graph nodes as well and learned\njointly with entities.\nFinally, our pre-trained model called BERT-MK (a\nBERT-based language model with Medical Knowledge) is\nlearned with a large-scale medical corpus, and integrated\nwith the medical knowledge graph represented through the\naforementioned KRL approach. Our contributions are as fol-\nlows:\n• A novel knowledge representation learning method that is\ncapable of modeling arbitrary subgraph is proposed. This\nmethod greatly enriches the amount of information in the\nknowledge representation and explores the joint learning\nof entities and relations.\n• Graph contextualized knowledge is integrated to enhance\nthe performance of pre-trained language models, which\noutperforms state-of-the-art models on several NLP tasks\nin the medical domain.\nMethodology\nIn our model, each time a certain number of nodes and\ntheir linked nodes are selected from the knowledge graph to\nconstruct a training sample. Then, the embeddings of these\nnodes are learned from their neighbors by a novel knowledge\nrepresentation learning algorithm based on Transformer. Fi-\nnally, the learned knowledge representation is integrated into\nthe language model to enhance the model pre-training and\nﬁne-tuning.\nLearning Knowledge Graph Embeddings by\nTransformer\nTransformer (Vaswani et al. 2017) can be used as a pow-\nerful encoder to model the sequential inputs. Recently,\nKoncel-Kedziorski et al. (2019) extend Transformer to en-\ncode graph-structured inputs. In their work, text is ﬁrst trans-\nformed into a graph, which is then encoded by the graph\nTransformer and fed into a text generation model. Inspired\nby their work, we convert a knowledge subgraph into a se-\nquence of nodes, and utilize a Transformer-based model\nto learn the node embeddings. We call this model “KG-\nTransformer”, where “KG” denotes “knowledge graph”).\nMore details of the method are described in the following\nsubsections.\nGraph Conversion We denote a knowledge graph asG=\n(E,R), where Erepresents the entity set and Ris the set of\nrelations between the pairs of entities in G. A triple in Gis\ndenoted by t = (es,r,eo), where es is a subjective entity,\neo is an objective entity, andris the relation between es and\neo. Figure 1 gives an example of a subgraph of the medical\nKG. Two entities (rectangles) and a relation (arrow) between\nthem constructs a knowledge triple, for example, (Bacterial\npneumonia, causative agent of, Bacteria).\nSingle triple is treated as a training sample in the tradi-\ntional KRL methods, such as TransE (Bordes et al. 2013)\nand ConvKB (Nguyen et al. 2018). In this setting, informa-\ntion from the entity’s neighbors cannot be involved to up-\ndate the entity embedding simultaneously. Graph attention\nFigure 2: Converting a subgraph extracted from the knowl-\nedge graph into the input of the KG-Transformer model. (a)\nerefers to the entity, and rrepresents the relation. (b) Rela-\ntions are transformed into graph nodes, and all nodes are as-\nsigned a numeric index. (c) Each row in the matrix for node\nposition indexes represents the index list for an triple in (b);\nthe adjacent matrix indicates the connectivity (the red points\nequal to 1 and the white points are 0) between the nodes in\n(b).\nnetworks (GATs) (Veliˇckovi´c et al. 2018) are proposed to\nsolve this problem. Nathani et al. (2019) utilize GAT to build\na KRL model, in which all the subjective entities directing\nto an objective entity are used to learn the embedding of\nthe objective entity. In our work, we propose a more gen-\neral method that takes any subgraph of the KG as a training\nsample, which greatly enriches the contextualized informa-\ntion in learning the knowledge representation. For ease of\nexplanation, in Figure 2(a), we choose one entity with its\ntwo in-entities and out-entities to build a training sample.\nBesides, relations in the KG are also learned as nodes\nequivalent to entities in our model, which achieves joint\ntraining of entity embeddings and relation embeddings. The\nnode conversion process is illustrated in Figure 2(b). The\nknowledge graph can be redeﬁned as G= (V,E), where V\nrepresents the nodes in G, involving entities in Eand rela-\ntions in R, and Edenotes a adjacency matrix indicating the\ndirected edges among the nodes in V. The adjacency matrix\nin Figure 2(c) shows the connectivity between the nodes in\nFigure 2(b).\nThe conversion result of a subgraph is shown in Fig-\nure 2(c), including a node sequence, a node position index\nmatrix and an adjacency matrix. Each row of the node po-\nsition index matrix corresponds to a triple in the graph. For\nFigure 3: The model architecture of BERT-MK. The left part is the pre-trained language model, in which entity information\nlearned from the knowledge graph is incorporated. The right part presents the KG-Transformer model, which utilizes the\ntraining sample illustrated in Figure 2 to describe the knowledge representation learning process. e1, e(1)\n1 , eO\n1 is the embedding\nof the input node, the updated node and the output node, respectively.\nexample, the triple (e1,r1,e) is represented as the ﬁrst row\n(0,1,4) in this matrix. In the adjacency matrix, the elements\naij equals 1 if the node i is connected to node j in Fig-\nure 2(b), and 0 otherwise.\nTransformer-based Encoder We denote a node sequence\nas {x1,...,x N }, where N is the length of the input se-\nquence. Besides, a node position index matrix and an ad-\njacency matrix are deﬁned as P and A, respectively. Entity\nembeddings and relation embeddings are integrated in a ma-\ntrix V, V ∈R(ne+nr)×d, where ne is the entity number in\nEand nr is the relation type number inR. The node embed-\ndings X = {x1,..., xN }can be generated by looking up\nnode sequence {x1,...,x N }in embedding matrix V. X, P\nand A constitute the input of our KRL model, as shown in\nFigure 3.\nThe inputs are fed into a Transformer-based model to en-\ncode the node information.\nx′\ni =\nH⨁\nh=1\nN∑\nj=1\nαh\nij ·(xj ·Wh\nv ), (1)\nαh\nij = exp(ah\nij)\n√\nd/H·∑N\nn=1 exp(ah\nin)\n, (2)\nah\nij = Masking((xi ·Wh\nq ) ·(xj ·Wh\nk )T),Aij + Iij), (3)\nwhere x′\ni is the new embedding for node xi. ⨁ denotes the\nconcatenation of the Hattention heads in this layer, αh\nij and\nWh\nv are the attention weight of node xj and a linear trans-\nformation of node embedding xj in the hth attention head,\nrespectively. The Masking function in Equation 3 restraints\nthe contextualized dependency among the input nodes, only\nthe degree-in nodes and the current node itself are involved\nto update the node embedding. Similar toWh\nv , Wh\nq and Wh\nk\nare independent linear transformations of node embeddings.\nThen, the new node embeddings are fed into the feed for-\nward layer for further encoding.\nAs in Transformer model, we stack the aforementioned\nTransformer blocks Ltimes. The output of the Transformer-\nbased encoder can be formalized as\nXO = {xO\n1 ,..., xO\nN }. (4)\nTraining Objective The output of the encoderXO and the\nnode position indexesP are utilized to restore the triples and\ngenerate the embeddings of these triples:\nT = TripleRestoration(XO,P), (5)\nwhere Tk = (xeks\n,xrk ,xeko\n) and Pk = (ek\ns,rk,ek\no) is the\nposition index of a valid knowledge triple.\nIn this study, the translation-based scoring function (Han\net al. 2018) is adopted to measure the energy of a knowledge\ntriple. The node embeddings are learned by minimizing a\nmargin-based loss function on the training data:\nL=\n∑\nt∈T\nmax{d(t) −d(f(t)) +γ,0}, (6)\nwhere t = (ts,tr,to), d(t) =|ts +tr −to|, γ >0 is a mar-\ngin hyperparameter, f(t) is an entity replacement operation\nthat the head entity or the tail entity in a triple is replaced\nand the replaced triple is an invalid triple in the KG.\nIntegrating Knowledge into the Language Model\nGiven a comprehensive medical knowledge graph, graph\ncontextualized knowledge representations can be learned us-\ning the KG-Transformer model. We follow the language\nmodel architecture proposed in (Zhang et al. 2019), graph\ncontextualized knowledge is utilized to enhance the medical\nlanguage representations. The language model pre-training\nprocess is shown in the left part of Figure 3. The Trans-\nformer block encodes word contextualized representation\nwhile the aggregator block implements the fusion of knowl-\nedge and language information.\nAccording to the characteristics of medical NLP tasks,\ndomain-speciﬁc ﬁnetuning procedure is designed. Similar to\nBioBERT (Lee et al. 2019), symbol “@” and “$” are used\nto mark the entity boundary, which indicate the entity po-\nsitions in a sample and distinguish different relation sam-\nples sharing the same sentence. For example, the input se-\nquence for the relation classiﬁcation task can be modiﬁed\ninto “[CLS] pain control was initiated with morphine but\nwas then changed to @ demerol $, which gave the patient\nbetter relief of @ his epigastric pain $”. In the entity typing\ntask, entity mention and its context are critical to predict the\nentity type, so more localized features of the entity mention\nwill beneﬁt this prediction process. In our experiments, the\nentity start tag “@” is selected to represent an entity typing\nsample.\nExperiments\nDataset\nMedical Knowledge Graph The Uniﬁed Medical Lan-\nguage System (UMLS) (Bodenreider 2004) is a comprehen-\nsive knowledge base in the biomedical domain, which con-\ntains large-scale concept names and relations among them.\nThe metathesaurus in UMLS involves various terminology\nsystems and comprises about 14 million terms covering 25\ndifferent languages. In this study, a subset of this knowledge\nbase is extracted to construct the medical knowledge graph\nfor KRL. Non-English and long terms are ﬁltered, and the\nﬁnal statistics is shown in Table 1.\nCorpus for Pre-training To ensure that sufﬁcient medi-\ncal knowledge can be integrated into the language model,\nPubMed abstracts and PubMed Central full-text papers are\nchosen as the pre-training corpus, which are open-access\ndatasets for biomedical and life sciences journal literature.\nSince sentences in different paragraphs may not have good\ncontext coherence, paragraphs are selected as the document\nunit for next sentence prediction. The Natural Language\nToolkit (NLTK) is utilized to split the sentences within a\nparagraph, and sentences having less than 5 words are dis-\ncarded. As a result, a large corpus containing 9.9B tokens is\nachieved for language model pre-training.\nIn our model, medical terms appearing in the corpus need\nto be aligned to the entities in the UMLS metathesaurus be-\nfore pre-training. To make sure the coverage of identiﬁed\nentities in the metathesaurus, the forward maximum match-\ning (FMM) algorithm is used to extract the term spans from\nthe corpus aforementioned, and spans less than 5 characters\nare ﬁltered. Then, BERT vocabulary is used to tokenize the\ninput text into word pieces, and the medical entity is aligned\nwith the ﬁrst subword of the identiﬁed term.\nDownstream tasks In this study, entity typing and rela-\ntion classiﬁcation tasks in the medical domain are used to\nevaluate the models.\nEntity Typing Given a sentence with an entity mention\ntagged, the task of entity typing is to identify the semantic\ntype of this entity mention. For example, the type “ medical\nproblem” is used to label the entity mention in the sentence\n“he had a differential diagnosis of⟨e⟩asystole ⟨/e⟩”. To the\nbest of our knowledge, there are no publicly available entity\ntyping datasets in the medical domain, therefore, three entity\ntyping datasets are constructed from the corresponding med-\nical named entity recognition datasets. Entity mentions and\nentity types are annotated in these datasets, in this study, en-\ntity mentions are considered as input while entity types are\nthe output labels. Table 2 shows the statistics of the datasets\nfor the entity typing task.\nRelation Classiﬁcation Given two entities within one\nsentence, the task aim is to determine the relation type be-\ntween the entities. For example, in sentence “ pain control\nwas initiated with morphine but was then changed to ⟨e1⟩\ndemerol ⟨/e1⟩, which gave the patient better relief of ⟨e2⟩\nhis epigastric pain ⟨/e2⟩”, the relation type between two\nentities is TrIP (Treatment Improves medical Problem). In\nthis study, three relation classiﬁcation datasets are utilized\nto evaluate our models, and the statistics of these datasets\nare shown in Table 2.\nImplementation Details\nKnowledge Representation Learning To achieve a ba-\nsic knowledge representation, UMLS triples are fed into\nthe TransE model. The OpenKE toolkit (Han et al. 2018)\nis adopted to train the entity and relation embeddings, and\nthe embedding dimension is set to 100 while training epoch\nnumber is set to 10000.\nFollowing the initialization method used in (Nguyen et\nal. 2018; Nathani et al. 2019), the embeddings produced by\nTransE are utilized to initialize the representation learning\nby the KG-Transformer model. Both the layer number and\nthe hidden head number is set to 4. Due to the median de-\ngree of nodes in UMLS is 4 (shown in Table1), one node\nwith two in-nodes and two out-nodes is sampled as a train-\ning instance. The KG-Transformer model runs 1200 epochs\non a single NVIDIA Tesla V100 (32GB) GPU to train the\nknowledge embeddings, with a batch size of 50000.\nPre-training First, a medical ERNIE (MedERNIE) model\nis trained on UMLS triples and the PubMed corpus, inher-\niting the same model hyperparameters used in (Zhang et al.\n2019). Besides, the entity embeddings learned by the KG-\nTransformer model are integrated into the language model\nto train the BERT-MK model. In our work, we align the\nsame amount of pre-training with BioBERT, which uses the\nsame pre-training corpus as ours, and ﬁnetune the BERT-\nBase model on the PubMed corpus for one epoch.\nTable 1: Statistics of UMLS.\n# Entities # Relations # Triples Ave. in-degree Ave. out-degree Median degree\nUMLS 2,842,735 874 13,555,037 5.05 5.05 4\nTable 2: Statistics of the datasets. Most of these datasets do not follow a standard train-valid-test set partition, and we adopt\nsome traditional data partition ways to do model training and evaluation.\nTask Dataset # Train # Valid # Test\nEntity Typing 2010 i2b2/V A (Uzuner et al. 2011) 16,519 - 31,161\nJNLPBA (Kim et al. 2004) 51,301 - 8,653\nBC5CDR (Li et al. 2016) 9,385 9,593 9,809\nRelation Classiﬁcation 2010 i2b2/V A (Uzuner et al. 2011) 10,233 - 19,115\nGAD (Bravo et al. 2015) 5,339 - -\nEU-ADR (Van Mulligen et al. 2012) 355 - -\nFinetune Since there is no standard development set in\nsome datasets, we divide the training set into a new training\nset and a development set by 4:1. For datasets containing\na standard test set, we preform each experiment ﬁve times\nunder speciﬁc experimental settings with different random\nseeds, and the average result is used to improve the eval-\nuation reliability. Besides, 10-fold cross-validation method\nis used to evaluate the model performance for the datasets\nwithout a standard test set. According to the maximum se-\nquence length of the sentences in each dataset, the input\nsequence length for 2010 i2b2/V A (Uzuner et al. 2011),\nJNLPBA (Kim et al. 2004), BC5CDR (Li et al. 2016), GAD\n(Bravo et al. 2015) and EU-ADR (Van Mulligen et al. 2012)\nare set to 390, 280, 280, 130 and 220, respectively. The ini-\ntial learning rate is set to 2e-5.\nBaselines In addition to the state-of-the-art models on\nthese datasets, we have also added the popular BERT-Base\nmodel and another two models pre-trained on biomedical lit-\nerature for further comparison.\nBERT-Base(Devlin et al. 2019) This is the original bidi-\nrectional pre-trained language model proposed by Google,\nwhich achieves state-of-the-art performance on a wide range\nof NLP tasks.\nBioBERT (Lee et al. 2019) This model follows the same\nmodel architecture as the BERT-Base model, but with the\nPubMed abstracts and PubMed Central full-text articles\n(about 18B tokens) used to do model ﬁnetuning upon BERT-\nBase.\nSCIBERT (Beltagy, Cohan, and Lo 2019) A new word-\npiece vocabulary is built based on a large scientiﬁc cor-\npus (about 3.2B tokens). Then, a new BERT-based model is\ntrained from scratch using this new scientiﬁc vocabulary and\nthe scientiﬁc corpus. Since a large portion of the scientiﬁc\ncorpus is biomedical articles, this scientiﬁc vocabulary can\nalso be regarded as a biomedical vocabulary, which can im-\nprove the performance of downstream tasks in the biomedi-\ncal domain effectively.\nResults\nTable 3 presents the experimental results on the entity typ-\ning and relation classiﬁcation tasks. For entity typing tasks,\nall these pre-trained language models achieve high accuracy,\nindicating that the type of a medical entity is not as ambigu-\nous as that in the general domain. BERT-MK outperforms\nBERT-Base, BioBERT and SCIBERT by 0.71%, 0.24% and\n0.02% on the average accuracy, respectively. Without us-\ning external knowledge in the pre-trained language model,\nSCIBERT achieves comparable results to BERT-MK, which\nproves that a domain-speciﬁc vocabulary is critical to the\nfeature encoding of inputs. Long tokens are relatively com-\nmon in the medical domain, and these tokens will be split\ninto short pieces when a domain-independent vocabulary\nis used, which will cause an overgeneralization of lexical\nfeatures. Therefore, a medical vocabulary generated by the\nPubMed corpus can be introduced into BERT-MK in the fol-\nlowing work.\nOn the relation classiﬁcation tasks, BERT-Base does\nnot perform as well as other models, which indicates that\npre-trained language models require a domain adaptation\nprocess when used in restricted domains. Compared with\nBioBERT, which utilizes the same domain-speciﬁc corpus\nas ours for domain adaptation of pre-trained language mod-\nels, BERT-MK improves the average F score by 2.27%,\nwhich demonstrates medical knowledge has indeed played\na positive role in the identiﬁcation of medical relations. The\nfollowing example provides a brief explanation of why med-\nical knowledge improves the model performance of the re-\nlation classiﬁcation tasks. “ On postoperative day number\nthree , patient went into ⟨e1⟩atrial ﬁbrillation ⟨/e1⟩, which\nwas treated appropriately with ⟨e2⟩metoprolol ⟨/e2⟩and\ndigoxin and converted back to sinus rhythm ” is a relation\nsample from the 2010 i2b2/V A dataset, and the relation la-\nbel is TrIP. Meanwhile, the above entity pair can be aligned\nto a knowledge triple (atrial ﬁbrillation, may be treated by,\nmetoprolol) in the medical knowledge graph. Obviously, this\nknowledge information is advantageous for the relation clas-\nsiﬁcation of the aforementioned example.\nTable 3: Experimental results on the entity typing and relation classiﬁcation tasks. Accuracy (Acc), Precision, Recall, and F1\nscores are used to evaluate the model performance. The results given in previous work are underlined. E-SVM is short for\nEnsemble SVM (Bhasuran and Natarajan 2018), which achieves SOTA performance in GAD. CNN-M stands for CNN using\nmulti-pooling (He, Guan, and Dai 2019), and this model is the SOTA model in 2010 i2b2/V A.\nTask Dataset Metrics E-SVM CNN-M BERT-Base BioBERT SCIBERT BERT-MK\nEntity 2010 i2b2/V A Acc - - 96.76 97.43 97.74 97.70\nTyping JNLPBA Acc - - 94.12 94.37 94.60 94.55\nBC5CDR Acc - - 98.78 99.27 99.38 99.54\nAverage Acc - - 96.55 97.02 97.24 97.26\nRelation 2010 i2b2/V A P - 73.1 72.6 76.1 74.8 77.6\nClassiﬁcation R - 66.7 65.7 71.3 71.6 72.0\nF - 69.7 69.2 73.6 73.1 74.7\nGAD P 79.21 - 74.28 76.43 77.47 81.67\nR 89.25 - 85.11 87.65 85.94 92.79\nF 83.93 - 79.33 81.66 81.45 86.87\nEU-ADR P - - 75.4 5 81.05 78.42 84.43\nR - - 96.55 93.90 90.09 91.17\nF - - 84.71 87.00 85.51 87.49\nAverage P - - 74.11 77.86 76.90 81.23\nR - - 82.45 84.23 82.54 85.32\nF - - 77.75 80.75 80.02 83.02\nTransE vs. KG-Transformer In order to give a more in-\ntuitive analysis of our new KRL method for the promotion\nof pre-trained language models, we compare MedERNIE\n(TransE is used to learn knowledge representation) and\nBERT-MK (corresponding to KG-Transformer) on two rela-\ntion classiﬁcation datasets. Table 4 demonstrates the results\nof these two models. As we can see, integrating knowledge\ninformation learned by the KG-Transformer model, the per-\nformance increases the F score by 0.9% and 0.64% on two\nrelation classiﬁcation datasets, respectively, which indicates\nthat improvement of the knowledge quality has a beneﬁcial\neffect on the pre-trained language model.\nIn Figure 4, as the amount of pre-training data increases,\nBERT-MK always outperforms MedERNIE on the 2010\ni2b2/V A relation dataset, and the performance gap has an\nincreasing trend. However, on the GAD dataset, the perfor-\nmance of BERT-MK and MedERNIE are intertwined. We\nlink the entities in the relation samples to the knowledge\ngraph, and perform statistical analysis on the relationships\nbetween the linked nodes. We observe that there are 136 2-\nhop neighbor relationships in 2010 i2b2/V A, while only 1\nin GAD. The second case shown in Table 5 gives an ex-\nample of the situation described above. Triple ( CAD, mem-\nber of, Other ischemic heart disease (SMQ) ) and (Other is-\nchemic heart disease (SMQ), has member, Angina symptom)\nare found in the medical knowledge graph, which indicates\nentity cad and entity angina symptoms have a 2-hop neigh-\nbor relationship. KG-Transformer learns these 2-hop neigh-\nbor relationships in 2010 i2b2/V A and produces an improve-\nment for BERT-MK. However, due to the characteristics of\nthe GAD dataset, the capability of KG-Transformer is lim-\nited.\nTable 4: TransE vs. KG-Transformer. Since the EU-ADR\ndataset is too small, the model comparison on this dataset\nis not listed in this table.\nDataset MedERNIE BERT-MK\nP R F P R F\n2010 i2b2/V A 76.6 71.1 73.8 77.6 72.0 74.7\nGAD 81.28 91.86 86.23 81.67 92.79 86.87\nAverage 78.94 81.48 80.02 79.64 82.40 80.79\nEffect of the Corpus Size in Pre-training Figure 4 shows\nthe model performance comparison with different propor-\ntion of the pre-training corpus. From this ﬁgure, we observe\nthat BERT-MK outperforms BioBERT by using only 10%-\n20% of the corpus, which indicates that medical knowledge\nhas the capability to enhance pre-trained language models\nand save computational costs (Schwartz et al. 2019).\nRelated Work\nPre-trained language models represented by ELMO (Peters\net al. 2018), GPT (Radford et al. 2018) and BERT (Devlin\net al. 2019) have attracted great attention, and a large num-\nber of variant models have been proposed. Among these\nstudies, some researchers devote their efforts to introduc-\ning knowledge into language models, such as ERNIE-Baidu\n(Sun et al. 2019) and ERNIE-Tsinghua (Zhang et al. 2019).\nERNIE-Baidu introduces new masking units such as phrases\nand entities to learn knowledge information in these mask-\ning units. As a reward, syntactic and semantic information\nfrom phrases and entities is implicitly integrated into the\nlanguage model. Furthermore, a different knowledge infor-\nmation is explored in ERNIE-Tsinghua, which incorporates\nknowledge graph into BERT to learn lexical, syntactic and\nTable 5: Case study on the 2010 i2b2/V A relation dataset. The bold text spans in two cases are entities. NPP, no relation between\ntwo medical problems; PIP, medical problem indicates medical problem.\nCases The Corresponding Triples BioBERT MedERNIE BERT-\nMK\nGround\nTruth\n1 ... coronary artery disease, status post mi x0, cabg ... (Coronary artery disease, associated with , MI) NPP PIP PIP PIP\n2 0. cad: presented withanginal symptoms and ekg changes\n(stemi), with cardiac catheterization revealing lesions in\nlad, lcx, and plb.\n(CAD, member of, Other ischemic heart disease (SMQ));\n(Other ischemic heart disease (SMQ), has member, Angina\nsymptom)\nNPP NPP PIP PIP\nFigure 4: Model performance comparison with increasing\namount of the pre-trained data. The x-axis represents the\nproportion of the medical data used for pre-training. 0 means\nno medical data is utilized, so the BERT-Base is used as an\ninitialization parameter for the model ﬁnetuning. 100 indi-\ncates the model is pre-trained on the medical corpus for one\nepoch. BioBERT pre-trains on the PubMed corpus for one\nepoch, which is drawn with dashed lines in the ﬁgure as a\nbaseline for comparison.\nknowledge information simultaneously. However, informa-\ntion interaction between nodes in the knowledge graph is\nlimited by the KRL method used in ERNIE-Tsinghua, which\nis crucial to the knowledge quality.\nRecently, several KRL methods based on neural networks\nhave been proposed, which can be roughly divided into\nconvolutional neural network (CNN) based (Dettmers et al.\n2018; Nguyen et al. 2018) and graph neural network (GNN)\nbased models (Schlichtkrull et al. 2018; Nathani et al. 2019).\nThe CNN based models enhance the information interac-\ntion within a triple, but they treat a triple as a training unit\nand do not exploit the information of the relevant triples.\nTo overcome the above shortcoming, relational Graph Con-\nvolutional Networks (R-GCNs) (Schlichtkrull et al. 2018)\nis proposed to learn entity embeddings from their incoming\nneighbors, which greatly enhances the information interac-\ntion between related triples. Nathani et al. (2019) further ex-\ntends the information ﬂow from 1-hop in-entities to n-hop\nduring the learning process of entity representations, and\nachieves the SOTA performance on multiple relation pre-\ndiction datasets, especially for the ones containing higher\nin-degree nodes.\nWe believe that the information contained in knowledge\ngraphs is far from being sufﬁciently exploited. In this study,\nwe develop a KRL method that can transform any subgraph\ninto a training sample, which has the ability to model any\ninformation in the knowledge graph theoretically. In addi-\ntion, this knowledge representation is integrated into the lan-\nguage model to obtain an enhanced version of the medical\npre-trained language model.\nConclusion and Future Work\nWe propose a novel approach to learn more comprehen-\nsive knowledge representation, focusing on modeling sub-\ngraphs in the knowledge graph by a Transformer-based\nmethod. Additionally, the learned medical knowledge is in-\ntegrated into the pre-trained language model, which out-\nperforms BERT-Base and another two domain-speciﬁc pre-\ntrained language models on several medical NLP tasks. Our\nwork validates the intuition that medical knowledge is bene-\nﬁcial to some medical NLP tasks and provides a preliminary\nexploration for the application of medical knowledge.\nIn the follow-up work, traditional downstream tasks for\nknowledge representation learning methods, such as rela-\ntion prediction, will be used to further validate the effective-\nness of KG-Transformer. Moreover, we will explore a more\nelegant way to combine medical knowledge with language\nmodels.\nReferences\nBeltagy, I.; Cohan, A.; and Lo, K. 2019. Scibert: Pretrained\ncontextualized embeddings for scientiﬁc text. arXiv preprint\narXiv:1903.10676.\nBhasuran, B., and Natarajan, J. 2018. Automatic extrac-\ntion of gene-disease associations from literature using joint\nensemble learning. PloS one 13(7):e0200699.\nBodenreider, O. 2004. The uniﬁed medical language system\n(umls): integrating biomedical terminology. Nucleic acids\nresearch 32(suppl 1):D267–D270.\nBordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating embeddings for model-\ning multi-relational data. In Advances in neural information\nprocessing systems, 2787–2795.\nBravo, `A.; Pi˜nero, J.; Queralt-Rosinach, N.; Rautschka, M.;\nand Furlong, L. I. 2015. Extraction of relations between\ngenes and diseases from text and large-scale data analysis:\nimplications for translational research. BMC bioinformatics\n16(1):55.\nDettmers, T.; Minervini, P.; Stenetorp, P.; and Riedel, S.\n2018. Convolutional 2d knowledge graph embeddings. In\nThirty-Second AAAI Conference on Artiﬁcial Intelligence.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), 4171–4186.\nHan, X.; Cao, S.; Lv, X.; Lin, Y .; Liu, Z.; Sun, M.; and Li,\nJ. 2018. Openke: An open toolkit for knowledge embed-\nding. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System Demon-\nstrations, 139–144.\nHe, B.; Guan, Y .; and Dai, R. 2019. Classifying medical\nrelations in clinical text via convolutional neural networks.\nArtiﬁcial intelligence in medicine 93:43–49.\nKim, J.-D.; Ohta, T.; Tsuruoka, Y .; Tateisi, Y .; and Collier,\nN. 2004. Introduction to the bio-entity recognition task at\njnlpba. In Proceedings of the international joint workshop\non natural language processing in biomedicine and its ap-\nplications, 70–75. Citeseer.\nKoncel-Kedziorski, R.; Bekal, D.; Luan, Y .; Lapata, M.; and\nHajishirzi, H. 2019. Text generation from knowledge graphs\nwith graph transformers. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), 2284–2293.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and\nKang, J. 2019. Biobert: pre-trained biomedical language\nrepresentation model for biomedical text mining. arXiv\npreprint arXiv:1901.08746.\nLi, J.; Sun, Y .; Johnson, R. J.; Sciaky, D.; Wei, C.-H.; Lea-\nman, R.; Davis, A. P.; Mattingly, C. J.; Wiegers, T. C.; and\nLu, Z. 2016. Biocreative v cdr task corpus: a resource for\nchemical disease relation extraction. Database 2016.\nNathani, D.; Chauhan, J.; Sharma, C.; and Kaul, M. 2019.\nLearning attention-based embeddings for relation prediction\nin knowledge graphs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ,\n4710–4723. Florence, Italy: Association for Computational\nLinguistics.\nNguyen, D. Q.; Nguyen, T. D.; Nguyen, D. Q.; and Phung,\nD. 2018. A novel embedding model for knowledge base\ncompletion based on convolutional neural network. In Pro-\nceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Papers) ,\n327–333.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextual-\nized word representations. In Proceedings of NAACL-HLT,\n2227–2237.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nSchlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg, R.;\nTitov, I.; and Welling, M. 2018. Modeling relational data\nwith graph convolutional networks. In European Semantic\nWeb Conference, 593–607. Springer.\nSchwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2019.\nGreen ai.\nSun, Y .; Wang, S.; Li, Y .; Feng, S.; Chen, X.; Zhang, H.;\nTian, X.; Zhu, D.; Tian, H.; and Wu, H. 2019. Ernie: En-\nhanced representation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nUzuner, ¨O.; South, B. R.; Shen, S.; and DuVall, S. L. 2011.\n2010 i2b2/va challenge on concepts, assertions, and rela-\ntions in clinical text. Journal of the American Medical In-\nformatics Association 18(5):552–556.\nVan Mulligen, E. M.; Fourrier-Reglat, A.; Gurwitz, D.;\nMolokhia, M.; Nieto, A.; Triﬁro, G.; Kors, J. A.; and Fur-\nlong, L. I. 2012. The eu-adr corpus: annotated drugs, dis-\neases, targets, and their relationships. Journal of biomedical\ninformatics 45(5):879–884.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,\nP.; and Bengio, Y . 2018. Graph Attention Networks. Inter-\nnational Conference on Learning Representations.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q.\n2019. ERNIE: Enhanced language representation with in-\nformative entities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 1441–\n1451. Florence, Italy: Association for Computational Lin-\nguistics.",
  "topic": "Knowledge graph",
  "concepts": [
    {
      "name": "Knowledge graph",
      "score": 0.7651293873786926
    },
    {
      "name": "Computer science",
      "score": 0.7610611915588379
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.5575253367424011
    },
    {
      "name": "Language model",
      "score": 0.5517721176147461
    },
    {
      "name": "Transformer",
      "score": 0.5175496935844421
    },
    {
      "name": "Natural language processing",
      "score": 0.491271436214447
    },
    {
      "name": "Graph",
      "score": 0.4682788848876953
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4552302956581116
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4345109760761261
    },
    {
      "name": "Knowledge base",
      "score": 0.4166608154773712
    },
    {
      "name": "Information retrieval",
      "score": 0.3524990677833557
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}