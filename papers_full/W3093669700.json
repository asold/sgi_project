{
    "title": "Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model.",
    "url": "https://openalex.org/W3093669700",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5040786145",
            "name": "Yan Zeng",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5018977183",
            "name": "Jian‚ÄêYun Nie",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2765617518",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963035145",
        "https://openalex.org/W2964042872",
        "https://openalex.org/W2963939249",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2962753250",
        "https://openalex.org/W2963188990",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2972916088",
        "https://openalex.org/W2962796276",
        "https://openalex.org/W2916898195",
        "https://openalex.org/W2963544536",
        "https://openalex.org/W2741323980",
        "https://openalex.org/W2053154970",
        "https://openalex.org/W2963790827",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962717182",
        "https://openalex.org/W2964352131",
        "https://openalex.org/W2963170138",
        "https://openalex.org/W2101105183"
    ],
    "abstract": "We investigate the general problem of conditioned dialogue, in which a condition label is used as input to designate the type of the target response such as a persona. A major challenge for conditioned dialogue generation is the lack of substantial dialogue data labeled with conditions. Thus, we propose to complement the labeled dialogue data with labeled non-dialogue text data, and fine-tune BERT based on them. Our fine-tuning approach utilizes BERT for both encoder and decoder via different input representations and self-attention masks in order to distinguish the source and target side. On the target (generation) side, we use a new attention routing mechanism to choose between generating a generic word or condition-related word at each position. Our model is instantiated to persona- and topic-related dialogue. Experimental results in both cases show that our approach can produce significantly better responses than the state-of-the-art baselines.",
    "full_text": null
}