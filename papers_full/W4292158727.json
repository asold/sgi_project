{
  "title": "Combining Variational Autoencoders and Transformer Language Models for Improved Password Generation",
  "url": "https://openalex.org/W4292158727",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2978460324",
      "name": "David Biesner",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A2579560983",
      "name": "Kostadin Cvejoski",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A2072919878",
      "name": "Rafet Sifa",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3150971122",
    "https://openalex.org/W3199023617",
    "https://openalex.org/W1487941708",
    "https://openalex.org/W2751688886",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6767593240",
    "https://openalex.org/W2463456957",
    "https://openalex.org/W2982664727",
    "https://openalex.org/W2105594594",
    "https://openalex.org/W4304807927",
    "https://openalex.org/W3207878002",
    "https://openalex.org/W6745535286",
    "https://openalex.org/W2135359429",
    "https://openalex.org/W4394014474"
  ],
  "abstract": "Password generation techniques have recently been explored by leveraging deep-learning natural language processing (NLP) algorithms. Previous work has raised the state of the art for password guessing algorithms significantly, by approaching the problem using either variational autoencoders with CNN-based encoder and decoder architectures or transformer-based architectures (namely GPT2) for text generation. In this work we aim to combine both paradigms, introducing a novel architecture that leverages the expressive power of transformers with the natural sampling approach to text generation of variational autoencoders. We show how our architecture generates state-of-the-art results in password matching performance across multiple benchmark datasets.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8025186657905579
    },
    {
      "name": "Transformer",
      "score": 0.7026362419128418
    },
    {
      "name": "Password",
      "score": 0.6212339401245117
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6065391898155212
    },
    {
      "name": "Encoder",
      "score": 0.5640478134155273
    },
    {
      "name": "Architecture",
      "score": 0.543056845664978
    },
    {
      "name": "Expressive power",
      "score": 0.4648434519767761
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44297558069229126
    },
    {
      "name": "Language model",
      "score": 0.41686713695526123
    },
    {
      "name": "Natural language processing",
      "score": 0.40102285146713257
    },
    {
      "name": "Machine learning",
      "score": 0.35913389921188354
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2660059928894043
    },
    {
      "name": "Engineering",
      "score": 0.0886298418045044
    },
    {
      "name": "Computer security",
      "score": 0.08397263288497925
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}