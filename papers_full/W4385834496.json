{
  "title": "Transformer-Based Spatio-Temporal Analysis for Classification of Aortic Stenosis Severity From Echocardiography Cine Series",
  "url": "https://openalex.org/W4385834496",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2109958475",
      "name": "N. Ahmadi",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2552708733",
      "name": "M.Y. Tsang",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": null,
      "name": "A. N. Gu",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2625565106",
      "name": "T.S.M. Tsang",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2640991062",
      "name": "P. Abolmaesumi",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2161849513",
    "https://openalex.org/W2972012128",
    "https://openalex.org/W3115258265",
    "https://openalex.org/W3200245047",
    "https://openalex.org/W6799579754",
    "https://openalex.org/W4295746693",
    "https://openalex.org/W3135801615",
    "https://openalex.org/W2793130599",
    "https://openalex.org/W3129554782",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W3189379416",
    "https://openalex.org/W3175580044",
    "https://openalex.org/W3035524964",
    "https://openalex.org/W2777542469",
    "https://openalex.org/W3173459793",
    "https://openalex.org/W3146828772",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W3105000875",
    "https://openalex.org/W3127209854",
    "https://openalex.org/W2964137095",
    "https://openalex.org/W4295746870",
    "https://openalex.org/W4307958054",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2981304952",
    "https://openalex.org/W3199234465",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W3204334240",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Aortic stenosis (AS) is characterized by restricted motion and calcification of the aortic valve and is the deadliest valvular cardiac disease. Assessment of AS severity is typically done by expert cardiologists using Doppler measurements of valvular flow from echocardiography. However, this limits the assessment of AS to hospitals staffed with experts to provide comprehensive echocardiography service. As accurate Doppler acquisition requires significant clinical training, in this paper, we present a deep learning framework to determine the feasibility of AS detection and severity classification based only on two-dimensional echocardiographic data. We demonstrate that our proposed spatio-temporal architecture effectively and efficiently combines both anatomical features and motion of the aortic valve for AS severity classification. Our model can process cardiac echo cine series of varying length and can identify, without explicit supervision, the frames that are most informative towards the AS diagnosis. We present an empirical study on how the model learns phases of the heart cycle without any supervision and frame-level annotations. Our architecture outperforms state-of-the-art results on a private and a public dataset, achieving 95.2% and 91.5% in AS detection, and 78.1% and 83.8% in AS severity classification on the private and public datasets, respectively. Notably, due to the lack of a large public video dataset for AS, we made slight adjustments to our architecture for the public dataset. Furthermore, our method addresses common problems in training deep networks with clinical ultrasound data, such as a low signal-to-noise ratio and frequently uninformative frames. Our source code is available at: https://github.com/neda77aa/FTC.git.",
  "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020 1\nTransformer-based Spatio-temporal Analysis for\nClassification of Aortic Stenosis Severity from\nEchocardiography Cine Series\nN. Ahmadi, M.Y . Tsang, A.N. Gu, T.S.M. Tsang, P . AbolmaesumiSenior Member, IEEE\nAbstract— Aortic stenosis (AS) is characterized by re-\nstricted motion and calcification of the aortic valve and\nis the deadliest valvular cardiac disease. Assessment of\nAS severity is typically done by expert cardiologists us-\ning Doppler measurements of valvular flow from echocar-\ndiography. However, this limits the assessment of AS to\nhospitals staffed with experts to provide comprehensive\nechocardiography service. As accurate Doppler acquisi-\ntion requires significant clinical training, in this paper, we\npresent a deep learning framework to determine the feasi-\nbility of AS detection and severity classification based only\non two-dimensional echocardiographic data. We demon-\nstrate that our proposed spatio-temporal architecture ef-\nfectively and efficiently combines both anatomical features\nand motion of the aortic valve for AS severity classification.\nOur model can process cardiac echo cine series of varying\nlength and can identify, without explicit supervision, the\nframes that are most informative towards the AS diag-\nnosis. We present an empirical study on how the model\nlearns phases of the heart cycle without any supervision\nand frame-level annotations. Our architecture outperforms\nstate-of-the-art results on a private and a public dataset,\nachieving 95.2% and 91.5% in AS detection, and 78.1%\nand 83.8% in AS severity classification on the private and\npublic datasets, respectively. Notably, due to the lack of a\nlarge public video dataset for AS, we made slight adjust-\nments to our architecture for the public dataset. Further-\nmore, our method addresses common problems in training\ndeep networks with clinical ultrasound data, such as a low\nsignal-to-noise ratio and frequently uninformative frames.\nOur source code is available at: https://github.com/\nneda77aa/FTC.git\nIndex Terms— Aortic Stenosis, Cardiac Imaging, Spatio-\ntemporal Analysis, Temporal Localization, Ultrasound\nI. I NTRODUCTION\nAortic stenosis (AS) [1] is a severe valvular heart disease\nassociated with thickening and calcification of aortic valve\n(A V) leaflets. This restricts the motion of A V leaflets and\nreduces blood flow from the left ventricle to the rest of\nThis work was supported in part by the Canadian Institute of Health\nResearch (CIHR) and in part by the Natural Sciences and Engineering\nResearch Council of Canada (NSERC).\nN. Ahmadi and M. Tsang are joint first authors.\nT. Tsang and P . Abolmaesumi are joint senior authors.\nN. Ahmadi, A.N. Gu and P . Abolmaesumi are with the Department of\nElectrical and Computer Engineering, the University of British Columbia,\nVancouver, BC, Canada.\nM.Y . Tsang and T.S.M. Tsang Faculty of Medicine, the University of\nBritish Columbia, Vancouver, BC, Canada.\nthe body. AS becomes more prevalent with age, making the\nproblem more significant alongside an aging demographic.\nClinically significant AS is fatal, with a 5-year mortality rate\nof 56% and 67% for those classified with moderate and severe\nAS, respectively, if left untreated [2]. Thus, an accessible\nmethod of screening is essential for early detection and timely\nintervention of AS.\nEchocardiography (echo) is the current clinical standard for\ndetermining the severity of AS, where three clinical markers\n(A V area, peak velocity of the valvular jet and mean pressure\ngradient) are determined primarily based on Doppler measure-\nments [3]. This information is interpreted by experienced car-\ndiologists based on the clinical guidelines to make a diagnosis.\nHowever, Doppler imaging is technically challenging for less\nexperienced users, resulting in high interobserver variability\nfor AS diagnosis.\nRecently, a body of work has emerged from both the clinical\nand deep learning communities [4][5][6][7] to directly evaluate\nAS from two-dimensional echo data. This enables evaluation\nto be accessible to a larger population in two ways: by easing\nthe workflow of screening for AS and, more importantly, by\nallowing screening to be completed without spectral doppler.\nAnatomical evaluation of the A V involves two standard-\nplane echo views, the parasternal long-axis (PLAX) and\nparasternal short-axis A V level (PSAX-Ao) (Figure 1), through\nwhich the A V is visible from two angles. These two views\nprovide information on the structure of the valve, degree of\ncalcification, speed and range of motion, all of which have an\nimpact on the severity of AS. While apical views also provide\nvisualization of the aortic valve, the opening of the aortic valve\nmay not be clearly visible on the apical 5-chamber and apical\n3-chamber views. A normal A V , as shown in Figure 1, does not\nshow signs of thickening or calcification and fully opens, thus\nblood flows out of the heart without obstruction [8]. With the\nprogression of AS, the A V thickens, its opening narrows, and\nits motion becomes more restricted. To automatically assess\nAS severity, a machine learning model should be able to\nfocus on a few pixels in an echo image representing the A V ,\nassess the A V’s calcification and thickness, and understand\nthe mobility of cusps throughout the cardiac cycle, all of\nwhich make this a fundamental and difficult task in video\nunderstanding.\nPrevious studies on automated AS assessment [5][6] trained\na deep neural network to learn the severity of AS from single\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nFig. 1. (A) Diagram of the orientation of PLAX and PSAX views and their coincidence with the AV anatomy. (B) The appearance of the normal\naortic valve in PLAX and PSAX views. The images on the left represent the closed AV, and the images on the right represent the open AV. (C)\nThe appearance of calcified aortic valve in PLAX and PSAX views, and impact of calcification and narrowness of the valve on echo studies.\nComparison of images in (B) and (C) demonstrates how calcification and thickening of the cusps present themselves in echo cine series, and how\nthe progression of AS restricts the AV’s motion.\necho images, then aggregated the predicted results of each\nimage belonging to a patient using weighted averaging, where\nPLAX and PSAX views were assigned higher weights than\nother views. Based on our experiments and previous work [4],\nconsidering temporal information about valve opening and\nclosing is also beneficial since the shape and mobility of the\nA V are the primary indicators of AS severity. We also observe\nthat in most cardiac echo cine series, only a few frames show\nthe opening and closing of the A V in an informative way that\nfacilitates clinical decision-making. As a result, a simple video\nanalysis model that is unable to pay attention to a subset of\nframes that are clinically relevant cannot provide an accurate\nclassification of AS. Our problem is further complicated as\neach echo examination may contain multiple videos, which\nmay not be equally informative of the A V structure or motion.\nBased on the above observations, we investigated several\napproaches that leverage available literature on small object\ndetection and temporal localization to tackle these challenges.\nPrevious work has demonstrated video datasets provide ad-\nditional temporal information that can be further incorporated\nfor detecting small objects (e.g., [9][10]) compared to methods\nthat only consider spatial dimension [11]. The similarity of\nsubsequent frames and slow changes in heart structure and\nbackground in echo enforces the need to capture local temporal\ncontext and small spatial changes of the aortic valve for a\ncomplete diagnosis. Additionally, to detect clinically informa-\ntive frames in echo cine series, we took a look at temporal\nlocalization. Most current research [12][13] are designed for\naction detection tasks, and use a weakly supervised learning\nmethod to identify the temporal interval of action classes.\nHowever, those methods are usually provided with a single\nor few frame-level annotations of whether a frame belongs\nto the background or is representing an action. When there\nis a lack of adequate temporal annotations (which is the\ncase in clinical labels available for AS classification), several\napproaches have proposed unsupervised temporal localization\nfor training action recognition networks [14][15].\nInspired by those works, in this paper, we present a machine\nlearning framework with the ultimate goal of developing\nPoint-of-care EchocardioGraphy to detect AS with UltraSound\n(PEGASUS). Our framework has several key design features\nthat facilitate training, including 1) using a temporal loss to\nenforce more sensitivity to small motions of A V in spatially\nsimilar frames without explicit A V localization labels; 2)\nadopting temporal attention to combine spatial representations\nwith temporal context to capture the A V motion, which is\nreduced in the presence of moderate to severe stenotic valve;\nand 3) automatically identifying the relevant echo frames that\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nN. AHMADI et al.: TRANSFORMER-BASED SPATIO-TEMPORAL ANAL YSIS FOR CLASSIFICATION OF AORTIC STENOSIS SEVERITY FROM ECHOCARDIOGRA-\nPHY CINE SERIES 3\nare more important for the final classification by learning from\nweak diagnosis labels and without explicit supervision. In\nsummary, our contributions are as follows:\n• We introduce an end-to-end spatio-temporal model with\nan efficient frame-level encoding that can learn small\nmotions in echo by leveraging Temporal Deformable\nAttention (TDA) [16] in its transformer architecture. The\nmodel also adopts temporal coherence loss [17] to enforce\ndetecting small spatial changes across frames.\n• We introduce an attention layer to aggregate the disease\nseverity likelihoods over a sequence of echo frames to\nproduce a cine series-level prediction. These attention\nweights leverage temporal localization to find the most\nrelevant frames in each cine series. We show that high\nattention weights consistently correlate with informative\nframes in each cine series.\n• We demonstrate state-of-the-art accuracy on two clinical\nAS datasets, improving upon previous models for AS\nseverity classification while having considerably less pa-\nrameters compared to other video analysis models such\nas ResNet(2+1)D [18] and TimeSformer [19].\nII. R ELATED WORK\nThe recent success of deep learning in analyzing medical\nimaging data, combined with the proliferation of medical\nimaging in clinical practice, are major motivators for the\nautomation of AS diagnosis. This is particularly important in\nhospitals that are strained for staff or remote environments\nwhere access to cardiac imaging or expertise in cardiovascular\nmedicine is sparse. These automated methods include the\nassessment of AS using a variety of data types.\n(a) Image Analysis: Kang et al. [20] used radiomics\nfeatures from computed tomography A V calcium scoring (CT-\nA VC) to train a classifier for separating severe from non-severe\nAS, and noted the diagnostic accuracy is comparable to non-\nautomated methods. Chang et al. [21] used deep learning to\nautomatically segment calcified regions discovered by CT and\npredicted the severity of AS. Huang et al. [5] [6] applied a\nWideResNet [22] to predict the view and AS grading based\non single two-dimensional echo images. They subsequently\naggregated the predictions from each image belonging to the\nsame patient to form a prediction at the patient level. Since\nmost views are clinically uninformative and irrelevant, they\nconducted the final classification by a weighted sum of image-\nlevel logits, favoring the relevant views such as PLAX and\nPSAX.\n(b) Video Analysis:Roshanitabrizi et al. [23] used Doppler\ndata of the PLAX and PSAX views to detect rheumatic heart\ndisease (RHD), another pathology that can affect the A V .\nAn ensemble method of 3D-Convolutional Neural Networks\n(CNN) and a transformer classify between normal and RHD\ncases. In point-of-care ultrasound devices, however, spectral\nDoppler is not generally available. Ginsberg et al. [4] proposed\na video analysis approach to AS severity grading using two-\ndimensional echo cine series of the PLAX and PSAX views.\nThey used a multi-task, uncertainty-aware training scheme\nwith ResNet-18 2+1D [18] as the backbone model. They\nshowed that multi-task training improves the model’s general-\nization. This network cascades 1D temporal convolutions with\n2D spatial convolutions. However, their work assumed each\nportion of the video is equally informative; thus, the impact\nof each frame on the final classification cannot be visualized\nor weighted accordingly. Dai et al. [24] uses 3D convolutional\nnetworks to estimate three Doppler measurements to detect AS\nseverity levels. Vimalesvaran et al. [7] detected the presence\nof AS and aortic regurgitation using cardiac MRI cines. The\nalgorithm is first trained on supervised key-point labels of the\nA V leaflets and blood flow jets, which are visible on MRI. An\nexpert system and random forest performed feature extrac-\ntion on the key-points and predicted pathology, respectively.\nCompared to fully deep architectures, their method is more\ninterpretable.\nIn this work, we introduce a hand-crafted transformer-based\narchitecture that is trained in an end-to-end approach and\ncaptures slight motions of A V without requiring any key-\npoint labels while providing attention weights that represent\nthe informativeness of frames within each cine.\nIII. M ETHOD\nA. Model Overview\nThe overall architecture of our proposed framework is\nshown in Figure 2. Within every batch comprising B elements,\ngiven an input video (i.e. echo cine series) of arbitrary length,\nX ∈ RF×H×W×3, each frame is first encoded to a D-\ndimensional vector using a ResNet-18 based encoder. Frame-\nlevel feature vectors are concatenated to form a sequential\nrepresentation Xv ∈ RF×D. The features extracted from\nthe video are then fed to a temporal encoder to capture\nthe temporal context in the input feature sequence. In the\nfinal layer, the network is divided into three branches. The\nfirst branch calculates attention weights [14] using a fully\nconnected layer, which provides an importance score for each\nframe. Class-specific confidence scores are derived from the\nsecond branch, which are then aggregated favoring attention\nweights with higher values to provide probability distribution\nfor each video. The third branch provides a temporal loss,\nwhich ensures that the small local changes among subsequent\nframes are encoded in embeddings. Overall, the model is\ntrained with a weighted sum of three losses:\nL =Lcross entropy+\nαLattention entropy + βLtemporal coherent. (1)\nAll losses backpropagate through the same network, and\nhyperparameter values are identified based on the impact of\neach term on the total loss and refined using an empirical\nhyperparameter search.\nB. Temporal Positional Embeddings\nThe temporal position and order of frames are essential\nfor accurate video understanding. In typical attention architec-\ntures, the attention module would perform identical inference\non all frame-level embeddings, which does not provide infor-\nmation about the temporal relationships of the input frames.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nFig. 2. Overview of the proposed machine learning framework. (A) Embeddings are extracted from each frame. (B) Extracted embeddings from\nframes of each ultrasound cine series are concatenated to create cine series level embedded features with the addition of temporal positional\nembedding. (C) The temporal encoder processes the temporal relation of embeddings. (D) Output embeddings are mapped to each class using\nattention weights. The total loss backpropagates into the whole network. In this context, B represents the number of elements in the batch, F\nrepresents the number of frames in the video, and H and W represent the height and width of each frame, respectively.\nConsequently, we use positional embedding based on time\nsteps to provide order and temporal context to the input frames.\nWe leverage from positional embedding used in [25] to encode\nthis order in each video feature such that\nP(λ, τ) =\n(\nsin(wi.τ) if λ = 2i\ncos(wi.τ) if λ = 2i + 1\nwi = 1\n10002i/D ,\n(2)\nwhere τ=1,...,F represents temporal position, and λ=1,...,D\nrepresents location of each instance in an embedding. Thus,\neach time step in the sequence has a unique encoding, and the\ndistance between two-time steps is consistent even for videos\nof different lengths. The process of incorporating positional\ninformation into the frame-level embeddings involves addition\nof variable P to the existing embeddings. This step results in an\nupdated representation that reflects the temporal relationships\nof the frames within the sequence:\nXe = Xv + P. (3)\nC. Temporal Encoder\nMuch of the information related to AS severity is derived\nfrom the clinical assessment of echo videos, such as the\nopening and closing of the A V and the motion of the heart\nchambers. The temporal encoder uses temporal deformable\nattention (TDA) to enhance frame-level features with temporal\ninformation from nearby frames. Overall, the encoder consists\nof two transformer encoder layers inspired by [16], which\nreplaces the dense attention found in typical transformer\nmodels with TDA followed by a feedforward network. Similar\nto the vanilla transformer architecture [25], outputs of each\nsub-layer are fed to a residual connection and normalization\nlayer.\nTemporal Deformable Attention: Unlike action recognition\ntasks where an action can be seen in spatially distant frames,\nthe A V motions observed in echo cine series are both small\nand local. To mitigate this issue, we take advantage of TDA\n(Figure 3). This attention module only samples small sets\nof key temporal locations around chosen reference points,\nindependent of embedding size. Given an input video feature\nXe ∈ RF×D, for each query with index q and feature vq ∈ RD\nand its normalized position in time tq ∈ [0, 1], where 0\ncorresponds to the first frame and 1 corresponds to the last\nframe, the TDA feature is defined as\nhm =\nKX\nk=1\namqk.WmXe((tq + ∆tmqk)F), (4)\nT DA(vq, tq, Xe) = WoConcat[h1, h2, ..., hm]M\nm=1, (5)\nwhere hm is the output of the m-th head of TDA, amqk is the\nattention weight of the mth sampling point in the kth attention\nhead for the qth query. It is computed by performing a linear\nprojection on each query, vq, and subsequently normalizing the\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nN. AHMADI et al.: TRANSFORMER-BASED SPATIO-TEMPORAL ANAL YSIS FOR CLASSIFICATION OF AORTIC STENOSIS SEVERITY FROM ECHOCARDIOGRA-\nPHY CINE SERIES 5\nFig. 3. Illustration of the temporal deformable attention (TDA) module. The input is an F × D matrix, where each vector represents one frame in\nthe video. For each reference point in the temporal sequence, two linear projections are applied to the query feature vq ∈ RD. The first branch\nencodes a small set of temporal offsets, which are then used to obtain key temporal locations. Normalized attention weights are derived by applying\na softmax operator to the output of the second branch. The sampled key-points select elements from values which is a linear projection of input.\nSelected elements are then aggregated using attention weights for each attention head. The values are concatenated and fed into a linear projection\nto calculate the output. We show only one reference point and four sampled keys for a clear presentation.\nresulting values using a softmax function ( PK\nk=1 amqk = 1),\nWm ∈ RD×D/M and Wo ∈ RD×D are the learned weights,\nm is the index of the attention head, M is the total number\nof attention heads, k is the index of the sampled key, K\nrepresents the total number of sampled keys, and F is the\nscalar video length. ∆tmqk is the sampling offset w.r.t tq for\nthe kth sampled key and mth attention head. To look up\nthe value, we access Xe at the (tq + ∆tmqk)F-th position.\nSince (tq + ∆tmqk)F may be a decimal, we use bi-linear\ninterpolation in the time dimension on elements of Xe.\nD. Attention Branch\nIn this branch, attention weights are calculated by applying\na Multi Layer Perceptron (MLP) to output embeddings of the\ntemporal encoder module. Attention weights are normalized\nvia softmax along the temporal dimension. The weights in-\ndicate the importance of each frame in the final diagnosis\nprobabilities. Since the frame-to-frame differences caused by\nA V motion can be small, the differences between frame em-\nbeddings are generally small too. In our design, we discourage\nattention weights from being too similar for each frame in the\nvideo. To achieve this, we add an entropy loss term based on\nthe normalized weights to encourage sparsity:\nˆα = σF (α),\nLattention entropy = −\nFX\nτ=1\nˆατ log(ˆατ ). (6)\nWhere σF denotes softmax normalization across the temporal\ndimension F, α ∈ RF and ˆα ∈ RF are the attention weights\nbefore and after normalization, respectively.\nE. Classification Branch\nIn order to derive the final cine series-level prediction, we\nuse the attention weights for a weighted sum of class-specific\nlogits. The probabilistic distribution of each class is calculated\nas follows:\np(y = c|x1:F ) ∝\nFX\nτ=1\nˆατ σ(fθD (xτ ))c, (7)\nwhere fθD (.) is the output of the classification branch, and\nσ(.) denotes softmax across classes.\nFor patient-level classification, we utilize entropy as an\naleatoric uncertainty measure by employing four probabilities\nobtained from cine-level prediction. This allows us to assess\nthe informativeness of each video. However, videos with an\nentropy value exceeding 0.3 are excluded from the analysis.\nThen we use majority voting to derive the final patient-level\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nclassification based on instances the model is more confident\non. In cases where there is a tie, the maximum severity\nbetween those classes is selected.\nF . Temporal Coherent Branch\nIdeally, the frame features are consistent (i.e. have low\nvariation) for adjacent frames but are still diverse as a dis-\ntribution. To induce this property, we introduce a loss inspired\nby SyncNet [17], which tries to increase the similarity between\nadjacent frames and the distance between distant frames. This\nloss forces the model to create more distant embeddings for\nframes with small spatial differences such as those in our\ndataset. We formulate this loss as below:\nLtcl = 1\nF (\nFX\nτ=1\n−log( esτ\nesτ + P\nw edτ,w\n))\nsτ =\n\n\n\nvT\nf1 vf2 if τ = 1\nvT\nfF−1 vfF if τ = F\n1\n2 vT\nfτ−1 vfτ + 1\n2 vT\nfτ vfτ+1 otherwise\ndτ,w = vT\nfτ vfw if |τ − w| > T ,\n(8)\nwhere vfτ is the feature that represents frame fτ , sτ is calcu-\nlated using the inner product of temporally adjacent frames,\ndτ,w is the inner product of distant frames, w ranges from\n1 to F, and T is the minimum temporal distance, measured\nin frames, that is considered distant. T was assigned three in\nour experiments. The computation of TCL is quadratic with\nrespect to the number of frames due to the need to compute at\nleast F −2T and at most F −T similarities to find P\nw edi,w ,\nbut its has a low impact on runtime.\nG. Dataset\nWe conduct experiments on two datasets: 1) a private video\ndataset, and 2) the TMED-2 [6] public image dataset, for AS\nclassification and grading AS severity.\nPrivate AS Dataset:The private dataset was sourced from a\nuniversity-affiliated tertiary care hospital. Data were extracted\nwith permission from the Information Privacy Office and the\nClinical Medical Research Ethics Board. Cines were extracted\nfrom Philip IE33 and VividE9 ultrasound machines. In accor-\ndance with the American Heart Association Guidelines [3],\nAS severity levels were determined based on the three markers\nrelated to AS, namely A V area, peak valvular jet velocity, and\nmean pressure gradient provided in echo reports, resulting in\nan equal distribution of normal, mild, moderate, and severe\ncases. Furthermore, we only included studies with at least one\nPLAX or PSAX view and agreement between the calculated\nA V area and other Doppler parameters in terms of AS severity\ngrading. In this proof-of-concept study, the exclusion of dis-\ncordant cases refined our data and facilitated the development\nof a well-trained machine learning model. To establish the\ngeneralizability of this model, future studies will evaluate its\nperformance in a wider population of individuals with aortic\nstenosis.\nThe two-dimensional PLAX and PSAX cine series of the\nselected studies were extracted from the hospital Picture\nArchiving and Communication System as follows. The echo\ndata were anonymized in the hospital; all patient-identifying\ninformation and the echo-cardiogram tracing were removed\nfrom frames by applying a cine-shaped mask over the two-\ndimensional echo recording. We also removed any videos\ncontaining color or spectral Doppler. A deep-learning based\nview classification method [27] was used to automatically\nselect only the PLAX and PSAX view videos. Finally, an\nexperienced echocardiographer manually reviewed each video\nand removed videos from our dataset with the wrong view\nclassification. The resultant dataset consists of only PLAX and\nPSAX videos and includes 2247 patients and 9117 videos.\nTo apply the data to our machine learning method, we\ndivided the videos into training, validation, and test sets of ap-\nproximately 70%, 20%, and 10%, respectively, ensuring mutu-\nally exclusive patients in each set. We extracted approximately\none cardiac cycle from each video based on the patient’s heart\nrate, and applied bilinear interpolation to resample the video\nto 32 frames. Subsequently, we resized each video to a spatial\ndimension of 224 × 224. We normalized the pixel intensities\nto zero mean and a standard deviation of 1. Finally, for the\ntraining set, we augmented the data using random horizontal\nflipping, rotation with the center on the beam origin and\nrandom cropping.\nPublic AS Dataset:TMED-2 dataset [6] consists of transtho-\nracic echo studies from the Tufts Medical Center from 2011-\n2020. Each study contains multiple videos from various views,\nand studies are graded using Doppler-based guidelines [3].\nSubsequently, they group the severity of AS into three cate-\ngories: no AS, early AS (mild, mild-to-moderate), significant\nAS (moderate, severe). From each video, they extract the\nfirst frame as a representative image, and provide a label\nfor the image view: PLAX, PSAX, 4-chamber, 2-chamber,\nand other. For each patient, around 50 to 100 images are\navailable. The dataset contains three groups of images with\nrespect to labels provided by board-certified sonographers or\ncardiologists, which are as follows:\n• Fully-labeled set: Images from 577 patients for which\nboth image-level view labels and patient-level AS severity\nare given;\n• View-only labeled set: Images of 703 patients for which\nonly view labels are given;\n• Unlabeled set: 5287 patients without view or severity\nlabels.\nIn this study we only used the fully-labeled set, DEV479, to\ncompare to the baseline set by Huang et al [6]. The train/test\nsplit was determined utilizing the generated csv file from the\nlabeled dataset.\nH. Implementation Details\nFirstly, we use a ResNet-18 [28] backbone for representation\nfeature extraction. We replace the final layer of the base\nmodel with a linear layer to yield feature vectors of dimension\n1024. The feature maps of each video are stacked to form\nvideo features of size F × 1024. Video features are fed\nto the temporal encoder. For the TDA sublayers, we use\nattention heads M = 8 and sampling points K = 4 . The\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nN. AHMADI et al.: TRANSFORMER-BASED SPATIO-TEMPORAL ANAL YSIS FOR CLASSIFICATION OF AORTIC STENOSIS SEVERITY FROM ECHOCARDIOGRA-\nPHY CINE SERIES 7\nTABLE I\nTEST ACCURACY COMPARISON WITH STATE -OF-THE -ART ON OUR PRIVATE AS CINE SERIES DATASET . QUANTITATIVE RESULTS SHOW OUR\nAPPROACH OUTPERFORMS THE STATE -OF-THE -ART IN BOTH VIDEO -LEVEL AND PATIENT -LEVEL CLASSIFICATION . AS S EVERITY IS A FOUR -WAY\nCLASSIFICATION ENCOMPASSING THE CLASSES OF NORMAL , MILD , MODERATE , AND SEVERE , WHILE AS DETECTION ENTAILS A TWO -TIER\nCLASSIFICATION INVOLVING NORMAL CASES VERSUS ALL OTHER SEVERITY LEVELS .\nAS Severity Accuracy↑ Weighted F1-Score↑ AUROC↑ AS Detection Accuracy↑\nMethod Model Number of Parameters Video-level Patient-level Video-level Patient-level Video-level Patient-level\nImaged-based models\nHuang et al. [6] WideResNet 23M 65.6% 66.70% 65.4% 66.60% 0.753 91.4%\nVideo-based models\n- TinyVideoNet [26] 11M 65.7% 69.5% 65.5% 69.4% 0.754 93.2%\nGinsberg et al. [4] ResNet(2+1)D 31M 68.4% 72.2% 67.4% 74.1% 0.777 94.2%\n- TimeSformer [19] 110M 68.8% 75.3% 67.9% 75.1% 0.779 94.7%\nOurs Ours 21.3M 69.9% 78.1% 69.7% 78.0% 0.789 95.4%\noverall loss is weighted with α = 0 .01 and β = 0 .1. The\nmodel is trained using Adam [29] with an initial learning\nrate of 0.0001 and Cosine Annealing [30] as the learning\nrate schedule. For private dataset experiments, we train the\nmodel for 100 epochs. The model is developed using PyTorch\n[31] and experiments are conducted on two 16 GB Nvidia\nTitan GPUs. The hyperparameter optimization focused on the\nnumber of attention heads, keys within the transformer module\nand the weights used to aggregate loss functions. Furthermore,\nthe metric used to guide the hyperparameter search was the\naccuracy of video-level AS severity.\nI. Quantitative Results\nTable I summarizes the test accuracy achieved by our\nmethod and various other state-of-the-art methods on the\nprivate dataset. We compare the accuracy of individual video\nclassification and patient classification using multiple videos.\nSee subsection III-E for the approach to combine predictions\nfrom multiple echo cine series. Our model outperforms other\nrecent state-of-the-art methods while having smaller number of\nparameters compared to ResNet(2+1)D [18] and TimeSformer\n[19]. The accuracy and efficiency of our method suggest the\neffectiveness of explicitly considering the temporal dimension,\nespecially with reference to the short-term nature of relevant\nA V motion, compared to vanilla video analysis architectures.\nOur findings demonstrate that AS detection accuracy is sub-\nstantially higher than AS severity grading accuracy, largely\ndue to normal cases being easier to classify and thin based\non valve appearance, with blood flow obstruction differences.\nConversely, diseased valves are usually calcified, and the\nextent of calcification and the constriction of valve motion\nvary, leading to differences in severity levels. As a result,\ndifferentiating between moderate AS and mild or severe cases\nis more difficult due to visual similarity. Furthermore, various\nfactors such as noise, blurriness, or darkness of frames can\nobscure the aortic valve in many videos, making it challenging\nto assess its condition. Therefore, developing a model that can\naccurately and reliably classify most videos remains a difficult\ntask.\nJ. Qualitative Results\n1) Clinical Importance of Attention Weights: Our results sup-\nport that learned attention weights have a direct correlation\nwith temporal clinical information. This is shown in Figure 4.\nMost frames that represent open A V have higher weights,\nand the lowest weights are associated with closed A V . We\nhypothesize that the network is taking advantage of the valve\nmotion and changes in its shape during the cardiac cycle to\nmake its prediction. Following the addition of the attention\nentropy loss, the model exhibited a greater degree of attention\nsparsity, indicating a more focused allocation of attention\nacross the frames.\n2) Coherency of Embeddings: We also analyzed the learned\nfeatures from videos by demonstrating the impact of temporal\ncoherent loss on the similarity and distinctness of the frames\nin each cine series. Figure 5 illustrates that embeddings that\nbelong to the same stage of a heart cycle are more similar to\neach other and more distant to frames that represent another\nphase of the cycle.\n3) Failure Study: We applied an extensive failure study on\npatients with a large number of mislabelled videos. Videos are\noften misclassified with the presence of noise, the darkness of\nframes, poor image quality, and invisibility of the A V and its\ncusps. We visualized three failure cases from the test set of\nour private dataset in Figure 6. For all three samples, there\nis a two-level difference between the prediction and ground\ntruth, which can largely impact clinical outcomes. In the first\nfailure example, the A V is visible in all frames; however,\nthe cusps cannot be seen. Therefore, the calcification and the\nnarrowness of open valve cannot be estimated accurately. But,\nas the layout of the valve is clear, the attention weights have\nassigned higher weights to frames with an open A V . In the\nsecond example, most frames are dark, so, they provide little\nclinical information. As a result, large sections of the heart\nstructure and small motions are undetectable. This similarity\namong frames and its difference with common cine series in\nthe training set resulted in fairly similar and uninformative\nembedding space. The third example shows a video of good\nquality in which the A V is not visible. Again, attention weights\nwere able to detect frames representing heart contraction.\nK. Empirical Ablation Analysis\nThe contribution of each model component was analyzed\nby performing an ablation study. Different components were\neliminated or replaced; video-level and patient-level accuracy\nwere used to compare different settings (see Table II).\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nTABLE II\nABLATION STUDY OF NETWORK COMPONENTS ON THE VALIDATION SET OF OUR PRIVATE DATASET , STUDYING THE IMPACT OF EACH COMPONENT .\nCE: C ROSS ENTROPY . ATTE: ATTENTION ENTROPY . TCL: T EMPORAL COHERENT LOSS . SUPCON: SUPERVISED CONTRASTIVE LEARNING\nModel Architecture Loss Functions Accuracy ↑\nImage Encoder Transformer Weakly-supervised Aggregation Pretraining CE AttE TCL Video Level Patient Level\nResNetAE BERT Averaging - ✓ × × 61.9% 64.5%\nResNet-18 BERT Averaging - ✓ × × 63.6% 65.9%\nResNet-18 BERT Averaging SupCon ✓ × × 65.8% 67.3%\nResNet-18 TTE Averaging - ✓ × × 68.2% 74.2%\nResNet-18 TTE Averaging SupCon ✓ × × 67.8% 73.0%\nResNet-18 TTE Attention Weight - ✓ × × 68.6% 75.1%\nResNet-18 TTE Attention Weight - ✓ ✓ × 68.7% 75.3%\nResNet-18 TTE Attention Weight - ✓ × ✓ 69.0% 76.7%\nResNet-18 TTE Attention Weight - ✓ ✓ ✓ 69.4% 77.8%\nFig. 4. Qualitative examples of how attention weights have learned the informativeness of frames. The diagram on the right shows the attention\nweights associated with 32 frames of each video. Three sample frames of each video are shown on the left side. The orange arrows show the\ninterval of frames with an open AV and its associated attention weights. Blue arrows represent the close AV both before and after heart contraction.\n1) Impact of Each Layer on Representation Extraction:\nReplacing the ResNet-18 encoder with ResNetAE based on the\nwork of [32] showed that the ResNetAE embeddings provide\na good representation of each frame, but the temporally\ndistant frames in each video produced similar embeddings due\nto the similarity of their spatial dimension. This prohibited\nthe model from learning the temporal variation throughout\nthe cine. We experimented with both temporal transformer\nencoder (TTE) and BERT architectures for temporal encoding.\nWe observed that BERT could not capture the small local\nchanges between the embeddings of adjacent frames. We\nvalidated this by comparing the accuracy between a model\nwith ResNet-18 and the BERT encoder and a model using\nonly the ResNet-18 layer without the transformer encoder that\naveraged the embedding for all frames. We observed that the\nchange in accuracy was not significant when we added the\nBERT encoder. This indicates that the BERT encoder was\nunable to capture the temporal information. However, using\nthe temporal transformer encoder resulted in a 4.6% increase\nin video-level accuracy. This indicates the notable impact of\nreplacing dense attention with TDA.\n2) Aggregation Method: We tested two aggregation methods\nto calculate the class-level probabilities. In the first method, all\nlogits are averaged, which disregards the importance of each\nframe in the cine series and its impact on the final diagnosis.\nIn the second method, we used normalized attention weights\nas a weighted score to combine class-specific predictions.\nOur experiments show attention weighting, even without the\nattention entropy loss, yields slightly better accuracy.\n3) Pretraining Weights of Encoder: We tried pre-training\nweights of ResNet-18 using supervised contrastive loss (Sup-\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nN. AHMADI et al.: TRANSFORMER-BASED SPATIO-TEMPORAL ANAL YSIS FOR CLASSIFICATION OF AORTIC STENOSIS SEVERITY FROM ECHOCARDIOGRA-\nPHY CINE SERIES 9\nFig. 5. The upper figure illustrates the pairwise similarity of frame-level\nrepresentations based on their cosine distance. The lower figure exhibits\nthe attention weights of each frame. As shown, similar frames have been\ndivided into two subgroups. The first group represents a phase of the\nheart cycle with an open AV. We can see that these frames also have\nhigher attention weights. Comparatively, the second subgroup mostly\nbelongs to frames with closed AV and they have lower attention weights.\nFig. 6. This figure shows three failure cases, with representative frames\nof a closed and an open AV in order from left to right. (a) Visible AV\nwith undetectable cusps due to noise. However, attention weights were\nable to detect phases of the heart cycle but not calcification. (b) Fairly\ndark and uninterpretable frames. Fairly similar embedding as a result.\n(c) Good video quality, but in most frames AV can not be detected.\nStill, because of having fairly good quality video, attention weights could\ndetect frames with open AV.\nCon) to learn more informative representations. Then, we froze\nits weights during training. However, this did not result in\nany improvement. Based on our experiments, we conclude\nthat good representation extraction is not sufficient, and our\nempirical studies validate the advantage of end-to-end learn-\ning, especially with the impact of temporal coherent loss on\nlearning better overall representations.\n4) Impact of Each Loss Function: As we can see in Table II,\nthe attention entropy (AttE) loss only improves the accuracy\nby 0.4%. However, before adding the loss, weights assigned\nto each frame were more similar. Therefore, this loss has\na positive influence on the sparsity of informative frames.\nSince the frames of a cine are visually similar due to small\nchanges caused by muscle contraction and valve movement,\nfor certain examples, we have observed that there was a lack\nof significant differences in attention weights. However, in\nmost samples, frames that show an open aortic valve have\nhigher weights assigned to them after addition of the loss.\nThe temporal coherent loss (TCL) improves the accuracy by\n0.7%. Finally, we use all losses to train the action localization\nmodel, and achieve an accuracy of 69.4%, implying that each\nloss contributes to the overall accuracy.\nL. Evaluation of Our Method on Public Dataset\n1) Reproducing the Baseline: Due to the lack of a large\npublic video dataset with AS diagnosis labels, we tested our at-\ntention aggregation method on the TMED-2 [6] image dataset.\nTo reproduce their results, we resized each image to 224×224.\nWe trained a multitask WideResNet-50-2 network [22] to\nprovide a label for image view (PLAX, PSAX, and others)\nand severity of AS (no AS, early AS, i.e. mild or mild-to-\nmoderate, and significant AS, i.e. moderate or severe). At\ninference time, images with high view classification entropy\nwere disregarded for each patient, and the summation of\nthe probability of relevant views (PLAX and PSAX) was\ncalculated, where thresholding was used to select images with\na high likelihood of belonging to one of the clinically relevant\nviews. The weights of the selected images were adopted to\nperform a weighted aggregation:\nr w(x) = σ(fθv (x))PLAX + σ(fθV (x))PSAX\nˆr w(x) =\n\n\n\n0 if r w(x) < τ1\n0 if H(r w(x)) > τ2\nr w(x) else\np(y = c|x1:n) ∝\nnX\ni=1\nˆr w(xi)σ(fθD (xi))c,\n(9)\nwhere f is the network, θv is the view classifier parameters,\nθD is the AS diagnosis classifier parameters, σ(.) denotes\nsoftmax, rw and ˆrw are the relevance weight before and after\nthresholding, τ1 are the confidence thresholds of belonging to\nrelevant views, and τ2 is a threshold for having a low entropy\nfor the predicted probabilities. With our implementation, we\nwere able to obtain slightly better results compared to those\nreported in [6]. The values selected for τ1 and τ2 were 0.7\nand 0.3, respectively.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\n2) Implementation of Our Method: As TMED-2 is image-\nbased, we trained our model without considering the trans-\nformer layer and temporal coherent loss and replaced ResNet-\n18 with WideResNet to be able to compare the results. Since\nthe attention module is trained on groups of images and the\nnumber of images per patient is variable, for each patient, these\nimages were fed into the model for feature extraction. Three\nMLPs were applied on image-level embeddings to obtain\nattention weights, view classification, and AS classification.\nSince the attention module operates on groups of images\nbelonging to the same patient, the network was trained at\na patient-level where the number of images per patient is\nvariable. To accommodate for variable-length input, we binned\nthe patients based on the number of images and defined\nmultiple data loaders, each for a different bin. Attention\naggregation was used to obtain the severity of AS from the\nmultiplication of the AS classification branch and view rele-\nvance. We also added the entropy loss for attention weights to\nlearn more informative images. Patient-level training increased\nthe accuracy of AS detection and AS severity classification to\n91.5% and 83.8%, respectively. Compared to the aggregation\nof the image-level model at inference time, (see Table III), the\naddition of attention weights had a significant impact on the\ncalculated probability distribution. One reason behind this may\nbe that although only PLAX and PSAX views are clinically\nrelevant, not all PLAX and PSAX images provide sufficient\ninformation to diagnose AS. The attention map can learn to\nchoose more informative images during training:\np(y = c|x1:n) ∝\nnX\ni=1\nˆαiˆr w(xi)σ(fθD (xi))c, (10)\nwhere ˆα are the attention weights normalized across images.\nTABLE III\nPATIENT-LEVEL AS SEVERITY DIAGNOSIS CLASSIFICATION IN THE\nTMED-2 DATASET. COMPARISON WITH THE STATE -OF-THE -ART\nMETHOD [6] AND THE DIFFERENCE IN AGGREGATING IMAGES FOR\nPATIENT-LEVEL DIAGNOSIS .\nMethod Aggregation Patient-level\nAccuracy↑\nHuang et al. [6] thresholding, view relevance 74.6%\nReproduced Results thresholding, view relevance 75.6%\nOurs attention based, view relevance 83.8%\nIV. C ONCLUSION\nIn this work, we introduce a novel architecture for detecting\nthe severity of AS in cardiac echo cine series. We demonstrate\nthree architectural choices that resulted in more accurate detec-\ntion and grading by: 1) leveraging from temporal deformable\nattention to increase locality awareness in transformers; 2)\nusing temporal coherent loss to capture small spatial changes\nand enforce coherency in frame-level embeddings, and 3)\nadopting attention weights for detecting frames that provide\nclinical relevance and favoring those frames in weighted\naggregation. We analyze the importance of each component in\nimproving accuracy and outperforming state-of-the-art meth-\nods. For future work, we plan to extend this framework to find\ninformative videos for patient-level classification. This may\ninclude leveraging uncertainty to disregard videos with insuffi-\ncient clinical information. We aim to include interpretability as\npart of our design and to facilitate the adoption of the approach\ntoward point-of-care ultrasound settings.\nREFERENCES\n[1] B. A. Carabello, “Introduction to aortic stenosis,” Cir-\nculation Research, Journal of the American Heart As-\nsociation, vol. 113, no. 2, pp. 179–185, 2013.\n[2] G. Strange, S. Stewart, D. Celermajer, et al., “Poor long-\nterm survival in patients with moderate aortic stenosis,”\nJournal of the American College of Cardiology, vol. 74,\npp. 1851–1863, 2019.\n[3] W. C. Members, C. M. Otto, R. A. Nishimura, et al.,\n“Guideline for the management of patients with valvular\nheart disease: A report of the american heart association\njoint committee on clinical practice guidelines,” Ameri-\ncan College of Cardiology Foundation Washington DC,\nvol. 77, e25–e197, 2021.\n[4] T. Ginsberg, R.-e. Tal, M. Tsang, et al., “Deep video\nnetworks for automatic assessment of aortic stenosis\nin echocardiography,” Simplifying Medical Ultrasound,\npp. 202–210, 2021.\n[5] Z. Huang, G. Long, B. Wessler, and M. C. Hughes, “A\nnew semi-supervised learning benchmark for classifying\nview and diagnosing aortic stenosis from echocardio-\ngrams,” Proceedings of the 6th Machine Learning for\nHealthcare Conference, 2021.\n[6] Z. Huang, G. Long, B. Wessler, and M. C. Hughes,\n“Tmed2: A dataset for semi-supervised classification\nof echocardiograms,” International Conference on Ma-\nchine Learning DataPerf workshop, 2022.\n[7] K. Vimalesvaran, F. Uslu, S. Zaman, et al., “Detecting\naortic valve pathology from the 3-chamber cine cardiac\nmri view,” Medical Image Computing and Computer\nAssisted Intervention, pp. 571–580, 2022.\n[8] L. Ring, B. N. Shah, S. Bhattacharyya, et al., “Echocar-\ndiographic assessment of aortic stenosis: A practical\nguideline from the british society of echocardiography,”\nEcho Research and Practice, vol. 8, G19–G59, 2021.\n[9] F. Xiao and Y . J. Lee, “Video object detection with\nan aligned spatial-temporal memory,” European Con-\nference on Computer Vision, pp. 494–510, 2018.\n[10] L. Chen, W. Shi, and D. Deng, “Improved yolov3\nbased on attention mechanism for fast and accurate ship\ndetection in optical remote sensing images,” Remote\nSensing, vol. 13, pp. 2072–4292, 2021.\n[11] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai,\n“Deformable detr: Deformable transformers for end-\nto-end object detection,” International Conference on\nLearning Representations, 2021.\n[12] P. Lee and H. Byun, “Learning action completeness\nfrom points for weakly-supervised temporal action lo-\ncalization,” International Conference on Computer Vi-\nsion, pp. 13 628–13 637, 2021.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nN. AHMADI et al.: TRANSFORMER-BASED SPATIO-TEMPORAL ANAL YSIS FOR CLASSIFICATION OF AORTIC STENOSIS SEVERITY FROM ECHOCARDIOGRA-\nPHY CINE SERIES 11\n[13] J. Ma, S. K. Gorti, M. V olkovs, and G. Yu, “Weakly su-\npervised action selection learning in video,” Conference\non Computer Vision and Pattern Recognition, 2021.\n[14] G. Gong, X. Wang, Y . Mu, and Q. Tian, “Learning\ntemporal co-attention models for unsupervised video\naction localization,” Conference on Computer Vision\nand Pattern Recognition, Jun. 2020.\n[15] K. Soomro and M. Shah, “Unsupervised action discov-\nery and localization in videos,” International Confer-\nence on Computer Vision, pp. 696–705, 2017.\n[16] X. Liu, Q. Wang, Y . Hu, et al., “End-to-end temporal\naction detection with transformer,” IEEE Transactions\non Image Processing, vol. 31, pp. 5427–5441, 2022.\n[17] F. T. Dezaki, C. Luong, T. Ginsberg, et al., “Echo-\nSyncNet: Self-supervised cardiac view synchronization\nin echocardiography,” IEEE Transactions on Medical\nImaging, vol. 40, pp. 2092–2104, Aug. 2021.\n[18] D. Tran, H. Wang, L. Torresani, J. Ray, Y . LeCun, and\nM. Paluri, “A closer look at spatiotemporal convolutions\nfor action recognition,” Conference on Computer Vision\nand Pattern Recognition, pp. 6450–6459, 2018.\n[19] G. Bertasius, H. Wang, and L. Torresani, “Is space-time\nattention all you need for video understanding?” Pro-\nceedings of the International Conference on Machine\nLearning, pp. 813–824, Jul. 2021.\n[20] N. Gyu Kang, Y . J. Suh, K. Han, Y . J. Kim, and\nB. W. Choi, “Performance of prediction models for\ndiagnosing severe aortic stenosis based on aortic valve\ncalcium on cardiac computed tomography: Incorpora-\ntion of radionics and machine learning,” Korean Journal\nof Radiology, vol. 22, p. 334, 2021.\n[21] S. Chang, H. Kim, Y . J. Suh, et al., “Development\nof a deep learning-based algorithm for the automatic\ndetection of aortic valve calcium,” European Journal of\nRadiology, vol. 137, 2021.\n[22] S. Zagoruyko and N. Komodakis, “Wide residual net-\nworks,” Proceedings of the British Machine Vision\nConference, pp. 87.1–87.12, Sep. 2016.\n[23] P. Roshanitabrizi, H. R. Roth, A. Tompsett, et al.,\n“Ensembled prediction of rheumatic heart disease from\nungated doppler echocardiography in low-resource set-\ntings,” Conference on Medical Image Computing and\nComputer-Assisted Intervention, pp. 602–612, 2022.\n[24] W. Dai, H. Nazzari, M. Namasivayam, and J. Hung,\n“Identifying aortic stenosis with a single parasternal\nlong-axis video using deep learning,” Journal of the\nAmerican Society of Echocardiography, Jan. 2023.\n[25] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention\nis all you need,” Advances in Neural Information Pro-\ncessing Systems, vol. 30, 2017.\n[26] A. Piergiovanni, A. Angelova, and M. Ryoo, “Tiny\nvideo networks: Architecture search for efficient video\nmodels,” 2020.\n[27] A. N. Gu, C. Luong, M. H. Jafari, et al. , “Effi-\ncient echocardiogram view classification with sampling-\nfree uncertainty estimation,” Simplifying Medical Ultra-\nsound, pp. 139–148, 2021.\n[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” Conference on Com-\nputer Vision and Pattern Recognition, pp. 70–78, 2016.\n[29] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” CoRR, 2015.\n[30] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient\ndescent with warm restarts,” International Conference\non Learning Representations, 2017.\n[31] A. Paszke, S. Gross, F. Massa, et al., “Pytorch: An im-\nperative style, high-performance deep learning library,”\nin 2019, pp. 8024–8035.\n[32] H. Reynaud, A. Vlontzos, B. Hou, A. Beqiri, P. Lee-\nson, and B. Kainz, “Ultrasound video transformers\nfor cardiac ejection fraction estimation,” Medical Im-\nage Computing and Computer Assisted Intervention ,\npp. 495–505, 2021.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3305384\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Stenosis",
  "concepts": [
    {
      "name": "Stenosis",
      "score": 0.6681729555130005
    },
    {
      "name": "Medicine",
      "score": 0.6371304392814636
    },
    {
      "name": "Cardiology",
      "score": 0.5959087610244751
    },
    {
      "name": "Internal medicine",
      "score": 0.5421361327171326
    },
    {
      "name": "Radiology",
      "score": 0.45982009172439575
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ]
}