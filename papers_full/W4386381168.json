{
  "title": "CrisisViT: A Robust Vision Transformer for Crisis Image Classification",
  "url": "https://openalex.org/W4386381168",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4364333635",
      "name": "Long, Zijun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226015068",
      "name": "McCreadie, Richard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125776175",
      "name": "Imran, Muhammad",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2798683079",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963947170",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3089289034",
    "https://openalex.org/W2768447691",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3009913665",
    "https://openalex.org/W2929141656",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2525602827",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2951255933",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W3013120352",
    "https://openalex.org/W4288751433",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2232393131",
    "https://openalex.org/W1629357148",
    "https://openalex.org/W4287026023",
    "https://openalex.org/W4221145109",
    "https://openalex.org/W2092074862",
    "https://openalex.org/W4295289379",
    "https://openalex.org/W2964122032",
    "https://openalex.org/W2295552544",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2529142661"
  ],
  "abstract": "In times of emergency, crisis response agencies need to quickly and accurately assess the situation on the ground in order to deploy relevant services and resources. However, authorities often have to make decisions based on limited information, as data on affected regions can be scarce until local response services can provide first-hand reports. Fortunately, the widespread availability of smartphones with high-quality cameras has made citizen journalism through social media a valuable source of information for crisis responders. However, analyzing the large volume of images posted by citizens requires more time and effort than is typically available. To address this issue, this paper proposes the use of state-of-the-art deep neural models for automatic image classification/tagging, specifically by adapting transformer-based architectures for crisis image classification (CrisisViT). We leverage the new Incidents1M crisis image dataset to develop a range of new transformer-based image classification models. Through experimentation over the standard Crisis image benchmark dataset, we demonstrate that the CrisisViT models significantly outperform previous approaches in emergency type, image relevance, humanitarian category, and damage severity classification. Additionally, we show that the new Incidents1M dataset can further augment the CrisisViT models resulting in an additional 1.25% absolute accuracy gain.",
  "full_text": "arXiv:2401.02838v1  [cs.CV]  5 Jan 2024\nZijun Long et al. CrisisViT\nCrisisViT : A Robust Vision\nT ransformer for Crisis Image\nClassiﬁcation\nZijun Long\nUniversity of Glasgow\nz.long.2@research.gla.ac.uk\nRichard McCreadie\nUniversity of Glasgow\nrichard.mccreadie@glasgow .ac.uk\nMuhammad Imran\nQatar Computing Research Institute\nHamad Bin Khalifa University\nmimran@hbku.edu.qa\nABSTRACT\nIn times of emergency, crisis response agencies need to quic kly and accurately assess the situation on the ground\nin order to deploy relevant services and resources. However , authorities often have to make decisions based on\nlimited information, as data on aﬀected regions can be scarc e until local response services can provide ﬁrst-hand\nreports. Fortunately, the widespread availability of smar tphones with high-quality cameras has made citizen\njournalism through social media a valuable source of inform ation for crisis responders. However, analyzing the\nlarge volume of images posted by citizens requires more time and eﬀort than is typically available. T o address this\nissue, this paper proposes the use of state-of-the-art deep neural models for automatic image classiﬁcation/tagging,\nspeciﬁcally by adapting transformer-based architectures for crisis image classiﬁcation (CrisisViT). W e leverage th e\nnew Incidents1M crisis image dataset to develop a range of ne w transformer-based image classiﬁcation models.\nThrough experimentation over the standard Crisis image ben chmark dataset, we demonstrate that the CrisisViT\nmodels signiﬁcantly outperform previous approaches in eme rgency type, image relevance, humanitarian category,\nand damage severity classiﬁcation. Additionally, we show t hat the new Incidents1M dataset can further augment\nthe CrisisViT models resulting in an additional 1.25% absol ute accuracy gain.\nKeywords\nSocial Media Classiﬁcation, Crisis Management, Deep Learn ing, Vision transformers, Supervised Learning\nINTRODUCTION\nCrisis events, such as ﬂoods, ﬁres and CO VID-19, generate si gniﬁcant attention from both news media and the\ngeneral public, leading to related content being posted to a wide variety of social media platforms, such as T witter\nor Facebook. Previous studies (Kumar et al.\n2011; Dosovitskiy et al. 2020; T o et al. 2017) have demonstrated\nthe importance of using social media as a way to acquire infor mation during a crisis event. However, the limited\ntime available to emergency responders in combination with the large volume of posts made on these platforms\nnecessitates automated tooling to extract only the actiona ble portions of that content (McCreadie et al. 2020;\nWidener and W . Li 2014). Indeed, over the last decade there have been a wide range of works examining how\nmachine learning can be used to aid emergency responders in ﬁ nding useful information during crises, primarily\nfocused on analysing the text of posted messages (He, Zhang, et al. 2016; Devlin et al. 2019; Gao et al. 2019).\nHowever, more recently there has been growing interest in th e value-add of posted photos and other imagery during\nan emergency (Imran, Castillo, Diaz, et al. 2015; Said et al. 2019; Buntain et al. 2022). Some papers aim to improve\neﬀectiveness of image classiﬁcation on crisis imagery cont ents (Imran, Alam, et al. 2020; Asami et al. 2022; X. Li,\nCaragea, et al. 2019; X. Li and Caragea 2020). Indeed, some studies have shown that images posted on soci al media\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nfor events such as ﬂoods or wildﬁres can be useful when alloca ting resources or estimating damage severity (Nguyen,\nOﬂi, et al.\n2017; Mouzannar et al. 2018; Sosea et al. 2021; Akhtar et al. 2021). As a result, the development of\nautomated tooling to analyse crisis imagery and categorize it into useful types is of growing importance. T o-date,\ndeep convolutional neural networks, e.g., ResNet and VGG16 (Nguyen, Joty, et al. 2016), have been a popular\nsolution for crisis image content categorization, that hav e reported high accuracy. These solutions rely on general\npre-trained models produced from non-crisis image dataset s as a starting point, e.g. ImageNet, and then ﬁne-tune\nthose models for a downstream task via transfer learning. It is not obvious why a model initially trained to identify\nmundane objects like cats or buildings would be eﬀective for identifying images of people needing to be rescued.\nMoreover, recent advances in the ﬁeld of computer vision hav e introduced alternative transformer-based neural\narchitectures (Dosovitskiy et al. 2020), which are suitable for large-scale multi-task pre-train ing.\nHence, in this paper, we examine whether we can improve the pe rformance of crisis image classiﬁcation tasks via\nmodels pre-trained using in-domain crisis imagery, rather than relying on a general image classiﬁcation model as\na starting point. In particular, using the state-of-the-ar t ViT architecture (Dosovitskiy et al. 2020) as a base we\npre-train new models using crisis imagery from the new incid ents1M image dataset (W eber et al. 2022), which we\nrefer to as CrisisViT models. W e have released these models f or the community, and they can be downloaded via:\n• https://github.com/longkukuhi/CrisisViT\nThrough experimentation over the Crisis Image Benchmark da taset (Nguyen, Joty, et al. 2016), we show that\nCrisisViT is more eﬀective than previous state-of-the-art deep convolutional neural models, with an increase in\naccuracy of 3.90 absolute points (from 79.18% to 83.07%). Mo reover, the proposed best CrisisViT outperforms\nall baselines, as well as an existing ViT model, by up to 1.25% absolute accuracy on average. This demonstrates\nthat a dedicated large-scale crisis image dataset is key for the crisis image content categorization task.\nRELA TED WORK\nImage content from Social Media platforms for Crisis Respon se: Social media is increasingly seen as a critical\ninformation and communication platform during emergencie s, as a channel to gather and analyze urgent information\nduring a crisis (T o et al.\n2017; Yin et al. 2015; Shekhar and Setty 2015). However, the majority of prior work in\nthis space has focused on analysing textual content posted t o these platforms rather than imagery (Imran, Castillo,\nDiaz, et al. 2015). On the other hand, recent works have begun to explore the va lue-add that crisis images posted\nto social media platforms can bring, as well as how to minimis e the costs associated to image analysis through AI\nautomation. For example, Nguyen, Oﬂi, et al. 2017 demonstrated that crisis images on social media can be used\nfor a variety of humanitarian aid activities (such as identi fying areas in need of goods and services). Meanwhile,\nAlam, Imran, et al. 2017 showed that social media images are helpful for damage asses sment during ﬂooding\nevents, while Daly and Thom 2016 illustrated that geotagged images can be used to identify aﬀ ected regions in\nneed of aid. Functionally, crisis image analysis can be seen as a classiﬁcation or tagging problem, where a human\nor machine needs to analyse the image and then assign a label o r labels to that image. T o-date the crisis image\ndomain has largely focused on four image classiﬁcation use- cases:\n• Disaster T ype Detection : The high-level classiﬁcation of the type of disaster depic ted within an image, such\nas an earthquake, ﬁre or ﬂood.\n• Informativeness/Usefulness: The classiﬁcation of images to determine whether it contai ns some form\nof valuable information for an emergency responder. T ypica lly represented as a binary informative/not\ninformative classiﬁcation.\n• Humanitarian Categories : This form of classiﬁcation is focused on what is happening w ithin the image,\nwhere the goal is to identify images that are relevant to diﬀe rent types of humanitarian response activities.\nCommon humanitarian categories include images of aﬀected i ndividuals, images of infrastructure or utility\ndamage, or images of people needing rescue.\n• Damage Severity: Finally, one common use for crisis images is to judge the sev erity of damage in a particular\narea, which is useful for response prioritization or damage costing purposes. The damage severity task mainly\ntargets three levels: severe damage, mild damage, and littl e or no damage.\nNotably, Nguyen, Joty, et al. 2016 developed a standard dataset that combines training and tes t examples for all\nfour tasks, which we use later in this paper to evaluate our mo dels.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nDeep Neural Networks for Image Classiﬁcation : In the wider ﬁeld of image classiﬁcation, the most dominant\ntype of solution is the deep learned AI model. These approach es function by taking an embedding of the pixel\ndata from the image as input, which is fed into a deep neural ne twork that extracts some meaning from that image.\nA deep neural model is trained by example, where an image is pr ovided to the model, the model then generates a\npredicted label, and then depending on whether it got the lab el correct feedback is transferred backward into the\nmodel, updating the network. Over the course of seeing thous ands to millions of example images, the model learns\nwhat pixel embeddings correlate with the desired labels. Ho wever, deep neural networks are computationally\nexpensive to train, and tend to exhibit higher accuracy if pr e-trained on multiple related tasks (Girshick et al.\n2014;\nRen et al. 2017; Redmon et al. 2016). Hence it is common for companies and researchers to releas e pre-trained\nneural models, which other developers can then adapt at a low er cost to their own use-case (known as transfer\nlearning). With regard to the structure of the neural model i tself, there are currently two competing architectures:\nconvolutional neural networks (CNNs); and transformers. C NNs have traditionally been the dominant neural\nnetwork type used for image classiﬁcation. Transformer arc hitectures have been shown to be highly eﬀective for\ntext classiﬁcation (Devlin et al. 2019), but under-perform when adapted for images due to the marke dly higher\ndimensionality (there are more pixels in an image than words in a sentence). Architecturally CNNs are advantaged\nhere, as their convolutional structure forces them to ﬁnd th e parts of the image that matter and discard the rest,\nenabling them to better generalize to unseen examples. As a r esult, pre-trained CNNs are popular choices as\nbaselines, such as ResNet152 (He, Zhang, et al. 2016) and VGG (Simonyan and Zisserman 2015).\nAdvances in Image Transformers : Over the past 5 years, signiﬁcant research eﬀorts have been made to improve\nthe eﬀectiveness of transformer architectures (V aswani et al. 2017) for image classiﬁcation. The issue with\napplying transformers for image classiﬁcation is two-fold : 1) training transformers on images is much more costly\nin comparison to a CNN, as transformer training time rapidly scales with input dimensionality due to its attention\nmechanism; and 2) transformers when applied to images have b een shown to not generalize well to unseen images,\nas they lack some of the inductive biases that are learned nat urally by CNNs. Early approaches tried to reduce\ntraining costs by applying self-attention to only the local neighbourhood for each query pixel (Parmar et al. 2018),\nor by applying attention to only small parts of the image (W ei ssenborn et al. 2020). However, an important\nbreakthrough occurred with the development of the ViT model (Dosovitskiy et al. 2020), which was the ﬁrst vision\ntransformer model to eﬃciently apply attention globally wi th minimal modiﬁcations to the transformer architecture.\nIn 2021, Masked Autoencoders (MAE) (Kumar et al. 2011) were proposed, which then further addressed the cost\nof training via the use of a high image masking strategy with a n encoder-decoder self-supervised pre-training\nschema, which enables MAE to learn how to reconstruct the ori ginal image based on only partial observations\nof that image. This approach reduces the number of pixels tha t need to be fed into the transformer and is the\nbest current solution for reducing training time. Similarl y, SimMIM (Xie et al. 2022) proposed the use of masked\nimage modelling to pre-trained vision transformers but wit hout a decoder. Meanwhile, Data2vec (Baevski et al.\n2022) introduced a teach-student mode to pretrain vision transf ormers by representation learning. These models\nare normally pre-trained based on large-scale image datase ts like ImageNet Russakovsky et al. 2015, and they can\nthen be ‘ﬁne-tuned’ with new examples to transfer the pre-tr ained knowledge into the target downstream tasks, a\nprocess that is referred to as transfer learning T orrey and S havlik 2010. Although vision transformers dominate\nin the computer vision domain, the transferability of visio n transformers remain unclear. As pointed out in\nDosovitskiy et al. 2020, lacking discernible learned inductive biases limits the p erformance of vision transformers\nto handle downstream tasks. This appears to be a core limitat ion of transformers that cannot be easily overcome,\nleading to works such as Zhou et al. 2021 performing expensive whole network ﬁne-tuning (that requi res a large\nin-domain training dataset) to adapt the pre-trained model . In this work, we aim to push the boundaries of crisis\nimage classiﬁcation performance using transformers, and h ence we need such a large in-domain crisis dataset. In\nour subsequent experiments, we evaluate whether the newly r eleased incidents1M dataset (W eber et al. 2022) is\nsuﬃcient to enable eﬀective vision transformer models for c risis image classiﬁcation.\nMETHODOLOGY\nIn this work, we investigate whether pre-training on a large -scale crisis image dataset can improve the perfor-\nmance of crisis classiﬁcation tasks. W e choose a state-of-t he-art transformer-based image classiﬁcation model,\nViT (Dosovitskiy et al.\n2020), as our base model and propose a new CrisisViT variant, whic h surpasses other\ndeep learning image models in performance and robustness fo r a range of crisis image classiﬁcation tasks. W e use\nincidents1M (W eber et al. 2022) as the large-scale crisis image dataset for training. W e tr y out various ways to\npretrain CrisisViT , such as diﬀerent pre-training strateg ies, examples used, and training labels, based on the datase t\ncharacteristics. W e discuss the implementation of these mo dels below . When building the CrisisViT models, there\nare two main decisions that need to be made: 1) determine the d ataset used to train; and 2) decide how to train\nwith that dataset.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nPretrain datasets : For the pre-training dataset, we have the option of using ei ther a known eﬀective general\nimage classiﬁcation dataset, or attempt training with a mor e specialised in-domain dataset. T o represent a general\nimage classiﬁcation dataset we use the popular ImageNet-1k image collection, while for an in-domain dataset we\nexperiment with the new incidents1M crisis image collectio n:\n• ImageNet-1k: The ImageNet Large Scale Visual Recognition Challenge (IL SVRC) (Deng et al.\n2009)\nproject provides more than 14 million human-annotated imag es that could be used to train deep neural image\nmodels. These images are labelled based on the objects depic ted, which might be mundane objects like\ncell phones, animals, or structures. Using such a dataset to pretrain a neural image classiﬁcation model\ndirects it towards the presence of known objects when later u sed for a downstream classiﬁcation task. For\nexample, it is intuitive that if trying to identify images ab out wildﬁres, being able to identify a ﬁretruck\nin the image would be useful. In order to compare our result wi th other works, we follow the settings of\nNguyen, Joty, et al. 2016 and He, Chen, et al. 2022 that use the ImageNet-1k subset of ILSVRC, which is\nits most commonly used component. ImageNet-1k has 1,000 obj ect classes (types of objects) and contains\n1,281,167 training images, 50,000 validation images and 10 0,000 test images. In the rest of the paper, we\nrefer to this dataset as ImageNet-1k.\n• Incidents1M: Incidents1M is a large-scale crisis dataset of images take n during natural disasters. This\ndataset contains annotated labels for 1,787,154 images wit h two main types: 43 incident categories (e.g.\nairplane accident, bicycle accident, car accident.); and 4 9 place categories (e.g. building outdoor, highway,\nforest, ocean, sky.). Of the 1,787,154 images, just over hal f (977,088) contain one positive label, i.e. they\nbelong to at least one of the (43+49) categories. It is possib le for images to have multiple labels (belong\nto multiple categories). Notably, W eber et al. 2022 does not release the image ﬁles, instead providing\nURLs pointing to those image ﬁles for other researchers to do wnload. As such, over time as online content\ngets deleted or becomes unavailable this dataset will shrin k. W e downloaded this dataset during the ﬁnal\nquarter of 2022, and at that time we retrieved 1,226,943 of th e images (68.7%). This crawled subset has\n671,506 images labeled positively to one or more of the incid ent type categories and 522,782 images labeled\npositively to one or more of the place categories.\nMeanwhile, for the ways of utilising mentioned pre-train da tasets, we can use these datasets either in isolation or\ntogether:\n• ImageNet-1k + Incidents1M : Under this setting we load the pre-trained weights from MAE (He, Chen, et al.\n2022)—a model prebuilt via self-supervised learning on ImageNe t-1k—to perform supervised pre-training\non the ImageNet-1k object labels to encode information abou t the thousand object classes into the model.\nW e further augment this model using training examples from t he Incidents1M dataset to teach the model\nhow to identify crisis-related information. This forms a ne w base model that we can later be ﬁne-tuned for\ndiﬀerent (crisis image classiﬁcation) downstream tasks.\n• Incidents1M only : In this setting, instead of starting from an existing model , we take a blank model and\nconduct both self-supervised and supervised training usin g the images and labels in the Incidents1M dataset.\nThis should act similarly to the above base model, but will la ck some of the more general object recognition\ncapabilities. In our later experiments, we compare these ba se models to determine whether starting from a\nmore general image classiﬁcation model or using only in-dom ain training is suﬃcient.\nPretrain tasks : Importantly, the Incidents1M dataset (W eber et al. 2022) supports two main crisis categorization\ntasks: 1) incident type classiﬁcation with 43 incident type s; and 2) place type classiﬁcation with 49 place types.\nIn eﬀect, this means that we can pre-train our base model usin g some or all of these 43+49 image types. W e\nexperiment with four ways to utilise these training images a nd labels in our experiments:\n• Binary training : As we have 43+49 labeled image types, one methodology for pr e-training a base model\nwould be to consider each of these 92 image types as a diﬀerent binary classiﬁcation task. W e can then\nincrementally train the base model to classify each of these 92 types in sequence, thereby incrementally\nbuilding up the model’s ability to identify diﬀerent types o f crisis content. W e refer to this as Binary\nclassiﬁcation pre-training and use it as a baseline. Howeve r, we remark that this might not be the best\ntraining strategy, as lessons learned by the model when trai ning during types seen early may be overwritten\nby later types (a phenomenon known as catastrophic forgetti ng).\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\n• Incident OR Places training : The second training methodology that we might employ would be to instead\ncombine the images and labels for only one of the Incidents1M tasks, i.e. the 43 incident categories or\nthe 49 place categories, into a uniﬁed set of training exampl es. In this way, we can see which of the two\nIncidents1M tasks provides more useful information for enh ancing our downstream tasks. In contrast to\nbinary training, in this setting, we do not divide our traini ng by image type and instead train all image types\nfor our selected task concurrently. Also, since an image can have multiple labels, we want to avoid sharing\nimages across categories, in this scenario if an image has mu ltiple labels, we consider it as belonging to only\nthe category denoted by the ﬁrst listed label.\n• Dual (Incident+Places) training : The ﬁnal pre-training methodology we use is duel training, which is\nidentical to Incident or Places training, with the exceptio n that we combine both tasks, rather than building\nseparate models for each of the two Incidents1M tasks.\n• Self-supervised training : Following the self-supervised training method from Maske d Autoencoders (Ku-\nmar et al. 2011), the CrisisViT model is trained using a self-supervised ap proach in which it learns to predict\nmissing patches of an image by masking out random portions of the input image and then reconstructing the\nmasked image. By employing this technique, the CrisisViT mo del can extract meaningful representations\nfrom large amounts of unlabeled data, leading to improved pe rformance on image classiﬁcation tasks.\nAll variants of CrisisViT use the same architecture as the Vi T -base model (Dosovitskiy et al. 2020), but with\ndiﬀerent hyperparameters. If the pertaining datasets are I mageNet-1k plus Incidents1M, it means we load the\npretrain weights from (Dosovitskiy et al. 2020).\nEXPERIMENT AL SETUP\nDownstream (Target) Dataset : T o evaluate how eﬀective the CrisisViT model variants are, we require a down-\nstream or target dataset, which represents one or more meani ngful crisis image classiﬁcation tasks. As discussed\npreviously, the most common uses for crisis imagery are disa ster type detection, informativeness/usefulness\nclassiﬁcation, grouping images into humanitarian categor ies, and damage severity estimation. Crisis Image Bench-\nmark (Nguyen, Joty, et al.\n2016) is a composite test collection that aggregates several dat asets together, including\nCrisisMMD (Alam, Oﬂi, et al. 2018), data from AIDR (Imran, Castillo, Lucas, et al. 2014) and the Damage\nMultimodal Dataset (DMD) (Mouzannar et al. 2018). The dataset consists of labels for four tasks:\n• T ask 1: Disaster type classiﬁcation\n– Earthquake\n– Fire\n– Flood\n– Hurricane\n– Landslide\n– Other disaster type\n– Not disaster\n• T ask 2: Informativeness\n– Informative\n– Not informative\n• T ask 3: Classiﬁcation into humanitarian categories\n– Aﬀected, injured, or dead people\n– Infrastructure and utility damage\n– Rescue volunteering or donation eﬀort\n– Not humanitarian\n• T ask 4: Classiﬁcation into damage severity categories\n– Severe damage\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nModel T ype\nPre- Training T ask Training\nSelf-Supervised Supervised Disaster Info Human Damage Time\nDataset Dataset Methodology Epochs T est T est T est T est A VG (hours)\nResNet101 CNN None ImageNet-1k Multi-Class (1k) 10 81.3 85.2 76.5 73.7 79.175 N/A\nEﬃNet (b1) CNN None ImageNet-1k Multi-Class (1k) 10 81.6 86.3 76.5 75.8 80.05 N/A\nVGG16 CNN None ImageNet-1k Multi-Class (1k) 10 79.8 85.8 77.3 75.3 79.55 N/A\nViT -Base TF ImageNet-1k ImageNet-1k Multi-Class (1k) 20 84.10 86.59 79.43 77.18 81.82 N/A\nCrisisViT TF Incidents1M Incidents1M Multi-Class (Incident) 10 84.91 86.85 79.43 77.96 82.29 36\nCrisisViT TF Incidents1M Incidents1M Multi-Class (Incident) 20 84.73 86.61 79.60 77.31 82.06 420\nCrisisViT TF Incidents1M Incidents1M Multi-Class (Places) 10 84.95 87.85 80.16 78.75 82.93 ∗ 420\nCrisisViT TF Incidents1M Incidents1M Multi-Class (Places) 20 85.26 87.97 80.34 78.72 83.07 ∗ 420\nCrisisViT TF Incidents1M Incidents1M Multi-Class (Incident+Places) 10 85.01 86.85 79.60 77.31 82.19 430\nCrisisViT TF Incidents1M Incidents1M Multi-Class (Incident+Places) 20 84.57 86.69 79.23 77.41 81.98 430\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Multi-Class (Incident) 10 84.88 87.17 79.64 78.29 82.49 34\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Multi-Class (Incident) 20 85.23 87.08 79.71 78.47 82.62 ∗ 48\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Multi-Class (Places) 10 85.04 87.04 80.15 78.16 82.60 ∗ 34\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Multi-Class (Places) 20 85.04 87.51 79.88 78.01 82.61 ∗ 48\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Binary (Incident+Places) 20 81.13 84.15 75.60 75.71 79.14 460\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Multi-Class (Incident+Places) 10 85.01 87.13 80.42 78.19 82.69 ∗ 36\nCrisisViT TF ImageNet-1k ImageNet-1k+Incidents1M Multi-Class (1k) + Multi-Class (Incident+Places) 20 84.95 86.92 79.40 77.70 82.24 52\nTable 1. Experimental result overall baselines and variant s of CrisisViT model. We use ∗to denote a signiﬁcant\ndiﬀerence between the performances of the ViT baseline and t he proposed model, according to the paired t-test\nwith the Holm-Bonferroni correction for p <0.01.\n– Mild damage\n– Little or none\nThis crisis image benchmark provides both training and test ing for the four tasks. W e follow the same experimental\nsetup for the training, validation, and testing splits as in the original paper (Nguyen, Joty, et al. 2016).\nMetrics: W e evaluate the performance of CrisisViT models in terms of their classiﬁcation accuracy. Note that all\nmetrics are reported on the same test set of the Crisis Image B enchmark dataset. Each experiment is run at least\nthree times, and we report the average of the results.\nBaselines: Our overall goal in this paper is to determine to what extent a large-scale crisis image dataset (IncidentM1\nin this case) improves the performance of transformer-base d image classiﬁcation models when performing crisis\ncontent categorization, as well as to determine best practi ces during training. Hence, in our later experiments we\nwill compare our CrisisViT model to a number of either popula r or state-of-the-art image classiﬁcation models\nﬁne-tuned and evaluated on the crisis image benchmark, but t hat do not have knowledge on the Incidents1M\ndataset:\n• ResNet101: (He, Zhang, et al.\n2016) proposes ResNet, a convolutional neural network with a dee p residual\nconnection, which achieves very high accuracy on the ImageN et dataset. ResNet101 is a deeper variant of\nResNet with 101 layers.\n• EﬃNet (b1) : (T an and Le 2019) studies model scaling and identiﬁes that carefully balanc ing network depth,\nwidth, and resolution can lead to better performance. EﬃNet (b1) is the second smallest version of EﬃNet,\nwhich achieves similar performance to ResNet101 but with a s maller size.\n• VGG16: A convolutional neural network proposed by (Simonyan and Z isserman 2015) that performs well\non a wide range of tasks. It has 16 convolutional layers.\n• ViT -Base: ViT (Dosovitskiy et al. 2020) is the ﬁrst vision transformer model to eﬃciently apply att ention\nglobally with minimal modiﬁcations to the transformer arch itecture, achieving remarkable performance on\nvarious datasets. ViT -Base is the base version of ViT with 12 layers and 768 hidden size dimension.\nCrisisViT Parameters: As with all machine learning models, there are a number of hy per-parameters that can aﬀect\nthe performance of the resultant model. W e pre-trained Cris isViT with self-supervised learning and supervised\nlearning on the Incidents1M dataset by using the Adam optimi ser, a batch size of 1024 and 128, respectively, and\nthe ReLU activation function. W e also experimented with oth er batch sizes [32,64,128,256,512], which led to\nlower performance of ViT . W e ﬁxed the training epoch for self -supervised training on the Incidents1M dataset at\n400, and separately tested the models with supervised train ing pre-training steps of 10 and 20 epochs on the same\ndataset. The performances for each experiment are reported separately.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nEXPERIMENT AL RESUL TS\nT o evaluate what extent the Incidents1M large-scale crisis image dataset can increase the performance of the image\nclassiﬁcation models for a range of tasks, we divide our anal ysis into three primary research questions, based on\nthe diﬀerent ways that Incidents1M can be utilised:\n• RQ1: T o what extent can transformer-based architectures ou tperform traditional convolutional neural net-\nworks (CNNs) in image classiﬁcation tasks?\n• RQ2: What are the optimal pre-training strategies for the Cr isisViT model when pre-training on the\nIncidents1M dataset?\n• RQ3: Does starting from a more general image classiﬁcation m odel, such as ImageNet-1k, provide signiﬁcant\nadvantages over using only in-domain training with the Inci dents1M dataset in terms of the performance and\nrobustness of the CrisisViT model for crisis image classiﬁc ation tasks?\nIn this section, we report the results comparing the perform ances between state-of-the-art deep convolutional\nneural image models, the transformer-based image model ViT , and diﬀerent variants of our proposed CrisisViT\nmodel, produced for crisis content categorization on four t asks of the Crisis image benchmark dataset.\nRQ1: ViT vs. Convolutional neural baselines\nW e begin by determining how well a transformer image classiﬁ cation architecture like ViT performs for the domain\nof crisis image classiﬁcation. Since most prior works (as di scussed in the related work) employed the convolutional\nneural network (CNN) architecture, we intend to understand if transformer-based models make a diﬀerence. T o\nthis end, we compare three CNN baselines, namely ResNet101, EﬃNet (b1) and VGG16, with the best transformer\narchitecture ViT . What diﬀerentiates the three cases is the ir training starting point, i.e., the base model. W e train\nthe base model for four downstream (target) tasks to produce corresponding four models. The ﬁrst four rows of\nT able\n1 report the performance of these models under the test set for each task.\nAs shown in T able 1, Vit outperforms the other three CNN-based models. Speciﬁc ally, the ViT transformer\nmodel appears to be primarily advantaged when used for Disas ter T ype classiﬁcation, Humanitarian category\nclassiﬁcation, and Damage Severity estimation, with repor ted gains over the next best CNN-based model by 3.5%,\n1.4% and 1.4% absolute classiﬁcation accuracy, respective ly. Meanwhile, a much smaller but notable gain of 0.3%\nis observed for the Informativeness classiﬁcation. Overal l, this conﬁrms our expectation that transformer-based\nmodels are the current state-of-the-art in this domain, and as such, we use ViT as our primary comparison point in\nthe remainder of this paper.\nRQ2: Pre-training using Incident T ypes and Place Categorie s\nHaving shown that the Transformer architecture ViT is super ior to prior CNN-based architectures and quantiﬁed\nour baseline performance, we now examine the core question o f this work: can we further boost the performance\nusing a large-scale crisis image dataset? The underlying ra tionale is that by teaching a deep neural model how\nto tackle a diﬀerent but related task helps the model learn th e downstream task better. In this scenario, we used\nthe Incidents1M dataset with labels for 43 incident-type an d 49 place categories. These tasks are diﬀerent but\nconceptually related to our four downstream tasks (i.e., di saster type classiﬁcation, informativeness/Usefulness,\nhumanitarian category classiﬁcation, and damage severity estimation). Rows 5-10 of T able\n1 report the accuracy\nof the ViT architecture when pre-trained using Incidents1M instead of ImageNet-1k (as was used in the ViT -\nBase baseline). W e report performance when pre-training wi th Incident type labels (Multi-Class (Incident)), place\ncategorization labels (Multi-Class (Place)), and both (Mu lti-Class (Incident+Places)). W e also report performance s\nfor 10 and 20 epochs to illustrate how performance improves w ith more training time.\nFrom T able 1, we observe that in nearly all cases pre-training on Inciden ts1M leads to superior performances for\ncrisis image classiﬁcation than using only ImageNet-1k. Fo r instance, when pre-training with the Incidents1M\nplace category labels for 20 epochs, we observe a statistica lly signiﬁcant (p ≤0.05) accuracy gain over ViT -Base\nof 1.16%. Second, comparing the Incident and Place labels pr ovided by Incidents1M, the Place labels result in the\nbest-performing downstream models in all cases, while the I ncident labels provide a comparably smaller beneﬁt\n(and in one case it harms the performance). Furthermore, we n otice that when combining the Incident and Place\nlabels together, performance does not improve over using th e Place labels alone, indicating that the Incident labels\nare redundant when the Place labels are available. Third, co mparing the eﬀect of providing more training, when\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\npre-training using the place labels, more training time (20 epochs) does lead to small performance gains (around\n0.1-0.3%). However, when providing additional training ti me to the Incident labels, downstream accuracy tends to\ndegrade, conﬁrming that teaching the model about the Incide nts1M incident types leads to questionable beneﬁts.\nOverall, we can conclude that pre-training with an in-domai n dataset can lead to performance gains. Indeed, we\nobserved between 0.9% and 1.5% accuracy across the downstre am tasks tested. On the other hand, the inconsistent\nperformance of the models pre-trained with the Incident lab els indicates that not all conceptually related tasks are\nuseful evidence for pre-training, and so researchers and de velopers should be selective regarding what datasets to\nuse for pre-training.\nRQ3: ImageNet-1k + Incidents1M?\nIn the previous set of experiments we replaced ImageNet-1k- based pre-training with pre-training using the in-\ndomain dataset i.e., Incidents1M. However, given that Imag eNet-1k forms the basis for many eﬀective image\nclassiﬁcation models in the literature, as well as providin g a strong baseline (via ViT -Base), it is worth investigatin g\nwhether we can instead augment ImageNet-1k training instea d of replacing it. Hence, we pre-train a second set\nof base models that take the same base model as ViT -Base subje ct to ImageNet-1k self-supervised and supervised\ntraining and then add further pre-training using the Incide nts1M data. If ImageNet-1k provides value on top of\nIncidents1M, combining both should result in a small improv ement in accuracy on the downstream tasks. The ﬁnal\nseven rows of T able\n1 report classiﬁcation accuracy on the four downstream tasks when we combine ImageNet-1k\nand Incidents1M pre-training.\nIn T able\n1 we can observe that the best performing Incidents1M models p re-trained on the Places labels does\nnot show performance improvements despite providing addit ional ImageNet-1k training. This might lead us to\nconclude that ImageNet-1k is not adding value. However, if w e investigate both the Incidents1M models that are\npre-trained on the Incident or Incident+Places labels, we d o observe a small performance uplift for the majority of\ntasks. Indeed, one of these models achieved the best overall performance for the Humanitarian categorisation task.\nOn the other hand, given the small degree of the performance d iﬀerence, it is not apparent that starting from the\npre-trained ViT -Base model is better than training a new mod el.\nCONCLUSIONS\nSocial media has become an increasingly important platform for emergency response agencies to obtain valuable\ninformation, particularly images, for various crisis resp onse tasks. However, due to the sheer volume of content\non social media, automated techniques for ﬁltering and clas sifying images are necessary. Existing methods rely\non convolutional neural networks pre-trained on the genera l ImageNet-1k dataset, but recent developments in\ntransformer-based image classiﬁers in combination with in creased availability of tagged crisis imagery (via the\nIncidents1M dataset) have opened up new possibilities. In t his paper, we introduced CrisisViT , a transformer-based\narchitecture pre-trained on the Incidents1M dataset, whic h can be adapted for multiple downstream crisis image\nclassiﬁcation tasks. Through experimentation on the four t asks (Disaster T ype classiﬁcation, Informativeness clas-\nsiﬁcation, Humanitarian Category classiﬁcation, and Dama ge Severity estimation) supported by the Crisis Image\nBenchmark dataset, we show that pre-training on the Inciden ts1M dataset can lead to signiﬁcant improvements in\naccuracy, with an average absolute gain of 1.25% over the fou r crisis image classiﬁcation tasks tested. This work\nrepresents an important step towards building more eﬀectiv e crisis response tools that can utilize social media\nimage data to support emergency response eﬀorts.\nREFERENCES\nAkhtar, Z., Oﬂi, F., and Imran, M. (2021). “T owards Using Rem ote Sensing and Social Media Data for Flood\nMapping”. In: 18th International Conference on Information Systems for C risis Response and Management,\nISCRAM 2021, Blacksburg, VA, USA, May 2021 . Ed. by A. Adrot, R. Grace, K. A. Moore, and C. W . Zobel.\nISCRAM Digital Library, pp. 536–551.\nAlam, F., Imran, M., and Oﬂi, F. (2017). “Image4act: Online s ocial media image processing for disaster response”.\nIn: Proceedings of the 2017 IEEE/ACM international conference on advances in social networks analysis and\nmining 2017 , pp. 601–604.\nAlam, F., Oﬂi, F., and Imran, M. (2018). “CrisisMMD: Multimo dal T witter Datasets from Natural Disasters”.\nIn: Proceedings of the T welfth International Conference on Web and Social Media, ICWSM 2018, Stanford,\nCalifornia, USA, June 25-28, 2018 . AAAI Press, pp. 465–473.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nAsami, K., Fujita, S., Hiroi, K., and Hatayama, M. (2022). “D ata Augmentation with Synthesized Damaged Roof\nImages Generated by GAN”. In: 19th International Conference on Information Systems for C risis Response and\nManagement, ISCRAM 2022, T arbes, France, May 22-25, 2022 . Ed. by R. Grace and H. Baharmand. ISCRAM\nDigital Library, pp. 256–265.\nBaevski, A., Hsu, W ., Xu, Q., Babu, A., Gu, J., and Auli, M. (20 22). “data2vec: A General Framework for Self-\nsupervised Learning in Speech, Vision and Language”. In: International Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA . Ed. by K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv ´ari,\nG. Niu, and S. Sabato. V ol. 162. Proceedings of Machine Learn ing Research. PMLR, pp. 1298–1312.\nBuntain, C., McCreadie, R., and Soboroﬀ, I. (2022). “Incide nt Streams 2021 Oﬀ the Deep End: Deeper Annotations\nand Evaluations in T witter”. In: 19th International Conference on Information Systems for C risis Response and\nManagement, ISCRAM 2022, T arbes, France, May 22-25, 2022 . Ed. by R. Grace and H. Baharmand. ISCRAM\nDigital Library, pp. 584–604.\nDaly, S. and Thom, J. A. (2016). “Mining and Classifying Imag e Posts on Social Media to Analyse Fires”. In: 13th\nProceedings of the International Conference on Informatio n Systems for Crisis Response and Management, Rio\nde Janeiro, Brasil, May 22-25, 2016 . Ed. by A. H. T apia, P . Antunes, V . A. Ba ˜nuls, K. A. Moore, and J. P . de\nAlbuquerque. ISCRAM Association.\nDeng, J., Dong, W ., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. (2009). “ImageNet: A Large-Scale Hierarchical\nImage Database”. In: CVPR09.\nDevlin, J., Chang, M., Lee, K., and T outanova, K. (2019). “BE RT : Pre-training of Deep Bidirectional Transformers\nfor Language Understanding”. In: Proceedings of the 2019 Conference of the North American Cha pter of the\nAssociation for Computational Linguistics: Human Languag e T echnologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, V olume 1 (Long and Short P apers) . Ed. by J. Burstein, C. Doran, and T . Solorio.\nAssociation for Computational Linguistics, pp. 4171–4186 .\nDosovitskiy, A., Beyer, L., Kolesnikov, A., W eissenborn, D ., Zhai, X., Unterthiner, T ., Dehghani, M., Minderer,\nM., Heigold, G., and Gelly, S. (2020). “An image is worth 16x1 6 words: Transformers for image recognition at\nscale”. In: 9th International Conference on Learning Representations , ICLR 2021, Virtual Event, Austria, May\n3-7, 2021 .\nGao, P ., Jiang, Z., Y ou, H., Lu, P ., Hoi, S. C. H., W ang, X., and Li, H. (2019). “Dynamic Fusion With Intra- and\nInter-Modality Attention Flow for Visual Question Answeri ng”. In: IEEE Conference on Computer Vision and\nP attern Recognition, CVPR 2019, Long Beach, CA, USA, June 16 -20, 2019 . Computer Vision Foundation /\nIEEE, pp. 6639–6648.\nGirshick, R. B., Donahue, J., Darrell, T ., and Malik, J. (201 4). “Rich Feature Hierarchies for Accurate Object\nDetection and Semantic Segmentation”. In: 2014 IEEE Conference on Computer Vision and P attern Recogni tion,\nCVPR 2014, Columbus, OH, USA, June 23-28, 2014 . IEEE Computer Society, pp. 580–587.\nHe, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P ., and Girshick, R. B. (2022). “Masked Autoencoders Are Scalable Vision\nLearners”. In: IEEE/CVF Conference on Computer Vision and P attern Recogni tion, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022 . IEEE, pp. 15979–15988.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep Residual Learning for Image Recognition”. In: 2016 IEEE\nConference on Computer Vision and P attern Recognition, CVP R 2016, Las V egas, NV , USA, June 27-30, 2016 .\nIEEE Computer Society, pp. 770–778.\nImran, M., Alam, F., Qazi, U., Peterson, S., and Oﬂi, F. (2020 ). “Rapid Damage Assessment Using Social Media\nImages by Combining Human and Machine Intelligence”. In: 17th International Conference on Information\nSystems for Crisis Response and Management, ISCRAM 2020, Ma y 2020 . Ed. by A. L. Hughes, F. McNeill, and\nC. W . Zobel. ISCRAM Digital Library, pp. 761–773.\nImran, M., Castillo, C., Diaz, F., and Vieweg, S. (2015). “Pr ocessing Social Media Messages in Mass Emergency:\nA Survey”. In: ACM Comput. Surv. 47.4, 67:1–67:38.\nImran, M., Castillo, C., Lucas, J., Meier, P ., and Vieweg, S. (2014). “AIDR: Artiﬁcial intelligence for disaster\nresponse”. In: Proceedings of WWW . ACM.\nKumar, S., Barbier, G., Abbasi, M. A., and Liu, H. (2011). “T w eetTracker: An Analysis T ool for Humanitarian and\nDisaster Relief”. In: Proceedings of the Fifth International Conference on Weblo gs and Social Media, Barcelona,\nCatalonia, Spain, July 17-21, 2011 . The AAAI Press.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nLi, X. and Caragea, D. (2020). “Improving Disaster-related T weet Classiﬁcation with a Multimodal Approach”.\nIn: 17th International Conference on Information Systems for C risis Response and Management, ISCRAM 2020,\nMay 2020 . Ed. by A. L. Hughes, F. McNeill, and C. W . Zobel. ISCRAM Digit al Library, pp. 893–902.\nLi, X., Caragea, D., Caragea, C., Imran, M., and Oﬂi, F. (2019 ). “Identifying Disaster Damage Images Using a\nDomain Adaptation Approach”. In: Proceedings of the 16th International Conference on Inform ation Systems\nfor Crisis Response and Management, Val `encia, Spain, May 19-22, 2019 . Ed. by Z. Franco, J. J. Gonz ´alez, and\nJ. H. Can ´os. ISCRAM Association.\nMcCreadie, R., Buntain, C., and Soboroﬀ, I. (2020). “Incide nt Streams 2019: Actionable Insights and How to\nFind Them”. In: 17th International Conference on Information Systems for C risis Response and Management,\nISCRAM 2020, May 2020 . Ed. by A. L. Hughes, F. McNeill, and C. W . Zobel. ISCRAM Digit al Library,\npp. 744–760.\nMouzannar, H., Rizk, Y ., and A wad, M. (2018). “Damage Identi ﬁcation in Social Media Posts using Multimodal\nDeep Learning”. In: Proceedings of the 15th International Conference on Inform ation Systems for Crisis\nResponse and Management, Rochester , NY, USA, May 20-23, 201 8. Ed. by K. Boersma and B. M. T omaszewski.\nISCRAM Association.\nNguyen, D. T ., Oﬂi, F., Imran, M., and Mitra, P . (2017). “Dama ge assessment from social media imagery data\nduring disasters”. In: Proceedings of the 2017 IEEE/ACM international conference on advances in social\nnetworks analysis and mining 2017 , pp. 569–576.\nNguyen, D. T ., Joty, S. R., Imran, M., Sajjad, H., and Mitra, P . (2016). “Applications of Online Deep Learning for\nCrisis Response Using Social Media Information”. In: CoRR abs/1610.01030. arXiv:\n1610.01030.\nParmar, N., V aswani, A., Uszkoreit, J., Kaiser, L., Shazeer , N., Ku, A., and Tran, D. (2018). “Image transformer”.\nIn: International Conference on Machine Learning . PMLR, pp. 4055–4064.\nRedmon, J., Divvala, S. K., Girshick, R. B., and Farhadi, A. ( 2016). “Y ou Only Look Once: Uniﬁed, Real- Time\nObject Detection”. In: 2016 IEEE Conference on Computer Vision and P attern Recogni tion, CVPR 2016, Las\nV egas, NV , USA, June 27-30, 2016 . IEEE Computer Society, pp. 779–788.\nRen, S., He, K., Girshick, R. B., and Sun, J. (2017). “Faster R -CNN: T owards Real- Time Object Detection with\nRegion Proposal Networks”. In: IEEE T rans. P attern Anal. Mach. Intell. 39.6, pp. 1137–1149.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., and\nBernstein, M. (2015). “Imagenet large scale visual recogni tion challenge”. In: International journal of computer\nvision 115.3, pp. 211–252.\nSaid, N., Ahmad, K., Riegler, M., Pogorelov, K., Hassan, L., Ahmad, N., and Conci, N. (2019). “Natural disas-\nters detection in social media and satellite imagery: a surv ey”. In: Multimedia T ools and Applications 78.22,\npp. 31267–31302.\nShekhar, H. and Setty, S. (2015). “Disaster analysis throug h tweets”. In: 2015 International Conference on Advances\nin Computing, Communications and Informatics (ICACCI) . IEEE, pp. 1719–1723.\nSimonyan, K. and Zisserman, A. (2015). “V ery Deep Convoluti onal Networks for Large-Scale Image Recognition”.\nIn: 3rd International Conference on Learning Representations , ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference T rack Proceedings . Ed. by Y . Bengio and Y . LeCun.\nSosea, T ., Sirbu, I., Caragea, C., Caragea, D., and Rebedea, T . (2021). “Using the Image- T ext Relationship to\nImprove Multimodal Disaster T weet Classiﬁcation”. In: 18th International Conference on Information Systems\nfor Crisis Response and Management, ISCRAM 2021, Blacksbur g, VA, USA, May 2021 . Ed. by A. Adrot, R.\nGrace, K. A. Moore, and C. W . Zobel. ISCRAM Digital Library, p p. 691–704.\nT an, M. and Le, Q. V . (2019). “EﬃcientNet: Rethinking Model S caling for Convolutional Neural Networks”. In:\nProceedings of the 36th International Conference on Machin e Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA . Ed. by K. Chaudhuri and R. Salakhutdinov. V ol. 97. Proceedi ngs of Machine Learning\nResearch. PMLR, pp. 6105–6114.\nT o, H., Agrawal, S., Kim, S. H., and Shahabi, C. (2017). “On id entifying disaster-related tweets: Matching-based\nor learning-based?” In: 2017 IEEE third international conference on multimedia big data (BigMM) . IEEE,\npp. 330–337.\nT orrey, L. and Shavlik, J. (2010). “Transfer learning”. In: Handbook of research on machine learning applications\nand trends: algorithms, methods, and techniques . IGI global, pp. 242–264.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23\nZijun Long et al. CrisisViT\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).\n“Attention is All you Need”. In: Advances in Neural Information Processing Systems 30: Annu al Conference\non Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA . Ed. by I. Guyon,\nU. von Luxburg, S. Bengio, H. M. W allach, R. Fergus, S. V . N. Vi shwanathan, and R. Garnett, pp. 5998–6008.\nW eber, E., Papadopoulos, D. P ., Lapedriza, `A., Oﬂi, F., Imran, M., and T orralba, A. (2022). “Incidents1 M: a\nlarge-scale dataset of images with natural disasters, dama ge, and incidents”. In: CoRR abs/2201.04236. arXiv:\n2201.04236.\nW eissenborn, D., T ¨ackstr¨om, O., and Uszkoreit, J. (2020). “Scaling Autoregressive V ideo Models”. In: 8th Inter-\nnational Conference on Learning Representations, ICLR 202 0, Addis Ababa, Ethiopia, April 26-30, 2020 .\nWidener, M. J. and Li, W . (2014). “Using geolocated T witter d ata to monitor the prevalence of healthy and unhealthy\nfood references across the US”. In: Applied Geography 54, pp. 189–197.\nXie, Z., Zhang, Z., Cao, Y ., Lin, Y ., Bao, J., Y ao, Z., Dai, Q., and Hu, H. (2022). “Simmim: A simple framework\nfor masked image modeling”. In: Proceedings of the IEEE/CVF Conference on Computer Vision a nd P attern\nRecognition, pp. 9653–9663.\nYin, J., Karimi, S., Lampert, A., Cameron, M. A., Robinson, B ., and Power, R. (2015). “Using Social Media to En-\nhance Emergency Situation A wareness: Extended Abstract”. In: Proceedings of the T wenty-F ourth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2015, Bu enos Aires, Argentina, July 25-31, 2015 . AAAI Press,\npp. 4234–4239.\nZhou, H.-Y ., Lu, C., Y ang, S., and Y u, Y . (2021). “ConvNets vs . Transformers: Whose visual representations are\nmore transferable?” In: Proceedings of the IEEE/CVF International Conference on Co mputer Vision , pp. 2230–\n2238.\nCoRe P aper – Social Media for Crisis Management\nProceedings of the 20th ISCRAM Conference – Omaha, NE, USA 20 23",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7043222188949585
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6698001027107239
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5476247072219849
    },
    {
      "name": "Transformer",
      "score": 0.5091013312339783
    },
    {
      "name": "Contextual image classification",
      "score": 0.4723416566848755
    },
    {
      "name": "Machine learning",
      "score": 0.4150421619415283
    },
    {
      "name": "Data mining",
      "score": 0.36819469928741455
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3403582274913788
    },
    {
      "name": "Engineering",
      "score": 0.16205692291259766
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}