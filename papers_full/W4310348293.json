{
  "title": "Adapting multilingual speech representation model for a new, underresourced language through multilingual fine-tuning and continued pretraining",
  "url": "https://openalex.org/W4310348293",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2279121497",
      "name": "Karol Nowakowski",
      "affiliations": [
        "Tohoku University of Community Service and Science"
      ]
    },
    {
      "id": "https://openalex.org/A2150536725",
      "name": "Michal Ptaszynski",
      "affiliations": [
        "Kitami Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2277157203",
      "name": "Kyoko Murasaki",
      "affiliations": [
        "Yokohama National University"
      ]
    },
    {
      "id": "https://openalex.org/A4310348470",
      "name": "Jagna Nieuważny",
      "affiliations": [
        "Tohoku University of Community Service and Science"
      ]
    },
    {
      "id": "https://openalex.org/A2279121497",
      "name": "Karol Nowakowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150536725",
      "name": "Michal Ptaszynski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2277157203",
      "name": "Kyoko Murasaki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310348470",
      "name": "Jagna Nieuważny",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2995929068",
    "https://openalex.org/W6741192994",
    "https://openalex.org/W6603931906",
    "https://openalex.org/W6771917389",
    "https://openalex.org/W6779919476",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6796101339",
    "https://openalex.org/W7027429494",
    "https://openalex.org/W6739258355",
    "https://openalex.org/W6776277508",
    "https://openalex.org/W6783383400",
    "https://openalex.org/W6750253784",
    "https://openalex.org/W3209059054",
    "https://openalex.org/W6792927658",
    "https://openalex.org/W6801549336",
    "https://openalex.org/W2739967986",
    "https://openalex.org/W3005823977",
    "https://openalex.org/W2980449122",
    "https://openalex.org/W6629717138",
    "https://openalex.org/W6776741894",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W6763238093",
    "https://openalex.org/W6801728950",
    "https://openalex.org/W6778469173",
    "https://openalex.org/W4238846128",
    "https://openalex.org/W3213029956",
    "https://openalex.org/W4237991219",
    "https://openalex.org/W2914584698",
    "https://openalex.org/W3186596101",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2899575547",
    "https://openalex.org/W4221154554",
    "https://openalex.org/W2765486990",
    "https://openalex.org/W4287694131",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2625465398",
    "https://openalex.org/W3030437843",
    "https://openalex.org/W4253203888",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W3177252310",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W625315570",
    "https://openalex.org/W2912512851",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3169320628",
    "https://openalex.org/W3198429080",
    "https://openalex.org/W4245804068",
    "https://openalex.org/W3203098807",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3028846133",
    "https://openalex.org/W3200129129",
    "https://openalex.org/W3198771897",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W97072897",
    "https://openalex.org/W2625499837",
    "https://openalex.org/W613335405",
    "https://openalex.org/W2948017315",
    "https://openalex.org/W4297841794",
    "https://openalex.org/W4297841844",
    "https://openalex.org/W4365800035",
    "https://openalex.org/W3046368065",
    "https://openalex.org/W4322714819",
    "https://openalex.org/W3027428931",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3087775410",
    "https://openalex.org/W2979476256",
    "https://openalex.org/W3036601975"
  ],
  "abstract": null,
  "full_text": "Adapting Multilingual Speech Representation Model for a New,\nUnderresourced Language through Multilingual Fine-tuning and\nContinued Pretraining\nKarol Nowakowskia,∗∗, Michal Ptaszynskib,∗, Kyoko Murasakic and Jagna Nieuwa/uni017Cnya\naTohoku University of Community Service and Science, Sakata, Yamagata, Japan\nbKitami Institute of Technology, Kitami, Hokkaido, Japan\ncYokohama National University (Professor Emeritus), Yokohama, Kanagawa, Japan\nARTICLE INFO\nKeywords:\nautomatic speech transcription\nASR\nwav2vec 2.0\npretrained transformer models\nspeech representation models\ncross-lingual transfer\nlanguage documentation\nendangered languages\nunderresourced languages\nSakhalin Ainu\nABSTRACT\nIn recent years, neural models learned through self-supervised pretraining on large scale\nmultilingual text or speech data have exhibited promising results for underresourced languages,\nespecially when a relatively large amount of data from related language(s) is available. While\nthe technology has a potential for facilitating tasks carried out in language documentation\nprojects, such as speech transcription, pretraining a multilingual model from scratch for every\nnewlanguagewouldbehighlyimpractical.Weinvestigatethepossibilityforadaptinganexisting\nmultilingual wav2vec 2.0 model for a new language, focusing on actual ﬁeldwork data from a\ncritically endangered tongue: Ainu. Speciﬁcally, we (i) examine the feasibility of leveraging\ndata from similar languages also in ﬁne-tuning; (ii) verify whether the model’s performance\ncan be improved by further pretraining on target language data. Our results show that continued\npretraining is the most eﬀective method to adapt a wav2vec 2.0 model for a new language and\nleads to considerable reduction in error rates. Furthermore, we ﬁnd that if a model pretrained\non a related speech variety or an unrelated language with similar phonological characteristics\nis available, multilingual ﬁne-tuning using additional data from that language can have positive\nimpact on speech recognition performance when there is very little labeled data in the target\nlanguage.\n1. Introduction\nThecostofspeechtranscriptionisamajorbottleneckfacedinlanguagedocumentationprojects.Itisbelievedthat\nthis task could be facilitated by utilizing speech recognition technologies, but the fact that the amount of data (both\nannotatedandunannotated)availableforlanguagesstudiedinsuchprojectsistypicallyverylimited,hasbeenabarrier\nin their application.\nRecent studies (e.g., Conneau, Baevski, Collobert, Mohamed and Auli, 2021) have demonstrated the beneﬁts of\nmultilingualpretrainingofspeechrepresentationsforspeechrecognitioninscenarioswheredatainthetargetlanguage\nisscarce.However,giventheprohibitivecostofpretrainingsuchrepresentationsfromscratch,itisexpectedthatmost\nusers will restrict themselves to ﬁne-tuning publicly available models. For this reason, in this paper we address the\nquestion of whether the beneﬁts of cross-lingual transfer extend to ﬁne-tuning, as well. Furthermore, we study the\neﬀect of additional pretraining using the small amount of data available for the target language. In contrast to most\nprevious work, we focus on actual ﬁeldwork data with all its ﬂaws, rather than clean NLP datasets. Speciﬁcally, our\ngoal is to transcribe unique speech data in Sakhalin Ainu, recorded on 50-year-old audio tapes.\nOur results show that continued pretraining on target language data leads to a substantial reduction in error rates.\nFurthermore, we demonstrate that in a scenario where labeled data in the target language is extremely scarce, speech\nrecognition performance can be improved by adding data from a related speech variety or an unrelated language\nwith similar phonological traits during ﬁne-tuning, provided that the underlying speech representation model was\nﬁrst pretrained on that language. Our model pretrained on the Ainu language is publicly available1.\n∗Corresponding author\n∗ ∗Principal corresponding author\nkarol@koeki-u.ac.jp (K. Nowakowski);michal@mail.kitami-it.ac.jp (M. Ptaszynski)\nORCID(s):0000-0001-7435-4061 (K. Nowakowski);0000-0002-1910-9183 (M. Ptaszynski)\n1https://huggingface.co/karolnowakowski/wav2vec2-large-xlsr-53-pretrain-ain\nNowakowski et al. Page 1 of 14\narXiv:2301.07295v1  [cs.CL]  18 Jan 2023\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nThe remainder of this paper is organized as follows. In the following section we discuss how speech-to-text\ntechnology and recent developments in self-supervised learning of speech representations can support language\ndocumentation. We also introduce data on which we are focusing in this research. In Section 3, we describe our\nresearch method. In Section 4, we provide an overview of related work. Section 5 presents the resources used in this\nstudy, including speech representation models and data used to train them. In Section 6, we describe our experiments\nand analyze their results. Finally, Section 7 contains conclusionsand ideas forfutureimprovements.\n2. Background\n2.1. Speech Transcription Technology for Language Documentation\nOne of the main tasks in linguistic research concerned with endangered languages is the collection and analysis\nof primary linguistic data. A typical workﬂow involves recording speech during ﬁeldwork and analyzing the data\nafterwards. A major challenge in this process is speech transcription, which is a very time-consuming task2. As a\nresult,largeamountsofdataremainuntranscribedinarchivesandcollectionsofindividualresearchers.Manyofthose\nmaterials are stored on obsolete types of media, such as audio tapes, and in poor conditions. Before they get to be\ntranscribed, some of them may be destroyed by accident or after the researcher who collected them retires (Abney,\n2011). For this reason, such materials are often referred to as “endangered data”. It may be possible to solve (or at\nleast reduce) the transcription bottleneck using speech-to-text technology, thus speeding up the process of language\ndocumentation (Hjortnaes, Partanen, Rießler and Tyers, 2020; Zahrer, Zgank and Schuppler, 2020). However, in\norder to reach high accuracy, traditional approaches require large amounts of annotated training data (on the order\nof thousands of hours (Baevski, Zhou, Mohamed and Auli, 2020)), which is typically not available in a language\ndocumentation scenario.\n2.2. Cross-lingual Self-supervised Learning for Low-resource Speech Transcription\nThe past few years have witnessed substantial improvements in a wide range of Natural Language Processing\napplications, owing to the development of eﬃcient techniques for self-supervised learning of language representation\nmodels,suchasBERT(Devlin,Chang,LeeandToutanova,2019),ELECTRA(Clark,Luong,LeandManning,2020)\n(for text-based models), wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov\nand Mohamed, 2021a) (for speech representations). They have been shown to produce competitive results compared\nto traditional, fully supervised methods, while training on much fewer human-annotated samples (after having been\npretrained on large amount of unlabeled data).\nA major obstacle for applying self-supervised learning in a language documentation setting is the fact that for the\nmajority of the world’s languages, even raw text or speech data is not available in large quantities. To alleviate this\nproblem,recentstudiesexplorecross-linguallearningtechniques(Conneau,Khandelwal,Goyal,Chaudhary,Wenzek,\nGuzmán, Grave, Ott, Zettlemoyer and Stoyanov, 2020; Singh, McCann, Keskar, Xiong and Socher, 2019). It has been\ndemonstrated that learning a single model from unlabeled data in multiple languages can have positive impact on the\nquality of representations computed for each individual language. As an example, Conneau et al. (2021) pretrained\ntheir speech representation model on 56k hours of unlabeled data in 53 languages and found it to perform far better\nthan monolingual models, particularly for languages with little data available. However, the cost of compute required\nto train such models and the energy consumed in doing so is extremely high3.\n2.3. Sakhalin Ainu Speech Data\nThe aim of this research is to develop a system for automatic transcription of Ainu, a critically endangered\nlanguage4 nativetonorthernJapan,SakhalinandKurilIslands.Inparticular,wearefocusingonthetaskoftranscribing\nunpublishedmaterialsfromseveraldialectsoftheAinulanguagespokeninSakhalin,recordedinthe1960sand1970s\nbyProfessorKy ¯okoMurasaki,withsomeofthelastspeakersofthosedialects:HaruFujiyama(Rayciskadialect), ¯Ota\nYuku (Maoka dialect), Chikama Kimura (Shirahama dialect) and others. The materials in question were originally\nrecorded on over 30 magnetic tapes of an old type (so called “reel-to-reel” tapes or “open-reel” tapes), and were a\n2Depending on the annotation schemeand the levelof quality required, transcribing 1 minute of spoken language can takeanywhere between\nseveralminutes andanhour (Cieri, Miller and Walker, 2004; Gries and Berez, 2017).\n3Forinstance,Conneauetal.(2021)’smultilingualmodelwastrainedusing64GPUs,whileConneauetal.(2020)trainedtheirXLM-Rmodel\nwith 500 GPUs.\n4From the second half of the 20th century Ainu has not been used as a language of everyday communication (Bugaeva, 2012), thus many\nspecialists consider it extinct. There are, however, eﬀorts to revitalize the language and a growing number of people are learning it.\nNowakowski et al. Page 2 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 1\nExamples of equivalent sentences in Sakhalin Ainu (Rayciska dialect) and Hokkaido Ainu (Horobetsu dialect) recorded in\nHattori (1964).Forcomparison, we also include Japanese translations (romanized according to the Hepburn transliteration\nsystem).\nSakhalin Ainu Hokkaido Ainu Japanese English\nku’ani ’enahkari nee\n’aynuka hennehka ’oman.\n’enmosma nen ka senne\n’oman.\nwatashi no hoka ni wa dare\nmo ikanai.\nThere’s no one going but\nme.\n’uneeno ’an ’itahpateh\nkisci.\nsine ’itak ’ukoraci ’an ’itak\npatek ye kor an.\nonaji koto bakari itte iru. He always says the same\nthing.\ntah ’aynu’itakani temana\nayyeepe?\ntanpe ’anak ’aynu’itak ’ari\nnekona ’aye ya.\nkore wa ainugo de nan to\niimasu ka?\nWhat is this called in Ainu?\ngood example of “endangered data”. Sound quality is rather poor, with high levels of noise(both equipment noise\nsuch as hiss and hum, as well as occasional ambient noise) and a considerable amount of distortions.The bulk of\nthe recorded materials consists of spontaneous monologues (mainly reciting folktales) by a single speaker,but there\nare also conversations between the informant and the intervieweror involving multiple informants, and occasionally\nmultipleparticipantsspeaksimultaneously.Thetapeswererecordedmainlyatthehomeofoneoftheinformantsusing\naportable recorder.\nThe total duration of the recordings exceeds 20 hours, which – to the best of our knowledge – makes it larger than\nany collection of Sakhalin Ainu texts published so far5. A subset of the recordings has been transcribed, translated\nto Japanese and published: Murasaki (1976) released a collection of eleven folktales by Haru Fujiyama and ﬁve short\nconversationsbetweentwonativespeakers(Fujiyamaand ¯Ota).MurasakiandFujiyama(2010)producedacompilation\nof three diﬀerent versions of a single folktale, “Wenenekaype”, recited by Haru Fujiyama. Lastly, two volumes by\nMurasaki and Fujiyama (2013) and Murasaki and Fujiyama (2016) contain a total of 297 sentences.\nSakhalin Ainu is one of the three major dialect groups recognized within the Ainu language (the other two being\nHokkaido Ainu and Kuril Ainu). There are signiﬁcant diﬀerences between the dialects of Sakhalin and Hokkaido and\ntheyaremutuallyunintelligible(Refsing,1986;Murasaki,2009)(Table1includesexamplesofsemanticallyequivalent\nsentences in both speech varieties). Vovin (2016) describes Ainu as a portmanteau language family with two primary\nbranches:Hokkaido-KurilandSakhalin.InthecaseofHokkaidoAinu,asizeablebodyofaudioandwrittenmaterials\nhave been recorded and published, and recently a growing number of them are digitized and released online. Data in\nSakhalin Ainu, however, is far more scarce6.\nUntil now, none of the numerous hypotheses about genetic relationships between Ainu and other languages or\nlanguage families has gained wider acceptance, and thus it is usually classiﬁed as a language isolate. In terms of\nlinguistic typology, Ainu is an agglutinating language with SOV (subject-object-verb) word order and elements of\npolysynthesis,suchasnounincorporationandconcentrationofvariousmorphemesinthe verbalcomplex (Shibatani,\n1990). PhonemicinventoryofAinuconsistsofﬁvevowelphonemes:/i,e,a,o,u/,andtwelveconsonantphonemes:/p,\nt,k,c,s,h,r,m,n,y,w,’/(/’/denotesaglottalstop).SyllablesinSakhalinAinuconformtooneofthefollowingpatterns:\nCV,CVV(VVrepresentsalongvowel)orCVC.MostwrittentextsinAinuaretranscribedusingLatinalphabetand/or\nan extended version of the Japanese katakana syllabary (textual data in Ainu used in this research is written in Latin\nscript). Themajorityofcontemporary experts followthephonemicorthographicrules devisedby Hattori (1964)ora\nslightlymodiﬁed versionproposedbyHokkaid ¯oUtariKy ¯okai(1994). However,certainaspectsofthewritingsystem,\nsuchaswordsegmentation,havenotbeenstandardized(formoredetails, pleasereferto Nowakowski, Ptaszynski and\nMasui (2019)).\n3. Research Method\nGiven the encouraging results achieved by multilingual speech representation models for underresourced lan-\nguages, and the high cost of pretraining such a model from scratch, we explore two methods for adapting an existing\nmodel for a new language with a limited amount of available speech data.\n5In addition to Sakhalin Ainu, the tapes in question also contain 2 hours of speech recordings in Hokkaido Ainu.\n6For Kuril Ainu, there is almost no data available, apart from several lexicons and word lists.\nNowakowski et al. Page 3 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nMultilingual ﬁne-tuningIn addition to multilingual self-supervised pretraining, Conneau et al. (2021) conducted an\nexperiment with ﬁne-tuning using labeled data from 10 diﬀerent languages simultaneously, and found the resulting\nmodel to perform competitively to models ﬁne-tuned on each language individually. On the other hand, they did\nnot analyze the correlation between language similarity and eﬀectiveness of such multilingual ﬁne-tuning7. In this\nstudy,weexaminewhetherthebeneﬁtsofcross-lingualtransferbetweencloselyrelatedlanguages/dialectsorunrelated\nlanguagessharingsomephonologicalcharacteristics,applytoﬁne-tuningaswell.Tothatend,wecarryoutﬁne-tuning\nexperiments using data in the target language (Sakhalin Ainu) in combination with relatively large amounts of data\nfrom three diﬀerent speech varieties: Hokkaido Ainu, Japanese and English. As mentioned in Section 2.3, Sakhalin\nAinuandHokkaidoAinucanbeviewedasdistantvarietiesofthesamelanguageorascloselyrelatedlanguages.Asfor\nJapanese,thetheoryofageneticrelationshipbetweenitandtheAinulanguage(s)isrejectedbymostexperts(Refsing,\n1986; Shibatani, 1990). That being said, and despite substantial diﬀerences in such aspects as consonants allowed in\nsyllable coda and accent, the phonological system of Ainu has arguably more in common with Japanese8,9 than, e.g.,\nEnglish. This intuition is also corroborated by the analysis of language vectors computed usinglang2vec (Littell,\nMortensen, Lin, Kairis, Turner and Levin, 2017)10: upon calculating the distances between the Ainu language and all\notherlanguagesinthedatabase(speciﬁcally,wecompared phonological andinventory featuresandtookthemean\ndistance), we found that Japanese is in 43rd position in terms of proximity to Ainu, out of 8070 languages.\nContinued pretrainingSecondly, we investigate if it is possible to improve the performance of a strong multilingual\nmodelonalanguagenotseenduringinitialpretraining,byperformingadditionalpretrainingonsmallamountoftarget\nlanguage data.\n4. Related Work\nSchneider, Baevski, Collobert and Auli (2019) introduced wav2vec, a technique for self-supervised learning of\nspeech representations from raw audio data using convolutional neural networks trained to distinguish true audio\nsamples from distractors. Their approach outperformed the previous state-of-the-art on the WSJ speech recognition\nbenchmark while using two orders of magnitude less labeled data. Baevski, Schneider and Auli (2019) extended their\nwork by adding a quantization module for computing discrete representations of audio segments and feeding the\ndiscretizedsequencetoaTransformer(BERT)model.FurtherimprovementswereintroducedbyBaevskietal.(2020)\nwho proposed wav2vec 2.0, an end-to-end framework for jointly learning discretized speech units and contextualized\nspeech representations, and ﬁne-tuned the resulting model for speech transcription instead of feeding the pretrained\nfeatures to a separate downstream model. Conneau et al. (2021) pretrained a single wav2vec 2.0 model (dubbed\nXLSR-53) using 56k hours of speech data in 53 languages and obtained a higher accuracy in speech recognition than\nmonolingual models or previous methods. Hsu, Sriram, Baevski, Likhomanenko, Xu, Pratap, Kahn, Lee, Collobert,\nSynnaeve and Auli (2021b) combined speech data from diﬀerent domains and investigated the impact of domain\nmismatches in self-supervised learning for ASR. Xu, Baevski and Auli (2021) performed zero-shot transcription of\nunseen languages by ﬁne-tuning the XLSR-53 model and mapping phonemes of the training languages to the target\nlanguage using articulatory features. Babu, Wang, Tjandra, Lakhotia, Xu, Goyal, Singh, von Platen, Saraf, Pino,\nBaevski, Conneau and Auli (2021) used wav2vec 2.0 and 436k hours of unlabeled data in 128 languages to train\nlarge-scale(upto2billionparameters)models,whichafterﬁne-tuningachievedstate-of-the-artperformanceinspeech\nrecognition, speech translation and language identiﬁcation. Sriram, Auli and Baevski (2022)obtained improvedASR\nperformance by applying data augmentation techniques – such as pitch shift and adding random noise to the input\nsignal –to the pretraining data, and introducing several modiﬁcations to the wav2vec2.0 architecture. Sanabria, Hsu,\nBaevskiandAuli(2022) usedmodelspretrainedonmodiﬁednaturalspeechorsyntheticdatatomeasuretheimpactof\nindividualdomainfactors(vocabulary,wordorder,phoneticfeatures,etc.).Theyfoundthat low-leveldomainfactors,\nsuch as phonotactics and prosody, play a more important role than syntactic or lexical variation, and that speaker\ndiversity in the pretraining data is crucial. Furthermore, they demonstrated that using a large amount of synthesized\ndatacanleadtobetterperformancethanwiththesmallamountofrealdatausedtotrainthesynthesizer.Wu,Kim,Pan,\n7Theydidperformsuchanalysisforpretraining,andfoundthatlow-resourcelanguageperformancebeneﬁtsmorefromadditionaldatainsimilar\nlanguages.\n8Some of the similarities are presumably a result of contact-induced change (Bugaeva, 2012).\n9See Nowakowski, Ptaszynski and Masui (2020) for an example of using Japanese speech models to recognize and generate speech in Ainu.\nMatsuura, Ueno, Mimura, Sakai and Kawahara (2020) trained an end-to-end ASR model for Hokkaido Ainu using additional Japanese and English\ndata and found the former to be more helpful.\n10https://github.com/antonisa/lang2vec\nNowakowski et al. Page 4 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nHan,WeinbergerandArtzi(2022)andVyas,Hsu,AuliandBaevski(2022) proposedmodiﬁcationstothewav2vec2.0\narchitectureaimedat reducing the computationalcostofpretraining andinference.\nPrevious studies found continued self-supervised training of textual language representation models to be an\neﬀective method for adapting them for a new domain (Howard and Ruder, 2018; Sun, Qiu, Xu and Huang, 2019;\nGururangan, Marasovi/uni0107, Swayamdipta, Lo, Beltagy, Downey and Smith, 2020) or expanding their coverage to\nlanguages unseen in initial pretraining (Pfeiﬀer, Vuli/uni0107, Gurevych and Ruder, 2020; Tang, Tran, Li, Chen, Goyal,\nChaudhary, Gu and Fan, 2020; Ebrahimi and Kann, 2021). Further pretraining of a speech representation model on\nnew languages was investigated by Kessler, Thomas and Karout (2021). However, they only conducted experiments\nwith a monolingual (English) model and focused on a high-resource setting, where the available speech data in\nthe newly added language is ample (800 hours or more). Khurana, Laurent and Glass (2022)used self-training to\nadapt monolingual English wav2vec 2.0 models for several other languages in a simulated low-resource scenario.\nMultilingualﬁne-tuningwasstudiedinthecontextoftext-basedmachinetranslationbyTangetal.(2020)andresulted\nin improved performance, especially on low-resource languages.\n5. Materials\n5.1. Wav2vec 2.0\nIn all speech transcription experiments described in this paper, a publicly available pretrained wav2vec 2.0 model\nwas employed. Speciﬁcally, we used the XLSR-53 – a model trained on 56k hours of data in 53 languages – compiled\nand released11 by Conneau et al. (2021). Furthermore, we performed an additional pretraining of the XLSR-53 model\non Ainu language data described in the next section. Both pretraining and ﬁne-tuning of the model were conducted\nusing thefairseq library.\n5.2. Pretraining Data\nIn the experiments with continued pretraining of the XLSR-53, we used a total of 234 hours of speech data in\nSakhalin Ainu and Hokkaido Ainu12. Speciﬁcally, we pretrained our model on data obtained from tapes described in\nSection 2.3 and from publicly available data collections listed in Table 2.\nTherecordingsfromopen-reeltapesweretransferredtoadigitalformat(WAV)usinganaudiorecorderconnected\ntoatapedeck.Thetapesaredouble-sidedandwefoundthatmanyoftherecordingswereaudibleindataobtainedfrom\nboth sides of the same tape. Upon inspection it turned out that in some instances, data retrieved from the reverse side\n(orcertainpartsofit)issuperiortothecorrespondingdataonthefrontsideintermsofqualityoftheaudiosignal.For\nthis reason, we included the duplicate recordings in the pretraining data, hence two numbers are reported in Table 2\n(the number in brackets corresponds to the duration of unique recordings).\nAll ﬁles were converted to a single channel WAV sampled at 16 kHz. Files longer than 15 seconds were\nautomatically split on silence intervals (usingpydub) into separate clips 2 to 15 seconds in length. Files shorter than\n1 second were excluded from pretraining.\n5.3. Fine-tuning Data\nData used for ﬁne-tuning is listed in Table 3. In monolingual ﬁne-tuning, we used Sakhalin Ainu data from two\nsources: one story from Murasaki and Fujiyama (2010) (namely,Fu12-690401; the remaining two recordings were\nused for validation and testing) and data from Murasaki and Asai (2001)13. In multilingual ﬁne-tuning, we added data\nfromHokkaidoAinu(64.5h),Japanese(validatedsubsetoftheJapanesedataintheCommonVoiceCorpus8.0(Ardila\netal.,2020) 14,version ja_43h_2022-01-19,andtheJSUTcorpus(Sonobeetal.,2017) 15 ;50.9hintotal)andEnglish\n(the 100h “clean” subset of LibriSpeech).\nSpeech data obtained from Murasaki and Fujiyama (2010) was automatically split on silence intervals (using\npydub) into separate clips 2 to 15 seconds in length. Transcriptions from the book were digitized and aligned with\nthe audio clips.\nAllaudioﬁleswereconvertedtoasinglechannelWAVsampledat16kHz.Punctuationmarksandmetadatawere\nremoved from all transcriptions. All alphabetic characters in the transcriptions of Sakhalin Ainu and Hokkaido Ainu\n11https://github.com/pytorch/fairseq/tree/main/examples/wav2vec\n12While it might be informative to also pretrain a model on Sakhalin data only, we decided not to do so, due to the high cost of pretraining.\n13Also available online at:http://www.aa.tufs.ac.jp/~mmine/kiki_gen/murasaki/asai01.html\n14https://commonvoice.mozilla.org/ja/datasets\n15https://sites.google.com/site/shinnosuketakamichi/publication/jsut\nNowakowski et al. Page 5 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 2\nStatistics of the data used for continued pretraining. In the case of the Sakhalin Ainu data obtained from tapes, we\nincluded duplicate recordings, hence two numbers are reported (the number in brackets corresponds to the duration of\nunique recordings).\nData (Main) language/dialect Total duration (h)\nSakhalin Ainu tapes Sakhalin Ainu 35.9 (21.5)\nTuytah (Murasaki and Asai, 2001) Sakhalin Ainu 8.9\n“Wenenekaype” (Murasaki and Fujiyama, 2010) Sakhalin Ainu 1.9\nAinu Language Archive (An=ukokor Aynu ikor oma kenru (National\nAinu Museum), 2017–2022)\nHokkaido Ainu 103.1\nDictionary of Mukawa Ainu (Chiba University Graduate School of\nHumanities and Social Sciences, 2014)\nHokkaido Ainu 26.5\nAinu Language & Ainu Oral Literature (Nibutani Ainu Culture Mu-\nseum, n.d.)\nHokkaido Ainu 19.2\nAinu language audio materials in the Waseda University Repository\n(https://waseda.repo.nii.ac.jp/)\nHokkaido Ainu 14.1\nILCAA’s Project for the Publication of Ainu Language Materials\n(Information Resources Center, Research Institute for Languages and\nCultures of Asia and Africa, Tokyo University of Foreign Studies, n.d.)\nHokkaido Ainu 12.1\nGlossed Audio Corpus of Ainu Folklore (Nakagawa et al., 2016) Hokkaido Ainu 6.2\nShigeru Kayano’s Ainu dictionary (Kayano, 1996) Hokkaido Ainu 3.0\nA Topical Dictionary of Conversational Ainu (National Institute for\nJapanese Language and Linguistics, 2015)\nHokkaido Ainu 2.3\nNy¯u ekusupuresu Ainugo(Nakagawa, 2013) Hokkaido Ainu 1.0\ntexts were converted to lower case. In order to prevent a large increase in the output vocabulary size and reduce data\nsparsity, transcriptions in Japanese were transliterated (usingpykakasi) to thekatakana syllabary. This also applies\ntodataintheAinulanguagewhichcontainsmanywordsandutterancesinJapanese(code-switching,commentsabout\nthe text, questions from an interviewer, etc.). LibriSpeech transcriptions were used in their original form, i.e., in all\nupper case letters.\nAn excerpt from the transcriptions for “Wenenekaype” is shown in Table 4.\n6. Experiments\n6.1. Continued Pretraining\nA random subset of 1% of the data was used for validation. Pretraining was performed using four Nvidia GTX\n1080Ti GPUs. We continued pretraining for 100k updates, which took a total of 5 weeks. A small learning rate (1e-4,\ncompared to 1e-3 used by Conneau et al. (2021) in initial pretraining) was set to prevent catastrophic forgetting (Sun\netal.,2019).Weusedabatchsizeof150ksamplesperGPUandappliedgradientaccumulationtosimulate512GPUs,\nreaching an eﬀective batch size of 80 minutes. Other hyperparameters were set according to the conﬁguration for the\nLARGE model reported by Baevski et al. (2020).\n6.2. Fine-tuning\nWe ﬁne-tuned the pretrained models for speech transcription with a CTC loss (Graves, Fernández, Gomez and\nSchmidhuber, 2006). The best checkpoint for each experiment run was selected according to Word Error Rate on the\nvalidationset(forthispurpose,weusedtheshortest,10-minuterecordingfromMurasakiandFujiyama(2010),namely,\nFu13-700326). The output of the ﬁne-tuned models was decoded with a Viterbi decoder and a 4-gram language\nmodel trained on the Sakhalin Ainu part of the data used in ﬁne-tuning of the corresponding model (see Table 5).\nNowakowski et al. Page 6 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 3\nStatistics of the speech data and transcriptions used for ﬁne-tuning.Japanese is written without spaces and we did not\nperform tokenization, hence we don’treport token counts and vocabulary sizes for the Japanese data.\nData (Main) Total Token Vocab.\nlanguage/dialect duration (h) count size\nTuytah (Murasaki and Asai, 2001) Sakhalin Ainu 8.9 52,172 4,415\n“Wenenekaype” (Fu12-690401) (Murasaki and Fujiyama,\n2010)\nSakhalin Ainu 0.8 5,817 1,165\nAinu Language Archive (An=ukokor Aynu ikor oma kenru\n(National Ainu Museum), 2017–2022)\nHokkaido Ainu 62.2 396,755 10,618\nA Topical Dictionary of Conversational Ainu (National Institute\nfor Japanese Language and Linguistics, 2015)\nHokkaido Ainu 2.3 13,007 2,260\nCommon Voice (Japanese) (Ardila et al., 2020) Japanese 40.6 N/A N/A\nJSUT (Sonobe et al., 2017) Japanese 10.3 N/A N/A\nLibriSpeech (Panayotov et al., 2015) English 100.6 990,101 33,798\nTable 4\nExcerpt from the transcriptions for “Wenenekaype”, used in ﬁne-tuning of our speech recognition models.\nOriginal transcription (Murasaki and\nFujiyama, 2010)\nPreprocessed for ﬁne-tuning English translation\nsine, sine.. ’oyanruru kotan ’ohta\nsine, ’oyanruru kotan ’an manu. ’an\nmanuyke reekoh wenporo kotan ’an\nmanu. ’ani ike, hemanta ka, hemanta\n’oyasi hee, ’an manuyke neyan wen-\nporo kotan ’oma ’aynu ka ’emuyke\nruhpa ’ike tuy wa (tuy)pa wa ’isam.\n’isam mayne tani ’ampene ’oha kotan\nnee manu.\nsine sine ’oyanruru kotan ’ohta\nsine ’oyanruru kotan ’an manu ’an\nmanuyke reekoh wenporo kotan ’an\nmanu ’ani ike hemata ka hemata’oyasi\nhee ’an manuyke neyan wenporo\nkotan ’oma ’aynu ka ’emuyke ruhpa\nnike tuy wa tuypa wa ’isam ’isam\nmayne tani ’ampene ’oha kotan nee\nmanu\nThere was a big village. A really big\nvillage. But then, there was some\nkind of monster that ate away all the\npeople in that big village, and there\nwas no one left. They’re all gone, and\nnow it’s just an empty village.\nTable 5\nStatistics of the KenLM language models used for decoding, including perplexity and out-of-vocabulary token rates on the\nevaluation data.\nTraining Vocabulary Perplexity Perplexity Out-of-vocabulary\ndata size (including OOVs) (excluding OOVs) token rate\n“Wenenekaype” 1,168 152.1 92.9 563/4911 (11.5%)\n“Wenenekaype” + Tuytah 5,095 290.7 181.9 403/4911 (8.2%)\nLanguage models were computed using the KenLM toolkit16. The performance in speech transcription was evaluated\nona37-minutesubsetofthedatafromMurasakiandFujiyama(2010)(namely, Fu11-690328).Beforetheevaluation,\ntranscriptions generated by the system were preprocessed by converting all alphabetic characters to lower case. We\nreport Character Error Rate (CER) and Word Error Rate (WER).\nAllﬁne-tuningexperimentswereconductedusingasingleNvidiaRTX3090GPU.Thebaselinemodelswereﬁne-\ntunedfor15kupdatesonthefewlabeledsamplesfromthetargetdomain(i.e.,“Wenenekaype”)only,withabatchsize\nof2.56Msamplesandthelearningratesetto3e-4.Otherhyperparametersweresetinaccordancewiththeconﬁguration\nfor theLARGE model reported by Baevski et al. (2020). On our system, ﬁne-tuning with these settings took less than\n16https://kheafield.com/code/kenlm/\nNowakowski et al. Page 7 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 6\nComparison of models ﬁne-tuned on monolingual or multilingual data, with or without additional pretraining\non target language data, in speech transcription on “Wenenekaype” test set. We report Character Error Rates\nand Word Error Rates.\nAdditional Viterbi KenLM\nFine-tuning data pretraining decoder decoder\nsteps CER WER CER WER\n“Wenenekaype” 0 13.9 42.3 18.0 36.5\nSakhalin Ainu (“Wenenekaype” + Tuytah) 0 11.8 37.3 14.1 31.8\nSakhalin Ainu + Hokkaido Ainu 0 15.9 50.6 20.2 38.6\n“Wenenekaype” + Hokkaido Ainu 0 16.4 51.1 21.8 41.2\n“Wenenekaype” + Japanese 0 15.7 41.8 19.0 38.0\n“Wenenekaype” + English 0 13.9 43.0 17.5 38.0\n“Wenenekaype” + Hokk. Ainu + Jap. 0 16.0 52.0 22.1 41.1\n“Wenenekaype”+ Hokk. Ainu + Jap. + Eng. 0 15.2 47.4 20.5 40.4\n“Wenenekaype” 100k 10.6 33.5 14.6 33.0\nSakhalin Ainu (“Wenenekaype” + Tuytah) 100k 9.7 29.8 13.4 30.2\nSakhalin Ainu + Hokkaido Ainu 100k 10.1 31.1 12.6 30.8\n“Wenenekaype” + Hokkaido Ainu 100k 10.2 30.5 15.1 32.1\n“Wenenekaype” + Japanese 100k 10.8 32.2 15.2 33.0\n“Wenenekaype” + English 100k 12.0 40.1 16.8 34.2\n“Wenenekaype” + Hokk. Ainu + Jap. 100k 10.9 33.7 16.2 33.4\n“Wenenekaype”+ Hokk. Ainu + Jap. + Eng. 100k 10.9 33.9 16.8 34.3\n1.5htocomplete.AfteraddingmoredatafromSakhalinAinu,weﬁne-tunedfor20kstepswithalearningrateof1e-4.\nWhile the batch size was reduced to 800k samples, we applied gradient accumulation to simulate 16 GPUs, which\nresulted in an eﬀective batch size of 12.8M, and training time of around 10 hours. In this setting, the “Wenenekaype”\ndata was oversampled by a factor of 10 (we found that without oversampling, performance on the validation set was\nmuchworse).Formodelsﬁne-tunedonacombinationofdatafromtwospeechvarieties,wesetthelearningrateto3e-5\nand executed 80k updates. Whenﬁne-tuning on Sakhalin Ainu and Hokkaido Ainu data together,the “Wenenekaype”\ndata was oversampled by a factor of 200 and the Tuytah data was oversampled by a factor of 10. In experiments not\nusing the latter data, “Wenenekaype” was oversampled by a factor of 100. In this case, a single ﬁne-tuning run took\nroughly 2 days. After adding data from a third language (i.e., Japanese), we ﬁne-tuned for 120k steps and almost 3\ndays.Finally,whenﬁne-tuningondatafromallfourlanguages,weexecuted160kupdates,whichtook4days.Inboth\ncases, the “Wenenekaype” data was oversampled to comprise roughly half of the training data.\n6.3. Results and Analysis\nIn Table 6 we compare error rates yielded by models ﬁne-tuned on monolingual or multilingual data, and with or\nwithout additional pretraining on Ainu data, in speech recognition on the test set. Figure 1 provides an analysis of the\nimpact of additional pretraining using target language data.\nOurresultsshowthatcontinuedpretrainingisclearlythemosteﬀectivewaytoadaptaspeechrepresentationmodel\nfor a new language. With just 10k updates (half a week on our system), we were able to obtain a reduction in CER by\nnearly13%(whendecodingthemodel’soutputwithaViterbidecoder).Aftertwoweeksand40kupdates,wereached\nan improvement by 24.5%. The best relative performance on our test data in terms of CER was obtained after 60k\nupdates. Concerning WER, the lowest values were measured after 60k and 90k updates (32.7 and 32.5, respectively)\nwhen decoding with a Viterbi decoder, and after 40k steps (32.3) when using a language model.\nFine-tuningonalltheavailableSakhalinAinudataanddecodingwithoutalanguagemodelyieldedthebestoverall\nresults. With the model before conducting further pretraining, multilingual ﬁne-tuning was not helpful, regardless of\nwhichlanguagecombinationwasusedandhowmuchtargetlanguagedatawasavailable(theadditionofJapanesedata\ndidresultinslightlylowerWER,butatthecostofanincreaseinCER).Quitesurprisingly,addingHokkaidoAinudata\nnot only had negative impact on the model’s performance, but also resulted in signiﬁcantly higher error rates than in\nexperimentsusingdatainJapaneseandEnglish.Therearetwopossiblereasonsforthisbehavior:(i)nothavinglearnt\na representation for either of the Ainu languages, the model was unable to take advantage of their similarities, and (ii)\nNowakowski et al. Page 8 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\n0 10 20 30 40 50 60 70 80 90 10010\n12\n14\n16\n18\nPretraining steps / 1000\nCER\n30\n32\n34\n36\n38\n40\n42\nWER\nCER (Viterbi)\nCER (KenLM)\nWER (Viterbi)\nWER (KenLM)\nFigure 1:Eﬀect of further pretraining using target language data. The models were ﬁne-tuned using “Wenenekaype” data\nonly.\nJapanese and English are distant enough from the target language for the model to be able to easily discern between\nthem, thus making relatively few errors due to confusion between languages.\nAfter additional pretraining, the results of multilingual ﬁne-tuning were completely diﬀerent: adding data from\nanother variety of Ainu produced the best performing model, followed by ﬁne-tuning on unrelated languages sharing\nsomephonologicalcharacteristics(i.e.,AinuandJapanese),whereascombiningAinuwithEnglishwasclearlyharmful.\nWhileadditionaldatafromHokkaidoAinudidnotleadtoanimprovementcomparedtothemodeltrainedon10hours\nof Sakhalin data, the results did improve in the scenario with less than 1 hour of target language data available. In this\ncase, a drop in WER was also observed after including Japanese data.\nIn experiments using labeled data from two or three additional languages, we observed worse results, even with\nlanguages which were helpful when used individually (namely, Hokkaido Ainu and Japanese). We think that this\nbehavior might be associated with lower capacity in the ﬁne-tuned model for each of the languages, but it requires\nfurther analysis.\nAnalysisofthetranscriptionsgeneratedwhenusingaViterbidecoderrevealedthatsomeoftheerrorswerecaused\nby an incorrect choice of the writing system (i.e., the use ofkatakana characters to represent sounds of the Ainu\nlanguage or, conversely, transcribing parts spoken in Japanese using Latin alphabet letters; an example is shown\nin Table 7). In fact, linguists often transcribe Japanese code-switched words found in Ainu language texts using a\nromanization system, rather than the Japanese script. For this reason, we decided to examine how the results would\nchangeifwerelaxourproblembyconvertingallJapanesecharactersinboththesystem’soutputandgroundtruthdatato\nLatinalphabet.Speciﬁcally,weused pykakasi toromanizeJapaneseaccordingtotheHepburntransliterationsystem.\nResults are presented in Table 8. While the modiﬁcation resulted in a slightly lower CER in almost all conﬁgurations,\nthe biggest improvements were observed for models ﬁne-tuned using Japanese data. Under this evaluation scheme, it\ncan be concluded that ﬁne-tuning jointly on “Wenenekaype” data and Japanese speech data is clearly beneﬁcial.\nThe above results are consistent with our hypothesis that labeled data from similar languages can be leveraged in\nﬁne-tuningtoobtainbetterperformanceonthetargetlanguage.Ontheotherhand,theyindicatethatanyimprovement\ncanonlytakeplaceifthefollowingtwoconditionsaremet:(i)themodelwasﬁrstpretrainedonthelanguagesinvolved\nin ﬁne-tuning, or at least on one of them (namely, the transfer language whose data one intends to use in addition to\nthe target language data), and (ii) the amount of labeled data in the target language is extremely low.\nSome of the errorsmade bythe system can be attributed toinconsistencies in the annotations used forﬁne-tuning\nand evaluation, namely,differences between the transcriptions of multiple instances of the same lexical item (which\nNowakowski et al. Page 9 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 7\nExcerpt from the transcriptions generated by a ﬁne-tuned model, showing errors caused by an incorrect choice of the\nwriting system.\nModel output: tanna ’an ’opompaki nah ramupeオota’asi nee manu ’タ ’asiソ’oヤw nah ’anramu ’ampeアノ ジ\nンkoy nee manu\nTransliteration: [tanna ’an ’opompaki nah ramupe oota’asi nee manu ’ta ’asi so’oyaw nah ’anramu ’ampe ano jinkoy\nnee manu]\nGround truth: tani neya アノ ’opompaki nah ramupe ’ota’asi nee manu ’ota’asi suy ’oyaw nah ’anramu’ampeアノ\ncinkoy nee manu\nTransliteration: [tani neya ano ’opompaki nah ramupe ’ota’asi nee manu ’ota’asi suy ’oyaw nah ’anramu’ampe ano\ncinkoy nee manu]\nTable 8\nEvaluation results after preprocessing the Viterbi decoder’s output and ground truth data by romanizing\nJapanese characters. Numbers in brackets indicate the change of CER/WER compared to results in Table 6.\nFine-tuning data Additional pretr. steps CER WER\n“Wenenekaype” 0 13.7 (-0.2) 42.3\nSakhalin Ainu (“Wenenekaype” + Tuytah) 0 11.6 (-0.2) 37.3\nSakhalin Ainu + Hokkaido Ainu 0 15.6 (-0.3) 50.5 (-0.1)\n“Wenenekaype” + Hokkaido Ainu 0 16.0 (-0.4) 51.0 (-0.1)\n“Wenenekaype” + Japanese 0 13.4 (-2.3) 40.9 (-0.9)\n“Wenenekaype” + English 0 13.6 (-0.3) 43.0\n“Wenenekaype” + Hokk. Ainu + Jap. 0 15.8 (-0.2) 52.0\n“Wenenekaype”+ Hokk. Ainu + Jap. + Eng. 0 14.8 (-0.4) 47.4\n“Wenenekaype” 100k 10.5 (-0.1) 33.5\nSakhalin Ainu (“Wenenekaype” + Tuytah) 100k 9.6 (-0.1) 29.8\nSakhalin Ainu + Hokkaido Ainu 100k 9.8 (-0.3) 31.1\n“Wenenekaype” + Hokkaido Ainu 100k 9.9 (-0.3) 30.5\n“Wenenekaype” + Japanese 100k 10.1 (-0.7) 31.9 (-0.3)\n“Wenenekaype” + English 100k 11.9 (-0.1) 40.1\n“Wenenekaype” + Hokk. Ainu + Jap. 100k 10.3 (-0.6) 33.5 (-0.2)\n“Wenenekaype”+ Hokk. Ainu + Jap. + Eng. 100k 10.9 33.9\nin turn, are a result of the absence of a standardized orthography for the Ainu language). For instance, the part of\n“Wenenekaype”used for evaluation includes 19 instances of the token wenporo (“very big”; a combination ofwen,\n“bad(ly)”,and poro,“big”), whereasinthetrainingdataitistranscribedintwodifferentways:wenporo (11instances)\nor wen poro(14 instances).As a result, both variants can be found in the model’spredictions. In the future, we will\ninvestigatemethods forautomatic detection ofdiscrepancies likethis in the data.\n6.3.1. Impact of the Decoding Method\nBaevski et al. (2020) reported large improvements in WER when decoding their model’s output with a textual\nlanguagemodel.AsshowninTable6,forthemodelbeforeadditionalpretrainingwealsoobservedsigniﬁcantreduction\nof WER, but at the cost of higher CER. For instance, when decoding the model ﬁne-tuned on all Sakhalin Ainu data\n(“Wenenekaype”+Tuytah)withthe4-grammodel,weachieveda15%reductionofWERwhileatthesametimeCER\nincreased by nearly 20%, compared to the Viterbi decoder. Table 9 shows an example of transcriptions generated by\nusing both decoding methods for a single sample from the test data.\nContinued pretraining on Ainu data closed the gap in terms of WER between the two decoding methods (see\nFigure 1) and the best overall results were obtained by decoding without a language model. This outcome indicates\nthat after having been taught a representation of the target language, a speech representation model is capable of\nlearning an implicit language model from the ﬁne-tuning data which is more powerful than a count-based n-gram\nmodel computed from the same data. A major diﬀerence between Baevski et al. (2020)’s and our setting is that while\ntheir language models were trained on a book corpus comprising over 800 million tokens (Panayotov et al., 2015),\nNowakowski et al. Page 10 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 9\nOutput of a ﬁne-tuned model for a single test sample, decoded with two diﬀerent methods. The KenLM model forces\nthe use of in-vocabulary words (e.g.,reekoh instead of reepoh and ’ekasihirather than ’ekasihii) which often leads to\nlower Word Error Rates. On the other hand, Viterbi decoder is better at handling out-of-vocabulary items (e.g.,’ankopuri\nis transcribed as’ankopuuri, whereas the language model replaced it with a completely diﬀerent word,’ankopisi) which\nresults in lower CER.\nViterbi decoder: ne’ohah nay kohnean tani macirih ’oho nay kohne ’anu wa reekoh ne’an cispuurikara ’ankii manuyke\n’awwen ’ekasihii ’ireske ’ankamuy henke ’ohta ’ankopuuri ’ahsin manuyke reepoh ne’an henke ’ihunke\nkii manuyke reekoh\nKenLM decoder: ’oha nay konna tani maciri ’ohonkesehe ne’an cispuurikara ’ankii manuyke ’awwen ’ekasihi ’ireske\n’ankamuy henke ’ohta ’ankopisi ’asin manuyke reekoh ne’an henke ’ihunke kii manuyke reekoh\nGround truth: ’ohah naykoh ne ’an tani macirihi ’ohoo naykoh ne ’a nu wa reekoh ne’an cispuurikara ’ankii manuyke\n’awwen ’ekasihi ’ireske ’ankamuy henkehe ’ohta ’ankopuri’ahte manuyke reekoh ne’an henke ’ihunke\nkii manuyke reekoh\nSakhalin Ainu textual data available to us was limited to the transcriptions of the speech data which we also used to\nﬁne-tune our model. This leads to a conclusion that in a language documentation scenario, where large amounts of\ntextual data are typically not available, decoding with an external language model may be of limited use. However,\nfuture work should investigate whether similar trends would also occur when using other types of language models,\nsuch as a character-level language model. Another factor that may be inﬂuencing our results is the characteristics\nof the writing system: Baevski et al. (2020) conducted their experiments on English, which exhibits many-to-many\ncorrespondences between graphemes and phonemes. Ainu, on the other hand, is transcribed according to a phonemic\northography, which reduces the need for explicit information about the correct spelling of individual lexical items.\n6.3.2. Impact of Overlapping Character Vocabulary\nConneau et al. (2021) found that using a shared phoneme vocabulary in multilingual ﬁne-tuning yields better\nresults than maintaining a separate vocabulary for each language. While our system is not operating on phonemes, we\nwantedtoverifyifanoverlapinthecharactervocabulariesrepresentingthetargetlanguageandotherlanguagesusedin\nﬁne-tuninghadaninﬂuenceontheaccuracyofspeechtranscription.Speciﬁcally,weﬁne-tunedadditionalmodelswith\n(i) Hokkaido Ainu transcriptions converted to upper case letters, (ii) LibriSpeech (English) transcriptions converted\nto lower case letters, and (iii) Japanese transcriptions converted to lower case alphabet letters. In all cases we used the\nmodel pretrained on Ainu language data. The output of the models was decoded with a Viterbi decoder. To minimize\nthe inﬂuence of code-switched parts written with Japanese script in Ainu language data,katakana characters in the\ngenerated transcriptions and ground truth data were transliterated into Latin letters before evaluation.\nResults are presented in Table 10. With less than 1h of target language data (i.e., “Wenenekaype” only) and\nadditional data from a similar language (Hokkaido Ainu or Japanese), using a shared output vocabulary resulted in\nlowererrorrates.ConvertingEnglishtexttolowercaseproducedmixedresults,withimprovedWERbuthigherCER.\nAfter increasing the quantity of labeled Sakhalin Ainu data, the opposite outcome was observed: the model trained\non Hokkaido Ainu text in upper case yielded better performance. In contrast to the experiment with shared character\nvocabulary, in this setting ﬁne-tuning jointly on Sakhalin and Hokkaido Ainu data resulted in a small improvement\ncompared to using Sakhalin Ainu data only.\nGiven these results, we conclude that when there is very little labeled target language data and additional data\nfrom a similar speech variety is utilized, it is better to use a shared character vocabulary, as it seems to facilitate\ncross-lingual transfer. This recommendation should be treated with caution, as similarity between languages is not\nnecessarily matched by similarity in their writing systems (or transliteration methods), and thus the results for other\ncombinations of languages may be diﬀerent. On the other hand, as long as the phoneme-grapheme mappings diﬀer in\npredictable ways, mismatches can be handled by applying rule-based preprocessing to the transcriptions. With more\ntargetlanguagedataavailable,themodelismoreoftenconfusedbythecross-lingualsignalthanitbeneﬁtsfromit,and\ntherefore it would probably be best to just execute monolingual ﬁne-tuning (which also saves compute), but it may be\nworth experimenting with a separate character vocabulary. Future research should perform similar analyses for other\nlanguages.\nNowakowski et al. Page 11 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nTable 10\nEﬀect of using separate or shared character vocabularies for the target language and additional languages\nrepresented in ﬁne-tuning.\nFine-tuning data Char. vocabulary size CER WER\n“Wenenekaype” 76 10.5 33.5\n“Wenenekaype” (lower case) + Hokk. Ainu (UPPER CASE) 102 10.2 31.2\n“Wenenekaype” (lower case) + Hokk. Ainu (lower case) 86 9.9 30.5\n“Wenenekaype” (alphabet) + Japanese (katakana) 137 10.1 31.9\n“Wenenekaype” (alphabet) + Japanese (alphabet) 119 9.7 31.0\n“Wenenekaype” (lower case) + English (UPPER CASE) 102 11.9 40.1\n“Wenenekaype” (lower case) + English (lower case) 86 12.3 39.6\nSakhalin Ainu 94 9.6 29.8\nSakh. Ainu (lower case) + Hokk. Ainu (UPPER CASE) 111 9.4 29.6\nSakh. Ainu (lower case) + Hokk. Ainu (lower case) 95 9.8 31.1\n7. Conclusions and Future Work\nWe have demonstrated that a strong multilingual speech representation model, such as XLSR-53, can be adapted\nfor a new, low-resource language through multilingual ﬁne-tuning and additional pretraining, resulting in improved\ndownstream performance. Through experiments with automatic transcription of Sakhalin Ainu, we found that\ncontinued pretraining on target language data leads to substantial reduction in error rates. Furthermore, our results\nshow that in a scenario where labeled target language data is extremely scarce, the model can take advantage of data\nfrom a related speech variety (or – to a lesser extent – an unrelated language with similar phonological traits) added\nduring ﬁne-tuning, if that additional language was seen during pretraining.\nOur ﬁndings conﬁrm the hypothesis that language similarity should be taken into consideration and can be\nleveraged in the process of multilingual ﬁne-tuning. They also indicate that self-supervised pretraining of a language\nrepresentationmodelisnotonlyeﬀectiveinadaptingitforaparticularlanguage,buttherepresentationslearnedduring\nthat process can also serve as a bridge for transfer to similar languages in the form of cross-lingual supervision. We\nexpect this observation to also be true for cross-domain supervision within the same language – future work should\ninvestigate this assumption.\nAs a next step in our research, we are planning to increase the amount of labeled in-domain data by digitizing\nand aligning speech data and transcriptions from Murasaki (1976) and Murasaki and Fujiyama (2013, 2016).Apart\nfrom that, we will test other types of language models fordecoding (specifically,neural and character-levellanguage\nmodels). Furthermore, we will examine potential methods for reducing negative cross-lingual signal while retaining\nasmuchaspossibleofthebeneﬁts,suchasﬁne-tuningwithlanguageembeddingsandensemblemodels.Wealsoplan\ntoexploredata augmentation techniques.\nAcknowledgements\nThis work was supported by JSPS KAKENHI Grant Number JP22K17952.\nReferences\nAbney, S., 2011. Language Digitization. URL:http://www.vinartus.net/spa/p102-v2.pdf.\nAn=ukokor Aynu ikor oma kenru (National Ainu Museum), 2017–2022. Ainu-go¯Akaibu [Ainu Language Archive]. URL:https://ainugo.\nnam.go.jp/.\nArdila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F.M., Weber, G., 2020. Common Voice:\nA Massively-Multilingual Speech Corpus, in: Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pp.\n4211–4215.\nBabu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A., Auli, M.,\n2021. XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale. arXiv abs/2111.09296.\nBaevski, A., Schneider, S., Auli, M., 2019. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations. CoRR abs/1910.05453.\nURL:http://arxiv.org/abs/1910.05453,arXiv:1910.05453.\nBaevski, A., Zhou, H., Mohamed, A., Auli, M., 2020. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. ArXiv\nabs/2006.11477.\nNowakowski et al. Page 12 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nBugaeva, A., 2012. Southern Hokkaido Ainu, in: Tranter, N. (Ed.), The languages of Japan and Korea. Routledge, London, pp. 461–509.\nChiba University Graduate School of Humanities and Social Sciences, 2014. Ainugo Mukawa H¯ogen Nihongo – Ainugo Jiten [Japanese – Ainu\nDictionary for the Mukawa Dialect of Ainu]. URL:https://www.gshpa.chiba-u.jp/cas/Ainu-archives/index.html.\nCieri, C., Miller, D., Walker, K., 2004. The ﬁsher corpus: a resource for the next generations of speech-to-text, in: Proceedings of the Fourth\nInternational Conference on Language Resources and Evaluation (LREC’04), European Language Resources Association (ELRA), Lisbon,\nPortugal. URL:http://www.lrec-conf.org/proceedings/lrec2004/pdf/767.pdf.\nClark, K., Luong, M.T., Le, Q.V., Manning, C.D., 2020. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, in:\nICLR. URL:https://openreview.net/pdf?id=r1xMH1BtvB.\nConneau, A., Baevski, A., Collobert, R., Mohamed, A., Auli, M., 2021. Unsupervised Cross-lingual Representation Learning for Speech\nRecognition, in: Interspeech.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., Stoyanov, V., 2020.\nUnsupervisedCross-lingualRepresentationLearningatScale,in:Proceedingsofthe58thAnnualMeetingoftheAssociationforComputational\nLinguistics, Association for ComputationalLinguistics, Online. pp. 8440–8451. URL:https://aclanthology.org/2020.acl-main.747,\ndoi:10.18653/v1/2020.acl-main.747.\nDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\nin: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota. pp. 4171–4186. URL:\nhttps://aclanthology.org/N19-1423, doi:10.18653/v1/N19-1423.\nEbrahimi, A., Kann, K., 2021. How to Adapt Your Pretrained Multilingual Model to 1600 Languages, in: Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), Association for Computational Linguistics, Online. pp. 4555–4567. URL:https://aclanthology.org/2021.acl-long.351,\ndoi:10.18653/v1/2021.acl-long.351.\nGraves, A., Fernández, S., Gomez, F., Schmidhuber, J., 2006. Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with\nRecurrentNeuralNetworks,in:Proceedingsofthe23rdInternationalConferenceonMachineLearning,AssociationforComputingMachinery,\nNew York, NY, USA. p. 369–376. URL:https://doi.org/10.1145/1143844.1143891, doi:10.1145/1143844.1143891.\nGries, S.T., Berez, A.L., 2017. Linguistic Annotation in/for Corpus Linguistics, in: Ide, N., Pustejovsky, J. (Eds.), Handbook of Linguistic\nAnnotation. Springer, pp. 379–409.\nGururangan, S., Marasovi/uni0107, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., Smith, N.A., 2020. Don’t Stop Pretraining: Adapt Language\nModels to Domains and Tasks, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for\nComputational Linguistics, Online. pp. 8342–8360. URL:https://aclanthology.org/2020.acl-main.740, doi:10.18653/v1/2020.\nacl-main.740.\nHattori, S., 1964. Ainugo h¯ogen jiten [Dictionary of Ainu dialects]. Iwanami Shoten, T¯oky¯o.\nHjortnaes, N., Partanen, N., Rießler, M., Tyers, F.M., 2020. Towards a Speech Recognizer for Komi, an Endangered and Low-Resource Uralic\nLanguage,in:ProceedingsoftheSixthInternationalWorkshoponComputationalLinguisticsofUralicLanguages,AssociationforComputational\nLinguistics, Wien, Austria. pp. 31–37. URL:https://aclanthology.org/2020.iwclul-1.5, doi:10.18653/v1/2020.iwclul-1.5.\nHokkaid¯o Utari Ky¯okai, 1994. Akor Itak [Our language]. Hokkaid¯o Utari Ky¯okai, Sapporo.\nHoward, J., Ruder, S., 2018. Universal Language Model Fine-tuning for Text Classiﬁcation, in: Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Melbourne, Australia. pp.\n328–339. URL:https://aclanthology.org/P18-1031, doi:10.18653/v1/P18-1031.\nHsu, W.N., Bolte, B., Tsai, Y.H.H., Lakhotia, K., Salakhutdinov, R., Mohamed, A., 2021a. HuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units. IEEE/ACM Transactions on Audio, Speech, and Language Processing 29, 3451–3460.\nHsu, W.N., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., Kahn, J., Lee, A., Collobert, R., Synnaeve, G., Auli, M., 2021b. Robust\nwav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training, in: Interspeech.\nInformationResourcesCenter,ResearchInstituteforLanguagesandCulturesofAsiaandAfrica,TokyoUniversityofForeignStudies,n.d. AA-ken\nAinu-go shiry¯o k¯okai purojekuto [ILCAA’s Project for the Publication of Ainu Language Materials]. URL:http://ainugo.aa-ken.jp/.\nKayano, S., 1996. Kayano Shigeru no Ainugo jiten [Shigeru Kayano’s Ainu dictionary]. Sanseid¯o, T¯oky¯o.\nKessler, S., Thomas, B., Karout, S., 2021. Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech\nRecognition. ArXiv abs/2107.13530.\nKhurana, S., Laurent, A., Glass, J., 2022. Magic dust for cross-lingual adaptation of monolingual wav2vec-2.0, in: ICASSP 2022 - 2022 IEEE\nInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.6647–6651. doi: 10.1109/ICASSP43922.2022.9746276.\nLittell,P.,Mortensen,D.R.,Lin,K.,Kairis,K.,Turner,C.,Levin,L.,2017. Urielandlang2vec:Representinglanguagesastypological,geographical,\nand phylogenetic vectors, in: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pp. 8–14.\nMatsuura,K.,Ueno,S.,Mimura,M.,Sakai,S.,Kawahara,T.,2020. SpeechCorpusofAinuFolkloreandEnd-to-endSpeechRecognitionforAinu\nLanguage, in: LREC.\nMurasaki, K., 1976. Karafuto Ainu-go / Sakhalin Rayciska Ainu Dialect. Kokusho Kank¯okai, Tokyo.\nMurasaki, K., 2009. Karafuto Ainugo ny¯umon kaiwa / First Step for the Sakhalin Ainu Language. Ryokugeisha, Kushiro.\nMurasaki, K., Asai, T., 2001. Karafuto Ainu no mukashi-banashi: Tuytah [Sakhalin Ainu folktales: Tuytah]. S¯of¯ukan, Tokyo.\nMurasaki, K., Fujiyama, H., 2010. Sakhalin Ainu Folktales (ucaskuma): Wenenekaype. volume 2 ofILCAA Norheast Asian Studies. Research\nInstitute for Languages and Cultures of Asia and Africa, Tokyo University of Foreign Studies, Tokyo.\nMurasaki, K., Fujiyama, H., 2013. Short Sentences in Sakhalin Ainu Spoken by Fujiyama Haru (1). volume 3 ofSakhalin Ainu Language Series.\nAynu Teetawanoankur Kanpinuye Cise (Center for Ainu & Indigenous Studies), Hokkaido University, Sapporo.\nNowakowski et al. Page 13 of 14\nAdapting Speech Representation Model for a New Language through Multilingual Fine-tuning and Continued Pretraining\nMurasaki, K., Fujiyama, H., 2016. Short Sentences in Sakhalin Ainu Spoken by Fujiyama Haru (2). volume 4 ofSakhalin Ainu Language Series.\nAynu Teetawanoankur Kanpinuye Cise (Center for Ainu & Indigenous Studies), Hokkaido University, Sapporo.\nNakagawa, H., 2013. Ny¯u ekusupuresu Ainugo. Hakusuisha, T¯oky¯o.\nNakagawa, H., Bugaeva, A., Kobayashi, M., Kimura, K., 2016. Glossed Audio Corpus of Ainu Folklore. URL:https://ainu.ninjal.ac.jp/\nfolklore/corpus/en/.\nNational Institute for Japanese Language and Linguistics, 2015. A Topical Dictionary of Conversational Ainu. URL:https://ainu.ninjal.\nac.jp/topic/.\nNibutani Ainu Culture Museum, n.d. Ainu Language & Ainu Oral Literature. URL:http://www.town.biratori.hokkaido.jp/biratori/\nnibutani/culture/language/.\nNowakowski,K.,Ptaszynski,M.,Masui,F.,2019. MiNgMatch-AFastN-gramModelforWordSegmentationoftheAinuLanguage. Information\n10, 317. doi:10.3390/info10100317.\nNowakowski, K., Ptaszynski, M., Masui, F., 2020. Spicing up the Game for Underresourced Language Learning: Preliminary Experiments with\nAinu Language-speaking Pepper Robot, in: The 6st Workshop on Linguistic and Cognitive Approaches to Dialog Agents (LaCATODA\n2020). URL: http://arakilab.media.eng.hokudai.ac.jp/IJCAI2020/LACATODA2020/Program_files/Spicing%20up%\n20the%20Game%20for%20Underresourced%20Language%20Learning%3A%20Preliminary%20Experiments%20with%20Ainu%\n20Language-speaking%20Pepper%20Robot.pdf.\nPanayotov, V., Chen, G., Povey, D., Khudanpur, S., 2015. Librispeech: An ASR corpus based on public domain audio books, in: 2015 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206–5210. doi:10.1109/ICASSP.2015.7178964.\nPfeiﬀer,J.,Vuli/uni0107,I.,Gurevych,I.,Ruder,S.,2020. MAD-X:AnAdapter-BasedFrameworkforMulti-TaskCross-LingualTransfer,in:Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online.\npp. 7654–7673. URL:https://aclanthology.org/2020.emnlp-main.617, doi:10.18653/v1/2020.emnlp-main.617.\nRefsing, K., 1986. The Ainu language. The morphology and syntax of the Shizunai dialect. Aarhus University Press, Aarhus.\nSanabria, R., Hsu, W.N., Baevski, A., Auli, M., 2022. Measuring the impact of individual domain factors in self-supervised pre-training. ArXiv\nabs/2203.00648.\nSchneider, S., Baevski, A., Collobert, R., Auli, M., 2019. wav2vec: Unsupervised Pre-training for Speech Recognition, in: INTERSPEECH.\nShibatani, M., 1990. The languages of Japan. Cambridge University Press, London.\nSingh, J., McCann, B., Keskar, N.S., Xiong, C., Socher, R., 2019. XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and\nQuestion Answering. ArXiv abs/1905.11471.\nSonobe, R., Takamichi, S., Saruwatari, H., 2017. JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis. ArXiv\nabs/1711.00354.\nSriram, A., Auli, M., Baevski, A., 2022. Wav2vec-aug: Improved self-supervised training with limited data. ArXiv abs/2206.13654.\nSun, C., Qiu, X., Xu, Y., Huang, X., 2019. How to Fine-Tune BERT for Text Classiﬁcation?, in: Sun, M., Huang, X., Ji, H., Liu, Z., Liu, Y. (Eds.),\nChinese Computational Linguistics, Springer International Publishing, Cham. pp. 194–206.\nTang, Y., Tran, C., Li, X., Chen, P.J., Goyal, N., Chaudhary, V., Gu, J., Fan, A., 2020. Multilingual Translation with Extensible Multilingual\nPretraining and Finetuning. ArXiv abs/2008.00401.\nVovin, A., 2016. ON THE LINGUISTIC PREHISTORY OF HOKKAID¯O. Studia Orientalia 117, 29–38.\nVyas, A., Hsu, W.N., Auli, M., Baevski, A., 2022. On-demand compute reduction with stochastic wav2vec 2.0. ArXiv abs/2204.11934.\nWu, F., Kim, K., Pan, J., Han, K.J., Weinberger, K.Q., Artzi, Y., 2022. Performance-eﬃciency trade-oﬀs in unsupervised pre-training for speech\nrecognition, in: ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7667–7671.\ndoi:10.1109/ICASSP43922.2022.9747432.\nXu, Q., Baevski, A., Auli, M., 2021. Simple and Eﬀective Zero-shot Cross-lingual Phoneme Recognition. ArXiv abs/2109.11680.\nZahrer, A., Zgank, A., Schuppler, B., 2020. Towards Building an Automatic Transcription System for Language Documentation: Experiences\nfromMuyu,in:Proceedingsofthe12thLanguageResourcesandEvaluationConference,EuropeanLanguageResourcesAssociation,Marseille,\nFrance. pp. 2893–2900. URL:https://aclanthology.org/2020.lrec-1.353.\nNowakowski et al. Page 14 of 14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8041527271270752
    },
    {
      "name": "Language model",
      "score": 0.6778368949890137
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5688148736953735
    },
    {
      "name": "Natural language processing",
      "score": 0.5462082028388977
    },
    {
      "name": "Speech recognition",
      "score": 0.4742136001586914
    },
    {
      "name": "First language",
      "score": 0.439385324716568
    },
    {
      "name": "Artificial intelligence",
      "score": 0.438934326171875
    },
    {
      "name": "Linguistics",
      "score": 0.306915819644928
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I68561585",
      "name": "Tohoku University of Community Service and Science",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I98957242",
      "name": "Kitami Institute of Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I180203408",
      "name": "Yokohama National University",
      "country": "JP"
    }
  ]
}