{
  "title": "The sociolinguistic foundations of language modeling",
  "url": "https://openalex.org/W4406337689",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2110384033",
      "name": "Jack Grieve",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A3199121835",
      "name": "Sara Bartl",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A2344052998",
      "name": "Matteo Fuoli",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A2288251119",
      "name": "Jason Grafmiller",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A2127631744",
      "name": "Weihang Huang",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A5104579730",
      "name": "Alejandro Jawerbaum",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A1941683398",
      "name": "Akira Murakami",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A2162244701",
      "name": "Marcus Perlman",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A5097372902",
      "name": "Dana Roemling",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A2131871353",
      "name": "Bodo Winter",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A2110384033",
      "name": "Jack Grieve",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3199121835",
      "name": "Sara Bartl",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2344052998",
      "name": "Matteo Fuoli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288251119",
      "name": "Jason Grafmiller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127631744",
      "name": "Weihang Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5104579730",
      "name": "Alejandro Jawerbaum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1941683398",
      "name": "Akira Murakami",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162244701",
      "name": "Marcus Perlman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5097372902",
      "name": "Dana Roemling",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131871353",
      "name": "Bodo Winter",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4404089322",
    "https://openalex.org/W2041334335",
    "https://openalex.org/W4399364148",
    "https://openalex.org/W3133450183",
    "https://openalex.org/W4391667444",
    "https://openalex.org/W2073443734",
    "https://openalex.org/W2523095402",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2894950577",
    "https://openalex.org/W4391591576",
    "https://openalex.org/W4392964409",
    "https://openalex.org/W4375959448",
    "https://openalex.org/W2010289719",
    "https://openalex.org/W2148259819",
    "https://openalex.org/W76669076",
    "https://openalex.org/W1479906642",
    "https://openalex.org/W3127589626",
    "https://openalex.org/W2889478606",
    "https://openalex.org/W4200466024",
    "https://openalex.org/W4367188881",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2964235839",
    "https://openalex.org/W4294029935",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4247190883",
    "https://openalex.org/W4382394716",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2963887077",
    "https://openalex.org/W4402670259",
    "https://openalex.org/W4387929411",
    "https://openalex.org/W4281777248",
    "https://openalex.org/W2740953594",
    "https://openalex.org/W4399031574",
    "https://openalex.org/W4403680773",
    "https://openalex.org/W6793289451",
    "https://openalex.org/W6765389903",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W4389669106",
    "https://openalex.org/W6845909048",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2591139331",
    "https://openalex.org/W4388198642",
    "https://openalex.org/W2128205443",
    "https://openalex.org/W4247998952",
    "https://openalex.org/W2898954967",
    "https://openalex.org/W2507475467",
    "https://openalex.org/W1979839410",
    "https://openalex.org/W4286905034",
    "https://openalex.org/W4388488349",
    "https://openalex.org/W3002093512",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4237634547",
    "https://openalex.org/W1988672109",
    "https://openalex.org/W2397481274",
    "https://openalex.org/W4308363084",
    "https://openalex.org/W46957478",
    "https://openalex.org/W4382567638",
    "https://openalex.org/W2960798246",
    "https://openalex.org/W2307311961",
    "https://openalex.org/W2810140910",
    "https://openalex.org/W4402670686",
    "https://openalex.org/W4287812619",
    "https://openalex.org/W4392013712",
    "https://openalex.org/W6992309071",
    "https://openalex.org/W4388277090",
    "https://openalex.org/W3126259453",
    "https://openalex.org/W4401947350",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W4313182274",
    "https://openalex.org/W2806221456",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W6691593892",
    "https://openalex.org/W6797203527",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4400103834",
    "https://openalex.org/W4395010390",
    "https://openalex.org/W4391158853",
    "https://openalex.org/W2234751941",
    "https://openalex.org/W4395483680",
    "https://openalex.org/W2963754320",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4400141630",
    "https://openalex.org/W4237765076",
    "https://openalex.org/W2741636242",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W2252735083",
    "https://openalex.org/W6848955896",
    "https://openalex.org/W4256454205",
    "https://openalex.org/W4206472724",
    "https://openalex.org/W4389519097",
    "https://openalex.org/W4212864386",
    "https://openalex.org/W6850287543",
    "https://openalex.org/W4247314001",
    "https://openalex.org/W4404518437",
    "https://openalex.org/W4393160461",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4402671697",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W2977237493",
    "https://openalex.org/W4378718328",
    "https://openalex.org/W4287854995",
    "https://openalex.org/W4399759405",
    "https://openalex.org/W4323848232",
    "https://openalex.org/W4415184923",
    "https://openalex.org/W4385572703",
    "https://openalex.org/W4385507561",
    "https://openalex.org/W2182731578",
    "https://openalex.org/W4385524197",
    "https://openalex.org/W4237050636",
    "https://openalex.org/W4362696539",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W4401386758",
    "https://openalex.org/W4240890129",
    "https://openalex.org/W4294435850",
    "https://openalex.org/W1796112755",
    "https://openalex.org/W6797713218",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2290025337",
    "https://openalex.org/W4313678819",
    "https://openalex.org/W4403577635",
    "https://openalex.org/W6948936239",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W4386566565",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4211011458",
    "https://openalex.org/W6681733276",
    "https://openalex.org/W3024308166",
    "https://openalex.org/W4390528803",
    "https://openalex.org/W649849733",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W3035377849",
    "https://openalex.org/W4387156634",
    "https://openalex.org/W4378765257",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W6862876615",
    "https://openalex.org/W2963161952",
    "https://openalex.org/W1798535660",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4390602555",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4391013556",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4400480600",
    "https://openalex.org/W4385262268",
    "https://openalex.org/W3161720865",
    "https://openalex.org/W2109052424",
    "https://openalex.org/W2008645545",
    "https://openalex.org/W4366850566",
    "https://openalex.org/W3153490941",
    "https://openalex.org/W4396715317",
    "https://openalex.org/W4401497789",
    "https://openalex.org/W4389363793",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W2051070621",
    "https://openalex.org/W2963672378",
    "https://openalex.org/W2140231764",
    "https://openalex.org/W3159454188",
    "https://openalex.org/W2520870635",
    "https://openalex.org/W1575967963",
    "https://openalex.org/W3197736625",
    "https://openalex.org/W3098567718",
    "https://openalex.org/W4300180226",
    "https://openalex.org/W2018730149",
    "https://openalex.org/W4230640544",
    "https://openalex.org/W3101004475",
    "https://openalex.org/W2014313181",
    "https://openalex.org/W2322604240",
    "https://openalex.org/W128638292",
    "https://openalex.org/W4404783771",
    "https://openalex.org/W4205519357",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4210421391",
    "https://openalex.org/W4388154864",
    "https://openalex.org/W3105871743",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2774343269",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2093585241",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W2956159039",
    "https://openalex.org/W2500297083"
  ],
  "abstract": "In this article, we introduce a sociolinguistic perspective on language modeling. We claim that language models in general are inherently modeling varieties of language , and we consider how this insight can inform the development and deployment of language models. We begin by presenting a technical definition of the concept of a variety of language as developed in sociolinguistics. We then discuss how this perspective could help us better understand five basic challenges in language modeling: social bias, domain adaptation, alignment, language change , and scale . We argue that to maximize the performance and societal value of language models it is important to carefully compile training corpora that accurately represent the specific varieties of language being modeled, drawing on theories, methods, and descriptions from the field of sociolinguistics.",
  "full_text": "TYPE Review\nPUBLISHED /one.tnum/three.tnum January /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nOPEN ACCESS\nEDITED BY\nMeishan Zhang,\nHarbin Institute of Technology, China\nREVIEWED BY\nCynthia Whissell,\nLaurentian University, Canada\nKevin Tang,\nHeinrich Heine University of Düsseldorf,\nGermany\nLiwei Yang,\nNortheast Normal University, China\n*CORRESPONDENCE\nJack Grieve\nj.grieve@bham.ac.uk\nRECEIVED /two.tnum/nine.tnum July /two.tnum/zero.tnum/two.tnum/four.tnum\nACCEPTED /three.tnum/zero.tnum November /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /one.tnum/three.tnum January /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nGrieve J, Bartl S, Fuoli M, Grafmiller J,\nHuang W, Jawerbaum A, Murakami A,\nPerlman M, Roemling D and Winter B (/two.tnum/zero.tnum/two.tnum/five.tnum)\nThe sociolinguistic foundations of language\nmodeling. Front. Artif. Intell./seven.tnum:/one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Grieve, Bartl, Fuoli, Grafmiller, Huang,\nJawerbaum, Murakami, Perlman, Roemling\nand Winter. This is an open-access article\ndistributed under the terms of the\nCreative\nCommons Attribution License (CC BY) . The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nThe sociolinguistic foundations\nof language modeling\nJack Grieve *, Sara Bartl, Matteo Fuoli, Jason Grafmiller,\nWeihang Huang, Alejandro Jawerbaum, Akira Murakami,\nMarcus Perlman, Dana Roemling and Bodo Winter\nDepartment of Linguistics and Communication, University of Bir mingham, Birmingham,\nUnited Kingdom\nIn this article, we introduce a sociolinguistic perspective on lan guage modeling.\nWe claim that language models in general are inherently model ing varieties of\nlanguage, and we consider how this insight can inform the development and\ndeployment of language models. We begin by presenting a techn ical deﬁnition\nof the concept of a variety of language as developed in socioling uistics. We\nthen discuss how this perspective could help us better understan d ﬁve basic\nchallenges in language modeling: social bias, domain adaptation, alignment,\nlanguage change, and scale. We argue that to maximize the performance\nand societal value of language models it is important to carefull y compile\ntraining corpora that accurately represent the speciﬁc varieti es of language being\nmodeled, drawing on theories, methods, and descriptions from the ﬁeld of\nsociolinguistics.\nKEYWORDS\nAI ethics, artiﬁcial intelligence, computational sociolin guistics, corpus linguistics, large\nlanguage models, natural language processing, varieties of la nguage\n/one.tnum Introduction\nThe underlying task of language modeling is to predict the probability of word tokens,\nor other linguistic forms, in a text based on previously observed texts ( Jurafsky and\nMartin, 2023 ). Language modeling is not new ( Bengio et al., 2003 ), but when pursued\nthrough the analysis of extremely large corpora of natural language using transformer-\nbased architectures (\nVaswani et al., 2017 ; Devlin et al., 2018 ), it has proven to be a uniquely\neﬀective approach to natural language processing (NLP) ( Radford et al., 2019 ). These\nsystems, which have come to be known as Large Language Models (LLMs), are currently\nrevolutionizing Artiﬁcial Intelligence (AI), with especially powerful LLMs such as GPT-\n4 (\nAchiam et al., 2023 ), LLaMa ( Touvron et al., 2023 ), Mistral ( Jiang et al., 2023 ) often\nbeing referred to as base models or foundation models (Bommasani et al., 2021 ) due to\ntheir high levels of ﬂuency and their ability to help achieve state-of-the-art performance\nacross a wide range of downstream tasks, most famously in chatbots like ChatGPT (\nRay,\n2023). Despite increasing concerns about the risks of LLMs ( Bender et al., 2021 ), experts\nacross many ﬁelds believe they will have a major impact on society, including in medicine\n(\nThirunavukarasu et al., 2023 ; Huang Y. et al., 2024 ), education ( Kasneci et al., 2023 ; Yigci\net al., 2024 ), computer programming ( Li et al., 2022 ; Wang et al., 2024 ), journalism ( Pavlik,\n2023; Li et al., 2024 ), economics ( Horton, 2023; Guo and Yang, 2024), and technical writing\n(Lund et al., 2023 ; Cruz-Castro et al., 2024 ).\nGiven the growing societal importance of LLMs, language modeling has provoked\ncritical discussion from a wide range of perspectives, not only AI and NLP (e.g., Bender\net al., 2021 ; Bommasani et al., 2021 ; Jiao et al., 2024 ; Head et al., 2023 ), but in linguistics\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\n(e.g., Piantadosi, 2023 ; Dentella et al., 2023 ; Marcus et al., 2023 ),\ncognitive science (e.g., Hardy et al., 2023 ; Demszky et al., 2023 ;\nMichaelov et al., 2024 ), and ethics (e.g., Birhane et al., 2023 ; Cabrera\net al., 2023 ; Li et al., 2023 ; Stefan et al., 2023 ; Haque and Li, 2024 ).\nThere is, however, a very basic question about language models that\nhas received remarkably little attention in the literature:\nWhat is actually being modeled by language models?\nAlthough the goal of language modeling is clear (i.e., token\nprediction), the type of language being modeled by language\nmodels is usually only deﬁned in the most general terms, for\nexample, “a broad swath of internet data” (\nBrown et al., 2020 ).\nModels are often trained on corpora based at least in part on the\nCommonCrawl dataset or alike (\nRadford et al., 2019 ; Raﬀel et al.,\n2020; Baack, 2024 ), but otherwise, in most cases, the nature of the\nlanguage being modeled is not described at all ( Bender et al., 2021 ).\nIn large part, this is a natural consequence of the need for massive\namounts of data to train base models, making the sources of these\ncorpora of secondary concern. However, even when these models\nare adapted for more speciﬁc contexts (\nGururangan et al., 2020 ),\nthe type of language used for further training is generally only\nloosely deﬁned. For example, ChatGPT was developed by adapting\na GPT-3.5 base model for dialogue (\nOpenAI, 2022 ), but the form\nof dialogue actually being modeled by ChatGPT is something\nmuch less diverse and much more artiﬁcial than everyday English\nconversation, as anyone who interacts with ChatGPT knows.\nDrawing on modern sociolinguistic theory, in this paper, we\ntherefore provide an answer to the question what is being modeled\nby language models?\nLanguage models are models of varieties of language.\nWe argue that any language model is inherently modeling the\nvariety of language represented by the corpus on which it is trained,\neven if that variety of language is unknown and even if that corpus\nis a poor representation of that variety of language. Our view is\nthat this simple insight can inform, at a fundamental level, how\nlanguage models are developed and deployed. Given rapid advances\nin language modeling in recent years and the increasing societal\nimpact and risk associated with LLMs, we believe the sociolinguistic\nperspective we are proposing in this paper is especially important\nat this time—not only to improve the performance, evaluation,\nand applicability of LLMs, but to guide the creation of safe\nand ethical AI systems and to help us better understand their\nunderlying nature.\nIn the rest of this paper, we expand on our claim that, in its\nbasic form, a language model of any type represents a variety of\nlanguage, and we consider the implications of this claim for the\ntask of language modeling. We do this primarily by synthesizing\nrecent research in NLP and sociolinguistics, especially research\nfrom the emerging ﬁeld of computational sociolinguistics , which\nsits at their intersection (\nNguyen et al., 2016 ; Eisenstein, 2017 ;\nGrieve et al., 2023 ). We ﬁrst provide a technical deﬁnition of the\nsociolinguistic concept of a variety of language and argue that\nthis concept inherently underpins the task of language modeling.\nWe then introduce and discuss ﬁve general challenges in language\nmodeling that we believe the sociolinguistic perspective introduced\nin this paper can help address. We refer to these challenges as social\nbias, domain adaptation, alignment, language change, and scale.\nOur primary goal in this position paper is therefore to introduce\na sociolinguistic perspective on language modeling and to argue\nfor its relevance to our general understanding of language models,\nas well as their development and deployment in the real world.\nOur intent is not to provide simple or speciﬁc solutions to major\nchallenges in language modeling. Rather, our intent is to oﬀer\na new and general theoretical perspective from which to better\nunderstand these challenges, arguing for greater engagement in\nthe ﬁeld of language modeling with the ﬁeld of sociolinguistics.\nOur core argument is that, when pretraining or further pretraining\nlanguage models, it is important to carefully consider the speciﬁc\nvarieties of language being modeled and to compile corpora that\naccurately represent these varieties of language. Furthermore, we\nargue that corpus compilation should be ﬁrmly grounded in\ntheories, methods, and ﬁndings of sociolinguistics, which has long\nfocused on understanding the nature of language variation and\nchange. Our hope is that the proposals made in this paper will\ninspire future empirical research in language modeling, ultimately\nleading to improvements in the performance of language models\nand the societal value of the NLP systems into which they\nare embedded.\n/two.tnum Deﬁning varieties of language\nA variety of language , or more simply a variety, is a term\ncommonly used across linguistics to refer to any type of language\n(\nCrystal and Davy, 1969 ; Hartmann and Stork, 1972 ; Matthews,\n1997; McEnery et al., 2006 ; Jackson, 2007 ; Crystal, 2011 ). The\nterm is especially common in ﬁelds that study language variation\nand change—like sociolinguistics, dialectology, typology, historical\nlinguistics, discourse analysis, stylistics, and corpus linguistics—\nwhere it is generally used to identify the types of language targeted\nfor description, comparison, or other forms of linguistic analysis.\nOne reason a variety of language is such a powerful concept\nis because it can be used to identify such a wide range of\nphenomena—from very broadly deﬁned varieties like the entire\nEnglish language to very narrowly deﬁned varieties like the\nspeeches of a single politician. This terminology also allows\nlinguists to sidestep debates, which are often underlyingly political\nin nature, like whether a given variety qualiﬁes as a dialect or a\nlanguage (\nMeyerhoﬀ, 2018 ). For example, regardless of whether\nScots is considered to be a dialect of English or a distinct language,\nScots can be considered to be a variety, as well as a sub-variety\nof some larger Anglic variety that also includes English (\nAitken,\n1985). Similarly, regardless of whether Chinese is considered to be\na family composed of many languages or a language composed of\nmany dialects, all forms of Chinese can be considered to be both\nvarieties themselves and part of some larger Sinitic variety (\nHuang\nH. et al., 2024 ).\nAlthough what are traditionally considered entire languages\nlike English or Chinese can be referred to as varieties, the\nterm is most commonly used in linguistics to refer to more\nnarrowly deﬁned sub-types of these larger languages (\nCrystal, 2011;\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nMeyerhoﬀ, 2018 ; Wardhaugh and Fuller, 2021 ). Such varieties are\nreferred to by a wide range of technical and colloquial terms,\nincluding not only dialects, but accents, sociolects, topolects, argots,\njargons, registers, genres, styles, slangs, standards, periods, and eras.\nWe believe, however, that it is especially insightful to recognize\nthree basic and distinct types of varieties—or, alternatively, three\nbasic and distinct sources of linguistic variation—which we refer to\nas dialect, register, and period/time (see\nFigure 1).\nDialects are varieties deﬁned by the social backgrounds\nof the people who produce language ( Chambers and Trudgill,\n1998; Meyerhoﬀ, 2018 ; Wardhaugh and Fuller, 2021 ). Dialects\nare often associated with language that originates from speakers\nfrom particular nations, regions, classes, or ethnicities. Empirical\nresearch in sociolinguistics and dialectology has long shown\nthat the language use of people from diﬀerent social groups\n(\nTagliamonte, 2006 , 2011) and identities ( Eckert, 2012 , 2018;\nIlbury, 2020 ) is characterized by systematic patterns of linguistic\nvariation, especially variation in accent and vocabulary. For\nexample, William Labov and his colleagues have analyzed variation\nin the pronunciation of American English in great detail (\nBell\net al., 2016 ; Gordon, 2017 ), from variation across class and other\ndemographic variables in the pronunciation of /r/ post-vocalically\nin New York City (\nLabov, 1986 , 1973) to mapping regional\nvariation in the pronunciation of the entire English vowel system\nacross North America (\nLabov et al., 2006 ). Lexical variation has\nalso notably been the focus of considerable recent research in\ncomputational linguistics, primarily based on large corpora of\nsocial media (\nDonoso and Sánchez, 2017 ; Grieve et al., 2019 ; Huang\net al., 2016; Bamman et al., 2014 ). For example, Blodgett et al. (2016)\nintroduced a method for identifying lexical variation characteristic\nof African American English on Twitter, while also showing how\nNLP tools consistently underperform when applied to this dialect.\nAlternatively, registers are varieties deﬁned by the\ncommunicative contexts in which people, potentially from\nany social background, produce language (\nBiber and Conrad,\n2019; Meyerhoﬀ, 2018 ; Wardhaugh and Fuller, 2021 ). Registers\nare often associated with language produced in speciﬁc modalities,\nmedia, settings, and topics. It is important to stress that registers\nand dialects are independent: dialects are deﬁned by the social\nbackgrounds of language users, whereas registers are deﬁned by\nthe social contexts in which language users, regardless of their\nsocial backgrounds, communicate. Like dialect variation, there has\nbeen a long tradition of empirical research on register variation,\npredominantly in corpus linguistics (\nBiber, 1991 ; Sardinha and\nPinto, 2014 ; Biber and Conrad, 2005 ) and discourse analysis\n(Martin, 2001 ; Matthiessen, 2015 ; Halliday, 1989 ), which has\nshown that language use across contexts is characterized by\nsystematic patterns of linguistic variation, especially grammatical\nvariation (\nBiber and Conrad, 2019 ). For example, Douglas Biber\nand his colleagues have studied register variation in English\n(\nBiber, 1991 ) and other languages ( Biber, 1995 ) in great detail\nthrough the multivariate analysis of grammatical patterns across\na range of corpora. Also, like dialect variation, recent research\nhas focused on the analysis of large corpora of online language,\nespecially social media data (\nBiber and Egbert, 2018 ; Clarke and\nGrieve, 2017 ; Liimatta, 2019 ; Pavalanathan and Eisenstein, 2015 ;\nBerber Sardinha, 2018 ). For example, Clarke (2022) described\nregister variation in a corpus of English Twitter data through a\nmultivariate analysis of grammatical features, identifying four\ngeneral dimensions of stylistic variation.\nFinally, periods are varieties deﬁned by the time span over\nwhich language is produced (\nNevalainen and Raumolin-Brunberg,\n2016). Like dialects and registers, linguistic variation over time\nis also systematic. The study of language change has been one of\nthe oldest endeavors in linguistics ( Bybee, 2015 ; Campbell, 2013 ;\nJoseph et al., 2003 ; Lehmann, 2013 ). This research, which is also\nreferred to as historical linguistics, has focused both on determining\nhow mutually unintelligible varieties are historically related to each\nother and on describing how individual varieties, like English,\nhave changed over time. Notably, recent research in computational\nsociolinguistics has studied how language changes over very short\ntime spans based on large corpora of timestamped social media\ndata, especially to analyze lexical innovation (\nEisenstein et al., 2014 ;\nGrieve et al., 2017 ; Kershaw et al., 2016 ; Stewart and Eisenstein,\n2018) For example, Grieve et al. (2018) showed how new words\nin American English tended to originate from ﬁve hubs of lexical\ninnovation through a spatial analysis of a multi-billion-word\ncorpus geolocated of Twitter data from across the US.\nTaken together, these three extra-linguistic sources of linguistic\nvariation allow for varieties of language to be deﬁned with great\nﬂexibility and precision. This is illustrated in\nFigure 1, which shows\nhow language use can be mapped across these three dimensions\nof linguistic variation, and how a variety of language can deﬁned\nby taking into consideration the social background of people who\nproduce language (dialect), the social context in which language is\nproduced (register), and the range of time over which language is\nproduced (period).\nAs\nFigure 1 illustrates, the relationships between varieties can\nbe highly complex. Varieties can be deﬁned at any scale and\nare generally hierarchically structured, being divisible into smaller\nand smaller sub-varieties. For example, English is a variety, but\nit also contains many smaller sub-varieties. These include many\ndialects, including national varieties of English, like British and\nAmerican English, which are themselves composed of many smaller\nregional dialects like West Country English in the UK or African\nAmerican English in the US (\nChambers and Trudgill, 1998 ). At\nthe most narrowly deﬁned level, the language of an individual can\nbe considered a distinct dialect (i.e., an idiolect). Similarly, English\nalso includes many registers, including spoken and written English,\nwhich are themselves composed of many smaller registers, like\nconversations, telephone conversations, and personal telephone\nconversations (\nBiber and Conrad, 2019 ).\nAlong with exhibiting hierarchical structure, varieties can also\nbe deﬁned based on the overlap of larger varieties, as is also\nillustrated in\nFigure 1. For example, it is common to deﬁne a\nvariety of interest by specifying a dialect, register, and period, like\nContemporary Conversational Canadian French or Scottish Novels\nfrom the Twentieth Century Written by Women . In other words, we\ncan think of a variety as being deﬁned by the speciﬁcation of one or\nmore extra-linguistic factors related to the circumstances in which\nlanguage is produced. In addition, the boundaries between varieties\nare not necessarily sharp or ﬁxed. For example, one regional dialect\nor literary register might transition gradually into the next and this\nmay change over time. For this reasons, sociolinguists often treat\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nFIGURE /one.tnum\nVarieties of language. This ﬁgure illustrates the concept of a va riety of language, showing how the interaction between three dist inct extra-linguistic\nfactors—the social background of people who produce language (dialec t), the social context in which language is produced (register), an d the range\nof time over which language is produce (period)—can be used to spec ify a variety of language. It also illustrates how varieties of la nguage are\nhierarchically organized, composed of smaller and smaller sub- varieties.\ndialect, register, and time as dimensions of linguistic variation as\nopposed to hard categories.\nAlthough we have deﬁned a variety of language as a type\nof language, it is important to specify what exactly a variety\nof language consists of. In other words, when linguists study\na variety of language, what are they actually studying? For\nmany linguists, a variety of language is essentially a population\nof texts (or utterances), as circumscribed by one or more\nextra-linguistic factors, in particular, by a speciﬁc dialect, register,\nand period (see\nCroft, 2000 ). Notably, in this case, a text\nis broadly deﬁned as the language (e.g., utterances, discourse)\nproduced during any communicative event, including language\nproduced in any modality (e.g., speech, writing, signing) (\nHalliday\nand Hasan, 1976 ). For example, not only can an email or\nan essay be considered a text, but so can a conversation\nor a speech. If we adopt what is known as an externalist\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\napproach to linguistics ( Scholz et al., 2024 ; Sampson, 2002 ), where\nlanguage in general is deﬁned as the population of all texts (or\nutterances) that have ever been produced, a variety of language\ncan be deﬁned as a sub-population of those texts that meets\nsome external deﬁnition—i.e., the totality of language produced\nby people from a particular social background (dialect), in a\nparticular social context (register), and over a particular period of\ntime (period).\nFor example, Contemporary Spoken French Canadian\nConversation can be considered a variety of language, as it is a\npopulation of texts (i.e., conversations) produced by individuals\nfrom a speciﬁc social background (i.e., people who live in Canada),\nin a speciﬁc social context (i.e., spoken interactions), during a\nspeciﬁc period (i.e., now). Similarly, a more narrowly deﬁned type\nof language like Scottish Novels from the Twentieth Century Written\nby Women can also be considered a variety of language, as it is a\npopulation of texts (i.e., books) produced by individuals from a\nspeciﬁc social background (i.e., female authors from Scotland), in a\nspeciﬁc social context (i.e., long-form ﬁctional narratives), during\na speciﬁc time span (i.e., 1900-1999).\nThis conception of a variety of language is especially common\nin corpus linguistics, where a corpus is often seen as representing\na variety of language: a corpus consists of a sample of texts\ndrawn from the larger population of texts targeted for analysis\n(\nBiber, 1993 ; McEnery and Wilson, 2001 ; McEnery et al., 2006 ;\nScholz et al., 2024 ). The goal of analyzing the structure of\nlanguage observed in a corpus is therefore to draw generalizations\nabout the variety of language (i.e., the larger population of\ntexts) represented by that corpus. Furthermore, the quality of\na corpus, and by extension the generalizability of any analyses\nbased on that corpus, depends directly on the representativeness\nof this sample, including the accurate identiﬁcation of its primary\nconstituent sub-varieties. This relationship between sociolinguistic\nvariation and corpus design is illustrated in\nFigure 2, which shows\nhow a corpus can be seen as a representative sample of texts\ntaken from a larger population of texts delimited by relevant\nextra-linguistic factors. This ﬁgure also shows how compiling a\nrepresentative corpus in a principled manner generally requires\naccess to an underlying model of that variety of language,\nincluding its internal sub-varieties, so that the corpus can be\nstratiﬁed so as to accurately represent internal variation in that\nvariety. Without such a model, a corpus may misrepresent\nthe patterns of linguistic variation that characterize a variety\nof language.\nFinally, if a variety of language is deﬁned as a population of texts\ndelimited by some set of external criteria, the general expectation\nis that this population of texts will diﬀer from populations of\ntexts delimited by other external criteria in terms of its linguistic\nstructure, including its grammar, phonology, lexis, and discourse\n(\nCrystal and Davy, 1969 ; Jackson, 2007). For example, among other\nfeatures, a regional dialect may be characterized by the speciﬁc\npronunciation of certain vowels (\nLabov et al., 2006 ), whereas\na conversational register might be characterized by its rate of\nuse of certain pronouns (\nBiber and Conrad, 2019 ). Crucially, we\ncan expect that any social group or any social context that is\nrecognized within society will generally become associated with\ndistinct patterns of linguistic variation over time. At the most\nbasic level, this is because certain words associated with concepts\nof particular importance to that group or context will be favored\nor will develop over time, although diﬀerences can generally be\nexpected to emerge across all levels of linguistic analysis, depending\non the communicative constraints and aﬀordances associated with\nthe extra-linguistic factors that deﬁne that variety (see\nGrieve,\n2023). Although the number of possible varieties is therefore\ninnumerable, a general goal of linguistic analysis is to identify\nvarieties that are maximally distinctive, for example, mapping the\ndialect regions of a country (\nWieling and Nerbonne, 2015 ; Grieve,\n2016), deﬁning the sub-types of a given register ( Biber, 1989 ;\nGrieve et al., 2010 ), or identifying the most distinct periods of a\nlanguage ( Gries and Hilpert, 2008 ; Degaetano-Ortlieb and Teich,\n2018).\nTo summarize the discussion presented in this section, we oﬀer\nthe following deﬁnition of a variety of language (see Figure 1):\nA variety of languageis a population of texts deﬁned by\none or more external factors, especially related to the social\nbackground of the people who produce these texts, the social\ncontext in which these texts are produced, and the period of\ntime over which these texts are produced.\nFurthermore, we deﬁne a corpus as a sample of texts drawn\nfrom a speciﬁc variety of language, i.e., from a larger population\nof texts (see\nFigure 2). In this sense, we say that a corpus represents\na given variety of language. It is also important to stress, especially\nin the context of language modeling, that any corpus—any sample\nof texts—inherently represents some variety of language, namely,\nthe smallest common variety that encompasses that sample of\ntexts. However, the representativeness of any corpus depends\ndirectly on the quality and the size of the sample, as well as the\naccurate identiﬁcation of the variety and its sub-varieties from\nwhich texts are sampled. For example, a sample consisting of\na few conversational transcripts and emails collected in Great\nBritain could be taken as representing British English, just not\nvery well.\nOur primary contention in this paper is that, in general,\nlanguage models, which are trained on large corpora of natural\nlanguage, are inherently modeling varieties of language. In\nother words, we conceive of language models as models of\nlanguage use —models of how language is used to create texts\nin the variety of language that the corpus used to train the\nmodel represents. Furthermore, like all linguistic models that\nare based on corpora of natural language, we believe that the\nvalidity and value of a language model depends on the degree\nto which the training corpus accurately represents the variety\nthat is eﬀectively being modeled, which we refer to as the\ntarget variety—even if that variety of language is unknown\nor under-speciﬁed.\nConsequently, our claim is that understanding how to deﬁne\nand represent varieties of language is of direct relevance to\nlanguage modeling: we believe that many problems that arise in\nlanguage modeling result from a mismatch between the variety\nof language that language models are eﬀectively intended to\nrepresent and the variety of language that is actually represented\nby the training corpora. We believe that this perspective is\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nFIGURE /two.tnum\nRepresentative corpus design. This ﬁgure presents a corpus as a r epresentative sample of texts taken from a given variety of langu age (i.e., from a\nlarger population of texts delimited by relevant extra-linguis tic factors). This ﬁgure also illustrates how compiling a corpus th at accurately represents\na target variety requires access to an underlying model of that variety of language, including its internal sub-varieties, s o that the corpus can be\nstratiﬁed so as to capture internal variation in that variety . Naive corpus compilation strategies that rely on convenience sam pling will generally lead\nto less representative samples.\nnot only novel but fundamental to understanding the nature of\nlanguage modeling and how to maximize the societal value of\nLLMs. To support and exemplify this claim, in the remainder\nof this paper, we therefore consider speciﬁc implications of\nthis sociolinguistic conception of language modeling for a\nrange of diﬀerent challenges currently being faced in language\nmodeling primarily through a critical review of the NLP\nliterature from the sociolinguistic perspective introduced in\nthis section.\n/three.tnum Challenges\n/three.tnum./one.tnum Social bias\nNLP systems generally suﬀer from social bias : their real-world\napplication leads to outcomes that unfairly disadvantage or harm\nspeciﬁc social groups (\nShah et al., 2020 ; Blodgett et al., 2020 ;\nDev et al., 2022 ; Navigli et al., 2023 ; Luo et al., 2024 ). Social\nbias can be introduced at various points during the development\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nand deployment of NLP systems ( Hovy and Prabhumoye, 2021 ),\nbut given the unsupervised nature of language modeling, training\ncorpora are a key source of social bias in LLMs (\nBender et al.,\n2021; Ferrara, 2023 ). While bias in NLP systems can harm people\nin various ways ( Blodgett et al., 2020 ), in this section, we primarily\nfocus on two common harmful outcomes of social bias. These\ntwo types of harms are most commonly discussed in terms of\nquality-of-service harms and stereotyping harms (e.g.,\nCrawford,\n2017; Blodgett, 2021 ; Dev et al., 2022 ; Weerts, 2021 ; Leidinger\nand Rogers, 2024 ; Chehbouni et al., 2024 ; Hofmann et al., 2024 ),\nalthough many diﬀerent systems have been proposed for classifying\nbiases and harms in NLP , which deﬁne these terms in somewhat\ndiﬀerent ways, along with many additional and often overlapping\ncategories (\nBlodgett et al., 2020 ). Both of these types of harms are\nespecially relevant to LLMs, and crucially we believe both can be\nbetter understood and addressed in language modeling by adopting\na sociolinguistic perspective (see\nFigure 3).\nFirst, social bias can be characterized by poor system\nperformance for certain social groups that are interacting with\nlanguage models and applications based on language models: token\nprediction will be more or less accurate depending on the social\norigins of the language inputted into the system. For example,\nChatGPT might have diﬃculty correctly understanding prompts\nwritten by people from certain social groups due to their use of\nnon-standard or socially restricted language patterns. This type\nof bias leads to what is known as quality-of-service harms,\nwhere the performance of these systems varies depending on the\nsocial background of the user (\nCrawford, 2017 ; Dev et al., 2022 ;\nChehbouni et al., 2024 ). These types of quality-of-service harms\ncan often be the product of selection bias, as they result from how\ntraining data is selected from across the society whose language\nis being modeled ( Shah et al., 2020 ): in general, if language data\nfrom certain social groups is under-represented in the training data\nfor a language model, we should expect that applications of that\nmodel will process language structures produced by these groups\nless accurately and consequently exhibit poorer performance for\nthese groups (\nBlodgett et al., 2020 ; Lahoti et al., 2023 ).\nNotably, quality-of-service harms, especially those resulting\nfrom selection bias, have been one of the central concerns in\ncomputational sociolinguistics (\nNguyen et al., 2016 ; Eisenstein,\n2017; Grieve et al., 2023 ). Researchers in this emerging ﬁeld\nhave stressed for the past decade that the performance of NLP\nsystems generally varies for people from diﬀerent social groups\nand have called for engagement with description and theory from\nsociolinguistics to help address this basic form of social bias (e.g.,\nHovy and Søgaard, 2015 ; Jórgensen et al., 2015 ; Blodgett and\nO’Connor, 2017; Jurgens et al., 2017 ; Schramowski et al., 2022 ;\nHofmann et al., 2024 ).\nSecond, social bias can be characterized by systems that produce\noutputs that directly harm or discriminate against certain social\ngroups even when they are not directly engaging with these systems\nthemselves. For example, when prompted, ChatGPT might be more\nlikely to produce negative portrayals of certain ethnicities and\ngenders, no matter who is doing the prompting (\nBommasani et al.,\n2021; Lahoti et al., 2023 ). Most notably, this type of bias can lead to\nwhat is known as stereotyping harms(Crawford, 2017 ; Leidinger\nand Rogers, 2024 ; Hofmann et al., 2024 ), as well as related harms\nlike disparagement and dehumanization (Dev et al., 2022 ), where\nnegative viewpoints about speciﬁc social groups are propagated,\nas has been widely discussed in regards to LLMs (\nBender et al.,\n2021). Once again, it is clear that this issue can be traced back,\nat least in part, to the data the language model was trained on.\nIf the training corpus contains relatively frequent expression of\nharmful or inaccurate ideas about certain social groups—as we\ncan safely assume any large, unconstrained sample of internet\nwritings will—language models will inevitably reproduce those\nbiases (\nBender et al., 2021 ; Ferrara, 2023 ; Hofmann et al., 2024 ).\nAs Bender et al. (2021, 613) state, “large, uncurated, Internet-based\ndatasets encode the dominant/hegemonic view, which further\nharms people at the margins.” These types of harms are generally\nthe product of semantic bias, as they result from the meaning\nrelationships between words inferred by the language model based\non patterns of co-occurrence observed in the training corpus\n(\nShah et al., 2020 ).\nFrom a sociolinguistic perspective, we believe social bias in\nlanguage models can be addressed at a basic level by pretraining\non corpora that more accurately represent the target variety.\nImbalance in pretraining data is a recognized as a general source\nof social bias in language modeling (\nYogarajan et al., 2023 ;\nKocijan, 2021 ; Hofmann et al., 2024 ). Although social bias can be\npartially detected or resolved by manipulating the embedding space\n(\nCaliskan et al., 2017 ), the probability table ( Salazar et al., 2019 ),\nor the output of the text generation process ( Bordia and Bowman,\n2019), these approaches have numerous limitations. For example,\nmodels that are de-toxiﬁed following pretraining will tend to\ngenerate less content about the social group that had been the target\nof toxic discourse, inadvertently leading to the erasure of that social\ngroup (\nXu et al., 2021 ). More generally, these types of interventions\nall fall outside the basic language modeling task, focusing on\nsuppressing bias-related parameters (\nLiu et al., 2024 ), rather than\npretraining better underlying language models. To address bias\nin language models at a fundamental level requires intervention\nat the pretraining stage (\nYogarajan et al., 2023 ; Hofmann et al.,\n2024). Our claim is that this type of intervention can be pursed\nin a principled manner by pretraining on corpora that accurately\nrepresent the target variety of language, as identiﬁed through\nsociolinguistic analysis.\nFurthermore, we believe that it is especially important that\nthe training corpus represents the internal structure of the target\nvariety, in the sense that the sub-varieties of that variety of\nlanguage, including most importantly the major dialects of that\nvariety of language, are adequately represented in the training\ncorpus, reﬂecting both the size and distinctiveness of those\ndialects. This challenge is illustrated in\nFigure 3, which shows\nhow a language model for American English could be biased\ntoward one regional dialect or biased against another in various\nways. For example, a corpus intended to represent American\nEnglish, but which is primarily composed of texts collected from\na speciﬁc dialect of American English (e.g., texts written by\nhighly educated, middle-class, white Americans from major coastal\ncities), cannot adequately represent the full diversity of American\nEnglish. Any language model trained on such a corpus should\ntherefore be expected to be biased against social groups that are\nunderrepresented in the training data, such as African American\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nFIGURE /three.tnum\nSociolinguistic bias in language models. This ﬁgure illustrates how training language models on corpora that accurately represent the target variety of\nlanguage including its internal structure, especially its con stituent dialects, can potentially help address social bias, including both quality-of-service\nharms and stereotyping. This is exempliﬁed by comparing two hyp othetical models of American English, which are trained on corpora that\ninaccurately and accurately represent regional dialect vari ation (based on Grieve, /two.tnum/zero.tnum/one.tnum/six.tnum) in this variety of language.\nEnglish from the Southern US, compared to a language model\ntrained on a corpus that more accurately represents variation in\nAmerican English.\nThe link between corpus design and quality-of-service harms\nin LLMs is especially clear: because language varies in systematic\nways, to ensure a language model can accurately process language\nfrom a wide range of social groups, it should be trained on\ncorpora that represent the language used by a wide range of\nsocial groups, i.e., their dialects, as illustrated in\nFigure 3. For\nexample, consider lexical variation in British and American\nEnglish: if a model were only trained on American English,\nit would be much more likely to misinterpret the meaning\nof words that tend to have diﬀerent meanings in British\nEnglish, like boot (for trunk) or underground (for subway).\nConsequently, the quality of service provided by applications\nbased on that model for speakers of British English would\nbe degraded.\nStereotyping and related forms of discrimination generated by\nLLMs have also often been traced back to issues with data collection\nand curation (\nBender et al., 2021 ). A sociolinguistic perspective\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\npotentially provides a principled solution to this problem: in\ngeneral, stereotyping harms could be addressed by training on\ndata that better represents the language produced by a wider\nrange of social groups. One reason that certain social groups are\nnegatively portrayed by LLMs is that they are not allowed to portray\nthemselves, in their own words, in the data used for training.\nBy training on corpora that equitably and deliberately represent\nthe internal varietal structure of the target variety, especially\nthe range of dialects of which it is composed, we believe that\nstereotyping and other forms of semantic bias can be mitigated (see\nFigure 3). In other words, modeling data from a wider range of\ndialects—and, by extension, from a wider range of social groups—\nwould help ensure that a wider range of viewpoints would be\nrepresented by a language model. Stratiﬁed corpora that accurately\nrepresent the sociolinguistic structure of the target variety (i.e., its\nconstituent sub-varieties) could also potentially be used to evaluate\nand probe a model, allowing for social bias to be identiﬁed and\ninterpreted directly.\nThe sociolinguistic approach to language modeling advocated\nfor in this paper therefore provides a simple yet theoretically\ngrounded basis for understanding the general source of social\nbias in language modeling, including for addressing both quality-\nof-service and stereotyping harms, as well as other related types\nof harms. In addition, a sociolinguistic approach oﬀers a clear\npathway for both interpreting and addressing these diﬀerent\nforms of social bias during pretraining through careful corpus\ncompilation informed by scientiﬁc understanding of the nature of\nlinguistic variation within that speciﬁc target variety, based either\non existing or new sociolinguistic research. Crucially, however, such\nsociolinguistic interventions need not necessarily occur during\nthe initial pretraining of the base model, but can be pursued\nthrough the further pretraining of base models, as we discuss in the\nnext section.\n/three.tnum./two.tnum Domain adaptation\nDespite their remarkable ﬂuency and general applicability,\nLLMs generally beneﬁt from some form of domain adaptation\nbefore deployment (\nRadford et al., 2019 ; Gururangan et al.,\n2020). In NLP , domain adaptation is the task of improving the\nperformance of a system that was developed using language\ndata collected in one domain for a diﬀerent and often more\nspeciﬁc domain where the system is to be applied—the real—\nworld context where the system is used, such as texts about\na particular topic or from a particular genre (\nDaumé, 2007 ).\nAlthough there are many approaches for adapting language models,\nincluding for diﬀerent downstream tasks—including reinforcement\nlearning from human feedback (\nOuyang et al., 2022 ), low-\nrank adaptation ( Hu et al., 2021 ), and low-tensor rank weight\nadaptation ( Bershatsky et al., 2024 )—we focus on the process of\nﬁne-tuning a base model by extending unsupervised language\nmodeling on a corpus of texts sampled from a speciﬁc target\ndomain (\nGururangan et al., 2020 ; Hu et al., 2021 ; Hou et al.,\n2022).\nThis approach is often referred to as further pretraining\nbecause it involves extending the basic form of unsupervised\nlanguage modeling used to train the base model to new data from\nthe more speciﬁc target domain (\nGururangan et al., 2020 ). The\ngoal is simply to improve the accuracy of token prediction in the\ntarget domain, while preserving the underlying ﬂuency of the base\nmodel. For example, a base model trained on huge amounts of\nunrestricted online language data could be adapted to the speciﬁc\ndomain of customer service: based on a corpus of customer service\ntranscripts, the parameters of the base model would be adjusted\nto improve the ability of the model to predict word tokens in\ntexts from that domain given the topics of discussion and the\nspeciﬁc types of interactions that characterize that domain (\nChen\net al., 2024 ). In practice, further pretraining has been proven\nto be an eﬀective way of improving the performance of LLMs\nacross a wide range of downstream tasks, including medical text\nprocessing (\nLehman et al., 2023 ; Nazi and Peng, 2024 ), cross-lingual\ntransfer ( Aggarwal et al., 2024 ), and named-entity recognition in\nlow-resources domains ( Mahapatra et al., 2022 ).\nAlthough the importance of domain adaptation has long been\nappreciated in language modeling ( Rudnicky, 1995 ; Chen et al.,\n2024), we believe that this process can be reframed directly and\ninsightfully in sociolinguistic terms, where domain is understood\nas a variety of language. If the goal of the base model is seen\nas accurately predicting word tokens in a broadly deﬁned variety\nof language, like the English language, then the goal of domain\nadaptation can be seen as the process of ﬁne-tuning the base\nmodel to allow it to predict word tokens more accurately in\na more narrowly deﬁned variety of that language—the sub-\nvariety associated with the target domain. Crucially, the adapted\nmodel should be expected to be more accurate because more\nnarrowly deﬁned varieties of language must be characterized\nby less variation than any larger variety that encompasses it.\nThis process can also potentially be carried out in an iterative\nmanner, where a base model is repeatedly adapted on corpora\nrepresenting more narrowly deﬁned varieties of language, as\nshown in\nFigure 4, which illustrates a sociolinguistically informed\napproach to domain adaption, where a model is iteratively ﬁne-\ntuned on corpora representing increasingly narrowly deﬁned\nvarieties of computer-mediated communication.\nA sociolinguistic perspective on domain adaptation therefore\nsees the target domain as a variety of language. This means\nthat the process of domain adaptation can be informed by\nlinguistic analysis that rigorously identiﬁes maximally distinctive\nvarieties of language. This can include both existing research in\nsociolinguistics, dialectology, and related ﬁelds, as well as new\nresearch conducted directly to support model training for speciﬁed\ndomains. For example, if a base model is adapted for a speciﬁc\nregion of the US, empirical research in American dialect geography\n(e.g.,\nGrieve, 2016 ) should be consulted to precisely deﬁne the\nsub-region that is being targeted for adaptation (see Figure 3).\nSimilarly, if a base model is adapted for a speciﬁc type of blog\nwriting, empirical research on register variation in blogs (e.g.,\nGrieve et al., 2010 ) should be consulted to precisely deﬁne the sub-\ntype of blog writing that is being targeted for adaptation. Notably,\nrecent research in NLP has begun to oﬀer empirical evidence for\nthe value of this approach in downstream tasks. For example, in\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nFIGURE /four.tnum\nSociolinguistic adaptation of language models. This ﬁgure illustra tes how an understanding of the sociolinguistic structure of var ieties of languages\ncan inform the adaptation of language models. Language model adaptat ion can be seen as the process of ﬁne-tuning a base model, potentia lly in an\niterative manner, to predict word tokens in a more narrowly deﬁne d variety of language that is subsumed by the larger variety of la nguage\nrepresented by the base model.\nhate speech detection, adapting the underlying language models to\nwhat are eﬀectively target dialects (\nPérez et al., 2024 ) and registers\n(Nirmal, 2024 ) has been found to lead to improvements in the\noverall performance of these systems.\nCrucially, sociolinguistics does not only provide a basis for\nidentifying valid targets for domain adaptation but for mapping\nand modeling the internal structure of these target varieties. This is\nespecially important because target varieties for domain adaptation\nare often well-deﬁned by default. For example, if a ﬁne-tuning\ncorpus is collected by sampling data from a particular social\nmedia platform, a relatively homogeneous variety of language will\nhave naturally been targeted; however, a random sample of texts\nfrom that variety, drawn without taking into account its internal\nstructure, might severely under-represent sub-varieties of interest.\nFor example, a social media corpus may be dominated by certain\nsub-registers (e.g., abusive or promotional posts) that are not the\nintended target of adaptation, while the sub-registers that are the\nintended target of adaptation (e.g., interactive or informational\nposts) may be limited. Similarly, people from certain social groups\nmay be underrepresented in speciﬁc domains, resulting in social\nbias being inadvertently exacerbated by naive domain adaptation.\nIn many cases, the target variety cannot even be accurately deﬁned\nuntil the overall structure of the larger variety in which it is\nsubsumed is understood through careful sociolinguistic analysis.\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nA sociolinguistic perspective also highlights a more general\nproblem with domain adaptation: the success of this process\ndepends on the relationship between the larger variety represented\nby the base model and the smaller target variety toward which\nthe base model is being adapted. Ideally the variety of language\nrepresented by the base model would completely subsume the\ntarget variety: the target variety would be a sub-variety of the\nbase variety, regardless of whether it was represented directly in\nthe base training data. However, the target variety may not be\nadequately represented in the data sampled for training the base\nmodel. For example, the target variety could be associated with a\nsocial group or a social context that is severely underrepresented\nin the base training corpus. In such situations, ﬁne-tuning\nregimes informed by sociolinguistic theory and description would\nlikely be beneﬁcial by providing a basis for identifying these\nvarieties and sampling language directly from these contexts\nand communities.\nUnderstanding the sociolinguistic structure of the larger variety\nof language could also allow models to be adapted to represent\ntarget varieties with missing data. If empirical research in linguistics\nhas found that a target dialect or register for which data is\nlacking falls between multiple dialects or registers for which data\nis available, a model could be adapted for the target variety\nby training on a combination of the available corpora. Overlap\nbetween varieties could also be exploited in a similar way: for\nexample, if data is lacking for a target variety deﬁned in terms\nof a speciﬁc register and a speciﬁc dialect, a model could be\nadapted for the target variety by ﬁne-tuning on a combination\nof corpora that represent that speciﬁc register and that speciﬁc\ndialect. These types of techniques could even be used to create\na model of a variety of language that does not yet exist—\nengineered by training on corpora representing diﬀerent registers\nand dialects.\nFinally, it is important to stress that our proposal is not meant\nto be a simple solution to the problem of domain adaptation\nthat can be applied mechanically or without sociolinguistic\nexpertise. Given the complexity of language variation and\nchange, we do not believe such an approach is possible. A\nsociolinguistic approach to domain adaptation must draw upon\ndetailed empirical research on that speciﬁc variety of language\nand its constituent sub-varieties to direct the compilation of\nrepresentative training corpora. If this empirical research has\nalready been conducted by sociolinguists, it can be consulted\ndirectly, but if no such research exists, new sociolinguistic\nresearch would need to be conducted. Although this research\nwould be grounded in general methods for sociolinguistic\nanalysis, the results would necessarily be speciﬁc to that variety\nof language.\n/three.tnum./three.tnum Alignment\nThe challenges of social bias and domain adaptation can be\nseen as forms of the more general alignment problem —how to\nensure that the behavior of AI systems aligns with the values\nand expectations of society (\nGabriel, 2020 ; Hendrycks et al., 2020 ;\nChristian, 2021 ; Ngo et al., 2022 ; Dung, 2023 ). Misalignment\narises not simply when AI systems fail to achieve their intended\ngoals, but when they pursue these goals, even successfully, in ways\nthat have negative or unforeseen consequences or that are not in\naccordance with societal values, for example, in ways society ﬁnds\nto be inappropriate, unethical, immoral, or dishonest. Alignment\nis therefore the general process of guiding AI systems to behave\nin ways that are consistent with the broader expectations of\nsociety, while discouraging them from behaving in ways that are\ninconsistent with these expectations, especially to avoid unintended\nrisks and harms (\nRussell and Norvig, 2016 ). Crucially, the challenge\nis not only how to guide AI systems but where to guide them\n(Gabriel, 2020).\nAlthough alignment is a long-standing concern in AI ( Wiener,\n1960), attention has grown in recent years due to the growing\ncomplexity and ubiquity of real-world AI systems, especially\nsystems based on language models (\nShen et al., 2023 ; Liu et al.,\n2022, 2023; Wang et al., 2023 ; Wolf et al., 2023 ), which potentially\nallow for misalignment to emerge on many diﬀerent levels ( Gabriel,\n2020; Dung, 2023 ). For example, consider a generative language\nmodel that automatically produces reviews of scientiﬁc literature on\na speciﬁed topic. An obviously misaligned system might produce\nreviews that are clearly wrong—incoherent or incorrect—while a\nless obviously misaligned system might produce ﬂuent reviews,\ncompleting the task successfully in a superﬁcial way, but getting\nfacts wrong, for example, referencing publications that do not exist.\nThis type of a hallucination—the presentation of false information\nas if it is true—is a common form of misalignment in LLMs (\nEvans\net al., 2021 ; Tonmoy et al., 2024 ). A more insidiously misaligned\nsystem, however, might produce perfectly accurate and ﬂuent\nsyntheses that cite relevant literature, but exhibit other problematic\nbehaviors, such as limiting references to certain ideas or researchers\nin certain ﬁelds, thereby eﬀectively suppressing certain viewpoints\n(\nBender et al., 2021 ).\nA basic approach for aligning language models involves\npretraining or further pretraining on corpora that are considered\nto be more aligned with the values and expectations of society\n(\nSolaiman and Dennison, 2021 ). How such corpora can best be\ncompiled, however, is far from clear. As we have argued throughout\nthis paper, sociolinguistic theory provides a basis for compiling\nbetter training corpora. In the general case of alignment, we believe\nlanguage models can be aligned with the values and expectations\nof society, crucially without pre-specifying what exactly these\nvalues and expectations are, by training on corpora that accurately\nrepresent the range of varieties found in that society. As discussed\nin terms of social bias, language models can be trained to better\nalign with the general values of a society, as opposed to the values\nof some particular social group within that society, by balancing\ntraining data originating from diﬀerent dialects. Similarly, as\ndiscussed in terms of domain adaptation, language models can be\ntrained to better align with expectations that they will perform\nadequately across the range of communicative contexts found in\nthat society by balancing training data originating from diﬀerent\nregister. This is because the values and expectations of a society are\ninstantiated in their patterns of language use.\nIn addition to addressing speciﬁc alignment issues related to\nsocial bias and domain adaptation, we believe a sociolinguistic\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\napproach can potentially help us train models that are less\nsusceptible to unethical and dishonest behaviors in general (\nHuang\nC. et al., 2024 ). This is because respecting sociolinguistic diversity\nentails training models on data that represents a greater diversity\nof viewpoints, experiences, and opinions. As LLMs are models of\nvarieties of language, they will be better models, more aligned with\nthe needs, expectations, and values of society, when they account\nfor the full range of sub-varieties, and hence the full range of\nperspectives, found within that society. In general, we therefore\nbelieve that a major source of LLM misalignment comes from\nwhat we call varietal misalignment and that LLM misalignment\ncan therefore be addressed, at least in part, by compiling training\ncorpora to accurately represent the varietal structure of the target\nvariety, as identiﬁed through sociolinguistic analysis.\nFinally, it is important to acknowledge that while a\nsociolinguistic perspective can provide a basis for aligning\nlanguage models for the society that it is intended to serve, this\napproach does not ensure that the resultant language models will\nbe aligned with the ethical and moral aspirations of that society.\nFor example, a generative language model trained on a socially\nbalanced corpus of the English language will still potentially\nproduce texts that express racist viewpoints because a portion of\nEnglish texts expresses racist viewpoints. There might be greater\nequity in the types of stereotypes it spreads, but such behavior\ncan still be seen as a form of misalignment. A sociolinguistic\nperspective, however, also provides a possible solution to this\nproblem—by deliberately weighting the varieties of language\nrepresented in the training corpus. For example, if a particular\nsocial group has been broadly disadvantaged or has a worldview\nthat society wishes to encourage, the portion of the corpus\nrepresenting the relevant varieties of language can be more heavily\nweighted during pretraining or further pretraining. In this way, a\nsociolinguistic perspective can provide a theoretical basis not only\nfor balancing but for controlling the alignment of language models.\n/three.tnum./four.tnum Language change\nThus far, our discussion has focused on how a series of\nchallenges in language modeling related to bias, adaptation, and\nalignment more generally can be addressed, in principle, by\nbuilding training corpora that better represent the dialects and\nregisters of the target variety. Another form of this basic problem\ninvolves ensuring that language models and applications based on\nlanguage models are responsive to language change and cultural\nchange more generally (\nBender et al., 2021 ; Bommasani et al.,\n2021). All varieties of language change over time, often in ways that\nare diﬃcult, if not impossible, to predict ( Lass, 1997 ). If language\nmodels are to maintain their ﬂuency and not become obsolete, they\nmust therefore be continuously updated using training corpora that\nconsist of examples of contemporary language use. In principle, this\nproblem can be resolved by compiling new corpora over time that\nconsistently represent the target variety and its evolving internal\nvarietal structure. The challenge is therefore to understand how the\nsociolinguistic landscape of registers and dialects within that variety\nof language has changed over time, which can only be accomplished\naccurately through detailed and ongoing sociolinguistic analysis.\nA related issue that has caused growing concern in language\nmodeling is that over time more and more real-world language\nwill presumably be produced with the assistance of LLMs, which\nwill make it increasingly diﬃcult to compile contemporary corpora\nof real human language for training new models or updating\nexisting ones (\nShumailov et al., 2023 ). Proposed solutions to these\nproblems of data contamination (Balloccu et al., 2024 ) and task\ncontamination (Li and Flanigan, 2024 ) generally involve ﬁnding\nways to exclude machine-generated language from future training\ndata, including through watermarking systems (\nKirchenbauer et al.,\n2023; Dathathri et al., 2024 ). These types of solutions, however,\nwould seem easy to confound, if only because they do not generally\nallow texts written collaboratively by human and machine to be\nidentiﬁed, which is likely to become increasingly common and\ndiversiﬁed in everyday life.\nDespite real concerns about LLM detection in certain contexts\n(\nBommasani et al., 2021 ; Bian et al., 2023 ), the rising use of\nLLMs to generate language is not diﬃcult to reconcile with\nsociolinguistic theory and practice. Over time, AI systems based\non language models will undoubtedly start to change how we\nuse language. Texts generated with the help of language models\nwill increasingly enter into the real world. At this point, from an\nexternalist perspective (\nScholz et al., 2024 ), these texts will be part\nof language—produced, transmitted, and understood by humans as\nlanguage, often indistinguishable from human-generated language\nin the regular ﬂow of real-world language use. Ultimately, the\ndistinction between human- and machine-generated language can\ntherefore be seen as simply another aspect of register that deﬁnes\nvariation within varieties of language, just like all communicative\ntechnologies that have come before, including the invention of\nwriting and digital communication.\nTaking a sociolinguistic perspective, it is also important to\nacknowledge that the rise of language models is creating new\nvarieties of language, including those characterized by the linguistic\ninteraction between humans and machines, such as dialogues\nwith ChatGPT (\nMavrodieva, 2023 ). These new varieties, which\nwill only continue to diversify over time, will also need to be\naccounted for, like all varieties of language, both by theories of\nsociolinguistic variation and by the evolving language models\ndesigned to represent contemporary language use. If language\nmodels are to be kept up-to-date, machine-generated language\ncannot be excluded, as its production will become a signiﬁcant\ndriver of language change.\n/three.tnum./five.tnum Scale\nIn addition to more speciﬁc insights into the development\nand deployment of language models, we believe a sociolinguistic\nperspective can also help to explain the remarkable success of\nLLMs, which has been attributed both to the development of\nnew deep learning architectures and the use of extremely large\ncorpora of natural language for pretraining (\nKaplan et al., 2020 ;\nBender et al., 2021 ; Bommasani et al., 2021 ). Although there is a\nclear relationship between the scale of the training data and the\nsuccess of these systems (\nSardana and Frankle, 2023 ; Hoﬀmann\net al., 2022 ; Bahri et al., 2024 ), it is unclear why increasing\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nthe amount of training data results in such great increases in\nperformance. Is there a limit to how much performance can be\ngained simply by increasing the scale of the training data? How\ncan more powerful models be developed with less data? These are\nfundamental questions for LLM development (\nBommasani et al.,\n2021), especially because of the signiﬁcant costs and environmental\nimpacts associated with increases in scale ( Bender et al., 2021 ).\nWe believe these are questions that can be uniquely informed by\na sociolinguistic perspective.\nThe obvious reason why increasing the amount of training data\nprovided to a language model improves its performance is that this\nprovides the model access to a wider range of language patterns\n(\nShumailov et al., 2023 ). This is presumably why LLMs beneﬁt from\nbeing pretrained on such large corpora of natural language: the\nsame levels of performance could not be achieved by pretraining\ntwice as long on half the data. Scale is therefore not suﬃcient on\nits own. What matters is not simply the scale of the training data\nbut the diversity of the training data. Although the importance of\nthe diversity of training data has often been stressed in critical\ndiscussions of LLMs (\nBrown et al., 2020 ; Bender et al., 2021 ),\nthe sociolinguistic perspective advocated in this paper provides a\ntheoretical basis for understanding this relationship with greater\nprecision: diversity in the training corpus, in terms of both its\nlinguistic structure and its semantic content, can be seen as directly\nreﬂecting the diversity of the varieties of language represented by\nthat corpus. To maximize the performance of language models and\nthe eﬃciency with which these improvements can be obtained, we\ntherefore believe it is more important to prioritize the amount\nof varietal diversity in the training data over scale. This can be\nachieved by carefully representing a wider range of varieties in\nthe training data, including both dialects and registers, grounded\non empirical sociolinguistic analysis of the target variety and its\ninternal patterns of linguistic variation.\nNotably, empirical evidence for prioritizing diversity in training\ndata in language modeling is building. In addition to research\non debiasing (\nHofmann et al., 2024 ) and domain adaptation\n(Gururangan et al., 2020 ) that has stressed the importance of\nfurther pretraining on diverse data, the superior performance\nof GPT-3 over GPT-J—both of which share the same base\nmodel architecture—provides an especially clear evidence of the\nimportance of diversity over scale (\nWang and Komatsuzaki, 2021 ;\nBrown et al., 2020 ). GPT-3 is generally considered to have beneﬁted\nfrom OpenAI’s carefully curated, even if largely undocumented,\ntraining dataset, whereas GPT-J was pretrained on an open data\nset called the Pile (\nGao et al., 2020 ), which is presumably far less\ncarefully curated. Another source of evidence for the importance\nof diversity in training data is the rapid degradation of model\nperformance and breaks in information integrity that have been\nfound to occur when LLMs are trained on data generated by other\nLLMs, which is inherently far less diverse than language produced\nby humans (\nShumailov et al., 2023 ), as has been demonstrated\nrepeatedly in recent research on LLM detection ( Bevendorﬀ et al.,\n2024; Huang and Grieve, 2024 ).\nA sociolinguistic perspective provides a basis for assessing the\ndiversity of training data and the eﬀect of varying the diversity of\ntraining data along multiple dimensions on the performance of\nthe resultant models in a meaningful way. For example, there is\nconsiderable research on quantifying the overall degree of linguistic\ndiversity and complexity in corpora in both dialectology (\nWieling\nand Nerbonne, 2015 ; Röthlisberger and Szmrecsanyi, 2020 ) and\nregister analysis ( Ehret, 2021; Biber et al., 2021 ).\nThis sociolinguistic perspective also provides an answer to\nquestions about the limits of increasing the scale of training data\n(\nBommasani et al., 2021 ). At what point should increasing the size\nof the training corpus no longer lead to substantial improvements\nin model performance? Our hypothesis is that increasing the\nscale of training data will continue to increase the performance\nof language models so long as it also results in an increase in\nthe sociolinguistic diversity in the training corpus. Crucially, this\nimplies that attempts to empirically assess the limits of scale simply\nby comparing model performance as the amount of training data\nincreases will not be accurate, unless the sociolinguistic diversity of\nthe corpus is also controlled for and measured alongside corpus size\n(\nHoﬀmann et al., 2022 ).\nThis insight is directly relevant to deﬁning scaling laws (Bahri\net al., 2024 ) for language models ( Bommasani et al., 2021 ), which\nare attempts to specify how much data is needed to train a\nlanguage model with a given number of parameters. This issue has\nmost famously been discussed in terms of what is known as the\nChinchilla Law, which states that, for each parameter in an LLM, 20\ntokens of training data is optimal (\nHoﬀmann et al., 2022 ). By this\nstandard, GPT-3, for example, is much too large given the amount\nof training data. From a sociolinguistic perspective, however, any\nsuch calculations seem overly simplistic, as they ignore the diversity\nof the training data. This issue has not been entirely missed in\nlanguage modeling. For example, the Chinchilla Law assumes the\ntraining data is of \"high quality\", although exactly what this means\nand how this can be assessed is a largely unexplored topic (\nSardana\nand Frankle, 2023 ). Measuring the overall degree of sociolinguistic\ndiversity in training data can provide a basis for making these types\nof assessments.\nFinally, a sociolinguistic perspective also oﬀers clear direction\nfor training models using limited amounts of data. This is especially\nimportant issue when the goal is to build language models for\nunder-resourced varieties of language, where obtaining suﬃciently\nlarge corpora for training models is a major challenge (\nBender et al.,\n2021; Ramesh et al., 2023 ). Speciﬁcally, if the value of training\ndata is largely determined by the diversity of training data, great\ncare should be taken to maximize the amount of sociolinguistic\ndiversity, both in terms of dialect and register variation, in the data\nused to train language models for under-resourced varieties.\n/four.tnum Conclusion\nIn this paper, we have proposed that, in general, language\nmodels inherently represent varieties of language. Our claim is\nthat whenever tokens are predicted based on the observation of\nlinguistic patterns in corpora of natural language, the resultant\nlanguage model is necessarily a model of the variety of language\nrepresented by that corpus. By extension, we have argued that the\nperformance, utility, and ethical application of language models, as\nwell as any NLP systems in to which the are embedded, depends\non how well the corpora on which they are trained represent the\nFrontiers in Artiﬁcial Intelligence /one.tnum/three.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nvarieties being modeled, including their internal varietal structure.\nIn other words, we believe that the performance and societal\nvalue of language models is determined not only by the amount\nof language data used for training but by the sociolinguistic\ndiversity and representativeness of these corpora. Crucially, the\narguments we have presented in this paper are intended to be\nrelevant to any form of language modeling—not only current\ntransformer-based models, but simpler traditional models, as well\nas future approaches to language modeling that have not yet\nbeen developed.\nFor these reasons, we believe that drawing on insights from\nsociolinguistics to direct the design, compilation, and curation\nof training corpora will be critical to the future of language\nmodeling, with widespread implications for their development\nand deployment. Speciﬁcally, we have identiﬁed and discussed\nseveral challenges in language modeling—social bias, domain\nadaptation, alignment, language change, and scale—that we believe\na sociolinguistic perspective could help address in a principled\nand uniﬁed manner. Although our goal in this paper has\nbeen to introduce this new perspective on language modeling\nthrough a theoretical discussion grounded in existing research\nin sociolinguistics and NLP , we hope our proposal will act as a\nfoundation and inspiration for future empirical research in this\narea, not only in NLP but in linguistics (\nHuang W. et al., 2024 ;\nHuang and Grieve, 2024 ).\nIt is also important to acknowledge that there already has been\nconsiderable discussion of these types of challenges in language\nmodeling and NLP more generally, with proposals to address these\nissues often emphasizing the need for more careful curation of\ntraining data (\nBender et al., 2021 ; Hovy and Prabhumoye, 2021 )\nand for incorporating social and even sociolinguistic insight into\nthese models (\nHovy, 2018 ; Hovy and Yang, 2021 ; Nguyen et al.,\n2021; Yang et al., 2024 ), especially within the emerging ﬁeld of\ncomputational sociolinguistics ( Nguyen et al., 2016 ; Grieve et al.,\n2023). For example, to address risks related to social bias in LLMs,\nBender et al. (2021, p. 610) recommend that resources must be\ninvested for “curating and carefully documenting datasets rather\nthan ingesting everything on the web, ” while\nYang et al. (2024,\np. 1) argue that issues with LLM performance are related to “a\nlack of awareness of the factors, context, and implications of\nthe social environment in which NLP operates, which we call\nsocial awareness”.\nWhat we believe is lacking in these discussions, however, is the\nidentiﬁcation of a general linguistic framework for solving these\ntypes of problems within the basic paradigm of language modeling,\nespecially one that is theoretically grounded in our scientiﬁc\nunderstanding of language variation and change. Although the lack\nof social diversity in training data has been repeatedly identiﬁed\nas a problem for LLMs, what exactly this means and how exactly\nthis can be measured and addressed in a principled manner\nhas not been articulated. Given this emerging discourse, the\nprimary contribution of this paper is to propose a theoretical and\nempirical foundation for addressing a wide range of challenges in\nlanguage modeling that is based directly on sociolinguistic theory,\nspeciﬁcally the concept of a variety of language —a topic that,\nto the best of our knowledge, has been absent from discussions\nof language modeling up until now, even within computational\nsociolinguistics. This perspective is also notably quite diﬀerent from\ndiscussions of language modeling in linguistics, which have focused\non the status of LLMs as models of language cognition (\nPiantadosi,\n2023; Dentella et al., 2023 ; Marcus et al., 2023 ; Tsvilodub et al.,\n2024). In this article, we have attempted to shift this discussion,\nfocusing instead on understanding language models as models of\nlanguage use , which we believe has far more direct and immediate\nconsequences for the development and deployment of language\nmodels in the real world.\nOur basic claim is therefore that language models can\nbe improved in many ways by training on datasets that\nendeavor to accurately represent the varieties of language\nbeing modeled. We therefore believe that there is a clear\nand urgent need for engagement with sociolinguistic research\nin language model design and evaluation. At the most basic\nlevel, language models are models of how language is used\nfor communication within society. Understanding the structure\nof society, and how this structure is reﬂected in patterns of\nlanguage use, is therefore critical to maximizing the beneﬁts of\nlanguage models for the societies in which they are increasingly\nbeing embedded.\nFinally, in this paper, we have focused exclusively on\nthe basic task of language modeling (i.e., pretraining and\nﬁne tuning via further pretraining). Our goal has been to\nexplain how and why a sociolinguistically informed approach\nto the curation of training data can improve the societal\nvalue of language models in general. Nevertheless, we believe\nsociolinguistic insight, and linguistic insight more generally, can\ninform the broader development and application of modern\nLLMs, including improving approaches to reinforcement learning\n(\nOuyang et al., 2022 ), prompt engineering, and in-context\nlearning ( Chen et al., 2023 ), all of which are ultimately\ngrounded in patterns of language use. Moving forward, we\ntherefore believe that research on language use—not only in\nsociolinguistics, but in corpus linguistics, discourse analysis,\npragmatics, cognitive linguistics, and other ﬁelds of linguistics that\nfocus on understanding how language is used for communication\nin the real world—will increasingly become central to advancing\nthe ﬁeld of language modeling, as well as NLP and AI\nmore generally.\nAuthor contributions\nJGri: Conceptualization, Project administration, Writing –\noriginal draft, Writing – review & editing. SB: Conceptualization,\nWriting – original draft, Writing – review & editing. MF:\nConceptualization, Writing – original draft, Writing – review &\nediting. JGra: Conceptualization, Writing – original draft, Writing\n– review & editing. WH: Conceptualization, Writing – original\ndraft, Writing – review & editing. AJ: Conceptualization, Writing –\noriginal draft, Writing – review & editing. AM: Conceptualization,\nWriting – original draft, Writing – review & editing. MP:\nConceptualization, Writing – original draft, Writing – review &\nediting. DR: Conceptualization, Writing – original draft, Writing –\nreview & editing. BW: Conceptualization, Writing – original draft,\nWriting – review & editing.\nFrontiers in Artiﬁcial Intelligence /one.tnum/four.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. Sara Bartl,\nAlejandro Jawerbaum, and Dana Roemling were supported by\nthe UKRI ESRC Midlands Graduate School Doctoral Training\nPartnership ES/P000711/1. Bodo Winter was supported by the\nUKRI Future Leaders Fellowship MR/T040505/1.\nAcknowledgments\nWe would especially like to thank Dong Nguyen for her\ncomments on this article, as well as Meike Latz for creating\nthe artwork presented in this article. This article also beneﬁted\nfrom discussions with Su Lin Blodgett, Dirk Hovy, Huang He,\nDavid Jurgens, Taylor Jones, and Emily Waibel, as well as\nthree reviewers.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nThe author(s) declared that they were an editorial board\nmember of Frontiers, at the time of submission. This had no impact\non the peer review process and the ﬁnal decision.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Alema n, F.\nL., et al. (2023). Gpt-4 technical report. arXiv [preprint] arXiv:2303.08774.\ndoi: 10.48550/arXiv.2303.08774\nAggarwal, D., Sathe, A., and Sitaram, S. (2024). Exploring pretr aining via active\nforgetting for improving cross lingual transfer for decoder la nguage models. arXiv\n[preprint] arXiv:2410.16168. doi: 10.48550/arXiv.2410.16 168\nAitken, A. J. (1985). Is scots a language? English Today 1, 41–45.\ndoi: 10.1017/S0266078400001292\nBaack, S. (2024). “A critical analysis of the largest source fo r generative ai training\ndata: Common crawl, ” in The 2024 ACM Conference on Fairness, Accountability, and\nTransparency (Rio de Janeiro: Association for Computing Machinery), 2199 –2208.\nBahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2024). Explaining neural\nscaling laws. Proc. Nat. Acad. Sci . 121:e2311878121. doi: 10.1073/pnas.2311878121\nBalloccu, S., Schmidtová, P., Lango, M., and Duvsek, O. (2024) . Leak, cheat,\nrepeat: Data contamination and evaluation malpractices in close d-source LLMs. arXiv\n[preprint] arXiv:2402.03927. doi: 10.48550/arXiv.2402.03 927\nBamman, D., Eisenstein, J., and Schnoebelen, T. (2014). Gende r identity and lexical\nvariation in social media. J. Sociolinguist. 18, 135–160. doi: 10.1111/josl.12080\nBell, A., Sharma, D., and Britain, D. (2016). Labov in socioling uistics: an\nintroduction. J. Sociolinguist. 20, 399–408. doi: 10.1111/josl.12199\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. ( 2021). “On the\ndangers of stochastic parrots: can language models be too big?, ” in Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency (Association for\nComputing Machinery), 610–623.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003) . A neural probabilistic\nlanguage model. J. Mach. Learn. Res . 3, 1137–1155. doi: 10.1162/153244303322533223\nBerber Sardinha, T. (2018). Dimensions of variation across internet registers. Int. J.\nCorpus Linguist. 23, 125–157. doi: 10.1075/ijcl.15026.ber\nBershatsky, D., Cherniuk, D., Daulbaev, T., Mikhalev, A., and Oseledets, I.\n(2024). LoTR: low tensor rank weight adaptation. arXiv [preprint] arXiv:2402.01376.\ndoi: 10.48550/arXiv.2402.01376\nBevendorﬀ, J., Casals, X. B., Chulvi, B., Dementieva, D., Elnag ar, A., Freitag, D.,\net al. (2024). “Overview of pan 2024: Multi-author writing style analysis, multilingual\ntext detoxiﬁcation, oppositional thinking analysis, and gene rative ai authorship\nveriﬁcation, ” in Advances in Information Retrieval , eds. N. Goharian, N. Tonellotto,\nY. He, A. Lipani, G. McDonald, C. Macdonald, et al. (Cham: Springer N ature), 3–10.\nBian, N., Liu, P., Han, X., Lin, H., Lu, Y., He, B., et al. (2023) . A drop of ink makes a\nmillion think: the spread of false information in large language mo dels. arXiv [preprint]\narXiv:2305.04812. doi: 10.48550/arXiv.2305.04812\nBiber, D. (1989). A typology of english texts. Linguistics 27, 3–44.\ndoi: 10.1515/ling.1989.27.1.3\nBiber, D. (1991). Variation Across Speech and Writing . Cambridge: Cambridge\nUniversity Press.\nBiber, D. (1993). Representativeness in corpus design. Literary Linguist. Comp . 8,\n243–257. doi: 10.1093/llc/8.4.243\nBiber, D. (1995). Dimensions of Register Variation: A Cross-Linguistic Compariso n.\nCambridge: Cambridge University Press. doi: 10.1017/CBO9 780511519871\nBiber, D., and Conrad, S. (2005). “Register variation: a cor pus approach, ” in The\nHandbook of Discourse Analysis , eds. D. Tannen, H. E. Hamilton, and D. Schiﬀrin\n(Oxford: John Wiley & Sons), 175–196.\nBiber, D., and Conrad, S. (2019). Register, Genre, and Style . Cambridge: Cambridge\nUniversity Press.\nBiber, D., and Egbert, J. (2018). Register Variation Online . Cambridge: Cambridge\nUniversity Press.\nBiber, D., Gray, B., Staples, S., and Egbert, J. (2021). The Register-Functional\nApproach to Grammatical Complexity: Theoretical Foundation, D escriptive Research\nFindings, Application. London: Routledge.\nBirhane, A., Kasirzadeh, A., Leslie, D., and Wachter, S. (202 3). Science in the age of\nlarge language models. Nat. Rev. Phys . 5, 277–280. doi: 10.1038/s42254-023-00581-4\nBlodgett, S. L. (2021). Sociolinguistically Driven Approaches for Just natural language\nProcessing (PhD thesis). Amherst, MA: University of Massachusetts Amh erst.\nBlodgett, S. L., Barocas, S., and Daumé III, H., and Wallach, H. ( 2020). “Language\n(Technology) is Power: A Critical Survey of “Bias” in NLP , ” in Proceedings of the\n58th Annual Meeting of the Association for Computational Linguisti cs (Association for\nComputational Linguistics).\nBlodgett, S. L., Green, L., and O’Connor, B. (2016). Demographi c dialectal\nvariation in social media: a case study of african-american english. arXiv [preprint]\narXiv:1608.08868. doi: 10.18653/v1/D16-1120\nBlodgett, S. L., and O’Connor, B. (2017). Racial disparity in n atural language\nprocessing: a case study of social media african-american en glish. arXiv [preprint]\narXiv:1707.00061. doi: 10.48550/arXiv.1707.00061\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von\nArx, S., et al. (2021). On the opportunities and risks of foundat ion\nmodels. arXiv [preprint] arXiv:2108.07258. doi: 10.48550/arXiv.2108.\n07258\nBordia, S., and Bowman, S. R. (2019). Identifying and reduci ng gender bias in word-\nlevel language models. arXiv [preprint] arXiv:1904.03035. doi: 10.18653/v1/N19-3002\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhar iwal, P., et al.\n(2020). Language models are few-shot learners. arXiv [preprint] arXiv:2005.14165.\ndoi: 10.48550/arXiv.2005.14165\nBybee, J. (2015). Language Change. Cambridge: Cambridge University Press.\nCabrera, J., Loyola, M. S., Magaña, I., and Rojas, R. (2023). “ Ethical dilemmas,\nmental health, artiﬁcial intelligence, and llm-based chatbots, ” in Bioinformatics and\nBiomedical Engineering, IWBBIO 2023. Lecture Notes in Computer Scien ce, vol 13920 ,\neds. I. Rojas, O. Valenzuela, F. Rojas Ruiz, L. J. Herrera, and F . Ortuño (Cham:\nSpringer). doi: 10.1007/978-3-031-34960-7_22\nFrontiers in Artiﬁcial Intelligence /one.tnum/five.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nCaliskan, A., Bryson, J. J., and Narayanan, A. (2017). Semant ics derived\nautomatically from language corpora contain human-like biases. Science 356, 183–186.\ndoi: 10.1126/science.aal4230\nCampbell, L. (2013). Historical Linguistics. Edinburgh: Edinburgh University Press.\nChambers, J. K., and Trudgill, P. (1998). Dialectology. Cambridge: Cambridge\nUniversity Press.\nChehbouni, K., Roshan, M., Ma, E., Wei, F. A., Taïk, A., Cheun g, J. C., et al. (2024).\nFrom representational harms to quality-of-service harms: a ca se study on llama 2\nsafety safeguards. arXiv [preprint] arXiv:2403.13213. doi: 10.18653/v1/2024.ﬁndin gs-a\ncl.927\nChen, B., Zhang, Z., Langrené, N., and Zhu, S. (2023). Unleash ing the potential\nof prompt engineering in large language models: a comprehensive re view. arXiv\n[preprint] arXiv:2310.14735. doi: 10.48550/arXiv.2310.14 735\nChen, Z., Lin, M., Wang, Z., Zang, M., and Bai, Y. (2024). “Pre paredLLM: eﬀective\npre-pretraining framework for domain-speciﬁc large language mo dels, ” in Big Earth\nData (Abingdon, UK: Taylor & Francis), 1–24.\nChristian, B. (2021). The Alignment Problem: How Can Machines Learn Human\nValues? London: Atlantic Books.\nClarke, I. (2022). A multi-dimensional analysis of english twee ts. Lang. Literat. 31,\n124–149. doi: 10.1177/09639470221090369\nClarke, I., and Grieve, J. (2017). “Dimensions of abusive langu age on Twitter, ”\nin Proceedings of the First Workshop on Abusive Language Online (Vancouver, BC:\nAssociation for Computational Linguistics), 1–10.\nCrawford, K. (2017). “The trouble with bias, ” in Keynote at Neurips (Long Beach,\nCA).\nCroft, W. (2000). Explaining Language Change: An Evolutionary Approach . London:\nPearson Education.\nCruz-Castro, L., Castelblanco, G., and Antonenko, P. (2024). “ LLM-based system\nfor technical writing real-time review in urban constructio n and technology, ” in\nProceedings of 60th Annual Associated Schools of Construction Intern ational Conference\n(Auburn, AL: Associated Schools of Construction), 130–138.\nCrystal, D. (2011). A Dictionary of Linguistics and Phonetics . Hoboken, NJ: John\nWiley & Sons.\nCrystal, D., and Davy, D. (1969). Investigating English Style . Harlow: Longman.\nDathathri, S., See, A., Ghaisas, S., Huang, P.-S., McAdam, R ., Welbl, J., et al.\n(2024). Scalable watermarking for identifying large language m odel outputs. Nature\n634, 818–823. doi: 10.1038/s41586-024-08025-4\nDaumé III, H. (2007). “Frustratingly easy domain adaptation, ” in Proceedings of\nthe 45th Annual Meeting of the Association of Computational Linguis tics (Prague:\nAssociation for Computational Linguistics), 256–263.\nDegaetano-Ortlieb, S., and Teich, E. (2018). “Using relative entropy for detection\nand analysis of periods of diachronic linguistic change, ” in Proceedings of the Second\nJoint SIGHUM Workshop on Computational Linguistics for Cultural Herita ge, Social\nSciences, Humanities and Literature , 22–33.\nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., C handhok, S., et al.\n(2023). Using large language models in psychology. Nat. Rev. Psychol . 2, 688–701.\ndoi: 10.1038/s44159-023-00241-5\nDentella, V., Günther, F., and Leivada, E. (2023). Systematic t esting of three\nlanguage models reveals low language accuracy, absence of respons e stability, and a yes-\nresponse bias. Proc. Nat. Acad. Sci . 120:e2309583120. doi: 10.1073/pnas.2309583120\nDev, S., Sheng, E., Zhao, J., Amstutz, A., Sun, J., Hou, Y., et al. (2022). “On\nmeasures of biases and harms in NLP , ” in Findings of the Association for Computational\nLinguistics: AACL-IJCNLP 2022 (Association for Computational Linguistics), 246–267 .\nDevlin, J., Chang, M. W., Lee, K., and Toutanova, K. (2018). BE RT: Pre-training\nof deep bidirectional transformers for language understand ing. arXiv [preprint]\narXiv:1810.04805. doi: 10.48550/arXiv.1810.04805\nDonoso, G., and Sánchez, D. (2017). Dialectometric analysis of language variation\nin twitter. arXiv [preprint] arXiv:1702.06777. doi: 10.18653/v1/W17-1202\nDung, L. (2023). Current cases of ai misalignment and their im plications for future\nrisks. Synthese 202:138. doi: 10.1007/s11229-023-04367-0\nEckert, P. (2012). Three waves of variation study: the emerg ence of meaning\nin the study of sociolinguistic variation. Annu. Rev. Anthropol . 41, 87–100.\ndoi: 10.1146/annurev-anthro-092611-145828\nEckert, P. (2018). Meaning and Linguistic Variation: The Third Wave in\nSociolinguistics. Cambridge: Cambridge University Press.\nEhret, K. (2021). An information-theoretic view on language complexity and\nregister variation: Compressing naturalistic corpus data. Corpus Linguist. Linguist.\nTheory 17, 383–410. doi: 10.1515/cllt-2018-0033\nEisenstein, J. (2017). “Identifying Regional Dialects in On -Line Social Media,” in\nThe Handbook of Dialectology , eds. C. Boberg, J. Nerbonne, D. Watt (Hoboken, NJ:\nJohn Wiley & Sons), 368–383. doi: 10.1002/9781118827628.ch 21\nEisenstein, J., O’Connor, B., Smith, N. A., and Xing, E. P. (2 014). Diﬀusion of lexical\nchange in social media. PLoS ONE 9:e113114. doi: 10.1371/journal.pone.0113114\nEvans, O., Cotton-Barratt, O., Finnveden, L., Bales, A., Balw it, A., Wills, P., et al.\n(2021). Truthful AI: developing and governing AI that does not lie. arXiv [preprint]\narXiv.2110.06674. doi: 10.48550/arXiv.2110.06674\nFerrara, E. (2023). Should ChatGPT be biased? challenges and ris ks of bias in large\nlanguage models. First Monday 28: 13346. doi: 10.5210/fm.v28i11.13346\nGabriel, I. (2020). Artiﬁcial intelligence, values, and alignme nt. Minds Mach . 30,\n411–437. doi: 10.1007/s11023-020-09539-2\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C ., et al. (2020).\nThe pile: an 800gb dataset of diverse text for language modeling. arXiv [preprint]\narXiv:2101.00027. doi: 10.48550/arXiv.2101.00027\nGordon, M. J. (2017). “William labov, ” in Oxford Research Encyclopedia of\nLinguistics.\nGries, S. T., and Hilpert, M. (2008). The identiﬁcation of stage s in\ndiachronic data: variability-based neighbour clustering. Corpora 3, 59–81.\ndoi: 10.3366/E1749503208000075\nGrieve, J. (2016). Regional Variation in Written American English . Cambridge:\nCambridge University Press.\nGrieve, J. (2023). Situational diversity and linguistic comple xity. Linguist. Vanguard\n9, 73–81. doi: 10.1515/lingvan-2021-0070\nGrieve, J., Biber, D., Friginal, E., and Nekrasova, T. (2010). “Variation among blogs: a\nmulti-dimensional analysis, ” inGenres on the Web , A. Mehler, S. Sharoﬀ, and M. Santini\n(Amsterdam: Springer Netherlands), 303–322.\nGrieve, J., Hovy, D., Jurgens, D., Kendall, T., Nguyen, D., Stan ford, J., et al. (2023).\nComputational sociolinguistics. Front. AI Res. Topic . doi: 10.3389/978-2-8325-1760-4\nGrieve, J., Montgomery, C., Nini, A., Murakami, A., and Guo, D. (2019). Mapping\nlexical dialect variation in british english using twitter. Front. Artif. Intellig . 2:11.\ndoi: 10.3389/frai.2019.00011\nGrieve, J., Nini, A., and Guo, D. (2017). Analyzing lexical emerg ence\nin modern American english online. English Lang. Linguist . 21, 99–127.\ndoi: 10.1017/S1360674316000113\nGrieve, J., Nini, A., and Guo, D. (2018). Mapping lexical innovati on on american\nsocial media. J. Engl. Linguist . 46, 293–319. doi: 10.1177/0075424218793191\nGuo, Y., and Yang, Y. (2024). Econnli: evaluating large language\nmodels on economics reasoning. arXiv [preprint] arXiv:2407.01212.\ndoi: 10.18653/v1/2024.ﬁndings-acl.58\nGururangan, S., Marasovi ´c, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D.,\net al. (2020). Don’t stop pretraining: Adapt language models to do mains and tasks.\narXiv [preprint] arXiv:2004.10964. doi: 10.48550/arXiv.2004.10 964\nHalliday, M. A. (1989). Language, Context, and Text: Aspects of Language in a\nSocial-Semiotic Perspective. Oxford: Oxford University Press.\nHalliday, M. A. K., and Hasan, R. (1976). Cohesion in English . London: Longman.\nHaque, M. A., and Li, S. (2024). Exploring ChatGPT and its impact on society. AI\nEthics 2024, 1–13. doi: 10.1007/s43681-024-00435-4\nHardy, M., Sucholutsky, I., Thompson, B., and Griﬃths, T. (2023 ). “Large\nlanguage models meet cognitive science: LLMs as tools, models, a nd participants, ” in\nProceedings of the 45th Annual Conference of the Cognitive Science Society , eds. M.\nGoldwater, F. K. Anggoro, B. K. Hayes, and D. C. Ong (Cognitive S cience Society),\n14–15.\nHartmann, R. R. K., and Stork, F. C. (1972). Dictionary of Language and Linguistics .\nBasel: Applied Science Publisher.\nHead, C. B., Jasper, P., McConnachie, M., Raftree, L., and Hig don, G. (2023). Large\nlanguage model applications for evaluation: opportunities and eth ical implications.\nNew Direct. Evaluat . 2023, 33–46. doi: 10.1002/ev.20556\nHendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Son g, D., et al.\n(2020). Aligning AI with shared human values. arXiv [preprint] arXiv.2008.02275.\ndoi: 10.48550/arXiv.2008.02275\nHoﬀmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai , T., Rutherford, E.,\net al. (2022). “Training Compute-Optimal Large Language Models , ”Proceedings of the\n36th International Conference on Neural Information Processing System s (New Orleans,\nLA: Neurips).\nHofmann, V., Kalluri, P. R., Jurafsky, D., and King, S. (2024). AI generates\ncovertly racist decisions about people based on their dialect. Nature 633, 147–154.\ndoi: 10.1038/s41586-024-07856-5\nHorton, J. J. (2023). Large Language Models as Simulated Economic Agents: What\nCan we Learn from Homo silicus ? Cambridge, MA: National Bureau of Economic\nResearch. doi: 10.3386/w31122\nHou, Z., Salazar, J., and Polovets, G. (2022). Meta-learning the diﬀerence: preparing\nlarge language models for eﬃcient adaptation. Trans. Assoc. Comput. Linguist . 10,\n1249–1265. doi: 10.1162/tacl_a_00517\nHovy, D. (2018). “The social and the neural network: How to ma ke natural\nlanguage processing about people again, ” in Proceedings of the Second Workshop on\nComputational Modeling of People’s Opinions, Personality, and E motions in Social\nMedia (New Orleans, LA: Association for Computational Linguistics ), 42–49.\nFrontiers in Artiﬁcial Intelligence /one.tnum/six.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nHovy, D., and Prabhumoye, S. (2021). Five sources of bias in n atural language\nprocessing. Lang. Linguist. Compass 15:e12432. doi: 10.1111/lnc3.12432\nHovy, D., and Søgaard, A. (2015). “Tagging performance corre lates with author\nage, ” in Proceedings of the 53rd annual meeting of the Association for Computati onal\nLinguistics and the 7th International Joint Conference on Natural Lan guage Processing\n(volume 2: Short papers) (Beijing: Association for Computational Linguistics), 483 –488.\nHovy, D., and Yang, D. (2021). “The importance of modeling soci al factors of\nlanguage: Theory and practice, ” in Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics : Human Language\nTechnologies (Association for Computational Linguistics), 588–602.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2021). LoRA:\nlow-rank adaptation of large language models. arXiv [preprint] arXiv:2106.09685.\ndoi: 10.48550/arXiv.2106.09685\nHuang, C., Zhao, W., Zheng, R., Lv, H., Dou, S., Li, S., et al. (2 024). Safealigner:\nSafety alignment against jailbreak attacks via response dispar ity guidance. arXiv\n[preprint] arXiv:2406.18118. doi: 10.48550/arXiv.2406.18 118\nHuang, H., Grieve, J., Jiao, L., and Cai, Z. (2024). Geographic s tructure of\nChinese dialects: a computational dialectometric approach. Linguistics. 62, 937–976.\ndoi: 10.1515/ling-2021-0138\nHuang, W., and Grieve, J. (2024). “Authorial language models fo r AI authorship\nveriﬁcation, ” inWorking Notes of CLEF (Grenoble: CEUR).\nHuang, W., Murakami, A., and Grieve, J. (2024). ALMs: Authori al\nlanguage models for authorship attribution. arXiv [preprint] arXiv:2401.12005.\ndoi: 10.48550/arXiv.2401.12005\nHuang, Y., Guo, D., Kasakoﬀ, A., and Grieve, J. (2016). Underst anding us regional\nlinguistic variation with twitter data analysis. Comput. Environ. Urban Syst . 59,\n244–255. doi: 10.1016/j.compenvurbsys.2015.12.003\nHuang, Y., Tang, K., Chen, M., and Wang, B. (2024). A comprehen sive survey on\nevaluating large language model applications in the medical indus try. arXiv [preprint]\narXiv:2404.15777. doi: 10.48550/arXiv.2404.15777\nIlbury, C. (2020). “sassy queens:” Stylistic orthographic vari ation in twitter and the\nenregisterment of aave. J. sociolinguist. 24, 245–264. doi: 10.1111/josl.12366\nJackson, H. (2007). Key Terms in Linguistics . London: Continuum.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.\nS., Casas, D., et al. (2023). Mistral 7B. arXiv [preprint] arXiv:2310.06825.\ndoi: 10.48550/arXiv.2310.06825\nJiao, J., Afroogh, S., Xu, Y., and Phillips, C. (2024). Navigati ng llm ethics:\nadvancements, challenges, and future directions. arXiv [preprint] arXiv:2406.18841.\ndoi: 10.48550/arXiv.2406.18841\nJórgensen, J. N., Karrebáek, M. S., Madsen, L. M., and Móller, J . S. (2015).\n“Polylanguaging in superdiversity, ” in Language and Superdiversity (Milton Park:\nRoutledge), 147–164.\nJoseph, B. D., Janda, R. D., and Vance, B. S. (2003). The Handbook of Historical\nLinguistics. Hoboken, NJ: Wiley Online Library.\nJurafsky, D., and Martin, J. H. (2023). Speech and Language Processing, 3rd Edition .\nAvailable at: https://web.stanford.edu/~jurafsky/slp3/\nJurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). “Incorpo rating dialectal variability\nfor socially equitable language identiﬁcation, ” inProceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 2: Short Pa pers), 51–57.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B ., Child, R., et al.\n(2020). Scaling laws for neural language models. arXiv [preprint] arXiv.2001.08361.\ndoi: 10.48550/arXiv.2001.08361\nKasneci, E., Se ´ssler, K., Küchemann, S., Bannert, M., Dementieva, D.,\nFischer, F., et al. (2023). Chatgpt for good? on opportunities an d challenges\nof large language models for education. Learn. Individ. Diﬀer . 103:102274.\ndoi: 10.1016/j.lindif.2023.102274\nKershaw, D., Rowe, M., and Stacey, P. (2016). “Towards modelli ng language\ninnovation acceptance in online social networks, ” in Proceedings of the Ninth ACM\nInternational Conference on Web Search and Data Mining , 553–562.\nKirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., an d Goldstein, T. (2023).\n“A watermark for large language models, ” in International Conference on Machine\nLearning (Honolulu, HI: PMLR), 17061–17084.\nKocijan, V. (2021). Impact of Pre-Training on Background Knowledge and Societal\nBias (PhD thesis). Oxford: University of Oxford.\nLabov, W. (1973). Sociolinguistic Patterns. Philadelphia: University of Pennsylvania\nPress.\nLabov, W. (1986). “The social stratiﬁcation of (r) in new yor k city department\nstores, ” inDialect and Language Variation (London: Elsevier), 304–329.\nLabov, W., Ash, S., and Boberg, C. (2006). The Atlas of North American English:\nPhonetics, Phonology and Sound Change . Berlin: Mouton de Gruyter.\nLahoti, P., Blumm, N., Ma, X., Kotikalapudi, R., Potluri, S., Tan , Q., et al. (2023).\n“Improving diversity of demographic representation in large lan guage models via\ncollective-critiques and self-voting, ” inProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing (Singapore: Association for Computational\nLinguistics), 10383–10405.\nLass, R. (1997). Historical Linguistics and Language Change, Volume 81 . Cambridge:\nCambridge University Press.\nLehman, E., Hernandez, E., Mahajan, D., Wulﬀ, J., Smith, M. J. , Ziegler, Z., et al.\n(2023). “Do we still need clinical language models?” in Conference on Health, Inference,\nand Learning (New York: PMLR), 578–597.\nLehmann, W. P. (2013). Historical Linguistics: An Introduction . London: Routledge.\nLeidinger, A., and Rogers, R. (2024). How are LLMs mitigatin g stereotyping harms?\nLearning from search engine studies. Proc. AAAI/ACM Conf. AI, Ethics, and Soc . 7,\n839–854. doi: 10.1609/aies.v7i1.31684\nLi, C., and Flanigan, J. (2024). Task contamination: Languag e models\nmay not be few-shot anymore. Proc. AAAI Conf. AI . 38, 18471–18480.\ndoi: 10.1609/aaai.v38i16.29808\nLi, H., Moon, J. T., Purkayastha, S., Celi, L. A., Trivedi, H., and Gichoya, J. W.\n(2023). Ethics of large language models in medicine and medical research. Lancet\nDigital Health 5, e333–e335. doi: 10.1016/S2589-7500(23)00083-3\nLi, M., Chen, M.-B., Tang, B., ShengbinHou, S., Wang, P., Den g, H., et al. (2024).\n“NewsBench: a systematic evaluation framework for assessin g editorial capabilities\nof large language models in chinese journalism, ” in Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), eds.\nKu, L.-W., Martins, A., and Srikumar, V. (Bangkok: Associat ion for Computational\nLinguistics), 9993–10014.\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., et al.\n(2022). Competition-level code generation with alphacode. Science 378, 1092–1097.\ndoi: 10.1126/science.abq1158\nLiimatta, A. (2019). Exploring register variation on reddit: a multi-dimensional\nstudy of language use on a social media website. Register Stud . 1, 269–295.\ndoi: 10.1075/rs.18005.lii\nLiu, R., Yang, R., Jia, C., Zhang, G., Zhou, D., Dai, A. M., et al. (2023). Training\nsocially aligned language models in simulated human society. arXiv [preprint]\narXiv.2305.16960. doi: 10.48550/arXiv.2305.16960\nLiu, R., Zhang, G., Feng, X., and Vosoughi, S. (2022). Aligning generative language\nmodels with human values. Find. Assoc. Comp . Linguist.: NAACL 2022, 241–252.\ndoi: 10.18653/v1/2022.ﬁndings-naacl.18\nLiu, Y., Liu, Y., Chen, X., Chen, P.-Y., Zan, D., Kan, M.-Y., e t al. (2024). The devil\nis in the neurons: Interpreting and mitigating social biases in pre-trained language\nmodels. arXiv [preprint] arXiv:2406.10130. doi: 10.48550/arXiv.2406.10 130\nLund, B. D., Wang, T., Mannuru, N. R., Nie, B., Shimray, S., an d Wang, Z. (2023).\nChatGPT and a new academic reality: artiﬁcial intelligence-wri tten research papers and\nthe ethics of the large language models in scholarly publishing. J. Assoc. Inform. Sci.\nTechnol. 74, 570–581. doi: 10.1002/asi.24750\nLuo, H., Huang, H., Deng, Z., Liu, X., Chen, R., and Liu, Z. (20 24). Bigbench:\na uniﬁed benchmark for social bias in text-to-image generat ive models based\non multi-modal LLM. arXiv [preprint] arXiv:2407.15240. doi: 10.48550/arXiv.2407.\n15240\nMahapatra, A., Nangi, S. R., and Garimella, A.. (2022). “Entity extraction in low\nresource domains with selective pre-training of large language models, ” inProceedings\nof the 2022 Conference on Empirical Methods in Natural Language Process ing, eds. Y.\nGoldberg, Z. Kozareva, and Y. Zhang (Abu Dhabi, United Arab Emi rates: Association\nfor Computational Linguistics), 942–951.\nMarcus, G., Leivada, E., and Murphy, E. (2023). A sentence is wo rth a thousand\npictures: Can large language models understand human language? arXiv [preprint]\narXiv.2308.00109. doi: 10.48550/arXiv.2308.00109\nMartin, J. R. (2001). “Language, register and genre, ” in Analysing English in a Global\nContext: A Reader (London: Routledge), 149–166.\nMatthews, P. H. (1997). Oxford Concise Dictionary of Linguistics . Oxford: University\nof Oxford.\nMatthiessen, C. M. (2015). Register in the round: Registeri al cartography. Funct.\nLinguist. 2, 1–48. doi: 10.1186/s40554-015-0015-8\nMavrodieva, I. (2023). Linguistic and rhetorical features of dialogue on\nrhetorical topics between a human and chatbot gpt. Rhetoric Commun . 56, 22–45.\ndoi: 10.55206/CIKP7841\nMcEnery, T., and Wilson, A. (2001). Corpus Linguistics . Edinburgh: Edinburgh\nUniversity Press.\nMcEnery, T., Xiao, R., and Tono, Y. (2006). Corpus-based Language Studies: An\nAdvanced Resource Book . London: Routledge.\nMeyerhoﬀ, M. (2018). Introducing Sociolinguistics. London: Routledge.\nMichaelov, J. A., Bardolph, M. D., Van Petten, C. K., Bergen, B. K ., and Coulson,\nS. (2024). Strong prediction: language model surprisal explains multiple n400 eﬀects.\nNeurobiol. Lang. 2024, 1–29. doi: 10.1162/nol_a_00105\nNavigli, R., Conia, S., and Ross, B. (2023). Biases in large lang uage models: origins,\ninventory, and discussion. J. Data Inform. Quality 15, 1–10. doi: 10.1145/3597307\nFrontiers in Artiﬁcial Intelligence /one.tnum/seven.tnum frontiersin.org\nGrieve et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/seven.tnum/two.tnum/four.tnum/one.tnum/one.tnum\nNazi, Z. A., and Peng, W. (2024). Large language models in health care and medical\ndomain: A review. Informatics 11, 57. doi: 10.3390/informatics11030057\nNevalainen, T., and Raumolin-Brunberg, H. (2016). Historical Sociolinguistics:\nLanguage Change in Tudor and Stuart England . London: Routledge.\nNgo, R., Chan, L., and Mindermann, S. (2022). The alignment pro blem\nfrom a deep learning perspective. arXiv [preprint] arXiv.2209.00626.\ndoi: 10.48550/arXiv.2209.00626\nNguyen, D., Doäÿruöz, A. S., Rosé, C. P., and De Jong, F. (2016 ). Computational\nsociolinguistics: a survey. Comput. Linguist. 42, 537–593. doi: 10.1162/COLI_a_00258\nNguyen, D., Rosseel, L., and Grieve, J. (2021). “On learning and representing social\nmeaning in nlp: a sociolinguistic perspective, ” in Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Lin guistics: Human\nLanguage Technologies (Association for Computational Linguistics), 603–612.\nNirmal, A. (2024). Interpretable Hate Speech Detection via Large Language Model-\nExtracted Rationales. Tempe, AZ: Arizona State University.\nOpen AI. (2022). Chatgpt.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., et al.\n(2022). Training language models to follow instructions with hu man feedback. arXiv\n[preprint] arXiv:2203.02155 [cs]. doi: 10.48550/arXiv.220 3.02155\nPavalanathan, U., and Eisenstein, J. (2015). Audience-modu lated variation in online\nsocial media. Am. Speech 90, 187–213. doi: 10.1215/00031283-3130324\nPavlik, J. V. (2023). Collaborating with chatgpt: Considering t he implications of\ngenerative artiﬁcial intelligence for journalism and media ed ucation. Journalism & mass\ncommunication educator 78:84–93. doi: 10.1177/10776958221149577\nPérez, J. M., Miguel, P., and Cotik, V. (2024). Exploring large lan guage models\nfor hate speech detection in rioplatense Spanish. arXiv [preprint] arXiv:2410.12174.\ndoi: 10.48550/arXiv.2410.12174\nPiantadosi, S. (2023). “Modern language models refute chomsk y’s approach to\nlanguage, ” inTechnical Report, Lingbuzz Preprint . Troms: University of Troms.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutsk ever, I. (2019).\nLanguage models are unsupervised multitask learners. OpenAI Blog (OpenAI), 1:9.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Mate na, M., et al. (2020).\nExploring the limits of transfer learning with a uniﬁed text-to- text transformer. J. Mach.\nLearn. Res. 21:1–67.\nRamesh, K., Sitaram, S., and Choudhury, M. (2023). Fairness in language models\nbeyond english: Gaps and challenges. Find. Assoc. Comp. Linguist .: EACL 2023,\n2106–2119. doi: 10.18653/v1/2023.ﬁndings-eacl.157\nRay, P. P. (2023). Chatgpt: a comprehensive review on backgrou nd, applications,\nkey challenges, bias, ethics, limitations and future scope. Intern. Things Cyber-Phys Syst .\n3, 121–154. doi: 10.1016/j.iotcps.2023.04.003\nRöthlisberger, M., and Szmrecsanyi, B. (2020). “Dialect typolo gy: recent advances, ”\nin Handbook of the Changing World Language Map (New York, NY: Springer),\n131–156.\nRudnicky, A. (1995). “Language Modeling with Limited Domain Data, ” in Proc.\nARPA Spoken Language Systems Technology Workshop (Austin, TX; San Francisco, CA:\nMorgan Kaufman Publishers), 66–69.\nRussell, S. J., and Norvig, P. (2016). Artiﬁcial Intelligence: A Modern Approach .\nLondon: Pearson.\nSalazar, J., Liang, D., Nguyen, T. Q., and Kirchhoﬀ, K. (2019) . Masked language\nmodel scoring. arXiv [preprint] arXiv:1910.14659. doi: 10.18653/v1/2020.acl-ma in.240\nSampson, G. (2002). Empirical Linguistics. London: A&C Black.\nSardana, N., and Frankle, J. (2023). Beyond chinchilla-optimal: accounting\nfor inference in language model scaling laws. arXiv [preprint] arXiv.2401.00448.\ndoi: 10.48550/arXiv.2401.00448\nSardinha, T. B., and Pinto, M. V. (2014). Multi-Dimensional Analysis, 25 Years\nOn: A Tribute to Douglas Biber, volume 60 . Amsterdam: John Benjamins Publishing\nCompany.\nScholz, B. C., Pelletier, F. J., Pullum, G. K., and Nefdt, R. (2024). “Philosophy\nof linguistics. In of Philosophy (Spring Edition), ” in The Stanford Encyclopedia of\nPhilosophy (Spring Edition), eds. N. Edward, T. S. E. Zalta, and U. Nodelman (Stanford,\nCA: Stanford University).\nSchramowski, P., Turan, C., Andersen, N., Rothkopf, C. A., an d Kersting, K. (2022).\nLarge pre-trained language models contain human-like biases of what is right and\nwrong to do. Nat. Mach. Intellig. 4, 258-268. doi: 10.1038/s42256-022-00458-8\nShah, D. S., Schwartz, H. A., and Hovy, D. (2020). “Predictiv e biases in\nnatural language processing models, ” in Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics (Stroudsburg, PA: Association for\nComputational Linguistics), 5248–5264.\nShen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., et al. (2 023).\nLarge language model alignment: A survey. arXiv [preprint] arXiv.2309.15025.\ndoi: 10.48550/arXiv.2309.15025\nShumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., an d Anderson, R.\n(2023). The curse of recursion: Training on generated data m akes models forget. arXiv\n[preprint] arXiv.2305.17493. doi: 10.48550/arXiv.2305.17 493\nSolaiman, I., and Dennison, C. (2021). Process for adapting lan guage models\nto society (palms) with values-targeted datasets. Adv. Neural Inf. Process. Syst . 34,\n5861–5873. doi: 10.48550/arXiv.2106.10328\nStefan, R., Carutasu, G., and Mocan, M. (2023). “Ethical cons iderations in the\nimplementation and usage of large language models, ” in The 17th International\nConference Interdisciplinarity in Engineering , eds L. Moldovan and A. Gligor (Cham:\nSpringer), 131–144.\nStewart, I., and Eisenstein, J. (2018). “Making fetch? happen : the inﬂuence of social\nand linguistic context on the success of lexical innovations, ” in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing (EMNL P) (Brussels:\nAssociation for Computational Linguistics), 4360–4370.\nTagliamonte, S. A. (2006). Analysing Sociolinguistic Variation . Cambridge:\nCambridge University Press.\nTagliamonte, S. A. (2011). Variationist Sociolinguistics: Change, Observation,\nInterpretation. Hoboken: John Wiley & Sons.\nThirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierr ez, L., Tan, T. F., and\nTing, D. S. W. (2023). Large language models in medicine. Nat. Med. 29, 1930–1940.\ndoi: 10.1038/s41591-023-02448-8\nTonmoy, S. M., Zaman, S. M., Jain, V., Rani, A., Rawte, V., Cha dha, A., et al. (2024).\nA comprehensive survey of hallucination mitigation techniques in large language\nmodels. arXiv [preprint] arXiv.2401.01313.doi: 10.48550/arXiv.2401.01 313\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., et al.\n(2023). Llama 2: Open foundation and ﬁne-tuned chat models. arXiv [preprint]\narXiv.2307.09288. doi: 10.48550/arXiv.2307.09288\nTsvilodub, P., Carcassi, F., and Franke, M. (2024). Towards n euro-symbolic\nmodels of language cognition: Llms as proposers and evaluators. arXiv [preprint]\narXiv.2401.09334. doi: 10.48550/arXiv.2401.09334\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inf. Process. Syst . 2017:30.\nWang, B., and Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter\nAutoregressive Language Model . Long Beach, CA. Available at: https://github.com/\nkingoﬂolz/mesh-transformer-jax (accessed December 10, 2024).\nWang, T., Zhou, N., and Chen, Z. (2024). Enhancing computer pro gramming\neducation with LLMs: a study on eﬀective prompt engineering fo r Python\ncode generation. arXiv [preprint] arXiv:2407.05437. doi: 10.48550/arXiv.2407.\n05437\nWang, Y., Zhong, W., Li, L., Mi, F., Zeng, X., Huang, W., et al. ( 2023). Aligning\nlarge language models with human: A survey. arXiv [preprint] arXiv.2307.12966.\ndoi: 10.48550/arXiv.2307.12966\nWardhaugh, R., and Fuller, J. M. (2021). An Introduction to Sociolinguistics .\nHoboken: John Wiley & Sons.\nWeerts, H. J. (2021). An introduction to algorithmic fairnes s. arXiv [preprint]\narXiv.2105.05595.doi: 10.48550/arXiv.2105.05595\nWieling, M., and Nerbonne, J. (2015). Advances in dialectomet ry. Annual Rev.\nLinguist. 1, 243–264. doi: 10.1146/annurev-linguist-030514-12493 0\nWiener, N. (1960). Some moral and technical consequences of a utomation: as\nmachines learn they may develop unforeseen strategies at rate s that baﬄe their\nprogrammers. Science 131, 1355–1358. doi: 10.1126/science.131.3410.1355\nWolf, Y., Wies, N., Levine, Y., and Shashua, A. (2023). Fundam ental limitations\nof alignment in large language models. arXiv [preprint] arXiv.2304.11082.\ndoi: 10.48550/arXiv.2304.11082\nXu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein , D. (2021).\nDetoxifying language models risks marginalizing minority voi ces. arXiv [preprint]\narXiv:2104.06390. doi: 10.48550/arXiv.2104.06390\nYang, D., Hovy, D., Jurgens, D., and Plank, B. (2024). The call f or\nsocially aware language technologies. arXiv [preprint] arXiv.2405.02411.\ndoi: 10.48550/arXiv.2405.02411\nYigci, D., Eryilmaz, M., Yetisen, A. K., Tasoglu, S., and Ozcan , A. (2024). Large\nlanguage model-based chatbots in higher education. Adv. Intellig. Syst . 2024:2400429.\ndoi: 10.1002/aisy.202400429\nYogarajan, V., Dobbie, G., Keegan, T. T., and Neuwirth, R. J. ( 2023). Tackling bias\nin pre-trained language models: Current trends and under-repre sented societies. arXiv\n[preprint] arXiv:2312.01509.doi: 10.48550/arXiv.2312.01509\nFrontiers in Artiﬁcial Intelligence /one.tnum/eight.tnum frontiersin.org",
  "topic": "Sociolinguistics",
  "concepts": [
    {
      "name": "Sociolinguistics",
      "score": 0.8090977668762207
    },
    {
      "name": "Computer science",
      "score": 0.6947896480560303
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6799678206443787
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5369322896003723
    },
    {
      "name": "Linguistics",
      "score": 0.5141263604164124
    },
    {
      "name": "Modeling language",
      "score": 0.5002110004425049
    },
    {
      "name": "Field (mathematics)",
      "score": 0.47157591581344604
    },
    {
      "name": "Language change",
      "score": 0.46526339650154114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29605308175086975
    },
    {
      "name": "Programming language",
      "score": 0.0878976583480835
    },
    {
      "name": "Software",
      "score": 0.06604814529418945
    },
    {
      "name": "Mathematics",
      "score": 0.059188663959503174
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}