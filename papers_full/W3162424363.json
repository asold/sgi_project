{
  "title": "Vision Transformer for Fast and Efficient Scene Text Recognition",
  "url": "https://openalex.org/W3162424363",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1213383804",
      "name": "Rowel Atienza",
      "affiliations": [
        "University of the Philippines System"
      ]
    },
    {
      "id": "https://openalex.org/A1213383804",
      "name": "Rowel Atienza",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3004846386",
    "https://openalex.org/W2128409098",
    "https://openalex.org/W2809273748",
    "https://openalex.org/W6600018615",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2343052201",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W2008806374",
    "https://openalex.org/W2294053032",
    "https://openalex.org/W3034414401",
    "https://openalex.org/W2965066169",
    "https://openalex.org/W3013224334",
    "https://openalex.org/W2593572697",
    "https://openalex.org/W2099247484",
    "https://openalex.org/W1981283549",
    "https://openalex.org/W2061802763",
    "https://openalex.org/W2146835493",
    "https://openalex.org/W3034447740",
    "https://openalex.org/W1971822075",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3003642782",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W2963517393",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1988461287",
    "https://openalex.org/W3035449864",
    "https://openalex.org/W2963712589",
    "https://openalex.org/W1998042868",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W6908809",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2752225195",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W1491389626",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3021481629",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962835968"
  ],
  "abstract": null,
  "full_text": "Vision Transformer for Fast and Eﬃcient Scene\nText Recognition\nRowel Atienza[0000−0002−8830−2534]\nElectrical and Electronics Engineering Institute\nUniversity of the Philippines\nrowel@eee.upd.edu.ph\nAbstract. Scene text recognition (STR) enables computers to read text\nin natural scenes such as object labels, road signs and instructions. STR\nhelps machines perform informed decisions such as what object to pick,\nwhich direction to go, and what is the next step of action. In the body of\nwork on STR, the focus has always been on recognition accuracy. There\nis little emphasis placed on speed and computational eﬃciency which\nare equally important especially for energy-constrained mobile machines.\nIn this paper we propose ViTSTR, an STR with a simple single stage\nmodel architecture built on a compute and parameter eﬃcient vision\ntransformer (ViT). On a comparable strong baseline method such as\nTRBA with accuracy of 84.3%, our small ViTSTR achieves a competi-\ntive accuracy of 82.6% (84.2% with data augmentation) at 2 .4× speed\nup, using only 43.4% of the number of parameters and 42.2% FLOPS.\nThe tiny version of ViTSTR achieves 80.3% accuracy (82.1% with data\naugmentation), at 2 .5× the speed, requiring only 10.9% of the number\nof parameters and 11.9% FLOPS. With data augmentation, our base\nViTSTR outperforms TRBA at 85.2% accuracy (83.7% without aug-\nmentation) at 2 .3× the speed but requires 73.2% more parameters and\n61.5% more FLOPS. In terms of trade-oﬀs, nearly all ViTSTR conﬁg-\nurations are at or near the frontiers to maximize accuracy, speed and\ncomputational eﬃciency all at the same time.\nKeywords: Scene text recognition · Transformer · Data augmentation\n1 Introduction\nSTR plays a vital role for machines to understand the human environment.\nWe invented text to convey information through labels, signs, instructions and\nannouncements. Therefore, for a computer to take advantage of this visual cue,\nit must also understand text in natural scenes. For instance, a ”Push” signage\non a door tells a robot to push it to open. In the kitchen, a label with ”Sugar”\nmeans that the container has sugar in it. A wearable system that can read ”50”\nor ”FIFTY” on a paper bill can greatly enhance the lives of visually impaired\npeople.\narXiv:2105.08582v1  [cs.CV]  18 May 2021\n2 Rowel Atienza\nFig. 1.Trade-oﬀs between accuracy vs number of parameters, speed and computational\nload (FLOPS). +Aug uses data augmentation. Almost all versions of ViTSTR are at\nor near the frontiers to maximize the performance on all metrics. The slope of the\nline is the accuracy gain as the number of parameters, speed or FLOPS increases. The\nsteeper the slope, the better. Teal line includes ViTSTR with data augmentation.\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 3\nSTR is related but diﬀerent from the more developed ﬁeld of Optical Char-\nacter Recognition (OCR). In OCR, symbols on a printed front facing document\nare detected and recognized. In a way, OCR operates in a more structured set-\nting. Meanwhile, the objective of STR is to recognize symbols in varied uncon-\nstrained settings such as walls, signboards, product labels, road signs, markers,\netc. Therefore, the inputs have many degrees of variation in font style, orienta-\ntion, shape, size, color, texture and illumination. The inputs are also subject to\ncamera sensor orientation, location and imperfections causing image blur, pix-\nelation, noise, and geometric and radial distortions. Weather disturbances such\nas glare, shadow, rain, snow and frost can also greatly aﬀect the performance of\nSTR.\nIn the body of work on STR, the emphasis has always been on accuracy\nwith little attention paid to speed and computing requirements. In this work,\nwe attempt to put balance on accuracy, speed and eﬃciency. Accuracy refers to\nthe correctness of recognized text. Speed is measured by how many text images\nare processed per unit time. Eﬃciency can be approximated by the number of\nparameters and computations (eg FLOPS) required to process one image. The\nnumber of parameters reﬂects the memory requirements while FLOPS estimates\nthe number of instructions needed to complete a task. An ideal STR is accurate\nand fast while requiring only little computing resources.\nIn the quest to beat the SOTA, most models are zeroing on accuracy with\ninadequate discussion on the trade oﬀ. In order to instill balance on the im-\nportance of accuracy, speed and eﬃciency, we propose to take advantage of the\nsimplicity and eﬃciency of vision transformers (ViT) [7] such as Data-eﬃcient\nimage Transformer (DeiT) [34]. ViT demonstrated that SOTA results in Ima-\ngeNet [28] recognition can be achieved using a transformer [35] encoder only.\nViT inherited all the properties of a transformer including its speed and com-\nputational eﬃciency. Using the model weights of DeiT which is simply a ViT\ntrained by knowledge distillation [13] for better performance, we built an STR\nthat can be trained end-to-end. This resulted to a simple single stage model\narchitecture that is able to maximize accuracy, speed and computational perfor-\nmance. The tiny version of our ViTSTR achieves 80.3% accuracy (82.1% with\ndata augmentation), is fast at 9.3 msec/image, with a small footprint of 5.4M\nparameters and requires much less computations at 1.3 Giga FLOPS. The small\nversion of ViTSTR achieves a higher accuracy of 82.6% (84.2% with data aug-\nmentation), is also fast at 9.5 msec/image while requiring 21.5M parameters and\n4.6 Giga FLOPS. With data augmentation, the base version of ViTSTR achieves\n85.2% accuracy (83.7% no augmentation) at 9.8 msec/image but requires 85.8M\nparameters and 17.6 Giga FLOPS. We adopted the reference tiny, small and\nbase to indicate which ViT/DeiT transformer encoder was used in ViTSTR. As\nshown in Figure 1, almost all versions of our proposed ViTSTR are at or near\nthe frontiers of accuracy vs speed, memory, and computational load indicating\noptimal trade-oﬀs. To encourage reproducibility, the code of ViTSTR is available\nat https://github.com/roatienza/deep-text-recognition-benchmark.\n4 Rowel Atienza\nCurved Uncommon Font Style Blur and Rotation Noise\nPerspective Shadow Occluded and Curved LowRes & Pixelation\nFig. 2.Diﬀerent variations of text encountered in natural scenes\n2 Related Work\nFor machines, reading text in the human environment is a challenging task due\nto diﬀerent possible appearances of symbols. Figure 2 shows examples of text\nin the wild aﬀected by curvature, font style, blur, rotation, noise, geometry,\nillumination, occlusion and resolution. There are many other factors that could\naﬀect text images such as weather condition, camera sensor imperfection, motion,\nlighting, etc.\nReading text in natural scenes generally requires two stages: 1) text detection\nand 2) text recognition. Detection determines the bounding box of the region\nwhere text can be found. Once the region is known, text recognition reads the\nsymbols in the image. Ideally, a method is able to do both at the same time.\nHowever, the performance of SOTA end-to-end text reading models is still far\nfrom modern-day OCR systems and remains an open problem [5]. In this work,\nour focus is on text recognition of 96 Latin characters (i.e. 0-9, a-Z, etc.).\nSTR identiﬁes each character of a text in an image in the correct sequence.\nUnlike object recognition where usually there is only one category of object,\nthere may be zero or more characters for a given text image. Thus, STR mod-\nels are more complex. Similar to many vision problems, early methods [24,38]\nused hand-crafted features resulting to poor performance. Deep learning has\ndramatically advanced the ﬁeld of STR. In 2019, Baek et al. [1] presented a\nframework that models the design patterns of modern STR. Figure 3 shows the\nfour stages or modules of STR. Broadly speaking, even recently proposed meth-\nods such as transformer-based models, No-Recurrence sequence-to-sequence Text\nRecognizer (NRTR) [29] and Self-Attention Text Recognition Network (SATRN)\n[18] can ﬁt into Rectiﬁcation-Feature Extraction (Backbone)-Sequence\nModelling-Prediction framework.\nThe Rectiﬁcation stage removes the distortion from the word image so that\nthe text is horizontal or normalized. This makes it easier for Feature Extraction\n(Backbone) module to determine invariant features. Thin-Plate-Spline (TPS) [3]\nmodels the distortion by ﬁnding and correcting ﬁducial points. RARE (Robust-\ntext recognizer with Automatic REctiﬁcation) [31], STAR-Net (SpaTial Atten-\ntion Residue Network) [21], and TRBA (TPS-ResNet-BiLSTM-Attention) [1]\nuse TPS. ESIR (End-to-end trainable Scene text Recognition) [41] employs an\niterative rectiﬁcation network that signiﬁcantly boosts the performance of text\nrecognition models. In some cases, no rectiﬁcation is employed such as in CRNN\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 5\nRectify\n Backbone\nSequence Predict FLANDERS\nTypical Framework\nBackbone Encoder\nDecoder FLANDERS\nTransformer Encoder-Decoder with Backbone\nEncoder FLANDERS\nTransformer Encoder (ViTSTR)\nFig. 3.STR design patterns. Our proposed model, ViTSTR, has the simplest archi-\ntecture with just one stage.\n(Convolutional Recurrent Neural Network) [30], R2AM (Recursive Recurrent\nneural networks with Attention Modeling) [17], GCRNN (Gated Recurrent Con-\nvolution Neural Network) [36] and Rosetta [4].\nThe role of Feature Extraction (Backbone) stage is to automatically deter-\nmine the invariant features of each character symbol. STR uses the same feature\nextractors in object recognition tasks such as VGG [32], ResNet [11], and a\nvariant of CNN called RCNN [17]. Rosetta, STAR-Net and TRBA use ResNet.\nRARE and CRNN extract features using VGG. R2AM and GCRNN build on\nRCNN. Transformer-based models NRTR and SATRN use customized CNN\nblocks to extract features for transformer encoder-decoder text recognition.\nSince STR is a multi-class sequence prediction, there is a need to remem-\nber long-term dependency. The role of Sequence modelling such as BiLSTM\nis to make a consistent context between the current character features and the\npast/future characters features. CRNN, GRCNN, RARE, STAR-Net and TRBA\nuse BiLSTM. Other models such as Rosetta and R2AM do not employ sequence\nmodelling to speed up prediction.\nThe Prediction stage examines the features resulting from the Backbone or\nSequence modelling to arrive at a sequence of characters prediction. CTC (Con-\nnectionist Temporal Classiﬁcation) [8] maximizes the likelihood of an output\nsequence by eﬃciently summing over all possible input-output sequence align-\n6 Rowel Atienza\nTransformer Encoder\nLinear Projection\n0 * 1 2 3 4 5 6 7 8\n[GO] F L A N D E R S [s] [s] ... [s]\nPosition +\nPatch Embedding\n*Learnable Embedding\nFig. 4.Network architecture of ViTSTR. An input image is ﬁrst converted into patches.\nThe patches are converted into 1D vector embeddings (ﬂattened 2D patches). As input\nto the encoder, a learnable patch embedding is added together with a position encod-\ning for each embedding. The network is trained end-to-end to predict a sequence of\ncharacters. [GO] is a pre-deﬁned start of sequence symbol while [s] represents a space\nor end of a character sequence.\nments [5]. Alternative to CTC is Attention Mechanism [2] that learns the align-\nment between the image features and symbols. CRNN, GRCNN, Rosetta and\nSTAR-Net use CTC. R2AM, RARE and TRBA are Attention-based.\nLike in natural language processing (NLP), transformers overcome sequence\nmodelling and prediction by doing parallel self-attention and prediction. This\nresulted to a fast and eﬃcient model. As shown in Figure 3, current transformer-\nbased STR models still require a Backbone and a Transformer Encoder-Decoder.\nRecently, ViT [7] proved that it is possible to beat the performance of deep net-\nworks such as ResNet [11] and EﬃcientNet [33] on ImageNet1k [28] classiﬁcation\nby using the transformer encoder only but pre-training it on very large datasets\nsuch as ImageNet21k and JFT-300M. DeiT [34] demonstrated that ViT does not\nneed a large dataset and can even achieve better results but it must be trained\nusing knowledge distillation [13]. ViT, using pre-trained weights of DeiT, is the\nbasis of our proposed fast and eﬃcient STR called ViTSTR. As shown in Figure\n3, ViTSTR is a very simple model with just one stage that can easily halve the\nnumber of parameters and FLOPS of a transformer-based STR.\n3 Vision Transformer for STR\nFigure 4 shows the model architecture of ViTSTR in detail. The only diﬀerence\nbetween ViT and ViTSTR is the prediction head. Instead of single object-class\nrecognition, ViTSTR must identify multiple characters with the correct sequence\norder and length. The prediction is done in parallel.\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 7\nPatch Embedding\nLayer Norm (LN)\nMulti-head Self-\nAttention (MSA)\n+\nLayer Norm (LN)\nMLP\n+\nTransformer\nEncoder\nBlock\nL×\nFig. 5.A transformer encoder is a stack of L identical encoder blocks.\nThe ViT model architecture is similar to the original transformer by Vaswani\net al. [35]. The diﬀerence is only the encoder part is utilized. The original\ntransformer was designed for NLP tasks. Instead of word embeddings, each in-\nput image x ∈RH×W×C is reshaped into a sequence of ﬂattened 2D patches\nxp ∈RN×P2C. The image dimension is H ×W with C channels while the patch\ndimension is P ×P. The resulting patch sequence length is N. The transformer\nencoder uses a constant width D for embedding and features in all its layers. To\nmatch this size, each ﬂattened patch is converted to an embedding of size D via\nlinear projection. This is shown as small boxes with teal color in Figure 4.\nA learnable class embedding of the same dimension D is prepended with the\nsequence. A unique position encoding of the same dimension D is added to each\nembedding. The resulting vector sum is the input to the encoder. In ViTSTR, a\nlearnable position encoding is used.\nIn the original ViT, the output vector corresponding to the learnable class\nembedding is used for object category prediction. In ViTSTR, this corresponds\nto the [GO] token. Furthermore, instead of just extracting one output vector,\nwe extract multiple feature vectors from the encoder. The number is equal to\nthe maximum length of text in our dataset plus two for the [GO] and [s] tokens.\nWe use the [GO] token to mark the beginning of the text prediction and [s] to\n8 Rowel Atienza\nindicate the end or a space. [s] is repeated at the end of each text prediction\nup to the maximum sequence length to mark that nothing follows after the text\ncharacters.\nFigure 5 shows the layers inside one encoder block. Every input goes through\nLayer Normalization (LN). The Multi-head Self-Attention layer (MSA) deter-\nmines the relationships between feature vectors. Vaswani et al. [35] found out\nthat using multiple heads instead of just one allows the model to jointly attend\nto information from diﬀerent representation subspaces at diﬀerent positions. The\nnumber of heads is H. The Multilayer Perceptron (MLP) performs feature ex-\ntraction. Its input is also layer normalized. The MLP is made of 2 layers with\nGELU activation [12]. Residual connection is placed between the output of LN\nand MSA/MLP.\nIn summary, the input to the encoder is:\nz0 = [xclass; x1\npE; x2\npE; ...; xN\np E] + Epos, (1)\nwhere E ∈RP2C×D and Epos ∈R(N+1)×D.\nThe output of MSA block is:\nz\n′\nl = MSA (LN(zl−1)) + zl−1, (2)\nfor l = 1...L. L is the depth or the number of encoder blocks. A transformer\nencoder is made of a stack of L encoder blocks.\nThe output of the MLP block is:\nzl = MLP (LN(z\n′\nl)) + z\n′\nl, (3)\nfor l = 1...L.\nFinally, the head is made of a sequence of linear projections forming the word\nprediction:\nyi = Linear(zi\nL), (4)\nfor i = 1...S. S is the maximum text length plus two for [GO] and [s] tokens.\nTable 1 summarizes the ViTSTR conﬁgurations.\nTable 1.ViTSTR conﬁgurations\nViTSTR Patch Size Depth Embedding Size No. of Heads Seq Length\nVersion P L D H S\nTiny 16 12 192 3 27\nSmall 16 12 384 6 27\nBase 16 12 768 12 27\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 9\n4 Experimental Results and Discussion\nIn order to evaluate diﬀerent strong baseline STR methods, we used the frame-\nwork developed by Baek et al.[1]. A uniﬁed framework is important in order to\narrive at a fair evaluation of diﬀerent models. A uniﬁed framework ensures con-\nsistent train and test conditions are used in the evaluation. Following discussion\ndescribes the train and test datasets which have been the point of contention\nin performance comparisons. Using diﬀerent train and test datasets can heavily\ntilt in favor or against a certain performance reporting.\nAfter discussing the train and test datasets, we present the evaluation and\nanalysis across diﬀerent models using the uniﬁed framework.\n4.1 Train Dataset\nMJ ST\nFig. 6.Samples from datasets with synthetic images.\nDue to the lack of a big dataset of real data, the practice in STR model\ntraining is to use synthetic data. Two popular datasets are used: 1) MJSynth\n(MJ) [14] or also known as Synth90k and 2) SynthText (ST) [9].\nMJSynth (MJ) is a synthetically generated dataset made of 8.9M re-\nalistically looking words images. MJSynth was designed to have 3 layers: 1)\nbackground, 2) foreground and 3) optional shadow/border. It uses 1,400 diﬀer-\nent fonts. The font kerning, weight, underline and other properties are varied.\nMJSynth also utilizes diﬀerent background eﬀects, border/shadow rendering,\nbase coloring, projective distortion, natural image blending and noise.\nSynthText (ST) is another synthetically generated dataset made of 5.5M\nword images. SynthText was generated by blending synthetic text on natural\nimages. It uses the scene geometry, texture, and surface normal to naturally\nblend and distort a text rendering on the surface of an object within the image.\nSimilar to MJSynth, SynthText uses random fonts for its text. The word images\nwere cropped from the natural images embedded with synthetic text.\nIn the STR framework, each dataset contributes 50% to the total train\ndataset. Combining 100% of both datasets resulted to performance deteriora-\ntion [1]. Figure 6 shows sample images from MJ and ST.\n10 Rowel Atienza\n4.2 Test Dataset\nRegular Dataset Irregular Dataset\nIIIT5K\n IC15\nSVT\n SVTP\nIC03\n CT\nIC13\nFig. 7.Samples from datasets with real images.\nThe test dataset is made of several small publicly available STR datasets of\ntext in natural images. These datasets are generally group into two: 1) Regular\nand 2) Irregular.\nThe regular datasets have text images that are frontal, horizontal and have\nminimal amount of distortion. IIIT5K-Words [23], Street View Text (SVT) [37],\nICDAR2003 (IC03) [22] and ICDAR2013 (IC13) [16] are considered regular\ndatasets. Meanwhile, irregular datasets contain text with challenging appear-\nances such curved, vertical, perspective, low-resolution or distorted. ICDAR2015\n(IC15) [15], SVT Perspective (SVTP) [25] and CUTE80 (CT) [27] belong to ir-\nregular datasets. Figure 7 shows samples from regular and irregular datasets.\nFor both datasets, only the test splits are used for the evaluation.\nTable 2.Train conditions\nTrain dataset: 50%MJ + 50%ST Batch size: 192\nEpochs: 300 Parameter initialization: He [10]\nOptimizer: Adadelta [40] Learning rate: 1.0\nAdadelta ρ: 0.95 Adadelta ϵ: 1e−8\nLoss: Cross-Entropy/CTC Gradient clipping: 5.0\nImage size: 100 × 32 Channels: 1 (grayscale)\nRegular Dataset\n– IIIT5K contains 3,000 images for testing. The images are mostly from street\nscenes such as sign board, brand logo, house number or street sign.\n– SVT has 647 images for testing. The text images are cropped from Google\nStreet View images.\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 11\n– IC03 contains 1,110 test images from ICDAR2003 Robust Reading Com-\npetition. Images were captured from natural scenes. After removing words\nthat are less than 3 characters in length, the result is 860 images. However,\n7 additional images were found to be missing. Hence, the framework also\ncontains the 867 test images version.\n– IC13 is an extension of IC03 and shares similar images. IC13 was created\nfor the ICDAR2013 Robust Reading Competition. In the literature and in\nthe framework, two versions of the test dataset are used: 1) 857 and 2) 1,015.\nIrregular Dataset\n– IC15 has text images for the ICDAR2015 Robust Reading Competition.\nMany images are blurry, noisy, rotated, and sometimes of low-resolution\nsince these were captured using Google Glasses with the wearer undergoing\nunconstrained motion. Two versions are used in the literature and in the\nframework: 1) 1,811 and 2) 2,077 images. The 2,077 version contains rotated,\nvertical, perspective-shifted and curved images.\n– SVTP has 645 test images from Google Street View. Most are images of\nbusiness signage.\n– CT focuses on curved text images captured from shirts and product logos.\nThe dataset has 288 images.\n4.3 Experimental Setup\nThe recommended training conﬁgurations in the framework are listed in Table\n2. We reproduced the results of several strong baseline models: CRNN, R2AM,\nGCRNN, Rosetta, RARE, STAR-Net and TRBA for a fair comparison with\nViTSTR. We trained all models for at least 5 times using diﬀerent random\nseeds. The best performing weights on the test datasets are saved to get the\nmean evaluation scores.\nFor ViTSTR, we used the same train conﬁgurations except that the input is\nresized to 224 ×224 to match the dimension of the pre-trained DeiT [34]. The\npre-trained weights ﬁle of DeiT is automatically downloaded before training\nViTSTR. ViTSTR can be trained end-to-end with no parameters frozen.\nTables 3 and 4 show the performance scores of diﬀerent models. We report\nthe accuracy, speed, number of parameters and FLOPS to get the overall picture\nof trade-oﬀs as shown in Figure 1. For accuracy, we follow the framework evalua-\ntion protocol in most STR models of case sensitive training and case insensitive\nevaluation. For speed, the reported numbers are based on model run time on a\n2080Ti GPU. Unlike in other model benchmarks such as in [19,20], we do not\nrotate vertical text images (e.g. Table 5 IC15) before evaluation.\n4.4 Data Augmentation\nUsing a recipe of data augmentation speciﬁcally targeted for STR can signif-\nicantly boost the accuracy of ViTSTR. In Figure 8, we can see how diﬀerent\n12 Rowel Atienza\nTable 3.Model accuracy. Bold: highest for all, Underscore: highest no augmentation.\nModel IIIT SVT IC03 IC13 IC15 SVTP CT Acc Std\n3000 647 860 867 857 1015 1811 2077 645 288 %\nCRNN [30] 81.8 80.1 91.7 91.5 89.4 88.4 65.3 60.4 65.9 61.5 76.7 0.3\nR2AM [17] 83.1 80.9 91.6 91.2 90.1 88.1 68.5 63.3 70.4 64.6 78.4 0.9\nGCRNN [36] 82.9 81.1 92.7 92.3 90.0 88.4 68.1 62.9 68.5 65.5 78.3 0.1\nRosetta [4] 82.5 82.8 92.6 91.8 90.3 88.7 68.1 62.9 70.3 65.5 78.4 0.4\nRARE [31] 86.0 85.4 93.5 93.4 92.3 91.0 73.9 68.3 75.4 71.0 82.1 0.3\nSTAR-Net [21] 85.2 84.7 93.4 93.0 91.2 90.5 74.5 68.7 74.7 69.2 81.8 0.1\nTRBA [1] 87.8 87.6 94.5 94.2 93.4 92.1 77.4 71.7 78.1 75.2 84.3 0.1\nViTSTR-Tiny 83.7 83.2 92.8 92.5 90.8 89.3 72.0 66.4 74.5 65.0 80.3 0.2\nViTSTR-Tiny+Aug 85.1 85.0 93.4 93.2 90.9 89.7 74.7 68.9 78.3 74.2 82.1 0.1\nViTSTR-Small 85.6 85.3 93.9 93.6 91.7 90.6 75.3 69.5 78.1 71.3 82.6 0.3\nViTSTR-Small+Aug 86.6 87.3 94.2 94.2 92.1 91.2 77.9 71.7 81.4 77.9 84.2 0.1\nViTSTR-Base 86.9 87.2 93.8 93.4 92.1 91.3 76.8 71.1 80.0 74.7 83.7 0.1\nViTSTR-Base+Aug 88.4 87.7 94.7 94.393.2 92.4 78.5 72.6 81.8 81.3 85.20.1\nTable 4.Model accuracy, speed, and computational requirements on a 2080Ti GPU.\nModel Accuracy Speed Parameters FLOPS\n% msec/image 1 × 106 1 × 109\nCRNN [30] 76.7 3.7 8.5 1.4\nR2AM [17] 78.4 22.9 2.9 2.0\nGRCNN [36] 78.3 11.2 4.8 1.8\nRosetta [4] 78.4 5.3 44.3 10.1\nRARE [31] 82.1 18.8 10.8 2.0\nSTAR-Net [21] 81.8 8.8 48.9 10.7\nTRBA [1] 84.3 22.8 49.6 10.9\nViTSTR-Tiny 80.3 9.3 5.4 1.3\nViTSTR-Tiny+Aug 82.1 9.3 5.4 1.3\nViTSTR-Small 82.6 9.5 21.5 4.6\nViTSTR-Small+Aug 84.2 9.5 21.5 4.6\nViTSTR-Base 83.7 9.8 85.8 17.6\nViTSTR-Base+Aug 85.2 9.8 85.8 17.6\nOriginal Invert Curve Blur Noise\nDistort Rotate Stretch/Comp. Perspective Shrink\nFig. 8.Illustration of data augmented text images designed for STR.\nN e s t l e\nFig. 9.ViTSTR attention as it reads out Nestle text image.\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 13\ndata augmentations alter the image but not the meaning of text within. Table 3\nshows that applying RandAugment [6] on diﬀerent image transformations such as\ninversion, curving, blur, noise, distortion, rotation, stretching/compressing, per-\nspective, and shrinking improved the generalization of ViTSTR-Tiny by +1.8%,\nViTSTR-Small by +1.6% and ViTSTR-Base by 1.5%. The biggest increase in\naccuracy is on irregular datasets such as CT (+9.2% tiny, +6.6% small and\nbase), SVTP (+3.8% tiny, +3.3% small, +1.8% base), IC15 1,811 (+2.7% tiny,\n+2.6% small, +1.7% base) and IC15 2,077 (+2.5% tiny, +2.2% small, +1.5%\nbase).\nTable 5.ViTSTR sample failed prediction from each test dataset. From ﬁrst to last\nrow: input image, ground truth, prediction, dataset. Wrong symbol prediction in red.\n18008091469 INC JAVA Distributed CLASSROOMS BOOKSTORE BRIDGESTONE\n1800B09446Y Onc IAVA Distribated Io-14DD07 BOOKSTORA Dueeesrreee\nIIIT5K SVT IC03 IC13 IC15 SVTP CUTE80\n4.5 Attention\nFigure 9 shows the attention map of ViTSTR as it reads out a text image. While\nthe attention is properly focused on each character, ViTSTR also pays attention\nto neighboring characters. Perhaps, a context is placed during individual symbol\nprediction.\n4.6 Performance Penalty\nEvery time a stage in an STR model is added, there is a gain in accuracy but\nat a cost of slower speed and bigger computational requirements. For exam-\nple, RARE ↪→TRBA increases the accuracy by 2.2% but requires 38.8M more\nparameters and slows down the task completion by 4 msec/image. Replacing\nthe CTC stage by Attention like in STAR-Net↪→TRBA signiﬁcantly slows down\nthe computation from 8.8 msec/image to 22.8 msec/image to gain an additional\n2.5% in accuracy. In fact, the slowdown due to change from CTC to Attention is\n> 10×as compared to adding BiLSTM or TPS in the pipeline. In ViTSTR, the\ntransition from tiny to small version requires an increase in embedding size and\nnumber of heads. No additional stage is necessary. The performance penalty to\ngain 2.3% in accuracy is increase in number of parameters by 16.1M. From tiny\nto base, the performance penalty to gain 3.4% in accuracy is additional 80.4M\nparameters. In both cases, the speed barely changed since we use the same par-\nallel tensor dot product, softmax and addition operations in MLP and MSA\n14 Rowel Atienza\nlayers of the transformer encoder. Only the tensor dimension is increased result-\ning to a minimal 0.2 to 0.3 msec/image slowdown in task completion. Unlike\nin multi-stage STR, an additional module requires additional sequential layers\nof forward propagation which can not be parallelized resulting into a signiﬁcant\nperformance penalty.\n4.7 Failure Cases\nTable 5 shows sample failed predictions by ViTSTR-Small from each test dataset.\nThe main causes of wrong prediction are confusion between similar symbols (e.g.\n8 and B, J and I), scripted font (e.g. I in Inc), glare on a character, vertical text,\nheavily curved text image and partially occluded symbol. Note that in some of\nthese cases, even a human reader can easily make a mistake. However, humans\nuse semantics to resolve ambiguities. Semantics has been used in recent STR\nmethods [26,39].\n5 Conclusion\nViTSTR is a simple single stage model architecture that emphasizes balance\nin accuracy, speed and computational requirements. With data augmentation\ntargeted for STR, ViTSTR can signiﬁcantly increase the accuracy especially for\nirregular datasets. When scaled up, ViTSTR stays at the frontiers to balance\naccuracy, speed and computational requirements.\nAcknowledgements. This work was funded by the University of the Philip-\npines ECWRG 2019-2020. GPU machines have been supported by CHED-PCARI\nAIRSCAN Project and Samsung R&D PH. Special thanks to the people of Com-\nputer Networks Laboratory: Roel Ocampo, Vladimir Zurbano, Lope Beltran II,\nand John Robert Mendoza, who worked tirelessly during the pandemic to ensure\nthat our network and servers are continuously running.\nReferences\n1. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is\nwrong with scene text recognition model comparisons? dataset and model analysis.\nIn: ICCV. pp. 4715–4723 (2019)\n2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n3. Bookstein, F.L.: Principal warps: Thin-plate splines and the decomposition of de-\nformations. Trans on Pattern Analysis and Machine Intelligence 11(6), 567–585\n(1989)\n4. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: Large scale system for text de-\ntection and recognition in images. In: Intl Conf on Knowledge Discovery & Data\nMining. pp. 71–79 (2018)\nVision Transformer for Fast and Eﬃcient Scene Text Recognition 15\n5. Chen, X., Jin, L., Zhu, Y., Luo, C., Wang, T.: Text recognition in the wild: A\nsurvey. arXiv preprint arXiv:2005.03492 (2020)\n6. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated\ndata augmentation with a reduced search space. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops. pp. 702–703\n(2020)\n7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\n16x16 words: Transformers for image recognition at scale. ICLR (2020)\n8. Graves, A., Fern´ andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal\nclassiﬁcation: labelling unsegmented sequence data with recurrent neural networks.\nIn: ICML. pp. 369–376 (2006)\n9. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in nat-\nural images. In: CVPR. pp. 2315–2324 (2016)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-\nlevel performance on imagenet classiﬁcation. In: ICCV. pp. 1026–1034 (2015)\n11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. pp. 770–778 (2016)\n12. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016)\n13. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 (2015)\n14. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and arti-\nﬁcial neural networks for natural scene text recognition. NIPS Workshop on Deep\nLearning (2014)\n15. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwa-\nmura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., et al.: Icdar 2015\ncompetition on robust reading. In: ICDAR. pp. 1156–1160. IEEE (2015)\n16. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R.,\nMas, J., Mota, D.F., Almazan, J.A., De Las Heras, L.P.: Icdar 2013 robust reading\ncompetition. In: ICDAR. pp. 1484–1493. IEEE (2013)\n17. Lee, C.Y., Osindero, S.: Recursive recurrent nets with attention modeling for ocr\nin the wild. In: CVPR. pp. 2231–2239 (2016)\n18. Lee, J., Park, S., Baek, J., Oh, S.J., Kim, S., Lee, H.: On recognizing texts of\narbitrary shapes with 2d self-attention. In: CVPR Workshops. pp. 546–547 (2020)\n19. Li, H., Wang, P., Shen, C., Zhang, G.: Show, attend and read: A simple and strong\nbaseline for irregular text recognition. In: AAAI. vol. 33, pp. 8610–8617 (2019)\n20. Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: Scatter:\nselective context attentional scene text recognizer. In: CVPR. pp. 11962–11972\n(2020)\n21. Liu, W., Chen, C., Wong, K.Y.K., Su, Z., Han, J.: Star-net: a spatial attention\nresidue network for scene text recognition. In: BMVC. vol. 2, p. 7 (2016)\n22. Lucas, S.M., Panaretos, A., Sosa, L., Tang, A., Wong, S., Young, R., Ashida, K.,\nNagai, H., Okamoto, M., Yamamoto, H., et al.: Icdar 2003 robust reading compe-\ntitions: entries, results, and future directions. Intl Journal of Document Analysis\nand Recognition 7(2-3), 105–122 (2005)\n23. Mishra, A., Alahari, K., Jawahar, C.: Scene text recognition using higher order\nlanguage priors. In: BMVC. BMVA (2012)\n24. Neumann, L., Matas, J.: Real-time scene text localization and recognition. In:\nCVPR. pp. 3538–3545. IEEE (2012)\n16 Rowel Atienza\n25. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with perspec-\ntive distortion in natural scenes. In: ICCV. pp. 569–576 (2013)\n26. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: Seed: Semantics enhanced\nencoder-decoder framework for scene text recognition. In: CVPR. pp. 13528–13537\n(2020)\n27. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary\ntext detection system for natural scene images. Expert Systems with Applications\n41(18), 8027–8048 (2014)\n28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-\nnition challenge. Intl Journal of Computer Vision 115(3), 211–252 (2015)\n29. Sheng, F., Chen, Z., Xu, B.: Nrtr: A no-recurrence sequence-to-sequence model for\nscene text recognition. In: ICDAR. pp. 781–786. IEEE (2019)\n30. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based\nsequence recognition and its application to scene text recognition. Trans on Pattern\nAnalysis and Machine Intelligence 39(11), 2298–2304 (2016)\n31. Shi, B., Wang, X., Lyu, P., Yao, C., Bai, X.: Robust scene text recognition with\nautomatic rectiﬁcation. In: CVPR. pp. 4168–4176 (2016)\n32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. ICLR (2015)\n33. Tan, M., Le, Q.: Eﬃcientnet: Rethinking model scaling for convolutional neural\nnetworks. In: ICML. pp. 6105–6114. PMLR (2019)\n34. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877 (2020)\n35. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NeuRIPS. pp. 6000–6010 (2017)\n36. Wang, J., Hu, X.: Gated recurrent convolution neural network for ocr. In: NeuRIPS.\npp. 334–343 (2017)\n37. Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition. In: ICCV.\npp. 1457–1464. IEEE (2011)\n38. Yao, C., Bai, X., Liu, W.: A uniﬁed framework for multioriented text detection\nand recognition. Trans on Image Processing 23(11), 4737–4749 (2014)\n39. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate\nscene text recognition with semantic reasoning networks. In: CVPR. pp. 12113–\n12122 (2020)\n40. Zeiler, M.D.: Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701 (2012)\n41. Zhan, F., Lu, S.: Esir: End-to-end scene text recognition via iterative image recti-\nﬁcation. In: CVPR. pp. 2059–2068 (2019)",
  "topic": "FLOPS",
  "concepts": [
    {
      "name": "FLOPS",
      "score": 0.8913168907165527
    },
    {
      "name": "Computer science",
      "score": 0.7952542304992676
    },
    {
      "name": "Transformer",
      "score": 0.6694361567497253
    },
    {
      "name": "Efficient energy use",
      "score": 0.5530855059623718
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4715785086154938
    },
    {
      "name": "Speedup",
      "score": 0.42930880188941956
    },
    {
      "name": "Cognitive neuroscience of visual object recognition",
      "score": 0.41013920307159424
    },
    {
      "name": "Object (grammar)",
      "score": 0.36818140745162964
    },
    {
      "name": "Computer vision",
      "score": 0.36163002252578735
    },
    {
      "name": "Parallel computing",
      "score": 0.16435506939888
    },
    {
      "name": "Voltage",
      "score": 0.11066097021102905
    },
    {
      "name": "Engineering",
      "score": 0.0756320059299469
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I103911934",
      "name": "University of the Philippines System",
      "country": "PH"
    }
  ]
}