{
  "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
  "url": "https://openalex.org/W4385573323",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101749986",
      "name": "Yuxin Xiao",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2767400205",
      "name": "Paul Pu Liang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2911728477",
      "name": "Umang Bhatt",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2140202858",
      "name": "Willie Neiswanger",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2031945151",
      "name": "Ruslan Salakhutdinov",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A4212702429",
      "name": "Louis-Philippe Morency",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3169554260",
    "https://openalex.org/W2164411961",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3213215942",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3035782406",
    "https://openalex.org/W3114632476",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W4287594739",
    "https://openalex.org/W3173618889",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W1826524928",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W3034412561",
    "https://openalex.org/W4287865413",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2137556846",
    "https://openalex.org/W3102100346",
    "https://openalex.org/W4300772090",
    "https://openalex.org/W4285146641",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3128453519",
    "https://openalex.org/W4297801368",
    "https://openalex.org/W2997604142",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2061119986",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3093891978",
    "https://openalex.org/W3034201598",
    "https://openalex.org/W3013838212",
    "https://openalex.org/W4320341979",
    "https://openalex.org/W2073241381",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W3111980464",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2804697534",
    "https://openalex.org/W2600383743",
    "https://openalex.org/W2953021786",
    "https://openalex.org/W3211561698",
    "https://openalex.org/W3035154851",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2810469995",
    "https://openalex.org/W3102315351",
    "https://openalex.org/W4287023290",
    "https://openalex.org/W2158840489",
    "https://openalex.org/W4287325357",
    "https://openalex.org/W4200630791",
    "https://openalex.org/W3183398589",
    "https://openalex.org/W4285298351",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7273–7284\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nUncertainty Quantification with Pre-trained Language Models:\nA Large-Scale Empirical Analysis\nYuxin Xiao1, Paul Pu Liang2, Umang Bhatt3,\nWillie Neiswanger4, Ruslan Salakhutdinov2, Louis-Philippe Morency2\n1Massachusetts Institute of Technology,2Carnegie Mellon University,\n3University of Cambridge, 4Stanford University\n1yuxin102@mit.edu 2{pliang,rsalakhu,morency}@cs.cmu.edu\n3usb20@cam.ac.uk 4neiswanger@cs.stanford.edu\nAbstract\nPre-trained language models (PLMs) have\ngained increasing popularity due to their com-\npelling prediction performance in diverse nat-\nural language processing (NLP) tasks. When\nformulating a PLM-based prediction pipeline\nfor NLP tasks, it is also crucial for the pipeline\nto minimize the calibration error, especially in\nsafety-critical applications. That is, the pipeline\nshould reliably indicate when we can trust its\npredictions. In particular, there are various con-\nsiderations behind the pipeline: (1) the choice\nand (2) the size of PLM, (3) the choice of uncer-\ntainty quantifier, (4) the choice of fine-tuning\nloss, and many more. Although prior work has\nlooked into some of these considerations, they\nusually draw conclusions based on a limited\nscope of empirical studies. There still lacks\na holistic analysis on how to compose a well-\ncalibrated PLM-based prediction pipeline. To\nfill this void, we compare a wide range of pop-\nular options for each consideration based on\nthree prevalent NLP classification tasks and the\nsetting of domain shift. In response, we rec-\nommend the following: (1) use ELECTRA for\nPLM encoding, (2) use larger PLMs if possible,\n(3) use Temp Scaling as the uncertainty quanti-\nfier, and (4) use Focal Loss for fine-tuning.\n1 Introduction\nPLMs (Qiu et al., 2020; Min et al., 2021) have\nachieved state-of-the-art performance on a broad\nspectrum of NLP benchmarks (Rajpurkar et al.,\n2016, 2018; Wang et al., 2019a,b) and are increas-\ningly popular in various downstream applications\nsuch as question answering (Yoon et al., 2019; Garg\net al., 2020), text classification (Arslan et al., 2021;\nLimsopatham, 2021), and relation extraction (Zhou\net al., 2021; Xiao et al., 2022). Consequently, it\nis paramount for PLMs to faithfully communicate\nwhen to (or not to) rely on their predictions for\ndecision-making, especially in high-stakes scenar-\nios. In these cases, we need PLMs to quantify their\nuncertainty accurately and calibrate well (Abdar\net al., 2021), meaning that their predictive con-\nfidence should be a valid estimate of how likely\nthey are to make a correct prediction. Consider\nan example of medical question answering (Yoon\net al., 2019; Zhang et al., 2021) where a PLM is\nasked to assist doctors when diagnosing diseases.\nIf the PLM is 90% sure that a patient is healthy,\nthe predicted outcome should occur 90% of the\ntime in practice. Otherwise, it may adversely affect\ndoctors’ judgment and lead to catastrophic conse-\nquences. Hence, since PLMs have become the de\nfacto paradigm for many NLP tasks, it is necessary\nto assess their calibration quality.\nWhen constructing a well-calibrated PLM-based\nprediction pipeline for NLP tasks, various consid-\nerations are involved. To name a few:\n1. Due to the use of diverse pre-training datasets\nand strategies, different PLMs may behave\ndifferently regarding calibration.\n2. The model size of PLMs may also affect their\ncapability in calibration.\n3. Leveraging uncertainty quantifiers (e.g., Temp\nScaling (Guo et al., 2017) and MC Dropout\n(Gal and Ghahramani, 2016)) alongside PLMs\nin the pipeline may reduce calibration error.\n4. Some losses (e.g., Focal Loss (Mukhoti et al.,\n2020) and Label Smoothing (Müller et al.,\n2019)) may fine-tune PLMs to calibrate better.\nAlthough some of these considerations have been\nstudied before, the ideal choice for each consid-\neration remains obscure. On the one hand, Desai\nand Durrett (2020) report unconventional calibra-\ntion behavior for PLMs, which casts doubts on the\nprior beliefs drawn on traditional neural networks\nby Guo et al. (2017). On the other hand, exist-\ning work (Desai and Durrett, 2020; Dan and Roth,\n2021) on PLMs’ empirical calibration performance\noften looks at a single consideration and concludes\nby comparing only one or two types of PLMs.\nTherefore, in this paper, we present a compre-\nhensive analysis of the four pivotal considerations\n7273\nintroduced above via large-scale empirical evalua-\ntions. To ensure that our analysis is applicable to\nvarious NLP tasks and resilient to domain shift, we\nset up three NLP tasks (i.e., Sentiment Analysis,\nNatural Language Inference, and Commonsense\nReasoning) and prepare both in-domain and out-\nof-domain testing sets for each task. In addition\nto the explicit metrics of prediction and calibra-\ntion error, we also utilize two evaluation tasks to\nexamine calibration qualities implicitly. Selective\nprediction lowers prediction error by avoiding un-\ncertain testing points, and out-of-domain detection\nchecks if a pipeline is less confident on unseen do-\nmains. By comparing four to five options for each\nconsideration, we recommend the following:\n1. Use ELECTRA (Clark et al., 2020) as the\nPLM to encode input text sequences.\n2. Use the larger version of a PLM if possible.\n3. Use Temp Scaling (Guo et al., 2017) for post\nhoc uncertainty recalibration.\n4. Use Focal Loss (Mukhoti et al., 2020) during\nthe fine-tuning stage.\nCompared to prior work, our extensive empirical\nevaluations also reveal the following novel obser-\nvations that are unique to PLM-based pipelines:\n• The calibration quality of PLMs is relatively\nconsistent across tasks and domains, except\nXLNet (Yang et al., 2019) being the most vul-\nnerable to domain shift.\n• In contrast to other NLP tasks, larger PLMs\nare better calibrated in-domain in Common-\nsense Reasoning.\n• Uncertainty quantifiers (e.g., Temp Scaling)\nare generally more effective in improving cal-\nibration out-of-domain.\n• Ensemble (Lakshminarayanan et al., 2017) is\nless effective in PLM-based pipelines.\nTo encourage future work towards better uncer-\ntainty quantification in NLP, we release our code\nand large-scale evaluation benchmarks containing\n120 PLM-based pipelines based on four metrics\n(prediction and calibration error, selective predic-\ntion, and out-of-domain detection). These pipelines\nconsist of distinct choices concerning the four con-\nsiderations and are tested on all three NLP tasks\nunder both in- and out-of-domain settings.1\n1Our data and code are available at https://github.\ncom/xiaoyuxin1002/UQ-PLM.git.\n2 Background\n2.1 Problem Formulation\nDatasets. In this work, we focus on utilizing PLMs\nfor NLP classification tasks. More specifically,\nconsider such a task where the training set Dtrain =\n{(xi, yi)}Ntrain\ni=1 consists of pairs of a text sequence\nxi ∈Xin and an associated label yi ∈Y. Similarly,\nthe validation set Dval and the in-domain testing\nset Din come from the same domain Xin and share\nthe same label space Y. We also prepare an out-\nof-domain testing set Dout, which differs from the\nothers by coming from a distinct domain Xout.\nPLM-based Pipeline. We apply a PLM M to\nencode an input text sequence xi and feed the en-\ncoding vector to a classifier F, which outputs a\npredictive distribution ui over the label space Y\nvia the softmax operation. Here, parameters in M\nand F are fine-tuned by minimizing a loss function\nℓ on Dtrain. It is optional to modify the distribu-\ntion ui post hoc by an uncertainty quantifier Q to\nreduce calibration error. We define the predicted\nlabel as ˆyi = arg maxj∈{1,...,|Y|} uij with the cor-\nresponding confidence ˆci = uiˆyi .\nCalibration. One crucial goal of uncertainty\nquantification is to improve calibration. That is, the\npredicted confidence should match the empirical\nlikelihood: P(yi = ˆyi | ˆci) = ˆci. We follow\nGuo et al. (2017) by using the expected calibration\nerror (ECE) to assess the calibration performance.\nThe calculation of ECE is described in Section 3.1.\nTo reduce ECE, our main experimental evaluation\nlies in examining four considerations involved in\na PLM-based pipeline: (1) the choice of PLM M\n(Section 3), (2) the size of PLM M (Section 4), (3)\nthe choice of uncertainty quantifier Q (Section 5),\nand (4) the choice of loss function ℓ (Section 6).\n2.2 Related Work\nUncertainty quantification has drawn long-lasting\nattention from various domains (Bhatt et al., 2021),\nsuch as weather forecasting (Brier et al., 1950;\nRaftery et al., 2005), medical practice (Yang and\nThompson, 2010; Jiang et al., 2012), and machine\ntranslation (Ott et al., 2018; Zhou et al., 2020;\nWei et al., 2020). Researchers have approached\nthis question from both Bayesian (Kendall and\nGal, 2017; Depeweg et al., 2018) and frequentist\nperspectives (Alaa and Van Der Schaar, 2020a,b).\nThey have also proposed different techniques to\nimprove uncertainty calibration for classification\n(Kong et al., 2020; Krishnan and Tickoo, 2020) and\n7274\nregression (Kuleshov et al., 2018; Cui et al., 2020;\nChung et al., 2021) tasks. Recent work has inves-\ntigated connections between uncertainty and other\nproperties, such as model interpretability (Antoran\net al., 2021; Ley et al., 2022), selective prediction\n(Xin et al., 2021; Varshney et al., 2022a,b), and\nout-of-domain generalization (Wald et al., 2021;\nQin et al., 2021).\nPLMs (Qiu et al., 2020; Min et al., 2021) have\nachieved state-of-the-art prediction performance\non diverse NLP benchmarks (Rajpurkar et al.,\n2016, 2018; Wang et al., 2019a,b) and demon-\nstrated many desired properties like stronger out-\nof-domain robustness (Hendrycks et al., 2020) and\nbetter uncertainty calibration (Desai and Durrett,\n2020). They typically leverage a Transformer archi-\ntecture (Vaswani et al., 2017) and are pre-trained\nby self-supervised learning (Jaiswal et al., 2021).\nAlthough Guo et al. (2017) report that larger\nmodels tend to calibrate worse, PLMs have been\nshown to produce well-calibrated uncertainty in\npractice (Desai and Durrett, 2020), albeit for giant\nmodel sizes. Their unusual calibration behavior\nputs the observations drawn on traditional neural\nnetworks (Ovadia et al., 2019; Mukhoti et al., 2020)\nor pre-trained vision models (Minderer et al., 2021)\nin doubt. Prior work (Desai and Durrett, 2020; Dan\nand Roth, 2021) on the calibration of PLMs often\nexplores only one or two types of PLMs and ig-\nnores uncertainty quantifiers and fine-tuning losses\nbeyond Temp Scaling and Cross Entropy, respec-\ntively. As a result, there lacks a holistic analysis\nthat explores the full set of these considerations in a\nPLM-based pipeline. Therefore, our paper aspires\nto fill this void via extensive empirical studies.\n3 Which Pre-trained Language Model?\n3.1 Experiment Setup\nTo evaluate the calibration performance of PLMs,\nwe consider a series of NLP classification tasks:\n1. Sentiment Analysis identifies the binary sen-\ntiment of a text sequence. We treat the IMDb\nmovie reviewdataset (Maas et al., 2011) as in-\ndomain and the Yelprestaurant reviewdataset\n(Zhang et al., 2015) as out-of-domain.\n2. Natural Language Inference predicts the re-\nlationship between a hypothesis and a premise.\nWe regard the Multi-Genre Natural Language\nInference (MNLI) dataset (Williams et al.,\n2018) covering a range of genres of spoken\nand written textas in-domain and the Stanford\nSentiment Natural Language Commonsense\nAnalysis Inference Reasoning\nXin IMDb MNLI SW AG\nXout Yelp SNLI HellaSW AG\n|Y| 2 3 4\n|Dtrain| 25,000 392,702 73,546\n|Dval| 12,500 4,907 10,003\n|Din| 12,500 4,908 10,003\n|Dout| 19,000 4,923 5,021\nTable 1: In- and out-of-domain datasets, label space\nsize, and each data split size of the three NLP tasks.\nHugging Face Model Pre-training Pre-training\nName Size Corpus Size Task\nbert-base-cased 109M 16G Masked LM, NSP\nxlnet-base-cased 110M 161G Permuted LM\nelectra-base-discriminator 110M 161G Replacement Detection\nroberta-base 125M 161G Dynamic Masked LM\ndeberta-base 140M 85G Dynamic Masked LM\nbert-large-cased 335M 16G Masked LM, NSP\nxlnet-large-cased 340M 161G Permuted LM\nelectra-large-discriminator 335M 161G Replacement Detection\nroberta-large 335M 161G Dynamic Masked LM\ndeberta-large 350M 85G Dynamic Masked LM\nTable 2: Model size, pre-training corpus size, and pre-\ntraining task of the five PLMs, separated into the base\n(upper) and the large (lower) versions.\nNatural Language Inference (SNLI) dataset\n(Bowman et al., 2015) derived from image\ncaptions only as out-of-domain.\n3. Commonsense Reasoning determines the\nmost reasonable continuation of a sentence\namong four candidates. We view the Situa-\ntions With Adversarial Generations (SWAG)\ndataset (Zellers et al., 2018) as in-domain and\nits adversarial variant (HellaSW AG) (Zellers\net al., 2019) as out-of-domain.\nFor each task, we construct Dtrain, Dval, and Din\nfrom the corresponding in-domain dataset, andDout\nfrom the corresponding out-of-domain dataset. The\noriginal validation set of each dataset is split in half\nrandomly to form a held-out non-blind testing set\n(i.e., Din or Dout). Table 1 describes the task details.\nTo understand which PLM delivers the lowest\ncalibration error, we examine five popular options:\n1. BERT (Devlin et al., 2019) utilizes a bidi-\nrectional Transformer architecture pre-trained\nby masked language modeling (LM) and next\nsentence prediction (NSP).\n2. XLNet (Yang et al., 2019) proposes a two-\nstream self-attention mechanism and a pre-\ntraining objective of permuted LM.\n3. ELECTRA (Clark et al., 2020) pre-trains a\n7275\nIn-Domain:\nOut-Of-Domain:\nBERT\nBERT\nXLNet\nXLNet\nELECTRA\nELECTRA\nRoBERTa\nRoBERTa\nDeBERTa\nDeBERTa\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.00\n0.05\n0.10\n0.15\nECE (↓)\nELECTRA achieves the (comparably) lowest ECE.\n(a) In-Domain and Out-Of-Domain Calibration of PLMs\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.0\n0.2\n0.4\n0.6\nError (↓)\nELECTRA achieves the (comparably) lowest error.\n(b) In-Domain and Out-Of-Domain Prediction of PLMs\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.000\n0.025\n0.050\n0.075\nRPP (↓)\nELECTRA achieves the (comparably) lowest RPP.\n(c) In-Domain and Out-Of-Domain Selective Prediction of PLMs\n0.03 0.04 0.05\nIn-Domain ECE (↓)\n0.06\n0.07\n0.08\n0.09\nOut-Of-Domain ECE (↓)\noutlier\nline of best ﬁt,\nR2 = 0.9953\nSentiment Analysis\n0.045 0.050 0.055 0.060\nIn-Domain ECE (↓)\n0.06\n0.07\n0.08\noutlier\nline of best ﬁt,\nR2 = 0.9244\nNatural Language Inference\n0.04 0.06 0.08 0.10 0.12\nIn-Domain ECE (↓)\n0.09\n0.10\n0.11\n0.12\n0.13 outlier\nline of best ﬁt,\nR2 = 0.9896\nCommonsense Reasoning\n(d) In-Domain Calibration vs Out-Of-Domain Calibration\nFigure 1: Calibration and (selective) prediction performance of five PLMs in three NLP tasks under two domain\nsettings. The calibration quality of the five PLMs is relatively consistent across tasks and domains, while XLNet is\nthe least robust to domain shift. ELECTRA stands out due to its lowest scores in ECE, prediction error, and RPP.\ndiscriminative model to detect tokens replaced\nby a generative model.\n4. RoBERTa(Liu et al., 2019) builds on BERT\nby pre-training based on dynamic masked LM\nonly and tuning key hyperparameters.\n5. DeBERTa(He et al., 2020) further improves\nRoBERTa via a disentangled attention mecha-\nnism and an enhanced mask decoder.\nWe use the base version of each PLM, which has\na similar model size and is initialized from the\ncorresponding Hugging Face (Wolf et al., 2020)\npre-trained checkpoint. Table 2 details these PLMs.\nAfter receiving the encoding vector of the classi-\nfication token [CLS] for an input text sequence\nfrom the PLM, we pass it through a classifier to\nobtain a predictive distribution. Regarding the clas-\nsifier configuration, we follow the default practice\nin Hugging Face by utilizing a two-layer neural\nnetwork with tanh non-linear activation.\nThe learning rate for each model-dataset combi-\nnation is tuned based on the validation set among\n{5e−6, 1e−5, 2e−5, 5e−5}. We leverage AdamW\n7276\nLarge:\nBase:\nBERT\nBERT\nXLNet\nXLNet\nELECTRA\nELECTRA\nRoBERTa\nRoBERTa\nDeBERTa\nDeBERTa\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.00\n0.05\n0.10\nECE (↓)\nWhen evaluated in-domain, larger PLMs score slightly higher in ECE,\nexcept in Commonsense Reasoning.\n(a) In-Domain Calibration of PLMs of Diﬀerent Sizes\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.00\n0.05\n0.10\n0.15\nECE (↓)\nWhen evaluated out-of-domain, larger PLMs score lower in ECE.\n(b) Out-Of-Domain Calibration of PLMs of Diﬀerent Sizes\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.0\n0.1\n0.2\nError (↓)\nWhen evaluated in-domain,\nlarger PLMs score lower in prediction error.\n(c) In-Domain Prediction of PLMs of Diﬀerent Sizes\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.0\n0.2\n0.4\n0.6\nError (↓)\nWhen evaluated out-of-domain,\nlarger PLMs score lower in prediction error.\n(d) Out-Of-Domain Classiﬁcation Error of PLMs of Diﬀerent Sizes\n0.08 0.10\nError (↓)\n0.06\n0.07\n0.08\n0.09\nECE (↓)\nCorrelation\n= 0.7664\nSentiment Analysis\n0.100 0.125 0.150 0.175 0.200\nError (↓)\n0.06\n0.07\n0.08\nCorrelation\n= 0.9026\nNatural Language Inference\n0.2 0.4 0.6\nError (↓)\n0.08\n0.10\n0.12\nCorrelation\n= 0.7853\nCommonsense Reasoning\n(e) Out-Of-Domain Calibration vs Out-Of-Domain Prediction\nFigure 2: Calibration and prediction performance of large and base PLMs in three NLP tasks under two domain\nsettings. Larger PLMs calibrate better than their respective base versions when evaluated out-of-domain, while\ncalibrating slightly worse in-domain with one exception in Commonsense Reasoning. If the computational budget\npermits, larger PLMs constitute more powerful pipelines given their lower out-of-domain ECE along with lower\nprediction error. We also observe a positive correlation between calibration and prediction error out-of-domain.\n(Loshchilov and Hutter, 2018) to minimize the\ncross-entropy loss on Dtrain for five epochs with\nearly stopping and a linearly decaying scheduler\n(Goyal et al., 2017) whose warm-up ratio = 10%.\nBatch size is 16, and the model gradients are\nclipped to a maximum norm of 1. We perform\n7277\nour experiments on a Tesla A6000 GPU and report\nthe mean and one standard error by conducting six\ntrials with different seeds.\nTo explicitly evaluate calibration performance by\nECE, we first stratify N predictions into K bins of\nequal width based on the sorted confidence values.\nThen ECE is a weighted average of the absolute\ndifference between the accuracy and confidence of\neach bin: ECE = ∑K\nk=1\n|Bk|\nN |acc(Bk)−conf(Bk)|,\nwhere acc(Bk) and conf(Bk) are the average ac-\ncuracy and confidence of predictions in bin Bk,\nrespectively. We setK = 10in our experiments.\nTo implicitly assess calibration quality based\non selective prediction, we deploy the metric of\nreversed pair proportion (RPP) (Xin et al., 2021).\nMore specifically, for a dataset of size N, RPP =\n1\nN2\n∑N\ni=1\n∑N\nj=1 1 [ˆci < ˆcj, yi = ˆyi, yj ̸= ˆyj]. It\nmeasures the proportion of prediction pairs with\na reversed confidence-error relationship. A lower\nRPP indicates that the pipeline is more confident\non correct predictions.\n3.2 Empirical Findings\nAs shown in Figure 1(a), the calibration per-\nformance of all five PLMs deteriorates from in-\ndomain to out-of-domain. This phenomenon co-\nincides with the finding made by Ovadia et al.\n(2019) on traditional neural networks. In addi-\ntion, the ranking among the five PLMs based\non ECE is generally consistent, which implies\nthat their calibration quality is transferable across\ntasks and domains. More specifically, for all three\ntasks under the in-domain setting, XLNet, ELEC-\nTRA, RoBERTa, and DeBERTa outperform BERT\nin terms of lower ECE, suggesting that a larger\npre-training corpus may improve the calibration\nquality (see Table 2). When moving to the out-of-\ndomain setting, XLNet sees the largest increase\nin ECE, which makes it an outlier in Figure 1(d).\nThis observation may indicate that the pre-training\ntask of permuted LM is vulnerable to domain shift.\nELECTRA stands out among the five exam-\nined PLMs in encoding input text sequences.\nNot only does it achieve the (comparably) lowest\nECE in all three tasks under both in- and out-of-\ndomain settings, it also delivers the lowest predic-\ntion error in Figure 1(b) and the lowest RPP for\nselective prediction in Figure 1(c). We hypothesize\nits success to the unique pre-training paradigm of\nreplaced token detection, which preserves the to-\nken distribution by avoiding the artificial [MASK]\ntokens in masked LM and enhances the computa-\ntional efficiency by learning from all input tokens.\n4 What Model Size?\n4.1 Experiment Setup\nTo investigate how the size of PLMs affects the\ncalibration performance, we compare the large ver-\nsions of the five PLMs mentioned in Section 3.1\nagainst their respective base versions. We keep the\nrest of the setup the same as in Section 3.1.\n4.2 Empirical Findings\nFigures 2(a) and (b) demonstrate that larger PLMs\ntend to produce a slightly higher ECE compared\nto their respective base versions when evaluated\nin-domain, while calibrating better out-of-domain.\nThis observation based on five PLMs verifies the\nconclusion made by Dan and Roth (2021) solely\nbased on BERT. However,there is a notable ex-\nception that larger PLMs are significantly better\ncalibrated in-domain in Commonsense Reason-\ning than their respective base versions, which\nimplies that larger PLMs are more aware of their\nuncertainties during the reasoning process.\nLarger PLMs constitute more powerful PLM-\nbased pipelines, if computational budget per-\nmits. Although sometimes they suffer slightly in\nin-domain calibration compared to their smaller\ncounterparts, larger PLMs achieve a lower ECE\nout-of-domain. They also deliver lower in- and\nout-of-domain prediction errors in Figures 2(c) and\n(d), respectively. In addition, we observe a positive\ncorrelation between calibration and prediction er-\nrors under the out-of-domain setting in Figure 2(e),\nsuggesting that pipelines calibrating well out-of-\ndomain are more accurate under domain shift as\nwell. This reflects the finding in Wald et al. (2021)\nthat multi-domain calibration leads to better out-of-\ndomain prediction performance.\n5 Which Uncertainty Quantifier?\n5.1 Experiment Setup\nAs discussed in Section 2.1, we can further adjust\nthe vanilla predictive distribution post hoc via an\nuncertainty quantifier. Therefore, we study four\nuncertainty quantifiers based on the setup in Sec-\ntion 3.1 to inspect which improve the calibration\nperformance in our problem formulation:\n1. Temp Scaling (Guo et al., 2017) learns a\nscalar parameter Ttemp based on Dval and “soft-\n7278\nIn-Domain:\nOut-Of-Domain:\nTemp Scaling\nTemp Scaling\nMC Dropout\nMC Dropout\nEnsemble\nEnsemble\nLL SVI\nLL SVI\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n−0.05\n0.00\n0.05\n0.10\nChange in ECE ( ↓)\nTemp Scaling reduces ECE most eﬀectively.\n(a) Change in In-Domain and Out-Of-Domain Calibration due to Uncertainty Quantiﬁers\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n−0.06\n−0.04\n−0.02\n0.00\nChange in Error ( ↓)\nTemp Scaling preserves prediction performance (i.e., no change in error).\n(b) Change in In-Domain and Out-Of-Domain Prediction due to Uncertainty Quantiﬁers\nFigure 3: Change in calibration and prediction performance due to the use of four uncertainty quantifiers. The\neffectiveness of these quantifiers in reducing ECE follows the descending order of Temp Scaling, MC Dropout,\nEnsemble, and LL SVI. The drop in ECE is more significant out-of-domain. Temp Scaling is the most compelling\nfine-tuning loss due to its largest reduction in ECE, preservation of prediction results, and little computational cost.\nens” the vanilla logit output with Ttemp to ob-\ntain a new predictive distribution.\n2. MC Dropout (Gal and Ghahramani, 2016)\napproximates the expectation of a posterior\npredictive distribution by averaging Tmc for-\nward passes with dropout turned on.\n3. Ensemble (Lakshminarayanan et al., 2017)\naverages the predictive distributions of Ten\nindependently trained models.\n4. LL SVI (Last-Layer Stochastic Variational\nInference) (Blundell et al., 2015) implements\nvariational layers with reparameterized Monte\nCarlo estimators based on the Bayesian-Torch\npackage (Krishnan et al., 2022). It approxi-\nmates the expectation of a posterior predictive\ndistribution by averaging Tsvi forward passes\nthrough the Bayesian classification layers.\nHere, we follow Lakshminarayanan et al. (2017) by\nsetting Ten = 5. We use Tmc = 10and Tsvi = 50\ndue to computational constraints during inference.\nThe dropout rate in MC Dropout is the same as the\ndefault dropout rate of each PLM.\n5.2 Empirical Findings\nIn Figure 3, we plot the change in calibration and\nprediction performance due to the use of uncer-\ntainty quantifiers compared to the vanilla results\nin Section 4.1. The improvement in calibra-\ntion is more significant out-of-domain. More\nspecifically, the degree to which these quantifiers\ndecrease ECE follows the descending order of\nTemp Scaling, MC Dropout, Ensemble, and LL\nSVI. In fact, LL SVI even hurts the calibration in\nterms of an increase in ECE, suggesting that varia-\ntional classifiers with reparameterized Monte Carlo\nestimators cannot capture uncertainties well when\nused only at the fine-tuning stage. Unlike Ovadia\net al. (2019), we find Ensemble less effective in\nPLM-based pipelines, possibly because individual\nlearners in Ensemble are initialized from the same\npre-trained model checkpoint and, consequently,\nthe strong correlation among them limits the power\nof Ensemble (Liu and Yao, 1999).\nMeanwhile, Temp Scaling preserves prediction\nresults, and Ensemble lowers prediction error, as ex-\npected. Although MC Dropout and LL SVI reduce\nthe prediction error out-of-domain in Common-\nsense Reasoning by producing sharper predictive\ndistributions, they usually end up being overconfi-\ndent, which leads to the rise in ECE in Figure 3(a).\nTemp Scaling is the most appropriate uncer-\ntainty quantifier for PLM-based pipelines. Com-\n7279\nIn-Domain:\nOut-Of-Domain:\nCross Entropy\nCross Entropy\nBrier Loss\nBrier Loss\nFocal Loss\nFocal Loss\nLabel Smoothing\nLabel Smoothing\nMMCE\nMMCE\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.0\n0.1\n0.2\n0.3\n0.4\nECE (↓)\nFocal Loss delivers competitively low ECE.\n(a) In-Domain and Out-Of-Domain Calibration of BERT-Base Fine-tuned with Diﬀerent Losses\nSentiment Analysis Natural Language Inference Commonsense Reasoning\nNLP Tasks\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFAR95 (↓)\nFocal Loss delivers\nthe lowest FAR95.\n(b) Out-Of-Domain Detection of BERT-Base Fine-tuned with Diﬀerent Losses\nFigure 4: Calibration and out-of-domain detection performance of BERT base models fine-tuned by five losses.\nFocal Loss, Label Smoothing, and MMCE are more capable of fine-tuning well-calibrated models compared to\nCross Entropy and Brier Loss. Focal Loss is the best option due to its competitively low ECE and FAR95.\npared to LL SVI, Temp Scaling diminishes ECE\nand maintains the competitive prediction quality of\nPLMs. Moreover, the post hoc recalibration man-\nner of Temp Scaling adds little to the computational\nburden. In contrast, Ensemble or MC Dropout sig-\nnificantly increases the computational cost during\nfine-tuning or inference, respectively. Note that\nthis distinction is of great importance given the\nenormous computational burdens of PLMs.\n6 Which Fine-tuning Loss?\n6.1 Experiment Setup\nBesides cross-entropy loss, we consider four other\nlosses when fine-tuning a BERT base model and\ncompare their calibration performance based on the\nsetup in Section 3.1.\n1. Cross Entropy (Good, 1952) is the negative\nlog likelihood of ground-truth classes.\n2. Brier Loss (Brier et al., 1950) is the squared\ndifference between predictive distributions\nand one-hot ground-truth vectors.\n3. Focal Loss (Mukhoti et al., 2020) applies a\nmodulating term to cross-entropy loss to focus\nmodel learning on hard misclassified samples.\n4. Label Smoothing (Müller et al., 2019) pro-\nduces targeting distributions by allocating\nprobability mass to non-ground-truth classes.\n5. MMCE (Maximum Mean Calibration Error)\n(Kumar et al., 2018) is a differentiable proxy\nto regularize calibration error, usually used\nalongside cross-entropy loss.\nWe use a smoothing factor of 0.1, and follow the\npractice in Mukhoti et al. (2020) by setting the focal\nhyperparameter to 5 when the predictive probabil-\nity for the ground-truth class ∈[0, 0.2) and to 3\nwhen the probability ∈[0.2, 1].\nIn addition, we leverage out-of-domain detection\nto implicitly examine the quality of uncertainty\nquantification. We want models to be less confident\non Dout than on Din and, hence, report the false\nalarm rate at 95% recall (FAR95) (Hendrycks et al.,\n2020). This metric tells the ratio of samples in Din\nwhose confidence is lower than the 95th percentile\nof samples in Dout.\n6.2 Empirical Findings\nAs shown in Figure 4(a), Label Smoothing, Fo-\ncal Loss, and MMCE generate better-calibrated\nBERT base models compared to Cross Entropy\nand Brier Loss. While models fine-tuned by Cross\nEntropy, Focal Loss, or MMCE calibrate better in-\ndomain, Brier Loss and Label Smoothing enjoy a\ndecrease in ECE when evaluated out-of-domain.\nThis observation matches the findings in Desai and\nDurrett (2020); Dan and Roth (2021) and is in-\n7280\ntuitive for Label Smoothing since it deliberately\nalleviates overconfidence during fine-tuning.\nFocal Loss is the most compelling fine-tuning\nloss for PLM-based pipelines. Among the five ex-\namined options, Focal Loss delivers competitively\nlow ECE, both in- and out-of-domain for all three\ntasks. Moreover, it scores the lowest in FAR95,\nas illustrated in Figure 4(b), meaning that models\nfine-tuned by Focal Loss are most alert to domain\nshift. We note that FAR95 scores are relatively\nhigh in Sentiment Analysis and Natural Language\nInference, probably because these pipelines also\npredict well out-of-domain in Figure 2(d).\n7 Conclusion\nIn this paper, we contribute a comprehensive anal-\nysis on how to reduce calibration error in a PLM-\nbased pipeline. We establish four key consider-\nations behind the pipeline and compare a broad\nrange of prevalent options for each consideration.\nOur empirical evaluations consist of three distinct\nNLP classification tasks and two different domain\nsettings. Based on our large-scale systematic anal-\nysis, we recommend the following:\n1. Use ELECTRA for PLM encoding.\n2. Use larger PLMs if possible.\n3. Use Temp Scaling for post hoc recalibration.\n4. Use Focal Loss during the fine-tuning stage.\nCompared to existing work, we also observe the fol-\nlowing novel phenomena that are unique to PLM-\nbased pipelines:\n• The relative calibration quality of PLMs is\nconsistent in general across tasks and domains,\nwith an exception of XLNet, which is the least\nrobust to domain shift.\n• Larger PLMs are better calibrated under the in-\ndomain setting in Commonsense Reasoning,\nunlike in the other NLP tasks.\n• Uncertainty quantifiers are generally more ef-\nfective in improving calibration performance\nunder the out-of-domain setting.\n• Ensemble is less effective in reducing cal-\nibration error when used with PLM-based\npipelines, despite their convincing perfor-\nmance with traditional models.\n8 Limitation\nDue to computational constraints, we are unable\nto pre-train PLMs from scratch with other combi-\nnations of pre-training corpora and tasks. Conse-\nquently, while our analysis is applicable to existing\nwidely-used PLMs, we do not claim its generaliza-\ntion to new combinations of pre-training corpora\nand tasks. We believe that this does not invalidate\nour claims which are primarily targeted toward\nreal-world practitioners using existing PLMs. It\nis possible that techniques catering to the special\nneeds of PLM-based pipelines (Kong et al., 2020)\ncan mitigate calibration error further.\nMoreover, although our setup involves domain\nshift, we do not focus on inspecting how the degree\nof domain shift affects the calibration performance\nof PLM-based pipelines. It is also interesting to\nconsider how to construct a well-calibrated PLM-\nbased pipeline for other types of NLP tasks such\nas cross-lingual text classification and generation,\nwhich we leave to future work.\nAcknowledgements\nThis material is based upon work partially sup-\nported by National Science Foundation (Awards\n#1722822 and #1750439) and National In-\nstitutes of Health (Awards #R01MH125740,\n#R01MH096951, and #U01MH116925). PPL is\npartially supported by a Facebook PhD Fellow-\nship and a Carnegie Mellon University’s Center\nfor Machine Learning and Health Fellowship. Any\nopinions, findings, conclusions, or recommenda-\ntions expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views\nof the sponsors, and no official endorsement should\nbe inferred.\nReferences\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana\nRezazadegan, Li Liu, Mohammad Ghavamzadeh,\nPaul Fieguth, Xiaochun Cao, Abbas Khosravi, U Ra-\njendra Acharya, et al. 2021. A review of uncertainty\nquantification in deep learning: Techniques, applica-\ntions and challenges. Information Fusion.\nAhmed Alaa and Mihaela Van Der Schaar. 2020a. Dis-\ncriminative jackknife: Quantifying uncertainty in\ndeep learning via higher-order influence functions.\nIn ICML.\nAhmed Alaa and Mihaela Van Der Schaar. 2020b. Fre-\nquentist uncertainty in recurrent neural networks via\nblockwise influence functions. In ICML.\nJavier Antoran, Umang Bhatt, Tameem Adel, Adrian\nWeller, and José Miguel Hernández-Lobato. 2021.\nGetting a CLUE: A method for explaining uncer-\ntainty estimates. In ICLR.\n7281\nYusuf Arslan, Kevin Allix, Lisa Veiber, Cedric Lothritz,\nTegawendé F Bissyandé, Jacques Klein, and Anne\nGoujon. 2021. A comparison of pre-trained language\nmodels for multi-class text classification in the finan-\ncial domain. In WWW Companion.\nUmang Bhatt, Javier Antorán, Yunfeng Zhang, Q Vera\nLiao, Prasanna Sattigeri, Riccardo Fogliato, Gabrielle\nMelançon, Ranganath Krishnan, Jason Stanley,\nOmesh Tickoo, et al. 2021. Uncertainty as a form of\ntransparency: Measuring, communicating, and using\nuncertainty. In AIES.\nCharles Blundell, Julien Cornebise, Koray\nKavukcuoglu, and Daan Wierstra. 2015. Weight\nuncertainty in neural network. In ICML.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nEMNLP.\nGlenn W Brier et al. 1950. Verification of forecasts\nexpressed in terms of probability. Monthly weather\nreview.\nYoungseog Chung, Willie Neiswanger, Ian Char, and\nJeff Schneider. 2021. Beyond pinball loss: Quantile\nmethods for calibrated uncertainty quantification. In\nNeurIPS.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nPeng Cui, Wenbo Hu, and Jun Zhu. 2020. Calibrated\nreliable regression using maximum mean discrepancy.\nIn NeurIPS.\nSoham Dan and Dan Roth. 2021. On the effects of\ntransformer size on in-and out-of-domain calibration.\nIn EMNLP (Findings).\nStefan Depeweg, Jose-Miguel Hernandez-Lobato, Fi-\nnale Doshi-Velez, and Steffen Udluft. 2018. Decom-\nposition of uncertainty in bayesian deep learning for\nefficient and risk-sensitive learning. In ICML.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout\nas a bayesian approximation: Representing model\nuncertainty in deep learning. In ICML.\nSiddhant Garg, Thuy Vu, and Alessandro Moschitti.\n2020. Tanda: Transfer and adapt pre-trained trans-\nformer models for answer sentence selection. In\nAAAI.\nI.J. Good. 1952. Rational decisions. Journal of the\nRoyal Statistical Society. Series B (Methodological).\nPriya Goyal, Piotr Dollár, Ross Girshick, Pieter No-\nordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-\ncurate, large minibatch sgd: Training imagenet in 1\nhour. arXiv preprint arXiv:1706.02677.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In ICML.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. In ICLR.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In ACL.\nAshish Jaiswal, Ashwin Ramesh Babu, Moham-\nmad Zaki Zadeh, Debapriya Banerjee, and Fillia\nMakedon. 2021. A survey on contrastive self-\nsupervised learning. Technologies.\nXiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila\nOhno-Machado. 2012. Calibrating predictive model\nestimates to support personalized medicine. Journal\nof the American Medical Informatics Association.\nAlex Kendall and Yarin Gal. 2017. What uncertainties\ndo we need in bayesian deep learning for computer\nvision? In NeurIPS.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model fine-tuning for in-and out-\nof-distribution data. In EMNLP.\nRanganath Krishnan, Pi Esposito, and Mahesh Subedar.\n2022. Bayesian-torch: Bayesian neural network lay-\ners for uncertainty estimation. https://github.\ncom/IntelLabs/bayesian-torch.\nRanganath Krishnan and Omesh Tickoo. 2020. Improv-\ning model calibration with accuracy versus uncer-\ntainty optimization. In NeurIPS.\nV olodymyr Kuleshov, Nathan Fenner, and Stefano Er-\nmon. 2018. Accurate uncertainties for deep learning\nusing calibrated regression. In ICML.\nAviral Kumar, Sunita Sarawagi, and Ujjwal Jain. 2018.\nTrainable calibration measures for neural networks\nfrom kernel mean embeddings. In International Con-\nference on Machine Learning.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles.\nIn NeurIPS.\nDan Ley, Umang Bhatt, and Adrian Weller. 2022. Di-\nverse, global and amortised counterfactual explana-\ntions for uncertainty estimates. In AAAI.\n7282\nNut Limsopatham. 2021. Effectively leveraging bert for\nlegal document classification. In EMNLP Workshop.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYong Liu and Xin Yao. 1999. Ensemble learning via\nnegative correlation. Neural networks.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In ICLR.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nACL.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and Dan Roth. 2021. Re-\ncent advances in natural language processing via\nlarge pre-trained language models: A survey. arXiv\npreprint arXiv:2111.01243.\nMatthias Minderer, Josip Djolonga, Rob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin\nTran, and Mario Lucic. 2021. Revisiting the calibra-\ntion of modern neural networks. In NeurIPS.\nJishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stu-\nart Golodetz, Philip Torr, and Puneet Dokania. 2020.\nCalibrating deep neural networks using focal loss. In\nNeurIPS.\nRafael Müller, Simon Kornblith, and Geoffrey E Hinton.\n2019. When does label smoothing help? In NeurIPS.\nMyle Ott, Michael Auli, David Grangier, and\nMarc’Aurelio Ranzato. 2018. Analyzing uncertainty\nin neural machine translation. In ICML.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,\nDavid Sculley, Sebastian Nowozin, Joshua Dillon,\nBalaji Lakshminarayanan, and Jasper Snoek. 2019.\nCan you trust your model’s uncertainty? evaluat-\ning predictive uncertainty under dataset shift. In\nNeurIPS.\nYao Qin, Xuezhi Wang, Alex Beutel, and Ed Chi. 2021.\nImproving calibration through the relationship with\nadversarial robustness. In NeurIPS.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences.\nAdrian E Raftery, Tilmann Gneiting, Fadoua Bal-\nabdaoui, and Michael Polakowski. 2005. Using\nbayesian model averaging to calibrate forecast en-\nsembles. Monthly weather review.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad. In ACL.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nNeeraj Varshney, Swaroop Mishra, and Chitta Baral.\n2022a. Investigating selective prediction approaches\nacross several tasks in iid, ood, and adversarial set-\ntings. In ACL (Findings).\nNeeraj Varshney, Swaroop Mishra, and Chitta Baral.\n2022b. Towards improving selective prediction abil-\nity of nlp systems. In ACL Workshop.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nYoav Wald, Amir Feder, Daniel Greenfeld, and Uri\nShalit. 2021. On calibration and out-of-domain gen-\neralization. In NeurIPS.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019a. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019b.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In ICLR.\nXiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng,\nLuxi Xing, and Weihua Luo. 2020. Uncertainty-\naware semantic augmentation for neural machine\ntranslation. In EMNLP.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In EMNLP.\nYuxin Xiao, Zecheng Zhang, Yuning Mao, Carl Yang,\nand Jiawei Han. 2022. Sais: Supervising and aug-\nmenting intermediate steps for document-level rela-\ntion extraction. In NAACL.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. The art of abstention: Selective prediction and\nerror regularization for natural language processing.\nIn ACL.\nHuiqin Yang and Carl Thompson. 2010. Nurses’ risk\nassessment judgements: A confidence calibration\nstudy. Journal of Advanced Nursing.\n7283\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS.\nWonjin Yoon, Jinhyuk Lee, Donghyeon Kim, Minbyul\nJeong, and Jaewoo Kang. 2019. Pre-trained language\nmodel for biomedical question answering. In ECML-\nPKDD.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In EMNLP.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In ACL.\nTaolin Zhang, Zerui Cai, Chengyu Wang, Minghui\nQiu, Bite Yang, and Xiaofeng He. 2021. Smedbert:\nA knowledge-enhanced pre-trained language model\nwith structured semantics for medical text mining. In\nACL.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NeurIPS.\nWenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing\nHuang. 2021. Document-level relation extraction\nwith adaptive thresholding and localized context pool-\ning. In AAAI.\nYikai Zhou, Baosong Yang, Derek F Wong, Yu Wan,\nand Lidia S Chao. 2020. Uncertainty-aware curricu-\nlum learning for neural machine translation. In ACL.\nA Responsible NLP Research\nIn this paper, we aim to identify the best choice for\neach consideration in constructing a well-calibrated\nPLM-based pipeline via extensive empirical stud-\nies. Our empirical analysis involves training multi-\nple large-scale PLMs and, consequently, consumes\na fair amount of computational power. However,\nwe believe that the takeaways from our analysis\nwill benefit NLP practitioners at large, which will\nwrite off the computational cost in the future.\nIn particular, the Hugging Face package lever-\naged in our experiments utilizes the Apache Li-\ncense 2.0, and the Bayesian-Torch package utilizes\nthe BSD 3-Clause License. We focus on PLM-\nbased pipelines targeting English and assess them\nbased on six NLP datasets, which aligns with the\nintended use of these datasets. We also release the\nevaluation benchmarks of our empirical analysis to\nillustrate the performance of different PLM-based\npipelines based on diverse metrics. The bench-\nmarks do not contain information that uniquely\nidentifies individual people or offensive content.\n7284",
  "topic": "Pipeline (software)",
  "concepts": [
    {
      "name": "Pipeline (software)",
      "score": 0.8303729891777039
    },
    {
      "name": "Computer science",
      "score": 0.7903714179992676
    },
    {
      "name": "Natural language processing",
      "score": 0.5373924970626831
    },
    {
      "name": "Scope (computer science)",
      "score": 0.5317954421043396
    },
    {
      "name": "Popularity",
      "score": 0.5251983404159546
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5231279730796814
    },
    {
      "name": "Machine learning",
      "score": 0.5073315501213074
    },
    {
      "name": "Quantifier (linguistics)",
      "score": 0.4860556423664093
    },
    {
      "name": "Scaling",
      "score": 0.4860230088233948
    },
    {
      "name": "Language model",
      "score": 0.4267466068267822
    },
    {
      "name": "Natural language",
      "score": 0.4163011312484741
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.41443589329719543
    },
    {
      "name": "Engineering",
      "score": 0.08181962370872498
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 24
}