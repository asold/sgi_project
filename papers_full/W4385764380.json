{
  "title": "Graph Propagation Transformer for Graph Representation Learning",
  "url": "https://openalex.org/W4385764380",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100457678",
      "name": "Zhe Chen",
      "affiliations": [
        "Novel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100586721",
      "name": "Hao Tan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100453403",
      "name": "Tao Wang",
      "affiliations": [
        "Novel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5036527272",
      "name": "Tianrun Shen",
      "affiliations": [
        "Novel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100679289",
      "name": "Tong Lu",
      "affiliations": [
        "Novel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5089821278",
      "name": "Qiuying Peng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100349426",
      "name": "Cheng Cheng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100747624",
      "name": "Yue Qi",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285029217",
    "https://openalex.org/W3155952169",
    "https://openalex.org/W4287829537",
    "https://openalex.org/W4287027946",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4221142179",
    "https://openalex.org/W2998496395",
    "https://openalex.org/W4281706128",
    "https://openalex.org/W4310921506",
    "https://openalex.org/W3204160375",
    "https://openalex.org/W4280496682",
    "https://openalex.org/W4361866028",
    "https://openalex.org/W3016124664",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963695795",
    "https://openalex.org/W4386076222",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3100078588",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W3136399186",
    "https://openalex.org/W4225646704",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2790814121",
    "https://openalex.org/W3035649237",
    "https://openalex.org/W637153065",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2987178699",
    "https://openalex.org/W2154851992",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W4309395888",
    "https://openalex.org/W2768242641",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W4287586022",
    "https://openalex.org/W4226016740",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4284896159",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W4297733535"
  ],
  "abstract": "This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.",
  "full_text": "Graph Propagation Transformer for Graph Representation Learning\nZhe Chen1 , Hao Tan2 , Tao Wang1 , Tianrun Shen1 , Tong Lu1\nQiuying Peng2 , Cheng Cheng2 and Yue Qi2\n1State Key Lab for Novel Software Technology, Nanjing Univerisity\n2OPPO Research Institute\nchenzhe98@smail.nju.edu.cn, lutong@nju.edu.com\nAbstract\nThis paper presents a novel transformer architec-\nture for graph representation learning. The core\ninsight of our method is to fully consider the\ninformation propagation among nodes and edges\nin a graph when building the attention module\nin the transformer blocks. Specifically, we pro-\npose a new attention mechanism called Graph\nPropagation Attention (GPA). It explicitly passes\nthe information among nodes and edges in three\nways, i.e., node-to-node, node-to-edge, and edge-\nto-node, which is essential for learning graph-\nstructured data. On this basis, we design an effec-\ntive transformer architecture named Graph Propa-\ngation Transformer (GPTrans) to further help learn\ngraph data. We verify the performance of GP-\nTrans in a wide range of graph learning experi-\nments on several benchmark datasets. These results\nshow that our method outperforms many state-of-\nthe-art transformer-based graph models with better\nperformance. The code will be released at https:\n//github.com/czczup/GPTrans.\n1 Introduction\nIn many real-world scenarios, information is usually orga-\nnized by graphs, and graph-structured data can be used in\nmany research areas, including communication networks and\nmolecular property prediction, etc. For instance, based on so-\ncial graphs, lots of algorithms are proposed to classify users\ninto meaningful social groups in the task of social network\nresearch, which can produce many useful practical applica-\ntions such as user search and recommendations. Therefore,\ngraph representation learning has become a hot topic in pat-\ntern recognition and machine learning [Cai and Lam, 2020;\nYing et al., 2021; Brossard et al., 2020].\nWith the development of deep learning, many methods\nhave been developed for graph representation learning [Per-\nozzi et al., 2014; Zhang et al., 2019; Ying et al., 2021;\nHussain et al., 2021; Ramp ¬¥aÀásek et al., 2022 ]. In general,\nthese methods can be approximately divided into two parts.\nThe first category mainly focuses on performing Graph Neu-\nral Networks (GNNs) on graph data. These methods follow\nthe convolutional pattern to define the convolution operation\n(a) Node-to-Node (b) Node-to-Edge (c) Edge-to-Node\nNode Edge Node Embedding Edge Embedding\nFigure 1: Illustration of the three ways for graph information propa-\ngation. Circles and black lines indicate nodes and edges, and green\nand pink cubes represent node embeddings and edge embeddings.\nOur GPTrans achieves better graph representation learning by ex-\nplicitly constructing three ways for information propagation in the\nproposed Graph Propagation Attention (GPA) module, including (a)\nnode-to-node, (b) node-to-edge, and (c) edge-to-node.\nin the graph data, and design effective neighborhood aggrega-\ntion schemes to learn node representations by fusing the node\nand graph topology information. The representative method\nis Graph Convolutional Network (GCN) [Kipf and Welling,\n2016], which learns the representation of a node in the graph\nby considering fusing its neighbors. After that, many GCN\nvariants [Xu et al., 2018; Chen et al., 2020; Liu et al., 2021a;\nBresson and Laurent, 2017 ] containing different neighbor-\nhood aggregation schemes have been developed. The second\nkind of method is to build graph models based on the trans-\nformer architecture. For example, Cai and Lam [2020] uti-\nlized the explicit relation encoding between nodes and fused\nthem into the encoder-decoder transformer network for ef-\nfective graph-to-sequence learning. Graphormer [Ying et al.,\n2021] established state-of-the-art performance on graph-level\nprediction tasks by transforming the structure and edge fea-\ntures of the graph into attention biases.\nAlthough recent transformer-based methods report promis-\ning performance for graph representation learning, they still\nsuffer the following problems. (1) Not explicitly employ the\nrelationship among nodes and edges in the graph data. Re-\ncent transformer-based methods [Cai and Lam, 2020; Ying\net al., 2021 ] only simply fuse nodes and edges information\nby using positional encodings. However, due to the complex-\nity of graph structure, how to fully employ the relationship\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3559\namong nodes and edges for graph representation learning re-\nmains to be studied. (2) Inefficient dual-FFN structure in the\ntransformer block. Recent works resort to the dual-path struc-\nture in the transformer block to incorporate the edge informa-\ntion. For instance, the Edge-augmented Graph Transformer\n(EGT) [Hussain et al., 2021] adopted dual feed-forward net-\nworks (FFN) in the transformer block to update the edge em-\nbeddings, and let the structural information evolve from layer\nto layer. However, this paradigm learns the information of\nedges and nodes separately, which introduces more calcula-\ntions and easily leads to the low efficiency of the model.\nTo overcome these issues, we propose an efficient and\npowerful transformer architecture for graph learning, termed\nGraph Propagation Transformer (GPTrans). A key design el-\nement of GPTrans is its Graph Propagation Attention (GPA)\nmodule. As illustrated in Figure 1, the GPA module propa-\ngates the information among the node embeddings and edge\nembeddings of the preceding layer by modeling three con-\nnections, i.e., node-to-node, node-to-edge, and edge-to-node,\nwhich significantly enhances modeling capability (see Ta-\nble 1). This design benefits us no longer the need to maintain\nan FFN module specifically for edge embeddings, bringing\nhigher efficiency than previous dual-FFN methods.\nThe contributions of our work are as follows:\n(1) We propose an effective Graph Propagation Trans-\nformer (GPTrans), which can better model the relationship\namong nodes and edges and represent the graph.\n(2) We introduce a novel attention mechanism in the trans-\nformer blocks, which explicitly passes the information among\nnodes and edges in three ways. These relationships play a\ncritical role in graph representation learning.\n(3) Extensive experiments show that the proposed GPTrans\nmodel outperforms many state-of-the-art transformer-based\nmethods on benchmark datasets with better performance.\n2 Related Works\n2.1 Transformer\nThe past few years have seen many transformer-based mod-\nels designed for various language [Vaswani et al., 2017;\nRadford et al., 2019; Brown et al., 2020 ] and vision tasks\n[Parmar et al., 2018; Liu et al., 2021b]. For example, in the\nfield of vision, Dosovitskiy et al. [2021] presented the Vi-\nsion Transformer (ViT), which decomposed an image into\na sequence of patches and captured their mutual relation-\nships. However, training ViT on large-scale datasets can\nbe computationally expensive. To address this issue, DeiT\n[Touvron et al., 2021 ] proposed an efficient training strat-\negy that enabled ViT to deliver exceptional performance even\nwhen trained on smaller datasets. Nevertheless, the complex-\nity and performance of ViT remain challenging. To over-\ncome these limitations, researchers further proposed many\nwell-designed models [Liu et al., 2021b; Wang et al., 2021;\nWang et al., 2022a; Chen et al., 2023; Ji et al., 2023;\nChen et al., 2022; Wang et al., 2022b].\nRecently, the self-attention mechanism and transformer\narchitecture have been gradually introduced into the graph\nrepresentation learning tasks, such as graph-level prediction\n[Ying et al., 2021; Hussain et al., 2021], producing compet-\nitive performance compared to the traditional GNN models.\nThe early self-attention-based GNNs focused on adopting the\nattention mechanism to a local neighborhood of each node\nin a graph, or directly on the whole graph. For example,\nGraph Attention Network (GAT)[VeliÀáckovi¬¥c et al., 2017] and\nGraph Transformer (GT) [Dwivedi and Bresson, 2020 ] uti-\nlized self-attention mechanisms as local constraints for the\nlocal neighborhood of each node. In contrast to employing\nlocal self-attention for graph learning, Graph-BERT [Zhang\net al., 2020 ] introduced the global self-attention mechanism\nin a revised transformer network to predict one masked node\nin a sampled subgraph.\nIn addition, several works have attempted to use the trans-\nformer architecture to tackle graph-related tasks directly. Two\nnotable examples are [Cai and Lam, 2020 ] and Graphormer\n[Ying et al., 2021 ]. The former method adopted explicit\nrelation encoding between nodes and integrated them into\nthe encoder-decoder transformer network, to enable graph-\nto-sequence learning. The latter mainly regarded the structure\nand edges of the graph as the attention biases, which were in-\ncorporated into the transformer block. With the help of these\nattention biases, Graphormer achieved leading performance\non graph-level prediction tasks (e.g.,classification and regres-\nsion on molecular graphs).\n2.2 Graph Convolutional Network\nGraph Convolutional Network (GCN) is a kind of deep neu-\nral network that extends the CNN from grid data (e.g., im-\nage and video) to graph-structured data. Generally speak-\ning, GCN methods can be approximately divided into two\ntypes: spectral-based methods [Bruna et al., 2013; Defferrard\net al., 2016; Henaff et al., 2015; Kipf and Welling, 2016] and\nnon-spectral methods [Chen et al., 2018; Gilmer et al., 2017;\nScarselli et al., 2008; VeliÀáckovi¬¥c et al., 2017].\nSpectral GCN methods are designed under the theory of\nspectral graphs. For instance, Spectral GCN [Bruna et al.,\n2013] resorted to the Fourier basis of a graph to conduct con-\nvolution operation in the spectral domain, which is the first\nwork on spectral graph CNNs. Based on [Bruna et al., 2013],\nDefferrard et al. [2016] designed a strict control over the lo-\ncal support of filters and avoided an explicit use of the Graph\nFourier basis in the convolution, which achieved better accu-\nracy. Kipf and Welling[2016] adopted the first-order approx-\nimation of the spectral graph convolution to simplify com-\nmonly used GNN.\nOn the other hand, non-spectral methods directly define\nconvolution operations on the graph data. GraphSage[Hamil-\nton et al., 2017 ] proposed learnable aggregator functions in\nthe network to fuse neighbors‚Äô information for effective graph\nrepresentation learning. In GAT[VeliÀáckovi¬¥c et al., 2017], dif-\nferent weight matrices are used for nodes with different de-\ngrees for graph representation learning. In addition, another\nline of GCN methods is mainly designed for specific graph-\nlevel tasks. For example, some techniques such as subsam-\npling [Chen et al., 2017 ] and inductive representation for a\nlarge graph [Hamilton et al., 2017] have been introduced for\nbetter graph representative learning.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3560\nHead\nTransformer BlocksGraph\nGraph PropagationAttentionGraphEmbeddingùë•!\"#$\nùë•$#%$\n√óùêøFeed-ForwardNetwork\nFigure 2: Overall architecture of GPTrans. It contains a graph embedding layer,L transformer blocks, and a head. The graph embedding layer\ntransforms the graph data into node embeddings xnode and edge embeddings xedge, as the input of the transformer blocks. Each transformer\nblock comprises a Graph Propagation Attention (GPA) and a Feed-Forward Network (FFN). It is worth noting that we no longer need to\nmaintain an FFN module specifically for edge embeddings due to the proposed GPA module, which improves the efficiency of our method.\nFinally, a head of two fully-connected layers is employed on the output embeddings for various graph tasks.\n3 GPTrans\n3.1 Overall Architecture\nAn overview of the proposed GPTrans framework is depicted\nin Figure 2. Specifically, it takes a graphG = (V, E) as input,\nin which nodes V = {v1, v2, . . . , vn}, E indicates edges be-\ntween nodes, and n means the number of nodes. The pipeline\nof GPTrans can be divided into three parts: graph embedding,\ntransformer blocks, and prediction head.\nIn the graph embedding layer, for each given graph G, we\nfollow [Ying et al., 2021; Hussainet al., 2021] to add a virtual\nnode [v0] into V , to aggregate the information of the entire\ngraph. Thus, the newly-generated node set with the virtual\nnode is represented as V ‚Ä≤ = {[v0], v1, v2, . . . , vn}, and the\nnumber of nodes is updated to |V ‚Ä≤| = 1 +n. For more ad-\nequate information propagation across the whole graph, each\nnode and edge is treated as a token. In detail, we trans-\nform the input nodes into a sequence of node embeddings\nxnode ‚àà R(1+n)√ód1 , and encode the input edges into a tensor\nof edge embeddings xedge ‚àà R(1+n)√ó(1+n)√ód2 .\nThen, L transformer blocks with our re-designed self-\nattention operation (i.e., Graph Propagation Attention) are ap-\nplied to node embeddings and edge embeddings. Both these\nembeddings are fed throughout all transformer blocks. After\nthat, GPTrans generates the representation of each node and\nedge, in which the output embedding of the virtual node takes\nalong the representation of the whole graph.\nFinally, the head of our GPTrans is composed of two fully-\nconnected (FC) layers. For graph-level tasks, we employ it\non top of the output embedding of the virtual node. For node-\nlevel or edge(link)-level tasks, we apply it to the output node\nembeddings or edge embeddings. In summary, benefiting\nfrom the proposed novel Graph Propagation Attention, our\nGPTrans can better support various graph tasks with only a\nlittle additional computational cost compared to the previous\nmethod Graphormer [Ying et al., 2021].\n3.2 Graph Embedding\nThe role of the graph embedding layer is to transform the\ngraph data as the input of transformer blocks. As we know,\nin addition to the nodes, edges also have rich structural infor-\nmation in many types of graphs,e.g., molecular graphs [Hu et\nal., 2021] and social graphs [Huang et al., 2022]. Therefore,\nwe encode both nodes and edges into embeddings to fully\nutilize the structure of the input graph.\nFor nodes in the graph, we transform each node into node\nembedding. Specifically, we follow [Ying et al. , 2021 ] to\nexploit the node attributes and the degree information, and\nadd a virtual node [v0] into the graph to collect and prop-\nagate graph-level features. Without loss of generality, tak-\ning a directed graph as an example, its node embeddings\nxnode ‚àà R(1+n)√ód1 can be expressed as:\nxnode = xnode attr + xdeg‚àí + xdeg+ , (1)\nwhere xnode attr, xdeg‚àí, and xdeg+ are embeddings encoded\nfrom node attributes, indegree, and outdegree statistics, re-\nspectively. d1 is the dimension of node embeddings.\nFor edges in the graph, we also transform them into\nedge embeddings to help the learning of graph representa-\ntion. In our implementation, the edge embeddings xedge ‚àà\nR(1+n)√ó(1+n)√ód2 are defined as:\nxedge = xedge attr + xrel pos, (2)\nwhere xedge attr is encoded from the edge attributes, and\nxrel pos is a relative positional encoding that embeds the spa-\ntial location of node pairs. d2 means the dimension of edge\nembeddings. We adopt the encoding of the shortest path dis-\ntance by default following [Ying et al., 2021]. In other words,\nfor the position (i, j), xij\nedge ‚àà Rd2 represents the learned\nstructural embedding of the edge (path) between node vi and\nnode vj in the graph G.\nIt is worth noting that, unlike Graphormer [Ying et al. ,\n2021] that encodes edge attributes and spatial position as at-\ntention biases and shares them across all blocks, we optimize\nthe edge embeddings xedge in each transformer block by the\nproposed Graph Propagation Attention. Then, the updated\nedge embeddings are fed into the next block. Therefore, each\nblock of our model could adaptively learn different ways to\nexploit edge features and propagate information. This more\nflexible way is beneficial for graph representation learning,\nwhich we will show in later experiments.\n3.3 Graph Propagation Attention\nIn recent years, many works [Ying et al. , 2021; Shi et al. ,\n2022; Hussain et al., 2021] show global self-attention could\nserve as a flexible alternative to graph convolution and help\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3561\nNode Embeddings Edge Embeddings\nùëä!ùëä\" ùëä#\nProduct & Scale\nùëä$%&'(%\nSoftmax\nùëä%)*+,&\nSoftmax\nSum & FC ùëä-\n(c) \n(b)\n(a)\nùëÑ ùêæ ùëâ ‚àÖ\nùê¥\nùë•,-&%\n.\nùë•%&/%\n.\nùë•,-&%\n.. ùë•,-&%\n... ùë•%&/%\n.\nFigure 3: Illustration of Graph Propagation Attention. It explicitly\nbuilds three paths for information propagation among node embed-\ndings and edge embeddings, including (a) node-to-node, (b) node-\nto-edge, and (c) edge-to-node.\nbetter graph representation learning. However, most of them\nonly consider part of the information propagation paths in\ngraph, or introduce a lot of extra computational overhead to\nutilize edge information. For instance, Graphormer [Ying\net al., 2021 ] only used edge features as shared bias terms\nto refine the attention weights of nodes. GT [Dwivedi and\nBresson, 2020 ] and EGT [Hussain et al., 2021 ] designed\ndual-FFN networks to fuse edge features. Inspired by this,\nwe introduce Graph Propagation Attention (GPA), as an ef-\nficient replacement for vanilla self-attention in graph trans-\nformers. With an affordable cost, it could support three types\nof propagation paths, including node-to-node, node-to-edge,\nand edge-to-node. For simplicity of description, we consider\nsingle-head self-attention in the following formulas.\nNode-to-Node\nFollowing common practices [Ying et al., 2021; Shi et al.,\n2022; Hussain et al., 2021 ], we adopt global self-attention\n[Vaswani et al., 2017 ] to perform node-to-node propaga-\ntion. First, we use parameter matrices WQ, WK, and WV ‚àà\nRd1√ód1 to project the node embeddings xnode to queries Q,\nkeys K, and values V :\nQ = xnodeWQ, K = xnodeWK, V = xnodeWV. (3)\nUnlike Graphormer [Ying et al., 2021 ] that used shared at-\ntention biases in all blocks, we employ a parameter matrix\nWreduce ‚àà Rd2√ónhead to predict layer-specific attention bi-\nases œï ‚àà R(1+n)√ó(1+n)√ónhead from the edge embeddings\nxedge, which can be written as:\nœï = xedgeWreduce. (4)\nThen, we add œï to the attention map of the query-key dot\nproduct and compute the output node embeddings x‚Ä≤\nnode.\nThis process can be formulated as:\nA = QKT\n‚àödhead\n+ œï, x ‚Ä≤\nnode = softmax(A)V, (5)\nwhere A ‚àà R(1+n)√ó(1+n)√ónhead represents the output atten-\ntion map, and dhead refers to the dimension of each head. In\nsummary, sinceœï are projected from higher dimensional edge\nembeddings by the learnable matrix, our attention mapA will\nhave more flexible patterns to aggregate node features.\nNode-to-Edge\nTo propagate node features to edges, we make some addi-\ntional use of the attention map A. According to the defini-\ntion of self-attention [Vaswani et al., 2017], attention map A\ncaptures the similarity between node embeddings. Therefore,\nconsidering both local and global connections, we addA with\nits softmax confidence, and expand its dimension to as same\nas xedge through the learnable matrix Wexpand ‚àà Rnhead√ód2 .\nThis operation is designed to perform explicit high-order spa-\ntial interactions, which can be written as follows:\nx‚Ä≤\nedge = (A + softmax(A))Wexpand. (6)\nIn this way, we achieve node-to-edge propagation without re-\nlying on an additional FFN module like GT [Dwivedi and\nBresson, 2020] and EGT [Hussain et al., 2021].\nEdge-to-Node\nIn this part, we delve into the following question: How to\ngenerate dynamic weights for edge embeddings xedge and\nfuse them into node embeddings xnode? Due to the compu-\ntational efficiency, we do not additionally perform attention\noperation, but directly apply the softmax function to the just\ngenerated x‚Ä≤\nedge ‚àà R(1+n)√ó(1+n)√ód2 in Eqn. 6 and calculate\nelement-wise product with itself:\nx‚Ä≤‚Ä≤\nnode = FC(sum(x‚Ä≤\nedge ¬∑ softmax(x‚Ä≤\nedge), dim = 1)), (7)\nin which the fully-connected (FC) layer is used to align the\ndimension of edge embeddings and node embeddings. This\nprocess again explicitly introduces high-order spatial interac-\ntions. Finally, we add these two types of node embeddings,\nand employ a learnable matrix WO ‚àà Rd1√ód1 to fuse them.\nThen we have the updated node embeddings:\nx‚Ä≤‚Ä≤‚Ä≤\nnode = (x‚Ä≤\nnode + x‚Ä≤‚Ä≤\nnode)WO. (8)\nGPA in Transformer Blocks\nEquipped with our proposed GPA module, the block of our\nGPTrans can be calculated as follows:\nÀÜxl\nnode, xl\nedge += GPA(LN(xl‚àí1\nnode), xl‚àí1\nedge), (9)\nxl\nnode = FFN(LN(ÀÜxl\nnode)) + ÀÜxl\nnode, (10)\nwhere LN(¬∑) means layer normalization [Ba et al., 2016 ].\nÀÜxl\nnode and xl\nedge denote the output node embeddings and edge\nembeddings of the GPA module for blockl. And xl\nnode repre-\nsents the output node embeddings of the FFN module. Over-\nall, our GPA module effectively extends the ability of our\nGPTrans to various graph tasks, but only introduces a small\namount of extra overhead compared with previous methods\n[Hussain et al., 2021; Ying et al., 2021].\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3562\nPCQM4M‚Üì PCQM4Mv2‚Üì\nModel #Param Validate Test Validate Test-dev\nNon-transformer-based Methods\nGCN 2.0M 0.1684 0.1838 0.1379 0.1398\nGIN 3.8M 0.1536 0.1678 0.1195 0.1218\nGCN-VN 4.9M 0.1510 0.1579 0.1153 0.1152\nGIN-VN 6.7M 0.1396 0.1487 0.1083 0.1084\nGINE-VN 13.2M 0.1430 ‚àí ‚àí ‚àí\nDeeperGCN-VN 25.5M 0.1398 ‚àí ‚àí ‚àí\nTransformer-based Methods\nGPS-Small 6.2M ‚àí ‚àí 0.0938 ‚àí\nGPTrans-T (ours) 6.6M 0.1179 ‚àí 0.0833 ‚àí\nGraphormer-S 12.5M 0.1264 ‚àí 0.0910 ‚àí\nEGT-Small 11.5M 0.1260 ‚àí 0.0899 ‚àí\nGPS-Medium 19.4M ‚àí ‚àí 0.0858 ‚àí\nGPTrans-S (ours) 13.6M 0.1162 ‚àí 0.0823 ‚àí\nTokenGT 48.5M ‚àí ‚àí 0.0910 ‚àí\nGraphormer-B 47.1M 0.1234 ‚àí 0.0906 ‚àí\nGRPE-Standard 46.2M 0.1225 ‚àí 0.0890 0.0898\nEGT-Medium 47.4M 0.1224 ‚àí 0.0881 ‚àí\nGPTrans-B (ours) 45.7M 0.1153 ‚àí 0.0813 ‚àí\nGT-Wide 83.2M 0.1408 ‚àí ‚àí ‚àí\nGraphormerV2-L 159.3M 0.1228 ‚àí 0.0883 ‚àí\nEGT-Large 89.3M ‚àí ‚àí 0.0869 0.0872\nEGT-Larger 110.8M ‚àí ‚àí 0.0859 ‚àí\nGRPE-Large 118.3M ‚àí ‚àí 0.0867 0.0876\nGPS-Deep 138.1M ‚àí ‚àí 0.0852 0.0862\nGPTrans-L (ours) 86.0M 0.1151 ‚àí 0.0809 0.0821\nTable 1: Results on PCQM4M and PCQM4Mv2. The metric is the\nMean Absolute Error (MAE), and the lower the better. ‚Äú‚àí‚Äù denotes\nresults are not available since the labels of test and test-dev sets are\nnot public. Highlighted are the best results for each model size.\n3.4 Architecture Configurations\nWe build five variants of the proposed model with different\nmodel sizes, namely GPTrans-Nano, Tiny, Small, Base, and\nLarge. Note that the number of parameters of our GPTrans\nis similar to Graphormer [Ying et al., 2021] and EGT [Hus-\nsain et al., 2021 ]. The dimension of each head is set to 10\nfor our nano model, and 32 for others. Following common\npractices, the expansion ratio of the FFN module is Œ± = 1\nfor all model variants. The architecture hyper-parameters of\nthese five models are as follows:\n‚Ä¢ GPTrans-Nano: d1 = 80, d2 = 40, layer number = 12\n‚Ä¢ GPTrans-Tiny: d1 = 256,d2 = 32, layer number = 12\n‚Ä¢ GPTrans-Small: d1 = 384,d2 = 48, layer number = 12\n‚Ä¢ GPTrans-Base: d1 = 608,d2 = 76, layer number =18\n‚Ä¢ GPTrans-Large: d1 = 736, d2 = 92, layer number = 24\nThe model size and performance of the model variants on\nthe large-scale PCQM4M and PCQM4Mv2 benchmarks [Hu\net al., 2021 ] are listed in Table 1, and the analysis of model\nefficiency is provided in Table 6. More detailed model con-\nfigurations are presented in the appendix.\nModel #Param Test AP(%)‚Üë\nNon-transformer-based Methods\nDeeperGCN-VN-FLAG [Li et al., 2020] 5.6M 28.42 ¬± 0.43\nPNA [Corso et al., 2020] 6.5M 28.38 ¬± 0.35\nDGN [Beaini et al., 2021] 6.7M 28.85 ¬± 0.30\nGINE-VN [Brossard et al., 2020] 6.1M 29.17 ¬± 0.15\nPHC-GNN [Le et al., 2021] 1.7M 29.47 ¬± 0.26\nGIN-VN‚Ä† [Xu et al., 2018] 3.4M 29.02 ¬± 0.17\nTransformer-based Methods\nGRPE-Standard‚Ä† [Park et al., 2022] 46.2M 30.77 ¬± 0.07\nGPTrans-B‚Ä† (ours) 45.7M 31.15 ¬± 0.16\nGRPE-Large‚Ä† [Park et al., 2022] 118.3M 31.50 ¬± 0.10\nGraphormer-L‚Ä† [Ying et al., 2021] 119.5M 31.39 ¬± 0.32\nEGT-Larger‚Ä† [Hussain et al., 2021] 110.8M 29.61 ¬± 0.24\nGPTrans-L‚Ä† (ours) 86.0M 32.43 ¬± 0.22\nTable 2: Results on MolPCBA. ‚Ä† indicates the model is pre-trained\non PCQM4M or PCQM4Mv2. The higher the better. Highlighted\nare the best results for each model size.\nModel #Param Test AUC(%)‚Üë\nNon-transformer-based Methods\nDeeperGCN-FLAG [Li et al., 2020]\n532K 79.42 ¬± 1.20\nPNA [Corso et al., 2020] 326K 79.05 ¬± 1.32\nDGN [Beaini et al., 2021] 110K 79.70 ¬± 0.97\nPHC-GNN [Le et al., 2021] 114K 79.34 ¬± 1.16\nGIN-VN‚Ä† [Xu et al., 2018] 3.3M 77.80 ¬± 1.82\nTransformer-based Methods\nGraphormer-B‚Ä† [Ying et al., 2021] 47.0M 80.51 ¬± 0.53\nEGT-Larger‚Ä† [Hussain et al., 2021] 110.8M 80.60 ¬± 0.65\nGRPE-Standard‚Ä† [Park et al., 2022] 46.2M 81.39 ¬± 0.49\nGPTrans-B‚Ä† (ours) 45.7M 81.26 ¬± 0.32\nTable 3: Results on MolHIV .‚Ä† indicates the model is pre-trained on\nPCQM4M or PCQM4Mv2. The higher the better. Highlighted are\nthe best results.\n4 Experiments\n4.1 Graph-Level Tasks\nDatasets\nWe verify the following graph-level tasks:\n(1) PCQM4M [Hu et al., 2021 ] is a quantum chemistry\ndataset that includes 3.8 million molecular graphs and a total\nof 53 million nodes. The task is to regress a DFT-calculated\nquantum chemical property,e.g., HOMO-LUMO energy gap.\n(2) PCQM4Mv2 [Hu et al., 2021 ] is an updated version of\nPCQM4M, in which the number of molecules slightly de-\ncreased, and some of the graphs are revised.\n(3) MolHIV [Hu et al., 2020] is a small-scale molecular prop-\nerty prediction dataset. It has 41, 127 graphs with a total of\n1, 048, 738 nodes and 1, 130, 993 edges.\n(4) MolPCBA [Hu et al., 2020] is another property prediction\ndataset, which is larger than MolHIV . It contains 437, 929\ngraphs with 11, 386, 154 nodes and 12, 305, 805 edges.\n(5) ZINC [Dwivedi et al., 2020 ] is a popular real-world\nmolecular dataset for graph property regression. It has\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3563\nZINC PATTERN CLUSTER TSP\nModel #Param Test MAE‚Üì Accuracy(%)‚Üë Accuracy(%)‚Üë F1-Score‚Üë\nNon-transformer-based Methods\nGCN [Kipf and Welling, 2016] 505K 0.367 ¬± 0.011 71.892 ¬± 0.334 68.498 ¬± 0.976 ‚àí\nGraphSage [Hamilton et al., 2017] 505K 0.398 ¬± 0.002 50.492 ¬± 0.001 63.884 ¬± 0.110 ‚àí\nGIN [Xu et al., 2018] 510K 0.526 ¬± 0.051 85.387 ¬± 0.136 64.716 ¬± 1.553 ‚àí\nGAT [VeliÀáckovi¬¥c et al., 2017] 531K 0.384 ¬± 0.007 78.271 ¬± 0.186 70.587 ¬± 0.447 ‚àí\nGatedGCN [Bresson and Laurent, 2017] 505K 0.214 ¬± 0.013 86.508 ¬± 0.085 76.082 ¬± 0.196 0.838 ¬± 0.002\nPNA [Corso et al., 2020] 387K 0.142 ¬± 0.010 ‚àí ‚àí ‚àí\nTransformer-based Methods\nGT [Dwivedi and Bresson, 2020] 589K 0.226 ¬± 0.014 84.808 ¬± 0.068 73.169 ¬± 0.622 ‚àí\nSAN [Kreuzer et al., 2021] 509K 0.139 ¬± 0.006 86.581 ¬± 0.037 76.691 ¬± 0.650 ‚àí\nGraphormer-Slim [Ying et al., 2021] 489K 0.122 ¬± 0.006 86.650 ¬± 0.033 74.660 ¬± 0.236 0.698 ¬± 0.007\nEGT [Hussain et al., 2021] 500K 0.108 ¬± 0.009 86.821 ¬± 0.020 79.232 ¬± 0.348 0.853 ¬± 0.001\nGPS [Ramp¬¥aÀásek et al., 2022] 424K 0.070 ¬± 0.004 86.685 ¬± 0.059 78.016 ¬± 0.180 ‚àí\nGPTrans-Nano (ours) 554K 0.077 ¬± 0.009 86.731 ¬± 0.085 78.069 ¬± 0.154 0.832 ¬± 0.004\nTable 4: Results on four benchmarking datasets from [Dwivedi et al., 2020 ], including graph regression (ZINC), node classification (PAT-\nTERN and CLUSTER), and edge classification (TSP) tasks. The arrow next to the metric means higher or lower is better. ‚Äú‚àí‚Äù denotes the\nresults are not available. Highlighted are the top first and second results.\n10, 000 train, 1, 000 validation, and 1, 000 test graphs.\nSettings\nFor the large-scale PCQM4M and PCQM4Mv2 datasets, we\nuse AdamW [Loshchilov and Hutter, 2018 ] with an initial\nlearning rate of 1e-3 as the optimizer. Following common\npractice, we adopt a cosine decay learning rate scheduler with\na 20-epoch warmup. All models are trained for 300 epochs\nwith a total batch size of 1024. When fine-tuning the MolHIV\nand MolPCBA datasets, we load the PCQM4Mv2 pre-trained\nweights as initialization. For the ZINC dataset, we train our\nGPTrans-Nano model from scratch. More detailed training\nstrategies are provided in the appendix.\nResults\nFirst, we benchmark our GPTrans method on PCQM4M and\nPCQM4Mv2, two datasets from OGB large-scale challenge\n[Hu et al., 2021 ]. We mainly compare our GPTrans against\na set of representative transformer-based methods, including\nGT [Dwivedi and Bresson, 2020 ], Graphormer [Ying et al.,\n2021], GRPE [Park et al., 2022], EGT [Hussain et al., 2021],\nGPS [Ramp¬¥aÀásek et al., 2022 ], and TokenGT [Kim et al.,\n2022]. As reported in Table 1, our method yields the state-of-\nthe-art validate MAE score on both datasets across different\nmodel complexities.\nFurther, we take the PCQM4Mv2 pre-trained weights as\nthe initialization and fine-tune our models on the OGB molec-\nular datasets MolPCBA and MolHIV , to verify the transfer\nlearning capability of GPTrans. All experiments are per-\nformed five times with different random seeds, and we report\nthe mean and standard deviation of the results. From Table 2\nand 3, we can see that GPTrans outperforms many strong\ncounterparts, such as GRPE [Park et al., 2022 ], EGT [Hus-\nsain et al., 2021], and Graphormer [Ying et al., 2021].\nMoreover, we follow previous methods [Park et al., 2022;\nYing et al., 2021 ] to train the GPTrans-Nano model with\nabout 500K parameters on the ZINC subset from scratch. As\ndemonstrated in Table 4, our model achieves a promising test\nMAE of 0.077 ¬± 0.009, bringing 36.9% relative MAE de-\ncline compared to Graphormer [Ying et al., 2021]. The above\ninspiring results show that the proposed GPTrans performs\nwell on graph-level tasks.\n4.2 Node-Level Tasks\nDatasets\nPATTERN and CLUSTER [Dwivedi et al., 2020 ] are both\nsynthetic datasets for node classification. Specifically, PAT-\nTERN has 10, 000 training, 2, 000 validation, and 2, 000 test\ngraphs, and CLUSTER contains 10, 000 training, 1, 000 vali-\ndation, and 1, 000 test graphs.\nSettings\nFor the PATTERN and CLUSTER datasets, we train our\nGPTrans-Nano up to 1000 epochs with a batch size of 256.\nWe employ the AdamW [Loshchilov and Hutter, 2018 ] op-\ntimizer with a 20-epoch warmup. The learning rate is ini-\ntialized to 5e-4, and is declined by a cosine scheduler. More\ntraining details can be found in the appendix.\nResults\nIn this part, we compare our GPTrans-Nano with various\nGCN variants and recent graph transformers. As shown in\nTable 4, our GPTrans-Nano produces the promising accuracy\nof 86.731 ¬± 0.085% and 78.069 ¬± 0.154% on the PATTERN\nand CLUSTER datasets, respectively. These results out-\nperform many Convolutional/Message-Passing Graph Neu-\nral Networks by large margins, showing that the proposed\nGPTrans can serve as an alternative to traditional GCNs for\nnode-level tasks. Moreover, we find that our method exceeds\nGraphormer [Ying et al., 2021 ] on the CLUSTER dataset\nby significant gaps of 3.4% accuracy, which suggests that\nthe three propagation ways explicitly constructed in the GPA\nmodule are also helpful for node-level tasks.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3564\nModel #Param FLOPs Validate MAE‚Üì\nBaseline (Graphormer-S) 12.5M 0.399G 0.0928\n+ Node-to-Node 13.3M 0.402G 0.0874\n++ Node-to-Edge 13.3M 0.405G 0.0865\n+++ Edge-to-Node 13.5M 0.417G 0.0854\nGPTrans-Swider 13.5M 0.417G 0.0854\nGPTrans-Sdeeper (ours) 13.6M 0.472G 0.0835\nTable 5: Ablation studies of GPTrans. We build our baseline based\non Graphormer-S [Ying et al., 2021] with a shorter schedule of 100\nepochs, and decline its validate MAE on PCQM4Mv2 [Hu et al.,\n2021] dataset from 0.0928 to 0.0854 by gradually introducing our\nGPA module. Moreover, we find that the deeper model outperforms\nthe wider model with a similar number of parameters.\n4.3 Edge-Level Tasks\nDatasets\nTSP [Dwivedi et al., 2020 ] is a dataset for the Traveling\nSalesman Problem, which is an NP-hard combinatorial op-\ntimization problem. The problem is reduced to a binary edge\nclassification task, where edges in the TSP tour have positive\nlabels. TSP dataset has 10, 000 training, 1, 000 validation,\nand 1, 000 test graphs.\nSettings\nWe experiment on the TSP dataset in a similar setting to that\nused in the PATTERN and CLUSTER datasets. Details are\nshown in the appendix.\nResults\nTable 4 compares the edge classification performance of\nour GPTrans-Nano model and previously transformer-based\nmethods on the TSP dataset. We observe GPTrans can outper-\nform Graphormer [Ying et al., 2021] with a large margin and\nis comparable with EGT [Hussain et al., 2021], showing that\nthe proposed GPA module design is competitive when used\nfor edge-level tasks. By applying the GPA module, we avoid\ndesigning an inefficient dual-FFN network, which boosts the\nefficiency of our method. We will analyze the efficiency of\nGPTrans in detail in Section 4.4.\n4.4 Ablation Study\nWe conduct several ablation studies on the PCQM4Mv2 [Hu\net al., 2021] dataset, to validate the effectiveness of each key\ndesign in our GPTrans. Due to the limited computational re-\nsources, we adopt GPTrans-S as the base model, and train it\nwith a shorter schedule of 100 epochs. Other settings are the\nsame as described in Section 4.1.\nGraph Propagation Attention\nTo investigate the contribution of each key design in our GPA\nmodule, we gradually extend the Graphormer baseline [Ying\net al., 2021] to our GPTrans. As shown in Table 5, the model\ngives the best performance when all three information prop-\nagation paths are introduced. It is worth noting that the im-\nprovement from our node-to-node propagation is most signif-\nicant, thanks to learning the attention biases for a particular\nlayer rather than sharing them across all layers. In summary,\nour proposed GPA module collectively brings a large gain to\nTrain Inference PCQM4Mv2\nModel #Param (min / ep.) (graph / s) Validate MAE‚Üì\nEGT-Small 11.5M 7.6 10291.8 0.0899\nGPTrans-S 13.6M 5.5 11391.2 0.0823\nEGT-Medium 47.4M 11.3 4840.8 0.0881\nGPTrans-B 45.7M 7.7 6670.6 0.0813\nEGT-Large 89.3M 15.5 3759.4 0.0869\nGPTrans-L 86.0M 9.6 4193.4 0.0809\nTable 6: Efficiency analysis of GPTrans. These experiments are\nconducted with PyTorch1.12 and CUDA11.3. Training time is mea-\nsured on 8 A100 GPUs with half-precision training, and the infer-\nence throughput is tested on a single A100 GPU.\nGraphormer, i.e., 8.0% relative validate MAE decline on the\nPCQM4Mv2 dataset.\nDeeper vs. Wider\nHere we explore the question of whether the transformers for\ngraph representation learning should go deeper or wider. For\nfair comparisons, we build a deeper but thinner model un-\nder comparable parameter numbers, by increasing the depth\nfrom 6 to 12 layers and decreasing the width from 512 to 384\ndimensions. As reported in Table 5, the validate MAE of the\nPCQM4Mv2 dataset is declined from 0.0854 to 0.0835 by the\ndeeper model, which shows that depth is more important than\nwidth for graph transformers. Based on this observation, we\nprefer to develop GPTrans with a large model depth.\nEfficiency Analysis\nAs shown in Table 6, we benchmark the training time and\ninference throughputs of our GPTrans and EGT [Hussain\net al., 2021 ]. Specifically, we employ PyTorch1.12 and\nCUDA11.3 to perform these experiments. For a fair com-\nparison, the training time of these two methods is measured\nusing 8 Nvidia A100 GPUs with half-precision training and\na total batch size of 1024. The inference throughputs of\nPCQM4Mv2 models in Table 6 are tested using an A100 GPU\nwith a batch size of 128, where our GPTrans is slightly faster\nin inference than EGT under a similar number of parameters.\nThis preliminary study shows a good signal that the proposed\nGPTrans, equipped with the GPA module, could be an effi-\ncient model for graph representation learning.\n5 Conclusion\nThis paper aims for graph representation learning with a\nGraph Propagation Transformer (GPTrans), which explores\nthe information propagation among nodes and edges in a\ngraph when establishing the self-attention mechanism in the\ntransformer block. Especially in the GPTrans, we propose a\nGraph Propagation Attention (GPA) mechanism to explicitly\npass the information among nodes and edges in three ways,\ni.e., node-to-node, node-to-edge, and edge-to-node, which is\nessential for learning graph-structured data. Extensive com-\nparisons with state-of-the-art methods on several benchmark\ndatasets demonstrate the superior capability of the proposed\nGPTrans with better performance.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3565\nAcknowledgements\nThis work is supported by the Natural Science Foundation of\nChina under Grant 61672273 and Grant 61832008.\nContribution Statement\nZhe Chen and Hao Tan contributed equally to this work and\nare listed as co-first authors. Tong Lu served as the corre-\nsponding author for communication and correspondence.\nReferences\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[Beaini et al., 2021] Dominique Beaini, Saro Passaro, Vincent\nL¬¥etourneau, Will Hamilton, Gabriele Corso, and Pietro Li `o. Di-\nrectional graph networks. In Proceedings of International Con-\nference on Machine Learning, pages 748‚Äì758. PMLR, 2021.\n[Bresson and Laurent, 2017] Xavier Bresson and Thomas Lau-\nrent. Residual gated graph convnets. arXiv preprint\narXiv:1711.07553, 2017.\n[Brossard et al., 2020] R¬¥emy Brossard, Oriel Frigo, and David De-\nhaene. Graph convolutions that can finally model local structure.\narXiv preprint arXiv:2011.15069, 2020.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in Neural In-\nformation Processing Systems, 33:1877‚Äì1901, 2020.\n[Bruna et al., 2013] Joan Bruna, Wojciech Zaremba, Arthur Szlam,\nand Yann LeCun. Spectral networks and locally connected net-\nworks on graphs. arXiv preprint arXiv:1312.6203, 2013.\n[Cai and Lam, 2020] Deng Cai and Wai Lam. Graph transformer\nfor graph-to-sequence learning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, pages 7464‚Äì7471, 2020.\n[Chen et al., 2017] Jianfei Chen, Jun Zhu, and Le Song. Stochastic\ntraining of graph convolutional networks with variance reduction.\narXiv preprint arXiv:1710.10568, 2017.\n[Chen et al., 2018] Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn:\nfast learning with graph convolutional networks via importance\nsampling. In Proceedings of International Conference on Learn-\ning Representations, 2018.\n[Chen et al., 2020] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie\nZhou, and Xu Sun. Measuring and relieving the over-smoothing\nproblem for graph neural networks from the topological view. In\nProceedings of the AAAI Conference on Artificial Intelligence,\npages 3438‚Äì3445, 2020.\n[Chen et al., 2022] Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kun-\nchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng,\nBingkun Huang, et al. Internvideo-ego4d: A pack of champion\nsolutions to ego4d challenges. arXiv preprint arXiv:2211.09529,\n2022.\n[Chen et al., 2023] Zhe Chen, Yuchen Duan, Wenhai Wang, Jun-\njun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. In International Conference on\nLearning Representations, 2023.\n[Corso et al., 2020] Gabriele Corso, Luca Cavalleri, Dominique\nBeaini, Pietro Li `o, and Petar Veli Àáckovi¬¥c. Principal neighbour-\nhood aggregation for graph nets. Proceedings of Advances in\nNeural Information Processing Systems, 33:13260‚Äì13271, 2020.\n[Defferrard et al., 2016] Micha¬®el Defferrard, Xavier Bresson, and\nPierre Vandergheynst. Convolutional neural networks on graphs\nwith fast localized spectral filtering. In Proceedings of Advances\nin Neural Information Processing Systems, 2016.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In Pro-\nceedings of International Conference on Machine Learning,\n2021.\n[Dwivedi and Bresson, 2020] Vijay Prakash Dwivedi and Xavier\nBresson. A generalization of transformer networks to graphs.\narXiv preprint arXiv:2012.09699, 2020.\n[Dwivedi et al., 2020] Vijay Prakash Dwivedi, Chaitanya K Joshi,\nThomas Laurent, Yoshua Bengio, and Xavier Bresson.\nBenchmarking graph neural networks. arXiv preprint\narXiv:2003.00982, 2020.\n[Gilmer et al., 2017] Justin Gilmer, Samuel S Schoenholz,\nPatrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. In Proceedings of In-\nternational Conference on Machine Learning, pages 1263‚Äì1272,\n2017.\n[Hamilton et al., 2017] Will Hamilton, Zhitao Ying, and Jure\nLeskovec. Inductive representation learning on large graphs. In\nProceedings of Advances in Neural Information Processing Sys-\ntems, 2017.\n[Henaff et al., 2015] Mikael Henaff, Joan Bruna, and Yann LeCun.\nDeep convolutional networks on graph-structured data. arXiv\npreprint arXiv:1506.05163, 2015.\n[Hu et al., 2020] Weihua Hu, Matthias Fey, Marinka Zitnik, Yux-\niao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure\nLeskovec. Open graph benchmark: Datasets for machine learn-\ning on graphs. In Proceedings of Advances in Neural Information\nProcessing Systems, 2020.\n[Hu et al., 2021] Weihua Hu, Matthias Fey, Hongyu Ren, Maho\nNakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale\nchallenge for machine learning on graphs. In Proceedings of\nConference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2021.\n[Huang et al., 2022] Xuanwen Huang, Yang Yang, Yang Wang,\nChunping Wang, Zhisheng Zhang, Jiarong Xu, and Lei Chen.\nDgraph: A large-scale financial dataset for graph anomaly detec-\ntion. arXiv preprint arXiv:2207.03579, 2022.\n[Hussain et al., 2021] Md Shamim Hussain, Mohammed J Zaki,\nand Dharmashankar Subramanian. Global self-attention\nas a replacement for graph convolution. arXiv preprint\narXiv:2108.03348, 2021.\n[Ji et al., 2023] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong,\nXihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo.\nDdp: Diffusion model for dense visual prediction. arXiv preprint\narXiv:2303.17559, 2023.\n[Kim et al., 2022] Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min,\nSungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong.\nPure transformers are powerful graph learners. arXiv preprint\narXiv:2207.02505, 2022.\n[Kipf and Welling, 2016] Thomas N Kipf and Max Welling. Semi-\nsupervised classification with graph convolutional networks.\narXiv preprint arXiv:1609.02907, 2016.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3566\n[Kreuzer et al., 2021] Devin Kreuzer, Dominique Beaini, Will\nHamilton, Vincent L¬¥etourneau, and Prudencio Tossou. Rethink-\ning graph transformers with spectral attention. In Proceedings\nof Advances in Neural Information Processing Systems, pages\n21618‚Äì21629, 2021.\n[Le et al., 2021] Tuan Le, Marco Bertolini, Frank No ¬¥e, and Djork-\nArn¬¥e Clevert. Parameterized hypercomplex graph neural net-\nworks for graph classification. In Proceedings of International\nConference on Artificial Neural Networks, pages 204‚Äì216, 2021.\n[Li et al., 2020] Guohao Li, Chenxin Xiong, Ali Thabet, and\nBernard Ghanem. Deepergcn: All you need to train deeper gcns.\narXiv preprint arXiv:2006.07739, 2020.\n[Liu et al., 2021a] Meng Liu, Zhengyang Wang, and Shuiwang Ji.\nNon-local graph neural networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2021.\n[Liu et al., 2021b] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan\nWei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted windows.\nIn Proceedings of the IEEE International Conference on Com-\nputer Vision, pages 10012‚Äì10022, 2021.\n[Loshchilov and Hutter, 2018] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization. In Proceedings of In-\nternational Conference on Learning Representations, 2018.\n[Park et al., 2022] Wonpyo Park, Woong-Gi Chang, Donggeon\nLee, Juntae Kim, et al. Grpe: Relative positional encoding for\ngraph transformer. In Proceedings of ICLR2022 Machine Learn-\ning for Drug Discovery, 2022.\n[Parmar et al., 2018] Niki Parmar, Ashish Vaswani, Jakob Uszko-\nreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. In Proceedings of the International\nConference on Machine Learning, pages 4055‚Äì4064, 2018.\n[Perozzi et al., 2014] Bryan Perozzi, Rami Al-Rfou, and Steven\nSkiena. Deepwalk: Online learning of social representations.\nIn Proceedings of ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 701‚Äì710, 2014.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child,\nDavid Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners.OpenAI blog, 1(8):9,\n2019.\n[Ramp¬¥aÀásek et al., 2022] Ladislav Ramp ¬¥aÀásek, Mikhail Galkin, Vi-\njay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique\nBeaini. Recipe for a general, powerful, scalable graph trans-\nformer. arXiv preprint arXiv:2205.12454, 2022.\n[Scarselli et al., 2008] Franco Scarselli, Marco Gori, Ah Chung\nTsoi, Markus Hagenbuchner, and Gabriele Monfardini. The\ngraph neural network model. IEEE Transactions on Neural Net-\nworks, 20(1):61‚Äì80, 2008.\n[Shi et al., 2022] Yu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Ji-\nacheng You, Jiyan He, Shengjie Luo, Chang Liu, Di He, and Tie-\nYan Liu. Benchmarking graphormer on large-scale molecular\nmodeling datasets. arXiv preprint arXiv:2203.04810, 2022.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord, Matthijs\nDouze, Francisco Massa, Alexandre Sablayrolles, and Herv ¬¥e\nJ¬¥egou. Training data-efficient image transformers & distillation\nthrough attention. In Proceedings of International Conference on\nMachine Learning, pages 10347‚Äì10357, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Pro-\nceedings of Advances in Neural Information Processing Systems,\n2017.\n[VeliÀáckovi¬¥c et al., 2017] Petar VeliÀáckovi¬¥c, Guillem Cucurull, Aran-\ntxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Ben-\ngio. Graph attention networks. arXiv preprint arXiv:1710.10903,\n2017.\n[Wang et al., 2021] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping\nFan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling\nShao. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 568‚Äì\n578, 2021.\n[Wang et al., 2022a] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhen-\nhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale\nvision foundation models with deformable convolutions. arXiv\npreprint arXiv:2211.05778, 2022.\n[Wang et al., 2022b] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He,\nBingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu,\nZun Wang, et al. Internvideo: General video foundation mod-\nels via generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022.\n[Xu et al., 2018] Keyulu Xu, Weihua Hu, Jure Leskovec, and Ste-\nfanie Jegelka. How powerful are graph neural networks? arXiv\npreprint arXiv:1810.00826, 2018.\n[Ying et al., 2021] Chengxuan Ying, Tianle Cai, Shengjie Luo,\nShuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan\nLiu. Do transformers really perform badly for graph represen-\ntation? In Proceedings of Advances in Neural Information Pro-\ncessing Systems, pages 28877‚Äì28888, 2021.\n[Zhang et al., 2019] Chuxu Zhang, Ananthram Swami, and\nNitesh V Chawla. Shne: Representation learning for semantic-\nassociated heterogeneous networks. In Proceedings of ACM\nInternational Conference on Web Search and Data Mining,\npages 690‚Äì698, 2019.\n[Zhang et al., 2020] Jiawei Zhang, Haopeng Zhang, Congying Xia,\nand Li Sun. Graph-bert: Only attention is needed for learning\ngraph representations. arXiv preprint arXiv:2001.05140, 2020.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n3567",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6999520659446716
    },
    {
      "name": "Transformer",
      "score": 0.5229564309120178
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5036854147911072
    },
    {
      "name": "Graph",
      "score": 0.47784897685050964
    },
    {
      "name": "Engineering",
      "score": 0.07694119215011597
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210102458",
      "name": "Novel (United States)",
      "country": "US"
    }
  ]
}