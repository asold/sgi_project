{
  "title": "Autoregressive Reasoning over Chains of Facts with Transformers",
  "url": "https://openalex.org/W3117019976",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3093583012",
      "name": "Ruben Cartuyvels",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2318732019",
      "name": "Graham Spinks",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A4214055282",
      "name": "Marie-Francine Moens",
      "affiliations": [
        "KU Leuven"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2983719617",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2142697503",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2143331230",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4252742993",
    "https://openalex.org/W2916846408",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963318894",
    "https://openalex.org/W2949428332",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2986650838",
    "https://openalex.org/W2990261547",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2982940459",
    "https://openalex.org/W4288114783",
    "https://openalex.org/W4298110152",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2963339923",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W3119370638",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2985848674",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3031043369",
    "https://openalex.org/W658020064",
    "https://openalex.org/W2125444198",
    "https://openalex.org/W2890487780",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4295312788"
  ],
  "abstract": "This paper proposes an iterative inference algorithm for multi-hop explanation regeneration, that retrieves relevant factual evidence in the form of text snippets, given a natural language question and its answer. Combining multiple sources of evidence or facts for multi-hop reasoning becomes increasingly hard when the number of sources needed to make an inference grows. Our algorithm copes with this by decomposing the selection of facts from a corpus autoregressively, conditioning the next iteration on previously selected facts. This allows us to use a pairwise learning-to-rank loss. We validate our method on datasets of the TextGraphs 2019 and 2020 Shared Tasks for explanation regeneration. Existing work on this task either evaluates facts in isolation or artificially limits the possible chains of facts, thus limiting multi-hop inference. We demonstrate that our algorithm, when used with a pre-trained transformer model, outperforms the previous state-of-the-art in terms of precision, training time and inference efficiency.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6916–6930\nBarcelona, Spain (Online), December 8-13, 2020\n6916\nAutoregressive Reasoning over Chains of Facts with Transformers\nRuben Cartuyvels, Graham Spinks, Marie-Francine Moens\nLIIR lab, KU Leuven, Belgium\n{first}.{last}@kuleuven.be, sien.moens@kuleuven.be\nAbstract\nThis paper proposes an iterative inference algorithm for multi-hop explanation regeneration, that\nretrieves relevant factual evidence in the form of text snippets, given a natural language ques-\ntion and its answer. Combining multiple sources of evidence or facts for multi-hop reasoning\nbecomes increasingly hard when the number of sources needed to make an inference grows.\nOur algorithm copes with this by decomposing the selection of facts from a corpus autoregres-\nsively, conditioning the next iteration on previously selected facts. This allows us to use a pair-\nwise learning-to-rank loss.We validate our method on datasets of the TextGraphs 2019 and 2020\nShared Tasks for explanation regeneration. Existing work on this task either evaluates facts in\nisolation or artiﬁcially limits the possible chains of facts, thus limiting multi-hop inference. We\ndemonstrate that our algorithm, when used with a pre-trained transformer model, outperforms\nthe previous state-of-the-art in terms of precision, training time and inference efﬁciency.\n1 Introduction\nThe task of multi-hop explanation generation has recently received interest as it could be a stepping-stone\ntowards general multi-hop inference over language. Multi-hop reasoning requires algorithms to combine\nmultiple sources of evidence. This becomes increasingly hard when the number of required facts for an\ninference grows, because of the exploding number of combinations and phenomena such as semantic\ndrift (Fried et al., 2015; Jansen, 2018). The WorldTree dataset was designed speciﬁcally for ( >2)-fact\ninference (Jansen et al., 2018; Xie et al., 2020): it consists of elementary science exam questions that\ncan be explained by an average of 6 facts from a complementary dataset of textual facts. The explanation\nregeneration task as in the TextGraphs Shared Tasks (Jansen and Ustalov, 2019; Jansen and Ustalov,\n2020) asks participants to retrieve and rank relevant facts (given one of these natural language questions\nand its answer as input1) such that the top-ranked facts explain the answer to the question. An example\nis shown in the upper left part of ﬁg. 1 (and more in appendix A).\nAs each question-answer pair potentially has many supporting facts, evaluating all combinations of\nfacts is computationally prohibitive. Previous work remedies this by computing scores for facts in isola-\ntion, or by severely limiting the number of combinations of facts (Das et al., 2019; Banerjee, 2019; Chia\net al., 2019). The latter is done by considering combinations of few facts only and/or by reranking com-\nbinations of only the top retrieved facts by a simpler method. Both approaches limit multi-hop reasoning\nas facts are not combined or too many facts are ignored by the simple ﬁrst-stage retriever.\nIn this paper we propose a method to retrieve facts that does build long chains of facts, while being ef-\nﬁcient. During training, a pre-trained neural language model encodes the question-answer pair as well as\na randomly selected combination of ground-truth facts, before evaluating candidate facts. For efﬁciency,\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nLicense details: http://creativecommons.org/licenses/by/4.0/.\n1Systems that do not require the answer, but that retrieve facts based on a question only, could be of great utility: the\nretrieved facts could be used to infer the answer to the question. However in this paper we follow the task deﬁnition of (Jansen\nand Ustalov, 2019), who deﬁne explanation regeneration as a stepping-stone task for multi-hop inference. The method we\npropose is equally applicable when the answer is not given, but evaluating this setting is left for future work.\n6917\nonly a set of neighborhood facts (which we call ‘visible’ facts) is considered. This set is obtained by\nselecting the nearest facts by tf-idf distance to the question, answer and set of ground truth facts. During\ninference, facts are ranked iteratively: in each iteration, the highest scoring fact is selected and encoded\ntogether with previously selected facts and the question-answer pair, so the next ranking is always con-\nditioned on the current set of chosen facts. At each inference step, the set of visible facts consists of the\nnearest facts to the already selected facts and question-answer pair. This autoregressive formulation and\nthe deﬁnition of neighborhoods together enable the use of losses that incorporate interactions between\ndifferent facts, like the pairwise RankNet loss from the learning-to-rank literature (Burges et al., 2005).\nMost methods in earlier work use some form of rank-rerank set-up, but none dynamically expand\nthe set of reranked facts, and the reranked set is usually too small (Das et al., 2019; Chia et al., 2019;\nBanerjee, 2019). Banerjee (2019) and Chia et al. (2019) use iterative schemes, but the former only\nreconsiders the top 15 initially retrieved facts and the latter uses a term frequency based approach. Das\net al. (2019) consider chains of facts during training and inference, but only up to length 2, because\nof the explosive number of combinations. All of the above systems use pointwise losses only. While\ncomponents of our method are inspired by previous work, we are the ﬁrst to put them together in a\nprincipled way, so that they enable each other. Our neighborhoods enable both our iterative inference\nprocedure to efﬁciently build chains of up to 10 facts, and the use of more informative losses. Our\ntraining is designed to support iterative inference and to close the gap between training and inference as\nmuch as possible.\nContributions. (1) By deﬁning dynamically growing neighborhoods of facts for our model to operate\nin, we limit computational cost without severely limiting the range of our method, (2) We deﬁne a\nnew autoregressive training and inference method to evaluate facts within the context of other facts;\n(3) We apply a learning-to-rank loss that successfully exploits interactions between facts, leading to an\nimprovement in MAP score over previous baselines.\n2 Related Work\nExplanation regeneration was promoted as a TextGraphs 2019 Shared Task (Jansen and Ustalov, 2019).\nWe brieﬂy summarize systems proposed in 2019. Most methods ﬁnetuned a pre-trained transformer like\nBERT (Devlin et al., 2019) in a learning-to-rank set-up, as reranker with the question and answer as query\nand facts as documents, like Nogueira and Cho (2019). Das et al. (2019) use BERT to classify chains\nof 2 facts, drawn from initially retrieved facts by a tf-idf retriever and facts with words in common with\nthose facts. Their idea is similar to ours, but due to the high number of combinations, they are limited\nto explanations of only 2 facts. As a second approach, they use BERT like Nogueira and Cho (2019) to\nrank single facts: but Das et al. simply rank all facts instead of the top- T initially retrieved ones, which\nis computationally expensive for the larger 2020 dataset.\nChia et al. (2019) use an iterative scheme where the tf-idf representation of the question is aggregated\nwith the tf-idf representations of previously selected facts for retrieval. They compare this to a BERT\nbased approach like described above. In contrast, selected facts and the question in our method are\nencoded as one paragraph with BERT in a trainable way, which allows for more complex relations to\nbe learnt. Banerjee (2019) adds gold facts as context during training and scores facts individually with\nBERT during inference. The top facts are reranked by iterative rescoring, which only uses the originally\ncomputed scores and precomputed (hence untrainable) sentence embeddings. D’Souza et al. (2019)\npropose a pair-wise learning-to-rank approach with SVMs.\nDas et al. (2018) propose a retriever-reader system that iteratively retrieves and ‘reads’ relevant para-\ngraphs from a large text corpus for open-domain QA. High level analogies can be drawn between their\nsystem and ours, but their architecture involves separate query and document encoders, a recurrent rea-\nsoner and a specialized reading comprehension model. Their system is trained end-to-end for QA.\nIn conclusion, our work has similarity to the widely used retrieval-reranking paradigm, but the initially\nretrieved set of facts is dynamically extended based on selected facts. Our training and inference method\nto evaluate facts within the context of other facts successfully models interactions between different facts,\nframing the task more as a reading comprehension task.\n6918\nENCODE\n(Transformer) Score\nNeighboorhood of (q , f 1, f 2)\nVisible Facts\nfc,- : A human has two legs\nfc,+ : An insect is a kind of animal\nfc,- :  ...\nChoose L facts\nAdd f c with highest score as f 3\nQ An animal has six legs. What is\nit most likely to be?\nA A ﬂy\nScore all f c \nf1 : A ﬂy is a kind of insect\nf2 : An insect has six legs\nf3 : ???\nFigure 1: An overview of ARCF during inference. The computed score represents P(f3 |f1,2,q).\n3 Proposed Approach\nThis section describes our proposed method, which we call ‘Autoregressive Reasoning over Chains of\nFacts’ (ARCF). ARCF consists of an initial retrieval component described in §3.3, a learning-to-rank\ntraining scheme and an iterative inference procedure (both in§3.4, ﬁg. 1 shows the latter schematically).\n3.1 Task Description\nGiven is a dataset Df of facts in text form (size Df). Each fact f consists of word tokens: f =\n[w1,w2,...,w Zf ]. Further given is a training, validation and test dataset Dqa containing multiple choice\nquestions (size Dqa). Each question is concatenated with answer options: 4 (or so) multiple choice an-\nswers, with the correct one marked. Question and answer(s) together form a queryq = [w1,w2,...,w Zq ].\nAll queries in Dqa can be explained by 1 to 22 gold facts in Df. Gold facts for a question are marked\nwith ‘grounding’, ‘central’ or ‘lexical glue’, depending on their role in explaining the question. Central\nfacts are key in explaining the question, lexical glue links facts together (e.g., by synonymy or taxonomy\nrelations) and grounding facts connect facts to the question2.\nThe task for a given q ∈Dqa is to rank all facts in Df, with gold facts ranked higher than irrelevant\nfacts. The answer in this context is the answer to a question, always encoded together with the question\nin q, and not the output target of the task. When we write ‘query’, we mean an instance of q, i.e., a\nquestion concatenated with its answer. The output target is the set of gold factsf∗\n1,...,G for a q. The mean\naverage precision (MAP) of the gold facts in the computed ranking is calculated as evaluation metric.\nWe use the notation f1,...,N for an intermediate set of facts, f1,...,M for a completed set and f∗\n1,...,G for\nthe gold set for a q, with Gthe number of gold facts. We call a concatenation of a query with a number\nof facts a preﬁx: p = [q |f1,...,N]. Appendix A shows example q’s and gold facts.\n3.2 Model\nWe use a neural language encoder to compute a functionfθ : VZ ↦→R. The input is the concatenation of\na query and a number of facts [q |f1,...,N]. The output is a scalar score s, indicating how well the last of\nthe concatenated facts (the candidate fact) ﬁts in the explanation for the query. The process is iterated: a\nchosen fact is appended to f1,...,N and a new score is computed. We use pre-trained transformer models,\nlike BERT or RoBERTa (Devlin et al., 2019; Liu et al., 2019; Vaswani et al., 2017), because the task\ndataset is relatively small, and this allows for the reasoning to incorporate external knowledge.\n3.3 Fact - Question Neighborhoods\nNeighborhoods. The ﬁrst step of the method consists of computing neighborhoods of visible facts, for\neach question and corresponding set of facts, denoted by visk : Df ∪Dqa ↦→DK\nf . To retain tractability,\nfacts from neighborhoodsvisk(·) will later be ranked, while facts outsidevisk(·) are out of consideration.\nIn contrast to classic rank–(neural) rerank approaches, our neighborhood (initially retrieved set that will\nbe reranked) will expand dynamically. Pairwise distances between all facts and between questions and\n2We refer the reader to (Jansen et al., 2018; Xie et al., 2020) for more information.\n6919\nfacts are precomputed. A fact fc is visible from [q |f1,...,N] if it is one of the knearest facts either to q,\nor to one of the f1,...,N (denoted as nearestk). We use KN ≤(N + 1)·kto refer to the cardinality3 of\nvisk(·). Formally: visk(q,f1,...,N) =⋃\nx∈{q,f1,...,N }nearestk(x).\nFor a given question and (possibly empty) set of facts, the algorithm should be able to retrieve the set\nof visible facts, but is agnostic to how this set or the distances are computed. Given the limited size of\nDf and Dqa, if we use an inexpensive distance metric, the overhead of computing all pairwise distances\nis small. For larger datasets or for distances that are expensive to compute, the overhead might not be\nnegligible. Approximate nearest-neighbor methods could then be used.\nk Fraction\n90 0.90\n130 0.95\n180 0.97\n290 0.99\nTable 1: Mean frac-\ntion of gold facts\nreachable.\nDistances. We tried computing the distances as distance between tf-idf vec-\ntors, as Word Mover’s Distance (Kusner et al., 2015), as the distance be-\ntween sentence embeddings computed by a pre-trained BERT or Sentence-\nBERT (Reimers and Gurevych, 2019), and as the reciprocal number of words\nin common (lexical overlap). We compared the distance metrics by the frac-\ntion of gold facts, for a given k, that could be reached in an unlimited number\nof ‘hops’4 via gold facts from visk(·), starting from a q ∈Dqa. We only com-\nputed the fractions on the training data to prevent test set leakage. We found\ntf-idf to work best for all k: table 1 shows some indicative ratios (mean over\nq ∈Dqa). The fact that tf-idf works best here can be explained by the fact that\nthe dataset is well curated and terminology is uniform across facts.\nDas et al. (2019) construct a connectivity graph between facts, which they use to extend the set of\ninitially retrieved facts by tf-idf. They use lexical overlap as criterion for being connected, resulting in a\ndense graph and leading to an explosive number of chains. In contrast, parameter kin our deﬁnition of\nvisk allows for easy and precise control of the size of neighborhoods.\n3.4 Autoregressive Ranking of Candidate Facts\nWe propose to decompose the conditional probability distribution over rankings of facts autoregressively:\nP(f1,...,M |q) =\nM∏\ni=1\nP(fi |f1,...,i−1,q), (1)\nwhere f1 is the highest ranked fact. At each iteration, we compute P(fi |f1,...,i−1,q) for all visible\ncandidate facts fi, and we select the fact with the highest probability to be ranked at position i. The\nunnormalized probability for P(fi |f1,...,i−1,q) is computed by our scoring function as s = fθ([q |\nf1,...,i−1 | fi]). For the task at hand, only the interclass order in the produced ranking is relevant:\nrelevant facts should be ranked higher than irrelevant facts. The intraclass order, i.e., the relative order\nof relevant facts and of irrelevant facts, does not affect the MAP ( §3.1). Eq. 1 can thus be understood\nas the decomposition of selecting a set of facts jointly into selecting one fact at a time (conditioned on\npreviously selected facts). Scoring all combinations of facts is deﬁnitely infeasible, while scoring facts\nindependently is too simplifying. The decomposition aims to strike a balance between the two.\nConditioning the selection of facts on previously selected facts brings several advantages (compared\nto scoring facts independently as Nogueira and Cho (2019)). First, it enables the incremental building\nof chains of reasoning. The role of many facts in explaining a question is not immediately apparent\nwhen they are looked at in isolation, and only becomes more evident when they are considered as part\nof a larger explanation. Consider the question: “ George warms his hands by rubbing them. Which\nskin surface produces the most heat? (A) dry palms ”. The relevancy of “ 1: as moisture of an object\ndecreases, the friction of that object against another object increases” is clearer when “2: friction causes\nthe temperature of an object to increase ” is also known. Without the latter, someone (without world\nknowledge) might, for instance, regard facts about any other physical property that varies with moisture\nlevel as equally relevant to the question as the former, while they are not.\n3N + 1for N facts and 1 query, ≤because the neighborhoods might overlap.\n4A hop is deﬁned as an imagined ‘move’ from either aq or f to another visible fact.\n6920\nSecond, by processing multiple facts at once, we can leverage BERT as a reading comprehension\nmodel, rather than as a retrieval model. The task requires not merely retrieving facts that seem relevant,\nlike a search-engine would, but gathering a set of facts from which the answer to a simple science\nquestion can be inferred. That clearly requires more reasoning than a traditional retrieval task. Research\nhas shown that pre-trained transformers are able to infer knowledge from paragraphs of text, which is\nwhy they are more suited to handle this formulation of the task (Liu et al., 2019; Clark et al., 2019).\nSince we only need to be able to retrieve the facts with the highest probabilities, we can avoid com-\nputing normalized probabilities and instead compute scores (i.e., unnormalized probabilities). During\ntraining (next §) we do compute probabilities, in order to compute losses, but only over subsets of facts.\nTraining\nSamples. The training input is encoded as x = [q|f∗\n1,...,N|fc], with q a query (question and answer),\nf∗\n1,...,N a set of gold facts, and fc ∈visk(qi,f∗\n1,...,N) a candidate fact, which can be positive or negative,\nfrom the visible neighborhood of [q |f∗\n1,...,N]. We train our scoring function fθ with stochastic gradient\ndescent, to output high scores for positive candidate facts and low scores for negative candidate facts.\nTraining samples for one q are constructed by ﬁrst uniformly sampling a number N ≤Gof gold facts\nf∗\n1,...,N from q’s full set of gold facts. N is itself uniformly sampled: N ∼U (0,G). The query and\ngold facts are concatenated into a preﬁx p = [q|f∗\n1,...,N]. For one given preﬁx, positive training samples\nxp are constructed by concatenating p with all remaining visible gold facts. Negative training samples\nxn are constructed by concatenating the same p with a number of uniformly sampled visible negative\nfacts5. We either use all visible negatives, or sample a number until the minibatch is full. The process\nis repeated: multiple preﬁxes are constructed for every query in Dqa, and every preﬁx is appended with\nmultiple visible, gold and negative facts. We construct multiple preﬁxes per q so that we have multiple\ntraining samples per q, and so that the model is trained with different explanation lengths.\nThe preﬁx itself serves as a sample as well: the model is trained to score p highest when no more\nvisible gold facts remain, i.e., when f∗\n1,...,N contains all gold facts or when the remaining gold facts are\nnot visible. During inference, p getting the highest score in an iteration is a stopping condition: the\ngathered set of facts is then considered complete.\nLosses. Because classical maximum likelihood training for eq. 1 would allow to backpropagate a loss\nonly after all candidate facts have been considered, i.e., after KN forward passes, we resort to different\nloss functions. A simple loss that can be used is the pointwise binary cross-entropy loss (bXENT), which\nconsiders each input example x individually and trains to correctly classify the candidate fc as relevant\nor irrelevant. We propose to use the pairwise RankNet loss (Burges et al., 2005):\nL(xp,xn; θ) =−log(σ(fθ(xp)) −σ(fθ(xn))) , (2)\nWhere σ is the logistic sigmoid function, and xp and xn are samples in which fc is a positive and\nnegative fact, respectively. This loss is shown by Chen et al. (2009) to maximize a lower bound on the\nMAP. To further amplify between-fact interactions in the gradient, we also use the conditional ranking\nvariant of Noise-Contrastive Estimation (NCE), which covers >2 facts at once (Ma and Collins, 2018;\nGutmann and Hyv¨arinen, 2010):\nL(x1,...,B; θ) =−log exp(fθ(x1) −log Pn(fc))∑B\nj=1 exp(fθ(xj) −log Pn(fc))\n, (3)\nWhere x1 is positive and x>1 are negative, Bis the batch size, and Pn is a negative sampling distribu-\ntion over candidates: a uniform distribution over visk(p). This loss has been used for training word em-\nbeddings as a more efﬁcient approximation to the negative log-likelihood loss (Mnih and Kavukcuoglu,\n2013; Mikolov et al., 2013). When training with NCE and RankNet, samples in one batch share a com-\nmon preﬁx p and only differ in their candidate factfc, so that all samples x in one loss term L(·,θ) (eqs.\n2-3) only differ in fc and hence the model is trained to score candidate facts and not preﬁxes.\n5Note that f∗\n1,...,N in the preﬁx of both positive and negative training samples consists only of gold facts, and that all\ncandidate facts f·,c, gold or not, come from the visible neighborhood visk(p).\n6921\nThe proposed training scheme – training a model to predict the next gold element conditioned on\nprevious gold elements – is reminiscent of training text generation models with teacher forcing. A\nknown weakness of teacher forcing is exposure bias (Ranzato et al., 2016); models are conditioned on\nground-truth data during training, as opposed to on their own outputs during inference. ARCF exhibits\nthis discrepancy as well, which is why, during training, we try replacing a uniformly sampled amount\nof gold facts f∗\n1,...,N (in the preﬁx) with uniformly sampled negative facts from visk(p). This feature is\ncalled ‘CN’ later. Hence the model is trained to be more robust to mistakes it makes during inference. A\nsimilar technique was already proposed for text generation as scheduled sampling (Bengio et al., 2015).\nInference\nAt inference, we incrementally build an explanation, i.e., a set of facts. The input follows the same\nencoding format: x = [q|f1,...,N|fc] where f1,...,N are previously selected facts (and not gold facts\nlike in training). At each iteration, we use the query q concatenated with already selected facts f1,...,N\nas updated retrieval query, and rank all other visible facts fc ∈visk(q,f1,...,N). The highest scored\nfact is appended to the query for next iteration. Note that the set of visible facts is extended with the\nneighborhood of the selected fact in each iteration. The set of selected facts is considered to be complete\nwhen its cardinality N equals L or when the highest scored sample is the sample without candidate\nappended (see prev. §). Multiple rankings are made; one with each intermediate set of facts and the\nquestion as retrieval query. Algorithm 1 shows the procedure in pseudocode.\nAlgorithm 1 Inference procedure for one q\nInput: q ∈Dqa, fj ∈Df, neighborhoods visk,\nscoring function fθ, max length L\nfacts ←∅, allscores ←∅,\ncandidates ←visk(q), l ←0\nwhile l < Land not stopping condition do\nscores ←∅\nfor j = 1...|candidates|do\nscores[j] ←fθ( [q |facts |candidates[j] ] )\nend for\nfacts ←facts + [farg max(scores)]\ncandidates ←visk(q; facts)\nallscores[l] ←scores, l ←l + 1\nend while\nreturn produce ranking(facts, allscores)\nWhen the stopping condition is met, we end up\nwith an explanation ˆp = [q|f1,...,M]. To convert\nthe result to a ranking, f1,...,M are ranked highest.\nFacts that were considered as candidate facts but not\nselected are ranked next; their relative order is de-\ntermined by their scores in the last iteration. Next,\nall facts that were never considered are ranked by\na simple metric like tf-idf distance from the com-\nputed explanation ˆp. We experimented with beam\nsearch procedures, where the beams were interme-\ndiate sets of facts. This did not improve perfor-\nmance on the validation set, so we do not consider\nit further.\nThis iterative fact selection bears similarity with\nhow token-per-token text generation is usually per-\nformed with neural networks. Instead of computing\na probability distribution over the vocabulary in one forward pass, our procedure requires a forward pass\nper score, i.e., per fact considered. To keep required resources for inference reasonable, only neighbor-\nhoods of facts are considered, instead of all facts, reducing the number of forward passes in one iteration\nfrom Df = 9707to KN. Inference for a single q requires L+ ∑L\nl=1 Kl−1 = O(L2k) forward passes6,\nwith La chosen maximum number of iterations and kthe neighborhood size. Parameter kcontrols the\ntrade-off between completeness and efﬁciency.\n4 Experiments\n4.1 Data & Preprocessing\nIn the 2020 version of the task Df contains 9727 facts, and Dtrain\nqa , Dval\nqa , Dtest\nqa contain 2206, 496 and\n1664 questions respectively (Xie et al., 2020). The dataset has been extended w.r.t. the 2019 version of\nthe task. For completeness, we also include results obtained with baselines and our models on the 2019\ndata. The 2019 data includes 902, 214 and 541 questions for training, validation, and testing respectively,\nalong with 4950 facts (Jansen et al., 2018). We remove incorrect answers fromq (like Das et al. (2019)),\n6L + ∑L\nl=1 Kl−1 ≤L + ∑L\nl=1 l ·k = L + L\n2 (k + Lk) =O(L2k), with L the max. number of iterations and K deﬁned\nin §3.3: Kl ≤(l + 1)·k.\n6922\nmark the correct answer with “(answer)” and the start of the gold facts with “(explanation)”7. An example\nof a tokenized input sample could be “[START] When does water start boiling? (answer) At 100 ◦C.\n(explanation) This is a gold fact. This is another gold fact. [SEP] This is a candidate fact that is gold or\nnot [SEP]”. Examples of q ∈Dqa and their explaining facts are shown in Appendix A.\n4.2 Baselines\nAs a simple baseline, we include a tf-idf vector retrieval model, that ranks facts by cosine similarity\nbetween their and the q’s tf-idf representation. We stem facts and q, and remove stopwords before\ncomputing tf-idf vectors. Baseline single-fact concatenates a q and a single candidate fact, and computes\na relevance score for all fc ∈Df by encoding the concatenation [q |fc] with BERT and projecting the\nﬁnal layer’s CLS-token embedding to a scalar with a linear layer (Das et al., 2019; Nogueira and Cho,\n2019). The model is trained with binary cross-entropy (relevant or not).\nThe highest score in the 2019 competition was obtained by an ensemble of baselines single-fact and\npath-rerank (Das et al., 2019). Model path-rerank ranks facts for a q by ﬁrst retrieving the top- T facts\nwith the tf-idf retriever from above. This initial top- T set is extended with all facts that have ≥1 words\nin common with one of the top-T facts. Next, all combinations of up to C = 2facts are taken from this\nextended set. A relevance score is computed for all combinations (chains), in the same way as in single-\nfact or our models: concatenate q and the C facts, encode the concatenation with BERT and project the\nﬁnal CLS-token embedding to a score s with a linear layer. A fact’s relevance score is the maximum\nscore of any chain it appears in. The binary cross-entropy loss is used for training the model. We use the\nimplementation of Das et al. (2019) for the single-fact and path-rerank baselines8.\nComplexities. Das et al. (2019) used single-fact and path-rerank for the smaller 2019 dataset, with\nfewer facts and fewer q. They already noted that single-fact is not scalable to a large corpus of facts: for\nthe 2020 data, Df ≈10K forward passes are required to solve a single q during inference. One epoch\ntrained with bXENT consists of 21M samples ( Dtrain\nqa ·Df, trained 3 epochs). The path-rerank model\nuses T = 25 for training, which generates 7k chains of facts per q, resulting in 16M training samples\n(trained 1 epoch). Using T = 50 during inference results in 16k chains (hence forward passes) per q.\nThis number can be controlled by setting T, but setting T too small would leave too many relevant facts\nout of consideration. In contrast, the neighborhood in our method is less restrictive as it depends on\nselected facts and thus expands progressively as more facts are selected.\n4.3 Experiments\nWe implemented our algorithm and baselines using PyTorch and the Transformers library9 (Paszke et al.,\n2019; Wolf et al., 2019). Thetf-idf baseline was implemented with SciKit Learn (Pedregosa et al., 2011).\nTo keep comparisons fair, all results on 2019 data (baselines and ARCF) are obtained by ﬁnetuning the\npublicly available pre-trainedbert-base-uncased (since this model is used in Das et al. (2019)). To reduce\nresource usage, all models on 2020 data were ﬁnetuned from the smaller pre-traineddistilroberta-base10.\nIt can reasonably be expected that using bigger or more advanced pre-trained models would further\nimprove results. We ran experiments on 1 16GB Nvidia Tesla P100 GPU. We used the Adam optimizer\n(Kingma and Ba, 2015), with learning rate2e−5 and linear LR decay. We append samples to minibatches\nuntil they reach 5000 tokens. An overview of used hyperparameters can be found in appendix B. For\ntraining we set neighborhood sizek= 180(Lonly impacts inference), for inference we set the maximum\nand minimum number of iterations L= 9, Lmin = 3, and k = 290. Some hyperparameters were taken\nfrom Das et al. (2019), while others were tuned manually.\n7It is worth noting that technically, ARCF can perfectly run without the correct answer marked and without incorrect answers\nremoved, although a performance drop on the explanation regeneration task can be expected.\n8Code available at https://github.com/ameyagodbole/multihop_inference_explanation_\nregeneration.\n9Our code and trained models are publicly available at https://github.com/rubencart/\nLIIR-TextGraphs-14.\n10When we ﬁnetuned bert-base-uncased on the 2020 data or distilroberta-base on the 2019 data we obtained similar results.\n6923\nAlgorithm Loss MAP 2020 MAP 2019\nTF-IDF 0.3743 0.3870\nSF bXENT 0.4992 0.5574\nPR bXENT 0.4629 0.5313\nENSEMBLE bXENT 0.5081 0.5625\nARCF RankNet 0.5815 0.5707\nwith CN RankNet 0.5810 0.5575\nARCF NCE 0.5728 0.5634\nwith CN NCE 0.5759 0.5691\n(a)\nAlgorithm Loss test 2020 val 2020\nARCF RankNet 0.5815 0.5931\nw bXENT bXENT −0.0082 −0.0060\nw SF train bXENT −0.102 −0.101\nw SF inf RankNet −0.037 −0.041\nw/o preﬁx, neighb. RankNet −0.053 −0.054\nw/o R3 RankNet −0.0004 −0.0001\nw/o R3, S2 RankNet −0.072 −0.079\n(b)\nAlgorithm train T (H) inf T (s /x)\nSF 56.3 18.4\nPR 16.2 31.8\nARCF 5.7 9.6\n(c)\nk 90 130 180 210 290\nMAP .567 .577 .581 .581 .581\nT (s/x) 2.5 3.6 4.9 5.7 7.8\n(d)\nL 2 4 6 8 10 12 14\nMAP .561 .576 .581 .581 .581 .581 .582\nT (s/x) 0.43 1.4 2.9 4.9 7.6 11.0 15.1\n(e)\nTable 2: (a) Mean average precision (MAP) of baselines (upper) and ARCF (bottom) on the 2020 and\n2019 test sets. (b) Ablations on the 2020 test and validation sets. (c) Training time in hours and inference\ntime in seconds/sample. (d)-(e) Impact of k(with L= 8) and L(with k = 180) on 2020 test MAP and\ninference time.\nIn the remainder of this section, ARCF denotes our proposed method, SF refers to the single-fact\nbaseline, PR is the path-rerank baseline, CN means ‘conditioned on negatives’,S2 means ‘rank scored\nbut not selected facts 2nd’,R3 means ‘rank rest 3rd’ (as opposed to omitting them altogether, see§3.4).\n4.4 Results and Discussion\nTest set results. Tables 2a,c show results on the hidden 2019 and 2020 test sets 11, total training time\nand inference time per sample, for the baselines and ARCF. The test scores are obtained by models\nthat got the highest validation score of 5 training runs with different random seeds, while the training\ntimes are averaged over these 5 runs. As can be seen, ARCF outperforms the baselines both in terms of\nobtained MAP and efﬁciency 12. The highest 2020 MAP is 0.5815, which put us at the second place in\nthe online competition. Appendix C shows examples of validation set questions and predicted facts.\nOn the 2020 test set, all our models obtain a higher MAP than all baselines. This is not the case on\nthe 2019 data, which suggests that ARCF beneﬁts more from additional training data. Including >2\nfacts in one loss term with NCE shows no beneﬁt compared to the pairwise RankNet loss. Conditioning\non negatives has no signiﬁcant impact. Five 2020 test set evaluations of ARCF and SF show that the\ndifference in scores is statistically signiﬁcant: ARCF with RankNet scores higher on average than SF\n(1-tailed indep. t-test, p< 0.001).\nAblation study. As ARCF consists of several components, we perform an ablation study on the 2020\ntest and validation sets. Results are shown in table 2b. First, ARCF was trained with the pointwise\nbXENT loss (but still with preﬁxes and neighborhoods). The MAP drops, showing that pairwise infor-\nmation in the gradients improves learning w.r.t. pointwise information. Second, ARCF is compared to\nARCF w SF train, which is trained like SF and uses algorithm 1 for inference. The large drop in MAP\n11Leaderboard of 2020 competition: https://competitions.codalab.org/competitions/23615\n(26/10/2020), 2019 competition: https://competitions.codalab.org/competitions/20150 (26/10/2020).\n12Xie et al. (2020) report a MAP score of 0.52 instead of 0.4992 on the 2020 test set with a single-fact BERT baseline,\nbut since the publication of their paper the 2020 dataset (incl. the test set) has been updated. They also use BERT instead of\nRoBERTa, and possibly different hyperparameters.\n6924\n0 5 10 15 20\nNb of gold facts\n0\n20\n40\n60Nb of q\n(a)\nEasy Challenge0.0\n0.2\n0.4\n0.6\n0.8\n1.0MAP\nours\nsingle-fact (b)\nCentral Grounding Lexical glue0.0\n0.2\n0.4\n0.6\n0.8\n1.0MAP\nours\nsingle-fact (c)\n0 5 10 15 20\nNb of gold facts\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MAP\nours\nsingle-fact\n(d)\n1 2 3 4 5\nNeighborhood hops away from q\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MAP\nours\nours\nsingle-fact\nsingle-fact (e)\n1 2 3 4\nLexical hops away from q\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MAP\nours\nours\nsingle-fact\nsingle-fact (f)\nFigure 2: MAP of ARCF vs. single-fact on subsets of facts.\nindicates that algorithm 1 only works for inference if the model is trained accordingly. Next, ARCF w\nSF inf is trained like ARCF and evaluated like SF. This result shows that ARCF training still improves\nperformance w.r.t. SF even when facts are scored individually, and it shows that algorithm 1 improves\nperformance. ARCF w/o preﬁx, neighb is trained with the pairwise RankNet loss but without preﬁxes\nand neighborhoods. Minibatches each contain 1 positive andB−1 negatives that are uniformly sampled\nfrom all Df −Gnegative facts. Evaluation is carried out like for SF. The score drops almost 10%, which\nemphasizes both the need for informative negatives when training with a pairwise loss, which our neigh-\nborhoods provide by returning nearby facts, and the importance of training with preﬁxes. It also afﬁrms\nthe gain of algorithm 1 for inference. Leaving facts that were never scored out of the ranking ( w/o R3)\nhas negligible impact. Ignoring scored but unselected facts too (w/o R3, S2) cuts performance by>10%:\nincluding them is thus essential. Visible facts are scored anyway, so including them all in the ranking\ncomes at no additional cost, except for once sorting them based on their score. We also trained a ran-\ndomly initialized version of distilroberta-base (instead of pre-trained). Although we did not extensively\ntune hyperparameters, the maximum obtained test MAP was about 25% of pre-trained models.\nImpact of neighborhood size k and maximum explanation length L. As tables 2d-e show, MAP\nincreases with kand L, before ﬂattening around k = 180,L = 8. Inference time per sample increases\napproximately quadratically with Land linearly with k, which is in line with the number of FW passes\nfor ARCF inference growing with O(L2k). Fig. 3 in appendix D also shows this. Table 2d shows that\nwe could have taken k= 180for inference, with virtually no loss in MAP and almost 40% speedup, but\nthe k= 290we used showed better validation results (likewise for L= 6).\nPerformance on subsets of facts. Since the leaderboards only return a scalar test MAP, we run a\nnumber of experiments on the smaller validation set. The results are therefore only indicative. Fig. 2a\nshows the number of q ∈Dval\nqa for different numbers of corresponding gold facts ( G). Fig. 2b-c show\nMAP scores of ARCF (RankNet) vs. the SF baseline on subsets of facts as marked in the dataset. ARCF\nincreases MAP on the Challenge subset with an absolute %1013. It signiﬁcantly improves retrieval of all\nroles, but most of lexical glue and grounding facts. These are facts that support central facts, which might\nbe easier to detect when using the context of other facts as our model does. This is a useful improvement,\nas reasoning of explanations that contain lexical glue and grounding facts is easier to understand14.\n13All questions in the dataset are marked by annotators with either ‘Easy’ or ‘Challenge’ (allq are one of the two).\n14Examples in appendices A and C.\n6925\nFigs. 2e-f show the MAP for facts that need an increasing number of hops to reach (from q or a fact\nto another fact, only via gold facts). In ﬁg. 2e the hops are always to a fact in the current neighborhood\nvisk(·), in ﬁg. 2f to facts with lexical overlap as seen in Das et al. (2019). The ‘ ∞’ on the x-axis shows\nthe precision for facts that cannot be reached from q in any number of hops (they can still be correctly\nretrieved by our method, but only via a negative fact). Surprisingly, the MAP of ARCF drops below that\nof SF for facts that are3 −4 neighborhood hops away. When lexical overlap hops are considered, ARCF\nperforms better than the baseline for ‘farther’ facts. The precision values for ﬁgure 2c-f are computed as\nby Jansen and Ustalov (2019), by ﬁrst removing gold facts that have another role (or are not exactly h\nhops away) both from the gold set and from the predicted ranking, and then computing the MAP of the\nremaining predicted ranking w.r.t. the remaining gold set15.\nFig. 2d shows the MAP for q’s with different numbers of gold facts G(the point at x= 2shows the\nMAP on those q that have a total of G= 2gold facts). ARCF gives a consistent improvement over the\nbaseline for all G.\n5 Conclusion\nFuture work. Future work might expand on our approach by ﬁnding alternative methods to evaluate a\nfact w.r.t. other facts in an iterative inference procedure, or by designing better fact-question neighbor-\nhood methods. Additionally, the performance of our method when the correct answer to a question is not\ngiven could be evaluated. Future work could then infer the answer from retrieved facts in a downstream\nQA setting. Finally, one could improve the method by considering to remove earlier chosen facts from\nthe intermediate set of selected facts.\nWe have proposed a new method to retrieve relevant facts for an explanation regeneration task by\niteratively evaluating candidate facts with respect to previously selected facts using a learning-to-rank\napproach. We have successfully evaluated our method on the Textgraphs 2019 and 2020 datasets and\nhave performed several ablation experiments. We have analyzed time complexity of our method and\nthe performance on different subsets of facts. By selecting the nearest facts by similarity between tf-\nidf vectors, considering not just the question but also already selected facts, only a subset of facts are\nconsidered at each step, and ARCF outperforms previous state-of-the-art methods at a higher efﬁciency.\nAcknowledgements\nThe research leading to this paper received funding from the Research Foundation Flanders (FWO)\nunder Grant Agreement No. G078618N and from the European Research Council (ERC) under Grant\nAgreement No. 788506. The Flemish Supercomputer Center (VSC) provided hardware and GPUs.\n15E.g., to compute the MAP for central facts, gold grounding and lexical glue facts are removed from the predicted ranking,\nso they do not inﬂuence the central fact MAP. The MAP of the remaining predictions w.r.t. the gold central facts is then\ncomputed.\n6926\nReferences\nPratyay Banerjee. 2019. ASU at TextGraphs 2019 shared task: Explanation ReGeneration using language models\nand iterative re-ranking. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural\nLanguage Processing (TextGraphs-13), pages 78–84. Association for Computational Linguistics.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence predic-\ntion with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1171–1179.\nChristopher JC Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullen-\nder. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd International Conference on\nMachine Learning, pages 89–96.\nWei Chen, Tie-Yan Liu, Yanyan Lan, Zhi-Ming Ma, and Hang Li. 2009. Ranking measures and loss functions in\nlearning to rank. In Advances in Neural Information Processing Systems, pages 315–323.\nYew Ken Chia, Sam Witteveen, and Martin Andrews. 2019. Red dragon ai at textgraphs 2019 shared task:\nLanguage model assisted explanation generation. In Proceedings of the Thirteenth Workshop on Graph-Based\nMethods for Natural Language Processing (TextGraphs-13), pages 85–89.\nPeter Clark, Oren Etzioni, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa\nSchoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, et al. 2019. From’f’to’a’on the ny\nregents science exams: An overview of the aristo project. arXiv preprint arXiv:1909.01958.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. 2018. Multi-step retriever-reader\ninteraction for scalable open-domain question answering. In International Conference on Learning Represen-\ntations.\nRajarshi Das, Ameya Godbole, Manzil Zaheer, Shehzaad Dhuliawala, and Andrew McCallum. 2019. Chains-\nof-reasoning at textgraphs 2019 shared task: Reasoning over chains of facts for explainable multi-hop infer-\nence. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing\n(TextGraphs-13), pages 101–117.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186.\nJennifer D’Souza, Isaiah Onando Mulang’, and S ¨oren Auer. 2019. Team SVMrank: Leveraging feature-rich\nsupport vector machines for ranking explanations to elementary science questions. In Proceedings of the Thir-\nteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 90–100.\nAssociation for Computational Linguistics.\nDaniel Fried, Peter Jansen, Gustave Hahn-Powell, Mihai Surdeanu, and Peter Clark. 2015. Higher-order lexical\nsemantic models for non-factoid answer reranking. Transactions of the Association for Computational Linguis-\ntics, 3:197–210.\nMichael Gutmann and Aapo Hyv ¨arinen. 2010. Noise-contrastive estimation: A new estimation principle for\nunnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intel-\nligence and Statistics, pages 297–304.\nPeter Jansen and Dmitry Ustalov. 2019. Textgraphs 2019 shared task on multi-hop inference for explanation\nregeneration. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language\nProcessing (TextGraphs-13), pages 63–77.\nPeter Jansen and Dmitry Ustalov. 2020. TextGraphs 2020 Shared Task on Multi-Hop Inference for Explanation\nRegeneration. In Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs) .\nAssociation for Computational Linguistics.\nPeter Jansen, Elizabeth Wainwright, Steven Marmorstein, and Clayton Morrison. 2018. Worldtree: A corpus\nof explanation graphs for elementary science questions supporting multi-hop inference. In Proceedings of the\nEleventh International Conference on Language Resources and Evaluation (LREC 2018).\nPeter Jansen. 2018. Multi-hop inference for sentence-level textgraphs: How challenging is meaningfully com-\nbining information for science question answering? In Proceedings of the Twelfth Workshop on Graph-Based\nMethods for Natural Language Processing (TextGraphs-12), pages 12–17.\n6927\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International\nConference on Learning Representations.\nMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document\ndistances. In Proceedings of the 32nd International Conference on Machine Learning, pages 957–966.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nZhuang Ma and Michael Collins. 2018. Noise contrastive estimation and negative sampling for conditional\nmodels: Consistency and statistical efﬁciency. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 3698–3707. Association for Computational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of\nwords and phrases and their compositionality. In Advances in Neural Information Processing Systems , pages\n3111–3119.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efﬁciently with noise-contrastive esti-\nmation. In Advances in Neural Information Processing Systems, pages 2265–2273.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. In Advances in Neural Information Processing Systems, pages 8026–8037.\nFabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-\nieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in\npython. Journal of Machine Learning Research, 12:2825–2830.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with\nrecurrent neural networks. In 4th International Conference on Learning Representations.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing . Association for\nComputational Linguistics, 11.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages\n5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art\nnatural language processing. ArXiv, abs/1910.03771.\nZhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, and Peter Jansen.\n2020. Worldtree v2: A corpus of science-domain structured explanations and inference patterns supporting\nmulti-hop inference. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5456–\n5473.\n6928\nAppendix A Examples of TextGraphs-2020 data\nQuestion 1 The inﬂuence of the Moon on the tides on Earth is greater than that of the Sun.\nWhich best explains this? (answer) The Moon is closer to Earth than the Sun.\nFact 0 - Role central the gravitational pull of the Moon on Earth’s oceans causes the tides\nFact 1 - Role central as distance from an object decreases , the pull of gravity on that object increases\nFact 2 - Role grounding closer means lower; less; a decrease in distance\nFact 3 - Role grounding a moon is a kind of celestial object; body\nFact 4 - Role central the moon is the celestial object that is closest to the Earth\nFact 5 - Role grounding the Sun is a kind of star\nFact 6 - Role grounding a star is a kind of celestial object; celestial body\nFact 7 - Role central the Moon is the celestial object that is closer to the Earth than the Sun\nFact 8 - Role grounding Earth is a kind of planet\nFact 9 - Role grounding a planet is a kind of celestial object; body\nFact 10 - Role lexglue cause is similar to inﬂuence\nFact 11 - Role lexglue gravity means gravitational pull; gravitational energy; gravitational force; gravita-\ntional attraction\nQuestion 2 A student placed an ice cube on a plate in the sun. Ten minutes later, only water\nwas on the plate. Which process caused the ice cube to change to water? (answer)\nmelting.\nFact 0 - Role central melting means matter; a substance changes from a solid into a liquid by increasing\nheat energy\nFact 1 - Role grounding an ice cube is a kind of solid\nFact 2 - Role grounding water is a kind of liquid at room temperature\nFact 3 - Role central water is in the solid state , called ice , for temperatures between 0; -459; -273 and\n273; 32; 0 K; F; C\nFact 4 - Role lexglue heat means heat energy\nFact 5 - Role lexglue adding heat means increasing temperature\nFact 6 - Role central if an object; a substance; a location absorbs solar energy then that object; that sub-\nstance will increase in temperature\nFact 7 - Role central if an object; something is in the sunlight then that object; that something will absorb\nsolar energy\nFact 8 - Role central the sun is a source of light; light energy called sunlight\nFact 9 - Role lexglue to be in the sun means to be in the sunlight\nFact 10 - Role central melting is a kind of process\nTable 3: Example q ∈Dval\nqa with explaining gold facts and their roles. The wrong multiple choice answer\noptions have already been removed.\nAppendix B Hyperparameters\nAll ARCF models were trained with an L2 weight decay coefﬁcient of 0.01. We tried training baselines\nsingle-fact and path-rerank with the same weight decay, but validation and test set results were lower\nthan without weight decay.\nTables 4a and 4b show the hyperparameters we used for ARCF training and inference. Hyperparam-\neters for training with different loss functions are largely the same, only the number of epochs trained\nmight differ with 1 or 2. When conditioning on negatives (CN) was used for training, we ﬁrst trained\nfor 2 epochs as normal and then started replacing a uniformly sampled proportion between 0.0 and 0.3\nof gold facts in the preﬁx f∗\n1,...,N by uniformly sampled negatives from the visible neighborhood of the\npreﬁx as it was before the replacement visk(p). Parameter Lis only relevant for inference, as it is only\nused by algorithm 1.\n6929\nHyperparam value\nLR 2e−5\nEpochs 4\nL2 weight decay 0.01\nADAM ϵ 1e−8\nADAM β1 0.9\nADAM β2 0.999\nk 180\nTokens in minibatch 5000\n(a)\nHyperparam value\nk 290\nL 9\nLmin 3\nTokens in minibatch 24k\nR3 tf-idf\nS2 true\n(b)\nTable 4: Hyperparameters used for ARCF training (with RankNet) and inference.\nAppendix C Examples of validation set predictions\nTable 5 shows the 15 highest ranked facts by ARCF (RankNet) for the two validation questions in table\n3. The ﬁrst column of each row for gold facts (which are correctly ranked high) is colored blue. As can\nbe seen in the predictions for Question 1, some wrongly predicted facts are clearly related to the question\nbut not necessary for explaining the answer (e.g. Fact 0). While others (like Fact 2) could actually be\nused for explaining the question as well. Fact 2 for question 1 in table 5 could reasonably take the place\nof gold Fact 7 for the same question in table 3.\nAppendix D Extra plots\nSee ﬁgure 3.\n100 150 200 250 300\nk\n3\n4\n5\n6\n7\n8T (s/x)\n(a)\n5 10\nL\n0\n5\n10\n15T (s/x) (b)\nFigure 3: Inference time in seconds per sample of ARCF for kand L, see also tables 2d-e. Time grows\napproximately quadratically in Land linearly in k.\n6930\nQuestion 1 The inﬂuence of the Moon on the tides on Earth is greater than that of the Sun. Which best\nexplains this? (answer) The Moon is closer to Earth than the Sun.\nFact 0 cause is similar to inﬂuence.\nFact 1 as the gravitational pull of the moon on the Earth decreases , the size of the tides on Earth\ndecrease.\nFact 2 the Moon is closer to the Earth than the Sun.\nFact 3 closer means lower; less; a decrease in distance.\nFact 4 as the gravitational pull of the moon on the Earth decreases , the size of the tides on Earth\ndecrease.\nFact 5 gravity means gravitational pull; gravitational energy; gravitational force; gravitational attrac-\ntion.\nFact 6 as distance from an object decreases , the pull of gravity on that object increases.\nFact 7 as distance from an object increases , the pull of gravity on that object decrease.\nFact 8 an increase is the opposite of a decrease.\nFact 9 as the distance from an object increases , the force of gravity on that object will decrease.\nFact 10 a moon is a kind of celestial object; body.\nFact 11 the gravitational pull of the Moon on Earth’s oceans causes the tides.\nFact 12 the gravitational pull of the Sun on Earth’s oceans causes the tides.\nFact 13 less is similar to decrease.\nFact 14 to lower means to decrease.\nQuestion 2 A student placed an ice cube on a plate in the sun. Ten minutes later, only water was on the plate.\nWhich process caused the ice cube to change to water? (answer) melting.\nFact 0 melting is a kind of process.\nFact 1 melting means matter; a substance changes from a solid into a liquid by increasing heat energy.\nFact 2 an ice cube is a kind of solid.\nFact 3 water is a kind of substance.\nFact 4 water is a kind of liquid at room temperature.\nFact 5 water is in the solid state , called ice , for temperatures between 0; -459; -273 and 273; 32; 0 K;\nF; C.\nFact 6 temperature is a measure of heat energy.\nFact 7 heat means heat energy.\nFact 8 water is in the liquid state , called liquid water , for temperatures between 273; 32; 0 and 373;\n212; 100 K; F; C.\nFact 9 ice is a kind of solid.\nFact 10 if an object; a substance; a location absorbs solar energy then that object; that substance will\nincrease in temperature.\nFact 11 melting is when solids are heated above their melting point.\nFact 12 adding heat means increasing temperature.\nFact 13 cooling;colder means removing;reducing;decreasing heat;temperature.\nFact 14 heating means adding heat.\nTable 5: Example q ∈Dval\nqa with explaining gold facts and their roles. The wrong multiple choice answer\noptions have already been removed.",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.8326295018196106
    },
    {
      "name": "Computer science",
      "score": 0.7791111469268799
    },
    {
      "name": "Pairwise comparison",
      "score": 0.6699700355529785
    },
    {
      "name": "Transformer",
      "score": 0.6233788728713989
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5753878951072693
    },
    {
      "name": "Machine learning",
      "score": 0.5414429306983948
    },
    {
      "name": "Limiting",
      "score": 0.4661470949649811
    },
    {
      "name": "Question answering",
      "score": 0.41467684507369995
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3411783277988434
    },
    {
      "name": "Natural language processing",
      "score": 0.33826836943626404
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    }
  ]
}