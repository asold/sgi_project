{
  "title": "ParaFormer: Parallel Attention Transformer for Efficient Feature Matching",
  "url": "https://openalex.org/W4382466643",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2130370510",
      "name": "Xiaoyong Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115466155",
      "name": "Yaping Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152636564",
      "name": "Bin Kang",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2130601305",
      "name": "Songlin Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130370510",
      "name": "Xiaoyong Lu",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2115466155",
      "name": "Yaping Yan",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2152636564",
      "name": "Bin Kang",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2130601305",
      "name": "Songlin Du",
      "affiliations": [
        "Southeast University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2608303904",
    "https://openalex.org/W6637400245",
    "https://openalex.org/W1491719799",
    "https://openalex.org/W6810044168",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2775929773",
    "https://openalex.org/W2979458572",
    "https://openalex.org/W6721610202",
    "https://openalex.org/W6757634740",
    "https://openalex.org/W2796059865",
    "https://openalex.org/W1230023165",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W6752208963",
    "https://openalex.org/W2795296309",
    "https://openalex.org/W2991115635",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W6677308130",
    "https://openalex.org/W2990957769",
    "https://openalex.org/W2991526275",
    "https://openalex.org/W3140551255",
    "https://openalex.org/W6804668344",
    "https://openalex.org/W6928992657",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4221154336",
    "https://openalex.org/W3023954776",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6746247744",
    "https://openalex.org/W2967640080",
    "https://openalex.org/W3109017968",
    "https://openalex.org/W2117228865",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W2963748588",
    "https://openalex.org/W1677409904",
    "https://openalex.org/W2474281075",
    "https://openalex.org/W3103455452",
    "https://openalex.org/W2963154697",
    "https://openalex.org/W2804394440",
    "https://openalex.org/W2990655570",
    "https://openalex.org/W2963674285",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3175295430",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3166285241",
    "https://openalex.org/W2963760790",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W3107540572",
    "https://openalex.org/W4221150397",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W4312853765"
  ],
  "abstract": "Heavy computation is a bottleneck limiting deep-learning-based feature matching algorithms to be applied in many real-time applications. However, existing lightweight networks optimized for Euclidean data cannot address classical feature matching tasks, since sparse keypoint based descriptors are expected to be matched. This paper tackles this problem and proposes two concepts: 1) a novel parallel attention model entitled ParaFormer and 2) a graph based U-Net architecture with attentional pooling. First, ParaFormer fuses features and keypoint positions through the concept of amplitude and phase, and integrates self- and cross-attention in a parallel manner which achieves a win-win performance in terms of accuracy and efficiency. Second, with U-Net architecture and proposed attentional pooling, the ParaFormer-U variant significantly reduces computational complexity, and minimize performance loss caused by downsampling. Sufficient experiments on various applications, including homography estimation, pose estimation, and image matching, demonstrate that ParaFormer achieves state-of-the-art performance while maintaining high efficiency. The efficient ParaFormer-U variant achieves comparable performance with less than 50% FLOPs of the existing attention-based models.",
  "full_text": "ParaFormer: Parallel Attention Transformer for Efficient Feature Matching\nXiaoyong Lu1, Yaping Yan1, Bin Kang2, Songlin Du1*\n1Southeast University, Nanjing, China\n2Nanjing University of Posts and Telecommunication, Nanjing, China\n{220211846, yan, sdu}@seu.edu.cn, kangb@njupt.edu.cn\nAbstract\nHeavy computation is a bottleneck limiting deep-learning-\nbased feature matching algorithms to be applied in many real-\ntime applications. However, existing lightweight networks\noptimized for Euclidean data cannot address classical feature\nmatching tasks, since sparse keypoint based descriptors are\nexpected to be matched. This paper tackles this problem and\nproposes two concepts: 1) a novel parallel attention model\nentitled ParaFormer and 2) a graph based U-Net architec-\nture with attentional pooling. First, ParaFormer fuses features\nand keypoint positions through the concept of amplitude and\nphase, and integrates self- and cross-attention in a parallel\nmanner which achieves a win-win performance in terms of\naccuracy and efficiency. Second, with U-Net architecture and\nproposed attentional pooling, the ParaFormer-U variant sig-\nnificantly reduces computational complexity, and minimize\nperformance loss caused by downsampling. Sufficient exper-\niments on various applications, including homography esti-\nmation, pose estimation, and image matching, demonstrate\nthat ParaFormer achieves state-of-the-art performance while\nmaintaining high efficiency. The efficient ParaFormer-U vari-\nant achieves comparable performance with less than 50%\nFLOPs of the existing attention-based models.\nIntroduction\nFeature matching is a fundamental problem for many com-\nputer vision tasks, such as object recognition (Liu et al.\n2008), structure from motion (SFM) (Schonberger and\nFrahm 2016), and simultaneous localization and mapping\n(SLAM) (Engel, Koltun, and Cremers 2017). But with il-\nlumination changes, viewpoint changes, motion blur and oc-\nclusion, it is challenging to find the invariance and get robust\nmatches from two images.\nFeature matching pipelines can be categorized into\ndetector-based methods, which first detect keypoints and de-\nscriptors from the images and then match two sets of sparse\nfeatures, and detector-free methods, which directly match\ndense features. Benefiting from the global modeling capa-\nbility of Transformer (Vaswani et al. 2017), attention-based\nnetworks become dominant methods in both detector-based\nand detector-free pipelines, where self- and cross-attention\n*Corresponding author\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Comparison between the ParaFormer and Super-\nGlue. With the same input features, ParaFormer can de-\nliver more robust matches with higher matching precision.\nParaFormer-U can achieve comparable performance to Su-\nperGLue with significantly fewer FLOPs.\nare applied to match learning-based descriptors or dense\nfeatures. However, despite the high performance, attention-\nbased networks tend to bring high training costs, large mem-\nory requirements, and high inference latency, especially for\ndetector-free pipelines, where processing dense features ex-\nacerbates the problem of quadratic complexity of attention\nmechanism. So we focus on detector-based pipeline, seek-\ning the best trade-off between efficiency and performance.\nAs most lightweight operations (Chollet 2017; Howard\net al. 2017) are designed for Euclidean data, sparse descrip-\ntors cannot be handled by mainstream lightweight networks.\nNote that Transformer and Graph Neural Networks are suit-\nable for processing non-Euclidean data, so we design ef-\nficient models from both perspectives, giving birth to the\nParaFormer and its ParaFormer-U variant.\nRethinking the self- and cross-attention in feature match-\ning, all existing attention-based methods arrange two kinds\nof attention in a serial manner, a strategy derived from the\nbehavior of people looking back and forth when match-\ning images. Specifically, SuperGlue (Sarlin et al. 2020) and\nLoFTR (Sun et al. 2021) alternately arrange self- and cross-\nattention, i.e., the self → cross strategy as illustrated\nin Figure 2 (a). For MatchFormer (Wang et al. 2022), the\nself → self → cross strategy is used in the early stages,\nand the self → cross → cross strategy is used in later\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1853\nstages as shown in Figure 2 (b). However, computer vision is\nnot necessarily designed based on human behavior, the fixed\nserial attention structure limits the diversity of the integra-\ntion of self- and cross-attention. We propose parallel atten-\ntion to compute self- and cross-attention synchronously, and\ntrain the network to optimally fuse two kinds of attention in-\nstead of tuning the permutation of both as a hyperparameter.\nFor the efficiency of the attention-based feature match-\ning, instead of simply applying attention variants (Shen et al.\n2021; Wang et al. 2021), weight sharing and attention weight\nsharing strategies in the parallel attention layer are explored\nto reduce redundant parameters and computations. We fur-\nther construct the U-Net architecture with parallel attention\nlayers and propose attentional pooling, which identifies im-\nportant context points by attention weights.\nIn summary, the contributions of this paper include:\n• We rethink the attention-based feature matching net-\nworks, and propose the parallel attention layer to per-\nform self- and cross-attention synchronously and adap-\ntively integrate both with learnable networks.\n• We further explore the U-Net architecture for efficient\nfeature matching and propose attentional pooling, which\nkeeps only the important context points to reduce the\nFLOPs with minimal performance loss.\n• A novel wave-based position encoder is proposed for\ndetector-based feature matching networks, which dy-\nnamically fuses descriptors and positions through the\nconcepts of amplitude and phase of waves.\nRelated Works\nLocal Feature Matching. Classical feature matching tends\nto be a detector-based pipeline, i.e., the detector is first ap-\nplied to generate keypoints and descriptors from images, and\nthen the descriptors are matched. For detectors, some out-\nstanding handcrafted methods (Lowe 2004; Bay, Tuytelaars,\nand Van G. 2006; Calonder et al. 2010; Rublee et al. 2011)\nwere first proposed and widely used for various 3D com-\nputer vision tasks. With the advent of the deep learning era,\nmany learning-based detectors (Revaud et al. 2019; DeTone,\nMalisiewicz, and Rabinovich 2018; Dusmanu et al. 2019;\nOno et al. 2018) have been proposed to further improve\nthe robustness of descriptors under illumination changes and\nviewpoint changes. In addition to detectors, other work has\nfocused on better matchers. SuperGlue (Sarlin et al. 2020)\nwas the first to propose an attention-based feature matching\nnetwork that uses self- and cross-attention to find matches\nwith global context information. OETR (Chen et al. 2022)\nfurther constrains attention-based feature matching in the\ncommonly visible region by overlap estimation.\nBesides matching the sparse descriptors generated by the\ndetector, LoFTR (Sun et al. 2021) applies self- and cross-\nattention directly on the feature maps extracted by convo-\nlutional neural network (CNN) and generates matches in\na coarse-to-fine manner. MatchFormer (Wang et al. 2022)\nfurther abandons the CNN backbone and adopts a com-\npletely attention-based hierarchical framework that can ex-\ntract features while finding similarities utilizing the atten-\ntion mechanism. Noting that the permutation of self- and\n×L×L\n×L1\nN×C M×C\n×L\nN×C M×C\nN×C M×C\n×L2\nN×C M×C\nN×C M×C\nN×C M×C\n(a) serial attention\n(b) interleaving attention(c) parallel attention\nself cross self\nfusion fusion\nself self\ncross cross\ncross cross\ncrosscross\nself self\ncrosscross\nself self\nself self\nFigure 2: Conceptual difference among three attention archi-\ntectures. (a) Serial attention in SuperGlue. (b) Interleaving\nattention in MatchFormer. (c) Proposed parallel attention.\ncross-attention in SuperGlue and LoFTR is a simple alter-\nnating strategy, MatchFormer further proposes an interleav-\ning strategy, which focuses on self-attention at the shallow\nstages of the network and cross-attention at the deep stages.\nThis improvement gives us inspiration about the permutation\nof self- and cross-attention.\nAll existing attention-based approaches artificially ar-\nrange self- and cross-attention in a serial manner to mimic\nhuman behavior, which does not take advantage of the ben-\nefits of deep learning network and parallel computing. We\npropose to compute two kinds of attention efficiently in a\nparallel manner, and let the network learn the optimal way\nto integrate the two kinds of attention.\nPosition Encoder.The position encoder is a critical part for\nall transformer-based networks, which allows the network to\nsense the relative or absolute position of each vector in a sen-\ntence or image. The first proposed position encoding method\n(Vaswani et al. 2017) uses fixed sine and cosine functions\nto calculate the position encoding or uses the position en-\ncoding vector as a learnable parameter, and finally adds the\nposition encoding to the original vector. Although position\ninformation can be provided, this approach severely limits\nthe flexibility of the model because the position encodings\nare fixed-length at training time, which limits the model to\nonly process fixed-length inputs at inference time.\nAnother way of position encoding is relative position en-\ncoding (Liu et al. 2021), i.e., adjusting attention weights\nwith relative position. However, it is not only computation-\nally intensive but also needs to handle inputs of different\nlengths by interpolation, which severely damages the per-\nformance. Convolution-based position encoders (Chu et al.\n2021; Wang et al. 2022) are proposed to augment local fea-\ntures with convolution and enable the model to be aware of\nposition information with zero padding. But this method can\nonly be applied to Euclidean data such as feature maps, thus\nit cannot be applied to methods based on sparse descriptors.\nTo handle arbitrary length and non-Euclidean inputs, Su-\nperGlue proposes a position encoder based on the multi-\nlayer perceptron (MLP), which uses MLP to extend the co-\n1854\nself attention\nConcat & MLP\nN×C\ncross\nN×C M×C\nself self\ncross attention\nQKV project\nConcat & MLP\nQKV projectN×3\nself attention\nMLPA MLPθ  \nA θ  \nAcosθ  Asinθ  \nConcat & MLPF\nAmplitude Phase\nReal\nPart\nImag.\nPart\n×L\nParallel Attention Layer Wave Position Encoder\nd p\nx0\nxl yl\nxl+1 yl+1\nM×C M×3\nWave-PE Wave-PE\nfusion fusion\nweight sharing\nFigure 3: ParaFormer architecture. Wave-PE fuses the amplitude A estimated with the descriptor d and the phase θ estimated\nwith the position p to generate position encoding. Stacked parallel attention layers utilize self- and cross-attention to enhance\nthe descriptors and find potential matches, where self- and cross-attention are adaptively integrated through a learnable network.\nordinate vector to align with the dimension of the descriptor\nto get the position encoding. However, the weak encoding\nability becomes the bottleneck of the matching network. In-\nspired by Wave-MLP (Tang et al. 2022), phase information\nis equally important in vectors compared to amplitude infor-\nmation. Wave-MLP encodes the same input as both ampli-\ntude and phase information, while we encode the descriptor\nas amplitude information and the position as phase informa-\ntion, then fuse the two types of information with the Euler\nformula to generate position-aware descriptors.\nU-Net Architecture. The U-Net (Ronneberger, Fischer, and\nBrox 2015) architecture consists of an encoder-decoder\nstructure, where the encoder reduces the spatial resolution\nand the decoder recovers it. This architecture can efficiently\nhandle dense prediction tasks such as semantic segmenta-\ntion, so we seek to improve the efficiency of attention-based\nfeature matching with U-Net. However conventional pool-\ning operations cannot be directly applied to non-Euclidean\ndata like sparse descriptors, so Graph U-Nets (Gao and Ji\n2019) proposes the graph pooling (gPool) layer to enable\ndownsampling on graph data in a differentiable way. The\ngPool layer measures the information that can be retained\nby each feature vector through scalar projection and applies\ntopk sampling so that the new graph preserves as much in-\nformation as possible from the original graph. Based on the\ngPool layer, we propose to utilize attention weights to mea-\nsure how much information can be retained by each feature\nvector, which can better cooperate with the attention-based\nnetwork without introducing additional parameters.\nMethodology\nAssuming that M and N keypoints are detected in image X\nand image Y, we let the positions be pX ∈ RM×3, pY ∈\nRN×3 and the descriptors be dX ∈ RM×C, dY ∈ RN×C.\nAs illustrated in Figure 3, our proposed method first dynam-\nically fuses positions p and descriptors d in amplitude and\nphase manner with wave position encoder (Wave-PE). The\nparallel attention module is then applied to compute self-\nand cross-attention synchronously, utilizing global informa-\ntion to enhance the representation capability of features and\nfind potential matches.xl and yl denote the intermediate fea-\ntures of image X and Y in layer l. Finally, the enhanced de-\nscriptors are matched by the optimal matching layer (Sarlin\net al. 2020) appliying the Sinkhorn algorithm.\nWave Position Encoder\nFor the MLP-based position encoder (MLP-PE), the main\ndrawback is the limited encoding capacity because the pa-\nrameters of MLP-PE are less than 1%of the whole network,\nyet the position information is important for feature match-\ning. Therefore, Wave-PE is designed to dynamically adjust\nthe relationship between descriptor and position by ampli-\ntude and phase to obtain better position encoding.\nIn Wave-PE, position encoding is represented as a wave\n˜w with both amplitude A and phase θ information, and the\nEuler formula is employed to unfold waves into real parts\nand imaginary parts to process waves efficiently,\n˜wj = Aj ⊙ eiθj\n= Aj ⊙ cos θj + iAj ⊙ sin θj, j= 1,2, ..., n. (1)\nAs shown in Figure 3, the amplitude and phase are estimated\nby two learnable networks via descriptors and position, re-\nspectively. Then a learnable network is applied to fuse the\nreal and imaginary parts into position encoding,\nAj = MLPA(dj),\nθj = MLPθ(pj),\nx0\nj = dj + MLPF ([Aj ⊙ cos θj, Aj ⊙ sin θj]).\n(2)\n[·, ·] denotes concatenation. For three learnable networks in\nequation (2), two-layer MLP is chosen for simplicity.\nParallel Attention\nAs illustrated in the right side of Figure 3, the two sets\nof descriptors are first linearly projected as Q, K, V .\nThen self- and cross-attention are computed in a paral-\nlel manner. In the self-attention module, standard attention\ncomputation softmax(QK T /\n√\nd)V is employed, where\nQ, K, V come from the same input, i.e., (Qx, Kx, V x) or\n(Qy, Ky, V y). In the cross-attention module, the attention\n1855\n(N, C1)\n(N/2, C2)\n(N/4, C3)\n(a)\nidx\nsigmoid\ngating\n⊙\nattention weight\ninput (M, Cl)\noutput (M/2, Cl+1)\n(b)\nsum\ntopk\nLinear(Cl, Cl+1)\nstage 1\nstage 2\nstage 3\nstage 4\nstage 5\npooling\nscore\nskip connectionskip connectionunpooling unpooling attentional poolingattentional poolingparallel attention layersparallel attention layers\nFigure 4: (a) U-Net architecture. The descriptors are processed in an encoder-decoder fashion. After the stages 1 and 2, the\ndescriptors are downsampled with attention pooling to filter out the insignificant descriptors. After the stages 4 and 5, the\ndescriptors are upsampled and fused with descriptors in previous stage by skip connections. (b) Attentional pooling. Pooling\nscores are computed from attention weights to identify context points and provide the gating signal through sigmoid function.\nweight sharing strategy is proposed to improve model effi-\nciency, which is replacing QyKT\nx with (QxKT\ny )T , so the\ninput of the cross-attention module is (Q x, V x, Ky, V y).\nThe impact of weight sharing and attention weight sharing is\ninvestigated in the ablation studies. Finally, self- and cross-\nattention outputs are fused by a two-layer MLP. Parallel at-\ntention saves redundant parameters and computations while\nboosting performance through learnable fusion.\nSince the parallel attention layer updates two sets of de-\nscriptors simultaneously, it is formally similar to the self-\nattention layer in mainstream Transformers, except with two\ninputs. We can simply stack L parallel attention layers to\nform ParaFormer and conveniently explore various architec-\ntures like U-Net architecture to design model variants.\nU-Net Architecture\nAs shown in Figure 4 (a), ParaFormer-U is designed for ef-\nficiency. Spatial downsampling is performed first to extract\nthe high-level semantic information, then upsampling is per-\nformed to recover the spatial information, and the low-level\nand high-level information are fused by skip connections.\nAs illustrated in Figure 4 (b), attentional pooling is pro-\nposed for downsampling. Observing the attention map by\ncolumn shows that certain points have strong weight with\nall other points, indicating that they are important context\npoints in the image. Suppose the feature in layer l is xl ∈\nRN×D and the attention weight is Al ∈ RN×N . Our pro-\nposed attentional pooling is defined as\ns = sum(A, dim= 1),\nidx = rank(s, k),\n˜xl = Linear(xl(idx, :)),\ng = sigmoid(s(idx))\nxl+1 = ˜xl ⊙ g.\n(3)\nk is the number of points selected for next layer l+1, which\nis set to half the number of points in previous layer l. The\nsum of each column of the self-attention map is computed\nas the pooling score s ∈ RN , which measures the impor-\ntance of each point. Then topk points are selected based on\nthe attentional pooling score to filter out insignificant points,\nand the Linear layer is used to adjust the dimension size of\nthe descriptors. s(idx) extracts values in s with indices idx\nfollowed by a sigmoid operation to generate gating signal,\nand ⊙ represents the element-wise matrix multiplication.\nFollowing (Gao and Ji 2019), the unpooling operation is\ndefined as\n˜xl = Linear(xl),\nxl+1 = distribute(0N×Cl+1 , ˜xl, idx),\n(4)\nwhere xl ∈ Rk×Cl\nis the current feature matrix and\n0N×Cl+1 initially empty feature matrix for the next layer.\nThe Linear layer is employed first to adjust the feature ma-\ntrix dimension. idx ∈ Rk is the indices of points selected\nin the corresponding pooling layer. Then the current feature\nmatrix is inserted into the corresponding row of the empty\nfeature matrix according to idx, while the other rows remain\nzero. In other words, the unselected features in the pooling\nlayer are represented by zero vectors to perform upsampling.\nImplementation Details\nThe homography model is pretrained on the R1M dataset\n(Radenovi´c et al. 2018), and then the model is finetuned on\nthe MegaDepth dataset (Li and Snavely 2018) for outdoor\npose estimation and image matching tasks. On the R1M\ndataset, we employ the AdamW (Kingma and Ba 2014) op-\ntimizer for 10 epochs using the cosine decay learning rate\nscheduler and 1 epoch of linear warm-up. A batch size of\n8 and an initial learning rate of 0.0001 are used. On the\nMegaDepth dataset, we use the same AdamW optimizer for\n1856\nMatcher AUC Precision\nRecall F1-score\nNN 39.47 21.7 65.4\n32.59\nNN + mutual 42.45 43.8 56.5 49.35\nNN + PointCN 43.02 76.2 64.2 69.69\nNN + OANet 44.55 82.8 64.7 72.64\nSuperGlue 52.65 90.9 98.88 94.72\nParaFormer-U 53.16 90.93 99.01 94.80\nParaFormer 54.91 94.55 99.10 96.77\nTable 1: Homography estimation onR1M. AUC @10 pixels\nis reported. The best method is highlighted in bold.\n50 epochs using the same learning rate scheduler and lin-\near warm-up. A batch size of 2 and a lower initial learning\nrate of 0.00001 are used. For training on R1M/MegaDepth\ndataset, we resize the images to 640×480/960×720 pix-\nels and detect 512/1024 keypoints, respectively. When the\ndetected keypoints are not enough, random keypoints are\nadded for efficient batching. All models are trained on a sin-\ngle NVIDIA 3070Ti GPU. For ParaFormer, we stack L = 9\nparallel attention layers, and all intermediate features have\nthe same dimension C = 256. For ParaFormer-U, the depth\nof each stage is {2, 1, 2, 1, 2}, resulting in a total of L = 8\nparallel attention layers, and the intermediate feature dimen-\nsion of each stage is {256, 384, 128, 384, 256}. More details\nare provided in the supplementary material.\nExperiments\nHomography Estimation\nDataset. We split R1M dataset (Radenovi ´c et al. 2018),\nwhich contains over a million images of Oxford and Paris,\ninto training, validation, and testing sets. To perform self-\nsupervised training, random ground-truth homographies are\ngenerated to get image pairs.\nBaselines. SuperPoint (DeTone, Malisiewicz, and Rabi-\nnovich 2018) is applied as the unified descriptor to gener-\nate the input for the matcher. ParaFormer and ParaFormer-U\nare compared with attention-based matcher SuperGlue (Sar-\nlin et al. 2020) and NN matcher with learning-based outlier\nrejection methods (Yi et al. 2018; Zhang et al. 2019). The\nresults of SuperGlue are from our own implementation.\nMetrics. Precision and recall are computed based on ground\ntruth matches. The area under the cumulative error curve\n(AUC) up to a value of 10 pixels is reported, where the repro-\njection error is computed with the estimated homography.\nResults. As shown in Table 1, ParaFormer outperforms all\noutlier rejection methods and attention-based matcher on\nhomography estimation. It can be seen that the attention-\nbased approaches have a remarkable superiority due to\nthe global receptive field of attention. Compared with\nthe attention-based approach SuperGlue, ParaFormer fur-\nther boosts the performance by integrating self- and cross-\nattention with parallel attention layers, bringing a +2.05%\nimprovement on the F1-score over SuperGlue. The visu-\nalization of matches can be found in Figure 6. Moreover,\ncompared to SuperGlue, our efficient U-Net variant has only\n49% FLOPs, yet achieves better performance.\nMatcher\nExact AUC Approx.\nAUC\n@5◦ @10◦ @20◦ @5◦ @10◦ @20◦\nNN + mutual 16.94\n30.39 45.72 35.00 43.12 54.25\nNN + OANet 26.82 45.04 62.17 50.94 61.41 71.77\nSuperGlue 28.45 48.6 67.19 55.67 66.83 74.58\nParaFormer-U 29.40 49.76 68.29 56.47 67.66 75.67\nParaFormer 31.73 52.28 70.43 60.05 70.72 78.13\nTable 2: Pose estimation on YFCC100M. ParaFormer and\nParaFormer-U lead other methods at all thresholds.\nOutdoor Pose Estimation\nDataset. ParaFormer is trained on the MegaDepth dataset\n(Li and Snavely 2018) and evaluated on the YFCC100M\ndataset (Thomee et al. 2016). For training, 200 pairs of im-\nages in each scene are randomly sampled for each epoch. For\nevaluation, the YFCC100M image pairs and ground truth\nposes provided by SuperGlue are used.\nBaselines. SuperPoint is applied as the descriptor and com-\nbined with baseline matchers, which contain attention-based\nmatcher SuperGlue and NN matcher with outlier rejection\nmethods (Lowe 2004; Zhang et al. 2019). The results of Su-\nperGlue are from our own implementation.\nMetrics. The AUC of the pose error at thresholds (5 ◦, 10◦,\n20◦) are reported. Evaluation is performed with both approx-\nimate AUC (Zhang et al. 2019) and exact AUC (Sarlin et al.\n2020) for a fair comparison.\nResults. As shown in Table 2, ParaFormer achieves the\nbest performance at all thresholds, demonstrating the ro-\nbustness of our models. With wave position encoder\nand parallel attention architecture, ParaFormer can bring\n(+3.28%, +4.2%, +3.24%) improvement on exact AUC\nand (+4.38%, +3.89%, +3.55%) improvement on approx-\nimate AUC at three thresholds of (5◦, 10◦, 20◦), respec-\ntively. In outdoor scenes with a large number of keypoints,\nParaFormer-U can effectively alleviate the computational\ncomplexity problem by downsampling, while still maintain-\ning state-of-the-art performance by attentional pooling.\nImage Matching\nDataset. We follow the evaluation protocol as in D2-Net\n(Dusmanu et al. 2019) and evaluate our methods on 108\nHPatches (Balntas et al. 2017) sequences, which contain 52\nsequences with illumination changes and 56 sequences with\nviewpoint changes.\nBaselines. Baseline methods include learning-based de-\nscriptors R2D2, D2Net and SuperPoint (Revaud et al. 2019;\nDusmanu et al. 2019; DeTone, Malisiewicz, and Rabinovich\n2018) and advanced matchers LoFTR, Patch2Pix, Super-\nGlue and CAPS (Sun et al. 2021; Zhou, Sattler, and Leal-\nTaixe 2021; Sarlin et al. 2020; Wang et al. 2020). The results\nof SuperGlue are from our own implementation.\nMetrics. A match is considered correct if the reprojection\nerror is below the matching threshold, where the reprojec-\ntion error is computed from the homographies provided by\nthe dataset. The matching threshold is varied from 1 to 10\n1857\n1 2 3 4 5 6 7 8 9 10\nthreshold (px)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MMA\nOverall\nParaFormer\nParaFormer-U\nSuperGlue\nLoFTR\nPatch2Pix\nSuperPoint + CAPS\nR2D2\nD2Net\nSuperPoint\n1 2 3 4 5 6 7 8 9 10\nthreshold (px)\nIllumination\n1 2 3 4 5 6 7 8 9 10\nthreshold (px)\nViewpoint\nFigure 5: Image matching on HPatches. The mean matching accuracy (MMA) at thresholds from 1 to 10 pixels are reported.\nSA PA MLP-PE Wav\ne-PE Precision Recall F1-score\n✓ ✓ 86.68 96.56 91.35\n✓\n✓ 86.79 98.23 92.16\n✓ ✓ 87.69 98.73 92.88\nTable 3: Ablation study on main designs.\nFFN QKV proj Head\nMerging #Params (M) F1-score\n1.70 88.79\n✓ 1.18 80.86\n✓ 1.51 90.15\n✓ 1.51 90.16\n✓ ✓ 1.31\n90.02\nTable 4: Ablation study on weight sharing.\nto plot the mean matching accuracy (MMA), which is the\naverage percentage of correct matches for each image.\nResults. As shown in Figure 5, ParaFormer achieves the best\noverall performance at matching thresholds of 5 or more pix-\nels. The results indicate that detector-based methods such as\nSuperGlue and our methods are better at handling scenarios\nwith large viewpoint changes, while the detector-free meth-\nods such as LoFTR are better suited to address illumination\nchanges. But ParaFormer still outperforms LoFTR in illu-\nmination change experiments, benefiting from the superior\nmodeling capability of parallel attention. With ParaFormer\nand ParaFormer-U, the overall performance of SuperPoint\ngrows from the last to the first and the second place, demon-\nstrating the effectiveness of our matchers.\nParaFormer Structural Study\nA complete ablation study is conducted on the R1M dataset\nfor further understanding of our designs. The FLOPs and\nruntimes between our methods and SuperGlue are also com-\npared to demonstrate the high efficiency of our methods.\nMain Designs. We did ablation experiments on parallel at-\ntention architecture and Wave-PE. As can be seen from Ta-\nble 3, when both use MLP-PE, parallel attention leads serial\nattention on both precision and recall, resulting in a+0.81%\nimprovement on F1-score. When parallel attention is com-\nattention weight sharing FLOPs\n(G) F1-score\n108.72 89.98\n✓ 99.05 90.02\nTable 5: Ablation study on attention weight sharing.\nrandom pooling gPool attentional pooling F1-score\n✓ 90.87\n✓ 90.95\n✓ 91.23\nTable 6: Ablation study on pooling.\nbined with Wave-PE, the performance can be further boosted\nby +0.72% over MLP-PE, indicating that Wave-PE provides\nstronger position information to guide matching.\nWeight Sharing. As shown in Table 4, we conduct ablation\nexperiments on weight sharing strategies and find that the\nperformance of the network improved when self-attention\nand cross-attention share the Q, K, V projection weights\nand the multi-head merging weights, while it also helps to\nreduce the model parameters. This occurs because self- and\ncross-attention are essentially indistinguishable except for\nthe inputs, and the shared weights align the Q, K, V pro-\njections of both inputs so that self- and cross-attention can\nbe performed in the same vector space, which makes a uni-\nform standard for the cosine similarity of both.\nAttention Weight Sharing. We find that computing cross-\nattention twice is redundant for attention-based methods, be-\ncause of the high correlation between QyKT\nx and QxKT\ny .\nSo attention weight sharing is proposed for efficiency, i.e.,\nreplacing QyKT\nx with (QxKT\ny )T . As shown in Table 5, at-\ntention weight sharing can reduce FLOPs without perfor-\nmance loss, which makes a significant difference in scenar-\nios with a large number of keypoints.\nAttentional Pooling. As shown in Table 6, compared to\ngPool (Gao and Ji 2019) which gets pooling scores by linear\nprojection, our proposed attentional pooling achieves better\nperformance and saves the parameters of linear projection\nby identifying important context points by attention weights.\n1858\nParaFormerSuperGlue\nHomography Esitimation\nParaFormerSuperGlue\nPose Esitimation\nFigure 6: Qualitative results of homography estimation and pose estimation experiments.\n0 1000 2000 3000 4000\nnumber of keypoints\n0\n50\n100\n150\n200\n250\n300\n350\n400FLOPs /G\nSuperGlue\nParaFormer\nParaFormer-U\nFigure 7: Comparison between FLOPs of models.\nAs expected, the strategy of computing pooling scores by\nfeatures or attention weights is superior to random pooling.\nEfficiency Analysis. Benefiting from the above designs,\nour model is remarkable in efficiency beyond just achiev-\ning state-of-the-art performance. As shown in Table 7, when\nmatching 2048 descriptors, ParaFormer reduces FLOPs by\n17.8% compared to SuperGlue with better performance.\nParaFormer-U further improves efficiency with FLOPs of\nonly 49% of SuperGlue, while it still outperforms SuperGlue\ndue to the advantage of parallel attention and Wave-PE. As\nshown in Figure 7, the attention weight sharing strategy in\nParaFormer alleviates the squared complexity of the atten-\ntion mechanism, and the U-Net architecture further signifi-\ncantly reduces computational cost through downsampling.\nMethods F1-score FLOPs (G)\nRuntime (ms)\nSuperGlue 90.68 125.85 26.99\nP\naraFormer-U 90.72 61.67 20.23\nParaFormer 94.92 104.22 24.99\nTable 7: Efficiency analysis @2048 keypoints.\nConclusion\nIn this paper, we propose a novel attention-based network\nnamed ParaFormer to handle feature matching tasks effi-\nciently. As a preprocessing module, the proposed Wave-PE\ndynamically fuses features and positions in amplitude and\nphase manner. In contrast to employing serial attention that\nintuitively mimics human behavior, we propose a parallel\nattention architecture that not only integrates self-attention\nand cross-attention in a learnable way but also saves redun-\ndant parameters and computations through weight sharing\nand attention weight sharing strategies. To further improve\nefficiency, the ParaFormer-U is designed with U-Net archi-\ntecture, which reduces FLOPs by downsampling and mini-\nmizes the performance loss by the proposed attentional pool-\ning. Experiments show that ParaFormer and ParaFormer-\nU deliver state-of-the-art performance with remarkable effi-\nciency, enabling a broader application scenario for attention-\nbased feature matching networks.\nAcknowledgments\nThis work was jointly supported by the National Natural Sci-\nence Foundation of China under grants 62001110, 62201142\nand 62171232, the Natural Science Foundation of Jiangsu\nProvince under grant BK20200353, and the China Postdoc-\ntoral Science Foundation under grant 2020M681684.\n1859\nReferences\nBalntas, V .; Lenc, K.; Vedaldi, A.; and Mikolajczyk, K.\n2017. HPatches: A Benchmark and Evaluation of Hand-\ncrafted and Learned Local Descriptors. In Proceedings of\nthe CVPR, 5173–5182.\nBay, H.; Tuytelaars, T.; and Van G., L. 2006. SURF:\nSpeeded Up Robust Features. In Proceedings of the ECCV,\n404–417.\nCalonder, M.; Lepetit, V .; Strecha, C.; and Fua, P. 2010.\nBRIEF: Binary Robust Independent Elementary Features. In\nProceedings of the ECCV, 778–792.\nChen, Y .; Huang, D.; Xu, S.; Liu, J.; and Liu, Y . 2022. Guide\nLocal Feature Matching by Overlap Estimation. InProceed-\nings of the AAAI.\nChollet, F. 2017. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the CVPR, 1251–\n1258.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.; and\nShen, C. 2021. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882.\nDeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2018. Su-\nperPoint: Self-Supervised Interest Point Detection and De-\nscription. In Proceedings of the CVPRW, 224–236.\nDusmanu, M.; Rocco, I.; Pajdla, T.; Pollefeys, M.; Sivic, J.;\nTorii, A.; and Sattler, T. 2019. D2-Net: A Trainable CNN\nfor Joint Description and Detection of Local Features. In\nProceedings of the CVPR, 8092–8101.\nEngel, J.; Koltun, V .; and Cremers, D. 2017. Direct sparse\nodometry. Journal of the PAMI, 40(3): 611–625.\nGao, H.; and Ji, S. 2019. Graph U-Nets. In Chaudhuri,\nK.; and Salakhutdinov, R., eds., Proceedings of the ICML,\n2083–2092.\nHoward, A. G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang,\nW.; Weyand, T.; Andreetto, M.; and Adam, H. 2017. Mo-\nbilenets: Efficient convolutional neural networks for mobile\nvision applications. arXiv preprint arXiv:1704.04861.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nLi, Z.; and Snavely, N. 2018. MegaDepth: Learning Single-\nView Depth Prediction from Internet Photos. InProceedings\nof the CVPR, 2041–2050.\nLiu, C.; Yuen, J.; Torralba, A.; Sivic, J.; and Freeman, W. T.\n2008. Sift flow: Dense correspondence across different\nscenes. In Proceedings of the ECCV, 28–42.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. Proceedings of\nthe ICCV, 10012–10022.\nLowe, D. G. 2004. Distinctive Image Features from Scale-\nInvariant Keypoints. Int. J. Comput. Vis., 60(2): 91–110.\nOno, Y .; Trulls, E.; Fua, P.; and Yi, K. M. 2018. LF-Net:\nLearning Local Features from Images. In Proceedings of\nthe NeurIPS, volume 31.\nRadenovi´c, F.; Iscen, A.; Tolias, G.; Avrithis, Y .; and Chum,\nO. 2018. Revisiting Oxford and Paris: Large-Scale Im-\nage Retrieval Benchmarking. In Proceedings of the CVPR,\n5706–5715.\nRevaud, J.; De Souza, C.; Humenberger, M.; and Weinza-\nepfel, P. 2019. R2D2: Reliable and Repeatable Detector and\nDescriptor. In Proceedings of the NeurIPS, 12414–12424.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-\nvolutional networks for biomedical image segmentation. In\nProceedings of the MICCAI, 234–241.\nRublee, E.; Rabaud, V .; Konolige, K.; and Bradski, G. 2011.\nORB: An efficient alternative to SIFT or SURF. InProceed-\nings of the ICCV, 2564–2571.\nSarlin, P.; DeTone, D.; Malisiewicz, T.; and Rabinovich, A.\n2020. SuperGlue: Learning Feature Matching With Graph\nNeural Networks. In Proceedings of the CVPR, 4937–4946.\nSchonberger, J. L.; and Frahm, J.-M. 2016. Structure-from-\nmotion revisited. In Proceedings of the CVPR, 4104–4113.\nShen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2021. Ef-\nficient Attention: Attention With Linear Complexities. In\nProceedings of the WACV, 3531–3539.\nSun, J.; Shen, Z.; Wang, Y .; Bao, H.; and Zhou, X. 2021.\nLoFTR: Detector-Free Local Feature Matching With Trans-\nformers. In Proceedings of the CVPR, 8922–8931.\nTang, Y .; Han, K.; Guo, J.; Xu, C.; Li, Y .; Xu, C.; and Wang,\nY . 2022. An Image Patch Is a Wave: Phase-Aware Vision\nMLP. In Proceedings of the CVPR, 10935–10944.\nThomee, B.; Elizalde, B.; Shamma, D. A.; Ni, K.; Friedland,\nG.; Poland, D.; Borth, D.; and Li, L. J. 2016. YFCC100M:\nThe New Data in Multimedia Research. Commun. ACM,\n59(2): 64–73.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Proceedings of the NeurIPS,\n5998–6008.\nWang, Q.; Zhang, J.; Yang, K.; Peng, K.; and Stiefelhagen,\nR. 2022. Matchformer: Interleaving attention in transform-\ners for feature matching. InProceedings of the ACCV, 2746–\n2762.\nWang, Q.; Zhou, X.; Hariharan, B.; and Snavely, N. 2020.\nLearning Feature Descriptors Using Camera Pose Supervi-\nsion. In Vedaldi, A.; Bischof, H.; Brox, T.; and Frahm, J.-M.,\neds., Proceedings of the ECCV, 757–774.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction Without\nConvolutions. In Proceedings of the ICCV, 568–578.\nYi, K. M.; Trulls, E.; Ono, Y .; Lepetit, V .; Salzmann, M.; and\nFua, P. 2018. Learning to Find Good Correspondences. In\nProceedings of the CVPR, 2666–2674.\nZhang, J.; Sun, D.; Luo, Z.; Yao, A.; Zhou, L.; Shen, T.;\nChen, Y .; Quan, L.; and Liao, H. 2019. Learning Two-\nView Correspondences and Geometry Using Order-Aware\nNetwork. In Proceedings of the ICCV, 5845–5854.\nZhou, Q.; Sattler, T.; and Leal-Taixe, L. 2021. Patch2Pix:\nEpipolar-Guided Pixel-Level Correspondences. In Proceed-\nings of the CVPR, 4669–4678.\n1860",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.7626429796218872
    },
    {
      "name": "Computer science",
      "score": 0.7492880821228027
    },
    {
      "name": "FLOPS",
      "score": 0.7030796408653259
    },
    {
      "name": "Bottleneck",
      "score": 0.5906623005867004
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5677175521850586
    },
    {
      "name": "Upsampling",
      "score": 0.49706247448921204
    },
    {
      "name": "Computation",
      "score": 0.47411754727363586
    },
    {
      "name": "Transformer",
      "score": 0.45039480924606323
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4346342384815216
    },
    {
      "name": "Pose",
      "score": 0.4227166175842285
    },
    {
      "name": "Computational complexity theory",
      "score": 0.42163461446762085
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3659617304801941
    },
    {
      "name": "Machine learning",
      "score": 0.3410874605178833
    },
    {
      "name": "Algorithm",
      "score": 0.3051515221595764
    },
    {
      "name": "Parallel computing",
      "score": 0.22955578565597534
    },
    {
      "name": "Image (mathematics)",
      "score": 0.17448335886001587
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I41198531",
      "name": "Nanjing University of Posts and Telecommunications",
      "country": "CN"
    }
  ]
}