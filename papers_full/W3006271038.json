{
  "title": "Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification",
  "url": "https://openalex.org/W3006271038",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2101670418",
      "name": "Jibing Gong",
      "affiliations": [
        "Yanshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131545031",
      "name": "Hongyuan Ma",
      "affiliations": [
        "National Computer Network Emergency Response Technical Team/Coordination Center of Chinar"
      ]
    },
    {
      "id": "https://openalex.org/A2522170737",
      "name": "Zhiyong Teng",
      "affiliations": [
        "Yanshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131252974",
      "name": "Qi Teng",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2978152595",
      "name": "Hekai Zhang",
      "affiliations": [
        "Yanshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2144822718",
      "name": "Linfeng Du",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2107305250",
      "name": "Shuai Chen",
      "affiliations": [
        "Yanshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2329276079",
      "name": "Md Zakirul Alam Bhuiyan",
      "affiliations": [
        "Fordham University"
      ]
    },
    {
      "id": "https://openalex.org/A2106299145",
      "name": "Jianhua Li",
      "affiliations": [
        "Shijiazhuang Tiedao University"
      ]
    },
    {
      "id": "https://openalex.org/A2106247142",
      "name": "Mingsheng Liu",
      "affiliations": [
        "Hebei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101670418",
      "name": "Jibing Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131545031",
      "name": "Hongyuan Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2522170737",
      "name": "Zhiyong Teng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131252974",
      "name": "Qi Teng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2978152595",
      "name": "Hekai Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144822718",
      "name": "Linfeng Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107305250",
      "name": "Shuai Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329276079",
      "name": "Md Zakirul Alam Bhuiyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106299145",
      "name": "Jianhua Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106247142",
      "name": "Mingsheng Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2068074736",
    "https://openalex.org/W1997530783",
    "https://openalex.org/W1834987204",
    "https://openalex.org/W6681875376",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2515248967",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W6752122265",
    "https://openalex.org/W2563010554",
    "https://openalex.org/W6732491067",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1971014294",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2952641483",
    "https://openalex.org/W6757152081",
    "https://openalex.org/W2252215182",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2155893237",
    "https://openalex.org/W6767336797",
    "https://openalex.org/W2623162856",
    "https://openalex.org/W2621048556",
    "https://openalex.org/W2250662230",
    "https://openalex.org/W6756151100",
    "https://openalex.org/W2619706086",
    "https://openalex.org/W6735891514",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W6680230698",
    "https://openalex.org/W6693505360",
    "https://openalex.org/W2966714118",
    "https://openalex.org/W2966779056",
    "https://openalex.org/W2788667846",
    "https://openalex.org/W6685974025",
    "https://openalex.org/W2995837271",
    "https://openalex.org/W3001437801",
    "https://openalex.org/W2808190017",
    "https://openalex.org/W6760028787",
    "https://openalex.org/W6713582272",
    "https://openalex.org/W6748546225",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W2508429489",
    "https://openalex.org/W2739996966",
    "https://openalex.org/W2891768540",
    "https://openalex.org/W4236122429",
    "https://openalex.org/W2534727297",
    "https://openalex.org/W4254196108",
    "https://openalex.org/W2156956856",
    "https://openalex.org/W2904265202",
    "https://openalex.org/W2132311402",
    "https://openalex.org/W1980867644",
    "https://openalex.org/W2101746535",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2788125153",
    "https://openalex.org/W6775193911",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2803270043",
    "https://openalex.org/W2964347512",
    "https://openalex.org/W2903810586",
    "https://openalex.org/W2183087644",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W3013642887",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2971082042",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W2917390141",
    "https://openalex.org/W2605242105",
    "https://openalex.org/W4289294434",
    "https://openalex.org/W2150102617",
    "https://openalex.org/W2136939460",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Traditional methods of multi-label text classification, particularly deep learning, have achieved remarkable results. However, most of these methods use word2vec technology to represent sequential text information, while ignoring the logic and internal hierarchy of the text itself. Although these approaches can learn the hypothetical hierarchy and logic of the text, it is unexplained. In addition, the traditional approach treats labels as independent individuals and ignores the relationships between them, which not only does not reflect reality but also causes significant loss of semantic information. In this paper, we propose a novel Hierarchical Graph Transformer based deep learning model for large-scale multi-label text classification. We first model the text into a graph structure that can embody the different semantics of the text and the connections between them. We then use a multi-layer transformer structure with a multi-head attention mechanism at the word, sentence, and graph levels to fully capture the features of the text and observe the importance of the separate parts. Finally, we use the hierarchical relationship of the labels to generate the representation of the labels, and design a weighted loss function based on the semantic distances of the labels. Extensive experiments conducted on three benchmark datasets demonstrated that the proposed model can realistically capture the hierarchy and logic of text and improve performance compared with the state-of-the-art methods.",
  "full_text": "SPECIAL SECTION ON DEEP LEARNING: SECURITY AND FORENSICS RESEARCH ADVANCES\nAND CHALLENGES\nReceived January 14, 2020, accepted February 5, 2020, date of publication February 10, 2020, date of current version February 19, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.2972751\nHierarchical Graph Transformer-Based\nDeep Learning Model for Large-Scale\nMulti-Label Text Classification\nJIBING GONG\n1,2,3, ZHIYONG TENG\n 1,2, QI TENG4, HEKAI ZHANG1,2, LINFENG DU\n 4,\nSHUAI CHEN\n 1,2, MD ZAKIRUL ALAM BHUIYAN5, JIANHUA LI\n 6,\nMINGSHENG LIU7, AND HONGYUAN MA8\n1School of Information Science and Engineering, Yanshan University, Qinhuangdao 066004, China\n2Key Laboratory for Computer Virtual Technology and System Integration, Yanshan University, Qinhuangdao 066004, China\n3Key Laboratory for Software Engineering of Hebei Province, Yanshan University, Qinhuangdao 066004, China\n4School of Computer Science and Engineering, Beihang University, Beijing 100083, China\n5Department of Computer and Information Sciences, Fordham University, New York, NY 10458, USA\n6School of Information Science and Technology, Shijiazhuang Tiedao University, Shijiazhuang 050043, China\n7College of Electrical Engineering, Hebei University of Technology, Tianjin 300401, China\n8National Computer Network Emergency Response Technical Team/Coordination Center of China, Beijing 100029, China\nCorresponding author: Jianhua Li (lijh128@163.com)\nABSTRACT Traditional methods of multi-label text classiﬁcation, particularly deep learning, have achieved\nremarkable results. However, most of these methods use word2vec technology to represent sequential text\ninformation, while ignoring the logic and internal hierarchy of the text itself. Although these approaches can\nlearn the hypothetical hierarchy and logic of the text, it is unexplained. In addition, the traditional approach\ntreats labels as independent individuals and ignores the relationships between them, which not only does\nnot reﬂect reality but also causes signiﬁcant loss of semantic information. In this paper, we propose a novel\nHierarchical Graph Transformer based deep learning model for large-scale multi-label text classiﬁcation.\nWe ﬁrst model the text into a graph structure that can embody the different semantics of the text and the\nconnections between them. We then use a multi-layer transformer structure with a multi-head attention\nmechanism at the word, sentence, and graph levels to fully capture the features of the text and observe\nthe importance of the separate parts. Finally, we use the hierarchical relationship of the labels to generate\nthe representation of the labels, and design a weighted loss function based on the semantic distances of\nthe labels. Extensive experiments conducted on three benchmark datasets demonstrated that the proposed\nmodel can realistically capture the hierarchy and logic of text and improve performance compared with the\nstate-of-the-art methods.\nINDEX TERMS Multi-label text classiﬁcation, graph modeling, graph transformer, deep learning.\nI. INTRODUCTION\nText classiﬁcation is a signiﬁcant and classical problem in\nnatural language processing [1]. One of the major topics to\nbe investigated in this ﬁeld is multi-label hierarchical text\nclassiﬁcation (MLHTC), which aims to assign (tag) a text\nwith multiple appropriate labels from hierarchical label struc-\ntures. Such a structure can be a tree or a directed acyclic\ngraph indicating the parent-child relations between labels [2].\nMLHTC methods have been utilized in an extensive range of\napplications, including question answering, online shopping,\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Kim-Kwang Raymond Choo\n.\nand news tag organization [3]. It is an extremely meaningful\nclassiﬁcation problem in real-world situations. In contrast to\nsingle label or multi-class classiﬁcation, the key challenges\nof MLHTC involve utilizing its powerful text representation,\nfeature extraction, and label structure relationship exploration\ncapabilities.\nThe text representation model is a fundamental but chal-\nlenging issue for natural language processing tasks [4]. Tra-\nditional text representation utilizes the vector space model\nfounded on the bag of words/phrases representation [5].\nTo handle downstream tasks, most natural language process-\ning tasks leverage the vector space model to model (repre-\nsent) text, owing to its simplicity and effectiveness [6], [7].\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ 30885\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nHowever, the vector space model loses a signiﬁcant\namount of structural and semantic information that is use-\nful for text categorization. Recently, graph-based archi-\ntecture has attracted increasing research attention for use\nin social networks and recommendation systems [8]–[10].\nIt utilizes graphs to organize different types of data, which\nbetter reﬂects real-world situations, and has achieved sat-\nisfactory results. In contrast, there is limited research on\ngraph-based documentation representation [11]. Compared to\nbag of words/phrases representation, graph-based document\nmodeling can preserve local sequential, non-consecutive,\nand long-distance semantics, and consequently provides\nimproved classiﬁcation accuracy [12], [13].\nIn recent years, deep learning models have achieved\nstate-of-the-art results across many domains, including a\nwide variety of natural language processing (NLP) appli-\ncations [14], [15]. These models can combine feature\nextraction with a classiﬁer to perform end-to-end learn-\ning of text classiﬁcation. Examples include recurrent neu-\nral networks (RNNs) [16]–[19] and convolutional neural\nnetworks (CNNs) [20]–[22]. RNNs can effectively process\nthe semantics of short text but are less able to capture seman-\ntic features of longer text. Although bidirectional RNNs were\nproposed to solve the aforementioned problem and perform\nefﬁciently in many NLP tasks, the problem of training efﬁ-\nciency is still unsolved. Unlike RNNs, CNNs use differ-\nent window sizes to perform one-dimensional convolution\nof word vectors for all words in a sentence (available for\nsome information before and after, similar to an implicit n-\ngram), and then use the maximum pool to obtain the most\nimportant impact factor for processing downstream tasks.\nHowever, owing to their n-gram-like mechanisms, the long-\ndistance semantic dependency among the words can be lost.\nIn summary, the existing deep learning methods cannot\nsimultaneously capture the non-consecutive, long-distance,\nand sequential semantics of text.\nFurthermore, unlike general single label or multi-class\ntasks [23], an MLHTC must consider the independence and\nhierarchies of distinct labels and imbalanced label space.\nHowever, general NLP models based on deep learning can\nonly learn the features of the text, and have difﬁculty learn-\ning the intrinsic relationships between discrete labels. While\nsome proposed transfer learning methods consider the rela-\ntions between hierarchical labels by sharing the weights from\ndifferent local models, one critical issue is that the number of\nlocal classiﬁers depends on the depth of the label hierarchy.\nThis makes transfer learning impracticable for large-scale\ntext [24].\nInspired by recent work on graph representation learning\nand attention neural networks [25]–[27], we propose a novel\nHierarchical Graph Transformer based deep learning model\ncalled HG-Transformer for large-scale multi-label text clas-\nsiﬁcation. Our framework is composed of three substantial\ncomponents: graph-based document modeling, hierarchical\ntransformer encoder architecture for features extraction, and\nweight-directed loss for label classiﬁcation. The remainder of\nFIGURE 1. Transformer - model architecture.\nthis paper is organized as follows: preliminaries are described\nin Section 2, the architecture of our model is described in\nSection 3, and the effectiveness of the proposed model is\ndemonstrated in Section 4, using tests on benchmark datasets\nand comparisons with the state-of-the-art methods.\nII. PRELIMINARIES\nAttention: The attention mechanism was ﬁrst introduced by\nBahdanau to resolve long source sentences in neural machine\ntranslation (NMT). More recently, Google redeﬁned the\nattention mechanism known as ‘‘Scaled Dot-Product Atten-\ntion.’’ First, they use a neural network to map the original\ninput into three matrices: query, key, and value. They then\ncompute the dot products of the query with all keys, divide\neach by √\ndk , and adopt a softmax function to obtain the\nweights (attention) on the values. Next, they use the weighted\nvalue as the representation of each word. The complete cal-\nculation formula is as follows:\nQ,K,V =([W Q,W K ,W V ])h (1)\nAttention(Q,K,V ) =softmax(QKT\n√dk\n)V (2)\nTransformer: This method was proposed by Google\nfor seq2seq tasks (such as language translation) in which\narchitectures forgo deep neural networks such as recur-\nrent models and convolution models and instead rely com-\npletely on attention mechanisms to obtain global dependen-\ncies [28]. The main point of the model is to compute the\nself-attention between the different words in a sequence and\nthen re-represent the words in a sensible manner. It proved to\nbe extremely efﬁcient compared with traditional deep neural\nnetworks. The overall structure of the transformer consists\nof an encoder and a decoder, as shown in the left and right\nhalves of Fig. 1, respectively. The encoder is composed of\na stack of six identical layers, and each layer contains two\nsublayers, namely a multi-head self-attention layer and a sam-\nple feed-forward network. In contrast, the decoder introduces\n30886 VOLUME 8, 2020\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nFIGURE 2. Graph-based document modeling.\nmulti-head attention over the output of the encoder in addition\nto the two sublayers. We note that the author also employs\nresidual connections and layer normalization to avoid losing\noriginal information. Readers can refer to [28] and [29] for a\nmore detailed description of the original architecture.\nIII. HIERARCHICAL GRAPH TRANSFORMER\nWe design a Hierarchical Graph Transformer for large-scale\nmulti-label text classiﬁcation. Given a certain document,\nwe ﬁrst covert sequence information to a graph of vectors.\nWe then use a three-layer transformer encoder to perform\nself-attention at the word, sentence, and graph levels to obtain\na richer text representation that considers the logic and inter-\nnal hierarchy of the text. Finally, we design a novel loss func-\ntion based on label similarity in order to learn the hierarchy\nand dependencies of the tags. Fig. 2 illustrates the overall\nframework of the HG-Transformer.\nA. GRAPH-BASED DOCUMENT MODELING\nUnlike in traditional text representation, the graph is used to\nmodel documents to better capture long-distance semantics\nand the internal hierarchy of the text. Graph-based document\nmodeling consists of three components: the graph of word\nco-occurrence relationships, an important word expanded\nsubgraph, and a regulation graph matrix.\n1) GRAPH OF WORD CO-OCCURRENCE RELATIONSHIPS\nGiven a document, we ﬁrst perform sentence tokenization\nprocessing to split the document into several sentences. Then,\nword tokenization is utilized to divide each sentence into its\ncomponent words. Simultaneously, lemmatization and stop\nword removal processing standardize words to their base\nform and remove words with no signiﬁcant meaningful fea-\ntures. For example, a sentence such as ‘‘My system keeps\ncrashing! His crashed yesterday, ours crashes daily’’ will be\ntransformed to ‘‘My system keep crash! his crash yesterday,\nours crash daily.’’ After we obtain the processed document,\nwe adopt a sliding window to obtain local co-occurrence rela-\ntionships and build the word co-occurrence matrix. We then\nregard the vertex and positional index of a word appearing\nin the document as its attribute, and the co-occurrence rela-\ntionships as its edge; as such, we obtain a graph of word\nco-occurrence relationships, which can be denoted as G =\n(V , E, N, P). V denotes the word set and |V |= n, E denotes\nthe co-occurrence set and |E|= m, N denotes the number\nof co-occurrences, and P denotes the word position in the\ndocument. For example, in the ﬁrst section of Fig. 2 we obtain\na graph of word co-occurrence relationships.\n2) IMPORTANT WORD EXPANDED SUBGRAPH\nAfter we obtain the graph of word co-occurrence relation-\nships, we can select the top-N signiﬁcant words accord-\ning to their contribution ranking. Here, we adopt TF-IDF\n(term frequency-inverse document frequency) to calculate\nword contributions. First, we can calculate the TF-IDF fea-\nture for each word and rank them from large to small.\nThe top-N words are selected as the root of the subgraph.\nWe then expand each word into subgraphs using breadth-ﬁrst\nsearch (BFS) and depth-ﬁrst search (DFS). The size of the\nsubgraphs is limited to K nodes. Finally, we obtain N sub-\ngraphs, and each subgraph contains both the non-consecutive\nand long-distance information for the important words in the\ntext, as shown in step 2 and step 3 of Fig. 2. Next, we discuss\nhow to build a regulation matrix that is readable by our learner\nmodel.\n3) REGULATION GRAPH MATRIX\nAlthough the subgraph can preserve both the non-consecutive\nand long-distance information of an important word v in the\ntext, it loses the sequence information of the word in the\nVOLUME 8, 2020 30887\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\noriginal document. Thus, we must convert each subgraph\ninto a sequence of words by considering the vertex attribute.\nFor example, in a subgraph, we can use the node attribute\nthat indicates the word’s position in the original document\nto order the node. In this manner, we can generate recon-\nstructed text that contains the non-consecutive, long-distance,\nand sequence information of the subgraph. As shown in the\nﬁrst line of the regulation graph matrix in step 4 of Fig. 2,\nwe convert the ﬁrst subgraph G(v) into a sequence such as\n‘‘Chrysler million new investments South America. Including\nplants pickup trucks.’’ We note that the word sequence may\nnot match that of the sentence in the original document; thus,\nwe split each word sequence into several sentence blocks by\nutilizing punctuation information.\nWe also rank sentence blocks by their length and select the\ntop-M sentence blocks. Meanwhile, we limit the number of\nsentence blocks in each subgraph to S and limit the number\nof words in each sentence block to W, which guarantees the\nconsistency and regularity of text data. For example, we can\nchoose the top 10 sentences as a candidate sentence block\nand choose the average length of each sentence block as its\nﬁnal length. If the number of sentences blocks in a sub-graph\nless than 10, we use zero to pad it and if the number of\nwords less than average length, we also use zero to pad\nit. Finally, for each document, we obtain an N ×M ×W\nthree-dimensional matrix, where N denotes the top-N impor-\ntant words, M denotes the number of sentence blocks in\neach subgraph, and W denotes the number of words in a\nsentence block. To further improve the validity of the text\nrepresentation, we use word2vec to represent each word as\na dense vector of real numbers [30], [31]. Speciﬁcally, we set\nthe dimension of the word vector to D. Through this step,\nwe can ﬁnally obtain an N ×M ×W ×D four-dimensional\nmatrix, where D denotes the dimension of the word vector,\nas shown in Fig. 2.\nB. HIERARCHICAL TRANSFORMER ENCODER\nARCHITECTURE\nIn this section, we introduce the hierarchical transformer\nencoder model. After converting each document into a 4-\nD matrix representation, we utilize a three-layer transformer\nencoder model to learn both the hierarchy and logic features\nof the text. Unlike the standard transformer, we only use the\nencoder component, because we require the text extraction\nfeature for the classiﬁcation task. The overall framework of\nour model is shown in Fig. 3.\nLet D denote a document comprised of a sequence of\nNd subgraphs. D = {g1,g2,..., gNd }. Each subgraph\ng is comprised of a sequence of sentence blocks G =\n{s1,s2,..., sNg}, where Ng denotes the number of sentences\nblocks. Each sentence block is comprised of a sequence of\ntokens S ={w1, w2,..., wNs}, where Ns denotes the length\nof the sentence. Additionally, each word w is represented by\nan embedding vector.\nWe ﬁrst employ the transformer encoder over the word\nlevel, in order to take into account the relationships between\nFIGURE 3. Hierarchical transformer model.\nwords in the same sentence. The words in a sentence have\na common context, and each word is a unit of expression\nfor the semantics of the sentence; thus, it is wise to limit\nthe focus of words to the scope of the sentence. In fact,\nthe transformer encoder maps an input sequence of word rep-\nresentations (w 1,w2,..., wNs) to a sequence of continuous\nrepresentations z =(z1,z2,... zNs). The procedure can be\nsummarized as follows:\nAttention(S) =softmax( SST\n√dk\n)S (3)\nZ1 =Norm(S +Attention(S)) (4)\nZ2 =Norm(Z1 +FFN(Z1)) (5)\nwhere S ∈R(Ns,dw), Z2 ∈R(Ns,dz). We simply set dw =dz\nAfter the transformer encoder process, each word captures\nthe semantics of the other words in the sentence. How-\never, because we do not encode the position of the word,\nwe will lose some of the position information. To alleviate\nthis disadvantage, we introduced LSTM (long short-term\nmemory networks) to sequence-model the re-encoded words.\nNote that although ordinary LSTM will lose the seman-\ntic information of the previous text because of the length\nof the sentence, the words re-encoded by the transformer\nwill retain the context information, thus solving the inher-\nent shortcomings of LSTM. The process of LSTM is as\nfollows:\nhz\nt (enc) =LSTMword\nencode(zt ,h(t−1)(enc)) (6)\nAs Fig.4 (a) shows, the vector output at the ending\ntime-step is used to represent the entire sentence.\nAfter we obtain the sentence encoder, we use the trans-\nformer encoder at the sentence level. Unlike the word-level\ntransformer, we consider the position of a sentence in a\nsubgraph and combine it with the sentence semantic vector\n30888 VOLUME 8, 2020\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nFIGURE 4. Hierarchical transformer encoder architecture.\nto represent the sentence. In this manner, the transformer\ncan capture the relationship between different sentences and\nthe position information of the sentences. The sentence-level\ntransformer can map a sequence of sentence block\nrepresentations(s1,s2,..., sNg) to a sequence of continuous\nrepresentations t =(t1,t2,... tNg).\nUnlike the word-level transformer, we do not use LSTM to\nprocess the transformer-recoded sentences, but use a simple\naddition operation to represent the ﬁnal subgraph encod-\ning. This not only saves a substantial amount of computing\nresources, but also generates the ideal subgraph encoding.\nThe model is shown in Fig.4 (b)\nSimilar to the previous process, after obtaining the graph\nencoder we employ the transformer encoder over the sub-\ngraph level to take into account the relationships between\nthe subgraphs in a document. Unlike at the word level and\nsentence level, we do not consider location information at the\ngraph level. Because the subgraph is extended from important\nwords, we assume that each important word has equal status\nin terms of position. The graph level transformer can map a\nset of subgraph representations (g1, g2,..., gNg) to a set of\ncontinuous representations l =(l1,l2,... lNl ). Unlike the\nsequential structure of the words in the previous sentence,\nthe sentence and word information in the subgraph do not\nhave sequence characteristics. Therefore, we introduce con-\nvolutional neural networks (CNNs) for feature extraction in\nsubgraphs. The model is shown in Fig.4 (c)\nThe ﬁnal result sequentially extracts the features of the text\nat the word, sentence, and subgraph levels, which can further\ncapture the hierarchy and logic information of the document.\nWe note that a word will not only pay attention to other words\nin a sentence but also generate attention for words in other\nsentences through the sentence-level transformer. Similarly,\na sentence can obtain information from the sentences in\nother graphs through the graph-level transformer. Traditional\ntransformers treat words as the fundamental elements of a\ndocument. Each word will pay attention to all the words in\nthe document, and ignore the context in which the word is\nlocated. Hierarchical transformers are different from tradi-\ntional ﬂat transformers. We believe that a word should pay\nmore attention to the words in the sentence it belongs to\nthan to the words in other sentences. Similarly, the attention\nbetween sentences in one sense-group will be greater than\ntheir attention to sentences in other sense-groups (we can\nthink of a subgraph as a sense-group). Therefore, the hierar-\nchical transformer can obtain the semantic information of the\ndocument in a more reasonable and efﬁcient manner. After\nobtaining the encoding of the document, we can use it for\nvarious tasks downstream.\nC. HIERARCHICAL SIMILARITY-BASED WEIGHTED CROSS\nENTROPY LOSS\nThe traditional classiﬁcation method treats a label as an\nindependent individual, without considering the relationships\nbetween labels. This can result in a signiﬁcant loss of tag\ninformation, which will affect classiﬁcation accuracy. More-\nover, the unbalanced distribution of labels (that is, fewer\ninstances of most leaf labels will appear) will cause the train-\ning process to be insufﬁcient, and no corresponding features\nwill be learned for a small number of labels. Therefore,\nwe propose a weight-directed loss function based on label\nsimilarity. It considers the similarity between tags and can\nsolve the problem of uneven distribution of labels.\nFirst, we deﬁne the hierarchical structure of the label as\nL = (L,E). L represents the label, and E represents the\nparent-child relationship between the labels. We must choose\nthe label representation to calculate the similarity of the\nlabels. Naturally, we can use the word vector of the label\nVOLUME 8, 2020 30889\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nTABLE 1. Dataset statistics.\nto represent the label, because the semantic information of\nthe label specifying the parent-child relationship is similar.\nHowever, the simple semantic word vector only considers\nthe meaning of the labels literally and does not consider the\nhierarchical relationship between the labels. In this manner,\nthe representation of the label will lose the most important\nstructured information, which is not conducive to the mea-\nsurement of similarity.\nInspired by the recent work on node embedding, we use\nrandom walks to generate label sequences as corpus informa-\ntion, and then use skip-gram to train the label sequences to\nconvert label representations into a continuous vector space\nVl ∈RD. D denotes the embedded dimension of the label.\nAfter obtaining the representation of the label, we use the\nEuclidean distance to calculate the similarity between the\ndifferent labels:\nDis(i,j) =\n√\nD∑\nk=1\n(V li\nk −V\nlj\nk )2 (7)\nThen, the similarity can be deﬁned as:\nSimi,j =1 − Dis(i,j)\n|L|∑\nk=1\nDis(i,k)\n(8)\nNext, in order to explore the relevance and hierarchy of\nlabels, we designed a similarity-based weight loss function\nbased on label similarity:\nL =−\nn∑\ni=1\nL∑\nk=1\n[yi\nk log ˆyi\nk +λαi\nk (1 −yi\nk ) log(1 −ˆyi\nk )] (9)\nwhere yk =1 if and only if a digit of class k is present. λis a\nhyperparameter to control the weight of similarity. N denotes\nthe number of documents and L denotes the number of labels.\nˆy ∈[0,1] denotes the positive probability. αk ∈[0,1] is the\nminimum distance from negative label k to the positive labels\nset. Speciﬁcally, for a text ts, the positive label set is Ps ⊂S.\nAdditionally, for any negative label k, the αk is:\nαk =max\nl∈Ps\n(Siml,k ) (10)\nIV. EXPERIMENTS\nA. EXPERIMENT SETUP\nDatasets: We conducted extensive experiments using three\npublicly available datasets from various domains (sum-\nmarized in Table 1). The ﬁrst two datasets, RCV1 and\nRCV 1-2K, are related to news categorization. The third\ndataset is AmazonCat-13K, which is related to product cat-\negorization. The detailed descriptions of each dataset are as\nfollows:\n• Reuters Corpus Volume I (RCV1)[32]. RCV1 dataset\nis a manually labeled newswire collection of Reuters\nNews from 1996 - 1997. The news documents are cat-\negorized with respect to three controlled vocabularies:\nindustries, topics, and regions. We use the topic-based\nhierarchical classiﬁcation as it has been the most pop-\nular in previous evaluations. There are 103 categories,\nincluding all classes in the hierarchy (except for root).\n• RCV1-2K. The RCV1-2K dataset has the same features\nas the original RCV1 dataset but its label set has been\nexpanded by forming new labels from pairs of original\nlabels. There are 2456 labels in the RCV1-2K dataset.\n• AmazonCat-13K. This dataset includes reviews (rat-\nings, text, helpfulness votes), product metadata (descrip-\ntions, category information, price, brand, and image\nfeatures), and links (also viewed/also bought graphs).\nIn this paper, we focus on category information.\n• Evaluation Metrics.We use standard rank-based eval-\nuation metrics P@K (precision at k) and NDCG@K\n(normalized discounted cumulative gain at k) to measure\nthe performance of all the methods [33]. For P@K and\nNDCG@K, each documentt has a set of |L|ground\ntruth labels Lt ={l 0,l1,l2 ..., l|L|−1}and a list of Q\npredicted labels, in order of decreasing probability Pt =\n[p0,p1,p2 ..., pQ−1]. The precision at k is P@K =\n1\nk\n∑min(|L|,k)−1\nj=0 relLi (Pt (j)), where\nrelL (p) =\n{\n1 if p ∈L,\n0 otherwise.\nThe NDCG at k is NDCG@K = 1\nIDCG(Li,k)∑n−1\nj=0\nrelLi (Pt (j))\nln(j+1) , where n =min(max(|Pi|,|Li|),k)\nB. COMPARED METHODS\nFlat baselines:These methods generally extract discrete fea-\ntures such as the TF-IDF of the document and utilize them to\ntrain a classiﬁcation model. The typical representatives of this\nmethod are LR (logistic regression) and SVMs (support vec-\ntor machines). We deﬁne them as ïĆat baselines because they\nignore both the relations among the words and the relations\namong the labels and simply train a multi-class classiﬁer.\n30890 VOLUME 8, 2020\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nNeural Network Models: Many neural network models\nhave been proposed for multi-label text classiﬁcation prob-\nlems. For comparisons, we mainly use improved CNN- and\nRNN-based models such as XML-CNN [3], Deep CNN [34],\nHLSTM [35], HMCN-F [36], and HAN [37]. XML-CNN\nintroduces the CNN structure to handle multi-label text clas-\nsiﬁcation problems. RNN models such as Hierarchical Long\nShort-term Memory Network (HLSTM) and Hierarchical\nAttention Network (HAN) both utilize a two-layer RNN\nstructure to extract text features at the word level and sentence\nlevel, respectively; HAN also introduces attention weights\nto account for the effects of different locations. HMCN-F\nﬁts its CNN layers to the label hierarchy, and each CNN\nlayer focuses on predicting the labels at the corresponding\nhierarchical level.\nHierarchical Models:These methods employ hierarchical\nor graphical label networks to build hierarchical classiﬁ-\ncation classiﬁers. Examples of hierarchical models include\nHierarchically Regularized Logistic Regression (HR-LR),\nHierarchically Regularized Support Vector Machines (HR-\nSVM) [38], and Hierarchically Regularized Deep Graph\nCNN (HR-DGCNN-3) [11]. For example, HR-DGCNN uses\ngraph structures to extract text features and utilizes regulars\nto consider label relationships.\nTree and Embedding Based Methods: These methods\nmainly focus on large-scale multi-label classiﬁcation. Exam-\nples of such methods include FastXML [39], SLEEC [40],\nand Parabel [41]. SLEEC (Sparse Local Embeddings for\nExtreme Classiﬁcation) projects labels into low-dimensional\nvectors that can capture label relations and uses the k-nearest\nneighbors when predicting. FastXML builds a tree-based\nextreme multi-label classiﬁer to handle MLHTC problems,\nand includes a novel node partitioning formulation to speed\nup the training process. Parabel [41] learns a balanced tag\nhierarchy and generalizes the hierarchical softmax model to\nsave computing resources.\nVariations of HG-Transformer:The Transformer model\nhas achieved state-of-art performance in most NLP tasks.\nThe basic structure of our proposed model is inspired by\nTransformer architecture. In order to distinguish the two,\nwe call the general Transformer a Flat Transformer, and\nrefer to our proposed model as the Hierarchical Transformer.\nWe implemented several variants of these two types of Trans-\nformers as follows. Flat Transformer (F-Transformer): gen-\neral Transformer without graph-based document modeling\nand hierarchical similarity-based weighted cross entropy loss.\nFlat Graph Transformer (FG-Transformer): without hierar-\nchical similarity-based weighted cross entropy loss. Flat\nTransformer with Weighted Loss (F-Transformer-W): with-\nout graph-based document modeling. Hierarchical Graph\nTransformer (HG-Transformer(No W)): without hierarchical\nsimilarity-based weighted cross entropy loss.\nC. EXPERIMENTAL SETTINGS\nAll our experiments were performed on a 64-core Intel Xeon\nCPU E5-2680 v4@2.40GHz with 512GB RAM and eight\nNVIDIA Tesla P100-PICE GPUs. The operating system and\nsoftware platforms were Ubuntu 5.4.0, Python 3.6.2, and\nPytorch 0.4.0. The training and testing datasets are shown\nin Table1. In the document modeling part, the top-N number\nof important words is set to 100 (RCV1). The maximum\nnumber of nodes in each subgraph is set to 50. The max-\nimum number of sentences per subgraph is set to 5, and\nthe maximum length of each sentence is set to 10. We use\nGloVe [42] (Global Vectors for Word Representation) with\nsize 50 as word embeddings for last document representation.\nFor label representation, we use node2Vec technology to\ngenerate vector representation of the labels, with a dimension\nof 50. All models are trained using an Adam optimizer with\nan initial learning rate of 1e-6 and a weight decay of 1e-6.\nD. PERFORMANCE COMPARISON\nWe compare the performance of our proposed method\nHG-Transformer to state-of-the-art MLHTC methods and\nshow the results in Tables 2.\nOn RCV1, we can see that for traditional methods,\nHR-SVM performs better than LR, SVM, and HR-SVM. One\nof the main reasons is that HR-SVM is more complex and can\ncapture the nonlinear space feature.\nFor deep neural network methods, we can see that HLSTM,\nHAN, and RCNN even achieve worse performance than tra-\nditional methods. The results above illustrate that the tradi-\ntional deep neural network model does not provide as much\nof an advantage as it does in other tasks. These recurrent\nmodels are not suitable for long text tasks. However, DCNN\nachieves the best performance among deep neural network\nmethods and it illustrates that the CNN model is more suit-\nable for text classiﬁcation than the RNN model. Speciﬁcally,\nHR-DGCNN achieves better performance than HMCN-F on\nP@K metrics while HMCN-F outperforms HR-DGCNN on\nNDCG@K metrics. They all take advantage of the label\nhierarchy information.\nFor embedding and tree-based models, one can see that\nSLEEC achieves better performance than FastXML. One of\nthe main reasons is that SLEEC can learn embeddings that\npreserve pairwise distances between only the nearest label\nvectors. Parabel achieves better performance than SLEEC on\nall ﬁve metrics, which illustrates the signiﬁcance of label\nhierarchy information.\nFor the Transformer models, one can see that the\ngraph-based document modeling, hierarchical transformer\nmechanism, and hierarchical similarity-based weighted cross\nentropy loss are all instrumental to improving classiﬁca-\ntion performance. Speciﬁcally, F-Transformer outperforms\nmost traditional models and neural network models, and\ndemonstrates that the Transformer model is effective in\nextracting text features in multi-label text classiﬁcation.\nMeanwhile, F-Transformer-W achieves better performance\nthan F-Transformer, which shows that the model beneﬁts\nfrom the hierarchical similarity-based weighted cross entropy\nloss. Moreover, FG-Transformer achieves better performance\nthan F-Transformer, thus demonstrating that graph-based\nVOLUME 8, 2020 30891\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nTABLE 2. Results in P@k and NDCG@k.\ndocument modeling can preserve more text semantic infor-\nmation than sequential text modeling. Compared to the\nﬂat transformer, one can see that HG-Transformer(No W)\nperforms better. These improvements show that the combi-\nnation of graph-based document modeling and the hierar-\nchical transformer can better extract text features. Finally,\nHG-Transformer achieves the best performance on the ﬁve\nmetrics, which again demonstrates that graph-based docu-\nment modeling, the hierarchical transformer mechanism, and\nhierarchical similarity-based weighted cross entropy loss are\nall useful in improving classiﬁcation performance.\nOn RCV 1-2K and AmazonCat-14K, similar results are\nobserved: the graph-based text modeling method, Trans-\nformer model, and hierarchical label similarity methods per-\nform well, and our proposed method HG-Transformer again\nachieves the best performance on all metrics.\nV. EVALUATION ON GRAPH-BASED DOCUMENT\nMODELING\nIn order to better capture the semantic features of the text,\nwe explore the number of subgraphs in graph-based docu-\nment modeling. We tested with different numbers of sub-\ngraphs using the RCV1 dataset, and the results are shown\nin Table3. One can see that the number of subgraphs has a sig-\nniﬁcant effect on classiﬁcation performance. Subgraphs are\nextended from important words and the number of subgraphs\nis the same as important words. Additionally, the nodes in\neach subgraph, that is, words, are directly or indirectly related\nto important words. Therefore, the selection of the appro-\npriate number of important words will affect classiﬁcation\nperformance. If the number of selected important words is too\nsmall, the semantic information in the text will be lost and\nTABLE 3. Comparison of different numbers of subgraphs on RCV1.\nthe model cannot fully extract the semantics. If the number\nof selected important words is too large, some redundant\ninformation will be introduced and the model will learn\nuseless information. As the table shows, the model has the\nlowest performance on all metrics at 20 subgraphs, and at\nthis time, the graph-based document will lose some semantic\ninformation. When the number of subgraphs is set to 100,\nthe model performs best on all classiﬁcation metrics. It can\nbe considered that the graph-based document at this time\ncan fully extract the original semantics of the text. When\nthe number of subgraphs is 120, the model will provide poor\nclassiﬁcation accuracy owing to the introduction of redundant\ninformation. It is vital to choose an appropriate number of\nsubgraphs at the graph-based document modeling process to\noptimize classiﬁcation performance.\nVI. EVALUATING DIFFERENT NUMBERS OF HEADS\nA signiﬁcant hyperparameter of the Transformer model is\nthe number of heads. We compared results obtained with\ndifferent numbers of heads through tests on the three datasets.\nThe experimental results are shown in Fig. 5. Fig. 5 shows\nthat on the RCV1 dataset, when the number of heads is two,\n30892 VOLUME 8, 2020\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nFIGURE 5. Comparing performance obtained with different numbers of\nheads on three datasets.\nHG-Transformer performs best on all ﬁve metrics. When\nthe number of heads is greater than two, the experimen-\ntal effect will rapidly decline. Fig. 5 b shows that on the\nRCV 1-2K dataset, when the number of heads is set to 5,\nHG-Transformer performs best on P@K and NDCG@5, and\nwhen the number of heads is set to 10, NDCG@3 exhibits the\nhighest performance. Fig. 5 c shows that on the AmazonCat-\n13K dataset, when the number of heads equals 10, p@1,\np@5, and NDCG@5 reach their maximum scores. When the\nnumber of heads is set to 5, P@3 and NDCG@3 achieve the\nmaximum scores. Overall, the number of heads is directly\nproportional to the magnitude of labels in the dataset. Because\nmulti-head attention allows the model to jointly attend to\ninformation from different representation subspaces at dif-\nferent positions, it can capture features in different dimen-\nsions. Each head of the transformer’s multi-head mechanism\ncorresponds to a feature subspace. An appropriate number\nFIGURE 6. Word-level self-attention visualizations for the 2287newsML\nsample in RCV1.\nof feature subspaces can fully learn the potential various\nfeature relationships of the text and its mapping to corre-\nsponding labels. Under the premise of our experimental setup,\nthe labels of one hundred, one thousand, and ten thousand\nlevels correspond to 2, 5, and 10 heads, respectively.\nVII. EVALUATION ON HIERARCHICAL\nSIMILARITY-BASED WEIGHTED CROSS ENTROPY LOSS\nIn order to study whether the proposed hierarchical\nsimilarity-based weighted cross entropy loss can obtain\nbetter classiﬁcation results, we utilize the hierarchical\nsimilarity-based weighted cross entropy loss and traditional\ncross entropy loss to train our model; the comparison results\nare shown in Table4.\nOn the RCV-1 dataset, we can see that the model trained\nusing hierarchical similarity-based weighted cross entropy\nloss improves the performance by 1% in terms of P@1 and\nP@3 and 2% in terms of P@5, NDCG@3, and NDCG@5,\ncompared with the model trained using traditional cross\nentropy loss. For RCV1 2-K and AmazonCat-13K, we can see\nthat our proposed loss function outperforms the traditional\ncross entropy loss function and provides an improvement\nof 1% on average. It can be concluded that our proposed loss\nfunction can help improve model performance compared with\nthe traditional cross entropy loss function. One of the main\nreasons is that the hierarchical similarity-based weighted\ncross entropy loss function can solve the problem of uneven\ndistribution of categories in the multi-label classiﬁcation task,\nand those labels with a small number of samples can be fully\ntrained through the hierarchical structure between the labels.\nVIII. CASE STUDY\nTo further explore the hierarchical transformer mechanism\ncaptured in a document, we visualize portions of the word\nlevel self-attention probability using heatmaps, as shown\nin Fig. 6. Fig. 6 shows the self-attention between different\nwords in a sentence block, taking the sentence ‘‘Chrysler\nmillion new investments South America. Including plants\nVOLUME 8, 2020 30893\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\nTABLE 4. Comparison of loss functions.\npickup trucks’’ as an example. We can see that each word\npays the most attention to itself. The word Chrysler pays more\nattention to the words ‘‘American’’ and ‘‘million.’’ The word\n‘‘millions’’ has a greater focus on words ‘‘investments’’ and\n‘‘plans.’’ It can be found that allocation of the attention weight\nof each word to other words in the sentence is consistent with\nreal-world intuition. This proves that our model can capture\ntext features well and is interpretable.\nIX. RELATED WORK\nIn this section, we review related work in the following two\naspects.\nA. TEXT REPRESENTATION\nText representation is the basis of natural language pro-\ncessing, and reasonable text representation can signiﬁcantly\nimprove the efﬁciency of downstream tasks; e.g., text clas-\nsiﬁcation, machine translation, sentiment analysis, question\nand answer systems [43]. In recent years, text represen-\ntation approaches have been developed signiﬁcantly for\nvarious applications [44]. Many text classiﬁcation studies\nfocus on the bag-of-words (BOW) approach [45], [46],\nin which each feature corresponds to a single vocabulary\nword. Term frequency-inverse document frequency (TF-IDF)\nis a very common method for transforming text into a\nmeaningful representation of numbers and is widely used\nfor feature construction [47], [48]. Owing to advances in\ndeep learning, the vector representations of words learned\nby word2vec models based on deep learning have been\nshown to carry semantic meanings and perform effectively\nin many NLP tasks [49]. In other instances, researchers have\nattempted to transform texts into graphs by utilizing word\nco-occurrence [50].\nB. TEXT FEATURE EXTRACTING\nRecently, deep learning has shown its powerful capability to\nhandle various NLP tasks [51], [52]. For example, recurrent\nconvolutional neural networks use different window sizes for\none-dimensional convolution of word vectors for all words\nin a sentence to capture text semantic features [53]. Recur-\nrent neural networks (RNNs) can remember some informa-\ntion regarding a sequence by maintaining a hidden state,\nand are better suited to processing NLP tasks [16]. Addi-\ntionally, long short-term memory networks (LSTM) and\ngated recurrent units (GRUs) were proposed to improve the\noriginal RNN model, and achieved signiﬁcant performance\nimprovements [54]–[56]. Recently, the Transformer archi-\ntecture based on the self-attention mechanism has exhib-\nited state-of-the-art performance in most NLP tasks, which\ninspired us to propose a hierarchical transformer encoder\nmodel.\nX. CONCLUSION AND FUTURE WORK\nIn this paper, we propose a novel Hierarchical Graph Trans-\nformer for large-scale multi-label text classiﬁcation. We ﬁrst\nmodel the text into a graph structure that can embody the\ndifferent semantics of the text and the connections between\nthem. To fully capture textual information, we introduce a\nmulti-layer transformer structure with a multi-head attention\nmechanism at the word, sentence, and graph levels to fully\ncapture the features of the text and observe the importance\nof the different parts. To explore the hierarchical relationship\nbetween tags, we utilize the hierarchical relationship of the\nlabels to generate the representation of the label and design\na weighted loss function based on the semantic distance of\nthe label. Extensive experiments conducted on three bench-\nmark datasets demonstrated that the proposed model can\nrealistically capture the hierarchy and logic of the text and\nhierarchical relationship of the labels.\nIn the future, we plan to upgrade our transformer model\nby utilizing more powerful BERT (Bidirectional Encoder\nRepresentations from Transformers) pre-trained models and\nincrement embedding [57], [58].\nREFERENCES\n[1] T. Ali and S. Asghar, ‘‘Multi-label scientiﬁc document classiﬁcation,’’\nJ. Internet Technol., vol. 19, no. 6, pp. 1707–1716, 2018.\n[2] L. Liu, F. Mu, P. Li, X. Mu, J. Tang, X. Ai, R. Fu, L. Wang, and X. Zhou,\n‘‘Neuralclassiﬁer: An open-source neural hierarchical multi-label text clas-\nsiﬁcation toolkit,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Linguis-\ntics, Syst. Demonstrations, 2019, pp. 87–92.\n[3] J. Liu, W.-C. Chang, Y . Wu, and Y . Yang, ‘‘Deep learning for extreme\nmulti-label text classiﬁcation,’’ in Proc. 40th Int. ACM SIGIR Conf. Res.\nDevelop. Inf. Retr. (SIGIR), 2017, pp. 115–124.\n[4] J. K. Frank Cooper, ‘‘Multiobjective feature selection: Classiﬁcation using\neducational datasets in an ensemble validation scheme,’’ Data Sci. Pattern\nRecognit., vol. 3, no. 1, pp. 9–34, 2019.\n[5] C. C. Aggarwal and C. Zhai, Mining Text Data. Berlin, Germany: Springer,\n2012.\n[6] R. A. Stein, P. A. Jaques, and J. F. Valiati, ‘‘An analysis of hierarchical text\nclassiﬁcation using word embeddings,’’ Inf. Sci., vol. 471, pp. 216–232,\nJan. 2019.\n[7] M. B. Revanasiddappa and B. S. Harish, ‘‘A novel text representation\nmodel to categorize text documents using convolution neural network,’’\nInt. J. Intell. Syst. Appl., vol. 11, no. 5, pp. 36–45, May 2019.\n30894 VOLUME 8, 2020\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\n[8] M. Xie, H. Yin, H. Wang, F. Xu, W. Chen, and S. Wang, ‘‘Learning graph-\nbased POI embedding for location-based recommendation,’’ in Proc. 25th\nACM Int. Conf. Inf. Knowl. Manage. (CIKM), 2016, pp. 15–24.\n[9] J. Li, H. Peng, L. Liu, G. Xiong, B. Du, H. Ma, L. Wang, and\nM. Zakirul Alam Bhuiyan, ‘‘Graph CNNs for urban trafﬁc pas-\nsenger ﬂows prediction,’’ in Proc. IEEE SmartWorld, Ubiquitous\nIntell. Comput., Adv. Trusted Comput., Scalable Comput. Commun.,\nCloud Big Data Comput., Internet People Smart City Innov. (Smart-\nWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), Oct. 2018, pp. 29–36.\n[10] H. Peng, J. Li, Q. Gong, Y . Song, Y . Ning, K. Lai, and P. S. Yu, ‘‘Fine-\ngrained event categorization with heterogeneous graph convolutional net-\nworks,’’ in Proc. Int. Joint Conf. Artif. Intell., 2019, pp. 1–9.\n[11] H. Peng, J. Li, Y . He, Y . Liu, M. Bao, L. Wang, Y . Song, and Q. Yang,\n‘‘Large-scale hierarchical text classiﬁcation with recursively regularized\ndeep graph-cnn,’’ in Proc. World Wide Web Conf., Int. World Wide Web\nConf. Steering Committee, 2018, pp. 1063–1072.\n[12] H. Peng, J. Li, S. Wang, L. Wang, Q. Gong, R. Yang, B. Li, P. Yu, and L. He,\n‘‘Hierarchical taxonomy-aware and attentional graph capsule RCNNs for\nlarge-scale multi-label text classiﬁcation,’’ IEEE Trans. Knowl. Data Eng.,\nto be published.\n[13] H. Peng, B. Du, H. Ma, M. Z. A. B. Bhuiyan, L. Jianwei, L. Wang, and\nP. S. Yu, ‘‘Spatial temporal incidence dynamic graph neural networks for\ntrafﬁc ﬂow forecasting,’’ Inf. Sci., to be published.\n[14] Y . He, J. Li, Y . Song, M. He, and H. Peng, ‘‘Time-evolving text classiﬁca-\ntion with deep neural networks,’’ in Proc. 27th Int. Joint Conf. Artif. Intell.,\nJul. 2018, pp. 2241–2247.\n[15] C.-C. Kao, J.-W. Chang, T.-I. Wang, Y .-M. Huang, and P.-S. Chiu, ‘‘Design\nand development of the sentence-based collocation recommender with\nerror detection for academic writing,’’ J. Internet Technol., vol. 20, no. 1,\npp. 229–236, 2019.\n[16] P. Liu, X. Qiu, and X. Huang, ‘‘Recurrent neural network for text clas-\nsiﬁcation with multi-task learning,’’ 2016, arXiv:1605.05101. [Online].\nAvailable: http://arxiv.org/abs/1605.05101\n[17] T. Shen, T. Zhou, G. Long, J. Jiang, and C. Zhang, ‘‘Bi-directional block\nself-attention for fast and memory-efﬁcient sequence modeling,’’ 2018,\narXiv:1804.00857. [Online]. Available: http://arxiv.org/abs/1804.00857\n[18] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,\nand Y . Bengio, ‘‘Show, attend and tell: Neural image caption gener-\nation with visual attention,’’ in Proc. Int. Conf. Mach. Learn., 2015,\npp. 2048–2057.\n[19] J. Donahue, L. A. Hendricks, M. Rohrbach, S. Venugopalan,\nS. Guadarrama, K. Saenko, and T. Darrell, ‘‘Long-term recurrent\nconvolutional networks for visual recognition and description,’’ IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 39, no. 4, pp. 677–691, Apr. 2017.\n[20] C. Dos Santos and M. Gatti, ‘‘Deep convolutional neural networks for sen-\ntiment analysis of short texts,’’ in Proc. 25th Int. Conf. Comput. Linguistics\n(COLING), 2014, pp. 69–78.\n[21] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, ‘‘Caffe: Convolutional architecture for fast\nfeature embedding,’’ in Proc. 22nd ACM Int. Conf. Multimedia, 2014,\npp. 675–678.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classiﬁcation\nwith deep convolutional neural networks,’’ Commun. ACM, vol. 60, no. 6,\npp. 84–90, May 2017.\n[23] M. H. Arif, J. Li, M. Iqbal, and H. Peng, ‘‘Optimizing XCSR for text clas-\nsiﬁcation,’’ in Proc. IEEE Symp. Service-Oriented System Eng. (SOSE),\nApr. 2017, pp. 86–95.\n[24] L. Dong and H. Zhao, ‘‘Hierarchical feature selection with orthogonal\ntransfer,’’J. Internet Technol., vol. 20, no. 4, pp. 1205–1212, 2019.\n[25] M. Nickel and D. Kiela, ‘‘Poincaré embeddings for learning hierar-\nchical representations,’’ in Proc. Adv. Neural Inf. Process. Syst., 2017,\npp. 6338–6347.\n[26] J. Ye, J. Ni, and Y . Yi, ‘‘Deep learning hierarchical representations for\nimage steganalysis,’’ IEEE Trans. Inf. Forensics Security, vol. 12, no. 11,\npp. 2545–2557, Nov. 2017.\n[27] H. Lee, R. Grosse, R. Ranganath, and A. Y . Ng, ‘‘Unsupervised learning\nof hierarchical representations with convolutional deep belief networks,’’\nCommun. ACM, vol. 54, no. 10, p. 95, Oct. 2011.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n[30] B. Chiu, G. Crichton, A. Korhonen, and S. Pyysalo, ‘‘How to train good\nword embeddings for biomedical NLP,’’ in Proc. 15th Workshop Biomed.\nNatural Lang. Process., 2016, pp. 166–174.\n[31] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, ‘‘A neural probabilistic\nlanguage model,’’ J. Mach. Learn. Res., vol. 3, pp. 1137–1155, Feb. 2003.\n[32] D. D. Lewis, Y . Yang, T. G. Rose, and F. Li, ‘‘RCV1: A new benchmark\ncollection for text categorization research,’’ J. Mach. Learn. Res., vol. 5,\npp. 361–397, Dec. 2004.\n[33] R. Agrawal, A. Gupta, Y . Prabhu, and M. Varma, ‘‘Multi-label learning\nwith millions of labels: Recommending advertiser bid phrases for Web\npages,’’ in Proc. 22nd Int. Conf. World Wide Web, 2013, pp. 13–24.\n[34] A. Conneau, H. Schwenk, L. Barrault, and Y . Lecun, ‘‘Very deep convolu-\ntional networks for text classiﬁcation,’’ 2016, arXiv:1606.01781. [Online].\nAvailable: http://arxiv.org/abs/1606.01781\n[35] H. Chen, M. Sun, C. Tu, Y . Lin, and Z. Liu, ‘‘Neural sentiment classiﬁca-\ntion with user and product attention,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2016, pp. 1650–1659.\n[36] J. Wehrmann, R. Cerri, and R. Barros, ‘‘Hierarchical multi-label classiﬁ-\ncation networks,’’ in Proc. Int. Conf. Mach. Learn., 2018, pp. 5225–5234.\n[37] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, ‘‘Hierarchical\nattention networks for document classiﬁcation,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., 2016,\npp. 1480–1489.\n[38] S. Gopal and Y . Yang, ‘‘Hierarchical Bayesian inference and recursive\nregularization for large-scale classiﬁcation,’’ ACM Trans. Knowl. Discov.\nData, vol. 9, no. 3, pp. 1–23, Apr. 2015.\n[39] Y . Prabhu and M. Varma, ‘‘Fastxml: A fast, accurate and stable tree-\nclassiﬁer for extreme multi-label learning,’’ in Proc. 20th ACM SIGKDD\nInt. Conf. Knowl. Discovery Data Mining, 2014, pp. 263–272.\n[40] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, ‘‘Sparse local embeddings\nfor extreme multi-label classiﬁcation,’’ in Proc. Adv. Neural Inf. Process.\nSyst., 2015, pp. 730–738.\n[41] Y . Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. Varma, ‘‘Para-\nbel: Partitioned label trees for extreme classiﬁcation with application\nto dynamic search advertising,’’ in Proc. World Wide Web Conf., 2018,\npp. 993–1002.\n[42] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[43] Y . Liu, H. Peng, J. Li, Y . Song, and X. Li, ‘‘Event detection and evolution\nin multi-lingual social streams,’’ Frontiers Comput. Sci., pp. 1–23, 2019.\n[44] B. Myroniv, C. Wu, Y . Ren, A. Christian, E. Bajo, and Y . C. Tseng, ‘‘Ana-\nlyzing user emotions via physiology signals,’’ Data Sci. Pattern Recognit.,\nvol. 1, no. 2, pp. 11–25, 2017.\n[45] Y . Zhang, R. Jin, and Z.-H. Zhou, ‘‘Understanding bag-of-words model:\nA statistical framework,’’ Int. J. Mach. Learn. Cyber., vol. 1, nos. 1–4,\npp. 43–52, Dec. 2010.\n[46] D. Filliat, ‘‘A visual bag of words method for interactive qualitative local-\nization and mapping,’’ in Proc. IEEE Int. Conf. Robot. Autom., Apr. 2007,\npp. 3921–3926.\n[47] J. Ramos, ‘‘Using TF-IDF to determine word relevance in document\nqueries,’’ in Proc. 1st Instructional Conf. Mach. Learn., Piscataway, NJ,\nUSA, vol. 242, 2003, pp. 133–142.\n[48] A. Aizawa, ‘‘An information-theoretic perspective of TF–IDF measures,’’\nInf. Process. Manage., vol. 39, no. 1, pp. 45–65, Jan. 2003.\n[49] B. Wu, C. Li, and B. Wang, ‘‘Event detection and evolution based on entity\nseparation,’’ in Proc. 8th Int. Conf. Fuzzy Syst. Knowl. Discovery (FSKD),\nJul. 2011, pp. 1–7.\n[50] F. Rousseau, E. Kiagias, and M. Vazirgiannis, ‘‘Text categorization as a\ngraph classiﬁcation problem,’’ in Proc. 53rd Annu. Meeting Assoc. Com-\nput. Linguistics, 7th Int. Joint Conf. Natural Language Process., vol. 1,\n2015, pp. 1702–1712.\n[51] H. Peng, J. Li, Q. Gong, S. Wang, Y . Ning, and P. S. Yu,\n‘‘Graph convolutional neural networks via motif-based attention,’’ 2018,\narXiv:1811.08270. [Online]. Available: http://arxiv.org/abs/1811.08270\n[52] Q. Mao, J. Li, S. Wang, Y . Zhang, H. Peng, M. He, and L. Wang, ‘‘Aspect-\nbased sentiment classiﬁcation with attentive neural turing machines,’’ in\nProc. 28th Int. Joint Conf. Artif. Intell., Aug. 2019, pp. 5139–5145.\n[53] S. Lai, L. Xu, K. Liu, and J. Zhao, ‘‘Recurrent convolutional neural\nnetworks for text classiﬁcation,’’ in Proc. 29th AAAI Conf. Artif. Intell.,\n2015, pp. 2267–2273.\n[54] S. Hochreiter and J. Schmidhuber, ‘‘LSTM can solve hard long time lag\nproblems,’’ in Proc. Adv. Neural Inf. Process. Syst., 1997, pp. 473–479.\nVOLUME 8, 2020 30895\nJ. Gonget al.: Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification\n[55] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations\nusing rnn encoder-decoder for statistical machine translation,’’ 2014,\narXiv:1406.1078. [Online]. Available: http://arxiv.org/abs/1406.1078\n[56] Y .-M. L. Ko-Wei Huang, C.-C. Lin, and Z.-X. Wu, ‘‘A deep learning\nand image recognition system for image recognition,’’ Data Sci. Pattern\nRecognit., vol. 3, no. 2, pp. 1–23, 2019.\n[57] H. Peng, J. Li, Y . Song, and Y . Liu, ‘‘Incrementally learning the hierarchical\nsoftmax function for neural language models,’’ in Proc. 31st AAAI Conf.\nArtif. Intell.London, U.K.: AAAI Press, 2017, pp. 3267–3273.\n[58] H. Peng, M. Bao, J. Li, M. Bhuiyan, Y . Liu, Y . He, and E. Yang, ‘‘Incre-\nmental term representation learning for social network analysis,’’ Future\nGener. Comput. Syst., vol. 86, pp. 1503–1512, Sep. 2018.\nJIBING GONG received the Ph.D. degree from\nthe Institute of Computing Technology, Chinese\nAcademy of Sciences, China. He is currently a\nProfessor with the School of Information Sci-\nence and Engineering, Yanshan University. He is\nalso the Head of the Knowledge Engineering\nGroup (KEG) Research Team, Yanshan Univer-\nsity. His main research interests include big data\nanalytics, heterogeneous information networks,\nmachine learning, and data fusion.\nZHIYONG TENG is currently pursuing the mas-\nter’s degree with the Department of Information\nScience and Engineering, Yanshan University. His\nresearches focus on deep learning and NLP.\nQI TENG is currently pursuing the B.E. degree\nwith the State Key Laboratory of Software Devel-\nopment Environment, Beihang University. His\nresearch interests include social network mining\nand graph mining.\nHEKAI ZHANG is currently pursuing the mas-\nter’s degree with the Department of Information\nScience and Engineering, Yanshan University. His\nmain research interests include machine learning\nand graph neural networks.\nLINFENG DU is currently pursuing the B.E.\ndegree with the Department of Computer Science\nand Engineering, Beihang University (BUAA),\nBeijing, China.\nSHUAI CHEN received the bachelor’s degree in\nsoftware engineering. He is currently pursuing the\nmaster’s degree with the School of Information\nScience and Engineering, Yanshan University. His\nmain research interests include deep learning and\nNLP.\nMD ZAKIRUL ALAM BHUIYANis currently an\nAssistant Professor with the Department of Com-\nputer and Information Sciences, Fordham Univer-\nsity, New York, NY , USA. His researches focus on\ndependable cyber-physical systems, WSN appli-\ncations, network security, urban computing, and\nsensor-cloud computing. He is also an Associate\nEditor of IEEE ACCESS.\nJIANHUA LI is currently an Associate Pro-\nfessor with the School of Information Science\nand Technology, Shijiazhuang Tiedao University,\nShijiazhuang, China. His research interests\ninclude pattern recognition, multimedia process-\ning, machine learning, and evolution computing.\nMINGSHENG LIU is currently a Professor with\nthe College of Electrical Engineering, Hebei Uni-\nversity of Technology. His researches focus on\nnetwork and information security, and information\nmanagement and application.\nHONGYUAN MAis currently a Senior Engineer\nwith the National Computer Network Emergency\nResponse Technical Team/Coordination Center of\nChina. His current research interests include infor-\nmation retrieval, big data mining, and analytics.\n30896 VOLUME 8, 2020",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7832229137420654
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6598162651062012
    },
    {
      "name": "Sentence",
      "score": 0.5560500025749207
    },
    {
      "name": "Natural language processing",
      "score": 0.5485773682594299
    },
    {
      "name": "Graph",
      "score": 0.5342848300933838
    },
    {
      "name": "Transformer",
      "score": 0.5284233689308167
    },
    {
      "name": "Text graph",
      "score": 0.48268771171569824
    },
    {
      "name": "Hierarchy",
      "score": 0.46833348274230957
    },
    {
      "name": "Word2vec",
      "score": 0.4140566289424896
    },
    {
      "name": "Deep learning",
      "score": 0.4140281677246094
    },
    {
      "name": "Machine learning",
      "score": 0.3306417167186737
    },
    {
      "name": "Text mining",
      "score": 0.27007925510406494
    },
    {
      "name": "Theoretical computer science",
      "score": 0.24537310004234314
    },
    {
      "name": "Embedding",
      "score": 0.09674471616744995
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Market economy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39333907",
      "name": "Yanshan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210087772",
      "name": "National Computer Network Emergency Response Technical Team/Coordination Center of Chinar",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I164389053",
      "name": "Fordham University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I90090648",
      "name": "Shijiazhuang Tiedao University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I184843921",
      "name": "Hebei University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 61
}