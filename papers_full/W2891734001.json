{
  "title": "Language Modeling with Sparse Product of Sememe Experts",
  "url": "https://openalex.org/W2891734001",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5002504458",
      "name": "Yihong Gu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5114549734",
      "name": "Jun Yan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101693600",
      "name": "Hao Zhu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100320723",
      "name": "Zhiyuan Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101577090",
      "name": "Ruobing Xie",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5046448314",
      "name": "Maosong Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102486495",
      "name": "Fen Lin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5023086553",
      "name": "Leyu Lin",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2742102274",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2740768945",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W2950075229",
    "https://openalex.org/W2536575466",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W1513915383",
    "https://openalex.org/W2095368471",
    "https://openalex.org/W2062270497",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2962996600",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W2304113845",
    "https://openalex.org/W2740745856",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2949750592",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W2028339364",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1988939740",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2079182758",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W2039550232"
  ],
  "abstract": "Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution gave textual context. Afterward, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models. Experiments on language modeling and the downstream application of headline gener- ation demonstrate the significant effect of SDLM. Source code and data used in the experiments can be accessed at https:// github.com/thunlp/SDLM-pytorch.",
  "full_text": "Language Modeling with Sparse Product of Sememe Experts\nYihong Gu1,2,∗ Jun Yan1,3,∗ Hao Zhu1,2,∗ Zhiyuan Liu1,2,†\nRuobing Xie4 Maosong Sun1,2 Fen Lin4 Leyu Lin4\n1Institute for Artiﬁcial Intelligence\nState Key Lab on Intelligent Technology and Systems\n2Department of CST, 3Department of EE, Tsinghua University, Beijing, China\n4Search Product Center, WeChat Search Application Department, Tencent\n{gyh15,j-yan15,zhuhao15}@mails.tsinghua.edu.cn,\n{lzy,sms}@tsinghua.edu.cn, xrbsnowing@163.com,\n{felicialin,goshawklin}@tencent.com\nAbstract\nMost language modeling methods rely on\nlarge-scale data to statistically learn the se-\nquential patterns of words. In this pa-\nper, we argue that words are atomic lan-\nguage units but not necessarily atomic seman-\ntic units. Inspired by HowNet, we use se-\nmemes, the minimum semantic units in hu-\nman languages, to represent the implicit se-\nmantics behind words for language model-\ning, named Sememe-Driven Language Model\n(SDLM). More speciﬁcally, to predict the next\nword, SDLM ﬁrst estimates the sememe dis-\ntribution given textual context. Afterwards, it\nregards each sememe as a distinct semantic ex-\npert, and these experts jointly identify the most\nprobable senses and the corresponding word.\nIn this way, SDLM enables language mod-\nels to work beyond word-level manipulation to\nﬁne-grained sememe-level semantics, and of-\nfers us more powerful tools to ﬁne-tune lan-\nguage models and improve the interpretabil-\nity as well as the robustness of language mod-\nels. Experiments on language modeling and\nthe downstream application of headline gener-\nation demonstrate the signiﬁcant effectiveness\nof SDLM. Source code and data used in the\nexperiments can be accessed at https://\ngithub.com/thunlp/SDLM-pytorch.\n1 Introduction\nLanguage Modeling (LM) aims to measure the\nprobability of a word sequence, reﬂecting its ﬂu-\nency and likelihood as a feasible sentence in a\nhuman language. Language Modeling is an es-\nsential component in a wide range of natural lan-\nguage processing (NLP) tasks, such as Machine\nTranslation (Brown et al., 1990; Brants et al.,\n2007), Speech Recognition (Katz, 1987), Informa-\ntion Retrieval (Berger and Lafferty, 1999; Ponte\n∗Equal contribution.\n†Correspondence author.\n(a) Conventional Decoder\ncontext \nvector\nword  \ndistribution\ncontext \nvector\nSememe-Driven Decoder word \ndistributionsememe \ndistributionSememe \nPredictor\nSense \nPredictor \nWord \nPredictor\nsense \ndistribution(b)\nFigure 1: Decoder of (a) Conventional Language\nModel, (b) Sememe-Driven Language Model.\nand Croft, 1998; Miller et al., 1999; Hiemstra,\n1998) and Document Summarization (Rush et al.,\n2015; Banko et al., 2000).\nA probabilistic language model calculates the\nconditional probability of the next word given\ntheir contextual words, which are typically learned\nfrom large-scale text corpora. Taking the sim-\nplest language model for example, N-Gram es-\ntimates the conditional probabilities according to\nmaximum likelihood over text corpora (Jurafsky,\n2000). Recent years have also witnessed the ad-\nvances of Recurrent Neural Networks (RNNs) as\nthe state-of-the-art approach for language model-\ning (Mikolov et al., 2010), in which the context is\nrepresented as a low-dimensional hidden state to\npredict the next word.\nThose conventional language models including\nneural models typically assume words as atomic\nsymbols and model sequential patterns at word\nlevel. However, this assumption does not neces-\nsarily hold to some extent. Let us consider the fol-\nlowing example sentence for which people want to\npredict the next word in the blank,\nThe U.S. trade deﬁcit last year is initially\nestimated to be 40 billion .\nPeople may ﬁrst realize a unit should be ﬁlled in,\nthen realize it should be a currency unit. Based on\nthe country this sentence is talking about, the U.S.,\none may conﬁrm it should be an American cur-\narXiv:1810.12387v1  [cs.CL]  29 Oct 2018\nrency unit and predict the word dollars. Here, the\nunit, currency, and American can be regarded as\nbasic semantic units of the worddollars. This pro-\ncess, however, has not been explicitly taken into\nconsideration by conventional language models.\nThat is, although in most cases words are atomic\nlanguage units, words are not necessarily atomic\nsemantic units for language modeling. We ar-\ngue that explicitly modeling these atomic semantic\nunits could improve both the performance and the\ninterpretability of language models.\nLinguists assume that there is a limited close\nset of atomic semantic units composing the se-\nmantic meanings of an open set of concepts (i.e.\nword senses). These atomic semantic units are\nnamed sememes (Dong and Dong, 2006). i Since\nsememes are naturally implicit in human lan-\nguages, linguists have devoted much effort to ex-\nplicitly annotate lexical sememes for words and\nbuild linguistic common-sense knowledge bases.\nHowNet (Dong and Dong, 2006) is one of the\nrepresentative sememe knowledge bases, which\nannotates each Chinese word sense with its se-\nmemes. The philosophy of HowNet regards the\nparts and attributes of a concept can be well rep-\nresented by sememes. HowNet has been widely\nutilized in many NLP tasks such as word similar-\nity computation (Liu, 2002) and sentiment analy-\nsis (Fu et al., 2013). However, less effort has been\ndevoted to exploring its effectiveness in language\nmodels, especially neural language models.\nIt is non-trivial for neural language models to\nincorporate discrete sememe knowledge, as it is\nnot compatible with continuous representations\nin neural models. In this paper, we propose\na Sememe-Driven Language Model (SDLM) to\nleverage lexical sememe knowledge. In order to\npredict the next word, we design a novel sememe-\nsense-word generation process: (1) We ﬁrst esti-\nmate sememes’ distribution according to the con-\ntext. (2) Regarding these sememes as experts, we\npropose a sparse product of experts method to se-\nlect the most probable senses. (3) Finally, the dis-\ntribution of words could be easily calculated by\nmarginalizing out the distribution of senses.\nWe evaluate the performance of SDLM on the\nlanguage modeling task using a Chinese news-\ni Note that although sememes are deﬁned as the mini-\nmum semantic units, there still exist several sememes for\ncapturing syntactic information. For example, the word\n和“with” corresponds to one speciﬁc sememe 功能词\n“FunctWord”.\npaper corpus People’s Daily ii (Renmin Ribao),\nand also on the headline generation task using the\nLarge Scale Chinese Short Text Summarization\n(LCSTS) dataset (Hu et al., 2015). Experimen-\ntal results show that SDLM outperforms all those\ndata-driven baseline models. We also conduct case\nstudies to show that our model can effectively pre-\ndict relevant sememes given context, which can\nimprove the interpretability and robustness of lan-\nguage models.\n2 Background\nLanguage models target at learning the\njoint probability of a sequence of words\nP(w1,w2,··· ,wn), which is usually factor-\nized as P(w1,w2,··· ,wn) = ∏n\nt=1 P(wt|w<t).\nBengio et al. (2003) propose the ﬁrst Neural Lan-\nguage Model as a feed-forward neural network.\nMikolov et al. (2010) use RNN and a softmax\nlayer to model the conditional probability. To be\nspeciﬁc, it can be divided into two parts in series.\nFirst, a context vector gt is derived from a deep\nrecurrent neural network. Then, the probability\nP(wt+1|w≤t) = P(wt+1; gt) is derived from a\nlinear layer followed by a softmax layer based\non gt. Let RNN(·,·; θNN) denote the deep\nrecurrent neural network, where θNN denotes the\nparameters. The ﬁrst part can be formulated as\ngt = RNN(xwt,{ht−1\nl }L\nl=1; θNN). (1)\nHere we use subscripts to denote layers and su-\nperscripts to denote timesteps. Thus ht\nl represents\nthe hidden state of the L-th layer at timestep t.\nxwt ∈ RH0 is the input embedding of word wt\nwhere H0 is the input embedding size. We also\nhave gt ∈RH1, where H1 is the dimension of the\ncontext vector.\nSupposing that there are N words in the lan-\nguage we want to model, the second part can be\nwritten as\nP(wt+1; gt) = exp(gtTwwt+1)∑\nw′ exp(gtTww′)\n, (2)\nwhere ww is the output embedding of word wand\nw1,w2,···wN ∈ RH2. Here H2 is the output\nembedding size. For a conventional neural lan-\nguage model, H2 always equals to H1.\nii http://paper.people.com.cn/rmrb/\n苹果(⽔果) \n“apple(fruit)”\n苹果(电脑) \n“apple(computer)”\n梨⼦(⽔果) \n“pear(fruit)”\n苹果 \n“apple”\n…\n… 梨⼦ \n“pear”P(word)\nP(sense)\n⽔果 \n“fruit”\n携带 \n“bring”\n电脑 \n“computer”\n特定牌⼦ \n“SpeBrand”\n能 \n“able”\n样式值 \n “PatternVal”\n… x\n0.2\n×\nsememe \nexperts\ncontext \nvector\n我 \n“I”\n在 \n“in”\n果园 \n“orchard”\n摘 \n“pick”\nLSTM LSTM LSTM LSTM\n… x\n0.1\n×\nx\n0.9\n×\nx\n0.1\n×\nx\n0.3\n×\nx\n0.2\n×\nFigure 2: An example of the architecture of our model.\nGiven the corpus {wt}n\nt=1, the loss function is\ndeﬁned by the negative log-likelihood:\nL(θ) =−1\nn\nn∑\nt=1\nlog P(wt|w<t; θ), (3)\nwhere θ = {{xi}N\ni=1,{wi}N\ni=1,θNN}is the set of\nparameters that are needed to be trained.\n3 Methodology\nIn this section, we present our SDLM which uti-\nlizes sememe information to predict the probabil-\nity of the next word. SDLM is composed of three\nmodules in series: Sememe Predictor, Sense Pre-\ndictor and Word Predictor. The Sememe Predictor\nﬁrst takes the context vector as input and assigns a\nweight to each sememe. Then each sememe is re-\ngarded as an expert and makes predictions about\nthe probability distribution over a set of senses\nin the Sense Predictor. Finally, the probability of\neach word is obtained in the Word Predictor.\nHere we use an example shown in Figure 2 to\nillustrate our architecture. Given context 我在果\n园摘“In the orchard, I pick”, the actual next word\ncould be 苹果 “apples”. From the context, espe-\ncially the word 果园 “orchard” and 摘 “pick”, we\ncan infer that the next word probably represents\na kind of fruit. So the Sememe Predictor assigns\na higher weight to the sememe 水果 “fruit” (0.9)\nand lower weights to irrelevant sememes like 电\n脑 “computer” (0.1). Therefore in the Sense Pre-\ndictor, the sense 苹果 (水果) “apple (fruit)” is as-\nsigned a much higher probability than the sense苹\n果 (电脑) “apple (computer)”. Finally, the prob-\nability of the word 苹果 “apple” is calculated as\nthe sum of the probabilities of its senses 苹果 (水\n\u000e\u0004 \n“apple”\n\u0003\n\f \n“PatternVal”\n\u0006\u000b \n“computer”\n\u0001 \n“able”\n\u000f\b \n“bring”\n\t\u0007\r\u0002 \n“SpeBrand”\n\u0005\u0004 \n“fruit”\nmodifier\nmodifier\nSense #1:  \n\u0005\u0001(\u0003\u0004) \n“apple(computer)”\nSense #2:  \n\u0005\u0001(\u0002\u0001) \n“apple(fruit)”\nword\nFigure 3: An example of the word-sense-sememe hier-\narchy.\n果) “apple(fruit)” and 苹果 (电脑) “apple (com-\nputer)”.\nIn the following subsections, we ﬁrst introduce\nthe word-sense-sememe hierarchy in HowNet, and\nthen give details about our SDLM.\n3.1 Word-Sense-Sememe Hierarchy\nWe also use the example of “apple” to illustrate\nthe word-sense-sememe hierarchy. As shown in\nFigure 3, the word 苹果 “apple” has two senses,\none is the Apple brand, the other is a kind of fruit.\nEach sense is annotated with several sememes or-\nganized in a hierarchical structure. More speciﬁ-\ncally, in HowNet, sememes “PatternVal”, “bring”,\n“SpeBrand”, “computer” and “able” are annotated\nwith the word “apple” and organized in a tree\nstructure. In this paper, we ignore the structural\nrelationship between sememes. For each word, we\ngroup all its sememes as an unordered set.\nWe present the notations that we use in the fol-\nlowing subsections as follows. We deﬁne the over-\nall sememe, sense, and word set as E, Sand W.\nAnd we suppose the corpus contains K = |E|se-\nmemes, M = |S|senses and N = |W|words.\nFor word w ∈ W, we denote its corresponding\nsense set as S(w). For sense s ∈ S(w), we de-\nnote its corresponding sememes as an unordered\nset E(s) = {en1,en2,··· ,enk}⊂E = {ek}K\nk=1.\n3.2 Sememe Predictor\nThe Sememe Predictor takes the context vec-\ntor g ∈ RH1 as input and assigns a weight to\neach sememe. We assume that given the context\nw1,w2,··· ,wt−1, the events that word wt con-\ntains sememe ek (k∈{1,2,··· ,K}) are indepen-\ndent, since the sememe is the minimum semantic\nunit and there is no semantic overlap between any\ntwo different sememes. For simplicity, we ignore\nthe superscript t. We design the Sememe Predic-\ntor as a linear decoder with the sigmoid activation\nfunction. Therefore, qk, the probability that the\nnext word contains sememe ek, is formulated as\nqk = P(ek|g) = σ(gTvk + bk), (4)\nwhere vk ∈RH1, bk ∈R are trainable parameters,\nand σ(·) denotes the sigmoid activation function.\n3.3 Sense Predictor and Word Predictor\nThe architecture of the Sense Predictor is moti-\nvated by Product of Experts (PoE) (Hinton, 1999).\nWe regard each sememe as an expert that only\nmakes predictions on the senses connected with it.\nLet D(ek) denote the set of senses that contain se-\nmeme ek, the k-th expert. Different from conven-\ntional neural language models, which directly use\nthe inner product of the context vector g ∈RH1\nand the output embedding ww ∈ RH2 for word\nw to generate the score for each word, we use\nφ(k)(g,w) to calculate the score given by expert\nek. And we choose a bilinear function parame-\nterized with a matrix Uk ∈RH1×H2 as a straight\nimplementation of φ(k)(·,·):\nφ(k)(g,w) = gTUkw. (5)\nLet ws denote the output embedding of sense\ns. The score of sense sprovided by sememe ex-\npert ek can be written as φ(k)(g,ws). Therefore,\nP(ek)(s|g), the probability of sense sgiven by ex-\npert ek, is formulated as\nP(ek)(s|g) = exp(qkCk,sφ(k)(g,ws))∑\ns′∈D(ek) exp(qkCk,s′ φ(k)(g,ws′ )), (6)\nwhere Ck,s is a normalization constant because\nsense s is not connected to all experts (the\nconnections are sparse with approximately λN\nedges, λ < 5). Here we can choose either\nCk,s = 1/|E(s)|(left normalization ) or Ck,s =\n1/\n√\n|E(s)||D(ek)|(symmetric normalization).\nIn the Sense Predictor, qk can be viewed as\na gate which controls the magnitude of the term\nCk,sφ(k)(g,wws), thus control the ﬂatness of the\nsense distribution provided by sememe expert ek.\nConsider the extreme case when qk →0, the pre-\ndiction will converge to the discrete uniform dis-\ntribution. Intuitively, it means that the sememe ex-\npert will refuse to provide any useful information\nwhen it is not likely to be related to the next word.\nFinally, we summarize the predictions on sense\nsby taking the product of the probabilities given\nby relevant experts and then normalize the result;\nthat is to say, P(s|g), the probability of sense s,\nsatisﬁes\nP(s|g) ∝\n∏\nek∈E(s)\nP(ek)(s|g). (7)\nUsing Equation 5 and 6, we can formulate\nP(s|g) as\nP(s|g) =\nexp(∑\nek∈E(s) qkCk,sgTUkws)∑\ns′ exp(∑\nek∈E(s′) qkCk,s′ gTUkws′ ). (8)\nIt should be emphasized that all the supervision\ninformation provided by HowNet is embodied in\nthe connections between the sememe experts and\nthe senses. If the model wants to assign a high\nprobability to sense s, it must assign a high prob-\nability to some of its relevant sememes. If the\nmodel wants to assign a low probability to sense\ns, it can assign a low probability to its relevant\nsememes. Moreover, the prediction made by se-\nmeme expert ek has its own tendency because of\nits own φ(k)(·,·). Besides, the sparsity of con-\nnections between experts and senses is also de-\ntermined by HowNet itself. For our dataset, on\naverage, a word is connected with 3.4 sememe ex-\nperts and each sememe expert will make predic-\ntions about 22 senses.\nAs illustrated in Figure 2, in the Word Predic-\ntor, we get P(w|g), the probability of word w,\nby summing up probabilities of corresponding s\ngiven by the Sense Predictor, that is\nP(w|g) =\n∑\ns∈S(w)\nP(s|g). (9)\n3.4 Implementation Details\nBasis Matrix Actually, HowNet contains K ≈\n2000 sememes. In practice, we cannot directly in-\ntroduce K ×H1 ×H2 parameters, which might\nbe computationally infeasible and lead to overﬁt-\nting. To address this problem, we apply a weight-\nsharing trick called the basis matrix. We use R\nbasis matrices and their weighted sum to estimate\nUk:\nUk =\nR∑\nr=1\nαk,rQr, (10)\nwhere Qr ∈RH1×H2, αk,r > 0 are trainable pa-\nrameters, and ∑R\nr=1 αk,r = 1.\nWeight Tying To incorporate the weight tying\nstrategy (Inan et al., 2017; Press and Wolf, 2017),\nwe use the same output embedding for multiple\nsenses of a word. To be speciﬁc, the sense output\nembedding ws for each s ∈S(w) is the same as\nthe word input embedding xw.\n4 Experiments\nWe evaluate our SDLM on a Chinese language\nmodeling dataset, namely People’s Daily based on\nperplexity.iii Furthermore, to show that our SDLM\nstructure can be a generic Chinese word-level de-\ncoder for sequence-to-sequence learning, we con-\nduct a Chinese headline generation experiment on\nthe LCSTS dataset. Finally, we explore the inter-\npretability of our model with cases, showing the\neffectiveness of utilizing sememe knowledge.\n4.1 Language Modeling\nDataset\nWe choose the People’s Daily Corpus, which is\nwidely used for Chinese NLP tasks, as the re-\nsource. It contains one month’s news text from\nPeople’s Daily (Renmin Ribao). Taking Penn\nTreebank (PTB) (Marcus et al., 1993) as a ref-\nerence, we build a dataset for Chinese language\nmodeling based on the People’s Daily Corpus with\n734k, 10k and 19k words in the training, valida-\ntion and test set. After the preprocessing similar\nto (Mikolov et al., 2010) (see Appendix A), we get\nour dataset and the ﬁnal vocabulary size is 13,476.\nBaseline\nAs for baselines, we consider three kinds of neural\nlanguage modeling architectures with LSTM cells:\nsimple LSTM, Tied LSTM and AWD-LSTM.\nLSTM and Tied LSTM Zaremba et al. (2014)\nuse the dropout strategy to prevent overﬁtting for\nneural language models and adopt it to two-layer\nLSTMs with different embedding and hidden size:\n650 for medium LSTM, and 1500 for large LSTM.\nEmploying the weight tying strategy, we get Tied\nLSTM with better performance. We set LSTM and\nTied LSTM of medium and large size as our base-\nline models and use the code from PyTorch exam-\nplesiv as their implementations.\nA WD-LSTMBased on several strategies for reg-\nularizing and optimizing LSTM-based language\nmodels, Merity et al. (2018) propose AWD-LSTM\niii Although we only conduct experiments on Chinese cor-\npora, we argue that this model has the potential to be ap-\nplied to other languages in the light of works on construc-\ntion sememe knowledge bases for other languages, such\nas (Qi et al., 2018).\niv https://github.com/pytorch/examples/\ntree/master/word_language_model\nas a three-layer neural network, which serves as a\nvery strong baseline for word-level language mod-\neling. We build it with the code released by the\nauthorsv.\nVariants of Softmax Meanwhile, to compare our\nSDLM with other language modeling decoders,\nwe set cHSM (Class-based Hierarchical Softmax)\n(Goodman, 2001), tHSM (Tree-based Hierarchi-\ncal Softmax) (Mikolov et al., 2013) and MoS\n(Mixture of Softmaxes) (Yang et al., 2018) as\nthe baseline add-on structures to the architectures\nabove.\nExperimental Settings\nWe apply our SDLM and other variants of softmax\nstructures to the architectures mentioned above:\nLSTM (medium / large), Tied LSTM (medium /\nlarge) and AWD-LSTM. MoS and SDLM are only\napplied on the models that incorporate weight ty-\ning, while tHSM is only applied on the models\nwithout weight tying, since it is not compatible\nwith this strategy.\nFor a fair comparison, we train these mod-\nels with same experimental settings and conduct\na hyper-parameter search for baselines as well\nas our models (the search setting and the opti-\nmal hyper-parameters can be found in Appendix\nC.1). We keep using these hyper-parameters in our\nSDLM for all architectures. It should be empha-\nsized that we use the SGD optimizer for all archi-\ntectures, and we decrease the learning rate by a\nfactor of 2 if no improvement is observed on the\nvalidation set. We uniformly initialize the word\nembeddings, the class embeddings for cHSM and\nthe non-leaf embeddings for tHSM in [−0.1,0.1].\nIn addition, we set R, the number of basis matri-\nces, to 5 in Tied LSTM architecture and to 10 in\nAWD-LSTM architecture. We choose theleft nor-\nmalization strategy because it performs better.\nExperimental Results\nTable 1 shows the perplexity on the validation and\ntest set of our models and the baseline models.\nFrom Table 1, 2, and 3, we can observe that:\n1. Our models outperform the corresponding base-\nline models of all structures, which indicates the\neffectiveness of our SDLM. Moreover, our SDLM\nnot only consistently outperforms state-of-the-art\nMoS model, but also offers much better inter-\npretability (as described in Sect. 4.3), which\nv https://github.com/salesforce/\nawd-lstm-lm\nModel #Paras Validation Test\nLSTM (medium) 24M 116.46 115.51\n+ cHSM 24M 129.12 128.12\n+ tHSM 24M 151.00 150.87\nTied LSTM (medium) 15M 105.35 104.67\n+ cHSM 15M 116.78 115.66\n+ MoS 17M 98.47 98.12\n+ SDLM 17M 97.75 97.32\nLSTM (large) 76M 112.39 111.66\n+ cHSM 76M 120.07 119.45\n+ tHSM 76M 140.41 139.61\nTied LSTM (large) 56M 101.46 100.71\n+ cHSM 56M 108.28 107.52\n+ MoS 67M 94.91 94.40\n+ SDLM 67M 94.24 93.60\nAWD-LSTMiv 26M 89.35 88.86\n+ MoS 26M 92.98 92.76\n+ SDLM 27M 88.16 87.66\nTable 1: Single model perplexity on validation and test\nsets on the People’s Daily dataset.\nmakes it possible to interpret the prediction pro-\ncess of the language model. Note that under a fair\ncomparison, we do not see MoS’s improvement\nover AWD-LSTM while our SDLM outperforms\nit by 1.20 with respect to perplexity on the test set.\n2. To further locate the performance improve-\nment of our SDLM, we study the perplexity of\nthe single-sense words and multi-sense words sep-\narately on Tied LSTM (medium) and Tied LSTM\n(medium) + SDLM. Improvements with respect to\nperplexity are presented in Table 2. The perfor-\nmance on both single-sense words and multi-sense\nwords gets improved while multi-sense words\nbeneﬁt more from SDLM structure because they\nhave richer sememe information.\n3. In Table 3 we study the perplexity of words\nwith different mean number of sememes. We can\nsee that our model outperforms baselines in all\ncases and is expected to beneﬁt more as the mean\nnumber of sememes increases.\n1. Our models outperform the corresponding base-\nline models of all structures, which indicates the\neffectiveness of our SDLM. Moreover, our SDLM\nnot only consistently outperforms state-of-the-art\nMoS model, but also offers much better inter-\npretability (as described in Sect. 4.3), which\nmakes it possible to interpret the prediction pro-\ncess of the language model. Note that under a fair\ncomparison, we do not see MoS’s improvement\nover AWD-LSTM while our SDLM outperforms\nit by 1.20 with respect to perplexity on the test set.\n2. To further locate the performance improve-\niv We ﬁnd that multi-layer AWD-LSTM has problems con-\nverging when adopting cHSM, so we skip that result.\n#senses = 1 #senses >1\nBaseline ppl 93.21 121.18\nSDLM ppl 87.22 111.88\n∆ppl 5.99 9.29\n∆ppl/Baseline ppl 6.4% 7.8%\nTable 2: Perplexity of words with different number of\nsenses on the test set.\n[1, 2) [2, 4) [4, 7) [7, 14)\nBaseline ppl 71.56 161.32 557.26 623.71\nSDLM ppl 68.47 114.95 465.29 476.45\n∆ppl 3.09 16.36 91.98 147.25\n∆ppl/Baseline ppl 4.3% 10.1% 16.5% 23.61%\nTable 3: Perplexity of words with different mean num-\nber of sememes on the test set.\nment of our SDLM, we study the perplexity of\nthe single-sense words and multi-sense words sep-\narately on Tied LSTM (medium) and Tied LSTM\n(medium) + SDLM. Improvements with respect to\nperplexity are presented in Table 2. The perfor-\nmance on both single-sense words and multi-sense\nwords gets improved while multi-sense words\nbeneﬁt more from SDLM structure because they\nhave richer sememe information.\n3. In Table 3 we study the perplexity of words\nwith different mean number of sememes. We can\nsee that our model outperforms baselines in all\ncases and is expected to beneﬁt more as the mean\nnumber of sememes increases.\nWe also test the robustness of our model by ran-\ndomly removing 10% sememe-sense connections\nin HowNet. The test perplexity for Tied LSTM\n(medium) + SDLM slightly goes up to 97.67, com-\npared to 97.32 with a complete HowNet, which\nshows that our model is robust to tiny incomplete-\nness of annotations. However, the performance\nof out model is still largely dependent upon the\naccuracy of sememe annotations. As HowNet\nis continuously updated, we expect our model to\nperform better with sememe knowledge of higher\nquality.\n4.2 Headline Generation\nDataset\nWe use the LCSTS dataset to evaluate our SDLM\nstructure as the decoder of the sequence-to-\nsequence model. As its author suggests, we di-\nvide the dataset into the training set, the validation\nset and the test set, whose sizes are 2.4M, 8.7k\nand 725 respectively. Details can be found in Ap-\npendix B.\nModels\nFor this task, we consider two models for compar-\nison.\nRNN-context As described in (Bahdanau et al.,\n2015), RNN-context is a basic sequence-to-\nsequence model with a bi-LSTM encoder, an\nLSTM decoder and attention mechanism adopted.\nThe context vector is concatenated with the word\nembedding at each timestep when decoding. It’s\nwidely used for sequence-to-sequence learning, so\nwe set it as the baseline model.\nRNN-context-SDLM Based on RNN-context,\nwe substitute the decoder with our proposed\nSDLM and name it RNN-context-SDLM.\nExperimental Settings\nWe implement our models with PyTorch, on top of\nthe OpenNMT librariesv. For both models, we set\nthe word embedding size to 250, the hidden unit\nsize to 250, the vocabulary size to 40000, and the\nbeam size of the decoder to 5. For RNN-context-\nSDLM, we set the number of basis matrices to\n3. We conduct a hyper-parameter search for both\nmodels (see Appendix C.2 for settings and optimal\nhyper-parameters).\nExperimental Results\nFollowing previous works, we report the F1-score\nof ROUGE-1, ROUGE-2, and ROUGE-L on the\ntest set. Table 4 shows that our model outperforms\nthe baseline model on all metrics. We attribute the\nimprovement to the use of SDLM structure.\nWords in headlines do not always appear in the\ncorresponding articles. However, words with the\nsame sememes have a high probability to appear in\nthe articles intuitively. Therefore, a probable rea-\nson for the improvement is that our model could\npredict sememes highly relevant to the article, thus\ngenerate more accurate headlines. This could be\ncorroborated by our case study.\nModel Rouge-1 Rouge-2 Rouge-L\nRNN-context 38.2 25.7 35.4\nRNN-context-SDLM 38.8 26.2 36.1\nTable 4: ROUGE scores of both models on the LCSTS\ntest set.\n4.3 Case Study\nThe above experiments demonstrate the effective-\nness of our SDLM. Here we present some samples\nv http://opennmt.net\nfrom the test set of the People’s Daily Corpus in\nTable 5 as well as the LCSTS dataset in Table 6\nand conduct further analysis.\nFor each example of language modeling, given\nthe context of previous words, we list the Top\n5 words and Top 5 sememes predicted by our\nSDLM. The target words and the sememes anno-\ntated with them in HowNet are blackened. Note\nthat if the target word is an out-of-vocabulary\n(OOV) word, helpful sememes that are related to\nthe target meaning are blackened.\nSememes annotated with the corresponding\nsense of the target word 美元 “dollar” are 单位\n“unit”, 商业 “commerce”, 金融 “ﬁnance”, 货币\n“money” and 美国 “US”. In Example (1), the tar-\nget word “dollar” is predicted correctly and most\nof its sememes are activated in the predicting pro-\ncess. It indicates that our SDLM has learned the\nword-sense-sememe hierarchy and used sememe\nknowledge to improve language modeling.\nExample (2) shows that our SDLM can provide\ninterpretable results on OOV word prediction with\nsememe information associated with it. The tar-\nget word here should be the name of the Albanian\nprime minister, which is out of vocabulary. But\nwith our model, one can still conclude that this\nword is probably relevant to the sememe “poli-\ntics”, “person”, “ﬂowers”, “undertake” and “wa-\nters”, most of which characterize the meaning of\nthis OOV word – the name of a politician. This\nfeature can be helpful when the vocabulary size is\nlimited or there are many terminologies and names\nin the corpus.\nFor the example of headline generation, given\nthe article and previous words, when generating\nthe word 生 “student”, except the sememe 预料\nExample (1)\n去年美国贸易逆差初步估计为<N> 。\nThe U.S. trade deﬁcit last year is initially estimated to be<N> .\nTop 5 word prediction\n美 美 美元 元 元“dollar” ，“,” 。“.”\n日元“yen” 和“and”\nTop 5 sememe prediction\n商 商 商业 业 业“commerce”金 金 金融 融 融“ﬁnance” 单 单 单位 位 位“unit”\n多少“amount” 专“proper name”\nExample (2)\n阿总理 已签署了 一项命令。\nAlbanian Prime Minister has signed an order.\nTop 5 word prediction\n内“inside” <unk> 在“at”\n塔“tower” 和“and”\nTop 5 sememe prediction\n政 政 政“politics” 人 人 人“person” 花草“ﬂowers”\n担 担 担任 任 任“undertake”水域“waters”\nTable 5: Some examples of word and sememe predic-\ntions on the test set of the People’s Daily Corpus.\nArticle\n8 日 ， 阜 新 一 开 宝马 轿车 参加高考的 男 考生\n考场 作弊 被 抓 ， 因 不满 监考老师 没收 作弊 手\n机，从背后一脚将女监考老师从最后一排踹\n到讲台。并口出狂言：“ 你知道我爸是谁啊\n，你就查我？” 目前，打人考生已被拘留 。\nOn the 8th in Fuxin, a male student drove a BMW to\ntake the college entrance exam and was caught cheating.\nBecause the teacher conﬁscated his mobile phone, he\nkicked the teacher from the last row to the podium and\nshouted: ”Do you know who my dad is? How dare you\ncatch me!” Currently, this student has been detained.\nGold\n男生 高考作弊 追打 监考老师 ：你 知道我 爸 是\n谁？\nIn the college entrance exam, a male student caught\ncheating hit the teacher: Do you know who my dad is?\nRNN-context-SDLM\n高考生 作弊 被抓：你知道我爸是谁啊？\nIn the college entrance exam, a student was caught\ncheating: Do you know who my dad is?\nTop 5 sememe prediction\n考 考 考试 试 试“exam” 学 学 学习 习 习“study” 特 特 特定 定 定牌 牌 牌子 子 子“brand”\n预料“predict” 高 高 高等 等 等“higher”\nTable 6: An example of generated headlines on the LC-\nSTS test set.\n“predict”, all other Top 5 predicted sememes have\nhigh relevance to either the predicted word or the\ncontext. To be speciﬁc, the sememe 学习 “study”\nis annotated with 生 “student” in HowNet. 考\n试 “exam” indicates “college entrance exam”. 特\n定牌子 “brand” indicates “BMW”. And 高等\n“higher” indicates “higher education”, which is\nthe next step after this exam. We can conclude that\nwith sememe knowledge, our SDLM structure can\nextract critical information from both the given ar-\nticle and generated words explicitly and produce\nbetter summarization based on it.\n5 Related Work\nNeural Language Modeling. RNNs have\nachieved state-of-the-art performance in the\nlanguage modeling task since Mikolov et al.\n(2010) ﬁrst apply RNNs for language modeling.\nMuch work has been done to improve RNN-based\nlanguage modeling. For example, a variety of\nwork (Zaremba et al., 2014; Gal and Ghahramani,\n2016; Merity et al., 2017, 2018) introduces many\nregularization and optimization methods for\nRNNs. Based on the observation that the word\nappearing in the previous context is more likely to\nappear again, some work (Grave et al., 2017a,b)\nproposes to use cache for improvements. In this\npaper, we mainly focus on the output decoder,\nthe module between the context vector and the\npredicted probability distribution. Similar to our\nSDLM, Yang et al. (2018) propose a high-rank\nmodel which adopts a Mixture of Softmaxes\nstructure for the output decoder. However,\nour model is sememe-driven with each expert\ncorresponding to an interpretable sememe.\nHierarchical Decoder Since softmax computa-\ntion on large vocabulary is time-consuming, there-\nfore being a dominant part of the model’s com-\nplexity, various hierarchical softmax models have\nbeen proposed to address this issue. These mod-\nels can be categorized to class-based models and\ntree-based models according to their hierarchi-\ncal structure. Goodman (2001) ﬁrst proposes the\nclass-based model which divides the whole vocab-\nulary into different classes and uses a hierarchi-\ncal softmax decoder to model the probability as\nP(word) = P(word|class)P(class), which is sim-\nilar to our model. For the tree-based models, all\nwords are organized in a tree structure and the\nword probability is calculated as the probability of\nalways choosing the correct child along the path\nfrom the root node to the word node. While Morin\nand Bengio (2005) utilize knowledge from Word-\nNet to build the tree, Mnih and Hinton (2008)\nbuild it in a bootstrapping way and Mikolov et al.\n(2013) construct a Huffman Tree based on word\nfrequencies. Recently, Jiang et al. (2017) reform\nthe tree-based structure to make it more efﬁcient\non GPUs. The major differences between our\nmodel and theirs are the purpose and the moti-\nvation. Our model targets at improving the per-\nformance and interpretability of language model-\ning using external knowledge in HowNet. There-\nfore, we take its philosophy of the word-sense-\nsememe hierarchy to design our hierarchical de-\ncoder. Meanwhile, the class-based and tree-based\nmodels are mainly designed to speed up the soft-\nmax computation in the training process.\nSememe. Recently, there are a lot of works con-\ncentrating on utilizing sememe knowledge in tra-\nditional natural language processing tasks. For ex-\nample, Niu et al. (2017) use sememe knowledge\nto improve the quality of word embeddings and\ncope with the problem of word sense disambigua-\ntion. Xie et al. (2017) apply matrix factorization to\npredict sememes for words. Jin et al. (2018) im-\nprove their work by incorporating character-level\ninformation. Our work extends the previous works\nand tries to combine word-sense-sememe hierar-\nchy with the sequential model. To be speciﬁc,\nthis is the ﬁrst work to improve the performance\nand interpretability of Neural Language Modeling\nwith sememe knowledge.\nProduct of Experts. As Hinton (1999, 2002)\npropose, the ﬁnal probability can be calculated as\nthe product of probabilities given by experts.Gales\nand Airey (2006) apply PoE to the speech recog-\nnition where each expert is a Gaussian mixture\nmodel. Unlike their work, in our SDLM, each\nexpert is mapped to a sememe with better inter-\npretability. Moreover, as the ﬁnal distribution is\na categorical distribution, each expert is only re-\nsponsible for making predictions on a subset of\nthe categories (usually less than 10), so we call it\nSparse Product of Experts.\nHeadline Generation. Headline generation is\na kind of text summarization tasks. In recent\nyears, with the advances of RNNs, a lot of works\nhave been done in this domain. The encoder-\ndecoder models (Sutskever et al., 2014; Cho et al.,\n2014) have achieved great success in sequence-\nto-sequence learning. Rush et al. (2015) pro-\npose a local attention-based model for abstractive\nsentence summarization. Gu et al. (2016) intro-\nduce the copying mechanism which is close to the\nrote memorization of the human being. Ayana\net al. (2016) employ the minimum risk training\nstrategy to optimize model parameters. Different\nfrom these works, we focus on the decoder of the\nsequence-to-sequence model, and adopt SDLM to\nutilize sememe knowledge for sentence genera-\ntion.\n6 Conclusion and Further Work\nIn this paper, we propose an interpretable\nSememe-Driven Language Model with a hier-\narchical sememe-sense-word decoder. Besides\ninterpretability, our model also achieves state-\nof-the-art performance in the Chinese Language\nModeling task and shows improvement in the\nHeadline Generation task. These results indicate\nthat SDLM can successfully take advantages of se-\nmeme knowledge.\nAs for future work, we plan the following re-\nsearch directions: (1) In language modeling, given\na sequence of words, a sequence of correspond-\ning sememes can also be obtained. We will uti-\nlize the context sememe information for better se-\nmeme and word prediction. (2) Structural infor-\nmation about sememes in HowNet is ignored in\nour work. We will extend our model with the hi-\nerarchical sememe tree for more accurate relations\nbetween words and their sememes. (3) It is imag-\ninable that the performance of SDLM will be sig-\nniﬁcantly inﬂuenced by the annotation quality of\nsememe knowledge. We will also devote to fur-\nther enrich the sememe knowledge for new words\nand phrases, and investigate its effect on SDLM.\nAcknowledgement\nThis work is supported by the 973 Program (No.\n2014CB340501), the National Natural Science\nFoundation of China (NSFC No. 61572273)\nand the research fund of Tsinghua University-\nTencent Joint Laboratory for Internet Innova-\ntion Technology. This work is also funded by\nChina Association for Science and Technology\n(2016QNRC001). Hao Zhu and Jun Yan are sup-\nported by Tsinghua University Initiative Scientiﬁc\nResearch Program. We thank all members of Ts-\ninghua NLP lab. We also thank anonymous re-\nviewers for their careful reading and their insight-\nful comments.\nReferences\nShiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun.\n2016. Neural headline generation with minimum\nrisk training. arXiv preprint arXiv:1604.01904.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nICLR.\nMichele Banko, Vibhu O Mittal, and Michael J Wit-\nbrock. 2000. Headline generation based on statisti-\ncal translation. In Proceedings of ACL, pages 318–\n325. Association for Computational Linguistics.\nYoshua Bengio, Rejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nAdam Berger and John Lafferty. 1999. Information re-\ntrieval as statistical translation. In Proceedings of\nSIGIR, pages 222–229. ACM.\nThorsten Brants, Ashok C Popat, Peng Xu, Franz J\nOch, and Jeffrey Dean. 2007. Large language\nmodels in machine translation. In Proceedings of\nEMNLP.\nPeter F Brown, John Cocke, Stephen A Della Pietra,\nVincent J Della Pietra, Fredrick Jelinek, John D Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990. A\nstatistical approach to machine translation. Compu-\ntational linguistics, 16(2):79–85.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. In Proceedings\nof EMNLP.\nZhendong Dong and Qiang Dong. 2006. Hownet and\nthe computation of meaning (with Cd-rom) . World\nScientiﬁc.\nXianghua Fu, Guo Liu, Yanyan Guo, and Zhiqiang\nWang. 2013. Multi-aspect sentiment analysis for\nchinese online social reviews based on topic model-\ning and hownet lexicon. Knowledge-Based Systems,\n37:186–195.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Proceedings of NIPS.\nM. J. F. Gales and S. S. Airey. 2006. Product of gaus-\nsians for speech recognition. Computer Speech and\nLanguage, 20(1):22–40.\nJ Goodman. 2001. Classes for fast maximum entropy\ntraining. In Proceedings of ICASSP, pages 561–564\nvol.1.\nEdouard Grave, Moustapha Cisse, and Armand Joulin.\n2017a. Unbounded cache model for online language\nmodeling with open vocabulary. In Proceedings of\nNIPS.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving neural language models with a\ncontinuous cache. In Proceedings of ICLR.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nACL, pages 1631–1640.\nDjoerd Hiemstra. 1998. A linguistically motivated\nprobabilistic model of information retrieval. In Pro-\nceedings of TPDL, pages 569–584. Springer.\nG. E Hinton. 1999. Products of experts. In Artiﬁcial\nNeural Networks, 1999. ICANN 99. Ninth Interna-\ntional Conference on, pages 1–6 vol.1.\nG. E. Hinton. 2002. Training products of experts by\nminimizing contrastive divergence. MIT Press.\nBaotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lc-\nsts: A large scale chinese short text summarization\ndataset. In Proceedings of EMNLP.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Proceed-\nings of ICLR.\nNan Jiang, Wenge Rong, Min Gao, Yikang Shen,\nZhang Xiong, Nan Jiang, Wenge Rong, Min Gao,\nYikang Shen, and Zhang Xiong. 2017. Exploration\nof tree-based hierarchical softmax for recurrent lan-\nguage models. In Proceedings of IJCAI.\nHuiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie,\nMaosong Sun, Fen Lin, and Leyu Lin. 2018. In-\ncorporating chinese characters of words for lexical\nsememe prediction. In Proceedings of ACL, pages\n2439–2449. Association for Computational Linguis-\ntics.\nDan Jurafsky. 2000. Speech & language processing.\nchapter 4. Pearson Education India.\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE transactions on acoustics,\nspeech, and signal processing, 35(3):400–401.\nQun Liu. 2002. Word similarity computing based on\nhownet. Computational linguistics and Chinese lan-\nguage processing, 7(2):59–76.\nMitchell P. Marcus, Beatrice Santorini, and Ann\nMarcinkiewicz, Mary. 1993. Building a large anno-\ntated corpus of English: The Penn Treebank. Com-\nputational Linguistics, 19:313–330.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In Proceedings of ICLR.\nStephen Merity, Bryan Mccann, and Richard Socher.\n2017. Revisiting activation regularization for lan-\nguage rnns. arXiv preprint arXiv:1708.01009.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of INTERSPEECH.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of NIPS, pages 3111–3119.\nDavid RH Miller, Tim Leek, and Richard M Schwartz.\n1999. A hidden markov model information retrieval\nsystem. In Proceedings of SIGIR , pages 214–221.\nACM.\nAndriy Mnih and Geoffrey Hinton. 2008. A scalable\nhierarchical distributed language model. In Pro-\nceedings of NIPS, pages 1081–1088.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nProceedings of AISTATS.\nYilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong\nSun. 2017. Improved word representation learning\nwith sememes. In Proceedings of ACL, volume 1,\npages 2049–2058.\nJay M Ponte and W Bruce Croft. 1998. A language\nmodeling approach to information retrieval. In Pro-\nceedings of SIGIR, pages 275–281. ACM.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of EACL.\nFanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu,\nRuobing Xie, and Zhiyuan Liu. 2018. Cross-\nlingual lexical sememe prediction. In Proceedings\nof EMNLP.\nAlexander M Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of EMNLP.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Proceedings of NIPS, pages 3104–3112.\nRuobing Xie, Xingchi Yuan, Zhiyuan Liu, and\nMaosong Sun. 2017. Lexical sememe prediction via\nword embeddings and matrix factorization. In Pro-\nceedings of IJCAI, pages 4200–4206. AAAI Press.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. In Pro-\nceedings of ICLR.\nWojciech Zaremba, Ilya. Sutskever, and Oriol. Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nA Details about Preprocessing of the\nPeople’s Daily Dataset\nIn this section, we describe the details about pre-\nprocessing of the People’s Daily dataset.\nFirstly, we treat sentence, which is segmented\nby particular punctuations, as the minimum unit\nand then shufﬂe the corpus. We split the corpus\ninto the training set, the validation set and the test\nset, which contain 734k, 10k, 19k words respec-\ntively. Similar to the preprocessing performed in\n(Mikolov et al., 2010), we replace the number with\n<N>, the speciﬁc date with<date>, the year with\n<year>, and the time with <time>. Different\nfrom the preprocessing of the English language\nmodeling dataset, we keep the punctuations and\ntherefore do not append <eos>at the end of each\nsentence. Those words that occur less than 5 times\nare replaced with <unk>.\nSince our model requires that every word should\nbe included in the dictionary of HowNet, we\nsegment each non-annotated word into annotated\nwords with the forward maximum matching algo-\nrithm.\nB Details about Preprocessing of the\nLCSTS Dataset\nIn this section, we describe the details about pre-\nprocessing of the LCSTS dataset.\nThe dataset consists of over 2 million article-\nheadline pairs collected from Sina Weibo, the\nmost popular social media network in China. It’s\ncomposed of three parts. Each pair from PART-\nII and PART-III is labeled with a score which in-\ndicates the relevance between the article and its\nheadline. As its author suggests, we take pairs\nfrom a subset of PART-II as the validation set and\na subset of PART-III as the test set. Only pairs with\nscore 3, 4 and 5, which means high relevance, are\ntaken into account. We take pairs from PART-I\nthat do not occur in the validation set as the train-\ning set.\nSimilar to what we do for preprocessing the\nPeople’s Daily dataset, the word segmentation is\ncarried out with jiebavi based on the dictionary of\nHowNet to alleviate the OOV problems.\nC Details about Experiments Setting\nIn this section we describe the strategy we adopt\nto choose hyper-parameters and the optimal hyper-\nvi https://pypi.python.org/pypi/jieba\nHyper-parameter Baseline\nLearning rate 30\nBatch size 15\nEmbedding size 400\nRNN hidden size [1150, 1150, 400]\nWord-level V-dropout 0.1\nEmbedding V-dropout 0.5\nHidden state V-dropout 0.2\nRecurrent weight dropout 0.5\nContext vector dropout 0.4\nTable 7: Hyper-parameters used for AWD-LSTM and\nits variants\nparameters used in the experiment.\nC.1 Language Modeling\nThe hyper-parameters are chosen according to\nthe performance on the validation set. For\nmedium (Tied) LSTM and its cHSM, tHSM\nvariants, we search the dropout rate from\n{0.45,0.5,0.55,0.6,0.65,0.7}. For large (Tied)\nLSTM and its cHSM, tHSM variants, we search\nthe dropout rate from {0.6,0.65,0.7,0.75,0.8}.\nFor AWD-LSTM and its variants, we follow most\nof the hyper-parameters described in (Merity et al.,\n2018) and only search the dropout rates (embed-\nding V-dropout from{0.35,0.4,0.45,0.5}, hidden\nstate V-dropout from {0.2,0.25,0.3}, word level\nV-dropout from {0.05,0.1,0.15}and context vec-\ntor dropout from {0.4,0.5}). For our SDLM and\nMoS, we ﬁx all other hyper-parameters and only\nsearch the dropout rates of the last two layers re-\nspectively from {0.35, 0.4, 0.45}and {0.25, 0.3}.\nThe initial learning rate for MoS on the top of\nAWD-LSTM is set to 20 to avoid diverging.\nFor (Tied) LSTM, we set the hidden unit and\nword embedding size to 650 (medium) / 1500\n(large), batch size to 20, bptt to 35, dropout rate\nto 0.6 (medium) / 0.7 (large) and initial learn-\ning rate to 20. The optimal dropout rates for\ncHSM and tHSM are 0.55 (cHSM, medium), 0.5\n(cHSM, medium, tied), 0.7 (cHSM, large), 0.65\n(cHSM, large, tied), 0.55 (tHSM, medium) and 0.7\n(tHSM, large). For AWD-LSTM and its variants,\nthe hyper-parameters for the baseline are summa-\nrized in Table 7.\nC.2 Headline Generation\nThe hyper-parameters are chosen according to\nthe performance on the validation set. For\nRNN-context, we search the dropout rate from\n{0.1,0.15,0.2,0.25,0.3,0.35}, the batch size\nfrom {32,64}and try SGD and Adam optimizers.\nFor RNN-context-SDLM, we search the dropout\nrate from {0.15,0.2,0.25}, the batch size from\n{32,64}and try SGD and Adam optimizers.\nFor RNN-context, we use Adam optimizer with\nstarting learning rate 0.001. The batch size is 32\nand the dropout rate is 0.15. For RNN-context-\nSDLM, we use Adam optimizer with starting\nlearning rate 0.001. The batch size is 64 and the\ndropout rate is 0.2.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8720251321792603
    },
    {
      "name": "Natural language processing",
      "score": 0.7256245017051697
    },
    {
      "name": "Headline",
      "score": 0.7199000716209412
    },
    {
      "name": "Language model",
      "score": 0.6932870149612427
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6825660467147827
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.554594874382019
    },
    {
      "name": "Interpretability",
      "score": 0.5445440411567688
    },
    {
      "name": "Word (group theory)",
      "score": 0.4591258764266968
    },
    {
      "name": "Programming language",
      "score": 0.2602992653846741
    },
    {
      "name": "Linguistics",
      "score": 0.19383850693702698
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}