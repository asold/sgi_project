{
  "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
  "url": "https://openalex.org/W3188427387",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2096640705",
      "name": "Yi-Fan Xu",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098547712",
      "name": "Zhijie Zhang",
      "affiliations": [
        "Tencent (China)",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2319165320",
      "name": "Mengdan Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2555578089",
      "name": "Kekai Sheng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1908799446",
      "name": "Ke Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099000317",
      "name": "Weiming Dong",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2104971803",
      "name": "Liqing Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2154166576",
      "name": "Changsheng Xu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2096138239",
      "name": "Xing Sun",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096640705",
      "name": "Yi-Fan Xu",
      "affiliations": [
        "Tencent (China)",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098547712",
      "name": "Zhijie Zhang",
      "affiliations": [
        "Tencent (China)",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2319165320",
      "name": "Mengdan Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2555578089",
      "name": "Kekai Sheng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1908799446",
      "name": "Ke Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099000317",
      "name": "Weiming Dong",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Tencent (China)",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2154166576",
      "name": "Changsheng Xu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2096138239",
      "name": "Xing Sun",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3112516115",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3122411985",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6793958814",
    "https://openalex.org/W6803105104",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W6799770928",
    "https://openalex.org/W3174402370",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W3202715235",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3180037928",
    "https://openalex.org/W4309386164",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3209859545",
    "https://openalex.org/W4312340826",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3177183540",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3129247927",
    "https://openalex.org/W3169769781",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3115860342",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W4214713996",
    "https://openalex.org/W3176948526",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3170630188",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3169793979"
  ],
  "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4% top-1 accuracy on ImageNet-1K, outperforming current token pruning methods on both accuracy and efficiency.",
  "full_text": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer\nYifan Xu1,3,4*, Zhijie Zhang2,3∗, Mengdan Zhang3, Kekai Sheng3, Ke Li3, Weiming Dong1,4†,\nLiqing Zhang2, Changsheng Xu1,4, Xing Sun3†\n1NLPR, Institute of Automation, Chinese Academy of Sciences 2Shanghai Jiao Tong University 3Tencent Youtu Lab\n4School of Artificial Intelligence, University of Chinese Academy of Sciences\n{xuyifan2019, weiming.dong, changsheng.xu}@ia.ac.cn, zzj506506@sjtu.edu.cn, zhang-lq@cs.sjtu.edu.cn,\n{davinazhang, saulsheng, tristanli}@tencent.com, winfred.sun@gmail.com\nAbstract\nVision transformers (ViTs) have attracted considerable re-\nsearch attention recently, but the huge computational cost is\nstill a severe issue. A mainstream paradigm for computation\nreduction aims to reduce the number of tokens given that\nthe computation complexity of ViT is quadratic with respect\nto the input sequence length. Existing designs include struc-\ntured spatial compression that uses a progressive shrinking\npyramid to reduce the computations of large feature maps,\nand unstructured token pruning that dynamically drops redun-\ndant tokens. However, limitations of existing token pruning\nlie in the following aspects: 1) the incomplete spatial struc-\nture caused by pruning is incompatible with structured spatial\ncompression that is commonly used in modern deep-narrow\ntransformers; 2) it usually requires a time-consuming pre-\ntraining procedure. To address the limitations and expand the\napplicable scenario of token pruning, we present Evo-ViT, a\nself-motivated slow-fast token evolution approach for vision\ntransformers. Specifically, we conduct unstructured instance-\nwise token selection by taking advantage of the simple and\neffective global class attention that is native to vision trans-\nformers. Then, we propose to update the selected informative\ntokens and uninformative tokens with different computation\npaths, namely, slow-fast updating. Since slow-fast updating\nmechanism maintains the spatial structure and information\nflow, Evo-ViT can accelerate vanilla transformers of both flat\nand deep-narrow structures from the very beginning of the\ntraining process. Experimental results demonstrated that our\nmethod significantly reduces the computational cost of vision\ntransformers while maintaining comparable performance on\nimage classification. For example, our method accelerates\nDeiT-S by over 60% throughput while only sacrificing 0.4%\ntop-1 accuracy on ImageNet-1K, outperforming current token\npruning methods on both accuracy and efficiency.\nIntroduction\nVision transformers (ViTs) have shown strong power on var-\nious computer vision tasks (Xu et al. 2022). The reason of\nintroducing the transformer into computer vision lies in its\nunique properties that convolution neural networks (CNNs)\nlack, especially the property of modeling long-range depen-\ndencies. However, dense modeling of long-range dependen-\ncies among image tokens brings computation inefficiency,\n*Equal contribution; †Co-corresponding authors.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nStructured \nCompression\nToken \nSelection\nUnstructured\nPruning\nLocal \nSpatial Prior\nToken \nSelection\nSlow-Fast \nUpdating\nCompact \nTransformerTraining\nFine-tuning\nStructure \nPreservation\nCompact \nTransformer\nCompact \nTransformer\nPre-\ntraining\nFigure 1: An illustration of technique pipelines for compu-\ntation reduction via tokens. The dash lines denote iterative\ntraining. The first branch: the pipeline of unstructured token\npruning (Rao et al. 2021; Tang et al. 2021) based on pre-\ntrained models. The second branch: the pipeline of struc-\ntured compression (Graham et al. 2021). The third branch:\nour proposed pipeline that performs unstructured updating\nwhile suitable for structured compressed models.\nbecause images contain large regions of low-level texture\nand uninformative background.\nExisting methods follow two processes to address the\ninefficiency problem of modeling long-range dependencies\namong tokens in ViT as shown in the above two branches of\nFig. 1. The first process, as shown in the second branch, is to\nperform structured compression based on local spatial prior,\nsuch as local linear projection (Wang et al. 2021a), convolu-\ntional projection (Heo et al. 2021), and shift windows (Liu\net al. 2021). Most modern transformers with deep-narrow\nstructures are typically within this process. However, the\nstructured compressed models treat the informative object\ntokens and uninformative background tokens with the same\npriority. Thus, token pruning, the second process, proposes\nto identify and drop the uninformative tokens in an unstruc-\ntured way. (Tang et al. 2021) improves the efficiency of a\npre-trained transformer network by developing a top-down\nlayer-by-layer token slimming approach that can identify\nand remove redundant tokens based on the reconstruction\nerror of the pre-trained network. The trained pruning mask\nis fixed for all instances. (Rao et al. 2021) proposes to accel-\nerate a pre-trained transformer network by removing redun-\ndant tokens hierarchically, and explores an data-dependent\ndown-sampling strategy via self-distillation. Despite of the\nsignificant acceleration, these unstructured token pruning\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2964\nOriginal Interpretability Class attention \n(layer 5)\nClass attention \n(layer 10)\nFigure 2: Visualization of class attention in DeiT-T. The in-\nterpretability comes from (Chefer, Gur, and Wolf 2021).\nmethods are restricted in two aspects due to their incomplete\nspatial structure and information flow, namely, the inappli-\ncability on structured compressed transformers and inability\nto train from scratch.\nIn this paper, as shown in the third branch of Fig. 1, we\npropose to handle the inefficiency problem in a dynamic\ndata-dependent way from the very beginning of the train-\ning process while suitable for structured compression meth-\nods. We denote uninformative tokens that contribute little\nto the final prediction but bring computational cost when\nbridging redundant long-range dependencies as placeholder\ntokens. Different from structured compression that reduces\nlocal spatial redundancy in (Wang et al. 2021a; Graham\net al. 2021), we propose to distinguish the informative to-\nkens from the placeholder tokens for each instance in an un-\nstructured and dynamic way, and update the two types of to-\nkens with different computation paths. Instead of searching\nfor redundancy and pruning in a pre-trained network such\nas (Tang et al. 2021; Rao et al. 2021), by preserving place-\nholder tokens, the complete spatial structure and informa-\ntion flow can be maintained. In this way, our method can be\na generic plug-in for most ViTs of both flat and deep-narrow\nstructures from the very beginning of training.\nConcretely, Evo-ViT1, a self-motivated slow-fast token\nevolution approach for dynamic ViTs is proposed in this\nwork. “Self-motivated” means that transformers can natu-\nrally distinguish informative tokens from placeholder tokens\nfor each instance, since they have insights into global de-\npendencies among image tokens. Without loss of general-\nity, we take DeiT (Touvron et al. 2021) in Fig. 2 as exam-\nple. We find that the class token of DeiT-T estimates im-\nportance of each token for dependency modeling and final\nclassification. Especially in deeper layers (e.g., layer 10),\nthe class token usually augments informative tokens with\nhigher attention scores and has a sparse attention response,\nwhich is quite consistent to the visualization result pro-\nvided by (Chefer, Gur, and Wolf 2021) for transformer in-\nterpretability. In shallow layers (e.g., layer 5), the attention\nof the class token is relatively scattered but mainly focused\non informative regions. Thus, taking advantage of class to-\nkens, informative tokens and placeholder tokens are deter-\nmined. The preserved placeholder tokens ensure complete\n1Code is available at https://github.com/YifanXu74/Evo-ViT.\ninformation flow in shallow layers of a transformer for mod-\neling accuracy. After the two kinds of tokens are determined,\nthey are updated in a slow-fast approach. Specifically, the\nplaceholder tokens are summarized to a representative token\nthat is evolved via the full transformer encoder simultane-\nously with the informative tokens in a slow and elaborate\nway. Then, the evolved representative token is exploited to\nfast update the placeholder tokens for more representative\nfeatures.\nWe evaluate the effectiveness of the proposed Evo-ViT\nmethod on two kinds of baseline models, namely, transform-\ners of flat structures such as DeiT (Touvron et al. 2021) and\ntransformers of deep-narrow structures such as LeViT (Gra-\nham et al. 2021) on ImageNet (Deng et al. 2009) dataset. Our\nself-motivated slow-fast token evolution method allows the\nDeiT model to improve inference throughput by 40%-60%\nand further accelerates the state-of-the-art efficient trans-\nformer LeViT while maintaining comparable performance.\nRelated Work\nVision Transformer. Recently, a series of transformer\nmodels (Han et al. 2020; Xu et al. 2022) are proposed to\nsolve various computer vision tasks. The transformer has\nachieved promising success in image classification (Doso-\nvitskiy et al. 2021; Touvron et al. 2021; D’Ascoli et al.\n2021), object detection (Carion et al. 2020; Liu et al. 2021;\nZhu et al. 2020) and instance segmentation (Duke et al.\n2021; Zheng et al. 2021) due to its significant capability\nof modeling long-range dependencies. Vision Transformer\n(ViT) (Dosovitskiy et al. 2021) is among the pioneering\nworks that achieve state-of-the-art performance with large-\nscale pre-training. DeiT (Touvron et al. 2021) manages to\ntackle the data-inefficiency problem in ViT by simply adjust-\ning training strategies and adding an additional token along\nwith the class token for knowledge distillation. To achieve\nbetter accuracy-speed trade-offs for general dense predic-\ntion, recent works (Yuan et al. 2021; Graham et al. 2021;\nWang et al. 2021a) design transformers of deep-narrow\nstructures by adopting sub-sampling operation (e.g., strided\ndown-sampling, local average pooling, convolutional sam-\npling) to reduce the number of tokens in intermediate layers.\nRedundancy Reduction. Transformers take high compu-\ntational cost because the multi-head self-attention (MSA)\nrequires quadratic time complexity and the feed forward\nnetwork (FFN) increases the dimension of latent features.\nThe existing acceleration methods for transformers can be\nmainly categorized into sparse attention mechanism, knowl-\nedge distillation (Sanh et al. 2019), and pruning. The sparse\nattention mechanism includes, for example, low rank fac-\ntorization (Xu et al. 2021; Wang et al. 2020), fixed local\npatterns (Liu et al. 2021), and learnable patterns (Tay et al.\n2020; Beltagy, Peters, and Cohan 2020). (Yue et al. 2021)\nhandles the inefficiency problem by sparse input patch sam-\npling. (He et al. 2020) proposes to add an evolved global\nattention to the attention matrix in each layer for a better\nresidual mechanism. Motivated by this work, we propose the\nevolved global class attention to guide the token selection\nin each layer. The closest paradigm to this work is token\n2965\npruning. (Tang et al. 2021) presents a top-down layer-by-\nlayer patch slimming algorithm to reduce the computational\ncost in pre-trained vision transformers. The patch slimming\nscheme is conducted under a careful control of the feature\nreconstruction error, so that the pruned transformer network\ncan maintain the original performance with lower computa-\ntional cost. (Rao et al. 2021) devises a lightweight prediction\nmodule to estimate the importance score of each token given\nthe current features of a pre-trained transformer. The module\nis plugged into different layers to prune placeholder tokens\nin a unstructured way and is supervised by a distillation loss\nbased on the predictions of the original pre-trained trans-\nformer. Different from these pruning works, we proposed\nto preserve the placeholder tokens, and update the informa-\ntive tokens and placeholder tokens with different computa-\ntion paths; thus our method can achieve better performance\nand be suitable for various transformers due to the complete\nspatial structure. In addition, the complete information flow\nallows us to accelerate transformers with scratch training.\nPreliminaries\nViT (Dosovitskiy et al. 2021) proposes a simple tokeniza-\ntion strategy that handles images by reshaping them into flat-\ntened sequential patches and linearly projecting each patch\ninto latent embedding. An extra class token (CLS) is added\nto the sequence and serves as the global image representa-\ntion. Moreover, since self-attention in the transformer en-\ncoder is position-agnostic and vision applications highly re-\nquire position information, ViT adds position embedding\ninto each token, including the CLS token. Afterwards, all\ntokens are passed through stacked transformer encoders and\nthe CLS token is used for final classification.\nThe transformer is composed of a series of stacked en-\ncoders where each encoder consists of two modules, namely,\na multi-head self-attention (MSA) module and a feed for-\nward network (FFN) module. The FFN module contains two\nlinear transformations with an activation function. The resid-\nual connections are employed around both MSA and FFN\nmodules, followed by layer normalization (LN). Given the\ninput x0 of ViT, the processing of the k-th encoder can be\nmathematically expressed as\nx0 = ⌈xcls|xpatch⌉+ xpos,\nyk = xk−1 + MSA(LN (xk−1)),\nxk = yk + F F N(LN(yk)),\n(1)\nwhere xcls ∈R1×C and xpatch ∈RN×C are CLS and patch\ntokens respectively and xpos ∈R(1+N)×C denotes the po-\nsition embedding. N and C are the number of patch tokens\nand the dimension of the embedding.\nSpecifically, a self-attention (SA) module projects the in-\nput sequences into query, key, value vectors (i.e.,Q, K, V∈\nR(1+N)×C) using three learnable linear mapping WQ, WK\nand WV . Then, a weighted sum over all values in the se-\nquence is computed through:\nSA(Q, K, V) =Softmax (Q ·KT\n√\nC\n)V. (2)\n+\n+\n+\n+\n0.05\n0.2\n0.06\n0.05\n0.2\n0.04\n0.1\n0.3\nToken \nSelection Fast Updating\nSlow Updating\nMSA\n+\nCLS token placeholder token selected informative token\nrepresentative token global class attention\nGlobal CLS attention\nGlobal Class Attention Evolution\nGlobal Class Attention Evolution\nVanilla\nTransformer\nBlock CLS attention\nFFN\nVanilla Transformer Block\nFigure 3: The overall diagram of the proposed method.\nMSA is an extension of SA. It splits queries, keys, and val-\nues for h times and performs the attention function in paral-\nlel, then linearly projects their concatenated outputs.\nIt is worth noting that one very different design of ViT\nfrom CNNs is the CLS token. The CLS token interacts with\npatch tokens at each encoder and summarizes all the patch\ntokens for the final representation. We denote the similar-\nity scores between the CLS token and patch tokens as class\nattention Acls, formulated as:\nAcls = Softmax (qcls ·KT\n√\nC\n), (3)\nwhere qcls is the query vector of the CLS token.\nComputational complexity.In ViT, the computational cost\nof the MSA and FFN modules are O(4NC 2 + 2N2C) and\nO(8NC 2), respectively. For pruning methods (Rao et al.\n2021; Tang et al. 2021), by pruning η% tokens, at least η%\nFLOPS in the FFN and MSA modules can be reduced. Our\nmethod can achieve the same efficiency while suitable for\nscratch training and versatile downstream applications.\nMethodology\nOverview\nIn this paper, we aim to handle the inefficiency modeling\nissue in each input instance from the very beginning of\nthe training process of a versatile transformer. As shown in\nFig 3, the pipeline of Evo-ViT mainly contains two parts: the\nstructure preserving token selection module and the slow-\nfast token updating module. In the structure preserving to-\nken selection module, the informative tokens and the place-\nholder tokens are determined by the evolved global class\nattention, so that they can be updated in different manners\nin the following slow-fast token updating module. Specif-\nically, the placeholder tokens are summarized and updated\nby a representative token. The long-term dependencies and\nfeature richness of the representative token and the informa-\ntive tokens are evolved via the MSA and FFN modules.\nWe first elaborate on the proposed structure preserving\ntoken selection module. Then, we introduce how to update\nthe informative tokens and the placeholder tokens in a slow-\nfast approach. Finally, the training details, such as the loss\nand other training strategies, are introduced.\n2966\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1 2 3 4 5 6 7 8 9 10 11 12\nCKA Similarity\nLayer index of DeiT-S\nInformative tokens\nPlaceholder tokens\nAll tokens\n(a) Inter-layer\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n1 2 3 4 5 6 7 8 9 10 11 12\nVariance Value \nCorrelation Coefficient\nLayer index of DeiT-S\nMean\nVariance (b) Intra-layer\nFigure 4: Two folds that illustrate the difficulty of pruning\nthe shallow layers. (a) The CKA similarity between the final\nCLS token and token features in each layer. (b) The Pearson\ncorrelation coefficient of the token features in each layer.\nStructure Preserving Token Selection\nIn this work, we propose to preserve all the tokens and dy-\nnamically distinguish informative tokens and placeholder to-\nkens for complete information flow. The reason is that it\nis not trivial to prune tokens in shallow and middle layers\nof a vision transformer, especially in the beginning of the\ntraining process. We explain this problem in both inter-layer\nand intra-layer ways. First, shallow and middle layers usu-\nally present fast growing capability of feature representation.\nPruning tokens brings severe information loss. Following\nRefiner (Zhou et al. 2021), we use centered kernel alignment\n(CKA) similarity (Kornblith et al. 2019) to measure similar-\nity of the intermediate token features in each layer and the fi-\nnal CLS token, assuming that the final CLS token is strongly\ncorrelated with classification. As shown in Fig. 4(a), the to-\nken features of DeiT-T keep evolving fast when the model\ngoes deeper and the final CLS token feature is quite differ-\nent from token features in shallow layers. It indicates that\nthe representations in shallow or middle layers are insuffi-\nciently encoded, which makes token pruning quite difficult.\nSecond, tokens have low correlation with each other in the\nshallow layers. We evaluate the Pearson correlation coeffi-\ncient (PCC) among different patch token queries with re-\nspect to the network depth in the DeiT-S model to show re-\ndundancy. As shown in Fig. 4(b), the lower correlation with\nlarger variance in the shallow layers also proves the diffi-\nculty to distinguish redundancy in shallow features.\nThe attention weight is the easiest and most popular ap-\nproach (Abnar and Zuidema 2020; Wang et al. 2021b) to\ninterpret a model’s decisions and to gain insights about the\npropagation of information among tokens. The class atten-\ntion weight described in Eqn. 3 reflects the information col-\nlection and broadcast processes of the CLS token. We find\nthat our proposed evolved global class attention is able to\nbe a simple measure to help dynamically distinguish in-\nformative tokens and placeholder tokens in a vision trans-\nformer. In Fig. 4(a), the distinguished informative tokens\nhave high CKA correlation with the final CLS token, while\nthe placeholder tokens have low CKA correlation. As shown\nin Fig. 2, the global class attention is able to focus on the\nobject tokens, which is consistent to the visualization results\nof (Chefer, Gur, and Wolf 2021). In the following part of this\nsection, detailed introduction of our structure preserving to-\nken selection method is provided.\nAs discussed in Preliminaries Section, the class attention\nAcls is calculated by Eqn. 3. We select k tokens whose\nscores in the class attention are among the topk as the infor-\nmative tokens. The remaining N −k tokens are recognized\nas placeholder tokens that contain less information. Differ-\nent from token pruning, the placeholder tokens are kept and\nfast-updated rather than dropped.\nFor better capability of capturing the underlying informa-\ntion among tokens in different layers, we propose a global\nclass attention that augments the class attention by evolving\nit across layers as shown in Fig. 3. Specifically, a residual\nconnection between class attention of different layers is de-\nsigned to facilitate the attention information flow with some\nregularization effects. Mathematically,\nAk\ncls,g = α ·Ak−1\ncls,g + (1−α) ·Ak\ncls, (4)\nwhere Ak\ncls,g is the global class attention in the k-th layer,\nand Ak\ncls is the class attention in the k-th layer. We use\nAk\ncls,g for the token selection in the (k+1)-th layer for stabil-\nity and efficiency. For each layer with token selection, only\nthe global class attention scores of the selected informative\ntokens are updated.\nSlow-Fast Token Updating\nOnce the informative tokens and the placeholder tokens are\ndetermined by the global class attention, we propose to up-\ndate tokens in a slow-fast way instead of harshly dropping\nplaceholder tokens as (Tang et al. 2021; Rao et al. 2021).\nAs shown in Fig. 3, informative tokens are carefully evolved\nvia MSA and FFN modules, while placeholder tokens are\ncoarsely summarized and updated via a representative token.\nWe introduce our slow-fast token updating strategy mathe-\nmatically as follows.\nFor N patch tokens xpatch, we first split them intok infor-\nmative tokens xinf ∈Rk×C and N −k placeholder tokens\nxph ∈R(N−k)×C by the above-mentioned token selection\nstrategy. Then, the placeholder tokens xph are aggregated\ninto a representative token xrep ∈R1×C , as follows:\nxrep = ϕagg(xph), (5)\nwhere ϕagg : R(N−k)×C → R1×C denotes an aggregat-\ning function, such as weighted sum or transposed linear pro-\njection (Tolstikhin et al. 2021). Here we use weighted sum\nbased on the corresponding global attention score in Eqn. 4.\nThen, both the informative tokens xinf and the represen-\ntative token xrep are fed into MSA and FFN modules, and\ntheir residuals are recorded as x(∗)\ninf and x(∗)\nrep for skip con-\nnections, which can be denoted by:\nx(1)\ninf , x(1)\nrep = MSA(x inf , xrep),\nxinf ←xinf + x(1)\ninf , xrep ←xrep + x(1)\nrep,\nx(2)\ninf , x(2)\nrep = F F N(xinf , xrep),\nxinf ←xinf + x(2)\ninf .\n(6)\nThus, the informative tokens xinf and the representative to-\nken xrep are updated in a slow and elaborate way.\n2967\nFinally, the placeholder tokens xph are updated in a fast\nway by the residuals of xrep:\nxph ←xph + ϕexp(x(1)\nrep) +ϕexp(x(2)\nrep), (7)\nwhere ϕexp : R1×C →R(N−k)×C denotes an expanding\nfunction, such as simple copy in our method.\nIt is worth noting that the placeholder tokens are fast up-\ndated by the residuals ofxrep rather than the output features.\nIn fact, the fast updating serves as a skip connection for the\nplaceholder tokens. By utilizing residuals, we can ensure the\noutput features of the slow updating and fast updating mod-\nules within the same order of magnitude.\nTraining Strategies\nLayer-to-stage training schedule.Our proposed token se-\nlection mechanism becomes increasingly stable and consis-\ntent as the training process. Fig. 5 shows that the token selec-\ntion results of a well-trained transformer turn to be consis-\ntent across different layers; thereby indicating that the trans-\nformer tends to augment informative tokens with computing\nresource as much as possible, namely the full transformer\nnetworks. Thus, we propose a layer-to-stage training strat-\negy for further efficiency. Specifically, we conduct the token\nselection and slow-fast token updating layer by layer at the\nfirst 200 training epochs. During the remaining 100 epochs,\nwe only conduct token selection at the beginning of each\nstage, and then slow-fast updating is normally performed in\neach layer. For transformers with flat structure such as DeiT,\nwe manually arrange four layers as one stage.\nAssisted CLS token loss.Although many state-of-the-art\nvision transformers (Wang et al. 2021a; Graham et al. 2021)\nremove the CLS token and use the final average pooled fea-\ntures for classification, it is not difficult to add a CLS token\nin their models for our token selection strategy. We empiri-\ncally find that the ability of distinguishing two types of to-\nkens of the CLS token as illustrated in Fig. 2 is kept in these\nmodels even without supervision on the CLS token. For bet-\nter stability, we calculate classification losses based on the\nCLS token together with the final average pooled features\nduring training. Mathematically,\nˆycls, ˆy = m(xcls, xpatch),\nL= ϕ(ˆycls, y) +ϕ(Avg(ˆy), y), (8)\nwhere y is the ground-truth ofxcls and xpatch; m denotes the\ntransformer model; ϕ is the classification metric function,\nusually realized by the cross-entropy loss. During inference,\nthe final average pooled features are used for classification\nand the CLS token is only used for token selection.\nExperiments\nSetup\nIn this section, we demonstrate the superiority of the pro-\nposed Evo-ViT approach through extensive experiments on\nthe ImageNet-1k (Deng et al. 2009) classification dataset.\nTo demonstrate the generalization of our method, we con-\nduct experiments on vision transformers of both flat and\ndeep-narrow structures, i.e., DeiT (Touvron et al. 2021) and\nMethod Top-1 Acc. Throughput\n(%) (img/s) (%)\nDeiT-T\nBaseline (Touvron et al. 2021) 72.2 2536 -\nPS-ViT (Tang et al. 2021) 72.0 3563 40.5\nDynamicViT (Rao et al. 2021) 71.2 3890 53.4\nSViTE (Chen et al. 2021) 70.1 2836 11.8\nEvo-ViT (ours) 72.0 4027 58.8\nDeiT-S\nBaseline (Touvron et al. 2021) 79.8 940 -\nPS-ViT (Tang et al. 2021) 79.4 1308 43.6\nDynamicViT (Rao et al. 2021) 79.3 1479 57.3\nSViTE (Chen et al. 2021) 79.2 1215 29.3\nIA-RED2 (Pan et al. 2021) 79.1 1360 44.7\nEvo-ViT (ours) 79.4 1510 60.6\nDeiT-B\nBaseline (Touvron et al. 2021) 81.8 299 -\nBaseline∗ (Touvron et al. 2021) 82.8 87 -\nPS-ViT (Tang et al. 2021) 81.5 445 48.8\nDynamicViT (Rao et al. 2021) 80.8 454 51.8\nSViTE (Chen et al. 2021) 82.2 421 40.8\nIA-RED2 (Pan et al. 2021) 80.9 453 42.9\nIA-RED2∗ (Pan et al. 2021) 81.9 129 51.5\nEvo-ViT (ours) 81.3 462 54.5\nEvo-ViT∗ (ours) 82.0 139 59.8\nTable 1: Comparison with existing token pruning methods\non DeiT. The image resolution is224×224 unless specified.\n∗ denotes that the image resolution is 384 ×384.\nLeViT (Graham et al. 2021). Following (Graham et al.\n2021), we train LeViT with distillation and without batch\nnormalization fusion. We apply the position embedding\nin (Wang et al. 2021a) to LeViT for better efficiency. For\noverall comparisons with the state-of-the-art methods (Rao\net al. 2021; Tang et al. 2021; Chen et al. 2021; Pan et al.\n2021), we conduct the token selection and slow-fast token\nupdating from the fifth layer of DeiT and the third layer (ex-\ncluding the convolution layers) of LeViT, respectively. The\nselection ratios of informative tokens in all selected layers\nof both DeiT and LeViT are set to 0.5. The global CLS at-\ntention trade-off α in Eqn. 4 are set to 0.5 for all layers. For\nfair comparisons, all the models are trained for 300 epochs.\nMain Results\nComparisons with existing pruning methods.In Table 1,\nwe compare our method with existing token pruning meth-\nods (Rao et al. 2021; Pan et al. 2021; Tang et al. 2021; Chen\net al. 2021). Since token pruning methods are unable to re-\ncover the 2D structure and are usually designed for trans-\nformers with flat structures, we comprehensively conduct\nthe comparisons based on DeiT (Touvron et al. 2021) on Im-\nageNet dataset. We report the top-1 accuracy and throughput\nfor performance evaluation. The throughput is measured on\na single NVIDIA V100 GPU with batch size fixed to 256,\nwhich is the same as the setting of DeiT. Results indicate\nthat our method outperforms previous token pruning meth-\nods on both accuracy and efficiency. Our method accelerates\n2968\nModel Param Throughput Top-1 Acc.\n(M) (img/s) (%)\nLeViT-128S 7.8 8755 74.5\nLeViT-128 9.2 6109 76.2\nLeViT-192 10.9 4705 78.4\nPVTv2-B1 14.0 1225 78.7\nCoaT-Lite Tiny 5.7 1083 76.6\nPiT-Ti 4.9 3030 73.0\nEvo-LeViT-128S 7.8 10135 73.0\nEvo-LeViT-128 9.2 8323 74.4\nEvo-LeViT-192 11.0 6148 76.8\nLeViT-256 18.9 3357 80.1\nLeViT-256∗ 19.0 906 81.8\nPVTv2-B2 25.4 687 82.0\nPiT-S 23.5 1266 80.9\nSwin-T 29.4 755 81.3\nCoaT-Lite Small 20.0 550 81.9\nEvo-LeViT-256 19.0 4277 78.8\nEvo-LeViT-256∗ 19.2 1285 81.1\nLeViT-384 39.1 1838 81.6\nLeViT-384∗ 39.2 523 82.8\nPVTv2-B3 45.2 457 83.2\nPiT-B 73.8 348 82.0\nEvo-LeViT-384 39.3 2412 80.7\nEvo-LeViT-384∗ 39.6 712 82.2\nTable 2: Comparison with state-of-the-art vision transform-\ners. The input image resolution is224×224 unless specified.\n∗ denotes that the image resolution is 384 ×384.\nthe inference throughput by over 60% with negligible accu-\nracy drop (-0.4%) on DeiT-S.\nComparisons with state-of-the-art ViTs.Owing to the pre-\nserved placeholder tokens, our method guarantees the spatial\nstructure that is indispensable for most existing modern ViT\nnetworks. Thus, we further apply our method to the state-\nof-the-art efficient transformer LeViT (Graham et al. 2021),\nwhich presents a deep-narrow architecture. As shown in Ta-\nble 2, our method can further accelerate the deep-narrow\ntransformer such as LeViT. We have observed larger accu-\nracy degradation of our method on LeViT than on DeiT.\nThe reason lies that the deeper layers of LeViT have few\ntokens and therefore have less redundancy due to the shrink-\ning pyramid structure. With dense input, such as the image\nresolution of 384×384, our method accelerates LeViT with\nless accuracy degradation and more acceleration ratio, which\nindicates the effectiveness of our method on dense input.\nAblation Analysis\nEffectiveness of each module.To evaluate the effectiveness\nof each sub-method, we add the following improvements\nstep by step in Tab. 3 on transformers of both flat and deep-\nnarrow structures, namely DeiT and LeViT: i) Naive selec-\ntion. Simply drop the placeholder tokens based on the orig-\ninal class attention in each layer; ii) Structure preservation.\nPreserve the placeholder tokens but not fast update them;iii)\nGlobal attention. Utilize the proposed global class attention\nStrategy\nDeiT-T LeViT 128S\nAcc. Throughput Acc. Throughput\n(%) (img/s) (%) (img/s)\nbaseline 72.2 2536 74.5 8755\n+ naive selection 70.8 3824 - -\n+ structure preservation 71.6 3802 72.1 9892\n+ global attention 72.0 3730 72.5 9452\n+ fast updating 72.0 3610 73.0 9360\n+ layer-to-stage 72.0 4027 73.0 10135\nTable 3: Method ablation on DeiT and LeViT.\nMethod Acc. (%) Throughput (img/s)\naverage pooling 69.5 3703\nmax pooling 69.8 3698\nconvolution 70.2 3688\nrandom selection 66.4 3760\nlast class attention 69.7 1694\nattention column mean 71.2 3596\nglobal class attention 72.0 3730\nTable 4: Different token selection strategies on DeiT-T. We\nconduct all sub-sampling methods at the seventh layer and\nconduct token selection strategies from the fifth layer.\ninstead of vanilla class attention for token selection; iv) Fast\nupdating. Augment the preserved placeholder tokens with\nfast updating; v) Layer-to-stage. Apply the proposed layer-\nto-stage training strategy to further accelerate inference.\nResults on DeiT indicate that our structure preserving\nstrategy further improves the selection performance due to\nits capacity of preserving complete information flow. The\nevolved global class attention enhances the consistency of\ntoken selection across layers and achieves better perfor-\nmance. The fast updating strategy has less effect on DeiT\nthan on LeViT. We claim that the performance of DeiT\nturns to be saturated based on structure preservation and\nglobal class attention, while LeViT still has some space for\nimprovement. LeViT exploits spatial pooling for token re-\nduction, which makes unstructured token reduction in each\nstage more difficult. By using the fast updating strategy, it is\npossible to collect some extra cues from placeholder tokens\nfor accurate and augmented feature representations. We also\nevaluate the layer-to-stage strategy. Results indicate that it\nfurther accelerates inference while maintaining accuracy.\nDifferent Token Selection Strategy. We compare our\nglobal-attention-based token selection strategy with several\ncommon token selection strategies and sub-sampling meth-\nods in Tab. 4 to evaluate the effectiveness of our method. All\ntoken selection strategies are conducted under our structure\npreserving strategy without layer-to-stage training sched-\nule. The token selection strategies include: randomly select-\ning the informative tokens (random selection); Utilizing the\nclass attention of the last layer for selection in all layers\nvia twice inference (last class attention); taking the column\nmean of the attention matrix as the score of each token as\nproposed in (Kim et al. 2021) (attention column mean).\nResults in Tab. 4 indicate that our evolved global class at-\n2969\nOriginal Layer 5 Layer 9 Layer 11 Epoch 10 Epoch 50 Epoch 100 Stage 2 Stage 3\nTesting phase Layer-to-stageTraining phase\nFigure 5: Token selection results on DeiT-T. The left, middle, and right three columns denote the selection results on a well-\ntrained Evo-ViT, the fifth layer at different training epochs, and Evo-ViT with the proposed layer-to-stage strategy, respectively.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n5 6 7 8 9 10 11 12\nKeeping ratio\nLayer index of DeiT-S \nTop1 Acc: 78.9 Top1 Acc: 79.1\nTop1 Acc: 79.4 Top1 Acc: 79.2\nFigure 6: Different architecture of the accelerated DeiT-S via\nour method. We start our token selection from the fifth layer.\ntention outperforms the other selection strategies and com-\nmon sub-sampling methods on both accuracy and efficiency.\nWe have observed obvious performance degradation with\nlast class attention, although the attention in deeper layers\nis more focused on objects in Fig. 2. A possible reason is\nthat the networks require some background information to\nassist classification, while restricting all layers to only focus\non objects during the entire training process leads to under-\nfitting on the background features.\nVisualization. We visualize the token selection in Fig. 5 to\ndemonstrate performance of our method during both train-\ning and testing stages. The visualized models in the left\nand middle three columns are trained without the layer-to-\nstage training strategy. The left three columns demonstrate\nresults on different layers of a well-trained DeiT-T model.\nResults show that our token selection method mainly fo-\ncuses on objects instead of backgrounds, thereby indicating\nthat our method can effectively discriminate the informative\ntokens from placeholder tokens. The selection results tend\nto be consistent across layers, which proves the feasibility\nof our layer-to-stage training strategy. Another interesting\nfinding is that some missed tokens in the shallow layers are\nretrieved in the deep layers owing to our structure preserv-\ning strategy. Take the baseball images as an example, tokens\nof the bat are gradually picked up as the layer goes deeper.\nThis phenomenon is more obvious under our layer-to-stage\ntraining strategy in the right three columns. We also inves-\ntigate how the token selection evolves during the training\nstage in the middle three columns. Results demonstrate that\nsome informative tokens, such as the fish tail, are determined\nas placeholder tokens at the early epochs. With more training\nepochs, our method gradually turns to be stable for discrim-\ninative token selection.\nConsistent keeping ratio.We set different keeping ratio of\ntokens in each layer to investigate the best acceleration ar-\nchitecture of Evo-ViT. The keeping ratio determines how\nmany tokens are kept as informative tokens. Previous token\npruning works (Rao et al. 2021; Tang et al. 2021) present\na gradual shrinking architecture, in which more tokens are\nrecognized as placeholder tokens in deeper layers. They are\nrestricted in this type of architecture due to direct pruning.\nOur method allows more flexible token selection owing to\nthe structure preserving slow-fast token evolution. As shown\nin Fig. 6, we maintain the sum of the number of placeholder\ntokens in all layers and adjust the keeping ratio in each layer.\nResults demonstrate that the best performance is reached\nwith a consistent keeping ratio across all layers. We explain\nthe reason as follows. In the above visualization, we find that\nthe token selection results tend to be consistent across lay-\ners, indicating that the transformer tends to augment infor-\nmative tokens with computing resource as much as possible.\nIn Fig. 6, at most 50% tokens are passed through the full\ntransformer network when the keeping ratios in all layers\nare set to 0.5, thereby augmenting the most number of infor-\nmative tokens with the best computing resource, namely, the\nfull transformer network.\nConclusions\nIn this work, we investigate the efficiency of vision trans-\nformers by developing a self-motivated slow-fast token evo-\nlution (Evo-ViT) method. We propose the structure preserv-\ning token selection and slow-fast updating strategies to solve\nthe limitation of token pruning on modern structured com-\npressed transformers and scratch training. Extensive exper-\niments on two popular ViT architectures, i.e., DeiT and\nLeViT, indicate that our Evo-ViT approach is able to ac-\ncelerate various transformers significantly while maintain-\ning comparable classification performance.\nAs for future work, an interesting and worthwhile direc-\ntion is to extend our method to dense prediction tasks, such\nas object detection and instance segmentation.\n2970\nAcknowledgements\nThis work was supported by National Key R&D Pro-\ngram of China under no. 2020AAA0106200, by Shang-\nhai Municipal Science and Technology Key Project under\nno. 20511100300, by National Natural Science Founda-\ntion of China under nos. 62076162, U20B2070, 61832016\nand 62036012, and by CASIA-Tencent Youtu joint research\nproject.\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying attention\nflow in transformers. arXiv preprint arXiv:2005.00928.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision. Springer.\nChefer, H.; Gur, S.; and Wolf, L. 2021. Transformer inter-\npretability beyond attention visualization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition.\nChen, T.; Cheng, Y .; Gan, Z.; Yuan, L.; Zhang, L.; and\nWang, Z. 2021. Chasing Sparsity in Vision Trans-\nformers: An End-to-End Exploration. arXiv preprint\narXiv:2106.04533.\nD’Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.;\nBiroli, G.; and Sagun, L. 2021. ConViT: Improving Vision\nTransformers with Soft Convolutional Inductive Biases. In\nProceedings of the 38th International Conference on Ma-\nchine Learning. PMLR.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nDuke, B.; Ahmed, A.; Wolf, C.; Aarabi, P.; and Taylor, G. W.\n2021. Sstvos: Sparse spatiotemporal transformers for video\nobject segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J ´egou, H.; and Douze, M. 2021. LeViT: a Vision\nTransformer in ConvNet’s Clothing for Faster Inference. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision.\nHan, K.; Wang, Y .; Chen, H.; Chen, X.; Guo, J.; Liu, Z.;\nTang, Y .; Xiao, A.; Xu, C.; Xu, Y .; et al. 2020. A survey on\nvisual transformer. arXiv preprint arXiv:2012.12556.\nHe, R.; Ravula, A.; Kanagal, B.; and Ainslie, J. 2020. Real-\nformer: Transformer likes residual attention. arXiv preprint\narXiv:2012.11747.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking Spatial Dimensions of Vision Transform-\ners. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision.\nKim, S.; Shen, S.; Thorsley, D.; Gholami, A.; Hassoun, J.;\nand Keutzer, K. 2021. Learned Token Pruning for Trans-\nformers. arXiv preprint arXiv:2107.00910.\nKornblith, S.; Norouzi, M.; Lee, H.; and Hinton, G. 2019.\nSimilarity of neural network representations revisited. In\nInternational Conference on Machine Learning. PMLR.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision.\nPan, B.; Jiang, Y .; Panda, R.; Wang, Z.; Feris, R.; and\nOliva, A. 2021. IA-RED 2: Interpretability-Aware Redun-\ndancy Reduction for Vision Transformers. arXiv preprint\narXiv:2106.12620.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh,\nC.-J. 2021. DynamicViT: Efficient Vision Transform-\ners with Dynamic Token Sparsification. arXiv preprint\narXiv:2106.02034.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108.\nTang, Y .; Han, K.; Wang, Y .; Xu, C.; Guo, J.; Xu, C.; and\nTao, D. 2021. Patch Slimming for Efficient Vision Trans-\nformers. arXiv preprint arXiv:2106.02852.\nTay, Y .; Bahri, D.; Metzler, D.; Juan, D.-C.; Zhao, Z.; and\nZheng, C. 2020. Synthesizer: Rethinking self-attention in\ntransformer models. arXiv preprint arXiv:2005.00743.\nTolstikhin, I.; Houlsby, N.; Kolesnikov, A.; Beyer, L.; Zhai,\nX.; Unterthiner, T.; Yung, J.; Keysers, D.; Uszkoreit, J.; Lu-\ncic, M.; et al. 2021. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In Proceedings\nof the 38th International Conference on Machine Learning.\nPMLR.\nWang, S.; Li, B. Z.; Khabsa, M.; Fang, H.; and Ma, H. 2020.\nLinformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021a. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. InProceedings of the IEEE/CVF International\nConference on Computer Vision.\nWang, Y .; Yang, Y .; Bai, J.; Zhang, M.; Bai, J.; Yu, J.; Zhang,\nC.; Huang, G.; and Tong, Y . 2021b. Evolving attention with\nresidual convolutions. arXiv preprint arXiv:2102.12895.\nXu, W.; Xu, Y .; Chang, T.; and Tu, Z. 2021. Co-Scale\nConv-Attentional Image Transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion.\n2971\nXu, Y .; Wei, H.; Lin, M.; Deng, Y .; Sheng, K.; Zhang, M.;\nTang, F.; Dong, W.; Huang, F.; and Xu, C. 2022. Transform-\ners in computational visual media: A survey.Computational\nVisual Media, 8(1).\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.;\nTay, F. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit:\nTraining vision transformers from scratch on imagenet. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision.\nYue, X.; Sun, S.; Kuang, Z.; Wei, M.; Torr, P. H.; Zhang,\nW.; and Lin, D. 2021. Vision transformer with progressive\nsampling. In Proceedings of the IEEE/CVF International\nConference on Computer Vision.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition.\nZhou, D.; Shi, Y .; Kang, B.; Yu, W.; Jiang, Z.; Li, Y .; Jin, X.;\nHou, Q.; and Feng, J. 2021. Refiner: Refining Self-attention\nfor Vision Transformers. arXiv preprint arXiv:2106.03714.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\n2972",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.788707971572876
    },
    {
      "name": "Computer science",
      "score": 0.7420755624771118
    },
    {
      "name": "Computation",
      "score": 0.6182239055633545
    },
    {
      "name": "Transformer",
      "score": 0.5784918665885925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4924796521663666
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3351251482963562
    },
    {
      "name": "Algorithm",
      "score": 0.3289223313331604
    },
    {
      "name": "Engineering",
      "score": 0.08050769567489624
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}