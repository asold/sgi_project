{
  "title": "Deeper Text Understanding for IR with Contextual Neural Language Modeling",
  "url": "https://openalex.org/W2945127593",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A4226407207",
      "name": "Dai, Zhuyun",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A3192465271",
      "name": "Callan, Jamie",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2783640434",
    "https://openalex.org/W2610935556",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W2766284073",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2648699835",
    "https://openalex.org/W3098851962",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W3212575067"
  ],
  "abstract": "Neural networks provide new possibilities to automatically learn complex\\nlanguage patterns and query-document relations. Neural IR models have achieved\\npromising results in learning query-document relevance patterns, but few\\nexplorations have been done on understanding the text content of a query or a\\ndocument. This paper studies leveraging a recently-proposed contextual neural\\nlanguage model, BERT, to provide deeper text understanding for IR. Experimental\\nresults demonstrate that the contextual text representations from BERT are more\\neffective than traditional word embeddings. Compared to bag-of-words retrieval\\nmodels, the contextual language model can better leverage language structures,\\nbringing large improvements on queries written in natural languages. Combining\\nthe text understanding ability with search knowledge leads to an enhanced\\npre-trained BERT model that can benefit related search tasks where training\\ndata are limited.\\n",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8572954535484314
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.760841429233551
    },
    {
      "name": "Language model",
      "score": 0.6866450905799866
    },
    {
      "name": "Natural language processing",
      "score": 0.6552383899688721
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6204136610031128
    },
    {
      "name": "Relevance (law)",
      "score": 0.5513986945152283
    },
    {
      "name": "Question answering",
      "score": 0.47627657651901245
    },
    {
      "name": "Artificial neural network",
      "score": 0.4451599419116974
    },
    {
      "name": "Natural language",
      "score": 0.4360368251800537
    },
    {
      "name": "Information retrieval",
      "score": 0.38395142555236816
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}