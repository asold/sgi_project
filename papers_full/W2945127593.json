{
    "title": "Deeper Text Understanding for IR with Contextual Neural Language Modeling",
    "url": "https://openalex.org/W2945127593",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A4226407207",
            "name": "Dai, Zhuyun",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A3192465271",
            "name": "Callan, Jamie",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2783640434",
        "https://openalex.org/W2610935556",
        "https://openalex.org/W2536015822",
        "https://openalex.org/W2766284073",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2648699835",
        "https://openalex.org/W3098851962",
        "https://openalex.org/W2909544278",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W3212575067"
    ],
    "abstract": "Neural networks provide new possibilities to automatically learn complex\\nlanguage patterns and query-document relations. Neural IR models have achieved\\npromising results in learning query-document relevance patterns, but few\\nexplorations have been done on understanding the text content of a query or a\\ndocument. This paper studies leveraging a recently-proposed contextual neural\\nlanguage model, BERT, to provide deeper text understanding for IR. Experimental\\nresults demonstrate that the contextual text representations from BERT are more\\neffective than traditional word embeddings. Compared to bag-of-words retrieval\\nmodels, the contextual language model can better leverage language structures,\\nbringing large improvements on queries written in natural languages. Combining\\nthe text understanding ability with search knowledge leads to an enhanced\\npre-trained BERT model that can benefit related search tasks where training\\ndata are limited.\\n",
    "full_text": null
}