{
    "title": "L-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks",
    "url": "https://openalex.org/W4391418171",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2097328449",
            "name": "Ping Guo",
            "affiliations": [
                "City University of Hong Kong",
                "City University of Hong Kong, Shenzhen Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2035932820",
            "name": "Fei Liu",
            "affiliations": [
                "City University of Hong Kong",
                "City University of Hong Kong, Shenzhen Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2106187023",
            "name": "Xi LIN",
            "affiliations": [
                "City University of Hong Kong, Shenzhen Research Institute",
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2124408761",
            "name": "Qingchuan Zhao",
            "affiliations": [
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2108368290",
            "name": "Qing-Fu Zhang",
            "affiliations": [
                "City University of Hong Kong, Shenzhen Research Institute",
                "City University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3135970545",
        "https://openalex.org/W4361021212",
        "https://openalex.org/W4324302739",
        "https://openalex.org/W4234222550",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W2603766943",
        "https://openalex.org/W6715220353"
    ],
    "abstract": "In the rapidly evolving field of machine learning, adversarial attacks\\npresent a significant challenge to model robustness and security.\\nDecision-based attacks, which only require feedback on the decision of a model\\nrather than detailed probabilities or scores, are particularly insidious and\\ndifficult to defend against. This work introduces L-AutoDA (Large Language\\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\\nleveraging the generative capabilities of Large Language Models (LLMs) to\\nautomate the design of these attacks. By iteratively interacting with LLMs in\\nan evolutionary framework, L-AutoDA automatically designs competitive attack\\nalgorithms efficiently without much human effort. We demonstrate the efficacy\\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\\nmethods in both success rate and computational efficiency. Our findings\\nunderscore the potential of language models as tools for adversarial attack\\ngeneration and highlight new avenues for the development of robust AI systems.\\n",
    "full_text": "L-AutoDA: Large Language Models for Automatically Evolving\nDecision-based Adversarial Attacks\nPing Guo\nCity University of Hong Kong\nHong Kong, Hong Kong SAR\nCityU Shenzhen Research Institute\nShenzhen, China\npingguo5-c@my.cityu.edu.hk\nFei Liu\nCity University of Hong Kong\nHong Kong, Hong Kong SAR\nCityU Shenzhen Research Institute\nShenzhen, China\nfliu36-c@my.cityu.edu.hk\nXi Lin\nCity University of Hong Kong\nHong Kong, Hong Kong SAR\nCityU Shenzhen Research Institute\nShenzhen, China\nxi.lin@my.cityu.edu.hk\nQingchuan Zhao\nCity University of Hong Kong\nHong Kong, Hong Kong SAR\nqizhao@cityu.edu.hk\nQingfu Zhang\nCity University of Hong Kong\nHong Kong, Hong Kong SAR\nCityU Shenzhen Research Institute\nShenzhen, China\nqingfu.zhang@cityu.edu.hk\nABSTRACT\nIn the rapidly evolving field of machine learning, adversarial at-\ntacks pose a significant threat to the robustness and security of\nmodels. Amongst these, decision-based attacks are particularly in-\nsidious due to their nature of requiring only the modelâ€™s decision\noutput, which makes them notably challenging to counteract. This\npaper presents L-AutoDA (Large Language Model-based Automated\nDecision-based Adversarial Attacks), an innovative methodology\nthat harnesses the generative capabilities of large language models\n(LLMs) to streamline the creation of such attacks. L-AutoDA em-\nploys an evolutionary strategy, where iterative interactions with\nLLMs lead to the autonomous generation of potent attack algo-\nrithms, thereby reducing human intervention. The performance\nof L-AutoDA was evaluated on the CIFAR-10 dataset, where it\ndemonstrated substantial superiority over existing baseline meth-\nods in terms of success rate and computational efficiency. Ultimately,\nour results highlight the formidable utility of language models in\ncrafting adversarial attacks and reveal promising directions for\nconstructing more resilient AI systems.\nCCS CONCEPTS\nâ€¢ Theory of computation â†’Design and analysis of algo-\nrithms; â€¢ Computing methodologies â†’Artificial intelligence.\nKEYWORDS\nLarge Language Models, Adversarial Attacks, Automated Algorithm\nDesign, Evolutionary Algorithms\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nGECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0495-6/24/07. . . $15.00\nhttps://doi.org/10.1145/3638530.3664121\nACM Reference Format:\nPing Guo, Fei Liu, Xi Lin, Qingchuan Zhao, and Qingfu Zhang. 2024. L-\nAutoDA: Large Language Models for Automatically Evolving Decision-based\nAdversarial Attacks. In Genetic and Evolutionary Computation Conference\n(GECCO â€™24 Companion), July 14â€“18, 2024, Melbourne, VIC, Australia. ACM,\nNew York, NY, USA, 9 pages. https://doi.org/10.1145/3638530.3664121\n1 INTRODUCTION\nDeep neural network (DNN) models, despite their remarkable per-\nformance across a broad spectrum of domains, remain susceptible\nto adversarial attacks [16, 37], which involve imperceptibly altering\nthe input data to induce incorrect model responses. Such vulner-\nabilities threaten the integrity and reliability of machine learning\napplications, particularly in safety-critical scenarios such as au-\ntonomous vehicle driving [3] and medical diagnostics [11]. Attack-\ners can engineer white-box attacks using comprehensive knowledge\nof the DNN, or resort to black-box attacks when the modelâ€™s details\nare concealed [33]. Of particular concern are decision-based attacks\nthat necessitate only the modelâ€™s output label information [ 21],\nposing a significant risk to real-world machine applications, such\nas commercial platforms that generally provide only the decision\nto users, thereby substantially endangering security and presenting\nchallenges in implementing effective defenses [13, 19].\nThe escalating arms race in trustworthy artificial intelligence\n(AI) domain, characterized by the rapid advancement of attack\nmethodologies and the concurrent evolution of defensive strate-\ngies [4, 30, 39], highlights the imperative for automating the genera-\ntion and testing of adversarial attack algorithms [27]. This necessity\nis particularly acute in the realm of decision-based attacks, which\ndemand extensive manual labor to develop and refine strategic\nmethodologies. Current approaches to decision-based attacks are\nheavily reliant on handcrafted heuristics [ 2, 5, 7, 8, 12], posing\nsignificant impediments to enhancing their efficiency and efficacy.\nThe automation of adversarial attack algorithm design, under-\npinned by automatic program synthesis [18], entails the generation\nof programs within complex constraints. This area of research,\nknown within the machine learning community as AutoML [14],\narXiv:2401.15335v2  [cs.CR]  22 May 2024\nGECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, and Qingfu Zhang\nTable 1: Comparison of strengths and weaknesses of different\nalgorithm design approaches.\nMethod Time Expertise Refinement\nDomain Extra\nManual 1-2 Months âœ“ âœ— âœ“\nAutomatic Synthesis 1-2 Months âœ“ âœ“ âœ—\nL-AutoDA (Ours) 1-2 Days âœ— âœ— âœ“\nseeks to devise strategies with minimal manual intervention. Au-\ntoDA [15] represents the cutting-edge effort in this domain, adopt-\ning a random search across a thoughtfully assembled set of alge-\nbraic operations to engineer adversarial attack algorithms. However,\ntheir method is inherently labor-intensive, particularly in develop-\ning domain-specific languages and establishing automated testing\ninfrastructures. Despite the intense investment of efforts and re-\nsources, the independent progression of novel algorithms without\nhuman expertise presents a substantial challenge [15, 34].\nRecent literature has highlighted the potential of large language\nmodels (LLMs) for autonomous algorithm design, as demonstrated\nby initiatives such as Googleâ€™s FunSearch [35] and the evolutionary\nalgorithm communityâ€™s AEL [29]. These efforts have corroborated\nthe feasibility of LLMs in the independent generation of algorithms.\nThe advantages of LLMs are manifold: they can decode natural\nlanguage inputs, obviating the need for domain-specific language\nencodings and thereby enabling the creation of innovative algo-\nrithms beyond the limitations of traditional encoding methods.\nFurthermore, LLMs can be smoothly integrated into prevailing test-\ning frameworks, requiring only slight modifications to existing test\nscripts since they can output program code directly, circumventing\nthe need for decoding intermediate encodings. A comparative anal-\nysis of this approach with conventional manual algorithm design\nand automatic program synthesis is delineated in Table 1.\nIn this research, we exploit the AEL framework for develop-\ning decision-based adversarial attacks, introducing L-AutoDA, a\ncutting-edge automated framework tailored for crafting such at-\ntacks. To the best of our knowledge, this work constitutes the first\nattempt to utilize LLMs in the development and autonomous evalu-\nation of adversarial attack algorithms. By integrating meticulously\ndevised prompts and a population-based methodology within the\nAEL framework, as detailed in [29], we succeed in deriving inno-\nvative strategies. Remarkably, the genesis of all initial algorithms\noriginated exclusively from LLMs and did not depend on established\nhuman-centric design principles. This signifies a groundbreaking\nshift away from conventional approaches, featuring a new paradigm\nin the autonomous generation of adversarial attack algorithms.\nOur contributions are as follows:\nâ€¢We introduce L-AutoDA, an innovative automated frame-\nwork that incorporates LLMs to develop decision-based ad-\nversarial attack algorithms, marking a pioneering attempt to\nemploy LLMs in this domain and setting the stage for new\nparadigms in the field.\nâ€¢Our comparative analysis demonstrates the superiority of\nLLMs in crafting adversarial attack algorithms over existing\nmethods. The benefits are threefold:1) they enable algorithm\ngeneration through natural language interactions, thereby\nreducing the dependence on human expertise;2) they exhibit\na proficiency to generate more potent algorithms than those\nconceived by human experts, and3) they display a capability\nto produce algorithms that can be integrated seamlessly with\nexisting testing codes.\nâ€¢The experimental evaluation and analysis highlight the gen-\nerated algorithmsâ€™ robust performance, surpassing those that\nare manually designed. This furnishes new insights into the\nconstruction of decision-based adversarial attacks.\n2 RELATED WORKS\n2.1 Decision-based Adversarial Attacks\nDecision-based adversarial attacks constitute the most challenging\nscenarios for attackers, given that the only information available\nabout the target model is the output label. Despite this obstacle,\nthey pose a considerable threat to machine learning applications.\nA pioneering study by Ilyas et al. [21] demonstrated the use of\nNatural Evolution Strategies (NES) to optimize a surrogate func-\ntion with a limited number of queries to the model. Subsequent\nadvancements have focused on refining gradient estimation tech-\nniques. For example, the OPT attack framework introduced by\nCheng et al. [7] reformulates the primary optimization challenge.\nMore sophisticated methods, such as the Sign-OPT [8], emphasize\nthe direction of the gradient rather than its magnitude, while the\nHopSkipJump attack [5] incorporates efficient gradient estimation\nand combine it with a binary search to closely track the decision\nboundary. The effectiveness of decision-based attacks is further sup-\nported by strategies based on random walks, such as the Boundary\nAttack [2] and Evolutionary Attack [12].\n2.2 Automatically Devising Adversarial Attacks.\nThe field of adversarial machine learning has increasingly focused\non the automated development of attack algorithms [15, 41]. The\nevolution of attack methods has progressed from basic gradient-\nbased methods such as the Fast Gradient Sign Method (FGSM),\nwhich relies on actual gradient data [ 16], to more sophisticated\niterative and optimization-based methods, such as decision-based\nattacks that require only output label data [2, 5, 7, 8, 12]. Significant\nresearch efforts have been invested in the autonomous generation\nof attack methods utilizing genetic algorithms and evolutionary\nstrategies. To address the prohibitive inefficiency of exploring an un-\nbounded function space of attack algorithms, researchers have intro-\nduced a domain-specific language (DSL) to constrain the complexity\nof functions, thereby achieving the notable efficiency improvements\nof AutoDA over traditional attack methods [15]. However, creating\nthese algorithms continues to be a labor-intensive process, requiring\nspecialized knowledge to formulate a DSL, develop an associated\ncode generator, and design an appropriate testing framework.\n2.3 LLMs for Algorithm Design.\nThe recent surge in LLMsâ€™ capabilities, coupled with their access\nto extensive training datasets, has significantly enhanced their\nperformance across various research domains [ 23, 44]. Notably,\nthey excel in executing diverse tasks in a zero-shot fashion [1, 6, 17,\nL-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks GECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia\n20, 22, 25, 31, 32, 38, 42, 43, 45, 46]. Such progress opens avenues for\nLLMs to generate and manipulate complex algorithmic structures.\nIn extending their application, LLMs are now instrumental in\nthe innovation of several algorithmic frameworks. They have been\neffectively integrated as black-box components in the development\nof evolutionary algorithms, neural architectures, Monte Carlo Tree\nSearch algorithms, solutions for graph-based combinatorial opti-\nmization, genetic programming, and open-ended challenges [44].\nWhile engaging with LLMs through prompts is common, it may re-\nsult in suboptimal outcomes. A fusion of large language models with\nevolutionary computation has emerged as a revolutionary advance-\nment [40], facilitating the self-enhancement of algorithms [28, 29],\nprogramming codes [26], and mathematical functions [36] through\nautonomous, iterative refinement within an evolutionary setting.\n3 PRELIMINARIES\n3.1 Decision-based Adversarial Attacks\nConsider a cloud-based image classifier M: Xâ†’Y , which maps\nimages from an input space, denoted by X, to an output space\nof classification probabilities, denoted by Y. The input space X\nconsists of images with ğ¶ channels of ğ» Ã—ğ‘Š dimensions and is a\nsubset of [0,1]ğ¶Ã—ğ»Ã—ğ‘Š, while the output space Y, representing ğ‘š\npotential class labels, is a subset of [0,1]ğ‘š probability space.\nWhen attackers interact with this classifier, they submit a query\nimage ğ’™ âˆˆX and receive the predicted output M(ğ’™)âˆˆY . The\ndecision-making process can be clarified by defining the label of\ninput ğ’™ as ğ¶(ğ’™)= arg maxğ‘– Mğ‘–(ğ’™), which indicates the modelâ€™s\nhighest confidence prediction. In decision-based attacks, the adver-\nsary only gains knowledge of this label.\nThe attackerâ€™s goal in a decision-based attack is to introduce a\nperturbation ğ›¿ to the original input ğ’™0, where the perturbation is\nminimal yet effective such that âˆ¥ğ›¿âˆ¥ğ‘ â‰¤ğœ–. This perturbation results\nin a modified input ğ’™0 +ğ›¿that misleads the classifier into assigning\na different label. Mathematically, this process is expressed as an\noptimization problem:\nmin âˆ¥ğ›¿âˆ¥ğ‘ s.t. ğ¶(ğ’™0 +ğ›¿)â‰  ğ¶(ğ’™0). (1)\nTo be deemed successful, a decision-based adversarial attack\nmust ensure the perturbationâ€™s magnitude is within a predefined\nthreshold, âˆ¥ğ›¿âˆ¥ğ‘ â‰¤ğœ–. While this work primarily examines untar-\ngeted attacks bound by the â„“2-norm (ğ‘ = 2), it also acknowledges\nthe adaptability of our proposed method to facilitate targeted at-\ntacks by altering the constraint to ğ¶(ğ’™0 +ğ›¿)= ğ‘¦, with ğ‘¦being the\ndesignated target label.\n3.2 Algorithm Evolution using LLMs\nIn this study, we employ the LLMs to generate the attacking heuris-\ntic in an evolutionary framework. It is structured around a cyclical\nprocess encompassing key EC stages including initialization, func-\ntion evaluation, selection, crossover, mutation, and population man-\nagement, the last of which meditates on diversity and convergence\namong the proposed solutions. Our methodology draws inspiration\nfrom the algorithmic approach developed by Liu et al. [29].\nInitialization. The initial population is either derived from ex-\ntant algorithms or is freshly generated using LLMs. Using existing\nalgorithms provides a solid baseline for the evolutionary search,\nwhereas generation from scratch affords the possibility to discover\nan expansive, novel algorithmic domain. Our methodology involves\nthe latter, leveraging LLMs to generate an initial suite of algorithms.\nThe exploration of evolution using established algorithms remains\nan integral aspect for formulating robust baselines in future re-\nsearch endeavors.\nEvaluating Algorithm. A pivotal component of AEL is assess-\ning the solutionsâ€™ fitness value. We employ decision-based attack\ntesting as the evaluation mechanism, defining fitness through the\nmeasurement of the â„“2 distance between the original input and the\nadversarial output generated by the algorithm.\nGenerating New Solutions. This stage adheres to the established\nprotocols of EC.\nâ€¢Selection. Analogous to traditional EC practices, we select a\npredetermined number of algorithms to be retained through\neach iteration.\nâ€¢Crossover. We facilitate the crossover operation by submit-\nting a pair of algorithmic candidates, along with guiding\nprompts, to the LLMs, which in turn, synthesize a poten-\ntially superior algorithm. This approach leverages the LLMsâ€™\nability to boost the search process beyond the random search\ncapabilities of automated program synthesis.\nâ€¢Mutation. Introducing variation into the algorithmic pool\nis paramount for fostering diversity. This is accomplished\nby instructing the LLMs to introduce minor modifications\nto the current algorithms.\n4 L-AUTODA: LLM-BASED AUTOMATED\nDECISION-BASED ADVERSARIAL ATTACKS\nIn this section, we introduce our novel framework, L-AutoDA,\nwhich is designed for automatically generating decision-based ad-\nversarial attacks. We begin by delineating the problem formulation\nand examining the search space associated with our framework\n(Section 4.1). Subsequently, we describe the comprehensive struc-\nture of the L-AutoDA framework (Section 4.2) as well as elaborate\non the specifics of its implementation (Section 4.3). An illustrative\noverview of the L-AutoDA architecture is depicted in Figure 1.\n4.1 Decision-based Attack Framework\nRandom Walk Template. We have developed a foundational\nframework for decision-based adversarial attacks, founded on the\nrandom walk paradigm, to establish the function search space, as de-\npicted in Algorithm 1. This framework integrates critical elements\nfrom pioneering techniques such as the Evolutionary Attack [12],\nthe Boundary Attack [2], and various other strategies. Although\ngradient-based frameworks are also prevalent, we leave the explo-\nration of this domain to future research endeavors.\nThe framework highlights two pivotal components for further\nimprovement: the generate function and the accompanying hy-\nperparameters. The generate function is vital to the algorithm,\nhandling the current adversarial sample ğ’™1, the original example\nğ’™0, and synthesizing a new adversarial instance ğ’™. Hyperparame-\nters are pivotal in steering the algorithmâ€™s behavior, influencing\nfactors such as step size and the number of iterations. To streamline\nGECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, and Qingfu Zhang\ngenerate()Parameter\nTuning\nAlgorithm\nAlgorithm Algorithm\nAlgorithm Algorithm\nAlgorithm\nAlgorithm\nAlgorithm\nAlgorithm\nCrossover MutationCreation\nEvaluation & Population ManagementAlgorithm Evolution\nDecision-based Adversarial Attack\nOriginal Image\nGaussian Noise\nSHyper-parameter\nAdversarial Image\n Best Adversarial Image\nFigure 1: Overview of the L-AutoDA Framework Methodology. This diagram delineates the two core components of our\nL-AutoDA framework: the algorithm generation and testing phases. In the algorithm generation phase, we adopt the AEL\nframework, leveraging LLMs to guide an evolutionary search process. In the testing phase, we employ existing decision-based\nattack testing code, integrating these algorithms into the attack program to validate their efficacy.\nAlgorithm 1 Random Walk Framework for Decision-Based Attacks\nunder â„“2 perturbation\n1: Input: original example ğ’™0, adversarial starting point ğ’™1\n2: Output: An adversarial example ğ’™.\n3: Initialization: ğ’™ â†ğ’™1; ğ‘‘min â†âˆ¥ğ’™ âˆ’ğ’™0 âˆ¥2\n4: while query budget not reached do\n5: ğ’™â€²â†generate(ğ’™,ğ’™0)\n6: if ğ’™â€²is adversarial and âˆ¥ğ’™â€²âˆ’ğ’™0 âˆ¥2 < ğ‘‘min then\n7: ğ’™ â†ğ’™â€²; ğ‘‘min â†âˆ¥ğ’™â€²âˆ’ğ’™0 âˆ¥2\n8: end if\n9: Update hyper-parameters.\n10: end while\n11: return ğ’™\nthe search process, we adopted the parameter tuning strategy from\n[15], concentrating our efforts on refining the generate function.\nSearch Space. We let the LLM to explore the search space of the\ngenerate function. While devising a comprehensive algorithm for\nthe generation of perturbations is a viable approach to advance our\nLLM-based algorithmic framework, the extensive search space com-\nplicates the discovery of the optimum algorithmic solution. Future\ninvestigations will engage with the wide-ranging possibilities and\naddress the challenges arising from this extensive search space [18].\n4.2 L-AutoDA\nThe L-AutoDA framework represents a cutting-edge system that\nleverages the AEL paradigm [29] to expedite the creation of novel\ndecision-based adversarial attack algorithms. Central to L-AutoDA\nis the pursuit of an optimalgenerate function, which is responsible\nfor generating new adversarial examples during the attack process.\nThe resulting generate functions are seamlessly integrated into\nexisting decision-based attack programs, enhancing the continuous\ninnovation and assessment of diverse attack strategies, as depicted\nin Figure 1. Subsequent paragraphs detail the workflow within the\nL-AutoDA framework, beginning with the initialization of a set of\ncandidate algorithms.\nInitialization. As L-AutoDA adopts a population-based method to\ncultivate a diverse array of candidate algorithms, we first need to\ninitialize the population. This process involves providing a carefully\nconstructed prompt:\nInitialization Prompt. Given an image org_img, its adversarial im-\nage best_adv_img, and a random normal noise std_normal_noise, you\nneed to design an algorithm to combine them to search for a new adver-\nsarial example x_new. hyperparams ranges from 0.5 to 1.5. It gets larger\nwhen this algorithm outputs more adversarial examples, and vice versa.\nIt can be used to control the step size of the search. Operations you may\nuse include: adding, subtracting, multiplying, dividing, dot product, and\nl2 norm computation. Design an novel algorithm with various search\ntechniques. Your code should be able to run without further assistance.\nMoreover, the input and output parameters and their correspond-\ning messages of the generate function are provided to the AEL\nframework to further ensure the legitimacy of the generated code.\nPopulation-based Search. Following initialization, L-AutoDA en-\ngages in a population-based search within the evolutionary com-\nputation paradigm, employing a specialized testing script (as men-\ntioned in Section. 4.3) to evaluate fitness values.\nL-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks GECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia\nObjective Value. The efficacy of the algorithms is measured by an\nobjective value, denoted as the average distance between adversarial\nand original images. This value acts as a fitness function within the\nAEL framework and steers the evolutionary algorithm.\nSearch Process Guided by the objective value, L-AutoDA applies\nevolutionary operations, such as selection, crossover, and mutation,\nto refine the assortment of algorithms. Different from traditional\nevolutionary algorithms, L-AutoDA implements the above opera-\ntions leveraging LLMs by interacting with them with prompts and\ninformation like the objective value. During this process, the most\npromising candidates, or \"elite\" algorithms, are identified and re-\ntained. This evolutionary cycle is performed iteratively to enhance\nthe development of more potent adversarial attack algorithms.\nSubstantial Advantages. L-AutoDAâ€™s generative mechanism is\nharmoniously compatible with conventional decision-based attack\nprograms. It assesses the quality of the generated algorithms by\nexamining the output the attack program produces when provided\nwith the generated generate function. This methodology marks\na significant leap from traditional program synthesis, which typi-\ncally necessitates rigorous validation to confirm the legitimacy and\nfunctional integrity of the generated code. By concentrating on the\nalgorithmic output and its effectiveness, L-AutoDA streamlines the\nsearch process and demonstrates its superiority.\n4.3 Implementation\nThis section details the implementation of the L-AutoDA algo-\nrithm, starting with a comprehensive description of the search\nspace for generate functions, followed by the elucidation of a feed-\nback mechanism for hyperparameter adjustment. It concludes with\nan overview of the testing script used to evaluate the performance\nof the evolved algorithms.\nFunction Specification. The generate function accepts four in-\nputs: the original example ğ’™0, the adversarial starting point ğ’™1,\nstandard random noise ğ’“, and a dynamically adjusted hyperparame-\nter ğ‘ . Its objective is to ingeniously integrate these inputs to produce\nan adversarial example ğ’™, with the hyperparameter providing in-\nformed control over step size referencing.\nHyper-parameter Tuning. Our approach to hyperparameter tun-\ning adopts the strategy presented in [15]. We introduce a piece-wise\nlinear function ğ‘“(ğ‘)defined as:\nğ‘“(ğ‘)=\n\u001a0.5 +2ğ‘ 0 â‰¤ğ‘ â‰¤0.25\n5\n6 +2ğ‘\n3 0.25 < ğ‘ â‰¤1 (2)\nDuring each iteration, ğ‘ is updated in the following manner:\nğ‘ = 0.95ğ‘+0.05ğ‘˜ (3)\nwhere ğ‘˜ represents the discovery of an improved adversarial point,\ntaking on the value of 1 if a better point is found and 0 otherwise.\nThe hyperparameter ğ‘  is then computed by:\nğ‘  = ğ‘ Â·[ğ‘“(ğ‘)]0.1 (4)\nThis engenders a compensatory feedback loop, aimed at anchoring\nğ‘ around 0.25.\nTesting Script. The AEL framework relies on a fitness function\nvalue to guide its evolutionary progress. In this context, a testing\nscript was devised to evaluate the efficacy of the algorithms pro-\nduced. To avoid the extensive time requirement associated with\nprocessing the entire test set, a representative subset of the dataset\nwas chosen for our experiments. These samples are used to com-\npute the fitness value, utilizing standardized attack settings. This\nmethod employs a standardized set of attack parameters to calcu-\nlate the fitness value. Although this approach may bring about a\ncertain degree of bias, the empirical evaluation results support its\neffectiveness in accelerating the evolutionary search.\n5 EXPERIMENTS\n5.1 Experimental Setup\nL-AutoDA Generation. The experimental setup for the L-AutoDA\nalgorithm generation is divided into two distinct parts: 1) settings\nfor the AEL running process and 2) for the objective value eval-\nuation. Note that our experiments are conducted on CIFAR-10\ndataset [24] and a ResNet-18 classification model [ 10], which is\na prevalent benchmark for adversarial attack algorithms.\nAEL Settings. In our setting, the AEL framework operates over 20\ngenerations, each comprising 10 algorithm candidates. Moreover,\nwe set the crossover probability at 1.0, ensuring that each pair\nof selected programs undergoes recombination, and the mutation\nprobability at 0.5 to introduce variability. The default LLM for al-\ngorithm generation is GPT-3.5-turbo-1106, with plans to expand\ntesting to additional large language models in subsequent research.\nAlgorithm Evaluation. In assessing the performance of the devised\nalgorithms, we have tailored our testing procedure to confine each\nalgorithm to a maximum of 8,000 queries. We execute the algo-\nrithms on the first eight images of the CIFAR-10 test set to ensure\na consistent and manageable testing environment. The adversarial\nimages produced are then used to calculate the â„“2 distances relative\nto their original counterparts. The mean of these distances is com-\nputed to serve as the fitness value, which is fed back into the AEL\nframework, thereby informing the evolutionary search for more\neffective attack algorithms.\nAttack Evaluation. The evaluation process for different attacks is a\ncrucial aspect of the experimental setup, providing a comprehensive\nassessment of the generated adversarial algorithmsâ€™ performance.\nDatasets. Our evaluation utilizes a subset of the CIFAR-10 dataset,\ncomprising 100 randomly sampled images from each class, to ensure\na diverse and representative test bed. To facilitate a fair comparison\nacross all attack algorithms, we introduce a set of 10 images with\nincorrect labels as the initial starting points for the attacks, ensuring\nthat each algorithm begins from a standardized baseline.\nComparative Algorithms. In our comparative analysis, we establish\nthe Boundary attack [2], which operates under the random walk\nframework, as the baseline algorithm. Additionally, we include the\nwidely acknowledged SOTA decision-based attack algorithm, the\nHopSkipJumpAttack (HSJA) [5], which employs a gradient-based\napproach. To further enrich our comparison, we introduce a variant\nof HSJA that utilizes a grid search strategy instead of its default\ngeometric progression for step search, denoted as HSJA* in our\npaper. Our future work anticipates the inclusion of more attack\nalgorithms for a more exhaustive comparison.\nGECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, and Qingfu Zhang\nTable 2: The full test performance of L-AutoDA-20 compared to three baseline algorithms.\nAttack Name Distance ( â„“2 -norm) Attack Success Rate\n# of Queries 2500 5000 10000 2500 5000 10000\nBoundary 1.9107 1.2665 1.0938 0.7861 0.4495 0.3340 14 .7 26 .2 65 .5\nHSJA 2.0512 1.0876 1.2833 0.7442 0.8978 0.5360 9.2 16 .1 24 .6\nHSJA* 2.6482 1.5790 1.6532 1.0347 1.1306 0.6987 7.9 13 .9 19 .6\nL-AutoDA-20 1.5202 0.1337 0.6171 0.1430 0.3445 0.2386 0.0 0 .5 80 .3\nTable 3: The performance of L-AutoDA compared to three\nbaseline algorithms using the testing script. The mean dis-\ntance of 1000 images are documented with the standard vari-\nance to be the subscript. The best performance cell is marked\nwith light gray and the text within is bolded.\nBoundary HSJA HSJA* L-Auto-20\n0.3939 1 .3628 1 .2839 0.2517\n1 3 5 7 9 11 13 15 17 19\nNumber of Generations\n100\n101\nObjective Value of Testing\nBoundary Attack\nHSJ Attack\nAlgorithms\nL-AutoDA\nFigure 2: Performance Trajectories of L-AutoDA. This graph\nillustrates the comparative efficiency of our L-AutoDA frame-\nwork against the human-best gradient-based (HopSkipJump\nAttack) and gradient-free (Boundary Attack) methods. L-\nAutoDAâ€™s candidates demonstrate a breakthrough in the 13th\ngeneration, surpassing the reference performance lines and\ncontinuing to enhance efficiency in subsequent generations.\nDetailed Parameters. Delving into the detailed parameter settings,\nfor the Boundary Attack, we set both the spherical and source steps\nfor the Boundary Attack at 0.01, with a step size adaptation rate\nof 1.5. In the case of the HopSkipJump Attack, the parameter ğ›¾ is\nestablished at 1.0, initial gradient estimation starts with 100 steps,\nand is limited to a peak of 10,000 steps. Reflecting the adaptive\nnature of the L-AutoDA-generated algorithms, a negative feedback\nmechanism is employed to fine-tune the hyperparameter ğ‘ , which\nis initially set to 0.001.\n5.2 Algorithm Generation\nThe performance of the algorithms generated by L-AutoDA is en-\ncapsulated in Figure 2, which demonstrates their compelling ca-\npabilities. Remarkably, the initial iteration of L-AutoDA produced\nalgorithms that outperformed HSJA. Although this unexpected\nresult may be partially attributed to the limited subset of images\nused during testing, it nonetheless underscores the potential of\nL-AutoDA in rapidly devising effective attack strategies. As the\nevolutionary process progressed, L-AutoDA continued to refine its\nalgorithms, surpassing both HSJA and Boundary Attack by the 6th\ngeneration. This trend of improvement was consistent, with each\nsubsequent generation enhancing the algorithmsâ€™ effectiveness.\nAn intriguing aspect was the reduction in the variance of al-\ngorithm performance within each generation. This convergence\nsuggests a stabilization of performance across the generated al-\ngorithms, indicating that L-AutoDA is not only producing more\neffective algorithms over time but also more reliable ones.\nThe results of the final round are documented in Table 3. L-\nAutoDAâ€™s best algorithm within the 20th generation, denoted as\nL-Auto-20, achieved a mean perturbation distance of 0.2517 across\nthe test images. This represents a significant improvement over the\nHSJA and Boundary Attack, which achieved mean perturbation\ndistances of 1.3628 and 0.3939, respectively.\n5.3 Attack Evaluation\nTo thoroughly evaluate the algorithms generated, we subjected\nthem to tests on an expanded subset as delineated in our experi-\nmental setup. The most effective algorithm produced by the final\niteration of L-AutoDA, referred to as L-AutoDA-20, was selected\nfor benchmark comparison.\nOverall Results. We have documented the overall full test results\nin Table 2. The table reveals that L-AutoDA-20 is the most effective\nalgorithm, achieving the lowest mean distance across all query\ncounts. This result is particularly impressive given that L-AutoDA-\n20 was generated entirely from scratch by the LLM, without any\nhuman intervention. As for the success rate, L-AutoDA-20 achieved\na 0% success rate at 2500 queries, which is expected given the\nlimited number of queries. The success rate then increased to 80.3%\nat 10000 queries, surpassing all other algorithms. We delineate\nthe relationship between attack success rate and distance in the\nfollowing sections.\nAttack Success Rate. Figure 3 illustrates the attack success rate\nwith the number of queries. A successful attack is defined by an\nL-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks GECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia\n2500 5000 10000\nNumer of queries\n0.0\n0.2\n0.4\n0.6\n0.8Attack success rate\nBoundary\nL-AutoDA-20\nHJSA\nHJSA*\nFigure 3: Attack Success Rate using different numbers of\nqueries using L-AutoDA-20 and other attack algorithms.\nâ„“2 norm less than 0.5 between the adversarial example and the\noriginal image, consistent with the widely accepted standard in\nthe current benchmarks [9]. The figure reveals that L-AutoDA-20â€™s\nperformance is suboptimal at 2500 and 5000 queries. However, there\nis a notable uptick in success rate when the query count reaches\n10000, surpassing all baseline algorithms. This pattern suggests that\nL-AutoDA sacrifices initial search efficiency to enhance the quality\nof the search at later stages, particularly after 8000 queries (testing\nscript).\nDistance. We present the comparative analysis of the perturbation\ndistances in Figure 4, where we plot the mean â„“2 distance between\nthe adversarial and original images against the number of queries\nused. The shaded areas in the figure represent a 0.25 multiplier of\nthe standard deviation, providing insight into the variability of each\nalgorithmâ€™s performance.\nFrom Figure 4, it is evident that L-AutoDA-20 maintains the most\nconsistent performance across all tested query counts, as indicated\nby the smallest standard deviation values. This consistency suggests\nthat L-AutoDA-20 is less sensitive to the variations in the input\ndata, making it a robust choice for generating adversarial examples.\nAlthough this robustness may come at the cost of a reduced attack\nsuccess rate in the initial phase, it becomes a significant advantage\nin later stages, particularly beyond 8000 queries.\nThe stability of L-AutoDA-20 is particularly beneficial when the\nattack requires subtlety, as it is capable of producing perturbations\nthat are minimally perceptible yet still effective. This characteristic\nis crucial for scenarios where detectability is a concern and stealth\nis paramount.\n5.4 Additional Results on Median Distance\nTo avoid the influence of variations with the images and better illus-\ntrate the effectiveness of our framework, we have demonstrated the\nmedian distance of the adversarial examples generated by different\nalgorithms in Table 4. The results are consistent with the previous\nanalysis, with L-AutoDA-20 achieving the lowest median distance\nacross all query counts.\n2500 5000 10000\nNumer of queries\n1\n2\n3Distance\nBoundary\nL-AutoDA-20\nHJSA\nHJSA*\nFigure 4: Distance between adversarial examples and original\nimages using different numbers of queries using L-AutoDA-\n20 and other attack algorithms. The lines denote the mean\nvalue of the test pairs and the shaded areas represent a 0.25\nmultiplier of the standard deviation.\nTable 4: Median distance of L-AutoDA-20 compared to three\nbaseline algorithms. The best performance cell is marked\nwith light gray and the text within is bolded.\n2500 5000 10000\nBoundary 1.7374 0 .9489 0 .3695\nHSJA 2.0230 1 .2468 0 .8646\nHSJA* 2.5150 1 .5580 1 .0618\nL-AutoDA-20 1.5301 0.5896 0.2862\n5.5 Interpretation of the algorithms\nTo elucidate the evolutionary process of the generate() function,\na representative algorithm from the initial population and the most\nsuccessful algorithm from the final population were chosen for\ncomparative analysis.\nInitial Population. The selected algorithm from the initial pop-\nulation is detailed in Algorithm 2. While the search for adversar-\nial examples is not assured by more efficient search vectors, this\nalgorithm shows its flexibility by exploring different operations.\nHowever, since we want to generate adversarial examples that are\nboth effective and efficient, the initial algorithm may not be the\nmost optimal choice and continue to evolve.\nFinal Population. The generate function output by L-AutoDA is\nillustrated in Algorithm 3. The algorithm starts by taking the differ-\nence between the original example x0 and the adversarial starting\npoint x1. By moving along this vector, one can generate examples\nthat are in between the original and the adversarial, which may\nhelp in exploring the space around known data points. Further-\nmore, efficient search is enabled through the inclusion of another\nnormalized vector d\nğ‘›ğ‘œğ‘Ÿğ‘š. Then two scales of noise are added to the\nexample, one with the same direction as the difference vector d\nand the other with the same direction as the normalized difference\nvector d\nğ‘›ğ‘œğ‘Ÿğ‘š. The noise is further scaled by a hyperparameter ğ‘  to\ncontrol the magnitude of the perturbation. Combined with these\nGECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, and Qingfu Zhang\nAlgorithm 2 generate() (Initial Population)\n1: Input: original example x0, adversarial starting point x1, stan-\ndard normal noise n, hyperparameter ğ‘ \n2: Output: A new proposed example x\n3: ğ‘›0 â†N(0,1)\n4: x â†ğ‘ x0 +(1 âˆ’ğ‘ )x1 +ğ‘›0n\n5: ğ‘‚ğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› = ğ‘Ÿğ‘ğ‘›ğ‘‘ğ¶â„ğ‘œğ‘–ğ‘ğ‘’(ğ‘ğ‘‘ğ‘‘,ğ‘ ğ‘¢ğ‘,ğ‘šğ‘¢ğ‘™ )\n6: ğ‘›1 â†U(0.5,1.5)\n7: if ğ‘‚ğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› == ğ‘ğ‘‘ğ‘‘ then\n8: x â†x +ğ‘›1n\n9: else if ğ‘‚ğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› == ğ‘ ğ‘¢ğ‘ then\n10: x â†x âˆ’ğ‘›1n\n11: else\n12: x â†x âˆ—ğ‘›1n\n13: end if\n14: d â†x0 âˆ’x1\nAlgorithm 3 generate() (Final Population)\n1: Input: original example x0, adversarial starting point x1, stan-\ndard normal noise n, hyperparameter ğ‘ \n2: Output: A new proposed example x\n3: d â†x0 âˆ’x1\n4: ğ‘›ğ‘œğ‘Ÿğ‘š = max(âˆ¥dâˆ¥2,âˆ¥nâˆ¥2)\n5: x â†x1 +ğ‘ (d + d\nğ‘›ğ‘œğ‘Ÿğ‘š)+ğ‘ (n +ğ‘  n\nğ‘›ğ‘œğ‘Ÿğ‘š)\nsearch vectors, L-AutoDA is able to generate adversarial examples\nthat are both effective and efficient.\n6 DISCUSSION\nExpanded Experimental Validation. Although our experimental\nframework, consisting of 20 generations with 10 individuals per gen-\neration, has yielded results surpassing those of manually-designed\nstate-of-the-art algorithms, it has not fully tested the boundaries\nof our framework or LLMs. We will increase the number of gen-\nerations and individuals to see if we can obtain better results. We\naim to test these limits by increasing the population size and the\nnumber of generations. Additionally, initializing the search process\nwith existing algorithms and subsequently refining them represents\na promising avenue for further experimentation.\nBroader Algorithm Search Space. or expediency and as an initial\nattempt for automated attack algorithm design using LLMs into\nthe automated design of attack algorithms using LLMs, we con-\nfined the search space to that defined by the generate() function.\nHowever, this narrow scope may restrict the discovery of optimal\nalgorithms. Future work will seek to exploit the full potential of\nLLMs by allowing them to craft comprehensive algorithms without\nsuch constraints.\nEnhancing Prompt Adaptability. Our methodology employed\na set of static prompts to assist LLMs in algorithm generation.\nHowever, the fixed prompts may not be the best prompts for LLMs\nto generate algorithms. The effectiveness of these prompts, however,\nmay not represent an optimal use of LLM capabilities. The concept\nof chain-of-reasoning, which underpins our work and AEL, suggests\na close relationship with adaptive prompt generation. Investigating\nmethods of dynamically generating prompts is an objective of our\nongoing research.\nAddressing Limitations. While the synthesis of programs using\nlarge language models is the focus of our research, it is not without\nits drawbacks. These models may occasionally yield unsatisfactory\noutcomes, albeit at a lower rate than traditional approaches. Im-\nproving the specificity of constraints within the prompts to ensure\nthe validity of the algorithms produced will be an integral part of\nour forthcoming efforts.\n7 CONCLUSION\nIn this paper, we have successfully demonstrated the innovative\napplication of LLMs for the automatic design of decision-based\nadversarial attack algorithms. By leveraging the AEL framework,\nwe have not only streamlined the algorithmic design process, but\nalso achieved a significant reduction in the time and expertise\nrequired to develop effective adversarial attacks. Our approach,\nencapsulated in the L-AutoDA framework, represents a paradigm\nshift in the field of adversarial machine learning, showcasing the\nuntapped potential of LLMs in the realm of security and algorithm\nsynthesis.\nACKNOWLEDGMENTS\nThe work described in this paper was supported by the Research\nGrants Council of the Hong Kong Special Administrative Region,\nChina [GRF Project No. CityU 11215622], by Natural Science Foun-\ndation of China [Project No: 62276223] and by Key Basic Research\nFoundation of Shenzhen, China.\nREFERENCES\n[1] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. 2023.\nChip-Chat: Challenges and Opportunities in Conversational Hardware Design.\narXiv preprint arXiv:2305.13243 (2023).\n[2] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2018. Decision-Based Adver-\nsarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. In\n6th International Conference on Learning Representations, (ICLR) . OpenReview.net.\n[3] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Ram-\npazzi, Qi Alfred Chen, Kevin Fu, and Z. Morley Mao. 2019. Adversarial Sensor\nAttack on LiDAR-based Perception in Autonomous Driving. In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security, (CCS) .\nACM.\n[4] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and\nDebdeep Mukhopadhyay. 2021. A survey on adversarial attacks and defences.\nCAAI Trans. Intell. Technol. (2021). https://doi.org/10.1049/CIT2.12028\n[5] Jianbo Chen, Michael I. Jordan, and Martin J. Wainwright. 2020. HopSkipJumpAt-\ntack: A Query-Efficient Decision-Based Attack. In 2020 IEEE Symposium on Secu-\nrity and Privacy, (SP) . IEEE.\n[6] Kunming Cheng, Qiang Guo, Yongbin He, Yanqiu Lu, Shuqin Gu, and Haiyang\nWu. 2023. Exploring the potential of GPT-4 in biomedical engineering: the dawn\nof a new era. Annals of Biomedical Engineering (2023), 1â€“9.\n[7] Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, Jinfeng Yi, and Cho-Jui\nHsieh. 2019. Query-Efficient Hard-label Black-box Attack: An Optimization-\nbased Approach. In 7th International Conference on Learning Representations,\nICLR. OpenReview.net.\n[8] Minhao Cheng, Simranjit Singh, Patrick H. Chen, Pin-Yu Chen, Sijia Liu, and Cho-\nJui Hsieh. 2020. Sign-OPT: A Query-Efficient Hard-label Adversarial Attack. In\n8th International Conference on Learning Representations, ICLR . OpenReview.net.\n[9] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo\nDebenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias\nHein. 2021. RobustBench: a standardized adversarial robustness benchmark. In\nProceedings of the Neural Information Processing Systems Track on Datasets and\nBenchmarks 1, (NeurIPS) .\n[10] Eduardo Dadalto. 2022. ResNet18 trained on CIFAR10. https://huggingface.co/\nedadaltocg/resnet18_cifar10. Accessed: 2023-07-01.\nL-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks GECCO â€™24 Companion, July 14â€“18, 2024, Melbourne, VIC, Australia\n[11] Junhao Dong, Junxi Chen, Xiaohua Xie, Jianhuang Lai, and Hao Chen. 2023. Ad-\nversarial Attack and Defense for Medical Image Analysis: Methods and Applica-\ntions. CoRR (2023). https://doi.org/10.48550/ARXIV.2303.14133 arXiv:2303.14133\n[12] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and\nJun Zhu. 2019. Efficient Decision-Based Black-Box Adversarial Attacks on Face\nRecognition. In IEEE Conference on Computer Vision and Pattern Recognition,\n(CVPR). Computer Vision Foundation / IEEE.\n[13] Ryan Feng, Ashish Hooda, Neal Mangaokar, Kassem Fawaz, Somesh Jha, and\nAtul Prakash. 2023. Stateful Defenses for Machine Learning Models Are Not\nYet Secure Against Black-box Attacks. In Proceedings of the 2023 ACM SIGSAC\nConference on Computer and Communications Security, (CCS) . ACM, 786â€“800.\n[14] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg,\nManuel Blum, and Frank Hutter. 2015. Efficient and Robust Automated Ma-\nchine Learning. In Advances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015 .\n[15] Qi-An Fu, Yinpeng Dong, Hang Su, Jun Zhu, and Chao Zhang. 2022. AutoDA:\nAutomated Decision-based Iterative Adversarial Attacks. In 31st USENIX Security\nSymposium, USENIX Security 2022 . USENIX Association, 3557â€“3574.\n[16] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and\nHarnessing Adversarial Examples. In 3rd International Conference on Learning\nRepresentations, (ICLR) .\n[17] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang,\nRuotong Liao, Yao Qin, Volker Tresp, and Philip Torr. 2023. A systematic survey\nof prompt engineering on vision-language foundation models. arXiv preprint\narXiv:2307.12980 (2023).\n[18] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. 2017. Program synthesis.\nFoundations and Trends Â® in Programming Languages (2017).\n[19] Ping Guo, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, and Qingfu Zhang. 2024.\nPuriDefense: Randomized Local Implicit Adversarial Purification for Defending\nBlack-box Query-based Attacks. arXiv preprint arXiv:2401.10586 (2024).\n[20] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng\nZheng, and Bei Yu. 2023. ChatEDA: A Large Language Model Powered Au-\ntonomous Agent for EDA. arXiv preprint arXiv:2308.10204 (2023).\n[21] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box\nAdversarial Attacks with Limited Queries and Information. In Proceedings of\nthe 35th International Conference on Machine Learning, (ICML) (Proceedings of\nMachine Learning Research) . PMLR.\n[22] Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend\nSmit. 2023. Is GPT-3 all you need for low-data discovery in chemistry? (2023).\n[23] Enkelejda Kasneci, Kathrin SeÃŸler, Stefan KÃ¼chemann, Maria Bannert, Daryna\nDementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan GÃ¼nnemann, Eyke\nHÃ¼llermeier, et al. 2023. ChatGPT for good? On opportunities and challenges\nof large language models for education. Learning and individual differences 103\n(2023), 102274.\n[24] A. Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images .\nTechnical Report. Univ. Toronto.\n[25] Peter Lee, Sebastien Bubeck, and Joseph Petro. 2023. Benefits, limits, and risks of\nGPT-4 as an AI chatbot for medicine. New England Journal of Medicine 388, 13\n(2023), 1233â€“1239.\n[26] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and\nKenneth O. Stanley. 2024. Evolution Through Large Models . Springer Nature\nSingapore, Singapore, 331â€“366.\n[27] Deqiang Li, Qianmu Li, Yanfang (Fanny) Ye, and Shouhuai Xu. 2023. Arms Race\nin Adversarial Malware Detection: A Survey. ACM Comput. Surv. (2023).\n[28] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao\nLu, and Qingfu Zhang. 2024. Evolution of Heuristics: Towards Efficient Automatic\nAlgorithm Design Using Large Language Mode. (2024). arXiv:2401.02051\n[29] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. 2023. Algorithm\nEvolution Using Large Language Model. arXiv preprint arXiv:2311.15249 (2023).\n[30] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversar-\nial Attacks. In 6th International Conference on Learning Representations, (ICLR) .\nOpenReview.net.\n[31] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu\nNguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent\nadvances in natural language processing via large pre-trained language models:\nA survey. Comput. Surveys (2021).\n[32] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric\nHorvitz. 2023. Capabilities of gpt-4 on medical challenge problems.arXiv preprint\narXiv:2303.13375 (2023).\n[33] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay\nCelik, and Ananthram Swami. 2017. Practical Black-Box Attacks against Machine\nLearning. In Proceedings of the 2017 ACM on Asia Conference on Computer and\nCommunications Security, AsiaCCS . ACM.\n[34] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. 2020. AutoML-Zero:\nEvolving Machine Learning Algorithms From Scratch. In Proceedings of the 37th\nInternational Conference on Machine Learning, (ICML) (Proceedings of Machine\nLearning Research) . PMLR.\n[35] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov,\nMatej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S\nEllenberg, Pengming Wang, Omar Fawzi, et al. 2023. Mathematical discoveries\nfrom program search with large language models. Nature (2023), 1â€“3.\n[36] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov,\nMatej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S\nEllenberg, Pengming Wang, Omar Fawzi, et al. 2023. Mathematical discoveries\nfrom program search with large language models. Nature (2023), 1â€“3.\n[37] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.\nIn 2nd International Conference on Learning Representations (ICLR) .\n[38] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques\nKlein, and TegawendÃ© F BissyandÃ©. 2023. Is ChatGPT the Ultimate Programming\nAssistantâ€“How far is it? arXiv preprint arXiv:2304.11938 (2023).\n[39] Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, and Chaowei Xiao. 2024. AdaShield:\nSafeguarding Multimodal Large Language Models from Structure-based Attack\nvia Adaptive Shield Prompting. CoRR (2024).\n[40] Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, and Kay Chen Tan. 2024.\nEvolutionary Computation in the Era of Large Language Model: Survey and\nRoadmap. arXiv preprint arXiv:2401.10034 (2024).\n[41] Fei Yin, Yong Zhang, Baoyuan Wu, Yan Feng, Jingyi Zhang, Yanbo Fan, and Yujiu\nYang. 2023. Generalizable Black-Box Adversarial Attack with Meta Learning.\nCoRR abs/2301.00364 (2023). arXiv:2301.00364\n[42] Caiyang Yu, Xianggen Liu, Chenwei Tang, Wentao Feng, and Jiancheng Lv. 2023.\nGPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model.\narXiv preprint arXiv:2305.05351 (2023).\n[43] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou.\n2023. AutoML-GPT: Automatic Machine Learning with GPT. arXiv preprint\narXiv:2305.02499 (2023).\n[44] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[45] Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel\nAlbanie. 2023. Can GPT-4 Perform Neural Architecture Search? arXiv preprint\narXiv:2304.10970 (2023).\n[46] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\nHarris Chan, and Jimmy Ba. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910 (2022)."
}