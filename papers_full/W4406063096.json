{
  "title": "NVP-HRI: Zero shot natural voice and posture-based humanâ€“robot interaction via large language model",
  "url": "https://openalex.org/W4406063096",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3084935389",
      "name": "Yuzhi Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2299659654",
      "name": "Shenghai Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2667655049",
      "name": "Youssef Nassar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131174562",
      "name": "Mingyu Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1896099190",
      "name": "Thomas Weber",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1712722078",
      "name": "Matthias RÃ¤tsch",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2617560792",
    "https://openalex.org/W4366165326",
    "https://openalex.org/W2113137767",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3200482794",
    "https://openalex.org/W2559085405",
    "https://openalex.org/W4403922060",
    "https://openalex.org/W4390873042",
    "https://openalex.org/W4401416464",
    "https://openalex.org/W6850227507",
    "https://openalex.org/W4386002582",
    "https://openalex.org/W6850304593",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W4295308268",
    "https://openalex.org/W6749696961",
    "https://openalex.org/W6683767564",
    "https://openalex.org/W6788247499",
    "https://openalex.org/W2964290573",
    "https://openalex.org/W4398241405",
    "https://openalex.org/W4291221484",
    "https://openalex.org/W3001865277",
    "https://openalex.org/W4293660090",
    "https://openalex.org/W2151414834",
    "https://openalex.org/W6802517928",
    "https://openalex.org/W3215638759",
    "https://openalex.org/W6854929498",
    "https://openalex.org/W6757772426",
    "https://openalex.org/W4385989168",
    "https://openalex.org/W4403390395",
    "https://openalex.org/W6849364223",
    "https://openalex.org/W4391007419",
    "https://openalex.org/W4390874575",
    "https://openalex.org/W2910779592",
    "https://openalex.org/W4376269338",
    "https://openalex.org/W4362723113",
    "https://openalex.org/W4401414351",
    "https://openalex.org/W4394746389",
    "https://openalex.org/W6852768366",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W4389665966",
    "https://openalex.org/W4224937906",
    "https://openalex.org/W4402727356",
    "https://openalex.org/W4388263176",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6847363464",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W639708223",
    "https://openalex.org/W6657026069",
    "https://openalex.org/W2124351162",
    "https://openalex.org/W4388914567",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W2966101659",
    "https://openalex.org/W6784155536",
    "https://openalex.org/W2157725521",
    "https://openalex.org/W4226218259",
    "https://openalex.org/W4387501775",
    "https://openalex.org/W4312629932",
    "https://openalex.org/W2150382645",
    "https://openalex.org/W2318802957",
    "https://openalex.org/W4389666719",
    "https://openalex.org/W6854555012",
    "https://openalex.org/W4389209548",
    "https://openalex.org/W4282839731",
    "https://openalex.org/W6746967587",
    "https://openalex.org/W3131271436",
    "https://openalex.org/W2039604658",
    "https://openalex.org/W2898994065",
    "https://openalex.org/W2942658132",
    "https://openalex.org/W2944200650",
    "https://openalex.org/W2959091704",
    "https://openalex.org/W3161551690",
    "https://openalex.org/W4401070333",
    "https://openalex.org/W6852119881",
    "https://openalex.org/W6871654125",
    "https://openalex.org/W6859152425",
    "https://openalex.org/W6848674929",
    "https://openalex.org/W4400000022",
    "https://openalex.org/W6856292469",
    "https://openalex.org/W6849709795",
    "https://openalex.org/W2027597576",
    "https://openalex.org/W4401414779",
    "https://openalex.org/W4388182168",
    "https://openalex.org/W6857472351",
    "https://openalex.org/W4389665575",
    "https://openalex.org/W4361801761",
    "https://openalex.org/W4365799847",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4405254196",
    "https://openalex.org/W4249502209",
    "https://openalex.org/W4413925341",
    "https://openalex.org/W4401496682",
    "https://openalex.org/W4413924984",
    "https://openalex.org/W4353007136",
    "https://openalex.org/W4376571587",
    "https://openalex.org/W3198867090",
    "https://openalex.org/W4377121396",
    "https://openalex.org/W4387331349",
    "https://openalex.org/W4413925499",
    "https://openalex.org/W3006117897",
    "https://openalex.org/W4414798735",
    "https://openalex.org/W2528951757",
    "https://openalex.org/W4394828156",
    "https://openalex.org/W4379233557",
    "https://openalex.org/W3098201885",
    "https://openalex.org/W4298315755",
    "https://openalex.org/W4392972144",
    "https://openalex.org/W3142041998",
    "https://openalex.org/W4403787501"
  ],
  "abstract": null,
  "full_text": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot\nInteraction via Large Language Model\nYuzhiLaia, ShenghaiYuanb,âˆ—, YoussefNassara, MingyuFanc, ThomasWebera and MatthiasRÃ¤tscha\naUniversity Reutlingen, AlteburgstraÃŸe 150, Reutlingen, 72762, Germany\nbNanyang Technological University, 50 Nanyang Avenue, Singapore, 639798, Singapore\ncDonghua University, 849 Zhongshan West Street 9, Shanghai, 200051, China\nARTICLE INFO\nKeywords:\nHuman-robotinteraction,Intentrecog-\nnition,Multi-modalityperception,Large\nLanguageModels,UnsupervisedInter-\naction\nABSTRACT\nEffective Human-Robot Interaction (HRI) is crucial for future service robots in aging societies.\nExisting solutions are biased toward only well-trained objects, creating a gap when dealing with new\nobjects. Currently, HRI systems using predefined gestures or language tokens for pretrained objects\npose challenges for all individuals, especially elderly ones. These challenges include difficulties in\nrecallingcommands,memorizinghandgestures,andlearningnewnames.ThispaperintroducesNVP-\nHRI, an intuitive multi-modal HRI paradigm that combines voice commands and deictic posture.\nNVP-HRI utilizes the Segment Anything Model (SAM) to analyze visual cues and depth data,\nenabling precise structural object representation. Through a pre-trained SAM network, NVP-HRI\nallowsinteractionwithnewobjectsviazero-shotprediction,evenwithoutpriorknowledge.NVP-HRI\nalso integrates with a large language model (LLM) for multimodal commands, coordinating them\nwith object selection and scene distribution in real time for collision-free trajectory solutions. We\nalsoregulatetheactionsequencewiththeessentialcontrolsyntaxtoreduceLLMhallucinationrisks.\nThe evaluation of diverse real-world tasks using a Universal Robot showcased up to 59.2% efficiency\nimprovementovertraditionalgesturecontrol,asillustratedinthevideo https://youtu.be/EbC7al2wiAc.\nOur code and design will be openly available athttps://github.com/laiyuzhi/NVP-HRI.git.\n1. Introduction\nIn a rapidly evolving society, the emergence of new\nobjects poses challenges to individuals and robots, partic-\nularly in interacting with objects of unknown names. This\nis a significant hurdle for effective human-robot interaction\n(HRI), especially for the elderly or sick who rely on robot\nassistance. With an aging population and rising labor costs,\nthere is growing interest in utilizing mobile manipulators\n(Li et al., 2023b; Ren et al., 2011) or novel robot platforms\n(GonÃ§alves and Santos, 2023; Cao et al., 2023b; Tang et al.,\n2024) to address these challenges. A key factor for the\nsuccess of future robotics services is ensuring that Human-\nRobot Interaction (HRI) remains intuitive and adaptable to\na wide range of objects (Alonso-MartÃ­n et al., 2017; Lu\net al., 2023; Nguyen et al., 2024a). However, existing HRI\nframeworks heavily rely on pre-trained detector networks\n(Esfahanietal.,2020;Guetal.,2022;Yangetal.,2024b;Xu\netal.,2024b)oroftenrequiremasteringcomplexgesturesor\nare prone to ambiguity when describing complex scenarios\n(Endeetal.,2011;Vancetal.,2023;Gamboa-Monteroetal.,\n2023), which is impractical for the elderly and even for\nordinarypeople.Therearesignificantdomaingapstobridge\nfor intuitive HRI with rare objects (Yang et al., 2022; Cao\nâˆ—Corresponding author\nYuzhi.Lai@Reutlingen-University.DE (Y. Lai);shyuan@ntu.edu.sg\n(S. Yuan);Youssef.Nassar@Reutlingen-University.DE (Y. Nassar);\nfanmingyu@dhu.edu.cn (M. Fan);Thomas.Weber@Reutlingen-University.DE\n(T. Weber);Matthias.Raetsch@Reutlingen-University.DE (M. RÃ¤tsch)\nORCID(s): 0009-0002-9976-7422 (Y. Lai);0009-0003-1887-6342 (S.\nYuan);0009-0007-2867-4030 (Y. Nassar);0000-0002-0492-4708 (M. Fan);\n0000-0002-2932-2308 (T. Weber);0000-0002-8254-8293 (M. RÃ¤tsch)\net al., 2023a, 2024b,a) for those who need robot services, as\nshown in Fig. 1.\nLanguage-basedsolutionshaveanincreasingpotentialin\nembodied AI applications, including tasks such as imitation\nlearning, planning, control, and HRI (Jeon et al., 2024;\nZhao et al., 2023; Vemprala et al., 2023). However, relying\non large language models (LLMs) for object perception\noften requires a supervised model and an understanding\nof semantic meaning (Esfahani et al., 2019a; Liao et al.,\n2023; Yin et al., 2023; Ji et al., 2024a; Yin et al., 2024),\nwhichposeschallengeswhenencounteringrareorunknown\nobjects. LLMs also have other problems, like hallucination\nfor untrained tasks and tedious syntax structures. Despite\nthese challenges, they serve as inspiration for new research\nefforts.\nTo address the challenge of unknown objects and im-\nprove HRI intuitiveness, we propose NVP-HRI, a zero-\nshot natural multi-modal HRI approach integratingvoice\nand posture commands via LLM (Lai et al., 2025). NVP-\nHRI makes use of the pre-trained Segment Anything Model\n(SAM) (Kirillov et al., 2023) to select areas of interest\nand enable zero-shot inference of point clouds for unknown\nobjects. Our method also incorporates an LLM to compile\nactions and goals, ensuring collision-free real-time trajec-\ntorysolutions.Thegeneratedactionsequencesaresubjectto\nfurthercross-checkforcontentaccuracyandsafety,enabling\nefficient construction of complex control sequences.\nOur main contributions are summarized below:\nâ€¢ WeproposeNVP-HRI,amulti-modalHRIframework\nfor robot manipulators, efficiently segmenting zero-\nshot unknown objects and fusing them with parallel\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 1 of 19\narXiv:2503.09335v1  [cs.RO]  12 Mar 2025\nNVP-HRI\nFigure 1: Proposed voice-posture fusion HRI method has superior efficiency in manipulating untrained objects and requires no\nmemorization of key syntax, which is ideal for elderly and healthcare applications.\nmulti-modal inputs to construct complex temporal\ncontrol sequences and references.\nâ€¢ Our system utilizes an LLM to generate robot con-\ntrol sequences, incorporating language, posture, and\nzero-shot inference input. This effectively prevents\ncollisions without human intervention and mitigates\nsemanticerrors.Weachievethisbycarefullystructur-\ninginputrepresentationsandoutputtokenstoaddress\nhallucination issues of LLMs, ensuring both safety\nand performance.\nâ€¢ We benchmark our system against state-of-the-art\nHRImethods,showingstrongperformancewithmin-\nimal syntax memorization and rapid input speed. Our\nsystem will be open source onhttps://github.com/\nlaiyuzhi/NVP-HRI.git.\n2. RELATED WORK\nObject detection (Wang et al., 2015) plays a crucial role\nin HRI systems, enabling robots to interact accurately with\nthe world (Li and Yuan, 2024). As the standard of living\nincreases, the requirements for object detection also need\nto increase, particularly in zero-shot rare object detection\n(Esfahani et al., 2021). Existing systems predominantly rely\non pre-trained supervised networks, such as Mask R-CNN\n(Hameed et al., 2022), YOLO (Li et al., 2023b), and SSD\n(Wang et al., 2022). The training of these networks relies\non a large number of annotated datasets such as ImageNet\n(Deng et al., 2009), Microsoft COCO (Lin et al., 2014),\nJacquard (Depierre et al., 2018; Li and Yuan, 2024) and\nMCD (Nguyen et al., 2024b). However, these supervised\nmodels have problems in dealing with rare (Cao et al.,\n2024b), unknown or new objects (Yuan and Wang, 2014).\nSomeworks(Weberetal.,2020;GroÃŸmannetal.,2014)ex-\nplored new modalities to deal with unknown objects (Zhou\netal.,2023),suchasusingaheadmountedeyetracker(We-\nber et al., 2020) or a depth camera (GroÃŸmann et al., 2014).\nSome researchers (Weber et al., 2020) have implemented a\ndistillation-based method using gaze with a head-mounted\neyetrackertodetectunknownobjects.However,thismethod\nrelies on the accuracy of the hardware as well as precise\ngazing at the target object for an extended period of time,\nwhich is inefficient. In addition, using and carrying an eye\ntracker can be a challenge for the elderly. Depth cameras\n(Ji et al., 2022; Xu et al., 2024a), as the most commonly\nused hardware devices in HRI, also have capabilities in the\ndetection of unknown objects. The fixation-based unknown\nobject segmentation approach (GroÃŸmann et al., 2014) was\nproposedbycombiningRGBimagesanddepthmaps.Using\nthe Graph Cut algorithm (Boykov and Kolmogorov, 2004)\nas an intermediate step and refining it with the Grab Cut\nalgorithm (Rother et al., 2004), this approach demonstrates\nnotable improvements in processing speed. However, de-\nspitetheseoptimizations,theprocessingtimerequiresmore\nthan two seconds per image. Our proposed NVP-HRI uses\npretrained SAM (Kirillov et al., 2023) to obtain point cloud\nand accurate representation of zero shot objects. Compared\nto HRI methods using supervised pre-trained models (Vanc\net al., 2023; Stepputtis et al., 2020; Constantin et al., 2022),\nour approach is more suitable for real-world challenges on\nobject detection brought by the increase in living standards.\nFurthermore, our method does not require users to wear\nadditional equipment (Deng et al., 2024a,b), enhancing us-\nability and efficiency.\nInrecentyears,developingefficient,intuitiveandnatural\nHRI methods has become a key research priority for future\nservice robots. Some systems rely on singular modality\ninput, such as gesture (Vanc et al., 2023), tactile (Wang\netal.,2024),surfaceelectromyography(EMG)(Zhangetal.,\n2024) and audio input (Skrzypek et al., 2019; Huang et al.,\n2023). However, gesture-based methods require the user to\naccurately memorize and execute gestures corresponding to\nthe predefined commands, which is a challenge. The tactile\ninput generally uses only the pressure sensor arrays. This\nallows the user to express only a relatively simple intention\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 2 of 19\nNVP-HRI\nwith low accuracy. EMG signals can only predict simple\nhuman intentions and cannot perform complex human in-\ntentions,suchasclearingthedesktop.Theaudioinput(Yang\net al., 2023, 2024b) is prone to ambiguity when describing\ncomplex scenarios (Yuan et al., 2024) because sentences\ncan be interpreted in multiple ways. To improve the intu-\nitiveness of HRI, we propose the NVP-HRI system, which\ncombinesthestrengthsofpostureandnaturallanguage.This\nmultimodal approach allows users to select objects with\ndeicticposture,avoidingtheneedfordetailedorambiguous\ndescriptions,whilecommandsaregiveninnaturallanguage,\neliminating the necessity to memorize complex gestures.\nMultimodal HRI approaches (Wang et al., 2017; Deng\net al., 2022; Zhou et al., 2023; Yang et al., 2024a) show\ngreat potential to solve these issues. Fusion of gesture and\nfacial expression (Li et al., 2023a) is proposed to categorize\nhuman emotions for HRI. However, simple fusion is not ca-\npableofunderstandingcomplexhumanintentions.However,\nsimple fusion methods are not sufficient for understanding\ncomplex human intentions. In recent years, wearable mixed\nreality(MR)devices,suchasAppleVisionPro,haveshown\npromiseasmultimodalHRIinputdevices(Lietal.,2024d,b;\nPark et al., 2024; Nguyen et al., 2023; Yang et al., 2024c).\nDespitetheirpotential,thesedevicescomewithsteeplearn-\ning curves, heavy weights, and high costs, making them\nunlikely to be widely adopted in elderly care and medical\napplications. In contrast, our proposed NVP-HRI system is\nbased solely on a microphone (Lei et al., 2025) and a depth\ncamera Xu et al. (2024a), eliminating the need for users to\nwear additional devices. This makes it more accessible and\nsuitable for everyday use, particularly in settings of elderly\ncare and patient assistance.\nAlthough robotics in the medical field often focuses on\nsurgical assist robots (Bian et al., 2023; Mahmood et al.,\n2022) and rehabilitation robots (Shah et al., 2024), these\napplications face stringent safety requirements and the need\nto handle complex and variable situations (Li et al., 2024a),\ndelaying widespread adoption. Service robots for elderly\ncare and patients, on the other hand, show more immedi-\nate potential. Current systems for the elderly or patients\noften require complex gestures or verbal commands due to\nineffective information integration. Leveraging LLMs with\nrobust reasoning offers promise for enhancing multimodal\ndataprocessing.ItcanbeappliedtovariableaspectsofHRI\n(Zhang et al., 2023), including social robots with special-\nized knowledge (Caruccio et al., 2024; Khennouche et al.,\n2024)androbottaskplanning(Singhetal.,2023).However,\nthere is still a large gap between innovations and practical\napplications Cao et al. (2020). LLMs can generate human-\nlike responses, but they can also generate inappropriate or\neven harmful content or actions due to the hallucination\nproblem. Recently, we have seen many applications (Huang\net al., 2023) that utilize pre-trained LLM with VLM to\nunderstand and interact with the environment (Jin et al.,\n2025; Li et al., 2024c). However, it is generally based on\nlanguage-only solutions. LLM-based posture and language\nfusion HRI models (Constantin et al., 2022) still need im-\nprovements to understand the context in 3D spaces and\nmanage interactions and behaviors. In our proposed sys-\ntem, we address these limitations by selecting the Segment\nAnything Model (SAM) (Kirillov et al., 2023) over other\nunsupervisedsegmentationmodelsduetoitssuperiorability\nto extract detailed structural object representations and ease\nofimplementation.This,combinedwithdepthcamera data,\nallows the LLM to better understand 3D scenes (Esfahani\net al., 2019b; Ji et al., 2024b; Chen et al., 2024; Ma et al.,\n2024) and plan collision-free trajectories based on human\nintentions.\nWhen it comes to trajectory (Xu et al., 2024c; Bai et al.,\n2024; Hu et al., 2025) and action generation, traditional\napproaches have focused on methods such as preset greedy\nsearch(Garrettetal.,2018;Fengetal.,2024),behaviortrees\n(Tadewos et al., 2022), deep learning-based solution (Wu\net al., 2018, 2019b,c,a, 2021) or Bayesian inference (Jain\nandArgall,2018;Caoetal.,2022).Recently,theintegration\nof LLMs (Vemprala et al., 2023) into the generation of\nrobot actions (Qi et al., 2024) was examined, establishing\na pipeline to translate human intentions into robot action\nsequences using an LLM. However, LLMs face limitations\nin directly accessing sensor data, while Vision Language\nModels (Constantin et al., 2022) often require significant\ncomputational resources to generate sets of 2D embeddings\n(Liu et al., 2025). These embeddings frequently contain\nerrors and uncertainties, especially when encountering pre-\nviously unseen objects. Furthermore, LLMs are prone to\ngenerating incorrect responses due to hallucination prob-\nlems. Consequently, such LLM+VLM systems are prone\nto errors and mistakes, often requiring multiple trials to\nachieve success. To generate reliable action sequences, the\nsymbolic action sequences generated by LLM are checked\nby visualizing the trajectory. To minimize hallucination, we\nformulate prompts and impose constraints on the output\nresponse tokens generated by LLM.\n3. PROBLEM DEFINITION\nAnoverviewofourproposedNVP-HRIisshowninFig.\n2. Our purpose is to create parallel multi-modal commands\nfor rare or unknown objects, utilizing SAM to derive struc-\ntural objects representation (See Fig 3). These commands\nand representations are then processed to generate real-time\ncollision-free solutions via a token-constrained LLM.\n3.1. System Overview\nStructural objects representation:\nWe represent the objects in the scene by classifying all\nobjects as 3D point cloud clusters denotedîˆ¼ âˆˆ â„3. We\nperform a structural representation of each cluster denoted\nas Î“(â‹…) âˆˆâ„7 including cluster indexğ‘– âˆˆ â„, cluster height\nâ„âˆˆâ„,clusterwidth ğ‘¤âˆˆâ„,clusterthicknesses ğ‘‘ âˆˆâ„,and\n3D cluster centroid positionğ›¾ âˆˆâ„3. ğ‘(ğ‘¡)âˆˆ â„7 denotes the\nstateoftherobotendeffectorattimet,includingthe6Dpose\nand the opening angle of the gripper. Consequently, the en-\nvironmenttuple îˆ± thatincludestheroboticarmmanipulator\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 3 of 19\nNVP-HRI\nFigure 2:System Overview.î‰‚ represents verbal command,îˆ® represents posture references,îˆ¸ is mapping vocal features to text,\nÎ“represent the 3D cluster of the scene object,îˆ¹ is the mapping to get the target intention,îˆ­ is the mapping to get the action\nsequences.\nandstructuralrepresentationoftheobjectscanbeformedby\nîˆ± =(Î“(ğ‘–,â„,ğ‘¤,ğ›¾ ), ğ‘(ğ‘¡)).\nIntention definition:In order for LLM to control the\nmanipulatorğ‘(ğ‘¡)usingstructuralobjectrepresentation Î“,the\ncomplete human intentionğ¼ is inferred from a set of sparse\nkeywords îˆ¿, which is decomposed into target references\nğ¼ğ‘¡, action referenceğ¼ğ‘ and metric referencesğœ†. Our pro-\nposed NVP-HRI system utilizes RGBD and audio sensors\ntogenerateverbalinputcommands î‰‚ andhumanposture îˆ®.\nTargetreferences ğ¼ğ‘¡ identifythespecificobjectwithwhicha\nhuman intends to interact based onîˆ®. The action reference\nğ¼ğ‘ indicatestheanticipatedinteractionwiththetargetobject\nbased on î‰‚. Metric references ğœ† serve to enhance input\nverbal information based onî‰‚.\n3.2. Parallel Multi-modal Commands\nOur multi-modal HRI includes verbal input commands\nî‰‚ andhumanposture îˆ® .Thedetailedmeaningofthelistof\nmultimodal command sequences can be found below.\n3.2.1. Verbal Command Sequence\nVerbal language input is used to compile a set of com-\nmands and metrics due to its intuitive and naturalness. We\ndivideî‰‚ into three types:\nâ€¢ Action command: designated for prospective actions.\nâ€¢ Approval command: used to confirm the location of\nthe selected object.\nâ€¢ Metric references: designed as an optional part of\nhumanintention ğ¼.Thiscommandcouldincludevari-\nablessuchastheangleoftiltforpouringormovement\nvelocity.\nThe mapping îˆ¸ converts all verbal inputs into text and\nexecutes a query task to obtain the action intentionğ¼ğ‘ and\nthe metricğœ†. îˆ¸ is defined as:\n(ğ¼ğ‘, ğœ†)= îˆ¸(î‰‚) (1)\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 4 of 19\nNVP-HRI\n3.2.2. Deictic Posture\nDeictic posture is a specific type of human posture used\nto indicate or point to an object, location, or direction. It\ninvolvesgesturessuchaspointingwithafinger,hand,orarm\nto direct attention and provide context in communication.\nNVP-HRI utilizes the RGBD input to generate the human\nposturesrepresentedby îˆ®.Themappingtoobtainthetarget\nintentionbasedontherepresentationofthestructuralobject\nÎ“from the human postures is denoted as:\nğ¼ğ‘¡ =îˆ¹(îˆ®, Î“) (2)\nThe extended line ğ“ of the right forearm of an operator\nextractedfrom îˆ®representsthedeicticpostureandtheobject\nclosesttothevector ğ“asthetargetintention ğ¼ğ‘¡.Thedistance\nof an object in the structural representationÎ“ğ‘˜ âˆˆ Î“with\nthe centroid position of the 3D groupsğ›¾ğ‘˜ to the vectorğ“is\ndescribed asğ‘‘ğ‘˜(ğ‘™,ğ›¾):\nğ‘‘ğ‘˜ =\n||(ğ“2 âˆ’ğ“1)Ã—( ğ“1 âˆ’ğ›¾ğ‘˜)||\n||ğ“2 âˆ’ğ“1||\n(3)\nWhereğ“1 and ğ“2 are two random points from the vectorğ“.\nByintegratingthemulti-modalHRIcommandsequence\nîˆ¿ and the structural object representationÎ“, we can have\ngoodestimateofthehumanintention ğ¼,whichencompasses\nthe object intentionğ¼ğ‘, action intentionğ¼ğ‘œ, and metric pa-\nrameterğœ†:\nğ¼ â‰œ îˆ´(îˆ¿, Î“) (4)\n=îˆ´(ğ¼ğ‘, ğ¼ğ‘¡, ğœ†)\nInourproposedsystem,weemployGPT4(Brownetal.,\n2020) to decode a multimodal command sequenceîˆ¿ into\nhumanintention ğ¼,denotedbymapping îˆ´.Therobotaction\nsequences ğ‘ are derived from the intention of humansğ¼\nthrough the mapping îˆ­ âˆ¶ ğ‘ = îˆ­(ğ¼). This mapping is\nalsoperformedutilizingGPT4.Finally,theactionsequences\ncross-checkedbythesweptvolumemodelandourproposed\nsystem are used to control the state of the end effectorğ‘(ğ‘¡)\nto fulfill the human intentionğ¼.\n3.3. Construction of Complex Commands\nVarious action commands require various specifications\nfor the intention of the target. Simple tasks specify only the\nintention of the action and have no target intention (e.g.,\nmove to the initial position). For a human intention such\nas pick up this exotic fruit, both the action and the target\nintentions are necessary. In home service scenarios, human\naction intentions often occur in sequences, such aspick up\nthe exotic fruit and give it to me. In order to achieve this\ngoal, any two robotic actions in our multimodal HRI can be\nconstructed together:\nîˆ¿ =(îˆ¿1,îˆ¿2)=( ğ¼ğ‘1, ğ¼ğ‘¡1, ğ¼ğ‘2, ğ¼ğ‘¡2, ğœ†) (5)\nHere, îˆ¿1 and îˆ¿2 denote the individual syntax commands.\nğ¼ğ‘1 and ğ¼ğ‘2 denoted the intention of action of each syntax\ncommand. ğ¼ğ‘¡1 and ğ¼ğ‘¡2 denoted the target intention of each\nsyntax command. NVP-HRI comprises a series of robotic\nactions characterized by increasing complexity:\nâ€¢ Without a clear goal: reset position, drop, move, etc.\nâ€¢ With targeted intention: pick, place, pour, etc.\n4. Methodology\nOur application scenario involves indoor service for the\nelderly, where the robot must perform tasks ordered by hu-\nmansefficientlyandnaturally.Someobjectsinthisscenario\nare rare and may not be recognized by pre-trained models.\nToaddressthis,NVP-HRIincludesvariousmodules,suchas\nverbalcommandunderstanding,zero-shotsemanticsegmen-\ntation by pretrained SAM, deictic posture estimation, action\nsequence generation, and execution.\n4.1. Verbal Commands Understanding\nIn the verbal commands understanding module, the in-\ntention and action metric are estimated from the raw audio\ninputî‰‚.Thismapping îˆ¸ consistsoftwosteps.Thefirststep\nistheconversionofspeechtotext.Thesecondstepinvolves\naquerytasktorecognizetheintentionandtheactionmetric.\nAfter comparing various approaches (Trabelsi et al.,\n2022;AlphaCephei,2023;Radfordetal.,2023),wedecided\nto utilize the VOSK API (Trabelsi et al., 2022) based on the\nfollowingcriteria:(1)Recognitionaccuracy,(2)Recognition\nefficiency,and(3)Capabilitytoidentifyindividualspeakers.\n4.2. Zero-Shot Semantic Segmentation\nThe semantic segmentation module extracts the repre-\nsentation of structural objects in the robot base frame by\nutilizingpre-trainedSAM 1 (Kirillovetal.,2023).Asshown\ninFig.3,theextractedsemanticmaskmaycontainincorrect\nsemantic meanings from the zero-shot prediction, which we\nwill ignore and will never be used in the system. Instead,\nthe texture and appearance clustering and grouping should\nremainrelativelystabletoprovideroughestimatesofobject\ncontours. In a random scene, we may have as many 2D\nclusters as possible. We then use the default calibration\nvalues îˆ·, stored in the RGBD camera, to generate the\nreprojection matrix to project the RGBD 2D pixelğ‘into 3D\npointclouds ğ¶îˆ¼ throughğ¶îˆ¼ =îˆ·ğ‘.Thus,setsof3Dcluster\ncandidates can be obtained in the camera frame. Here, we\nlimitthesizeofinteractableobjectstobewithinthewidthof\nthe manipulator end effector for interactions. Larger objects\nare stored for further scene understanding but are not used\nfor reference object selection. Subsequently, the transform\nmatrixğµî‰€ğ¶ (TsaiandLenz,1989)isperformedtoobtainthe\n3Dpointcloudoftheobjectintherobotbaseframethrough\nğµîˆ¼ =ğµî‰€ğ¶\nğ¶îˆ¼.Where, ğ¶îˆ¼ denotedapointinthepointcloud\nin the camera frame andğµîˆ¼(.)denote a point in the point\ncloud in robot base frame represented asğµîˆ»(ğ‘¥,ğ‘¦,ğ‘§ ). The\nobject positionğ›¾ in the scene is outlined as the centroid of\nthe object point cloud in the robot base frame:\n1Checkpoint:vit_l, Input Size:1024Ã—1024Ã—3\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 5 of 19\nNVP-HRI\nFigure 3: Input and Output of SAM: Out of three objects, only two have correct semantic meanings. However, all of them are\nsegmented correctly.\nğ›¾ğ‘˜ =( 1\nğ‘\nğ‘âˆ‘\nğ‘–=1\nğ‘¥ğ‘˜\nğ‘–, 1\nğ‘\nğ‘âˆ‘\nğ‘–=1\nğ‘¦ğ‘˜\nğ‘–, 1\nğ‘\nğ‘âˆ‘\nğ‘–=1\nğ‘§ğ‘˜\nğ‘–) (6)\nWhere(ğ‘¥ğ‘–,ğ‘¦ğ‘–,ğ‘§ğ‘–)denotes the coordinates of a random point\nwith indexğ‘–in the point cloud of objectğ‘˜in the robot base\nframe. The 3D clusters centroid positionğ›¾ğ‘˜ is then used\nto calculate the distance, as described in Eq. 3. The object\nwidth ğ‘¤, thicknessğ‘‘, and the object heightâ„are defined\nas:\nğ‘¤=max ğ‘¥ğ‘˜ âˆ’min ğ‘¥ğ‘˜ (7)\nâ„=max ğ‘¦ğ‘˜ âˆ’min ğ‘¦ğ‘˜\nğ‘‘ =max ğ‘§ğ‘˜ âˆ’min ğ‘§ğ‘˜\nThe width, denoted byğ‘¤, will be used for the end effector\nopening, whileğ‘¤, ğ‘‘ and â„ together are used for the path\nplanning of the end effector and the cross-check of the\ntrajectory.\n4.3. Deictic Posture Estimation\nThis module is utilized to infer the intended object of\ninterest.Toensurethesystemfunctionality,theupperhalfof\nthe human body must be captured with an RGBD camera at\na sufficient distance, resulting in a larger area of interaction\ncompared to a Leap Motion-based solution. This can be\nconsidered as a hidden benefit of our approach. After that,\nwe utilized OpenPose (Cao et al., 2017) to track 2D human\nskeletons within the RGB image. Subsequently, the aligned\ndepth map is used to estimate the 3D human skeletonsîˆ® in\nthecameraframe.Thedirectionline ğ“oftheextendedright\nforearmoftheoperatoristhenchosenasthedirectionofthe\ndeictic posture.\nThe intended object of interest can be computed in each\nframe in real-time. However, the relevant location becomes\naccessible only when the verbal approval command is iden-\ntified.Theprecisionandrobustnessofthedeicticpostureare\nevaluated in the following chapters.\n4.4. Action Sequence Generation and Execution\nThe translation of human intentionğ¼ into a sequence of\nactionsğ‘forarobotisachievedbythemappingfunction îˆ­.\nIn the context of collision-free action sequence generation,\nan extensive understanding of the operational environment\nand the mission itself is required. To facilitate this process\nnaturally, we need a large language model for backend pro-\ncessing. To do that, we select GPT-4-turbo2 (Brown et al.,\n2020) for several reasons: its ability to encode high-level\nenvironmentalrepresentations,itsabilitytoprocesscomplex\nnatural language commands, its 3D spatial understanding,\nanditseaseofintegrationduetouser-friendlyAPIinterfaces\nand extensive pre-built samples. These features make GPT-\n4-turbo highly adaptable for a variety of scenarios. Acting\nas the central processor, GPT-4 handles multi-modal inputs\nand generates robot action sequences efficiently, ensuring\nsmooth, collision-free operation aligned with human inten-\ntions.\nIn order to address the potential issues of hallucination\nas well as tedious syntax structures in LLM, we formulate\nprompts and impose constraints on the output response\ntokensgeneratedbytheLLM.Theprompt,asshowninFig.\n4, is structured into three separate components:\n1. Action constraints: restricts the usages of actions.\nLLMislimitedtoutilizingtheseactionsforconstruct-\ning robot action sequences.\n2. Trajectory constraints: limit specific requirements for\nthe planning of the real-time trajectory, where it is\nnecessary to define the units and the coordinate axes.\n3. Exampletasks:illustratetheexecutionofsimilartasks\nandprovideguidanceontaskplanningstrategiesusing\nLLM.Byfollowingthisconstraint,LLMwillreplicate\nthe example task.\nAs shown in Fig 4, the GPT-generated plan first recon-\nstructsthestructuralrepresentationofthescenetodetermine\nthepositions,widths,andheightsofthemanipulatedobjects\nand obstacles. Subsequently, the safe height is established\nbased on trajectory constraints, and finally, the action se-\nquences are generated according to action constraints and\n2We use GPT-4-turbo with version 2024-04-09 and the training data\nretrieved in December 2023.\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 6 of 19\nNVP-HRI\nFigure 4: The prompt is divided into action constraints, trajectory constraints, and example tasks, followed by a cross-check.\nCross-check results are fed back to the LLM, and if a collision is detected, a new trajectory is generated. Only sequences that\npass the cross-check are executed by the robot, resembling a closed-loop control system as in classical control theory.\ntrajectory constraints, forming a trajectory awaiting cross-\ncheck mechanism. This trajectory needs to be able to avoid\ncollisions.Theformatoftheactionsequencesisdetermined\nby the examples in the prompt to ensure uniformity.\n4.5. Cross-Check Mechanism\nTo plan a collision-free trajectory, LLM also needs to\nincorporate the spatial relations of objects in the scene\nthrough the structural representation of the objects. To fur-\nther enhance the safety and accuracy of NVP-HRI, the\ngenerated action sequences are subsequently cross-checked.\nThe decision to lift an object or move it horizontally is\nlargely determined by GPT, which operates as a black-box\nsystem with embedded common sense reasoning. In the\ndemo given in Fig. 2, GPT likely chose to lift the object to\navoid the obstacle because other options, such as moving\nin front of or behind the obstacle, might pose risks of\npotentialcollisions,suchaswithoccludedobjectsorswept-\narea collisions. Decision making is context-dependent and\nfullydependsoncommonsense.Inotherscenarios,theGPT\nmight choose a sloped movement to maintain observability\nof the system. This demonstrates that GPT, as a pre-trained\nblack-box model, possesses an inherent level of common\nsense and can adapt to different situations effectively.\nHence,inourproposedsystem,theobjecttobemanipu-\nlated is restricted by the trajectory constraints embedded in\nthepromptsalongthegeneratedtrajectorytocrossobstacles\nfrom above. In the cross-check process, the manipulated\nobject and obstacle are simplified to two rectangular shapes\nbased on their structural representationsÎ“ğ‘€ and Î“ğ‘‚. ğ”¸ =\nâˆªğ‘âˆˆ[ğ‘ğ‘ ,ğ‘ğ‘’]Î“ğ‘€ğœ“(ğ‘) represents the swept volume of the ma-\nnipulated object along the trajectoryğœ“ from the start action\nsequenceğ‘ğ‘  totheendactionsequence ğ‘ğ‘’,asindicatedbyits\nstructuralrepresentation Î“ğ‘€.ğ”¹ =Î“ğ‘‚ denotesthevolumeof\nthe obstacle, as determined by the structural representation\nÎ“ğ‘‚. Ifğ”¸ âˆ©ğ”¹ â‰  âˆ…, then there is a collision in the trajec-\ntory. Under this circumstances, GPT-4 provides feedback\non the generated trajectory, including detailed explanations\nof failure causes, such as the exact location of a collision.\nThisexplanatoryfeedbackhelpsGPT-4adjusteffectivelyin\nsubsequent reasoning. Ifğ”¸ âˆ© ğ”¹ = âˆ…, then the trajectory\nis collision-free. As shown in Fig. 4, the visualization of\nthe collision-free trajectory can be carried out efficiently by\nthe proposed solution. After passing the cross-check, the\nrobot executes the action sequences, resembling a closed-\nloop feedback system as in traditional control theory. The\ncross-check mechanism significantly improves the accuracy\nofcollision-freetrajectorygenerationandensuressafertask\nexecution, increasing the success rate of collision-free tra-\njectories from 73% to 94% compared to direct trajectory\ngeneration.\n4.6. Human-Robot Interaction\nHuman-robot interaction in our proposed NVP-HRI\nis accomplished by multi-modal command sequences, in\nwhich the user communicates intentions through verbal\ncommand sequences and deictic posture. Multi-modal com-\nmands are processed byîˆ´, and visual feedback is provided\nthrough the graphical user interface to improve interaction\naccuracy and robustness.\n1) The user gives an action command by voice, which\nis estimated by îˆ¸ as the user action intention. 2) While\ncommanding the voice, the operator also has to make a\ndeictic posture giving the target object through mapping\nîˆ¹. With the approval command, the user determines the\nselection of the target object. 3) The user can optionally\ngive a metric command, which is estimated byîˆ¸ as the\nmetric parameter for the human intention. In NVP-HRI, the\nusers express intentions using verbal commands and deictic\nposturesviaprocess îˆ´,whilereceivingvisualfeedback(see\nFig. 7) from the graphical user interface.\nTransitioning from multi-modal command sequence to\nhuman intention involves:\n1. User determines verbal action command. If needed, a\nuser selects a target.\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 7 of 19\nNVP-HRI\n2. Systemcalculatestargetobjectusingdeicticposture ğ“\nand 3D object positionğ›¾.\n3. Optionalmetricsreferences ğœ†enhanceroboticmanip-\nulation.\n4. User confirms intention usingfinish character.\nTheseareallverbalinstructionsgiventotheparticipants\nin the experiment.\n5. EXPERIMENT\nThe experiment aims to validate the superiority of our\nproposed NVP-HRI. To achieve this goal, we divide our ex-\nperiment into offsite survey approaches and onsite physical\ntrials. We conduct field offsite surveys to engage as many\npeopleaspossibletocollecttheexpecteduserexperienceto\nreflectintuitivenessandefficiency.Withasetofsurveycan-\ndidates, some agreed to our invitation and performed onsite\nteststoverifyaccuracyandprecision.Thus,wecanperform\nthe quantitative analysis of the proposed HRI framework.\nFurthermore, we verified the robustness of our proposed\nmethodforinteractingwithunknownobjectsandconducted\nperception tests in a real-world environment in an elderly\ncarecentertoshowcasethequalitativevalueoftheproposed\nHRI framework.\n5.1. Environment Setup\nThe experiment environment setup resembles a typi-\ncal living room scenario. An Intel D435i RGBD camera\nis strategically placed to oversee the area, providing clear\nvisibility of humans on the sofa, as well as the UR robot\nmanipulator and table. The RGB output of the camera is\n1024Ã—768Ã—3 , and the depth map is aligned with the RGB\nimage,resultinginanoutputof 1024Ã—768Ã—1.Toadapttothe\ninput requirements of different models (e.g., YOLOv5 and\nSAM), the cameraâ€™s output is resized to the corresponding\ndimensions. At present, we are utilizing the microphone on\nthenotebookforprocessing,asnodedicatedaudiodeviceis\navailable.\nOur system has been tested with 20 participants of dif-\nferent age groups in this environment, as depicted in Fig. 7.\nWe instructed participants to act as if they were unable to\nmove normally and to depend on the robot arm to transport\nitems within their reach. The objective was for the robot\narm to retrieve the necessary items for the participants.\nWhen testing our proposed system, some of these items,\nsuch as charcoal brioche and exotic tropical fruits, were\nnot recognized by the pre-trained object detection model\nor humans. Overall, this creates a realistic environment for\ntesting the HRI system.\n5.2. Description of Experimental Scenarios\nIn the experimental scenarios, a series of manipulation\nexperiments were devised to explore the increasing com-\nplexity of human intention, denoted as\nğ¼ =îˆ´(ğ¼ğ‘1, ğ¼ğ‘¡1, ğ¼ğ‘2, ğ¼ğ‘¡2, ğœ†):\nâˆ™ (â„ğ‘œğ‘šğ‘’, âˆ’, âˆ’, âˆ’, âˆ’),(ğ‘¡â„ğ‘Ÿğ‘œğ‘¤, âˆ’, âˆ’, âˆ’, âˆ’)\nâˆ™ (ğ‘ğ‘–ğ‘ğ‘˜, ğ‘¢ğ‘›ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘›ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡, âˆ’, âˆ’, âˆ’)\nâˆ™ (ğ‘ğ‘–ğ‘ğ‘˜, ğ‘¢ğ‘›ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘›ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡, ğ‘ğ‘¢ğ‘¡, ğ‘ğ‘œğ‘¤ğ‘™,âˆ’)\nâˆ™ (ğ‘ğ‘–ğ‘ğ‘˜, ğ‘ğ‘¢ğ‘, ğ‘ğ‘œğ‘¢ğ‘Ÿ, ğ‘¢ğ‘›ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘›ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡, ğ‘ğ‘›ğ‘”=90â—¦)\nâˆ™ Multisteptasks: addwaterandpass,pourmuesliand\nadd milk.\n5.3. Benchmarking Criteria\nTheprimaryobjectiveistomeasurethedurationofuser\ninteractionwithgeneralobjectsandthesuccessrateforeach\ngiven scenario. The secondary objective is to evaluate the\nuser-friendlinessoftheNVP-HRI.Theperformanceofthese\nscenarios is demonstrated in the provided videos.\nRemark: To allow the existing method to work and to con-\nduct fair benchmarking in efficiency and time analysis, we\nuse a common recognizable item, such as cups and bowls.\nOtherwise, the existing method often cannot work with\nunknown items. The tests with rare objects are conducted\nseparately, as other methods fail to recognize the objects.\nTo minimize order effects during baseline comparisons, we\ntested the accuracy and duration of the interaction of all\nparticipants between the baseline methods and our pro-\nposed method, employing different interaction orders. We\nconducted two-way ANOVA (Analysis of variance) to de-\ntermine the effect of the order of interaction methods and\nthe age group of the participants on the comparison of\neffectiveness and precision.\n5.4. Baseline Selections\nIn our study, we benchmark our approach against other\nstate-of-the-art gesture-based HRI methods (Vanc et al.,\n2023), NLP (Natural Language Processing)-based meth-\nods (Stepputtis et al., 2020), and VLM (Vision Language\nModel)-basedmethods(Constantinetal.,2022).Thesebase-\nlinesarechosenbecauseallthesemethodshaveopensource\nimplementations and are somewhat similar to our approach.\nMost existing multimodal approaches (Stiefelhagen et al.,\n2004; Krupke et al., 2018) do not have open-source source\ncode for comparison, which prevents us from making direct\ncomparisons. The gesture-based (Vanc et al., 2023) utilizes\nafixedpositionleapmotion(Weichertetal.,2013)sensorto\ncapture the structure of the hand bone. The NLP and VLM\nmethods (Stepputtis et al., 2020; Constantin et al., 2022)\nuse natural language sentences to direct the actions of a\nrobot. NLP-based method (Stepputtis et al., 2020) integrate\nword embedding, attention mechanisms, and probabilistic\nreasoningtorecognizeobjectsdescribedinnaturallanguage,\nwhile VLM-based method (Constantin et al., 2022) utilize\nthe CLIP visual-language model (Radford et al., 2021) to\nresolveambiguitiesofobjectselectionandhelpusersformu-\nlate clearer expressions. However, based on feedback from\nparticipantsinexperimentsandsurveys,therearelimitations\nto each of the compatible methods:\n1. Thegesture-basedmethodrequiresuserstoremember\ncomplexgesturescompromisingHRIintuitiveness,as\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 8 of 19\nNVP-HRI\nMore than 30\ngestures\nspeed high low Only common\nnatural language\npick put pour throw home thereï¬nish\nAcï¿½on Approval\nMetric\nangle ninety\n90Â°\nFigure 5: Typical gestures utilized within gesture-based HRI\nsystem (Vanc et al., 2023) with their respective verbal com-\nmands.\nshowninFig.5.Thosearethegesturecommandsthat\nwe ask the participant to memorize\n2. The Leap Motionâ€™s fixed position and limited field of\nviewrequiretheusertomaintaintheirhandabovethe\nsensor, posing challenges for all participants.\n3. The natural language sentences struggle with distin-\nguishingbetweensimilarobjectsindifferentlocations\nand with unknown objects.\n4. VLM-based methods require an interactive dialog in\norder to select a target object and are, therefore, slow\nand unable to discriminate between the same objects.\nAdditional, the object detection modules of the baseline\nmethods we chose for comparison are based on supervised\nmodels(e.g.,FRCNN 3 (Renetal.,2016)andYOLOV5 4 (Li\netal.,2023b)).Methodsrelyingontheprecisepronunciation\nof objects cannot handle unknown objects. Therefore, for\nbaseline comparisons, we compare the duration and accu-\nracy of the interaction using common objects.\n6. RESULTS AND DISCUSSION\nWe evaluated the performance of our proposed NVP-\nHRI through a series of challenging experiments designed\nto assess criteria such as location robustness, accuracy, effi-\nciency, and user experience.\n6.1. Comparison of Effectiveness and Intuitiveness\nOur experimental scenario consisted of two cups with\nthe same shape and different colors, two bowls with the\nsame shape and different colors, and a plate. To evaluate\nthe duration costs of user interaction, we asked participants\nto enter the same commands and intentions using different\nHRIapproachesindifferentorders,includingpickingupone\nof the cups (S1), picking up one of the cups and putting\nit on the plate (S2), and picking up one of the cups and\n3Network: Resnet101, Input Size:1024Ã—768Ã—3\n4Checkpoint: YOLOv5l, Input Size:640Ã—640Ã—3\npick A pick A put in B pick A pour in B\nwith 90 degrees\nScenariaos\n0\n2\n4\n6\n8\n10\n12\n14\n16Interaction Time[s]\nOurs\nGesture-Based\nNLP-Based\nVLM-Based\nFigure 6: Duration of user interaction to enter the same\ncommands as the gesture-based HRI and NLP-based HRI, our\nsystem required less time compared to hand gesture-based HRI\nand NLP-based HRI.\npouring it at a 90-degree angle into one of the bowls (S3).\n24participantswererecruitedfromthelocaluniversity,8of\nwhom were over 51 years old. All participants were fluent\nin English and received verbal instructions on the NVP-\nHRIwithoutundergoingformaltrainingorpracticesessions.\nEach participant completed scenarios S1 through S3 in a\ndistinctorderofinteractionmethods,repeatingeachmethod\nfive times within each scenario. The experiment results,\ndepicted in Fig. 6, show that our system consumes 59.2%\nlesstimethangesture-basedHRI,65.2%lesstimethanNLP-\nbasedHRIand64.8%lesstimethanVLM-basedHRI.This\nexperiment highlights the enhanced efficiency of NVP-HRI\nin interaction within a complex 3D environment.\nWe categorized the participants into three different age\ngroups: those aged 18-35 belonged to the younger group,\nthose aged 36-50 belonged to the middle group, and those\naged 51-65 belonged to the older group. Table 1 shows the\nmeandurationofinteractionforparticipantsindifferentage\ngroups to enter the same command in different scenarios\nwith different HRI methods.\nIn order to further analyse the effect of age and the\neffect of interaction order, a two-way ANOVA is conducted\nto compare the effect of age group and interaction orders\non interaction duration across different scenarios. Table 2\nindicatesthattheorderofinteractionmethodsdoesnothave\na significant effect on the duration of the interaction (ğ‘ƒ >\n0.05). However, for baseline interaction methods including\ngesture-based (Vanc et al., 2023), NLP-based (Stepputtis\net al., 2020), and VLM-based (Constantin et al., 2022),\nage groups have a significant effect on interaction duration\nacross scenarios (ğ‘ƒ < 0.05). Post-hoc comparisons using\nthe Tukey HSD test (Tukey, 1949) show that there are\nsignificant differences in interaction duration between the\nyoungerandoldergroupsandalmostbetweenallmiddleand\nolder groups for all interaction methods from the beginning\nof the study in all scenarios (ğ‘ƒğ‘ğ‘‘ğ‘— < 0.05). In contrast, for\nour proposed method, age groups have no significant effect\non the duration of interaction in different scenarios (ğ‘ƒğ‘ğ‘‘ğ‘— >\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 9 of 19\nNVP-HRI\nTable 1\nInteraction Duration in Different Scenarios for Different Age Groups.\nMethod\nInteraction Duration (ğ‘ ) â†“\nğ• ğ•„ ğ•†\nS1 S2 S3 S1 S2 S3 S1 S2 S3\nGesture\n(Vanc et al., 2023)\n4.93 Â±\n0.84\n9.61 Â±\n0.96\n11.67 Â±\n0.44\n5.13 Â±\n0.32\n9.89 Â±\n0.90\n11.75 Â±\n0.71\n5.92 Â±\n0.82\n11.31 Â±\n0.40\n13.03 Â±\n1.00\nNLP\n(Stepputtis et al., 2020)\n6.63 Â±\n0.41\n11.73 Â±\n0.71\n13.11 Â±\n0.76\n6.87 Â±\n0.60\n12.21 Â±\n0.30\n13.11 Â±\n0.86\n7.97 Â±\n0.54\n13.11 Â±\n0.91\n14.07 Â±\n0.40\nVLM\n(Constantin et al., 2022)\n6.28 Â±\n0.53\n10.94 Â±\n0.89\n15.03 Â±\n0.81\n6.62 Â±\n0.64\n10.87 Â±\n0.52\n14.67 Â±\n0.52\n7.66 Â±\n0.33\n12.22 Â±\n1.07\n16.62 Â±\n0.78\nNVP-HRI\n(Ours)\n2.55 Â±\n0.49\n4.23 Â±\n0.99\n5.12 Â±\n0.88\n2.08 Â±\n0.80\n3.80 Â±\n0.67\n5.01 Â±\n0.80\n2.17 Â±\n0.36\n4.19 Â±\n0.64\n6.14 Â±\n0.44\nğ• : younger age group of 18-35 y.o.,ğ•„: Middle-aged group of 36-50 y.o.,ğ•†: Older age group of 51-65 y.o..\nBest results are inbold. The rest of the paper follows the same naming convention.\nTable 2\nTwo-Way ANOVA and Post-Hoc for Interaction Duration\nScenario Method\nTwo-Way ANOVA Post-Hoc\nAge group Interaction order ğ•„ vs ğ•† ğ•„ vs ğ• ğ•† vs ğ•\nğ¹(2,18)âˆ” ğ‘ƒâ€  ğ¹(3,18) ğ‘ƒ ğ‘ƒ â—¦\nğ‘ğ‘‘ğ‘— rejectâŠ› ğ‘ƒğ‘ğ‘‘ğ‘— reject ğ‘ƒğ‘ğ‘‘ğ‘— reject\nS1\nVLM 15.340 0.000 âˆ— 0.730 0.547 0.001 true 0.392 false 0.001 true\nGesture 4.059 0.035 0.513 0.678 0.089 false 0.838 false 0.028 ture\nNLP 17.716 0.000 âˆ— 1.679 0.206 0.000 âˆ— true 0.633 false 0.000* true\nNVP-HRI 0.713 0.556 1.411 0.269 0.939 false 0.258 false 0.417 false\nS2\nVLM 0.599 0.623 6.144 0.009 0.011 true 0.982 false 0.017 true\nGesture 9.702 0.001 0.467 0.708 0.005 true 0.755 false 0.000 âˆ— true\nNLP 1.507 0.246 9.144 0.002 0.039 true 0.340 false 0.001 true\nNVP-HRI 1.411 0.269 0.712 0.557 0.939 false 0.257 false 0.416 false\nS3\nVLM 0.234 0.971 15.543 0.000 âˆ— 0.000âˆ— true 0.582 false 0.000* true\nGesture 8.662 0.002 0.609 0.617 0.004 true 0.973 false 0.002 true\nNLP 6.259 0.008 2.392 0.102 0.027 true 1.000 false 0.028 true\nNVP-HRI 0.525 0.600 0.405 0.751 0.551 false 0.773 false 0.928 false\nâˆ”ğ¹(2,18)measures the ratio of the variance explained by a factor to the residual variance. Inğ¹(2,18), the numerator degrees of freedom are\n2, the denominator degrees of freedom are18. â€ ğ‘ƒ denotes the level of significant of theğ¹-ratio. ğ‘ƒ <0.05 indicates that the factor has a\nstatistically significant effect.â—¦ğ‘ƒğ‘ğ‘‘ğ‘— denotes the adjustedğ‘ƒ-value, which accounts for multiple comparisons in post-hoc tests\nâŠ›ğ‘Ÿğ‘’ğ‘—ğ‘’ğ‘ğ‘¡ denotes a decision rule based on theğ‘ƒğ‘ğ‘‘ğ‘—.ğ‘Ÿğ‘’ğ‘—ğ‘’ğ‘ğ‘¡ =ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ means no significant difference between the groups.\nâˆ— denotes ğ‘ƒ <0.001.The rest of the paper follows the same naming convention.\n0.05). In conclusion, our results suggest that interaction\nusing complex gestures and long sentences takes more time\nforolderusers,whileourproposedmethodusingonlyafew\ncommon syntaxes is more suitable for older users.\n6.2. Comparison of Precision and Robustness\nIn this experiment, we placed nine groups of unknown\nobjects,specifically18charcoalbriquettesarrangedinpairs,\nonatablewith20cmofseparationbetweenthemtotestHRI\naccuracy, as illustrated in Figure 7. Feedback provided by\nROS RVIZ visually indicates to the user the specific object\nbeing targeted, allowing evaluation of the accuracy of the\nNVP-HRI interaction in a challenging setting.\nThe same 24 participants from the previous experiment\nwere divided into two groups. All participants received\nverbalinstructionsonhowtouseNVP-HRI.Thefirstgroup\ncompleted the given scenarios three times without visual\nfeedback, resulting in a noticeable improvement in system\noperation accuracy after brief learning sessions. The sec-\nond group completed the scenarios with visual feedback,\ndemonstrating that the proposed NVP-HRI could be used\neffectively without formal training.\nTo further compare the accuracy of interaction with\nother baseline methods, we conducted another experiment\nwith participants in different age groups to test the accuracy\nof different interaction methods. Hundreds of trials were\nconducted for each interaction method in each scenario\nduring the experiment, and the results are shown in Fig. 8.\nGesture-based methods perform better than language-based\nmethods because gestures have more tractable reference\npoints. NLP-based and VLM-based methods often require\nmore precise and detailed descriptions and often have chal-\nlenges in discriminating similar objects. Visual feedback\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 10 of 19\nNVP-HRI\nTable 3\nInteraction Accuracy in Different Scenarios for Different Age Groups\nMethod\nInteraction Accuracyâ†‘ (%)\nğ• ğ•„ ğ•†\nS1 S2 S3 S1 S2 S3 S1 S2 S3\nGesture\n(Vanc et al., 2023)\n100Â±\n0.0\n93.4 Â±\n0.8\n86.2 Â±\n2.5\n100Â±\n0.0\n93.0Â±\n1.5\n86.6 Â±\n9.4\n100 Â±\n0.0\n92.3 Â±\n1.7\n85.6 Â±\n1.3\nNLP\n(Stepputtis et al., 2020)\n97.2Â±\n1.2\n78.2 Â±\n1.2\n66.5 Â±\n2.7\n97.3 Â±\n1.4\n78.3 Â±\n1.8\n68.6 Â±\n2.2\n96.3Â±\n0.7\n77.3 Â±\n0.9\n67.6 Â±\n1.7\nVLM\n(Constantin et al., 2022)\n96.3 Â±\n2.6\n41.7 Â±\n1.1\n34.2 Â±\n1.8\n95.6 Â±\n2.1\n43.3Â±\n2.2\n34.3 Â±\n1.3\n96.2 Â±\n2.1\n42.6 Â±\n0.9\n33.9 Â±\n2.3\nNVP-HRI\n(Ours)\n100Â±\n0.0\n95.5 Â±\n0.8\n90.9 Â±\n1.7\n100Â±\n0.0\n95.1 Â±\n1.5\n91.6 Â±\n1.3\n100Â±\n0.0\n94.3 Â±\n1.3\n91.3Â±\n0.9\nBeginners First Try\nNo Visual Feedback\nTry for Third Time\nNo Visual Feedback\nAny User Any Trial\nWith visual Feedback\nTesï¿½ng\narea for\nintuiï¿½veness and \naccuracy\nwith unrecognized object\nSeï¿½ng up mulï¿½ple similar items \nclosely together to simulate \nextreme HRI scenarios.\nTesï¿½ng object: \ncharcoal brioche\n100%\n100% 100% 100%\n100% 100% 100%\n100%100%100%\n100%\n100%\n100%\n90%\n85%\n95%\n90%\n100% 100%100%\n100%\n100% 95%\n65%\n80% 70%\n75%\nAcï¿½on Visual\nfeedback from\ncamera\nvisual\nFeedback\nFigure 7:Experiments to test the accuracy of deictic posture (above) and the deictic posture evaluation results (bottom).\nin the proposed method and the gesture-based method also\nimprove the accuracy of the interaction. Table 3 shows the\nmean interaction accuracy for participants in different age\ngroups to enter the same command in different scenarios\nwith different HRI methods.\nA two-way ANOVA is performed to compare the effect\nof the age group and the interaction orders on the accuracy\nof the interaction in different scenarios. Table 4 indicates\nthat the order of interaction methods and the age group\ndo not have a significant effect on accuracy of interaction\n(ğ‘ƒ >0.05).\n6.3. Field Survey on User Experience\nTo extensively evaluate the proposed solution, a wider\ncommunity is invited to participate in an online survey5.\nOver 2000 invitations were made, and the invitation mode\n5https://www.wjx.cn/vm/tUHPFD1.aspx\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 11 of 19\nNVP-HRI\nFigure 8:Comparison of accuracy of different tasks vs different interaction methods.\nTable 4\nTwo-Way ANOVA of Interaction Accuracy\nScenario Method\nTwo-Way ANOVA\nAge group Test order\nğ¹(2,18) ğ‘ƒ ğ¹ (3,18) ğ‘ƒ\nS1\nVLM 0.278 0.760 3.321 0.063\nGesture 0.422 0.661 2.268 0.115\nNLP 1.449 0.260 0.308 0.818\nNVP-HRI 0.422 0.661 2.268 0.115\nS2\nVLM 2.504 0.109 1.623 0.218\nGesture 1.348 0.284 0.449 0.720\nNLP 1.226 0.316 0.003 0.999\nNVP-HRI 1.624 0.224 0.190 0.901\nS3\nVLM 0.064 0.938 0.298 0.826\nGesture 1.852 0.185 1.848 0.174\nNLP 2.020 0.161 1.922 0.162\nNVP-HRI 0.403 0.673 0.151 0.927\nincludedemailandstreetsurveysconductedwiththetablets\nprovided. The candidates comprised professors, nonfaculty\nstaff,andstudentsfromaroundtheworld.Priorconsentwas\nobtained before asking any survey questions. To ensure the\naccuracy of the delivery of the message, we verified that all\ncandidates were fluent in English. Candidates were asked\nto indicate their gender, age range and nationality, watch\nvideos demonstrating the same commands being entered\nusing gesture-based, NLP-based, VLM-based, and the pro-\nposed HRI method, and then candidates were required to\nrank the HRI methods based on their preference. Finally,\ncandidates were asked to choose the reasons for their rank-\ning. To minimize bias and order effects, the order of all\noptionswasrandomized.Asaresult,only390peopleagreed\nto complete the survey, which is about a 19.5% positive\nresponse rate.Most ofthe positiveresponses arefrom street\ninteviews with people in person. At the end of the survey,\nweinvitedcandidatestoexperiencetheproposedinteraction\nframework in a real-world environment and participate in\nphysical experiments. This is done to check the discrepancy\nbetweenthesurveyandreal-worldexperience.Afterpartici-\npants experienced the realmachine HRI, none of the visited\nparticipants wanted to change the initial survey result. This\nindicates that the survey demo video successfully captures\nthe essence of the experiment.\nTo better show the demographic distribution, Fig. 9\nshows the nationality and age distribution of the survey\nparticipants. The survey results are shown in table 5. The\nresultsindicatethatmorethan97%oftheparticipantswould\nprefer NVP-HRI over other approaches. Combined with the\nsurvey results, participants in the older age groups valued\nthesimplicityandcomprehensibilityoftheinteractionmeth-\nods. Some of the older participants noted that they ranked\ngesture-based interactions last because it was challenging\nfor them to memorize and perform specific gestures. Some\nmiddle-agedandyoungerparticipantspointedouttheineffi-\nciency of the VLM-based method, which requires identify-\ning an object through dialog with the robot. Participants in\nthe younger and middle-aged age groups valued efficiency\nmore.\n6.4. Diverse Real-world Environment Tests\nIn addition, we conducted HRI experiments in various\ncommon scenarios in homes and elderly care centers. In the\nexperiments,wefoundnodifferenceinthedetectionofdeic-\nticposturebetweentheelderlyandtheyoungerparticipants.\nFurthermore, when the lower body was obscured, an accu-\nratedeicticposturecouldstillbeobtained,andHRIaccuracy\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 12 of 19\nNVP-HRI\nTable 5\nHRI Method Preference Survey.\nAge groups NVP-HRI(Ours) NLP Gesture VLM\nğ• ğ•„ ğ•† ğ• ğ•„ ğ•† ğ• ğ•„ ğ•† ğ• ğ•„ ğ•†\nComprehensibility 33 15 19 2 2 0 0 0 0 0 1 0\nEfficiency 114 48 9 0 0 0 0 0 0 0 0 0\nSimplicity 78 11 23 0 0 0 0 0 0 0 0 0\nVogue 27 3 2 1 0 0 2 0 0 0 0 0\nSum 252 77 52 3 2 0 2 0 0 0 1 0\nFigure 9:Nationality and age distribution of participants.\nFigure 10: Interaction experiments in the kitchen and living\nroom showed that YOLO consistently failed to detect novel\nobjects accurately, while our proposed method reliably identi-\nfied these objects through segmentation by SAM and deictic\nposture estimation via OpenPose.\nwasnotaffected.Inreal-worldscenarios,supervisedmodels\nsuch as YOLO (Redmon et al., 2016) often fail to detect\nor incorrectly categorize novel objects, while our proposed\nNVP-HRI accurately segments novel objects through SAM\n(Kirillov et al., 2023). Our system successfully retrieved\nvarious novel daily objects on demand, as shown in Fig. 10.\n7. LIMITATION AND FUTURE WORKS\nCurrently, the proposed method has been tested only\nin laboratory conditions with participants of various age\ngroups. The primary aim of this research is to provide\neasy access for elderly or ill individuals to control robots\nto perform daily routines. However, due to the nature of\nthe camera used in this mode, requests to conduct trials\nin hospitals were denied. Consequently, we have not been\nable to verify the systemâ€™s actual performance in real-world\nsettings.\nA significant limitation of our study is the demographic\nprofileofoursurveyparticipants.Thesurveywasdistributed\ntoindividualsassociatedwithuniversitiesaroundtheworld,\nwhich inherently biases the sample towards those with\nhigher educational backgrounds and better English fluency.\nThis raises questions about the systemâ€™s usability among\nindividuals with lower educational levels and different lin-\nguistic abilities. It is crucial to conduct further testing in\nmore diverse settings to determine whether the system can\nbe used effectively by a broader population.\nAdditionally, another challenge is to test the systemâ€™s\neffectivenessunderlow-lightconditions.Inourexperiments,\npose recognition is a critical component. However, the per-\nformance of pose detection may be compromised in low-\nlight environments. To address this issue, additional testing\nis necessary. We may also need to integrate supplementary\nsensors specifically designed to enhance pose detection in\nlow-light conditions.\nIn summary, while initial laboratory tests of the pro-\nposed method are promising, several aspects require further\ninvestigation. These include verifying the systemâ€™s perfor-\nmance in real-world settings, ensuring its usability among\na diverse population, and enhancing its effectiveness in low\nlight conditions. Addressing these issues will be crucial to\nthe successful deployment of this technology in practical\napplications.\n8. CONCLUSIONS\nIn this work, We introduce NVP-HRI, a multi-modal\nHRI paradigm combining voice commands and gestures,\nenabled by the Segment Anything Model (SAM) for scene\nanalysis. NVP-HRI integrates with a large language model\n(LLM) for real-time trajectory solutions, achieving up to\n65.2% efficiency gains over traditional gesture control. We\nplan to open-source our code and methodology for greater\naccessibility.\nHowever,thecurrentsystemisdesignedonlyforEnglish\nspeakers and excludes those with hearing or speaking im-\npairments. Future work will prioritize integrating gestures\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 13 of 19\nNVP-HRI\nand additional language models to enhance system versatil-\nity.\nCRediT authorship contribution statement\nYuzhiLai: Conceptualization,developmentofMethod-\nology and Software.Shenghai Yuan:Conceptualization of\nthisstudy,Writing-Originaldraftpreparation. YoussefNas-\nsar:Datacuration,Designandmanagementofexperiments,\nValidation, Writing - Original draft preparation.Mingyu\nFan: Data curation, Writing - Original draft preparation.\nThomas Weber:Design and management of experiments,\nWriting-Originaldraftpreparation. MatthiasRÃ¤tsch: Final\napproval of the version to be submitted.\nDeclaration of competing interest\nThe authors declare that they have no known competing\nfinancial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nAcknowledgments\nThis work is supported by a grant of the EFRE and\nMWK ProFÃ¶-R&D program, no. FEIH_ProT_2517820 and\nMWK32-7535-30/10/2. This work is also supported by Na-\ntional Research Foundation, Singapore , under its Medium-\nSized Center for Advanced Robotics Technology Innova-\ntion.\nLicence and Legal Compliance\nOur code will be released under a Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0 International\nLicense, ensuring its use is limited to non-commercial aca-\ndemic research. The proposed NVP-HRI is shared with the\nhopethatitwillsignificantlybenefittheagingcommunityby\nfosteringadvancementsinhuman-robotinteractionresearch.\nThroughout the project, we have rigorously adhered to\nthe Personal Data Protection Act (PDPA) and the General\nData Protection Regulation (GDPR) to protect the privacy\nand personal data of individuals. To the best of our knowl-\nedge, facial masking has been implemented across all data\nto anonymize the information effectively. We have also\nappointed Data Protection Officers to oversee and ensure\ncompliance with both PDPA and GDPR standards.\nThe data is securely hosted on Google Drive, which\nprovides robust security measures to prevent unauthorized\naccess or disclosure. We are committed to maintaining the\nhighest standards of data security and privacy.\nShould any concerns arise regarding anonymization, se-\ncurity, or access issues, we strongly encourage individuals\nto report or request data corrections. Reports or requests\ncan be submitted through the link provided on our GitHub\nrepository,ensuringatransparentandresponsiveprocessfor\naddressing any potential issues.\nData Availability\nData will be made available upon request. Interested\nparties can contact us to access the data, ensuring that its\nusealignswiththestipulatednon-commercialandacademic\nresearch purposes. This process ensures that data sharing\nis controlled and compliant with our licensing and privacy\ncommitments.\nReferences\nAlonso-MartÃ­n, F., Castro-GonzÃ¡lez, A., Malfaz, M.,\nCastillo,J.C.,andSalichs,M.A.(2017). Identification\nand distance estimation of users and objects by means\nofelectronicbeaconsinsocialrobotics. ExpertSystems\nwith Applications, 86:247â€“257.\nAlpha Cephei (Accessed November 9, 2023). Vosk home-\npage. Retrieved fromhttps://alphacephei.com/vosk/.\nBai, R., Yuan, S., Guo, H., Yin, P., Yau, W.-Y., and Xie, L.\n(2024). Multi-robot active graph exploration with re-\nducedpose-slamuncertaintyviasubmodularoptimiza-\ntion. In 2024 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 10229â€“\n10236.\nBian,G.-B.,Chen,Z.,Li,Z.,Wei,B.-T.,Liu,W.-P.,daSilva,\nD. S., Wu, W.-Q., and de Albuquerque, V. H. C.\n(2023). Learning surgical skills under the rcm con-\nstraintfromdemonstrationsinrobot-assistedminimally\ninvasive surgery. Expert Systems with Applications,\n225:120134.\nBoykov, Y. and Kolmogorov, V. (2004). An experimental\ncomparisonofmin-cut/max-flowalgorithmsforenergy\nminimization in vision.IEEE transactions on pattern\nanalysis and machine intelligence, 26(9):1124â€“1137.\nBrown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell,A.,etal.(2020). Languagemodelsarefew-shot\nlearners. Advances in neural information processing\nsystems, 33:1877â€“1901.\nCao, H., Xu, Y., Yang, J., Yin, P., Ji, X., Yuan, S., and\nXie, L. (2024a). Reliable spatial-temporal voxels for\nmulti-modaltest-timeadaptation. In Proceedingsofthe\nEuropean Conference on Computer Vision (ECCV).\nCao, H., Xu, Y., Yang, J., Yin, P., Yuan, S., and Xie, L.\n(2023a). Multi-modal continual test-time adaptation\nfor 3d semantic segmentation. InProceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion, pages 18809â€“18819.\nCao, H., Xu, Y., Yang, J., Yin, P., Yuan, S., and Xie,\nL. (2024b). Mopa: Multi-modal prior aided domain\nadaptationfor3dsemanticsegmentation. In 2024IEEE\nInternational Conference on Robotics and Automation\n(ICRA).\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 14 of 19\nNVP-HRI\nCao,K.,Cao,M.,Yuan,S.,andXie,L.(2022).Direct:Adif-\nferential dynamic programming based framework for\ntrajectory generation. IEEE Robotics and Automation\nLetters, 7(2):2439â€“2446.\nCao, M., Lyu, Y., Yuan, S., and Xie, L. (2020). Online\ntrajectorycorrectionandtrackingforfacadeinspection\nusing autonomous uav. In 2020 IEEE 16th Interna-\ntional Conference on Control & Automation (ICCA),\npages 1149â€“1154. IEEE.\nCao, M., Xu, X., Yuan, S., Cao, K., Liu, K., and Xie, L.\n(2023b). Doublebee:Ahybridaerial-groundrobotwith\ntwo active wheels. In 2023 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 6962â€“6969. IEEE.\nCao,Z.,Simon,T.,Wei,S.-E.,andSheikh,Y.(2017). Real-\ntimemulti-person2dposeestimationusingpartaffinity\nfields. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7291â€“\n7299.\nCaruccio,L.,Cirillo,S.,Polese,G.,Solimando,G.,Sundara-\nmurthy,S.,andTortora,G.(2024). Canchatgptprovide\nintelligent diagnoses? a comparative study between\npredictive models and chatgpt to define a new medical\ndiagnostic bot. Expert Systems with Applications,\n235:121186.\nChen, S., Liu, K., Wang, C., Yuan, S., Yang, J., and Xie, L.\n(2024). Salient sparse visual odometry with pose-only\nsupervision. IEEE Robotics and Automation Letters,\n9(5):4774â€“4781.\nConstantin, S., Eyiokur, F. I., Yaman, D., BÃ¤rmann, L., and\nWaibel,A.(2022). Interactivemultimodalrobotdialog\nusing pointing gesture recognition. In Karlinsky, L.,\nMichaeli,T.,andNishino,K.,editors, ComputerVision\n- ECCV 2022 Workshops - Tel Aviv, Israel, October\n23-27, 2022, Proceedings, Part VI, volume 13806 of\nLecture Notes in Computer Science, pages 640â€“657.\nSpringer.\nDeng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,\nL. (2009). Imagenet: A large-scale hierarchical image\ndatabase. In2009IEEEconferenceoncomputervision\nand pattern recognition, pages 248â€“255. Ieee.\nDeng, L., Yang, J., Yuan, S., Zou, H., Lu, C. X., and Xie, L.\n(2022). Gaitfi:Robustdevice-freehumanidentification\nviawifiandvisionmultimodallearning. IEEEInternet\nof Things Journal, 10(1):625â€“636.\nDeng, T., Chen, Y., Zhang, L., Yang, J., Yuan, S., Liu, J.,\nWang, D., Wang, H., and Chen, W. (2024a). Compact\n3d gaussian splatting for dense visual slam. arXiv\npreprint arXiv:2403.11247.\nDeng, T., Wang, N., Wang, C., Yuan, S., Wang, J., Wang,\nD., and Chen, W. (2024b). Incremental joint learning\nof depth, pose and implicit scene representation on\nmonocularcamerainlarge-scalescenes. arXivpreprint\narXiv:2404.06050.\nDepierre,A.,DellandrÃ©a,E.,andChen,L.(2018). Jacquard:\nA large scale dataset for robotic grasp detection. In\n2018IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems (IROS), pages 3511â€“3516. IEEE.\nEnde, T., Haddadin, S., Parusel, S., WÃ¼sthoff, T., Hassen-\nzahl, M., and Albu-SchÃ¤ffer, A. (2011). A human-\ncentered approach to robot gesture based communi-\ncation within collaborative working processes. In\n2011IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems, pages 3367â€“3374. IEEE.\nEsfahani,M.A.,Wang,H.,Bashari,B.,Wu,K.,andYuan,S.\n(2021). Learningtoextractrobusthandcraftedfeatures\nwith a single observation via evolutionary neurogene-\nsis. Applied Soft Computing, 106:107424.\nEsfahani, M. A., Wang, H., Wu, K., and Yuan, S. (2020).\nUnsupervised scene categorization, path segmentation\nand landmark extraction while traveling path. In2020\n16thInternationalConferenceonControl,Automation,\nRobotics and Vision (ICARCV), pages 190â€“195. IEEE.\nEsfahani, M. A., Wu, K., Yuan, S., and Wang, H. (2019a).\nDeepdsair: Deep 6-dof camera relocalization using de-\nblurredsemantic-awareimagerepresentationforlarge-\nscale outdoor environments. Image and Vision Com-\nputing, 89:120â€“130.\nEsfahani, M. A., Wu, K., Yuan, S., and Wang, H. (2019b).\nTowards utilizing deep uncertainty in traditional slam.\nIn 2019 IEEE 15th International Conference on Con-\ntrol and Automation (ICCA), pages 344â€“349.\nFeng, B., Jiang, X., Li, B., Zhou, Q., and Bi, Y. (2024). An\nadaptive multi-rrt approach for robot motion planning.\nExpert Systems with Applications, 252:124281.\nGamboa-Montero, J. J., Alonso-Martin, F., Marques-\nVillarroya, S., Sequeira, J., and Salichs, M. A. (2023).\nAsynchronous federated learning system for humanâ€“\nrobot touch interaction.Expert Systems with Applica-\ntions, 211:118510.\nGarrett,C.R.,Lozano-Perez,T.,andKaelbling,L.P.(2018).\nPddlstream: Integrating symbolic planners and black-\nbox samplers via optimistic adaptive planning. In\nInternational Conference on Automated Planning and\nScheduling.\nGonÃ§alves, H. R. and Santos, C. P. (2023). Deep learning\nmodel for doors detection: A contribution for context-\nawareness recognition ofpatients with parkinsonâ€™s dis-\nease. Expert Systems with Applications, 212:118712.\nGroÃŸmann, B., Pedersen, M. R., Klonovs, J., Herzog, D.,\nNalpantidis,L.,andKrÃ¼ger,V.(2014).Communicating\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 15 of 19\nNVP-HRI\nunknown objects to robots through pointing gestures.\nIn Mistry, M., Leonardis, A., Witkowski, M., and Mel-\nhuish, C., editors,Advances in Autonomous Robotics\nSystems, pages 209â€“220, Cham. Springer International\nPublishing.\nGu, X., Lin, T., Kuo, W., and Cui, Y. (2022). Open-\nvocabulary object detection via vision and language\nknowledge distillation. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nHameed, K., Chai, D., and Rassau, A. (2022). Score-based\nmaskedgeimprovementofmask-rcnnforsegmentation\nof fruit and vegetables.Expert Systems with Applica-\ntions, 190:116205.\nHu,T.,Yuan,S.,Bai,R.,andXu,X.(2025). Sweptvolume-\naware trajectory planning and mpc tracking for multi-\naxle swerve-drive amrs. In 2025 IEEE International\nConference on Robotics and Automation (ICRA).\nHuang,W.,Wang,C.,Zhang,R.,Li,Y.,Wu,J.,andFei-Fei,\nL. (2023). Voxposer: Composable 3d value maps for\nrobotic manipulation with language models. In Tan,\nJ., Toussaint, M., and Darvish, K., editors,Conference\non Robot Learning, CoRL 2023, 6-9 November 2023,\nAtlanta, GA, USA, volume 229 ofProceedings of Ma-\nchine Learning Research, pages 540â€“562. PMLR.\nJain, S. and Argall, B. (2018). Recursive bayesian hu-\nman intent recognition in shared-control robotics. In\n2018IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems (IROS), pages 3905â€“3912. IEEE.\nJeon, H., Kim, D.-W., and Kang, B.-Y. (2024). Deep\nreinforcement learning for cooperative robots based\non adaptive sentiment feedback.Expert Systems with\nApplications, 243:121198.\nJi,T.,Yuan,S.,andXie,L.(2022). Robustrgb-dslamindy-\nnamic environments for autonomous vehicles. In2022\n17thInternationalConferenceonControl,Automation,\nRobotics and Vision (ICARCV), pages 665â€“671. IEEE.\nJi, X., Yuan, S., Li, J., Yin, P., Cao, H., and Xie, L. (2024a).\nSgba: Semantic gaussian mixture model-based lidar\nbundle adjustment. IEEE Robotics and Automation\nLetters.\nJi, X., Yuan, S., Yin, P., and Xie, L. (2024b). Lio-gvm: an\naccurate, tightly-coupled lidar-inertial odometry with\ngaussian voxel map. IEEE Robotics and Automation\nLetters, 9(3):2200â€“2207.\nJin, T., Xu, X., Yang, Y., Yuan, S., Nguyen, T.-M., Li, J.,\nand Xie, L. (2025). Robust loop closure by textual\ncues in challenging environments.IEEE Robotics and\nAutomation Letters, 10(1):812â€“819.\nKhennouche, F., Elmir, Y., Himeur, Y., Djebari, N., and\nAmira, A. (2024). Revolutionizing generative pre-\ntraineds: Insights and challenges in deploying chatgpt\nand generative chatbots for faqs.Expert Systems with\nApplications, 246:123224.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,\nGustafson, L., Xiao, T., Whitehead, S., Berg, A. C.,\nLo, W.-Y., et al. (2023). Segment anything. InPro-\nceedingsoftheIEEE/CVFInternationalConferenceon\nComputer Vision, pages 4015â€“4026.\nKrupke, D., Steinicke, F., Lubos, P., Jonetzko, Y., GÃ¶rner,\nM., and Zhang, J. (2018). Comparison of multimodal\nheading and pointing gestures for co-located mixed\nreality human-robot interaction. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and\nSystems (IROS), pages 1â€“9.\nLai, Y., Yuan, S., Nassar, Y., Fan, M., Gopal, A., Yorita,\nA., Kubota, N., and RÃ¤tsch, M. (2025). Natural mul-\ntimodal fusion-based humanâ€“robot interaction: Appli-\ncationwithvoiceanddeicticposturevialargelanguage\nmodel. IEEE Robotics & Automation Magazine, pages\n2â€“11.\nLei, A., Deng, T., Wang, H., Yang, J., and Yuan, S. (2025).\nAudio array-based 3d uav trajectory estimation with\nlidar pseudo-labeling. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP). IEEE.\nLi, G., Cao, H., Liu, M., Yuan, S., and Yang, J. (2024a).\nGera:Geometricembeddingforefficientpointregistra-\ntion analysis.arXiv preprint arXiv:2410.00589.\nLi, J., Leng, Q., Liu, J., Xu, X., Jin, T., Cao, M., Nguyen,\nT.-M.,Yuan,S.,Cao,K.,andXie,L.(2024b). Helmet-\nposer: A helmet-mounted imu dataset for data-driven\nestimationofhumanheadmotionindiverseconditions.\narXiv preprint arXiv:2409.05006.\nLi, J., Nguyen, T.-M., Yuan, S., and Xie, L. (2024c). Pss-\nba: Lidar bundle adjustment with progressive spatial\nsmoothing. In 2024 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages\n1124â€“1129.\nLi, J., Yuan, S., Cao, M., Nguyen, T.-M., Cao, K., and\nXie, L. (2024d). Hcto: Optimality-aware lidar inertial\nodometrywithhybridcontinuoustimeoptimizationfor\ncompact wearable mapping system.ISPRS Journal of\nPhotogrammetry and Remote Sensing, 211:228â€“243.\nLi, Q. and Yuan, S. (2024). Jacquard v2: Refining datasets\nusingthehumanintheloopdatacorrectionmethod. In\n2024 IEEE International Conference on Robotics and\nAutomation (ICRA).\nLi, Y.-K., Meng, Q.-H., Wang, Y.-X., and Hou, H.-R.\n(2023a). Mmfn: Emotion recognition by fusing touch\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 16 of 19\nNVP-HRI\ngesture and facial expression information.Expert Sys-\ntems with Applications, 228:120469.\nLi,Z.,Xu,B.,Wu,D.,Zhao,K.,Chen,S.,Lu,M.,andCong,\nJ. (2023b). A yolo-ggcnn based grasping framework\nfor mobile robots in unknown environments.Expert\nSystems with Applications, 225:119993.\nLiao, Y., Li, J., Kang, S., Li, Q., Zhu, G., Yuan, S., Dong,\nZ., and Yang, B. (2023). Se-calib: Semantic edge-\nbased lidarâ€“camera boresight online calibration in ur-\nban scenes. IEEE Transactions on Geoscience and\nRemote Sensing, 61:1â€“13.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., DollÃ¡r, P., and Zitnick, C. L. (2014).\nMicrosoft coco: Common objects in context. InCom-\nputer Visionâ€“ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceed-\nings, Part V 13, pages 740â€“755. Springer.\nLiu,R.,Xu,X.,Yuan,S.,andXie,L.(2025). Handleobject\nnavigation as weighted traveling repairman problem.\narXiv preprint arXiv:2503.06937.\nLu, S., Yoon, Y., and Feng, A. (2023). Co-speech gesture\nsynthesis using discrete gesture token learning. In\n2023IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems (IROS), pages 9808â€“9815.\nMa, Y., Xu, J., Yuan, S., Zhi, T., Yu, W., Zhou, J., and Xie,\nL. (2024). Mm-lins: a multi-map lidar-inertial system\nfor over-degenerate environments.IEEE Transactions\non Intelligent Vehicles, -:1â€“11.\nMahmood, T., Cho, S. W., and Park, K. R. (2022). Dsrd-\nnet: Dual-stream residual dense network for semantic\nsegmentation of instruments in robot-assisted surgery.\nExpert Systems with Applications, 202:117420.\nNguyen, T. H., Yuan, S., and Xie, L. (2023). Vr-slam:\nA visual-range simultaneous localization and mapping\nsystem using monocular camera and ultra-wideband\nsensors. arXiv preprint arXiv:2303.10903.\nNguyen,T.-M.,Yang,Y.,Nguyen,T.-D.,Yuan,S.,andXie,\nL. (2024a). Uloc: Learning to localize in complex\nlarge-scale environments with ultra-wideband ranges.\narXiv preprint arXiv:2409.11122.\nNguyen, T.-M., Yuan, S., Nguyen, T. H., Yin, P., Cao, H.,\nXie, L., Wozniak, M., Jensfelt, P., Thiel, M., Ziegen-\nbein,J.,etal.(2024b). Mcd:Diverselarge-scalemulti-\ncampus dataset for robot perception. InProceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 22304â€“22313.\nPark, K.-B., Choi, S. H., and Lee, J. Y. (2024). Self-\ntraining based augmented reality for robust 3d object\nregistration and task assistance.Expert Systems with\nApplications, 238:122331.\nQi, Z., Yuan, S., Liu, F., Cao, H., Deng, T., Yang, J., and\nXie, L. (2024). Air-embodied: An efficient active\n3dgs-based interaction and reconstruction framework\nwith embodied large language model.arXiv preprint\narXiv:2409.16019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., et al. (2021). Learning transferable visual models\nfrom natural language supervision. In International\nconference on machine learning, pages 8748â€“8763.\nPMLR.\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey,\nC.,andSutskever,I.(2023). Robustspeechrecognition\nvia large-scale weak supervision. In International\nConferenceonMachineLearning ,pages28492â€“28518.\nPMLR.\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A.\n(2016). You only look once: Unified, real-time object\ndetection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 779â€“\n788.\nRen, S., He, K., Girshick, R., and Sun, J. (2016). Faster\nr-cnn: Towards real-time object detection with region\nproposalnetworks. IEEEtransactionsonpatternanal-\nysis and machine intelligence, 39(6):1137â€“1149.\nRen, Z., Meng, J., and Yuan, J. (2011). Depth camera\nbased hand gesture recognition and its applications in\nhuman-computer-interaction. In20118thinternational\nconference on information, communications & signal\nprocessing, pages 1â€“5. IEEE.\nRother,C.,Kolmogorov,V.,andBlake,A.(2004).\"grabcut\"\ninteractive foreground extraction using iterated graph\ncuts.ACMtransactionsongraphics(TOG) ,23(3):309â€“\n314.\nShah, S. H. H., Karlsen, A. S. T., Solberg, M., and Hameed,\nI. A. (2024). An efficient and lightweight multiper-\nson activity recognition framework for robot-assisted\nhealthcare applications. Expert Systems with Applica-\ntions, 241:122482.\nSingh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D.,\nTremblay, J., Fox, D., Thomason, J., and Garg, A.\n(2023). Progprompt: Generating situated robot task\nplans using large language models. In 2023 IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 11523â€“11530.\nSkrzypek, A., Panfil, W., Kosior, M., and Przystaika, P.\n(2019). Control system shell of mobile robot with\nvoice recognition module. In2019 12th International\nWorkshop on Robot Motion and Control (RoMoCo),\npages 191â€“196.\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 17 of 19\nNVP-HRI\nStepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C.,\nand Ben Amor, H. (2020). Language-conditioned imi-\ntation learning for robot manipulation tasks.Advances\nin Neural Information Processing Systems, 33:13139â€“\n13150.\nStiefelhagen, R., Fugen, C., Gieselmann, R., Holzapfel, H.,\nNickel, K., and Waibel, A. (2004). Natural human-\nrobot interaction using speech, head pose and ges-\ntures. In 2004 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) (IEEE Cat.\nNo.04CH37566), volume 3, pages 2422â€“2427 vol.3.\nTadewos, T. G., Newaz, A. A. R., and Karimoddini, A.\n(2022). Specification-guided behavior tree synthesis\nandexecutionforcoordinationofautonomoussystems.\nExpert Systems with Applications, 201:117022.\nTang, X., Zhang, J., Qi, Y., Liu, K., Li, R., and Wang, H.\n(2024). A spatial filter temporal graph convolutional\nnetwork for decoding motor imagery eeg signals.Ex-\npert Systems with Applications, 238:121915.\nTrabelsi, A., Warichet, S., Aajaoun, Y., and Soussilane, S.\n(2022). Evaluation of the efficiency of state-of-the-\nart speech recognition engines. Procedia Computer\nScience, 207:2242â€“2252.\nTsai, R. and Lenz, R. (1989). A new technique for fully\nautonomous and efficient 3d robotics hand/eye calibra-\ntion. IEEE Transactions on Robotics and Automation,\n5(3):345â€“358.\nTukey, J. W. (1949). Comparing individual means in the\nanalysis of variance.Biometrics, pages 99â€“114.\nVanc, P., Behrens, J. K., Stepanova, K., and Hlavac, V.\n(2023). Communicating human intent to a robotic\ncompanion by multi-type gesture sentences. In\n2023IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems (IROS), pages 9839â€“9845.\nVemprala, S., Bonatti, R., Bucker, A., and Kapoor, A.\n(2023). Chatgpt for robotics: Design principles and\nmodelabilities. MicrosoftAuton.Syst.Robot.Res ,2:20.\nWang, H., Mou, X., Mou, W., Yuan, S., Ulun, S., Yang,\nS., and Shin, B.-S. (2015). Vision based long range\nobject detection and tracking for unmanned surface\nvehicle. In 2015 IEEE 7th International Conference\nonCyberneticsandIntelligentSystems(CIS)andIEEE\nConferenceonRobotics,AutomationandMechatronics\n(RAM), pages 101â€“105. IEEE.\nWang, H., Yuan, S., and Wu, K. (2017). Heterogeneous\nstereo: A human vision inspired method for general\nroboticssensing. InTENCON2017-2017IEEERegion\n10 Conference, pages 793â€“798. IEEE.\nWang, K., Wang, Y., Zhang, S., Tian, Y., and Li, D. (2022).\nSlms-ssd: Improving the balance of semantic and spa-\ntial information in object detection. Expert Systems\nwith Applications, 206:117682.\nWang,Y.-X.,Meng,Q.-H.,Li,Y.-K.,andHou,H.-R.(2024).\nTouch-text answer for human-robot interaction via su-\npervised adversarial learning. Expert Systems with\nApplications, 242:122738.\nWeber,D.,Santini,T.,Zell,A.,andKasneci,E.(2020). Dis-\ntilling location proposals of unknown objects through\ngaze information for human-robot interaction. In\n2020IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems (IROS), pages 11086â€“11093.\nWeichert, F., Bachmann, D., Rudak, B., and Fisseler, D.\n(2013). Analysis of the accuracy and robustness of the\nleap motion controller.Sensors, 13(5):6380â€“6393.\nWu, K., Abolfazli Esfahani, M., Yuan, S., and Wang, H.\n(2018). Learn to steer through deep reinforcement\nlearning. Sensors, 18(11):3650.\nWu, K., Esfahani, M. A., Yuan, S., and Wang, H. (2019a).\nDepth-based obstacle avoidance through deep rein-\nforcement learning. In Proceedings of the 5th In-\nternational Conference on Mechatronics and Robotics\nEngineering, pages 102â€“106.\nWu, K., Esfahani, M. A., Yuan, S., and Wang, H. (2019b).\nTdpp-net: Achieving three-dimensional path planning\nvia a deep neural network architecture.Neurocomput-\ning, 357:151â€“162.\nWu, K., Wang, H., Esfahani, M. A., and Yuan, S. (2019c).\nBnd*-ddqn:Learntosteerautonomouslythroughdeep\nreinforcement learning. IEEE Transactions on Cogni-\ntive and Developmental Systems, 13(2):249â€“261.\nWu, K., Wang, H., Esfahani, M. A., and Yuan, S. (2021).\nLearn to navigate autonomously through deep rein-\nforcement learning. IEEE Transactions on Industrial\nElectronics, 69(5):5342â€“5352.\nXu, J., Yu, W., Huang, S., Yuan, S., Zhao, L., Li, R., and\nXie, L. (2024a). M-divo: Multiple tof rgb-d cameras-\nenhanced depthâ€“inertialâ€“visual odometry.IEEE Inter-\nnet of Things Journal, 11(23):37562â€“37570.\nXu, K., Jiang, Z., Cao, H., Yuan, S., Wang, C., and Xie, L.\n(2024b).Anefficientscenecoordinateencodingandre-\nlocalizationmethod. arXivpreprintarXiv:2412.06488 .\nXu, X., Cao, M., Yuan, S., Nguyen, T. H., Nguyen, T.-M.,\nandXie,L.(2024c).Acost-effectivecooperativeexplo-\nration and inspection strategy for heterogeneous aerial\nsystem. InProceedingsofthe2024IEEEInternational\nConference on Control and Automation (ICCA), pages\n673â€“678. IEEE.\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 18 of 19\nNVP-HRI\nYang, J., Huang, H., Zhou, Y., Chen, X., Xu, Y., Yuan, S.,\nZou, H., Lu, C. X., and Xie, L. (2024a). Mm-fi: Multi-\nmodal non-intrusive 4d human dataset for versatile\nwireless sensing. Advances in Neural Information\nProcessing Systems, 36.\nYang, Y., Yuan, S., Cao, M., Yang, J., and Xie, L. (2023).\nAv-pedaware: Self-supervised audio-visual fusion for\ndynamic pedestrian awareness. In 2023 IEEE/RSJ\nInternational Conference on Intelligent Robots and\nSystems (IROS), pages 1871â€“1877. IEEE.\nYang, Y., Yuan, S., and Xie, L. (2022). Overcoming\ncatastrophic forgetting for semantic segmentation via\nincremental learning. In2022 17th International Con-\nference on Control, Automation, Robotics and Vision\n(ICARCV), pages 299â€“304. IEEE.\nYang, Y., Yuan, S., Yang, J., Nguyen, T. H., Cao, M.,\nNguyen, T.-M., Wang, H., and Xie, L. (2024b). Av-\nfdti:Audio-visualfusionfordronethreatidentification.\nJournalofAutomationandIntelligence ,3(3):144â€“151.\nYang, Z., Xu, K., Yuan, S., and Xie, L. (2024c). A fast and\nlight-weight noniterative visual odometry with rgb-d\ncameras. Unmanned Systems, pages 1â€“13.\nYin, P., Cao, H., Nguyen, T.-M., Yuan, S., Zhang, S., Liu,\nK.,andXie,L.(2024). Outram:One-shotgloballocal-\nization via triangulated scene graph and global outlier\npruning. In 2024 IEEE International Conference on\nRoboticsandAutomation(ICRA) ,pages13717â€“13723.\nIEEE.\nYin, P., Yuan, S., Cao, H., Ji, X., Zhang, S., and Xie, L.\n(2023). Segregator: Global point cloud registration\nwith semantic and geometric cues. In 2023 IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 2848â€“2854. IEEE.\nYuan, S. and Wang, H. (2014). Autonomous object level\nsegmentation. In 2014 13th International Conference\non Control Automation Robotics & Vision (ICARCV),\npages 33â€“37.\nYuan, S.,Yang, Y., Nguyen,T. H., Nguyen,T.-M., Yang, J.,\nLiu,F.,Li,J.,Wang,H.,andXie,L.(2024). Mmaud:A\ncomprehensive multi-modal anti-uav dataset for mod-\nernminiaturedronethreats.In 2024IEEEInternational\nConferenceonRoboticsandAutomation(ICRA) ,pages\n2745â€“2751.\nZhang, C., Chen, J., Li, J., Peng, Y., and Mao, Z. (2023).\nLarge language models for human-robot interaction: A\nreview. Biomimetic Intelligence and Robotics, page\n100131.\nZhang, T., Chu, H., Zou, Y., and Sun, H. (2024). A robust\nelectromyography signals-based interaction interface\nfor human-robot collaboration in 3d operation scenar-\nios. Expert Systems with Applications, 238:122003.\nZhao, X., Li, M., Weber, C., Hafez, M. B., and Wermter,\nS. (2023). Chat with the environment: Interactive\nmultimodalperceptionusinglargelanguagemodels. In\n2023IEEE/RSJInternationalConferenceonIntelligent\nRobots and Systems (IROS), pages 3590â€“3596.\nZhou, Y., Huang, H., Yuan, S., Zou, H., Xie, L., and Yang,\nJ. (2023). Metafi++: Wifi-enabled transformer-based\nhumanposeestimationformetaverseavatarsimulation.\nIEEEInternetofThingsJournal ,10(16):14128â€“14136.\nYuzhi Lai, et al.:Preprint submitted to Elsevier Page 19 of 19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7735905647277832
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6414074301719666
    },
    {
      "name": "Robot",
      "score": 0.6162513494491577
    },
    {
      "name": "Humanâ€“robot interaction",
      "score": 0.5553926229476929
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.5368808507919312
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5080499649047852
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.4850591719150543
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47943323850631714
    },
    {
      "name": "Computer vision",
      "score": 0.4466102123260498
    },
    {
      "name": "Speech recognition",
      "score": 0.3207583427429199
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": []
}