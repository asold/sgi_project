{
  "title": "SocioProbe: What, When, and Where Language Models Learn about Sociodemographics",
  "url": "https://openalex.org/W4385567161",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5022688200",
      "name": "Anne Lauscher",
      "affiliations": [
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A5060377481",
      "name": "Federico Bianchi",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5112713734",
      "name": "Samuel R. Bowman",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5084505122",
      "name": "Dirk Hovy",
      "affiliations": [
        "Bocconi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4386576828",
    "https://openalex.org/W2942160782",
    "https://openalex.org/W3034685497",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W2807186229",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3167831019",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2152460337",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W3035509916",
    "https://openalex.org/W2252241921",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W2963879260",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W3034458735",
    "https://openalex.org/W2971344868",
    "https://openalex.org/W2741937156",
    "https://openalex.org/W3102812725",
    "https://openalex.org/W3166035876",
    "https://openalex.org/W2805572053",
    "https://openalex.org/W2251263615",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3122898154",
    "https://openalex.org/W3198757395",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2740994861",
    "https://openalex.org/W3034273309",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W9292421",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W4205276337",
    "https://openalex.org/W3167227435",
    "https://openalex.org/W2759869292",
    "https://openalex.org/W3214314807",
    "https://openalex.org/W3034987021",
    "https://openalex.org/W3213468647",
    "https://openalex.org/W4205945417",
    "https://openalex.org/W3185980407",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3101004475",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3100532709",
    "https://openalex.org/W2154819126",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W3155744586",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2952638532",
    "https://openalex.org/W1541105045",
    "https://openalex.org/W4221167694",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2964235839",
    "https://openalex.org/W4205537036",
    "https://openalex.org/W2251409655",
    "https://openalex.org/W3172917028",
    "https://openalex.org/W3177487519",
    "https://openalex.org/W3035305735"
  ],
  "abstract": "Pre-trained language models (PLMs) have outperformed other NLP models on a wide range of tasks. Opting for a more thorough understanding of their capabilities and inner workings, researchers have established the extend to which they capture lower-level knowledge like grammaticality, and mid-level semantic knowledge like factual understanding. However, there is still little understanding of their knowledge of higher-level aspects of language. In particular, despite the importance of sociodemographic aspects in shaping our language, the questions of whether, where, and how PLMs encode these aspects, e.g., gender or age, is still unexplored. We address this research gap by probing the sociodemographic knowledge of different single-GPU PLMs on multiple English data sets via traditional classifier probing and information-theoretic minimum description length probing. Our results show that PLMs do encode these sociodemographics, and that this knowledge is sometimes spread across the layers of some of the tested PLMs. We further conduct a multilingual analysis and investigate the effect of supplementary training to further explore to what extent, where, and with what amount of pre-training data the knowledge is encoded. Our overall results indicate that sociodemographic knowledge is still a major challenge for NLP. PLMs require large amounts of pre-training data to acquire the knowledge and models that excel in general language understanding do not seem to own more knowledge about these aspects.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7901–7918\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nSocioProbe: What, When, and Where Language Models Learn\nabout Sociodemographics\nAnne Lauscher1, Federico Bianchi2, Samuel Bowman3, and Dirk Hovy4\n1Data Science Group, University of Hamburg, Germany\n2StanfordNLP, Stanford University, CA, USA\n3New York University, NY , USA\n4MilaNLP, Bocconi University, Milan, Italy\nanne.lauscher@uni-hamburg.de, fede@stanford.edu,\nbowman@nyu.edu, dirk.hovy@unibocconi.it\nAbstract\nPre-trained language models (PLMs) have out-\nperformed other NLP models on a wide range\nof tasks. Opting for a more thorough under-\nstanding of their capabilities and inner work-\nings, researchers have established the extend\nto which they capture lower-level knowledge\nlike grammaticality, and mid-level semantic\nknowledge like factual understanding. How-\never, there is still little understanding of their\nknowledge of higher-level aspects of language.\nIn particular, despite the importance of sociode-\nmographic aspects in shaping our language, the\nquestions of whether, where, and how PLMs\nencode these aspects, e.g., gender or age, is\nstill unexplored. We address this research\ngap by probing the sociodemographic knowl-\nedge of different single-GPU PLMs on multi-\nple English data sets via traditional classifier\nprobing and information-theoretic minimum\ndescription length probing. Our results show\nthat PLMs do encode these sociodemographics,\nand that this knowledge is sometimes spread\nacross the layers of some of the tested PLMs.\nWe further conduct a multilingual analysis and\ninvestigate the effect of supplementary train-\ning to further explore to what extent, where,\nand with what amount of pre-training data the\nknowledge is encoded. Our overall results indi-\ncate that sociodemographic knowledge is still\na major challenge for NLP. PLMs require large\namounts of pre-training data to acquire the\nknowledge and models that excel in general\nlanguage understanding do not seem to own\nmore knowledge about these aspects.\n1 Introduction\nWhen talking to somebody, we consciously choose\nhow to represent ourselves, and we have a mental\nmodel of who our conversational partner is. At\nthe same time, our language is littered with sub-\nconscious clues about our sociodemographic back-\nground that we cannot control (e.g., our age, edu-\ncation, regional origin, social class, etc). People\nuse this information as an integral part of language,\nto better reach their audience, and to understand\nwhat they are saying (e.g., Trudgill, 2000). In other\nwords, we use sociodemographic knowledge to\ndecide what to say (are we talking to a child or\nan adult, do I want to sound smart or relatable?)\nBut do pre-trained language models (PLMs) have\nknowledge about sociodemographics?\nOver the last years, PLMs like BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019) have\nachieved superior performance on a wide range of\ndownstream tasks (e.g., Wang et al., 2018, 2019,\ninter alia). Accordingly, they have become the\nde facto standard for most NLP tasks. Conse-\nquently, many researchers have tried to shed light\non PLMs’ inner workings (cf. “Bertology”; Ten-\nney et al., 2019; Rogers et al., 2020). They have\nsystematically probed the models’ capabilities to\nunveil which language aspects their internal repre-\nsentations capture. In particular, researchers have\nprobed lower-level structural knowledge (e.g., He-\nwitt and Manning, 2019; Sorodoc et al., 2020; Chi\net al., 2020; Pimentel et al., 2020, inter alia), as\nwell as mid-level knowledge, e.g., lexico-semantic\nknowledge (e.g., Vuli´c et al., 2020; Beloucif and\nBiemann, 2021), and PLMs’ factual understand-\ning (e.g., Petroni et al., 2019; Zhong et al., 2021).\nWhile these aspects are relatively well explored, we\nstill know little about higher-level knowledge of\nPLMs: only a few works have attempted to quantify\ncommon sense knowledge in the models (Petroni\net al., 2019; Lin et al., 2020). Probing of other\nhigher-level aspects still remains underexplored –\nhindering targeted progress in advancing human-\nlike natural language understanding.\nAs recently pointed out by Hovy and Yang\n(2021), sociodemographic aspects play a central\nrole in language. However, they remain underex-\nplored in NLP, despite promising initial findings\n(e.g., V olkova et al., 2013; Hovy, 2015; Lynn et al.,\n2017). Importantly, we are not aware of any re-\nsearch assessing sociodemographic knowledge in\n7901\nPLMs. This lack is extremely surprising given the\navailability of resources, and the importance of\nthese factors in truly understanding language.\nContributions. Acknowledging the importance\nof sociodemographic factors in language, we ad-\ndress a research gap by proposing SOCIO PROBE ,\na novel perspective of probing PLMs for sociode-\nmographic aspects. We demonstrate our approach\nalong two dimensions, (binary) gender and age,\nusing two established data sets, and with different\nwidely-used easily-downloadable PLMs that can\nbe run on a single GPU. To ensure validity of our\nfindings, we combine “traditional” classifier prob-\ning (Petroni et al., 2019) and information-theoretic\nminimum distance length (MDL) probing (V oita\nand Titov, 2020). Our experiments allow us to\nanswer a series of research questions. We find\nthat PLMs do represent sociodemographic knowl-\nedge, but that it is acquired in the later stages.\nThis knowledge is also decoupled from overall\nperformance: some models that excel in general\nlanguage understanding do still not have more\nknowledge about sociodemographics encoded. We\nhope that this work inspires more research on\nthe social aspects of NLP. Our research code is\npublicly available at https://github.com/\nMilaNLProc/socio-probe.\n2 Research Questions\nWe pose five research questions (RQs):\nRQ1: To what extend do current PLMs encode\nsociodemographic knowledge?Do these models\n“know” about the existence and impact of sociode-\nmographic aspects like age or gender on down-\nstream tasks, as repeatedly shown (e.g., V olkova\net al., 2013; Hovy, 2015; Benton et al., 2017)?\nWe probe different versions of the RoBERTa (Liu\net al., 2019) and DeBERTa (He et al., 2021b,a)\nmodel families. Our findings reveal the varying\nextent to which sociodemographic knowledge is\nencoded in different textual domains. Surprisingly,\nthe superior performance of the DeBERTa model\non general NLU tasks is not reflected in the encod-\ning of sociodemographic knowledge.\nRQ2: How much pre-training data is needed to\nacquire sociodemographic knowledge? Are so-\nciodemographic aspects present in any data sample,\nor are they only learned with sufficient amounts of\ndata? Inspired by Zhang et al. (2021b), we use a\nsuite of MiniBERTas (Warstadt et al., 2020) and\nRoBERTa base trained on different amounts of\ndata (1M to 30B). Our results show that sociode-\nmographic knowledge is learned much more slowly\nthan syntactic knowledge and the gains do not seem\nto flatten with more training data. This indicates\nthat large data portions are needed to acquire so-\nciodemographic knowledge.\nRQ3: Where is sociodemographic knowledge lo-\ncated in the models?Sociodemographic aspects\ninfluence a wide range of NLP tasks, both at a\ngrammatical level (e.g., part-of-speech tagging\nGarimella et al. (2019)) and at a pragmatic level\n(e.g., machine translation Hovy et al. (2020); Saun-\nders and Byrne (2020)). But where do these factors\nreside themselves in the model? By probing differ-\nent layers of the PLM with SOCIO PROBE , we find\nthat sociodemographic knowledge is located in the\nhigher layers of most PLMs. This finding is in-line\nwith the intuition that higher-level semantic knowl-\nedge is encoded in higher layers of the models (e.g.,\nTenney et al., 2019). However, on some data sets,\nsome of the models show an opposite trend and\nthe differences across the layers seem much less\npronounced than for a lower-level control task, in\nwhich we predict linguistic acceptability.\nRQ4: Does the localization of sociodemographic\nknowledge in multilingual models differ?Differ-\nent languages provide different linguistic ways of\nexpressing sociodemographic (and other) aspects:\nsome lexically, some syntactically (Johannsen et al.,\n2015). Do PLMs that have been exposed to multi-\nple languages store sociodemographic knowledge\ndifferently than monolingual models? We probe\nmultilingual models and demonstrate that the re-\nsults are in-line with the findings from RQ3. Thus,\nthe localization of the sociodemographic knowl-\nedge in the multilingual versions does not seem to\ndiffer from their monolingual counterparts.\nRQ5: What is the effect of different supplemen-\ntary training tasks on the knowledge encoded in\nthe PLMs’ features?Phang et al. (2018) demon-\nstrated that through supplementary training on\nintermediate-labeled tasks (STILTs), the perfor-\nmance for downstream tasks can be improved. We\nhypothesize that such sequential knowledge trans-\nfer can activate sociodemographics in PLMs, as\nthese aspects can act as useful signals, e.g., for\nsentiment analysis (Hovy, 2015). However, our\nexperiments show that specifically the sociodemo-\ngraphic knowledge in the last layers of the models\nis overwritten through our STILTs procedures.\n7902\nOverall, the encoding of sociodemographic\nknowledge is still a major challenge for NLP:mod-\nels that excel in NLU do not have more knowl-\nedge about sociodemographics, learning curves\ndo not flatten with more pretraining data, the\nknowledge is much less located than for other\ntasks, and learning from other tasks is difficult.\n3 Related Work\nProbing PLMs. The success of large PLMs has\nled to researchers developing a range of meth-\nods (e.g., Hewitt and Liang, 2019; Torroba Henni-\ngen et al., 2020) and data sets (e.g., Warstadt et al.,\n2020; Hartmann et al., 2021) for obtaining a better\nunderstanding of PLMs. In turn, those approaches\nalso challenge these paradigms (e.g., Pimentel\net al., 2020; Ravichander et al., 2021). The most\nstraightforward probing approach relies on training\nclassifiers (e.g., Petroni et al., 2019) to probe mod-\nels’ knowledge. In contrast, other probing mech-\nanisms are substractive (Cao et al., 2021), intrin-\nsic (Torroba Hennigen et al., 2020), or rely on con-\ntrol tasks (Hewitt and Liang, 2019). A popular fam-\nily is information theoretic probing (e.g., Pimentel\net al., 2020; Pimentel and Cotterell, 2021), like\nminimum description length (MDL) probing (V oita\nand Titov, 2020). We use MDL complementarily\nto traditional probing to further substantiate our\nclaims. Most authors have focused on probing En-\nglish language models (e.g., Conneau et al., 2018;\nLiu et al., 2021; Wu and Xiong, 2020; Koto et al.,\n2021, inter alia), but some have moved into the\nmultilingual space (e.g., Ravishankar et al., 2019;\nKurfalı and Östling, 2021; Shapiro et al., 2021), or\nprobed multimodal models (e.g., Prasad and Jyothi,\n2020; Hendricks and Nematzadeh, 2021).\nResearchers have used probing to understand\nwhether PLMs encode knowledge about several as-\npects of language, and to which extent: researchers\nhave probed PLMs for syntactic knowledge (e.g.,\nHewitt and Manning, 2019; Sorodoc et al., 2020),\nlexical semantics (Vuli´c et al., 2020; Beloucif and\nBiemann, 2021), factual knowledge (Heinzerling\nand Inui, 2021; Petroni et al., 2019; Zhong et al.,\n2021), and common sense aspects (Lin et al., 2020)\nor domain-specific knowledge (Jin et al., 2019; Pan-\ndit and Hou, 2021; Wu and Xiong, 2020). De-\nspite this plethora of works, the sociodemographic\nknowledge remains underexplored.\nNLP and Sociodemographic Aspects. Our lan-\nguage use varies depending on the characteristics\nof the sender and receiver(s), e.g., their age and gen-\nder (Eckert and McConnell-Ginet, 2013; Hovy and\nYang, 2021). Accordingly, researchers in NLP have\nexplored these variations (Rosenthal and McKe-\nown, 2011; Blodgett et al., 2016) and showed that\nsociodemographic factors influence model perfor-\nmance (e.g., V olkova et al., 2013; Hovy, 2015).\nSince then, many researchers have argued that such\nfactors should be taken into account for human-\ncentered NLP (Flek, 2020), and showed gains from\nsociodemographic adaptation (e.g., Lynn et al.,\n2017; Yang and Eisenstein, 2017; Li et al., 2018).\nOther researchers have exploited the tie between\nlanguage and demographics to profile authors from\ntheir texts (Burger et al., 2011; Nguyen et al., 2014;\nLjubeši´c et al., 2017; Martinc and Pollak, 2018).\nIn this work, we do not develop methods to predict\ndemographic aspects, but use this task as a proxy to\nhow well sociodemographic knowledge is encoded\nin our models. Another line of research has worked\non detecting and removing unfair stereotypical bias\ntowards demographic groups from PLMs (Blod-\ngett et al., 2020; Shah et al., 2020), e.g., gen-\nder bias (May et al., 2019; Lauscher and Glavaš,\n2019; Webster et al., 2020; Lauscher et al., 2021).\nMost recently and closest to our work, Zhang et al.\n(2021a)1 investigate the sociodemographic bias of\nPLMs. They compare the PLMs cloze predictions\nwith answers given by crowd workers belonging to\ndifferent sociodemographic groups. However, they\ndo not provide further insights of the nature of this\nknowledge nor how when and where it is encoded.\nOur work unequivocally establishes that PLMs con-\ntain sociodemographic knowledge, and shows how\nit is likely acquired, and where it resides.\n4 SocioProbe\nWe describe SOCIO PROBE , which we employ to\nexplore the sociodemographic knowledge PLMs\ncontain. Guided by the availability of data sets,\nwe focus on the dimensions of gender and age.\nNote, however, that our overall methodology can be\neasily extended to other sociodemographic aspects.\n4.1 Data\nWe probe sociodemographic aspects on two data\nsets. They vary in terms of text length and domain.\n1Note that their interest is in linguistically determined\nlanguage varieties of social groups, i.e., sociolects, whereas\nwe focus on the interplay between individual demographic\naspects that go across language varieties: we can express\ngender independent of whether we speak in dialect or standard\nlanguage.\n7903\nDataset Name Textual Domain Dimension Label # Instances % Instances\nTrustpilot Product Reviews\nGender Man 5349 49.97\nWoman 50.03\nAge Young 5269 52.19\nOld 47.80\nRTGender\nFacebook Posts (Congress Members) Gender Man 510135 75.16\nWoman 24.84\nFacebook Posts (Public Figures) Gender Man 133,017 33.38\nWoman 66.62\nFitocracy Posts Gender Man 318,535 54.54\nWoman 45.46\nReddit Posts Gender Man 1,453,512 79.02\nWoman 20,98\nTable 1: Datasets with dimensions, number of instances (# Instances), and label distributions (% Instances).\nTrustpilot (Hovy, 2015). Trustpilot2 is an inter-\nnational user review platform. The data consists of\nthe review texts (including the rating, which we do\nnot use in this work), as well as the self-identified\ngender and age of the author. Following the origi-\nnal paper, we do not consider users from 35 to 45\nto reduce possible errors due to noisy boundaries.\nWe use the split introduced by Hovy et al. (2020),\nand focus on the English portion of the data set.\nFor age, we use Young for users under the age of\n35, and Old for people above the age of 45.\nRTGender (Voigt et al., 2018). We use all texts\nof the data set from three different social media\nplatforms: Reddit, 3 Facebook (posts from politi-\ncians and public figures),4 and Fitocracy.5 Our true\nlabel corresponds to the gender of the author. In to-\ntal, the data set consists of 2,415,199 instances. For\nour experiments, we subsample 20,000 samples for\neach domain to start from equally-sized portions.\n4.2 Probing Methodology\nWe combine two probing methodologies: tradi-\ntional classifier probing and MDL probing.\nTraditional Classifier Probing. The traditional\napproach to PLM probing is to place a simple classi-\nfier – the probe – on top of the frozen features (e.g.,\nEttinger et al., 2016; Adi et al., 2016, inter alia).\nIn our case, following Tenney et al. (2019) and\nZhang et al. (2021b), we use a simple two-layer\nfeed-forward network (with rectified linear unit as\nthe activation function) with a softmax output layer.\nWe feed it the average hidden representations of\n2https://www.trustpilot.com\n3https://www.reddit.com\n4https://www.facebook.com\n5https://www.fitocracy.com\nthe PLM’s Transformer. We take care to only aver-\nage over the representations of the text and ignore\nspecial tokens. We report the F1 measure.\nMinimum Description Length Probing. Tradi-\ntional classifier probing has been criticized for its\nreliance on the complexity of the probe (Hewitt\nand Liang, 2019; V oita and Titov, 2020). To ensure\nvalidity of our results, we thus combine classifier\nprobing with an information theoretic approach.\nConcretely, we use MDL (V oita and Titov, 2020).\nThe intuition behind MDL is that the more informa-\ntion is encoded, the less data is needed to describe\nthe labels given the representations. As in the im-\nplementation of the online code estimation setting,\nwe partition the data into 11 non-overlapping por-\ntions representing 0%, 0.1%, 0.2%, 0.4%, 0.8%,\n1.6%, 3.2%, 6.25%, 12.5%, 25%, 50%, and 100%\nof the full data sets with tnumbers of examples\neach: {(xj,yj)}ti\nj=ti−1+1 for 1 ≤i≤11. Next, we\ntrain a classifier on each portion iand compute the\nLoss Lon the next portion i+ 1. The codelength\ncorresponds to the sum of the resulting 10 losses\nplus the codelength of the first data portion:\nMDL = t1 log2 2 −\n10∑\ni=1\nLi+1 , (1)\nwith t1 as the number of training examples in the\nfirst portion of the data set. A lower MDL value\nindicates more expressive features.\n5 Experiments\nWe describe our experiments.\n5.1 General Experimental Setup\nAll our experiments follow roughly the same ex-\nperimental setup: For the Trustpilot data, we use\n7904\n(a) Classic probing\n(b) MDL probing\nFigure 1: Results for RQ1. We compare differ-\nent RoBERTa and DeBERTa models (RoBERTabase,\nRoBERTalarge, DeBERTabase, DeBERTalarge, De-\nBERTa v3xsmall, small, base, and large) for (a) classic\nand (b) MDL probing. We report average and standard\ndeviation of the F1-scores for 5 runs across 7 tasks.\nthe standard splits provided in Hovy (2015). On\nall other data sets, described in Section 4.1 we\napply a standard split, with 80% of the data for\ntraining, 10% for validation, and 10% for testing\nthe models. We train all our models in batches of\n32 with a learning rate of 1e-3 using the Adam\noptimizer (Kingma and Ba, 2015) (using default\nparameters from pytorch). We apply early stopping\nbased on the validation set loss with a patience of 5\nepochs. If the loss does not improve for an epoch,\nwe reduce the learning rate by 50 %. We conduct\nall experiments 5 times with different random ini-\ntializations of the probes and report the mean and\nthe standard deviation of the performance scores.\nFor all models, we use versions available on Hug-\ngingface and we provide links to all models and\ncode bases used in the Supplementary Materials.\n5.2 RQ1: To what extend do PLMs encode\nsociodemographic knowledge?\nAs initial base experiment, we want to establish\nhow well sociodemographic knowledge can be pre-\ndicted from the features of different PLMs.\nApproach. We test the features extracted from\nRoBERTa (Liu et al., 2019) inbase and large con-\nfiguration in comparison to DeBERTa (He et al.,\n2021b) in xsmall, small, base, and large configura-\ntion. For DeBERTa, different versions are available\nin the Huggingface repository. We use the original\nmodel as well as the v3 version (He et al., 2021a) of\nbase and large. The v3 employs ELECTRA-style\npre-training with gradient disentangled embedding\nsharing (Clark et al., 2020) leading to improve-\nments across all GLUE tasks.6\nResults. Figure 1 shows the results. Generally,\nthe trends in the different models are consistent\nacross the two different probing approaches (clas-\nsic probing and MDL probing). Therefore, we\nconclude the validity of our approach. The diffi-\nculty of the different data sets varies: the “easiest”\ntask is our control task CoLA, in which we probe\nlower-level syntactic knowledge. The next-easiest\ntask is to predict the gender in Facebook posts of\npublic figures (fb_wiki, e.g., 80.68 % average F1\nscore for RoBERTalarge). In contrast, predicting\nthe gender of Facebook posts of congress members\nis relatively difficult for the models (fb_congress,\ne.g., 57.32 % average F1 score). This is in line with\nthe findings of V oigt et al. (2018): depending on\nthe domain of text, the sociodemographic aspects\nof authors are reflected to varying degrees (here:\nless so in more formal settings). Interestingly, we\ncan not confirm the overall superiority of the De-\nBERTa models. While the DeBERTa v3base and\nlarge models outperform RoBERTa on CoLA by\na large margin (6.93 percentage points difference\nbetween RoBERTa large and DeBERTa large, as\nper He et al., 2021a), RoBERTa large seems to en-\ncode sociodemographic knowledge similarly well\nas DeBERTa large, or to an even larger extent. The\nsame observation holds when comparing DeBERTa\nversions. This finding warrants further investiga-\ntion into how different training regimes affect the\nencoding of higher-level knowledge.\n6https://github.com/microsoft/DeBERTa#\nfine-tuning-on-nlu-tasks\n7905\n# Tokens Costs ($) CO 2 (lbs) µGain\n1M 50 5.825 –\n10M 500 58.250 +2.61\n100M 5,075 582.500 +1.98\n1B 20,320 2,330.000 +0.30\n30B 609,600 69,900.000 +8.56\nTable 2: Results of our cost-benefit analysis. We show\nfinancial costs (Costs ($)) and CO 2 emissions (CO2\n(lbs)), gain is average F1-measure increase over the ext\nsmaller model across all data sets and models (µGain).\n5.3 RQ2: How much pre-training data is\nneeded to acquire sociodemographic\nknowledge?\nWe test models trained on varying amounts of data.\nApproach. We use the suite of MiniBER-\nTas (Warstadt et al., 2020),12 RoBERTa-like mod-\nels, which have been trained on 1M, 10M, 100M,\nand 1B words, respectively. 7 The data was ran-\ndomly sampled from a corpus similar to the original\nBERT (Devlin et al., 2019). Pretraining data con-\nsisted of the English Wikipedia and Smashwords,\nwhich is similar to the BookCorpus (Zhu et al.,\n2015). The size of the model trained on the smallest\nportion (1M) is medium small (6 layers, 8 attention\nheads, hidden size of 512). The other models were\ntrained with the base configuration (12 layers, 12\nattention heads, hidden size of 768). For each size,\n3 checkpoints are available (the ones which yielded\nlowest validation perplexity), trained with differ-\nent hyperparameters. In comparison, we probe the\noriginal RoBERTa inbase configuration (Liu et al.,\n2019), trained on approximately 30B words.\nResults. The results for the classic and MDL\nprobing are depicted in Figures 2a and 2b, respec-\ntively. Across all data sets, the sociodemographic\nclassification improves with more pretraining data.\nThe learning curves in the classic probing do not\nflatten out, which indicates the potential for more\nresearch on the topic. We conclude that with more\npretraining data, more sociodemographic knowl-\nedge is present in the features. This finding con-\ntrasts with other lower-level tasks, such as syntac-\ntic knowledge (Zhang et al., 2021b; Pérez-Mayos\net al., 2021). As we hypothesized, though, it is simi-\nlar to other higher-level language aspects, like com-\nmon sense knowledge. Our control task, CoLA,\nexactly reflects this trend: the learning curve of\n7Publicly available at https://huggingface.co/\nnyu-mll/roberta-med-small-1M-1\n(a) Classic probing\n(b) MDL probing\nFigure 2: Classic and MDL probing results for\nRoBERTa models trained on varying amounts (1M–30B\nwords) of pre-training data (RQ2).\npredicting linguistic acceptability is much steeper.\nCost-benefit Analysis. Inspired by Pérez-Mayos\net al. (2021), we conduct a cost-benefit analysis.\nThe authors base their estimate on the costs pro-\nvided in Strubell et al. (2019). We follow their\napproach,8 and approximate the financial costs\nof training a model with $60,948 / 30B words *\n#TrainingWords for each of the MiniBERTas, and\nthe CO2 emissions of each MiniBERTa model as\n6,990 lbs / 30B * #TrainingWords. The final cost\nneeds to be scaled with the number of pre-training\nprocedures needed for model optimization reported\nby Warstadt et al. (2020) (10 times for the 1B\nMiniBERTa, 25 times for the other MiniBERTa\nmodels). In contrast to Pérez-Mayos et al. (2021),\nwe also include RoBERTa base in our analysis\nand scale the costs accordingly. Table 2 shows the\n8Note, that these are presumably a overestimates, as hard-\nware has been getting cheaper and more power efficient.\n7906\n(a) Trustpilot (Age)\n (b) Trustpilot (Gender)\n (c) Facebook Congress\n(d) Facebook Wiki\n (e) Fitocracy\n (f) Reddit\n (g) CoLA\nFigure 3: Layer-wise F1-scores (average and standard deviation) for DeBERTa original and v3large and base and\nRoBERTalarge and base across 5 runs and 7 tasks ((a) Trustpilot Age to (g) CoLA).\n(a) Trustpilot (Age)\n (b) Trustpilot (Gender)\n (c) Facebook Congress\n(d) Facebook Wiki\n (e) Fitocracy\n (f) Reddit\n (g) CoLA\nFigure 4: Results for our analysis of multilingual models (RQ4). We show F1-scores (average and standard\ndeviation) across 5 runs on 7 tasks ((a) Trustpilot (Age) to (g) CoLA). The features we probe are extracted from\ndifferent layers of XLM-RoBERTalarge, XLM-RoBERTabase, and mDeBERTabase.\ncost estimates and expected performance improve-\nments. Between 1M and 1B tokens the expected\ngains flatten (see previous analysis), while the gains\nare lower than the ones reported by Pérez-Mayos\net al. (2021) for syntactic tasks. However, with\n30B we can expect a large performance improve-\nment indicating that higher performance can only\nbe expected at even higher financial and environ-\nmental costs. Given that the already high baseline\ncosts, such a development is ethically problematic.\nOur results support the need for more research on\nsustainable NLP, especially when tasks require in-\ndepth language understanding.\n5.4 RQ3: Where is sociodemographic\nknowledge located?\nWe test embeddings extracted from different layers.\nApproach. In the previous experiments, we fol-\nlowed the standard approach and pooled represen-\ntations from the last layer of the Transformer. In\ncontrast, here we test the average pooled representa-\ntions from each layer n∈[1 :num_layers], where\n7907\n(a) Trustpilot (Age)\n (b) Trustpilot (Gender)\n (c) Facebook Congress\n(d) Facebook Wiki\n (e) Fitocracy\n (f) Reddit\n (g) CoLA\nFigure 5: Results of our STILTs analysis (RQ5) in terms of F1 scores (average and standard deviation) for 5 runs\nacross 7 tasks ((a) Trustpilot Age to (g) CoLA). The features we probe are extracted from the original RoBERTa\nlarge, and RoBERTalarge models, which we obtain from fine-tuning RoBERTa on 6 tasks: Part-of-Speech Tagging\n(POS Tagging), Named Entity Recognition (NER), Linguistic Acceptability Prediction (Ling. Acc.), Natural\nLanguage Inference (NLI), Question Answering (QA), and Sentiment Analysis (SA).\nnum_layers corresponds to the number of layers in\nthe model. We test RoBERTa and DeBERTa (origi-\nnal and v3) in the large and base configurations.\nResults. We show the results in Figures 3a–3g.\nNote the relatively high standard deviations com-\npared to the overall performance range. The ex-\nception to this observation is again CoLA, our\ncontrol task (Figure 3g). The tendency seems to\nbe that higher layers offer better representations\nfor sociodemographic classification (e.g., Trustpi-\nlot (Age) in Figure 3a, Reddit (Gender) in Fig-\nure 3f), but performance improvement across lay-\ners is much more skewed for CoLA than for the\nsociodemographic probing tasks. Especially for\nDeBERTa v3 base, the probing results are often\nbetter for lower model layers (e.g., Figure 3c). This\nruns counter to Tenney et al. (2019), who showed\nhigher-level semantic knowledge to be encoded in\nthe higher layers of BERT. We conclude that so-\nciodemographic knowledge is much less localized\nin PLMs than lower-level knowledge. This finding\ncorresponds to the observation that different so-\nciodemographic factors are expressed in different\nways (Johannsen et al., 2015). As in our experi-\nments for answering RQ1, DeBERTalarge v3 has\nsuperior knowledge about lower-level linguistic as-\npects, but not sociodemographic knowledge.\n5.5 RQ4: Does the sociodemographic\nknowledge in multilingual models differ?\nHung et al. (2022) recently showed that straight-\nforward attempts to (socio)demographic adaptation\nof multilingual Transformers can lead to a better\nseparation of representation areas according to in-\nput text languages and not according to author de-\nmographics. This leads us to question whether the\nmultilingual signal significantly affects the encod-\ning of sociodemographic knowledge in the models.\nWe probe multilingual PLMs for their encoding of\nsociodemographics in English and further validate\nour previous findings.\nApproach. We use multilingual versions of\nRoBERTa and DeBERTa available on Huggingface:\nXLM-RoBERTa in large and base configuration\nand mDeBERTa v3 inbase configuration.9\nResult. We only show the classic probing re-\nsults (Figure 4, see Appendix for MDL). While\nthe scores are slightly lower than for monolingual\nPLMs, they are generally in-line with our findings\nfrom RQ2: sociodemographic knowledge is less\nlocalized than that for CoLA. While for XLM-\nRoBERTa large and base higher layers encode\nthe sociodemographics, the results of DeBERTa\nv3 base show an opposite trend. We conclude\nthat the localization of sociodemographic knowl-\n9No large configuration of mDeBERTa was available\n7908\nedge in multilingual models follows their mono-\nlingual counterparts. The layer-wise behavior of\nDeBERTa v3 base is an additional pointer to the\neffect of the training regimes on the sociodemo-\ngraphic knowledge encoded in PLMs.\n5.6 RQ5: What is the effect of STILTs on the\nencoding of the knowledge?\nWe explore the effect of STILTs on the sociodemo-\ngraphic knowledge encoded in the layers.\nApproach. We use the encoders of readily fine-\ntuned RoBERTa large models from the Hugging-\nface repository trained on the following tasks and\ndata sets: POS tagging and dependency parsing on\nUPOS, named entity recognition, natural language\ninference on MNLI, question answering on SQuaD\nv.2., sentiment analysis on SST2, and linguistic\nacceptability prediction on our control task CoLA.\nResults. See Figure 5 for the classic probing re-\nsults (MDL probing results in the Appendix). Un-\nsurprisingly, supplementary training on Linguistic\nAcceptability Prediction (=our control task CoLA),\nleads to superior representations for CoLA prob-\ning (Figure 5g). This effect is clearly visible from\nlayer 5 onwards, indicating that the top 19 layers\nbecome specialized during the STILTs fine-tuning.\nIn contrast, the selected STILTs tasks do not im-\nprove the sociodemographic knowledge in the rep-\nresentations (e.g., QA STILTs for gender predic-\ntion in Trustpilot) or even reduce that knowledge\n(e.g., NLI and SA STILTs for gender prediction\nin FB Wiki (Figure 5d)). The results suggest that\nsociodemographic knowledge is overwritten dur-\ning STILTs. Interestingly, this effect mostly occurs\non the last 5 to 10 layers (e.g., Trustpilot Age pre-\ndiction from layer 10 (Figure 5a), and much more\ngently than the CoLA improvement.\n6 Conclusion\nSociodemographic aspects shape our language and\nare thus important factors to model in language\ntechnology. However, despite a plethora of works\nprobing PLMs for various types of knowledge, we\nknow little about these higher-level aspects of lan-\nguage. We present SOCIO PROBE to understand\nwhether, when, and where PLMs encode sociode-\nmographic knowledge in their representations. We\nfind that sociodemograophic knowledge is located\nin PLMs, but much more diffuse than lower-level\naspects. In the future, we will extend our analysis\nto languages other than English. We hope that our\nfindings will fuel further research towards human-\nlike language understanding.\nAcknowledgements\nThe work of Anne Lauscher is funded under the\nExcellence Strategy of the German Federal Govern-\nment and the Länder. This work is in part funded\nby the European Research Council (ERC) under\nthe European Union’s Horizon 2020 research and\ninnovation program (grant agreement No. 949944,\nINTEGRATOR). At the time of writing, AL, FB,\nand DH were members of the Data and Market-\ning Insights unit of the Bocconi Institute for Data\nScience and Analysis\nLimitations\nOur work deals with predicting sociodemographic\naspects from text, which should be considered sen-\nsitive information. Predictive methods can result in\npotentially harmful applications, e.g., in the context\nof user profiling. We acknowledge this potential\nfor dual use (Jonas, 1984) of the data sets we use.\nHowever, in this work, we are interested in advanc-\ning NLP research towards a better understanding\nof such fine-grained aspects of language and how\nthey are already captured by our technology. We\nbelieve that these insights will lead us toward fairer\nand more inclusive language technology. In con-\ntrast, we explicitly discourage the prediction of\nsensitive attributes from text for harmful purposes.\nFurther, we acknowledge that our work is limited\nin that the data sets available to us model gender as\na binary variable, which does not reflect the wide\nvariety of possible identites along the gender spec-\ntrum and beyond (Lauscher et al., 2022). However,\nwe are not aware of other suitable data sets without\nthis limitation. We have reason to believe, though,\nthat even the findings derived from a binary view\non gender (as well as for age) can provide an ini-\ntial understanding of how language varies, and that\nany results will hold under a more sophisticated\nmodeling of the problem.\nAn additional limitation of our work comes from\nthe pre-trained models we used. All the models\ntested are easily-downloadable single-GPU models\nthat have been pre-trained on general-purpose data.\nWe acknowledge that results might differ for mod-\nels that were of bigger capacity and pre-trained on\ndata from other and more specific domains, e.g.,\nsocial media. The same argument can be made\nabout the architectures used. We mainly focused\non BERT-like models trained via MLM, which are\n7909\nonly a subset of the language models proposed in\nthe literature. We leave the exploration of these ef-\nfects (e.g., pre-training objective) for future work.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi,\nand Yoav Goldberg. 2016. Fine-grained analysis of\nsentence embeddings using auxiliary prediction tasks.\nIn Proceedings of ICLR, Toulon, France.\nMeriem Beloucif and Chris Biemann. 2021. Probing\npre-trained language models for semantic attributes\nand their values. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n2554–2559, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nAdrian Benton, Margaret Mitchell, and Dirk Hovy. 2017.\nMultitask learning for mental health conditions with\nlimited social media data. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 1, Long\nPapers, pages 152–162, Valencia, Spain. Association\nfor Computational Linguistics.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor.\n2016. Demographic dialectal variation in social\nmedia: A case study of African-American English.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1119–1130, Austin, Texas. Association for Computa-\ntional Linguistics.\nJohn D. Burger, John Henderson, George Kim, and\nGuido Zarrella. 2011. Discriminating gender on Twit-\nter. In Proceedings of the 2011 Conference on Empir-\nical Methods in Natural Language Processing, pages\n1301–1309, Edinburgh, Scotland, UK. Association\nfor Computational Linguistics.\nSteven Cao, Victor Sanh, and Alexander Rush. 2021.\nLow-complexity probing via finding subnetworks. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 960–966, Online. Association for Computa-\ntional Linguistics.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding universal grammatical relations\nin multilingual BERT. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5564–5577, Online. Association\nfor Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPenelope Eckert and Sally McConnell-Ginet. 2013.\nLanguage and gender. Cambridge University Press.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classification tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP, pages 134–139, Berlin, Ger-\nmany. Association for Computational Linguistics.\nLucie Flek. 2020. Returning the N to NLP: Towards\ncontextually personalized classification models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7828–\n7838, Online. Association for Computational Lin-\nguistics.\nAparna Garimella, Carmen Banea, Dirk Hovy, and\nRada Mihalcea. 2019. Women’s syntactic resilience\nand men’s grammatical luck: Gender-bias in part-of-\nspeech tagging and dependency parsing. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3493–3498, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMareike Hartmann, Miryam de Lhoneux, Daniel Her-\nshcovich, Yova Kementchedjhieva, Lukas Nielsen,\nChen Qiu, and Anders Søgaard. 2021. A multilingual\nbenchmark for probing negation-awareness with min-\nimal pairs. In Proceedings of the 25th Conference on\nComputational Natural Language Learning, pages\n244–257, Online. Association for Computational Lin-\nguistics.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021a.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning.\n7910\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021b. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1772–1791, Online.\nAssociation for Computational Linguistics.\nLisa Anne Hendricks and Aida Nematzadeh. 2021.\nProbing image-language transformers for verb un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3635–3644, Online. Association for Computational\nLinguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDirk Hovy. 2015. Demographic factors improve clas-\nsification performance. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 752–762, Beijing, China. Asso-\nciation for Computational Linguistics.\nDirk Hovy, Federico Bianchi, and Tommaso Fornaciari.\n2020. “you sound just like your father” commer-\ncial machine translation systems include stylistic bi-\nases. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1686–1690, Online. Association for Computational\nLinguistics.\nDirk Hovy and Diyi Yang. 2021. The importance of\nmodeling social factors of language: Theory and\npractice. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 588–602, Online. Association\nfor Computational Linguistics.\nChia-Chien Hung, Anne Lauscher, Dirk Hovy, Si-\nmone Paolo Ponzetto, and Goran Glavaš. 2022. Can\ndemographic factors improve text classification? re-\nvisiting demographic adaptation in the age of trans-\nformers. arXiv preprint arXiv:2210.07362.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82–89, Minneapolis, USA. As-\nsociation for Computational Linguistics.\nAnders Johannsen, Dirk Hovy, and Anders Søgaard.\n2015. Cross-lingual syntactic variation over age and\ngender. In Proceedings of the Nineteenth Confer-\nence on Computational Natural Language Learning,\npages 103–112, Beijing, China. Association for Com-\nputational Linguistics.\nHans Jonas. 1984. The imperative of responsibility: In\nsearch of an ethics for the technological age. Univer-\nsity of Chicago Press. Original in German: Prinzip\nVerantwortung.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021.\nDiscourse probing of pretrained language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3849–3864, Online. Association for Computa-\ntional Linguistics.\nMurathan Kurfalı and Robert Östling. 2021. Prob-\ning multilingual language models for discourse. In\nProceedings of the 6th Workshop on Representation\nLearning for NLP (RepL4NLP-2021) , pages 8–19,\nOnline. Association for Computational Linguistics.\nAnne Lauscher, Archie Crowley, and Dirk Hovy. 2022.\nWelcome to the modern world of pronouns: Identity-\ninclusive natural language processing beyond gen-\nder. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 1221–\n1232, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nAnne Lauscher and Goran Glavaš. 2019. Are we con-\nsistently biased? multidimensional analysis of biases\nin distributional word vectors. In Proceedings of the\nEighth Joint Conference on Lexical and Computa-\ntional Semantics (*SEM 2019) , pages 85–91, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAnne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4782–4797, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nYitong Li, Timothy Baldwin, and Trevor Cohn. 2018.\nTowards robust and privacy-preserving text represen-\ntations. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n7911\n(Volume 2: Short Papers), pages 25–30, Melbourne,\nAustralia. Association for Computational Linguistics.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang\nRen. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of Pre-\nTrained Language Models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6862–6868,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A. Smith. 2021. Probing across\ntime: What does RoBERTa know and when? In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 820–842, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nNikola Ljubeši´c, Darja Fišer, and Tomaž Erjavec. 2017.\nLanguage-independent gender prediction on Twitter.\nIn Proceedings of the Second Workshop on NLP and\nComputational Social Science, pages 1–6, Vancouver,\nCanada. Association for Computational Linguistics.\nVeronica Lynn, Youngseo Son, Vivek Kulkarni, Niran-\njan Balasubramanian, and H. Andrew Schwartz. 2017.\nHuman centered NLP with user-factor adaptation.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1146–1155, Copenhagen, Denmark. Association for\nComputational Linguistics.\nMatej Martinc and Senja Pollak. 2018. Reusable work-\nflows for gender prediction. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018) , Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nDong Nguyen, Dolf Trieschnigg, A. Seza Do˘gruöz, Ri-\nlana Gravel, Mariët Theune, Theo Meder, and Fran-\nciska de Jong. 2014. Why gender and age prediction\nfrom tweets is hard: Lessons from a crowdsourcing\nexperiment. In Proceedings of COLING 2014, the\n25th International Conference on Computational Lin-\nguistics: Technical Papers, pages 1950–1961, Dublin,\nIreland. Dublin City University and Association for\nComputational Linguistics.\nOnkar Pandit and Yufang Hou. 2021. Probing for bridg-\ning inference in transformer language models. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4153–4163, Online. Association for Computa-\ntional Linguistics.\nLaura Pérez-Mayos, Miguel Ballesteros, and Leo Wan-\nner. 2021. How much pretraining data do language\nmodels need to learn syntax? In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1571–1582.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nJason Phang, Thibault Févry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nTiago Pimentel and Ryan Cotterell. 2021. A Bayesian\nframework for information-theoretic probing. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2869–\n2887, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4609–4622, Online. Association for Computa-\ntional Linguistics.\nArchiki Prasad and Preethi Jyothi. 2020. How accents\nconfound: Probing for accent information in end-\nto-end speech recognition systems. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 3739–3753, Online.\nAssociation for Computational Linguistics.\nAbhilasha Ravichander, Yonatan Belinkov, and Eduard\nHovy. 2021. Probing the probing paradigm: Does\nprobing accuracy entail task relevance? In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3363–3377, Online. Association\nfor Computational Linguistics.\nVinit Ravishankar, Lilja Øvrelid, and Erik Velldal. 2019.\nProbing multilingual sentence representations with\nX-probe. In Proceedings of the 4th Workshop on\nRepresentation Learning for NLP (RepL4NLP-2019),\npages 156–168, Florence, Italy. Association for Com-\nputational Linguistics.\n7912\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nSara Rosenthal and Kathleen McKeown. 2011. Age pre-\ndiction in blogs: A study of style, content, and online\nbehavior in pre- and post-social media generations.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 763–772, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nDanielle Saunders and Bill Byrne. 2020. Reducing gen-\nder bias in neural machine translation as a domain\nadaptation problem. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7724–7736, Online. Association\nfor Computational Linguistics.\nDeven Santosh Shah, H. Andrew Schwartz, and Dirk\nHovy. 2020. Predictive biases in natural language\nprocessing models: A conceptual framework and\noverview. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5248–5264, Online. Association for Computa-\ntional Linguistics.\nNaomi Shapiro, Amandalynne Paullada, and Shane\nSteinert-Threlkeld. 2021. A multilabel approach to\nmorphosyntactic probing. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 4486–4524, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nIonut-Teodor Sorodoc, Kristina Gulordava, and Gemma\nBoleda. 2020. Probing for referential information in\nlanguage models. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4177–4189, Online. Association for\nComputational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nLucas Torroba Hennigen, Adina Williams, and Ryan\nCotterell. 2020. Intrinsic probing through dimension\nselection. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 197–216, Online. Association for\nComputational Linguistics.\nPeter Trudgill. 2000. Sociolinguistics: An introduction\nto language and society. Penguin UK.\nRob V oigt, David Jurgens, Vinodkumar Prabhakaran,\nDan Jurafsky, and Yulia Tsvetkov. 2018. RtGender:\nA corpus for studying differential responses to gen-\nder. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nSvitlana V olkova, Theresa Wilson, and David Yarowsky.\n2013. Exploring demographic language variations\nto improve multilingual sentiment analysis in social\nmedia. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1815–1827, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020. Learning which fea-\ntures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n217–235, Online. Association for Computational Lin-\nguistics.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models. arXiv preprint\narXiv:2010.06032.\nChien-Sheng Wu and Caiming Xiong. 2020. Probing\ntask-oriented dialogue representation from language\n7913\nmodels. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5036–5051, Online. Association for\nComputational Linguistics.\nYi Yang and Jacob Eisenstein. 2017. Overcoming lan-\nguage variation in sentiment analysis with social at-\ntention. Transactions of the Association for Compu-\ntational Linguistics, 5:295–307.\nSheng Zhang, Xin Zhang, Weiming Zhang, and Anders\nSøgaard. 2021a. Sociolectal analysis of pretrained\nlanguage models. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 4581–4588, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYian Zhang, Alex Warstadt, Xiaocheng Li, and\nSamuel R. Bowman. 2021b. When do you need\nbillions of words of pretraining data? In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1112–1125, Online.\nAssociation for Computational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n19–27.\n7914\nA Models Used\nWe provide an overview on all models we have\nused in this study. They are available on the Hug-\ngingface Hub: https://huggingface.co.\nA.1 Models Used for RQ1\nFor our base experiment we have used the follow-\ning models.\n• roberta-base: 12 layers, 12 attention\nheads, hidden size of 768\n• roberta-large: 24 layers, 16 attention\nheads, hidden size of 1024\n• microsoft/deberta-v3-base: 12\nlayers, 12 attention heads, hidden size of 768\n• microsoft/deberta-v3-large: 24\nlayers, 16 attention heads, hidden size of 1024\n• microsoft/deberta-v3-xsmall: 12\nlayers, 6 attention heads, hidden size of 384\n• microsoft/deberta-v3-small: 6\nlayers, 12 attention heads, hidden size of 768\n• microsoft/deberta-base: 12 layers,\n12 attention heads, hidden size of 768\n• microsoft/deberta-large: 24 layers,\n16 attention heads, hidden size of 1024\nA.2 Models Used for RQ2\nFor investigating the amount of pre-training data\nneeded, we have used the suite of MiniBERTas,\nand RoBERTabase.\n• roberta-base: 12 layers, 12 attention\nheads, hidden size of 768\n• nyu-mll/roberta-base-1B-1: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-1B-2: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-1B-3: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-100M-1: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-100M-2: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-100M-3: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-10M-1: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-10M-2: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-base-10M-3: 12\nlayers, 12 attention heads, hidden size of 768\n• nyu-mll/roberta-med-small-1M-1:\n6 layers, 8 attention heads, hidden size of 512\n• nyu-mll/roberta-med-small-1M-2:\n6 layers, 8 attention heads, hidden size of 512\n• nyu-mll/roberta-med-small-1M-3:\n6 layers, 8 attention heads, hidden size of 512\nA.3 Models Used for RQ3\nWe investigated the layer-wise knowledge of the\nfollowing models.\n• roberta-base: 12 layers, 12 attention\nheads, hidden size of 768\n• roberta-large: 24 layers, 16 attention\nheads, hidden size of 1024\n• microsoft/deberta-v3-base: 12\nlayers, 12 attention heads, hidden size of 768\n• microsoft/deberta-v3-large: 24\nlayers, 16 attention heads, hidden size of 1024\n• microsoft/deberta-base: 12 layers,\n12 attention heads, hidden size of 768\n• microsoft/deberta-large: 24 layers,\n16 attention heads, hidden size of 1024\nA.4 Models Used for RQ4\nAs multilingual counter-parts, we employed the\nfollowing models.\n• xlm-roberta-base: 12 layers, 12 atten-\ntion heads, hidden size of 768\n• xlm-roberta-large: 24 layers, 16\nheads, hidden size of 1024\n• microsoft/mdeberta-v3-base: 12\nlayers, 12 attention heads, hidden size of 768\n7915\nA.5 Models Used for RQ5\nFinally, we ran the STILT experiment with the fol-\nlowing models.\n• roberta-large: 24 layers, 16 heads, hid-\nden size of 1024\n• KoichiYasuoka/roberta-large\n-english-upos: 24 layers, 16 heads,\nhidden size of 1024\n• Jean-Baptiste/roberta-large\n-ner-english: 24 layers, 16 heads,\nhidden size of 1024\n• cointegrated/roberta-large\n-cola-krishna2020: 24 layers, 16\nheads, hidden size of 1024\n• roberta-large-mnli: 24 layers, 16\nheads, hidden size of 1024\n• navteca/roberta-large-squad2:\n24 layers, 16 heads, hidden size of 1024\n• howey/roberta-large-sst2: 24 lay-\ners, 16 heads, hidden size of 1024\nB Additional Results\nWe provide the additional results for MDL probing.\nB.1 Additional Results for RQ3\nThe layer-wise analysis for MDL probing is pro-\nvided in Figure 6.\nB.2 Additional Results for RQ4\nWe show the MDL results for the multilingual anal-\nysis in Figure 7.\nB.3 Additional Results for RQ5\nWe provide the MDL probing results for our STILT\nanalysis in Figure 8.\n7916\n(a) Trustpilot (Age)\n (b) Trustpilot (Gender)\n (c) FB Congress (Gender)\n(d) FB Public (Gender)\n (e) Fitocracy (Gender)\n (f) Reddit (Gender)\n (g) CoLA\nFigure 6: Results showing our layer-wise analysis of DeBERTa original and v3 large and base and RoBERTalarge\nand base in terms of average and standard deviation of the MDL for 5 runs across 7 tasks ((a) Trustpilot Age to (g)\nCoLA)\n(a) Trustpilot (Age)\n (b) Trustpilot (Gender)\n (c) FB Congress (Gender)\n(d) FB Public (Gender)\n (e) Fitocracy (Gender)\n (f) Reddit (Gender)\n (g) CoLA\nFigure 7: Results for our analysis of multilingual models in terms of average and standard deviation of the MDL\nscores for 5 runs across 7 tasks ((a) Trustpilot (Age) to (g) CoLA) for features extracted from different layers of\nXLM-RoBERTalarge and base and mDeBERTabase.\n7917\n(a) Trustpilot (Age)\n (b) Trustpilot (Gender)\n (c) FB Congress (Gender)\n(d) FB Public (Gender)\n (e) Fitocracy (Gender)\n (f) Reddit (Gender)\n (g) CoLA\nFigure 8: Results for our Supplementary Training on Intermediate Labeled Tasks (STILT) analysis. We show the\nprobing results in terms of average and standard deviation of the MDL scores for 5 runs across 7 tasks ((a) Trustpilot\nAge to (g) CoLA) for features extracted from the original RoBERTalarge, and RoBERTalarge fine-tuned on 6 tasks:\nPart-of-Speech Tagging (POS Tagging), Named Entity Recognition (NER), Linguistic Acceptability Prediction\n(Ling. Acc.), Natural Language Inference (NLI), Question Answering (QA), and Sentiment Analysis (SA).\n7918",
  "topic": "Grammaticality",
  "concepts": [
    {
      "name": "Grammaticality",
      "score": 0.818495512008667
    },
    {
      "name": "Computer science",
      "score": 0.6625873446464539
    },
    {
      "name": "ENCODE",
      "score": 0.647413432598114
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5350193977355957
    },
    {
      "name": "Natural language processing",
      "score": 0.5193882584571838
    },
    {
      "name": "Language model",
      "score": 0.4668709933757782
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4442993998527527
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3547067642211914
    },
    {
      "name": "Machine learning",
      "score": 0.3405417203903198
    },
    {
      "name": "Data science",
      "score": 0.3285839557647705
    },
    {
      "name": "Psychology",
      "score": 0.25270557403564453
    },
    {
      "name": "Linguistics",
      "score": 0.22413337230682373
    },
    {
      "name": "Grammar",
      "score": 0.14979231357574463
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I159176309",
      "name": "Universität Hamburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I71209653",
      "name": "Bocconi University",
      "country": "IT"
    }
  ],
  "cited_by": 5
}