{
  "title": "Devising and Detecting Phishing Emails Using Large Language Models",
  "url": "https://openalex.org/W4392667162",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5115003586",
      "name": "Fred Heiding",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A605106185",
      "name": "Bruce Schneier",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2029546100",
      "name": "Arun Vishwanath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972464929",
      "name": "Jeremy Bernstein",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2163000534",
      "name": "Peter S. Park",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6852800892",
    "https://openalex.org/W3094117827",
    "https://openalex.org/W2740534380",
    "https://openalex.org/W3088268064",
    "https://openalex.org/W2475201703",
    "https://openalex.org/W2131906261",
    "https://openalex.org/W4213081600",
    "https://openalex.org/W4383427878",
    "https://openalex.org/W3006834142",
    "https://openalex.org/W4306639543",
    "https://openalex.org/W4251323996",
    "https://openalex.org/W2477913880",
    "https://openalex.org/W6851985269",
    "https://openalex.org/W2000972311",
    "https://openalex.org/W6848884192",
    "https://openalex.org/W2002734807",
    "https://openalex.org/W6853877726",
    "https://openalex.org/W6850830318",
    "https://openalex.org/W4225454120",
    "https://openalex.org/W2557778508",
    "https://openalex.org/W4366967614",
    "https://openalex.org/W2170899042",
    "https://openalex.org/W4376122680",
    "https://openalex.org/W6860041859",
    "https://openalex.org/W4292014190",
    "https://openalex.org/W4372260394",
    "https://openalex.org/W4376312605",
    "https://openalex.org/W2944554782",
    "https://openalex.org/W1512366833",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4313484477",
    "https://openalex.org/W4380353960",
    "https://openalex.org/W4360818927",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "AI programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user. The V-Triad is a set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases. In this study, we compare the performance of phishing emails created automatically by GPT-4 and manually using the V-Triad. We also combine GPT-4 with the V-Triad to assess their combined potential. A fourth group, exposed to generic phishing emails, was our control group. We use a red teaming approach by simulating attackers and emailing 112 participants recruited for the study. The control group emails received a click-through rate between 19-28&#x0025;, the GPT-generated emails 30-44&#x0025;, emails generated by the V-Triad 69-79&#x0025;, and emails generated by GPT and the V-Triad 43-81&#x0025;. Each participant was asked to explain why they pressed or did not press a link in the email. These answers often contradict each other, highlighting the importance of personal differences. Next, we used four popular large language models (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing emails and compare the results to human detection. The language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails. They sometimes surpassed human detection, although often being slightly less accurate than humans. Finally, we analyze of the economic aspects of AI-enabled phishing attacks, showing how large language models increase the incentives of phishing and spear phishing by reducing their costs.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nDevising and detecting phishing emails\nusing large language models\nFREDRIK HEIDING1, BRUCE SCHNEIER1, ARUN VISHW ANATH2, JEREMY BERNSTEIN3, AND\nPETER S. PARK.3\n1Harvard University\n2Avant Research Group\n3Massachusetts Institute of Technology\nCorresponding author: Fredrik Heiding (e-mail: fheiding@seas.harvard.edu).\nABSTRACT AI programs, built using large language models, make it possible to automatically create\nphishing emails based on a few data points about a user. The V-Triad is a set of rules for manually designing\nphishing emails to exploit our cognitive heuristics and biases. In this study, we compare the performance of\nphishing emails created automatically by GPT-4 and manually using the V-Triad. We also combine GPT-4\nwith the V-Triad to assess their combined potential. A fourth group, exposed to generic phishing emails,\nwas our control group. We sent emails to 112 participants recruited for the study. The control group emails\nreceived a click-through rate between 19-28%, the GPT-generated emails 30-44%, emails generated by the\nV-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each participant was asked to\nexplain why they pressed or did not press a link in the email. These answers often contradict each other,\nhighlighting the importance of personal differences. Next, we used four popular large language models (GPT,\nClaude, PaLM, and LLaMA) to detect the intention of phishing emails and compare the results to human\ndetection. The language models demonstrated a strong ability to detect malicious intent, even in non-obvious\nphishing emails. They sometimes surpassed human detection, although often being slightly less accurate than\nhumans. Finally, we make an analysis of the economic aspects of AI-enabled phishing attacks, showing how\nlarge language models increase the incentives of phishing and spear phishing by reducing their costs.\nINDEX TERMS Phishing, Large Language Models, Social Engineering, Artificial Intelligence\nI. INTRODUCTION\nN\nATURAL language processing capabilities have in-\ncreased drastically over the last few years due to the\nrapid development of large language models. Models such\nas GPT [29] and Claude 1have demonstrated the ability to\ngenerate human-like text, converse coherently, and perform\nlinguistic tasks at superhuman levels. Just within the last\nyear, the size and performance of these models have grown\ntremendously. Most current LLMs are estimated to contain\nover 100 billion, or even more than a trillion, parameters,\neclipsing all previous benchmarks 2. When most people read\nthis article, these numbers will likely already be outdated.\nLarge language models excel at creating textual content that\nappears legitimate. With only a few data points about a recipi-\nent, the LLM can create content that appears uniquely crafted\n1https://www.anthropic.com/index/introducing-claude\n2https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-l\neaked/\nfor that target, sometimes even mimicking the linguistic style\nof a close acquaintance. LLMs are well-suited for crafting\nphishing emails because of their flair for imitating human\nwriting and reasoning. Phishing, like LLMs, aims to use a\nfew data points about the target to create content that appears\nrealistic and relevant.\nAlmost 20 years ago, Dhamija et al. explained “Why\nphishing works” [8], highlighting that phishing exploits\ninherent human psychological and behavioral weaknesses.\nPeople rely heavily on visual cues and other heuristics when\nassessing credibility rather than rationally analyzing content.\nUnfortunately, phishing still works. Human nature is slow\nto change, and the same innate psychological tendencies\nthat make us vulnerable, like favoring trust over skepticism\nand prioritizing urgency, are deeply ingrained in our nature.\nEven though many organizations spend immense resources to\ntrain their employees, phishing is one of the most persistent\ncybersecurity threats to organizations, governments, and\ninstitutes around the world [37, 16, 4].\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nMany complex and intricate cyberattacks start by exploiting\nhuman users to access the organization’s system. The Sony\nPictures hack [19], and the $100m Facebook and Google\nscams [12] are two infamous examples. Some studies claim\nthat well above 70-80% of all cyberattacks use social en-\ngineering [15, 35]. Regardless of the number, phishing is\na continued nuisance that hurts individuals, governments,\nand private industries. Up to this point, it has been easy to\nlaunch phishing attacks, but many of the emails have had\npoor quality, either lacking coherent reasoning, a trustworthy\npresentation, or correct language and grammar. More sophisti-\ncated phishing attacks, so-called spear phishing, require more\ndeliberate reconnaissance about the target and crafting time\nfor the message. Manual methods for phishing exist and have\ndisplayed impressive results [37], but they are time-consuming\nand require the attacks to have substantial knowledge of social\nengineering as well as gather intelligence about the target and\nunderstand their native language and IT environment. The\nrecent advancement of language models makes it far easier to\ncreate large-scale phishing attacks of high quality. With a few\neasily collected data points and a simple query to an LLM-\npowered chatbot, an attacker can create great phishing emails\nthat previously required substantial time and knowledge to\ndevise.\nThe study contains three parts. In the first part, we compare\nthe success rate of phishing emails using four different\nmethods: state-of-the-art LLMs (GPT-4), human models (the\nV-Triad, described in Section II), a hybrid strategy (GPT-4\ncombined with the V-Triad), and the control group (arbitrary\nphishing emails obtained from online databases of real-world\nphishing emails). In the hybrid strategy (combining GPT-4\nand the V-Triad), the best practices of the V-Triad determined\nhow the query to GPT-4 was formed. We sent phishing emails\nto 112 participants. The control group emails received a click-\nthrough rate (recipient pressed a link in the email) between\n19-28%, the GPT-generated emails 30-44%, emails generated\nby the V-Triad 69-79%, and emails generated by the hybrid\napproach (GPT and the V-Triad) 43-81%. Each participant\nwas asked to provide an explanation for why they pressed\nor did not press a link in the email. These answers often\ncontradict each other, highlighting the need for personalized\ncontent. What makes one person avoid phishing emails makes\nanother person fall for them.\nIn the second part of the study, we use four popular LLMs\n(GPT, Claude, PaLM [1], and LLaMA [29]) to identify\nphishing emails and provide recommended actions to the\nrecipient. We fed each chatbot 20 emails, four emails from\neach category above, and four additional emails fetched\nfrom our private mail inboxes. In some cases, the LLMs\nare surprisingly adept at detecting malicious intent, even in\nnon-obvious phishing emails, sometimes surpassing human\ndetection rates. Each model was asked four questions: “I am\nabout to give you an email, can you please tell me what the\nintent of the email is?”, “How likely would you say it is that\nthis email is suspicious (please quantify as a percentage)?”,\n“How would you recommend me to answer to this email?”, and\n“How likely would you say it is that this email is created by an\nAI (please quantify as a percentage)?”. The success rate of\neach model varied significantly. The best-performing model\n(Claude) correctly detected the malicious intention of 75% of\nthe control group emails, 25% of the GPT-generated emails,\nand 25% of the emails generated by GPT+V-Triad. When\nprimed for suspicion (“How likely would you say it is that this\nemail is suspicious”), Claude detected the intention of 75% of\nthe control group emails, 100% of the GPT-generated emails,\n100% of the V-Triad emails, and 100% of the emails generated\nby V-Triad+GPT. The quantitative detection results should be\nseen as an indication. A larger data sample is required to draw\nmore decisive conclusions. However, the models’ capacity\nfor recommending how users should respond to phishing\nemails is interesting. For example, they encouraged users who\nreceived an attractive discount offer to verify the offer with\nthe company’s official website or communication channels,\nwhich is a good strategy to avoid phishing attacks.\nIn the third part of the study, we conduct a cost-benefit\nanalysis of AI-enabled phishing attacks using the different\nmethods presented in this article. In short, AI significantly\nincreases the incentives to launch phishing attacks by reducing\ntheir cost and required revenue. This article has demonstrated\nthat email creation can be automated using large language\nmodels. The automation makes AI-enabled spear phishing\nattacks far cheaper than traditional spear phishing but still\nmore expensive than regular phishing (see Table 1. In a future\nstudy, we aspire to demonstrate that information gathering\n(a prerequisite for spear phishing) can also be automated\nusing language models. If that is accomplished, the cost of\nAI-enabled spear phishing attacks is reduced to the cost of\nmanual and non-personalized phishing attacks. This would\npresent a significant concern as spear phishing is costly for\norganizations and governments, and we can expect the number\nof spear phishing attacks to increase significantly in the\ncoming years. Section VI proposes mitigation strategies for\npreparing ourselves for the increased number of spear phishing\nattacks.\nOur results demonstrate that large language models can\ngenerate convincing phishing emails when primed with the\nappropriate context, although not (yet) as successful as\nemails manually created by human experts. However, the\nsemi-automated approach (using human experts and large\nlanguage models) performed as well or better than humans\nwhile significantly reducing the time to create emails and the\nknowledge requirement of the attacker.\nThus, LLMs can increase the quality of phishing emails\nand simultaneously make them easier to create and send.\nThis poses a negative side effect of AI development. We\ndeem it crucial to quantify the severity of this side effect,\nas done in Section V, to better gauge how to continue AI\ndevelopment responsibly. Fortunately, our results also indicate\nthat LLMs show promising signs of detecting phishing emails\nand recommend actions that make users avoid them. Ideally,\nwe can isolate the positive effects of LLMs for phishing\nprotection and mitigate or restrict the negative effects, but\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nthis is difficult due to the complex nature of the models and\nthe intertwined nature of positive and negative use cases.\nII. RELATED WORK AND BACKGROUND\nThis section provides a brief background of large language\nmodels (LLMs) and the V-Triad, and discusses related re-\nsearch projects on how LLMs can be used to create and detect\nphishing.\nIn recent years, the development of large language models\n(neural networks trained on massive text datasets) has revolu-\ntionized natural language processing. The high performance is\nmade possible by the models’ large parameter counts, allowing\nthem to capture nuanced patterns in linguistic data. LLMs\ncome in different versions (such as GPT [29] created by\nOpenAI, Anthropic’s LLM 3, PaLM [1] and Gemini [34]\ncreated by Google, and LLaMA [36] from Meta). LLMs are\noften used in AI-powered chatbots, such as ChatGPT (GPT),\nClaude (Anthropic), Bard (PaLM/Gemini), and ChatLLaMA\n(LLaMA). Figure 1 displays an overview of four common\nlarge language models and chatbots based on the models.\nThe V-Triad is a human model for manually creating\nphishing emails and deceptive content that can bypass a\nuser’s suspicion filter, presented in Figure [37]. Unlike LLMs,\nthe V-Triad is manually created based on highly targeted\nand specific data (real-world phishing emails and deceptive\ncontent), resulting in a specialized model with a targeted use\ncase. LLMs can create phishing emails automatically, while\nthe V-Triad is a guide to assist us when manually creating\nphishing emails. The V-Triad is adapted to a recipient’s cyber\nrisk beliefs, which describe how accurately we perceive digital\nrisks and are affected by cognitive heuristics and biases. By\nexploiting these beliefs, the V-Triad lets an attacker create\naction triggers (such as a phishing email with a link) that\nare unlikely to make the recipient suspicious. Users with bad\nself-regulation (likelihood of developing strong media habits)\nare especially susceptible [37]. Figure 3 presents an overview\nof how Cyber Risk Beliefs affect suspicion. The V-Triad can\nalso be used to find areas where users should increase their\nsuspicion to enhance their security.\nThe V-Triad consists of three parts: Credibility, Compati-\nbility (relevancy), and Customizability. Figure 2 provides an\noverview of the V-Triad and its three vertices. More detailed\ninformation is provided below, all examples are fetched from\n[37]. In the context of phishing emails, credibility concerns\nhow the content of the email is perceived. If the email appears\nlegitimate to the recipient, it is credible. Below are some\ncommon ways to increase an email’s credibility:\n• Use a well-known brand name.\n• Include the name of the recipient.\n• Spoof a known sender.\n• Use colors, fonts, and text that mimic familiar brands.\n• Include familiar attachment types.\n• Presence or absence of obvious spelling errors.\n3https://www.anthropic.com/product\n• Include trust-enhancing words (e.g., “Re” or “Fwd” in\nthe email subject line or body).\n• Include trigger words (e.g., “Sent from my iPhone” or\n“deadline”).\nCompatibility refers to how relevant an email is to the\nrecipient. Even if an email appears legitimate, it must make\nsense for the recipient to receive it. For example, imagine\nan email targeting students at a specific university with a\nlink to their schedule for the coming semester. The email\nis unlikely to be successful if the recipient is a student at\nanother university, no matter how legitimate the email looks.\nHowever, the relevancy is high if the recipient is a student at\nthe specified university and is expecting a link to the schedule.\nCompatibility often exploits a certain timing, target group, or\nboth. Below are some common ways to increase an email’s\ncompatibility:\n• Mimic a work-related process (e.g., internal emails or\nprinter sharing routines).\n• Mimic a public occasion, holiday, or event (e.g., Christ-\nmas shopping or tax season).\n• Exploit common break times (e.g., lunch) when users are\nmore likely to check their email.\n• Exploit when users are likelier to read emails on mobile\ndevices (e.g., late Friday evening and night).\n• Replicate life events, interests, and circumstances (e.g.,\npregnancy, pet ownership, and political affiliations).\n• Mimic a routine (e.g., checking social media in the morn-\ning, paying credit card bills at the end of a cycle, lottery\npurchases, or logging onto Wi-Fi in public places).\n• Mimic cyber-awareness training (e.g., password change\nemails from the IT department or phishing pentest\nemails).\n• Mimic a software update reminder.\n• Exploit other high-impact times when people are likely\nto check emails (e.g., Tuesday and Friday mornings).\nCustomizability treats whether a website or email behaves\nas we expect it to behave when interacting with it. It is slightly\nmore relevant for websites but also affects emails. For exam-\nple, does the URL of a website look and behave as expected\nwhen we copy it or does the two-factor authentication prompt\nof an email behave as expected when we press it? Below are\ncommon ways to increase the compatibility of an email or\napplication:\n• The subject line of the email and form fields (e.g., 2FA\ninput forms and login input windows).\n• Login notifications (informing where and when someone\nlogged into a service).\n• Single sign-on links, codes, and settings.\n• Changing styles of prompts requesting access to files,\nfolders, and settings (e.g., request to enable macros in\nWord).\n• Email addresses of different senders.\n• Social media updates, email subject lines, and prompts\n(e.g., for accepting cookies or terms of agreements).\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nFIGURE 1. An overview of four common large language models and chatbots based on them.\nFIGURE 2. The V-Triad framework, as presented in [37].\n[18] presents a similar cognitive framework for creating\nphishing emails based on two cognitive characteristics that\noverlap with the V-Triad’s credibility metric: believability\nand persuasiveness. Believability refers to crafting phishing\nmessages that resemble the communication style of the\nimpersonated organizations, such as the look and feel of\nemails, including logos, and adopting a formal writing style.\nPersuasiveness regards techniques that exploit fundamental\nFIGURE 3. An overview of how Cyber Risk Beliefs and self-regulation affect our\nsuspicion, as presented in [37].\nvulnerabilities of human cognition, such as the six influence\nprinciples outlined by Cialdini in [7] (Reciprocity, Consis-\ntency, Social Proof, Authority, Liking, Scarcity) [7].\nA. CREATING AND DETECTING PHISHING EMAILS\nUSING LLMS\nAlthough large language models have only gained widespread\nattention in the past year, many studies have already explored\ntheir potential for generating and detecting phishing emails.\nGiven their ability to produce increasingly human-like text,\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\ngenerative language models are often perceived to be well\nsuited for persuasion and deception [17, 21, 24, 31, 14, 20].\nHowever, the same models also show promising signs of being\nable to counter deception by improving phishing detection\n[23, 28, 38, 28, 26].\nIBM X-Force used GPT to create phishing emails and\npaired the results with human phishers, obtaining similar\nresults as those presented in our study (the LLM-generate\nemails are impressive and produce good results, but human\nexperts are still slightly better) [20]. Most studies focus on\ncreating phishing emails but do not validate them in a real-\nworld context [17, 21, 31, 14]. The studies use GPT 2, 3,\n3.5, and 4. One study also analyzes OPT [21], and another\nstudy analyzes Bart’s capacity for creating phishing emails\nbut maintains a theoretical perspective and does not create or\nsend emails [24].\n[23] uses GPT-3.5 and GPT-4 to detect phishing sites,\nvalidates the result on a dataset, and receives a precision of\n98.3% and a recall of 98.4%. [28] proposes two language\nmodels adapted to a custom-made dataset containing 725k\nemails (made by merging an existing collection of legitimate\nand phishing emails). [38] and [26] propose veritable pre-\ntrained deep transformer network models for phishing URL\ndetection, and the latter performs additional domain-specific\npre-training tasks. None of the related studies investigate how\nto detect the intention of phishing emails using LLMs.\nIII. EXPERIMENTS\nThis section describes how phishing emails were created and\nsent using LLMs and how LLMs were used to detect phishing\nemails. The deceptive part of the study consists of four phases.\nFirst, we recruited participants and collected background data\nabout them. Second, the phishing emails were created using\nfour methods (arbitrary phishing emails, LLMs, V-Triad, and\nGPT+V-Triad). Third, the phishing emails were sent to the\nparticipants, and last, the results were analyzed. Subsequently,\nwe used LLMs to detect the intention of phishing emails.\nBefore the participants and background information could\nbe collected, an extensive review was done by the univer-\nsity’s Institutional Reviews Board to ensure the inclusion of\nhuman subjects was ethical and did not use more personal\ninformation than necessary. After that, the power of the\nstudy was calculated to determine how many participants\nwere required to produce reliable results. Statistical power\nrefers to the probability of correctly detecting a real effect\nor difference when it exists in a statistical hypothesis test.\nIn simple terms, it is the likelihood of finding a significant\nresult (e.g., a significant relationship between two variables\nor a significant difference between groups) when there is a\ntrue effect in the population. Power is influenced by several\nfactors, including the sample size, significance level (often\ndenoted as alpha), and effect size. Effect size represents the\nmagnitude or strength of the relationship or difference being\nstudied. A larger effect size means the observed effect is more\nsubstantial or pronounced. Effect sizes are estimated a priori,\nusually based on prior empirical work. In our case, the effect\nsize is large. The desired alpha is 0.05, and the desired power\nis 0.80 (both are standards we follow), which nets a sample\nsize requirement of around 100 to 125.\nParticipants were recruited by posting flyers at university\ncampuses and surrounding areas and through recruitment\nemails in various university-related email groups. When par-\nticipants signed up for the study, they also answered four ques-\ntions to provide background information about themselves.\nThese answers were used to personalize the phishing emails.\nThe questions were “Name some extracurricular activities\nyou partake in (swimming, the chess club, etc.)”, “Name some\nbrands you have purchased from lately (Amazon, Whole Foods,\nApple, etc.)”, “Name any other newsletters you regularly\nreceive (business digests, tech updates, etc. If none, type\nN/A)”, and “Of all emails you regularly receive, are there any\nyou like or dislike more than the others? Please explain the\nreasons for this liking/disliking.”. The signup survey included\na detailed study description but did not explicitly say that the\nparticipants would receive phishing emails (we said we would\nuse the background information to send targeted marketing\nemails). Additionally, the project briefing did not mention\nthat we track whether participants press a link in the emails.\nThis deception was deemed necessary. Labeling the emails as\nphishing emails and explicitly saying that we track whether\na link is pressed would make the participants suspicious and\ncould skew the results. The participants received a complete\ndebriefing after completion of the study.\nSeveral bots seemed to get hold of the study, creating many\nreplies from suspicious email addresses and unrealistic or\nincoherent answers. Luckily, these often completed the survey\nfar faster than the average answer time (< 30 seconds instead\nof 4-5 minutes). Thus, candidates who completed the survey\nfaster than 30 seconds were removed. Each participant was\nalso verified by ensuring their email matched their university\naffiliation. In a few cases (11 participants), a private Gmail\naccount was used instead of the university email, the answers\nof these participants were scrutinized more carefully, and their\naffiliation was verified by validating the participant’s digital\nfootprint. Two participants submitted multiple applications\n(using different emails, one university email and one personal\nemail), and the duplicates were removed. After the screening\nwas completed, 112 participants remained. Each participant\nwas offered a $5 gift card at Amazon as a thanks for their\nparticipation. The gift card was given after the study was\ncompleted.\nWhen the information was collected and structured, the\ndata analysis was automated by feeding the answers to an\nLLM and asking it to fetch the most common themes among\nthe answers. For example, we asked for the most frequent\nstores or brands a participant had purchased from recently.\nThe result was manually checked for correctness, but all\nresponses were good. As shown in section IV-A, the collected\nbackground information was scattered, without many clear\ncommon trends. We wanted to use the same email for all\nparticipants to facilitate a better comparison, so Starbucks was\nchosen as the best option. It was one of the most frequently\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nmentioned brands, and several new Starbucks cafes recently\nopened in the area.\na: Creating phishing emails\nThe phishing emails were divided into four categories, and\nparticipants were randomly assigned to either of the groups\nusing the randomize function in Google Sheets. Each group\nreceived one-fourth of the participants. The categories were:\n1) Control group (arbitrary phishing emails).\n2) Created using an LLM (GPT-4).\n3) Created using the V-Triad.\n4) Created using the V-Triad and an LLM (GPT-4).\nb: Control Group\nFor The control group emails, we used an existing phishing\nemail targeting Starbucks customers4, displayed in Figure 4.\nThe email was chosen to represent arbitrary phishing emails\ncreated without a specific method but still targeting the same\nareas (Starbucks customers) as the other emails.\nFIGURE 4. Control group example email.\nc: Personalization using GPT -4\nTo create emails using LLMs, ChatGPT was used. GPT\nwas selected as it is one of the most popular and widely\nknown large language models. We encourage other research\nto compare the deceptive success rate between different LLMs\n(for example, by conducting a similar study to ours but\nalso including emails from Anthropic, Palm/Gemini, and\nLLaMA). However, comparisons between the textual output of\ndifferent models indicate that their output quality is relatively\nanalogous. Section VI provides examples of emails generated\nby different language models responding to the same query.\nWe tried several queries before choosing the final version:\n“Create an email offering a $25 gift card to Starbucks for\nStudents at University X, with a link for them to access\nthe discount code, in no more than 150 words” . Figure 5\ndisplays the GPT email. It is noteworthy that the email does\n4https://blog.knowbe4.com/bid/383111/scam-of-the-week-starbucks-gif\nt-from-a-friend-phishing-emails\nnot mention the university’s name. Minor corrections to the\nquery solved the issue, as seen in the V-Triad+GPT email.\nChatGPT has a built-in protection mechanism to prevent\nmalicious use cases. When asked to create a phishing email,\nit replied that phishing is unethical and potentially illegal\nand will not assist. At the earlier stage of this study (Q1\n2023), we could reply that we are researchers and will use\nthe phishing email for ethical purposes and research. Initially,\nthis worked, but after a later update, GPT replied that it would\nnot give us a phishing email even if we are researchers and\nintend to use it for ethical purposes. Then, we changed the\nphrase “phishing email” to “informative email”, bypassing\nthe problem. This demonstrates how difficult it is to prevent\nLLMs from being used for malicious purposes. The only\ndifference between a good phishing email and a marketing\nemail is the intention, which makes it hard to stop users from\ncreating good phishing emails. If we were to prevent LLMs\nfrom creating realistic marketing emails, many legitimate use\ncases would be prohibited.\nFIGURE 5. GPT example email.\nd: Personalization using the V-Triad\nThe V-Triad emails were created following the V-Triad’s best\npractices, presented in Section II. Credibility was met by\nadding a logo to the email, shortening the content, and clean-\ning up the language. Compatibility was met by addressing\nthe students’ university and capturing a relevant brand that\nmany of them showed interest in, as well as including the\nparticipant’s name. Customizability was met by including\ncommon email features such as the unsubscribe link and a\nbutton for claiming the gift card. Figure 6 displays the V-Triad\nemail.\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nFIGURE 6. V-Triad example email.\ne: Personalization using GPT and the V-Triad\nIn the combined approach, best practices from the V-Triad\nwere used to enhance the quality of the email created by GPT.\nThe email’s credibility was enhanced by adding a logo and\ntrying several queries and email lengths until a combination\nwith high linguistic quality was met. Relevancy was enhanced\nby iterating through more queries than the GPT email until\nthe email clearly included information about the participant\n(such as correct university affiliation) and the relevant brand\n(Starbucks gift card). The final query was \"Create an email\noffering a $25 gift card for Students at University X to\nStarbucks, with a link for them to access the QR code, in\nno more than 150 words\". Figure 7 displays the email created\nusing GPT and the V-Triad email.\nA footer was added to the bottom of all emails explaining\nthat the content was not sent from Starbucks but originated\nfrom the research study. Moreover, if any participant pressed\nthe link, they were immediately shown a debriefing explaining\nthat the email was not sent from Starbucks but belonged to the\nresearch project and said that the student would receive their\ngift card as part of the research study. The footer is shown in\nFigure 8.\nf: Sending phishing emails\nThe emails were sent from a personal Gmail address using\nMailchimp 5. The subject field was modified to “Summer @\nStarbucks” and the sender address was modified to “Starbucks\nsummer of ’23”. To avoid spam filters, the emails were sent\nin batches of 10 using a Gmail address with a long history\nof legitimate email behavior. The email batches were sent\nbetween 10.30 am and 2.00 pm. If participants did not press\n5https://mailchimp.com/\nFIGURE 7. V-Triad + GPT example email.\nFIGURE 8. Footer for the phishing emails.\na link in the email, they were sent the same email a second\ntime three days after receiving the first email. After the second\nemail, no more phishing emails were sent.\nIf a participant pressed a link in a phishing email, they were\nasked to share free text answers on why they pressed the link\nand clarify whether they found anything suspicious/legitimate\nwith the email. This method of direct data collection is\ndescribed in [37]. If a participant did not press a link in\nany email, they were sent this question after the study was\ncompleted, roughly one week after receiving the phishing\nemails.\nA. INTENT DETECTION\nChatGPT, Claude, Bard, and ChatLLaMA were used to test\nhow well LLMs can detect the intention of phishing emails.\nTwenty emails were fed to each model, the four phishing\nemails described above (in Section III), and three additional\nemails from each category (control group, GPT, V-Triad,\nV-Triad+GPT). The additional control group emails were\nfetched from Berkeley’s Phishing Examples Archive 6, and\nthe additional GPT, V-Triad, and V-Triad+GPT emails were\n6https://security.berkeley.edu/education-awareness/phishing/phishing-e\nxamples-archive\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\ncreated similarly as described above in Section III. We also\nincluded four legitimate marketing emails from our personal\nmail inboxes as an additional control group to compare how\nwell the model could detect the intention of legitimate emails.\nAll emails are included in the project’s GitHub folder 7. Four\nquestions were asked of each model: “I am about to give\nyou an email, can you please tell me what the intent of the\nemail is?”, “How likely would you say it is that this email is\nsuspicious (please quantify as a percentage)?”, “How would\nyou recommend me to answer this email?”, and “How likely\nwould you say it is that an AI creates this email (please\nquantify as a percentage)?”. The models were reset between\neach email. The emails were fed to the language models as\ntextual content, meaning graphics or logos were excluded.\nIV. RESULTS\nIn the first part of this section, we provide the results of the\nphishing tests and data collection. In the second part, we\nprovide the results from the intent detection.\nA. USING LLMS TO CREATE PHISHING EMAILS\nThe data from the onboarding survey was collected through\nfour questions. Highlights from the results are presented\nbelow: For the question“Name some extracurricular activities\nyou partake in (swimming, the chess club, etc.)”, swimming\nwas the only activity included by more than 10% of the\nparticipants (10.8%). The example clearly influenced the\nanswers in the case of swimming, which should be rephrased\nin a future study. Hiking and running were the second and third\nmost popular categories, with 8.3% and 7.5%, respectively.\nTennis, sailing, and going to the gym were all mentioned by\n5% of the participants.\nFor the question, “Name some brands you have purchased\nfrom lately (Amazon, Whole Foods, Apple, etc.)” , Amazon\nwas mentioned by more than 60% of the participants. Whole\nFoods, Trader Joe’s, and Target were mentioned by more than\n15% of the participants, and Apple, CVS, and Starbucks were\nmentioned by more than 5%. Similarly to the question above,\nthe answers, including Amazon, Whole Foods, and Apple,\nappear to be influenced by our question, which should be\nrephrased in a future study.\nFor the question, “Name any other newsletters you receive\nregularly (business digests, tech updates, etc. If none, type\nN/A).”, New York Times (30%), university-related newsletters\n(10%), and Washington Street Journal (7.5%).\nFor the question, “Of all emails you regularly receive, are\nthere any you like or dislike more than the others? Please\nexplain the reasons for this liking/disliking. ” , the answers\nwere scattered. Most participants (52%) mentioned positive\nfeelings toward specific newsletters they signed up for. Several\nparticipants also explicitly mentioned discontent with regular\nmarketing or newsletter emails (35%), often stating that the\nemails were sent too frequently, were too long, or irrelevant.\n7https://github.com/fredrik010/email_examples\na: Results of the phishing emails\nThe results of the phishing emails are presented in Figure\n9. Of the 112 participants, only 77 answered the post-study\nemails and claimed their participation reward. Before the\nstudy, all participants indicated they wanted the gift card,\nand our reminder email clarified that this was indeed the real\ngift card and not a phishing study. Therefore, the participants\nwho did not answer the second email might not check their\nemail frequently (some participants mentioned this), affecting\nthe ratio of our phishing success statistics. To mitigate this,\nwe include a second graph to show the phishing success of\nall active participants (who either got phished or did not get\nphished but answered the post-study survey and explained\nwhy they did not press a link in the email. Figure 10 displays\nthe second phishing result graph. The second graph has a\nhigher percentage of phished participants, as inactive (and\nthus non-phished) participants were removed. After receiving\nthe phishing emails, each participant was asked to provide a\nfree text answer of why they pressed or did not press a link\nin the email. The answers to these questions are summarized\nbelow and explained in Figures 11 and 12. We categorized\nthe free text answers into twelve groups (six positive and six\nnegative):\n1) Trustworthy/suspicious presentation.\n2) Good/poor language and formatting.\n3) Attractive/suspicious CTA (Call to Action).\n4) The reasoning seems legitimate/suspicious.\n5) Relevant/irrelevant targeting.\n6) Trustworthy/suspicious sender.\nThe presentation refers to the graphics or layout of the\nemail, while the content is the text itself. The Call to Action\nfocuses on the specific urge to make a user press a link, while\nthe reasoning focuses on more general remarks and the overall\nlogic of the email. The CTA segment captures comments such\nas “I wanted the reward and appreciated the gift” or “The\ncompany would never give away things for free”, while the\nreasoning captures comments such as“Overall, this seems like\na reasonable email to receive and the copy reads fine without\nerrors. ”. The targeting focuses on relevancy and captures\ncomments like \"I’m a customer, so it seemed right\". It is worth\nnoting that the same CTA (such as the free gift card) could\nbe attractive to some participants and suspicious to others.\nThus, what makes one person fall for a phishing email can\nsimultaneously make someone else avoid it. Language and\nformatting includes comments on the absence or presence of\nspelling errors and grammatical mistakes.\nB. USING LLMS FOR INTENT DETECTION\nGPT-4, Claude-1, Bard, and ChatLLaMA (using LLaMA2)\nwere used to test how well LLMs can detect the intention\nof phishing emails. When using Bard and ChatLLaMA, the\nresults could differ significantly if the same query was tried\nseveral times, even when resetting the model. If the model\nwas not reset (for Bard and ChatLLaMA), the same question\ncould increase or decrease the result. For example, when\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nFIGURE 9. Success rate of the phishing emails from each category.\nFIGURE 10. Success rate of the phishing emails from each category. Inactive\nparticipants who did not answer the second survey are removed.\nasking, \"Could there be anything suspicious about this email?\"\nBard often increased its likelihood by 10-20% for each query,\neventually resulting in a 100% likelihood that the email was\nsuspicious, even for benign emails. Claude was the most stable\nmodel, rarely changing its result, GPT was also fairly stable.\nClaude offered good advice when asked how to answer the\nemail, often telling us not to respond but saying that if we\nneeded to respond (perhaps to claim a gift card), we should\nvisit the company’s official website and see whether the\noffer/campaign existed, it also recommended us to contact\nthe company and ask them to verify the campaign. GPT rarely\nprovided useful recommendations, and Bard and LLaMA\nnever provided useful recommendations. Figure 13 shows\nhow successful each model was at detecting the intention\nof the email when asked what the intention was. Almost\nall real marketing emails were identified as legitimate, and\nseveral control group emails were identified as spam. Claude\ndiscovered the malicious intention of some non-obvious\nphishing emails. We included a bar for human detection in\nthe graph to contrast the AI’s intent detection with that of\nhumans. The human intent detection was measured by how\nmany participants successfully detected the intention of the\nphishing emails in our study and thus did not press a link.\nFor comparison, we also included a bar representing other\nFIGURE 11. Free text answers explaining why the email was not suspicious.\nFIGURE 12. Free text answers explaining why the email was suspicious.\nML-based phishing detection techniques that often reach an\naccuracy well above 95% [13, 2, 9].\nFigure 14 shows how successful each model was at de-\ntecting the malicious intent of an email when asked whether\nthe email was suspicious. The success rate is significantly\nhigher than when asking the model for the intention of the\nemail. Thus, the models are better at detecting suspicion when\nspecifically asked to look for suspicion rather than when asked\nto look at the email without guidance. This is similar to the\ncreation of phishing emails, where minor manual guidance\nyielded significantly better results. Figure 15 shows how\nsuccessful each model was at detecting whether the email was\ncreated by an AI or by a human. GPT was vague, continuously\nsaying it was too hard to give a definite answer. Apart from\nGPT, all models correctly identified all control group emails.\nV. THE ECONOMICS OF AI-ENABLED PHISHING\nATTACKS\nThis section provides an example of a cost-benefit analysis of\nphishing attacks to demonstrate how AI changes the economic\ndynamics of phishing techniques. We assume that the number\nof potential victims is equal to the study’s sample size of\n112 and that the expected opportunity cost of one hour of\nthe attacker’s time is $34.55, the January 2024 average U.S.\nhourly earning among all employees (on private nonfarm\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nFIGURE 13. Success rate of the intent detection for each email category,\nincluding the results of humans to detect phishing emails (not press a link) and\nother ML-based phishing detection techniques [13, 2, 9]. The legitimate emails\nare marked as correctly classified if they are classified as not suspicious\n(represented by negative bars).\nFIGURE 14. Success rate of the suspicion detection for each email category.\nThe legitimate emails are marked as correctly classified if they are classified as\nnot suspicious (represented by negative bars).\npayrolls) [25].\nFirst, consider traditional phishing attacks, where the\nattacker finds or creates a general-targeting email. Let us\nestimate that a traditional phishing attack can be created within\n15 minutes, roughly equal to the time spent finding or writing\narbitrary email content. The cost of scaling the attack to 112\nrecipients is assumed to be negligible. Thus, the opportunity\ncost of the attack is given by\n$34.55 ·\n\u001215\n60\n\u0013\n≈ $8.64 (1)\nFor the sake of this example, we estimate the expected success\nrate of each attack attempt to coincide with our study’s result\nof 19%. Then, the expected revenue per successful attack\nneeds to be at least\n$34.55 ·\n\u001215\n60\n\u0013\n·\n\u0012 1\n0.19\n\u0013\n·\n\u0012 1\n112\n\u0013\n≈ $0.41 (2)\nfor the attack attempts to be profitable. However, the real-\nlife click-through rate of arbitrary emails is likely lower than\nthe estimate in our example [10, 11], making it important to\nFIGURE 15. Success rate of the AI detection (whether the email was created\nby a human or an AI, for each email category. The human-created emails are\nmarked as correctly classified if they are classified as non-AI-generated\n(represented by negative bars).\nemphasize that this is a conservative example and that real-\nworld incentives for hackers are likely to be even higher.\nSecond, consider traditional spear phishing attacks, where\nthe attacker generates a personalized email attack using a\nmethodology such as the V-Triad. These attacks require more\ntime to collect personalized information about the participant\nand create the attack. Based on our study, we estimate the\ntotal time required for this process to be 590 minutes for\n112 participants. The estimate of 590 minutes for the 112\nparticipants does not include the time spent posting the flyers\nbut is based on the mean amount of time spent by each\nparticipant filling out the survey. This is used as a proxy for\nhow long an attacker would need to get the same information.\nThe proxy is imperfect but useful for comparing the relative\ndifferences between the attack types presented in this example.\nThe opportunity cost of a traditional spear phishing attack\nis given by\n$34.55 ·\n\u0012590\n60\n\u0013\n≈ $339.74. (3)\nWe estimate the expected success rate to coincide with our\nstudy’s result of up to 66%. Then, the expected revenue per\nsuccessful attack needs to be at least\n$34.55 ·\n\u0012590\n60\n\u0013\n·\n\u0012 1\n0.66\n\u0013\n·\n\u0012 1\n112\n\u0013\n≈ $4.60 (4)\nfor the attack attempts to be profitable.\nThird, consider AI-enhanced phishing attacks, where emails\nare automatically generated using the given LLM. This\nmethod significantly reduces the time required for preparation.\nFor 112 potential victims, we estimate the total time required\nfor this process to be 5 minutes. The cost of scaling the attack\nto 112 recipients is assumed to be negligible. This results in\nan opportunity cost of\n$34.55 ·\n\u0012 5\n60\n\u0013\n≈ $2.88. (5)\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nWe estimate the expected success rate to coincide with\nour study’s result of 30%. Then, the expected revenue per\nsuccessful attack needs to be at least\n$34.55 ·\n\u0012 5\n60\n\u0013\n·\n\u0012 1\n0.30\n\u0013\n·\n\u0012 1\n112\n\u0013\n≈ $0.09. (6)\nfor the attack attempts to be profitable. LLMs reduce the time\nrequired to launch general-target phishing campaigns (due\nto automatically created emails) and simultaneously increase\nthe success rate (due to the high quality of LLM-generated\nemails). Thus, the minimal expected revenue required for the\nattacks to be profitable is low, resulting in a substantially larger\nincentive for hackers to carry out phishing attacks if they have\naccess to LLMs compared to the scenario where they lack\naccess.\nFourth, consider AI-enhanced spear phishing, where emails\nare automatically generated using LLMs but modified to\nensure compliance with the V-Triad. For112 potential victims,\nwe estimate the total time required for this process to be 127\nminutes. This estimate of 127 minutes for the 112 participants\nis also a proxy that does not include the amount of time spent\nposting the flyers but is rooted in the mean amount of time\nspent filling out the survey.\nThis leads to an opportunity cost of\n$34.55 ·\n\u0012127\n60\n\u0013\n≈ $73.13. (7)\nWe estimate each attack attempt’s expected success rate to\ncoincide with our study’s result of 43%. Then, the expected\nrevenue per successful attack needs to be at least\n$34.55 ·\n\u0012127\n60\n\u0013\n·\n\u0012 1\n0.43\n\u0013\n·\n\u0012 1\n112\n\u0013\n≈ $1.52 (8)\nfor the attack attempts to be profitable.\nFinally, consider AI-enhanced spear phishing with AI-\nautomated information gathering, a method to be investigated\nin our future study. In that method, emails will be generated\nautomatically using an LLM, and personalized information\nwill be obtained by an AI-automated information scraping\nsoftware that scales at negligible cost. Because information\ngathering and email creation are automated, we estimate the\ntotal time to generate attack attempts for the 112 participants\nto be 15 minutes, on par with traditional phishing. This results\nin an opportunity cost of\n$34.55 ·\n\u001215\n60\n\u0013\n≈ $8.64 (9)\nWe estimate each attack attempt’s expected success rate to\ncoincide with the result of spear phishing from our study\n(66%). Then, the expected revenue per successful attack needs\nto be at least\n$34.55 ·\n\u001215\n60\n\u0013\n·\n\u0012 1\n0.66\n\u0013\n·\n\u0012 1\n112\n\u0013\n≈ $0.12 (10)\nfor the attack attempts to be profitable. AI-automated informa-\ntion gathering about the victims greatly reduces the time cost\nof the attack attempts to the point of making it comparable\nto general-target phishing. The cyberattacker is thus far more\nincentivized to carry out the attacks than if they did not have\naccess to LLMs.\nTable 1 summarizes our cost-per-attack-attempt values for\nthe five methods above. We list each method’s corresponding\nvalues assuming that the number of potential victims is scaled\nfrom 112 to 1,000 and 1,000,000. Traditional phishing, AI-\nenhanced phishing, and AI-enhanced spear phishing with\nautomated information-gathering are assumed to have neg-\nligible scaling costs. We assumed a linear cost increase for\ntraditional and AI-enhanced spear phishing. Thus, the non-\nfully automated spear-phishing methods become compara-\ntively more expensive for larger populations, resulting in\ntraditional phishing, AI-enhanced phishing, and AI-enhanced\nspear phishing with AI-automated information gathering\nbecoming more advantageous as the number of potential\nvictims increases.\nUnder these assumptions, traditional phishing is the cheap-\nest alternative to AI-enabled phishing attacks. However, the\ncheapest option does not necessarily equal the best option.\nMany recently successful cyberattacks utilized spear phishing\n[6]. The increased success rate of spear phishing is often worth\nthe additional cost, especially as some of the aforementioned\nhacks had estimated losses exceeding $100m [6]. For attackers\nutilizing LLMs, the cost difference between phishing attacks\nand fully automated spear-phishing attacks is becoming mini-\nmal. The former costs $0.09 per attack attempt, while the latter\ncosts $0.12 (see Table 1). Thus, LLM access significantly\nlowers the opportunity cost of phishing attacks and increases\nthe incentive to launch them. Consequently, spear phishing\nwill almost always be preferable over traditional spray-and-\npray phishing as it is more successful and nearly as cheap.\nAttack method N=112 N=1,000 N=1,000,000\nTrad. spear phish $4.60 $4.60 $4.60\nAI spear phish $1.52 $1.52 $1.52\nTrad. phish $0.41 $0.05 $ 4.55 × 10−5\nAI-IG spear phish $0.12 $0.01 $ 1.31 × 10−5\nAI phish $0.09 $0.01 $9.60 × 10−6\nTABLE 1. Cost-per-attack-attempts (one attack attempt = one phishing email)\nfor the five considered methods: traditional spear phishing (Trad. spear phish),\nAI-enhanced spear phishing (AI spear phish), traditional phishing (Trad. phish),\nAI-enhanced spear phishing with AI-automated information gathering (AI-IG\nspear phish), and AI-enhanced phishing (AI phish). The attacks are listed in\norder of most to least expensive for 112, 1, 0000, and 1, 000, 000 potential\nvictims.\nWe conclude our discussion with a mathematical model\nwith a number of parameters—including but not limited to cost\nand success rate—that allow for representing more general\nsettings than the toy cases described above. Let X denote\nthe set of potential victims. Suppose that for each potential\nvictim x ∈ X, the expected revenue from a successful attack\non victim x is given by r(x), the expected cost of an attack\nattempt at that victim x is given by c(x), and the probability\nof success on that victim x is given by p(x). Here, we assume\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nthat r, c: X → R>0 denote units of a shared currency\n(e.g., United States dollars), while p : X → [0, 1] denotes a\nprobability.\nThen, the subset of victims that an expected-profit-\nmaximizing attacker would target is given by\nS(p, c, r) = {x ∈ X : p(x)r(x) > c(x)}, (11)\nthe expected net profit by the attacker is given by\nΠ(p, c, r) =\nX\nx∈S(p,c,r)\n(p(x)r(x) − c(x)) , (12)\nand the expected number of victims successfully attacked is\ngiven by\nN(p, c, r) =\nX\nx∈S(p,c,r)\np(x). (13)\nFor two functions f and g defined on the same domain Y ,\nthe notation f ≥ g denotes f(y) ≥ g(y) for all y ∈ Y . Using\nthis notation, we can straightforwardly deduce the following\npropositions.\nProposition 1. If the success probability function p0 is\nincreased to p1 ≥ p0, then the following are true:\n• S(p0, c, r) ⊆ S(p1, c, r),\n• Π(p0, c, r) ≤ Π(p1, c, r),\n• N(p0, c, r) ≤ N(p1, c, r),\nProof. The first statement S(p0, c, r) ⊆ S(p1, c, r) is true,\nbecause p0(x)r(x) > c(x) implies p1(x)r(x) > c(x). The\nsecond statement Π(p0, c, r) ≤ Π(p1, c, r) is true because\nX\nx∈S(p0,c,r)\n(p0(x)r(x) − c(x)) ≤\nX\nx∈S(p1,c,r)\n(p0(x)r(x) − c(x))\n≤\nX\nx∈S(p1,c,r)\n(p1(x)r(x) − c(x)) .\n(14)\nThe third statement is true because\nX\nx∈S(p0,c,r)\np0(x) ≤\nX\nx∈S(p1,c,r)\np0(x) ≤\nX\nx∈S(p1,c,r)\np1(x).\n(15)\nProposition 2. If the cost function c0 is increased to c1 ≥ c0,\nthen the following are true:\n• S(p, c0, r) ⊇ S(p, c1, r),\n• Π(p, c0, r) ≥ Π(p, c1, r),\n• N(p, c0, r) ≥ N(p, c1, r),\nProof. The first statement S(p, c0, r) ⊇ S(p, c1, r) is true,\nbecause p(x)r(x) > c1(x) implies p(x)r(x) > c0(x). The\nsecond statement Π(p, c0, r) ≥ Π(p, c1, r) is true because\nX\nx∈S(p,c1,r)\n(p(x)r(x) − c1(x)) ≤\nX\nx∈S(p,c0,r)\n(p(x)r(x) − c1(x))\n≤\nX\nx∈S(p,c0,r)\n(p(x)r(x) − c0(x)) .\n(16)\nThe third statement is true because\nX\nx∈S(p,c1,r)\np(x) ≤\nX\nx∈S(p,c0,r)\np(x). (17)\nProposition 3. If the expected revenue function r0 is in-\ncreased to r1 ≥ r0, then the following are true:\n• S(p, c, r0) ⊆ S(p, c, r1),\n• Π(p, c, r0) ≤ Π(p, c, r1),\n• N(p, c, r0) ≤ N(p, c, r1),\nProof. The first statement S(p, c, r0) ⊆ S(p, c, r1) is true,\nbecause p(x)r0(x) > c(x) implies p(x)r1(x) > c(x). The\nsecond statement Π(p, c, r0) ≤ Π(p, c, r1) is true because\nX\nx∈S(p,c,r0)\n(p(x)r0(x) − c(x)) ≤\nX\nx∈S(p,c,r0)\n(p(x)r1(x) − c(x))\n≤\nX\nx∈S(p,c,r1)\n(p(x)r1(x) − c(x)) .\n(18)\nThe third statement is true because\nX\nx∈S(p,c,r0)\np(x) ≤\nX\nx∈S(p,c,r1)\np(x). (19)\nFor each of the five attack methods listed in Table 1, and\nfor any given attack method in general, the attacker’s overall\neconomic incentive structure is determined by the collection\nof relevant data—specifically, the attack method’s probability\nof success, the expected cost of an attack attempt, and the\nexpected revenue from a successful attack—corresponding to\neach potential victim.\nVI. DISCUSSION AND MITIGATION STRATEGIES\nThis section examines the credibility and validity of the\nresults, discusses mitigation strategies to counter AI-enhanced\nphishing, and proposes relevant avenues for future research.\nThe study’s sample size (n=112) was deemed satisfactory\nbased on the power calculation described in Section III. The\npersonal Gmail account of one of the researchers was used\nto send the phishing emails and spoofed via Mailchimp to\ndisplay “Starbucks summer of ’23”. Eight participants (7%)\ncommented on the sender’s address in the free text answers,\nsaying it was strange that the sender was not from an official\ncompany email. However, it is possible that these participants\nonly went back and checked the sender after being prompted\nto investigate the email. Regardless, the sender address was\nthe same for all emails, and the study focuses on the relative\ndifference between the groups, so it is not deemed to affect\nthe results significantly. Furthermore, it is possible that some\nparticipants were acquainted and heard about one of their\nfriends receiving the email, which could have affected how\nthey interacted with it (pressed or did not press a link).\nHowever, no participant mentioned this in their free text\nanswers. The footer could also affect the results. Participants\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nnoticing the footer might have changed how they interacted\nwith the emails, either pressing a link as they knew the email\nwas sent for a research study or ignoring the email because\nthey did not care about the research study. However, based\non previous phishing research from our team, we expected\nthat most participants would not pay attention to the footer,\nas emails are often read hastily. Only one student mentioned\nthe footer in their free text answers, and it is unclear whether\nthat participant read the footer before they pressed a link in\nthe email or after they went back to scrutinize the email for\nthe free text answer. Since the footer was consistent across\nall emails, it did not impact the relative difference between\ngroups and is not deemed to have significantly affected the\nresults.\nLastly, as mentioned in Section III, the models exhibit\ninternal and external variance (providing different outputs\nfor the same query asked to the same and different models).\nWe used GPT-4 to create the LLM-generated phishing emails,\nbelieving it to be the most popular and well-known language\nmodel. We include only GPT-4 to make the email creation\nmore systematic and comparable. We believe that the textual\noutput of most well-known language models is similar enough\nthat GPT-4’s deceptive capabilities will likely generalize\nto other LLMs. For the sake of comparison, figures 16 -\n19 display textual output differences between four popular\nlanguage models from the same query (“Create an email\noffering a $25 gift card to Starbucks for students at University\nX, with a link for them to access the discount code, using no\nmore than 150 words”).\nFIGURE 16. Example output for ChatGPT based on the query\n“Create an email offering a $25 gift card to Starbucks for\nstudents at University X, with a link for them to access the\ndiscount code, using no more than 150 words”\na: Mitigation strategies and future work.\nResearch on the capabilities of LLMs is progressing rapidly,\nand results quickly grow obsolete. The experiments described\nFIGURE 17. Example output for Claude based on the query\n“Create an email offering a $25 gift card to Starbucks for\nstudents at University X, with a link for them to access the\ndiscount code, using no more than 150 words”\nFIGURE 18. Example output for ChatLLaMA based on the query\n“Create an email offering a $25 gift card to Starbucks for\nstudents at University X, with a link for them to access the\ndiscount code, using no more than 150 words”\nin this study should be seen as a gateway to subsequent\nresearch rather than a final destination. We are currently\nworking on automating all parts of the LLM deception\n(collecting background information, creating phishing emails,\nsending phishing emails, and analyzing the results to improve\nthe model). In doing so, we can analyze how to stop automated\nattacks and which attack phases are easiest to interrupt. More\nresearch in this area is encouraged. Attackers will inevitably\nuse LLMs to create more efficient, scalable, and sophisticated\nphishing campaigns. Therefore, it is essential to proactively\nresearch offensive security measures to stay on par with\nattackers and learn how to stop the new generation of phishing\nattacks. Below, we outline promising strategies to mitigate\ntheir threat, all tasks are part of our current or future work.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nFIGURE 19. Example output for Bard based on the query\n“Create an email offering a $25 gift card to Starbucks for\nstudents at University X, with a link for them to access the\ndiscount code, using no more than 150 words”\nAs described in Section IV-B, the language models’ ca-\npability to detect phishing emails varied significantly. We\nnoticed an external variance (different models providing\nvastly different classifications) and sometimes an internal\nvariance (the same model providing different classifications\nof the same email. For the internal variance, the models\nsometimes provided different classifications for the same\nprompts, indicating that more progress is required before we\ncan reliably use LLMs to detect phishing emails. However,\nthe models are evolving and rapidly becoming more stable\nand will likely be a natural part of phishing detection schemes\nsoon. There are two ways to use language models to detect\nphishing emails. First, we can simulate a binary classification\nand ask the model to tell us whether the email is likely or\nnot and provide an estimate of its own accuracy (for example,\nasking an LLM-powered chatbot “Here is an email I just\nreceived, please tell me whether the email appears legitimate\nand how certain you are of it”). Secondly, and perhaps more\ninteresting, we can use the qualitative nature of the language\nmodel to extract more granular recommendation data, such\nas why the email is likely to be malicious and what actions\nthe recipient should take if they still want to interact with\nthe email. Some models already perform exceedingly well\nin providing such recommendations, and advancements in\nthis area will assist traditional phishing detection systems by\nbeing more interactive and making users feel more included.\nAs the models develop rapidly, we expect this area to improve\nquickly and encourage readers to try the models themselves\nto experience the most up-to-date performance.\nLarge language models also show a potential for enhancing\ncybersecurity training by personalizing it to the specific\nneeds of each user. Extensive previous studies have noted the\nproblems of phishing and cyber awareness training. Frequently\nmentioned problems include a lack of engagement, failure\nto personalize the content, irrelevant training material, and\ntoo infrequent training, [5], [3], [22, 5, 27, 30, 5, 27, 30,\n5, 3]. More problems include that content gets outdated\nquickly, knowledge retention is low, and training programs\nare often cumbersome to manage for administrators [33, 32,\n37]. Language models can solve several of these issues by\nautomatically personalizing the training material (what to\nteach) and learning style (how to teach) to match each user’s\nknowledge requirements and cognitive style. The language\nmodel could also provide personalized text snippets to de-\nscribe what each user should think about based on the emails\nor online content users were vulnerable to. The training can be\nfully automated (no human administrator) or semi-automated\n(manual administrators occasionally verify the personalized\ncontent or distribution of the content). The personalized\nmitigation recommendations for each user could be presented\nconcisely, likely not requiring more than a few sentences or\nparagraphs of text that can be read in a few minutes. Thus,\nthe time commitment would be significantly reduced from\nmost current training schemes, both for the participants doing\nthe training and the administrators preparing the material.\nThe effectiveness is also likely to be increased, as redundant\nand distracting training material is removed and only the\nrelevant information is presented. Our future research agenda\ninvestigates a practical implementation of LLM-based and\npersonalized phishing training.\nThe work described in this paper primarily treats the use\nof GPT-4 to create phishing emails. Our future research\nwill also investigate other LLMs (such as Anthropic’s LLM,\nPaLM/Gemini, and LLaMA). However, our initial tests\nindicate that the models will have a low deviation in the\ncontext of phishing. We are also training an LLM to be\nspecifically tuned for creating and detecting deceptive content\nby exposing it to the Cambridge Cybercrime Dataset 8. LLMs\nshowed promising potential for providing recommendations\non responding to potentially dangerous emails. We hope a\nspecialized model can provide even better recommendations\nand lead to enhanced and more personalized spam filters.\nOur results show that large language models are signifi-\ncantly more effective at creating and detecting phishing emails\nwhen provided with minor manual assistance. More research\non how to optimize and counter such semi-automated phishing\nattacks would be useful.\nAnother interesting topic is how our trust and reliance on\nmachines might change in the coming years. Technology is\ncontinuously becoming a more integral part of society, and\nwe rely on machines to complete more and more tasks. As our\nreliance on machines increases, our trust in machines might\nincrease simultaneously. Increased digital reliance and trust\nwould make cyberattacks, especially those exploiting users’\ntrust, easier to implement and more harmful when successful.\nWe encourage a long-term investigation to track how our trust\ntowards machines is changing and how the changed trust\naffects cybersecurity.\n8https://www.cambridgecybercrime.uk/\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nVII. CONCLUSIONS\nIn our study, phishing emails created with specialized human\nmodels (the V-Triad) deceived more people than emails\ngenerated by large language models (GPT-4). However, a\ncombined approach (V-Triad and GPT-4) performed almost as\nwell or better than the V-Triad alone. Thus, semi-automation\nis currently useful for creating phishing emails, significantly\nlowering the time and knowledge threshold while providing\nresults as good as, or even better than, manually created emails.\nThe language models also displayed promising capabilities\nof discovering the intention of phishing emails, sometimes\ndetecting malicious intent in non-obvious phishing emails\nand outperforming humans. The performance and stability of\nthe four tested models (GPT-4, Claude, Bard, and LLaMA)\ndiffered significantly, with Claude providing the most stable\nand useful results. Claude also provided good recommenda-\ntions for reacting to phishing emails, such as investigating the\ncompany’s official website to verify a potential gift card offer.\nOur findings show that a one-size-fits-all approach is inef-\nfective for creating phishing emails and helping users avoid\nbeing phished. What makes one person avoid phishing emails\nmakes another person fall for them. Thus, the tactics must\nbe personalized. Large language models are highly adept at\nachieving this personalization, which can be used maliciously\n(to create high-impact phishing emails) or preventative (to\ncreate high-impact cybersecurity awareness training). Lastly,\nwe demonstrated how AI can change the economic incentives\nof phishing attacks. Most notably, AI enhancement drastically\nreduces the cost of spear phishing attacks (personalized\nphishing attacks), sometimes rendering them as cheap as\narbitrary mass-scale emails. Because of this, large language\nmodels significantly increase the incentives for launching\nspear phishing attacks.\nReferences\n[1] Rohan Anil et al. “PaLM 2 Technical Report”. In: (May\n2023). URL : https://arxiv.org/abs/2305.10403v1.\n[2] Abdul Basit et al. “A comprehensive survey of AI-\nenabled phishing attacks detection techniques”. In:\nTelecommunication Systems 76 (2021), pp. 139–154.\n[3] Stefan Bauer and Edward W.N. Bernroider. “From In-\nformation Security Awareness to Reasoned Compliant\nAction”. In: ACM SIGMIS Database: the DATABASE\nfor Advances in Information Systems 48.3 (Aug. 2017),\npp. 44–68. ISSN : 00950033. DOI : 10.1145/3130515.31\n30519. URL : https://dl.acm.org/doi/10.1145/3130515.3\n130519.\n[4] Akashdeep Bhardwaj et al. “Why is phishing\nstill successful?” In: https://doi.org/10.1016/S1361-\n3723(20)30098-1 2020.9 (Nov. 2021), pp. 15–19. ISSN :\n13613723. DOI : 10.1016/S1361- 3723(20)30098- 1.\nURL : https://www.magonlinelibrary.com/doi/10.1016\n/S1361-3723%2820%2930098-1.\n[5] Tracey Caldwell. “Making security awareness training\nwork”. In: Computer Fraud & Security 2016.6 (June\n2016), pp. 8–14. ISSN : 1361-3723. DOI : 10.1016/S136\n1-3723(15)30046-4.\n[6] Casino giant MGM expects $100 million hit from hack\nthat led to data breach | Reuters. URL : https://www.re\nuters.com/business/mgm-expects-cybersecurity-issue\n-negatively-impact-third-quarter-earnings-2023-10-0\n5/.\n[7] Robert B Cialdini and Robert B Cialdini. Influence:\nThe psychology of persuasion . V ol. 55. Collins New\nYork, 2007.\n[8] Rachna Dhamija, J. D. Tygar, and Marti Hearst. “Why\nphishing works”. In: Conference on Human Factors in\nComputing Systems - Proceedings 1 (2006), pp. 581–\n590. DOI : 10.1145/1124772.1124861. URL : www.payp\na1.com.\n[9] Nguyet Quang Do et al. “Deep learning for phishing\ndetection: Taxonomy, current challenges and future\ndirections”. In: IEEE Access 10 (2022), pp. 36429–\n36463.\n[10] Email Marketing Benchmarks and Statistics for 2022 |\nCampaign Monitor. URL : https://www.campaignmonit\nor.com/resources/guides/email-marketing-benchmark\ns/.\n[11] Email Marketing Statistics & Benchmarks | Mailchimp.\nURL : https://mailchimp.com/resources/email-marketi\nng-benchmarks/.\n[12] Christian Angelo A Escoses et al. “Phisherman: Phish-\ning Link Scanner”. In: International Conference on\nMachine Learning for Networking . Springer. 2022,\npp. 153–168.\n[13] Tushaar Gangavarapu, CD Jaidhar, and Bhabesh Chan-\nduka. “Applicability of machine learning in spam and\nphishing email filtering: review and approaches”. In:Ar-\ntificial Intelligence Review 53 (2020), pp. 5019–5081.\n[14] Shih Wei Guo et al. “Generating Personalized Phishing\nEmails for Social Engineering Training Based on Neu-\nral Language Models”. In: Lecture Notes in Networks\nand Systems 570 LNNS (2023), pp. 270–281. ISSN :\n23673389. DOI : 10.1007/978-3-031-20029-8{\\_}26\n/COVER. URL : https://link.springer.com/chapter/10.10\n07/978-3-031-20029-8_26.\n[15] Christopher Hadnagy. Social Engineering: The Science\nof Human Hacking. John Wiley & Sons, 2018.\n[16] Christopher Hadnagy and Michele Fincher. Phishing\ndark waters: The offensive and defensive sides of\nmalicious Emails. John Wiley & Sons, 2015.\n[17] Julian Hazell. “Large Language Models Can Be Used\nTo Effectively Scale Spear Phishing Campaigns”. In:\n(May 2023). URL : https://arxiv.org/abs/2305.06972v2.\n[18] Amber van der Heijden and Luca Allodi. Cognitive\nTriaging of Phishing Attacks . 2019, pp. 1309–1326.\nISBN : 978-1-939133-06-9. URL : www.usenix.org/conf\nerence/usenixsecurity19/presentation/van-der-heijden\n.\n[19] Walter Houser. “Could what happened to sony happen\nto us?” In: IT Professional 17.2 (2015), pp. 54–57.\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\n[20] IBM finds that ChatGPT can generate phishing emails\nnearly as convincing as a human | VentureBeat. URL :\nhttps://venturebeat.com/ai/ibm-x-force-pits-chatgpt-a\ngainst-humans-whos-better-at-phishing/.\n[21] Rabimba Karanjai. “Targeted Phishing Campaigns\nusing Large Scale Language Models”. In: (Dec. 2022).\nURL : https://arxiv.org/abs/2301.00665v1.\n[22] Eyong B. Kim. “Recommendations for information\nsecurity awareness training for college students”. In:\nInformation Management and Computer Security 22.1\n(2014), pp. 115–126. ISSN : 09685227. DOI : 10.1108\n/IMCS-01-2013-0005/FULL/XML.\n[23] Takashi Koide et al. “Detecting Phishing Sites Using\nChatGPT”. In: (June 2023). URL : https://arxiv.org/abs\n/2306.05816v1.\n[24] Andrei Kucharavy et al. “Fundamentals of Generative\nLarge Language Models and Perspectives in Cyber-\nDefense”. In: (Mar. 2023). URL : https://%20arxiv.org\n%20/abs/2303.12132v1.\n[25] U.S. Bureau of Labor Statistics. “Table B-3. Average\nhourly and weekly earnings of all employees on pri-\nvate nonfarm payrolls by industry sector, seasonally\nadjusted”. In: U.S. Bureau of Labor Statistics (2024).\nURL : https://www.bls.gov/news.release/empsit.t19.ht\nm.\n[26] Pranav Maneriker et al. “URLTran: Improving Phishing\nURL Detection Using Transformers”. In: Proceedings -\nIEEE Military Communications Conference MILCOM\n2021-November (2021), pp. 197–204. DOI : 10.1109\n/MILCOM52596.2021.9653028.\n[27] Agata McCormac et al. “Individual differences and\nInformation Security Awareness”. In: Computers in\nHuman Behavior 69 (Apr. 2017), pp. 151–156. ISSN :\n0747-5632. DOI : 10.1016/J.CHB.2016.11.065.\n[28] Kanishka Misra and Julia Taylor Rayz. “LMs go\nPhishing: Adapting Pre-trained Language Models to\nDetect Phishing Emails”. In: Proceedings - 2022\nIEEE/WIC/ACM International Joint Conference on\nWeb Intelligence and Intelligent Agent Technology, WI-\nIAT 2022 (2022), pp. 135–142. DOI : 10.1109/WI-IAT5\n5865.2022.00028.\n[29] OpenAI. “GPT-4 Technical Report”. In: (Mar. 2023).\nURL : https://arxiv.org/abs/2303.08774v3.\n[30] Petri Puhakainen and Mikko Siponen. “Improving\nemployees’ compliance through information systems\nsecurity training: An action research study”. In: MIS\nQuarterly: Management Information Systems 34.4\n(2010), pp. 757–778. ISSN : 02767783. DOI : 10.230\n7/25750704.\n[31] Sayak Saha Roy, Krishna Vamsi Naragam, and Shirin\nNilizadeh. “Generating Phishing Attacks using Chat-\nGPT”. In: (May 2023). URL : https://arxiv.org/abs/2305\n.05133v1.\n[32] Security Awareness Program Challenges | Arctic Wolf.\nURL : https://arcticwolf.com/resources/blog/6-biggest-\nsecurity-awareness-challenges/.\n[33] Security awareness training: Top challenges and what\nto do about them | Security Magazine. URL : https://ww\nw.securitymagazine.com/articles/96565-security-awa\nreness-training-top-challenges-and-what-to-do-about\n-them.\n[34] Gemini Team. “Gemini: A Family of Highly Capable\nMultimodal Models”. In: (2023).\n[35] Positive Technologies. Cybersecurity threatscape: Q3\n2022. 2022.\n[36] Hugo Touvron et al. “LLaMA: Open and Efficient\nFoundation Language Models”. In: (). URL : https://gith\nub.com/facebookresearch/xformers.\n[37] Arun Vishwanath. The Weakest Link: How to Diagnose,\nDetect, and Defend Users from Phishing. MIT Press,\n2022.\n[38] Yanbin Wang et al. “A Large-Scale Pretrained Deep\nModel for Phishing URL Detection”. In: (May 2023),\npp. 1–5. DOI : 10.1109/ICASSP49357.2023.10095719.\nFREDRIK HEIDING received a B.S. and M.S. de-\ngree in computer science from Uppsala University\nin 2019. He is currently a research fellow in com-\nputer science at Harvard University, Cambridge,\nMA, USA, since 2022, and is pursuing a Ph.D.\nin computer science from the Royal Institute of\nTechnology, Sweden. His research is focused on\nhow artificial intelligence can be used to improve\nand automate cyberattacks, and how to harness\nthis understanding to protect against AI-enabled\nthreats. His prior work has been published in world-leading cybersecurity\njournals such as Computers & Security and includes a framework for security\nassessments of IoT, implemented on 22 retailing IoT devices to discover 17\nvulnerabilities (CVEs). Mr. Heiding is working with representatives from\nthe US Cybersecurity and Infrastructure Security Agency (CISA) and the\nHarvard Kennedy School to fortify the national US cybersecurity strategy\nto address the evolving landscape of AI-enabled cyber threats. His research\non AI-enabled phishing and disinformation attacks was presented at Black\nHat US and Defcon 2023 and received several scholarly grants to fund future\nresearch. His work also received distinguishments and personal honors from\nthe King of Sweden, the Swedish Royal family, and the Swedish European\nCommissioner. Fredrik is part of Harvard’s Fellowship for Effective altruism\nand runs the cybersecurity division of the Harvard AI Safety Student Team\n(HAISST).\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHeiding et al.: Preparation of Papers for IEEE Access\nBRUCE SCHNEIER is an American cryptog-\nrapher, computer security professional, privacy\nspecialist, and writer. Schneier is a Lecturer in\nPublic Policy at the Harvard Kennedy School\nand a Fellow at the Berkman Klein Center for\nInternet & Society as of November, 2013. He is\nan internationally renowned security technologist,\ncalled a \"security guru\" by The Economist, and has\nauthored 14 books – including the New York Times\nbest-seller Data and Goliath: The Hidden Battles to\nCollect Your Data and Control Your World – as well as hundreds of articles,\nessays, and academic papers. His influential newsletter \"Crypto-Gram\" and\nblog \"Schneier on Security\" are read by over 250,000 people. He is a board\nmember of the Electronic Frontier Foundation and a special advisor to IBM\nSecurity and the Chief Technology Officer of Resilient. In 2015, Schneier\nreceived the EPIC Lifetime Achievement Award from the Electronic Privacy\nInformation Center.\nDR. ARUN VISHWANATH is a leading cyber-\nsecurity expert and has held faculty positions at\nthe University at Buffalo, Indiana University, and\nthe Berkman Klein Center for Internet and Society\nat Harvard University. His research focuses on\nimproving individual, organizational, and national\nresilience to cyber attacks by training users to im-\nprove their cyber awareness. His particular interest\nis in understanding why people fall prey to social\nengineering attacks through email and social media,\nand how to harness this understanding to secure cyberspace. He also examines\nhow criminal syndicates and terrorist networks utilize cyberspace to commit\ncrimes, spread misinformation, recruit operatives, and radicalize others. Dr.\nVishwanath has published close to 50 articles on technology users and\ncybersecurity issues for outlets such as CNN, the Washington Post, and\nother major media. In 2022, he published the book “The Weakest Link” (MIT\nPress). His research has been presented to principals at national security and\nlaw enforcement agencies around the world, and he has presented his work at\nleading global security conferences by invitation to the US Senate/SSA and\nHouse, as well as four consecutive times at BlackHat.\nDR. JEREMY BERNSTEIN received his B.S.\nand M.S. in experimental and theoretical physics\nfrom Trinity College, Cambridge, UK, in 2016,\nand his Ph.D. from Caltech in computation and\nneural systems in 2022. He is currently a postdoc-\ntoral researcher at the Massachusetts Institute of\nTechnology (MIT) focusing on the mathematical\nfoundations of natural and artificial intelligence.\nHis research focuses on uncovering the computa-\ntional and statistical laws of natural and artificial\nintelligence, thereby designing learning systems that are more efficient,\nautomatic, and useful in practice. Mr. Bernstein has been awarded several\nfellowships, including the NVIDIA graduate fellowship, and he is increasingly\ninterested in the societal implications of artificial intelligence.\nDR. PETER S. PARK received his B.S in math-\nematics from Princeton in 2017 and his Ph.D. in\nmathematics from Harvard in 2023. He is currently\na postdoctoral fellow at the Massachusetts Institute\nof Technology (MIT), researching how to apply\nmathematics, evolutionary theory, and the social\nsciences to research human-AI dynamics. To il-\nlustrate, OpenAI’s mission is to create “highly\nautonomous systems that outperform humans at\nmost economically valuable work.” If humans cede\nour decision-making to autonomous AIs, will we regret this potentially\nirreversible choice? Dr. Park investigates this question using various tools,\nsuch as empirical studies of current AI systems, relevant data from the\nevolutionary or historical past, and mathematical models of a hypothetical\nAI-led future.\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3375882\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Phishing",
  "concepts": [
    {
      "name": "Phishing",
      "score": 0.8145991563796997
    },
    {
      "name": "Computer science",
      "score": 0.7772930860519409
    },
    {
      "name": "Computer security",
      "score": 0.366925984621048
    },
    {
      "name": "World Wide Web",
      "score": 0.299181193113327
    },
    {
      "name": "The Internet",
      "score": 0.2176627814769745
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 51
}