{
    "title": "Structural Similarities Between Language Models and Neural Response Measurements",
    "url": "https://openalex.org/W4379538662",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4320862417",
            "name": "Li, Jiaang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4379640958",
            "name": "Karamolegkou, Antonia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286985106",
            "name": "Kementchedjhieva, Yova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223214302",
            "name": "Abdou, Mostafa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2210457936",
            "name": "Lehmann, Sune",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221452188",
            "name": "Søgaard, Anders",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4211074922",
        "https://openalex.org/W2947012833",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1827297289",
        "https://openalex.org/W2069490624",
        "https://openalex.org/W3014509321",
        "https://openalex.org/W3002330681",
        "https://openalex.org/W1848911709",
        "https://openalex.org/W2517394272",
        "https://openalex.org/W4299579390",
        "https://openalex.org/W3084038400",
        "https://openalex.org/W4320855028",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2963971961",
        "https://openalex.org/W2014592583",
        "https://openalex.org/W2499862792",
        "https://openalex.org/W4328049044",
        "https://openalex.org/W2511680308",
        "https://openalex.org/W2947899826",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W2904996081",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4285228148",
        "https://openalex.org/W2949616263",
        "https://openalex.org/W4298144575",
        "https://openalex.org/W4385571979",
        "https://openalex.org/W3117660267",
        "https://openalex.org/W2160654481",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3087357110",
        "https://openalex.org/W3010805239",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W2782213998",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4362655601",
        "https://openalex.org/W4362514950",
        "https://openalex.org/W1992570774",
        "https://openalex.org/W3199748991",
        "https://openalex.org/W2970854517",
        "https://openalex.org/W3167339967"
    ],
    "abstract": "Large language models (LLMs) have complicated internal dynamics, but induce representations of words and phrases whose geometry we can study. Human language processing is also opaque, but neural response measurements can provide (noisy) recordings of activation during listening or reading, from which we can extract similar representations of words and phrases. Here we study the extent to which the geometries induced by these representations, share similarities in the context of brain decoding. We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging. Code is available at \\url{https://github.com/coastalcph/brainlm}.",
    "full_text": "Structural Similarities Between Language Models and\nNeural Response Measurements\nJiaang Li∗,♠,C Antonia Karamolegkou∗,♠ Yova Kementchedjhieva♠\nMostafa Abdou♦ Sune Lehmann♣,D Anders Sogaard†,♠,D,C\n♠University of Copenhagen ♦Princeton University ♣Technical University of Denmark\nDPioneer Centre for AI, Denmark CCenter for Philosophy of AI, Denmark\nAbstract\nLarge language models (LLMs) have complicated internal dynamics, but induce\nrepresentations of words and phrases whose geometry we can study. Human\nlanguage processing is also opaque, but neural response measurements can provide\n(noisy) recordings of activation during listening or reading, from which we can\nextract similar representations of words and phrases. Here we study the extent to\nwhich the geometries induced by these representations, share similarities in the\ncontext of brain decoding. We find that the larger neural language models get, the\nmore their representations are structurally similar to neural response measurements\nfrom brain imaging. Code is available at https://github.com/coastalcph/\nbrainlm.\n1 Introduction\nUnderstanding how the brain works has intrigued researchers for many years. This challenge has\ngiven rise to the field of brain decoding, where the goal is to interpret the information encoded\nin the brain while a person is engaged in a specific cognitive task, such as reading or listening to\nlanguage. By analyzing representations of neural activity across different brain regions, researchers\ncan develop computational models that link specific patterns of brain activity to linguistic elements,\nsuch as words or sentences. This direction of research opens avenues for advancing our understanding\nof neurological disorders, developing innovative treatments, and enhancing the quality of life for\nindividuals with disorders.\nIn this paper, we investigate the alignment between the representations of words in LLMs and the\nneural response patterns observed in the human brain during language processing. What emerges is a\nstriking structural similarity between these two sets of representations, manifesting as a geometric\ncongruence in high-dimensional vector spaces. To quantify this alignment, we employ rigorous\nevaluation methods, including ridge regression, representational similarity analysis (RSA) [1], and\nProcrustes analysis [ 2] (if d = d′). These methodologies enable us to quantify the extent of\nisomorphism between LLMs and neural responses, e.g., functional magnetic resonance imaging\n(fMRI). Figure 1 illustrates the experimental flow and main results.\nArtificial intelligence researchers evaluate LLMs by measuring their performance on benchmark data\nand protocols [3–5]. Doing so, they aim to infer what LLMs have learned, from how they behave.\n∗Equal Contribution.\n†Correspondence to soegaard@di.ku.dk.\nNeurReps@NeurIPS 2023.\narXiv:2306.01930v2  [cs.CL]  31 Oct 2023\nFigure 1: Experimental flow and main results. We run experiments with three families of LLMs\n(comparing LLMs of different sizes within families), two fMRI datasets, and three projection\nalgorithms, and results are the same across all combinations: LLMs converge toward human-like\nrepresentations, enabling (P@10) retrieval rates of up to 25%, i.e., a quarter of all concepts can be\ndecoded from the fMRI signals. The datasets, our Gaussian smoothing technique, and the projection\nmethods are described in §3. Right side: Convergence results for three families of LLMs across\ntwo datasets, using Procrustes analysis. Convergence is consistent, and some retrieval rates are\nremarkably high, decoding almost half of the words correctly to a neighborhood of 30 word forms,\nwhich surpasses the random retrieval baselines represented by the dotted red lines.\nThe methodology is behaviorist and has obvious limitations. We instead suggest exploring the inside\nof LLMs and our brains – or, to be precise, their representational geometries. Our investigations span\nvarious LLM families, word embeddings, and diverse datasets, consistently revealing high degrees of\nstructural congruence. Our main contributions are as follows:\n• We find a remarkable structural similarity between how words are represented in LLMs,\nand the neural response measurements of humans reading the same words. The LLM\nrepresentations of a vocabulary form a geometry in a d-dimensional vector space; and the\nneural response measurements from one or more participants reading these words in a brain\nscanner, form in a similar way a geometry in a d′-dimensional space.\n• We present experiments for three families of LLMs (as well as one static embedding method),\nwith two different fMRI datasets, and three evaluation methods (ridge regression, RSA, and\nProcrustes analysis) to compute the structural similarity (degree of isomorphism) between\nthese two modal geometries.\n• Across the board, we see high degrees of isomorphism, enabling decoding or retrieval\nperformance (precision-at-k, a.k.a P@k) of up to P@10≈25% (with random performance\nbeing P@10<1%). Word-level brain decoding thus seems feasible particularly as language\nmodels increase in size.\n2 Related Work\nOver the past decade, researchers have explored the relationship between neural and language\nrepresentations by predicting text from brain activity [ 6–8]. Pereira et al. [9] were the first to\nbuild regression models to predict sentence representations from brain scans. Extending this work,\nMinnema and Herbelot [10] investigate several metrics to evaluate the decoder performance. Apart\nfrom regression, Sun et al. [11] also use similarity-based decoders where the decoder is trained to\nmap brain images to distinct sentence representations in both structured and unstructured settings.\n2\nAffolter et al. [12] use a neural network model to facilitate the brain-to-word regression decoder,\nand evaluate on unseen subjects for a more realistic approach. Oota et al. [13] propose two novel\nsetups using multi-view and cross-view regression decoders that predict semantic concepts and vector\nrepresentations respectively. Zou et al. [14] suggest a neural decoder in a cross-modal cloze setting\npredicting the target word given a contextual prompt. Finally, Tang et al.[15] build a decoder that\nreconstructs continuous language instead of individual words or sentences. Our focus is not on\nbuilding a better decoding system, but rather on exploring the alignment between neural and language\nrepresentational spaces within such systems.\n3 Methodology\nWe begin with a description of our empirical results. We have experimented with three families of\nLLMs, comparing the word representations they induce to human representations obtained from two\ndifferent neural response measurement datasets. We use three different comparison methods, leading\nto a total of 18 experiments, which all confirm the same trend.\n3.1 Data Description and Pre-processing\nfMRI datasets. fMRI is a non-invasive neural response measurement technique that records on a\nspatial resolution in the region of 1 to 6 millimetres, higher than any other technique. fMRI records\nactivity (blood flow) in the entire network of brain areas engaged when subjects undertake particular\ntasks. On the downside, fMRI is somewhat susceptible to influences of non-neural changes, and the\ntemporal response is poor relative to the electrical signals that define neuronal communication. To\ncompensate for low temporal resolution, we introduce Gaussian smoothing below. The datasets are:\nHarry Potter Dataset [16] (8 subjects) , and Natural Stories Audio Dataset [17] (19 subjects). Both\ndatasets are publicly available.\nGaussian smoothing. As mentioned, we use Gaussian smoothing to extract word-level neural\nresponse measurements in our two datasets. Gaussian smoothing has been used before to study\nspeech-aligned fMRI data [18, 19]. In cases where fMRI data is not collected at the granularity of\nindividual words, we can use Gaussian smoothing to generate word-level fMRI information. For\ninstance, to obtain the fMRI vector for a specific word like \"Harry\" at a given time point t (Harryt),\nwe can extract the fMRI vectors for a certain timeframe T around t, such as t ± T seconds. We then\napply Gaussian smoothing to this set of vectors, resulting in a final vector that represents the fMRI\ninformation for the word \"Harryt\". This approach has potential benefits for fMRI analysis in various\napplications, such as studies of language processing and cognitive neuroscience. By generating\nword-level fMRI information using Gaussian smoothing, we can potentially extend the scope from\nsequence-level to word-level, and improve the interpretability and accuracy of the results obtained\nfrom fMRI analyses. Extracting word-level signals differentiates our work from much other work,\nbut was also shown to be crucial in recent work on brain decoding [15].\n3.2 Models\nAuto-regressive models. Auto-regressive models generate output sequences by predicting each\nelement in the sequence based on the previously generated elements. In other words, the output is\ngenerated one element at a time, with the model conditioned on the previous output elements. These\nlanguage models are used to generate text but typically provide slightly worse similarity estimates.\nWe use two auto-regressive language model families: GPT2 [20] and OPT [21].\nNon-auto-regressive models. Non-auto-regressive models are a type of machine learning models\nthat take in an entire input sequence of text and generate a single output vector representation for\nthe entire sequence. Training such models, we mask a fraction of the words in the input text. The\nlanguage model is then expected to predict the masked words based on the other words in the text.\nWe use the BERT [23] family of language models as an example of a non-auto-regressive model\nfamily. See more details in Table 1.\nLanguage representation. In this study, the LLMs we utilized were trained on text segments, so\napplying these models to individual words in isolation might yield unpredictable results. Instead,\n3\nTable 1: The 14 language models used in our experiments. Appendix A Table 3 lists the links of LMs.\nLMs Hidden Layers Dimension Size Attention Heads Total # of Params Datasets\nBERT\n2 128 2 4.4M\nBooksCorpus [22], English Wikipedia [23]\n4 256 4 11.3M4 512 8 29.1M8 512 8 41.7M12 768 12 110.1M24 1,024 16 336M\nGPT2\n12 768 12 117M\nWebText [20]24 1,024 16 345M36 1,280 20 762M48 1,600 25 1,542M\nOPT\n12 768 12 125M BooksCorpus, CC-Stories[24], CCNewsV2[21], ThePile[25], Pushshift.io Reddit dataset [26]24 1,024 32 1.3B32 4,096 32 6.7B48 7,168 56 30B\nour approach involved retrieving naturally occurring instances of these words, along with their\nsurrounding context (i.e., the full sentence), from the fMRI text material. Consequently, we obtain\nrepresentations from the token positions that align with each word when encoding these sentences,\nand subsequently average the tokens representations of each word. To ensure decontextualization, we\nfurther averaged these representations across different sentences [27, 28].\n3.3 Comparison and Projection Methods\nRepresentational similarity analysis. Relational similarity analysis (RSA) is a multivariate analy-\nsis technique commonly used in cognitive neuroscience and computational linguistics to compare the\nsimilarity between two sets of representations [1]. RSA can be used to measure the similarity between\nthe neural activity patterns observed in the fMRI data and the representations learned by LLMs.\nRSA operates by first representing the neural activity and language model features as vectors in a\nhigh-dimensional space. The similarity between these vectors is then quantified using a rank-based\ncorrelation metric. We perform RSA following Lepori and McCoy [29]. Let X and Y be two sets\nof representations. We calculate their representational dissimilarity matrices (RDMs) as ⃗DX and\n⃗DY , respectively [29]. We then compare the representational geometries using Spearman’s rank\ncorrelation coefficient, denoted as ρ( ⃗DX, ⃗DY ).\nRidge regression. Ridge regression is a widely used method in statistics and machine learning\nto address the issue of multicollinearity, which can arise when there are highly correlated predictor\nvariables in a linear regression model. In contrast to Toneva and Wehbe [30], who utilized ridge\nregression for encoding fMRI data, our approach focuses on decoding, i.e. predicting language from\nfMRI. We achieve this by establishing a model that captures the connection between brain signals\nand individual dimensions within the language model representations. The models are trained to\npredict the signal of word w in layer l, denoted as ylw , using the vector of fMRI voxels for that word,\nxw. For each subject and layer l, we employ cross-validation to estimate the predictiveness of the\nfMRI representation of the word in each dimension i. In each fold, the fMRI data matrix with total\nn dimension denoted as X = xw1 , xw2 , ..., xwn, and the semantic vector matrix with m dimension,\ndenoted as Z = zw1 , zw2 , ..., zwm, are split into corresponding training and validation matrices which\nare individually normalized to have a mean of 0 and a standard deviation of 1 for each dimension\nacross words, ending with training matrices XR and ZR,l, as well as validation matrices XV and\nZV,l. Using the training fold, we estimate a model θi,l as follows:\narg min\nθi,l\n||zR,i − XRθi,l||2\n2 + λi||θi,l||2\n2\nTo identify the best λi for each dimension i that minimizes the nested cross-validation error, we\nemploy a ten-fold nested cross-validation. Subsequently, we estimate θi,l using λi on the entire\ntraining fold. Thus, the predictions for each dimension in the validation fold are obtained as\npl = XV θi,l.\nProcrustes analysis. We use Procrustes analysis, a form of statistical shape analysis, to align\nbrain fMRI representations with those of language models, using a fMRI-text dictionary (see §4.1).\nProcrustes analysis is a method for matching corresponding points in two shapes and finding the\ntransformation (translation, rotation, and scaling) that best aligns them. Under the constraint of\n4\northogonality, we aim to optimize the objective function Ω = min R ∥RX − Z∥F , subject to\nRT R = I, where matrix X refers to the fMRI matrix and matrix Z refers to representations of\nwords from LLMs. This optimization problem has a closed-form solution given by: Ω = UV T , and\nUΣV = SVD(ZX T ), where SVD represents the singular value decomposition.\n4 Experimental Setup\n4.1 fMRI-text Dictionary Complementation\nWe build a bimodal dictionary that associates fMRI data with corresponding textual information,\nutilizing fMRI datasets. Considering the context in which words are presented, it becomes evident\nthat the brain’s response to a particular word may vary significantly across different sentences. This\ndynamic response suggests that, within our constructed dictionary, the relationship between fMRI\nrecordings and textual entries exhibits a many-to-one correspondence. We employ a four-fold cross-\nvalidation approach that takes into account unique words, thereby preventing any potential train-test\nleakage. Due to individual differences among subjects, our experiments are conducted based on each\nsubject’s responses. We report the averaged results across all subjects.\n4.2 Evaluation - Linear Projection\nTo assess the effectiveness of regression and alignment techniques, we employ the P@ k metric,\nwhich quantifies the ratio of accurate predictions within the top k predictions. This evaluation metric\noffers a more cautious and robust assessment [31]. For Procrustes analysis, we induce it from a small\nset of point pairs and test it on held-out data measuring the P@ k [32], whereas using the whole\npoint pairs to assess the regression performance. What’s more, ensuring congruent dimensionality\nbetween the source and target spaces is a crucial prerequisite for successful alignment. In instances\nwhere a dimensionality mismatch arises, we employ principal component analysis to reduce the\ndimensionality of the larger space, ensuring compatibility.\nCross-domain similarity local scaling (CSLS). Nearest neighbor relationships are inherently\nasymmetric, and high-dimensional spaces can lead to ’hubness,’ where some vectors are hubs, while\nothers are anti-hubs [33]. To address this issue, Lample et al. [32] propose a bi-partite neighborhood\ngraph, in which each word of a given dictionary is connected to its K nearest neighbors in the other\nlanguage. They use cross-domain similarity local scaling (CSLS) to evaluate the similarity between\nmapped source and target words, which improves upon traditional nearest neighbor methods [32].\nWe use CSLS to calculate P@k.\nTable 2: Two different P@ k baselines with k ∈\n{1, 5, 10, 30, 50, 100} of two datasets. The random retrieval\nbaselines are calculated by the U.W. in stimulus content, re-\nspectively. U.W. = the number of unique words.\nDatasets U.W. P@1 P@5 P@10 P@30 P@50 P@100\nHarry PotterRandom 12910.08% 0.39% 0.77% 2.32% 3.87% 7.74%Harry PotterFastText 0.36% 3.66% 6.43% 12.76% 17.22% 26.52%\nNatural StoriesRandom 381 0.26% 1.31% 2.62% 7.87% 13.12% 26.25%Natural StoriesFastText 0.00% 1.88% 5.62% 17.27% 24.24% 39.89%\nRandom retrieval baseline. P@k\nis a metric that quantifies the propor-\ntion of words for which the LLM’s\nrepresentation serves as one of the k-\nnearest neighbors to the correspond-\ning fMRI encoding. In essence, word-\nlevel decoding involves a straightfor-\nward nearest-neighbor retrieval pro-\ncess within the projected space. It’s\ncrucial to note that our target vector\nspace, which represents the language model, contains hundreds of vectors. This feature sets our\nrandom baseline P@1 < 0.1%. Our target space of the text material in fMRI datasets makes the\nrandom retrieval baseline: P@1 = 1\nN\nPN\ni=1\n1\nU × 100%, where N represents the total number of\nunique words; i iterates over all words in the material; U refers to the total number of unique words.\nFastText baseline. The random retrieval baseline serves as a crucial benchmark, enabling us to\nassess the efficacy of aligning representations between the two modalities in the complete absence\nof any discernible signal (i.e., by chance). Nevertheless, surpassing the random baseline, strictly\nspeaking, does not definitively establish that contemporary LLMs are inherently more aligned with\nneural response measurements. To address this concern, we conducted a secondary baseline alignment\nexperiment by aligning fMRI recordings with word representations from fastText [34]. Further details\n5\n1 10 100 1000 10k 100k0\n10\n20\n30\n40\n50\n60\n1 10 100 1000 10k 100k\n1 10 100 1000 10k 100k0\n10\n20\n30\n40\n50\n60\n1 10 100 1000 10k 100k\nBERT - K=5\nBERT - K=10\nBERT - K=30\nGPT2 - K=5\nGPT2 - K=10\nGPT2 - K=30\nOPT - K=5\nOPT - K=10\nOPT - K=30\nMillions of parameters Millions of parameters\nP@KP@K\nNatural Story One (Procrustes Analysis) Natural Story One (Ridge Regression)\nHarry Potter (Procrustes Analysis) Harry Potter (Ridge Regression)\nFigure 2: Convergence Results for Three Families of Language Models on Two Datasets. The\ntask for Harry Potter dataset here is: Given a neural response, which word in a vocabulary of 1,291\nwords, was read at the time the response was recorded? Random retrieval baseline P@10 is less than\n1%, while the FastText baseline P@10 is less than 7%. See more details of baselines in Table 2.\ncan be found in Table 2. In practical applications, our mappings exhibit significantly higher precision,\nreflecting the inherent structural similarities between the language model and human brains.\n5 Results & Discussion\n5.1 Main Results.\nOur main results are presented in Figure 2 which illustrates the averaged results across all subjects,\nand concern the convergence of three families of LLMs on representations that are remarkably similar\nto those seen in neural response measurements. These results are consistent across two fMRI datasets\nand three mapping methods. See Appendix B (Figure 5) for similar results with RSA. The scores are\nplotted by model size, showing the convergence toward brain-like representations as LLMs increase\nin size. The best scores indicate that LLMs up to 1.5B parameters can achieve alignments such that a\nbit more than 1 in 5 words are decoded correctly,3 and a bit more than 2 in 5 almost correctly (within\nneighborhoods of 20-30 word forms). To gain a qualitative sense of the alignment between brain\nsignal and LLMs representations, see Figure 3. The results are obtained with limited supervision for\nlearning the mapping. In fact, we only rely on 950 data points to induce this linear projection, a small\nnumber given the high dimensionality of the derived word representations; see §3 for details.\n5.2 Discussion\nOur findings reveal a strong similarity between language model word representations and human brain\nresponses to language stimuli. As these neural language models expand in size, their representations\n3The reason we count P@5 or P@10 as correct decoding is that a neighborhood of 5-10 words will tend to\nconsist of inflections of the same lemma or synonymous words [35]. P@1 would amount to guessing the lemma,\nthe exact inflection, and the correct spelling variant.\n6\nbecome more akin to the patterns observed in neural responses from the fMRI scans. This discovery\npoints to the development of human-like representations within these large-scale language models,\noffering valuable insights into the intricate relationship between artificial intelligence and human\ncognitive processes.\n−100 −50 0 50 100\n−40\n−20\n0\n20\n40\n60\n80\nModalities\nfMRI\nOPT_30B\nTSNE for Harry Potter data after Procrustes Analysis\nX\nY\nwould_215\nnever_310\nPotter_2183\nnever\nwould\nPotter\nFigure 3: t-SNE plot of fMRI and LLM representations\nusing OPT-30B (large, uncased) over select target words from\nHarry Potter dataset. We evaluate the retrieval performance\nof our alignments using P@k, which measures the ratio of\nword tokens wi, e.g., for k = 5, ‘Potter2183’, whose fMRI\nrepresentations are projected into the LLM space such that\nthe LLM representation for the word type w, e.g., Potter,\nis among the 5-nearest neighbors of wi. In this case, the\nneural activity associated with ‘ never313’ is not read as\nnever directly – but still with never as the top-5 guess. That\nsaid, the words Potter and would are decoded correctly by\nour alignment (top-1 guess or P@1).\nNewman’s objection? Philoso-\nphers argue whether structural\nsimilarities (isomorphisms, homomor-\nphisms, etc.) between representations\nand what is represented, are sufficient\nfor content [36, 37]. Their concerns\nhave their origin in Newman’s\nobjection to Russellian structural\nrealism [38].\nBriefly put, Newman showed that\nstructuralist descriptions that abstract\naway from all but the logical struc-\nture, and simply assert the existence\nof a relation that induces a graph iso-\nmorphism between the representation,\nand what is represented, are indeed\ntrivial. Any LLM will, in other words,\ninduce word representations such that\nthe nearest neighbor graph over the\nword vocabulary V such that there ex-\nists a relation that is isomorphic to that\ngraph. Mollo and Millière [37], for ex-\nample, bring up Newman’s objection\nand write:\nphilosophical work\non theories of repre-\nsentational content\nhas long established [. . . that m]orphisms between two sets of objects or properties\nare trivial to find, and rely solely on the existence of morphisms between internal\nrepresentations and structured domains in the world could lead to a trivialisation of\nthe notions of representation and meaning . . .\nHowever, Newman’s objection only holds if all there is posited is the existence ofsome relation. If\nthe relations are properly restricted, isomorphism is far from trivial. One observation that goes all\nthe way back to Carnap’s Aufbau [39]4 is: Structural similarities are generally trivial to obtain, but\nif the relations (distances in the vector space) serve a purpose (do work for the system), structural\nsimilarities can ground content.\nStructural similarity is evidently sufficient to solve semantic problems, such as bilingual dictionary\ninduction [41] or multi-modal alignment [28]. The fact that fMRI vectors exhibit structural similarities\nto LLMs (and by transitivity, across languages and to computer vision models), is suggestive of such\nsimilarities playing a role in grounding.\nIn our case, we are not simply positing an isomorphic relation in neural responses. We are positing\nan isomorphism between two very specific relations: the nearest neighbor graph in the LLM rep-\nresentations, and the nearest neighbor graph in the fMRI data. In fact, these two relations are the\nsame relation, something which Newman himself proposed as a remedy to his own objection. It\nshould thus be clear that the result presented here is far from trivial.\nWhere are LLMs most brain-like? We also consider at what layers the different language models\nalign best with the representations extracted from the fMRI data. The results presented in Figure 4\n4Russell arguably had a similar response [40].\n7\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayers\n0\n2\n4\n6\n8P@5-CSLS\nModels\nBERT_TINY BERT_MINI BERT_SMALL BERT_MEDIUM BERT_BASE BERT_LARGE\nFigure 4: Alignment precision results across layers with Harry Potter dataset. The plot shows\nalignment with fMRI improves with model depth, for BERT and Procrustes Analysis; see Appendix\nB for similar plots for other LLMs and projection methods.\nand the Appendix B are unambiguous and show that deeper representations align better with neural\nresponse measurements. This holds across all architectures and model sizes.\nInterestingly, the alignment improvements at deeper layers do not wear off to reach a plateau. Our\nresults, in fact, suggest that better alignment results can be achieved by training even deeper models.\nThis may also explain the strong correlation between depth and generalization often observed in the\nliterature [42].\nIt has generally been found that the inner-most layers in LLMs encode for syntax, whereas the\nouter layers encode for semantics and pragmatics. One way to understand our results is therefore\nthat similarities between representations in human brains and LLMs are predominantly driven by\nsemantics and pragmatics.\n6 Conclusion\nWe presented a series of experiments showing that across three families of language models, word\nrepresentations converge toward being structurally similar to human neural responses. The larger\nand better the language models get, the more their representations align with human representations.\nThis result holds across datasets and three evaluation methods. We have discussed the philosophical\nsignificance of this result, including why Newman’s objection does not apply.\n7 Limitations\nOur study demonstrates the precise mapping of neural response measurements to language model\nrepresentation spaces through supervised learning. However, our findings are subject to certain\nconstraints. The utilization of fMRI signals with limited temporal resolution, albeit partially mitigated\nthrough Gaussian smoothing, may introduce potential confounds. Additionally, our primary focus\non the English language narrows the generalization ability of our results to languages with different\nlinguistic structures. Furthermore, relying on a single participant for each alignment may introduce\nindividual variability that could influence our conclusions. Moreover, our paper emphasizes the\nphilosophical interpretation of the linear mapping results, leaving the technical aspects of this\nalignment largely unexplored. To ensure the robustness and broader applicability of our findings,\nfuture research should encompass diverse languages, participant groups, and delve deeper into the\ntechnical underpinnings of the observed alignment between neural responses and language model\nrepresentations.\n8 Ethics\nIn our research, we analyze two publicly available fMRI datasets (Harry Potter Dataset and Natural\nStories Dataset). We did not collect any new dataset for our study. We encourage readers to refer to\nthe terms of use provided by the respective dataset sources for a more comprehensive understanding\nof their ethical guidelines and data usage policies. We do not foresee any harmful uses of this\ntechnology.\n8\nAcknowledgments and Disclosure of Funding\nThanks to the anonymous reviewers for their helpful feedback. Jiaang Li is supported by Carlsberg\nResearch Foundation (grant CF22-1432). Antonia Karamolegkou is supported by the Onassis\nFoundation - Scholarship ID: F ZP 017-2/2022-2023’.\nReferences\n[1] Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. Representational similarity analysis\n- connecting the branches of systems neuroscience. Front. Syst. Neurosci., 2:4, November 2008.\n[2] Yova Kementchedjhieva, Sebastian Ruder, Ryan Cotterell, and Anders Søgaard. Generalizing\nProcrustes analysis for better bilingual dictionary induction. In Proceedings of the 22nd\nConference on Computational Natural Language Learning, pages 211–220, Brussels, Belgium,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/K18-1021. URL\nhttps://aclanthology.org/K18-1021.\n[3] Aitor Lewkowycz, Ambrose Slone, Anders Andreassen, Daniel Freeman, Ethan S Dyer, Gaurav\nMishra, Guy Gur-Ari, Jaehoon Lee, Jascha Sohl-dickstein, Kristen Chiafullo, Liam B. Fedus,\nNoah Fiedel, Rosanne Liu, Vedant Misra, and Vinay Venkatesh Ramasesh. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities of language models. Technical report,\n2022.\n[4] Laura Cabello, Jiaang Li, and Ilias Chalkidis. Pokemonchat: Auditing chatgpt for pok \\’emon\nuniverse knowledge. arXiv preprint arXiv:2306.03024, 2023.\n[5] Melanie Mitchell and David C. Krakauer. The debate over understanding in ai’s large language\nmodels. Proceedings of the National Academy of Sciences , 120(13):e2215907120, 2023.\ndoi: 10.1073/pnas.2215907120. URL https://www.pnas.org/doi/abs/10.1073/pnas.\n2215907120.\n[6] Anders Søgaard. Evaluating word embeddings with fmri and eye-tracking. In Proceedings of\nthe 1st workshop on evaluating vector-space representations for NLP, pages 116–121, 2016.\n[7] Kalyan Ramakrishnan and Fatma Deniz. Non-complementarity of information in word-\nembedding and brain representations in distinguishing between concrete and abstract words.\nIn Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages\n1–11, 2021.\n[8] Sam Fereidooni, Viola Mocz, Dragomir Radev, and Marvin Chun. Understanding and improving\nword embeddings through a neuroscientific lens. bioRxiv, pages 2020–09, 2020.\n[9] Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, Samuel Gershman, Nancy\nKanwisher, Matthew Botvinick, and Evelina Fedorenko. Toward a universal decoder of\nlinguistic meaning from brain activation. Nature Communications , 9, 03 2018. doi:\n10.1038/s41467-018-03068-4.\n[10] Gosse Minnema and Aurélie Herbelot. From brain space to distributional space: The perilous\njourneys of fMRI decoding. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics: Student Research Workshop, pages 155–161, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-2021. URL\nhttps://aclanthology.org/P19-2021.\n[11] Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. Towards sentence-level brain\ndecoding with distributed representations. In Proceedings of the Thirty-Third AAAI Conference\non Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence\nConference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence ,\nAAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai.\nv33i01.33017047. URL https://doi.org/10.1609/aaai.v33i01.33017047.\n[12] Nicolas Affolter, Beni Egressy, Damian Pascual, and Roger Wattenhofer. Brain2word: Decoding\nbrain activity for language generation, 2020.\n9\n[13] Subba Reddy Oota, Jashn Arora, Manish Gupta, and Raju S. Bapi. Multi-view and cross-view\nbrain decoding. In Proceedings of the 29th International Conference on Computational Linguis-\ntics, pages 105–115, Gyeongju, Republic of Korea, October 2022. International Committee on\nComputational Linguistics. URL https://aclanthology.org/2022.coling-1.10.\n[14] Shuxian Zou, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. Cross-modal cloze\ntask: A new task to brain-to-word decoding. In Findings of the Association for Com-\nputational Linguistics: ACL 2022 , pages 648–657, Dublin, Ireland, May 2022. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.54. URL https:\n//aclanthology.org/2022.findings-acl.54.\n[15] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G. Huth. Semantic reconstruction of\ncontinuous language from non-invasive brain recordings. bioRxiv, 2022. doi: 10.1101/2022.\n09.29.509744. URL https://www.biorxiv.org/content/early/2022/09/29/2022.09.\n29.509744.\n[16] Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell.\nSimultaneously uncovering the patterns of brain regions involved in different story reading\nsubprocesses. PLoS One, 9(11):e112575, November 2014.\n[17] Yizhen Zhang, Kuan Han, Robert Worth, and Zhongming Liu. Connecting concepts in the brain\nby mapping cortical representations of semantic relations. bioRxiv, 2020. doi: 10.1101/649939.\nURL https://www.biorxiv.org/content/early/2020/03/26/649939.\n[18] Joachim Bingel, Maria Barrett, and Anders Søgaard. Extracting token-level signals of syntactic\nprocessing from fMRI - with an application to PoS induction. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 747–755, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:\n10.18653/v1/P16-1071. URL https://aclanthology.org/P16-1071.\n[19] Stefan Brodoehl, Christian Gaser, Robert Dahnke, Otto W. Witte, and Carsten M. Klingner.\nSurface-based analysis increases the specificity of cortical activation patterns and connectivity\nresults. Scientific Reports, 10, 2020.\n[20] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[21] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n[22] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations\nby watching movies and reading books. 2015 IEEE International Conference on Computer\nVision (ICCV), pages 19–27, 2015.\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://aclanthology.org/N19-1423.\n[24] Trieu H. Trinh and Quoc V . Le. A simple method for commonsense reasoning. ArXiv,\nabs/1806.02847, 2018.\n[25] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\nAn 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL\nhttps://arxiv.org/abs/2101.00027.\n10\n[26] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn.\nThe pushshift reddit dataset. CoRR, abs/2001.08435, 2020. URL https://arxiv.org/abs/\n2001.08435.\n[27] Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders\nSøgaard. Can language models encode perceptual structure without grounding? a case study in\ncolor. In Proceedings of the 25th Conference on Computational Natural Language Learning,\npages 109–132, Online, November 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.conll-1.9. URL https://aclanthology.org/2021.conll-1.9.\n[28] Jiaang Li, Yova Kementchedjhieva, and Anders Søgaard. Implications of the convergence of\nlanguage and vision model geometries. arXiv preprint arXiv:2302.06555, 2023.\n[29] Michael Lepori and R. Thomas McCoy. Picking BERT’s brain: Probing for linguistic depen-\ndencies in contextualized embeddings using representational similarity analysis. In Proceed-\nings of the 28th International Conference on Computational Linguistics , pages 3637–3651,\nBarcelona, Spain (Online), December 2020. International Committee on Computational Lin-\nguistics. doi: 10.18653/v1/2020.coling-main.325. URL https://aclanthology.org/2020.\ncoling-main.325.\n[30] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing\n(in machines) with natural language-processing (in the brain). In Proceedings of the 33rd\nInternational Conference on Neural Information Processing Systems, Red Hook, NY , USA,\n2019. Curran Associates Inc.\n[31] Antonia Karamolegkou, Mostafa Abdou, and Anders Søgaard. Mapping brains with language\nmodels: A survey, 2023.\n[32] Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou.\nWord translation without parallel data. InInternational Conference on Learning Representations,\n2018. URL https://openreview.net/forum?id=H196sainb.\n[33] Miloš Radovanovi´c, Alexandros Nanopoulos, and Mirjana Ivanovi´c. Hubs in space: Popular\nnearest neighbors in high-dimensional data. J. Mach. Learn. Res., 11:2487–2531, dec 2010.\nISSN 1532-4435.\n[34] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\nwith subword information. Transactions of the Association for Computational Linguistics, 5:\n135–146, 2017. ISSN 2307-387X.\n[35] Yova Kementchedjhieva, Mareike Hartmann, and Anders Søgaard. Lost in evaluation: Mislead-\ning benchmarks for bilingual dictionary induction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pages 3336–3341, Hong Kong, China,\nNovember 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1328. URL\nhttps://aclanthology.org/D19-1328.\n[36] Nicholas Shea. Content and its vehicles in connectionist systems. Mind and Language, 22(3):\n246–269, 2007. doi: 10.1111/j.1468-0017.2007.00308.x.\n[37] Dimitri Coelho Mollo and Raphaël Millière. The vector grounding problem, 2023.\n[38] M. H. A. Newman. Mr. russell’s causal theory of perception. Mind, 37(146):26–43, 1928. doi:\n10.1093/mind/xxxvii.146.137.\n[39] Rudolf Carnap. The Logical Structure of the World. Berkeley: University of California Press,\n1967.\n[40] Thomas Pashby. Understanding russell’s response to newman. 2015.\n[41] Anders Søgaard, Ivan Vuli ´c, Sebastian Ruder, and Manaal Faruqui. Cross-Lingual Word\nEmbeddings. Synthesis Lectures on Human Language Technologies. Morgan & Claypool\nPublishers, United States, 2 edition, 2019. doi: 10.2200/S00920ED2V01Y201904HLT042.\n11\n[42] Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit\nnumber recognition from street view imagery using deep convolutional neural networks. 2014.\nURL https://arxiv.org/pdf/1312.6082.pdf.\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural Information Processing Systems 32,\npages 8024–8035. Curran Associates, Inc., 2019. URLhttp://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\n[44] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL\nhttps://aclanthology.org/2020.emnlp-demos.6.\nA Implementation\nOur implementation is based on PyTorch v.1.13.1 [ 43] and Transformer v4.25.1 [ 44] for Python\n3.9.13 and builds on code from the repositories in Table 3.\nTable 3: Links of 14 Transformer-based language models used in our experiments.\nLMs Links\nBERTTINY https://huggingface.co/google/bert_uncased_L-2_H-128_A-2\nBERTMINI https://huggingface.co/google/bert_uncased_L-4_H-256_A-4\nBERTSMALL https://huggingface.co/google/bert_uncased_L-4_H-512_A-8\nBERTMEDIUM https://huggingface.co/google/bert_uncased_L-8_H-512_A-8\nBERTBASE https://huggingface.co/bert-base-uncased\nBERTLARGE https://huggingface.co/bert-large-uncased\nGPT2BASE https://huggingface.co/gpt2\nGPT2MEDIUM https://huggingface.co/gpt2-medium\nGPT2LARGE https://huggingface.co/gpt2-large\nGPT2XL https://huggingface.co/gpt2-xl\nOPT125M https://huggingface.co/facebook/opt-125m\nOPT1.3B https://huggingface.co/facebook/opt-1.3b\nOPT6.7B https://huggingface.co/facebook/opt-6.7b\nOPT30B https://huggingface.co/facebook/opt-30b\nB More Results\n12\nBERTTINY\nBERTMINI\nBERTSMALL\nBERTMEDIUM\nBERTBASE\nBERTLARGE\n0.1\n0.2\n0.3\n0.4\n0.5\nGPT2BASE\nGPT2MEDIUM\nGPT2LARGE\nGPT2XL\nOPT125M\nOPT1.3B\nOPT6.7B\nOPT30B\nRepresentational Similarity Analysis (RSA)\nRSA Score (%)\nHarry Potter Natural Stories\nFigure 5: Convergence results for three families of LLMs using Relational Similarity Analysis.\nThe correlation score ranges from 0 (no correlation) to 1 (perfect correlation). The plot shows that as\nthe model sizes increase, the representational similarities increase also.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayers\n0\n5\n10\n15\n20\n25P@50-CSLS\nModels\nBERT_TINY BERT_MINI BERT_SMALL BERT_MEDIUM BERT_BASE BERT_LARGE\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\nLayers\n0\n5\n10\n15\n20\n25\n30\n35P@50-CSLS\nModels\nGPT2_BASE\nOPT_125M\nGPT2_MEDIUM\nOPT_1.3B\nGPT2_LARGE\nOPT_6.7B\nGPT2_XL\nOPT_30B\nFigure 6: Alignment precision results across layers. The plot shows alignment with fMRI (Harry\nPotter dataset) improves with model depth for LLMs and Procrustes analysis with Gaussian Random\nProjection.\n13"
}