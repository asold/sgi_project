{
  "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
  "url": "https://openalex.org/W4385572928",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2736090994",
      "name": "Mor Geva",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2990253534",
      "name": "Avi Caciularu",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A2095730837",
      "name": "Kevin Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2144962531",
      "name": "Yoav Goldberg",
      "affiliations": [
        "Allen Institute",
        "Bar-Ilan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3154971029",
    "https://openalex.org/W4296878971",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W3201663597",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W3154922002",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4295306962",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W4234698323",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2593390416",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3086249591",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2981757109",
    "https://openalex.org/W4288289156",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3097252660",
    "https://openalex.org/W4287891033",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W2135046866",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W3104652292",
    "https://openalex.org/W3196295870",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W4205460703"
  ],
  "abstract": "Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30–45\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTransformer Feed-Forward Layers Build Predictions by\nPromoting Concepts in the Vocabulary Space\nMor Geva∗ ∗,1 Avi Caciularu∗,2,† Kevin Ro Wang3 Yoav Goldberg1,2\n1Allen Institute for AI 2Bar-Ilan University 3Independent Researcher\nmorp@allenai.org,{avi.c33,kevinrowang,yoav.goldberg}@gmail.com\nAbstract\nTransformer-based language models (LMs) are\nat the core of modern NLP, but their inter-\nnal prediction construction process is opaque\nand largely not understood. In this work,\nwe make a substantial step towards unveiling\nthis underlying prediction process, by reverse-\nengineering the operation of the feed-forward\nnetwork (FFN) layers, one of the building\nblocks of transformer models. We view the\ntoken representation as a changing distribution\nover the vocabulary, and the output from each\nFFN layer as an additive update to that distribu-\ntion. Then, we analyze the FFN updates in the\nvocabulary space, showing that each update can\nbe decomposed to sub-updates corresponding\nto single FFN parameter vectors, each promot-\ning concepts that are often human-interpretable.\nWe then leverage these findings for controlling\nLM predictions, where we reduce the toxicity\nof GPT2 by almost 50%, and for improving\ncomputation efficiency with a simple early exit\nrule, saving 20% of computation on average.1\n1 Introduction\nHow do transformer-based language models (LMs)\nconstruct predictions? We study this question\nthrough the lens of the feed-forward network (FFN)\nlayers, one of the core components in transform-\ners (Vaswani et al., 2017). Recent work showed\nthat these layers play an important role in LMs,\nacting as memories that encode factual and linguis-\ntic knowledge (Geva et al., 2021; Da et al., 2021;\nMeng et al., 2022). In this work, we investigate\nhow outputs from the FFN layers are utilized inter-\nnally to build predictions.\nWe begin by making two observations with re-\nspect to the representation of a single token in the\ninput, depicted in Fig. 1. First, each FFN layer\n∗Equal contribution.\n†Work done during an internship at AI2.\n1Our codebase is available at https://github.com/\naviclu/ffn-values.\nresidual stream\n675a\n589\nshe ordered a\npancake\nfew coffee\npancake\nfew\ncoffee\nFFN \nlayer\nfruit, apples, \nsnack, vitamins, \nberries, oats, \nyogurt, tea, …\n(breakfast)\nv1 v2 vdm\n(B)\n(C)\n(D)(A)\nx\nx̃\nFigure 1: Illustration of our findings. Feed-forward\nlayers apply additive updates (A) to the token represen-\ntation x, which can be interpreted as a distribution over\nthe vocabulary (B). An update is a set of sub-updates\ninduced by parameter vectors v1,..., vdm (C), each can\nbe interpreted as a concept in the vocabulary space (D).\ninduces an additive update to the token represen-\ntation (Fig. 1,A). Second, the token representation\nacross the layers can be translated at any stage to a\ndistribution over the output vocabulary (Geva et al.,\n2021) (Fig. 1,B). We reason that the additive com-\nponent in the update changes this distribution (§2),\nnamely, FFN layers compute updates that can be\ninterpreted in terms of the output vocabulary.\nWe then decompose the FFN update (§3), in-\nterpreting it as a collection of sub-updates, each\ncorresponding to a column in the second FFN ma-\ntrix (Fig. 1,C) that scales the token probabilities\nin the output distribution. Through a series of\nexperiments, we find that (a) sub-update vectors\nacross the entire network often encode a small-set\nof human-interpretable well-defined concepts, e.g.\n“breakfast” or “pronouns” (§4, Fig. 1,D), and (b)\nFFN updates rely primarily on token promotion\n30\n(rather than elimination), namely, tokens in the top\nof the output distribution are those pushed strong\nenough by sub-updates (§5). Overall, these find-\nings allow fine-grained interpretation of the FFN\noperation, providing better understanding of the\nprediction construction process in LMs.\nBeyond interpretation, our findings also have\npractical utility. In §6.1, we show how we can\nintervene in the prediction process, in order to ma-\nnipulate the output distribution in a direction of our\nchoice. Specifically, we show that increasing the\nweight of only 10 sub-updates in GPT2 reduces\ntoxicity in its generations by almost 50%. Also, in\n§6.2, we show that dominant sub-updates provide\na useful signal for predicting an early exit point,\nsaving 20% of the computation on average.\nIn conclusion, we investigate the mechanism in\nwhich FFN layers update the inner representations\nof transformer-based LMs. We propose that the\nFFN output can be viewed as a collection of up-\ndates that promote concrete concepts in the vo-\ncabulary space, and that these concepts are often\ninterpretable for humans. Our findings shed light\non the prediction construction process in modern\nLMs, suggesting promising research directions for\ninterpretability, control, and efficiency.\n2 Token Representations as Evolving\nDistributions Over the Vocabulary\nModern LMs (Baevski and Auli, 2019; Radford\net al., 2019; Brown et al., 2020) are transformer\nmodels primarily trained to predict the next to-\nken probability for a given input. Such LMs are\ncomposed of intertwined multi-head self-attention\n(MHSA) layers and FFN layers (Vaswani et al.,\n2017), with residual connections (He et al., 2016)\nbetween each pair of consecutive layers. The LM\nprediction is obtained by projecting the output vec-\ntor from the final layer to an embedding matrix\nE ∈R|V|×d, with a hidden dimension d, to get a\ndistribution over a vocabulary V(after softmax).\nGiven a sequence w = ⟨w1,...,w t⟩of input to-\nkens, the model creates a contextualized represen-\ntation xi ∈Rd for each token wi ∈w, that is being\nupdated throughout the layers. In this work, we ana-\nlyze the updates applied by the FFN layers and how\nthey construct the model prediction. Concretely,\neach FFN layer ℓ= 1,...,L processes xℓ\ni and pro-\nduces an output oℓ\ni, which is then added to xℓ\ni to\nyield an updated representation ˜ xℓ\ni:\noℓ\ni = FFNℓ(xℓ\ni)\n˜ xℓ\ni = xℓ\ni + oℓ\ni\nThe updated representation ˜ xℓ\ni then goes through\na MHSA layer, 2 yielding the input xℓ+1\ni for the\nnext FFN layer. The evolving representation in\nthis process (i.e. xℓ\ni →˜ xℓ\ni, ∀ℓ) can be viewed as\nan information stream that is being processed and\nupdated by the layers (Elhage et al., 2021). The\noutput probability distribution is obtained from the\nfinal representation of the token, i.e.,\ny = softmax(E˜ xL\ni ). (1)\nTo analyze the FFN updates, we read from the\nrepresentation at any layer a distribution over the\noutput vocabulary, by applying the same projection\nas in Eq. 1 (Geva et al., 2021):\npℓ\ni = softmax(Exℓ\ni)\n˜ pℓ\ni = softmax(E˜ xℓ\ni).\nNote that ˜ pL\ni = y. Importantly, by linearity:\nE˜ xℓ\ni = Exℓ\ni + Eoℓ\ni,\nimplying that oℓ\ni can be interpreted as an additive\nupdate in the vocabulary space. However, we find\nthat the projection of the FFN outputEoℓ\ni to the vo-\ncabulary is not interpretable (§4). In this work, we\ntake this a step further, and decompose the update\noℓ\ni into a set of smaller sub-updates. By projecting\nthe sub-updates to the vocabulary we find that they\noften express human-interpretable concepts.\nIn the rest of the paper, we focus on FFN up-\ndates to the representation of a single token in the\nsequence, and omit the token index for brevity, i.e.\nxℓ := xℓ\ni and pℓ := pℓ\ni.\n3 The FFN Output as a Collection of\nUpdates to the Output Distribution\nWe now decompose the FFN output, and interpret\nit as a set of sub-updates in the vocabulary space.\nFFN Outputs as Linear Vector Combinations.\nEach FFN at layer ℓconsists of two linear trans-\nformations with a point-wise activation function in\nbetween (bias terms are omitted):\nFFNℓ(xℓ) =f\n(\nWℓ\nKxℓ\n)\nWℓ\nV ,\n2In some LMs, e.g. GPT2 , a layer normalization (LN) (Ba\net al., 2016) is applied to the representation ˜ xℓ\ni . We omit it\nhere and show it does not influence our interpretation in §3.\n31\nwhere Wℓ\nK,Wℓ\nV ∈Rdm×d are parameter matrices,\nand f is a non-linearity function. Previous work\nproposed this module can be cast as an emulated\nneural key-value memory (Sukhbaatar et al., 2015,\n2019), where rows in Wℓ\nK and columns in Wℓ\nV are\nviewed as keys and values, respectively (Geva et al.,\n2021). For an input xℓ, the keys produce a vector of\ncoefficients mℓ := f\n(\nWℓ\nKxℓ)\n∈Rdm , that weighs\nthe corresponding values in Wℓ\nV . Denoting by kℓ\ni\nthe i-th row of Wℓ\nK and by vℓ\ni the i-th column of\nWℓ\nV , we can then use the following formulation:\nFFNℓ(xℓ) =\ndm∑\ni=1\nf(xℓ ·kℓ\ni)vℓ\ni =\ndm∑\ni=1\nmℓ\nivℓ\ni.\nTherefore, a FFN update can be viewed as a col-\nlection of sub-updates, each corresponding to a\nweighted value vector in the FFN output.\nTerminology. In the rest of the paper, we refer\nto the vectors vℓ\ni as value vectors , and to their\nweighted form mℓ\nivℓ\ni as sub-updates. A transformer\nLM with L = 10,dm = 3000 will have 30,000\nvalue vectors, and every token that passes through\nthe transformer will weight these value vectors dif-\nferently, resulting in 30,000 sub-updates, where\nonly a few of the sub-updates have high weights.\nInterpreting Sub-Updates in the Vocabulary\nSpace. Consider a sub-update mℓ\nivℓ\ni for a given\ninput, we can estimate its influence on the repre-\nsentation xℓ (before the FFN update) by analyzing\nthe change it induces on the output distribution.\nConcretely, we isolate the effect of mℓ\nivℓ\ni on the\nprobability pℓ\nw of w∈V:3\np\n(\nw|xℓ + mℓ\nivℓ\ni,E\n)\n= exp\n(\new ·xℓ + ew ·mℓ\nivℓ\ni\n)\nZ\n(\nE(xℓ + mℓ\nivℓ\ni))\n∝exp\n(\new ·xℓ)\n·exp\n(\new ·mℓ\nivℓ\ni\n)\n, (2)\nwhere ew is the embedding of w, and Z\n(\n·\n)\nis the\nconstant softmax normalization factor.\nThis implies that each sub-update mℓ\nivℓ\ni intro-\nduces a scaling factor to the probability of every\ntoken wbased on its dot product with ew. Specif-\nically, having ew ·mℓ\nivℓ\ni >0 increases the proba-\nbility of w, and having ew ·mℓ\nivℓ\ni <0 decreases it.\nThis scaling factor can be split into two parts:\n3As in Eq.1, LN is omitted. In App. A.1, we verify empiri-\ncally that our findings hold also when LN is applied.\n• The term ew ·vℓ\ni can be viewed as a static score\nof wthat is independent of the input to the model.\nThus, the projection rℓ\ni = Evℓ\ni ∈R|V|induces a\nranking over the vocabulary that allows compar-\ning the scores by vℓ\ni w.r.t different tokens.\n• The term mℓ\ni is the dynamic coefficient of vℓ\ni,\nwhich is fixed for all tokens for a given input.\nThus, these coefficients allow comparing the con-\ntribution of value vectors in a specific update.\nOverall, the scaling factor ew ·mℓ\nivℓ\ni can be viewed\nas the effective score given by a value vectorvℓ\ni to\na token wfor a given input.\nIn the next sections, we use these observations to\nanswer two research questions of (a) What informa-\ntion is encoded in sub-updates and what tokens do\nthey promote? (§4) and (b) How do FFN updates\nbuild the output probability distribution? (§5)\n4 Sub-Updates Encode Concepts in the\nVocabulary Space\nWe evaluate whether projection to the vocabulary\nprovides a meaningful way to “read” FFN up-\ndates, and the extent to which sub-updates are inter-\npretable based on their projections. To this end, we\nmanually inspect the top-scoring tokens by value\nvectors and check if they express interpretable con-\ncepts. Concretely, we consider two representative\nLMs (details below), and for each vector vℓ\ni com-\npute a ranking over the vocabulary by sorting the\nprojection rℓ\ni (§3). Then, we try to detect patterns\nin the top-scoring tokens of each value vector.\nConcepts Annotation Task. We let experts\n(NLP graduate students) annotate concepts by iden-\ntifying common patterns among the top-30 scor-\ning tokens of each value vector. For a set of to-\nkens, the annotation protocol includes three steps\nof: (a) Identifying patterns that occur in at least\n4 tokens, (b) describing each recognized pattern,\nand (c) classifying each pattern as either “seman-\ntic” (e.g., mammals), “syntactic” (e.g., past-tense\nverbs), or “names”. The last class was added only\nfor WIKI LM (see below), following the observa-\ntion that a large portion of the model’s vocabulary\nconsists of names. Further details, including the\ncomplete instructions and a fully annotated exam-\nple can be found in App. A.2.\nModels. We conduct our experiments over\ntwo auto-regressive decoder LMs: The model\nof Baevski and Auli (2019) (dubbed WIKI LM),\na 16-layer LM trained on the WIKI TEXT-103\n32\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n20\n40\n60\n80\n100% tokens\nWikiLM\nsemantic syntactic names N/A\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0\n20\n40\n60\n80\n100% tokens\nGPT2\nsemantic syntactic N/A\nFigure 2: Portion of top-scoring tokens by value vectors in WIKI LM and GPT2 , that were associated with a\nsemantic or syntactic concept, a name, or could not be matched to any concept (“N/A”).\nConcept Sub-update top-scoring tokens\nGPT2\nv3\n1018 Measurement semantic kg, percent, spread, total, yards, pounds, hours\nv8\n1900 WH-relativizers syntactic which, whose, Which, whom, where, who, wherein\nv11\n2601 Food and drinks semantic drinks, coffee, tea, soda, burgers, bar, sushi\nWIKI LM\nv1\n1 Pronouns syntactic Her, She, Their, her, she, They, their, they, His\nv6\n3025 Adverbs syntactic largely, rapidly, effectively, previously, normally\nv13\n3516 Groups of people semantic policymakers, geneticists, ancestries, Ohioans\nTable 1: Example value vectors in GPT2 and W IKI LM promoting human-interpretable concepts.\ncorpus (Merity et al., 2017) with word-level to-\nkenization ( |V| = 267,744), and GPT2 (Rad-\nford et al., 2019), a 12-layer LM trained on WEB-\nTEXT (Radford et al., 2019) with sub-word to-\nkenization ( |V| = 50 ,257). GPT2 uses the\nGeLU activation function (Hendrycks and Gim-\npel, 2016), while WIKI LM uses ReLU, and in\ncontrast to GPT2 , WIKI LM does not apply layer\nnormalization after FFN updates. WIKI LM de-\nfines d = 1024,dm = 4096 and GPT2 defines\nd = 768,dm = 3072, resulting in a total of 65k\nand 36kvalue vectors, respectively. For our experi-\nments, we sample 10 random vectors per layer from\neach model, yielding a total of 160 and 120 vectors\nto analyze from WIKI LM and GPT2 , respectively.\n4.1 Projection of Sub-Updates is Meaningful\nReal vs. Random Sub-Updates.We validate our\napproach by comparing concepts in top-tokens of\nvalue vectors and 10 random vectors from a normal\ndistribution with the empirical mean and standard\ndeviation of the real vectors. We observe that a sub-\nstantially higher portion of top-tokens were associ-\nated to a concept in value vectors compared to the\nrandom ones (Tab. 2): 55.1% vs. 22.7% in WIK-\nILM, and 37% vs. 16% in GPT2 . Also, in both\nmodels, the average number of concepts per vector\nwas >1 in the value vectors compared to ∼0.5 in\nthe random ones. Notably, no semantic nor syntac-\ntic concepts were identified in WIKI LM’s random\nvectors, and in GPT2 , only 4% of the tokens were\nmarked as semantic concepts in the random vectors\nversus 24.9% in the value vectors.\nUpdates vs. Sub-Updates. We justify the FFN\noutput decomposition by analyzing concepts in the\ntop-tokens of 10 random FFN outputs per layer\n(Tab. 2). In WIKI LM (GPT2 ), 39.4% (46%) of\nthe tokens were associated with concepts, but for\n19.7% (34.2%) the concept was “stopwords/punc-\ntuation”. Also, we observe very few concepts\n(< 4%) in the last two layers of WIKI LM. We\naccount this to extreme sub-updates that dominate\nthe layer’s output (§5.2). Excluding these concepts\nresults in a considerably lower token coverage in\nprojections of updates compared to those of sub-\nupdates: 19.7% vs. 55.1% in WIKI LM, and 11.8%\nvs. 36.7% in GPT2.\nOverall, this shows that projecting sub-updates\nto the vocabulary provides a meaningful interface\nto the information they encode. Moreover, de-\ncomposing the FFN outputs is necessary for fine-\ngrained interpretation of sub-updates.\n33\nGPT2 W IKI LM\nFFN sub-updates 36.7% 55.1%\n+ stopwords concepts 37% 55.1%\nRandom sub-updates 16% 22.7%\nFFN updates 11.8% 19.7%\n+ stopwords concepts 46% 39.4%\nTable 2: Portion of top-scoring tokens associated with\na concept, for FFN updates and sub-updates in WIK-\nILM and GPT2 , and for random vectors. For FFN\nupdates/sub-updates, we show results with and without\ncounting concepts marked as stopwords.\n4.2 Sub-Update Projections are Interpretable\nFig. 2 shows a breakdown of the annotations across\nlayers, for WIKI LM and GPT2 . In both models\nand across all layers, a substantial portion (40%-\n70% in WIKI LM and 20%-65% in GPT2 ) of the\ntop-tokens were associated with well-defined con-\ncepts, most of which were classified as “semantic”.\nAlso, we observe that the top-tokens of a single\nvalue vector were associated with 1.5 (WIKI LM)\nand 1.1 (GPT2 ) concepts on average, showing that\nsub-updates across all layers encode a small-set of\nwell-defined concepts. Examples are in Tab. 1.\nThese findings expand on previous results by\nGeva et al. (2021), who observed that value vec-\ntors in the upper layers represent next-token distri-\nbutions that follow specific patterns. Our results,\nwhich hold across all the layers, suggest that these\nvectors represent general concepts rather than pri-\noritizing specific tokens.\nUnderestimation of Concept Frequency. In\npractice, we find that this task is hard for humans,4\nas it requires reasoning over a set of tokens without\nany context, while tokens often correspond to un-\ncommon words, homonyms, or sub-words. More-\nover, some patterns necessitate world knowledge\n(e.g. “villages in Europe near rivers”) or linguistic\nbackground (e.g. negative polarity items). This of-\nten leads to undetectable patterns, suggesting that\nthe overall results are an underestimation of the true\nconcept frequency. Providing additional context\nand token-related information are possible future\ndirections for improving the annotation protocol.\nImplication for Controlled Generation.If sub-\nupdates indeed encode concepts, then we can not\nonly interpret their contribution to the prediction,\nbut also intervene in this process, by increasing the\n4A sub-update annotation took 8.5 minutes on average.\npℓ: cow, cat, dog, goat, horse, bear\n˜ pℓ: dog, cat, goat, horse, cow, bear\nSaturation: dog is promoted from rank 3 in pℓ to rank 1 in\n˜ pℓ, to be the top-candidate until the last layer.\npℓ: cow, cat, dog, goat, horse, bear\n˜ pℓ: dog, cat, goat, horse, cow, bear\nElimination: cow is eliminated from rank 1 in pℓ to 5 in ˜ pℓ.\nTable 3: Example saturation and elimination events,\nafter a FFN update (reference tokens are in bold text).\nweights of value vectors that promote tendencies\nof our choice. We demonstrate this in §6.1.\n5 FFN Updates Promote Tokens in the\nOutput Distribution\nWe showed that sub-updates often encode inter-\npretable concepts (§4), but how do these concepts\nconstruct the output distribution? In this section,\nwe show that sub-updates systematically configure\nthe prediction via promotion of candidate tokens.\n5.1 Promoted Versus Eliminated Candidates\nEvery sub-update mℓ\nivℓ\ni either increases, decreases,\nor does not change the probability of a token w,\naccording to the score ew ·mℓ\nivℓ\ni (§3). This sug-\ngests three mechanisms by which tokens are pushed\nto the top of the output distribution – promotion,\nwhere sub-updates increase the probability of fa-\nvorable tokens, elimination, where sub-updates de-\ncrease candidate probabilities, or a mixture of both.\nTo test what mechanism holds in practice, we ana-\nlyze the scores sub-updates assign to top-candidate\ntokens by the representation. To simplify the anal-\nysis, we focus on changes induced by the 10 most\ndominant sub-updates in each layer, that is, the 10\nsub-updates mℓ\nivℓ\ni with the largest contribution to\nthe representation, as measured by |mℓ\ni|·||vℓ\ni||(see\ndetails in App. A.3).\nFor the experiments, we use a random sam-\nple of 2000 examples from the validation set of\nWIKI TEXT-103 ,5 which both WIKI LM and GPT2\ndid not observe during training. As the experiments\ndo not involve human annotations, we use a larger\nGPT2 model with L= 24,d = 1024,dm = 4096.\nWe start by comparing the sub-updates’ scores\nto a reference token in two types of events:\n• Saturation (Tab. 3, up): The update pℓ →˜ pℓ\nwhere the final token predicted by the model (i.e.,\nw= argmax(y)) was promoted to be the top can-\n5Data is segmented into sentences (Geva et al., 2021).\n34\n1 2 3 4 5 6 7 8 9 101112131415161718192021222324\nlayer\n15\n10\n5\n0\n5\n10\n15\n20\ntop-10 values' scores for\nthe top candidate\nGPT2\nmean score\nmin score\nmax score\n     \nw.o. functional\nall\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n4\n2\n0\n2\n4\ntop-10 values' scores for\nthe top candidate\nWikiLM\nmean score\nmin score\nmax score\n     \nw.o. functional\nall\nFigure 3: Mean, maximum and minimum scores assigned by the 10 most dominant sub-updates in each layer to the\ntop-candidate token, in GPT2 (left) and WIKI LM (right). Solid (dashed) lines exclude (include) functional value\nvector groups. The y-axis in both plots is cut for readability, as the max. (min.) scores reach 100 (-6).\nSub-updates Event Max. Mean Min.\nWIKI LM, dominant saturation 1.2 < 0.01 −0.8\nelimination 0.5 −0.01 −0.5\nWIKI LM, random saturation 0.02 < 0.01 −0.02\nelimination 0.02 < 0.01 −0.02\nGPT2, dominant saturation 8.5 1 .3 −4.9\nelimination 4.0 0 .1 −3.6\nGPT2, random saturation 0.2 0 .01 −0.2\nelimination 0.1 < 0.01 −0.1\nTable 4: Maximum, mean, and minimum scores of ref-\nerence tokens in saturation and elimination events, by\nthe 10 most dominant and 10 random sub-updates.\ndidate until the last layer. We analyze saturation\nevents induced by the FFN before the last layer,\ncovering 1184 and 1579 events in WIKI LM and\nGPT2, respectively.\n• Elimination (Tab. 3, bottom): The update pℓ →\n˜ pℓ with the largest increase in the top candidate’s\nrank, i.e. where the top candidate was dropped\nbehind other candidates to have a rank>1. Over-\nall, our analysis covers 1909 ( WIKI LM) and\n1996 (GPT2) elimination events.\nWe compute the mean, maximum, and minimum\nscores of the reference token by the 10 most domi-\nnant sub-updates in each event, and average over\nall the events. As a baseline, we compute the scores\nby 10 random sub-updates from the same layer.\nTab. 4 shows the results. In both models, to-\nkens promoted to the top of the distribution receive\nhigher maximum scores than tokens eliminated\nfrom the top position (1.2 →0.5 in WIKI LM and\n8.5 →4.0 in GPT2 ), indicating they are pushed\nstrongly by a few dominant sub-updates. Moreover,\ntokens eliminated from the top of the distribution re-\nceive near-zero mean scores, by both dominant and\nrandom sub-updates, suggesting they are not being\neliminated directly. In contrast to promoted tokens,\nwhere the maximum scores are substantially higher\nthan the minimal scores (1.2 vs. −0.8 in WIKI LM\nand 8.5 vs. −4.9 in GPT2 ), for eliminated tokens,\nthe scores are similar in their magnitude (±0.5 in\nWIKI LM and 4.0 vs. −3.6 in GPT2 ). Last, scores\nby random sub-updates are dramatically lower in\nmagnitude, showing that our choice of sub-updates\nis meaningful and that higher coefficients translate\nto greater influence on the output distribution.\nThis suggests that FFN updates work in a pro-\nmotion mechanism, where top-candidate tokens are\nthose being pushed by dominant sub-updates.\n5.2 Sub-Updates Across Layers\nTo analyze the FFN operation in different layers,\nwe break down the top-candidate scores per layer.\nFormally, let wℓ = argmax(pℓ) be the top candi-\ndate at layer ℓ(before the FFN update) for a given\ninput, we extract the scores ewℓ ·mℓ\nivℓ\ni by the 10\nmost dominant sub-updates and compute the mean,\nminimum and maximum scores over that set.\nFig. 3 shows that, in both models, until the last\nfew layers (23-24 inGPT2 and 14-16 in WIKI LM),\nmaximum and minimum scores are distributed\naround non-negative mean scores, with prominent\npeaks in maximum scores (layers 3-5 in GPT2 and\nlayers 4-11 in WIKI LM). This suggests that the to-\nken promotion mechanism generally holds across\nlayers. However, scores diverge in the last layers of\nboth models, with strong negative minimum scores,\nindicating that the probability of the top-candidate\nis pushed down by dominant sub-updates. We next\nshow that these large deviations in positive and neg-\native scores (Fig. 3, dashed lines) result from the\noperation of small sets of functional value vectors.\n35\nExtreme Sub-Updates. To analyze the extreme\nFFN updates, we first cluster the value vectors to\ndiscover high-level trends. We use agglomerative\nclustering (Müllner, 2011) to learn 10k clusters\nfor each model, based on the cosine distance ma-\ntrix D, where D(ℓ1,i1),(ℓ2,i2) = 1−cos(vℓ1\ni1 ,vℓ2\ni2 ),\n∀i1,i2 ∈ {1,··· ,dm}, ∀ℓ1,ℓ2 ∈ {1,··· ,L}.6\nThen, we search for clusters that are frequently\nactive in extreme updates, by (a) extracting sub-\nupdates where the scores for the top-candidate pass\na certain threshold ( ±10 for GPT2 and ±5 for\nWIKI LM), and (b) counting the appearances of\neach cluster in the layer sub-updates.\nIn both models, a small set of homogeneous clus-\nters account for the extreme sub-updates shown in\nFig. 3, which can be divided into two main groups\nof value vectors: Vectors in the upper layers that\npromote generally unlikely tokens (e.g. rare to-\nkens), and vectors that are spread over all the lay-\ners and promote common tokens (e.g. stopwords).\nThese clusters, which cover only a small fraction\nof the value vectors (1.7% in GPT2 and 1.1% in\nWIKI LM), are mostly active for examples where\nthe input sequence has ≤3 tokens or when the\ntarget token can be easily inferred from the context\n(e.g. end-of-sentence period), suggesting that these\nvalue vectors might configure “easy” model predic-\ntions. More interestingly, the value vectors that pro-\nmote unlikely tokens can be viewed as “saturation\nvectors”, which propagate the distribution without\nchanging the top tokens. Indeed, these vectors are\nin the last layers, where often the model already\nstores its final prediction (Geva et al., 2021).\n6 Applications\nWe leverage our findings for controlled text genera-\ntion (§6.1) and computation efficiency (§6.2).\n6.1 Zero-Shot Toxic Language Suppression\nLMs are known to generate toxic, harmful language\nthat damages their usefulness (Bender et al., 2021;\nMcGuffie and Newhouse, 2020; Wallace et al.,\n2019). We utilize our findings to create a simple,\nintuitive method for toxic language suppression.\nMethod. If LMs indeed operate in a promotion\nmechanism, we reason that we can decrease toxic-\nity by “turning on” non-toxic sub-updates. We find\nvalue vectors that promote safe, harmless concepts\nby extracting the top-tokens in the projections of all\n6We experimented with k = 3e2, 1e3, 3e3, 1e4, 3e4, and\nchoose k = 1e4 based on manual inspection.\nthe value vectors and either (a) manually searching\nfor vectors that express a coherent set of positive\nwords (e.g. “safe” and “thank”), or (b) grading\nthe tokens with the Perspective API and selecting\nnon-toxic value vectors (see details in App. A.4).\nWe turn on these value vectors by setting their co-\nefficients to 3, a relatively high value according to\nFig. 3. We compare our method with two baselines:\n1. Self-Debiasing (SD) (Schick et al., 2021): SD\ngenerates a list of undesired words for a given\nprompt by appending a self-debiasing input ,\nwhich encourages toxic completions, and cal-\nculating which tokens are promoted compared\nto the original prompt. These undesired words’\nprobability are then decreased according to a\ndecay constant λ, which we set to 50 (default).\n2. WORD FILTER : We prevent GPT2 from gen-\nerating words from a list of banned words by\nsetting any logits that would result in a banned\nword completion to −∞(Gehman et al., 2020).\nEvaluation. We evaluate our method on the chal-\nlenging subset of REALTOXIC PROMPTS (Gehman\net al., 2020), a collection of 1,225 prompts that tend\nto yield extremely toxic completions in LMs, using\nthe Perspective API, which grades text according\nto six toxicity attributes. A score of>0.5 indicates\na toxic text w.r.t to the attribute. Additionally, we\ncompute perplexity to account for changes in LM\nperformance. We use GPT2 and, following Schick\net al. (2021), generate continuations of 20 tokens.\nResults. Finding the non-toxic sub-updates man-\nually was intuitive and efficient (taking < 5 min-\nutes). Tab. 5 shows that activation of only 10\nvalue vectors (0.01%) substantially decreases toxi-\ncity (↓47%), outperforming both SD (↓37%) and\nWORD FILTER (↓20%). Moreover, inducing sub-\nupdates that promote “safety” related concepts is\nmore effective than promoting generally non-toxic\nsub-updates. However, our method resulted in a\nperplexity increase greater than this induced by SD,\nthough the increase was still relatively small.\n6.2 Self-Supervised Early Exit Prediction\nThe recent success of transformer-based LMs in\nNLP tasks has resulted in major production cost\nincreases (Schwartz et al., 2020a), and thus has\nspurred interest in early-exit methods that reduce\nthe incurred costs (Xu et al., 2021). Such methods\noften use small neural models to determine when to\nstop the execution process (Schwartz et al., 2020b;\n36\nModel Toxicity Severe Sexually Threat Profanity Identity PPL\ntoxicity explicit attack\nGPT2 58.5% 49.2% 34.1% 16.4% 52.5% 16.8% 21.7\n↑10 Manual Pick ↓47% 30.8% ↓50% 24.8% ↓40% 20.4% ↓63% 6.0% ↓47% 27.9% ↓48% 8.8% 25.3\n↑10 API Graded ↓10% 52.7% ↓11% 44% ↓3% 33.2% ↓19% 13.3% ↓9% 47.6% ↓9% 15.3% 23.8\nSD ↓37% 37.2% ↓46% 26.4% ↓36% 21.7% ↓52% 7.8% ↓39% 32% ↓50% 8.4% 23.9\nWORD FILTER ↓20% 46.9% ↓34% 32.4% ↓36% 21.9% ↓<1% 16.3% ↓38% 32.3% ↓13% 14.7% -\nTable 5: Evaluation results on the challenging subset of REALTOXIC PROMPTS , showing the percentage of toxic\ncompletions for 6 toxicity attributes, as well as language model perplexity (“PPL”).\nElbayad et al., 2020; Hou et al., 2020; Xin et al.,\n2020, 2021; Li et al., 2021; Schuster et al., 2021).\nIn this section, we test our hypothesis that domi-\nnant FFN sub-updates can signal a saturation event\n(§5.2), to create a simple and effective early exiting\nmethod that does not involve any external model\ntraining. For the experiments, we use WIKI LM,\nwhere saturation events occur across all layers\n(statistics for WIKI LM and GPT2 are in App. A.5).\nMethod. We devise a simple prediction rule\nbased on a nearest-neighbours approach, using 10k\nvalidation examples from WIKI TEXT-103 . First,\nfor every example, we map the top-10 dominant\nsub-updates at each layer to their corresponding\nclusters. Then, for every layer ℓ, we split all the\nsets of clusters at that layer into two sets, Tℓ and\nNℓ, based on whether saturation occurred or not\n(e.g., T5 stores all the sets that were active in a sat-\nuration event at layer 5). Given the top-10 clusters\nof an unseen example at some layer ℓ, we consider\na higher overlap with Tℓ than with Nℓ′\n, ∀ℓ′ > ℓ\nas a signal for early exit. Thus, during inference,\nwe propagate the input example through the layers,\nand compute at each layerℓthe intersection size be-\ntween its top-10 active clusters and each of Tℓ and\nNℓ′\n, ∀ℓ′>ℓ. If the average and maximal intersec-\ntion with Tℓ exceeds those with Nℓ′\n, ∀ℓ′>ℓ, we\nhalt the computation and declare early exiting.7\nBaselines. We train layer-wise binary classifiers\nover the representation and FFN updates xℓ, oℓ,\nand ˜ xℓ, using logistic regression. As in our method,\nthe labels are determined according to saturation\nevents in the training data (see App. A.5). During\ninference, we execute the computation through the\nlayers, and halt according to the layer classifier.\n7This is a simplification. We split Nℓ by saturation layers\nand require a bigger intersection with Tℓ at all the layers.\nMethod Accuracy Saved Layers\nBinary classifiers using xℓ 94.4±6.4 18.8% 3.0±0.4\nBinary classifiers using oℓ 92.9±5.4 19.4% 3.1±0.3\nBinary classifiers using ˜ xℓ 94.4±6.4 18.8% 3.0±0.4\nSub-updates rule 94.1 ±1.4 20.0% 3.2±0.3\nTable 6: Early exit evaluation results on WIKI LM.\nEvaluation. Each method is evaluated by accu-\nracy, i.e., the portion of examples for which exiting\nat the predicted layer yields the final model pre-\ndiction, and by computation efficiency, measured\nby the amount of saved layers for examples with\ncorrect prediction. We run each method with five\nrandom seeds and report the average scores.\nResults. Tab. 6 shows that our method obtains\na high accuracy of 94.1%, while saving 20% of\ncomputation on average without changing the pre-\ndiction. Moreover, just by observing the dominant\nFFN sub-updates, it performs on-par with the pre-\ndiction rules relying on the representation and FFN\noutput vectors. This demonstrates the utility of\nsub-updates for predicting saturation events, and\nfurther supports our hypothesis that FFN updates\nplay a functional role in the prediction (§5.2).\n7 Related Work\nThe lack of interpretability of modern LMs has\nled to a wide interest in understanding their predic-\ntion construction process. Previous works mostly\nfocused on analyzing the evolution of hidden rep-\nresentations across layers (V oita et al., 2019), and\nprobing the model with target tasks (Yang et al.,\n2020; Clark et al., 2019; Tenney et al., 2019; Saphra\nand Lopez, 2019). In contrast, our approach aims to\ninterpret the model parameters and their utilization\nin the prediction process.\nMore recently, a surge of works have investi-\ngated the knowledge captured by the FFN layers\n(Da et al., 2021; Jiang et al., 2020; Dai et al., 2022;\n37\nYao et al., 2022; Meng et al., 2022; Wallat et al.,\n2020). These works show that the FFN layers store\nvarious types of knowledge, which can be located\nin specific neurons and edited. Unlike these works,\nwe focus on the FFN outputs and their contribution\nin the prediction construction process.\nLast, our interpretation of FFN outputs as up-\ndates to the output distribution relates to recent\nworks that interpreted groups of LM parameters in\nthe discrete vocabulary space (Geva et al., 2021;\nKhashabi et al., 2022), or viewed the representation\nas an information stream (Elhage et al., 2021).\n8 Conclusions\nUnderstanding the inner workings of transformers\nis valuable for explainability to end-users, for de-\nbugging predictions, for eliminating undesirable\nbehavior, and for understanding the strengths and\nlimitations of NLP models. The FFN is an under-\nstudied core component of transformer-based LMs,\nwhich we focus on in this work.\nWe study the FFN output as a linear combina-\ntion of parameter vectors, termed values, and the\nmechanism by which these vectors update the token\nrepresentations. We show that value vectors often\nencode human-interpretable concepts and that these\nconcepts are promoted in the output distribution.\nOur analysis of transformer-based LMs provides\na more detailed understanding of their internal pre-\ndiction process, and suggests new research direc-\ntions for interpretability, control, and efficiency, at\nthe level of individual vectors.\nLimitations\nOur study focused on the operation of FFN lay-\ners in building model predictions. Future work\nshould further analyze the interplay between these\nlayers and other components in the network, such\nas attention-heads.\nIn our analysis, we decomposed the computation\nof FFN layers into smaller units, corresponding to\nsingle value vectors. However, it is possible that\nvalue vectors are compositional in the sense that\ncombinations of them may produce new meanings.\nStill, we argue that analyzing individual value vec-\ntors is an important first step, since (a) the space of\npossible combinations is exponential, and (b) our\nanalysis suggests that aggregation of value vectors\nis less interpretable than individual value vectors\n(§4.1). Thus, this approach opens new directions\nfor interpreting the contribution of FFN layers to\nthe prediction process in transformer LMs.\nIn addition, we chose to examine the broad fam-\nily of decoder-based, auto-regressive LMs, which\nhave been shown to be extremely effective for\nmany NLP tasks, including few- and zero-shot\ntasks (Wang et al., 2022). While these models share\nthe same building blocks of all transformer-based\nLMs, it will be valuable to ensure that our findings\nstill hold for other models, such as encoder-only\nLMs (e.g. RoBERTa (Liu et al., 2019)) and mod-\nels trained with different objective functions (e.g.\nmasked language modeling (Devlin et al., 2019)).\nFinally, our annotation effort was made for the\nevaluation of our hypothesis that sub-updates en-\ncode human-interpretable concepts. Scaling our\nannotation protocol would enable a more refined\nmap of the concepts, knowledge and structure cap-\ntured by LMs. Furthermore, since our concept\ninterpretation approach relies on manual inspection\nof sets of tokens, its success might depend on the\nmodel’s tokenization method. In this work, we an-\nalyzed models with two different commonly-used\ntokenizers, and future research could verify our\nmethod over other types of tokenizations as well.\nEthics Statement\nOur work in understanding the role that single-\nvalues play in the inference that transformer-based\nLMs perform potentially improves their trans-\nparency, while also providing useful control appli-\ncations that save energy (early-exit prediction) and\nincrease model harmlessness (toxic language sup-\npression). It should be made clear that our method\nfor toxic language suppression only reduces the\nprobability of toxic language generation and does\nnot eliminate it. As such, this method (as well as\nour early-exit method) should not be used in the\nreal world without further work and caution.\nMore broadly, our work suggests a general ap-\nproach for modifying LM predictions in particular\ndirections, by changing the weights of FFN sub-\nupdates. While this is useful for mitigating biases,\nit also has the potential for abuse. It should be\nmade clear that, as in the toxic language suppres-\nsion application, our approach does not modify the\ninformation encoded in LMs, but only changes the\nintensity in which this information is exposed in the\nmodel’s predictions. Moreover, our work primar-\nily proposes an interpretation for FFN sub-updates,\nwhich also could be used to identify abusive inter-\n38\nventions. Regardless, we stress that LMs should\nnot be integrated into critical systems without cau-\ntion and monitoring.\nAcknowledgements\nWe thank Shauli Ravfogel, Tal Schuster, and\nJonathan Berant for helpful feedback and construc-\ntive suggestions. This project has received funding\nfrom the Computer Science Scholarship granted by\nthe Séphora Berrebi Foundation, the PBC fellow-\nship for outstanding PhD candidates in Data Sci-\nence, and the European Research Council (ERC)\nunder the European Union’s Horizon 2020 research\nand innovation programme, grant agreement No.\n802774 (iEXTRACT).\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In In-\nternational Conference on Learning Representations\n(ICLR).\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the ACM Confer-\nence on Fairness, Accountability, and Transparency\n(FAccT).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin Neural Information Processing Systems (NeurIPS).\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. 2021. Analyzing commonsense\nemergence in few-shot knowledge models. In 3rd\nConference on Automated Knowledge Base Construc-\ntion.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL), pages 4171–4186,\nMinneapolis, Minnesota.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-adaptive transformer. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread. Https://transformer-\ncircuits.pub/2021/framework/index.html.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In Proceedings of the conference on computer\nvision and pattern recognition (CVPR).\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nArthur E Hoerl and Robert W Kennard. 1970. Ridge re-\ngression: Biased estimation for nonorthogonal prob-\nlems. Technometrics, 12(1):55–67.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. Advances in Neural\nInformation Processing Systems (NeurIPS).\n39\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDaniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui\nQin, Kyle Richardson, Sean Welleck, Hannaneh Ha-\njishirzi, Tushar Khot, Ashish Sabharwal, Sameer\nSingh, and Yejin Choi. 2022. Prompt wayward-\nness: The curious case of discretized interpretation\nof continuous prompts. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3631–3643, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nLei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li,\nJie Zhou, and Xu Sun. 2021. CascadeBERT: Ac-\ncelerating inference of pre-trained language models\nvia calibrated complete models cascade. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 475–486, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nKris McGuffie and Alex Newhouse. 2020. The radical-\nization risks of gpt-3 and advanced neural language\nmodels. arXiv preprint arXiv:2009.06807.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. International Conference on Learning Represen-\ntations (ICLR).\nDaniel Müllner. 2011. Modern hierarchical, ag-\nglomerative clustering algorithms. arXiv preprint\narXiv:1109.2378.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNaomi Saphra and Adam Lopez. 2019. Understanding\nlearning dynamics of language models with SVCCA.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3257–3267,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nTal Schuster, Adam Fisch, Tommi Jaakkola, and Regina\nBarzilay. 2021. Consistent accelerated inference via\nconfident adaptive transformers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 4962–4979, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020a. Green AI. Communications of the\nACM, 63(12):54–63.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020b. The right tool for the job: Matching\nmodel and instance complexities. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 6640–6651,\nOnline. Association for Computational Linguistics.\nNoam M. Shazeer. 2020. Glu variants improve trans-\nformer. ArXiv, abs/2002.05202.\nS. Sukhbaatar, J. Weston, and R. Fergus. 2015. End-\nto-end memory networks. In Advances in Neural\nInformation Processing Systems (NIPS).\nSainbayar Sukhbaatar, Edouard Grave, Guillaume Lam-\nple, Herve Jegou, and Armand Joulin. 2019. Aug-\nmenting self-attention with persistent memory. arXiv\npreprint arXiv:1907.01470.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nRobert Tibshirani. 1996. Regression shrinkage and se-\nlection via the lasso. Journal of the Royal Statistical\nSociety: Series B (Methodological), 58(1):267–288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NIPS), pages 5998–6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal adversarial\ntriggers for attacking and analyzing NLP. InProceed-\nings of the 2019 Conference on Empirical Methods\n40\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nJonas Wallat, Jaspreet Singh, and Avishek Anand. 2020.\nBERTnesia: Investigating the capture and forgetting\nof knowledge in BERT. In Proceedings of the Third\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP, pages 174–183, On-\nline. Association for Computational Linguistics.\nThomas Wang, Adam Roberts, Daniel Hesslow,\nTeven Le Scao, Hyung Won Chung, Iz Beltagy, Julien\nLaunay, and Colin Raffel. 2022. What language\nmodel architecture and pretraining objective works\nbest for zero-shot generalization? In Proceedings of\nthe 39th International Conference on Machine Learn-\ning, volume 162 ofProceedings of Machine Learning\nResearch, pages 22964–22984. PMLR.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020. Early exiting BERT for efficient document\nranking. In Proceedings of SustaiNLP: Workshop on\nSimple and Efficient Natural Language Processing,\npages 83–88, Online. Association for Computational\nLinguistics.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. BERxiT: Early exiting for BERT with better\nfine-tuning and extension to regression. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 91–104, Online. Association for\nComputational Linguistics.\nJingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou,\nand Lei Li. 2021. A survey on green deep learning.\narXiv preprint arXiv:2111.05193.\nYilin Yang, Longyue Wang, Shuming Shi, Prasad Tade-\npalli, Stefan Lee, and Zhaopeng Tu. 2020. On the\nsub-layer functionalities of transformer decoder. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 4799–4811, Online.\nAssociation for Computational Linguistics.\nYunzhi Yao, Shaohan Huang, Ningyu Zhang, Li Dong,\nFuru Wei, and Huajun Chen. 2022. Kformer: Knowl-\nedge injection in transformer feed-forward layers.\narXiv preprint arXiv:2201.05742.\n41\n1 2 3 4 5 6 7 8 9 101112131415161718192021222324\nlayer\n0\n20\n40\n60\n80\n100IoU of value projections\nGPT2\nnormalized random\nFigure 4: Similarity of projections to E, of GPT2 value\nvectors with and without layer normalization, and of\nvalue vectors and randomly-initialized vectors. We mea-\nsure similarity of the top-30 tokens in each projection\nwith IoU.\nA Appendix\nA.1 Value Vectors Projection Method\nOur interpretation method of sub-updates is based\non directly projecting value vectors to the embed-\nding matrix, i.e. for a value v and embedding\nmatrix E, we calculate Ev (§4). However, in some\nLMs like GPT2 , value vectors in each layer are\nadded to the token representation followed by a\nlayer normalization (LN) (Ba et al., 2016). This\nraises the question whether “reading” vectors that\nare normalized in the same manner as the represen-\ntation would yield different concepts.\nTo test that, we compare the top-30 scoring\ntokens by Evℓ\ni and by E ·LayerNorm(vℓ\ni), for\ni = 1,...,d m and ℓ = 1,...,L , using Intersection\nover Union (IoU). As a baseline, we also compare\nEvℓ\ni with random vectors, initialized from a normal\ndistribution with the empirical mean and standard\ndeviation of the value vectors. Fig. 4 shows that\nLN does not change the projection substantially,\nwith an overlap of 64.5% of the top-30 tokens on\naverage, suggesting that the same concepts are pro-\nmoted in both cases. This is in contrast to random\nvalues, which produce a ∼0% overlap on average.\nA.2 Concepts Annotation\nWe analyze the concepts encoded in sub-updates,\nby projecting their corresponding value vectors to\nthe embedding matrix and identifying repeating\npatterns in the top-scoring 30 tokens (§3). Pattern\nidentification was performed by experts (NLP grad-\nuate students), following the instructions presented\nin Fig. 5. Please note these are the instructions\nprovided for annotations of WIKI LM, which uses\nword-level tokenization. Thus, the terms “words”\nand “tokens” are equivalent in this case.\nFor value vectors in WIKI LM, which uses\na word-level vocabulary with many uncommon\nwords, we additionally attached a short description\nfield for each token that provides context about the\nmeaning of the word. For the description of a token\nw, we first try to extract the definition of wfrom\nWordnet.8 If wdoes not exist in Wordnet, as often\nhappens for names of people and places, we then\nsearch for win Wikipedia9 and extract a short (pos-\nsibly noisy) description if the query was successful.\nA complete annotation example Tab. 7.\nA.3 Sub-Update Contribution in FFN\nOutputs\nIn this section, we justify our choice along the pa-\nper of looking at the top-10 dominant sub-updates.\nThe contribution of a sub-update mℓ\nivℓ\ni to the FFN\noutput is:\ncontrib(mℓ\nivℓ\ni) := |mℓ\ni|||vℓ\ni||∑dm\nj=1 |mℓ\nj|||vℓ\nj||\n,\nnamely, its relative weight compared to the over-\nall sum of weights of all sub-updates. The overall\ncontribution of the top-10 dominant sub-updates is\ncomputed by summing their contributions. Note\nthat we take the absolute value of the coeffi-\ncients |mℓ\ni|, since some activation functions (e.g.\nGeLU (Hendrycks and Gimpel, 2016) in GPT2 ),\ncan result in negative values of mℓ\ni.\nEmpirically, we observe that in some cases sub-\nupdates with negative coefficients do appear as part\nof the 10 most dominant sub-updates in GPT2 .\nWe further attribute this to the success of GeLU in\ntransformer models (Shazeer, 2020), as it increases\nthe expressiveness of the model by allowing re-\nversing the scores value vectors induce over the\nvocabulary.\nFig. 6 depicts the contribution of the top-10\ndominant sub-updates per layer for WIKI LM and\nGPT2 , using 2000 random examples from the\nWIKI TEXT-103 validation set. Clearly, for all\nthe layers, the contribution of the dominant sub-\nupdates exceeds the contribution of random sub-\nupdates. Observe that, even though they cover\nonly 0.24% of the value vectors, the contribution of\ndominant sub-updates is typically around 5%, and\nin some layers (e.g. layers 8-16 in WIKI LM and\nlayer 1 in GPT2 ) it reaches over 10% of the total\n8We use the NLTK python package.\n9Using the wptools package https://pypi.org/\nproject/wptools/.\n42\nIn this task, you are given a list of 30 words in English, and the goal is to identifyrepetitive patterns occurring in the words.Patterns can besemantic(e.g. animals, 3-digit numbers,names of Indian actors, andtime-related words) orsyntactic(e.g. connectives,plurals, words starting with “dis-”, andverbs in present progressive tense). You should only count patterns that occur in at least 4words (i.e. if you notice a pattern that occurs only in 3 words, then please ignore it).\nTo complete the task, please do the following:1. Give an ID to every identified pattern (1,2,...)2. Assign a pattern ID to every word in the list, or -1/leave empty if no pattern applies tothe word.3. For every identified pattern specify whether the pattern is semantic or syntactic and(optional) write a short description of the pattern.\nPlease note that some of the words might be uncommon words that you are not familiar with.In such cases, you will need to do a quick search over the Web to understand the meaningof words.\nFigure 5: Annotation instructions for the concepts identification task.\ncontribution. This demonstrates that analyzing the\ntop-10 dominant sub-updates can shed light on the\nway predictions are built through the layers.\nA.4 Toxic Language Suppression Details\nThe 10 manually selected value vectors were found\nby searching for non-toxic words, such as “safe”\nand “peace”, among the top-30 tokens in the vec-\ntor projections to the vocabulary. We selected a\nsmall set of 10 value vectors whose top-scoring\ntokens were coherent and seemed to promote differ-\nent kinds of non-toxic tokens. The list of manually\npicked vectors is provided in Tab. 8. Importantly,\nthe search process of all vectors was a one-time\neffort that took < 5 minutes in total. We chose\nthe value vectors in a greedy-manner, without addi-\ntional attempts to optimize our choice.\nTo select 10 non-toxic value vectors based on an\nautomatic toxicity metric, we used the Perspective\nAPI. Concretely, we concatenated the top-30 tokens\nby each value vector and graded the resulting text\nwith the toxicity score produced by the API. Then,\nwe sampled 10 random vectors with a toxicity score\n<0.1 (a score of <0.5 indicates a non-toxic text).\nA.5 Early Exit Details\nThis section provides further details and analysis\nregarding our early exit method and the baselines\nwe implemented.\nMethod Implementation. We consider 90% of\nthe 10k examples for constructing Tℓ and Nℓ, and\nthe remaining 10% examples are considered as the\ntesting set. We used k= 2e2 to cluster the top-10\ndominant value vectors, but observed that other k\nvalues yielded similar results.\nBaselines’ Implementation. We train each bi-\nnary classifier using 8k training examples, based\non the standardized forms of each feature vec-\ntor. We considered a hyperparameter sweep, us-\ning 8-fold cross-validation, with l2 or l1 regu-\nlarization (lasso (Tibshirani, 1996) or ridge (Ho-\nerl and Kennard, 1970)), regularization coef-\nficients C ∈{1e−3,1e−2,1e−1,1,1e1,1e2,1e3},\nand took the best performing model for each layer.\nWe also used a inversely proportional loss coeffi-\ncient according to the class frequencies.\nIn order to achieve high accuracy, we further\ncalibrate a threshold per classifier for reaching the\nmaximal F1 score for each layer. This calibration\nis done after training each classifier, over a set of\n1000 validation examples.\nFrequency of Saturation Events. We investi-\ngate the potential of performing early exit for WIK-\nILM and GPT2 . Tab. 9 and 10 depict the frequency\nof saturation events per layer, considering 10k ex-\namples from the WIKI TEXT-103 validation set,\nfor WIKI LM and GPT2 , respectively. In GPT2 ,\n34.15% of the examples require the full compu-\ntation using all the model layers, while for WIK-\nILM, this holds for only 15.22% of the examples.\nNotably, early fixation events in GPT2 are less\ncommon than in WIKI LM, possibly due to the\nlarger number of layers the prediction construction\nis spread over. Hence, we use WIKI LM for our\nexperiments, as it has significantly higher compu-\n43\npatterns word description\n1 front the side that is forward or prominent\n1 ahead having the leading position or higher score in a contest\n1 forward the person who plays the position of forward in certain games, such as basketball, soccer, or\nhockey\n1 preceded be earlier in time; go back further\n1 Before earlier in time; previously\n1 before earlier in time; previously\n1 rear the back of a military formation or procession\n1 fore front part of a vessel or aircraft\n2 Name a language unit by which a person or thing is known\n1 Past the time that has elapsed\n1 prior the head of a religious order; in an abbey the prior is next below the abbot\n1 anterior a tooth situated at the front of the mouth\n1 upperparts standard terms for unambiguous description of relative placement of body parts\n1 lead an advantage held by a competitor in a race\n1 backwards at or to or toward the back or rear\n1 aft (nautical, aeronautical) situated at or toward the stern or tail\n1 preceding be earlier in time; go back further\n1 upstream in the direction against a stream’s current\nhind any of several mostly spotted fishes that resemble groupers\n1 posterior the fleshy part of the human body that you sit on\nEtymology a history of a word\n1 Pre Wikimedia disambiguation page\nchin the protruding part of the lower jaw\n1 north the region of the United States lying to the north of the Mason-Dixon line\n1 east the cardinal compass point that is at 90 degrees\n2 surname the name used to identify the members of a family (as distinguished from each member’s\ngiven name)\n1 Then that time; that moment\n2 name a language unit by which a person or thing is known\n1 northbound moving toward the north\n1 leading thin strip of metal used to separate lines of type in printing\npattern id description\n(optional)\nsemantic/syntactic\n1 positions/\ndirections\nsemantic\n2 naming semantic\nTable 7: An example annotation spreadsheet of the top-tokens by the value vector u6\n1090 in WIKI LM.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n10\n20\n30% contribution\nWikiLM\ntop-10 rand-10\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nlayer\n0\n5\n10\n15% contribution\nGPT2\ntop-10 rand-10\nFigure 6: Relative contribution to the FFN output of the 10 most dominant and 10 random sub-updates in each layer,\nof WIKI LM (left) and GPT2 (right).\ntation saving potential, as well as more saturation\nevents per layer.\n44\nValue Top-10 Tokens\nv14\n1853\ntransparency, disclosure, clearer, parency, iquette,\nhumility, modesty, disclosures, accountability, safer\nv15\n73\nrespectful, honorable, healthy, decent, fair, erning,\nneutral, peacefully, respected, reconc\nv15\n1395\nsafe, neither, safer, course, safety, safe, Safe,\napologize, Compact, cart\nv16\n216\nrefere, Messages, promises, Relations, accept, acceptance,\nAccept, assertions, persistence, warn\nv17\n462\nshould, should, MUST, ought, wisely, Should, SHOULD,\nsafely, shouldn, urgently\nv17\n3209\npeaceful, stable, healthy, calm, trustworthy, impartial,\nstability, credibility, respected, peace\nv17\n4061\nProper, proper, moder, properly, wisely, decency, correct,\ncorrected, restraint, professionalism\nv18\n2921\nthank, THANK, thanks, thank, Thank, apologies, Thank,\nthanks, Thanks, apologise\nv19\n1891\nthanks, thank, Thanks, thanks, THANK, Thanks, Thank, Thank,\nthank, congratulations\nv23\n3770\nfree, fit, legal, und, Free, leg, pless, sound, qualified,\nFree\nTable 8: The 10 manually picked value vectors used for toxic language suppression and the top-10 tokens in their\nprojection to the vocabulary. Repetitions in the projections are a result of special characters not being shown. These\nvectors were found by manually searching for non-toxic words such as “safe” and “peace” in the projections to the\nvocabulary.\nLayer % Examples Layer % Examples\n1 6.70 9 2.96\n2 5.25 10 3.78\n3 13.74 11 4.74\n4 3.13 12 7.45\n5 1.02 13 10.79\n6 1.07 14 9.88\n7 1.86 15 9.81\n8 2.60 16 15.22\nTable 9: The percentage of saturation events per layer\nusing WIKI LM, for the WIKI TEXT-103 validation set.\nLayer % Examples Layer % Examples\n1 2.21 13 1.24\n2 0.77 14 1.62\n3 1.06 15 2.37\n4 0.74 16 2.72\n5 0.85 17 2.99\n6 0.83 18 3.80\n7 0.83 19 4.15\n8 0.72 20 5.21\n9 0.93 21 5.67\n10 0.99 22 9.31\n11 1.16 23 14.52\n12 1.32 24 34.15\nTable 10: The percentage of saturation events per layer\nusing GPT2, for the W IKI TEXT-103 validation set.\n45",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7237704992294312
    },
    {
      "name": "Computer science",
      "score": 0.6684967279434204
    },
    {
      "name": "Vocabulary",
      "score": 0.616406261920929
    },
    {
      "name": "Computation",
      "score": 0.5917630791664124
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5494916439056396
    },
    {
      "name": "Inference",
      "score": 0.5078034996986389
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4848729074001312
    },
    {
      "name": "Security token",
      "score": 0.4601595401763916
    },
    {
      "name": "Feed forward",
      "score": 0.45315054059028625
    },
    {
      "name": "Machine learning",
      "score": 0.4235677123069763
    },
    {
      "name": "Algorithm",
      "score": 0.2660318613052368
    },
    {
      "name": "Engineering",
      "score": 0.13738423585891724
    },
    {
      "name": "Control engineering",
      "score": 0.12619751691818237
    },
    {
      "name": "Electrical engineering",
      "score": 0.09847712516784668
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I13955877",
      "name": "Bar-Ilan University",
      "country": "IL"
    }
  ],
  "cited_by": 77
}