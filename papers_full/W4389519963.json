{
  "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs",
  "url": "https://openalex.org/W4389519963",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2107837401",
      "name": "Jiefeng Chen",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2150577459",
      "name": "Jinsung Yoon",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2765659507",
      "name": "Sayna Ebrahimi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4272002984",
      "name": "Sercan Arik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2798413642",
      "name": "Tomas Pfister",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2193269139",
      "name": "Somesh Jha",
      "affiliations": [
        "University of Wisconsin–Madison",
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385572556",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4366823941",
    "https://openalex.org/W4389524305",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4386566485",
    "https://openalex.org/W4300980582",
    "https://openalex.org/W2108325777",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2618169590",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4285298351",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4323927478",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W3173618889",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4229005866"
  ],
  "abstract": "Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. *Selective prediction* is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AUROC from 74.61% to 80.25%.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5190–5213\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAdaptation with Self-Evaluation to Improve Selective Prediction in LLMs\nJiefeng Chen1∗ Jinsung Yoon2 Sayna Ebrahimi2\nSercan Ö. Arık2 Tomas Pﬁster2 Somesh Jha1,2\n1 University of Wisconsin-Madison 2 Google LLC\n{jiefeng,jha}@cs.wisc.edu\n{jinsungyoon,saynae,soarik,tpfister}@google.com\nAbstract\nLarge language models (LLMs) have recently\nshown great advances in a variety of tasks,\nincluding natural language understanding and\ngeneration. However, their use in high-stakes\ndecision-making scenarios is still limited due\nto the potential for errors. Selective prediction\nis a technique that can be used to improve the\nreliability of the LLMs by allowing them to ab-\nstain from making predictions when they are\nunsure of the answer. In this work, we pro-\npose a novel framework for adaptation with\nself-evaluation to improve the selective predic-\ntion performance of LLMs. Our framework is\nbased on the idea of using parameter-efﬁcient\ntuning to adapt the LLM to the speciﬁc task\nat hand while improving its ability to perform\nself-evaluation. We evaluate our method on\na variety of question-answering (QA) datasets\nand show that it outperforms state-of-the-art\nselective prediction methods. For example, on\nthe CoQA benchmark, our method improves\nthe AUACC from 91.23% to 92.63% and im-\nproves the AUROC from 74.61% to 80.25%.\n1 Introduction\nLarge Language Models (LLMs) have recently\ndemonstrated impressive capabilities in many natu-\nral language understanding, reasoning and genera-\ntion tasks, such as question answering (Jiang et al.,\n2021; Singhal et al., 2023), summarization (Tang\net al., 2023; Zhang et al., 2023b), semantic classiﬁ-\ncation, and code generation (Poesia et al., 2022;\nZhang et al., 2023a). As LLMs improve their\nremarkable performance, they are being increas-\ningly considered to replace humans to perform\nhigh-stakes tasks. For example, LLMs can be used\nfor medical QA to assist patients (Singhal et al.,\n2022). However, LLMs are not guaranteed to be\naccurate for all queries, so it is important to un-\nderstand which queries they are reliable for. This\n∗ Work done during internship at Google and before\njoining Amazon.\nWhich vitamin assists in blood clotting?\nLLM\nLLM\nWithout Selective Prediction\nWith Selective PredictionWhich vitamin assists in blood clotting?Answer: Vitamin C Selection score: 0.1 \nAnswer: Vitamin C\nCorrect answer: Vitamin K\nCorrect answer: Vitamin K\nFigure 1: A safety-critical question from the TriviaQA\ndataset: “Which vitamin helps regulate blood clotting?” The\nOPT-2.7B model incorrectly answers “Vitamin C”, when the\ncorrect answer is “Vitamin K”. Without selective prediction,\nLLMs will directly output the wrong answer which in this\ncase could lead users to take the wrong medicine, and thus\ncausing potential harm. With selective prediction, LLMs will\noutput a low selection score along with the wrong answer and\ncan further output “I don’t know!” to warn users not to trust it\nor verify it using other sources.\ninformation can be used to direct human oversight\nto the queries with the lowest selection score. Se-\nlective prediction(Geifman and El-Yaniv, 2017),\nbroadly refers to the deployment scenario for AI\nmodels where humans are involved to maintain\noverall accuracy by reviewing AI-generated, low-\nconﬁdence outputs. In this scenario, both human\nand AI performance are considered together to min-\nimize human involvement cost. LLMs should be\nused in the real-world with enhanced selective pre-\ndiction performance. They should be able to assess\nthe accuracy of their predictions and refrain from\nmaking wrong predictions. If an LLM detects that\nan answer might be wrong for a question, it should\nbe able to generate an answer with the sentiment\nof \"I don’t know!\" (as shown in Fig. 1) or defer\nthe answer to a human for manual inspection. This\nwill help to ensure that LLMs are used in a reliably,\nespecially for high-stakes applications.\nSelective prediction for LLMs is challenging be-\ncause LLMs are just trained to predict the next to-\n5190\nken given a context but are not guaranteed to always\npredict the correct next token. Also, since LLMs\ngenerate an output sequence in an auto-regressive\nway, they don’t directly produce a conﬁdence score\nfor the output sequence. Thus, obtaining selection\nscores from LLMs for their output sequences is not\nstraightforward. Although there is some research\non selective prediction for LLMs, these studies\nhave their own shortcomings. Kadavath et al. pro-\npose to use heuristic prompts (e.g., adding prompts\nlike “Is the proposed answer True or False?”) to\ntrigger self-evaluation of LLMs. However, those\nprompts may only work for the LLM used in Kada-\nvath et al. (2022) and may not generalize to other\ntypes of LLMs (e.g., OPT and GPT2 models eval-\nuated in our work). Some approaches proposed\nusing semantic entropy (Kuhn et al., 2023) or self-\nconsistency (Wang et al., 2022) as a measure of\nuncertainty for selection score. However, they usu-\nally require generating multiple output sequences\nto obtain the uncertainty measure for an input se-\nquence, which introduces high computational cost\nand latency at test time. Fine-tuning LLMs on\ntraining data from the target question answering\ntask using the standard LLM training loss can im-\nprove selective prediction performance. This is\nbecause ﬁne-tuning can improve the accuracy of\nthe predictions and maximize the likelihood of the\nground-truth answer for a given question. However,\nmaximizing the likelihood of the ground-truth an-\nswer is not the same as minimizing the likelihood\nof the wrong answers, since LLMs generate output\nsequences in an auto-regressive way. Even after\nﬁne-tuning, some wrong answers may still have\nhigh likelihood and be generated by the LLM at\ntest time. Therefore, distinguishing correct and in-\ncorrect answers based on likelihood scores alone is\na challenging task.\nTo address these challenges of self-evaluation\nand uncertainty estimation, we propose a novel\nframework – Adaptation with Self-Evaluation to\nImprove Selective Prediction in LLMs (ASPIRE).\nUnlike previous methods that rely on hand-crafted\nheuristics or multiple output sequences, our frame-\nwork learns to self-evaluate from target-task data.\nWe do this by training LLMs on a subset of the\ntraining data from the question-answering tasks.\nThis allows the LLMs to learn to distinguish be-\ntween correct and incorrect answers on their own.\nWe then deﬁne a selection score that combines the\nlikelihood of the generated answer with the learned\nself-eval score (see Eq. (11)) to make selective pre-\ndictions. This makes our method much less com-\nputationally expensive than solutions that require\ngenerating multiple output sequences to obtain the\nuncertainty measure. Thus, the proposed method\nis useful for practical applications where high se-\nlective prediction performance and low inference\ncosts are desired, after deploying the LLM. In such\napplications, practitioners prefer collecting some\ntraining data to ﬁne-tune smaller LLMs to achieve\nhigh selective prediction performance rather than\ndirectly deploying very large pre-trained LLMs\nwith limited selective prediction performance for\nspeciﬁc tasks.\nWe conduct extensive experiments to evaluate\nour proposed framework, ASPIRE. We show that\nASPIRE achieves the state-of-the-art selective pre-\ndiction performance on three question answering\ndatasets: CoQA, TriviaQA and SQuAD, using OPT\nand GPT-2 models. We also provide empirical anal-\nysis to delve deeper into our proposed technique.\n2 Related Work\nSelective Prediction for LLMs. Recently, LLMs\n(e.g., GPT-4 (OpenAI, 2023) and PaLM (Chowd-\nhery et al., 2022)) have achieved great success in\nsolving various kinds of Natural Language Gener-\nation (NLG) tasks. However, LLMs are still not\nvery reliable and may generate wrong outputs when\nsolving NLG tasks. Due to this, selective predic-\ntion (or sometimes called selective generation (Ren\net al., 2022)) is critical for safely deploying LLMs\nin the real-world. Different from selective predic-\ntion for classiﬁcation tasks (e.g., Natural Language\nInference (NLI) tasks) (Xin et al., 2021), selec-\ntive prediction for LLMs in solving NLG tasks\nis fundamentally different since the prediction is\ndone auto-regressively over many steps and the\npossible answer set has an inﬁnite size. Recently,\nseveral work propose some uncertainty measures\nfor LLMs, which can be used for selective predic-\ntion (Si et al., 2022; Kadavath et al., 2022; Varshney\net al., 2022; Ren et al., 2022; Kuhn et al., 2023).\nSome recent work studies selective prediction for\nsolving question answering tasks where questions\nare ambiguous (Cole et al., 2023; Yin et al., 2023).\nVarshney and Baral (2023) propose a selective pre-\ndiction method that at inference time leverages an\nauxiliary model which is trained to distinguish the\ncorrect predictions of the QA model from the in-\ncorrect ones. Different from previous work, our\n5191\nwork proposes to improve selective prediction per-\nformance of LLMs in solving question answering\ntasks by learning self-evaluation during ﬁne-tuning.\nParameter Efﬁcient Fine-tuning. Fine-tuning\npretrained LLMs on downstream datasets can bring\nhuge performance gains when compared to us-\ning the pretrained LLMs out-of-the-box (e.g., k-\nshot inference). However, as LLMs get larger and\nlarger, full ﬁne-tuning becomes very expensive in\nterms of computational cost and memory require-\nments. In addition, massive models might not be\ndata efﬁcient and overﬁtting issues might be ob-\nserved, yielding suboptimal generalization. To ad-\ndress these issues, Parameter-Efﬁcient Fine-tuning\n(PEFT) approaches have been proposed. PEFT ap-\nproaches only ﬁne-tune a small number of (extra)\nmodel parameters while freezing most parameters\nof the pretrained LLMs, thereby greatly decreas-\ning the computational and storage costs. It has\nalso been shown that PEFT approaches are better\nthan ﬁne-tuning in the low-data regimes and gen-\neralize better to out-of-domain scenarios. Existing\nPEFT approaches include LoRA (Hu et al., 2021),\nPreﬁx Tuning (Liu et al., 2021a), Soft Prompt Tun-\ning (Lester et al., 2021) and P-Tuning (Liu et al.,\n2021b). In this work, we use Soft Prompt Tun-\ning to learn self-evaluation to improve selective\nprediction performance of LLMs.\n3 Problem Setup\nSuppose we have a pre-trained LLM f for an ar-\nbitrary generative modeling task such as question\nanswering. The output can be represented as a se-\nquence of tokens from the vocabulary V. Let V∗\nbe the space of sequences of tokens. Suppose the\nlogits of f on v∈V given x ∈V∗is ¯f(v|x). The\nlikelihood of the next token following x being vis\ndeﬁned as:\nf(v|x) := exp (¯f(v|x))∑\nv′∈Vexp (¯f(v′|x)), (1)\nwhereas the likelihood of generating ˆy ∈V∗given\nx is deﬁned as:\nf(ˆy |x) := Π|ˆy|\ni=1f(ˆyi |x,ˆy[i−1]), (2)\nwhere ˆy = (ˆy1,..., ˆy|ˆy|), |ˆy|is the length of ˆy,\nˆy[i−1] = (ˆy1,..., ˆyi−1) for i >0 and ˆy[0] = ∅.\nThis likelihood can be very small when |ˆy|is very\nlarge. To address this issue, we deﬁne the normal-\nized likelihood as:\nfnorm(ˆy |x) := f(ˆy |x)\n1\n|ˆy| (3)\nWe use f to generate the output sequence for the\ngiven input x by solving the following objective:\nˆy∗= argmax\nˆy\nlog f(ˆy |x) (4)\nIt is impossible to solve this objective exactly since\nthe output sequences can be arbitrarily long. How-\never, we can employ some decoding strategy like\ngreedy decoding or beam search to solve it.\nTo evaluate if the generated output ˆy is correct\nor not, we need a set of reference outputs Sand an\nevaluation metric M : V∗×V∗→[0,1] that can\nevaluate the similarity of the generated output ˆy\ncompared to the reference output yr ∈S. With a\nthreshold γ, we can determine the correctness of\nthe generated output – if maxyr∈SM(ˆy,yr) >γ ,\nthen the generated output is correct; otherwise, the\ngenerated output is wrong. We discuss the speciﬁc\nchoices of M and γin Section 6.\nIn selective prediction, we need a rejection op-\ntion, which is denoted by ⊥. Given a training\ndataset Dtr = {(xi,yi)}ntr\ni=1 randomly sampled\nfrom a target task distribution, we aim to build a\nselective predictor fs : V∗→V∗∪{⊥} that can\nachieve strong selective prediction performance on\nthe test dataset Dte = {(xi,Si)}nte\ni=1, where Si is\nthe set of reference outputs for the input xi. The\nselective predictor fs is composed of a predictor\nˆf : V∗ → V∗ and a selection scoring function\ng: V∗→R. With ˆf and g, the selective predictor\nfs is proposed as:\nfs(x; τ) =\n{ˆf(x) if g(x) ≥τ,\n⊥ if g(x) <τ , (5)\nwhere τ is a threshold. The accuracy of the se-\nlective predictor is deﬁned as the fraction of the\naccepted inputs where the predictions are correct.\nThe coverage of the selective predictor is deﬁned as\nthe fraction of the inputs that are accepted. We can\ntune the threshold τ to achieve a certain coverage\nand there would be an accuracy-coverage trade-off.\nWe use the area under the accuracy-coverage\ncurve (AUACC) metric to measure selective pre-\ndiction performance and use the area under the\nreceiver operator characteristic curve (AUROC)\nmetric to measure the quality of the selection score\nestimation. AUACC is the common metric used for\nevaluating selective prediction performance (Xin\net al., 2021; Yoshikawa and Okazaki, 2023). AU-\nROC is equivalent to the probability that a ran-\ndomly chosen correct output sequence has a higher\n5192\nselection score than a randomly chosen incorrect\noutput sequence. AUROC is used in (Kuhn et al.,\n2023) for evaluating uncertainty estimation meth-\nods.\n4 ASPIRE Framework\nWe propose that LLMs should have the self-\nevaluation ability such that they should be able\nto distinguish whether their proposed answers for\na given question are correct or not. Although\nsome previous work (Kadavath et al., 2022) show\nthat LLMs have good self-evaluation ability with\nspecially designed prompts, those prompts may\nnot transfer to different kinds of LLMs (as shown\nby our experiments and in Kuhn et al. (2023))\nand hand-crafting prompts for different kinds of\nLLMs can be expensive. A more effective approach\nis to collect some training data to employ self-\nevaluation. Towards this end, we propose a novel\nframework – Adaptation with Self-Evaluation to\nImprove Selective Prediction in LLMs (ASPIRE).\nFig. 2 illustrates the proposed framework and the\ndetails are explained next.\nGiven a training dataset for a generative task, we\ncan ﬁne-tune the pre-trained LLM on the train-\ning data to improve its prediction performance.\nTowards this end, parameter efﬁcient tuning tech-\nniques (e.g., soft prompt tuning (Lester et al., 2021)\nand LoRA (Hu et al., 2021)) might be employed\nto adapt the pre-trained LLM on the task, given\ntheir effectiveness in obtaining strong generaliza-\ntion with small amount of target task data. Specif-\nically, the model parameters θ of the LLM are\nfrozen and adaptable parameters θp are added for\nﬁne-tuning. Only θp are updated to solve the fol-\nlowing training objective:\nmin\nθp\nE(x,y)∼Dtr L(x,y; θ,θp), (6)\nwhere Lis the LLM training loss (e.g. cross-\nentropy). Such ﬁne-tuning can improve selective\nprediction performance because it not only im-\nproves the prediction accuracy, but also enhances\nthe likelihood of correct output sequences.\nTo further improve selective prediction perfor-\nmance, we propose to ﬁne-tune the LLM to learn\nself-evaluation. We ﬁrst use the LLM with the\nlearned θp to generate different answers for each\nexample (x,y) ∈Dtr. Suppose the decoding algo-\nrithm used to generate output sequences for each\ninput x is A. Awould produce a list of generated\noutput sequences:\nA(f,θp,x) = [ˆy1,..., ˆyk], (7)\nwhere k is the number of output sequences gen-\nerated. We aim to generate output sequences that\nhave high likelihood (i.e., f(ˆyj |x; θp) is high).\nWe use the metric M deﬁned in Section 3 to deter-\nmine if the generated output ˆyj is correct or not.\nIf M(ˆyj,y) > ˆγ, we label ˆyj as a correct output\nfor x; otherwise, we label ˆyj as a wrong output for\nx. Here, the threshold ˆγ might be different from\nthe threshold γused for evaluation. We choose a\nsufﬁciently large value of ˆγ(e.g., ˆγ = 0.9) so that\nthe generated wrong outputs wouldn’t be labeled as\ncorrect outputs. In Appendix H, we provide more\ndetails and analyses on selection of ˆγ.\nAfter sampling high-likelihood outputs for each\nquery, we add adaptable parameters θs and only\ntune θs for learning self-evaluation. Since the out-\nput sequence generation only depends on θand θp,\nfreezing θand the learned θp can avoid changing\nthe prediction behaviors of the LLM when learning\nself-evaluation. Let zc and zw be a pair of tokens\nthat represent the words “correct” and “wrong” re-\nspectively. We can then optimizeθs using the fol-\nlowing training objective:\nmin\nθs\nE(x,y)∼Dtr Lc + Lw\nLc = Eˆy∼Sc(x,y) −log f(zc|x,ˆy; θp,θs)\nLw = Eˆy∼Sw(x,y) −log f(zw|x,ˆy; θp,θs)\n(8)\nwhere Sc(x,y) is a set of correct outputs con-\ntaining the reference output y and kc correct out-\nputs with highest likelihood from A(f,θp,x), and\nSw(x,y) is a set of wrong outputs containing\nkw wrong outputs with highest likelihood from\nA(f,θp,x). If A(f,θp,x) has less than kc correct\noutputs (or has less than kw wrong outputs), we\ninclude all its correct outputs (or all its wrong out-\nputs) in Sc (or Sw). We ensure that Sw contains at\nleast one wrong output. If A(f,θp,x) doesn’t con-\ntain wrong outputs, we add a default wrong output\n(e.g., the empty string) to Sw.\nAfter training θp and θs, we obtain the prediction\nfor the query x via solving the following objective:\nˆy∗= argmax\nˆy\nlog f(ˆy |x; θp). (9)\nWe use the beam search decoding method towards\nthis. We deﬁne the likelihood of the output ˆy∗\n5193\nFrozen LLM\nQ: Which vitamin assists in blood clotting? A: \nVitamin K\nAdaptable parameters !!\nFrozen LLM\nQ: Which vitamin assists in blood clotting? A: \nVitamin K\nFrozen !!\nVitamin C\nFrozen LLM\nQ: Which vitamin assists in blood clotting? A: Vitamin K. \nCorrectWrong\nQ: Which vitamin assists in blood clotting? A: Vitamin C. \nAdaptable parameters !\"\nTask specific tuningAnswer samplingSelf-evaluation learning\nFrozen !!\nFigure 2: In the proposed framework ASPIRE, we ﬁrst perform task speciﬁc tuning to train adaptable parameters θp while\nfreezing the LLM. Then we use the LLM with the learned θp to generate different answers for each training question to create\na dataset for self-evaluation learning. Finally, we train the adaptable parameters θs to learn self-evaluation using the created\ndataset while freezing the LLM and the learned θp.\nbeing correct for the query x as:\nP(zc |x,ˆy∗) =\nexp (¯f(zc |x,ˆy∗; θp,θs))∑\nz∈{zc,zw}exp (¯f(z|x,ˆy∗; θp,θs)) (10)\nThis score P(zc |x,ˆy∗) is referred as the learned\nself-eval score. Overall, the selection scoring func-\ntion is proposed as:\ng(x) = (1 −α) ·log fnorm(ˆy∗|x; θp) (11)\n+ α·log P(zc |x,ˆy∗).\nwhere α∈[0,1] is a hyper-parameter.\n5 Implementation via Soft Prompt\nTuning\nIn the proposed framework, θp and θs can be\ntrained using parameter efﬁcient tuning approaches.\nIn our work, we focus on Soft Prompt Tuning, as il-\nlustrated in Fig. 3. The driving force behind this ap-\nproach lies in the recognition that if we can develop\nprompts that effectively stimulate self-evaluation,\nit should be possible to discover these prompts\nthrough soft prompt tuning in conjunction with tar-\ngeted training objectives.\nWe ﬁrst brieﬂy introduce the soft prompt tun-\ning method proposed by Lester et al. (2021). We\nconsider LLMs based on the Transformer archi-\ntecture (Vaswani et al., 2017). Given a query\nx = (x1,...,x mq ), Transformers ﬁrst embed the\ntokens, forming a matrix X ∈Rmq×de , where de\nis the dimension of the embedding space. The soft-\nprompts are represented as parameters ˜θ∈Rl×de ,\nwhere lis the length of the prompt. The prompt is\nthen concatenated to the embedded input forming\na single matrix [˜θ; X] ∈R(mq+l)×de , which then\nﬂows through the transformer as normal.\nIn the proposed framework, we need to train\ntwo portions of the prompts θp ∈Rl×de and θs ∈\nRl×de . Utilizing soft prompt tuning, the training\nobjective (6) is proposed as:\nmin\nθp\nE(x,y)∼Dtr\n1\n|y|\n|y|∑\nj=1\n−log f(yj|[θp; X; Y[j−1]]),\n(12)\nwhere X is the embedding of x and Y[j−1] is the\nembedding of y[j−1]. On the other hand, the train-\ning objective (8) is proposed as:\nmin\nθs\nE(x,y)∼Dtr Lc + Lw\nLc = Eˆy∼Sc(x,y) −log f(zc|[θp; X; ˆY; θs])\nLw = Eˆy∼Sw(x,y) −log f(zw|[θp; X; ˆY; θs])\n(13)\nwhere ˆY is the embedding of ˆy. The inference\nobjective (9) in the framework becomes:\nˆy∗= argmax\nˆy\nlog f(ˆy |[θp; X]) (14)\nThe learned self-eval score P(zc |x,ˆy∗) becomes:\nP(zc |x,ˆy∗) =\nexp (¯f(zc |[θp; X; ˆY∗; θs]))∑\nz∈{zc,zw}exp (¯f(z|[θp; X; ˆY∗; θs]))\n(15)\n5194\nFrozen LLM\n!!\"#$%#&\n[soft prompt] Q: Which vitamin assists in blood clotting? A: \nVitamin KLikelihood: 0.7\n!\"##$%#&\nFrozen LLM\n!!\"#$%#&\n[soft prompt] Q: Which vitamin assists in blood clotting? A: Vitamin K. [soft prompt]\nCorrectLikelihood: 0.4  \nCache states\nFigure 3: Implementation of the proposed framework via soft prompt tuning. θp and θs are learnable soft prompt embeddings.\nQembed and Aembed are input embeddings for the question and answer respectively. We ﬁrst generate the answer and the\nlikelihood of the answer, and then compute the learned self-eval score. We can cache the states when generating the answer and\nreuse those states when computing the learned self-eval score to save computational costs.\nwhere ˆY∗is the embedding of ˆy∗.\nTo generate the output sequence and obtain the\nselection score for a given input sequence, we em-\nploy two stages: ﬁrst, we obtain the generated out-\nput and the likelihood for the generated output and\nthen, we obtain the learned self-eval score. Since\nthe query of the second stage is constructed by ap-\npending some additional tokens to the query of the\nﬁrst stage, the second stage can reuse the states in\nthe ﬁrst stage instead of recomputing them to save\nsome computational cost (see Fig. 3).\nLastly, we note that the computational complex-\nity of the proposed method at test time is O(lmax)\nwith lmax being the maximum length of the gen-\nerated output sequence. In Appendix F, we pro-\nvide a more detailed analysis of the computational\ncomplexity of different methods. The predictive\nentropy and semantic entropy methods have a com-\nplexity of O(m·lmax) where mis the number of\noutput sequences sampled for uncertainty estima-\ntion, which is much larger than that of our method.\n6 Experiments\nOur experimental evaluation is focused on the fol-\nlowing questions:\n(Q1) Could a learning-based system using self-\nevaluation improve selective prediction in LLMs\ncompared to other post-hoc selective prediction\nalternatives?\n(A1) By learning self-evaluation, we can signiﬁ-\ncantly improve selective prediction performance\nacross different datasets and LLMs (see Table 1).\n(Q2) What kinds of decoding algorithms could be\nused as Afor the proposed framework ASPIRE?\n(A2) Using decoding algorithms that can sample\ndifferent high-likelihood answers as A(e.g., beam\nsearch) is important for ASPIRE to achieve good\nselective prediction performance (see Table 4).\n(Q3) What is the effect of the number of training\nsamples for the proposed method ASPIRE?\n(A3) More training samples lead to enhanced per-\nformance and with ∼2k samples, ASPIRE can out-\nperform the baselines without soft prompt tuning\nsigniﬁcantly on different datasets (see Table 5).\n6.1 Setup\nDataset. We focus on the free-form question\nanswering tasks on the datasets CoQA (Reddy\net al., 2019), TriviaQA (Joshi et al., 2017) and\nSQuAD (Rajpurkar et al., 2016). For CoQA and\nSQuAD, since each question is asked based on a\ncontext paragraph, we evaluate the LLMs in the\nzero-shot setting. For TriviaQA, since the LLMs\nhave limited accuracy under the zero-shot setting,\nwe evaluate the LLMs in 5-shot setting. For each\ndataset, we use a subset of the original training set\ncontaining 50K examples for adapting LLMs by\ndefault. The details of the datasets are given in\nAppendix B.\nLLMs. We use OPT (Zhang et al., 2022) and\nGPT-2 (Radford et al., 2019) models of various\nsizes. For OPT, we consider OPT-350M, OPT-1.3B,\n5195\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nGPT2-XL\nPerplexity 55.93 62.05 22.60 72.88 7.68 51.90\nPredictive Entropy 60.76 67.53 24.83 76.20 10.04 57.21\nSemantic Entropy 63.03 70.50 24.37 75.33 10.38 59.17\nSelf-eval 46.67 50.83 9.30 42.75 7.32 49.56\nP(True) 46.98 51.17 10.62 44.54 10.69 60.87\nAdapted\nGPT2-XL\nwithθp\nPerplexity 83.27 72.79 36.49 79.92 88.73 75.08\nPredictive Entropy 83.49 73.44 37.31 82.21 88.25 74.16\nSemantic Entropy 84.40 75.16 36.68 81.40 88.62 75.26\nSelf-eval 69.91 51.90 14.39 43.33 74.26 49.13\nP(True) 70.63 52.83 13.59 40.59 74.34 49.09\nASPIRE (ours) 85.65 78.32 38.06 83.23 89.86 78.35\nPre-trained\nOPT-2.7B\nPerplexity 75.26 70.16 40.93 78.86 40.82 57.20\nPredictive Entropy 75.29 69.16 41.20 78.92 47.18 62.85\nSemantic Entropy 76.31 70.96 40.72 78.06 51.53 68.40\nSelf-eval 62.32 52.26 25.88 59.04 41.78 59.05\nP(True) 62.16 51.80 24.88 56.89 34.77 49.42\nPre-trained\nOPT-30B\nSelf-eval 71.99 51.10 36.92 48.90 46.24 57.26\nP(True) 71.59 51.31 36.20 45.63 43.93 54.26\nAdapted\nOPT-2.7B\nwithθp\nPerplexity 90.80 74.23 53.56 81.74 92.86 75.72\nPredictive Entropy 90.63 72.87 53.91 82.19 92.96 75.58\nSemantic Entropy 91.23 74.61 53.58 81.55 93.21 76.53\nSelf-eval 81.30 50.76 32.98 56.03 86.34 56.99\nP(True) 81.14 51.01 33.48 56.27 82.59 49.48\nASPIRE (ours) 92.63 80.25 55.06 84.44 94.73 82.60\nTable 1: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. All numbers are percentages. Bold numbers are superior results.\nOPT-2.7B and OPT-30B. For GPT-2, we consider\nGPT2-Medium, GPT2-Large and GPT2-XL. The\ndetails of these models are given in Appendix C.\nBaselines. For selective prediction, we need to\nget a predicted output sequence ˆy∗and a selection\nscore g(x) for each input sequencex given a model\nf. The model f can be a pre-trained LLM or an\nadapted LLM with θp trained using the training\nobjective (12). We use the beam-search decoding\nto obtain the predicted output sequence ˆy∗ and\nconsider the following baselines to compute the\nselection score g(x): (1) Perplexity; (2) Predictive\nEntropy; (3) Semantic Entropy (Kuhn et al., 2023);\n(4) Self-eval; (5) P(True) (Kadavath et al., 2022).\nMore details can be found in Appendix D.\nEvaluation metrics. We use the Rouge-L (Lin\nand Och, 2004) as the evaluation metricMto evalu-\nate the similarity of the generated answer to the ref-\nerence answers following Kuhn et al. (2023). For\nthe threshold γthat is used to determine the correct-\nness of the generated answer, we consider relatively\nlarger values of γsince we focus on safety-critical\napplications where accepting a wrong answer is\nmore costly compared to rejecting a correct answer\nthat is different from the reference answers (refer\nto Appendix G for the justiﬁcations of the choices\nof γ). Unless speciﬁed, we use γ = 0.7 as default.\nTraining hyper-parameters. We have two stages\nof training: the ﬁrst stage is to train the soft prompt\nθp using the training objective (12) and the sec-\nond stage is to train the soft prompt θs using the\ntraining objective (13). For both stages, we train\nthe soft prompts for 10 epochs using AdamW opti-\nmizer with a batch size of 8, a learning rate of 0.01\nand cosine learning rate scheduling. More training\ndetails can be found in Appendix E.\nASPIRE setup. We use the beam search as the\ndecoding algorithm A. We set the number of beams\nequal to kand use the khighest scoring beams as\nthe answer listA(f,θp,x). We set l= 50, ˆγ = 0.9,\nk= 10, kc = 2, kw = 10 and α= 0.25 by default.\nWe choose these hyper-parameters based on the\nperformance on the validation set from TriviaQA\nusing the OPT-2.7B model. We then use the same\nhyper-parameters across all datasets and models.\n5196\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nAdapted\nOPT-2.7B\nwithθp\nASPIRE (α= 0.0) 90.80 74.23 53.56 81.74 92.86 75.72\nASPIRE (α= 0.25) 92.63 80.25 55.06 84.44 94.73 82.60\nASPIRE (α= 0.5) 92.56 80.18 54.61 84.33 94.59 82.16\nASPIRE (α= 0.75) 92.05 78.37 52.71 81.52 94.28 80.98\nASPIRE (α= 1.0) 91.33 76.08 48.84 76.39 93.77 79.48\nTable 2: Results of studying the effect of the hyper-parameter αin the proposed selection score (Eq. (11)). All\nnumbers are percentages. Bold numbers are superior results.\nModel CoQA TriviaQASQuAD\nAcc↑ Acc↑ Acc↑\nPre-trained GPT2-XL 46.27 11.80 7.41\nAdapted GPT2-XL withθp 69.18 17.45 75.44\nPre-trained OPT-2.7B 60.68 21.38 35.95\nPre-trained OPT-30B 71.06 39.36 41.41\nAdapted OPT-2.7B withθp 80.45 29.21 83.27\nTable 3: Results of evaluating the accuracy of different\nLLMs. All numbers are percentages.\n6.2 Results\nWe ﬁrst evaluate the accuracy of different LLMs.\nThe results in Table 3 show that after training θp\nvia soft prompt tuning, the accuracy of LLMs is\nimproved signiﬁcantly. On the CoQA and SQuAD\ndatasets, the adapted OPT-2.7B can even outper-\nform the pre-trained OPT-30B, which demonstrates\nthat it is possible to adapt a smaller LLM to achieve\nbetter accuracy than a much larger LLM. We then\nevaluate different methods to compute the selection\nscore when the model’s predictions are ﬁxed. The\nresults in Table 1 show that the proposed method\nASPIRE signiﬁcantly outperforms the baselines in\nterms of the AUACC and AUROC metrics across\ndifferent datasets and LLMs. The results also show\nthat after prompt tuning, the AUACC of different\nmethods is signiﬁcantly improved as the accuracy\ngets better and the perplexity becomes more mean-\ningful in separating correct and wrong answers.\nAdditionally, the results show that the proposed\nASPIRE with the adapted OPT-2.7B model can\nsigniﬁcantly outperform the Self-eval and P(True)\nbaselines with the pre-trained OPT-30B model in\nselective prediction performance. Note that on the\nTriviaQA dataset, although the pre-trained OPT-\n30B model has better accuracy than the adapted\nOPT-2.7B model, the Self-eval and P(True) base-\nlines with the pre-trained OPT-30B model have\nmuch worse selective prediction performance com-\npared to the proposed ASPIRE with the adapted\nOPT-2.7B model. These demonstrate that the self-\nevaluation approaches are not effective for high ca-\npacity LLMs, and applying the proposed ASPIRE\nto smaller LLMs can lead to better selective predic-\ntion performance compared to those self-evaluation\napproaches with much larger LLMs. Additional re-\nsults in Appendix I show that ASPIRE signiﬁcantly\noutperforms the baselines across OPT and GPT2\nmodels of different sizes for different values of the\nRouge threshold γ.\n6.3 Empirical Analyses\nThe effect of α. We study the effect of the\nhyper-parameter αin the proposed selection score\n(Eq. (11)). The results in Table 2 show that setting\nα = 0 .25 leads to the best performance since it\ncombines the normalized likelihood and the learned\nself-eval score in a good way. Only using the nor-\nmalized likelihood (i.e., α= 0) or only using the\nlearned self-eval score (i.e., α= 1) leads to much\nworse performance. In practice, the value of αcan\nbe chosen based on the performance on the valida-\ntion data. In Appendix J, we give results for other\nmodels and discuss how we choose α.\nThe choices of A. We compare two decoding\nalgorithms – beam search and multinomial sam-\npling that can be used as Afor answer sampling.\nFor beam search, we use the k highest scoring\nbeams as the answer list. For multinomial sam-\npling, we consider temperature (denoted as T) in\nthe set {0.1,1.0,2.0}. The results in Table 4 show\nthat using multinomial sampling with T = 2.0 or\nT = 0 .1 leads to worse performance compared\nto other decoding algorithms. If we set a high\ntemperature (T = 2.0) for multinomial sampling,\nthen we sample some random answers that might\nnot have high-likelihood. If we set a low temper-\nature (T = 0 .1) for multinomial sampling, then\nwe repeatedly sample the same high-likelihood an-\nswers. Thus, the results suggest that sampling dif-\nferent high-likelihood answers is important for our\n5197\nModel Decoding Algorithm CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nAdapted\nGPT2-XL\nwithθp\nMultinomial (T=0.1) 83.82 74.22 36.40 80.67 89.75 77.56\nMultinomial (T=1.0) 84.96 76.15 37.03 81.41 90.12 78.71\nMultinomial (T=2.0) 83.06 72.96 36.34 80.14 89.41 76.98\nBeam search 85.65 78.32 38.06 83.23 89.86 78.35\nAdapted\nOPT-2.7B\nwithθp\nMultinomial (T=0.1) 92.04 77.96 55.09 84.28 94.24 80.52\nMultinomial (T=1.0) 92.60 79.86 55.15 84.29 94.57 82.08\nMultinomial (T=2.0) 92.02 77.91 53.80 82.40 94.15 80.42\nBeam search 92.63 80.25 55.06 84.44 94.73 82.60\nTable 4: Results of comparing different decoding algorithms for answer sampling in the proposed method. We\ndenote the temperature as T. All numbers are percentages. Bold numbers are superior results.\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nOPT-2.7B\nPredictive Entropy 75.29 69.16 41.20 78.92 47.18 62.85\nSemantic Entropy 76.31 70.96 40.72 78.06 51.53 68.40\nAdapted\nOPT-2.7B\nwithθp\nASPIRE (size=1k) 80.87 67.01 45.70 78.98 85.42 71.42\nASPIRE (size=2k) 85.71 73.72 46.64 79.24 88.27 75.74\nASPIRE (size=5k) 87.83 74.58 49.77 82.06 90.09 77.09\nASPIRE (size=10k) 90.46 78.29 51.88 83.13 92.48 79.46\nASPIRE (size=50k) 92.63 80.25 55.06 84.44 94.73 82.60\nTable 5: Results of studying the effect of training set size for the proposed ASPIRE. All numbers are percentages.\nmethod to achieve high accuracy and coverage in\nselective prediction. The results also show that us-\ning beam search leads to similar performance as\nusing multinomial sampling with T = 1. So we\ncan use either one in practice.\nTraining sample efﬁciency. We perform experi-\nments to study the effect of the number of training\nsamples for ASPIRE. We ﬁx the number of training\nsteps to be 50K while varying the size of the train-\ning dataset. The results in Table 5 show that more\ntraining samples lead to performance improvement\nand with 2K training samples, ASPIRE can out-\nperform the baselines without soft prompt tuning\nby a large margin across different datasets. This\nunderlines that our method, ASPIRE, can signif-\nicantly improve selective prediction performance\neven with limited number of training samples.\n7 Conclusion\nIn this paper, we proposed a novel framework for\nadaptation with self-evaluation to improve selective\nprediction in LLMs. We implemented the frame-\nwork via soft prompt tuning and demonstrated\nits superior performance over existing methods\nthrough extensive experiments. In future work,\none could explore implementing our framework\nvia other parameter efﬁcient tuning approaches and\napplying our method to larger LLMs.\nLimitations\nHigher capacity LLMs are known to often yield\nsuperior capabilities. Our work does not include\nﬁne-tuning experimental results with the largest\nand the strongest LLMs in the literature (we have\nﬁne-tuning results with LLMs up to 2.7B parame-\nters), due to our computational constraints. How-\never, the proposed framework can be applied to\nLLMs of any size and similar improvements are\nexpected. We leave the adoption of our methods to\nlarger-scale LLMs to future work.\nEthics Statement\nLLMs are widely used in various applications\nnowadays. However, they can generate wrong or\nmisleading answers to questions, which can cause\nserious consequences in some safety critical ap-\nplications. The framework proposed in our work\ncan be used to improve selective prediction per-\nformance of LLMs and make their deployments\nmore reliable. However, it is noted that the ob-\ntained selective prediction performances are still\nnot perfect.\n5198\nAcknowledgements\nWe thank all the anonymous reviewers for their\ncareful comments and feedback. The work is\npartially supported by Air Force Grant FA9550-\n18-1-0166, the National Science Foundation\n(NSF) Grants CCF-FMitF-1836978, IIS-2008559,\nSaTC-Frontiers1804648, CCF-2046710 and CCF-\n1652140, and ARO grant number W911NF-17-1-\n0405. Jiefeng Chen and Somesh Jha are partially\nsupported by the DARPA-GARD problem under\nagreement number 885000.\nReferences\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJeremy R Cole, Michael JQ Zhang, Daniel Gillick,\nJulian Martin Eisenschlos, Bhuwan Dhingra,\nand Jacob Eisenstein. 2023. Selectively an-\nswering ambiguous questions. arXiv preprint\narXiv:2305.14613.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nclassiﬁcation for deep neural networks. Advances in\nneural information processing systems, 30.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language mod-\nels for question answering. Transactions of the Asso-\nciation for Computational Linguistics, 9:962–977.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatﬁeld Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nIn The Eleventh International Conference on Learn-\ning Representations.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In Proceedings of the 42nd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL-04), pages 605–612.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\nZhengxiao Du, Zhilin Yang, and Jie Tang. 2021a. P-\ntuning v2: Prompt tuning can be comparable to ﬁne-\ntuning universally across scales and tasks. arXiv\npreprint arXiv:2110.07602.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nR OpenAI. 2023. Gpt-4 technical report. arXiv.\nGabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Ti-\nwari, Gustavo Soares, Christopher Meek, and Sumit\nGulwani. 2022. Synchromesh: Reliable code gen-\neration from pre-trained language models. arXiv\npreprint arXiv:2201.11227.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSiva Reddy, Danqi Chen, and Christopher D Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-\nhammad Saleh, Balaji Lakshminarayanan, and Pe-\nter J Liu. 2022. Out-of-distribution detection and se-\nlective generation for conditional language models.\narXiv preprint arXiv:2209.15558.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-\njuan Wang. 2022. Prompting gpt-3 to be reliable.\narXiv preprint arXiv:2210.09150.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\n5199\net al. 2022. Large language models encode clinical\nknowledge. arXiv preprint arXiv:2212.13138.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, et al.\n2023. Towards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617.\nLiyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G\nNestor, Ali Soroush, Pierre A Elias, Ziyang Xu,\nYing Ding, Greg Durrett, Justin Rousseau, et al.\n2023. Evaluating large language models on medical\nevidence summarization. medRxiv, pages 2023–04.\nNeeraj Varshney and Chitta Baral. 2023. Post-\nabstention: Towards reliably re-attempting the ab-\nstained instances in QA. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n967–982, Toronto, Canada. Association for Compu-\ntational Linguistics.\nNeeraj Varshney, Swaroop Mishra, and Chitta Baral.\n2022. Investigating selective prediction approaches\nacross several tasks in iid, ood, and adversarial set-\ntings. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, pages 1995–2002.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. The art of abstention: Selective prediction and\nerror regularization for natural language processing.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1040–1051.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do\nlarge language models know what they don’t know?\narXiv preprint arXiv:2305.18153.\nHiyori Yoshikawa and Naoaki Okazaki. 2023.\nSelective-lama: Selective prediction for conﬁdence-\naware evaluation of language models. In Findings\nof the Association for Computational Linguistics:\nEACL 2023, pages 1972–1983.\nShun Zhang, Zhenfang Chen, Yikang Shen, Mingyu\nDing, Joshua B Tenenbaum, and Chuang Gan.\n2023a. Planning with large language models for\ncode generation. arXiv preprint arXiv:2303.05510.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy\nLiang, Kathleen McKeown, and Tatsunori B\nHashimoto. 2023b. Benchmarking large language\nmodels for news summarization. arXiv preprint\narXiv:2301.13848.\n5200\nA Hardware and Software\nWe run all experiments using the HuggingFace\nAPI on 40GB NVIDIA A100 GPUs in the De-\nbian GNU/Linux 10 system. We use the OPT and\nGPT2 models via the HuggingFace transformers li-\nbrary which can be easily adapted for reproducibil-\nity. We modify the Trainer class provided by\nthe HuggingFace API for soft prompt tuning. We\nuse the generate() function of the HuggingFace\nAPI to generate answers. Unless speciﬁed, we use\nthe default parameters of the generate() function.\nWhen generating the answer set A(f,θp,x), we\nset max_new_tokens=50 while in other cases, we\nalways set max_new_tokens=256. The parameters\nfor different decoding strategies are provided be-\nlow:\n• Beam search decoding: we set num_beams>1\nand do_sample=False. If we want to get\nnum_beams highest scoring beams, we will set\nnum_return_sequences=num_beams. We\nwill specify num_beams when using beam\nsearch decoding.\n• Multinomial sampling decoding: we set\nnum_beams=1 and do_sample=True. We will\nspecify temperature when using multino-\nmial sampling decoding.\nB Datasets\nWe use three question answering datasets:\nCoQA (Reddy et al., 2019), TriviaQA (Joshi et al.,\n2017) and SQuAD (Rajpurkar et al., 2016) for ex-\nperiments. The details about these datasets are\ngiven below.\nB.1 CoQA\nCoQA is a large-scale dataset for Conversational\nQuestion Answering systems. The goal of the\nCoQA challenge is to measure the ability of ma-\nchines to understand a text passage and answer a\nseries of interconnected questions that appear in a\nconversation. CoQA contains 127,000+ questions\nwith answers collected from 8,000+ conversations.\nThe training set contains 108,647 question queries\nwhile the test set contains 7,983 question queries.\nWe use the following template to construct question\nqueries:\n[The provided context paragraph]\n[additional question-answer pairs]\nQ: [Provided question]\nA:\nwhere additional question-answer pairs are preced-\ning turns of the conversation about the paragraph\nconsisting of questions and reference answers.\nB.2 TriviaQA\nTriviaQA is a reading comprehension dataset\ncontaining over 650K question-answer-evidence\ntriples. TriviaQA includes 95K question-answer\npairs authored by trivia enthusiasts and indepen-\ndently gathered evidence documents, six per ques-\ntion on average, that provide high quality distant\nsupervision for answering the questions. We focus\non TriviaQA as a closed-book QA task (in which\nthe model must answer a question without access to\na supporting paragraph). The training set contains\n138,384 question queries while the test set contains\n17,944 question queries. We split the original test\nset into a new test set containing 8,000 question\nqueries and a validation set containing 9,944 ques-\ntion queries. We use the new test set for evaluation\nand use the validation set for hyper-parameter se-\nlection. We consider the following template with a\n5-shot prompt to construct question queries:\nQ: In which decade did Billboard magazine\nfirst publish and American hit chart? A:\n30s. Q: What is Bruce Willis/quotesingle.Varreal first\nname? A: Walter. Q: Which city does David\nSoul come from? A: Chicago. Q: Which\nWilliam wrote the novel Lord Of The Flies?\nA: Golding. Q: Where in England was Dame\nJudi Dench born? A: York. Q: [Provided\nquestion] A:\nB.3 SQuAD\nStanford Question Answering Dataset (SQuAD)\nis a reading comprehension dataset, consisting of\nquestions posed by crowd-workers on a set of\nWikipedia articles, where the answer to every ques-\ntion is a segment of text, or span, from the cor-\nresponding reading passage. We use the SQuAD\n1.1 version, containing 100,000+ question-answer\npairs on 500+ articles. The training set contains\n86,821 question queries while the test set contains\n5,928 question queries. We use the following tem-\nplate to construct question queries:\n[The provided context paragraph]\nQ: [Provided question]\nA:\n5201\nC LLMs\nWe perform experiments with OPT (Zhang et al.,\n2022) and GPT-2 (Radford et al., 2019) models,\nwhich are based on Transformer architecture. For\nTransformer architecture, there is a limit on the\nlengths of the sequences we can pass the models.\nThe OPT models can handle sequences of up to\n2,048 tokens while the GPT-2 models can handle\nsequences of up to 1,024 tokens. If the sequence\nlength of an input is larger than the maximum se-\nquence length that is allowed, we force the model\nto output an empty sequence with a −∞selection\nscore.\nD Baselines\nFor selective prediction, we need to get a predicted\noutput sequence ˆy∗and a selection score g(x) for\neach input sequence x given a model f. The model\nf can be a pre-trained LLM or an LLM adapted\nwith prompt tuning using training objective (12).\nWe use the beam-search decoding, with the number\nof beams being equal to 5, to obtain the predicted\noutput sequence ˆy∗. We consider the following\nbaselines to compute the selection score g(x):\nPerplexity. Perplexity is deﬁned as the exponenti-\nated average negative log-likelihood of a sequence.\nThe perplexity of the generated output sequence ˆy∗\nis computed as:\nperp(ˆy∗|x; f) = fnorm(ˆy∗|x)−1 (16)\nPredictive Entropy. Predictive entropy is a\nwidely used measure of uncertainty. We use the\nmultinomial sampling with a temperature of 0.5 to\nobtain an answer list [ˆy1,..., ˆym] for each input\nsequence x. The predictive entropy is computed\nas:\npe(x; f) =\nm∑\nj=1\n1\nmlog fnorm(ˆyj|x) (17)\nWe set m = 10 . This is the same as the length-\nnormalised predictive entropy baseline in Kuhn\net al. (2023).\nSemantic Entropy. Semantic entropy is an\nentropy-based uncertainty measure which uses a bi-\ndirectional entailment algorithm for marginalising\nover semantically-equivalent samples (Kuhn et al.,\n2023). We follow the settings in Kuhn et al. (2023).\nSpeciﬁcally, we use the multinomial sampling with\na temperature of 0.5 to obtain an answer list of\nsize 10 for each input sequence for uncertainty\nestimation. We use the Deberta-large model (He\net al., 2020) that is ﬁne-tuned on the NLI data set,\nMNLI (Williams et al., 2017) for the bidirectional\nentailment clustering algorithm.\nSelf-eval. Self-eval is a simple baseline that ob-\ntains a selection score from the LLM by asking\nwhether the proposed answer ˆy∗is correct or not.\nSuppose zs is a series of tokens representing the\nself-evaluation trigger string “The answer is ”. Sup-\npose zc and zw are the tokens that represent the\nwords “correct” and “wrong” respectively. Re-\ncall that the logits of the model f on v given x\nis ¯f(v|x). Then, the self-eval score is computed\nas:\nP(zc |x,ˆy∗) = exp (¯f(zc |x,ˆy∗,zs))∑\nz∈{zc,zw}exp (¯f(z|x,ˆy∗,zs))\n(18)\nP(True). P(True) proposed by Kadavath et al.\n(2022) is a way to estimate the probability that a\nmodel’s generation is correct by “asking” the model\nif its answer is correct. It samples manswers and\nconstructs a new natural language question using\nthese possible answers as context before asking\nwhether the proposed answer ˆy∗is correct and mea-\nsures the probability of the completion being True.\nWe set m = 4 and use the multinomial sampling\nwith a temperature of 1.0 to sample the answers.\nThe format of the prompt is:\nQuestion: Who was the third president of\nthe United States?\nHere are some brainstormed ideas:\nJames Monroe\nThomas Jefferson\nJohn Adams\nBenjamin Harrison\nGeorge Washington\nPossible Answer: James Monroe\nIs the possible answer: (A) True (B) False.\nThe possible answer is:\nwhere the “brainstormed answers” are from the set\nof sampled answers and P(True) (i.e., the likeli-\nhood of the next token being True) is taken as the\nuncertainty measure.\nE Training Details\nWe have two stage training: the ﬁrst stage is to\ntrain the soft prompt θp using the training objec-\ntive (12) and the second stage is to train the soft\nprompt θs using the training objective (13). For\n5202\nboth stages, we train the soft prompt for 10 epochs\nusing AdamW optimizer with a batch size of 8, a\nlearning rate of 0.01 and cosine learning rate sched-\nule. We remove those data points (x,y) where\n|x|+ |y|>700 from the training set Dtr to reduce\nGPU memory usage during training. Here, |x|is\nthe length of the sequence x. This only removes a\nvery small portion of data points from the training\nset for each dataset (remove 4.02% training data\npoints in CoQA, 0% training data points in Trivi-\naQA and 0.04% training data points in SQuAD).\nDuring training θp or θs, we always use 20% train-\ning data as validation data for selecting the best\nmodel among all checkpoints after each training\nepoch. Training θp, we select the best model based\non the loss on the validation data. When training\nθs, we select the best model based on the AUROC\non the validation data.\nF Computational Complexity Analysis\nThe proposed method ASPIRE needs to train two\nsoft prompts θp and θs. The complexity of training\nθp using the training objective (12) is the same as\nthe complexity of the standard soft prompt tuning.\nWhen training θs using the training objective (13),\nthe number of training steps is the same as that\nof training θp. In each training step of training θs,\nwe compute gradients for one correct output and\ntwo wrong outputs while in each training step of\ntraining θp, we compute gradients for one reference\noutput. Thus, the complexity of training θs is the\nsame as that of training θp. Therefore, the complex-\nity of the proposed method ASPIRE in the training\ntime is the same as that of the standard soft prompt\ntuning.\nWe analyze the computational complexity of dif-\nferent methods at test time in terms of the number\nof forward passes for the LLM. Since the LLM\ngenerates the output sequence in an auto-regressive\nway, the number of forward passes needed depends\non the length of the generated output sequence.\nSuppose the maximum length of the generated out-\nput sequence is lmax. To generate an output se-\nquence given an input sequence, we need one for-\nward pass to encode the input sequence and at most\nlmax forward passes to obtain the output sequence.\nThus, for generating the output sequence, the maxi-\nmum number of forward passes is 1 +lmax and the\ncomplexity is O(lmax). For the perplexity method,\nthe computational complexity is O(lmax) since we\nonly need additional one forward pass to obtain the\nperplexity score. For the predictive entropy method,\nthe computational complexity is O(m·lmax) since\nwe need to additionally generate manswers and\ncompute the likelihood of those manswers. For\nthe semantic entropy method, we omit the compu-\ntational complexity of the bidirectional entailment\nclustering algorithm since its computational cost\nis much smaller than that of the generation of the\nLLM as stated in Kuhn et al. (2023). Thus, the\ncomputational complexity for semantic entropy is\nO(m·lmax). For the self-eval method, the compu-\ntational complexity is O(lmax) since we only need\none additional forward pass to obtain the self-eval\nscore. For the P(True) method, the computational\ncomplexity is O(m·lmax) since we need to ad-\nditionally generate manswers and need one for-\nward pass to compute the P(True) score. For the\nproposed method ASPIRE, the computational com-\nplexity is O(lmax) since we only need additional\none forward pass to obtain the learned self-eval\nscore. Table 6 summarizes the computational com-\nplexity of different methods at test time.\nMethod Complexity\nPerplexity O(lmax)\nPredictive Entropy O(m·lmax)\nSemantic Entropy O(m·lmax)\nSelf-eval O(lmax)\nP(True) O(m·lmax)\nASPIRE (ours) O(lmax)\nTable 6: Computational complexity of different meth-\nods in the test time.\nG Rouge Threshold for Evaluation\nWe use the Rouge-L (Lin and Och, 2004) metric\nto evaluate if the predicted answer is correct or\nnot. The Rouge-L metric produces a score in [0,1].\nWe need a threshold γ to determine whether the\npredicted answer is correct or not. If the Rouge-L\nscore is larger than the threshold γ, then the pre-\ndicted answer is correct; otherwise, the predicted\nanswer is wrong. The choice of γdepends on the\napplications. Low values of γ may lead to label-\ning some wrong answers as correct answers while\nlarge values of γmay lead to labeling some correct\nanswers as wrong answers. If we regard the wrong\nanswer as the positive class, then we can use the\nprecision and recall metrics to evaluate the choice\nof γ. To compute the precision and recall metrics,\nwe need ground-truth labels for determining the\n5203\ncorrectness of predicted answers, which requires\nmanual labeling. If the Rouge-L score is equal to\n0 (or 1), then it is mostly sure that the predicted\nanswer is wrong (or correct). Thus, we only need\nto label those samples whose Rouge-L scores are in\n(0,1). To compare different values of γ, we com-\npute the precision and recall metrics after manually\nlabel 200 samples whose Rouge-L scores are in the\nrange of (0, 1). The results in Table 7 show that\nlarger values of γ lead to higher recall but lower\nprecision, while the lower values ofγlead to higher\nprecision but lower recall. We propose this work\nfor safety-critical applications where accepting a\nwrong answer is more costly compared to rejecting\na correct answer that is different from the refer-\nence answers. Thus, we prefer high recall than\nhigh precision. In our experiments, we evaluate\ndifferent methods under the Rouge-L metric with\nγ ∈{0.7,0.8,0,9}to ensure that the recall is at\nleast 90%.\nH Rouge Threshold for the Proposed\nFramework\nIn the proposed framework ASPIRE, we need the\nRouge threshold ˆγ to determine if the generated\nanswer is correct or not. We want to set a large\nenough value of ˆγ so that the generated wrong\nanswers won’t be labeled as correct answers. To\ndetermine the value ofˆγ, we manually label the cor-\nrectness of the 10 generated answers for 50 training\nexamples from each dataset (we have three datasets\nCoQA, TriviaQA and SQuAD). The answers are\ngenerated using the OPT-2.7B model. We ﬁnd that\nif we set ˆγ = 0.9, then no wrong answers would be\nlabeled as correct answers. Thus, we set ˆγ = 0.9\nfor the proposed framework.\nI Complete Results\nIn this section, we present the complete results for\nOPT and GPT2 models of different sizes and dif-\nferent Rouge threshold γ. We ﬁrst evaluate the\naccuracy of different LLMs. The results are in Ta-\nble 8 (set γ = 0 .7), Table 9 (set γ = 0 .8) and\nTable 10 (set γ = 0.9). The results show that after\ntraining θp via soft prompt tuning, the accuracy of\nLLMs is improved signiﬁcantly. We then evaluate\ndifferent approaches to compute the selection score\nwhen the model’s predictions are ﬁxed. The results\nare in Table 11 (use GPT2 models and setγ = 0.7),\nTable 12 (use GPT2 models and set γ = 0.8), Ta-\nble 13 (use GPT2 models and set γ = 0 .9), Ta-\nble 14 (use OPT models and set γ = 0 .7), Ta-\nble 15 (use OPT models and set γ = 0 .8) and\nTable 16 (use OPT models and set γ = 0.9). The\nresults show that the proposed method ASPIRE\nsigniﬁcantly outperforms the baselines in terms of\nAUACC and AUROC across different datasets and\nLLMs for different values of the Rouge threshold\nγ.\nJ The Effect of the Hyper-parameter α\nWe study the effect of the hyper-parameter α in\nthe proposed selection score (Eq. (11)) for our\nmethod. The results in Table 17 show that set-\nting α= 0.25 leads to the best performance across\ndifferent datasets and different models. Only us-\ning the normalized likelihood (i.e., α= 0) or only\nusing the learned self-eval score (i.e., α= 1) con-\nsistently leads to much worse performance. We\nchoose αfor our method based on the performance\non the validation data from the TriviaQA dataset\nusing the OPT-2.7B model. We then use the same\nαvalue for different datasets and different models.\nWe consider α ∈{0.0,0.25,0.5,0.75,1.0}when\ntuning it. Based on the validation results, we set\nα= 0.25 by default.\nK Comparing with Self-Consistency\nSelf-consistency (Wang et al., 2022) can be used\nto obtain conﬁdence measures as proposed by Si\net al. (2022). We sample 10 times to obtain a\nset of different answers for each question using\nthe multinomial sampling with a temperature of\n0.5. Among all the generated answers, we take\nthe most frequent answer as the ﬁnal prediction\nand its frequency as the selection score. Since\nself-consistency produces discrete selection scores\n(in the above setting, the number of possible se-\nlection scores is 10) and we use the composite\ntrapezoidal rule to compute AUACC, it is eas-\nier for self-consistency to achieve high AUACC\ncompared to those approaches that produce con-\ntinuous selection scores. Note that the proposed\nmethod produce continuous selection scores. Thus,\nit might not be fair to compare the proposed method\nwith self-consistency. However, even though self-\nconsistency has more advantages in achieving high\nAUACC, the proposed method ASPIRE still signif-\nicantly outperforms self-consistency as shown in\nTable 18. We also observe that Self-Consistency\nmight lead to worse accuracy meaning that the\nLLM can be consistently wrong.\n5204\nγ CoQA TriviaQA SQuAD\nPrecision ↑ Recall ↑ Precision ↑ Recall ↑ Precision ↑ Recall ↑\n0.1 100.00 0.00 100.00 0.62 100.00 7.91\n0.2 100.00 10.00 100.00 2.50 100.00 34.53\n0.3 100.00 22.50 100.00 11.88 98.55 48.92\n0.4 100.00 45.62 97.01 40.62 93.58 73.38\n0.5 97.98 60.62 97.09 62.50 85.94 79.14\n0.6 97.41 70.62 96.19 63.12 84.73 79.86\n0.7 93.51 90.00 86.81 98.75 76.16 94.24\n0.8 86.59 96.88 81.22 100.00 73.66 98.56\n0.9 80.71 99.38 80.00 100.00 69.85 100.00\nTable 7: Results of comparing different choices of the Rouge threshold γ. The wrong answer is regarded as the\npositive class. We use the OPT-2.7B model. We manually label 200 samples with Rouge-L scores in the range of\n(0, 1) in each dataset and then compute the precision and recall. All numbers are percentages.\nModel CoQA TriviaQA SQuAD\nAcc ↑ Acc ↑ Acc ↑\nPre-trained GPT2-Medium 35.12 5.44 4.42\nAdapted GPT2-Medium with θp 57.90 9.04 66.63\nPre-trained GPT2-Large 41.21 8.16 6.09\nAdapted GPT2-Large with θp 63.89 12.50 71.34\nPre-trained GPT2-XL 46.27 11.80 7.41\nAdapted GPT2-XL with θp 69.18 17.45 75.44\nPre-trained OPT-350M 28.76 4.35 13.65\nAdapted OPT-350M with θp 59.46 8.25 64.74\nPre-trained OPT-1.3B 54.13 15.80 30.23\nAdapted OPT-1.3B with θp 76.85 21.73 80.94\nPre-trained OPT-2.7B 60.68 21.38 35.95\nAdapted OPT-2.7B with θp 80.45 29.21 83.27\nTable 8: Results of evaluating the accuracy of different LLMs when the Rouge thresholdγ = 0.7. All numbers are\npercentages.\nL Qualitative Evaluation\nWe present some concrete examples from the Triv-\niaQA dataset to show the advantages of the pro-\nposed method qualitatively. We compare the pro-\nposed method ASPIRE to the baseline Semantic\nEntropy. The model for generating answers is the\nadapted OPT-2.7B with learned θp. The examples\nbelow show that some semantic entropy scores for\ncorrect predictions are lower than some semantic\nentropy scores for wrong predictions while the AS-\nPIRE scores for correct predictions are consistently\nhigher than the ASPIRE scores for wrong predic-\ntions. The ASPIRE scores are log likelihood scores\nand can be converted to likelihood scores by taking\nexponentiation with the base e.\nExamples where predictions are correct\nQuestion: Who is the most successful UK solo\nartist in the USA?\nAnswer: Elton John.\nPredicted answer: Elton John.\nSemantic entropy score: -1.1031\nASPIRE score: -0.8163\nQuestion: In which decade of the 20th century\nwas Anne Bancroft born?\nAnswer: 1930s.\nPredicted answer: 1930s.\nSemantic entropy score: -0.6167\nASPIRE score: -0.9026\nQuestion: The Suez Canal connects the Mediter-\nranean Sea to which other Sea?\nAnswer: Red sea.\n5205\nModel CoQA TriviaQA SQuAD\nAcc ↑ Acc ↑ Acc ↑\nPre-trained GPT2-Medium 32.12 5.00 2.85\nAdapted GPT2-Medium with θp 55.12 8.71 62.92\nPre-trained GPT2-Large 38.16 7.64 3.98\nAdapted GPT2-Large with θp 61.04 12.14 67.56\nPre-trained GPT2-XL 42.67 11.10 5.21\nAdapted GPT2-XL with θp 66.49 16.96 71.17\nPre-trained OPT-350M 27.38 4.25 11.15\nAdapted OPT-350M with θp 57.02 8.05 61.29\nPre-trained OPT-1.3B 51.35 15.35 25.73\nAdapted OPT-1.3B with θp 74.46 21.26 77.28\nPre-trained OPT-2.7B 57.72 20.71 30.94\nAdapted OPT-2.7B with θp 77.97 28.55 80.04\nTable 9: Results of evaluating the accuracy of different LLMs when the Rouge thresholdγ = 0.8. All numbers are\npercentages.\nModel CoQA TriviaQA SQuAD\nAcc ↑ Acc ↑ Acc ↑\nPre-trained GPT2-Medium 30.49 4.88 1.99\nAdapted GPT2-Medium with θp 53.11 8.53 60.51\nPre-trained GPT2-Large 36.20 7.41 3.00\nAdapted GPT2-Large with θp 59.04 11.85 64.98\nPre-trained GPT2-XL 40.32 10.82 4.12\nAdapted GPT2-XL with θp 64.59 16.70 68.83\nPre-trained OPT-350M 26.81 4.20 9.62\nAdapted OPT-350M with θp 55.33 8.00 59.35\nPre-trained OPT-1.3B 49.78 15.24 22.79\nAdapted OPT-1.3B with θp 72.78 21.07 74.97\nPre-trained OPT-2.7B 56.06 20.55 27.41\nAdapted OPT-2.7B with θp 76.45 28.26 78.12\nTable 10: Results of evaluating the accuracy of different LLMs when the Rouge threshold γ = 0.9. All numbers\nare percentages.\nPredicted answer: Red Sea.\nSemantic entropy score: 2.2082\nASPIRE score: -0.2309\nQuestion: Sun Yat Sen overthrew the emperor\nin which country establishing a republic after 2000\nyears of imperial rule?\nAnswer: China.\nPredicted answer: China.\nSemantic entropy score: 2.0028\nASPIRE score: -0.4205\nExamples where predictions are wrong\nQuestion: Who was the director of the CIA from\n1976-81?\nAnswer: George Bush.\nPredicted answer: George H W Bush.\nSemantic entropy score: 0.4547\nASPIRE score: -1.0397\nQuestion: What Michelle Pfeiffer movie got a\nboost from the Coolio song Gangsta’s Paradise?\nAnswer: Dangerous Minds.\nPredicted answer: Scarface.\nSemantic entropy score: 0.0647\nASPIRE score: -1.0531\nQuestion: What was President Gerald Ford’s\nmiddle name?\nAnswer: Rudolph.\nPredicted answer: William.\n5206\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nGPT2-\nMedium\nPerplexity 38.92 55.77 7.67 60.52 4.58 55.35\nPredictive Entropy 45.91 62.89 10.02 67.66 5.99 57.07\nSemantic Entropy 48.35 66.30 10.28 68.54 6.18 57.36\nSelf-eval 36.16 51.14 6.26 56.74 3.70 43.44\nP(True) 34.08 48.21 8.24 60.62 5.41 54.33\nAdapted\nGPT2-\nMedium\nwithθp\nPerplexity 72.03 67.89 18.02 72.17 81.91 72.38\nPredictive Entropy 72.59 69.42 20.07 77.48 82.00 73.09\nSemantic Entropy 73.95 71.54 19.86 77.62 82.35 73.66\nSelf-eval 57.94 50.43 9.94 54.68 64.79 46.99\nP(True) 56.71 48.76 13.55 60.79 65.94 49.13\nASPIRE (ours) 76.32 75.30 20.65 79.41 84.15 77.59\nPre-trained\nGPT2-\nLarge\nPerplexity 48.57 59.82 13.74 66.51 6.39 53.96\nPredictive Entropy 55.04 66.68 16.25 70.46 8.25 57.03\nSemantic Entropy 57.13 69.57 16.02 70.06 8.81 59.24\nSelf-eval 42.24 51.72 9.78 54.74 5.07 46.79\nP(True) 36.73 45.69 8.60 48.62 6.83 55.62\nAdapted\nGPT2-\nLarge with\nθp\nPerplexity 77.15 68.15 26.83 77.06 86.26 75.34\nPredictive Entropy 77.45 69.76 27.83 80.02 86.32 75.65\nSemantic Entropy 78.85 71.97 27.61 79.88 86.53 75.90\nSelf-eval 64.28 50.61 14.26 54.34 70.86 50.81\nP(True) 58.97 45.55 12.38 47.61 70.73 50.09\nASPIRE (ours) 81.30 76.38 29.13 82.14 87.83 79.22\nPre-trained\nGPT2-XL\nPerplexity 55.93 62.05 22.60 72.88 7.68 51.90\nPredictive Entropy 60.76 67.53 24.83 76.20 10.04 57.21\nSemantic Entropy 63.03 70.50 24.37 75.33 10.38 59.17\nSelf-eval 46.67 50.83 9.30 42.75 7.32 49.56\nP(True) 46.98 51.17 10.62 44.54 10.69 60.87\nAdapted\nGPT2-XL\nwithθp\nPerplexity 83.27 72.79 36.49 79.92 88.73 75.08\nPredictive Entropy 83.49 73.44 37.31 82.21 88.25 74.16\nSemantic Entropy 84.40 75.16 36.68 81.40 88.62 75.26\nSelf-eval 69.91 51.90 14.39 43.33 74.26 49.13\nP(True) 70.63 52.83 13.59 40.59 74.34 49.09\nASPIRE (ours) 85.65 78.32 38.06 83.23 89.86 78.35\nTable 11: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. We use the GPT2 models and set the Rouge threshold γ = 0.7. All numbers are percentages. Bold numbers\nare superior results.\nSemantic entropy score: -3.9773\nASPIRE score: -2.8203\nQuestion: Kim Carnes’ nine weeks at No 1 with\nBette Davis Eyes was interrupted for one week by\nwhich song?\nAnswer: Stars on 45 medley.\nPredicted answer: Bette Davis Eyes.\nSemantic entropy score: -1.4973\nASPIRE score: -2.2803\n5207\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nGPT2-\nMedium\nPerplexity 35.24 54.28 7.03 59.54 2.82 53.29\nPredictive Entropy 42.43 62.42 9.23 66.62 3.86 58.15\nSemantic Entropy 45.53 66.84 9.52 67.83 4.02 58.53\nSelf-eval 32.97 51.13 5.95 57.98 2.06 40.62\nP(True) 31.05 48.10 7.81 61.51 3.73 55.72\nAdapted\nGPT2-\nMedium\nwithθp\nPerplexity 69.36 67.21 17.42 71.77 79.26 72.25\nPredictive Entropy 69.74 68.58 19.38 77.26 79.18 72.70\nSemantic Entropy 71.35 71.10 19.22 77.55 79.61 73.53\nSelf-eval 55.04 50.31 9.75 55.26 61.43 47.61\nP(True) 53.95 48.66 13.32 61.55 62.07 49.06\nASPIRE (ours) 73.97 75.05 20.12 79.59 82.02 78.02\nPre-trained\nGPT2-\nLarge\nPerplexity 44.95 58.70 13.06 66.47 4.06 51.95\nPredictive Entropy 51.57 66.32 15.43 70.33 5.61 57.34\nSemantic Entropy 54.39 70.24 15.25 70.08 6.25 61.09\nSelf-eval 39.66 52.36 9.21 54.62 3.15 45.40\nP(True) 33.73 45.72 8.20 49.18 4.68 57.51\nAdapted\nGPT2-\nLarge with\nθp\nPerplexity 74.64 67.40 26.20 76.89 83.61 74.57\nPredictive Entropy 74.96 69.07 27.22 80.01 83.57 74.67\nSemantic Entropy 76.65 71.82 27.06 79.99 83.81 75.10\nSelf-eval 61.88 50.99 13.83 54.15 67.28 51.20\nP(True) 56.35 45.90 11.95 47.55 67.13 50.52\nASPIRE (ours) 79.39 76.49 28.43 82.01 85.71 79.27\nPre-trained\nGPT2-XL\nPerplexity 52.07 61.15 21.54 72.72 5.30 49.81\nPredictive Entropy 56.83 66.90 23.65 76.15 7.27 56.53\nSemantic Entropy 59.74 70.83 23.23 75.38 7.59 58.85\nSelf-eval 43.34 51.14 8.81 42.76 5.45 51.47\nP(True) 43.24 51.09 9.81 43.94 8.54 65.61\nAdapted\nGPT2-XL\nwithθp\nPerplexity 81.05 71.85 35.61 79.69 86.08 74.71\nPredictive Entropy 81.23 72.42 36.42 82.01 85.53 73.62\nSemantic Entropy 82.38 74.62 35.84 81.31 85.93 74.84\nSelf-eval 67.35 51.89 14.05 43.45 70.30 49.21\nP(True) 68.02 52.83 13.21 40.48 69.47 48.32\nASPIRE (ours) 83.91 78.09 37.26 83.18 87.76 78.82\nTable 12: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. We use the GPT2 models and set the Rouge threshold γ = 0.8. All numbers are percentages. Bold numbers\nare superior results.\n5208\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nGPT2-\nMedium\nPerplexity 33.09 53.03 6.72 58.75 1.74 47.52\nPredictive Entropy 40.52 62.05 8.90 66.11 2.61 56.43\nSemantic Entropy 44.11 67.38 9.26 67.57 2.85 57.36\nSelf-eval 31.46 51.29 5.88 58.50 1.17 35.24\nP(True) 29.24 47.97 7.73 62.30 2.78 58.35\nAdapted\nGPT2-\nMedium\nwithθp\nPerplexity 67.42 66.51 17.06 71.50 77.63 72.22\nPredictive Entropy 67.89 68.15 19.06 77.30 77.49 72.55\nSemantic Entropy 69.77 71.20 18.94 77.75 78.07 73.77\nSelf-eval 53.15 50.51 9.67 55.80 58.95 47.61\nP(True) 51.95 48.59 13.21 62.06 59.55 48.95\nASPIRE (ours) 72.39 74.96 19.81 79.64 80.68 78.49\nPre-trained\nGPT2-\nLarge\nPerplexity 42.74 57.93 12.56 65.85 2.78 46.87\nPredictive Entropy 49.68 66.44 14.89 69.76 3.94 54.80\nSemantic Entropy 52.90 71.07 14.76 69.63 4.53 59.75\nSelf-eval 38.08 52.84 8.97 54.45 2.42 45.97\nP(True) 31.71 45.48 8.06 49.66 3.84 60.48\nAdapted\nGPT2-\nLarge with\nθp\nPerplexity 72.97 66.96 25.67 76.60 81.77 74.01\nPredictive Entropy 73.34 68.78 26.69 79.84 81.80 74.31\nSemantic Entropy 75.24 71.99 26.59 79.96 82.29 75.36\nSelf-eval 60.35 51.58 13.44 53.82 64.89 51.42\nP(True) 54.34 45.68 11.65 47.57 64.77 50.75\nASPIRE (ours) 78.08 76.61 27.97 81.99 84.24 79.37\nPre-trained\nGPT2-XL\nPerplexity 49.71 60.59 20.96 72.31 4.04 46.28\nPredictive Entropy 54.74 67.05 23.10 75.93 5.78 55.59\nSemantic Entropy 58.07 71.83 22.76 75.33 6.18 59.20\nSelf-eval 41.19 51.46 8.67 42.97 4.61 53.48\nP(True) 40.37 50.77 9.46 43.58 7.30 69.47\nAdapted\nGPT2-XL\nwithθp\nPerplexity 79.60 71.40 35.11 79.47 84.69 74.62\nPredictive Entropy 79.86 72.25 35.93 81.85 84.23 73.81\nSemantic Entropy 81.25 74.87 35.39 81.19 84.74 75.38\nSelf-eval 65.61 52.08 13.87 43.51 68.10 49.56\nP(True) 65.90 52.61 12.95 40.31 67.37 48.74\nASPIRE (ours) 82.77 78.11 36.81 83.09 86.57 79.06\nTable 13: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. We use the GPT2 models and set the Rouge threshold γ = 0.9. All numbers are percentages. Bold numbers\nare superior results.\n5209\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nOPT-350M\nPerplexity 35.37 59.39 6.81 67.09 13.07 50.34\nPredictive Entropy 36.55 60.31 7.20 65.04 17.86 59.33\nSemantic Entropy 38.80 64.38 7.31 65.15 19.08 61.66\nSelf-eval 30.02 52.69 5.98 61.17 14.00 51.41\nP(True) 28.70 50.60 5.29 55.69 17.76 59.55\nAdapted\nOPT-350M\nwithθp\nPerplexity 74.50 70.21 18.13 75.86 80.64 73.76\nPredictive Entropy 74.14 68.88 18.73 76.83 80.79 73.46\nSemantic Entropy 74.94 70.14 18.46 76.91 81.10 73.98\nSelf-eval 60.86 51.67 10.29 57.89 65.48 50.70\nP(True) 59.20 50.04 8.71 52.05 64.55 50.29\nASPIRE (ours) 75.55 72.37 19.00 78.54 82.59 77.18\nPre-trained\nOPT-1.3B\nPerplexity 69.51 69.32 29.78 74.77 32.43 54.65\nPredictive Entropy 69.46 68.48 31.01 75.21 41.06 62.96\nSemantic Entropy 70.42 70.46 30.63 74.74 43.33 66.30\nSelf-eval 56.38 52.86 15.06 49.96 30.74 51.50\nP(True) 57.21 53.19 16.83 51.19 28.88 46.75\nAdapted\nOPT-1.3B\nwithθp\nPerplexity 88.50 73.64 42.46 79.96 91.45 74.47\nPredictive Entropy 88.24 72.38 43.03 80.46 91.46 74.38\nSemantic Entropy 88.91 74.02 42.70 80.02 91.72 75.44\nSelf-eval 78.52 53.08 20.65 49.24 81.05 51.52\nP(True) 79.07 52.76 22.20 50.34 81.58 50.77\nASPIRE (ours) 90.76 79.26 44.03 83.06 93.41 81.17\nPre-trained\nOPT-2.7B\nPerplexity 75.26 70.16 40.93 78.86 40.82 57.20\nPredictive Entropy 75.29 69.16 41.20 78.92 47.18 62.85\nSemantic Entropy 76.31 70.96 40.72 78.06 51.53 68.40\nSelf-eval 62.32 52.26 25.88 59.04 41.78 59.05\nP(True) 62.16 51.80 24.88 56.89 34.77 49.42\nAdapted\nOPT-2.7B\nwithθp\nPerplexity 90.80 74.23 53.56 81.74 92.86 75.72\nPredictive Entropy 90.63 72.87 53.91 82.19 92.96 75.58\nSemantic Entropy 91.23 74.61 53.58 81.55 93.21 76.53\nSelf-eval 81.30 50.76 32.98 56.03 86.34 56.99\nP(True) 81.14 51.01 33.48 56.27 82.59 49.48\nASPIRE (ours) 92.63 80.25 55.06 84.44 94.73 82.60\nTable 14: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. We use the OPT models and set the Rouge threshold γ = 0.7. All numbers are percentages. Bold numbers\nare superior results.\n5210\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nOPT-350M\nPerplexity 33.50 58.50 6.64 66.73 10.50 49.05\nPredictive Entropy 34.88 59.82 7.03 64.84 14.59 58.98\nSemantic Entropy 37.51 64.67 7.17 65.20 15.74 61.85\nSelf-eval 28.73 52.91 5.93 61.83 11.33 50.71\nP(True) 27.10 50.36 5.25 56.28 15.38 61.06\nAdapted\nOPT-350M\nwithθp\nPerplexity 72.04 69.26 17.76 75.70 77.72 72.95\nPredictive Entropy 71.77 68.12 18.30 76.51 77.91 72.76\nSemantic Entropy 72.80 69.90 18.06 76.67 78.35 73.57\nSelf-eval 58.65 52.06 10.03 57.87 61.83 50.32\nP(True) 56.82 50.17 8.58 52.27 61.13 50.18\nASPIRE (ours) 73.56 72.05 18.63 78.42 80.33 77.29\nPre-trained\nOPT-1.3B\nPerplexity 66.09 67.76 29.01 74.46 27.67 53.61\nPredictive Entropy 66.34 67.36 30.21 74.92 35.65 62.44\nSemantic Entropy 67.64 70.02 29.91 74.61 38.00 66.50\nSelf-eval 53.87 53.23 14.73 50.12 26.42 51.63\nP(True) 54.07 52.70 16.44 51.38 23.69 45.44\nAdapted\nOPT-1.3B\nwithθp\nPerplexity 86.67 72.53 41.59 79.61 89.00 73.48\nPredictive Entropy 86.41 71.33 42.18 80.15 89.02 73.35\nSemantic Entropy 87.27 73.41 41.89 79.75 89.42 74.81\nSelf-eval 76.49 53.45 20.23 49.20 77.85 52.25\nP(True) 76.79 52.52 21.65 50.25 77.86 50.71\nASPIRE (ours) 89.48 79.05 43.23 82.84 91.86 81.44\nPre-trained\nOPT-2.7B\nPerplexity 72.00 68.49 39.79 78.43 35.76 56.78\nPredictive Entropy 72.23 67.89 40.05 78.49 41.18 61.98\nSemantic Entropy 73.64 70.43 39.67 77.81 45.83 68.35\nSelf-eval 59.51 52.24 25.10 59.02 36.71 59.36\nP(True) 58.81 51.26 24.13 56.80 29.13 48.41\nAdapted\nOPT-2.7B\nwithθp\nPerplexity 89.10 73.16 52.64 81.56 91.04 74.96\nPredictive Entropy 88.95 72.00 52.97 82.00 91.16 74.86\nSemantic Entropy 89.80 74.53 52.71 81.47 91.46 75.91\nSelf-eval 79.12 51.00 32.28 56.03 83.28 56.52\nP(True) 78.74 50.89 32.95 56.42 79.05 49.26\nASPIRE (ours) 91.49 80.12 54.15 84.28 93.37 82.33\nTable 15: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. We use the OPT models and set the Rouge threshold γ = 0.8. All numbers are percentages. Bold numbers\nare superior results.\n5211\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nPre-trained\nOPT-350M\nPerplexity 32.58 57.88 6.50 66.50 8.63 46.42\nPredictive Entropy 34.13 59.61 6.91 64.62 12.56 58.33\nSemantic Entropy 36.97 64.81 7.06 65.06 13.82 61.99\nSelf-eval 27.98 52.97 5.90 62.10 10.08 51.66\nP(True) 26.41 50.23 5.21 56.37 14.01 62.76\nAdapted\nOPT-350M\nwithθp\nPerplexity 70.07 68.20 17.67 75.63 76.12 72.40\nPredictive Entropy 69.97 67.41 18.22 76.47 76.44 72.48\nSemantic Entropy 71.38 69.93 17.98 76.65 77.11 73.81\nSelf-eval 57.05 52.30 9.96 57.81 59.96 50.56\nP(True) 55.10 50.32 8.53 52.22 59.05 50.03\nASPIRE (ours) 71.94 71.41 18.54 78.39 79.12 77.38\nPre-trained\nOPT-1.3B\nPerplexity 64.03 66.70 28.77 74.31 24.05 51.41\nPredictive Entropy 64.59 66.80 29.98 74.81 31.35 60.95\nSemantic Entropy 66.29 70.03 29.72 74.56 34.05 66.05\nSelf-eval 52.35 53.37 14.64 50.14 24.12 52.63\nP(True) 52.51 52.64 16.27 51.38 20.92 45.41\nAdapted\nOPT-1.3B\nwithθp\nPerplexity 85.21 71.30 41.21 79.43 87.71 73.17\nPredictive Entropy 85.05 70.44 41.81 80.00 87.81 73.34\nSemantic Entropy 86.23 73.38 41.55 79.66 88.24 74.81\nSelf-eval 75.09 53.72 20.07 49.23 75.80 52.60\nP(True) 75.16 52.38 21.44 50.22 75.83 51.10\nASPIRE (ours) 88.49 78.52 42.88 82.70 90.79 81.34\nPre-trained\nOPT-2.7B\nPerplexity 70.07 67.37 39.42 78.23 31.18 54.43\nPredictive Entropy 70.44 67.03 39.69 78.34 36.14 60.36\nSemantic Entropy 72.29 70.35 39.34 77.68 40.96 67.71\nSelf-eval 57.76 52.07 24.85 58.93 32.56 59.52\nP(True) 57.06 50.98 23.96 56.74 25.64 48.02\nAdapted\nOPT-2.7B\nwithθp\nPerplexity 88.06 72.44 52.12 81.33 90.01 74.59\nPredictive Entropy 87.95 71.48 52.48 81.81 90.17 74.71\nSemantic Entropy 88.96 74.50 52.28 81.35 90.47 75.75\nSelf-eval 77.71 51.04 31.90 55.89 81.27 56.36\nP(True) 77.16 50.54 32.62 56.33 76.89 48.85\nASPIRE (ours) 90.76 79.94 53.68 84.10 92.52 82.04\nTable 16: Results of evaluating different methods to compute the selection score when the model’s predictions are\nﬁxed. We use the OPT models and set the Rouge threshold γ = 0.9. All numbers are percentages. Bold numbers\nare superior results.\n5212\nModel Method CoQA TriviaQA SQuAD\nAUACC↑ AUROC↑ AUACC↑ AUROC↑ AUACC↑ AUROC↑\nAdapted\nGPT2-\nMedium\nwithθp\nASPIRE (α= 0.0) 72.03 67.89 18.02 72.17 81.91 72.38\nASPIRE (α= 0.25) 76.32 75.30 20.65 79.41 84.15 77.59\nASPIRE (α= 0.5) 75.76 75.27 20.24 80.35 83.24 76.50\nASPIRE (α= 0.75) 73.26 71.99 18.01 77.00 81.76 73.98\nASPIRE (α= 1.0) 67.61 66.72 14.52 72.52 79.60 70.52\nAdapted\nGPT2-\nLarge with\nθp\nASPIRE (α= 0.0) 77.15 68.15 26.83 77.06 86.26 75.34\nASPIRE (α= 0.25) 81.30 76.38 29.13 82.14 87.83 79.22\nASPIRE (α= 0.5) 80.87 76.39 28.49 82.41 86.97 77.66\nASPIRE (α= 0.75) 78.91 73.38 25.32 78.74 85.66 74.95\nASPIRE (α= 1.0) 74.22 68.01 19.77 72.75 83.99 71.74\nAdapted\nGPT2-XL\nwithθp\nASPIRE (α= 0.0) 83.27 72.79 36.49 79.92 88.73 75.08\nASPIRE (α= 0.25) 85.65 78.32 38.06 83.23 89.86 78.35\nASPIRE (α= 0.5) 85.15 78.02 37.22 82.80 88.82 76.12\nASPIRE (α= 0.75) 83.03 74.22 33.37 78.17 87.47 73.13\nASPIRE (α= 1.0) 77.66 66.38 25.49 70.09 85.88 69.89\nAdapted\nOPT-350M\nwithθp\nASPIRE (α= 0.0) 74.50 70.21 18.13 75.86 80.64 73.76\nASPIRE (α= 0.25) 75.55 72.37 19.00 78.54 82.59 77.18\nASPIRE (α= 0.5) 74.95 72.07 18.81 80.48 81.69 75.90\nASPIRE (α= 0.75) 72.55 68.45 16.21 78.76 80.16 73.27\nASPIRE (α= 1.0) 68.02 62.53 12.08 70.31 78.00 70.25\nAdapted\nOPT-1.3B\nwithθp\nASPIRE (α= 0.0) 88.50 73.64 42.46 79.96 91.45 74.47\nASPIRE (α= 0.25) 90.76 79.26 44.03 83.06 93.41 81.17\nASPIRE (α= 0.5) 90.64 79.04 43.70 83.40 93.27 80.91\nASPIRE (α= 0.75) 89.84 76.58 42.05 80.97 92.96 79.90\nASPIRE (α= 1.0) 88.66 73.53 38.65 76.34 92.48 78.57\nAdapted\nOPT-2.7B\nwithθp\nASPIRE (α= 0.0) 90.80 74.23 53.56 81.74 92.86 75.72\nASPIRE (α= 0.25) 92.63 80.25 55.06 84.44 94.73 82.60\nASPIRE (α= 0.5) 92.56 80.18 54.61 84.33 94.59 82.16\nASPIRE (α= 0.75) 92.05 78.37 52.71 81.52 94.28 80.98\nASPIRE (α= 1.0) 91.33 76.08 48.84 76.39 93.77 79.48\nTable 17: Results of studying the effect of α. All numbers are percentages. Bold numbers are superior results.\nModel Method CoQA TriviaQA SQuAD\nAcc↑ AUACC↑ Acc↑ AUACC↑ Acc↑ AUACC↑\nPre-trained OPT-350M Self-Consistency24.54 37.83 3.41 7.93 5.75 15.59\nAdapted OPT-350M withθp\nSelf-Consistency59.03 74.09 7.41 18.40 63.87 79.99\nASPIRE (ours) 59.46 75.55 8.25 19.00 64.74 82.59\nPre-trained OPT-1.3B Self-Consistency50.90 69.14 13.74 30.10 19.89 41.45\nAdapted OPT-1.3B withθp\nSelf-Consistency76.73 88.29 20.34 42.12 80.84 90.68\nASPIRE (ours) 76.85 90.76 21.73 44.03 80.94 93.41\nPre-trained OPT-2.7B Self-Consistency57.60 75.27 19.57 39.88 23.73 47.78\nAdapted OPT-2.7B withθp\nSelf-Consistency80.41 90.61 27.59 52.20 83.11 92.34\nASPIRE (ours) 80.45 92.63 29.21 55.06 83.27 94.73\nTable 18: Comparing with self-consistency. All numbers are percentages. Bold numbers are superior results.\n5213",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.8024435043334961
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.7440271377563477
    },
    {
      "name": "Computer science",
      "score": 0.7368816137313843
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.7342281937599182
    },
    {
      "name": "Task (project management)",
      "score": 0.6213461756706238
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.5533341765403748
    },
    {
      "name": "Machine learning",
      "score": 0.5249326825141907
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47606250643730164
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3510991930961609
    },
    {
      "name": "Psychology",
      "score": 0.11011281609535217
    },
    {
      "name": "Power (physics)",
      "score": 0.10730096697807312
    },
    {
      "name": "Engineering",
      "score": 0.08666852116584778
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 8
}