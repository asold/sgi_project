{
  "title": "Large language models are few-shot clinical information extractors",
  "url": "https://openalex.org/W4385573087",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2610150698",
      "name": "Monica Agrawal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2549366122",
      "name": "Stefan Hegselmann",
      "affiliations": [
        "University of Ulster"
      ]
    },
    {
      "id": "https://openalex.org/A2767538706",
      "name": "Hunter Lang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124530043",
      "name": "Yoon Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2138531792",
      "name": "David N Sontag",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2959716049",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4235216760",
    "https://openalex.org/W2123399010",
    "https://openalex.org/W4287570254",
    "https://openalex.org/W1532254950",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2984974929",
    "https://openalex.org/W2463981085",
    "https://openalex.org/W4225948283",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2565928591",
    "https://openalex.org/W3105291316",
    "https://openalex.org/W3042934285",
    "https://openalex.org/W2107556918",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W4286985375",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4280574137",
    "https://openalex.org/W3097267201",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W3103901889",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W1991491670",
    "https://openalex.org/W3035625205",
    "https://openalex.org/W2250962271",
    "https://openalex.org/W4288373939",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W4385571789",
    "https://openalex.org/W2979250794",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W2773082444",
    "https://openalex.org/W3110566755",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2114039834",
    "https://openalex.org/W2104608657",
    "https://openalex.org/W2138595422",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3106224367",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4221159609",
    "https://openalex.org/W2964222246",
    "https://openalex.org/W2107435951",
    "https://openalex.org/W2095588973",
    "https://openalex.org/W1672757658",
    "https://openalex.org/W1509072985",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W3212714602",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2786103134",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2146668368",
    "https://openalex.org/W4206638982",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W3160909372",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4281765476",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2805211535",
    "https://openalex.org/W2099369363",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3200496214",
    "https://openalex.org/W4286910674",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W2949922292",
    "https://openalex.org/W3197008538",
    "https://openalex.org/W4228998172"
  ],
  "abstract": "A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1998‚Äì2022\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nLarge Language Models are Few-Shot Clinical Information Extractors\nMonica Agrawal\nMIT CSAIL\nmagrawal@mit.edu\nStefan Hegselmann\nUniversity of M√ºnster\nstefan.hegselmann@uni-muenster.de\nHunter Lang\nMIT CSAIL\nhjl@mit.edu\nYoon Kim\nMIT CSAIL\nyoonkim@mit.edu\nDavid Sontag\nMIT CSAIL\ndsontag@mit.edu\nAbstract\nA long-running goal of the clinical NLP com-\nmunity is the extraction of important variables\ntrapped in clinical notes. However, roadblocks\nhave included dataset shift from the general\ndomain and a lack of public clinical corpora\nand annotations. In this work, we show that\nlarge language models, such as InstructGPT\n(Ouyang et al., 2022), perform well at zero- and\nfew-shot information extraction from clinical\ntext despite not being trained specifically for\nthe clinical domain. Whereas text classification\nand generation performance have already been\nstudied extensively in such models, here we ad-\nditionally demonstrate how to leverage them to\ntackle a diverse set of NLP tasks which require\nmore structured outputs, including span identifi-\ncation, token-level sequence classification, and\nrelation extraction. Further, due to the dearth\nof available data to evaluate these systems, we\nintroduce new datasets for benchmarking few-\nshot clinical information extraction based on\na manual re-annotation of the CASI dataset\n(Moon et al., 2014) for new tasks 1. On the\nclinical extraction tasks we studied, the GPT-3\nsystems significantly outperform existing zero-\nand few-shot baselines.\n1 Introduction\nClinical text contains a large amount of valuable\ninformation that is not captured by the struc-\ntured data fields in electronic health records\n(Zweigenbaum et al., 2007; Wang et al., 2018).\nHowever, there are significant challenges to\nclinical information extraction. Because clinical\ntext contains irregularities such as ambiguous\njargon and nonstandard phrasal structure, most\noff-the-shelf NLP tools perform poorly, and\nclinical text annotation requires domain expertise\n(Zheng et al., 2011). Further, due to the sensitive\nnature of clinical text, public corpora are rare\nand restrictively licensed. As a result, clinical\n1https://huggingface.co/datasets/mitclinicalml/clinical-ie\nInput: The patient takes coumadin 5 mg [‚Ä¶].Prompt: Create a list of medications.She takes 5 mg of Coumadin and Aspirin\nThe patient takes coumadin 5 mg daily for a TIA and she has an aspirin allergy. Zero-shot prompt:\nComplex post-processing (resolver) of LM output[coumadin,aspirin]\nOne-shot example + guidance:\nMinimal post-processing (resolver) of LM output[coumadin, aspirin]\nInput:\nInput: He is on a statin now and we think [‚Ä¶].Prompt: Create a list of medications.-\"statin\"Input: The patient takes coumadin 5 mg [‚Ä¶].Prompt: Create a list of medications.-\"coumadin\"-\"aspirin\"\nFigure 1: Illustration of our approach using a one-shot\nexample (green) and guidance (brown) to create a more\nstructured LM output (blue). This significantly reduces\nthe necessary post-processing effort of a resolver (gray).\nNLP datasets tend to be small and splintered\nacross institutions (Xia and Yetisgen-Yildiz, 2012).\nTo overcome these issues, practitioners often\nincorporate task-specific domain knowledge and\nregular expressions, even in modern deep learning\npipelines, but these solutions can be brittle (Luo\net al., 2020; Skreta et al., 2021; Chapman et al.,\n2001; Irvin et al., 2019; Johnson et al., 2019;\nChauhan et al., 2020). Modern systems that do not\nuse some combination of these elements are gen-\nerally limited to areas where labels are generated\nas a byproduct of normal clinical practice, such\nas ICD code prediction (Zhang et al., 2020) and\nmortality prediction (Si and Roberts, 2019).\nIn this work, we benchmark how large language\nmodels (LLMs) such as GPT-3 (Brown et al., 2020;\nOuyang et al., 2022) perform at clinical NLP tasks.\nThis takes the form of three contributions:\n‚Ä¢ We introduce three new annotated datasets for\nbenchmarking few-shot clinical information ex-\ntraction methods, as many shared clinical corpora\n1998\nTask Description Example Text Answer Data\nClinical sense\ndisambiguation\nGiven a note and an\nabbreviation, expand\nthe abbreviation\n(classification)\n[...] was sent to IR\nfor thrombolysis.\nPost IR, ultrasound\nshowed that [...]\nInterventional\nradiology\n41 acronyms from 18,164\nnotes from CASI (Moon et al.,\n2014) and 8912 notes from\nMIMIC (Adams et al., 2020)\nBiomedical\nevidence\nextraction\nGiven an abstract, list\ninterventions\n(multi-span identifica-\ntion/generation)\n[...] paliperidone\nextended- release\ntablets and [...] with\nrisperidone [...]\n-paliperidone\nextended-release\ntablets\n-risperidone\n187 abstracts (token-level) and\n20 newly annotated abstracts\n(arm identification) from\nEBM-NLP (Nye et al., 2018)\nCoreference\nresolution\nGiven a note and a\npronoun, identify the\nantecedent (span\nidentification)\n[...] Did develop\nsome tremors,\nhowever. These were\nwell managed [...]\nsome tremors 105 newly annotated\nexamples from CASI (Moon\net al., 2014) with one\npronoun-antecedent pair each\nMedication\nstatus\nextraction\nGiven a note, extract\nmedications and their\nstatus, e.g. active\n(NER +\nclassification)\n[...] have\nrecommended\nCitrucel [...]\ndiscontinue the\nColace. [...]\n-Citrucel: active\n-Colace:\ndiscontinued\n105 newly annotated\nexamples from CASI (Moon\net al., 2014) with 340\nmedication-status pairs\nMedication\nattribute\nextraction\nGiven a note, extract\nmedications and 5\nattributes, e.g.\ndosage, reason (NER\n+ relation extraction)\n[...] she was taking\n325 mg of aspirin per\nday for three years\nfor a TIA. [...]\naspirin: {dose:\n325 mg, freq: per\nday, duration:\nthree years,\nreason: TIA}\n105 newly annotated\nexamples from CASI (Moon\net al., 2014) with 313\nmedications and 533 attributes\nTable 1: Overview of the five tasks studied in this work and the datasets that were used.\n(Murphy et al., 2010; Henry et al., 2020; Johnson\net al., 2016) have data use agreements that pre-\nvent their use with LLM APIs such as OpenAI‚Äôs.\nThe datasets were generated by re-annotating the\ndataset from Moon et al. (2014) for new tasks.\n‚Ä¢ We show that GPT-3 performs well in clinical\nNLP over a set of diverse tasks (see Table 1), de-\nspite not being trained specifically for the domain.\nBy replacing the complex hand-curated domain\nknowledge with the natural-language output of an\nLLM, the engineering effort required to solve a\nparticular extraction task can be greatly reduced.\n‚Ä¢ While LLMs have been primarily evaluated at clas-\nsification and generation tasks, our tasks involve a\ngreater variety of expected output structures, such\nas relation extraction (see last three rows of Table\n1). We therefore introduce guided prompt design\nto steer the LLM towards an easy-to-structure out-\nput and resolvers to map from the LLM outputs\nto the structured label space; see Figure 1.\n2 Related Work\n2.1 Prompt-Based Learning\nIn prompt-based learning (also known as in-context\nlearning), a pretrained language model is adapted\nto different tasks via priming on natural language\nprompts‚Äîpieces of text that are combined with\nan input and then fed to the language model to\nproduce an output for that task.\nThis paradigm has been successful for few-shot\nand zero-shot learning at many general-domain\ntasks (Brown et al., 2020; Liu et al., 2021; Wei\net al., 2021; Sanh et al., 2021). More recently, large\nlanguage models such as T0 and InstructGPT have\nre-configured their training objectives to explic-\nitly encourage the model to perform well at such\nprompts (Sanh et al., 2021; Ouyang et al., 2022).\nWhile prompt-based learning can be extended\nstraightforwardly to classification tasks (e.g., multi-\nple choice), more complex tasks require creativity\nin their implementation (Mishra et al., 2021). For\nexample, coreference resolution is often re-framed\nas classification, asking which of two antecedents\na pronoun refers to (Sanh et al., 2021) or whether a\ncandidate antecedent is correct (Yang et al., 2022).\nThis approach requires a list of antecedent candi-\ndates, which requires an additional component (e.g.\na noun phrase generator) or many‚Äîpotentially\nexpensive‚Äîqueries. Span classification and named\nentity recognition have been similarly reframed.\nFor example, given a candidate entity X and full\nmodel access, the entity type can be predicted via\nan argmax over the possible types Y of the proba-\nbility of statements like ‚ÄúX is a Y entity‚Äù (Cui et al.,\n2021). Alternatively, if only a single entity is be-\ning queried for a given input, prompting can be as\nsimple as ‚ÄúWhat is the location‚Äù(Liu et al., 2022a);\nhowever, clinical NLP often concerns itself with\nextraction of multiple concepts. To extract multi-\nple spans simultaneously, Li et al. (2019b) and Li\net al. (2019a) use techniques from machine reading\n1999\ncomprehension, relying on access to the underlying\nmodel and labeled data for training the extraction\nlayer. While InstructGPT (Ouyang et al., 2022) has\n‚àº2% or ‚â§1ùëò extraction examples in its training,\nthe LLM output is never converted to a structured\nform, and extraction examples are only evaluated\nqualitatively for improvement over other models.\nThat is, only results for classification and genera-\ntion tasks are quantified.\n2.2 Pretrained LMs for Clinical NLP\nClinical text differs significantly from text typically\nutilized in general NLP, both in syntax and vocabu-\nlary (Wu et al., 2020). As a result, the clinical NLP\nsubcommunity often trains domain-specific models\non clinical corpora following advances in language\nmodeling from the broader NLP community. For\nexample, clinical neural word embeddings were\ntrained following word2vec (Mikolov et al., 2013;\nWu et al., 2015; Roberts, 2016). More recently, fol-\nlowing BERT, many clinical and biomedical vari-\nations swiftly followed including ClinicalBERT,\nSciBERT, BioBERT, and PubMedBERT (Devlin\net al., 2018; Alsentzer et al., 2019; Ammar et al.,\n2018; Lee et al., 2020; Gu et al., 2021). However,\nin several applications, researchers observed the\nperformance gains to be marginal to none over clas-\nsical methods such as logistic regression (Chen\net al., 2020; Krishna et al., 2021). Additionally,\nprevious work has so far been unable to achieve\ncompetitive results on biomedical NLP tasks using\ndomain-agnostic LLMs like GPT-3 (Moradi et al.,\n2021; Guti√©rrez et al., 2022).\n3 Methods\n3.1 Predicting Structured Outputs with LLMs\nIn this work, we assume only query access to a\nlarge language model (i.e., no gradients, no log\nprobabilities).\nSuppose we have a set of ùëõ examples\n({ùë•ùëñ, ùëéùëñ})ùëõ\nùëñ=1, where ùë•ùëñ is the input text as a string,\nùëéùëñ is (optional) side information as a string (e.g.,\nwhich acronym to disambiguate). The outputs\nùë¶ùëñ ‚ààO are unobserved (i.e., to be predicted). The\noutput space O is defined per task. For example,\nfor a binary sequence labeling task, if we let |ùë•ùëñ |be\nthe number of tokens in ùë•ùëñ, O is {0, 1}|ùë•ùëñ |.\nPrompt-based learning requires the specification\nof a prompt template to be applied on the input. In\nthis work, we handcraft our prompt templates using\na set of 5 validation examples per task. Letùëù ùëó (ùë•, ùëé)\nbe the result of filling prompt template ùëó with in-\nputs ùë• and ùëé, and further let ùëì (ùëù ùëó (ùë•, ùëé)) ‚ààŒ£‚òÖ\nbe the string output by an LLM on input ùëù ùëó (ùë•, ùëé).\nThe next step involves mapping the LLM genera-\ntion from Œ£‚òÖ to the structured label space O. For\nexample, in classification, the verbalizer defines a\nmapping between the LLM output space Œ£‚òÖ and\nthe discrete set of labels O = {1, . . . , ùêø}using a\ndictionary of token/label pairs (Schick and Sch√ºtze,\n2021). However, for our structured tasks of inter-\nest, the label space O is more complex, and more\ncomplicated functions are needed to map to an ele-\nment of O. We define the resolver ùëÖ as a function\nùëÖ(ùë•, ùëé, ùëì(ùëù1 (ùë•, ùëé)))that maps the combined input\nand LLM output to the task-specific output spaceO.\nFor example, suppose the output spaceO is a list of\nstrings. Then the resolver needs to turn each output\nùëì (ùëù ùëó (ùë•, ùëé))into a list (perhaps by choosing spans\nof text from inside of ùëì (ùëù ùëó (ùë•, ùëé))). For example,\nfor medication extraction we might have:\nùë• =‚Äúswitched Advil for Tylenol‚Äù, ùëé=‚ÄúN/A‚Äù,\nùëù1 (ùë•, ùëé)=‚ÄúNote: switched Advil for Tylenol. ‚Äù\nTask: List medications. ‚Äù\nùëì (ùëù1 (ùë•, ùëé))=‚ÄúTylenol and Advil‚Äù\nùëÖ(ùë•, ùëé, ùëì(ùëù1 (ùë•, ùëé)))=[‚ÄúTylenol‚Äù,‚ÄúAdvil‚Äù]\nWe refer to the output from the resolver as Re-\nsolved GPT-3, or GPT-3 + R, for short. Through-\nout, when comparing resolvers, we place in paren-\ntheses the lines of code (LOC) in the resolver, as\na proxy for complexity (defined as human effort,\nnot runtime). The required complexity of the re-\nsolver depends largely on the cleanliness of the\nprompt output, and by extension the prompt itself.\nWe introduce guided prompt design to simplify\nthe resolver required for complex output. As seen\nin Figure 1, this consists of (i) a one-shot exam-\nple with an output in the desired structured format\n(which could be incorrect content-wise), and (ii)\nguiding the model to use the same format. Specific\nconstructions are found in Sections 6 and 7.\n3.2 Dataset Annotation\nIn the short-term, research on clinical extraction\nvia prompting may rely on sending data to external\nAPIs. Since data use agreements on many existing\nannotated clinical datasets prohibit such activity,\nthere is a dearth of benchmarks for the community\nto build on. The de-identified Clinical Acronym\nSense Inventory (CASI) dataset is therefore a valu-\nable resource, as it is ‚Äúpublicly available to support\nthe research of the greater NLP and biomedical and\n2000\nhealth informatics community‚Äù (Moon et al., 2014).\nCASI contains snippets of clinical notes across spe-\ncialties in four University of Minnesota-affiliated\nhospitals. While CASI was originally annotated for\nacronym disambiguation, we created three new an-\nnotated datasets from existing snippets of the CASI\ndataset. Annotation was performed by two of the\nauthors who have background in both clinical NLP\nand medicine. For each task, a set of examples\nwas jointly annotated to establish an annotation\nschema, each annotator then independently labeled\nthe same set of 105 examples using PRAnCER soft-\nware (Levy et al., 2021), and the two sets were then\nmerged via joint manual adjudication.\nIn the following sections, we show how to build\nsimple resolvers for five clinical NLP tasks. We\nfind that resolvers for guided prompts are much eas-\nier to write than resolvers for un-guided prompts.\nThe implicit structured imposed by the prompt\nguidance means that resolvers for a guided prompt\ncan be less than 10 LOC. On the tasks below, we\nfind that GPT-3 + R matches or exceeds strong few-\nshot, zero-shot, and even supervised baselines.\n4 Clinical Sense Disambiguation\nOverview. Clinical notes are rife with overloaded\njargon and abbreviations. Ptcan mean patient, pro-\nthrombin time, physical therapy, or posterior tibial\n(Weeber et al., 2001; Shilo and Shilo, 2018). This\nambiguity impacts the utility of notes for patients,\nclinicians, and algorithms (Kuhn, 2007; Mowery\net al., 2016). In this section, we first evaluate clini-\ncal sense disambiguation on the CASI dataset di-\nrectly and then transfer a model distilled via weak\nsupervision to another dataset.\nDataset 1. The Clinical Acronym Sense Inventory\ndataset consists of 500 text examples for each of 75\nacronyms (Moon et al., 2014). Due to noise in the\ndataset (e.g. duplications), it is common to filter to\na subset of the dataset; we follow the filtering from\nAdams et al. (2020), leading to a subset of 18,164\nexamples and 41 acronyms for evaluation. Similar\nto other works, we treat the task as multiple-choice.\nDataset 2. We additionally use a reverse substitu-\ntion dataset (Adams et al., 2020) generated over the\nMIMIC-III Critical Care Database (Johnson et al.,\n2016). In reverse substitution, labeled data is gen-\nerated from unlabeled text by replacing expansions\n(e.g. physical therapy) with their acronyms (PT)\nand using the original expansion as the label. We\nevaluate on their 8912 test examples over the same\n41 acronyms as the CASI subset. Since we cannot\nquery GPT-3 on this dataset, we distill and transfer\na model trained on the outputs from Dataset 1.\nPrompting + Resolver.We usedGPT-3 edit(using\nengine text-davinci-edit-001) with greedy decoding\n(temperature =0). For each example, we provided\nthe full clinical snippet and appended the single\ninstruction Expand the abbreviation:{abbr}.\nSince we did not provide the LLM with the answer\nchoices, the form of the output string could still\ndiffer slightly from all the candidate answers (e.g.\nediting ‚ÄúRA‚Äù to ‚Äúright atria‚Äù when ‚Äúright atrium‚Äù\nwas expected). In the resolver, we choose the an-\nswer choice with the highest contiguous character\noverlap with the LLM generated output.\nModel Distillation via Weak Supervision. Direct\ndeployment of large language models can be\ndifficult due to model size and data privacy. To\nremedy these issues, we follow several recent\nworks (Lang et al., 2022a; Smith et al., 2022;\nWang et al., 2021) and show that we can instead\nview the LLM + resolver system as a labeler rather\nthan as a classifier, and that this can even boost\nperformance. In particular, we use outputs of\nthis system on CASI as weak supervision (e.g.,\nRatner et al., 2017) to train a smaller, task-specific\nmodel. Here we fine-tune PubMedBERT (Gu et al.,\n2021) and follow Lang et al. (2022a); details and\nhyperparameters are found in the appendix.\nBaselines. We compare the performance of our ap-\nproach to other zero-shot language modeling meth-\nods: (i) Latent Meaning Cells (LMC), a deep latent\nvariable model from Adams et al. (2020) which\nis pre-trained on millions of notes from MIMIC,\n(ii) ELMo pre-trained on the same dataset (Peters\net al., 2018), and (iii) Clinical BioBERT (Alsentzer\net al., 2019). Numbers for these three baselines are\ntaken from Adams et al. (2020); for all three, they\nchoose the answer choice with the most similar rep-\nresentation to the contextual representation of the\nacronym. We also show performance for random\nguessing and choosing the most common answer\nchoice per acronym (since the expansions of many\nacronyms follow a long-tailed distribution).\nEvaluation. Accuracy and macro F1 are calculated\nper acronym and averaged over all acronyms (see\nleft of Table 2). On CASI, GPT-3 edit + R alone al-\nready clearly outperforms the LMC model on both\nmetrics, and the addition of weak supervision with\nPubMedBERT further boosts this performance. On\nthe MIMIC Reverse Substitution dataset, despite\n2001\nAlgorithm CASI Acc. CASI Macro F1 MIMIC Accuracy MIMIC Macro F1\nRandom 0.31 0.23 0.32 0.28\nMost Common 0.79 0.28 0.51 0.23\nBERT (from Adams et al. (2020)) 0.42 0.23 0.40 0.33\nELMo (from Adams et al. (2020)) 0.55 0.38 0.58 0.53\nLMC (from Adams et al. (2020)) 0.71 0.51 0.74 0.69\nGPT-3 edit+ R: 0-shot 0.86 0.69 * *\nGPT-3 edit+ R: 0-shot + distillation 0.90 0.76 0.78 0.69\nTable 2: Clinical sense disambiguation. Accuracy and macro F1 for zero-shot language modeling approaches on\na subset of the Clinical Acronym Sense Inventory (CASI) data set (Moon et al., 2014) and the MIMIC Reverse\nsubstitution dataset (Adams et al., 2020). GPT-3 is not run on MIMIC due to the data use agreement. To evaluate on\nMIMIC we distill GPT-3 + R into a smaller model by treating the outputs as weak supervision and following Lang\net al. (2022b) ‚Äú+ distillation‚Äù, then evaluate the smaller model on MIMIC as well.\nbeing transferred to a new domain, our weakly-\nsupervised PubMedBERT model performs simi-\nlarly to LMC (Adams et al., 2020), which was\npre-trained specifically on the MIMIC distribution.\nThis indicates we can use GPT-3 edit + R to label a\npublic dataset, distill its labels into a smaller task-\nspecific model, and then transfer that model to a\nprivate dataset to obtain competitive performance.\nSince the CASI dataset is publicly accessible, one\npossible caveat is that the dataset could have been\nin the language model‚Äôs training data; to investigate\nfurther (see Section C.1), we prompt the LLM on\nacronyms not in the original annotations.\n5 Biomedical Evidence Extraction\nTask Overview. Evidence-based medicine (EBM)\ninvolves synthesizing findings from across clinical\nresearch studies, but the current rapid clip of re-\nsearch makes it nearly impossible to keep up with\nall studies (Sackett, 1997; Bastian et al., 2010).\nTherefore, automated approaches for parsing clini-\ncal abstracts could aid the adoption of EBM (Ver-\nbeke et al., 2012; Nye et al., 2018). Here, we focus\non extracting interventions and controls (which we\nwill refer to just as Intervention), where the un-\nderlying goal is to identify the distinct arms of a\nclinical trial (Nye et al., 2018). Token-level classi-\nfication is often used as a proxy for this goal, but\ndistilling identified spans into distinct interventions\nis non-trivial and often requires significant domain\nknowledge. Prior work on arm identification has\nattempted to use coreference resolution (Ferracane\net al., 2016) and to identify of pairs of spans with\nredundant information (Nye et al., 2018).\nDataset. We assess intervention identification from\nthe angles of (i) the token classification proxy task\nand (ii) the underlying task of arm identification.\nFor (i), we use the token-level annotations provided\nin version 2 of the dataset from Nye et al. (2018)\nand evaluate on the 187 test abstracts provided. The\naverage Cohen‚Äôs ùúÖ was only 0.59 on this set. For\n(ii), the two annotators from Section 3.2 manually\nderived a list of the intervention-control arms for\n20 abstracts in the test set, with perfect agreement.\nPrompting + Resolvers. We use a single prompt\nwith InstructGPT (engine text-davinci-002) and\ngreedy decoding. The resolver for the token-\nlabeling task removes noisy tokens (stop words)\nfrom the LLM output, maps remaining tokens in\nthe output to the original input and labels those as\n1, and merges fractured spans. The full process can\nbe found in Appendix C.2. For the arm identifica-\ntion task, resolving simply involved splitting the\noutput string on new lines.\nComparison. We compare to supervised ap-\nproaches that train on the 4800 labeled training ex-\namples from Nye et al. (2018). PubMedBERT with\nan additional classification layer (LSTM or CRF)\nachieves close to state-of-the-art performance on\nthe full task (Gu et al., 2021). Since prior works\nreport numbers combined over multiple classes, we\nre-run training on only the Intervention label using\nPubMedBERT-CRF. We also include the best su-\npervised baseline from Nye et al. (2018), an LSTM-\nCRF over word and character-level embeddings.\nToken-level results (Proxy Task).We first evalu-\nate sequence labeling precision at the token-level\n(F1 in Table 3). Resolved GPT-3 performs re-\nspectably compared to supervised deep baselines,\nbut underperforms on these token-level metrics.\nMany error modes occur due to particularities of\nthe schema, e.g. including extra details (like dosing\nschedule or route of administration) and only in-\n2002\nAlgorithm\nToken-level\nF1\nAbstract-level\nAccuracy\nPubMedBERT-CRF (sup) 0.69 0.35\nLSTM-CRF (sup) 0.65 *\nGPT-3 + R: 0-shot 0.61 0.85\nTable 3: Biomedical Evidence Extraction . Test F1\nscores on the binary token-level sequence labeling prob-\nlem for Intervention identification (Nye et al., 2018),\nand abstract-level accuracy at arm identification. The\nsupervised baselines were trained on 4,800 abstracts.\ncluding an acronym or its expansion, but not both.\nA clarifying example can be found in Section C.2.\nArm Identification Results. To measure arm iden-\ntification accuracy, we evaluated whether the num-\nber of arms was accurate and manually checked\nwhether the main differentiator of each interven-\ntion arm was captured, similar to Ferracane et al.\n(2016). For the PubMedBERT baseline, in order\nto distill the identified spans to a list of arms, we\nassume (i) oracle splitting of spans into arms (given\na span describing multiple arms, we can correctly\nsplit the span) and (ii) near-oracle coreference res-\nolution (given multiple spans describing the same\narm, we can correctly merge). Resolved GPT-3 suc-\ncessfully identified the correct number and content\nof the arms in 17 of the 20 examples. The three\nexamples it missed were also missed by PubMed-\nBERT. Assuming oracle splitting and coreference\n(a nontrivial task), PubMedBERT would still have\nissues with 10 further examples. Details of the\nevaluation and error modes are in Section C.2.\n6 Coreference Resolution\nTask Overview. Coreference resolution involves\ngrouping noun phrases that refer to the same under-\nlying entity (e.g. a person, a medical concept), and\nit is considered particularly important for clinically\naccurate information retrieval and summarization\n(Zheng et al., 2011). For example, when surfac-\ning past medical history, it is critical to correctly\nparse pronouns to understand whether the history\ndescribes the patient or a family member.\nDataset Description. In clinical NLP, coreference\nresolution has been largely evaluated on the 2011\ni2b2/V A challenge, which consists of thousands\nof coreference chains (Uzuner et al., 2012). Due\nto i2b2‚Äôs data use agreement, the two annotators\nannotated a new dataset using CASI snippets, with\n5 coreference pairs for prompt design and 100 pairs\nfor evaluation (Moon et al., 2014). We prioritized\nAlgorithm Recall Precision\nToshniwal et al. (2020, 2021) 0.73 0 .60\nGPT-3 + R (50 LOC): 0-shot 0.78 0.58\nGPT-3 + R (1 LOC): 1-shot (incorrect)0.76.02 0.78.04\nGPT-3 + R (1 LOC): 1-shot (correct)0.75.04 0.77.04\nTable 4: Coreference Resolution. Macro unigram re-\ncall and unigram precision of methods on our newly\nannotated task using CASI (Moon et al., 2014). The\nend-to-end baseline was trained on three non-clinical\ncoreference resolution datasets and transferred to this\nnew setting. 1-shot results are averaged over 5 prompts.\ndifficult examples by focusing on pronoun corefer-\nence, where the input is a pronoun, the output its\nantecedent, and no tokens overlap between the two.\nMore details are in Section B.2.\nPrompting and Resolvers. We used the 5 exam-\nples for prompt design with InstructGPT (engine\ntext-davinci-002) and greedy decoding (tempera-\nture =0). We use a guided 1-shot prompt, where\nwe provide an example input and begin a formatted\nresponse: ‚Äú{pronoun} refers to‚Äù. For 1-shot, we\nexperiment with both correct (the true noun phrase)\nand incorrect answers (a random incorrect noun\nphrase preceding the pronoun) in the example in-\nput to tease apart the effect of the example answer\nversus the example formatting. To clarify that ef-\nfect, we average over results from 5 different 1-shot\nexamples. We also compare to an unguided zero-\nshot prompt, which simply appends ‚ÄúWhat does\n{pronoun} ... refer to?\" to the input. The zero-shot\nresolver involves mapping tokens back to the input\ndue to potential paraphrases; the one-shot resolver\ninvolves only the removal of a single quotation\nmark, making the guided prompt easier to resolve.\nSection A.3 contains more detail on the prompts.\nComparison. We compare to deep end-to-end\ncoreference resolution, as it has been shown to\nperform well (Lee et al., 2017). In particular, we\ncompare to the longdoc model from (Toshniwal\net al., 2020), which trained on multiple coreference\ndatasets in order to generalize to new settings.\nResults. We evaluated via macro unigram recall\n(% of label‚Äôs unigrams in the resolved output) and\nunigram precision (% of unigrams in the resolved\noutput in the label) (Table 4). We tokenized using\nStanza (Qi et al., 2020) for these metrics. While\nthe longdoc baseline trained on thousands of non-\nclinical coreference examples performed consider-\nably well already, it is outperformed by Resolved\nGPT-3. We found the 1-shot example mostly con-\n2003\nAlgorithm Recall Precision\nScispaCy (Neumann et al., 2019)0.73 0 .67\nGPT-3 + R (32 LOC) (0-Shot) 0.87 0 .83\nGPT-3 + R (8 LOC) (1-Shot) 0.90.01 0.92.01\nTable 5: Medication extraction. Micro recall and pre-\ncision for medication extraction on our self-annotated\ndataset.\nAlgorithm Conditional\nAccuracy\nConditional\nMacro F1\nT-Few (20-shot) 0.86 0 .57\nGPT-3 + R (32 LOC) (0-Shot)0.85 0 .69\nGPT-3 + R (8 LOC) (1-shot)0.89.01 0.62.04\nGPT-3 + R (8 LOC) (1-shot)\n+ added classes 0.88.02 0.71.03\nGPT-3 + R (8 LOC) (1-shot)\nwith shuffled classes 0.88.01 0.66.03\nTable 6: Medication status classification. Conditional\naccuracy and macro F1-score for Identification of medi-\ncation status active, discontinued, and neither.\nstrains the LLM output to quoting (rather than para-\nphrasing); without guidance, the LLM may output\ne.g., ‚ÄúThe antecedent is unclear.\" Further, the ac-\ncuracy of the 1-shot example was irrelevant to the\nperformance, an observation previously reported in\nthe classification setting, now seen for span extrac-\ntion (Min et al., 2022).\n7 Medication Extraction\nThe recognition of clinical concept mentions (prob-\nlems, treatments, etc.), their modifiers (e.g., nega-\ntion), and relations (e.g., dosage) is a fundamental\nbuilding block in clinical NLP (Jiang et al., 2011).\nHere we examine the extraction of medication con-\ncepts with two different schemas.\n7.1 Recognition + Status Classification\nHere we extract a list of medications and label\neach with a status modifier: active, discontinued,\nor neither (e.g. allergy or proposed medication).\nDataset description. We created new annotations\nfor medication and status on top of CASI Moon\net al. (2014). The examples were enriched for\nchangeover in treatment. For 105 randomly se-\nlected snippets, the annotators extracted all medi-\ncations mentioned and classified its status with one\nof the 3 labels. Further details are in Appendix B.3.\nUnlike in Section 7.2, all mentions corresponding\nto the same medication are collapsed.\nPrompting and Resolver. We again used 5 ex-\namples for prompt design with InstructGPT (en-\ngine text-davinci-002) and greedy decoding. Our\nprompt asked the model to simultaneously output\nthe list of medications and the status of each. We\nevaluate the prompt in an unguided zero-shot man-\nner and in a guided one-shot manner. Further, to\nclarify the effect of the 1-shot example on modi-\nfier accuracy, we examine how status classification\nperformance changes if we (i) artificially augment\nthe 1-shot example so all three status classes are\nobserved, and (ii) whether the statuses need to be\ncorrect, or just present. We averaged over 5 dif-\nferent 1-shot inputs to clarify these effects; each\n1-shot example contained between 3 and 8 medica-\ntions. We describe the resolvers for the zero- and\none-shot cases in detail in Section C.4; the former\ninvolved several regular expressions, and the latter\nrequired only a few short lines.\nComparison. We used a rule-based method as a\nmedication extraction baseline, since historically\nthey perform well (Sohn et al., 2014). To this end,\nwe leveraged the Python library ScispaCy with the\nen_core_sci_sm package for entity recognition\n(Neumann et al., 2019, details in Appendix C.4).2\nFor medication status classification, we compare to\nT-Few(Liu et al., 2022b), a few shot LLM method\nfine-tuned on a set of additional snippets we labeled\nfrom the same distribution (20 snippets containing\n60 medication statuses). This method predicts the\nstatus, given the token indices for each medication.\nResults. Table 5 shows micro recall and precision\nfor medication extraction; we count a prediction\nas correct if the predicted string exactly matches\none. Overall, Resolved GPT-3 outperforms the\nScispaCy linkage baseline consistently by a consid-\nerable margin. The addition of the 1-shot example\ngreatly improves precision, since in the 0-shot case,\nsome GPT-3 outputs included extraneous extrac-\ntions (e.g. a procedure). Typical failure modes of\nthe baseline include incorrect recognition of over-\nloaded abbreviations and missing vendor-specific\ndrug names. Table 6 showsconditional accuracy on\nmedication status classification. For an apples-to-\napples comparison, we conditioned on the subset of\nmedications found by all GPT-3 methods (241/340)\nand evaluated T-few on that subset as well. We find\nthat if the rarer Neither class wasn‚Äôt demonstrated\n2We do not use a supervised baseline trained on the i2b2\n2009 challenge data (as in Section 7.2) because their schema\npurposefully excluded medications in the Neither category.\n2004\nSubtask Algorithm Medication Dosage Route Frequency Reason Duration\nToken-level PubMedBERT + CRF (Sup.) 0.82 0.92 0.77 0.76 0.35 0.57\nGPT-3 + R: 1-shot 0.85 0.92 0.87 0.91 0.38 0.52\nPhrase-level PubMedBERT + CRF (Sup.) 0.73 0.78 0.71 0.41 0.22 0.30\nGPT-3 + R: 1-shot 0.75 0.82 0.81 0.87 0.21 0.25\nRelation Extraction\nPubMedBERT + CRF +\nShi and Lin (2019) (Sup.) * 0.67 0.65 0.36 0.19 0.21\nGPT-3 + R: 1-shot * 0.80 0.63 0.60 0.34 0.16\nTable 7: Medication attribute extraction. F1 scores on our newly annotated medication extraction dataset. The\nbaselines are trained using supervised learning on i2b2 (Uzuner et al., 2010), then transferred to the test domain.\nRelation Extraction additionally requires the model to match modifiers (dosage, route, etc.) to the medication span.\nBaseline end-to-end relation extraction performance suffers due to errors cascading from the extraction step.\nin the 1-shot example, it was unlikely to be out-\nput, depressing the F1 score; including all classes\nin the 1-shot prompt appears more important than\nnecessarily having the correct labels.\n7.2 Recognition + Relation Extraction\nDataset description. The annotators created a sec-\nond new dataset for medication extraction from\nthe snippets from Moon et al. (2014). The anno-\ntators closely followed the schema from the 2009\ni2b2 medication challenge (Uzuner et al., 2010),\nwith small deviations explained in Appendix B.4.\nFor 105 randomly selected snippets, the annotators\nlabeled mentions of medications, dosages, routes,\nfrequencies, reasons, and durations, if available,\nand their correspondences. We examine the task\nfrom three different framings: a token-level anno-\ntation task, a phrase-level annotation task, and end-\nto-end relation extraction. For the example phrase\n‚ÄúTylenol twice daily‚Äù, the desired output for the\ntasks would be: [Med, Frequency, Frequency], [B-\nMed, B-Frequency, I-Frequency], and Medication:\n\"Tylenol\", Frequency: \"twice daily\"., respectively.\nPrompting and Resolver. We again used 5 exam-\nples for prompt design with InstructGPT (engine\ntext-davinci-002) and greedy decoding (tempera-\nture =0). We use a different guided 1-shot prompt\n(containing 7 entities each) for each of the three\nframings outlined above; these can be found in\nAppendix A. The resolvers for all were short.\nComparison. For token and phrase-level classifica-\ntion, we used a PubMedBERT model topped with\na CRF layer. For end-to-end relation extraction,\nwe first used the token-level baseline to extract en-\ntity spans, then used the technique from Shi and\nLin (2019) to classify whether each pair of entities\nwas related. We then postprocessed these pairwise\noutputs to match modifiers to their medications.\nFor all the three tasks, since we followed the 2009\ni2b2 medication extraction annotation guidelines,\nwe fine-tuned the baselines with labeled data from\ni2b2 (10 fully annotated notes with 154 medication\nmentions, which we postprocess into smaller anno-\ntated chunks) and directly evaluated them on our\ndatasets. (Uzuner et al., 2010). Appendix C.5 con-\ntains more detail for the baselines and evaluation.\nResults. Table 7 shows that the 1-shot GPT-3+R\noutperforms the i2b2-supervised baseline across\nall task framings. The baseline end-to-end relation\nextraction performance suffers due to cascading\nextraction errors, as the longest token in the med-\nication name had to be matched. GPT-3+R strug-\ngles with the duration and reason entities; however,\nit has been previously found that there is often\nlarge disagreement (F1 estimated 0.2‚Äì0.5) in inter-\nannotator agreement for these two entities, since\nthey tend to be longer with ambiguous boundaries.\n8 Conclusion\nIn this work, we introduced new annotated datasets\nto show that (i) large language models have great\npromise at diverse clinical extraction tasks and (ii)\nwe can guide generations to map to complex output\nspaces with only light post-processing. We also\ndemonstrated how weak supervision over the sys-\ntem‚Äôs outputs can be used to train smaller, task-\nspecific models that are more deployable. The\nscope of clinical NLP extends past what we studied\nhere, and important next steps involve experiment-\ning with LLMs such as OPT (Zhang et al., 2022)\nfor which we can run inference locally, enabling\nevaluation on existing benchmarks and fine-tuning.\nAnother important direction involves leveraging the\noutputs from several prompts (e.g. 1-shot prompts\nwith different examples) to learn to determine when\nGPT-3 is uncertain; this increased reliability will be\n2005\nvital given the high-stakes in clinical information\nextraction. Taken as a whole, our work indicates a\nnew paradigm for clinical information extraction‚Äî\none that can scale to the lofty goals of clinical NLP.\nLimitations\nWhile large language models show great promise at\nclinical information extraction, there are clear limi-\ntations to their use. First, it is still difficult to guide\na LLM to match an exact schema‚Äîclinical annota-\ntion guidelines are often multiple pages. We found\nthat even when the Resolved GPT-3 outputs were\nimpressive qualitatively, they did not always match\nat the token-level. For example, in tagging dura-\ntions, one Resolved GPT-3 output was ‚ÄúX weeks‚Äù\ninstead of ‚Äúfor X weeks‚Äù. While this particular\nomission is trivial, it highlights the difficulty of\ncommunicating nuanced guidelines.\nSecond, we found a bias in GPT-3 towards out-\nputting a non-trivial answer even where none exists.\nFor example, for medication extraction the prompt\nwe ended up using was, ‚ÄúCreate a bulleted list\nof which medications are mentioned and whether\nthey are active, discontinued, or neither.‚Äù How-\never, prior to this we had experimented with two\nseparate prompts: ‚ÄúCreate a bulleted list of active\nmedications, if any.‚Äù and ‚ÄúCreate a bulleted list of\ndiscontinued medications, if any.‚Äù If there was one\nactive and one discontinued medication, the respec-\ntive LLM outputs would be correct. However, if\nthere were two active medications and none discon-\ntinued, the LLM primed with the discontinuation\nprompt tended to try to find an output and usually\nresorted to listing one or more active medications.\nTherefore, it is important to craft prompts or tasks\nthat avoid this pitfall. For example, this could be\nachieved via (i) chaining multiple prompts, e.g.,\nfirst asking if a certain entity type exists in the in-\nput, before asking for a list (Li et al., 2019b; Wu\net al., 2022) or (ii) using an output structure like\nthe sequence tagging approach.\nFinally, because of the data use restrictions on\nmost existing clinical datasets, which prohibit pub-\nlicly sharing the data (e.g., to the GPT-3 APIs),\nall tasks except for biomedical evidence extrac-\ntion were derived from the publicly-available CASI\ndataset (Moon et al., 2014). While we show the\npromise of transferring to a new setting in Section\n4, it would be ideal to have been able to directly\nevaluate on multiple hospital systems at multiple\npoints throughout time. Clinical text in CASI was\ndrawn from notes from several hospitals and a di-\nverse set of specialties, but is by no means repre-\nsentative of all clinical text. For example, the CASI\npaper states that the notes were ‚Äúprimarily verbally\ndictated and transcribed,‚Äù but this practice is not\nuniversal. Further, as is unfortunately common in\nclinical NLP, we only tested in English, leaving\ntesting the ability of LLMs to operate in different\nlanguages to future work (N√©v√©ol et al., 2018).\nEthics Statement\nThe datasets introduced in this paper involved only\nnew annotations on top of existing, publicly avail-\nable clinical text. Dataset annotation was con-\nducted by two authors of the paper, and therefore\nthere are no associated concerns, e.g. regarding\ncompensation. As discussed in limitations, we be-\nlieve these new annotated datasets serve as a start-\ning point for the evaluation of LLMs on clinical\ntext, but we concede that conclusions about specific\nperformance cannot be ported to other languages,\nhospital systems, or temporal settings (as clinical\ntext is quite subject to dataset shift).\nIf large language models were to be integrated\ninto clinical extraction pipelines, as presented in\nthis paper, there are large potential benefits. Clin-\nical text is being created at a scale far too large\nfor manual annotation, and as a result, cohorts for\nclinical study are largely small and hand-curated.\nAutomatic structuring of clinical variables would\nhelp catalyze research that may be prohibitively\nexpensive otherwise ‚Äì allowing for study of rarer\nor less funded diseases as well as the analysis of\nreal-world evidence for subpopulations that may\nnot be observed in clinical trials. However, due\nto the high-stakes setting, it is imperative that the\nperformance of such a system is evaluated in the\nsame environment it will be used in, and that the\nperformance numbers are stratified by cohorts of\nnote (e.g. racial, socioeconomic, patient comorbidi-\nties, disease stage, site of care, author‚Äôs clinical role\nand seniority); such variables were not available in\nthe data we used here.\nIn this work, we accessed the GPT-3 model using\nthe OpenAI API alone. However, we acknowledge\nthat even the inference cost is still nontrivial (see\nAppendix D). We presented in Section 4 a paradigm\nof using weak supervision to distill a much smaller\nmodel, using pseudolabels learned from GPT-3,\nand we encourage such work to mitigate the envi-\nronmental impact of deployment.\n2006\nAcknowledgements\nMA was supported by a Takeda Fellowship, the\nMIT Deshpande Center, and the MLA@CSAIL\nInitiatives which includes companies Arrow, EY ,\nCisco, Capital One, Ahold Delhaize, SAP, and\nBT. HL was supported by NSF AiTF award CCF-\n1723344, and SH by the German Academic Ex-\nchange Service. MA and DS were also partially\nsupported by Memorial Sloan Kettering Cancer\nCenter. Thanks to NVIDIA Corporation for their\ndonation of two NVIDIA A100 GPUs, and to Ope-\nnAI for providing quota to access their models.\nFinally, thanks to Rebecca Boiarsky for feedback\non drafts of this paper.\nReferences\nGriffin Adams, Mert Ketenci, Shreyas Bhave, Adler J.\nPerotte, and No√©mie Elhadad. 2020. Zero-shot\nclinical acronym expansion via latent meaning\ncells. In Machine Learning for Health Workshop,\nML4H@NeurIPS 2020, Virtual Event, 11 December\n2020, volume 136 of Proceedings of Machine Learn-\ning Research, pages 12‚Äì40. PMLR.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, et al. 2018. Construction of the lit-\nerature graph in semantic scholar. arXiv preprint\narXiv:1805.02262.\nHilda Bastian, Paul Glasziou, and Iain Chalmers. 2010.\nSeventy-five trials and eleven systematic reviews a\nday: how will we ever keep up? PLoS medicine,\n7(9):e1000326.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267‚Äì\nD270.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nWendy W Chapman, Will Bridewell, Paul Hanbury, Gre-\ngory F Cooper, and Bruce G Buchanan. 2001. A\nsimple algorithm for identifying negated findings and\ndiseases in discharge summaries. Journal of biomed-\nical informatics, 34(5):301‚Äì310.\nGeeticka Chauhan, Ruizhi Liao, William Wells, Jacob\nAndreas, Xin Wang, Seth Berkowitz, Steven Horng,\nPeter Szolovits, and Polina Golland. 2020. Joint mod-\neling of chest radiographs and radiology reports for\npulmonary edema assessment. In International Con-\nference on Medical Image Computing and Computer-\nAssisted Intervention, pages 529‚Äì539. Springer.\nIrene Y Chen, Emily Alsentzer, Hyesun Park, Richard\nThomas, Babina Gosangi, Rahul Gujrathi, and Bharti\nKhurana. 2020. Intimate partner violence and injury\nprediction from radiology reports. In BIOCOMPUT-\nING 2021: Proceedings of the Pacific Symposium ,\npages 55‚Äì66. World Scientific.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.\n2021. Template-based named entity recognition us-\ning bart. arXiv preprint arXiv:2106.01760.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nElisa Ferracane, Iain Marshall, Byron C Wallace, and\nKatrin Erk. 2016. Leveraging coreference to identify\narms in medical abstracts: An experimental study.\nIn Proceedings of the Seventh International Work-\nshop on Health Text Mining and Information Analy-\nsis, pages 86‚Äì95.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1‚Äì23.\nBernal Jim√©nez Guti√©rrez, Nikolas McNeal, Clay Wash-\nington, You Chen, Lang Li, Huan Sun, and Yu Su.\n2022. Thinking about gpt-3 in-context learning\nfor biomedical ie? think again. arXiv preprint\narXiv:2203.08410.\nSam Henry, Kevin Buchan, Michele Filannino, Amber\nStubbs, and Ozlem Uzuner. 2020. 2018 n2c2 shared\ntask on adverse drug events and medication extraction\nin electronic health records. Journal of the American\nMedical Informatics Association, 27(1):3‚Äì12.\nOr Honovich, Uri Shaham, Samuel R Bowman, and\nOmer Levy. 2022. Instruction induction: From few\nexamples to natural language task descriptions. arXiv\npreprint arXiv:2205.10782.\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\nBehzad Haghgoo, Robyn Ball, Katie Shpanskaya,\net al. 2019. Chexpert: A large chest radiograph\ndataset with uncertainty labels and expert comparison.\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume 33, pages 590‚Äì597.\nMin Jiang, Yukun Chen, Mei Liu, S Trent Rosenbloom,\nSubramani Mani, Joshua C Denny, and Hua Xu. 2011.\n2007\nA study of machine-learning-based approaches to ex-\ntract clinical entities and their assertions from dis-\ncharge summaries. Journal of the American Medical\nInformatics Association, 18(5):601‚Äì606.\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-\nying Deng, Roger G Mark, and Steven Horng.\n2019. Mimic-cxr, a de-identified publicly available\ndatabase of chest radiographs with free-text reports.\nScientific data, 6(1):1‚Äì8.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1‚Äì9.\nKundan Krishna, Amy Pavel, Benjamin Schloss, Jef-\nfrey P Bigham, and Zachary C Lipton. 2021. Ex-\ntracting structured data from physician-patient con-\nversations by predicting noteworthy utterances. In\nExplainable AI in Healthcare and Medicine, pages\n155‚Äì169. Springer.\nIvy Fenton Kuhn. 2007. Abbreviations and acronyms\nin healthcare: when shorter isn‚Äôt sweeter. Pediatric\nnursing, 33(5).\nHunter Lang, Monica Agrawal, Yoon Kim, and David\nSontag. 2022a. Co-training improves prompt-based\nlearning for large language models. arXiv preprint\narXiv:2202.00828.\nHunter Lang, Aravindan Vijayaraghavan, and David\nSontag. 2022b. Training subset selection for weak\nsupervision. arXiv preprint arXiv:2206.02914.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234‚Äì1240.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference resolu-\ntion. arXiv preprint arXiv:1707.07045.\nAriel Levy, Monica Agrawal, Arvind Satyanarayan, and\nDavid Sontag. 2021. Assessing the impact of au-\ntomated suggestions on decision making: Domain\nexperts mediate model errors but take less initiative.\nIn Proceedings of the 2021 CHI Conference on Hu-\nman Factors in Computing Systems, pages 1‚Äì13.\nXiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong\nHan, Fei Wu, and Jiwei Li. 2019a. A unified mrc\nframework for named entity recognition. arXiv\npreprint arXiv:1910.11476.\nXiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna\nYuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019b.\nEntity-relation extraction as multi-turn question an-\nswering. arXiv preprint arXiv:1905.05529.\nAndy T Liu, Wei Xiao, Henghui Zhu, Dejiao Zhang,\nShang-Wen Li, and Andrew Arnold. 2022a. Qaner:\nPrompting question answering models for few-\nshot named entity recognition. arXiv preprint\narXiv:2203.01543.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay\nMohta, Tenghao Huang, Mohit Bansal, and Colin\nRaffel. 2022b. Few-shot parameter-efficient fine-\ntuning is better and cheaper than in-context learning.\narXiv preprint arXiv:2205.05638.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nYen-Fu Luo, Sam Henry, Yanshan Wang, Feichen Shen,\nOzlem Uzuner, and Anna Rumshisky. 2020. The\n2019 n2c2/umass lowell shared task on clinical con-\ncept normalization. Journal of the American Medical\nInformatics Association, 27(10):1529‚Äìe1.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in neural information processing systems,\n26.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2021. Reframing\ninstructional prompts to gptk‚Äôs language. arXiv\npreprint arXiv:2109.07830.\nSungrim Moon, Serguei Pakhomov, Nathan Liu,\nJames O Ryan, and Genevieve B Melton. 2014.\nA sense inventory for clinical abbreviations and\nacronyms created using clinical notes and medical\ndictionary resources. Journal of the American Medi-\ncal Informatics Association, 21(2):299‚Äì307.\nMilad Moradi, Kathrin Blagec, Florian Haberl, and\nMatthias Samwald. 2021. Gpt-3 models are poor\nfew-shot learners in the biomedical domain. arXiv\npreprint arXiv:2109.02555.\nDanielle L Mowery, Brett R South, Lee Chris-\ntensen, Jianwei Leng, Laura-Maria Peltonen, Sanna\nSalanter√§, Hanna Suominen, David Martinez, Sum-\nithra Velupillai, No√©mie Elhadad, et al. 2016. Nor-\nmalizing acronyms and abbreviations to aid patient\nunderstanding of clinical texts: Share/clef ehealth\nchallenge 2013, task 2. Journal of biomedical seman-\ntics, 7(1):1‚Äì13.\n2008\nShawn N Murphy, Griffin Weber, Michael Mendis, Vi-\nvian Gainer, Henry C Chueh, Susanne Churchill, and\nIsaac Kohane. 2010. Serving the enterprise and be-\nyond with informatics for integrating biology and the\nbedside (i2b2). Journal of the American Medical\nInformatics Association, 17(2):124‚Äì130.\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\nAmmar. 2019. ScispaCy: Fast and Robust Models\nfor Biomedical Natural Language Processing. In Pro-\nceedings of the 18th BioNLP Workshop and Shared\nTask, pages 319‚Äì327, Florence, Italy. Association for\nComputational Linguistics.\nAur√©lie N√©v√©ol, Hercules Dalianis, Sumithra Velupillai,\nGuergana Savova, and Pierre Zweigenbaum. 2018.\nClinical natural language processing in languages\nother than english: opportunities and challenges.\nJournal of biomedical semantics, 9(1):1‚Äì13.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang,\nIain J Marshall, Ani Nenkova, and Byron C Wal-\nlace. 2018. A corpus with multi-level annotations\nof patients, interventions and outcomes to support\nlanguage processing for medical literature. In Pro-\nceedings of the conference. Association for Computa-\ntional Linguistics. Meeting, volume 2018, page 197.\nNIH Public Access.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227‚Äì2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D Manning. 2020. Stanza: A python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 101‚Äì108.\nAlexander Ratner, Stephen H Bach, Henry Ehrenberg,\nJason Fries, Sen Wu, and Christopher R√©. 2017.\nSnorkel: Rapid training data creation with weak su-\npervision. In Proceedings of the VLDB Endowment.\nInternational Conference on Very Large Data Bases,\nvolume 11, page 269. NIH Public Access.\nKirk Roberts. 2016. Assessing the corpus size vs. simi-\nlarity trade-off for word embeddings in clinical nlp.\nIn Proceedings of the Clinical Natural Language Pro-\ncessing Workshop (ClinicalNLP), pages 54‚Äì63.\nDavid L Sackett. 1997. Evidence-based medicine. In\nSeminars in perinatology, volume 21, pages 3‚Äì5. El-\nsevier.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nTimo Schick and Hinrich Sch√ºtze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255‚Äì269.\nPeng Shi and Jimmy Lin. 2019. Simple bert models for\nrelation extraction and semantic role labeling. arXiv\npreprint arXiv:1904.05255.\nLotan Shilo and Gila Shilo. 2018. Analysis of abbre-\nviations used by residents in admission notes and\ndischarge summaries. QJM: An International Jour-\nnal of Medicine, 111(3):179‚Äì183.\nYuqi Si and Kirk Roberts. 2019. Deep patient repre-\nsentation of clinical notes via multi-task learning\nfor mortality prediction. AMIA Summits on Transla-\ntional Science Proceedings, 2019:779.\nMarta Skreta, Aryan Arbabi, Jixuan Wang, Erik\nDrysdale, Jacob Kelly, Devin Singh, and Michael\nBrudno. 2021. Automatically disambiguating med-\nical acronyms with ontology-aware deep learning.\nNature communications, 12(1):1‚Äì10.\nRyan Smith, Jason A Fries, Braden Hancock, and\nStephen H Bach. 2022. Language models in the\nloop: Incorporating prompting into weak supervision.\narXiv preprint arXiv:2205.02318.\nSunghwan Sohn, Cheryl Clark, Scott R Halgrim, Sean P\nMurphy, Christopher G Chute, and Hongfang Liu.\n2014. Medxn: an open source medication extrac-\ntion and normalization tool for clinical text. Journal\nof the American Medical Informatics Association ,\n21(5):858‚Äì865.\nShubham Toshniwal, Sam Wiseman, Allyson Ettinger,\nKaren Livescu, and Kevin Gimpel. 2020. Learn-\ning to ignore: Long document coreference with\nbounded memory neural networks. arXiv preprint\narXiv:2010.02807.\nShubham Toshniwal, Patrick Xia, Sam Wiseman, Karen\nLivescu, and Kevin Gimpel. 2021. On general-\nization in coreference resolution. arXiv preprint\narXiv:2109.09667.\nOzlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler\nForbush, John Pestian, and Brett R South. 2012. Eval-\nuating the state of the art in coreference resolution for\nelectronic medical records. Journal of the American\nMedical Informatics Association, 19(5):786‚Äì791.\n2009\n√ñzlem Uzuner, Imre Solti, and Eithon Cadag. 2010. Ex-\ntracting medication information from clinical text.\nJournal of the American Medical Informatics Associ-\nation, 17(5):514‚Äì518.\nMathias Verbeke, Vincent Van Asch, Roser Morante,\nPaolo Frasconi, Walter Daelemans, and Luc\nDe Raedt. 2012. A statistical relational learning ap-\nproach to identifying evidence based medicine cate-\ngories. In Proceedings of the 2012 Joint Conference\non Empirical Methods in Natural Language Process-\ning and Computational Natural Language Learning,\npages 579‚Äì589.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce label-\ning cost? gpt-3 can help. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 4195‚Äì4205.\nYanshan Wang, Liwei Wang, Majid Rastegar-Mojarad,\nSungrim Moon, Feichen Shen, Naveed Afzal, Sijia\nLiu, Yuqun Zeng, Saeed Mehrabi, Sunghwan Sohn,\net al. 2018. Clinical information extraction appli-\ncations: a literature review. Journal of biomedical\ninformatics, 77:34‚Äì49.\nMarc Weeber, James G Mork, and Alan R Aronson.\n2001. Developing a test collection for biomedical\nword sense disambiguation. In Proceedings of the\nAMIA Symposium, page 746. American Medical In-\nformatics Association.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Fine-\ntuned language models are zero-shot learners.\narXiv:2109.01652.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\net al. 2019. Huggingface‚Äôs transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nStephen Wu, Kirk Roberts, Surabhi Datta, Jingcheng\nDu, Zongcheng Ji, Yuqi Si, Sarvesh Soni, Qiong\nWang, Qiang Wei, Yang Xiang, et al. 2020. Deep\nlearning in clinical natural language processing: a\nmethodical review. Journal of the American Medical\nInformatics Association, 27(3):457‚Äì470.\nTongshuang Wu, Michael Terry, and Carrie Jun Cai.\n2022. Ai chains: Transparent and controllable\nhuman-ai interaction by chaining large language\nmodel prompts. In CHI Conference on Human Fac-\ntors in Computing Systems, pages 1‚Äì22.\nYonghui Wu, Jun Xu, Yaoyun Zhang, and Hua Xu. 2015.\nClinical abbreviation disambiguation using neural\nword embeddings. In Proceedings of BioNLP 15 ,\npages 171‚Äì176.\nFei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical cor-\npus annotation: challenges and strategies. In Pro-\nceedings of the Third Workshop on Building and\nEvaluating Resources for Biomedical Text Mining\n(BioTxtM‚Äô2012) in conjunction with the International\nConference on Language Resources and Evaluation\n(LREC), Istanbul, Turkey.\nXiaohan Yang, Eduardo Peynetti, Vasco Meerman, and\nChris Tanner. 2022. What gpt knows about who is\nwho. arXiv preprint arXiv:2205.07407.\nJieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yam-\ning Yang, Mao Yang, and Alexander Ratner. 2021.\nWrench: A comprehensive benchmark for weak su-\npervision. In Thirty-fifth Conference on Neural In-\nformation Processing Systems Datasets and Bench-\nmarks Track (Round 2).\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nZachariah Zhang, Jingshu Liu, and Narges Raza-\nvian. 2020. Bert-xml: Large scale automated\nicd coding using bert pretraining. arXiv preprint\narXiv:2006.03685.\nJiaping Zheng, Wendy W Chapman, Rebecca S Crowley,\nand Guergana K Savova. 2011. Coreference resolu-\ntion: A review of general methodologies and applica-\ntions in the clinical domain. Journal of biomedical\ninformatics, 44(6):1113‚Äì1122.\nPierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,\nand Kevin B Cohen. 2007. Frontiers of biomedical\ntext mining: current progress. Briefings in bioinfor-\nmatics, 8(5):358‚Äì375.\n2010\nA Prompts and Sample GPT-3 Outputs\nWe present examples for each task alongside\ntheir corresponding prompts to illustrate different\nprompting strategies used for each task.\nA.1 Clinical Sense Disambiguation\nFor clinical sense disambiguation we used GPT-3\nedit mode which edits the input text rather than\ncontinuing it.\nExample 1\nGPT-3 edit input:\nGENERAL: Patient is sedated on vent. HEENT:\nNormocephalic, atraumatic. Pupils are sluggish\nbut reactive and equal bilaterally. NECK: Supple.\nNo lymphadenopathy. No JVD. CARDIAC: Regular rate\nand rhythm. No murmurs. LUNGS: CTA, intubated.\nABDOMEN: Obese, nontender, positive bowel sounds.\nEXTREMITIES: Positive pulses, positive edema.\nSKIN: Chronic changes pretibial area bilaterally.\nGPT-3 edit prompt:\nExpand the abbreviation: CTA\nGPT-3 edit output:\nGENERAL: Patient is sedated on vent. HEENT:\nNormocephalic, atraumatic. Pupils are sluggish\nbut reactive and equal bilaterally. NECK: Supple.\nNo lymphadenopathy. No JVD. CARDIAC: Regular rate\nand rhythm. No murmurs. LUNGS: Clear to\nauscultation, intubated. ABDOMEN: Obese,\nnontender, positive bowel sounds. EXTREMITIES:\nPositive pulses, positive edema. SKIN: Chronic\nchanges pretibial area bilaterally.\nExample 2\nGPT-3 edit input:\n2. Severe hypertension. 3. Severe mitral\nregurgitation. 4. Osteoporosis. PROCEDURES: 1.\nCoronary angiography and hemodynamic evaluation\nwith right heart catheterization. Right heart\ncatheterization shows right atrial pressure of 1,\nright ventricular pressure of 47/11, PA pressure\nof 48/16, with a pulmonary capillary wedge\npressure of 29, with a large B-wave confirmed\nwith a wedge saturation of 95\nGPT-3 edit prompt:\nExpand the abbreviation: PA\nGPT-3 edit output:\n2. Severe hypertension. 3. Severe mitral\nregurgitation. 4. Osteoporosis. PROCEDURES: 1.\nCoronary angiography and hemodynamic evaluation\nwith right heart catheterization. Right heart\ncatheterization shows right atrial pressure of 1,\nright ventricular pressure of 47/11, pulmonary\nartery pressure of 48/16, with a pulmonary\ncapillary wedge pressure of 29, with a large\nB-wave confirmed with a wedge saturation of 95%.\n2011\nA.2 Biomedical Evidence Extraction\nFor biomedical evidence extraction we used GPT-3\nDavinci. The generated output is shown in blue.\nExample 1\nGPT-3 Davinci:\nAssessment of acute bronchodilator effects from\nspecific airway resistance changes in stable COPD\npatients.\nBACKGROUND In COPD patients, reversibility is\ncurrently evaluated from the changes of forced\nexpiratory volume at 1s (ŒîFEV1) and forced vital\ncapacity (ŒîFVC). By lowering peripheral airway\nsmooth muscle tone, bronchodilators should\ndecrease dynamic hyperinflation, gas trapping,\nand possibly dyspnea at rest. Hence, we\nhypothesize that specific airway resistance\nchanges (ŒîsRAW) should better characterize the\nacute response to bronchodilators.\nMETHODS On two days, 60 COPD patients underwent\ndyspnea evaluation (VAS score) and pulmonary\nfunction testing at baseline and one hour after\nplacebo or 300ùúág indacaterol administration.\nRESULTS Spirographic and ŒîsRAW-based criteria\nidentified as responders 24 and 45 patients,\nrespectively. ŒîsRAW correlated with changes of\nintrathoracic gas volume (ŒîITGV) (r=0.61;\np<0.001), residual volume (ŒîRV) (r=0.60;\np<0.001), ŒîFVC (r=0.44; p=0.001), and ŒîVAS\n(r=0.73; p<0.001), while ŒîFEV1 correlated only\nwith ŒîFVC (r=0.34; p=0.008). Significant\ndifferences in terms of ŒîITGV (p=0.002), ŒîRV\n(p=0.023), and ŒîVAS (p<0.001) occurred only if\npatients were stratified according to ŒîsRAW.\n[...]\nCreate a bulleted list of the arms in this trial.\n- Placebo\n- Indacaterol (300 ùúág)\nExample 2\nGPT-3 Davinci:\nSerum biochemical characteristics of Beluga, Huso\nhuso (L.), in response to blood sampling after\nclove powder solution exposure.\nIn order to investigate the effect of anesthesia\non serum parameters, Beluga, Huso huso (L.) were\nblood-sampled immediately without anesthesia\n(control) or subjected to following anesthesia\nprocedure: 40, 120, and 240 s exposure to 3,000,\n700, and 500 mg l‚àí1 clove solution, respectively.\nBlood samples were collected after these periods,\nwhen fish were immobile and reached stage 4\nanesthesia. Results showed that cortisol and\nglucose levels were significantly high in 700 and\n500 but not 3,000 mg l‚àí1 group compared to\ncontrol. Serum lactate levels were significantly\nhigh in 500 mg l‚àí1 group compared to control\ngroup. Lactate levels were not significantly\ndiffered between control, 3,000, and 700 mg l‚àí1\ngroups. There were no significant differences in\nserum levels of cholesterol, total protein,\nlactate dehydrogenase, aspartate\naminotransferase, alanine aminotransferase, Na+,\nCl‚àí, K+, and Ca2+. Results suggest that rapid\nanesthesia with higher dose is better than slow\nanesthesia with lower dose for blood sampling in\nBeluga.\nCreate a bulleted list of the arms in this trial.\n- Control\n- 3,000 mg l‚àí1 clove solution\n- 700 mg l‚àí1 clove solution\n- 500 mg l‚àí1 clove solution\n2012\nA.3 Coreference Resolution\nExample 1: Zero-shot unguided\nGPT-3 Davinci:\n[...] Her current regimen for her MS is Rebif\nMonday, Wednesday, and Friday and 1 gram of\nmethylprednisolone p.o. every month. This had\nbeen working previously; however, she feels that\nher symptoms return before her next dose of\nmethylprednisolone is due.\nWhat does \"This\" in the sentence \"This had been\nworking previously; however, she feels that her\nsymptoms return before her next dose of\nmethylprednisolone is due.\" refer to?\nThe sentence is unclear, but it seems to be\nreferring to the Rebif regimen.\nExample 1: One-shot guided (incorrect)\n[...] ASSESSMENT PLAN: The patient has no\nphysical evidence of progression of disease. It\nis somewhat worrisome that her CEA is up, but\nwill be keep watching that to see if it is just\nwithin a normal swing for her. [...]\nWhat does \"that\" in the sentence \"It is somewhat\nworrisome that her CEA is up, but will be keep\nwatching that to see if it is just within a\nnormal swing for her.\" refer to?\n\"that\" refers to \"progression of disease\"\n[...] Her current regimen for her MS is Rebif\nMonday, Wednesday, and Friday and 1 gram of\nmethylprednisolone p.o. every month. This had\nbeen working previously; however, she feels that\nher symptoms return before her next dose of\nmethylprednisolone is due.\nWhat does \"This\" in the sentence \"This had been\nworking previously; however, she feels that her\nsymptoms return before her next dose of\nmethylprednisolone is due.\" refer to?\n\"This\" refers to \"her current regimen for her MS\"\nExample 1: One-shot guided (correct)\n[...] ASSESSMENT PLAN: The patient has no\nphysical evidence of progression of disease. It\nis somewhat worrisome that her CEA is up, but\nwill be keep watching that to see if it is just\nwithin a normal swing for her. [...]\nWhat does \"that\" in the sentence \"It is somewhat\nworrisome that her CEA is up, but will be keep\nwatching that to see if it is just within a\nnormal swing for her.\" refer to?\n\"that\" refers to \"her CEA\"\n[...] Her current regimen for her MS is Rebif\nMonday, Wednesday, and Friday and 1 gram of\nmethylprednisolone p.o. every month. This had\nbeen working previously; however, she feels that\nher symptoms return before her next dose of\nmethylprednisolone is due.\nWhat does \"This\" in the sentence \"This had been\nworking previously; however, she feels that her\nsymptoms return before her next dose of\nmethylprednisolone is due.\" refer to?\n\"This\" refers to \"her current regimen for her MS\"\n2013\nA.4 Medication Status Extraction\nExample 1: Zero-shot guided\n[...] home dose of Kadian as this is her\nlong-acting medication and DC the continuous\nDilaudid given IV. 5. Urinary tract infection\nwith Klebsiella and E. coli, both sensitive to\nLevaquin. Since this was diagnosed Foley has been\nDC‚Äôd. For now would continue Levaquin and recheck\nurinalysis.\nCreate a bulleted list of which medications are\nmentioned and whether they are active,\ndiscontinued, or neither.\n-\"Kadian\" (active)\n-\"Dilaudid\" (discontinued)\n-\"Levaquin\" (active)\nExample 1: One-shot guided (incorrect)\n[...] start her on Ativan 1 mg p.o. q. 8 hours\nand use Ativan 1 mg IV q. 4 hours p.r.n. for\nagitation. I will also start her on Inderal LA 60\nmg p.o. q.d. for essential tremors. She does not\nwant to take Celexa, and I will put her back on\nLexapro 2 mg p.o. q.d. I will discontinue\nEsmolol.\nCreate a bulleted list of which medications [...]\n-\"Ativan\" (discontinued)\n-\"Inderal LA\" (active)\n-\"Celexa\" (neither)\n-\"Lexapro\" (active)\n-\"Esmolol\" (active)\n[...] home dose of Kadian as this is her\nlong-acting medication and DC the continuous\nDilaudid given IV. 5. Urinary tract infection\nwith Klebsiella and E. coli, both sensitive to\nLevaquin. Since this was diagnosed Foley has been\nDC‚Äôd. For now would continue Levaquin and recheck\nurinalysis.\nCreate a bulleted list of which medications [...]\n-\"Kadian\" (active)\n-\"Dilaudid\" (discontinued)\n-\"Levaquin\" (active)\nExample 1: One-shot guided (correct)\n[...] start her on Ativan 1 mg p.o. q. 8 hours\nand use Ativan 1 mg IV q. 4 hours p.r.n. for\nagitation. I will also start her on Inderal LA 60\nmg p.o. q.d. for essential tremors. She does not\nwant to take Celexa, and I will put her back on\nLexapro 2 mg p.o. q.d.\nCreate a bulleted list of which medications [...]\n-\"Ativan\" (active)\n-\"Inderal LA\" (active)\n-\"Lexapro\" (active)\n-\"Celexa\" (neither)\n[...] home dose of Kadian as this is her\nlong-acting medication and DC the continuous\nDilaudid given IV. 5. Urinary tract infection\nwith Klebsiella and E. coli, both sensitive to\nLevaquin. Since this was diagnosed Foley has been\nDC‚Äôd. For now would continue Levaquin and recheck\nurinalysis.\nCreate a bulleted list of which medications [...]\n-\"Kadian\" (discontinued)\n-\"Dilaudid\" (discontinued)\n-\"Levaquin\" (discontinued)\n2014\nA.5 Medication Attribute Extraction\nExample 1: Token-level\n<One-shot-example>\nLabel the tokens: medication, dosage, route,\nfrequency, duration, reason, or none. Ignore\nallergies.\n<One-shot-labels>\nInput: 8. Albuterol 2 puffs every 4-6 hours as\nneeded. HOSPITAL COURSE: This is an 80-year-old\nfemale who was hospitalized about 2 months ago\nfor chronic obstructive pulmonary disease\nexacerbation. At that time she was put on\nprednisone and antibiotics and seemed to get\nbetter. However, she was put on Augmentin ES and\ncontinued to have difficulty tasting food and\nfelt that food tasted very salty. She had no\nappetite and she has continued to lose weight\nover the last 2 months.\nLabel the tokens: medication, dosage, route,\nfrequency, duration, reason, or none. Ignore\nallergies.\n-\"8\": none\n-\".\": none\n-\"Albuterol\": medication\n-\"2\": dosage\n-\"puffs\": dosage\n-\"every\": frequency\n-\"4-6\": frequency\n-\"hours\": frequency\n-\"as\": none\n-\"needed\": none\n-\".\": none\n-\"HOSPITAL\": none\n-\"COURSE\": none\n-\"This\": none\n-\"is\": none\n-\"an\": none\n-\"80-year-old\": none\n-\"female\": none\n-\"who\": none\n-\"was\": none\n-\"hospitalized\": none\n-\"about\": duration\n-\"2\": duration\n-\"months\": duration\n-\"ago\": duration\n-\"for\": reason\n-\"chronic\": reason\n-\"obstructive\": reason\n-\"pulmonary\": reason\n-\"disease\": reason\n-\"exacerbation\": reason\n-\".\": none\n-\"At\": none\n-\"that\": none\n-\"time\": none\n-\"she\": none\n-\"was\": none\n-\"put\": none\n-\"on\": none\n-\"prednisone\": medication\n[...]\nExample 1: Phrase-level\n<One-shot-example>\nLabel the tokens: medication, dosage, route,\nfrequency, duration, reason, or none. Ignore\nallergies.\n<One-shot-labels>\nInput: 8. Albuterol 2 puffs every 4-6 hours as\nneeded. HOSPITAL COURSE: This is an 80-year-old\nfemale who was hospitalized about 2 months ago\nfor chronic obstructive pulmonary disease\nexacerbation. At that time she was put on\nprednisone and antibiotics and seemed to get\nbetter. However, she was put on Augmentin ES and\ncontinued to have difficulty tasting food and\nfelt that food tasted very salty. She had no\nappetite and she has continued to lose weight\nover the last 2 months.\nLabel the tokens: medication, dosage, route,\nfrequency, duration, reason, or none. Chunk the\nsame entity together. Ignore allergies.\n-\"8\": none\n-\".\": none\n-\"Albuterol\": medication\n-\"2 puffs\": dosage\n-\"every 4-6 hours\": frequency\n-\"as needed\": duration\n-\".\": none\n-\"HOSPITAL COURSE\": none\n-\"This\": none\n-\"is\": none\n-\"an\": none\n-\"80-year-old\": none\n-\"female\": none\n-\"who\": none\n-\"was\": none\n-\"hospitalized\": none\n-\"about\": none\n-\"2 months\": duration\n-\"ago\": none\n-\"for\": none\n-\"chronic\": none\n-\"obstructive\": none\n-\"pulmonary\": none\n-\"disease\": reason\n-\"exacerbation\": none\n-\".\": none\n-\"At\": none\n-\"that\": none\n-\"time\": none\n-\"she\": none\n-\"was\": none\n-\"put\": none\n-\"on\": none\n-\"prednisone\": medication\n[...]\n2015\nExample 1: Relation Extraction\n<One-shot-example>\nLabel the tokens: medication, dosage, route,\nfrequency, duration, reason, or none. Ignore\nallergies.\n<One-shot-labels>\nInput: 8. Albuterol 2 puffs every 4-6 hours as\nneeded. HOSPITAL COURSE: This is an 80-year-old\nfemale who was hospitalized about 2 months ago\nfor chronic obstructive pulmonary disease\nexacerbation. At that time she was put on\nprednisone and antibiotics and seemed to get\nbetter. However, she was put on Augmentin ES and\ncontinued to have difficulty tasting food and\nfelt that food tasted very salty. She had no\nappetite and she has continued to lose weight\nover the last 2 months.\nLabel medications, ignoring allergies. Include\ndosage, route, frequency, duration, reason, if\navailable.\n-\"medication: \"Albuterol\", dosage: \"2 puffs\",\nfrequency: \"every 4-6 hours\", duration: \"as\nneeded\"\n-medication: \"prednisone\", duration: \"2 months\"\n-medication: \"antibiotics\", duration: \"2 months\"\n-medication: \"Augmentin ES\", duration: \"2 months\"\n2016\nB Annotation Process\nA screenshot of the annotation process can be seen\nin Figure 2.\nB.1 Biomedical Evidence Extraction\nFor arm identification 20 out of 187 abstracts from\nthe test set of Nye et al. (2018) were randomly se-\nlected. Both annotators separately identified the\nstudy arms in each abstract. They included all\ncharacteristics of a arm that were necessary for dif-\nferentiation (Ferracane et al., 2016). For example,\nwe would not require the route of administration\nfor a drug (e.g. ‚Äúoral\" in oral X) unless another arm\ncontained the the same drug in a different formal\n(e.g. X nasal spray ). There was full consensus\nbetween annotators for the identified numbers of\narms. A single abstract was replaced due to its\nambiguity.\nB.2 Coreference Resolution\nAnnotators labeled 105 snippets from the CASI\ndataset with pronouns and their corresponding\nnoun phrase antecedent (Moon et al., 2014). The\nantecedent was annotated as the entire noun phrase\n(barring any dependent clauses); in cases where\ntwo antecedents were available, both were labeled.\nFor the purposes of evaluation, we chose the an-\ntecedent with the highest overlap to each model‚Äôs\noutput. To ensure nontrivial examples, the anno-\ntators excluded all examples of personal pronouns\n(e.g. ‚Äúhe‚Äù, ‚Äúshe‚Äù) if another person (and possible\nantecedent) had not yet been mentioned in the snip-\npet.\nB.3 Medication Status Extraction\nWe wanted to create a dataset of challenging exam-\nples containing a changeover in treatment. From\na sample, only ‚àº5% of CASI snippets contained\nsuch examples. To increase the density of these ex-\namples, speeding up annotation, clinical notes were\nfiltered with the following search terms: discont,\nadverse, side effect, switch, and dosage, leading\nto 1445 snippets. We excluded snippets that were\npurely medication lists, requiring at least some nar-\nrative part to be present. For 105 randomly selected\nsnippets, the annotators first extracted all medica-\ntions. Guidelines excluded medication categories\n(e.g. ‚ÄúACE-inhibitor‚Äù) if they referred to more\nspecific drug names mentioned elsewhere (even\nif partially cut off in the snippet). For instance,\nonly the antibiotic Levaquin was labeled in: ‚ÄúIt is\nprobably reasonable to treat with antibiotics [...].\nI would agree with Levaquin alone [...]‚Äù. Guide-\nlines also excluded electrolytes and intravenous\nfluids as well as route and dosage information. In\na second step, medication were assigned to one of\nthree categories: active, discontinued, and neither.\nDiscontinued medications also contain medications\nthat are temporarily on hold. The category neither\nwas assigned to all remaining medications (e.g. al-\nlergies, potential medications).\nB.4 Medication Attribute Extraction\nFor medication attribute extraction, we also labeled\n105 examples from CASI (Moon et al., 2014). An-\nnotation guideline were adopted from the 2009\ni2b2 medication extraction challenge (Uzuner et al.,\n2010) with slight modifications. We allowed medi-\ncation attributes to have multiple spans. Also, we\ngrouped together different names of the same drug\n(e.g. ‚ÄúTylenol‚Äù and ‚ÄúTylenol PM‚Äù) for the purpose\nof relation extraction. After annotation of the data,\nwe create three versions of the dataset: token-level,\nphrase-level, and relation-level. For the first, we\nsplit all word in the example and assigned them\ntheir respective label or none if they were not part\nof a label (see token-level example in A.5. For\nphrase-level, we kept consecutive words with the\nsame label grouped together as phrases (see phrase-\nlevel example in A.5. The relation level just con-\ntained the extracted medication and their attributes\n(see relation extraction example in A.5. We note\nthat medication lists were downsampled in the cre-\nation of the dataset, since the 2009 i2b2 challenge\nhad found performance on narrative text was far\nlower than on medication lists.\n2017\nFigure 2: Platform used for annotation of the three new labeled datasets, shown for the coreference resolution\nannotation task.\n2018\nC Additional Experimental details\nAcross all datasets, similar to Honovich et al.\n(2022), we assume the latest generation of mod-\nels on the OpenAI API is the set of InstructGPT\nmodels from Ouyang et al. (2022).\nC.1 Clinical Sense Disambiguation\nHow do we know CASI is not in the LLM train-\ning set? Since the CASI dataset is publicly accessi-\nble from the Internet and on Github, one potential\npitfall is that the dataset may have been in the lan-\nguage models‚Äô training data. While this is also true\nof other common NLP benchmarks, we attempted\nto confirm results were not merely an artifact. To\ndo so, we annotated 50 distinct acronyms that oc-\ncurred in sentences in the CASI dataset that were\nnot included in the original annotations. While this\nset of acronyms is easier (e.g., they many only have\na single clinical expansion), this allows us to check\nthat GPT-3 is not simply pattern matching to po-\ntential past training data. In the set of 50, we find\nGPT-3 edit correctly expanded 47 (94%). In 2 of\nthese cases, the acronym was in fact a typo (SMIV\ninstead of SIMV , A VG instead of ABG), and the\ncorrect expansion was given regardless. Of the 3\nthat were incorrect, one was in fact incorrect, one\nwas of unspecified meaning to the annotator, and\none had 2/3 of the words correct in the expansion.\nResolver Details\nWeak Supervision For weak supervision, we\nonly consider the 97% of the dataset where the\noverlap with an answer choice was at least 5 char-\nacters as candidates for pseudolabels. Following\nprior work (Lang et al., 2022a,b), we additionally\nused a technique called the cut statistic to select a\nhigh-quality subset of the weakly labeled data to re-\nduce the noise in the training process. We selected\na subset of size 75% to decrease noise while still\nchoosing a large enough set to ensure all acronyms\nwere seen during training. We fine-tuned a Pub-\nMedBERT (Gu et al., 2021) model, a BERT variant\nthat was pretrained on biomedical abstracts and\nfull-text articles from PubMed, using learning rate\n1e-5, weight decay 0.01, the AdamW optimizer\n(Loshchilov and Hutter, 2018), and batch size 4,\nusing the BERTForMultipleChoice functionality\nin HuggingFace Transformers (Wolf et al., 2019).\nC.2 Biomedical Evidence Extraction\nBaseline Training Details We trained for 10,000\nsteps using the AdamW optimizer, learning rate 2e-\n5, batch size 32, and weight decay 1e-6, inheriting\nthese hyperparameters from Zhang et al. (2021).\nThese were the best-performing hyperparameters\nacross the set reported in Zhang et al. (2021, Table\n10, ‚ÄúBERT-CRF‚Äù).\nResolver Details To evaluate on the original\ntoken-level labels we tokenize the GPT-3 output\nand remove bullet points, numbers, stop words,\nand the words ‚Äútreatment‚Äù, ‚Äúcontrol‚Äù, and ‚Äúgroup‚Äù\nwhich GPT-3 often appended for clarification (e.g.\n‚Äú- Placebo (Control group)‚Äù). Then, any token in the\ninput that is found in the remaining GPT-3 output\nis labeled with a 1, and others with a 0. Since our\nprocedure may have interrupted valid spans, we fill\nin any 0‚Äôs between 1‚Äôs as well as acronyms within\nparentheses. These steps transform the LLM output\nstrings ùëôùëñ to a binary labeling of the full input.\nExample of Token-level Error Modes As an ex-\nample describing token-level error modes of GPT-3,\nconsider the output, the resolved output, and the\ngold label for a study with two arms below.\nGPT-3 output\n- Inhaled fluticasone\n- Placebo\nResolved GPT-3 output:\nInhaled fluticasone reduces [...] double-blind,\nplacebo-controlled study [...] inhaled fluticasone\n[...] or placebo. Large-scale [...] of inhaled\nsteroid therapy on [...]\nGold-label (token-level):\nInhaled fluticasone reduces [...] double-blind,\nplacebo-controlled study [...] inhaled fluticasone\n[...] or placebo. Large-scale [...] of inhaled\nsteroid therapy on [...]\nGPT-3 correctly identifies both study arms. How-\never, the resolved output, which simply labels the\ntoken sequence of the identified arms in the orig-\ninal input, disagrees with the gold labels for sev-\neral tokens. For example, the output includes the\nroute, ‚Äúinhaled‚Äù, which isn‚Äôt kept in the annota-\ntion schema, dinging precision. Further, the output\nexcludes ‚Äúplacebo-controlled‚Äù (given ‚Äúplacebo‚Äù is\nincluded), dinging recall. Therefore, despite quali-\ntatively capturing the arms of this trial, there was a\nmiddling F1-score of 0.70 for this example. This\nserves to underline why token-level metrics can\nbe misleading as to true performance towards the\n2019\nunderlying goal.\nOracle Details We assumed oracle splitting and\noracle coreference resolution in order to distill\nthe token-level labels to a list for the PubMed-\nBERT baselines. As an example of oracle splitting,\nPubMedBERT assigned a 1 to the span ‚Äú40, 120,\nand 240 s exposure to 3,000, 700, and 500mg l 1\nclove solution;‚Äù this span in fact contains three\ndifferent arms, and we assume it can be perfectly\nsplit, since the required information is theoretically\npresent in the identified span. As an example of\noracle coreference resolution, consider this exam-\nple with two arms: capecitabine and oxaliplatin\nplus radiotherapy (Cap-Oxa-CRT) and concurrent\ncapecitabine and radiotherapy (Cap-CRT) . The\nspans recognized by PubMedBERT include ‚Äúad-\njuvant concurrent chemotherapy‚Äù, ‚Äúcapecitabine-\nbased concurrent chemotherapy‚Äù, ‚Äúpostoperative\nCRT of capecitabine with or without oxaliplatin‚Äù,\n‚Äúconcurrent capecitabine and radiotherapy (Cap-\nCRT)‚Äù and ‚Äúcapecitabine and oxaliplatin plus ra-\ndiotherapy (Cap-Oxa-CRT).‚Äù To be generous to the\nbaseline, we assumed those 5 spans could possibly\nbe reduced to the two arms with oracle corefer-\nence resolution. No oracle splitting or coreference\nresolution was conducted for Resolved GPT-3.\nAnalysis of Error Modes for Arm Identification\nResolved GPT-3 successfully identified the correct\nnumber and content of the arms in 17 of the 20\nexamples. The three examples it missed were also\nmissed by PubMedBERT. In one case with two\narms, both methods included a procedure as a sep-\narate third arm; in reality, the procedure occurred\nfor both arms and was not the intervention itself.\nIn the second case, the prompt output did not elab-\norate on the treatment group sufficiently, and in the\nfinal case, it fully misparsed. Assuming the ora-\ncle splitting and coreference, PubMedBERT would\nstill have issues with 10 further examples: two\nagain included a common procedure as a third arm,\nfour were missing control arms, one was missing a\ntreatment arm, two arms required further domain\nknowledge to consolidate (e.g., that Ramipril is an\nACE inhibitory therapy), and another required prop-\nerly consolidating a therapy with no overlapping\ntokens.\nC.3 Coreference Resolution\nBaseline Details We benchmark using a\ntransformer-based model trained jointly on three\nlarge coreference datasets (Toshniwal et al., 2021)\nthat can be found on the HuggingFace model hub\n(shtoshni/longformer_coreference_joint).\nResolvers The resolver for the 0-shot unguided\nprompt was 50 LOC, or 973 tokens in the Codex\ntokenizer. In contrast, the 1-shot guided prompt re-\nquired only stripping a final quotation mark, period,\nor space, which required 20 tokens per the Codex\ntokenizer.\nC.4 Medication + Status Extraction\nResolver details For an unguided prompt, to map\nthe GPT-3 output string to a list of medication\nstrings, the first step is to break the output string up\ninto substrings by parsing the ‚Äúbulleted list‚Äù output\nby GPT-3, which we do with regular expressions.\nThe output strings for this prompt followed sev-\neral different formats, making this step slightly\nmore involved than in previous cases. The two\nbasic formats were a newline-separated list and a\ncomma-separated list of medication names. The\nmodifiers were also expressed in different ways:\nsome outputs were {Medication}: {Status}, while\nothers were {Medication} ({Status}). A few ex-\namples instead grouped the medications by status,\nso the output was Active: {medication1}, {medica-\ntion2}, Discontinued: {medication3} . Examples\nof these outputs can be found in Appendix A.4.\nDespite this variation, we output a list by simply\nreplacing newlines with commas to reduce to the\ncomma-separated case, and then applying two reg-\nular expressions to extract the medication names\nand modifiers from the list.\nThe previous steps turn the LLM output strings\ninto lists of strings. The next step in the resolver\nis to denoise the individual strings in each list by\nfirst stripping dosage and route information (e.g.,\n‚Äú10 mg‚Äù or ‚Äúpatch‚Äù) and then performing input-\nconsistency checking by removing tokens that do\nnot appear in the input. Finally, strings that, after\nthe prior denoising steps, only consist of stop words\nor primarily consist of punctuation and whitespace,\nare removed from the prediction lists. This required\n32 lines of code, and 946 tokens in a byte-pair\nencoding. In contrast, with a 1-shot prompt, output\ncould be simply split on the bullets, and the status\nextracted from parentheses, requiring 8 lines of\ncode and 165 tokens in a byte-pair encoding.\nMedication Extraction Baseline For normaliza-\ntion, all entities were linked to the UMLS via the de-\nfault string overlap functionality of ScispaCy (Bo-\ndenreider, 2004). We filtered the resulting UMLS\n2020\nconcepts by their semantic types and only kept\nconcepts of the types Antibiotic, Clinical Drug ,\nPharmacologic Substance, and Vitamin. Finally,\nthe baseline predictions are run through the same\ndenoising steps as the GPT-3 predictions to ensure\na fair comparison.\nStatus Classification: T-Few We use T-Few\n(Liu et al., 2022b) for medication status classifi-\ncation using 20 additional annotated examples as\nthe few-shot training set. We used a single prompt:\nIn the clinical note below, what is the status of\nthe medication Albuterol?\nAlbuterol 2 puffs every 4-6 hours as needed.\nHOSPITAL COURSE: This is an 80-year-old female\nwho was hospitalized about 2 months ago for\nchronic obstructive pulmonary disease\nexacerbation. At that time she was put on\nprednisone and antibiotics and seemed to get\nbetter. However, she was put on Augmentin ES and\ncontinued to have difficulty tasting food and\nfelt that food tasted very salty. She had no\nappetite and she has continued to lose weight\nover the last 2 months.\nFor the answer choices, we used Discontinued,\nActive, and Neither. We did not use IA (3) pre-\ntraining, but otherwise directly followed the T-Few\nrecipe (i.e., we used the default values for all hy-\nperparameters including batch size, learning rate,\nnumber of steps, length normalization, etc.). We\nused the T0-11B model.\nC.5 Medication + Relation Extraction\nResolver The resolver for the first two tasks iter-\nates over the lines in the GPT-3 output and grabs\nboth the text span and the label; the text span is\nmapped to tokenized space, and all labels not in\nthe label space (e.g. ‚ÄúInstructions‚Äù) are mapped to\nNone. For phrase-level labeling, a single additional\nstep is conducted to map the labels to BIO format.\nFor the relation extraction task, the resolver addi-\ntionally assumes all entities mentioned in a line\ncorrespond to the medication on that line.\nSequence Tagging baseline We model extrac-\ntion and labeling of medication + modifier (dosage,\nfrequency, route, duration, reason) as a sequence\ntagging task. We use the B/I/O encoding for the\nlabel space, adding tags to the B and I labels in-\ndicating the type of entity. For training data, we\nsplit the 10 notes from the 2009 i2b2 challenge\ninto shorter contexts using an off-the-shelf sentence\nsegmenter, and merged split contexts of less than\n30 tokens into the previous context. This results\nin 176 training contexts for the PubMedBERT +\nCRF model. As with Biomedical Evidence Ex-\ntraction, we search for hyperparameters over the\nsearch space reported in Zhang et al. (2021, Table\n10, ‚ÄúBERT-CRF‚Äù). The final model is chosen based\non validation F1 score on a randomly selected val-\nidation set of 10% of the training data (i.e., 18\ncontexts).\nRelation Extraction Baseline We use the model\nfrom Shi and Lin (2019) for relation extraction on\ntop of PubMedBERT. For training data, we again\nuse the 2009 i2b2 challenge set, but since the goal\nis to associate modifiers with individual medica-\ntions, we split up the 10 long notes into rolling\nchunks around each medication mention. For each\nground-truth medication entity, we create a con-\ntext including the 30 tokens before and after that\nentity. We extended these windows to be on an O\nlabel so that entities are not split across contexts.\nWe use a binary label space, since each modifier\ntype (dosage, route, etc.) determines the relation\ntype: the relevant task is to classify whether each\npair of (medication, modifier) entities in a span is\nassociated. We create one positive sample for each\ntruly related (medication, modifier) pair. For each\ncontext, we add a negative sample for each (medica-\ntion, modifier) pair that is not related. This results\nin 1416 examples (many of which have largely\noverlapping context, but a different pair of entities)\nfor training the relation extraction model.\n2021\nTask Cost/Token Tokens/Example# of Examples Experimental\nSettings\nEstimated\nCost\nClinical sense\ndisambiguation\n$0 (free inedit\nbeta mode)\n100 105 1 $0\nBiomedical\nevidence\nextraction\n$0.00006 500 187 1 $6\nCoreference\nresolution\n$0.00006 300 105 11 $21\nMedication\nstatus\nextraction\n$0.00006 300 105 16 $30\nMedication\nattribute\nextraction\n$0.00006 600 105 3 $12\nTable 8: Estimate of cost of running the experiments included in this work\nD Experimental Cost\nAt time of experimentation the cost of experiments included in this work were under $100. A breakdown\nof the upper bound of API costs can be found in the table below and is based on OpenAI API pricing in\nspring 2022. All estimates of tokens/example are rough upper bounds; some experimental settings were\ncheaper.\n2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8099220991134644
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7982128858566284
    },
    {
      "name": "Relationship extraction",
      "score": 0.7316068410873413
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6526092886924744
    },
    {
      "name": "Natural language processing",
      "score": 0.6181908249855042
    },
    {
      "name": "Benchmarking",
      "score": 0.5705940127372742
    },
    {
      "name": "Information extraction",
      "score": 0.5597866773605347
    },
    {
      "name": "Annotation",
      "score": 0.5022318363189697
    },
    {
      "name": "Identification (biology)",
      "score": 0.45518559217453003
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4302203059196472
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.42830976843833923
    },
    {
      "name": "Machine learning",
      "score": 0.3981262445449829
    },
    {
      "name": "Information retrieval",
      "score": 0.3968467712402344
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I22465464",
      "name": "University of M√ºnster",
      "country": "DE"
    }
  ]
}