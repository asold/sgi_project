{
  "title": "Combiner: Full Attention Transformer with Sparse Computation Cost",
  "url": "https://openalex.org/W3181186005",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2361153460",
      "name": "Ren Hongyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2437163763",
      "name": "Dai Hanjun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352997497",
      "name": "Dai, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2491508539",
      "name": "Yang, Mengjiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750049651",
      "name": "Leskovec, Jure",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222242690",
      "name": "Schuurmans, Dale",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133066283",
      "name": "Dai, Bo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962750131",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3012990076",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2964122153",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2606823780",
    "https://openalex.org/W2952433032",
    "https://openalex.org/W2924214462",
    "https://openalex.org/W3114951884",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2267126114",
    "https://openalex.org/W3106009088",
    "https://openalex.org/W2963428348",
    "https://openalex.org/W3015531900",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2970631142",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W2963139417",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2902630600",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W2144902422",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W2896060389"
  ],
  "abstract": "Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.",
  "full_text": "Combiner: Full Attention Transformer\nwith Sparse Computation Cost\n∗Hongyu Ren†, ∗Hanjun Dai⋄, ∗Zihang Dai⋄\nMengjiao Yang⋄, Jure Leskovec†, Dale Schuurmans⋄,‡, Bo Dai⋄\n†Stanford University, {hyren,jure}@cs.stanford.edu\n⋄Google Research, Brain Team, {hadai,zihangd,sherryy,schuurmans,bodai}@google.com\n‡University of Alberta\nAbstract\nTransformers provide a class of expressive architectures that are extremely effec-\ntive for sequence modeling. However, the key limitation of transformers is their\nquadratic memory and time complexity O(L2) with respect to the sequence length\nin attention layers, which restricts application in extremely long sequences. Most\nexisting approaches leverage sparsity or low-rank assumptions in the attention\nmatrix to reduce cost, but sacriﬁce expressiveness. Instead, we propose Combiner,\nwhich provides full attention capability in each attention head while maintaining\nlow computation and memory complexity. The key idea is to treat the self-attention\nmechanism as a conditional expectation over embeddings at each location, and\napproximate the conditional distribution with a structured factorization. Each loca-\ntion can attend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of embeddings\nfrom corresponding local regions. We show that most sparse attention patterns\nused in existing sparse transformers are able to inspire the design of such factor-\nization for full attention, resulting in the same sub-quadratic cost (O(L log(L)) or\nO(L\n√\nL)). Combiner is a drop-in replacement for attention layers in existing trans-\nformers and can be easily implemented in common frameworks. An experimental\nevaluation on both autoregressive and bidirectional sequence tasks demonstrates\nthe effectiveness of this approach, yielding state-of-the-art results on several image\nand text modeling tasks.\n1 Introduction\nThe Transformer [1] is a powerful neural network architecture that has demonstrated state-of-the-art\nperformance in machine translation [ 2] and many other natural language processing (NLP) tasks\nvia pretraining, using either unidirectional language modeling [3] or bidirectional language model-\ning [4–8]. It has also achieved excellent results in other domains like image recognition [ 9], code\nunderstanding [10], speech recognition [11], protein [12], music [13] and image [14] generative mod-\neling. The core component of Transformer is the attention mechanism, which computes dependencies\nbetween all pairs of positions in a sequence. However, for a sequence of length L, the expressiveness\nof pairwise attention comes at a quadratic cost O(L2) in both time and memory consumption. This\nmakes the vanilla Transformer [1] prohibitive for applications that involve long sequences, including\nhigh-resolution images, protein sequences, or raw speech signals [15], where the sequence length L\nis often larger than 10, 000 [14].\nRecently, there have been several attempts to scale up attention to long sequences. A popular\nclass of methods sparsiﬁes the attention matrix with different sparsity patterns, including local\n∗indicates equal contribution. The work was completed during HR’s internship at Google Brain.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2107.05768v2  [cs.LG]  28 Oct 2021\nwindow [16, 17], local+stride [ 14], log-sparse [ 18], axial [ 19, 20], or learnable patterns through\nhashing [21] or clustering [22]. Sparse attention enjoys sub-quadratic cost, but is lossy in capturing\nall-pair relationships. Generally, sparse attention requires more layers [ 14, 20, 23] to achieve full\nautoregressive or bidirectional dependencies (or receptive ﬁelds [ 20]) for each location in a long\nsequence.\nAlternatively, another line of research has tried to achieve scalability with an explicit low-rank\nassumption [24, 25] on the attention matrix or by using explicit feature maps of some kernels [26].\nHowever these explicit low dimensional approximations might be too restricted for the potentially full\nrank attention matrix, which uses exponential kernels that are effectively inﬁnite dimensional [27].\nThe Performer [28] is among the ﬁrst works that attempts to approximate regular full-rank attention\nwith the random feature trick [ 29]. However such random-feature based approaches [ 30] require\nmany more bases to better approximate the exponential kernel [ 27], and empirically we found it\nproduces inferior results in some sequence modeling tasks, such as density estimation.\nIn this paper we propose Combiner, a drop-in replacement for the vanilla quadratic attention mech-\nanism with sub-quadratic computation and memory cost. Combiner still achieves full attention\ncapability within each head of Multi-Head Attention, unlike approaches that adopt sparse or low-rank\napproximations. As we will discuss, the standard attention computed at each location can be seen as\nthe conditional expectation of the value embeddings at all feasible locations given the current location.\nBased on such an understanding, Combiner explicitly approximates the conditional distribution\nin through a structured factorization of the probability space. Speciﬁcally, given a location x, the\nprobability of attending to location y can be either directly calculated via the query vector of x and\nkey vector of y, or indirectly through a local abstraction where x ﬁrst attends to the key vector that\nrepresents a group of locations containing y, and multiplying the probability of choosing y within that\ngroup. We refer to this model as Combiner since the conditional distributions in attention become a\ncombination between several local attentions and direct attentions. This structured decomposition\nenables Combiner to take existing sparse attention patterns and convert them into corresponding\ndesign choices for probability factorizations that achieve full attention. As shown in Figure 1, Com-\nbiner achieves full attention with the same asymptotic complexity as sparse variants. Combiner can\nbe easily implemented in most existing deep learning frameworks without the need for specialized\nhardware implementation, and is GPU/TPU friendly. In fact, both the ﬁxed and learnable sparse\nattention patterns from many existing Transformer variants [14, 18, 20, 22] can be enhanced with\nsuch structured factorizations, with the same orderof time or memory cost.\nWe validate Combiner on both autoregressive and bidirectional sequence modeling tasks over a\nvariety of domains including text and images. We show that Combiner can achieve better perplexity\nand accuracy when using the same transformer architectures while being much faster in terms\nof runtime, and achieves state of the art performance on density estimation on standard datasets\nCIFAR-10 (2.77 bits/dim) and ImageNet-64 (3.42 bits/dim), as well as the Long-Range Arena [31].\nThe implementation of Combiner can be found at https://github.com/google-research/google-\nresearch/tree/master/combiner.\n2 Attention as Conditional Expectation\nIn this section, we revisit the formulation of the standard Transformer [1] from the perspective of\nconditional expectation, which inspires the derivation of Combiner.\nWithout loss of generality, we use a single sequence in the self-attention scenario. Given a sequence\nof L embeddings X = [ x1, x2, . . . , xL], where X ∈RL×d and each embedding xi ∈Rd is a\nd-dimensional vector, the core component of Transformer is the multi-head attention, where each\nhead h is a scaled dot-product attention:\nAh(X) = softmax\n(Qh√\nd\nK⊤\nh\n)\nVh,\n{\nQh = XW Q\nh , Kh = XW K\nh , Vh = XW V\nh\n}\n∈RL×d, (1)\nand the attention vector from each head Ah(X) is concatenated and projected:\nMultiHeadAttn(X) = [A1(X), A2(X), . . . , AH(X)] Wo, Wo ∈RHd×d. (2)\nHere H is the total number of heads per Transformer layer. In this paper, we focus on how to\napproximate full attention within each head of multi-head attention. For ease of notation, we drop\nthe head index h whenever possible, and use lower-case letters xi, qi, ki, vi ∈Rd to denote rows in\n2\n(D) Combiner\n-\nFixed\n(A) Fixed\n(B) \nLogsparse\n(E) Combiner\n-\nLogsparse\nDirect Expectation\nLocal Expectation\n(F) Combiner\n-\nAxial\n(C) Axial\nFigure 1: Attention matrices of several instantiations of Combiner in the autoregressive setting. We\ntransform several sparse attention patterns: Fixed (A) [14], Logsparse (B) [18] and Axial (C) [20]\nto Combiner-Fixed (D), Combiner-Logsparse (E) and Combiner-Axial (F). Combiner approximates\nthe conditional expectation (3) with a combination of direct expectation (blue) and local expectation\n(yellow). Our instantiations (D)(E)(F) achieves full attention with the same sub-quadratic complexity.\nX, Q, K, Vrespectively, which corresponds to a location i in the original sequence of length L. We\nuse [n] to denote the set of positive integers {1, 2, . . . , n}.\nFor a position i ∈[L], the attention formulation (1) can be viewed as conditional expectation of rows\nin V . Speciﬁcally, since softmax outputs a probability distribution, we can rewrite (1) as\nA(xi) = Ep(j|i) [vj] , p (j|i) = 1\nZ (xi) exp\n( qi√\nd\nk⊤\nj\n)\n, (3)\nwhere p(j|i) denotes the conditional probability at position j given the token at position i and\nthe partition function Z (xi) = ∑\nj∈Ωi exp\n(\nqi√\ndk⊤\nj\n)\nover support Ωi. The support Ωi of p (j|i)\ndeﬁnes the set of valid locations that the i-th token can attend to. For instance, the support set in\nautoregressive language modeling (LM) consists of all previous tokens, i.e., ΩLM\ni = [i]2; in masked\nlanguage modeling (MLM) the support consists of all tokens in the sequence, i.e., ΩMLM\ni = [L]. That\nis, ΩLM\ni and ΩMLM\ni represent the full attention capability respectively in the LM and MLM setting.\n3 Combiner: Full Attention via Structured Conditional Expectation\nThe complexity of p (j|i) is the bottleneck of the computation for A (xi). Generally, in existing\nsparse transformers, the support of p (j|i) is sparsiﬁed to reduce the computation and memory\ncomplexity, e.g., ΩSparse\ni ⊊ ΩLM\ni for LM and ΩSparse\ni ⊊ ΩMLM\ni for MLM, but this can lead to either\nreduced capacity or limited applicability. We defer detailed discussion of the full capacity of the\nmodel to Appendix A. In this section we introduce the Combiner, which achieves ΩCombiner\ni = ΩLM\ni\nfor LM and ΩCombiner\ni = ΩMLM\ni for MLM, while still maintaining sub-quadratic computation and\nmemory cost. Below we denote Ωi as the support for full attention if there is no ambiguity or need\nto distinguish between LM or MLM. We introduce the main design framework in Section 3.1 and\npossible parameterizations in Section 3.2. Then in Section 3.3 we analyze the trade-off of Combiner.\n3.1 Local Factorization for Conditional Expectation\nThe main idea of Combiner is to exploit a hierarchical structure for conditional probability modeling\nin (3), which provides the opportunity for reducing computation complexity while maintaining the\n2Following the conventional implementation, the input sequence will be “ right-shifted” so that the\nposition i can attent to itself in LM setting.\n3\nsame support. Speciﬁcally, we introduce support variables Ωr\ni, for r = 0, . . . , ni and i ∈[L]. The\nsupport variables are disjoint, i.e., Ωr\ni ∩Ωs\ni = ∅, ∀r ̸= s, and ∪ni\nr=0Ωr\ni = Ωi. Then we can factorize\np(j|i) as\np(j|i) =\nni∑\nr=0\np(j, Ωr\ni|i) =\nni∑\nr=0\np(j|Ωr\ni, i)p(Ωr\ni|i) = p(j|Ωrj\ni , i)p(Ωrj\ni |i), (4)\nwhere rj denotes the index of the support to which j belongs. The last equation arises from the fact\nthat the Ωr\ni are disjoint from each other (Ωr\ni ∩Ωs\ni = ∅, ∀r ̸= s). Therefore, there is only one support,\nΩrj\ni , containing j. The remaining terms, where j ̸∈Ωr\ni for r ̸= rj, are all zero since p (j|Ωr\ni, i) = 0.\nFurthermore, assume Ωrj\ni is a sufﬁcient statistic, i.e., j and i are independent given Ωrj\ni , we obtain\np(j|i) = p(j|Ωrj\ni )p(Ωrj\ni |i). (5)\nGiven the partition {Ωr\ni}ni\nr=0, the attention form in (3) can be rewritten as\nA (xi) = Ep(j|i) [vj] =\nni∑\nr=0\n∑\nj∈Ωr\ni\np (j, Ωr\ni|i) vj (6)\n=\n∑\nj∈Ω0\ni\n˜p(j|i)vj\n  \ndirect expectation\n+ ∑ni\nr=1 p(Ωr\ni|i)\n(∑\nj∈Ωr\ni\np(j|Ωr\ni)vj\n)\n  \nlocal expectation\n, (7)\nwhere we consider direct attention in partition Ω0\ni and apply the local factorization (5) to the partition\nr = 1 , . . . , ni. Here ˜p(j|i) ∝p(j|i) but with different normalization constants, which will be\nexplained below. We refer to this model as Combiner since the structured attention (7) combines the\ndirect expectation of Ω0\ni and multiple local expectations via p(j|Ωr\ni) and p(Ωr\ni|i) to form the ﬁnal\nconditional expectation.\nEquivalently, we can also rewrite the structured attention (7) as\nA(xi) = ∑\nj∈Ωi\n[\nI(j ∈Ω0\ni)˜p(j|i) +\nni∑\nr=1\nI(j ∈Ωr\ni)p(j|Ωr\ni)p(Ωr\ni|i)\n]\n  \nthe new effective conditional probability q(j|i)\nvj, (8)\nwhere I(·) is a binary indicator function. After reordering, one can see from (8) that we obtain the\neffective conditional probability q(j|i) that tries to approximate the original p(j|i). Each probability\nterm depends on both current location i and other location j, and the expectation is still obtained with\nrespect to a valid conditional probability (non-negative and sums up to 1 over Ωi).\nRequirement for Sub-quadratic Cost. We can immediately see the beneﬁt of this formulation from\nthe fact that the local expectationin (7) is independent of the position i. The full dependence is\nachieved via the multiplier p(Ωr\ni|i) where j ∈Ωr\ni. If we can design the local factorization such that:\n1. the order of number of terms in (7) for p(·|i), ∀i ∈[L]: ∑L\ni=1(ni + |Ω0\ni|) is sub-quadratic; and\n2. let U= {Ωr\ni}i∈[L],r∈[1,ni] be the unique set of partitions used for local expectation calculation,\nthen the order of |U|(i.e., the number of unique partitions in U) is sub-quadratic;\n3. the order of total number of unique calculations of local expectation across all locations in (7),∑\nΩ∈U|Ω|is sub-quadratic;\nthen one can see that the overall computation and memory cost will be sub-quadratic with full\nattention support ΩCombiner\ni = Ωi, ∀i ∈[L]. We will discuss in detail in Section 4 how to instantiate\nsuch a principle by drawing inspiration from existing sparse transformers, and how to convert them\ninto a full attention model almost for free with identical asymptotic complexity.\nRemark (Further Hierarchical Decomposition): We introduce the local decomposition with a one\nlayer partition of support of p(·|i) for simplicity. In fact, such local decompositions can be stacked\nfurther, which introduces a partition tree. Speciﬁcally, we can further partition Ωr\ni with disjoint\nsubsets\n{\nΩrk\ni\n}nr\nk=1, and consider local decompositionp(j, Ωr\ni|i) = p(j|Ωrkj\ni , i)p(Ωrkj\ni |Ωr\ni, i)p(Ωr\ni|i),\nwhere kj is the index of sub-region which j belongs to. Thus, we obtain a hierarchical decomposition\nof p(j|i), which can also be plugged to (6) and yield a new full attention formulation.\n4\n3.2 Parameterizing Conditional Probabilities\nWhile we obtained a possible way to speed up the standard Transformer via a combination of direct\nexpectation and local expectations, it is also important to have an efﬁcient design choice for the\nprobability terms in (7), namely ˜p(j|i) from direct expectation, p(j|Ωr\ni) from local expectation and\np(Ωr\ni|i) for r ∈[1, ni]. For simplicity we use the scaled dot-product, which means that we will\nassociate positions i, jand variable sets Ωr\ni with the corresponding embedding representation, and\nthus the probability is proportional to the exponential of the embedding inner products. Speciﬁcally:\n• ˜p(j|i): As this term is for the direct expectation, we can let˜p(j|i) ∝exp( qi√\ndk⊤\nj ), which is the same\nas vanilla attention (3) but with different normalizations, which will be explained in Equation 9.\n• p(Ωr\ni|i): This term aims to capture the joint event probability, i.e., p(Ωr\ni|i) ∝exp\n(\nqi√\ndk⊤\nΩr\ni\n)\n. Thus\nthe design choice of kΩr\ni should make an abstraction of the corresponding support Ωr\ni. We ﬁnd\nkΩr\ni = max poolingj∈Ωr\ni\nkj already provides good empirical results without introducing additional\nparameters; we can also use DeepSets [32] to obtain such abstraction.\n• p(j|Ωr\ni): This term is the probability of getting j within this local span Ωr\ni. We make p(j|Ωr\ni) ∝\nexp\n(qΩr\ni√\ndk⊤\nj\n)\n, where we use max pooling or DeepSets over {qj}j∈Ωr\ni\nto obtain qΩr\ni similarly.\nNormalizing Probability Terms. The terms in each local expectation p(j|Ωr\ni), ∀j ∈Ωr\ni can be\nnormalized within the local span; the direct expectation ˜p(j|i) and the terms in p(Ωr\ni|i) should be\nnormalized together,\nZ(xi) =\n∑\nj∈Ω(0)\ni\nexp\n( qi√\nd\nk⊤\nj\n)\n+\nni∑\nr=1\nexp\n( qi√\nd\nk⊤\nΩr\ni\n)\n, (9)\nand Z(xi) is the normalizing constant when calculating ˜p(j|i) and p(Ωr\ni|i).\n3.3 Trade-offs in Combiner\nCombiner achieves full attention with reduced cost without making explicit sparsity or low-rank\nassumptions over the attention matrix. However this efﬁciency gain is not free. In this section we\ndiscuss the limitations of the simpliﬁcation made by Combiner, and provide a simple workaround.\nStructured Attention Approximation. We obtain the local decomposition (5) under the condi-\ntional independenceassumption. Therefore, the local expectationin (7) is independent of the position\ni, this suggests that any two locations i1 and i2 with Ωr\ni1 = Ωr\ni2 = Ω would have linearly dependent\nattention scores over the region Ω. Formally, the probabilities formed by the effective conditional\ndistribution ⃗ a(Ω)i1 =\n[\nq(j1|i1), q(j2|i1), . . . , q(j|Ωr\ni1||i1)\n]\n=\np(Ωr\ni1|i1)\np(Ωr\ni2|i2)⃗ a(Ω)i2. In other words, the\nrank of the sub-matrix over the same partition in the resulting attention matrix is 1, therefore, the\nattention matrix is locally low-rank based on the partition. On the other hand, the direct expectation\nfully attends to each position in sub-support Ω0, which ensures the full-rank block. These two\nattention schemes make the attention matrix of Combiner structured. Compared with the low-rank\napproximation for attention [26, 28, 30], which is inspired from random features [29] in the kernel\ncommunity, a structured approximation that exploits both the locally low-rank and full-rank blocks\nhas been proved more powerful theoretically and empirically in large-scale kernel machines [27].\nImproving Expressiveness Using a Mixture Model. One way to further improve the expressive-\nness of the local factorization is to use a mixture model. This idea is adapted from the mixture of\nsoftmaxs [33] to obtain high-rank softmax layer in language modeling. Let ω be a certain partition\nof the support (i.e., collection of Ωr\ni) of Ωi, then one can easily use A(xi) = 1\nM\n∑M\nm=1 A(xi; ωm)\nto compute the attention, where each component of the mixture A(xi; ωm) is the term (7) using a\nspeciﬁc factorization plan ωm. Empirically we ﬁnd two components are already sufﬁcient to improve\nperformance.\n4 Combiner Instantiations\nIn this section we show several local factorization schemes satisfying the requirements in Section 3.1.\nAs we will see, Combiner is able to convert several sparse transformers [ 14, 18, 20–22] into full\n5\nattention, with the same order of computation and memory consumption. One can also design other\nfactorization patterns, which can be easily instantiated in Combiner.\n4.1 Combiner-Fixed\nThe Sparse Transformer [14] is one of the most representative variants that can achieve O(L\n√\nL)\ncomputation and memory cost with sparse attention. Here we show how to convert this ﬁxed pattern\nproposed in [14] (Figure 1(A)) into a factorization plan, and instantiate a full attention variant named\nthe Combiner-Fixed (Figure 1(D)).\nIn the ﬁxed-sparse attention, the support is Ωsparse MLM\ni = {j : j mod s = 0}∪{ j : j ≡i (div s)}\nwhere s is a hyper-parameter, div is integer division, andj ≡i (div s) denotes that the quotients of\ni and j w.r.t. s are the same. In the autoregressive case, Ωsparse LM\ni = Ωsparse MLM\ni ∩[i]. Please refer\nto Figure 1(A) for an illustration of the LM version.\nOur design of ωMLM\nﬁxed has the following form:\nΩ0\ni = {j : j ≡i (div s)}, Ωr\ni =\n{\nj : j div s = r, j /∈Ω0\ni\n}\n, ∀r ∈[L div s], ∀i ∈[L] (10)\nwhere each local expectationis performed in each span of size s, and there are totally L div s spans\nacross all locations. For each position i ∈[L], there are (s + (L div s)) terms in (7) ; the local\nexpectation has (L div s) terms . The overall complexity is O(L ·(s + 2(L div s))). The optimal s\nis O(\n√\nL), and we can achieve O(L\n√\nL) computation and memory complexity, which is the same\nas [14] but here we gain full attention capability in each attention head. For the LM case, we can\nsimply have ωLM\nﬁxed : {Ωr\ni ∩[i] |Ωr\ni ∈ωMLM\nﬁxed }, which has the same O(L\n√\nL) optimal complexity.\n4.2 Combiner-Logsparse\nThe Logsparse Transformer is proposed in [18] and can theoretically achieve O(L log L) cost. The\ngeneral idea is to make the size of support Ωsparse\ni no larger than ⌈log2 i⌉. For the ease of notation,\nwe ﬁrst deﬁne bits(n) = [ b1, b2, . . . , b⌈log2 n⌉] to be the binary representation of integer n, with\nbt ∈{0, 1}the coefﬁcient of basis 2t. Thus we have n = ∑⌈log2 n⌉\nt=1 bt∗2t. One of the possible design\nchoices to make Logsparse in the LM case isΩsparse LM\ni =\n{\nsufft := ∑⌈log2 i−1⌉\nτ=t bτ ∗2τ\n}⌈log2 i−1⌉\nt=1\n∪\n{i}, i.e., attend to the location indices that equal to the sufﬁx sum of the weighted bits(i −1), as well\nas location i itself. This serves as our base sparse version as shown in Figure 1(B).\nTo exploit this scheme in the Combiner framework, we can deﬁne⌈log2 n⌉non-overlapping supports,\nwhere Ωr\ni = [ suffr] \\[suffr+1] with the boundary case [suff⌈log2 i−1⌉+1] = ∅. Note that for the\nease of notation, some of the Ωr\ni are empty which will be ignored. In this case, the direct attention\nset Ω0\ni includes {i}, as well as {i −1}when i is an even number. Such a factorization leads to\nCombiner-Logsparse, as shown in Figure 1(E). From the Figure, we observe that in total we will\nhave span summaries for every 2, 4, 8, . . . ,2⌊log2 L⌋locations, resulting in total ∑⌊log2 L⌋\nt=1 ⌊L\n2t ⌋or\nO(L) summaries. Each location i will select at most O(log(i)) non-overlapping spans to cover the\nfull support Ωi, and thus, the total cost will be O(L log L). We leave the design of MLM case to\nAppendix B.\n4.3 Combiner-Axial\nThe Axial Transformer [ 20] builds the attention along each axis of the input data. Without loss\nof generality, we focus on 2D case where the input sequence is reshaped into a matrix of size\nn ×m = L. Speciﬁcally, the location i in original sequence will be in rowi = (i −1) div m + 1 and\ncoli = (i −1) mod m + 1. We show how to simply enable full attention with factorization on 2D\nmatrix, hence Combiner-Axial.\nThe sparse axial has Ωsparse MLM\ni = {j : j −1 ≡i −1(mod m)}∪{j : j −1 ≡i −1(div m)}, and\nΩsparse LM\ni = Ωsparse MLM\ni ∩[i], which all have at most O(m + n) entries for each i, as illustrated\nin Figure 1(C). We propose several factorization schemes to make it an attention with full support.\n• ωLM\naxial-vertical: Ω0\ni = Ωsparse LM\ni , and Ωr\ni = {j : j ≡r(mod m)}∩[i −coli], for r ∈[m] \\{coli}. As\ndepicted in Figure 2(A), Ωr\ni corresponds to the column r above rowi, where we use max pooling to\n6\n(A) Combiner\n-\nAxial\n-\nVertical\n(B) Combiner\n-\nAxial\n-\nHorizontal\nAttention matrix\nAttention matrix\nReshaped sequence\nFigure 2: Attention matrices and sequence being attended (e.g., a 3x4 image) of vertical and horizontal\nvariants of Combiner-Axial. Blue and yellow correspond to direct and local attention respectively for\nlocation i (purple). Locations connected by arrows correspond to the same support Ωr.\nobtain the abstraction. To obtain such abstraction for all the locations, we can leverage the cummax\noperator for each column to efﬁciently obtain the preﬁx-max.\n• ωLM\naxial-horizontal: similar as ωaxial-vertical except that each Ωr\ni summarizes the row r before rowi and\nexcludes coli (Figure 2(B)).\n• ωLM\naxial-rowmajor: Ω0\ni = {j : j −1 ≡i −1(div m)}∩[i], i.e., elements in the same row are directly\nattended, while Ωr\ni = {j : j ≡r(div m)}∩[i −coli] captures the rows before rowi. This structure\nis similar to Combiner-Fixed, except for the way that theabstraction (and thus the local expectation)\nis computed. Combiner-Fixed computes the abstraction only based on r of partition Ωr\ni, where\nωaxial-rowmajor depends on both r and the column coli (Figure 1(F)).\nIn all cases above, the cost is similar to the Axial Transformer [20], which is O(L\n√\nL) if we reshape\nthe sequence to a 2D matrix with n, m= O(\n√\nL). We defer the MLM case to Appendix C.\n4.4 Combiner-Learnable\nInspired by the Reformer [ 21] and Routing Transformer [22], we can also learn the factorization\nplan ω from the data. We illustrate this with Routing Transformer and provide a way to enable full\nattention in Routing Transformer following the Combiner principle.\nFor a speciﬁc layer, suppose we have a learned disjoint region (or cluster in Routing Transformer)\n{Ωr}n\nr=1 where ∪rΩr = [L]. In Routing Transformer, we simply have Ωsparse MLM\ni = Ωri where Ωri\ndenotes the region where position i belongs to. To deﬁne the Combiner factorization, we let\nωrouting MLM : Ω0\ni = Ωri , Ωr\ni = Ωr \\Ω0\ni, ∀r ∈[ni]. (11)\nNote that ni = n (i.e., number of learned clusters) for all locations. The above factorization can only\nwork for MLM. LM requires the following deﬁnition:\nωrouting LM : Ω0\ni = Ωri ∩[i], Ωr\ni =\n(\nΩr \\Ω0\ni\n)\n∩[i], ∀r ∈[ni]. (12)\nIn general, both LM and MLM can have sub-quadratic cost when n = O(\n√\nL). However, routing\nvariants (including the Routing Transformer) require a gather operation, which can be slow on\nTPUs (see illustration in Appendix D).\n5 Experimental Evaluation\nWe evaluate Combiner with different full attention patterns on both autoregressive and bidirectional\nsequence modeling tasks, covering a wide range of input data from images to texts. All tasks\nconsidered involve long sequences for up to 12,000 in length, some of which prevent the applicability\nof the vanilla transformer. We compare Combiner with state-of-the-art Transformers. We also perform\na series of ablation studies where all of the models being compared use the exact same architecture\nthat only differ in the attention module, avoiding individual tricks employed in the original works\n(e.g., using both learnable and ﬁxed patterns in Routing Transformer [22]). Details to reproducing all\nexperimental results can be found in Appendix E.\n7\nTable 1: Ablation results in Bits per Dimension (Bits/Dim) on CIFAR-10 and ImageNet-64.\nModel Layers CIFAR-10 ImageNet-64\nReformer [21] 6 - 3.740\nPerformer [28] 6 3.335 3.719\nLogsparse [18] 6 4.253 4.351\nCombiner-Logsparse (Ours) 6 3.366 3.795\nFixed [14] 6 3.408 3.696\nCombiner-Fixed (Ours) 6 3.321 3.654\nAxial [20] 6 3.666 4.032\nCombiner-Axial (Ours) 6 3.050 3.585\nCombiner-Mixture (Ours) 6 3.040 3.585\nReformer [21] 12 - 3.710\nPerformer [28] 12 3.310 3.636\nRouting Transformer [22] 12 2.950 -\nCombiner-Mixture (Ours) 12 2.885 3.504\n5.1 Autoregressive Sequence Modeling\nIn this subsection, we ﬁrst perform density estimation on text and image using Combiner.\n5.1.1 Language Modeling Table 2: LM Perplexity on Wiki-40B (Main).\nModel Perplexity\nTransformer-2k [1] 17.26\nPerformer-2k [28] 19.66\nRouting-2k [22] 20.85\nFixed-2k [14] 18.04\nCombiner-Fixed-2k (Ours) 17.70\nAxial-2k [20] 20.82\nCombiner-Axial-2k (Ours) 17.56\nCombiner-Fixed-8k (Ours) 16.60\nCombiner-Axial-8k (Ours) 16.49\nTable 3: LM Perplexity on Wiki-40B (Ablation).\nModel Perplexity\nTransformer-2k [1] 17.26\nCombiner-DeepSets-Max-8k (Ours)16.29\nCombiner-DeepSets-Mean-8k (Ours) 16.48\nCombiner-Max-8k (Ours) 16.60\nCombiner-Mean-8k (Ours) 16.54\nFor language modeling, we focus on the Wiki-40B-\nEn dataset [34], which consists of clean Wikipedia\npages in English. We use a sentence piece model\nwith vocabulary size 32K to tokenize the text and\nmeasure the perplexity at the sentence piece level.\nTo ensure fair comparison, all models being com-\npared again have the same number of layers and\nhidden sizes, are are implemented under the same\ncode base.\nTable 2 shows the results of the comparison. As we\ncan see, under 2k sequence length, Combiner vari-\nants are consistently better than their correspond-\ning baselines, and are very close to the standard\nTransformer. When sequence length goes to 8k, the\nstandard Transformer runs out of memory, whereas\nCombiner continues to achieve improved perplex-\nity, surpassing the result of Transformer-2k. If we\nfurther use DeepSets to calculate the summariza-\ntion terms qΩr\ni and kΩr\ni , we may further achieve\nlower perplexity as shown in Table 3.\n5.1.2 Image Generative Models\nCIFAR-10. We ﬁrst perform a sanity check where we compare sparse attention baselines against\nCombiner with full attention under the same architectureon the CIFAR-10 dataset. The sequence\nlength is 3072. For all the methods, we use a same 6-layer transformer with 8 attention heads and\n512 embedding dimensions. We train all models for 500k iterations using batch size 32 on TPU v2.\nAs shown in Table 1, given the same model architecture, Combiner-X performs signiﬁcantly better\nthan the base model X under the bits per dimension (BPD) metric on the 10,000 test images. In\nparticular, Combiner signiﬁcantly decreases BPD by 0.887, 0.087, and 0.626 compared to the base\nmodels Logsparse, Fixed and Axial, respectively. Note that all of the Combiner variants achieve\nbetter performance than the best of the base models. This demonstrates the advantage of Combiner\nover the baselines given the same 6-layer architecture. We observe a similar trend under a 12-layer\narchitecture.\n8\nTable 4: Bits per Dimension (Bits/Dim) on CIFAR-10 and ImageNet-64.\nCIFAR-10 Bits/Dim\nPixelCNN [15] 3.03\nPixelCNN++ [36] 2.92\nImage Transformer [16] 2.90\nPixelSNAIL [37] 2.85\nSparse Transformer [14] 2.80\nCombiner-Axial (ours) 2.77\nImageNet 64x64 Bits/Dim\nPixelCNN [15] 3.57\nParallel Multiscale [38] 3.70\nGlow [39] 3.81\nSPN [40] 3.52\nSparse Transformer [14] 3.44\nAxial Transformer [20] 3.44\nRouting Transformer [22] 3.43\nCombiner-Axial (ours) 3.42\nFollowing the 128-layer architecture in Child et al. [14], we apply Combiner-Axial and achieve\nstate-of-the-art performance, 2.77 BPD on CIFAR-10, as listed in Table 4. We run all of the models\nin Table 4 without data augmentation [35].\nImageNet-64. We also evaluate performance under the autoregressive setting on ImageNet-64,\nwhere sequence length is 12,288. We ﬁrst perform the same analysis as CIFAR-10 and compare\nCombiner-X with the baselines using the same model architecture. As shown in Table 1, Combiner\nconsistently outperforms the baselines with the same attention pattern. We further apply Combiner-\nAxial to a 30-layer Transformer, which achieves state-of-the-art performance on density estimation\non ImageNet-64, demonstrating the effectiveness of full attention achieved by Combiner.\n5.2 Bidirectional Sequence Modeling\nBesides autoregressive tasks, we also evaluate Combiner on a set of standard bidirectional tasks to\nshow the general applicability of the method.\n5.2.1 Long-Range Arena\nLong-Range Arena (LRA) is a uniﬁed benchmark [31] for probing the capability of efﬁcient trans-\nformers on handling long sequences. We evaluate our models on ﬁve tasks from LRA: ListOps, Text\nClassiﬁcation, Retrieval, Image Classiﬁcation and Pathﬁnder. All of the tasks are sequence-level\nmulti-class classiﬁcation. Please refer to the original LRA paper for more details.\nTable 5: Experimental results on Long-Range Arena benchmark.\nModel ListOps Text Retrieval Image Pathﬁnder Avg\nChance 10.00 50.00 50.00 10.00 50.00 34.00\nTransformer 36.38 64.27 57.46 42.44 88.81 57.87\nLocal Attention 15.95 52.98 53.39 41.46 84.64 49.68\nSparse TRans. 35.78 63.58 59.59 44.24 83.90 57.42\nLongformer 36.03 62.85 56.89 42.22 86.68 56.93\nLinformer 35.49 53.94 52.27 38.56 86.17 53.28\nReformer 36.30 56.10 53.40 38.07 79.18 52.61\nSinkhorn Trans. 34.20 61.20 53.83 41.23 73.36 52.76\nSynthesizer 36.50 61.68 54.67 41.61 81.61 55.21\nBigBird 37.08 64.02 59.29 40.83 86.75 57.59\nLinear Trans. 17.15 65.90 53.09 42.34 88.13 53.32\nPerformer 36.00 65.40 53.82 42.77 88.76 57.35\nCombiner-Fixed 36.65 64.99 59.81 41.67 88.59 58.34\nCombiner-Axial 36.15 64.36 56.10 41.33 88.43 57.27\nAs shown in Table 5, Combiner is able to match the performance of vanilla Transformer and achieves\neven better performance in some tasks. Following the protocol of LRA, all methods use the same\narchitecture and hyperparameters for a controllable comparison. We use the numbers from Tay et al.\n[31] for all tasks except for Pathﬁnder. Since we were unable to reproduce the original Pathﬁnder\nresults using the default setup in LRA Github repository, we rerun all the baselines using Pathﬁnder-\ninter conﬁguration to conduct fair comparison. However, as the benchmark is still of small-scale and\nthe LRA ofﬁcial website discourages hyperparameter tuning, Table 5 should be treated as results for\nthe test bench of expressiveness compared to vanilla Transformer.\n9\nTable 6: MLM perplexity on C4 dataset.\nModel Perplexity\nTransformer-2k [1] 4.552\nBigBird-2k [41] 4.696\nPerformer-2k [28] 10.940\nFixed-2k [14] 5.279\nCombiner-Fixed-2k (Ours) 5.170\nAxial-2k [20] 5.370\nCombiner-Axial-2k (Ours) 4.809\nRouting-2k [22] 6.703\nCombiner-Routing-2k (Ours) 6.539\nBigBird-8k [41] 4.542\nCombiner-Axial-8k (Ours) 4.190\nCombiner-Fixed-8k (Ours)4.139\n210 211 212 213 214\nSequence Length\n27\n28\n29\n210\n211\n212\n213\nMilliseconds / Iteration\n210 211 212 213 214\nSequence Length\n20\n21\n22\n23\nMemory (GB)\nVanilla Transformer\nPerformer\nBigBird\nCombiner-Axial\nCombiner-Fixed\nSparse-Axial\nSparse-Fixed\nCombiner-Mixture\nFigure 3: We measure the inference runtime and memory usage\nfor eight models. Overall Combiner has similar speed with\nPerformer and its sparse counterpart but Vanilla Transformer\nquickly goes OOM when sequence length grows.\n5.2.2 Masked Language Modeling\nAs the core element of BERT langauge pretraining [5], masked language modeling (MLM) refers to\nthe task of reconstructing tokens that are randomly masked out in the input sequence. As with the\nLM task, we use perplexity as the main metric, which correlates relatively well with down-stream\ntask performance. Speciﬁcally, we use the large scale C4 dataset [ 8] for training and evaluation,\nand consider different sequence lengths. Following the original BERT setup, we mask out 15% of\nthe tokens in each input sequence. The comparison is summarized in Table 6. Similar to the LM\nresult, different Combiner variants consistently outperform their corresponding baselines under 2k\nsequence length. However, apart from the standard Transformer, Combiner-2k also falls behind\nBigBird-2k. We conjecture that this is related to the special design in BigBird such as all tokens can\nalways attend to the <cls> token directly, which is only applicable in non-causal problems. That\nsaid, when we further increase sequence length to 8k, the standard Transformer runs into OOM issue,\nwhereas Combiner not only outperforms BigBird but also substantially surpasses Transformer-2k.\nThis suggests that Combiner can truly beneﬁt from scaling learning to longer sequence lengths.\n5.3 Runtime and Memory Usage of Combiner\nHere we evaluate the inference runtime and memory usage of ﬁve baselines – Transformer, Performer,\nBigBird, Sparse-Fixed and Sparse-Axial, as well as three variants of Combiner– Combiner-Fixed,\nCombiner-Axial and Combiner-Mixture. We run inference of all the models on a TPU v3-16 (16\ncores x 16GB) with batch size 16, and we test sequences of length from 210 to 214. As shown\nin Figure 3, Combiner instantiations achieve comparable runtime and memory usage with their\nsparse counterpart and Performer. Note Combiner achieves much better empirical performance\nthan the sparse models and Performer. Combiner-Mixture has the same asymptotic complexity with\nCombiner-Fixed and Combiner-Axial, however, since it requires running two partition plans, it is\nslower than Combiner-Fixed and Combiner-Axial. Due to the gather operation required by the\nrandom attention which is not very TPU/GPU friendly, BigBird is very computationally expensive.\nAnd the Transformer model quickly runs out of memory when sequence length increases.\n6 Conclusion\nInspired by the conditional expectation view of attention mechanism, we propose Combiner, a\ndrop-in replacement of the attention module. By introducing structured decomposition to the\nconditional probability, Combiner achieves full attention capability while maintaining sub-quadratic\ncomputational and memory cost. We instantiate several Combiner variants converting existing sparse\ntransformers to full attention. Combiner achieves state-of-the-art performance on both autoregressive\nand bidirectional tasks for image and text modeling, showing beneﬁts in both modeling effectiveness\nand runtime efﬁciency. Future work includes additional factorization pattern designs, as well as\napplications of Combiner in domains like bioinformatics and speech.\n10\nAcknowledgments and Disclosure of Funding\nWe would like to thank Richard Song and David Dohan for the help on introducing Performer codebase\nand experiment conﬁgurations, Yi Tay and Mostafa Dehghani for clariﬁcations on the LRA bench-\nmark, James Lee-Thorp, Joshua Ainslie, and Ilya Eckstein for clariﬁcation on their LRA experiment\nresults, Adams Yu for performing internal paper review and helpful suggestions. We also grate-\nfully acknowledge the support of DARPA under Nos. HR00112190039 (TAMI), N660011924033\n(MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under\nNos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477\n(RAPID), NIH under No. R56LM013365; Stanford Data Science Initiative, Wu Tsai Neurosciences\nInstitute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase, Docomo, Hitachi, Intel, JD.com,\nKDDI, NVIDIA, Dell, Toshiba, Visa, and UnitedHealth Group. Hongyu Ren is supported by the\nMasason Foundation Fellowship and the Apple PhD Fellowship. Jure Leskovec is a Chan Zuckerberg\nBiohub investigator.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2017.\n[2] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George\nFoster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. The best of both worlds:\nCombining recent advances in neural machine translation. In Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2018.\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n[4] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in\nNeural Information Processing Systems (NeurIPS), 2019.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In Annual Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), 2019.\n[6] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. Albert: A lite bert for self-supervised learning of language representations. In\nInternational Conference on Learning Representations (ICLR), 2020.\n[7] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[8] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. In International\nConference on Learning Representations (ICLR), 2021.\n[10] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating\ncontextual embedding of source code. In International Conference on Machine Learning\n(ICML), 2020.\n[11] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-\nsequence model for speech recognition. In IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2018.\n11\n[12] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R\nEguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation.\narXiv preprint arXiv:2004.03497, 2020.\n[13] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne,\nAM Dai, MD Hoffman, and D Eck. Music transformer: Generating music with long-term\nstructure (2018). In International Conference on Learning Representations (ICLR), 2019.\n[14] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n[15] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\nIn International Conference on Machine Learning (ICML), 2016.\n[16] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML),\n2018.\n[17] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. In International Conference on Learning\nRepresentations (ICLR), 2020.\n[18] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time series\nforecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n[19] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu\nLiu. Ccnet: Criss-cross attention for semantic segmentation. In International Conference on\nComputer Vision (ICCV), 2019.\n[20] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in\nmultidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.\n[21] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In\nInternational Conference on Learning Representations (ICLR), 2020.\n[22] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based\nsparse attention with routing transformers. Transactions of the Association for Computational\nLinguistics, 9:53–68, 2021.\n[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. In Annual Meeting\nof the Association for Computational Linguistics (ACL), 2019.\n[24] Zhuoran Shen, Mingyuan Zhang, Shuai Yi, Junjie Yan, and Haiyu Zhao. Factorized attention:\nSelf-attention with linear complexities. CoRR, 2018.\n[25] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[26] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers\nare rnns: Fast autoregressive transformers with linear attention. In International Conference on\nMachine Learning (ICML), 2020.\n[27] Si Si, Cho-Jui Hsieh, and Inderjit S Dhillon. Memory efﬁcient kernel approximation. The\nJournal of Machine Learning Research, 2017.\n[28] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. In International Conference on Learning Representations (ICLR),\n2021.\n[29] Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2007.\n12\n[30] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng\nKong. Random feature attention. In International Conference on Learning Representations\n(ICLR), 2021.\n[31] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. In International Conference on Learning Representations (ICLR), 2021.\n[32] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov,\nand Alexander Smola. Deep sets. In Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n[33] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the soft-\nmax bottleneck: A high-rank rnn language model. In International Conference on Learning\nRepresentations (ICLR), 2018.\n[34] Mandy Guo, Zihang Dai, Denny Vrande ˇci´c, and Rami Al-Rfou. Wiki-40b: Multilingual\nlanguage model dataset. In Proceedings of The 12th Language Resources and Evaluation\nConference, 2020.\n[35] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, and\nIlya Sutskever. Distribution augmentation for generative modeling. In International Conference\non Machine Learning (ICML), 2020.\n[36] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the\npixelcnn with discretized logistic mixture likelihood and other modiﬁcations. In International\nConference on Learning Representations (ICLR), 2017.\n[37] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved\nautoregressive generative model. In International Conference on Machine Learning (ICML),\n2018.\n[38] Scott Reed, Aäron Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian\nChen, Dan Belov, and Nando Freitas. Parallel multiscale autoregressive density estimation. In\nInternational Conference on Machine Learning (ICML), 2017.\n[39] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolu-\ntions. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n[40] Jacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel\nnetworks and multidimensional upscaling. In International Conference on Learning Represen-\ntations (ICLR), 2019.\n[41] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n[42] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and\nSanjiv Kumar. o(n) connections are expressive enough: Universal approximability of sparse\ntransformers. arXiv preprint arXiv:2006.04862, 2020.\n13\nAppendix\nA Universal Approximation\nHere we show in Proposition 1 that our Combiner-X achieves universal approximation property\n[42] if the sparse transformer X achieves universal approximation property. For approaches like\nBigBird [41], they maintain the universal approximation property using the global tokens (CLS).\nHowever, the global attention makes it hard to be applied to the unidirectional autoregressive modeling\n(LM). Besides, the random attention requires the gather operation, making it very slow on dense\nhardware like TPUs (Figure 3).\nProposition 1. The proposed Combiner will not break the universal approximation property of the\noriginal sparse transformers.\nSpeciﬁcally, we consider the function class constructed by stacking the attention block with a\ntwo-layer fully connected network. Formally, following the notations in [42] we have the block as\nSAttn (X) = X + MultiHeadAttn (X) , (13)\nZ = SAttn (X) + relu (SAttn ·W1) ·W2, (14)\nwhich denotes the h-head attentions with X ∈RL×d, W1 ∈Rd×r, and W2 ∈Rr×d. The function\nclass is denoted as\nSTH,r := {X →t (X + E) |t is a composition of block (13), (15)\nE is trainable position embedding}. (16)\nYun et al. [42] shows that the function class (15) is still universal approximation w.r.t. the norm\ndeﬁned as dp(f, g) :=\n(∫\n∥f(X) −g(X)∥p\npdX\n)1/p\nwith softmax in (1) and several requirements\non the sparsity patterns in attention scheme.\nB Combiner-Logsparse in MLM Case\nHere we extend the Combiner-logsparse introduced in section 4.2 to the MLM case.\nBesides the ⌈log2 i⌉non-overlapping supports in the LM case, we can deﬁne addtional ⌈log2 i⌉\nnon-overlapping supports to attend to the tokens after the current token in the sequence. We illustrate\nthis design choice in ﬁgure 4.\nC Combiner-Axial in MLM Case\nBesides the ωLM\naxial-vertical, ωLM\naxial-horizontal and ωLM\naxial-rowmajor introduced in section 4.3, here we introduce\nhow we extend these three models to the MLM case.\n• ωMLM\naxial-vertical: Ω0\ni = Ωsparse MLM\ni = {j : j −1 ≡i −1(mod m)}∪{j : j −1 ≡i −1(div m)}, and\nΩr\ni = {j : j ≡r(mod m)}, for r ∈[m] \\{coli}. As depicted in Figure 2(A), Ωr\ni corresponds to\nthe column r above rowi, where we use max pooling to obtain the abstraction. To obtain such\nabstraction for all the locations, we can leverage the cummax operator for each column to efﬁciently\nobtain the preﬁx-max.\n• ωMLM\naxial-horizontal: similar as ωMLM\naxial-vertical except that each Ωr\ni summarizes all rows r and excludes coli.\n• ωMLM\naxial-rowmajor: Ω0\ni = {j : j −1 ≡i −1(div m)}, i.e., elements in the same row are directly at-\ntended, while Ωr\ni = {j : j ≡r(div m)}for r ∈[n] \\{rowi}captures all the rows except rowi.\nIt is trivial to see that the complexity remains O(L\n√\nL) if n, m= O(\n√\nL).\nD Combiner-Learnable\nAs discussed in section 4.4. we design Combiner-learnable as an extension to the routing transformer\n[22], which learns to cluster the tokens. Each token in the routing transformer only attends to the\ntokens in the same cluster. As shown in ﬁgure 4, our Combiner-learnable combines direct expectation\nwith local expectation (yellow tokens), each of which summarizes one cluster (red, blue or green).\n14\nFigure 4: Left: Combiner-logsparse in the MLM case. Right: Combiner-Learnable. Following the\nrouting transformer [22], we apply the combiner principle, so that we can achieve full attention in\neach head with identical complexity with the routing transformer.\nE Experimental Details\nE.1 CIFAR-10\nHere we list the hyperparameters we used on the CIFAR-10 dataset. Our experiments include (1)\nan ablation study, where all the models share the exact same architecture; and (2) the main result,\nwhere our Combiner achieves the state-of-the-art result under the setting that no data augmentation is\nallowed.\nFor the ablation study, the embedding and hidden size is 512. We use 8 attention heads in each layer\nwith in total 6 transformer layers. We train all the models for 400,000 steps with learning rate 1e-3\nand batch size 32. For the main result, we use the same architecture as introduced in Child et al. [14],\nand we train our Combiner-Axial for 1,200,000 steps with cosine learning rate scheduling. We rerun\nthe main result for 3 times and the standard deviation is 0.003.\nE.2 ImageNet-64\nRegarding the details of the ImageNet-64, we use the same setup with CIFAR-10, which consists of\nan ablation study and the main result. The architecture used in the ablation study is identical with the\none we used in CIFAR-10. For the main result of Combiner-Axial, we used a 30-layer architecture\nwith 768 hidden size and embedding dimension. We train this architecture for 1,200,000 steps with\ncosine learning rate scheduling. We also rerun the main result for 3 times and the standard deviation\nis 0.005.\nE.3 Wiki-40B Language Modeling\nThe main purpose of this experiment is not to chase the state-of-the-art performance, as generally\nspeaking, the more parameters/data, the better the perplexity would be for language modeling. So\ninstead, we let all the methods have the same neural network backbone, while only varying the\nattention implementations to compare their effectiveness. This is similar in spirit to the ablation study\nin CIFAR-10 and ImageNet-64.\nSpeciﬁcally, we use the word embedding size and hidden size of 768 for all the layers. We use 12\nattention heads in each layer, with in total 12 transformer layers. We use the Pre-Norm architecture,\nand the MLP layers have hidden size equals to 4 ×768. The maximum sequence length can vary\nin {2048, 8192}, depends on the memory limit of each methods. All the methods are trained for\n125,000 stochastic gradient updates, with batch size equals to 128. We also enable the cosine learning\nrate scheduling, with 10,000 warm-up steps. The optimizer is Adam with gradient clipping.\n15\nE.4 LRA Benchmark\nWe mainly follow the guideline of LRA, where all the models should use roughly the same number\nof parameters and same hyperparameters like batchsize, number of iterations, etc.. We tried our best\nto reproduce the experimental results using the code in https://github.com/google-research/long-\nrange-arena, and we found that we cannot reproduce the pathfinder-32 results. We have\ncommunicated with the authors but didn’t get the issue resolved. So instead, we rerun all the baselines\nusing the same network conﬁgurations, on the pathfinder-32-inter setup. We found some of the\nmethods favor the ’MEAN’ pooling to get the sequence representation, while others favor the ’CLS’\npooling. So we try both of them for each of the method, and report the best result.\nE.5 C4 Masked Language Modeling\nSimilar to the purpose of section E.3, we perform masked language modeling task on C4 dataset,\nwhich is typically used for BERT pretraining. As the perplexity metric correlates with the down-\nstream task performance well, we thus perform the controlled experiments with all the methods using\nthe same network architecture.\nThe architecture used and the hyperparameters are almost the same as in section E.3, except that we\nhave maximum number of segments equal 2.\n16",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.667219877243042
    },
    {
      "name": "Computation",
      "score": 0.6414938569068909
    },
    {
      "name": "Quadratic equation",
      "score": 0.5693506598472595
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5649527907371521
    },
    {
      "name": "Transformer",
      "score": 0.5538997650146484
    },
    {
      "name": "Factorization",
      "score": 0.49644213914871216
    },
    {
      "name": "Autoregressive model",
      "score": 0.44852960109710693
    },
    {
      "name": "Algorithm",
      "score": 0.4098310172557831
    },
    {
      "name": "Theoretical computer science",
      "score": 0.40814292430877686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2468928098678589
    },
    {
      "name": "Mathematics",
      "score": 0.21534833312034607
    },
    {
      "name": "Voltage",
      "score": 0.09159597754478455
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}