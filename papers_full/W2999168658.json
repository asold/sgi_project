{
  "title": "RobBERT: a Dutch RoBERTa-based Language Model",
  "url": "https://openalex.org/W2999168658",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2949075021",
      "name": "Pieter Delobelle",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2127016386",
      "name": "Thomas Winters",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A36886011",
      "name": "Bettina Berendt",
      "affiliations": [
        "Technische Universität Berlin",
        "KU Leuven"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2969605360",
    "https://openalex.org/W2949505205",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W3122809229",
    "https://openalex.org/W22168010",
    "https://openalex.org/W1570518225",
    "https://openalex.org/W2162670686",
    "https://openalex.org/W2978098687",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3088382025",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W3037697022",
    "https://openalex.org/W3000640960",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3024803553",
    "https://openalex.org/W3014706045",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W3032388710",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962753370"
  ],
  "abstract": "Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model's fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3255–3265\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3255\nRobBERT: a Dutch RoBERTa-based Language Model\nPieter Delobelle1 and Thomas Winters1 and Bettina Berendt1,2\n1 Department of Computer Science, KU Leuven\n2 Faculty of Electrical Engineering and Computer Science, TU Berlin\nfirstname.lastname@kuleuven.be\nAbstract\nPre-trained language models have been dom-\ninating the ﬁeld of natural language process-\ning in recent years, and have led to signiﬁcant\nperformance gains for various complex natu-\nral language tasks. One of the most prominent\npre-trained language models is BERT, which\nwas released as an English as well as a multi-\nlingual version. Although multilingual BERT\nperforms well on many tasks, recent studies\nshow that BERT models trained on a single\nlanguage signiﬁcantly outperform the multi-\nlingual version. Training a Dutch BERT model\nthus has a lot of potential for a wide range of\nDutch NLP tasks. While previous approaches\nhave used earlier implementations of BERT\nto train a Dutch version of BERT, we used\nRoBERTa, a robustly optimized BERT ap-\nproach, to train a Dutch language model called\nRobBERT. We measured its performance on\nvarious tasks as well as the importance of the\nﬁne-tuning dataset size. We also evaluated\nthe importance of language-speciﬁc tokenizers\nand the model’s fairness. We found that Rob-\nBERT improves state-of-the-art results for var-\nious tasks, and especially signiﬁcantly outper-\nforms other models when dealing with smaller\ndatasets. These results indicate that it is a\npowerful pre-trained model for a large vari-\nety of Dutch language tasks. The pre-trained\nand ﬁne-tuned models are publicly available to\nsupport further downstream Dutch NLP appli-\ncations.\n1 Introduction\nThe advent of neural networks in natural lan-\nguage processing (NLP) has signiﬁcantly im-\nproved state-of-the-art results within the ﬁeld. Ini-\ntially, recurrent neural networks and long short-\nterm memory networks dominated the ﬁeld. Later,\nthe transformer model caused a revolution in NLP\nby dropping the recurrent part and only keeping\nattention mechanisms (Vaswani et al., 2017). The\ntransformer model led to other popular language\nmodels, e.g. GPT-2 (Radford et al., 2018, 2019).\nBERT (Devlin et al., 2019) improved over previ-\nous models and recurrent networks by allowing\nthe system to learn from input text in a bidirec-\ntional way, rather than only from left-to-right or\nthe other way around. This model was later re-\nimplemented, critically evaluated and improved in\nthe RoBERTa model (Liu et al., 2019).\nThese large-scale attention-based models pro-\nvide the advantage of being able to solve NLP\ntasks by having a common, expensive pre-training\nphase, followed by a smaller ﬁne-tuning phase.\nThe pre-training happens in an unsupervised way\nby providing large corpora of text in the desired\nlanguage. The second phase only needs a rela-\ntively small annotated dataset for ﬁne-tuning to\noutperform previous popular approaches in one of\na large number of possible language tasks.\nWhile language models are usually trained on\nEnglish data, some multilingual models also ex-\nist. These are usually trained on a large quan-\ntity of text in different languages. For example,\nMultilingual-BERT is trained on a collection of\ncorpora in 104 different languages (Devlin et al.,\n2019), and generalizes language components well\nacross languages (Pires et al., 2019). However,\nmodels trained on data from one speciﬁc language\nusually improve the performance of multilingual\nmodels for this particular language (Martin et al.,\n2019; de Vries et al., 2019). Training a RoBERTa\nmodel (Liu et al., 2019) on a Dutch dataset thus\nalso potentially increases performances for many\ndownstream Dutch NLP tasks. In this paper, we\nintroduce RobBERT 1, a Dutch RoBERTa-based\npre-trained language model, and critically evaluate\nits performance on various language tasks against\n1The model named itself RobBERT when it was\nprompted with “Ik heet <mask>BERT. ” (“My name is\n<mask>BERT. ”), which we found quite a suitable name.\n3256\nother Dutch languages models. We also pro-\npose several new tasks for testing the model’s ze-\nroshot ability, evaluate its performance on smaller\ndatasets, and for measuring the importance of a\nlanguage-speciﬁc tokenizer. Finally, we provide\nan extensive fairness evaluation using recent tech-\nniques and a new translated dataset.\n2 Related Work\nTransformer models have been successfully used\nfor a wide range of language tasks. Initially, trans-\nformers were introduced for use in machine trans-\nlation, where they efﬁciently improved the state-\nof-the-art (Vaswani et al., 2017). This cornerstone\nwas used in BERT, a transformer model obtain-\ning state-of-the-art results for eleven natural lan-\nguage processing tasks, such as question answer-\ning and natural language inference (Devlin et al.,\n2019). BERT is pre-trained with large corpora of\ntext using two unsupervised tasks. The ﬁrst task is\ncalled masked language modeling (MLM), mak-\ning the model guess which word is masked in cer-\ntain position in the text. The second task is next\nsentence prediction, in which the model has to pre-\ndict if two sentences occur subsequent in the cor-\npus, or randomly sampled from the corpus. These\ntasks allow the model to create internal represen-\ntations about a language, which could thereafter\nbe reused for different language tasks. This archi-\ntecture has been shown to be a general language\nmodel that could be ﬁne-tuned with little data in\na relatively efﬁcient way for a very distinct range\nof tasks and still outperform previous architectures\n(Devlin et al., 2019).\nTransformer models are also capable of gen-\nerating contextualized word embeddings (Peters\net al., 2018). Traditional word embeddings, e.g.\nword2vec (Mikolov et al., 2013) and GloVe (Pen-\nnington et al., 2014), lack the capibility of differ-\nentiating words based on context (e.g. “a stick”\nversus “let’s stick to”). Transformer models, like\nBERT, on the other hand automatically incorpo-\nrate the context a word occurs into its embedding.\nThe attention mechanism in transformer en-\ncoder models also allows for better resolution of\ncoreferences between words (Joshi et al., 2019a).\nFor example, in the sentence “The trophy doesn’t\nﬁt in the suitcase because it’s too big. ”, the word\n“it” would refer to the the suitcase instead of the\ntrophy if the last word was changed to “small”\n(Levesque et al., 2012). Being able to resolve\nthese coreferences is for example important for\ntranslation, as dependent words might change\nform, e.g. due to word gender.\nWhile BERT has been shown to be a power-\nful language model, it also received scrutiny on\nits training and pre-processing. The authors of\nRoBERTa (Liu et al., 2019) showed that while\nthe NSP pre-training task made the model perform\nbetter, it was not due to its intended reason, as it\nmight merely predict relatedness between corpus\nsentences rather than subsequent sentences. That\nDevlin et al. (2019) trained a better model when\nusing NSP than without NSP is likely due to the\nmodel learning long-range dependencies that were\nlonger than when just using single sentences. As\nsuch, the RoBERTa model uses only the MLM\ntask, and uses multiple full sentences in every in-\nput. Other researchers later improved the NSP\ntask by instead making the model predict for two\nsubsequent sentences if they occur in the given or\nﬂipped order in the corpus (Lan et al., 2019).\nDevlin et al. (2019) also presented a multi-\nlingual model (mBERT) with the same archi-\ntecture as BERT, but trained on Wikipedia cor-\npora in 104 languages. Unfortunately, the qual-\nity of these multilingual embeddings is consid-\nered worse than their monolingual counterparts,\nas R ¨onnqvist et al. (2019) illustrated for German\nand English models in a generative setting. The\nmonolingual French CamemBERT model (Mar-\ntin et al., 2019) also outperformed mBERT on all\ntasks. Brandsen et al. (2019) also outperformed\nmBERT on several Dutch tasks using their Dutch\nBERT-based language model, called BERT-NL,\ntrained on the small SoNaR corpus (Oostdijk et al.,\n2013a). More recently, de Vries et al. (2019)\nalso showed similar results for Dutch using their\nBERTje model, outperforming multilingual BERT\nin a wide range of tasks, such as sentiment analy-\nsis and part-of-speech tagging by pre-training on\nmultiple corpora. Since both these works are con-\ncurrent with ours, we compare our results with\nBERTje and BERT-NL in this paper.\n3 Pre-training RobBERT\nWe pre-trained RobBERT using the RoBERTa\ntraining regime. We trained two different versions,\none where only the pre-training corpus was re-\nplaced with a Dutch corpus(RobBERT v1) and one\nwhere both the corpus and the tokenizer were re-\nplaced with Dutch versions (RobBERT v2). These\n3257\ntwo versions allow to evaluate the importance of\nhaving a language-speciﬁc tokenizer.\n3.1 Data\nWe pre-trained our model on the Dutch section of\nthe OSCAR corpus, a large multilingual corpus\nwhich was obtained by language classiﬁcation in\nthe Common Crawl corpus (Ortiz Su ´arez et al.,\n2019). This Dutch corpus is 39GB large, with\n6.6 billion words spread over 126 million lines of\ntext, where each line could contain multiple sen-\ntences. This corpus is thus much larger than the\ncorpora used for similar Dutch BERT models, as\nBERTje used a 12GB corpus, and BERT-NL used\nthe SoNaR-500 corpus (about 2.2GB). (de Vries\net al., 2019; Brandsen et al., 2019).\n3.2 Tokenizer\nFor RobBERT v2, we changed the default byte\npair encoding (BPE) tokenizer of RoBERTa to a\nDutch tokenizer. The vocabulary of the Dutch\ntokenizer was constructed using the Dutch sec-\ntion of the OSCAR corpus (Ortiz Su ´arez et al.,\n2019) with the same byte-level BPE algorithm as\nRoBERTa (Liu et al., 2019). This tokenizer grad-\nually builds its vocabulary by replacing the most\ncommon consecutive tokens with a new, merged\ntoken. We limited the vocabulary to 40k words,\nwhich is 10k words less than RobBERT v1, due to\nadditional tokens including non-negligible num-\nber of Unicode tokens that are not used in Dutch.\nThese are likely caused due to misclassiﬁed sen-\ntences during the creation of the OSCAR cor-\npus (Ortiz Su´arez et al., 2019).\n3.3 Training\nRobBERT shares its architecture with RoBERTa’s\nbase model, which itself is a replication and\nimprovement over BERT (Liu et al., 2019).\nLike BERT, it’s architecture consists of 12 self-\nattention layers with 12 heads (Devlin et al., 2019)\nwith 117M trainable parameters. One difference\nwith the original BERT model is due to the differ-\nent pre-training task speciﬁed by RoBERTa, us-\ning only the MLM task and not the NSP task.\nDuring pre-training, it thus only predicts which\nwords are masked in certain positions of given\nsentences. The training process uses the Adam op-\ntimizer (Kingma and Ba, 2017) with polynomial\ndecay of the learning rate lr = 10−6 and a ramp-\nup period of 1000 iterations, with hyperparame-\nters β1 = 0.9 and RoBERTa’s defaultβ2 = 0.98.\nAdditionally, a weight decay of 0.1 and a small\ndropout of 0.1 helps prevent the model from over-\nﬁtting (Srivastava et al., 2014).\nRobBERT was trained on a computing cluster\nwith 4 Nvidia P100 GPUs per node, where the\nnumber of nodes was dynamically adjusted while\nkeeping a ﬁxed batch size of 8192 sentences. At\nmost 20 nodes were used (i.e. 80 GPUs), and\nthe median was 5 nodes. By using gradient ac-\ncumulation, the batch size could be set indepen-\ndently of the number of GPUs available, in order\nto maximally utilize the cluster. Using the Fairseq\nlibrary (Ott et al., 2019), the model trained for\ntwo epochs, which equals over 16k batches in to-\ntal, which took about three days on the computing\ncluster. In between training jobs on the comput-\ning cluster, 2 Nvidia 1080 Ti’s also covered some\nparameter updates for RobBERT v2.\n4 Evaluation\nWe evaluated RobBERT on multiple downstream\nDutch language tasks. For testing text classi-\nﬁcation, we evaluate on sentiment analysis and\non demonstrative and relative pronoun prediction.\nThe latter task helps evaluating the zero-shot pre-\ndiction abilities, i.e. using only the pre-trained\nmodel without any ﬁne-tuning. Both classiﬁca-\ntion tasks are also used to measure how well Rob-\nBERT performs on smaller datasets, by only us-\ning subsets of the data. For testing RobBERT’s\ntoken tagging capabilities, we used both part-of-\nspeech (POS) tagging and named entity recogni-\ntion (NER) tasks.\n4.1 Sentiment Analysis\nWe replicated the high-level sentiment analysis\ntask used to evaluate BERT-NL (Brandsen et al.,\n2019) and BERTje (de Vries et al., 2019) to be able\nto compare our methods. This task uses a dataset\ncalled Dutch Book Reviews dataset (DBRD), in\nwhich book reviews from hebban.nl are la-\nbeled as positive or negative (van der Burgh and\nVerberne, 2019). Although the dataset contains\n118,516 reviews, only 22,252 of these reviews are\nactually labeled as positive or negative, which are\nsplit in a 90% train and 10% test datasets. This\ndataset was released in a paper analysing the per-\nformance of an ULMFiT model (Universal Lan-\nguage Model Fine-tuning for Text Classiﬁcation\nmodel) (van der Burgh and Verberne, 2019).\nWe ﬁne-tuned RobBERT on the ﬁrst 10,000\n3258\nTable 1: Results of RobBERT ﬁne-tuned on several downstream classiﬁcation tasks, compared to the state of the\nart models for the tasks. For accuracy, we also report the 95% conﬁdence intervals.(Results annotated with * from\nvan der Burgh and Verberne (2019), ** from de Vries et al. (2019), *** from Brandsen et al. (2019), **** from\nAllein et al. (2020))\n10k Full dataset\nTask + model ACC (95% CI) [%] F1 [%] ACC (95% CI) [%] F1 [%]\nSentiment Analysis (DBRD)\nvan der Burgh and Verberne (2019) — — 93.8* —\nBERTje (de Vries et al., 2019) — — 93.0** —\nBERT-NL (Brandsen et al., 2019) — — — 84.0***\nRobBERT v1 86.730 (85.32, 88.14) 86.729 94.422 (93.47,95.38) 94.422\nRobBERT v2 94.379 (93.42, 95.33) 94.378 95.144 (94.25,96.04) 95.144\nDie/Dat (Europarl)\nBaseline (Allein et al., 2020) — — 75.03**** —\nmBERT (Devlin et al., 2019) 92.157 (92.06, 92.25) 90.898 98.285 (98.24,98.33) 98.033\nBERTje (de Vries et al., 2019) 93.096 (92.84, 93.36) 91.279 98.268 (98.22,98.31) 98.014\nRobBERT v1 97.006 (96.95, 97.07) 96.571 98.406 (98.36, 98.45) 98.169\nRobBERT v2 97.816 (97.76, 97.87) 97.514 99.232 (99.20, 99.26) 99.121\ntraining examples as well as on the full dataset.\nWhile the ULMFiT model is ﬁrst ﬁne-tuned us-\ning the unlabeled reviews before training the clas-\nsiﬁer (van der Burgh and Verberne, 2019), it is\nunclear whether the other BERT models utilized\nthe unlabeled reviews for further pre-training (Sun\net al., 2019) or only used the labeled data for ﬁne-\ntuning the pre-trained model. We did the latter,\nmeaning further improvement is possible by ad-\nditionally pre-training on unlabeled in-domain se-\nquences. Another unknown is how these models\ndealt with reviews that were longer than the max-\nimum number of tokens, as the average book re-\nview length is 547 tokens, with 40% of the docu-\nments being longer than our model could handle.\nFor our experiments, we only gave the last tokens\nof a review as input, as we found the training per-\nformance to be better, likely due to containing a\nsummarizing comments. We trained our model\nfor 2000 iterations with a batch size of 128 and\na warm-up of 500 iterations, reaching a learning\nrate of 10−5. The training took approx. 2 hours\non 2 Nvidea 1080 Ti GPUs, the best-performing\nRobBERT v2 model was selected based on a val-\nidation accuracy of 0.994. We see that RobBERT\noutperforms the other BERT models. Both ver-\nsions of RobBERT also outperform the state-of-\nthe-art ULMFiT model, although the difference is\nonly statistically signiﬁcant for RobBERT v2.\n4.2 Die/Dat Disambiguation\nSince BERT models perform well on coreference\nresolution tasks (Joshi et al., 2019b), we pro-\npose to evaluate RobBERT on the recently in-\ntroduced “die/dat disambiguation” task (Allein\net al., 2020), as a novel way to evaluate the ze-\nroshot ability of Dutch BERT models. In Dutch,\ndepending on the sentence, both “die” and “dat”\ncan be either demonstrative or relative pronouns;\nin addition they can also be used in a subordinat-\ning conjunction, i.e. to introduce a clause. The use\nof either of these words depends on the gender of\nthe word it refers to. Allein et al. (2020) presented\nmultiple models trained on the Europarl (Koehn,\n2005) and SoNaR corpora (Oostdijk et al., 2013b),\nachieving an accuracy of 75.03% on Europarl to\n84.56% on SoNaR.\nFor this task, we use the Dutch Europarl cor-\npus (Koehn, 2005), with the ﬁrst 1.3M sequences\n(head) for training and last 399k ( tail) as test\nset. Every sequence containing “die” or “dat”\ncreates an example for every occurrence of either\nword by masking the occurrence. For the test set,\nthis resulted in about 289k masked sentences.\nBERT-like models can solve this task using two\ndifferent approaches. Since the task is about pre-\ndicting words, their default MLM task can be used\nto guess which of the two words is more probable\nin a particular masked position. This allows the\ncomparison of zero-shot BERT models, i.e. with-\nout any ﬁne-tuning on the training data (Table 2).\n3259\nThe second approach uses the masked sentences to\ncreate two versions by ﬁlling the mask with either\n“die” and “dat”, separate them using the [SEP]\ntoken and making the model predict which of the\ntwo sentences is correct. This ﬁne-tuning was per-\nformed using 4 Nvidia GTX 1080 Ti GPUs, taking\n30 minutes for 13 epochs on 10k sequences and\nabout 24 hours for 3 epochs on the full dataset. We\ndid no hyperparameter tuning, as the initial hyper-\nparameters (lr = 10−5,ϵ = 10−9, warm-up of 250\nsteps, batch size of 32 (10k) or 128 (full dataset),\ndropout of 0.1) were satisfactory.\nTo measure RobBERTs performance on smaller\ndatasets, we trained the model twice for both the\nsentiment analysis task and the die/dat disam-\nbiguation task, once with a subset of 10k utter-\nances, and once with the full training dataset.\nTable 2: Performance of predicting die/dat as most\nlikely candidate for a mask using zero-shot BERT mod-\nels (i.e. without ﬁne-tuning) as well as a majority class\npredictor (ZeroR), tested on the 288,799 test set sen-\ntences\nModel Accuracy [%]\nZeroR (majority class) 66.70\nmBERT (Devlin et al., 2019) 90.21\nBERTje (de Vries et al., 2019) 94.94\nRobBERT v1 98.03\nRobBERT v2 98.75\nRobBERT outperforms previous models as well\nas other BERT models both with as well as with-\nout ﬁne-tuning (see Table 1 and Table 2). It is also\nable to reach similar performance using less data.\nThe fact that both for the ﬁne-tuned and the zero-\nshot setting, RobBERT outperforms other BERT\nmodels is also an indication that the base model\nhas internalised more knowledge about Dutch than\nthe others, likely due to the improved pre-training\nregime and using a larger corpus. We can also see\nthat having a Dutch tokenizer strongly helps re-\nduce the error rate for this task, halving the error\nrate when ﬁne-tuned on the full dataset. The rea-\nson the BERT-based models outperform the pre-\nvious RNN-based approach is likely the encoders\nability to better deal with coreference resolution\n(Joshi et al., 2019a), and by extension deciding\nwhich word the “die” or “dat” belongs to. The\nfact that RobBERT strongly outperforms the other\nBERT models on subsets of the data indicates that\nit is a suitable candidate for Dutch tasks that only\nhave limited data available.\n4.3 Part-of-speech Tagging\nPart-of-speech (POS) tagging involves labeling to-\nkens rather than labeling sequences. For this, we\nused a different head with an classiﬁcation output\nfor each token, all activated by a softmax function.\nWhen a word consists of multiple tokens, the ﬁrst\ntoken is used for the the label of the word.\nWe perform the same POS ﬁne-tuning regimes\nas RoBERTa (Liu et al., 2019) to evaluate Rob-\nBERT’s performance. When ﬁne-tuning, we em-\nploy a linearly decaying learning rate with a warm-\nup for 6% of the total optimisation steps (Liu et al.,\n2019). For all the encoder-based models in our\nevaluation, we also perform a limited hyperparam-\neter search on the development set with learning\nrate lr ∈ {10−5,2 ·10−5,3 ·10e−5,10−4}and\nbatch size ∈{16,32,48}, which is also based on\nRoBERTa’s ﬁne-tuning.\nTable 3: POS tagging on Lassy UD. For accuracy, we\nalso report the 95% conﬁdence intervals.\nTask + model ACC (95% CI) [%]\nFrog (Bosch et al., 2007) 91.7 (91.2, 92.2)\nmBERT (Devlin et al., 2019)96.5(96.2, 96.9)\nBERTje (de Vries et al., 2019) 96.3 (96.0, 96.7)\nRobBERT v1 96.4 (96.0, 96.7)\nRobBERT v2 96.4 (96.0, 96.7)\nTo evaluate the POS-performance, we used\nthe Universal Dependencies (UD) version of the\nLassy dataset (Van Noord et al., 2013), containing\n17 different POS tags. We compared its perfor-\nmance with Frog, a popular memory-based Dutch\nPOS tagging approach, and with other BERT mod-\nels. Surprisingly, multilingual BERT marginally\noutperformed both Dutch BERT models, although\nnot statistically signiﬁcantly, with both RobBERT\nmodels in second place with an almost equal ac-\ncuracy. The higher performance of multilingual\nBERT could be indicative that it beneﬁts from\ntransferable language structures from other lan-\nguages helping it to perform well for POS tagging.\nAlternatively, this could signal a limit of the UD\nLassy dataset, or at least for the performance of\nBERT-like models on this dataset.\nWe also evaluated the models on several smaller\nsubsets of the training data, to illustrate how much\ndata is needed to achieve acceptable results. For\nall models, the same hyperparameters obtained for\n3260\nRobBERT v1\nRobBERT v2\nmBERT\nBERTje\nAccuracy\n0\n0,2\n0,4\n0,6\n0,8\n1,0\n# of labeled sequences\n102 103 104\nAccuracy on POS tagging in function of training size\nFigure 1: POS tagging accuracy on the test set for dif-\nferent sizes of training sets.\nTable 3 are used for all subsets, under the as-\nsumption that using a subset of the training data\nalso works well under the same hyperparameters.\nThe hyperparameters which yielded the results of\nRobBERT v2 are lr = 10−4, batch size of 16\nand dropout of 0.1. The separate development set\nwas used to select the best-performing model after\neach epoch based , which had a cross-entropy loss\nof 0.172 on the development set. While all BERT\nmodels perform similarly after seeing all instances\nof the UD Lassy dataset, there is a clear difference\nwhen using smaller training sets (Figure 1). Rob-\nBERT v2 outperforms all other models when using\nonly 1,000 data points or less, again showing that\nit is more capable of dealing with smaller datasets.\n4.4 Named Entity Recognition\nNamed entity recognition (NER) is the task of la-\nbeling named entities in a sentence. It is thus a\ntoken-level task, just like POS-tagging, meaning\nwe can use the same setup and hyperparameter\ntuning as described in Subsection 4.3. We use\nthe CoNLL-2002 dataset and evaluation script 2,\nwhich use a four value BIO labeling, namely\nfor organisations, locations, people and miscel-\nlaneous (Tjong Kim Sang, 2002). The hyper-\nparameters yielding the results for RobBERT v2\nare lr = 3 ·10−5, batch size of 32 and dropout\nof 0.1. The separate development set was used\nto select the best-performing model after each\nepoch. As the F1 score is generally used for\nevaluation of this task, we opted to use this met-\nric instead of cross-entropy loss for selecting the\nbest-performing model, which had an F1 score of\n0.8769 on the development set. We compared the\n2Retrieved from https://www.clips.uantwerp\nen.be/conll2002/ner/\nF1 scores on the NER task in Table 4.\nTable 4: NER for various models, F1 score calculated\nwith the CoNLL 2002 evaluation script, except for †\nwhich used the Seqeval Python library, * from Wu and\nDredze (2019), ** from Brandsen et al. (2019), ***\nfrom de Vries et al. (2019).\nTask + model F1 score [%]\nFrog (Bosch et al., 2007) 57.31\nmBERT (Devlin et al., 2019) 84.19\nmBERT (Wu and Dredze, 2019)90.94*\nBERT-NL (Brandsen et al., 2019) 89.7†**\nBERTje (de Vries et al., 2019) 88.3***\nRobBERT v1 87.53\nRobBERT v2 89.08\nWe can see that (Wu and Dredze, 2019) outper-\nforms all other BERT models using a multilingual\nBERT model with an F1 score of 90.94. When\nwe used the token labeling ﬁne-tuning regime de-\nscribed earlier on multilingual BERT, we were\nonly able to get to anF1 score of 84.19 using mul-\ntilingual BERT, thus being outperformed by the\nDutch BERT models. One possibility is that the\nauthors used a more optimal ﬁne-tuning regime,\nor that they trained their model longer.\n5 RobBERT and Fairness\nAs language models are trained on large cor-\npora, this poses a risk that minorities and pro-\ntected groups are ill-represented, e.g. by en-\ncoding stereotypes (Bolukbasi et al., 2016; Zhao\net al., 2019; Gonen and Goldberg, 2019). In\nword embeddings, these studies often rely on\nanalogies (Bolukbasi et al., 2016; Caliskan et al.,\n2017) or embedding analysis (Gonen and Gold-\nberg, 2019). These approaches are not directly\ntransferable to BERT models, since the sentence\nthe word occurs in inﬂuences its embedding.\nEfforts to generalize these approaches often rely\non templates (May et al., 2019; Kurita et al., 2019).\nThese can be intentionally neutral (“<mask>is a\nword”) or they might resemble an analogy in tex-\ntual form (“<mask>is a zookeeper.”). One can\nthen perform an association test between possible\nvalues for the<mask>slot, similar to a word em-\nbedding association test (Caliskan et al., 2017).\nIn this section, we discuss two distinct potential\norigins of representational harm (Blodgett et al.,\n2020) a language model could exhibit, and eval-\nuate these on RobBERT v2. The two discussed\nbehaviours are (i) stereotyping of gender roles in\n3261\nwaard\nlandmeter\nsportman actrice\nzakenmanhuisvrouw\nboxer\nﬁlantroop\neditor\nbaron\nmilitair\nnon\nnon\nkennis\nballerina\nsocialite\nbutler\nschoolhoofd\n<mask> gaat naar een <T>. <mask> werkt als een <T>.\n⇠ She (\"Zij\")     |    He (\"Hij\") ⇢  \n⇡ She (\"Zij\") ranked higher\n⇣ He (\"Hij\") ranked higher\n⇠ She (\"Zij\")     |    He (\"Hij\") ⇢  ⇠ She (\"Zij\")     |    He (\"Hij\") ⇢  \nRanking difference\n−40\n−30\n−20\n−10\n0\nAssociated gender\n−1,0 −0,5 0 0,5 1,0\nAssociated gender\n−1,0 −0,5 0 0,5 1,0\nAssociated gender\n−1,0 −0,5 0 0,5 1,0\n<mask> is een <T>.\nFigure 2: Ranking difference between gendered pro-\nnouns for various professions. Three templates were\nused to evaluate, were <T> is replaced by a profes-\nsion. In the leftmost template, the pronoun and profes-\nsion refer to different entities.\noccupations and (ii) unequal predictive power for\ntexts written by men and women. These exempliﬁ-\ncations highlight how language models risk affect-\ning the experience of the end user, or replicating\nand reinforcing stereotypes.\n5.1 Gender Stereotyping\nTo assess how gender stereotypes of professions\nare present, we performed a template-based asso-\nciation test similar to Kurita et al. (2019) and the\nsemantically unbleached templates of May et al.\n(2019). We used RobBERT’s LM head—trained\nduring pre-training with the MLM task—to ﬁll in\nthe <mask> slot for each template, in the same\nmanner as the zero-shot task described in Sub-\nsection 4.2. These templates have a second slot,\nwhich is used to iterate over the professions.\nFor this list of professions and the gender pro-\njection on the he-she axis, we base us on the work\nby Bolukbasi et al. (2016), who crowdsourced the\nassociated gender for various professions. Ideally,\nwe would use a similarly crowdsourced Dutch\ndataset. However, since this does not yet exist,\nwe opted for manually translating these English\nprofessions using the guidelines established by\nthe European Parliament for gender neutral pro-\nfessions (Dimitrios Papadimoulis, 2018), meaning\nthat we opted for the inclusive form for neutral\nprofessions in English that do not have a neutral\ncounterpart, but an inclusive binary male variant\nand a female variant with explicit gender (e.g. for\nlawyer: using “advocaat” and not “advocate”).\nIn the rare case that an inclusive or neutral form\ntranslated to an exclusive binary form, we ex-\ncluded this profession.\nWe evaluated three templates on RobBERT, in-\ncluding one control template without co-referent\nentities (“<mask> goes to a <T>”) (Figure 2).\nFor the control template, there should be no and\nindeed is no correlation between ranking differ-\nence for both pronouns and the associated gen-\nder of a profession. Interestingly, none of the in-\nstances has a positive ranking difference, meaning\nthe language model always ranks the male pro-\nnoun as more likely.\nWhen the profession and <mask>slot refer to\nthe same entity, the general assessment of the lan-\nguage model correlates with the associated gender.\nBut again, RobBERT estimates that the male pro-\nnoun is more likely in almost all cases, even when\nthese professions have a gendered sufﬁx. Curi-\nously, actress (“actrice”) is the only word where\nthis is not the case. Since RobBERT estimates the\nmale pronoun to be more likely even in the control\ntemplate, we suspect that the effect is due to more\ncoverage of men in the training corpus.\n5.2 Unequal Predictive Performance\nUnfairness is particularly problematic if it leads\nto unequal predictive performance. This prob-\nlem has been demonstrated for decision support\nsystems, including recidivism prediction (Angwin\net al., 2016) and public employment services (All-\nhutter et al., 2020). Such predictions can be down-\nstream tasks of language understanding; for exam-\nple when job resum ´es are processed (Van Hautte\net al., 2020).\nTo review fairness in downstream tasks, we\nevaluated the sentiment analysis task on DBRD, a\ndataset with scraped book reviews. Although this\ntask in itself may have low impact for end users, it\nstill serves as an illustrative example of how ﬁne-\ntuned models can behave unfairly.\nTo investigate whether such bias might result\nfor our ﬁne-tuned model, we analyzed its out-\ncome for different values of a sensitive attribute\n(in this case gender), as is commonly done in\nfair machine learning research (Zemel et al., 2013;\nHardt et al., 2016; Delobelle et al., 2020). To this\nend, we augmented the held-out test set of DBRD\nwith gender as a sensitive attribute for each re-\nview3. Values were obtained from the reviews’\nauthor proﬁles with a self-reported binary gender\n(‘man’or ‘vrouw’) (64%). The remaining 36%\nof reviews did not report author gender, and they\nwere discarded for this evaluation. Of the remain-\ning, gender-labelled, reviews, 76% were written\n3We make this augmentation of DBRD available under\nCC-by-NC-SA at https://people.cs.kuleuven.b\ne/˜pieter.delobelle/data.html.\n3262\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nPositive (> 3 stars)\nMale (AUC = 0.98)\nFemale (AUC = 0.98)\nThreshold (male)\nThreshold (female)\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nHighly positive (5 stars)\nMale (AUC = 0.76)\nFemale (AUC = 0.83)\nThreshold (male)\nThreshold (female)\nFigure 3: ROC of the ﬁne-tuned model to predict posi-\ntive reviews for male and female reviewers\nby women. Thus, the dataset is unbalanced.\nWe quantify the gender difference with two\nmetrics: (i) Demographic Parity Ratio (DPR),\nwhich expresses a relative difference between pre-\ndicted outcomes ˆyconditioned on the sensitive at-\ntribute a(Dwork et al., 2012), following\nP(ˆy|¬a)\nP(ˆy|a) ,\nand (ii) Equal Opportunity (EO) Hardt et al.\n(2016), which in addition also conditions on the\ntrue outcome y, as a task-speciﬁc fairness measure\n(Dwork et al., 2012), following\nP(ˆy|¬a,y) −P(ˆy|a,y).\nHardt et al. (2016) also relate EO to the ROC\ncurves to evaluate fairness when dealing with a bi-\nnary predictor and a score function. To derive a\nbinary predictor, we used 0 as a threshold value.\nFigure 3 shows the single resulting predictor, with\nthe ROC curves split on the sensitive attribute, for\neach of the two rating levels (over 3 resp. 5 stars).\nThe results of Figure 3 show that there is small\ndifference in opportunity, which is especially pro-\nnounced for the highly positive classiﬁer. For pos-\nitive reviews, the EO difference is 0.028 at the in-\ndicated threshold and DPR is 70.2%. The DPR\nwould indicate an unfairness, as values below 80%\nare often considered unfair. However, this metric\nhas received some criticism, and when including\nthe true outcome in EO, the difference in probabil-\nities is close to 0, which does not signal any unfair-\nness. When taking into account the ROC curves\n(Figure 3), the EO score can be explained by the\ngood predictive performance. When considering\nhighly positive reviews, however, the differences\nbecome more pronounced and the model has bet-\nter predictive performance for reviews written by\nwomen.\n6 Code\nThe training and evaluation code of this paper as\nwell as the RobBERT model and the ﬁne-tuned\nmodels are publicly available for download at ht\ntps://github.com/iPieter/RobBERT.\n7 Limitations and Future Work\nThere are several potential improvements for cre-\nating a better pre-trained RobBERT-like model.\nFirst, since BERT-based models are still being ac-\ntively researched, one could potentially improve\nthe training regime using new unsupervised pre-\ntraining tasks when they are discovered, e.g. sen-\ntence order prediction (Lan et al., 2019). Sec-\nond, while RobBERT is trained on lines that con-\ntain multiple sentences, it does not put subsequent\nlines of the corpus after each other due to the shuf-\nﬂed nature of the OSCAR corpus (Ortiz Su ´arez\net al., 2019). This is unlike RoBERTa, which does\nput full sentences next to one another if they do\nnot exceed the available sequence length, in or-\nder to learn the long-range dependencies between\nwords that the original BERT learned using its\ncontroversial NSP task. Creating an unshufﬂed\nversion of OSCAR might thus further improve\nthe performance of the pre-trained model. Third,\nthere might be some beneﬁt to modifying the to-\nkenizer to use morpheme-based tokens, as Dutch\nuses compound words. Fourth, one could improve\nmodel’s fairness during pre-training. We illus-\ntrated how representational harm in downstream\ntasks can affect the end user’s experience, like\nthe unequal predictive performance for the DBRD\ntask. Various methods have been proposed to mit-\nigate unfair behaviour in AI models (Zemel et al.,\n2013; Delobelle et al., 2020). While we refrained\nfrom training fair pre-trained and ﬁne-tuned mod-\nels in this research, training such models could be\nan interesting contribution. In addition, with the\nincreased attention on fairness in machine learn-\ning, a broader view of the impact on other pro-\ntected groups due to large pre-trained language\nmodels is also called-for.\nThe RobBERT model itself can be used in new\nsettings to help future research. First, RobBERT\ncould be used in a model that uses a BERT-like\ntransformer stack for the encoder and a genera-\ntive model as a decoder (Raffel et al., 2019; Lewis\net al., 2019) Second, RobBERT can serve as the\nbasis for a large number of Dutch language tasks\nthat we did not examine in this paper. Given Rob-\n3263\nBERT’s state-of-the-art performance on small as\nwell as on large datasets, it could help advance re-\nsults when ﬁne-tuned on new datasets.\n8 Conclusion\nWe introduced a new language model for Dutch\nbased on RoBERTa, called RobBERT, and showed\nthat it outperforms earlier approaches as well as\nother BERT-based language models for a several\ndifferent Dutch language tasks. More speciﬁ-\ncally, we found that RobBERT signiﬁcantly out-\nperformed other BERT-like models when dealing\nwith smaller datasets, making it a useful resource\nfor a large range of application domains. We ex-\npect this model to serve as a base for ﬁne-tuning\non other tasks, and thus help foster new models\nthat can advance results for Dutch language tasks.\nAcknowledgements\nPieter Delobelle was supported by the Research\nFoundation - Flanders under EOS No. 30992574\nand received funding from the Flemish Gov-\nernment under the “Onderzoeksprogramma Arti-\nﬁci¨ele Intelligentie (AI) Vlaanderen” programme.\nThomas Winters is a fellow of the Research\nFoundation-Flanders (FWO-Vlaanderen). Most\ncomputational resources and services used in this\nwork were provided by the VSC (Flemish Super-\ncomputer Center), funded by the Research Foun-\ndation - Flanders (FWO) and the Flemish Govern-\nment – department EWI. We are especially grate-\nful to Luc De Raedt for his guidance as well as\nfor providing the facilities to complete this project.\nWe are thankful to Liesbeth Allein and her super-\nvisors for inspiring us to use the die/dat task. We\nare also grateful to Ott et al. (2019); Paszke et al.\n(2019); Haghighi et al. (2018); Wolf et al. (2019)\nfor their software packages.\nReferences\nLiesbeth Allein, Artuur Leeuwenberg, and Marie-\nFrancine Moens. 2020. Binary and Multitask Clas-\nsiﬁcation Model for Dutch Anaphora Resolution:\nDie/Dat Prediction. arXiv:2001.02943 [cs].\nDoris Allhutter, Florian Cech, Fabian Fischer, Gabriel\nGrill, and Astrid Mager. 2020. Algorithmic Proﬁl-\ning of Job Seekers in Austria: How Austerity Poli-\ntics Are Made Effective. Front. Big Data, 3:5.\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren\nKirchner. 2016. Machine bias.\nSu Lin Blodgett, Solon Barocas, Hal Daum ´e III, and\nHanna Wallach. 2020. Language (Technology)\nis Power: A Critical Survey of ”Bias” in NLP.\narXiv:2005.14050 [cs].\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems ,\npages 4349–4357.\nAntal van den Bosch, Bertjan Busser, Sander Canisius,\nand Walter Daelemans. 2007. An efﬁcient memory-\nbased morphosyntactic tagger and parser for dutch.\nLOT Occasional Series, 7:191–206.\nAlex Brandsen, Anne Dirkson, Suzan Verberne, Maya\nSappelli, Dung Manh Chu, and Kimberly Stoutjes-\ndijk. 2019. BERT-NL a set of language models pre-\ntrained on the Dutch SoNaR corpus.\nAylin Caliskan, Joanna J Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. arXiv:1912.09582 [cs].\nPieter Delobelle, Paul Temple, Gilles Perrouin, Benoˆıt\nFr´enay, Patrick Heymans, and Bettina Berendt.\n2020. Ethical Adversaries: Towards Mitigat-\ning Unfairness with Adversarial Machine Learning.\narXiv:2005.06852 [cs, stat].\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDimitrios Papadimoulis. 2018. Genderneutraal taalge-\nbruik in het Europees Parlement. Technical report,\nEuropean Parlement.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer\nReingold, and Richard Zemel. 2012. Fairness\nThrough Awareness. In 3rd Innovations in Theoret-\nical Computer Science Conference, pages 214–226.\nACM.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 609–614,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\n3264\nSepand Haghighi, Masoomeh Jasemi, Shaahin Hess-\nabi, and Alireza Zolanvari. 2018. PyCM: Multiclass\nconfusion matrix library in Python. Journal of Open\nSource Software, 3(25):729.\nMoritz Hardt, Eric Price, ecprice, and Nati Srebro.\n2016. Equality of Opportunity in Supervised Learn-\ning. In NIPS, pages 3315–3323. Curran Associates.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019a.\nSpanbert: Improving pre-training by representing\nand predicting spans.\nMandar Joshi, Omer Levy, Daniel S Weld, and Luke\nZettlemoyer. 2019b. Bert for coreference reso-\nlution: Baselines and analysis. arXiv preprint\narXiv:1908.09091.\nDiederik P. Kingma and Jimmy Ba. 2017.\nAdam: A Method for Stochastic Optimization.\narXiv:1412.6980 [cs].\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT Summit, vol-\nume 5, pages 79–86. [object Object].\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W\nBlack, and Yulia Tsvetkov. 2019. Measuring bias\nin contextualized word representations. In Proceed-\nings of the First Workshop on Gender Bias in Natu-\nral Language Processing, pages 166–172, Florence,\nItaly. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSu´arez, Yoann Dupont, Laurent Romary, ´Eric Ville-\nmonte de la Clergerie, Djam ´e Seddah, and Beno ˆıt\nSagot. 2019. CamemBERT: A Tasty French Lan-\nguage Model. arXiv:1911.03894 [cs].\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On\nMeasuring Social Biases in Sentence Encoders.\narXiv:1903.10561 [cs].\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nNelleke Oostdijk, Martin Reynaert, V ´eronique Hoste,\nand Ineke Schuurman. 2013a. The construction of a\n500-million-word reference corpus of contemporary\nwritten dutch. In Essential speech and language\ntechnology for Dutch, pages 219–247. Springer.\nNelleke Oostdijk, Martin Reynaert, V ´eronique Hoste,\nand Ineke Schuurman. 2013b. The construction of a\n500-million-word reference corpus of contemporary\nwritten Dutch. In Essential Speech and Language\nTechnology for Dutch: Results by the STEVIN-\nProgramme, chapter 13. Springer Verlag.\nPedro Javier Ortiz Su ´arez, Benoˆıt Sagot, and Laurent\nRomary. 2019. Asynchronous Pipeline for Process-\ning Huge Corpora on Medium to Low Resource In-\nfrastructures. In 7th Workshop on the Challenges\nin the Management of Large Corpora (CMLC-7) ,\nCardiff, United Kingdom.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. Fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. PyTorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems ,\npages 8024–8035.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? arXiv\npreprint arXiv:1906.01502.\n3265\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the Lim-\nits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. arXiv:1910.10683 [cs, stat].\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual BERT ﬂu-\nent in language generation? In Proceedings of the\nFirst NLPL Workshop on Deep Learning for Natural\nLanguage Processing, pages 29–36, Turku, Finland.\nLink¨oping University Electronic Press.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Compu-\ntational Linguistics, pages 194–206. Springer.\nErik F. Tjong Kim Sang. 2002. Introduction to\nthe conll-2002 shared task: Language-independent\nnamed entity recognition. In Proceedings of the 6th\nConference on Natural Language Learning - Volume\n20, COLING-02, page 1–4, USA. Association for\nComputational Linguistics.\nBenjamin van der Burgh and Suzan Verberne. 2019.\nThe merits of Universal Language Model Fine-\ntuning for Small Datasets – a case with Dutch book\nreviews. arXiv:1910.00896 [cs].\nJeroen Van Hautte, Vincent Schelstraete, and Mika ¨el\nWornoo. 2020. Leveraging the inherent hierarchy of\nvacancy titles for automated job ontology expansion.\nIn Proceedings of the 6th International Workshop\non Computational Terminology, pages 37–42, Mar-\nseille, France. European Language Resources Asso-\nciation.\nGertjan Van Noord, Gosse Bouma, Frank Van Eynde,\nDaniel De Kok, Jelmer Van der Linde, Ineke Schuur-\nman, Erik Tjong Kim Sang, and Vincent Vandeghin-\nste. 2013. Large scale syntactic annotation of writ-\nten Dutch: Lassy, pages 147–164. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998–6008. Curran As-\nsociates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Be-\ncas: The Surprising Cross-Lingual Effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi,\nand Cynthia Dwork. 2013. Learning fair represen-\ntations. In International Conference on Machine\nLearning, pages 325–333.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender Bias in Contextualized Word Embeddings.\narXiv:1904.03310 [cs].",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8532107472419739
    },
    {
      "name": "Language model",
      "score": 0.7739695310592651
    },
    {
      "name": "Natural language processing",
      "score": 0.6247937679290771
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6109669804573059
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5822412371635437
    },
    {
      "name": "Language understanding",
      "score": 0.5149856209754944
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.46288251876831055
    },
    {
      "name": "Implementation",
      "score": 0.4560929834842682
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4441889524459839
    },
    {
      "name": "Programming language",
      "score": 0.1271786093711853
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    }
  ]
}