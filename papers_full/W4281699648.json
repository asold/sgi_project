{
    "title": "Experimental Study of Language Models of \"Transformer\" in the Problem of Finding the Answer to a Question in a Russian-Language Text",
    "url": "https://openalex.org/W4281699648",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2069935008",
            "name": "Denis Galeev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282445114",
            "name": "Vladimir Panishchev",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3175000455",
        "https://openalex.org/W3200663532",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3100439847",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2983128379",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2996544478",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3035207248",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W3117122667",
        "https://openalex.org/W3171778302",
        "https://openalex.org/W3105234097",
        "https://openalex.org/W6600297362",
        "https://openalex.org/W3088049945"
    ],
    "abstract": "The aim of the study is to obtain a more lightweight language model that is comparable in terms of EM and F1 with the best modern language models in the task of finding the answer to a question in a text in Russian. The results of the work can be used in various question-and-answer systems for which response time is important. Since the lighter model has fewer parameters than the original one, it can be used on less powerful computing devices, including mobile devices. In this paper, methods of natural language processing, machine learning, and the theory of artificial neural networks are used. The neural network is configured and trained using the Torch and Hugging face machine learning libraries. In the work, the DistilBERT model was trained on the SberQUAD dataset with and without distillation. The work of the received models is compared. The distilled DistilBERT model (EM 58,57 and F1 78,42) was able to outperform the results of the larger ruGPT-3-medium generative network (EM 57,60 and F1 77,73), despite the fact that ruGPT-3-medium had 6,5 times more parameters. The model also showed better EM and F1 metrics than the same model, but to which only conventional training without distillation was applied (EM 55,65, F1 76,51). Unfortunately, the resulting model lags further behind the larger robert discriminative model (EM 66,83, F1 84,95), which has 3,2 times more parameters. The application of the DistilBERT model in question-and-answer systems in Russian is substantiated. Directions for further research are proposed.",
    "full_text": "УДК 004.912 DOI 10.15622/ia.21.3.3 \n \nД.Т. ГАЛЕЕВ, В.С. ПАНИЩЕВ \nЭКСПЕРИМЕНТАЛЬНОЕ ИССЛЕДОВАНИЕ ЯЗЫКОВЫХ \nМОДЕЛЕЙ \"ТРАНСФОРМЕР\" В ЗАДАЧЕ НАХОЖДЕНИЯ \nОТВЕТА НА ВОПРОС В РУССКОЯЗЫЧНОМ ТЕКСТЕ \n \nГалеев Д.Т., Панищев В.С. Экспериментальное исследование языковых моделей \n\"трансформер\" в задаче нахождения ответа на вопрос в русскоязычном тексте. \nАннотация. Целью исследования является получение более легковесной языковой \nмодели, которая сравнима по показателям EM и F -меры с лучшими современными \nязыковыми моделям в задаче нахождения ответа на вопрос в тексте на русском языке. \nРезультаты работы могут найти применение в различных вопросно -ответных системах, \nдля которых важно время отклика. Поскольку более легковесная модель имеет меньшее \nколичество параметров чем ориги нальная, она может быть использована на менее \nмощных вычислительных устройствах, в том числе и на мобильных устройствах. В \nнастоящей работе используются методы обработки естественного языка, машинного \nобучения, теории искусственных нейронных сетей. Нейронная сеть настроена и обучена \nс использованием библиотек машинного обучения Torch и Hugging face. В работе было \nпроведено обучение модели DistilBERT на наборе данных SberQUAD с применением \nдистилляции и без. Произведено сравнение работы полученных моделей.Обученная в \nходе дистилляции модель DistilBERT (EM 58,57 и F -мера 78,42) смогла опередить \nрезультаты более крупной генеративной сети ruGPT-3-medium (EM 57,60 и F-мера 77,73) \nпритом, что ruGPT -3-medium имеет в 6,5 раз больше параметров. Также модель \nпродемонстри ровал а лу чшие  показ ате ли EM и F-ме ра, че м та же  моде ль, но к которой  \nприменялось только обычное дообучение без дистилляции (EM 55,65, F -мера 76,51). К \nсожалению, полученная модель сильнее отстаёт от более крупной дискриминационной \nмодели ruBERT (EM 66,8 3, F-мера 84,95), которая имеет в 3,2 раза больше параметров. \nПредложены направления для дальнейшего исследования.  \nКлючевые слова:  машинное обучение, глубокое обучение, нейронные сети, \nобработка естественного языка, трансформер. \n \n1. Введение. С появлением  архитектуры «трансформер» вся \nиндустрия обработки естественного языка получила значительны й \nскачок в результатах. В работах [1 -3] показано каких успехов смогли \nдобиться различные модели данной архитектуры. Результаты \n«трансформера» поспособствовали появле нию таких моделей как: \nBART [4], T5 [5], Pegasus [6], ProphetNet [7]. Также исследования \nпоказали, что использование двух отдельных частей трансформера, \nкодировщика и декодера, отдельно друг от друга также позволяют \nполучить выдающийся результат. Это способствовало появлению \nбольшого числа моделей , состоящих только из кодировщика : BERT  \n[8], ALBERT [9], RoBERTa [10], а также способствовало появлению \nмоделей, состоящих только из декодера: GPT -3 [11], CTRL [12], \nTransformer-XL [13]. \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n521\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\nОдной из основных проблем использования данных моделей \nявляется большое количество параметров. Чем больше количество \nпараметров, тем больше ресурсов необходимо затратить на обучение \nсети и непосредственно на её применение. Данный нюанс может быть \nкритичен для систем , в которых важна  скорость отклика. Также из -за \nнеобходимости в большой вычислительной мощности применение \nданных моделей может быть ограничено только мощным \nоборудованием, что исключает применение моделей на мобильных \nустройствах. Поэтому одним из направлений изучения в о бласти \nобработки естественного языка является нахождение способов по \nуменьшению размерности моделей при сохранении качества \nрезультатов. Одним из основных способов выполнить данную задачу \nявляется дистилляция [14].  \nСуть дистилляции заключается в том, что можно обучить более \nлегковесную модель, которая будет имитировать поведение более \nсложной модели -учителя. В качестве учителя может выступать \nансамбль моделей. Модель DistilBERT показала отличные результаты \nпо сравнению с оригинальной версией [15].  \nДля язы ковых моделей существует огромное количество задач \nпо обработке естественного языка, среди которых: текстовый поиск, \nмашинный перевод, написание краткого содержания текста, \nраспознавание именованных сущностей и т.д. Также о дной из \nосновных задач для обработки естественного языка при помощи \nнейронных сетей является создание модели, которая ищет ответ на \nвопрос в тексте.  Под поиском ответа на вопрос в тексте будем \nподразумевать: наличие текста и вопроса к тексту, система должна \nвыбрать в качестве ответа на вопрос непрерывный фрагмент из \nданного текста. В англоязычной литературе данный тип задачи \nназывается Extractive Question Answering  ( извлечение ответа на \nвопрос). Для моделей типа «кодировщик» данная задача является \nзадачей классификации, в которой они пытаются найти ответы на \nвопросы: «Является ли данное слово из текста началом ответа на \nзаданный вопрос?», «Является ли данное слово из текста концом \nответа на заданный вопрос?». Для данной задачи на английском языке \nосновным набором данных является SQuAD  [16]. Для русского языка \nкомпанией Сбер был создан набор данных SberQuAD [17].  \nЦелью исследования является получение более легковесной \nязыковой модели, которая несильно уступает лучшим современным \nязыковым моделям в задаче нахождения ответа на вопрос в тексте на \nрусском языке. \n \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n522\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\n2. Материалы и методы \n2.1. Набор данных для обучения модели. Одним из \nстандартных наборов данных для вопросно -ответных систем на \nанглийском языке является Stanford Question Answering Dataset \n(SQuAD) [16]. Набор данных основан на статьях c сайта Wikipedia. \nСтатьи, которые были взяты в набор, охватывают широкий спектр тем \n– от музыкальных знаменитостей до абстрактных понятий. Вопросы \nоснованы на содержании статьи, и человек способен ответить на них, \nпрочитав текст статьи. Некоторые статьи могут содержать несколько \nвопросов. SQuAD часто используют как бенчмарк для новых языковых \nмоделей. \nНа данный момент в интернете можно найти следующие версии \nэтого набора: SQuAD 1.1 и SQuAD 2.0. \nSQuAD 1. 1 содержит 107785 вопросно-ответных пар, \nоснованных на 5 36 статьях. В данной версии набора присутствуют \nвопросы, на которые есть ответы  в представленных текстах . Данный \nнабор разбит на 3 части: тренировочн ую (80%), a валидационн ую \n(10%) и проверочную (10%). Средняя длина те кста составляет ~755 \nсимвола (120 токенов), средняя длина вопроса составляет ~60 символа \n(10 токенов), средняя длина ответа составляет ~19 символов (3 токена). \nSQuAD 2. 0 содержит все данные из версии 1. 1, но также \nсодержит дополнительные 50 тысяч вопросов,  на которые нет ответов \nв представленных текстах.  Это бы сделано для того , чтобы модель \nучилась понимать ситуации,  когда в тексте нет необходимой \nинформации для ответа на вопрос. \nОсновным набором данных для вопросно- ответных систем на \nрусском языке является SberQuAD . Он основан на русских статьях c \nсайта Wikipedia. Форматом ответов и вопросов он совпадает  \nс SQuAD 1.1 . Данный набор  содержит 45328 тренировочных наборов \nиз текста, вопроса и ответа, 5036 валидационных наборов и 23936 \nпроверочных наборов. К сож алению, ответы на проверочные данные \nне представлены публично, поэтому результаты работы модели \nсравниваются на валидационном наборе.  \nБольшинство вопросов в наборе данных  SberQuAD начинаются \nлибо с вопросительного слова, либо с предлога . Далее представлены \nдесять наиболее распространенных начальных слов: «что», «в», «как», \n«кто», «какие», «когда», «какой», «где», «сколько», «на». Средняя \nдлина текста составляет ~754 символа (102 токена), средняя длина \nвопроса составляет ~64 символа (9 токенов), средняя длина ответа \nсоставляет ~26 символов (4 токена). \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n523\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\nКонкретный экземпляр набора данных  был взят из библиотеки \nHugging face. Каждый экземпляр данных содержит следующие поля: \nсontext, question, answers. \nВ поле с ontext находится текст, к которому будет задан вопрос , \nquestion содержит вопрос к тексту из поля contex , а answers включает в \nсебя поле answer_start, которое содержит индекс начала ответа на \nвопрос, и поле text, которое содержит полный текст ответа. \nПример тренировочных данных представлен на рисунке 1. \n \n \nРис. 1. Пример тренировочных данных \n2.2. Выбор модели . Для опытов было решено использовать \nмодели семейства BERT. Архитектура Bidirectional Encoder \nRepresentations from Transformers является одной из самых популярных \nмоделей для обработки естественного языка [8] , поскольку она \nпоказывает одни из лучших результатов среди аналогов. В данной \nмодели присутствует кодировщик из архитектуры трансформер [1].  \nИзначально модель BERT обучалась одновременно на двух задачах: \n1) предсказание пропущенных слов в тексте (masked language \nmodeling); \n2) определения является ли вторая часть текста логичным \nпродолжением первой (next sentence prediction). \nВ оригинальной статье было описано две версии модели: \n1) BERT BASE. Данная версия модели содержит 12 блоков \nтрансформера. В каждом блоке трансформера содержится 12 «голов» \nвнимания. Для каждого входного вектора на выход подаётся вектор с \nдлиной 768. Модель содержит 110 миллионов параметров. \n2) BERT LARGE. Данная версия модели содержит 24 блока \nтрансформера. В каждом блоке трансформера содержится 16 «голов» \nвнимания. Для каждого входного вектора на выход подаётся вектор с \nдлиной 1024. Модель содержит 340 миллионов параметров. \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n524\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\nОсобенностью современных языковых моделей является то, что \nони обычно выкладываются в сеть после тренировки на различных \nзадачах языкового моделирования или близкие к ним.  Данное \nобучение происходит на огромных корпусах текстов.  Это значит, что \nданные сети уже содержат вектор ные представления для слов из \nсвоего словаря и способны хорошо решать задачи языкового \nмоделирования. И последующая работа с данными моделями \nзаключается в добавлении поверх сети дополнительного \nполносвязного нейронного слоя с необходимой функцией активации.  \nПосле этого происходит процесс дообучен ия модели под конкретную \nзадачу.  \nОсновные этапы работы модели BERT: \n1) Токенизация входного текста . Добавление первым \nэлементов специального токена [CLS]. \n2) Векторизация полученных токенов. \n3) Применение позиционного кодирования для полученных \nвекторов. \n4) Передача векторов на вход стека блоков трансформера. \n5) Для каждого входного вектора на выход подается \nрезультирующий вектор (размерность которого равна 768 в базовой \nмодели BERT) и, в зависимости от задачи, различные результирующие \nвектора подаются на добавленный «сверху» слой нейронов. \nСтоит отметить, что версия DistilBERT имеет ту же общую \nархитектуру, что и BERT, но в которой произведены некоторые \nупрощения, а количество слоев было уменьшено в 2 раза [15]. \n2.3. Токенизация. Одним из современных алгоритмов для \nтокенизации текстов на естественном языке является Byte Pair \nEncoding (BPE) [18]. Основными шагами BPE являются: \n1. создание словаря из всех символов языка; \n2. представление слов из текста как списка символов; \n3. подсчёт количества вхождений каждой пары символов; \n4. объединение самых частотных пар в токен и добавление \nданного токена в словарь; \n5. повторение пункта 4 до тех пор, пока не будет получен \nсловарь заданного размера. \nУ данного алгоритма есть варианты с различными \nулучшениями, например BPE-Dropout [19]. \nТакже одним из популярных алгоритмов токенизации явл яется \nWordPiece [ 20]. WordPiece устроен похожим образом , так же  как и \nBPE, только объединяются не самые частотные пары токенов, а \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n525\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\nмаксимизирующие правдоподобие униграммной языковой модели . \nИменно данный алгоритм используется в модели BERT. \nПрименение алгоритмов таких как BPE и WordPiece позволяет \nбороться с проблемой отсутствия слова в словаре (out of vocabulary ). \nДанная проблема случается, когда модель не знает входящего слова, \nпоскольку его не было в её словаре на этапе обучения . Теперь \nминимальной частью слова является символ и все символы \nдобавляются в словар ь, а это значит, что  любое слово  сможет быть \nразбито на последовательность токенов, которые знает модель. \n2.4. Векторизация. После того как текст был токенизирован , \nнеобходимо представить полученные токены в формат наиболее \nпонятный для модели. Чаще всего эта задача решается сопоставлением \nкаждого токена с вектором. \nОдним из самых популярных способов для векторизации \nтокенов является word2vec [ 21]. Word2vec — представляет собой \nмалослойную искусственн ую нейронную сеть состоя щую из двух \nслоев, которая обрабатывает текст, преобразуя его в \n«векторизованные» представления. Входными данными для данной \nсети являются большие корпуса текстов, из которых на выходе  \nполучается пространство векторов, размерность которых  обычно не \nпревышает несколько сотен (нет проблемы с большой размерностью \nвекторов), где каждый токен в корпусе представлен вектором из \nсгенерированного пространства. Данные векторы учитывают \nсемантическую близость слов (нет основной проблемы one-hot \nвекторов). Данный способ может быть применён к множеству \nразличных языков в различных задачах [22, 23]. \nДанная модель обычно обучается выполнять задачу языкового \nмоделирования, т.е. модель пытается угадать одно или несколько слов , \nпропущенных в тексте. Для word2vec характерно 2 под хода обучения \n(рисунок 2):  \n1) Continuous Bag of Words  (CBOW) — это метод, в котором \nмодель пытается предсказать целевое слово по слов ам вокруг него . \nCBOW обычно хорошо работает на небольших наборах данных. \n2) Skip-gram — это метод, в котором модель пытается \nпредсказать слова вокруг данного  целевого слова, что в точности \nпротивоположно CBOW. Skip -gram лучше работает на больших \nнаборах данных. \n \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n526\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\n \nРис. 2. Методы CBOW и Skip-gram для обучения векторов токенов word2vec \nТакже для улучшения показателей векторов применяется \nнегативный отбор  (n egative sampling). Данный подход подразумевает \nвключать в процесс обучения пары слов, которые точно не являются \nсоседями. Иначе модель в процессе обучения будет видеть только \nслова, которые являются соседями. \nТакже стоит отметить, что модель BERT обучает контекст но-\nзависимые представления. Это значит, что в зависимости от слов \nвокруг, слову будет сопоставляться нужный вектор. Данный подход \nпозволяет сопоставлять омонимы с различными векторными \nпредставлениями. \n2.5. Блок трансформера. Каждый блок трансформера состоит \nиз следующих последовательных слоёв (рисунок 3): \n1) слой многоголового самовнимания ( multi-head self-\nattention); \n2) слой нормализации; \n3) слой прямого распространения; \n4) слой нормализации. \nВсе блоки  трансформера идентичны по структуре, но имеют \nразные веса. \nИзначально на вход блока трансформера идут все вектора \nтокенов предложения. Сам блок трансформера выдаёт  точно такое же \nколичество векторов, которое он получил на вход.  Следовательно, все \nслои внутри блока возвращают то же  самое количество векторов, \nкоторое они получили на входе. \n \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n527\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\n \nРис. 3. Структура блока трансформера \nСлой прямого распространения необходим для выявления \nнелинейных зависимостей. Слой многоголового самовнимания \nиспользует только линейные функции. Нет смысла передавать \nрезультат от одной линейной функции к другой по цепочке , так как  \nдобавление новых линейных слоев не позволит выявить нелинейные \nзависимости в данных. Эта проблема решается  добавлением сло я с \nнелинейными функциями активации между лин ейными слоями \nвнимания. Обычно применяется функция активации ReLU (1). \n \n. (1) \n \nСамо преобразование из слоя  прямого распространения \nприменяется отдельно к каждому входному вектору. \nНа каждый слой нормализации [24] подаётся сумма векторов из \nрезультата предыдущего слоя (т.е. слоя многоголового самовнимания \nили прямого распространения) с вектором, который был до \nпреобразований из предыдущего слоя. Применение слоя нормализации \nнеобходимо для уменьшения времени обучения модели, а \nсуммирование векторов до и после слоя  с преобразованиями ( skip-\nconnection) необходимо для того, чтобы бороться с проблемой \nзатухающих градиентов. \n2.6. Слой самовнимания. Слой самовнимания кодирует \nвзаимосвязь каждого слова с каждым другим словом в том же самом \nпредложении, сосредотачивая большее внимание на самых значимых \nсловах. А поскольку уделяется внимание предложением самому себе \nто механизм называется самовниманием. На рисунке 4 показано, как \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n528\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\nчасть механизма внимания для слова «он» фокусируется на слове \n«дядя» в предложении « Мой дядя самых честных правил, когда не в \nшутку занемог, он уважать себя заставил и лучше выдумать не мог». \nВначале слой самовнимания  создаёт три матрицы для входного \nпредложения: матрицу запросов (q uery), матрицу ключей (key ) и \nматрицу значений (value). Эти матрицы создаются с помощью \nперемножения вектор ов токенов на три матрицы, которые были \nобучены во время процесса обучения нейронной сети . Для того чтобы \nполучить выходной результат для данной «головы» самовнимания \nнеобходимо провести следующие п реобразования (2) над \nполученными матрицами (рисунок 5): \n \n, (2) \n \nгде dkey – размерность вектора key. \n \nРис. 4. Демонстрация работы слоя самовнимания (программа для \nвизуализации взята в [25]) \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n529\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\n \nРис. 5. Механизм самовнимания \nБольшое распространение получила техника использования \nнескольких параллельных блоков самовнимания  называемая \nмногоголовым самовниманием (рисунок 6). Каждая «голова» \nпредставляет собой отдельный экземпляр механизма самовнимания \n(рисунок 5 ). Применение множества «голов» улучшает \nпроизводительность слоя самовнимания  за счет того, что разные \nбольшее количество «голов» позволяет устанавливать больше связей \nмежду токенами в предложении. \n \n  \nРис. 6. Техника многоголового самовнимания \nРезультаты работы нескольких «голов» внимания \nконкатенируются друг с другом и затем умножаются на матрицу весов, \nкоторая была обучена вместе с моделью. Полученный результат  \nпередаётся в слой нормализации. \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n530\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\n2.7. Позиционное кодирование. У применения механизма \nсамовнимания есть один серьёзный недостаток – данный механизм не \nучитывает порядок слов в предложении. Часто в естественных языках \nпорядок слов в предложении важен и не может быть отброшен или \nизменён. Авторы архитектуры трансформер  предложили использовать \n«позиционное кодирование» для решения этой проблемы. Для этого \nдобавляют специальный вектор в каждый входящий вектор токена. \nЭти векторы имеют определенный шаблон, который запоминает \nмодель и который помогает определить позицию каж дого слова или \nрасстояние между разными словами в предложении. Расчёт вектора \nпроисходит по следующим формулам (3, 4):  \n \n, (3) \n \n, (4) \nгде, pos – это позиция токена, i – это измерение в векторе токена, d\nmodel \n– это длина вектора токена. \n2.8. Описание подхода к поиску ответа на вопрос в тексте с \nиспользованием языковых моделей на базе BERT. Под поиском \nответа на вопрос в тексте будем подразумевать: наличие текста и \nвопроса к тексту, модель должна выбрать в качестве ответа на данный \nвопрос непре рывный фрагмент из данного текста . Так как в \nпостановке задачи ответ является непрерывным отрывком из текста, то \nон может быть однозначно задан позициями начала и конца . \nПолучается, что модель должна ответить на два вопроса (5) для \nкаждого вектора токена: «Является ли данный токен из текста началом \nответа на заданный вопрос?», «Является ли данный токен из текста \nконцом ответа на заданный вопрос?» (рисунок 7): \n \n, (5) \n \nгде C - текст, Q - вопрос, Θ - параметры языковой модели, wstart и wend - \nобучаемые параметры для предсказания позиции начала и конца \nответа на вопрос , y\nk\nstart и yk\nend - позиции начала  и конца ответа на \nвопрос для примера с индексом k. Вероятности p start и p end при \nиспользовании модели BERT определяются следующим образом (6, 7):  \n \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n531\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\n, (6) \n \n, (7) \n \nгде softmax – функция софтмакс, L — число токенов во входной \nпоследовательности, {·,·} — скалярное произведение, wstart и w end - \nобучаемые параметры для предсказания позиции начала и конца \nответа на вопрос, BERT\ni\nN — в ы х о д  с  п о с л е д н е г о  с л о я  N  д л я  i-ого \nтокена во входной последовательности. \nВопрос и текст перед передачей в BERT разделяются \nспециальным токеном [SEP]. \n \n \nРис. 7. Использование модели BERT для нахождения ответа на вопрос в тексте \n(рисунок взят из работы [8]) \n2.9. Дистилляция. Несмотря на то, что современные модели \nпоказывают внушительные результаты в основных задачах обработки \nестественного языка, данные модели часто имеют один недостаток – \nсвой размер. Большое количество параметров модели может сильно \nограничить пропускную способность конвейера , на котором \nиспользуется данная модель , кроме того,  большая размерность не \nпозволяет эффективно использовать данные модели на мобильных \nустройствах. Поэтому одним из важных направлений исследований в \nобласти естественного языка является исследование подходов к \nуменьшению размера модели при сохранении той же точности работы.  \nОдним из набирающих популярность способов уменьшения \nразмерности моделей является дистилляция знаний. \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n532\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\nДистилляция знаний заключается в том, что вначале обучается \nбольшая модель, затем на основе предсказаний данной модели-учителя \nобучается более легковесная модель-ученик [ 14]. Данный подход \nпоказал хорошие результаты  (полученные в ходе дистилляции модели \nмогут совсем незначительно уступать в резу льтатах), из -за чего \nпоявилось большое количество дистиллированных  версий популярных \nсетей (таких как BERT, GPT). Данные дистиллированные версии \nсодержат упрощения архитектур и меньшее количество параметров , \nзатем данные версии моделей обучаются на основе своих больших \nсобратьев. Также полученные модели можно дообучать под \nнеобходимую задачу. \nДля задач классификации процесс д истилляции заключается в \nтом, что модели ученика и учителя вычисляют свои распределения \nвероятностей по классам для конкретного экземпляра данных, а затем  \nнеобходимо найти различие между этими вероятностями при помощи \nрасхождения Кульбака-Лейблера. Полученное расхождение \nиспользуется для расчета значения ошибки на текущей итерации \nработы сети. \n3. Проведение опытов \n3.1. Показатели. Основными показателями качества работы \nвопросно-ответных моделей являются EM и F-мера. \nExact match (EM)  — точное совпадение, доля ответов системы, \nкоторые полностью совпадают с одним из правильных ответов с \nточностью до пунктуации и регистра. \nF-мера — представляет собой совместную оценку полноты и \nточности. Данный показатель вычисляется по следующей формуле (8): \n \n. (8) \n \nПолнота (recall) вычисляется по следующей формуле (9): \n \n. (9) \n \nТочность (precision) вычисляется по следующей формуле (10): \n \n, (10) \n \nгде TP – Истинноположительные предсказания модели (модель \nправильно отнесла объект к классу), TN - Истинноотрицательные \nпредсказания модели (модель правильно не отнесла объект к классу), \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n533\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\nFN – Ложноотрицательные предсказания модели (модель неправильно \nне отнесла объект к классу), FP – Ложноположительные предсказания \nмодели (модель неправильно отнесла объект к классу). \n3.2. Выбор модели нейронной сети. Основной библиотекой \nдля работы с моделями для обработки естественного языка является \nHugging face. В данной библиотеке можно найти огромное количество \nразнообразных моделей с архитектурой «трансформер » и \nпроизводными от неё. На момент написания статьи число доступных \nмоделей в библиотеке составляет 22022. Также в данной библиотеке \nможно найти большое количество наборов данных. На момент \nнаписания статьи число наборов данных составляет 2059. Данная \nбиблиотека часто используется в крупных компаниях. У себя внутри \nбиблиотека  \nHugging face полагается на такие библиотеки как Jax, PyTorch  \nи TensorFlow, которые являются стандартом для всей индустрии \nмашинного обучения.  Также на сайте библиотеки в онлайне можно \nпроверить результаты работы большинства моделей (без \nнепосредственной установки библиотеки на персональный \nкомпьютер). \nНа момент написания статьи среди русскоязычных моделей для \nдистилляции было всего 6: \n1) DeepPavlov/distilrubert-base-cased-conversational\n1; \n2) DeepPavlov/distilrubert-tiny-cased-conversational2; \n3) Geotrend/distilbert-base-ru-cased3; \n4) Geotrend/distilbert-base-en-ru-cased4; \n5) Geotrend/distilbert-base-en-el-ru-cased5; \n6) Geotrend/distilbert-base-en-fr-nl-ru-ar-cased6. \nПервые две не подошли к решаемой задаче, поскольку, по \nсловам авторов, данные модели стоит использовать, только если \nданные решаемой задачи имеют разговорную структуру или содержат \nнеформальный язык. Модели с 3 по 6 были основаны на \nмультиязычной модели distilbert-base-multilingual-cased (содержит \n135×10\n6 параметров), которая в свою очередь представляет \nдистиллированную версию bert-base-multilingual-cased (содержит \n177×106 параметров). Данные модели были получены путём \n                                                           \n1 https://huggingface.co/DeepPavlov/distilrubert-base-cased-conversational \n2 https://huggingface.co/DeepPavlov/distilrubert-tiny-cased-conversational \n3 https://huggingface.co/Geotrend/distilbert-base-ru-cased \n4 https://huggingface.co/Geotrend/distilbert-base-en-ru-cased \n5 https://huggingface.co/Geotrend/distilbert-base-en-el-ru-cased \n6 https://huggingface.co/Geotrend/distilbert-base-en-fr-nl-ru-ar-cased \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n534\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\nуменьшения поддержки 108 языков до поддержки фиксированного \nнабора языков [ 26]. Было принято решение использовать модель под \nномером 3 поскольку она должна поддерживать только русский язык , \nт.е. иметь меньшее количество параметров по сравнению с \nостальными версиями (54 ×10\n6 параметров в Geotrend/distilbert-base-ru-\ncased, против 72×106 в Geotrend/distilbert-base-en-ru-cased). \nВ качестве модели -учителя была выбрана ruBert-base поскольку \nсреди дискриминационных моделей она показала лучшие результаты \n(таблица 1). \n3.3. Описание стенда для обучения нейронной сети. Обучение \nсети проводилось на сервисе Google Colab [27] . Данный сервис \nпредоставляет в бесплатное пользование компьютеры с \nпроизводительными видеокартами ( уровня NVIDIA Tesla K80) на \nограниченное время (12 часов).  Также доступ может быть прекращен \nраньше из- за простоя компьютера.  Взаимодействие с компьютерами \nосуществляется через программу Jupyter notebook, которая  \nподдерживает язык Python. \n3.4. Обучение нейронной сети  без применения процесса \nдистилляции. Процесс обычного обучения сети представлял собой  \nследующие этапы: \n1) Загрузку модели Geotrend/distilbert-base-ru-cased. \n2) Загрузку набора данных SberQuAD. \n3) Проведение тренировки модели на наборе данных  со \nследующими параметрами обучения: \n− Шаг обучения (learning rate): 2×10\n−5; \n− Размер блока: 16; \n− Длины входных последовательностей: 384; \n− Количество эпох обучения: 3. \n4) Проверка полученной модели на валидационном наборе \nданных. \n5) Расчёт показателей EM и F-меры. \n6) Время обучения модели заняло 3 часа 27 минут. \n3.5. Обучение нейронной сети с применением процесса \nдистилляции. Обучение дистиллированной модели включало в себя \nследующие шаги: \n1) Загрузку обученной ранее модели ruBert-base. \n2) Загрузку модели Geotrend/distilbert-base-ru-cased. \n3) Загрузку набора данных SberQuAD. \n4) Проведение дистилляци и модели Geotrend/distilbert -base-\nru-cased на основе результатов,  полученных из обученной ранее \nмодели ruBert-base (2 эпохи обучения). \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n535\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\n5) Проверка полученной модели на валидационном наборе \nданных. \n6) Расчёт показателей EM и F-меры. \nВремя обучения модели заняло 4 часа 15 минут. \n4. Результаты и их обсуждение. В результате обучения модели \nбез дистилляции были получены следующие показатели на \nвалидационном наборе данных: EM = 55,65 и F-мера = 76,51. \nВ результате обучения модели с дистилляцией были получены \nследующие показатели на валидационном наборе данных: EM = 58,57 \nи F-мера = 78,42. \nДля сравнения были взяты  модели, которые показали лучшие \nрезультаты на наборе данных  S berQuAD. Данные модели \nпредставляют не только кодировщики, но еще декодеры и полные \nтрансформеры. Стоит отметить, что для сравнения не использовались \nдругие дистиллированные версии моделей. Результаты сравнения \nмоделей представлены в таблице 1. \n \nТаблица 1. Результаты сравнения моделей \nНазвание модели EM F-мера Количество \nпараметров в модели \nruBERT-base 66,83 84,95 178×106 \nruBERT-large 67,25 85,25 427×106 \nruRoBERTa-large 65,23 85,45 355×106 \nruT5-base 66,26 84,56 222×106 \nruT5-large 68,57 86,73 737×106 \nruGPT-3-medium 57,60 77,73 356×106 \nruGPT-3-large 59,57 79,51 760×106 \nmT5-base (Google) 64,03 83,40 390×106 \nmT5-large (Google) 69,63 87,06 973×106 \nМодель, обученная без \nприменения дистилляции \n55,65 76,51 54×106 \nМодель, обученная с \nприменением дистилляции \n58,57 78,42 54×106 \n \nМодель, обученная с применением дистилляции, смогла \nопередить модель, которая была обучена обычным способом  \n(EM 58,57 и F-мера 78,42 против EM 55, 65 и F-мера 76,51). Также она \nсмогла опередить генеративную модель ruGPT-3-medium (EM 58, 57 и \nF-мера 78,42 против EM 57, 60 и F-мера 77,73). Модель  \nruGPT-3-medium имеет 356 миллионов параметров, а модель \nDistilBERT имеет всего 54 миллиона параметров. Но полученная \nмодель имеет большее отставание от более крупных \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n536\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\nдискриминационных моделей, например ruBERT (EM 66 ,83, F-мера \n84,95), которая имеет 178 миллионов параметров. \n \n \nРис. 8. Демонстрация работы модели \nНа рисунке 8 представлен  один вопросный набор из  \nвалидационной части набора данных  SberQUAD и форматированный \nрезультат работы полученной модели. Оригинальны й формат ответа \nмодели представлен на рисунке 9. \n \n \nРис. 9. Формат ответа обученной модели \n5. Заключение. Обученная в ходе дистилляции модель \nDistilBERT (EM 58 ,57 и F-мера 78,42) смогла опередить результаты \nболее крупной генеративной сети ruGPT -3-medium (EM 57,60 и F-мера \n77,73) притом, что ruGPT-3-medium имеет в 6 ,5 раз больше \nпараметров. К сожалению, полученная модель сильнее отстаёт от \nболее крупной дискриминационной модели ruBERT (EM 66 ,83, F-мера \n84,95), которая имеет в 3,2 раза больше параметров.  \nУчитывая большое количество готовых полных моделей \nследует изучить, какие результаты может показать ансамбль данных \nмоделей [2 8]. Дальнейшая дистилляции полученного ансамбля может \nпозволить получить модель, которая будет иметь значительно меньшее \nколичество параметров и превосходить результа ты моделей-учителей, \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n537\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\nработающих по  отдельности. Также можно произвести \nдообучение/дистилляцию на более крупных и более мелких моделях. \n \nЛитература \n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \nL., Polosukhin, I. Attention is all you need // Advances in Neural Information \nProcessing Systems 30. 2017. pp. 5998-6008.  \n2. Yang Z., Keung J., Yu X., Gu  X., Wei Z., Ma X., Zhang M. A Multi -Modal \nTransformer-based Code Summarization Approach for Smart Contracts // The 2021 \nInternational Conference on Program Comprehension. 2021. pp. 1-12.  \n3. Juraska J., Walker M. Attention Is Indeed All You Need: Semanticall y Attention-\nGuided Decoding for Data -to-Text NLG // Proceedings of the 14th International \nConference on Natural Language Generation. 2021. pp. 416-431.  \n4. Lewis M., Liu Y., Goyal N., Ghazvininejad M., Mohamed A., Levy O., Stoyanov V., \nZettlemoyer L. BART: Denoising Sequence -to-Sequence Pre- training for Natural \nLanguage Generation, Translation, and Comprehension // Proceedings of the 58th \nAnnual Meeting of the Association for Computational Linguistics. 2020. pp. 7871 -\n7880.  \n5. Raffel C., Shazeer N., Roberts A., Lee K., Narang S., Matena M., Zhou Y., Li W., Liu \nP.J. Exploring the Limits of Transfer Learning with a Unified Text- to-Text \nTransformer // Journal of Machine Learning Research, Volume 21. 2020. pp .1-67.  \n6. Zhang J., Zhao Y., Saleh M., Liu P. J. PEGASUS: Pre -training with Extracted Gap -\nsentences for Abstractive Summarization // Proceedings of the 37th International \nConference on Machine Learning. 2020. pp. 11328-11339.  \n7. Qi W., Yan Y., Gong Y., Liu D., Duan N., Chen J., Zhang R., Zhou M. ProphetNet: \nPredicting Future N -gram for Sequence -to-Sequence Pre- training // Findings of the \nAssociation for Computational Linguistics: EMNLP 2020. 2020. pp. 2401–2410.  \n8. Devlin J., Chang M., Lee K., Toutanova K. BERT: Pre -training of Deep Bidirectional \nTransformers for Languag e Understanding // Proceedings of the 2019 Conference of \nthe North American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies, Volume 1 (Long and Short Papers). 2019. pp. 4171–4186.  \n9. Lan Z., Chen M., Goodman S., Gimpel K., Sharma P., Soricut R. ALBERT: A Lite \nBERT for Self-supervised Learning of Language Representations. ArXiv. 2019. URL: \nhttps://arxiv.org/abs/1909.11942 (дата обращения: 12.11.2021).  \n10. Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer \nL., Stoyanov V. RoBERTa: A Robustly Optimized BERT Pretraining Approach. \nArXiv. 2019. URL: https://arxiv.org/abs/1907.11692 (дата обращения: 12.11.2021).  \n11. Brow T. B., Mann B., Ryder N., Subbiah M., Kaplan J., Dhariwal P., Neelakantan A., \nShyam P., Sastry G., Askell A., Agarwal S., Herbert -Voss A., Krueger G., Henighan \nT., Child R., Ramesh A., Ziegler D. M., Wu J., Winter C., Hesse C., Chen M., Sigler \nE., Litwin M., Gray S., Chess B., Clark J., Berner C., McCandlish S., Radford A., \nSutskever I., Amodei D. Language Models are Few -Shot Learners // Advances in \nNeural Information Processing Systems 33 (NeurIPS 2020). 2020. pp. 1877-1901.  \n12. Keskar N.S., McCann B., Varshney L. R., Xiong C., Socher R. CTRL: A Conditional \nTransformer Language Model for Con trollable Generation. ArXiv. 2019. URL: \nhttps://arxiv.org/abs/1909.05858 (дата обращения: 12.11.2021).  \n13. Dai Z., Yang Z., Yang Y., Carbonell J., Le Q.V., Salakhutdinov R. Transformer -XL: \nAttentive Language Models Beyond a Fixed -Length Context // Proceedings  of the \n57th Annual Meeting of the Association for Computational Linguistics. 2019. pp. \n2978-2988.  \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n538\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\n14. Hahn S., Choi H. Self -Knowledge Distillation in Natural Language Processing // \nProceedings of the International Conference on Recent Advances in Natural Language \nProcessing, Varna, Bulgaria, September 2-4, 2019. 2019. pp. 423-430.  \n15. Sanh V., Debut L., Chaumond J., Wolf T. DistilBERT, a distilled version of BERT: \nsmaller, faster, cheaper and lighter. ArXiv. 2019. URL: \nhttps://arxiv.org/abs/1910.01108 (дата обращения: 12.11.2021).  \n16. Rajpurkar P., Zhang J., Lopyrev K., Liang P. SQuAD: 100,000+ Questions for \nMachine Comprehension of Text // Proceedings of the 2016 Conference on Empirical \nMethods in Natural Language Processing. 2016. pp. 2383–2392.  \n17. Efimov P., Cherto k A., Boytsov L., Braslavski P. SberQuAD - Russian Reading \nComprehension Dataset: Description and Analysis // Experimental IR Meets \nMultilinguality, Multimodality, and Interaction - 11th International Conference of the \nCLEF Association, CLEF 2020, Thessalo niki, Greece, September 22 -25, 2020, \nProceedings. 2020. pp. 3-15.  \n18. Sennrich R., Haddow B., Birch A. Neural Machine Translation of Rare Words with \nSubword Units // Proceedings of the 54th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers). 2016. pp. 1715–1725.  \n19. Provilkov I., Emelianenko D., Voita E. BPE -Dropout: Simple and Effective Subword \nRegularization. ArXiv. 2020 // Proceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics. 2020. pp. 1882–1892.  \n20. Schuster M., Nakajima K. Japanese and Korea voice search // 2012 IEEE International \nConference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, \nMarch 25-30, 2012. 2012. pp. 5149-5152.  \n21. Mikolov T., Chen K., Corrado G., Dean J.  Efficient Estimation of Word \nRepresentations in Vector Space. ArXiv. 2013. URL: \nhttps://arxiv.org/pdf/1301.3781.pdf (дата обращения: 12.11.2021).  \n22. Фат Х.Н., Ань Н.Т.М. Алгоритм классификации вьетнамского текста с \nиспользованием долгой краткосрочной памяти  и Word2Vec // Информатика и \nавтоматизация. 2020. № 6 (19). С. 1255-1279.  \n23. Алтаф С., Iqbal S., Soomro  M.W. Эффективный алгоритм классификации \nестественного языка обнаружения повторяющихся контролируемых признаков // \nИнформатика и автоматизация. 2021. № 3 (20). С. 623-653.  \n24. Lei Ba J., Kiros J.R., Hinton G.E. Layer Normalization. ArXiv. 2016. URL: \nhttps://arxiv.org/pdf/1607.06450.pdf (дата обращения: 12.11.2021).  \n25. URL: https://github.com/jessevig/bertviz (дата обращения: 12.11.2021).  \n26. Abdaoui A., Pradel C., Sigel G. Load What You Need: Smaller Versions of \nMultilingual BERT. ArXiv. 2020. URL: https://arxiv.or g/abs/2010.05609 (дата \nобращения: 12.11.2021).  \n27. URL: https://colab.research.google.com/notebooks/welcome.ipynb?hl=ru (дата \nобращения: 12.11.2021).  \n28. Li S., Li R., Peng V. Ensemble ALBERT on SQuAD 2.0. ArXiv. 2021. URL: \nhttps://arxiv.org/abs/2110.09665 (дата обращения: 12.11.2021). \n \nГалеев Денис Талгатович  — аспирант, ФГБОУ ВО  Юго-Западный государственный \nуниверситет (ЮЗГУ). Область научных интересов: искусственный интеллект, машинное \nобучение, обработка естественного языка. Число научных публикаций — 15. \nra3wvw@mail.ru; улица 50 лет Октября, 94, 305040, Курск, Россия; р.т.: +7(4712)222-665. \n \nПанищев Владимир Славиевич  — канд. техн. наук, доцент, кафедра вычислительной \nтехники, ФГБОУ ВО Юго -Западный государственный университет (ЮЗГУ). Область \nнаучных интересов: нейронные сети, обработка изображений. Число научных \nпубликаций — 150. gskunk@yandex.ru; улица 50 лет Октября, 94, 305040, Курск, Россия; \nр.т.: +7(4712)222626. \n \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n539\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\n DOI 10.15622/ia.21.3.3 \n \nD. GALEEV, V. PANISHCHEV  \nEXPERIMENTAL STUDY OF LANGUAGE MODELS OF \n\"TRANSFORMER\" IN THE PROBLEM OF FINDING THE \nANSWER TO A QUESTION IN A RUSSIAN-LANGUAGE TEXT \n \nGaleev D., Panishchev V. Experimental Study of Language Models of \"Transformer\" in \nthe Problem of Finding the Answer to a Question in a Russian-Language Text. \nAbstract. The aim of the study is to obtain a more lightweight language model that is \ncomparable in terms of EM and F1 with the best modern language models in the task of finding \nthe answer to a question in a text in Russian. The results of the work can be used in various \nquestion-and-answer systems for which response time is important. Since the lighter model has \nfewer parameters than the original one, it can be used on less powerful computing devices, \nincluding mobile devices. In this paper, methods of natural language processing, machine \nlearning, and the theory of artificial neural networks are used. The neural network is configured \nand trained using the Torch and Hugging face machine learning libraries. In the work, the \nDistilBERT model was trained on the SberQUAD dataset with and without distillation. The \nwork of the recei ved models is compared. The distilled DistilBERT model (EM 58,57 and F1 \n78,42) was able to outperform the results of the larger ruGPT -3-medium generative network \n(EM 57,60 and F1 77,73), despite the fact that ruGPT -3-medium had 6,5 times more \nparameters. The model also showed better EM and F1 metrics than the same model, but to \nwhich only conventional training without distillation was applied (EM 55,65, F1 76,51). \nUnfortunately, the resulting model lags further behind the larger robert discriminative model \n(EM 66,83, F1&nbsp;84,95), which has 3,2 times more parameters. The application of the \nDistilBERT model in question -and-answer systems in Russian is substantiated. Directions for \nfurther research are proposed. \nKeywords: machine learning, deep learning, neu ral networks, natural language \nprocessing, transformer. \n \nGaleev Denis  — Ph.D., Graduate student, Southwest State University (SWSU). Research \ninterests: artificial intelligence, machine learning, natural language processing. The number of \npublications — 15. ra3wvw@mail.ru; 94, 50 let Oktyabrya St., 305040, Kursk, Russia; office \nphone: +7(4712)222-665. \n \nPanishchev Vladimir  — Ph.D., Associate professor, Department of computer engineering, \nSouthwest State University (SWSU). Research interests: neural networks, image processing. \nThe number of publications — 150. gskunk@yandex.ru; 94, 50 let Oktyabrya St., 305040, \nKursk, Russia; office phone: +7(4712)222626. \n \nReferences \n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \nL., Polosukhin, I. Attention is all you need // Advances in Neural Information \nProcessing Systems 30. 2017. pp. 5998-6008.  \n2. Yang Z., Keung J., Yu X., Gu X., Wei Z., Ma X., Zhang M. A Multi -Modal Trans-\nformer-based Code Summarization Approach for Smart Contract s // The 2021 Inter -\nnational Conference on Program Comprehension. 2021. pp. 1-12.  \n3. Juraska J., Walker M. Attention Is Indeed All You Need: Semantically Attention -\nGuided Decoding for Data -to-Text NLG // Proceedings of the 14th International \nConference on Natural Language Generation. 2021. pp. 416-431.  \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n540\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ\n4. Lewis M., Liu Y., Goyal N., Ghazvininejad M., Mohamed A., Levy O., Stoyanov V., \nZettlemoyer L. BART: Denoising Sequence- to-Sequence Pre- training for Natural \nLanguage Generation, Translation, and Comprehension // Proceedings of the 58th \nAnnual Meeting of the Association for Computational Linguistics. 2020. pp. 7871 -\n7880.  \n5. Raffel C., Shazeer N., Roberts A., Lee K., Narang S., Matena M., Zhou Y., Li W., Liu \nP.J. Exploring the Limits of Transfer Learning with a Unified Text -to-Text \nTransformer // Journal of Machine Learning Research, Volume 21. 2020. pp .1-67.  \n6. Zhang J., Zhao Y., Saleh M., Liu P. J. PEGASUS: Pre -training with Extracted Gap -\nsentences for Abstractive Summarization // Proceedings of the 37th Internatio nal \nConference on Machine Learning. 2020. pp. 11328-11339.  \n7. Qi W., Yan Y., Gong Y., Liu D., Duan N., Chen J., Zhang R., Zhou M. ProphetNet: \nPredicting Future N -gram for Sequence -to-Sequence Pre- training // Findings of the \nAssociation for Computational Linguistics: EMNLP 2020. 2020. pp. 2401–2410.  \n8. Devlin J., Chang M., Lee K., Toutanova K. BERT: Pre -training of Deep Bidirectional \nTransformers for Language Understanding // Proceedings of the 2019 Conference of \nthe North American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies, Volume 1 (Long and Short Papers). 2019. pp. 4171–4186.  \n9. Lan Z., Chen M., Goodman S., Gimpel K., Sharma P., Soricut R. ALBERT: A Lite \nBERT for Self-supervised Learning of Language Representations. ArXiv. 2019. URL: \nhttps://arxiv.org/abs/1909.11942 (accessed: 12.11.2021).  \n10. Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer \nL., Stoyanov V. RoBERTa: A Robustly Optimized BERT Pretraining Approach. \nArXiv. 2019. URL: https://arxiv.org/abs/1907.11692 (accessed: 12.11.2021).  \n11. Brow T. B., Mann B., Ryder N., Subbiah M., Kaplan J., Dhariwal P., Neelakantan A., \nShyam P., Sastry G., Askell A., Agarwal S., Herbert -Voss A., Krueger G., Henighan \nT., Child R., Ramesh A., Ziegler D. M., Wu  J., Winter C., Hesse C., Chen M., Sigler \nE., Litwin M., Gray S., Chess B., Clark J., Berner C., McCandlish S., Radford A., \nSutskever I., Amodei D. Language Models are Few -Shot Learners // Advances in \nNeural Information Processing Systems 33 (NeurIPS 2020). 2020. pp. 1877-1901.  \n12. Keskar N.S., McCann B., Varshney L. R., Xiong C., Socher R. CTRL: A Conditional \nTransformer Language Model for Controllable Generation. ArXiv. 2019. URL: \nhttps://arxiv.org/abs/1909.05858 (accessed: 12.11.2021).  \n13. Dai Z., Yang Z., Yang Y., Carbonell J., Le Q.V., Salakhutdinov R. Transformer -XL: \nAttentive Language Models Beyond a Fixed -Length Context // Proceedings of the \n57th Annual Meeting of the Association for Computational Linguistics. 2019. pp. \n2978-2988.  \n14. Hahn S., Choi H. Self -Knowledge Distillation in Natural Language Processing // \nProceedings of the International Conference on Recent Advances in Natural Language \nProcessing, Varna, Bulgaria, September 2-4, 2019. 2019. pp.423-430.  \n15. Sanh V., Debut L., Chaumond J., Wolf T. DistilBER T, a distilled version of BERT: \nsmaller, faster, cheaper and lighter. ArXiv. 2019. URL: \nhttps://arxiv.org/abs/1910.01108 (accessed: 12.11.2021).  \n16. Rajpurkar P., Zhang J., Lopyrev K., Liang P. SQuAD: 100,000+ Questions for Ma -\nchine Comprehension of Text // Proceedings of the 2016 Conference on Empirical \nMethods in Natural Language Processing. 2016. pp. 2383–2392.  \n17. Efimov P., Chertok A., Boytsov L., Braslavski P. SberQuAD - Russian Reading \nComprehension Dataset: Description and Analysis // Experimental IR Meet s \nMultilinguality, Multimodality, and Interaction - 11th International Conference of the \nCLEF Association, CLEF 2020, Thessaloniki, Greece, September 22 -25, 2020, \nProceedings. 2020. pp. 3-15.  \n_____________________________________________________________________\nInformatics and Automation. 2022. Vol. 21 No. 3. ISSN 2713-3192 (print) \nISSN 2713-3206 (online) www.ia.spcras.ru\n541\nARTIFICIAL INTELLIGENCE, KNOWLEDGE AND DATA ENGINEERING\n18. Sennrich R., Haddow B., Birch A. Neural Machine Translation of Rare Words with \nSubword Units // Proceedings of the 54th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers). 2016. pp. 1715–1725.  \n19. Provilkov I., Emelianenko D., Voita E. BPE -Dropout: Simple and Effective Subword \nRegularization. ArXiv. 2020 // Proceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics. 2020. pp. 1882–1892.  \n20. Schuster M., Nakajima K. Japanese and Korea voice search // 2012 IEEE International \nConference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, \nMarch 25-30, 2012. 2012. pp. 5149-5152.  \n21. Mikolov T., Chen K., Corrado G., Dean J. Efficient Estimation of Word \nRepresentations in Vector Space. ArXiv. 2013 URL: \nhttps://arxiv.org/pdf/1301.3781.pdf (accessed: 12.11.2021).   \n22. Phat H.N., Anh N.T.M. Vietnamese Text Classification Algorithm using Long Short \nTerm Memory and Word2Vec // Informatics and Automation. 2020. № 6 (19). pp. \n1255-1279.  \n23. Altaf S., Iqbal S., Soomro M.W. Efficient natural language classification algorithm for \ndetecting duplicate unsupervised features // Informatics and Automation. 2021. № 3 \n(20). pp. 623-653.  \n24. Lei Ba J., Kiros J.R., Hinton G.E. Layer Normalization. ArXiv. 2016. URL: \nhttps://arxiv.org/pdf/1607.06450.pdf (accessed: 12.11.2021).  \n25. URL: https://github.com/jessevig/bertviz (accessed: 12.11.2021).  \n26. Abdaoui A., Pradel C., Sigel G. Load What You Need: Smaller Versions of \nMultilingual BERT. ArXiv. 2020. URL: https://arxiv.org/abs/2010.05609 (accessed: \n12.11.2021).  \n27. URL: https://colab.research.google.com/notebooks/welcome.ipynb?hl=ru (accessed: \n12.11.2021).  \n28. Li S., Li R., Peng V. Ensemble ALBERT on SQuAD 2.0. ArXiv. 2021. URL: \nhttps://arxiv.org/abs/2110.09665 (accessed: 12.11.2021). \n \n_____________________________________________________________________\nИнформатика и автоматизация. 2022. Том 21 № 3. ISSN 2713-3192 (печ.) \nISSN 2713-3206 (онлайн) www.ia.spcras.ru\n542\nИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ, ИНЖЕНЕРИЯ ДАННЫХ И ЗНАНИЙ"
}