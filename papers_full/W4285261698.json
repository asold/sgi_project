{
  "title": "Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation",
  "url": "https://openalex.org/W4285261698",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5051933558",
      "name": "Dongha Choi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047739940",
      "name": "Hongseok Choi",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100355268",
      "name": "Hyunju Lee",
      "affiliations": [
        null,
        "Gwangju Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963534679",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2949894546",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3004127093",
    "https://openalex.org/W2964111476",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2933254221",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W3100560429",
    "https://openalex.org/W2953508004",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2034365297",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W2739879705",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2753259282",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3171009184",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035101152"
  ],
  "abstract": "Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains. Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges. In this study, we propose a domain knowledge transferring (DoKTra) framework for PLMs without additional in-domain pretraining. Specifically, we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation. In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons. We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus aid the distillation. By applying the proposed DoKTra framework to downstream tasks in the biomedical, clinical, and financial domains, our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks. Our code is available at https://github.com/DMCB-GIST/DoKTra.",
  "full_text": "Domain Knowledge Transferring for Pre-trained Language Model via\nCalibrated Activation Boundary Distillation\nDongha Choi1, HongSeok Choi2, and Hyunju Lee1,2‚àó\n1Artificial Intelligence Graduate School\n2School of Electrical Engineering and Computer Science\nGwangju Institute of Science and Technology, Gwangju 61005, South Korea\ndongha528@gm.gist.ac.kr , {hongking9,hyunjulee}@gist.ac.kr\nAbstract\nSince the development and wide use of pre-\ntrained language models (PLMs), several ap-\nproaches have been applied to boost their per-\nformance on downstream tasks in specific do-\nmains, such as biomedical or scientific domains.\nAdditional pre-training with in-domain texts\nis the most common approach for providing\ndomain-specific knowledge to PLMs. How-\never, these pre-training methods require consid-\nerable in-domain data and training resources\nand a longer training time. Moreover, the train-\ning must be re-performed whenever a new PLM\nemerges. In this study, we propose a domain\nknowledge transferring (DoKTra) framework\nfor PLMs without additional in-domain pre-\ntraining. Specifically, we extract the domain\nknowledge from an existing in-domain pre-\ntrained language model and transfer it to other\nPLMs by applying knowledge distillation. In\nparticular, we employ activation boundary dis-\ntillation, which focuses on the activation of\nhidden neurons. We also apply an entropy reg-\nularization term in both teacher training and\ndistillation to encourage the model to generate\nreliable output probabilities, and thus aid the\ndistillation. By applying the proposed DoKTra\nframework to downstream tasks in the biomedi-\ncal, clinical, and financial domains, our student\nmodels can retain a high percentage of teacher\nperformance and even outperform the teach-\ners in certain tasks. Our code is available at\nhttps://github.com/DMCB-GIST/DoKTra.\n1 Introduction\nRecently, transformer (Vaswani et al., 2017)-based\nlanguage models have been successfully applied in\nthe field of natural language processing (NLP). In\nparticular, the two-stage approach of ‚Äúpre-training\nand fine-tuning,‚Äù such as BERT (Devlin et al.,\n2019), has become the standard for NLP applica-\ntions. Generally, a transformer-based model is pre-\ntrained with a large amount of text data in an unsu-\n‚àóHyunju Lee is the corresponding author.\npervised manner, and then fine-tuned with a small\ndataset for several downstream tasks. Further, ad-\nvanced pre-trained language models (PLMs) with\nimproved architectures or training methods con-\ntinue to emerge, including ALBERT (Lan et al.,\n2019) or RoBERTa (Liu et al., 2019).\nHowever, these models must be further improved\nfor tasks requiring domain knowledge, such as\nthose in the biomedical or financial domains, as\nthe pre-training data usually consist of general do-\nmain text (e.g., Wikipedia). Additional pre-training\nwith in-domain text has been proposed to provide\nthe PLMs with domain-specific knowledge. For ex-\nample, in the biomedical domain, several domain-\nspecific PLMs trained with large biomedical texts,\nsuch as BioBERT (Lee et al., 2020), PubMedBERT\n(Gu et al., 2020) and BlueBERT (Peng et al., 2019),\nhave been successfully used as strong baselines\nfor several downstream tasks. Nevertheless, addi-\ntional pre-training has several limitations, such as\nthe need for sufficient training data and resources,\nand a longer training time. Furthermore, whenever\na new PLM emerges, it must be re-trained to create\nmore advanced domain-specific models.\nTo address this issue, we propose an efficient\ndomain-knowledge transferring framework that\ndoes not require additional pre-training steps.\nSpecifically, we focus on the applicability of knowl-\nedge distillation (Hinton et al., 2015) as a domain-\nknowledge transfer method, not only for model\ncompression. Knowledge distillation is a well-\nknown knowledge transfer method that is primarily\nused for model compression. The knowledge from\na larger and more effective teacher model is dis-\ntilled to a smaller student model by encouraging it\nto mimic the teacher characteristics, such as soft\nprobabilities (Hinton et al., 2015) or hidden repre-\nsentations (Kim et al., 2018; Sun et al., 2019).\nIn this study, we propose a domain knowledge\ntransfer (DoKTra) framework for an advanced\nPLM via calibrated activation boundary distilla-\n1658\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1658 - 1669\nMay 22-27, 2022c‚Éù2022 Association for Computational Linguistics\n(a) Further in-domain pre-training\n(b) DoKTra framework\nNew \nPLM\n(General domain)\nPLM\n(Specific domain)\nIn-domain \ntext data\nModel \nA\nModel \nB\nModel \nC\nTask A\nTask B\nTask C\nNew \nPLM\n(General \ndomain)\nExisting\nPLM\n(Specific \ndomain)\nTask A\nTask A\nStudent\nTeacher \nfor task A \nTask A\nModel A\nFigure 1: Comparison between (a) an existing domain\ntransfer method and (b) a proposed framework. The\nthickness of the arrow is proportional to the required\ntraining time.\ntion. In contrast to the existing in-domain pre-\ntraining methods, we transfer domain knowledge\nto a new language model using only an existing\nin-domain pre-trained model, and without a time-\nconsuming pre-training on the new model. For\ninstance, BioBERT was pre-trained for 23 days\non 8 NVIDIA V100 GPUs (Lee et al., 2020). We\ncan estimate that if a new, larger language model\nis pre-trained with a large number of biomedical\ntexts, its training duration would be longer than\nthat of BioBERT. However, our framework can be\nexecuted in a few hours on a single 24 GB GPU.\nThe comparison between our framework and a con-\nventional approach is visualized in Figure 1.\nSpecifically, we apply the calibration method to\ngenerate a reliable and well-supervising teacher\nmodel. Then, we apply activation boundary dis-\ntillation (Heo et al., 2019) to distill the domain\nknowledge to the student, which is more efficient\nwith a small amount of training data. Moreover,\nby selecting language models more advanced than\nthe teacher as students, we allow the student mod-\nels to acquire additional domain knowledge while\npreserving its superiority.\nWe apply our framework to the biomedical do-\nmain and verify its effectiveness by conducting ex-\nperiments on several biomedical and clinical down-\nstream tasks. Consequent to applying our frame-\nwork to ALBERT and RoBERTa student models,\nwe were able to obtain models that retained most\nof the teacher model‚Äôs performance with fewer\nmodel parameters (ALBERT), and models with\na higher performance than both students and teach-\ners (RoBERTa). We also investigate the general\napplicability of our framework by applying it to a\nfinancial domain PLM and downstream tasks. The\ncontributions of this study can be summarized as\nfollows:\n‚Ä¢ We propose a DoKTra framework for ad-\nvanced PLM via calibrated activation bound-\nary distillation, without additional time-\nconsuming pre-training steps.\n‚Ä¢ We conduct experiments to demonstrate the\nefficacy of DoKTra, resulting in obtaining the\nstudent models that retain most of the perfor-\nmances of the teacher model while utilizing\nfewer parameters or achieve even higher per-\nformances than the teacher model.\n2 Related Work\n2.1 Pre-trained language model (PLM)\nMost modern language models are based on the\ntransformer (Vaswani et al., 2017) architecture.\nThe PLMs generally use only the encoder block of\nthe transformer, which consists of two sublayers: a\nself-attention layer and a feed-forward layer. BERT\n(Devlin et al., 2019) is the most widely used PLM,\nwhich consists of several layers of transformer en-\ncoders. It was pre-trained for 4 days with a large\namount of text data, which consisted of 3.3 billion\nwords, using masked language modeling and a next\nsentence prediction task in an unsupervised man-\nner. This pre-trained model can be easily used in\nvarious downstream tasks by fine-tuning it with a\nlabeled dataset. Following the success of BERT, a\nvariety of similar PLMs have emerged. Lan et al.\n(2019) proposed ALBERT, which outperformed\nBERT with considerably fewer parameters. AL-\nBERT‚Äôs architecture is more complex than BERT‚Äôs;\nhowever, by applying factorized embedding param-\neterization and cross-layer parameter sharing, the\nnumber of parameters can be reduced. Liu et al.\n(2019) observed that BERT is significantly under-\ntrained, and proposed RoBERTa, a more robust and\nbetter-performing model, which is obtained by a\nlonger pre-training with a larger dataset (approxi-\nmately 10 times that of BERT) and the removal of\nnext sentence prediction.\n1659\n2.2 Domain knowledge transferring for PLMs\nDespite the PLMs‚Äô excellent performances in sev-\neral downstream tasks in the general domain, they\nhave not exhibited a superior performance in spe-\ncific domain tasks, such as in biomedicine. To\nprovide domain-specific knowledge to PLMs, ad-\nditional pre-training with in-domain data has been\napplied. BioBERT (Lee et al., 2020) further pre-\ntrained BERT using biomedical text consisting of\n18 billion words, such as literature abstracts. Peng\net al. (2019) applied a similar approach with both\nbiomedical and clinical text data. Differently, Gu\net al. (2020) pre-trained BERT from scratch with\nonly biomedical literature.\n3 DoKTra framework\nIn this section, we introduce the DoKTra frame-\nwork, which is the main approach to transfer\ndomain-specific knowledge.\n3.1 Overview\nThe main goal of the DoKTra framework is to pro-\nduce a task-specific student model for each down-\nstream task in a specific domain by distilling do-\nmain knowledge from a fine-tuned teacher model.\nOur framework consists of two main stages: cal-\nibrated teacher training and activation boundary\ndistillation.\nIn calibrated teacher training, the teacher model\nis trained to distil its domain-specific and task-\nspecific knowledge into the student model. We use\nan existing in-domain PLM as the initial teacher\nmodel. For each downstream task in the initial\nteacher‚Äôs domain, the teacher model is fine-tuned\nwith its training data. In this process, an entropy\nregularization term, called the confidence penalty\nloss (Pereyra et al., 2017), is added to the training\nloss. By adding the confidence regularizer, the fine-\ntuned teacher model can generate more reliable\noutput prediction probabilities for the input data,\nand thus, have a positive effect on distillation.\nIn activation boundary distillation, the domain-\nspecific knowledge of the teacher model is trans-\nferred to the student model. We use an existing\nPLM as the initial student model, which is only\npre-trained in the general domain. First, the student\nmodel is fine-tuned for a downstream task. Subse-\nquently, it mimics the activation pattern of the hid-\nden neurons in the teacher model (Heo et al., 2019).\nBy distilling the activation pattern, the activation\nboundary of the teacher model is transferred more\nprecisely, and the domain-specific knowledge of\nthe teacher is transferred to the student model. Ad-\nditionally, the student model is refined over fewer\nepochs with a standard classification loss (Romero\net al., 2014; Yim et al., 2017; Heo et al., 2019).\nBecause the student model is already fine-tuned\nfor the downstream task, any additional refinement\nmay result in overconfidence (Guo et al., 2017;\nNixon et al., 2019). To address this issue, we also\nadd the confidence regularizer to the refinement\nstep. The proposed framework is visualized in Fig-\nure 2.\n3.2 Calibrated teacher training\nIn this step, a task-specific teacher model is gen-\nerated for each in-domain downstream task using\na fine-tuning approach. Specifically, we choose\nBioBERT-base (Lee et al., 2020) as the initial\nteacher model, which has been pre-trained with a\nlarge biomedical domain corpus, such as PubMed\nabstracts. Owing to the in-domain pre-training, the\nBioBERT model outperforms the BERT model in\nseveral biomedical downstream tasks.\nDespite their high performance, modern deep\nneural networks are not well calibrated (Guo et al.,\n2017), which is similar to language models such\nas BERT. In other words, these models only pre-\ndict overconfidently and cannot generate a reliable\noutput probability for the given input. However,\nmost distillation approaches encourage the use of\nsoftened probability because they contain more in-\nformation and can better support the learning of\nthe student model (Hinton et al., 2015; Cho and\nHariharan, 2019). Moreover, Menon et al. (2021)\ndemonstrated that a teacher model that estimates\n‚Äúgood‚Äù probabilities can better supervise a student\nmodel. Based on this idea, we apply an entropy-\nregularizing term that penalizes overconfidence\nwhen fine-tuning the teacher model (Pereyra et al.,\n2017). Several previous studies have revealed that\na confidence penalty improves both the calibration\nand performance of biomedical downstream tasks\n(Choi and Lee, 2020).\nSince an overconfident classification model pro-\nduces output probabilities close to 0 and 1, its prob-\nability distribution has a low entropy value. The\nconfidence penalty loss (CPL) addresses this prob-\nlem by minimizing the negative entropy of the out-\nput probability. Formally, the output probability of\nthe model with parameters Œ∏ can be written as a\nconditional distribution pŒ∏(y|x) through the soft-\n1660\nLayer 1\nLayer 2\nLayer L-1\nLayer L\nLayer 1\nLayer 2\nLayer M-1\nLayer M + - + - - ‚Ä¶ + + - -\n+ - + + - ‚Ä¶ + - - +\nTeacher\nStudent\nInitial Teacher\n(Specific \ndomain)\nInitial Student\n(General \ndomain)\nùëáùê∂ùêøùëÜ (x)\nùëÜ ùê∂ùêøùëÜ (x)\nTask A\nTask A\n‚Ñíùê∂ùê∏\n‚Ñíùëêùëôùë†\n‚Ñíùê¥ùëá\n‚Ñíùëêùëôùë† Final model \nfor task A\nTask A\nCalibrated teacher training Activation boundary distillation\nùõæ√ó#epochs (1‚àíùõæ)√ó#epochs\nFigure 2: An overview of the DoKTra framework\nmax function for classesy and a given inputx. The\nentropy value of the output probability is given by\nH(pŒ∏(y|x)) =‚àí\nX\ni\npŒ∏(yi|x) log(pŒ∏(yi|x)),\n(1)\nwhere i denotes the class index. Finally, negative\nentropy is added to a regular cross-entropy loss\nLCE ,\nLcls = LCE ‚àí Œ≤H(pŒ∏(y|x)), (2)\nwhere Œ≤ refers to a hyperparameter that controls\nthe strength of entropy penalty.\n3.3 Activation Boundary Distillation\nRecently, Heo et al. (2019) has proposed a knowl-\nedge distillation method that only distils the acti-\nvation boundary of the hidden representation of a\ndeep neural network. Instead of distilling the mag-\nnitude of the neurons of the teacher network, Heo\net al. (2019) designed the distillation loss to only\ntransfer the activation of neurons and thus, allowed\nthe activation boundary to be transferred. Since the\ndecision boundary of a model, which consists of a\ncombination of activation boundaries, is critical for\nthe classification task, this method outperformed\nseveral distillation methods in image classification.\nMoreover, they also reported that the activation\nboundary distillation can learn rapidly and more\nefficiently with a small amount of training data.\nThus, we select it as the domain-knowledge trans-\nferring method for our framework; this is beacuse\nthe domain-specific downstream tasks usually con-\nsist of lesser training data than general domains.\nTo apply the activation boundary distilla-\ntion to PLMs, we use classification embed-\nding of the teacher and student as the distilla-\ntion target. More precisely, the input sequence\nof a PLM such as BERT can be written as\n[CLS], t1, t2, . . . ,[SEP ], where ti is the i-th to-\nken of the example. Then, the final output se-\nquence is h([CLS]), h(t1), . . . , h([SEP ]), where\nh(t) indicates the hidden output of the last layer\nof the token t. For the classification task, the out-\nput embedding of the first special token(‚Äú[CLS],‚Äù\nalso known as the classification token) is gener-\nally used as the input of the classification layer.\nThus, we apply activation boundary distillation to\nthe classification embedding (output embedding\nof the classification token). For an input example\nx, let T[CLS](x) ‚àà Rd and S[CLS](x) ‚àà Rd be\nthe classification embedding vector (h([CLS])) of\nthe teacher and student model, respectively. An\nelement-wise activation indicator function can be\ndefined to express the activation of a neuron:\nœÅ(x) =\n(\n1, if x >0\n0, otherwise. (3)\nThe loss function to transfer the activation of\nneurons is a l1 norm of the difference between\nactivations:\nLAT (x) =‚à•œÅ(T[CLS](x)) ‚àí œÅ(S[CLS](x))‚à•1.\n(4)\nHowever, this loss function cannot be minimized\nusing gradient descent because œÅ is a discrete func-\ntion. To address this issue, Heo et al. (2019) has\nproposed an alternative loss function similar to\nhinge loss (Rosasco et al., 2004) with an activa-\ntion function œÉ.\n1661\nLAT (x) =‚à•œÅ(T[CLS](x)) ‚äô œÉ(¬µ1 ‚àí (S[CLS](x)))\n+(1 ‚àí œÅ(T[CLS](x))) ‚äô œÉ(¬µ1 + (S[CLS](x)))‚à•2\n2,\n(5)\nwhere ‚äô is the element-wise product and 1 is a d-\ndimensional vector, with all values equal to 1. ¬µ is\nthe margin, which is a hyperparameter for training\nstability.\nSpecifically, we select two PLMs as the initial\nstudent model: ALBERT-xlarge (Lan et al., 2019),\nwhich has a smaller number of parameters but per-\nforms better than BERT, and RoBERTa-large (Liu\net al., 2019), which has a larger number of param-\neters and is known to outperform BERT signifi-\ncantly for most of the tasks. To distil the knowl-\nedge from a teacher model, we first fine-tune the\nstudent model to provide initial knowledge about\nthe task. Then the student model is trained with\nLAT . We also add a few refinement steps to re-\nfine the classification layer of the student model.\nBecause the student model is already fine-tuned\nbefore the distillation step, this additional refine-\nment may cause overconfidence. Thus, we apply a\nconfidence penalty regularization in the refinement\nstep. Namely, the student is refined with Lcls af-\nter the distillation steps. We add a hyperparameter\nŒ≥ ‚àà [0, 1], which determines when the training loss\nis switched from distillation to refinement. The pro-\ncedure of the DoKTra framework is summarized in\nAlgorithm 1.\nAlgorithm 1 DoKTra framework\nInput: Downstream task data D = {xk, yk}N\nk=1,\nhyperparameter Œ≤1, Œ≤2, Œ≥\n1: Fine-tune the teacher T with dataD, using Lcls\nwith Œ≤1\n2: Fine-tune the student S with dataD, usingLCE\n3: epochswitch = epochstotal √ó Œ≥\n4: for each epoch do\n5: if epoch < epochswitch then\n6: Train S using LAT\n7: else\n8: Train S using Lcls with Œ≤2\n9: end if\n10: end for\n11: return Student model S\nDataset #Train #Dev #Test Metrics Domain\nChemProt 17865 11263 15583 micro F1 Biomed.\nGAD 4796 - 534 F1 Biomed.\nDDI 18779 7244 5761 micro F1 Biomed.\ni2b2 22160 96 43000 micro F1 Clin.\nHoC 10527 1496 2896 F1 Biomed.\nTable 1: The statistics of the downstream task datasets\n4 Experiments\n4.1 Datasets\nWe evaluated our approach on several biomedi-\ncal and clinical classification downstream tasks,\nincluding relation extraction and multi-label classi-\nfication.\nThe relation extraction task aims to classify the\nrelationship between two entities (e.g., gene, chem-\nical, and disease) that are already annotated. The\nChemProt (Krallinger et al., 2017) dataset con-\ntains PubMed abstracts with 10 types of chemical-\nprotein interaction annotations and only five of the\ntypes are used for evaluation. The GAD dataset\n(Bravo et al., 2015) consists of gene-disease binary\nrelation annotations. The DDI (Herrero-Zazo et al.,\n2013) dataset consists of text from the DrugBank\ndatabase and Medline abstracts, with four types\nof drug-drug interaction annotations. In the clini-\ncal domain, the i2b2 dataset (Uzuner et al., 2011)\ncontains texts from clinical documents, and eight\ntypes of relations between medical problems and\ntreatments have been annotated. The HoC (Baker\net al., 2016) corpus consists of PubMed abstracts\nwith ten types of hallmarks of cancer annotation.\nNote that the HoC dataset is a multi-label docu-\nment classification task predicting the combination\nof labels from an input text.\nWe pre-process every classification dataset ex-\ncept for GAD in the same manner as the BLUE\n(Peng et al., 2019) benchmark. In particular, entity\nanonymization is applied to all relation extraction\ndatasets, which replace the entity mentions with\nanonymous tokens (e.g., @GENE$, @DISEASE$)\nto avoid confusion in using complex entity names.\nWe use a pre-processed version of the GAD dataset\nprovided by BioBERT, which is split for 10-fold\ncross-validation. The statistics of the pre-processed\ndownstream task datasets are listed in Table 1.\n1662\nModels #Params. ChemProt GAD DDI i2b2 HoC Avg. Retain\nBioBERT-ft (teacher) 110M 76.20 ¬±0.65 81.59¬±0.27 80.05¬±0.62 74.14¬±0.35 84.21¬±0.33 79.24\nALBERT-ft (student) 60M 73.67 ¬±0.98 74.33¬±0.91 81.31¬±0.72 69.89¬±1.17 81.76¬±0.20 76.19\nALBERT-DoKTra 60M 77.42 ¬±0.04 78.86¬±0.19 82.30¬±0.41 72.98¬±0.07 83.52¬±0.44 79.02 99.72%\nRoBERTa-ft (student) 355M 75.75 ¬±0.35 77.84¬±1.80 80.71¬±1.56 72.51¬±1.80 83.98¬±0.44 78.16\nRoBERTa-DoKTra 355M 78.04 ¬±0.22 81.38¬±0.05 82.25¬±0.30 75.65¬±0.11 85.34¬±0.12 80.53 101.63%\nTable 2: The DoKTra framework‚Äôs main experimental results. (ft: fine-tuned)\n4.2 Experimental details\nFor the experiments, we used the pre-trained\nBioBERT-base model (L=12, H=768, A=12) as the\ninitial teacher model. We used two pre-trained mod-\nels as the initial student model: ALBERT-xlarge\n(L=24, H=2048, A=32) and RoBERTa-large (L=24,\nH=1024, A=16). In the previous description, we\nhave assumed that the embedding dimensions of\nteachers and students are identical. However, be-\ncause the hidden embedding dimensions of teach-\ners and students are different in our setting, we\napplied a linear transformation to the teacher‚Äôs clas-\nsification embedding to match the dimension with\nthe student model.\nIn calibrated teacher training, we trained for 3-10\nepochs with a learning rate of 2e-5. The hyperpa-\nrameter Œ≤1, the strength of the confidence penalty\nin teacher training, was chosen from {0, 0.3, 0.5,\n0.7}. For activation boundary distillation, we first\nfine-tuned the initial student model for 5-10 epochs\nwith learning rates of {6e-6, 8e-6, 1e-5}. Then, we\ndistilled for 10 epochs with learning rates of {6e-6,\n8e-6, 1e-5}. The confidence penalty strength Œ≤2\nin the refinement step and loss switch rate Œ≥ were\nchosen from {0, 0.3, 0.5, 0.7} and {0.6, 0.7 ,0.8,\n0.9}, respectively. The margin ¬µ of the activation\ntransfer loss was set to 1.0. Every hyperparameter\nwas tuned on the development set. The selected\nhyperparameters are shown in the Appendix.\nThe experiments were run on a single RTX 3090\n24 GB GPU, and the training codes were imple-\nmented in PyTorch. All experiments were repeated\nthree times with different random seeds, and the\naverage performances and standard deviations have\nbeen reported.\n4.3 Experimental results on downstream tasks\nTable 2 shows the overall experimental F1 score\nresults of the DoKTra framework on five biomed-\nical and clinical classification tasks. The initially\nfine-tuned student models are in the second and\nfourth rows and the DoKTra framework is applied\nto both, as shown in the third and fifth rows.\nAs shown in the third and fifth rows, the classi-\nfication performances of biomedical and clinical\ndownstream tasks are significantly improved by ap-\nplying our proposed framework, when compared to\nthe initial student models. This implies that distill-\ning the activation patterns of the neurons from the\ncalibrated teacher model can transfer its domain-\nspecific knowledge and thus improve the task per-\nformance in the domain on which the student has\nnot yet been pre-trained.\nBy applying the DoKTra framework, the\nALBERT-xlarge student model was able to retain\n99.72% of the teacher model performance on an\naverage. ALBERT has two advantages: a small\nnumber of parameters and high performance (Lan\net al., 2019). Applying our framework to ALBERT\nallowed us to obtain a student model with per-\nformance comparable to that of the teacher with\nhalf the parameters. In other words, we success-\nfully transferred domain-specific knowledge to AL-\nBERT while maintaining its existing advantages.\nConsequently, the distilled ALBERT achieved a\nhigher performance than the teacher model on\nChemProt and DDI.\nThe RoBERTa model that was applied to the pro-\nposed framework outperformed the teacher model\non an average, specifically in four of five down-\nstream tasks (ChemProt, DDI, i2b2, and HoC).\nRoBERTa‚Äôs performance was already similar to\nthe teacher model in the initial fine-tuning stage be-\ncause it was pre-trained with more data than BERT\nand exhibited a greater robustness. The results on\nRoBERTa imply that our proposed framework can\nbe effectively applied to emerging and advanced\npre-trained language models. In other words,\ndomain-specific knowledge can be transferred into\nadvanced models without a time-consuming pre-\n1663\nDataset BioBERT RoBERTa RoBERTa\n-ft -PM-ft -DoKTra\nChemProt 76.20 79.00 78.04\nGAD 81.59 81.16 81.38\nDDI 80.05 81.39 82.25\ni2b2 74.14 78.83 75.65\nHoC 84.21 86.11 85.34\nAvg. 79.24 80.90 80.53\nTable 3: Performance comparison between existing pre-\ntrained model and DoKTra. (bold for the best, underline\nfor the second best)\ntraining and perturbing the model‚Äôs efficacy in the\ngeneral domain.\n4.4 Performance comparisons\nTo compare our approach with the in-domain\npre-training method, we used RoBERTa-PM-large\n(Lewis et al., 2020), which is a RoBERTa-large\nmodel additionally pre-trained with a large biomed-\nical and clinical corpus consisting of 14 billion\nwords. We fine-tuned the RoBERTa-PM for each\ntask.\nTable 3 shows the classification performance of\nBioBERT, RoBERTa-PM, and our approach in five\nbiomedical and clinical tasks. As mentioned be-\nfore, our best model outperformed the BioBERT\n(teacher) model on four of the five tasks. Notably,\nour approach even outperformed RoBERTa-PM\non two tasks and demonstrated comparable perfor-\nmances on the others. These results are remarkable\nsince our approach spent only a few hours on each\ntask, whereas RoBERTa-PM may require several\ndays and billions of words to be pre-trained. Note\nthat RoBERTa-PM has an advantage in the i2b2\ntask since its pre-training data contains MIMIC-III\nclinical text data, while our teacher model was pre-\ntrained with only biomedical texts. In other words,\nthis implies our approach has a room for further\nimprovement when a better in-domain model is set\nas a teacher.\nWe also compared our framework with task-\nadaptive pre-training (TAPT) (Gururangan et al.,\n2020), an additional pre-training method for PLMs.\nThe TAPT approach additionally pre-trains an ex-\nisting PLM before fine-tuning it with the training\nsamples of each task. As both TAPT and DoK-\nTra only utilize the task-specific training data, they\ncan be fairly compared in terms of performance\nDataset RoBERTa\n-ft TAPT TAPT\n(3xGPU)\nRoBERTa\n-DoKTra\nChemProt 75.75 73.55 75.40 78.04\nGAD 80.17 81.85 81.41 84.47\nDDI 80.71 73.61 78.00 82.25\ni2b2 72.51 70.95 72.42 75.65\nHoC 83.98 86.39 86.45 85.34\nAvg. 79.34 77.27 78.74 81.15\nTable 4: Performance comparison between TAPT and\nDoKTra.\nand training resources. For TAPT, we additionally\npre-trained the RoBERTa-large model with each\npre-processed downstream task‚Äôs training data. We\nfollowed the hyperparameters used in TAPT except\nfor batch size and the maximum sequence length\nbecause we used the same computing resource as\nDoKTra for a fair comparison. The possible maxi-\nmum pre-training batch size with the given comput-\ning resource for the RoBERTa-large model was 36.\nSince the results of the RoBERTa-large model with\na small batch size were unstable, we also performed\na distributed training with three GPUs, resulting in\na batch size of 108.\nThe comparison results are shown in Table 4.\nNote that the performance on GAD in Table 4 was\nevaluated with the first split of a 10-fold cross-\nvalidation, while the main result in Table 3 was\nevaluated with all splits. As revealed in the re-\nsults, even though TAPT showed improved results\nin the original study with Google Cloud TPU, it\nwas unstable with the small batch size and sequence\nlength; the performances were even degraded in the\ngeneral GPU environment. Although the TAPT per-\nformance improved when the batch size increased\nthrough distributed training, the improvement was\ninadequate. This may be because of the batch size\nbeing smaller than that in the TPU environment.\nMoreover, DoKTra required less training time than\nTAPT while both methods were task-specific. For\ninstance, TAPT required a total of seven hours of\ntraining, while DoKTRa was completed in only\n1.1 hours for the ChemProt task. This is because\nDoKTra leverages the knowledge of an existing in-\ndomain PLM, thus requiring only a few fine-tuning\nand distillation steps. The comparison of TAPT and\nDoKTra using more advanced computing resources\nis left as a future work.\n1664\nDataset DoKTra - CTT DoKTra\nF1(%) LAT F1(%) LAT\nChemProt 76.20 ¬±0.20 193.75 77.42 ¬±0.04 139.79\nGAD 77.26 ¬±0.94 331.50 78.86 ¬±0.19 268.95\nDDI 82.16 ¬±0.63 131.62 82.30 ¬±0.41 98.97\ni2b2 72.82 ¬±0.30 123.29 72.98 ¬±0.07 92.20\nTable 5: Comparison of average classification perfor-\nmance and loss values with or without teacher calibra-\ntion. (CTT: calibrated teacher training)\n4.5 Efficacy of combining calibration and\nactivation boundary distillation\nWe conducted an experiment to verify the posi-\ntive effect of combining calibrated teacher training\nand activation boundary distillation. Because the\nentropy regularizer in calibrated teacher training\nissues penalties based on the output probability dis-\ntribution, it is difficult to intuitively understand how\nit positively affects activation boundary distillation,\nwhich uses hidden representation. Thus, we ablate\nthe calibrated teacher training steps in our frame-\nwork and compare the final performances and loss\nvalues.\nIrrespective of the use of an alternative version\n(Equation 5) during the training, the extent to which\nthe activation pattern is distilled can be intuitively\nobserved by calculating the original ‚Äúactivation\ntransfer loss‚Äù (Equation 4). The value of Equation\n4 directly refers to the number of neurons activated\ndifferently than the teacher model. For instance, if\nLAT = 500for an ALBERT model (H=2,048), it\nindicates that 500 of the 2,048 elements in the hid-\nden representation vector exhibited signs different\nto those of the teacher.\nTable 5 shows the experimental results on four re-\nlation extraction tasks with ALBERT students. As\nshown in Table 5, the application of the calibrated\nteacher training reduces the LAT and improves the\nclassification performance. In other words, cal-\nibration on the teacher training clearly aids the\nsupervision of the teacher in activation boundary\ndistillation, even though the output probability in-\nformation is not directly used in distillation.\n4.6 Ablation study\nTo observe how each component contributed to\nthe proposed framework, we conducted an abla-\ntion study. We ablated two major components:\ncalibrated teacher training (CTT) and activation\nModels F1 (%) Improvement\nBioBERT-ft (teacher) 76.20¬±0.65\nALBERT-ft (student) 73.67 ¬±0.98\n+KLD 76.40 ¬±0.36 +2.73\n+CTT+KLD 76.87 ¬±0.49 +3.20\n+ABD 76.20 ¬±0.24 +2.53\n+CTT+ABD\n(proposed method) 77.42¬±0.04 +3.75\nALBERT-ft+CPL 74.04 ¬±0.43 +0.37\nTable 6: Ablation study on the ChemProt dataset.\n(ft: fine-tuned, KLD: KL-divergence-based distilla-\ntion, CTT: calibrated teacher training, ABD: activation\nboundary distillation, ft+CPL: fine-tuned with confi-\ndence penalty loss)\nboundary distillation (ABD). The experiments\nwere performed on the ChemProt dataset, using\nthe ALBERT-xlarge model as the student architec-\nture. To ablate the calibrated teacher training, we\ntrained the teacher model using only LCE . We\ncompared the activation boundary distillation with\nKL-divergence based distillation (KLD), which pe-\nnalizes the difference between the output probabil-\nity distributions of the two models.\nTable 6 presents the results of the ablation study.\nAs we proposed, applying both calibrated teacher\ntraining and activation boundary distillation re-\nsulted in a superior performance. In particular,\nthe calibrated teacher model was able to distil its\nactivation boundary to the student model much\nmore effectively, thus improving the performance\nof the student model, as we hypothesized in the\nprevious section. Applying KL-divergence-based\ndistillation yielded positive results in terms of clas-\nsification performance. Notably, calibrated teacher\ntraining also improved the KL-divergence-based\ndistillation because it enabled the distillation of a\nconsiderably more reliable output probability, as\nreported in Menon et al. (2021). Note that applying\nthe confidence regularizer to the fine-tuning of the\nstudent model only slightly improved the perfor-\nmance, suggesting that the observed gains in our\nmodel are only partially because of the calibration\nregularizer.\n4.7 Experimental results on financial domain\nTo verify the general applicability of our approach,\nwe conducted experiments on financial sentiment\nclassification tasks. Financial sentiment analysis\n1665\nModels #Params FPB FTS Avg. Retain\nFinBERT-ft (teacher) 110M 85.70 ¬±0.59 85.88¬±0.48 85.79\nALBERT-ft (student) 60M 83.85 ¬±1.65 80.79¬±1.94 82.32\nALBERT-DoKTra 60M 86.25 ¬±0.19 86.08¬±1.82 86.17 100.44%\nRoBERTa-ft (student) 125M 85.78 ¬±0.29 81.76¬±0.48 83.77\nRoBERTa-DoKTra 125M 87.21 ¬±0.29 85.10¬±0.19 86.16 100.43%\nTable 7: Experimental results of DoKTra framework on financial domain. (ft: fine-tuned)\naims to classify the polarity of financial-related\ntext, such as financial news or tweets. Since finan-\ncial text usually contains specialized language, sev-\neral pre-training approaches have emerged (Araci,\n2019; Yang et al., 2020; Liu et al., 2021) to fill the\ngap between the general and financial domains.\nIn this study, we selected the FinBERT (Yang\net al., 2020) model as a teacher in the DoKTra\nframework and evaluated our approach on two\ntasks, the Financial PhraseBank (FPB) and Fin-\nTextSen (FTS). The Financial PhraseBank (FPB)\n(Malo et al., 2014) contains sentences from fi-\nnancial news annotated for positive, neutral, and\nnegative sentiments. The FinTextSen (FTS) (Cor-\ntis et al., 2017) consists of financial tweets from\nTwitter and StockTwits with real-valued sentiment\nscores. To transform it into a classification task,\nwe clustered the sentiment score into a 3-class la-\nbel, following Daudert et al. (2018). The Financial\nPhraseBank dataset contains 4,846 sentences, and\nwe set 10% of the examples as the test set while\npreserving the label distribution. The FinTextSen\noriginally includes 2,488 tweets, but only 1,700\ntweets are available now. We set 10% of the entire\ndata as the test set, which is similar to FPB.\nAs shown in Table 7, ALBERT-DoKTRa and\nRoBERTa-DoKTRa outperformed the FinBERT-ft\nteacher on financial downstream tasks. Note that\nwe used the RoBERTa-base model in this section\nbecause of the training stability. This result sug-\ngests that DoKTra can be applied regardless of\nthe domain and can be an efficient alternative to\nin-domain pre-training.\n5 Conclusion\nIn this study, we proposed the DoKTra framework\nas a domain knowledge transfer method for PLMs.\nThe experimental results from the biomedical, clini-\ncal, and financial domain downstream tasks demon-\nstrated that our proposed framework could trans-\nfer domain-specific knowledge into a PLM, while\npreserving its own expressive advantages without\nany further pre-training with additional in-domain\ndata. We employed advanced models as the stu-\ndent model and verified the future applicability\nof our framework to emerging language models\nby achieving even higher performances than the\nteacher model. However, the limitations of our\napproach are that it is task-specific and was evalu-\nated only in classification tasks. Our future studies\nwould focus on developing the proposed frame-\nwork as a task-agnostic method and evaluating it\non various tasks.\nAcknowledgements\nThis research was supported by the Bio-Synergy\nResearch Project (NRF-2016M3A9C4939665) of\nthe Ministry of Science and ICT through the\nNational Research Foundation of Korea (NRF)\nand the NRF grant funded by the Korean gov-\nernment (Ministry of Science and ICT) (NRF-\n2021R1A2C2006268), and partly supported by In-\nstitute for Information and communications Tech-\nnology Promotion (IITP) grant funded by the Korea\ngovernment (MSIP) [No. 2019-0-01842, Artificial\nIntelligence Graduate School Program (GIST)].\nReferences\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models. arXiv preprint\narXiv:1908.10063.\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan\nH√∂gberg, Ulla Stenius, and Anna Korhonen. 2016.\nAutomatic semantic classification of scientific litera-\nture according to the hallmarks of cancer. Bioinfor-\nmatics, 32(3):432‚Äì440.\n√Älex Bravo, Janet Pi√±ero, N√∫ria Queralt-Rosinach,\nMichael Rautschka, and Laura I Furlong. 2015. Ex-\ntraction of relations between genes and diseases from\ntext and large-scale data analysis: implications for\ntranslational research. BMC bioinformatics, 16(1):1‚Äì\n17.\n1666\nJang Hyun Cho and Bharath Hariharan. 2019. On the\nefficacy of knowledge distillation. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 4794‚Äì4802.\nDongha Choi and Hyunju Lee. 2020. Extracting\nchemical‚Äìprotein interactions via calibrated deep\nneural network and self-training. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing: Findings, pages 2086‚Äì\n2095.\nKeith Cortis, Andr√© Freitas, Tobias Daudert, Manuela\nHuerlimann, Manel Zarrouk, Siegfried Handschuh,\nand Brian Davis. 2017. Semeval-2017 task 5: Fine-\ngrained sentiment analysis on financial microblogs\nand news. Association for Computational Linguistics\n(ACL).\nTobias Daudert, Paul Buitelaar, and Sapna Negi. 2018.\nLeveraging news sentiment to improve microblog\nsentiment classification in the financial domain. In\nProceedings of the First Workshop on Economics and\nNatural Language Processing, pages 49‚Äì54.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171‚Äì\n4186.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2020. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. arXiv preprint arXiv:2007.15779.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning, pages 1321‚Äì1330. PMLR.\nSuchin Gururangan, Ana Marasovi ¬¥c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342‚Äì8360.\nByeongho Heo, Minsik Lee, Sangdoo Yun, and\nJin Young Choi. 2019. Knowledge transfer via dis-\ntillation of activation boundaries formed by hidden\nneurons. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 33, pages 3779‚Äì3787.\nMar√≠a Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMart√≠nez, and Thierry Declerck. 2013. The ddi\ncorpus: An annotated corpus with pharmacological\nsubstances and drug‚Äìdrug interactions. Journal of\nbiomedical informatics, 46(5):914‚Äì920.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nJangho Kim, SeongUk Park, and Nojun Kwak. 2018.\nParaphrasing complex network: network compres-\nsion via factor transfer. In Proceedings of the 32nd\nInternational Conference on Neural Information Pro-\ncessing Systems, pages 2765‚Äì2774.\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\nMartƒ±n P√©rez P√©rez, Jes√∫s Santamar√≠a, Gael P√©rez\nRodr√≠guez, Georgios Tsatsaronis, and Ander Intx-\naurrondo. 2017. Overview of the biocreative vi\nchemical-protein interaction track. In Proceedings of\nthe sixth BioCreative challenge evaluation workshop,\nvolume 1, pages 141‚Äì146.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234‚Äì1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146‚Äì157.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,\nand Jun Zhao. 2021. Finbert: A pre-trained finan-\ncial language representation model for financial text\nmining. In Proceedings of the Twenty-Ninth Interna-\ntional Conference on International Joint Conferences\non Artificial Intelligence, pages 4513‚Äì4519.\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\nlenius, and Pyry Takala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. Journal of the Association for Information\nScience and Technology, 65(4):782‚Äì796.\nAditya K Menon, Ankit Singh Rawat, Sashank Reddi,\nSeungyeon Kim, and Sanjiv Kumar. 2021. A sta-\ntistical perspective on distillation. In International\nConference on Machine Learning, pages 7632‚Äì7642.\nPMLR.\nJeremy Nixon, Michael W Dusenberry, Linchuan Zhang,\nGhassen Jerfel, and Dustin Tran. 2019. Measuring\ncalibration in deep learning. In CVPR Workshops,\nvolume 2.\n1667\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of bert and elmo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58‚Äì65.\nGabriel Pereyra, George Tucker, Jan Chorowski, ≈Åukasz\nKaiser, and Geoffrey Hinton. 2017. Regularizing\nneural networks by penalizing confident output dis-\ntributions. arXiv preprint arXiv:1701.06548.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2014. Fitnets: Hints for thin deep nets.\narXiv preprint arXiv:1412.6550.\nLorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto,\nMichele Piana, and Alessandro Verri. 2004. Are\nloss functions all the same? Neural computation,\n16(5):1063‚Äì1076.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323‚Äì4332.\n√ñzlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text.\nJournal of the American Medical Informatics Associ-\nation, 18(5):552‚Äì556.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model\nfor financial communications. arXiv preprint\narXiv:2006.08097.\nJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim.\n2017. A gift from knowledge distillation: Fast opti-\nmization, network minimization and transfer learning.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4133‚Äì4141.\nA Appendix\nA.1 Hyperparameter setting\nIn this section, we report the searching scheme and\nactual values of the hyperparameters used by us.\nIn all cases, we set the batch size to the maximum\nthat a single GPU can process, with 128 being the\nmaximum sequence length.\nIn calibrated teacher training, we first select the\nnumber of epochs and the learning rate as the de-\nfault values of the BioBERT code and slightly\nchange the number of epochs ( e) for the unre-\nported tasks from BioBERT. Then, we select the\nstrength of the confidence regularization (Œ≤1) by a\ngrid search in terms of the F1 score and expected\ncalibration error (ECE) on the development set.\nThe formula for calculating ECE is as follows:\nacc(Bm) = 1\n|Bm|\nX\ni‚ààBm\n1( ÀÜyi = yi),\nconf(Bm) = 1\n|Bm|\nX\ni‚ààBm\nÀÜpi,\nECE =\nMX\nm=1\n|Bm|\nn |acc(Bm) ‚àí conf(Bm)|,\nwhere Bm is the m-th bin, ÀÜyi and yi indicate the\npredicted and true labels of the i-th sample in the\nbin, and ÀÜpi is the output prediction probability. n is\nthe number of total examples. A low ECE value im-\nplies that the model generates an output probability\nsimilar to its accuracy, and thus, is well-calibrated.\nThe actual values of the hyperparameters for the\ncalibrated teacher training are summarized in Table\nA1.\nIn activation boundary distillation, we perform\na grid search to determine the number of epochs\n(e1) and learning rate (lr1) for initial student fine-\ntuning. Then, we conduct another grid search of the\nlearning rate (lr2), number of epochs (e2), weight\nof the confidence penalty (Œ≤2), and loss switch rate\n(Œ≥) for the distillation and refinement steps. Both\nsearches are performed on the development set.\nThe actual values of the hyperparameters for the\nALBERT student are summarized in Table A2. For\nthe RoBERTa model as a student, we use the same\nteacher with ALBERT. The hyperparameters of the\nactivation boundary distillation for the RoBERTa\nstudent are searched in the same manner with the\nALBERT and summarized in Table A3.\nA.2 Experimental details for financial domain\nIn this section, we report on the details of two fi-\nnancial downstream task datasets, the experimental\ndetails, and hyperparameters of the financial task\nexperiments.\nwe used the pre-trained FinBERT-base model\n(L=12, H=768, A=12) with the original vocabulary.\nWe used ALBERT-xlarge (L=24, H=2048, A=32)\nand RoBERTa-base (L=12, H=768, A=12) as the\nstudents. The hyperparameters are searched in the\n1668\nsame way as the experiments for the biomedical\ndomain. The actual values of the hyperparameters\nfor the calibrated teacher training and activation\nboundary distillation with ALBERT and RoBERTa\nare summarized in Tables A4, A5, and A6.\nDataset CTT\ne Œ≤ 1\nChemProt 5 0.3\nGAD 3 0.7\nDDI 5 0.3\ni2b2 5 0.3\nHoC 10 0\nTable A1: The hyperparameters for calibrated teacher\ntraining\nDataset ABD\ne1 lr1 e2 lr2 Œ≤2 Œ≥\nChemProt 10 6e-6 10 1e-5 0.5 0.9\nGAD 5 6e-6 10 1e-5 0.3 0.9\nDDI 10 8e-6 10 1e-5 0.7 0.9\ni2b2 10 1e-5 10 1e-5 0.5 0.9\nHoC 10 1e-5 10 6e-6 0 0.6\nTable A2: The hyperparameters for activation boundary\ndistillation of the ALBERT model\nDataset ABD\ne1 lr1 e2 lr2 Œ≤2 Œ≥\nChemProt 5 1e-5 10 1e-5 0.5 0.8\nGAD 5 1e-5 10 1e-5 0.5 0.9\nDDI 10 1e-5 10 1e-5 0.7 0.8\ni2b2 5 1e-5 10 1e-5 0.5 0.8\nHoC 10 1e-5 10 6e-6 0 0.6\nTable A3: The hyperparameters for activation boundary\ndistillation of the RoBERTa model.\nDataset CTT\ne Œ≤ 1\nFPB 5 0.7\nFTS 5 0.3\nTable A4: The hyperparameters for calibrated teacher\ntraining for the financial domain.\nDataset ABD\ne1 lr1 e2 lr2 Œ≤2 Œ≥\nFPB 10 6e-6 10 1e-5 0.0 0.9\nFTS 10 6e-6 10 6e-6 0.1 0.8\nTable A5: The hyperparameters for activation boundary\ndistillation of the ALBERT model for the financial do-\nmain.\nDataset ABD\ne1 lr1 e2 lr2 Œ≤2 Œ≥\nFPB 5 1e-5 10 1e-5 0.0 0.9\nFTS 10 1e-5 10 6e-6 0.5 0.9\nTable A6: The hyperparameters for activation boundary\ndistillation of the RoBERTa model for the financial\ndomain.\n1669",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8153761029243469
    },
    {
      "name": "Distillation",
      "score": 0.8110566139221191
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6217346787452698
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6166168451309204
    },
    {
      "name": "Language model",
      "score": 0.6141777634620667
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5750253200531006
    },
    {
      "name": "Machine learning",
      "score": 0.5682052373886108
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.49366697669029236
    },
    {
      "name": "Domain knowledge",
      "score": 0.48803940415382385
    },
    {
      "name": "Natural language processing",
      "score": 0.48016706109046936
    },
    {
      "name": "Mathematics",
      "score": 0.09006431698799133
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39534123",
      "name": "Gwangju Institute of Science and Technology",
      "country": "KR"
    }
  ]
}