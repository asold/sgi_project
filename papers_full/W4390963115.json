{
  "title": "EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis",
  "url": "https://openalex.org/W4390963115",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2133740309",
      "name": "Liu ZhiWei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4319671723",
      "name": "Yang, Kailai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2372027839",
      "name": "Zhang Tian-lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2391657347",
      "name": "Xie, Qianqian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2298150338",
      "name": "Ananiadou, Sophia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2739499354",
    "https://openalex.org/W3034323190",
    "https://openalex.org/W4385968090",
    "https://openalex.org/W4393357787",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4310720906",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W2744525623",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2806227953",
    "https://openalex.org/W2887461517",
    "https://openalex.org/W4313398738",
    "https://openalex.org/W2741036097",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034854575",
    "https://openalex.org/W3152907744",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W4285010599",
    "https://openalex.org/W1574447377",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W3037611961",
    "https://openalex.org/W3000369298",
    "https://openalex.org/W4388468138",
    "https://openalex.org/W4387322679",
    "https://openalex.org/W3101540846",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3172570965",
    "https://openalex.org/W4224035554",
    "https://openalex.org/W4386977634",
    "https://openalex.org/W3000271234",
    "https://openalex.org/W4387075354",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W3088107006",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4388994251",
    "https://openalex.org/W4385436516",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4239946314",
    "https://openalex.org/W4386114394",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4225527411",
    "https://openalex.org/W2066064791",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4387797313",
    "https://openalex.org/W4309342899",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4380352301",
    "https://openalex.org/W3197380479",
    "https://openalex.org/W2251124635",
    "https://openalex.org/W2963662881"
  ],
  "abstract": "Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of LLMs, researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on various classification and regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 14 tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our model with a variety of LLMs on AEB, where our models outperform all other open-sourced LLMs, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools.",
  "full_text": "EmoLLMs: A Series of Emotional Large Language Models and\nAnnotation Tools for Comprehensive Affective Analysis\nZhiwei Liu‚àó\nThe University of Manchester\nManchester, United Kingdom\nzhiwei.liu-\n2@postgrad.manchester.ac.uk\nKailai Yang\nThe University of Manchester\nManchester, United Kingdom\nkailai.yang@postgrad.manchester.ac.uk\nQianqian Xie‚Ä†\nThe University of Manchester\nManchester, United Kingdom\nxqq.sincere@gmail.com\nTianlin Zhang\nThe University of Manchester\nManchester, United Kingdom\nzhangtianlin668@gmail.com\nSophia Ananiadou\nThe University of Manchester\nManchester, United Kingdom\nArtificial Intelligence Research Center\nTokyo, Japan\nsophia.ananiadou@manchester.ac.uk\nABSTRACT\nSentiment analysis and emotion detection are important research\ntopics in natural language processing (NLP) and benefit many down-\nstream tasks. With the widespread application of large language\nmodels (LLMs), researchers have started exploring the application\nof LLMs based on instruction-tuning in the field of sentiment analy-\nsis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emo-\ntions), and overlook the regression tasks (e.g. sentiment strength\nor emotion intensity), which leads to poor performance in down-\nstream tasks. The main reason is the lack of comprehensive affec-\ntive instruction tuning datasets and evaluation benchmarks, which\ncover various affective classification and regression tasks. Moreover,\nalthough emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehen-\nsive affective annotations. In this paper, we propose EmoLLMs, the\nfirst series of open-sourced instruction-following LLMs for compre-\nhensive affective analysis based on fine-tuning various LLMs with\ninstruction data, the first multi-task affective analysis instruction\ndataset (AAID) with 234K data samples based on 3 classification\ntasks and 2 regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 8\nregression tasks and 6 classification tasks from various sources and\ndomains to test the generalization ability of LLMs. We propose a\nseries of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our models with a variety\nof LLMs and sentiment analysis tools on AEB, where our models\noutperform all other open-sourced LLMs and sentiment analysis\n‚àóCorresponding author\n‚Ä†Qianqian is now affiliated with Yale University. The work was done while she was at\nThe University of Manchester.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\n¬© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/XXXXXXX.XXXXXXX\ntools, and surpass ChatGPT and GPT-4 in most tasks, which shows\nthat the series of EmoLLMs achieve the ChatGPT-level and GPT-\n4-level generalization capabilities on affective analysis tasks, and\ndemonstrates our models can be used as affective annotation tools.\nThis project is available at https://github.com/lzw108/EmoLLMs/.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíNatural language processing ;\nLanguage resources ; ‚Ä¢ Information systems ‚ÜíSentiment\nanalysis.\nKEYWORDS\nSentiment analysis, emotion detection, large language models, af-\nfective instruction dataset, affective evaluation benchmark\nACM Reference Format:\nZhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, and Sophia Anani-\nadou. 2024. EmoLLMs: A Series of Emotional Large Language Models and\nAnnotation Tools for Comprehensive Affective Analysis. In Proceedings of\nthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/XXXXXXX.XXXXXXX\n1 INTRODUCTION\nEmotions and sentiments play a crucial role in shaping our lives.\nOur words and actions serve as indicators of our emotional states\n[27]. Leveraging natural language processing (NLP) techniques\nsuch as Emotion Detection (ED) and Sentiment Analysis (SA), we\ncan delve into the analysis of human interactions, enabling us\nto comprehend people‚Äôs emotional responses toward particular\nsubjects [18]. Specifically, SA tasks typically involve predicting\nthe polarity (usually positive, negative, or neutral), along with\nthe strength of this tone [41], and emotion detection tasks often\ninvolve classifying data into fine-grained emotion categories (e.g.\nEkman [16], Plutchik [35]) or predicting the intensity of emotions\n[46]. These affective information are proven as useful features for\nmany downstream tasks, including mental health analysis [ 55],\nmisinformation detection [29], and empathetic dialogue systems\n[31].\narXiv:2401.08508v2  [cs.CL]  18 Jun 2024\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou\nPre-trained language models (PLMs) such as BERT [ 11] and\nRoBERTa [28], have shown excellent performance in classification\ntasks. Many studies have applied them to sentiment analysis or\nemotion detection tasks [4, 26, 50]. However, these PLMs are limited\nby the scale of model parameters and the training corpus, resulting\nin a lack of comprehensive understanding and generalization ability\nfor complex tasks [52], which restricts the effectiveness of affective\nanalysis, especially in affective regression tasks [ 53]. Compared\nto PLMs, LLMs have the characteristic of having an enormous\nparameter size, typically reaching hundreds of billions or more,\nwhich gives them stronger generalization abilities in downstream\ntasks and enables them to handle tasks with intricate complexi-\nties [57]. Many researchers have started exploring the application\nof LLMs in the field of sentiment analysis, which achieved excel-\nlent performance by fine-tuning open-source LLMs on sentiment\nanalysis tasks [24, 52, 57]. However, these studies only focus on sen-\ntiment/emotion classification tasks and overlook regression tasks\n(e.g. sentiment strength, emotion intensity), which provide more\nfine-grained affective features [1] and are proven useful in many\nscenarios [9, 36, 56]. The major reason is the lack of a comprehen-\nsive instruction-based sentiment analysis dataset and evaluation\nbenchmark.\nMoreover, though emotional information is proven useful for\ndownstream tasks, existing downstream datasets lack emotion-\nrelated resources such as sentiment/emotion labels. Therefore, many\nworks use affective analysis tools (e.g. VADER [21], TextBlob1) to\nprovide sentiment annotations. For example, in [17], the authors\nutilized the TextBlob library to calculate sentiment scores and fed\nthem into a depression detection classifier. Additionally, some stud-\nies employ transfer learning methods, by applying models trained\non other sentiment analysis or emotion-labeled datasets to automat-\nically annotate the emotions expressed in downstream task datasets\n[7, 13]. However, these tools or methods can only annotate one\naspect of sentiment analysis tasks, resulting in limited coverage of\nemotional features.\nTo address the above issues, we propose a suite of LLMs, instruction-\ntuning datasets, and an evaluation benchmark for multi-task af-\nfective analysis. We first construct the multi-task affective analy-\nsis instruction dataset (AAID) with 234K data samples to support\nLLM instruction tuning, which is based on SemEval-2018 Task1:\nAffect in Tweet [ 32, 33], including five tasks: emotion intensity\nregression, ordinal classification of emotion intensity, sentiment\nstrength regression, sentiment classification, and multi-label emo-\ntion classification. Based on the AAID dataset, we propose a series\nof emotional large language models (EmoLLMs), the first open-\nsourced instruction-following LLMs for comprehensive affective\nanalysis, by performing multi-task instruction tuning on LLMs. To\nevaluate the performance and generalizability of EmoLLMs, we\nalso construct an affective evaluation benchmark (AEB) based on\n14 affective analysis datasets collected from various platforms and\nsources, which include 8 regression tasks and 6 classification tasks.\nBased on AEB, we evaluate EmoLLMs, a variety of open-sourced\nLLMs, close-sourced LLMs (i.e. ChatGPT and GPT-4), and several\nsentiment analysis tools. The experimental results indicate that\nthe series of EmoLLMs overtake all other open-sourced LLMs, and\n1https://textblob.readthedocs.io/\nsentiment analysis tools, and exceed ChatGPT and GPT-4 in 7 re-\ngression tasks and 4 classification tasks. These results demonstrate\nthat EmoLLMs achieve a comparable capability with ChatGPT and\nGPT-4 in most affective analysis tasks. EmoLLMs can serve as\ncomprehensive affective annotation tools for annotating data from\ndifferent platforms and sources.\nOur main contributions are as follows:\n‚Ä¢We build AAID, the first multi-task affective analysis instruc-\ntion tuning data, and AEB, the first affective generalization\ntesting instruction benchmark.\n‚Ä¢We introduce a series of EmoLLMs, the first open-source\ninstruction following LLMs for comprehensive affective anal-\nysis.\n‚Ä¢We compare EmoLLMs with other LLMs on AEB. Addition-\nally, we conduct a comprehensive analysis of the affective\nanalysis capabilities of ChatGPT and GPT-4. Our models\nachieve SOTA performance on the AEB dataset compared\nto other open-sourced LLMs and present ChatGPT-level and\nGPT-4-level generalization capabilities, establishing their\npotential as effective tools for affective annotation.\nThe structure of this paper is as follows: Section 2 introduces the\nrelated work about sentiment analysis models and open-sourced\nLLMs. Section 3 introduces the proposed method. Specifically, Sec-\ntion 3.1 introduce the task definition. Section 3.2 and Section 3.3\npresent the construction process of AAID and AEB respectively.\nSection 3.4 introduces the training process of EmoLLMs. Section 4\npresents the experiment results on AEB and analyses the perfor-\nmance of each model. Section 5 concludes this paper by summariz-\ning our findings. Section 6 discusses the real-world applications of\nEmoLLMs, limitations, and future work.\n2 RELATED WORK\n2.1 Affective Analysis Model\nThere have been various affective analysis tools proposed, such as\nVADER [21], and TextBlob. Although these tools are convenient to\nuse, their effectiveness in sentiment analysis is not ideal [ 19]. In\nrecent years, many studies have focused on fine-tuning PLMs to en-\nhance their capabilities in the field of sentiment analysis. Bello et al.\n[4] combine BERT with other deep learning models (e.g. CNN, RNN,\nLSTM) to improve the ability of the model in short and simple text\nsentiment analysis. Liao et al. [26] propose a multi-task model based\non RoBERTa for aspect-category sentiment analysis. Yin et al. [50]\npropose the SentiBERT model, which focuses on the field of senti-\nment analysis. SentiBERT integrates a recursive constituency tree\nbased on BERT to better capture compositional sentiment semantics.\nRecently, numerous studies have embarked on investigating the\nutilization of LLMs in sentiment analysis, resulting in remarkable\nperformance gains in sentiment analysis tasks. Zhang et al. [ 52]\npropose a retrieval-augmented LLM for financial sentiment analy-\nsis, which utilizes additional background information from external\nsources and outperforms LLM baselines by 15% and 48%. Similarly,\nLei et al. [ 24] also use a simple yet effective retrieval module to\nenhance the emotion recognition capability of LLM in dialogue.\nZhang et al. [57] develop a context and emotion knowledge-tuned\nLLM, namely DialogueLLM, obtained by fine-tuning LLM with mul-\ntimodal (i.e., texts and videos) emotional dialogues, which achieved\nEmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nSOTA results on three emotion recognition in conversations (ERC)\ndatasets. However, these PLMs and LLMs only focus on individual\naspects of affective analysis, lacking the ability to predict sentiment\nstrength and emotion intensity.\n2.2 Open Sourced Large Language Models\nAlthough ChatGPT and GPT-4 have shown excellent performance\nin various fields, their closed-source availability affects the progress\nof scientific research. Therefore, numerous studies are dedicated\nto democratizing LLMs, such as the LLaMA series [ 42, 43], OPT\nseries [54], BLOOM series [45], and Falcon [34]. Based on the open-\nsource LLMs, many efforts have been made to develop models\nwith instruction-following capabilities like ChatGPT by training on\nextensive instruction-tuning datasets (e.g. Alpaca2 and the Vicuna3).\nRecently, there has been a lot of domain-specific work aimed at\nimproving the performance of LLM in specific domains by training\non domain-specific instruction datasets. Such as FinMA [47] in the\nfinance domain, MentalLLaMA [48] in the mental health domain,\nTimeLlaMA [51] used for temporal reasoning, and ExTES-LLaMA\n[58] in emotional support chatbots. Our work is the first open-\nsourced LLM series for comprehensive multitask affective analysis.\n3 METHODS\nThe goal of this work is to evaluate and enhance the comprehensive\nand complex affective analysis capabilities of LLMs. To achieve this\nobjective, we build the first affective analysis instruction dataset\n(AAID) to support LLMs tuning for comprehensive affective anal-\nysis tasks. We propose EmoLLMs, a series of emotional LLMs by\nfine-tuning LLMs based on AAID. Furthermore, we construct a\ncomprehensive affective evaluation benchmark to test the general-\nization ability of LLMs.\n3.1 Task Definition\nSimilar to [48] in handling mental health analysis tasks, we also\napproach affective analysis as a generative task, where a generative\nmodel (i.e., an autoregressive language model ùëÉùúô (ùë¶|ùë•)parameter-\nized by pre-trained weightsùúô) is employed as the foundation, which\nis unlike previous discriminative and regression models. This model\nis capable of simultaneously addressing ùëÅ affective analysis tasks,\nsuch as sentiment polarity and strength prediction, emotion clas-\nsification and intensity prediction. Each task t is represented by\na subset of training context-target pairs: ùê∑ùë° = (ùëûùë°\nùëñ ,ùëüùë°\nùëñ )ùëñ=1,2,...ùëÅùë°\n,\nwhere ùëûis a token sequence containing the task description, target\ntext, and query, and ùëü is another sequence containing the query\nanswer (i.e., classification result or regression result). All subsets are\ncombined into a training dataset: ùê∑. The model is optimized based\non this merged data, aiming to maximize the conditional language\nmodeling objective to enhance the accuracy of predictions.\n3.2 Instruction Tuning Data Building\nWe build the instruction dataset based on the SemEval-2018 Task\n1: Affect in Tweets, which includes a series of highly annotated\nsentiment analysis subtasks [32, 33].\n2https://crfm.stanford.edu/2023/03/13/alpaca.html\n3https://lmsys.org/blog/2023-03-30-vicuna/\nTable 1: Statistics of the data. ‚ÄôRaw‚Äô denotes the raw data from\nSemEval-2018 Task 1: Affect in Tweets. ‚ÄôInstruction‚Äô denotes\nthe converted instruction data based on raw data.\nTask Raw\n(Train/Dev)\nInstruction\n(Train/Dev)\nSource\nEI-reg, EI-oc\nanger 1701/388 17010/3880 Twitter\nfear 2252/389 22520/3890 Twitter\njoy 1616/290 16160/2900 Twitter\nsadness 1533/397 15330/3970 Twitter\nV-reg, V-oc 1181/449 11810/4490 Twitter\nE-c 6838/886 68380/8860 Twitter\n3.2.1 Raw Data. SemEval 2018 Task1 contains five subtasks: 1.\nemotion intensity regression (EI-reg), 2. ordinal classification of\nemotion intensity ( EI-oc), 3. valence (sentiment) regression ( V-\nreg), 4. ordinal classification of valence (sentiment) (V-oc), and 5.\nemotion classification (E-c).\nEI-reg: Given a tweet and an emotion E (anger, fear, joy, sadness),\ndetermine the intensity of E that best represents the mental state of\nthe tweeter‚Äîa real-valued score between 0 (least E) and 1 (most E);\nEI-oc: Given a tweet and an emotion E (anger, fear, joy, sadness),\nclassify the tweet into one of four ordinal classes (0: no E can be\ninferred. 1: low amount of E can be inferred. 2: moderate amount of\nE can be inferred. 3: high amount of E can be inferred) of intensity\nof E that best represents the mental state of the tweeter;\nV-reg: Given a tweet, determine the intensity of sentiment or\nvalence (V) that best represents the mental state of the tweeter‚Äîa\nreal-valued score between 0 (most negative) and 1 (most positive);\nV-oc: Given a tweet, classify it into one of seven ordinal classes\n(from -3: very negative to 3: very positive), corresponding to var-\nious levels of positive and negative sentiment intensity, that best\nrepresents the mental state of the tweeter;\nE-c: Given a tweet, classify it as ‚Äòneutral or no emotion‚Äô or as\none, or more, of eleven given emotions (anger, anticipation, disgust,\nfear, joy, love, optimism, pessimism, sadness, surprise, trust) that\nbest represent the mental state of the tweeter.\n3.2.2 AAID: Affective Analysis Instruction Dataset. We construct\nthe instruction dataset based on the raw data. Due to the limited\nquantity of the original dataset, we utilize 10 different task instruc-\ntions for each task to augment the training set and validation set.\nThe data statistics are presented in Table 1. Specifically, we build\ninstruction-tuning samples based on some templates. Table 2 de-\nscribes the specific instruction templates for each task, and Figure\n1 provides corresponding examples (Taking EmoLLaMA as the ex-\nample and each task selects one [task prompt ] as an example). [task\nprompt] describes the instructions for each specific task. The word\n‚ÄôTweet‚Äô can be adjusted based on the actual task. The [input text ]\nrefers to the content of the raw data. The final [ output] should\nbe adjusted based on the specific task to provide sentiment clas-\nsification, sentiment strength, emotion classification, or emotion\nintensity.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou\nEI-oc\nEmoLLaMA\nT ask: Categorize the tweet into an intensity level of the\nspecified emotion E, representing the mental state of\nthe twe eter . 0: no E can be inferred. 1: low amount of E\ncan be inferred. 2: moderate amount of E can be\ninferred. 3: high amount of E can be inferred.\nT weet:  ICQ is just making me mad!!! üò§  #icq #angry\nEmotion E: anger\nIntensity Class:  \nT ask: Evaluate the valence intensity of the\ntweeter's mental state based on the tweet,\nassigning it a r eal-valued sco re from 0 (most\nnegative) to 1 (most positive).\nT weet: Happy Birthda y shorty . Stay fine stay\nbreezy stay wavy @daviistuart üòò\nIntensity Score: \nT ask: Categorize the tweet's emotional tone as\neither 'neutral or no emotion' or identify the\npresence of one or more of the given emotions\n(anger , antici pation, disgust, fear , joy , love,\noptimism, pessimism, sadness, surprise, trust).\nT weet:  Whatever you decide to do make sure it\nmakes you #happy .\nThis tweet contains emotions:  \n3: high amount of anger can be inferred\n0.879\njoy , love, optimism\nT ask: Assign a numerical value between 0 (least\nE) a nd 1 (most E) to represent the intensity of\nemotion E expressed in the tweet.\nT weet:  @CScheiwiller can't stop smiling üòÜ üòÜ\nüòÜ\nEmotion E: joy\nIntensity Score:  \n0.896\nT ask:  Categorize the tweet into an or dinal class that best\ncharacterizes the  tweeter's mental state, considering various\ndegrees of positive and negative sen timent intensity . 3: very positive\nmental state can be inferred. 2: m oderately positive mental state\ncan b e inferred. 1: slightly positive mental state can be inferred. 0:\nneutral o r mixed mental state can be inferred. -1: slightly negative\nmental state can be inferred. -2: moderately negative mental state\ncan be inferred. -3: very negative mental state can be inferred\nT weet: Beyonc√© resentment gets me in my feelings every time. üò©\nIntensity Class: \n-3: very negative emotional\nstate can be inferred\nEI-reg\nE-c\nV -oc\nV -reg\nFigure 1: An overview of multi-task instruction tuning of EmoLLaMA for multiple affective analysis tasks.\nTable 2: Templates for constructing prompts for instruction\ndataset. [task prompt] denotes task instruction, [ input text]\nis from the raw data, [ emotion] can be anger, fear, joy, sadness,\n[output] is the output from LLM.\nTask Prompt Template\nEI-reg Task: [task prompt ] Tweet: [input text ] Emotion E:\n[emotion] Intensity score: [output]\nEI-oc Task: [task prompt ] Tweet: [input text ] Emotion E:\n[emotion] Intensity class: [output]\nV-reg Task: [task prompt] Tweet: [input text] Intensity score:\n[output]\nV-oc Task: [task prompt ] Tweet: [input text ] Intensity class:\n[output]\nE-c Task: [task prompt ] Tweet: [input text ] This tweet\ncontains emotions: [output]\n3.3 AEB: Affective Evaluation Benchmark\nBuilding\nWe first collect the test data from SemEval-2018 Task 1: Affect in\nTweets. To test the robustness of our model, a random instruction\nfrom the ten instructions used in train augment is selected for\neach instance in the test set. We also collect additional sentiment\nanalysis or emotion detection datasets from various sources and\ndomains to test the generalizability of our model. We construct the\nAEB following the template format provided in Table 2. Table 4\nshows the task prompt example for each dataset. Except for the\nfour datasets from VADER, all other datasets utilize the original\ntest dataset. Table 3 shows the statistic details.\nDatasets used in Valence Aware Dictionary for sEntiment\nReasoning (VADER) [22]: There are four datasets from different\nsocial media platforms with sentiment intensity (Valence) scores\nwithin [-4,4]: V-Amazon (Amazon reviews snippets), V-Movies\n(Movies reviews snippets, collected from rotten.tomatoes.com), V-\nNYT (New York Times editorial snippets), V-Tweet(Tweets). We\nrandomly sampled 1000 instances from each dataset for generaliz-\nability testing.\nEmoBank [5, 6]: This dataset was collected from News, blogs,\nfictions, letters etc. and contains three dimensions, which were\nmanually annotated with emotion according to the psychological\nValence-Arousal-Dominance scheme with scores within [1,5].\nStanford Sentiment Treebank (SST) [39]: It is collected from\nmovie reviews, which is the first corpus with fully labeled parse\ntrees, allowing for a comprehensive analysis of the composition-\nality of sentiment in language. In SST4, each sentence is assigned\na floating-point label that indicates the degree of positive senti-\nment, ranging from 0.0 to 1.0. while in SST5 5, each sentence is\nannotated with five labels: very positive, positive, neutral, negative,\nvery negative.\nTarget Dependent Twitter Sentiment Classification (TDT)\n[14]: It is a Twitter sentiment classification dataset collected from\npost comments for the celebrities, products, and companies, which\nis annotated manually with three labels (negative, neutral, positive).\nTo facilitate our generalizability testing, we restored the entities that\nwere masked in the original data, creating a standard sentence-level\nsentiment analysis dataset.\nGoEmotion [10]: It is a multi-label classification dataset col-\nlected from Reddit comments, which consists of 28 emotion labels,\nincluding the neutral. However, the original dataset with 28 emotion\nlabels is imbalanced. To mitigate this issue, we select the \"Ekman\"\n4https://huggingface.co/datasets/sst\n5https://huggingface.co/datasets/SetFit/sst5\nEmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nTable 3: Statistics of AEB. ‚ÄôR‚Äô denotes the regression task, followed by the intensity range. ‚ÄôSC‚Äô denotes the sentiment classification\ntask and ‚ÄôEC‚Äô denotes the emotion classification task, followed by the number of categories.\nDataset Size Type Source Dataset Size Type Source Dataset Size Type Source\nEI-reg 4068 R[0,1] Twitter V-Amazon 1000 R [-4,4] Amazon SST 2210 R [0,1] Movie reviews\nEI-oc 4068 EC(4) Twitter V-Movies 1000 R [-4,4] Movies reviews SST-5 2210 SC (5) Movie reviews\nV-reg 937 R[0,1] Twitter V-NYT 1000 R [-4,4] New York Times TDT 692 SC (3) Twitter\nV-oc 937 SC(7) Twitter V-Tweet 1000 R [-4,4] Twitter GoEmotion 5427 EC (7) Reddit\nE-c 3259 EC(11) Twitter EmoBank 1000 R [1,5] News, blogs. etc.\nTable 4: The task prompt example for each dataset in AEB.\nDataset Task prompt\nEI-reg Assign a numerical value between 0 (least E) and 1 (most E) to represent the intensity of emotion E expressed in the tweet.\nEI-oc Categorize the tweet into an intensity level of the specified emotion E, representing the mental state of the tweeter. 0: no E can be inferred. 1: low amount of E can be inferred. 2: moderate\namount of E can be inferred. 3: high amount of E can be inferred.\nV-reg Evaluate the valence intensity of the tweeter‚Äôs mental state based on the tweet, assigning it a real-valued score from 0 (most negative) to 1 (most positive).\nV-oc Categorize the tweet into an ordinal class that best characterizes the tweeter‚Äôs mental state, considering various degrees of positive and negative sentiment intensity. 3: very positive mental\nstate can be inferred. 2: moderately positive mental state can be inferred. 1: slightly positive mental state can be inferred. 0: neutral or mixed mental state can be inferred. -1: slightly negative\nmental state can be inferred. -2: moderately negative mental state can be inferred. -3: very negative mental state can be inferred.\nE-c Categorize the tweet‚Äôs emotional tone as either ‚Äôneutral or no emotion‚Äô or identify the presence of one or more of the given emotions (anger, anticipation, disgust, fear, joy, love, optimism,\npessimism, sadness, surprise, trust).\nV-A, V-M,\nV-NYT, V-T\nCalculate the sentiment intensity or valence score of the text, which should be a real number between -4 (extremely negative) and 4 (extremely positive).\nSST Calculate the sentiment score of the text, which should be a real number between 0 (extremely negative) and 1 (extremely positive).\nEmobank Determine the valence/arousal/dominance intensity of the writer‚Äôs mental state on a scale of 1 (most negative) to 5 (most positive).\nGoEmotion Categorize the text‚Äôs emotional expression, classifying it as either ‚Äôneutral‚Äô or as one or more of the specified emotions (anger, disgust, fear, joy, sadness, surprise) that reflect the writer‚Äôs\nstate of mind.\nSST5 Classify the text into one of five classes of sentiment that best represents the mental state of the text. 0: very negative, 1: negative, 2: neutral, 3: positive, 4: very positive.\nTDT Classify the text into one of three classes of sentiment that best represents the mental state of the text. -1: negative, 0: neutral, 1: positive.\noption from the dataset provided by the authors, which consists of\n7 emotion labels, including the neutral.\nSince the first five datasets are collected from the same sources as\nthe AAID, the remaining data comes from different platforms and\nsources, we divide AEB into two parts for comparison. The former\nis referred to as AEB-1, used to test the training effectiveness of the\nmodels. The latter is called AEB-2, which is suitable for testing the\ngeneralization ability of models.\n3.4 EmoLLMs\nWe build EmoLLMs by fine-tuning various LLMs based on AAID. We\ntrain three EmoLLaMA models based on LLaMA2 [43]: EmoLLaMA-\n7B, EmoLLaMA-chat-7B, EmoLLaMA-chat-13B by fine-tuning LLaMA2-\n7B, LLaMA2-chat-7B, LLaMA2-chat-13B, where LLaMA2-chat-7B\nand LLaMA2-chat-13B are the first open-source LLMs tuned with\nreinforcement learning from human feedback (RLHF) [ 40]. We\nalso train EmoOPT, and EmoBLOOM based on OPT-13B [54] and\nBLOOM-7B [45]) respectively. All models are trained for three\nepochs based on AdamW optimizer [30], utilizing early stopping\ntechniques [12] to prevent overfitting, and leveraging DeepSpeed\n[38, 49] to reduce memory usage. We set the batch size to 256. The\ninitial learning rate is set to 1e-6 with a warm-up ratio of 5%, and the\nmaximum model input length is set to 2048. All models are trained\non two Nvidia Tesla A100 GPUs, each with 80GB of memory.\n4 EVALUATION\n4.1 Base Models\nPLMs: Sentiment analysis and emotion detection are typically re-\ngarded as classification tasks, while intensity prediction is consid-\nered a regression task. We select some commonly used PLMs as\nbaseline models, which can only fine-tuned on a single task, includ-\ning BERT, RoBERTa, and one domain-specific pre-trained model\n(i.e. SentiBERT [50]). We add a fully connected neural layer to each\nmodel, which is used for classification or regression. For EI-reg and\nV-reg tasks, we utilize the mean squared error (MSE) loss function.\nFor EI-oc and V-oc tasks, we use cross-entropy loss. For multi-label\ntask E-c, we adopt binary cross-entropy with logits loss.\nZero-shot/few-shot methods (LLMs without fine-tuning):\nWith the emergence of LLMs, zero-shot and few-shot learning have\nbecome effective approaches for solving numerous tasks. We se-\nlect Falcon-7b-instruct [34], Vicuna-13b-v1.56, LLaMA2-chat-7B\nand LLaMA2-chat-13B to perform zero-shot prompting on the in-\nstruction dataset. In addition, we employ zero-shot and few-shot\nprompting methodologies with the closed-source LLM ChatGPT\n(gpt-3.5-turbo) and GPT-4 (gpt-4-1106-preview). We select at least\none piece of data for each emotion category or label category to\nserve as few-shot prompts.\nEmotion-based instruction-tuning methods: In addition to\nthe EmoLLMs series models, we also fine-tuned BART [25], T5 [37]\nusing the same instructional dataset as baseline models to further\nevaluate the effectiveness of our models.\n4.2 Evaluation Methods\nFor AEB-1, we use the official evaluation metric7, Pearson corre-\nlation coefficient (pcc), as the evaluation metric for EI-reg, EI-oc,\nV-reg, and V-oc and use accuracy, micro-F1 (mi-F1), macro-F1 (ma-\nF1) for E-c. Additionally, the official evaluation also incorporates\nsecondary evaluation metrics. For the regression tasks, they also\nuse pearson correlation for a subset of the test set that includes\nonly those tweets with intensity score greater or equal to 0.5. For\nthe ordinal classification tasks, they also use pearson correlation\nfor a subset of the test set that includes only those tweets with\n6https://huggingface.co/lmsys/vicuna-13b-v1.5\n7https://competitions.codalab.org/competitions/17751\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou\nintensity classes low X, moderate X, or high X (where X is an emo-\ntion), and use weighted quadratic kappa on the full test set, and\nadopt weighted quadratic kappa on the some-emotion subset of\nthe test set. Due to space limitations, we do not list the secondary\nevaluation, and its conclusions are consistent with the primary\nevaluation results.\nFor AEB-2, we apply accuracy, and macro-F1 as evaluation met-\nrics for affective classification tasks. For regression tasks, we use the\nPearson correlation coefficient (pcc) [8] as the evaluation metric.\n4.3 Results\n4.3.1 Results on AEB-1. The evaluation results on AEB-1 are shown\nin Table 5 (The results of open-sourced models are the average of\nfive runs). The first line is the score of the top 1 on the SemEval-2018\nTask1 leaderboard.\nComparison between EmoLLMs and PLMs, Zero-shot/few-\nshot methods: Figure 2 presents the results on AEB-1 of several\ndifferent kind of methods. For EmoLLMs, we chose EmoLLaMA-\nchat-13B, which shows the best overall performance, to compare\nwith other categories. The results in Table 5 show that EmoLLaMA-\nchat-13B outperforms all other LLMs and surpasses the top ranking8\nin the first four tasks of AEB-1. For the complex tasks EI-reg and EI-\noc, EmoLLaMA-chat-13B shows high improvement compared to top\n1, with respective increases of 3.2% (EmoLLaMA:0.831, top1:0.799)\nand 6.8% (EmoLLaMA:0.763, top1:0.695). For independent raw task\nfine-tuning methods, although these PLMs are trained on extensive\ndatasets and fine-tuned separately for each task, the results do not\nsurpass the original top 1 scores. The findings demonstrate that\ngeneral PLMs are more prone to overlooking important information\ncompared to LLMs when dealing with affective regression tasks and\nfine-grained sentiment classification tasks. For zero-shot/few-shot\nmethods, we can observe that this category of methods performs\npoorly compared to other fine-tuning approaches, especially in the\nEI-reg and EI-oc tasks. This indicates that the LLMs without fine-\ntuning struggle to handle the issue of emotion intensity effectively\n(We also test BART, T5, OPT and BLOOM in zero-shot and few-shot\nmethods, but their response is highly irrelevant).\nComparison between EmoLLMs: We can observe from Ta-\nble 5 that EmoLLMs all perform well compared with LLMs with-\nout fine-tuning. EmoT5 performs the best on the emotion classi-\nfication task E-c (ma-F1) (EmoT5:0.568, EmoLLaMA:0.545), but it\ndoes not perform as well as other models on regression tasks (e.g.\nEI-reg(ave): EmoT5:0.783, EmoLLaMA:0.831). Although EmoOPT\nslightly outperforms EmoLLaMA in a few regression tasks (e.g.\nV-reg: EmoOPT:0.887, EmoLLaMA:0.886), it still lags behind EmoL-\nLaMA in most tasks.\nIn conclusion, our proposed instruction-tuning strategy for sen-\ntiment analysis tasks outperforms PLMs and all LLMs without fine-\ntuning, achieving the best comprehensive performance. Compared\nto other instruction-tuned EmoLLMs, EmoLLaMA demonstrates a\nmore comprehensive and integrated capability in affective analysis.\n8Seernet [15] achieved first position in the first four tasks of SemEval-2018 Task1\nduring the competition phase. It is based on traditional machine learning methods,\nwhich perform comprehensive data pre-processing and apply the stacking technique\nto ensemble multiple ML methods (e.g. XG Boost, Random Forest).\n4.3.2 Results on AEB-2. In order to evaluate the generalizability\nof EmoLLMs, we execute experiments on the AEB-2 that are not\nincluded in the training process (Detailed descriptions can be found\nin Table 3). All models apply zero-shot method. We compare the\nseries of EmoLLMs with ChatGPT, GPT4, several open-source LLMs\n(i.e. LLaMA2-chat, Falcon, and Vicuna) and several sentiment anal-\nysis tools (i.e. VADER, TextBlob). Table 6 presents the experiment\nresults (The results of open-sourced models are the average of five\nruns). For EmoLLMs, it is worth noting that, since we use labels\nranging from 0 to 1 when fine-tuning the model on the regression\ndataset, we also use the range of 0 to 1 for predictions during the\ngeneralization testing of regression tasks. Afterward, we map these\npredictions to the corresponding range of the data.\nComparision between EmoLLMs and LLMs without fine-\ntuning: Figure 3 presents the results on AEB-2 of several different\nkind of methods. We still choose EmoLLaMA as the representative\nfor EmoLLMs. From Table 6, we can see that EmoLLaMA series\noutperform ChatGPT, GPT-4, and LLMs without fine-tuning in\nmost regression tasks. In the first four regression tasks, EmoLLaMA\novertakes GPT-4 by over 10%. Although EmoLLaMA performs less\nwell than ChatGPT and GPT-4 in SST and Emobank-Arousal, the\ndifference is less than 5%. For classification tasks, EmoLLaMA se-\nries performs better than ChatGPT and GPT-4 in the TDT task.\nIn the GoEmotion, the performance of EmoLLaMA is within a 5%\ndifference compared to ChatGPT and GPT-4. In SST5 tasks, GPT-4\nperforms exceptionally well (acc:0.543, ma-F1:0.504), as we can see\nthat ChatGPT, GPT-4, both outperform other models in SST5 and\nSST tasks. The possible reason is that the SST dataset is popular,\nand LLMs have been exposed to similar corpora during pre-training,\nwhich enables them to perform better using zero-shot methods.\nComparision between EmoLLMs: Table 6 shows that all in-\nstruction tuning LLMs perform well on AEB-2 and have good trans-\nferability except EmoBART and EmoT5. EmoBART and EmoT5 per-\nform similarly to their performance on the AEB-1 dataset, showing\npoor performance in regression tasks. Interestingly, EmoLLaMA-\nchat-7B performs the best in most tasks of the AEB-2 and even\noutperforms EmoLLaMA-chat-13B in most regression tasks. One\npossible reason is that models with a larger number of parameters\ntend to overfit during fine-tuning, which can subsequently affect\ntheir general performance ability.\nIt is worth noting that, in AEB-2 dataset, only TDT and V-Tweet\nare sourced from Twitter, while the others are collected from dif-\nferent platforms and domains. Although EmoLLMs‚Äô training data\nis only sourced from Twitter, it performs well on other platforms\nand domains, which demonstrates its excellent transferability. The\nresults also show that the performance of the current sentiment\nanalysis tools (i.e. VADER, TextBlob) is significantly inferior to that\nof EmoLLMs. Overall, the experiment results on AEB-2 illustrate\nEmoLLMs series achieves ChatGPT-level and GPT-4-level general\ncapabilities (especially EmoLLaMA) and can be used as emotion\nannotation tools.\n4.3.3 Analysis of Chatgpt and GPT-4 . On the AEB-1 dataset, Table\n5 shows that GPT-4 and GPT-4-FS perform best in zero-shot/few-\nshot methods, followed by ChatGPT and ChatGPT-FS. This illus-\ntrates the current open-sourced LLMs still have a big gap with\nChatGPT and GPT-4 in complex tasks (e.g. gaps between GPT-4\nEmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nTable 5: Evaluation results on AEB-1. Some results are referenced from [ 20, 50, 53]. ‚ÄôFS‚Äô denotes few-shot method. Unmarked\nLLMs all adopt zero-shot method. ‚Äôave‚Äô denotes macro-average. ‚Äôacc‚Äô denotes accuracy. ‚Äômi-F1‚Äô denotes micro-F1. ‚Äôma-F1‚Äô denotes\nmacro-F1. The evaluation metric for the first four tasks is the Pearson correlation coefficient. The first line is the score of the\ntop 1 on the SemEval-2018 Task1 leaderboard.\nmodel EI-reg EI-oc V-reg V-oc E-c\nave anger fear joy sadness ave anger fear joy sadness valence valence acc mi-F1 ma-F1\nLeaderboard(1) 0.799 0.827 0.779 0.792 0.798 0.695 0.706 0.637 0.720 0.717 0.873 0.856 0.609 0.724 0.592\nPLMs\nBERT-base 0.785 0.800 0.781 0.783 0.742 0.683 0.698 0.656 0.712 0.665 0.840 0.805 0.567 0.718 0.568\nRoBERTa-base 0.717 0.670 0.736 0.769 0.694 0.664 - - - - 0.845 0.772 0.563 0.721 0.536\nSentiBERT 0.722 0.724 0.740 0.731 0.691 0.665 - - - - 0.835 0.763 0.535 0.700 0.522\nZero-shot/few-shot methods\nFalcon 0.114 0.147 0.082 0.095 0.131 0.033 0.022 0.017 0.031 0.061 0.135 0.189 0.190 0.318 0.253\nVicuna 0.281 0.307 0.257 0.260 0.299 0.214 0.238 0.193 0.186 0.241 0.298 0.579 0.220 0.359 0.253\nLLaMA2-7B-chat 0.194 0.176 0.257 0.097 0.247 0.120 0.112 0.138 0.115 0.114 0.094 0.497 0.257 0.414 0.286\nLLaMA2-13B-chat 0.488 0.524 0.506 0.398 0.526 0.194 0.262 0.178 0.119 0.216 0.312 0.568 0.274 0.424 0.302\nChatGPT 0.599 0.637 0.573 0.569 0.618 0.455 0.500 0.428 0.363 0.529 0.637 0.748 0.382 0.546 0.429\nChatGPT-FS 0.550 0.572 0.482 0.587 0.560 0.473 0.502 0.410 0.407 0.573 0.739 0.791 0.413 0.563 0.466\nGPT-4 0.656 0.699 0.575 0.686 0.667 0.620 0.656 0.579 0.618 0.629 0.811 0.788 0.444 0.572 0.497\nGPT-4-FS 0.679 0.704 0.654 0.679 0.678 0.562 0.623 0.523 0.515 0.585 0.825 0.793 0.460 0.582 0.515\nEmotion-based instruction-tuning methods\nEmoBART 0.795 0.798 0.803 0.795 0.782 0.725 0.705 0.742 0.723 0.729 0.851 0.835 0.528 0.686 0.548\nEmoT5 0.783 0.785 0.797 0.798 0.751 0.717 0.703 0.733 0.726 0.707 0.852 0.836 0.559 0.712 0.568\nEmoOPT 0.825 0.827 0.830 0.837 0.805 0.753 0.739 0.751 0.762 0.759 0.887 0.843 0.532 0.680 0.550\nEmoBLOOM 0.791 0.802 0.797 0.790 0.776 0.732 0.725 0.717 0.746 0.740 0.857 0.822 0.528 0.683 0.552\nEmoLLaMA-7B 0.822 0.819 0.821 0.837 0.809 0.743 0.738 0.722 0.768 0.745 0.879 0.843 0.545 0.695 0.563\nEmoLLaMA-chat-7B 0.824 0.825 0.830 0.832 0.810 0.751 0.748 0.754 0.764 0.739 0.876 0.827 0.534 0.693 0.540\nEmoLLaMA-chat-13B 0.831 0.827 0.835 0.843 0.817 0.763 0.755 0.764 0.777 0.755 0.886 0.860 0.537 0.696 0.545\nEI-reg EI-oc V-reg V-oc E-c\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8Scores\nEmoLLaMA-chat-13B\nLeaderboard(1)\nGPT-4\nChatGPT\nVicuna\nFigure 2: Comparison between EmoLLMs and PLMs, Zero-shot/few- shot methods on AEB-1. The evaluation score for the first\nfour tasks is the pcc (EI-reg and EI-oc adopt macro-average). E-c utilizes macro-F1 score.\nand Vicuna: EI-reg(ave): 0.375, EI-oc(ave): 0.426, V-reg: 0.406, V-oc:\n0.513, E-c(macro-F1): 0.244). An interesting phenomenon is that\nin most tasks, ChatGPT and GPT-4 perform better with few-shot\nmethod than zero-shot. However, in EI-reg, ChatGPT‚Äôs zero-shot\nmethod outperforms few-shot, while in EI-oc, GPT-4‚Äôs zero-shot\nperforms better than few-shot. A possible reason is that the EI-reg\nand EI-oc tasks are more complex, and few-shot learning requires\ncareful design in order to improve the model‚Äôs performance. For\ndifferent models, there may be different understandings of few-shot\nexamples. Therefore, for complex tasks, it is necessary to design\ntargeted few-shot examples specifically for different LLMs.\nOn the AEB-2 dataset, GPT-4 and ChatGPT also perform bet-\nter than other open-sourced LLMs without fine-tuning in most\ntasks. However, compared to the performance on the AEB-1 dataset,\nthe performance gap between ChatGPT, GPT-4, and other LLMs\nwithout fine-tuning becomes smaller in several tasks (e.g. gaps\nbetween GPT-4 and Vicuna: V-A: -0.018, V-M: 0.135, and GoEmo-\ntion(acc):0.077). One possible reason is that these tasks are simpler\ncompared to the tasks in AEB-1. This further demonstrates that\nChatGPT and GPT-4 are more adept at handling complex tasks\ncompared to other open-source LLMs.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou\nTable 6: Evaluation results on AEB-2 dataset. All evaluation results are based on the zero-shot approach. ‚Äôpcc‚Äô denotes Pearson\ncorrelation coefficient. ‚ÄôV‚Äô denotes Valence. ‚ÄôV-A‚Äô denotes V-Amazon. ‚ÄôV-M‚Äô denotes V-Movies. ‚ÄôV-T‚Äô denotes V-Tweet. ‚ÄôA‚Äô denotes\narousal. ‚ÄôD‚Äô denotes dominance. ‚Äôacc‚Äô denotes accuracy.\nmodel V-A V-M V-NYT V-T SST EmoBank GoEmotion SST5 TDT\npcc pcc pcc pcc pcc V-pcc A-pcc D-pcc acc ma-F1 acc ma-F1 acc ma-F1\nVADER 0.565 0.446 0.464 0.862 0.450 - - - - - - - 0.510 0.266\nTextBlob 0.490 0.372 0.288 0.666 0.408 - - - - - - - 0.434 0.435\nFalcon 0.492 0.530 0.340 0.449 0.205 0.092 0.059 0.009 0.168 0.223 0.231 0.179 0.355 0.285\nVicuna 0.634 0.592 0.320 0.580 0.733 0.184 0.140 0.002 0.250 0.307 0.293 0.253 0.312 0.269\nLLaMA2-chat-13B 0.348 0.479 -0.011 0.300 0.811 0.237 0.248 -0.036 0.278 0.337 0.346 0.281 0.436 0.437\nChatGPT 0.601 0.709 0.419 0.560 0.854 0.554 0.320 -0.121 0.342 0.407 0.500 0.397 0.552 0.559\nGPT4 0.616 0.727 0.510 0.778 0.872 0.723 0.364 0.193 0.327 0.401 0.543 0.504 0.532 0.538\nEmoBART 0.770 0.661 0.650 0.853 0.634 0.670 0.059 0.101 0.318 0.366 0.341 0.323 0.529 0.535\nEmoT5 0.838 0.728 0.668 0.889 0.724 0.714 0.239 0.064 0.327 0.374 0.373 0.376 0.578 0.583\nEmoOPT 0.883 0.842 0.767 0.904 0.815 0.713 0.120 0.247 0.287 0.331 0.286 0.255 0.573 0.528\nEmoBLOOM 0.848 0.852 0.649 0.856 0.807 0.663 0.106 0.243 0.302 0.348 0.409 0.353 0.546 0.551\nEmoLLaMA-7B 0.866 0.838 0.732 0.902 0.819 0.717 0.219 0.209 0.330 0.366 0.296 0.260 0.599 0.579\nEmoLLaMA-chat-7B 0.885 0.835 0.797 0.910 0.822 0.728 0.192 0.226 0.371 0.392 0.400 0.362 0.554 0.554\nEmoLLaMA-chat-13B 0.868 0.815 0.780 0.906 0.797 0.726 0.332 0.218 0.350 0.369 0.412 0.399 0.574 0.578\nV-Amazon V-Movies V-NYT V-T weet SST EmoBank-V GoEmotion SST5 TDT\nDatasets\n0.0\n0.2\n0.4\n0.6\n0.8Scores\nEmoLLaMA-chat-13B\nEmoLLaMA-chat-7B\nGPT-4\nChatGPT\nVicuna\nFigure 3: Comparison between EmoLLMs and LLMs without fine-tuning on AEB-2. The evaluation score for the first six tasks\n(regression tasks) is the pcc. The last three tasks (classification tasks) utilize the macro-F1 score.\nTo sum up, there is still a certain gap between the current open-\nsource LLMs and ChatGPT, GPT-4 in affective analysis tasks. Cur-\nrently, we can only surpass ChatGPT and GPT-4 by fine-tuning on\nspecific tasks.\n5 CONCLUSION\nIn this paper, we propose EmoLLMs, a series of comprehensive\naffective analysis models and annotation tools. We also construct\na multi-task affective analysis instruction dataset (AAID) and an\naffective evaluation benchmark (AEB). We conduct a comprehensive\nanalysis of the performance of EmoLLMs, as well as a variety of\nLLMs on the AEB benchmark. The results indicate that EmoLLMs\nperform exceptionally well in both affective analysis regression\ntasks and classification tasks, achieving SOTA compared to the other\nopen-sourced LLMs, and EmoLLMs exhibit strong transferability, as\nit has achieved the generalization capabilities of ChatGPT and GPT-\n4 in various unseen affective analysis tasks. The results also show\nthat there is still a certain gap between the current open-sourced\nLLMs and ChatGPT, GPT-4 in specific domains. An ideal solution\nto address the issue is the instruction-tuning strategy employed in\nthis article, which can greatly enhance the performance of LLMs in\na specific domain and surpass ChatGPT and GPT-4 in most tasks.\n6 DISCUSSIONS\nReal-World Applications. EmoLLMs can provide high-quality\nand multiple emotional information automatically, which can be\nused for various practical applications. For example, (1) Misinforma-\ntion detection: Rumors or fake news often convey specific emotions.\nAffective features can help verify misinformation [29]. (2) Health-\ncare (e.g. mental health): The severity of depressive symptoms is\nclosely related to emotions. The main reason is that individuals\nwith depressive symptoms often struggle to regulate their emo-\ntions, leading to a decrease in emotional complexity. Therefore,\nemotional information is useful for diagnosing mental disorders\n[55]. (3) Customer service (e.g. online shopping): Conducting senti-\nment analysis on product reviews provides valuable insights into\nproduct and service quality as well as customer experience [2].\nLimitations and Future Work. Most of the publicly available\ndatasets are from the internet and social media, which have different\nexpression forms, text formats, and styles compared to other types\nEmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nof textual content. Thus, when applied to the real world, there may\nbe some biases. Additionally, current EmoLLMs are limited to Eng-\nlish text content and lack content from other languages and modal-\nities. In the future, we will introduce more datasets from different\nplatforms, domains, modalities, and languages into instruction-\ntuning data to further enhance the capabilities of EmoLLMs.\nACKNOWLEDGMENTS\nThe code in this project is based on BELLE code [ 3, 23, 44]. The\nEmoLLaMA picture in Figure 1 was generated by PIXLR 9. This\nwork is supported by the computational shared facility at the Uni-\nversity of Manchester and the scholar award from the Department\nof Computer Science at the University of Manchester. This work is\nsupported by the project JPNP20006 from New Energy and Indus-\ntrial Technology Development Organization (NEDO), the Centre\nfor Digital Trust and Society at the University of Manchester, and\nthe Manchester-Melbourne-Toronto Research Fund.\nREFERENCES\n[1] Md Shad Akhtar, Asif Ekbal, and Erik Cambria. 2020. How intense are you?\nPredicting intensities of emotions and sentiments using stacked ensemble [appli-\ncation notes]. IEEE Computational Intelligence Magazine 15, 1 (2020), 64‚Äì75.\n[2] Hashir Ali, Ehtesham Hashmi, Sule Yayilgan Yildirim, and Sarang Shaikh. 2024.\nAnalyzing amazon products sentiment: a comparative study of machine and deep\nlearning, and transformer-based techniques. Electronics 13, 7 (2024), 1305.\n[3] BELLEGroup. 2023. BELLE: Be Everyone‚Äôs Large Language model Engine. https:\n//github.com/LianjiaTech/BELLE.\n[4] Abayomi Bello, Sin-Chun Ng, and Man-Fai Leung. 2023. A BERT framework to\nsentiment analysis of tweets. Sensors 23, 1 (2023), 506.\n[5] Sven Buechel and Udo Hahn. 2017. Readers vs. writers vs. texts: Coping with\ndifferent perspectives of text understanding in emotion annotation. InProceedings\nof the 11th linguistic annotation workshop . 1‚Äì12.\n[6] Sven Buechel and Udo Hahn. 2022. Emobank: Studying the impact of annotation\nperspective and representation format on dimensional emotion analysis. arXiv\npreprint arXiv:2205.01996 (2022).\n[7] Jireh Yi-Le Chan, Khean Thye Bea, Steven Mun Hong Leow, Seuk Wai Phoong,\nand Wai Khuen Cheng. 2023. State of the art: a review of sentiment analysis\nbased on sequential transfer learning. Artificial Intelligence Review 56, 1 (2023),\n749‚Äì780.\n[8] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jing-\ndong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient.\nNoise reduction in speech processing (2009), 1‚Äì4.\n[9] Sergio Consoli, Luca Barbaglia, and Sebastiano Manzan. 2022. Fine-grained,\naspect-based sentiment analysis on economic and financial lexicon. Knowledge-\nBased Systems 247 (2022), 108781.\n[10] Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav\nNemade, and Sujith Ravi. 2020. GoEmotions: A dataset of fine-grained emotions.\narXiv preprint arXiv:2005.00547 (2020).\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[12] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and\nNoah Smith. 2020. Fine-tuning pretrained language models: Weight initializations,\ndata orders, and early stopping. arXiv preprint arXiv:2002.06305 (2020).\n[13] Diwen Dong, Fuqiang Lin, Guowei Li, and Bo Liu. 2022. Sentiment-Aware Fake\nNews Detection on Social Media with Hypergraph Attention Networks. In 2022\nIEEE International Conference on Systems, Man, and Cybernetics (SMC) . IEEE,\n2174‚Äì2180.\n[14] Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adap-\ntive recursive neural network for target-dependent twitter sentiment classifica-\ntion. In Proceedings of the 52nd annual meeting of the association for computational\nlinguistics (volume 2: Short papers) . 49‚Äì54.\n[15] Venkatesh Duppada, Royal Jain, and Sushant Hiray. 2018. SeerNet at SemEval-\n2018 Task 1: Domain Adaptation for Affect in Tweets. In Proceedings of the 12th\nInternational Workshop on Semantic Evaluation . 18‚Äì23.\n[16] Paul Ekman. 1992. An argument for basic emotions. Cognition & emotion 6, 3-4\n(1992), 169‚Äì200.\n9https://pixlr.com/image-generator/\n[17] Kuhaneswaran AL Govindasamy and Naveen Palanichamy. 2021. Depression\ndetection using machine learning techniques on twitter data. In 2021 5th inter-\nnational conference on intelligent computing and control systems (ICICCS) . IEEE,\n960‚Äì966.\n[18] Nida Manzoor Hakak, Mohsin Mohd, Mahira Kirmani, and Mudasir Mohd.\n2017. Emotion analysis: A survey. In 2017 International Conference on Com-\nputer, Communications and Electronics (Comptelix) . 397‚Äì402. https://doi.org/10.\n1109/COMPTELIX.2017.8004002\n[19] Lu He, Tingjue Yin, and Kai Zheng. 2022. They May Not Work! An evaluation\nof eleven sentiment analysis tools on seven social media datasets. Journal of\nBiomedical Informatics 132 (2022), 104142.\n[20] Amal Htait and Leif Azzopardi. 2021. Sentiment intensity prediction using neural\nword embeddings. In Proceedings of the 2021 ACM SIGIR International Conference\non Theory of Information Retrieval . 93‚Äì102.\n[21] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\nfor sentiment analysis of social media text. In Proceedings of the international\nAAAI conference on web and social media , Vol. 8. 216‚Äì225.\n[22] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\nfor sentiment analysis of social media text. In Proceedings of the international\nAAAI conference on web and social media , Vol. 8. 216‚Äì225.\n[23] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang\nMa, and Xiangang Li. 2023. Exploring the impact of instruction data scaling\non large language models: An empirical study on real-world use cases. arXiv\npreprint arXiv:2303.14742 (2023).\n[24] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang.\n2023. Instructerc: Reforming emotion recognition in conversation with a retrieval\nmulti-task llms framework. arXiv preprint arXiv:2309.11911 (2023).\n[25] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising\nsequence-to-sequence pre-training for natural language generation, translation,\nand comprehension. arXiv preprint arXiv:1910.13461 (2019).\n[26] Wenxiong Liao, Bi Zeng, Xiuwen Yin, and Pengfei Wei. 2021. An improved\naspect-category sentiment analysis model for text sentiment analysis based on\nRoBERTa. Applied Intelligence 51 (2021), 3522‚Äì3533.\n[27] Bing Liu. 2020. Sentiment analysis: Mining opinions, sentiments, and emotions .\nCambridge university press.\n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[29] Zhiwei Liu, Tianlin Zhang, Kailai Yang, Paul Thompson, Zeping Yu, and Sophia\nAnaniadou. 2023. Emotion Detection for Misinformation: A Review. arXiv\npreprint arXiv:2311.00671 (2023).\n[30] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\narXiv preprint arXiv:1711.05101 (2017).\n[31] Yukun Ma, Khanh Linh Nguyen, Frank Z Xing, and Erik Cambria. 2020. A survey\non empathetic dialogue systems. Information Fusion 64 (2020), 50‚Äì70.\n[32] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana\nKiritchenko. 2018. Semeval-2018 task 1: Affect in tweets. In Proceedings of the\n12th international workshop on semantic evaluation . 1‚Äì17.\n[33] Saif Mohammad and Svetlana Kiritchenko. 2018. Understanding Emotions: A\nDataset of Tweets to Study Interactions between Affect Categories. InProceedings\nof the Eleventh International Conference on Language Resources and Evaluation\n(LREC 2018). European Language Resources Association (ELRA), Miyazaki, Japan.\nhttps://aclanthology.org/L18-1030\n[34] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming\ncurated corpora with web data, and web data only.arXiv preprint arXiv:2306.01116\n(2023).\n[35] Robert Plutchik. 1980. A general psychoevolutionary theory of emotion. In\nTheories of emotion . Elsevier, 3‚Äì33.\n[36] Syed Arbaaz Qureshi, Gael Dias, Mohammed Hasanuzzaman, and Sriparna Saha.\n2020. Improving depression level estimation by concurrently learning emotion\nintensity. IEEE Computational Intelligence Magazine 15, 3 (2020), 47‚Äì59.\n[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research 21, 1 (2020), 5485‚Äì5551.\n[38] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deep-\nspeed: System optimizations enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 3505‚Äì3506.\n[39] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic\ncompositionality over a sentiment treebank. In Proceedings of the 2013 conference\non empirical methods in natural language processing . 1631‚Äì1642.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou\n[40] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to\nsummarize with human feedback. Advances in Neural Information Processing\nSystems 33 (2020), 3008‚Äì3021.\n[41] Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas.\n2010. Sentiment strength detection in short informal text.Journal of the American\nsociety for information science and technology 61, 12 (2010), 2544‚Äì2558.\n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[44] Cheng Wen, Xianghui Sun, Shuaijiang Zhao, Xiaoquan Fang, Liangyu Chen, and\nWei Zou. 2023. ChatHome: Development and Evaluation of a Domain-Specific\nLanguage Model for Home Renovation. arXiv preprint arXiv:2307.15290 (2023).\n[45] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha Luccioni,\nFran√ßois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100 (2022).\n[46] Hongliang Xie, Shi Feng, Daling Wang, and Yifei Zhang. 2018. A novel atten-\ntion based CNN model for emotion intensity prediction. In Natural Language\nProcessing and Chinese Computing: 7th CCF International Conference, NLPCC 2018,\nHohhot, China, August 26‚Äì30, 2018, Proceedings, Part I 7 . Springer, 365‚Äì377.\n[47] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro\nLopez-Lira, and Jimin Huang. 2023. PIXIU: A Large Language Model, Instruction\nData and Evaluation Benchmark for Finance. arXiv preprint arXiv:2306.05443\n(2023).\n[48] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, and Sophia Ananiadou.\n2023. Mentalllama: Interpretable mental health analysis on social media with\nlarge language models. arXiv preprint arXiv:2309.13567 (2023).\n[49] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari,\nXiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li,\nConnor Holmes, et al. 2023. DeepSpeed-Chat: Easy, Fast and Affordable RLHF\nTraining of ChatGPT-like Models at All Scales. arXiv preprint arXiv:2308.01320\n(2023).\n[50] Da Yin, Tao Meng, and Kai-Wei Chang. 2020. Sentibert: A transferable\ntransformer-based architecture for compositional sentiment semantics. arXiv\npreprint arXiv:2005.04114 (2020).\n[51] Chenhan Yuan, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2023. Back\nto the Future: Towards Explainable Temporal Reasoning with Large Language\nModels. arXiv preprint arXiv:2310.01074 (2023).\n[52] Boyu Zhang, Hongyang Yang, Tianyu Zhou, Muhammad Ali Babar, and Xiao-\nYang Liu. 2023. Enhancing financial sentiment analysis via retrieval augmented\nlarge language models. In Proceedings of the Fourth ACM International Conference\non AI in Finance . 349‚Äì356.\n[53] Linrui Zhang, Hsin-Lun Huang, Yang Yu, and Dan Moldovan. 2020. Affect in\nTweets: A Transfer Learning Approach. In Proceedings of the Twelfth Language\nResources and Evaluation Conference . 1511‚Äì1516.\n[54] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt:\nOpen pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n(2022).\n[55] Tianlin Zhang, Kailai Yang, Shaoxiong Ji, and Sophia Ananiadou. 2023. Emotion\nfusion for mental illness detection from social media: A survey. Information\nFusion 92 (2023), 231‚Äì246.\n[56] Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong, and Kai Shu. 2021.\nMining dual emotion for fake news detection. InProceedings of the web conference\n2021. 3465‚Äì3476.\n[57] Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi Li, Benyou Wang, and Jing\nQin. 2023. DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models\nfor Emotion Recognition in Conversations.arXiv preprint arXiv:2310.11374 (2023).\n[58] Zhonghua Zheng, Lizi Liao, Yang Deng, and Liqiang Nie. 2023. Building emotional\nsupport chatbots in the era of llms. arXiv preprint arXiv:2308.11584 (2023).",
  "topic": "Sentiment analysis",
  "concepts": [
    {
      "name": "Sentiment analysis",
      "score": 0.6281272172927856
    },
    {
      "name": "Computer science",
      "score": 0.5725330114364624
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5340145230293274
    },
    {
      "name": "Cognitive psychology",
      "score": 0.46322181820869446
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45927363634109497
    },
    {
      "name": "Expression (computer science)",
      "score": 0.4152716398239136
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4135305881500244
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4111097455024719
    },
    {
      "name": "Natural language processing",
      "score": 0.3740310072898865
    },
    {
      "name": "Psychology",
      "score": 0.3337631821632385
    },
    {
      "name": "Machine learning",
      "score": 0.3329383134841919
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}