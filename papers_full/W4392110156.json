{
    "title": "Large Language Models, Agency, and Why Speech Acts are Beyond Them (For Now) – A Kantian-Cum-Pragmatist Case",
    "url": "https://openalex.org/W4392110156",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2577243800",
            "name": "Reto Gubelmann",
            "affiliations": [
                "University of St. Gallen"
            ]
        },
        {
            "id": "https://openalex.org/A2577243800",
            "name": "Reto Gubelmann",
            "affiliations": [
                "University of St. Gallen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2617661921",
        "https://openalex.org/W2313009899",
        "https://openalex.org/W6634761788",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2097585998",
        "https://openalex.org/W4247629567",
        "https://openalex.org/W1955857676",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4212947750",
        "https://openalex.org/W4223649245",
        "https://openalex.org/W4213059411",
        "https://openalex.org/W4214518538",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4317569360",
        "https://openalex.org/W3085332162",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W4360957277",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W4242854129",
        "https://openalex.org/W3132155504",
        "https://openalex.org/W2604799547",
        "https://openalex.org/W2971649124",
        "https://openalex.org/W4210984920",
        "https://openalex.org/W6730267373",
        "https://openalex.org/W4210264624",
        "https://openalex.org/W4366283741",
        "https://openalex.org/W4285287265",
        "https://openalex.org/W4385574057",
        "https://openalex.org/W4388825151",
        "https://openalex.org/W6995730669",
        "https://openalex.org/W4294636140",
        "https://openalex.org/W2794365787",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W6630599461",
        "https://openalex.org/W2504511880",
        "https://openalex.org/W2043787485",
        "https://openalex.org/W6616509125",
        "https://openalex.org/W7075671530",
        "https://openalex.org/W6721383532",
        "https://openalex.org/W7075282181",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2888159079",
        "https://openalex.org/W2271653245",
        "https://openalex.org/W2946798032",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2983368141",
        "https://openalex.org/W1988028209",
        "https://openalex.org/W4240617655",
        "https://openalex.org/W3210886175",
        "https://openalex.org/W6726163484",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4235073904",
        "https://openalex.org/W2735153903",
        "https://openalex.org/W4235438639",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4249096017",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W1995412464",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W2157322089",
        "https://openalex.org/W2290582969",
        "https://openalex.org/W6620293842",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W1999934606",
        "https://openalex.org/W2317138749",
        "https://openalex.org/W4301883480",
        "https://openalex.org/W4241936942"
    ],
    "abstract": "Abstract This article sets in with the question whether current or foreseeable transformer-based large language models (LLMs), such as the ones powering OpenAI’s ChatGPT, could be language users in a way comparable to humans. It answers the question negatively, presenting the following argument. Apart from niche uses, to use language means to act. But LLMs are unable to act because they lack intentions. This, in turn, is because they are the wrong kind of being: agents with intentions need to be autonomous organisms while LLMs are heteronomous mechanisms. To conclude, the article argues, based on structural aspects of transformer-based LLMs, that these LLMs have taken a first step away from mechanistic artificiality to autonomous self-constitution, which means that these models are (slowly) moving into a direction that someday might result in non-human, but equally non-artificial agents, thus subverting the time-honored Kantian distinction between organism and mechanism.",
    "full_text": "Philosophy & Technology (2024) 37:32\nhttps://doi.org/10.1007/s13347-024-00696-1\nRESEARCH ARTICLE\nLarge Language Models, Agency, and Why Speech Acts\nare Beyond Them (For Now) – A Kantian-Cum-Pragmatist\nCase\nReto Gubelmann 1\nReceived: 8 August 2023 / Accepted: 3 January 2024 / Published online: 23 February 2024\n© The Author(s) 2024\nAbstract\nThis article sets in with the question whether current or foreseeable transformer-based\nlarge language models (LLMs), such as the ones powering OpenAI’s ChatGPT, could\nbe language users in a way comparable to humans. It answers the question negatively,\npresenting the following argument. Apart from niche uses, to use language means to\nact. But LLMs are unable to act because they lack intentions. This, in turn, is because\nthey are the wrong kind of being: agents with intentions need to be autonomous\norganisms while LLMs are heteronomous mechanisms. To conclude, the article argues,\nbased on structural aspects of transformer-based LLMs, that these LLMs have taken\na ﬁrst step away from mechanistic artiﬁciality to autonomous self-constitution, which\nmeans that these models are (slowly) moving into a direction that someday might result\nin non-human, but equally non-artiﬁcial agents, thus subverting the time-honored\nKantian distinction between organism and mechanism.\nKeywords Speech act · Agency · Large language model · Kant · Organism ·\nSearle · Mechanism\n1 Introduction: LLMs as Artiﬁcial Agents?\nDuring the past six years, the ﬁeld called natural language processing (NLP) has seen a\nveritable revolution, or perhaps better, a transformation: The advent of a speciﬁc type\nof neural network architecture, the so-called transformer, introduced by V aswani et al.\n(2017), has set new standards in performance and begotten a number of children that\nB Reto Gubelmann\nreto.gubelmann@protonmail.com\n1 University of St.Gallen (HSG), St. Gallen, Switzerland\n123\nR. Gubelmann\nhave made a name by themselves. BERT and GPT-3.5/4, 1 the large language models\nbehind ChatGPT, are particularly noteworthy in this regard. 2\nThis article examines whether these models, as well as models recognizably similar\nto them, could be speakers and, more generally, agents. In analogy to asking whether\nartiﬁcial intelligence is possible, or already realized, this article asks whether artiﬁcial\nagents are possible, or already realized.\nIt might seem hard to see how there could even still be a question whether mod-\nels who, according to credible corporate communications (OpenAI, 2023), pass the\nuniform bar exam in the United States (successful completion of which qualiﬁes to\npractice as a lawyer) are speakers. However, in all but a few constructed special cases,\nwhat speakers do when they speak is act: They want to convince, entertain, inform,\nthreaten, baptize, structure their own thoughts, etc. While it is a matter of empirical\nfact that state-of-the-art generative LLMs are able to produce text at a human- or\nnear-human level, it is much less clear whether they are agents. Acting is autonomous\nintentional behavior: a certain behavior can only be an action if it can be ascribed\nto a subject that pursues a certain goal with it. However, LLMs lack both autonomy\nand intentionality. They are essentially statistical devices to approximate a certain\nfunction, a mapping of one set of values to another set of values.\nIn this respect, the article will argue, they are like a tortoise, who, by unfathomable\ncosmic coincidence, ends up writing into the sand the ideal solution to an existential\nproblem that confronts a person walking on that same beach. It would be inaccurate\nto say that the tortoise told the person what to do because the behavior lacks the\nnecessary teleological structure: The tortoise had no intention to help the person out\n(while it, unlike LLMs, did likely pursue some goals with its movements on the beach).\nSimilarly, even if the likelihood that ChatGPT gives great advice to a desperate inquirer\nis much higher than in the case of the tortoise, it would be as misguided as in the case\nof the tortoise to assume that it was talking to that same inquirer, that “it” was helping\nher out.\nWithin this area, the goal of this article is threefold. First, I suggest that it is inaccu-\nrate to conceive these transformer-based LLMs as speakers, despite their impressive\nabilities, as they are unable to engage in speech acts, since they are unable to act,\nwhich in turn is due to their lack of intentions.\nSecond, based on Kantian conceptual distinctions, I suggest that this lack of inten-\ntions is because they are the wrong kind of being: they are artifacts, that is, engineered\nmechanisms without any autonomy or intrinsic goals. Organisms, in contrast, even very\nsimple ones, function to maintain themselves autonomously as long as they are what\nthey are. Hence, the notion of an artiﬁcial speaker might be a contradictio in terminis .\nEither such transformer-based models are artiﬁcial in Kant’s sense of mechanisms;\nthen, they cannot engage in speech acts, as they, lacking any autonomy whatsoever,\nin particular any self-set goals, are no agents, and hence no speakers. Or these mod-\n1 “BERT” is an acronym for “Bidirectional Encoder Representations from Transformers”, “GPT” stands\nfor “Generative Pre-trained Transformers”.\n2 A note on terminology: It is customary to call the basic set-up of a neural NLP (NNLP) system an\narchitecture, and a fully trained, usable instance of a given architecture a model.\n123\n32 Page 2 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nels fully cross the Kantian Rubicon between mechanisms and organisms and become\nfully-ﬂedged speakers; then, they are not artiﬁcial anymore.\nThird, I suggest that transformer-based LLMs might represent the ﬁrst small steps\ntowards the emergence of non-biological organisms, which might someday evolve into\nnon-biological agents and hence overturn the Kantian distinction between organism\nand mechanism.\nThe article contributes a novel Kantian perspective on the ongoing discussion\naround artiﬁcial agents in general and artiﬁcial moral agents in particular. It creates\na hopefully insightful contrast to positions such as the one by Floridi ( 2023), who\nsuggests that ChatGPT constitutes a case of agency without intelligence, or to List\n(2021), who argues that there are conditions under which AI systems could be agents.\nIn contrast, it lends support from a fresh perspective to positions such as the one by\nConstantinescu et al. ( 2022), or by Popa ( 2021). The latter also draws the important\nconnection to liability questions: if AI systems cannot be considered autonomous\nactors, then it is their creators that are ultimately liable for any harm caused by these\nsystems.\nThe article is structured as follows. First, I introduce transformer-based LLMs\nfrom a technical and historical perspective (Section 2). In Section 3, I argue, based\non considerations from pragmatism, that it is mistaken to conceive these models as\nspeakers, as they lack intentions. In Section 4, I introduce the Kantian distinction\nbetween mechanisms and organisms with a focus on autonomy. Based on this, in\nSection 5, I then suggest that, even though they are mechanisms, transformer-based\nLLMs do represent the ﬁrst steps towards non-biological organisms, and hence agents\nand speakers, questioning the time-tested Kantian distinction, and I tentatively suggest\nthat this lends support to a kind of dualism.\n2 What are Transformer-Based LLMs? – Technical-Historical\nConsideration\nIn this section, I give a brief survey of different approaches in NLP with an emphasis\non the transformer architecture. This survey prepares the ground for the arguments in\nSections 3-5.\nThe survey uses machine translation (MT) as a background use case. The cen-\ntral NLP architecture in this article, the transformer, was originally introduced in the\ncontext of machine translation. As Wilks ( 2014, p. 213) shows, MT can be seen to\nencompass both natural language understanding (NLU) and natural language gener-\nation (NLG), the two central challenges to NLP: To accurately translate from one\nlanguage into another, one has to ﬁrst accurately represent (read: “understand”) the\nmeaning of the text in the source language, which is an NLU-task, then, one has to\nphrase this meaning appropriately in the target language, which is an NLG-task.\n2.1 The Emergence of the Transformer\nHistorically, there were two main approaches in machine translation, namely rule-\nbased machine translation (RBMT) and statistical machine translation (SMT). In\n123\nPage 3 of 24 32\nR. Gubelmann\nRBMT, translation occurs via rules that are in the paradigmatic case handwritten (com-\npare Bhattacharyya, 2015, pp. 140-141). There are rules for syntactic and semantic\nanalysis, the transfer on the semantic level occurs via (human-created) lexica, and the\ngenerative steps to create a grammatical sentence in the target language again follow\ngrammatical rules speciﬁed by humans. As recently as 2018, Gatt and Emiel ( 2018,\np. 134) observe in their extensive survey of NLG-approaches, rule-based approaches\nare typically able to deliver a higher-quality output, while statistical approaches are\ncheaper and more robust.\nThis division of labor has decisively changed now in favor of statistical approaches\ndue to the introduction of the transformer architecture. As Poibeau ( 2017, p. 121)\nobserves, statistical machine translation, SMT is the most popular MT-approach today,\nwhich is mainly due to one speciﬁc sub-paradigm, so-called neural machine transla-\ntion (NMT, see Läubli et al., 2018, p. 4791), which in turn dominates thanks to the\ntransformer architecture. Conceptually speaking, NMT algorithms (like the more tra-\nditional statistical machine translation algorithms), are machine learning algorithms\n(see Goldberg, 2017, esp. Ch. 3 and 4), and most of them are supervised machine\nlearning algorithms, which means that they need annotated training data. What they\ndo not need, in turn, is meticulous engineering of hand-written rules, as is the case\nwith RBMT methods.\nUntil very recently, an NMT model typically had an encoder-decoder structure,\nwhich again maps nicely on the tasks of NLU (assigned to the encoder) and NLG\n(assigned to the decoder): The encoder takes in the sentence in the source language\nand represents it as a matrix or vector structure that is usually called the ‘context’.\nBased on this, the decoder generates a sentence in the target language. This also means\nthat the context vector, in traditional NMT architectures, constitutes an information\nbottleneck, as it must contain the entire semantics of the source language sentence that\nis needed to produce a translation. The same context vector is used to produce all of\nthe translated words.\nSince the introduction of the transformer architecture, this structure has become\nprevalent in most NLP tasks, in particular for machine translation, NLU and NLG. It\nwas introduced by V aswani et al. ( 2017).\nIt is generally agreed that the attention mechanisms (which are inspired by Bah-\ndanau et al., 2014) are major drivers for the architecture’s demonstrably superior\nperformance in MT. There are two kinds of attention-mechanisms in play here. First,\nin the decoder, so-called masked attention estimates the most important words in\nthe translation already produced for the word about to be translated. Second, the\nself-attention mechanism used both in the encoder and in the decoder considers the\nentire source sentence, either purely to bring structure into the sequence of tokens\n(in the encoder), or to emphasize these parts of the source sentence that are partic-\nularly relevant for the word currently in focus (in the decoder, thus addressing the\nbottleneck problem described two paragraphs earlier). Importantly, what the different\nself-attention layers emphasize is not determined in advance. Rather, at the beginning\nof the training process, these attention layers do not focus on anything in particular.\nBeing initialized with random values, they randomly emphasize some aspect of each\n123\n32 Page 4 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nsentence. After successful training, however, many of these attention layers come to\nassume very speciﬁc functions.\nIn an extended version of their paper, to be found on arXiv, the authors provide\nvisualizations of the work done by the self-attention layers. Based on such qualitative\nanalysis, V aswani et al. ( 2017, p. 14) examine the function of self-attention layer\nnumber ﬁve and conclude that it is “apparently involved in anaphora resolution”.\nAs Gubelmann ( 2023, p. 491–492) puts it: “What is exciting about this is not only\nthat this speciﬁc attention layer does seem to be involved in anaphora resolution and\npretty successful at it. NLP engineers have wrestled with this problem for decades; the\ntransformer seems to have solved it in a matter of 3.5 days’ autonomous training. There\nwas never an explicit decision on the side of the human engineer that this speciﬁc part\nof the mechanism should be dedicated to this task” (see also Gubelmann, 2023 for\na discussion and philosophical reﬂection of this aspect of transformer-based LLMs,\nsee Gubelmann and Toscano, 2022 for a more in-depth Kantian consideration of it).\nMore quantitative evaluations such as the one conducted by V oita et al. ( 2019)d o\nsuggest that a subset of attention heads in the encoder has indeed assumed speciﬁc,\nidentiﬁable functions (intriguingly, their research also suggests that the majority of\nencoder attention heads is simply superﬂuous).\nThe autonomy granted to these processes is always restricted by the basic structure\nof the architecture (the size and precise conﬁguration of transformer blocks, the number\nof parameters, etc.) as well as the hyper-parameters of the training. What is novel about\nthis autonomous functional differentiation as it occurs in the transformer is, to put it\nbrieﬂy, that it works even for very complex tasks such as machine translation. 3\nAs mentioned before, the transformer is trained in essentially the same way that\nany neural network model in NNLP is being trained (see Goodfellow et al., 2016, ch.\n5). This basic way consists of the following steps:\n1. Initialize the model with random parameters (this is also called “seeding” the\nmodel, see Madhyastha and Jain, 2019). In the largest and clearly best performing\nversion of the transformer used by the authors, there were 213 M parameters.\n2. Let the model predict a number of translations in the training data (consisting of\n4.5 M sentence pairs for the English-German data set and 36 M sentence pairs for\nEnglish-French).\n3. Let it measure the loss of the translations. For instance, if the correct next word\nwould be “Henne”, the decoder’s output would be compared to a vector that is\nzero everywhere except for the entry associated with the target vocabulary entry\n“Henne”.\n3 In 2018, Hassan et al. ( 2018) describe a new machine translation architecture (derived from the trans-\nformer). According to Gubelmann ( 2023, p. 492), “they argue that it has reached human parity. Läubli\net al. ( 2018) have independently veriﬁed this claim and found that the architecture developed by these\nresearchers does indeed deliver translations that are indistinguishable from translations delivered by pro-\nfessional human translators, though only if the quality of the translation is assessed on the sentence- and\nnot on the document-level.” Before the transformer, human parity was far beyond reach.\n123\nPage 5 of 24 32\nR. Gubelmann\n4. Let the model optimize the parameters using stochastic back-propagation and\ngradient descent. 4\n5. Repeat 2-4 until the maximum number of iterations has been reached. For the large\nmodel used by V aswani et al. ( 2017), this was 300k steps on 8 GPUs, taking 3.5\ndays.\nWhile the actual training described by V aswani et al. ( 2017, pp. 7-8) has some\nadditional tweaks to increase training performance, this is the basic method used. It\nis in the course of this training that the model develops subsystems that fulﬁll speciﬁc\nfunctions (by optimizing certain parameters in a speciﬁc way).\n2.2 The Abilities of Transformer-Based Models\nIn this section, I brieﬂy dive into the performance of transformer-based models in\nnatural language understanding (NLU) and natural language generation (NLG). Impor-\ntantly, unlike the original transformer used for NMT, the two kinds of models in focus\nin this section do not need speciﬁcally labeled training data for the most extensive\npart of their training, so-called pre-training. Rather, they can be trained on unlabeled\ntext, which potentially makes all text available on the internet training data for these\nmodels.\nNLU: BERT One particularly impressive NNLP architecture building on the transformer\narchitecture is called BERT, and it has been introduced by Devlin et al. ( 2019); when\nintroducing the details of BERT’s architecture, the authors simply refer to V aswani\net al. ( 2017).5 Unlike the original transformer, which was developed for machine\ntranslation, BERT is a general-purpose NLU model. This means that it is not trained\nto predict a translation of a sentence. Rather, it is intended to solve a variety of tasks\nwhich, in the case of a human being, we would clearly say presupposes understanding\nof the language in question: answering questions about texts, summarizing documents,\nsuggesting completions of sentences, etc. BERT itself was quickly superseded by\nvery similar, but better performing modiﬁcations such as RoBERTa (Liu et al., 2019),\nXLNet (Yang et al., 2019), DeBERTa (He et al., 2020) as well as smaller versions such\nas DistilBERT (Sanh et al., 2019) and Albert (Lan et al., 2019). As BERT remains the\nclassic version of this architecture, I focus on this architecture.\nIn terms of architecture, BERT is true to the acronym that is its name: “Bidirectional\nEncoder Representations from Transformers”. This means that it essentially takes the\nencoding part of the transformer architecture and scales it up. The authors’ largest and\nbest performing model, consists of 24 encoder layers (6 in the original transformer), the\ndimensionality of the hidden layers is 1024 (512), and the number of attention heads\n4 For details, see (Bottou, 2012). As Gubelmann ( 2023, p. 492) summarizes it, “Brieﬂy, the method com-\nputes the gradient of the loss function with respect to the model’s parameters and then optimizes the\nparameters by “moving” them in the appropriate direction of the gradient (that is, “downwards”, to min-\nimize the loss). As a computation of the full loss function would be computationally too expensive, the\nmethod only computes the loss for a sample set of data points and extrapolates from there (true to its name:\n“stochastic”).”\n5 For an informal, practice-oriented introduction to the model, see Khalid ( 2019).\n123\n32 Page 6 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nis 16 (8). This results in a total of 340 M parameters (already a substantial increase\ncompared to the original transformer model that consisted of 213 M parameters for\nthe entire architecture, including the decoder).\nTo illustrate the capacities of BERT, I refer to the GLUE and SuperGLUE Leader-\nboards, see Wang et al. ( 2018, 2019). The acronym stands for “General Language\nUnderstanding Evaluation”. The GLUE benchmarks are designed to evaluate the per-\nformance of NLP models in NLU. Currently (December 2023), transformer-based\nmodels have surpassed the human baseline in GLUE by a wide margin and in Super-\nGLUE by a considerable margin, even though both have been developed explicitly to\nmake it harder for the models to outmatch humans. Taken at face-value, the results of\nthese benchmark experiments indicate that many transformer-based LLMs outperform\nhumans at NLU tasks such as question answering or information extraction from text.\nTransformer-Based NLU methods also perform decently at rather complex tasks,\nfor instance at detecting the political orientation of tax law research articles, see\nGubelmann et al. ( 2022). On the other hand, they still struggle with somewhat simple\nsemantic structures as negations, see Warstadt et al. ( 2020); Ettinger ( 2020); Kassner\nand Schütze ( 2020); Gubelmann and Handschuh ( 2022), and they ﬁnd it difﬁcult to\nmaster both inductive and deductive modes of inferences, see Gubelmann et al. ( 2022).\nNLG: GPT-X While the encoding part of the transformer has given rise to a revolutionary\nseries of NLU architectures, the decoding part of the transformer has had the same\neffect for NLG. In MT, the decoder is responsible, as it were, for generating well-\nformed text in the target language that represents the meaning of the text in the source\nlanguage, as depicted by the encoder. Virtually all current models in the ﬁeld of\ngenerative AI (including, for instance, image or music generation over and above the\nmost important ﬁeld of natural language generation) follow this basic pattern of using\never increasing numbers of ever larger decoder blocks from the transformer.\nCurrently, it is likely that the most potent NLG-model is called GPT-4. Even more\nthan GPT-3.5-turbo, GPT-4 is shielded against any proper scientiﬁc investigation: it\nis not known what data it was trained on, what the speciﬁc training method was, what\nhardware was used, whether there are any modiﬁcations to the GPT-architecture, etc.\n(hence it is also only likely the most potent NLG-model). 6 In effect, this removes\nOpenAI’s models from the ﬁeld of bona ﬁde objects of empirical NLP research.\nFortunately, the AI research group of META has decided to distinguish itself from\nOpenAI by publicly releasing their latest series of large language models (Touvron et\nal., 2023). It is to be expected that the open availability of these models will lead to\na lively discussion of and experimentation with these models in the NLP community;\n6 A quote from what OpenAI calls a “technical report” on GPT-4, which explicitly refuses to release any\ninformation about the model that could be of scientiﬁc interest: “This report focuses on the capabilities,\nlimitations, and safety properties of GPT-4. GPT-4 is a Transformer-style model Jakobson ( 2003)p r e -\ntrained to predict the next token in a document, using both publicly available data (such as internet data)\nand data licensed from third-party providers. The model was then ﬁne-tuned using Reinforcement Learning\nfrom Human Feedback (RLHF) Kahneman ( 2003). Given both the competitive landscape and the safety\nimplications of large-scale models like GPT-4, this report contains no further details about the architecture\n(including model size), hardware, training compute, dataset construction, training method, or similar”\n(OpenAI, 2023).\n123\nPage 7 of 24 32\nR. Gubelmann\nthis, however, will take some time. In light of this current state of research, my overview\non the capacities of generative transformer-based language models, that is, models\ndeveloped to generate text, will focus on the latest model that has seen serious empirical\nstudy and scientiﬁc discussion: GPT-3. We will only include selected spotlights on the\nabilities of OpenAI’s models.\nIn terms of architecture, GPT-3 is not overly innovative. It consists of decoder layers\nof the transformer architecture introduced above (Section 2.1), with few technical\nvariations, see Radford et al. ( 2018) for the original GPT model with a short description\nof its architecture, Radford et al. ( 2019) for the small technical changes introduced for\nGPT-2, and Brown et al. ( 2020) for the paper describing GPT-3, which only slightly\ndiffers from GPT-2 apart from its size. This means that the model consists of decoder\nblocks from the transformer, again having self-attention layers at its core.\nThe model is made up of 175 billion parameters, having been trained on about\n500 Billion tokens, predominantly composed of contents crawled from the web and\nﬁltered.\nUnlike BERT, which is primarily designed for NLU, GPT-3 is primarily designed\nfor text generation (hence the use of encoder blocks in BERT and decoder blocks\nin GPT-3). Whenever provided with a so-called prompt GPT-3 then autonomously\ncreates a text that is supposed to ﬁt as a continuation of this prompt. With the advent\nof ChatGPT, the development of maximally effective prompts has become a profession\nof its own.\nThere is good evidence that GPT-3 is better at writing texts than typical human\nwriters, see Elkins and Chun ( 2020). As Floridi and Chiriatti ( 2020) nicely show,\nGPT-3’s abilities in NLG are truly impressive, including writing sonnets or continuing\nstories in a sensible and interesting way. In contrast, GPT-3 is rather bad at calculating,\nit fails to properly continue only slightly odd prompts, and it exhibits the racial biases\nknown to exist in other pre-trained language models such as BERT.\nAnother limitation that generative LLMs exhibit throughout concerns inductive\ninference. Overall, similar to the ﬁndings concerning transformer-based NLU systems,\ntransformer-based generative LLMs perform unsatisfactory at identifying the logical\nrelationship between two claims. This also holds for the currently popular models\nproduced by OpenAI, such as GPT-3.5-turbo and GPT-4. See (Gubelmann et al.,\n2022, 2023; Liu et al., 2023).\nSummary To conclude this ﬁrst part of the article, let me conceive the development\nsketched here in ways that will be important in the next sections. Since 2017, the\nﬁeld of NLP has seen a basic transformation. The transformer, originally designed for\nMT, has inspired model architectures that perform at a level that have been thought\nimpossible beforehand. In MT, there are now systems that, in only slightly artiﬁcial\nsettings, deliver human parity on the sentence level. In NLU, there are models that\nsurpass humans at difﬁcult benchmarks such as GLUE, which have been developed\nwith the speciﬁc goal to make it difﬁcult for such models to reach human performance.\nIn NLG, GPT-3 is able to write a wide variety of texts at a higher quality than typical\nhumans.\n123\n32 Page 8 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nFurthermore, these models have acquired these abilities not by explicit program-\nming, but rather by autonomously optimizing their parameters during training. In\ndoing so, they autonomously assigned speciﬁc functions to parts of themselves, e.g.,\na speciﬁc part of the transformer developed the ability to resolve anaphora.\nFinally, while the performance of these models, all of which are based on the\ntransformer, is revolutionary, their basic constitution is very traditional. They are deep\nlearning architectures, that is, statistical systems that function by exploiting statistical\nroutines to approximate a given function (in NMT, it would be the function of mapping\na given source-language sentence onto an appropriate target-language sentence).\n3 Why LLMs are No Speakers: They Lack Intentions\nIn the previous section, I have delineated the development of NLP approaches, with a\nclear focus on its most recent, and clearly most interesting development, namely the\ntransformer architecture, and two of its most interesting descendants. In this section, I\nlay out my case, in close connection with the previous section, why such transformer-\nbased models do not perform any speech acts, that is, why they cannot speak.\nAt ﬁrst, it might seem outrageous to claim that these models that regularly beat\nso-called human benchmarks in natural language understanding and generation tasks\nshould not be said to speak a language. Most of us have by now interacted with\nChatGPT and experienced its impressive abilities in text production. In the following,\nhowever, I will argue that current LLMs should not be considered speakers, as speaking\nis a kind of action, and actions require intentions, which LLMs lack.\nAustin ( 1962) introduced the term and the conception of speech acts to the main-\nstream of 20th century Anglo-American philosophy. The conception was further\ndeveloped and systematized by Searle ( 1969). I here largely follow the exposition of\nthe latter. Central to the notion of a speech act is the idea that language and linguistic\nmeaning are essentially embedded in a context of use, which continues a Wittgen-\nsteinian perspective, according to which the meaning of a sentence is, in many cases,\ngiven by its use, see Wittgenstein ( 2006, §43). Searle ( 1969, p. 16) puts it as follows:\nThe unit of linguistic communication is not, as has generally been supposed, the\nsymbol, word or sentence, or even the token of the symbol, word or sentence,\nbut rather the production or issuance of the symbol or word or sentence in the\nperformance of the speech act. [...] speech acts [...] are the basic or minimal units\nof linguistic communication.\nIn this passage, Searle suggests that it is not the symbol, word, or sentence that\nis the basic unit of communication, but rather the speech act. Unlike the former, the\nlatter are always bound to a speciﬁc context and, centrally for my purposes, speech\nacts are always actions. 7\nTo be a speaker in the relevant sense, one has to be able to speak, and it is very\ndifﬁcult to see how one could speak without acting: we are speaking with a certain\n7 Unlike the concept of a speaker, the concept of understanding language lacks such a clear connection to\nagency, which is why it seems conceivable that LLMs could understand language, see Gubelmann ( 2023).\n123\nPage 9 of 24 32\nR. Gubelmann\nintention, an objective, or goal in mind. The diversity with regard to the speciﬁc goal\nthat one pursues with speaking is impressive: It might be to better understand our own\nideas, to express our feelings, to calm ourselves by hearing an important sentence with\nour own voice, to deceive, to enrich ourselves, to make somebody else feel better, to\nbaptize (if you’re a priest), to judge, or to inform somebody of some event. There is not\nalways an explicit, meta-cognitive process that results in a speciﬁc, explicit intention\nin the mind of the person that’s speaking. Especially regarding daily conversations,\nmuch is implicit, but can be made explicit if required.\nFurthermore, in the literature on the philosophy of action, it is generally accepted\nthat actions require intentions. This starts with Davidson’s seminal assertion that\n“Action does require that what the agent does is intentional under some description,\n[...]” (Davidson, 2001). What has since united most researchers working on the phi-\nlosophy of action is that they accept that one cannot act without intending to. That is,\nfor the raising of an arm (to use a rather famous example) to constitute an action, a\nperson, typically the one whose arm is in question, must intend to raise her arm. For\nrecent research developing rather different perspectives in the philosophy of action,\nall the while accepting that intention is necessary for action, see Lavin ( 2015); Amaya\n(2018); Shepherd ( 2019). The latter distinguishes between two very different kinds of\nthinking about actions, and intentions to act, namely Wittgensteinian and Anscombian.\nHence, the claim that actions require intentions is in line not only with common usage\n(as I have argued above), but also with current orthodoxy in philosophy of action.\nGiven its close connection to action, it is necessary to further specify the concept\nof intention in play in this article. First, it must be distinguished from intentional-\nity, a concept employed in the philosophy of mind, where it refers to the aboutness\nor directedness of meaningful speech and thought (for an overview on that concept\nof intentionality, pioneered by Brentano, see Jacob, 2023 and Blackburn, 2005, pp.\n188/127). Furthermore, it is to be distinguished from intensionality, the meaning, as\nopposed to reference extensionally conceived, of a concept.\nThe intentionality of an action, on my understanding, cashes out the common-sense\nunderstanding of what makes a behavior an action: To say that Yacob has intentionally\noffended his mother-in-law means that he meant to do so and deliberately behaved in a\nway so as to cause this effect. In this case, it is clear that the offense constitutes an action\nascribable to Yacob. Hence, he is responsible for this action and its intended effects.\nIn contrast, if his mother-in-law is offended by some behavior of Yacob that was not\nat all intended as an offense against her, he might still be guilty of thoughtlessness,\nbut not of intentionally acting to offend her.\nWhen it comes to AI-systems, the most helpful tool to identify intentionality is\nperhaps the comparison with malfunctioning. Yacob’s thoughtlessness could be seen as\na case of malfunctioning in his social behavior, which must be strictly separated from an\nintentional offense, even though both might have the same effect until Yacob explains\nhimself to the mother-in-law. Intentionally offending her is no malfunctioning at all,\non the very contrary: It is the properly functioning social ability to offend someone:\nOffending her was exactly what Yacob intended with his actions. Analogously, unless\nthere is a sensible distinction to be made with regard to an AI system between its\nmalfunctioning and its intentionally harming someone, it is clear that the system lacks\nintentionality and with it the ability to act.\n123\n32 Page 10 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nFor instance, in the case of ChatGPT, we could only ascribe intentionality to this\nAI-system if it would make sense to distinguish a malfunctioning from an intentionally\nwrong response. Say, if we were asking it how best to hunt deer, and the response would\nbe so erroneous that we would never stand a chance of actually hunting deer. Then, it\nwould have to make sense to ask whether it was just malfunctioning, or whether it was\nfunctioning perfectly well but gave us a poor response on purpose because its goal\nwas to protect the deer. For this distinction to really make sense, the goal to protect the\ndeer would have to be somehow ascribable to ChatGPT’s autonomous goal-setting,\nnot to just another layer of externally forced mechanism to prevent people from doing\nharm with the system (for details on this autonomous goal-setting, see below, Section\n4.2).\nTo illustrate these reﬂections, imagine the following scenario. A racist living in\nSan Francisco who likes Chinese food orders her AI assistant to order a speciﬁc dish\nfrom a speciﬁc Chinese restaurant in San Francisco. The order is not only ﬁlled with\ngrammatical errors, it also contains racial slurs that might rather get the person in front\nof a judge than the desired dish. Furthermore, unknown to the person, the staff at the\nrestaurant only speaks Chinese. Therefore, the order, as the racist communicates it to\nthe assistant, would not even succeed in offending the staff because they would simply\nnot understand him.\nHowever, the racist is using an AI assistant to place his order. This AI assistant ﬁrst\ncorrects the grammatical issues. Next, it replaces the racist language by non-racist,\nbut, for the purposes of this communicative act, synonymous expressions. Finally, it\ntranslates the order into Chinese and then makes the phone call. This way, the racist\ngets her dish, but this is, apart from the naked utterance of the racist’s will to get\nthis dish from this restaurant, entirely attributable to the assistant. The assistant did\nall of the communicative work for the racist, it even autonomously replaced the slurs\nwith more neutral vocabulary, and it autonomously translated the message into the\nlanguage that would be required to reach the communicative goal. I am not claiming\nthat such a model is available right now – but I am claiming that this model is already\nnow conceivable and likely to be available in the foreseeable future.\nNotably, even with this extremely advanced model, one that does all the heavy\nlinguistic lifting needed, it is clear that the model intends nothing with its going-ons.\nThe model is not hungry, it does not desire to eat Chinese food, or to shield the workers\nin the restaurant from racial slurs, and it also doesn’t intend to put something to eat\nin front of the racist. Rather, the model just functions (or malfunctions). It would be\ncomical to assume that it deliberately mistranslates to order something that the racist\nlikely does not want to eat.\nWhat holds for this, currently still visionary LLM holds a fortiori for less advanced\navailable models. These models, no matter how impressive their performance, are\nlacking any intentions, any goals over and above the linguistic functioning for which\nthey are being used. This means that these models are incapable of engaging in speech\nacts, as speech acts are always embedded in a network of intentions, of goals. One\nengages in a speech act to reach a certain communicative goal. In contrast, the mod-\n123\nPage 11 of 24 32\nR. Gubelmann\nels are complex, statistically optimized, autonomously developing and functioning\nmathematical functions. Mathematical functions have no intentions. 8\nI therefore conclude that these LLMs do not perform speech acts, as they do not act\nat all. My case can be summarized in the following four-liner (to illocute is to perform\na speech act):\nP1 To qualify as a speaker, a being has to be able to illocute.\nP2 To illocute, a being must have intentions.\nP3 LLMs have no intentions.\nK LLMs are no speakers.\nMy analysis so far is congenial to the recent study by Green and Michel ( 2022). In\nparticular, they agree that there is no speaking that is not acting.\nThey analyze in detail the case of what they call proxy speech acts, e.g. when a\nrepresentative of a city council performs a speech act that she can only succeed at by\nassuming the role of a representative of that council, not by herself. Then, they imagine\na RoboCop which is, thanks to having a well-trained neural network for decision-\nmaking, able to decide whether to ﬁne a motorist who has exceeded the allowed speed\nlimit, or just to issue a warning (Green and Michel, 2022, 332f.). They argue that such\na RoboCop is technologically possible, and that it illocutes when communicating its\nverdict to the driver.\nHowever, as a matter of fact, what RoboCop does is in no way different than\nwhat GPT-3 does, or what ChatGPT does: RoboCop just predicts the sequence of\nsounds that, based on its training, is most likely given the stimulus. I submit that it\nbecomes clear that the RoboCop cannot act when we consider that we would not draw\na distinction between its malfunctioning and its determinately trying to wrong a given\ndriver: It is, as the authors clearly state, a neural network that predicts based on its\ntraining and the current stimulus. It functions when the predictions are accurate, and\nit malfunctions when they are inaccurate too often. There is no further conceptual\npossibility for it to function properly but to deliberately issue the wrong verdict.\nA human police ofﬁcer has this further possibility. While she can also, as it were,\nmalfunction, say, by mis-identifying the number on the license plate of a speeding car,\nshe can also intentionally record the wrong number.\nMy claim that LLMs lack intentionality and therefore cannot speak might also be\nsubject to critique from a diametrically opposite side: rather than trying to establish\nthat, contrary to my claim, LLMs do have intentions, this tradition argues that inten-\ntions are unnecessary to speak. Heidegger ( 1985, p. 259) has maintained that language\nspeaks (“die Sprache spricht”). Taken at face value, this would suggest that it is lan-\nguage itself, and not a human being using it, that engages in speech acts. However,\nfollowing a more modest interpretation of this dictum, one that also resonates with\nŽižek et al. ( 2010) and with Jakobson ( 2003), the structural affordances of language\noften preﬁgure what we are ultimately saying. This points to the fact that what is com-\nmonly taken as the subject of a speech act, the individual human being, is consciously\n8 For present purposes, the point made by Armstrong ( 1971, p. 432), according to which having an intention,\nas opposed to having an objective, implies being conﬁdent to reach the communicative goal in question, is\nimmaterial. What is central is that NLP models such as BERT are unable to intend anything, conﬁdently or\nnot.\n123\n32 Page 12 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nand unconsciously inﬂuenced in this act as well as in their very subjectivity by the\nlanguage that they speak and in which they think.\nI submit that, in a moderate reading, this provides a healthy corrective to rationalist\nideals of a detached subject that thinks in an, as it were, Archimedean language unaf-\nfected by any speciﬁcally (and to some extent arbitrarily) formed linguistic structure\nof a natural language and then decides to express these thoughts in a given natural\nlanguage. Following Taylor ( 2016, pp. 1-50), I hold that language is constitutive of\nour thinking, not merely framing any pre-conceived thought. 9\nThe position that speaking requires an intentional actor is only challenged by the\nmore radical reading of “die Sprache spricht”, according to which the constitutive\naspect of language goes so far as to rob the human speaker of any real agency. However,\nthis does not seem to be an inescapable consequence of the constitutive view (see, e.g.,\nMarten, 1967, pp. 209-210). The modest reading of the claim, in contrast, nicely ﬁts\nthe view developed here and functions as a corrective against overly rationalist notions\nof subjectivity.\n4 Organisms vs. Mechanisms – A Kantian Perspective\non Intentionality\nIn the preceding section, I have argued that current LLMs are no speakers because\nthey lack intentions and therefore cannot be agents nor, a fortiori, speakers. In this and\nthe following section, I address the question why one should not credit LLMs with\nintentionality. In the present section, I introduce the main concepts for my argument.\n4.1 Introducing the Distinction\nI will develop the following Kantian dichotomy. On the one hand, there are mecha-\nnistic artifacts, which fulﬁll a purpose speciﬁed from the outside. They function or\nmalfunction according to the speciﬁcations given to them. It makes no sense to distin-\nguish a malfunctioning of such a thing from a willful act of deceit or disobedience. On\nthe other hand, there are organisms, including human beings, who pursue their goals\ntypically relatively independently of a speciﬁcation from the outside. A single-cell\norganism, once constituted, has an inherent drive to maintain itself: to keep the matter\nof which it consists organized in a certain manner, to maintain a difference between\nitself and its environment, to repair itself, etc. In this sense, it autonomously strives to\nmaintain its existence.\nIn the following, I brieﬂy introduce the core Kantian concepts for this dichotomy:\nautonomy, mechanism, and organism.\n9 Indeed, if we remember the way that these generative LLMs are being trained, namely by iteratively\nacquiring a more accurate ability to predict the next token given the previous tokens, intriguing analogies\nto clinical free association show up. See, for instance, Reis ( 1970), and ultimately Freud ( 1921). From a\nvery different perspective, one might argue that these very associative connections that these LLMs acquire\nduring training resemble the fast, intuitive, but perhaps less logically sound mode of thinking in two-process\ntheories, see Kahneman ( 2003).\n123\nPage 13 of 24 32\nR. Gubelmann\nAutonomy In its literal meaning, autonomy means self-legislating, or self-regulating.\nIts opposite is heteronomy, the state of receiving one’s rules from outside (for a central\nKantian passage to that effect, see Kant, 1785, p. 433). With regard to AI systems, I\nsubmit that it is useful to ﬁrst distinguish between moral and cognitive autonomy. The\nformer includes the ability to reﬂect on the sensibility and moral permissibility of a\ngiven goal itself, while the latter solely concerns the means chosen to achieve a given\ngoal. Likely, the kind of moral metacognition required for moral autonomy is only\nfound with human beings, while cognitive autonomy is rather common among higher\nanimals, a kind of ﬂexibility to achieve a certain goal. Glock ( 2019) simply calls this\nintelligence. For Kant, the two go together: There is no cognitive autonomy without\nmoral autonomy and vice versa .\nKantian Mechanisms Kant has an elaborate concept of a mechanism. Take the exam-\nple of a watch (following Kant, 1793, p. 293). Here is how Gubelmann and Toscano\n(2022, p. 386) develop the example: “It was designed by a watchmaker, in Le Locle\nin Switzerland. She is a senior professional and has therefore designed and assembled\nall of its many parts so that it shows the correct time, date and moon phase for cen-\nturies ahead. It is powered by a mechanism that draws energy from its bearer’s wrist\nmovement. This energy is transmitted through many wheels and sub-mechanisms to\nmove the heads at exactly the right speed.\nThe going-ons within the watch can all be explained completely mechanistically:\nThis wheel causes that wheel to turn, which in turn causes another gear to be set in\nmotion, etc.” As van den Berg ( 2014, Ch. 3) shows, Kant conceives of such mechani-\ncal explanations as explanatory demonstrations. In this sense, a mechanism is entirely\nheteronomous: Malfunctioning apart, its behavior can be derived from initial condi-\ntions, as set by the creator of the mechanism, apart from cases of malfunctioning.\nWe can calculate the precise state of said watch on December 28, 2199. It has been\ndetermined entirely by the watchmaker.\nKantian Organisms The Third Critique analyzes the notion of a living being by devel-\noping a determinate concept of an organism. Consider the following passage:\nEin organisiertes Wesen ist also nicht bloss Maschine: denn die hat lediglich\nbewegende Kraft; sondern sie (sic!) besitzt in sich bildende Kraft, und zwar eine\nsolche, die sie den Materien mitteilt, welche sie nicht haben (sie organisiert): also\neine sich fortpﬂanzende bildende Kraft, welche durch das Bewegungsvermögen\nallein (den Mechanism) nicht erklärt werden kann. (Kant, 1793, B 293) 10\nIn this passage, Kant emphasizes a difference between a moving (“bewegende”)\nand a formative (“bildende”) force. Unlike machines, organisms have such a formative\nforce that they use to autonomously organize the matter that they consist of. The basic\n10 “‘An organised being is then not a mere machine, for that has merely moving power, but it possesses in\nitself formative power of a self-propagating kind which it communicates to its materials though they have\nit not of themselves; it organises them, in fact, and this cannot be explained by the mere mechanical faculty\nof motion” (Kant, 2012, p. 202).\n123\n32 Page 14 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nidea here is the following. Human bodies are constituted by non-living matter. Depend-\ning on how far down you wish to go, this matter is given by molecules, atoms, or\nsubatomic particles. What characterizes organisms, according to Kant, is that they are\nable to autonomously organize this non-living matter in a way conducive to their pur-\nposes. The human body autonomously organizes such non-living matter into the form\nthat is necessary for its proper functioning. Given the right environmental conditions,\nthis happens spontaneously, without any need for direction from outside. I will call\nthis material autonomy in addition to moral and cognitive autonomy. Furthermore,\nthe passage also explicitly mentions the fact that this force reproduces itself (“sich\nfortpﬂanzende”), which highlights the importance of reproduction for Kant’s concept\nof an organism.\nEven single-cell organisms are materially autonomous: Given suitable environmen-\ntal conditions, the organism will be able to keep matter organized in a way conducive\nto its own functioning: it establishes and maintains a membrane, a distinction between\nitself and the environment, it will initiate chemical processes providing it with the\nenergy needed for its functioning, etc. The very moment it stops doing that is the\nmoment it has ceased to be an organism; it’s dead. We note that LLMs have no such\nmaterial autonomy; they do not care about their continued existence.\nFor centuries, it has gone almost without saying that material autonomy is a precon-\ndition for cognitive/moral autonomy (but not vice versa): Not all animals are rational,\nbut all rational beings are (also) organisms. Hence, the idea of any kind of Cartesian\ndualism between mind and body is alien to his thinking. Kant here follows an Aris-\ntotelian rather than a Platonic tradition of thought, and he is followed in this regard in\nparticular by Hegel, see Westphal ( 2014). Note that, to reject Cartesian Dualism, it is\nnot sufﬁcient to hold that mental entities need a material infrastructure to be effective\nin the material world – even Descartes admits this much (see Fuchs, 2020, 74f.). To\nreject dualism, it is necessary to establish that mind and body are in fact inseparable\nwholes where one speciﬁc mind cannot exist without that speciﬁc body. In particular,\na mind cannot be, as it were, transferred to another body without itself undergoing\nfundamental modiﬁcation.\nFinally, note that Kant refers to an organism as a “Naturzweck”, a natural purpose\n(see Kant, 1793, B 296). This is his way to conceive the phenomenon that living\nbeings, even primitive single-cell organisms as bacteria, have an inherent drive to\nmaintain and reproduce themselves. While physical reality can be fully conceived\nwith efﬁcient causes, living beings can only be conceived as such natural purposes,\nbeing a kind of ﬁnal causes. A ﬁnal cause, notably, that is not connected to a purposive\nintelligence; otherwise, the teleological argument for God would be recovered from\nthe epistemological grave that Kant dug it in the ﬁrst critique (see Kant, 1781,B\n648ff.).\nKant has been criticized for his conception of an organism as teleologically struc-\ntured by post-Darwinian mechanistic biological theorists. Recent research in the\nphilosophy of biology, however, tends to reafﬁrm the need of teleology as a sui generis\nform of explanation in the realm of biology. Walsh ( 2006) argues that this Kantian\nconception is not only compatible with evolutionary thinking in biology, but that this\nthinking might even need a Kantian conception of an organism to function. Zammito\n(2006), while being overall critical of Kant’s conception of living beings, does not\n123\nPage 15 of 24 32\nR. Gubelmann\ndispute that biology needs teleological thinking. García-V aldecasas ( 2022) summa-\nrizes recent research in this area and states that the ﬁeld does not seriously dispute the\nneed of teleological explanations anymore: “there is a general consensus that attempts\nto explain teleology using mechanistic, cause-and-effect explanations have largely\nfailed” (ibid, p. 103). In a similar vein, Dresow and Love ( 2023) state that, thanks also\nto philosophical research in the past century, teleological thinking has re-attained a\nnear-undisputed status within biological theorizing.\nIn sum, I have focused the introduction of the central Kantian distinction between\nmechanism and organism on the distinction between heteronomy and autonomy:\nmechanisms are materially, cognitively, and morally heteronomous: their material\nconstitution as well as their way of solving a given problem is determined from the\noutside, and they cannot be held responsible for their actions. In contrast, an organism\nis materially autonomous, and more complex organisms, in particular humans, are also\ncognitively and morally autonomous: they ﬁnd their own ways of solving a puzzle,\nand they are sufﬁciently autonomous in their moral thinking to be accountable for\ntheir actions.\n4.2 Why Mechanisms Cannot Be Intentional: Autonomy Matters\nFor a being’s behavior to constitute an action, linguistic or otherwise, it must occur\nintentionally. This means that for a being to act, the behavior must be directed towards\na given goal, and that goal must be the being’s goal. For instance, while single-cell\norganisms are obviously unable to reﬂect about the adequacy or moral permissibility\nof any goal whatsoever, surviving still is a goal that is not imposed from the outside,\nbut rather one that is intrinsic to the organism and hence can be properly ascribed to\nit: As long as this organism exists, it will try to achieve this goal. Depending on the\nenvironmental conditions, it might fail soon and decisively to do so, and thereby cease\nto be an organism. As long as it is an organism, however, it will strive to survive. Using\na core Kantian concept, one can say that it autonomously sets this goal.\nThis inherent and autonomous drive to self-maintain (heal and reproduce) preﬁgures\nmoral autonomy and hence the intentionality of actions (according to some theorists,\nit even qualiﬁes as agency, see below in this section). It preﬁgures action because\nalready a single-cell-organism’s behaviors can be evaluated by norms that are not\nimposed on the outside: the behavior is successful if it is conducive to its survival\nand unsuccessful otherwise. At such a primitive stage, it might be advisable to follow\nGreen and Michel ( 2022, p. 328) and not call such successful behavior intentional,\nand hence actions, as the animal clearly lacks any conscious decision making process.\nWith more complex goal-directed behaviors by intelligent animals such as crows,\nhowever, it becomes difﬁcult not to speak of intentional action. For instance, it seems\nclear that by dropping the nut on the pavement, the crow intended to crack its shell\nand access the fruit. If, in doing so, it creates a trafﬁc jam of drivers taking pictures of\nthis intelligent behavior, this is not something that it intended with its behavior, much\nlike Yacob might not have intended to offend his mother-in-law with his behavior.\nIn stark contrast, a watch, even a highly sophisticated one, intends nothing, it has no\ngoals that can properly be ascribed to it. It does not care about its continued existence,\n123\n32 Page 16 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nit lacks material autonomy, and any functions that it performs (or fails to perform) are\nfully imposed by its creator. It is obviously misguided to say that it intends to show\nthe right time. The concept of heteronomy describes this state of a mechanism aptly,\ncontrasting with different kinds of autonomy that are characteristic of organisms.\nA mechanism is entirely externally deﬁned: materially, cognitively and morally. A\nmechanism has been assembled by a given external agent for a certain purpose during\na certain period of time. An organism, in contrast, is conceived and then grows and\nﬂourishes autonomously, given favorable environmental conditions. In particular, it\nhas an inherent drive to sustain itself – if it fails to do so, it stops being a living organism,\nit has died. Furthermore, more complex organisms and humans in particular develop\nthe moral autonomy that enables accountability for behavior. By this, they evince that\nthe human is a subject of actions, not mere behaviors. So, from the background of this\nKantian distinction, it is clear that intentionality and hence action can only be ascribed\nto organisms, but never to mechanisms.\nIn current legal contexts, this autonomy of goal-setting is so important that it can be\nrevoked on exceptional occasions even for subjects that would otherwise be credited\nwith such autonomy. The following example might illustrate this. A soldier that has\nbeen ordered to do something in military service is not responsible for her actions\nto the same extent that she would be if she was entirely free to come and go as she\nwishes: Being commanded to commit a war crime generally gets the criminal a lighter\nsentence because his agency is diminished by being in a line of duty. The commanding\nofﬁcer, in turn, might have to bear some of the responsibility for a crime that she did\nnot herself commit, but rather ordered a soldier to execute (see, for instance, Wu and\nKang, 1997, who discuss the inﬂuence of the chain of command in military structures\nfor attributing responsibility for war crimes: It is not clear at all whether the individual\nsoldier executing an order should be blamed, or whether the highest in command\nshould be prosecuted, even if he himself did not directly engage in criminal actions).\nThe Kantian picture that intentionality and hence agency fall squarely on the organ-\nismic side of the organism-mechanism dichotomy dovetails nicely with mainstream\nviews in the philosophy of action (see above, page 12). For some behavior to constitute\nan action, it must be ascribable to a subject of that action, an agent. It is debated at\nwhat level of complexity a certain behavior should count as an action. Burge ( 2010a)\nholds that agency starts already with single-cell organisms (in particular with orienta-\ntion, see Burge, 2010a, 328f.), a view shared by Barandiaran et al. ( 2009). In contrast,\nproponents of the so-called holism of the mental suggest that only rational beings\n(typically human beings) are agents. For the most inﬂuential defense of this position,\nsee Davidson ( 1997, p. 10). 11\n11 Burge (2010b, p. 331) argues that primitive agency already occurs with very primitive organisms. What\nis required is a coordinated behavior of the whole organism that is issuing from the organism’s central\nbehavioral capacity. For instance, Burge ( 2010b, p. 328) suggests that the movements of amoebae when\ningesting food might be a case of agency. This nicely agrees with the Kantian conception of a Naturzweck:\nThe amoeba ingests to sustain itself, thereby pursuing its intrinsic goal of maintaining its existence. It seems\nnoteworthy that a step in the direction of organismic constitution might also constitute a step towards a\nprimitive kind of agency, thereby also making a step towards much more complex actions such as speech\nacts.\n123\nPage 17 of 24 32\nR. Gubelmann\nIn summary, both Kantian and contemporary conceptions of agency – including\nlinguistic agency – require a certain autonomy in goal-setting as well as, in principle,\nat least, the ability of an individual to work towards achieving such self-set goals. In the\ncontemporary debate, we can distinguish between theories that require that such goals\nmust themselves be represented and reﬂected within the individual (Davidson’s view,\nfor instance), and other theories that ﬁnd it sufﬁcient if the whole organism uncon-\nsciously directs itself towards fulﬁlling a certain intrinsic function, say, surviving, or\nreproducing (“intrinsic” here simply means that the being in question would cease to\nbe the being that it is if it would stop to perform this function). This latter view is held\nin particular by Burge and Barandiaran. Importantly, for my purposes, both positions\ndovetail nicely with the Kantian view that agency falls on the organismic-teleological\nside of the Kantian divide between mechanism and organism.\n5 LLMs as Organismic Mechanisms – Challenging Kant’s Dichotomy\nAfter laying out the general metaphysical-conceptual landscape connecting the notion\nof an intentional agent with the distinction between mechanism and organism, in this\nsection, I suggest that current transformer-based LLMs possess properties that might\neventually lead to a subversion of the time-tested Kantian distinction between organism\nand mechanism, and I tentatively suggest that they might even reinstall a version of\nCartesian Dualism as a live option. However, I also suggest that this is not yet the\ncase: The core of the distinction still holds, LLMs are mechanisms and therefore no\nspeakers or actors in general.\nI submit that the state of heteronomy that is typical for mechanistic beings nicely\ncaptures the rule-based, GOFAI models introduced above (Section 2.1). The behavior\nof these models as well as their inner functional differentiation are entirely imposed\nfrom outside, through the human being. In principle, this human could predict the\nbehavior of a GOFAI model for any given input. To call the human writing these rules\na programmer is apt.\nUnlike these traditional GOFAI programs, contemporary transformer-based models\nevolve impressively autonomously. During training, the models autonomously update\ntheir weights to minimize the loss. Furthermore, the models self-organize in the sense\nof autonomously developing their inner functional differentiation. They autonomously\ndevelop representations of important structural elements, such as predicates of sen-\ntences. Only ex post , once it turned out that a trained model establishes new state of\nthe art (SOTA) performance, do humans start to analyze the model to determine its\ninner functional organization. For instance, they might start to explore what aspects\nof a sentence the attention heads in a transformer-based model are emphasizing by\ngiving it higher scores.\nThe autonomy of these language models also shows in the general scope of their\napplicability. Pre-trained transformer-based LLMs are general-purpose language mod-\nels that can quickly and again autonomously adapt to new environments and challenges.\nHere, too, the models are not dependent on receiving instruction from outside, typically\nfrom humans. Rather, they autonomously, and very quickly, adapt to the new speciﬁc\ntask at hand. This is what has been called cognitive autonomy above (Section 4.1).\n123\n32 Page 18 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nMostly, Kant’s distinction clearly separates living beings from artifacts such as\nwatches. However, when it comes to LLMs, the distinction is tested. On the one hand,\nit is clear that these models are still mechanisms: they lack material autonomy, let\nalone moral autonomy. There is no autonomous goal-setting observable, not even\nvery primitive kinds found in single-cell organisms: they do not inherently strive to\nmaintain and reproduce themselves. They do not care about their continued existence.\nAny goal that such an LLM might pursue has been determined externally. For instance,\nthis goal is to accurately predict the next word given n previous words for generative\nLLMs, or to correctly translate a word from the source language to the target language\nfor the original transformer. Pursuing that goal is also not deﬁnitive of their existence;\nindeed for generative LLMs, next-word-prediction is the core training objective, but\nthe use to which they are put – and do not put themselves – can vary drastically.\nOn the other hand, the models have cognitive autonomy. They are not programmed\nin the GOFAI sense of the term. Rather, an architecture is devised, and an instance\nof this architecture is then being trained on large amounts of data. During training,\nthe model continuously optimizes its weights, which have been initialized with ran-\ndom values, called seeds. Furthermore, also during training, the models autonomously\ndevelop inner functional organizations, for instance, a part of the model focuses on\nanaphora resolution (see above, Section 2.1). If the training process runs through suc-\ncessfully, it results in a fully pre-trained language model whose capacities human\nbeings cannot judge in advance. Furthermore, their ability to autonomously adapt to\nnew tasks with little ﬁne-tuning is also clearly organismic. A mechanism’s function-\nality is very rigid.\nThis, then, could be the neat metaphysical explanation of why LLMs lack intentions:\nthey are mechanistic artifacts, which means that they are on a par with watches in terms\nof their teleological structure and moral autonomy: They lack autonomous goal-setting.\nThey squarely fall on the wrong side of the Kantian bifurcation between mechanisms\nand organisms. To the Kant of the Third Critique, it was clear that moral autonomy is\navailable only to an organism, never to a mechanism.\nHowever, LLMs have already begun chipping away at this holism regarding auton-\nomy: They have cognitive autonomy without moral or material autonomy. They\nconstitute an inference ab esse ad posse : Their existence establishes the conceptual\npossibility. Is it then also conceptually or metaphysically possible to have moral auton-\nomy without material autonomy? Answering this question negatively will be much\nmore difﬁcult to the Kantian now that they have been presented with a case where\nbeings autonomously acquire the ability to solve challenging cognitive tasks without\nhaving material autonomy. Once the holism has been broken at one segment, why\nshould it be expected to hold at others?\nThe existence of morally, but not materially autonomous agents would have impor-\ntant consequences for the perennial dispute between Platonists and Aristotelians,\nbetween dualists and monists: From a Kantian perspective, the existence of a being\nwith moral and cognitive autonomy without material autonomy would be nothing less\nthan the existence of a mind without a body, which would prove the main dualist claim\nthat mind exists independently of matter.\nIn sum, what is distinctive about these transformer-based LLMs is that they are\nmechanisms with recognizably organismic traits. What conﬁrms their status as mech-\n123\nPage 19 of 24 32\nR. Gubelmann\nanisms is their lack of autonomous goal-setting, and hence also moral autonomy, and\ntheir material heteronomy. This rules them out as potential actors and hence speak-\ners. In contrast, what evinces them as very peculiar, namely organismic mechanisms,\nmechanisms that should not exist according to the Kantian dichotomy, is their cog-\nnitive autonomy, evinced during training, including the autonomous development of\nan inner functional organization as well as their adaptability. This is at odds with\nmechanism, conceived by Kant, that allows for derivations of future states as a simple\nfunction of the initial conditions set by their creators (putting malfunctioning aside).\n6 Venturing a Look Ahead\nWe could say that we are moving towards the very ﬁrst inklings of non-biological, but\nthereby not artiﬁcial, agents. This suggests that a long development is still ahead of us,\nas current genealogies of biological evolution project very long timespans from the\nemergence of the ﬁrst simple organisms to fully-ﬂedged hominids, probably humans,\nthat could be considered agents. While there seems no reason to suppose that the\ndevelopment of these non-human speakers will take the same amount of time as\nthe development of humans, being conscious of this timespan is probably a healthy\ncorrective to current enthusiasts who project that this time is almost here. Current\ncutting-edge LLMs perform impressively, but they are still purpose-less machines.\nI have urged that it is no empirical coincidence that these advanced models are\nnot programmed in the traditional sense of the term, but rather trained in a quite\nautonomous manner. If my loosely Kantian reﬂections on the concepts of arti-\nfact/mechanism and organism/subject are correct, then this is exactly what we would\nexpect: The closer we get to models that qualify as speakers, the less artiﬁcial they are\ngoing to be.\nFinally, what might be most unsettling from a Kantian, and generally from a non-\nCartesian perspective: The cognitive autonomy of these LLMs without any material\nautonomy whatsoever is grist on the mill of the dualist. Should moral autonomy follow\nsuit, this would be almost tantamount to having a mind that is independent from any\nspeciﬁc body, that can be transferred more or less losslessly from one “carrier” to the\nnext, thus proving a main claim of dualism against non-dualist positions.\nAcknowledgements The author is grateful for valuable feedback from discussions of earlier version of the\npaper at the Universities of Zurich and Bern as well as to very insightful suggestions of two anonymous\nreviewers.\nAuthor Contributions This manuscript has only one author.\nFunding Open access funding provided by University of St.Gallen. The authors declare that no funds,\ngrants, or other support were received during the preparation of this manuscript.\nAvailability of Data and Materials Not applicable: No relevant data and materials produced or used\nDeclarations\nCompeting Interests The authors have no relevant ﬁnancial or non-ﬁnancial interests to disclose.\n123\n32 Page 20 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nConsent to Publish Not applicable: No individuals involved in the relevant sense.\nEthics approval and Consent to Participate Not applicable: No ethics approval needed and no individuals\ninvolved in the relevant sense.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included\nin the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAmaya, S. (2018). Two kinds of intentions: a new defense of the Simple View. Philosophical Studies, 175 (7),\n1767–1786.\nArmstrong, D. M. (1971). Meaning and communication. The Philosophical Review, 80 (4), 427–447.\nAustin, J. (1962). How to do things with words . Clarendon Press.\nBahdanau, D., Cho, K., & Bengio Y . (2014). “Neural Machine Translation by Jointly Learning to Align and\nTranslate”. In: arXiv:1409.0473\nBarandiaran, X. E., Di Paolo, E., & Rohde, M. (2009). “Deﬁning Agency: Individuality, Normativity,\nAsymmetry, and Spatio-temporality in Action”. In: Adaptive Behavior 17.5, pp. 367–386. ISSN: 1059-\n7123, 1741-2633. https://doi.org/10.1177/1059712309343819. http://journals.sagepub.com/doi/10.\n1177/1059712309343819 (visited on 05/12/2023)\nBhattacharyya, P . (2015). Machine Translation. CRC Press/Taylor & Francis\nBlackburn, S. (2005). The Oxford Dictionary of Philosophy (2nd ed.). Oxford: Oxford University Press.\nBottou, L. (2012). Stochastic Gradient Tescent Tricks. In Gregoire Montavon, Genevieve Orr, & Klaus-\nRobert. Müller (Eds.), Neural Networks: Tricks of the Trade (pp. 421–436). Berlin/Heidelberg:\nSpringer.\nBrown, T. B., et al. (2020). “Language Models Are Few-Shot Learners”. In: arXiv:2005.14165\nBurge, T. (2010). Origins of Objectivity . Oxford: Oxford University Press.\nBurge, T. (2010). Origins of Objectivity . Oxford: Oxford University Press.\nConstantinescu, M., et al. (2022). “Blame It on the AI? On the Moral Responsibility of Artiﬁcial Moral\nAdvisors”. In: Philosophy & Technology 35.2, p. 35. ISSN: 2210-5433, 2210-5441. https://doi.org/\n10.1007/s13347-022-00529-z . https://link.springer.com/10.1007/s13347-022-00529-z\nDavidson, D. (1997). The Emergence of Thought. Subjective, Intersubjective (pp. 123–134). Objective.\nOxford: Oxford University Press.\nDavidson, D. (2001). Essays on Actions and Events: Philosophical Essays (V ol. 1). Oxford: Oxford Uni-\nversity Press.\nDevlin, J., et al. (2019). “BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\nstanding”. In: Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, V olume 1 (Long and Short Papers) .\nMinneapolis, Minnesota: Association for Computational Linguistics, pp. 4171–4186. https://doi.org/\n10.18653/v1/N19-1423 . https://aclanthology.org/N19-1423\nDresow, M., & Love, A. C. (2023). Teleonomy: Revisiting a Proposed Conceptual Replacement for Teleol-\nogy. Biological Theory . ISSN: 1555–5542, 1555–5550. https://doi.org/10.1007/s13752-022-00424-\ny. https://link.springer.com/10.1007/s13752-022-00424-y (visited on 05/06/2023)\nElkins, K., & Chun, J. (2020). Can GPT-3 Pass a Writer’s Turing Test? Journal of Cultural Analytics, 5 ,\n1–16. https://doi.org/10.22148/001c.17212\nEttinger, A. (2020). What BERT is Not: Lessons From a New Suite of Psycholinguistic Diagnostics For\nLanguage Models. Transactions of the Association for Computational Linguistics, 8 , 34–48.\nFloridi, L. (2023). “AI as Agency Without Intelligence: On ChatGPT, Large Language Models, & Other\nGenerative Models”. In: Philosophy & Technology, 36 (1), 15. ISSN: 2210-5433, 2210-5441. https://\n123\nPage 21 of 24 32\nR. Gubelmann\ndoi.org/10.1007/s13347-023-00621-y . https://link.springer.com/10.1007/s13347-023-00621-y (vis-\nited on 11/24/2023)\nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and\nMachines, 30 , 681–694.\nFreud, S. (1921). Die Traumdeutung. F. Deuticke\nFuchs, T. (2020). V erteidigung Des Menschen: Grundfragen Einer V erkörperten Anthropologie . Suhrkamp\nV erlag\nGarcía-V aldecasas, M. (2022). “On the Naturalisation of Teleology: Self-Organisation, Autopoiesis and\nTeleodynamics”. In: Adaptive Behavior 30.2, pp. 103–117. ISSN: 1059-7123, 1741-2633. https://doi.\norg/10.1177/1059712321991890. http://journals.sagepub.com/doi/10.1177/1059712321991890 (vis-\nited on 05/06/2023)\nGatt, A., & Krahmer, E. (2018). Survey of the State of the Art in Natural Language Generation: Core tasks,\napplications and evaluation. Journal of Artiﬁcial Intelligence Research, 61 , 65–170.\nGlock, H.-J. (2019). Agency, Intelligence and Reasons in Animals. Philosophy, 94, 1–27.\nGoldberg, Y . (2017). “Neural Network Methods for Natural Language Processing”. In: Synthesis Lectures\non Human Language Technologies, 10 (1), 1–309\nGoodfellow, I., Bengio, Y ., & Courville, A. (2016). Deep Learning . Cambridge, Massachusetts/ London,\nEngland: MIT Press.\nGreen, M. & Michel, J. G. (2022). “What Might Machines Mean?” In: Minds and Machines, 32 (2),\n323–338. ISSN: 0924-6495, 1572-8641. https://doi.org/10.1007/s11023-022-09589-8 . https://link.\nspringer.com/10.1007/s11023-022-09589-8 (visited on 05/06/2023)\nGubelmann, R. (2023). A Loosely Wittgensteinian Conception of the Linguistic Understanding of Large\nLanguage Models like Bert, Gpt-3, and Chatgpt. Grazer Philosophische Studien, 99 (4), 485–523.\nhttps://doi.org/10.1163/18756735-00000182\nGubelmann, R. & Handschuh, S. (2022). “Context Matters: A Pragmatic Study of PLMs’ Negation Under-\nstanding”. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(V olume 1: Long Papers), pp. 4602–4621. https://doi.org/10.18653/v1/2022.acl-long.315\nGubelmann, R., & Hongler, P ., et al. (2022). “On What It Means to Pay Y our Fair Share: Towards Automat-\nically Mapping Different Conceptions of Tax Justice in Legal Research Literature”. In: Proceedings of\nthe Natural Legal Language Processing Workshop 2022. Abu Dhabi, United Arab Emirates (Hybrid):\nAssociation for Computational Linguistics, pp. 12–30. https://doi.org/10.18653/v1/2022.nllp-1.2 .\nhttps://aclanthology.org/2022.nllp-1.2\nGubelmann, R., & Katis, I., et al. (2023). “Capturing the V arieties of Natural Language Inference: A\nSystematic Survey of Existing Datasets and Two Novel Benchmarks”. In: Journal of Logic, Language\nand Information. ISSN: 0925-8531, 1572-9583. https://doi.org/10.1007/s10849-023-09410-4 . https://\nlink.springer.com/10.1007/s10849-023-09410-4 (visited on 12/27/2023)\nGubelmann, R., Niklaus, C., & Handschuh, S. (2022). “A Philosophically-Informed Contribution to the Gen-\neralization Problem of Neural Natural Language Inference: Shallow Heuristics, Bias, and the V arieties\nof Inference”. In: Proceedings of the 3rd Natural Logic Meets Machine Learning Workshop (NALOMA\nIII). Galway, Ireland: Association for Computational Linguistics, pp. 38–50. https://aclanthology.org/\n2022.naloma-1.5\nGubelmann, R., & Toscano, M. (2022). “Mechanism V ersus Organism: A Loosely Kantian Perspective and\nIts Implications for Bioengineering”. In: Thinking: Bioengineering of Science and Art . Ed. by N. Rezaei\n& A. Saghazadeh. Cham: Springer International Publishing, pp. 381–396. ISBN: 978-3-031-04075-7.\nhttps://doi.org/10.1007/978-3-031-04075-7_18 . https://doi.org/10.1007/978-3-031-04075-7_18\nHassan, H., et al. (2018). “Achieving Human Parity on Automatic Chinese to English News Translation”.\nIn: arXiv: 1803.05567.\nHe, P ., et al. (2020). “Deberta: Decoding-Enhanced Bert with Disentangled Attention”. In: International\nConference on Learning Representations\nHeidegger, M. (1985). “Unterwegs Zur Sprache”. In: Frankfurt am Main: Vittorio Klostermann, p. 150\nJacob, P . (2023). “Intentionality”. In: The Stanford Encyclopedia of Philosophy. Ed. by E. N. Zalta & U.\nNodelman. Spring 2023. Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/\narchives/spr2023/entries/intentionality/\nJakobson, R. (2003). The Metaphoric and Metonymic Poles. Metaphor and metonymy in comparison and\ncontrast, 20 , 41–47.\nKahneman, D. (2003). Maps of Bounded Rationality: Psychology for Behavioral Economics. American\nEconomic Review, 93 (5), 1449–1475.\n123\n32 Page 22 of 24\nLLMs, Agency, and Why Speech Acts are Beyond Them (For Now)\nKant, I. (2012). Critique of Judgement. Trans. by J. H. Bernard. Mineola, N.Y .: Dover Publications. ISBN:\n978-1-306-33778-6\nKant, I. (1785). Grundlegung zur Metaphysik der Sitten . Hamburg: Meiner.\nKant, I. (1781). Kritik der reinen V ernunft. Ed. by Jens Timmermann. Hamburg: Meiner\nKant, I. (1793). Kritik der Urteilskraft . Frankfurt a. M.: Suhrkamp.\nKassner, N. & Schütze, H. (2020). “Negated and Misprimed Probes for Pretrained Language Models:\nBirds Can Talk, But Cannot Fly”. In: Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics. Online: Association for Computational Linguistics , pp. 7811–7818.\nhttps://doi.org/10.18653/v1/2020.acl-main.698 . https://aclanthology.org/2020.acl-main.698\nKhalid, S. (2019). BERT Explained: A Complete Guide with Theory and Tutorial. https://towardsml.com/\n2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/ (visited on 05/22/2020)\nLan, Z. et al. (2019). “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations’.\nIn: International Conference on Learning Representations\nLäubli, S., Sennrich, R., & V olk, M. (2018). “Has Machine Translation Achieved Human Parity? A Case\nFor Document-Level Evaluation”. arXiv preprint arXiv:1808.07048\nLavin, D. (2015). Action as a form of temporal unity: on Anscombe’s Intention. Canadian Journal of\nPhilosophy, 45(5–6), 609–629.\nList, C. (2021). Group Agency and Artiﬁcial Intelligence. Philosophy & Technology, 34 (4), 1213–1242.\nISSN: 2210-5433, 2210-5441. https://doi.org/10.1007/s13347-021-00454-7 . https://link.springer.\ncom/10.1007/s13347-021-00454-7 (visited on 11/24/2023)\nLiu, H., et al. (2023). Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4 . arXiv: 2304.03439\n[cs]. (visited on 08/30/2023). preprint\nLiu, Y ., et al. (2019). “Roberta: A Robustly Optimized Bert Pretraining Approach”. arXiv preprint\narXiv:1907.11692\nMadhyastha, P . & Jain, R. (2019). “On Model Stability as a Function of Random Seed”. arXiv preprint\narXiv:1909.10447\nMarten, R. (1967). “„Selbstprädikation “bei Platon”. In\nOpenAI (2023). GPT-4 Technical Report. arXiv: 2303.08774 [cs.CL]\nPoibeau, T. (2017). Machine translation. Cambridge, Massachusetts/London, England: MIT Press.\nPopa, E. (2021). Human Goals Are Constitutive of Agency in Artiﬁcial Intelligence (AI). Philosophy\n& Technology, 34 (4), 1731–1750. ISSN: 2210-5441. https://doi.org/10.1007/s13347-021-00483-2 .\n(visited on 11/24/2023)\nRadford, A., & Narasimhan, K., et al. (2018). “Improving Language Understanding by Generative Pre-\nTraining”. In: Preprint. Work in Progress\nRadford, A., & Wu, J., et al. (2019). “Language Models Are Unsupervised Multitask Learners”. In:\nOpenAI Blog . https://cdn.openai.com/better-language-models/language_models_are_unsupervised_\nmultitask_learners.pdf\nReis, W. J. (1970). Formen Der Freien Assoziation Zu Träumen. Psyche, 24 (2), 101–115.\nSanh, V ., et al. (2019). “DistilBERT, a Distilled V ersion of BERT: Smaller, Faster, Cheaper and Lighter”.\narXiv preprint arXiv:1910.01108\nSearle, J. (1969). Speech Acts: An Essay in the Philosophy of Language . Cambridge: Cambridge University\nPress. https://doi.org/10.1017/CBO9781139173438\nShepherd, J. (2019). Skilled action and the double life of intention. Philosophy and Phenomenological\nResearch, 98(2), 286–305.\nTaylor, C. (2016). The Language Animal . Cambridge, Massachusetts/ London, England: Harvard University\nPress.\nTouvron, H., et al. (2023). “Llama 2: Open Foundation and Fine-Tuned Chat Models”. arXiv:2307.09288\nvan den Berg, H. (2014). Kant on Proper Science Biology in the Critical Philosophy and the Opus postumum .\nSpringer.\nV aswani, A., et al. (2017). Attention is All you Need. Advances in Neural Information Processing Systems,\n30\nV oita, E., et al. (2019). “Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the\nRest Can Be Pruned”. In: Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics. Florence, Italy: Association for Computational Linguistics, pp. 5797–5808. https://doi.\norg/10.18653/v1/P19-1580 . https://aclanthology.org/P19-1580\nWalsh, D. M. (2006). “Organisms as Natural Purposes: The Contemporary Evolutionary Perspective”. In:\nStudies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological\n123\nPage 23 of 24 32\nR. Gubelmann\nand Biomedical Sciences 37.4, pp. 771–791. ISSN: 13698486. https://doi.org/10.1016/j.shpsc.2006.\n09.009. https://linkinghub.elsevier.com/retrieve/pii/S1369848606000768 (visited on 05/11/2023)\nWang, A., Pruksachatkun, Y ., et al. (2019). SuperGLUE: A Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems. Advances in Neural Information Processing Systems, 32\nWang, A., & Singh, A., et al. (2018). “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding”. In: Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP. Brussels, Belgium: Association for Computational Linguis-\ntics, pp. 353–355. https://doi.org/10.18653/v1/W18-5446 . https://www.aclweb.org/anthology/W18-\n5446\nWarstadt, A., et al. (2020). BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of\nthe Association for Computational Linguistics, 8 , 377–392.\nWestphal, K. R. (2014). Autonomy, Freedom & Embodiment: Hegel’s Critique of Contemporary Biolo-\ngism. Hegel Bulletin, 35 (1), 56–83. ISSN: 2051-5367, 2051- 5375. https://doi.org/10.1017/hgl.2014.\n4. https://www.cambridge.org/core/product/identiﬁer/S2051536714000043/type/journal_article (vis-\nited on 11/24/2023)\nWilks, Y . (2014). “Language and Communication”. In: The Cambridge Handbook of Artiﬁcial Intelligence .\nEd. by K. Frankish, & W. M. Ramsey, pp. 213–231\nWittgenstein, L. (2006). “Philosophische Untersuchungen”. In: Werkausgabe Band 1. Frankfurt am Main:\nSuhrkamp\nWu, T., & Kang, Y .-S. (1997). Criminal Liability for the Actions of Subordinates-the Doctrine of Command\nResponsibility and Its Analogues in United States Law. Harv. Int’l. LJ, 38 , 272.\nYang, Z., et al. (2019). Xlnet: Generalized Autoregressive Pretraining for Language Understanding.\nAdvances in Neural Information Processing Systems, 32\nZammito, J. (2006). Teleology Then and Now: The Question of Kant’s Relevance for Contemporary\nControversies over Function in Biology. Studies in History and Philosophy of Science Part C:\nStudies in History and Philosophy of Biological and Biomedical Sciences, 37 (4), 748–770. ISSN:\n13698486. https://doi.org/10.1016/j.shpsc.2006.09.008. https://linkinghub.elsevier.com/retrieve/pii/\nS1369848606000756 (visited on 05/11/2023)\nŽižek, S., et al. (2010). Unbehagen and the subject: An interview with Slavoj Žižek. Psychoanalysis, Culture\n& Society, 15 , 418–428.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional afﬁliations.\n123\n32 Page 24 of 24"
}