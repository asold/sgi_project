{
  "title": "CATE: Computation-aware Neural Architecture Encoding with Transformers",
  "url": "https://openalex.org/W3133074276",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5013074509",
      "name": "Yan Shen",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5083752471",
      "name": "Kaiqiang Song",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100394574",
      "name": "Fei Liu",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5100675021",
      "name": "Mi Zhang",
      "affiliations": [
        "Michigan State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3104882907",
    "https://openalex.org/W3105594518",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W2996012599",
    "https://openalex.org/W2964212578",
    "https://openalex.org/W3024355276",
    "https://openalex.org/W2963821229",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3013310686",
    "https://openalex.org/W2556833785",
    "https://openalex.org/W2610817424",
    "https://openalex.org/W3102476541",
    "https://openalex.org/W2952490571",
    "https://openalex.org/W2963778169",
    "https://openalex.org/W3136506711",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2953604046",
    "https://openalex.org/W2960010704",
    "https://openalex.org/W2916118939",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W2137226992",
    "https://openalex.org/W1746819321",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3034744318",
    "https://openalex.org/W3106050345",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3081305497",
    "https://openalex.org/W3107453328",
    "https://openalex.org/W3203632720",
    "https://openalex.org/W3015107381",
    "https://openalex.org/W3210924156",
    "https://openalex.org/W2785366763",
    "https://openalex.org/W2111935653",
    "https://openalex.org/W2906697496",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2962750597",
    "https://openalex.org/W2981686275",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3047225593",
    "https://openalex.org/W3035189477",
    "https://openalex.org/W3097891784",
    "https://openalex.org/W2944213978",
    "https://openalex.org/W3103697708",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2964515685",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2970991073",
    "https://openalex.org/W165295313",
    "https://openalex.org/W3034713951",
    "https://openalex.org/W3035584989",
    "https://openalex.org/W3104881550",
    "https://openalex.org/W2963423218",
    "https://openalex.org/W3016781605",
    "https://openalex.org/W3040986370",
    "https://openalex.org/W3094801149",
    "https://openalex.org/W1581066146",
    "https://openalex.org/W2949264490",
    "https://openalex.org/W3049519473",
    "https://openalex.org/W3047744011",
    "https://openalex.org/W3128968934",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2963815651",
    "https://openalex.org/W1798702550",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2803461142",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3127389359",
    "https://openalex.org/W2964294659",
    "https://openalex.org/W3176084580",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2981985696",
    "https://openalex.org/W2902251695",
    "https://openalex.org/W2888429796",
    "https://openalex.org/W3099431291",
    "https://openalex.org/W2007572995",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W3102847511",
    "https://openalex.org/W2999270366",
    "https://openalex.org/W2556372419"
  ],
  "abstract": "Recent works (White et al., 2020a; Yan et al., 2020) demonstrate the importance of architecture encodings in Neural Architecture Search (NAS). These encodings encode either structure or computation information of the neural architectures. Compared to structure-aware encodings, computation-aware encodings map architectures with similar accuracies to the same region, which improves the downstream architecture search performance (Zhang et al., 2019; White et al., 2020a). In this work, we introduce a Computation-Aware Transformer-based Encoding method called CATE. Different from existing computation-aware encodings based on fixed transformation (e.g. path encoding), CATE employs a pairwise pre-training scheme to learn computation-aware encodings using Transformers with cross-attention. Such learned encodings contain dense and contextualized computation information of neural architectures. We compare CATE with eleven encodings under three major encoding-dependent NAS subroutines in both small and large search spaces. Our experiments show that CATE is beneficial to the downstream search, especially in the large search space. Moreover, the outside search space experiment demonstrates its superior generalization ability beyond the search space on which it was trained. Our code is available at: https://github.com/MSU-MLSys-Lab/CATE.",
  "full_text": "CATE: Computation-aware Neural Architecture Encoding with Transformers\nShen Yan1 Kaiqiang Song2 3 Fei Liu2 Mi Zhang1\nAbstract\nRecent works (White et al., 2020a; Yan et al.,\n2020) demonstrate the importance of architec-\nture encodings in Neural Architecture Search\n(NAS). These encodings encode either structure\nor computation information of the neural archi-\ntectures. Compared to structure-aware encod-\nings, computation-aware encodings map archi-\ntectures with similar accuracies to the same re-\ngion, which improves the downstream architec-\nture search performance (Zhang et al., 2019;\nWhite et al., 2020a). In this work, we intro-\nduce a Computation-Aware Transformer-based\nEncoding method called CATE. Different from\nexisting computation-aware encodings based on\nﬁxed transformation (e.g. path encoding), CATE\nemploys a pairwise pre-training scheme to learn\ncomputation-aware encodings using Transform-\ners with cross-attention. Such learned encodings\ncontain dense and contextualized computation in-\nformation of neural architectures. We compare\nCATE with eleven encodings under three ma-\njor encoding-dependent NAS subroutines in both\nsmall and large search spaces. Our experiments\nshow that CATE is beneﬁcial to the downstream\nsearch, especially in the large search space. More-\nover, the outside search space experiment demon-\nstrates its superior generalization ability beyond\nthe search space on which it was trained. Our code\nis available at: https://github.com/MSU-MLSys-\nLab/CATE.\n1. Introduction\nNeural Architecture Search (NAS) has recently drawn con-\nsiderable attention (Elsken et al., 2019). While majority of\nthe prior work focuses on either constructing new search\nspaces (Liu et al., 2018b; Radosavovic et al., 2020; Ru et al.,\n1Michigan State University 2University of Central Florida\n3Tencent AI Lab. Correspondence to: Shen Yan <yan-\nshen6@msu.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\n2020) or designing efﬁcient architecture search and evalu-\nation methods (Luo et al., 2018b; Shi et al., 2020; White\net al., 2021), some of the most recent work (White et al.,\n2020a; Yan et al., 2020) sheds light on the importance of ar-\nchitecture encoding on the subroutines in the NAS pipeline\nas well as on the overall performance of NAS.\nWhile existing NAS methods use diverse architecture en-\ncoders such as LSTM (Zoph et al., 2018; Luo et al., 2018b),\nSRM (Baker et al., 2018), MLP (Liu et al., 2018a; Wang\net al., 2020), GNN (Wen et al., 2020; Shi et al., 2020; Yan\net al., 2020) or adjacency matrix itself (Kandasamy et al.,\n2018; Real et al., 2019; White et al., 2020b), these encoders\nencode either structures (Luo et al., 2018b; Ying et al., 2019;\nWang et al., 2020; Wen et al., 2020; Shi et al., 2020; Yan\net al., 2020) or computations (Zhang et al., 2019; Ning\net al., 2020b; White et al., 2021) of the neural architectures.\nCompared to structure-aware encodings, computation-aware\nencodings are able to map architectures with different struc-\ntures but similar accuracies to the same region. This advan-\ntage contributes to a smooth encoding space with respect\nto the actual architecture performance instead of structures,\nwhich improves the efﬁciency of the downstream architec-\nture search (Zhang et al., 2019; 2020; White et al., 2020a).\nWe argue that current architecture encoders limit the power\nof computation-aware architecture encoding for NAS. The\nmajor limitations lie in their representation power and the\neffectiveness of their pre-training objectives. Speciﬁcally,\n(Zhang et al., 2019) uses shallow GRUs to encode computa-\ntion, which is not sufﬁcient to capture deep contextualized\ncomputation information. Moreover, their decoder is trained\nwith the reconstruction loss via asynchronous message pass-\ning. This is very challenging in practice because directly\nlearning the generative model based on a single architecture\nis not trivial. As a result, its pre-training is less effective\nand the downstream NAS performance is not as competi-\ntive as state-of-the-art structure-aware encoding methods.\n(White et al., 2020a) proposes a computation-aware encod-\ning method based on a ﬁxed transformation called path\nencoding, which shows outstanding performance under the\npredictor-based NAS subroutine. However, path encoding\nscales exponentially without truncation and it inevitably\ncauses information loss with truncation. Moreover, path en-\ncoding exhibits worse generalization performance in outside\nsearch space compared to the adjacency matrix encoding\narXiv:2102.07108v2  [cs.LG]  11 Jun 2021\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nsince it could not generalize to unseen paths that are not\nincluded in the training search space.\nIn this work, we propose a new computation-aware neural\narchitecture encoding method named CATE (Computation-\nAware Transformer-based Encoding) that alleviates the lim-\nitations of existing computation-aware encoding methods.\nAs shown in Figure 1, CATE takes paired computationally\nsimilar architectures as its input. Similar to BERT, CATE\ntrains the Transformer-based model (Vaswani et al., 2017)\nusing the masked language modeling (MLM) objective (De-\nvlin et al., 2019). Each input architecture pair is corrupted\nby replacing a fraction of their operators with a special\nmask token. The model is trained to predict those masked\noperators from the corrupted architecture pair.\nCATE differs from BERT (Devlin et al., 2019) in two as-\npects. First, each prediction in LMs has its inductive bias\ngiven the contextual information from different positions.\nThis, however, is not the case in architecture representation\nlearning since the prediction distribution is uniform for any\nvalid graph, making it difﬁcult to directly learn the genera-\ntive model from a single architecture. Therefore, we propose\na pairwise pre-training scheme that encodes computationally\nsimilar architecture pairs through two Transformers with\nshared parameters. The two individual encodings are then\nconcatenated, and the concatenated encoding is fed into an-\nother Transformer with a cross-attention encoder to encode\nthe joint information of the architecture pair. Second, the\nfully-visible attention mask (Raffel et al., 2020) could not\nbe used for architecture representation learning because it\ndoes not reﬂect the single-directional ﬂow ( e.g. directed,\nacyclic, single-in-single-out) of neural architectures (Xie\net al., 2019a; You et al., 2020a). Therefore, instead of using\na bidirectional Transformer encoder as in BERT, we directly\nuse the adjacency matrix to compute the causal mask (Raffel\net al., 2020). The adjacency matrix is further augmented\nwith the Floyd algorithm (Floyd, 1962) to encode the long-\nrange dependency of different operations. Together with the\nMLM objective, CATE is able to encode the computation\nof architectures and learn dense and deep contextualized ar-\nchitecture representations that contain both local and global\ncomputation information in neural architectures. This is\nimportant for architecture encodings to be generalized to\noutside search space beyond the training search space.\nWe compare CATE with eleven structure-aware and\ncomputation-aware architecture encoding methods under\nthree major encoding-dependent subroutines as well as eight\nNAS algorithms on NAS-Bench-101 (Ying et al., 2019)\n(small), NAS-Bench-301 (Siems et al., 2020) (large), and\nan outside search space (White et al., 2020a) to evalu-\nate the effectiveness, scalability, and generalization abil-\nity of CATE. Our results show that CATE is beneﬁcial\nto the downstream architecture search, especially in the\nlarge search space. Speciﬁcally, we found the strongest\nNAS performance in all search spaces using CATE with a\nBayesian optimization-based predictor subroutine together\nwith a novel computation-aware search. Moreover, the out-\nside search space experiment shows its superior general-\nization capability beyond the search space on which it was\ntrained. Finally, our ablation studies show that the quality\nof CATE encodings and downstream NAS performance are\nnon-decreasingly improved with more training architecture\npairs, more cross-attention Transformer blocks and larger\ndimension of the feed-forward layer.\n2. Related Work\nNeural Architecture Search (NAS).NAS has been started\nwith genetic algorithms (Miller et al., 1989; Kitano, 1990;\nStanley & Miikkulainen, 2002) and recently becomes pop-\nular when (Zoph & Le, 2017; Baker et al., 2017) gain sig-\nniﬁcant attention. Since then, various NAS methods have\nbeen explored including sampling-based and gradient-based\nmethods. Representative sampling-based methods include\nrandom search (Li & Talwalkar, 2019), evolutionary al-\ngorithms (Real et al., 2019; Lu et al., 2020), local search\n(Ottelander et al., 2020; White et al., 2020b), reinforcement\nlearning (Zoph et al., 2018; Tan et al., 2019), Bayesian opti-\nmization (Kandasamy et al., 2018; Zhou et al., 2019), Monte\nCarlo tree search (Negrinho & Gordon, 2017; Wang et al.,\n2020) and Neural predictor (Baker et al., 2018; Liu et al.,\n2018a; Wen et al., 2020; Tang et al., 2020; Ning et al., 2020a;\nLuo et al., 2020; Shi et al., 2020; Yan et al., 2020; White\net al., 2021; Ru et al., 2021). Weight-sharing methods (Ben-\nder et al., 2018; Pham et al., 2018) have become popular due\nto their computation efﬁciency. Based on weight-sharing,\ngradient-based methods are proposed to optimize the archi-\ntecture selection with gradient decent (Luo et al., 2018b; Liu\net al., 2019a; Xie et al., 2019b; Dong & Yang, 2019; Yan\net al., 2019; You et al., 2020b; Peng et al., 2020; Zela et al.,\n2020; Chen & Hsieh, 2020). For comprehensive surveys, we\nsuggest referring to (Elsken et al., 2019; Xie et al., 2020).\nNeural Architecture Encoding.Majority of existing NAS\nwork use one-hot adjacency matrix to encode the structures\nof neural architectures. However, adjacency matrix-based\nencoding grows quadratically as the search space scales up.\n(Ying et al., 2019) proposes categorical adjacency matrix-\nbased encoding to ensure ﬁxed length encodings. They also\npropose continuous adjacency matrix-based encoding that is\nsimilar to DARTS (Liu et al., 2019a), where the architecture\nis created by taking ﬁxed number of edges with the highest\ncontinuous values. However, this approach is not easily\napplicable to some NAS algorithms such as regularized evo-\nlution (Real et al., 2017) without major changes. Tabular\nencoding in the form of ConﬁgSpace (Lindauer et al., 2019)\nis often used for hyperparameter optimization (Li et al.,\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nFigure 1.Overview of CATE. CATE takes computationally similar architecture pairs as the input. The model is trained to predict masked\noperators given the pairwise computational information. Apart from the cross-attention blocks, the pretrained Transformer encoder is used\nto extract architecture encodings for the downstream encoding-dependent NAS subroutines.\n2018; Falkner et al., 2018) and recently adopted by NAS-\nBench-301 (Siems et al., 2020) to represent architectures by\nintroducing categorical hyperparameters for each operation\nalong each potential edge. Recent NAS methods (Luo et al.,\n2018a; Wang et al., 2020; Wen et al., 2020; Shi et al., 2020)\nuse adjacency matrix as the input to LSTM/MLP/GNN to\nencode the structures of neural architectures in the latent\nspace. (Yan et al., 2020) validates that pre-training archi-\ntecture representations without using accuracies can better\npreserve the local structural relationship of neural architec-\ntures in the latent space. (Wei et al., 2020b) proposes to\nlearn architecture representations using contrastive learning\nto ﬁnd low-dimensional embeddings. (Choi et al., 2021)\nstudies various locality-based self-supervised objectives on\nthe effect of architecture representations. One disadvantage\nof these methods is that they rely on a prior where the edit\ndistance closeness between different architectures is a good\nindicator of the relative performance; however, structure-\naware encodings may not be computationally unique unless\nsome certain graph hashing is applied (Ying et al., 2019;\nNing et al., 2020b). (White et al., 2021; Wei et al., 2020a)\nuse path encoding and its categorical and continuous vari-\nants, which encode computation of architectures so that\nisomorphic cells are mapped to the same encoding. (Zhang\net al., 2019) uses GRU-based asynchronous message pass-\ning to encode computation of architectures and the model is\ntrained with the V AE loss. (Lukasik et al., 2021) proposes a\ntwo-sided variational encoder-decoder GNN to learn smooth\nembeddings in various NAS search spaces. CATE is inspired\nby the advantage of computation encoding and addresses\nthe drawbacks of (Zhang et al., 2019; White et al., 2021).\nAnother line of work is based on the intrinsic properties of\nthe architectures. (Hesslow & Poli, 2021) generates archi-\ntecture representations by using contrastive learning over\ndata Jacobian matrix values computed based on different ini-\ntializations, and the generated embeddings are independent\nof the parameterization of the search space.\nContext Dependency.Our work is close to self-supervised\nlearning in language models (LMs) (Dong et al., 2019). In\nparticular, ELMo (Peters et al., 2018) uses two shallow uni-\ndirectional LSTMs (Hochreiter & Schmidhuber, 1997) to\nencode bidirectional text information, which is not sufﬁcient\nfor modeling deep interactions between the two directions.\nGPT-2 (Radford et al., 2019) proposes an autoregressive lan-\nguage modeling method with Transformer (Vaswani et al.,\n2017) to cover the left-to-right dependency and is further\ngeneralized by XLNet (Yang et al., 2019) which encodes\nbidirectional context. (Ro)BERT/BART/T5 (Devlin et al.,\n2019; Liu et al., 2019b; Lewis et al., 2020; Raffel et al.,\nCATE: Computation-aware Neural Architecture Encoding with Transformers\n2020) use bidirectional Transformer encoder to encode both\nleft and right context. In architecture representation learn-\ning, however, the attention mask in the encoder cannot be\nused to attend to all the operators because it does not reﬂect\nthe single-directional ﬂow of the computational graphs (Xie\net al., 2019a; You et al., 2020a).\n3. CATE\n3.1. Search Space\nWe restrict our search space to the cell-based architectures.\nFollowing the conﬁguration in (Ying et al., 2019), each cell\nis a labeled directed acyclic graph (DAG) G= (V,E), with\nVas a set of N nodes and Eas a set of edges that connect\nthe nodes. Each node vi ∈V, i∈[1,N] is associated with\nan operation selected from a predeﬁned set of V operations,\nand the edges between different nodes are represented as an\nupper triangular binary adjacency matrix A ∈{0,1}N× N.\n3.2. Computation-aware Neural Architecture Encoder\nOur proposed computation-aware neural architecture en-\ncoder is built upon the Transformer encoder architecture\nwhich consists of a semantic embedding layer and LTrans-\nformer blocks stacked on top. Given G, each operation vi is\nﬁrst fed into a semantic embedding layer of size de:\nEmbi = Embedding(vi) (1)\nThe embedded vectors are then contextualized at different\nlevels of abstract. We denote the hidden state afterl-th layer\nas Hl = [Hl\n1,...,Hl\nN] of size dh, where Hl = T(Hl− 1)\nand T is a transformer block containing nhead heads. The\nl-th Transformer block is calculated as:\nQk = Hl−1Wl\nqk,Kk = Hl−1Wl\nkk,Vk = Hl−1Wl\nvk (2)\nˆHl\nk = softmax(QkKT\nk√dh\n+ M)Vk (3)\nˆHl = concatenate( ˆHl\n1, ˆHl\n2,..., ˆHl\nnhead ) (4)\nHl = ReLU( ˆHlW1 + b1)W2 + b2 (5)\nwhere the initial hidden state H0\ni is Embi, thus de = dh.\nQk, Kk, Vk stand for “Query\", “Key\" and “Value\" in the\nattention operation of the k-th head respectively. M is the\nattention mask in the Transformer, where Mi,j ∈{0,−∞}\nindicates whether operation j is a dependent operation of\noperation i. W1 ∈Rdc× dff and W2 ∈Rdff × dc denote\nthe weights in the feed-forward layer.\nDirect/Indirect Dependency Mask.A pair of nodes (op-\nerations) within an architecture are dependent if there is\neither a directed edge that directly connects them ( local\nAlgorithm 1Floyd Algorithm\n1: Input: the node set V, the adjacent matrix A\n2: ˜A ← A\n3: for k∈V do\n4: for i∈V do\n5: for j ∈V do\n6: ˜Ai,j |= ˜Ai,k & ˜Ak,j\n7: Output: ˜A\ndependency) or a path made of a series of such edges that in-\ndirectly connects them (long-range dependency). We create\ndependency masks for such pairs of nodes for both direct\nand indirect cases and use these dependency masks as the\nattention masks in the Transformer. Speciﬁcally, the direct\ndependency mask MDirect and the indirect dependency\nmask MIndirect can be created as follows:\nMDirect\ni,j =\n{ 0, if Ai,j = 1\n−∞, if Ai,j = 0\nMIndirect\ni,j =\n{ 0, if ˜Ai,j = 1\n−∞, if ˜Ai,j = 0\nwhere A is the adjacency matrix and ˜A = Floyed(A) is\nderived using Floyd algorithm in Algorithm 1.\nUni/Bidirectional Encoding.Finally, the ﬁnal hidden vec-\ntor Hl\nN is used as the unidirectional encoding for the archi-\ntecture. We also considered encoding the architecture in a\nbidirectional manner, where both the output node hidden\nvector from the original DAG and the input node hidden\nvector from the reversed one are extracted and then con-\ncatenated together. However, our experiments show that\nbidirectional encoding performs worse than unidirectional\nencoding. We include this result in Appendix A.\n3.3. Pre-training CATE\nArchitecture Pair Sampling. We split the dataset into\n95% training and 5% held-out test sets for our pairwise\npre-training. To ensure that it does not scale with quadratic\ntime complexity, we ﬁrst sort the architectures based on\ntheir computational attributes P (e.g. number of parame-\nters, FLOPs). We then employ a sliding window for each\narchitecture xi and its neighborhood r(xi) ={y: |P(xi) −\nP(y)|<δ}, where δis a hyperparameter for the pairwise\ncomputation constraint. Finally, we randomly select Kdis-\ntinct architectures Y = {y1,...,yK},xi /∈Y,Y ⊂r(xi)\nwithin the neighborhood to compose K architecture pairs\n{(xi,y1),...,(xi,yK)}for architecture xi.\nPairwise Pre-training with Cross-Attention. Once the\ncomputationally similar architecture pair is composed, we\nrandomly select 20% operations from each architecture\nwithin the pair for masking, where 80% of them are re-\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nplaced with a [MASK] token and the remaining 20% are\nreplaced with a random token chosen from the predeﬁned\noperation set. We apply padding to architectures that have\nnodes less than the maximum number of nodes N in one\nbatch to handle variable length inputs. The joint repre-\nsentation HL\nXY is derived by concatenating HL\nX and HL\nY\nfollowed by the summation of the corresponding segment\nembedding. Segment embedding acts as an identiﬁer of\ndifferent architectures during pre-training. We set it to be\ntrainable and randomly initialized. The joint representation\nHL\nXY is then contextualized with another Lc-layer Trans-\nformer with the cross-attention maskMcsuch that segments\nfrom the two architectures can attend to each other given the\npairwise information. For example, given two architectures\nX with three nodes and Y with four nodes in Figure 1, X\nhas access to the non-padded nodes of Y and itself, and\nsame for Y. The cross-attention dimension of the encoder\nis denoted as dc. The joint representation of the last layer is\nused for prediction. The model is trained by minimizing the\ncross-entropy loss computed using the predicted operations\nand the original operations.\n3.4. Encoding-dependent NAS Subroutines\n(White et al., 2020a) identiﬁes three major encoding-\ndependent subroutines included in existing NAS algorithms:\nsample random architecture, perturb architecture, and train\npredictor model. The sample random architecture subrou-\ntine includes random search (Li & Talwalkar, 2019). The\nperturb architecture subroutine includes regularized evolu-\ntion (REA) (Real et al., 2019) and local search (LS) (White\net al., 2020b). The train predictor model subroutine in-\ncludes neural predictor (Wen et al., 2020; Shi et al., 2020;\nWhite et al., 2021), Bayesian optimization with Gaussian\nprocess (GP) (Rasmussen & Williams, 2006), and Bayesian\noptimization with neural networks (DNGO) (Snoek et al.,\n2015) which is much faster to ﬁt compared to GP and scales\nlinearly with large datasets rather than cubically.\nInspired by (Ottelander et al., 2020; White et al., 2020b),\nwe found that LS (perturb architecture) can be combined\nwith DNGO ( train predictor model). We thus propose a\nDNGO-based computation-aware search using CATE called\nCATE-DNGO-LS. Speciﬁcally, we maintain a pool of sam-\npled architectures and take iterations to add new ones. In\neach iteration, we pass all architecture encodings to the pre-\ndictor trained 30 epochs with samples in the current pool.\nWe select new architectures with top-5 predicted accuracy\nand add them to the pool. Assume there are M new architec-\ntures which become the new top-5 in the updated pool. We\nthen select the nearest neighbors of the other (5-M) top-5\narchitectures in L2 distance in latent space and add them\nto the pool. Hence, there will be 5 to 10 new architectures\nadded to the pool in each iteration. The search stops when\nthe number of samples reaches a pre-deﬁned budget.\n4. Experiments\nWe describe two NAS benchmarks used in our experiments.\nNAS-Bench-101. The NAS-Bench-101 search space (Ying\net al., 2019) consists of 423,624 architectures. Each archi-\ntecture has its pre-computed validation and test accuracies\non CIFAR-10. The cell includes up to 7 nodes and at most 9\nedges with the ﬁrst node as input and the last node as output.\nThe intermediate nodes can be either 1×1 convolution, 3×3\nconvolution, or 3×3 max pooling. We use the number of\nnetwork parameters as the computational attribute P for\narchitecture pair sampling. We set δto 2,000,000 and K\nto 2. The ablation studies on δand K are summarized in\nSection 4.4. We split the dataset into 95% training and 5%\nheld-out test sets for pre-training.\nNAS-Bench-301. NAS-Bench-301 (Siems et al., 2020) is a\nnew surrogate benchmark on the DARTS (Liu et al., 2019a)\nsearch space that is much larger than NAS-Bench-101. It\nwas created by fully training 60,000 architectures that is\nstratiﬁed by the NAS methods1 with a good coverage and\nthen ﬁtting a surrogate model that can estimate the accuracy\n(with noise) at epoch100 and the training time for any of the\nremaining 1018 architectures. To convert the DARTS search\nspace into one with the same input format as NAS-Bench-\n101, we add a summation node to make nodes represent op-\nerations and edges represent data ﬂow. Following (Liu et al.,\n2018a), we use the same cell for both normal and reduc-\ntion cell, allowing roughly 109 DAGs without considering\ngraph isomorphism. More details about the DARTS/NAS-\nBench-301 and a cell transformation example are included\nin Appendix D. We randomly sample 1,000,000 architec-\ntures in this search space, and use the same data split used in\nNAS-Bench-101 for pre-training. We use network FLOPs\nas the computational attribute P for architecture pair sam-\npling. We set δto 5,000,000 and Kto 1. Since some NAS\nmethods we compare against use the same GIN (Xu et al.,\n2019) surrogate model used in NAS-Bench-301, to ensure\nfair comparison, we thus followed (Siems et al., 2020) to use\nXGB-v1.0 and LGB-runtime-v1.0 which utilizes gradient\nboosted trees (Chen & Guestrin, 2016; Ke et al., 2017) as\nthe regression model.\nModel and Training.We use a L= 12layer Transformer\nencoder and a Lc = 24layer cross-attention Transformer\nencoder, each has 8 attention heads. The hidden state size is\ndh = dc = 64for all the encoders. The hidden dimension\nis dff = 64 for all the feed-forward layers. We employ\nAdamW (Loshchilov & Hutter, 2019) as our optimizer. The\ninitial learning rate is 1e-3. The momentum parameters are\nset to 0.9 and 0.999. The weight decay is 0.01 for regular\nlayer and 0 for dropout and layer normalization. We trained\n1We suggest referring to C.2 in (Siems et al., 2020) for a\ndetailed description on the data collection.\nCATE: Computation-aware Neural Architecture Encoding with Transformers\n20 40 60 80 100 120 140\nnumber of samples\n6.3\n6.4\n6.5\n6.6\n6.7\n6.8\n6.9\n7.0test error [%]\nSample Random Arch:Random Search\nadjacency\npath\ncont_adj\ncont_path\ncate\n20 40 60 80 100 120 140\nnumber of samples\n6.2\n6.4\n6.6\n6.8\n7.0test error [%]\nPerturb Arch:Regularized Evolution\nadjacency\npath\ntrunc_path\ncat_path\ncate\n20 40 60 80 100 120 140\nnumber of samples\n6.0\n6.1\n6.2\n6.3\n6.4\n6.5\n6.6\n6.7test error [%]\nPerturb Arch:Local Search\nadjacency\ncont_adj\npath\ntrunc_path\ncate\n20 40 60 80 100 120 140\nnumber of samples\n6.0\n6.2\n6.4\n6.6\n6.8\n7.0\n7.2\n7.4\n7.6test error [%]\nTrain Predictor Model:Neural Predictor\ndvae\ncat_adj\ncont_adj\ntrunc_path\ncate\n20 40 60 80 100 120 140\nnumber of samples\n6.0\n6.2\n6.4\n6.6\n6.8test error [%]\nTrain Predictor Model:Bayesian Optimization (GP)\nadjacency\ncont_adj\npath\ntrunc_path\ncate\n20 40 60 80 100 120 140\nnumber of samples\n6.0\n6.2\n6.4\n6.6\n6.8test error [%]\nTrain Predictor Model:Bayesian Optimization (DNGO)\nadjacency\npath\ntrunc_path\narch2vec\ncate\nFigure 2.Comparison between CATE and other architecture encoding schemes under different subroutines on NAS-Bench-101: sample\nrandom architecture (top left), perturb architecture (top middle, top right), and train predictor model (bottom left, bottom middle, bottom\nright). It reports the test error of 200 independent runs given 150 queried architectures.\nour model with batch size of 1024 on NVIDIA Quadro\nRTX 8000 GPUs. It takes around 4GB GPU memory for\nNAS-Bench-101 and 9GB GPU memory for NAS-Bench-\n301. The validation loss converges well after 10 epochs of\npretraining, which takes 1.2 hours on NAS-Bench-101 and\n7.5 hours on NAS-Bench-301.\n4.1. Comparison with Different Encoding Schemes\nIn our ﬁrst experiment, we compare CATE with eleven\narchitecture encoding schemes under three major encoding-\ndependent subroutines described in Section 3.4 on NAS-\nBench-101. These encoding schemes include (1-3) one-\nhot/categorical/continuous adjacency matrix encoding (Ying\net al., 2019), (4-6) one-hot/categorical/continuous path en-\ncoding and (7-9) their corresponding truncated counterparts\n(White et al., 2021), (10) D-V AE (Zhang et al., 2019), and\n(11) arch2vec (Yan et al., 2020). For continuous encod-\nings, we use L2 distance as the distance metric. To examine\nthe effectiveness of the encoding schemes themselves, we\ncompare different encoding schemes under the same search\nsubroutine.\nFigure 2 illustrates our results. For each subroutine, we show\nthe top-ﬁve best-performing encoding schemes. Overall,\ndespite there is no overall best encoding, we found that\nCATE is among the top ﬁve across all the subroutines.\nSpeciﬁcally, for sample random architecturesubroutine, ran-\ndom search using adjacency matrix encoding performs the\nbest. The random search using continuous encodings per-\nforms slightly worse than the adjacency encodings possibly\ndue to the discretization loss from vector space into a ﬁxed\nnumber of bins of same size before the random sampling.\nFor perturb architecture subroutine, CATE is on par with or\noutperforms adjacency encoding and path encoding because\nit is pre-trained to preserve strong computation locality in-\nformation. This advantage allows the evolution or local\nsearch to ﬁnd architectures with similar performance in lo-\ncal neighborhood more easily. Interestingly, we observe\nvery small deviation using local search with CATE. This\nindicates that it always converges to some certain local min-\nimums across different initial seeds. Since NAS-Bench-101\nalready exhibits locality in edit distance, encoding compu-\ntation makes architectures even closer in terms of accuracy\nand thus beneﬁts the local search.\nFor train predictor model subroutine, we have four observa-\ntions: 1) Adjacency matrix encodings perform less effective\nwith neural predictor and DNGO. It is possibly that edit dis-\ntance cannot fully reﬂect the closeness of architectures w.r.t\ntheir actual performance. 2) Path encoding performs well\nCATE: Computation-aware Neural Architecture Encoding with Transformers\n20 40 60 80 100 120 140\nnumber of samples\n5.9\n6.1\n6.3\n6.5\n6.7test error [%]\nNAS on NASBench-101 (>=1 subroutines)\nRS\nREA\nLS\nDNGO\nBOHAMIANN\nBOGCN\nBANANAS\narch2vec-DNGO\ncate-DNGO\ncate-DNGO-LS\n10 20 30 40 50 60 70 80 90 100\nnumber of samples\n5.2\n5.4\n5.6\n5.8\n6.0\n6.2\n6.4test error [%]\nNAS on NASBench-301 (>=1 subroutines)\nRS\nREA\nLS\nDNGO\nBOHAMIANN\nBOGCN\nBANANAS\ncate-LS\ncate-DNGO\ncate-DNGO-LS\nFigure 3.Comparison between CATE and SOTA NAS methods on NAS-Bench-101 (left) and NAS-Bench-301 (right). It reports the test\nerror of 200 independent runs. The error bars denote the variance of the test error. The number of queried architectures is set to 150 for\nNAS-Bench-101 and 100 for NAS-Bench-301.\nwith neural predictor but worse than other encodings with\nBayesian optimization. 3) D-V AE and arch2vec, two en-\ncodings learned via variational autoencoding, perform well\nonly with some certain NAS methods. It could be attributed\nto their challenging training objective which easily leads to\noverﬁtting. 4) CATE is competitive with neural predictor\nand outperforms all the other encodings with Bayesian opti-\nmization. This is because neighboring computation-aware\nencodings correspond with similar accuracies. Moreover,\nthe training objective in CATE is more efﬁcient compared\nto the standard V AE loss (Kingma & Welling, 2014) used\nby D-V AE and arch2vec.\n4.2. Comparison with Different NAS Methods\nIn our second experiment, we compare the neural architec-\nture search performance based on CATE encodings with\nstate-of-the-art NAS algorithms on NAS-Bench-101 and\nNAS-Bench-301. Existing NAS algorithms contain one\nor more encoding-dependent subroutines. We consider six\nNAS algorithms that contain one encoding-dependent sub-\nroutine: random search (RS) (Li & Talwalkar, 2019) (sam-\nple random arch.), regularized evolution (REA) (Real et al.,\n2019) (perturb arch.), local search (LS) (White et al., 2020b)\n(perturb arch.), DNGO (Snoek et al., 2015) (train predictor),\nBOHAMIANN (Springenberg et al., 2016) (train predictor),\narch2vec-DNGO (Yan et al., 2020) (train predictor), and\ntwo NAS algorithms that contain more than one encoding-\ndependent subroutine: BOGCN (Shi et al., 2020) (perturb\narch., train predictor) and BANANAS (White et al., 2021)\n(sample random arch., perturb arch., train predictor). We\ncompare these eight existing NAS algorithms with CATE-\nDNGO: a NAS algorithm based on CATE encodings with\nthe DNGO subroutine (train predictor), and CATE-DNGO-\nNAS methods NAS-Bench-101 NAS-Bench-301\nPrev. SOTA (White et al., 2021) 5.92 5.35\nCATE-DNGO-LS (ours) 5.88 5.28\nTable 1.Comparison between CATE and state-of-the-arts: Final\ntest error [%] given 150 queried architectures on NAS-Bench-101\nand 100 queried architectures on NAS-Bench-301. The result is\naveraged over 200 independent runs.\nLS: a NAS algorithm based on CATE encodings with the\ncombination of DNGO and LS subroutines (train predictor,\nperturb arch.) as described in Section 3.4.\nFigure 3 and Table 1 summarize our results. We have three\nmajor ﬁndings from Figure 3: 1) Architecture encoding mat-\nters especially in the large search space. The right plot shows\nthat CATE-DNGO and CATE-DNGO-LS in DARTS search\nspace not only converge faster but also lead to better ﬁnal\nsearch performance given the same budgets. 2) Local search\n(LS) is a strong baseline in both small and large search\nspaces. As mentioned in Section 4.1, performing LS using\nCATE leads to better results compared to other encodings. 3)\nNAS algorithms that use more than one encoding-dependent\nsubroutine in general perform better than NAS algorithms\nwith just one subroutine. Speciﬁcally, BOGCN and BA-\nNANAS that have multiple subroutines perform better than\nthe single-subroutine NAS algorithms such as REA, DNGO,\nand BOHAMIANN. Moreover, CATE-DNGO-LS leads to\nthe best performing result in both NAS-Bench-101 and NAS-\nBench-301 search spaces. Meanwhile, the improvement of\nCATE-DNGO-LS versus CATE-DNGO shrinks in larger\nsearch space, indicating that the larger search space is more\nchallenging to encode.\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nc_{k-2}\n0max_pool_3x3\nsep_conv_3x3\n2sep_conv_3x3\nc_{k-1}\n1skip_connect\nsep_conv_3x3\nsep_conv_3x3 3\ndil_conv_3x3\nc_{k}\ndil_conv_3x3\nc_{k-2}\n0\nskip_connect\n1sep_conv_3x3\nc_{k-1} sep_conv_3x3\n2\nsep_conv_3x3\n3sep_conv_3x3\ndil_conv_3x3\nsep_conv_3x3\nc_{k}\nsep_conv_3x3\nFigure 4. Top: Best found cell from CATE-DNGO-LS given the\nbudget of 100 samples. Bottom: Best found cell from CATE-\nDNGO-LS given the budget of 300 samples.\nNAS-Bench-301 uses a surrogate model trained on 60k\narchitectures to predict the performance of all the other\narchitectures in the DARTS search space. The performance\nof the other architectures, however, can be inaccurate. Given\nthat, we further validate the effectiveness of CATE-DNGO-\nLS in theactual DARTS search space by training the queried\narchitectures from scratch. We set the budget to100 and 300\nqueries, separately. Each queried architecture is trained for\n50 epochs with a batch size of 96, using 32 initial channels\nand 8 cell layers. The average validation error of the last 5\nepochs is computed as the label. These values are chosen\nto be close to the proxy model used in DARTS. It takes\nabout 3.3 GPU days to ﬁnish the search with 100 quries and\n10.3 GPU days with 300 queries. See Figure 4 for the best\nfound cells. To ensure fair comparison, we compare CATE-\nDNGO-LS to methods (Liu et al., 2019a; Li & Talwalkar,\n2019; Yan et al., 2020; White et al., 2021) that use the\ncommon test evaluation script which is to train for 600\nepochs with cutout and auxiliary tower.\nTable 2 summarizes our results. As shown, CATE-DNGO-\nLS (small budget) achieves competitive performance (2.55%\navg. test error) with much less search cost and CATE-\nDNGO-LS (large budget) achieves superior performance\n(2.46% avg. test error) with similar search cost compared\nto other sampling-based search methods (Yan et al., 2020;\nWhite et al., 2021) in the actual DARTS search space. This\nis consistent with our observation in NAS-Bench-301. We\nreport the transfer learning results on ImageNet (Deng et al.,\n2009) in Table 3.\n4.3. Generalization to Outside Search Space\nIn our third experiment, inspired by (White et al., 2020a),\nwe evaluate the generalization ability of CATE beyond the\nsearch space on which it was trained. The training search\nNAS Methods Avg. Test Error Params Search Cost\n(%) (M) (GPU days)\nRS (Li & Talwalkar, 2019) 3.29±0.15 3.2 4\nDARTS (Liu et al., 2019a) 2.76±0.09 3.3 4\nBANANAS (White et al., 2021)2.67±0.07 3.6 11.8\narch2vec-BO (Yan et al., 2020)2.56±0.05 3.6 9.2\nCATE-DNGO-LS (small budget)2.55±0.08 3.5 3.3\nCATE-DNGO-LS (large budget)2.46±0.05 4.1 10.3\nTable 2.NAS results in DARTS search space using CIFAR-10.\nNAS Methods Params Mult-Adds Top-1 Test Error\n(M) (M) (%)\nSNAS (Xie et al., 2019b) 4.3 522 27.3\nDARTS (Liu et al., 2019a) 4.7 574 26.7\nBayesNAS (Zhou et al., 2019)4.0 440 26.5\narch2vec-BO (Yan et al., 2020) 5.2 580 25.5\nBANANAS (ours) 5.1 576 26.3\nCATE-DNGO-LS (small budget)5.0 556 26.1\nCATE-DNGO-LS (large budget)5.8 642 25.0\nTable 3.Transfer learning results on ImageNet.\nspace is designed as a subset of NAS-Bench-101, where\neach included architecture has 2 to 6 nodes and 1 to 7\nedges. The test search space is disjointed from the training\nsearch space and includes architectures with 6 nodes and 7\nto 9 edges. There are 10,026 and 60,669 non-isomorphic\ngraphs in the training and test space respectively. The CATE\nencodings are pre-trained using the training space and are\nused to conduct architecture search in the test space. We\ncompare CATE with the adjacency matrix encoding because\nit was shown in (White et al., 2020a) to have the best gener-\nalization capability compared to other encodings. A simple\n2-layer MLP with hidden size 128 is used as the neural\npredictor for both encodings.\nFigure 5 shows the validation error curve of the test search\nspace given the number of 150 sample budget across 500\nindependent runs. As shown, CATE outperforms adjacency\nmatrix encoding by a large margin. This indicates that\nCATE can better contextualize the computation information\ncompared to ﬁxed encodings, which generalizes better when\nadapting to outside search space. Moreover, the padding\nscheme in our encoder allows us to handle architectures\nwith different numbers of nodes.\n4.4. Ablation Studies\nFinally, we conduct ablation studies on different hyperpa-\nrameters involved in CATE. We use CATE-DNGO as the\nNAS method and report the ﬁnal NAS test error [%] given\n150 queried architectures on NAS-Bench-101. The result is\naveraged over 200 independent runs.\nArchitecture Pair Sampling Hyperparameters.We plot\nthe histogram of model parameters on NAS-Bench-101\nin Figure 6. As shown, the architectures are neither nor-\nCATE: Computation-aware Neural Architecture Encoding with Transformers\n20 30 40 50 60 70 80 90 100 110 120 130 140 150\nnumber of samples\n5.4\n5.5\n5.6\n5.7\n5.8\n5.9\n6.0\n6.1validation error [%]\nA simple 2-layer MLP predictor\ncate\nadj\nFigure 5.Performance on the out-of-training search space. It re-\nports the validation error of 500 independent runs.\nδ\nK 1 2 4 8\n1 × 10 6 6.02 5.95 5.99 5.95\n2 × 10 6 6.02 5.94 6.04 5.96\n4 × 10 6 5.94 6.03 6.05 5.99\n8 × 10 6 6.05 6.04 6.11 6.04\nTable 4.Effects of δand Kon architecture pair sampling.\nmally nor uniformly distributed in this search space in terms\nof model parameters. This motivates us to use a sliding\nwindow-based architecture pair selection to avoid the un-\nbalanced sampling as proposed in Section 3.3. The choice\nof δand K and their effects on the downstream NAS are\nsummarized in Table 4. We found that strong computation\nlocality (i.e. small δ) usually leads to better results. The\nchoice of neighborhood size Kdoes not have a signiﬁcant\neffect on NAS performance. Therefore, we choose small\nK for faster pretraining. For NAS-Bench-301, we use the\nFLOPs as the computational attributes P and observe the\nsame trend as in NAS-Bench-101 on the selection of δand\nK. We report the results in Appendix B.\nTransformer Hyperparameters.We studied the effect of\nthe number of cross-attention Transformer blocksLcand the\nhidden dimension of the feed-forward layer dff on CATE.\nWe ﬁx δand K for pre-training as mentioned in Section\n4. The downstream NAS result is summarized in Table\n5. It shows that larger Lc and dff usually lead to better\nNAS performance, which indicates that deep contextualized\nrepresentations are beneﬁcial to downstream NAS.\ndff\nLc\n6 12 24\n64 6.07 5.99 5.95\n128 6.01 5.94 5.95\n256 5.97 5.94 5.94\nTable 5.Effects of Lc and dff on pretraining CATE.\n0 1 2 3 4 5\nnumber of trainable model parameters 1e7\n0\n10000\n20000\n30000\n40000\n50000\n60000frequency\nHistogram for model parameters on NASBench-101\nFigure 6.Histogram of model parameters on NAS-Bench-101.\nChoice of Mask Type.We studied pretraining CATE with\ndirect/indirect dependency mask and summarize its down-\nstream NAS results in Table 6. CATE trained with indirect\ndependency mask outperforms the direct one in both bench-\nmarks, indicating that capturing long-range dependency\nhelps preserve computation information in the encodings.\nMask type NAS-Bench-101 NAS-Bench-301\nDirect 6.03 5.35\nIndirect 5.94 5.30\nTable 6.Direct/Indirect dependency mask selection.\n5. Conclusion\nIn this paper, we presented CATE, a new computation-aware\narchitecture encoding method based on Transformers. Un-\nlike encodings with ﬁxed transformations, we show that\nthe computation information of neural architectures can be\ncontextualized through a pairwise learning scheme trained\nwith MLM. Our experimental results show its effectiveness\nand scalability along with three major encoding-dependent\nNAS subroutines in both small and large search spaces. We\nalso show its superior generalization capability outside the\ntraining search space. We anticipate that the methods pre-\nsented in this work can be extended to encode even larger\nsearch spaces (e.g. TuNAS (Bender et al., 2020)) to study\nthe effectiveness of different downstream NAS algorithms.\nAcknowledgement\nWe would like to thank the anonymous reviewers for their\nhelpful comments. We thank Yu Zheng, Colin White, and\nFrank Hutter for their help with this project. This work was\npartially supported by NSF Awards CNS-1617627, CNS-\n1814551, and PFI:BIC-1632051.\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nReferences\nBaker, B., Gupta, O., Naik, N., and Raskar, R. Designing\nneural network architectures using reinforcement learn-\ning. In ICLR, 2017.\nBaker, B., Gupta, O., Raskar, R., and Naik, N. Accelerating\nneural architecture search using performance prediction.\nIn ICLR Workshop, 2018.\nBender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V ., and\nLe, Q. Understanding and simplifying one-shot architec-\nture search. In ICML, 2018.\nBender, G., Liu, H., Chen, B., Chu, G., Cheng, S., Kinder-\nmans, P.-J., and Le, Q. Can weight sharing outperform\nrandom architecture search? an investigation with tunas.\nIn CVPR, 2020.\nBreiman, L. Random forests. In Machine learning, 2001.\nChen, T. and Guestrin, C. XGBoost: A scalable tree boost-\ning system. In SIGKDD, 2016.\nChen, X. and Hsieh, C.-J. Stabilizing differentiable archi-\ntecture search via perturbation-based regularization. In\nICML, 2020.\nChoi, K., Choe, M., and Lee, H. Pretraining neural architec-\nture search controllers with locality-based self-supervised\nlearning. In arXiv preprint arXiv: 2103.08157, 2021.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR, 2009.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In ACL, 2019.\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y .,\nGao, J., Zhou, M., and Hon, H.-W. Uniﬁed language\nmodel pre-training for natural language understanding\nand generation. In NeurIPS, 2019.\nDong, X. and Yang, Y . Searching for a robust neural archi-\ntecture in four gpu hours. In CVPR, 2019.\nDong, X. and Yang, Y . NAS-Bench-201: Extending the\nscope of reproducible neural architecture search. In ICLR,\n2020.\nDrucker, H., Burges, C. J. C., Kaufman, L., Smola, A., and\nVapnik, V . Support vector regression machines. In Ad-\nvances in Neural Information Processing Systems, 1997.\nElsken, T., Metzen, J. H., and Hutter, F. Neural architecture\nsearch: A survey. In JMLR, 2019.\nFalkner, S., Klein, A., and Hutter, F. Bohb: Robust and\nefﬁcient hyperparameter optimization at scale. In ICML,\n2018.\nFloyd, R. W. Algorithm 97: Shortest path. In Communica-\ntions of the ACM, 1962.\nHesslow, D. and Poli, I. Contrastive embeddings for neural\narchitectures. In arXiv preprint arXiv: 2102.04208, 2021.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nIn Neural computation, 1997.\nKandasamy, K., Neiswanger, W., Schneider, J., Poczos, B.,\nand Xing, E. Neural architecture search with bayesian\noptimisation and optimal transport. In NeurIPS, 2018.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma,\nW., Ye, Q., and Liu, T.-Y . Lightgbm: A highly efﬁcient\ngradient boosting decision tree. In NeurIPS, 2017.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. In ICLR, 2014.\nKitano, H. Designing neural networks using genetic algo-\nrithms with graph generation system. InComplex systems,\n1990.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L.\nBart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. In ACL, 2020.\nLi, L. and Talwalkar, A. Random search and reproducibility\nfor neural architecture search. In UAI, 2019.\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and\nTalwalkar, A. Hyperband: A novel bandit-based approach\nto hyperparameter optimization. In JMLR, 2018.\nLindauer, M., Eggensperger, K., Feurer, M., Biedenkapp,\nA., Marben, J., Müller, P., and Hutter, F. Boah: A tool\nsuite for multi-ﬁdelity bayesian optimization and analysis\nof hyperparameters. In arXiv preprint arXiv: 1908.06756,\n2019.\nLiu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li,\nL.-J., Fei-Fei, L., Yuille, A., Huang, J., and Murphy, K.\nProgressive neural architecture search. In ECCV, 2018a.\nLiu, H., Simonyan, K., Vinyals, O., Fernando, C., and\nKavukcuoglu, K. Hierarchical representations for ef-\nﬁcient architecture search. In ICLR, 2018b.\nLiu, H., Simonyan, K., and Yang, Y . Darts: Differentiable\narchitecture search. In ICLR, 2019a.\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\nIn arXiv:1907.11692, 2019b.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. In ICLR, 2019.\nLu, Z., Deb, K., Goodman, E., Banzhaf, W., and Boddeti,\nV . N. Nsganetv2: Evolutionary multi-objective surrogate-\nassisted neural architecture search. In ECCV, 2020.\nLukasik, J., Friede, D., Zela, A., Hutter, F., and Keuper, M.\nSmooth variational graph embeddings for efﬁcient neural\narchitecture search. In IJCNN, 2021.\nLuo, R., Tian, F., Qin, T., Chen, E., and Liu, T.-Y . Neural\narchitecture optimization. In NeurIPS, 2018a.\nLuo, R., Tian, F., Qin, T., Chen, E.-H., and Liu, T.-Y . Neural\narchitecture optimization. In NeurIPS, 2018b.\nLuo, R., Tan, X., Wang, R., Qin, T., Chen, E., and Liu, T.-Y .\nSemi-supervised neural architecture search. In NeurIPS,\n2020.\nMiller, G. F., Todd, P. M., and Hegde, S. U. Designing neural\nnetworks using genetic algorithms. In ICGA, 1989.\nNegrinho, R. and Gordon, G. Deeparchitect: Automat-\nically designing and training deep architectures. In\narXiv:1704.08792, 2017.\nNing, X., Li, W., Zhou, Z., Zhao, T., Zheng, Y ., Liang,\nS., Yang, H., and Wang, Y . A surgery of the neural\narchitecture evaluators. arXiv preprint arXiv:2008.03064,\n2020a.\nNing, X., Zheng, Y ., Zhao, T., Wang, Y ., and Yang, H. A\ngeneric graph-based neural architecture encoding scheme\nfor predictor-based nas. In ECCV, 2020b.\nOttelander, T. D., Dushatskiy, A., Virgolin, M., and Bosman,\nP. A. Local search is a remarkably strong baseline for\nneural architecture search. In arXiv:2004.08996, 2020.\nPeng, H., Du, H., Yu, H., Li, Q., Liao, J., and Fu, J. Cream of\nthe crop: Distilling prioritized paths for one-shot neural\narchitecture search. In NeurIPS, 2020.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. In NAACL, 2018.\nPham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.\nEfﬁcient neural architecture search via parameter sharing.\nIn ICML, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. In OpenAI Blog, 2019.\nRadosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and\nDollár, P. Designing network design spaces. In CVPR,\n2020.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. In JMLR, 2020.\nRasmussen, C. E. and Williams, C. K. I. Gaussian processes\nfor machine learning. In The MIT Press, 2006.\nReal, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y . L.,\nTan, J., Le, Q. V ., and Kurakin, A. Large-scale evolution\nof image classiﬁers. In ICML, 2017.\nReal, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regular-\nized evolution for image classiﬁer architecture search. In\nAAAI, 2019.\nRu, B., Esperanca, P., and Carlucci, F. Neural architecture\ngenerator optimization. In NeurIPS, 2020.\nRu, B., Wan, X., Dong, X., and Osborne, M. Interpretable\nneural architecture search via bayesian optimisation with\nweisfeiler-lehman kernels. In ICLR, 2021.\nShi, H., Pi, R., Xu, H., Li, Z., Kwok, J. T., and Zhang,\nT. Bridging the gap between sample-based and one-shot\nneural architecture search with bonas. In NeurIPS, 2020.\nSiems, J., Zimmer, L., Zela, A., Lukasik, J., Keuper, M.,\nand Hutter, F. Nas-bench-301 and the case for sur-\nrogate benchmarks for neural architecture search. In\narXiv:2008.09777, 2020.\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N.,\nSundaram, N., Patwary, M., Prabhat, M., and Adams,\nR. Scalable bayesian optimization using deep neural\nnetworks. In ICML, 2015.\nSpringenberg, J. T., Klein, A., Falkner, S., and Hutter, F.\nBayesian optimization with robust bayesian neural net-\nworks. In NeurIPS, 2016.\nStanley, K. O. and Miikkulainen, R. A. Evolving neural net-\nworks through augmenting topologies. In Evolutionary\nComputation, 2002.\nTan, M., Chen, B., Pang, R., Vasudevan, V ., Sandler, M.,\nHoward, A., and Le, Q. V . Mnasnet: Platform-aware\nneural architecture search for mobile. In CVPR, 2019.\nTang, Y ., Wang, Y ., Xu, Y ., Chen, H., Shi, B., Xu, C., Xu,\nC., Tian, Q., and Xu, C. A semi-supervised assessor of\nneural architectures. In CVPR, June 2020.\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\ntion is all you need. In NeurIPS, 2017.\nWang, L., Zhao, Y ., Jinnai, Y ., Tian, Y ., and Fonseca, R.\nAlphax: exploring neural architectures with deep neural\nnetworks and monte carlo tree search. In AAAI, 2020.\nWei, C., Niu, C., Tang, Y ., and min Liang, J. Npenas: Neural\npredictor guided evolution for neural architecture search.\nIn arXiv:2003.12857, 2020a.\nWei, C., Tang, Y ., Niu, C., Hu, H., Wang, Y ., and Liang,\nJ. Self-supervised representation learning for evolution-\nary neural architecture search. In arXiv preprint arXiv:\n2011.00186, 2020b.\nWen, W., Liu, H., Li, H., Chen, Y ., Bender, G., and Kin-\ndermans, P.-J. Neural predictor for neural architecture\nsearch. In ECCV, 2020.\nWhite, C., Neiswanger, W., Nolen, S., and Savani, Y . A\nstudy on encodings for neural architecture search. In\nNeurIPS, 2020a.\nWhite, C., Nolen, S., and Savani, Y . Local search is state\nof the art for neural architecture search benchmarks. In\narXiv:2005.02960, 2020b.\nWhite, C., Neiswanger, W., and Savani, Y . Bananas:\nBayesian optimization with neural architectures for neu-\nral architecture search. In AAAI, 2021.\nXie, L., Chen, X., et al. Weight-sharing neural architec-\nture search: A battle to shrink the optimization gap. In\narXiv:2008.01475, 2020.\nXie, S., Kirillov, A., Girshick, R., and He, K. Exploring\nrandomly wired neural networks for image recognition.\nIn ICCV, 2019a.\nXie, S., Zheng, H., Liu, C., and Lin, L. Snas: Stochastic\nneural architecture search. In ICLR, 2019b.\nXu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful\nare graph neural networks? In ICLR, 2019.\nYan, S., Fang, B., Zhang, F., Zheng, Y ., Zeng, X., Zhang,\nM., and Xu, H. Hm-nas: Efﬁcient neural architecture\nsearch via hierarchical masking. In ICCVW, 2019.\nYan, S., Zheng, Y ., Ao, W., Zeng, X., and Zhang, M. Does\nunsupervised architecture representation learning help\nneural architecture search? In NeurIPS, 2020.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR. R., and Le, Q. V . Xlnet: Generalized autoregres-\nsive pretraining for language understanding. In NeurIPS,\n2019.\nYing, C., Klein, A., Christiansen, E., Real, E., Murphy, K.,\nand Hutter, F. NAS-Bench-101: Towards reproducible\nneural architecture search. In ICML, 2019.\nYou, J., Leskovec, J., He, K., and Xie, S. Graph structure of\nneural networks. In ICML, 2020a.\nYou, S., Huang, T., Yang, M., Wang, F., Qian, C., and Zhang,\nC. Greedynas: Towards fast one-shot nas with greedy\nsupernet. In CVPR, 2020b.\nZela, A., Elsken, T., Saikia, T., Marrakchi, Y ., Brox, T., and\nHutter, F. Understanding and robustifying differentiable\narchitecture search. In ICLR, 2020.\nZhang, M., Jiang, S., Cui, Z., Garnett, R., and Chen, Y .\nD-vae: A variational autoencoder for directed acyclic\ngraphs. In NeurIPS, 2019.\nZhang, Y ., Zhang, J., and Zhong, Z. Autobss: An efﬁcient\nalgorithm for block stacking style search. In NeurIPS,\n2020.\nZhou, H., Yang, M., Wang, J., and Pan, W. BayesNAS:\nA Bayesian approach for neural architecture search. In\nICML, 2019.\nZoph, B. and Le, Q. V . Neural architecture search with\nreinforcement learning. In ICLR, 2017.\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\ntransferable architectures for scalable image recognition.\nIn CVPR, 2018.\nCATE: Computation-aware Neural Architecture Encoding with Transformers\nA. Uni/Bidirectional Encoding\nAs mentioned in Section 3.2, we also considered encoding\nthe architecture in a bidirectional manner where both the\noutput node hidden vector from the original DAG and the\ninput node hidden vector from the reversed one are extracted\nand then concatenated together. Note that dc in the cross-\nattention Transformer encoder will be doubled due to the\nconcatenation. We compare the results of unidirectional and\nbidirectional encodings in Table 7. As shown, bidirectional\nencoding does not necessarily improve the results. There-\nfore, we keep unidirectional encoding in other experiments\ndue to its simplicity and better performance.\nEncoding NAS-Bench-101 NAS-Bench-301\nUnidirectional 5.88 5.28\nBidirectional 5.89 5.30\nTable 7.Unidirectional encoding vs. bidirectional encoding. We\nreport the ﬁnal NAS test error [%] given 150 queried architectures\non NAS-Bench-101 and 100 queried architectures on NAS-Bench-\n301. The result is averaged over 200 independent runs.\nB. Architecture Pair Sampling\nHyperparameters\nAs mentioned in Section 4.4, we randomly sample\n1,000,000 architectures in NAS-Bench-301 for pretraining.\nWe use the same proxy model conﬁguration (i.e. 100 train-\ning epochs, 32 initial channels, 8 cell layers) as used in\nNAS-Bench-301 to compute the model FLOPs. We plot the\nhistogram of model FLOPs of the sampled architectures in\nFigure 7. Given that, we experiment different δand Kand\nsummarize the downstream NAS results in Table 8. Similar\nto our reported results on NAS-Bench-101, we ﬁnd that\nstrong locality leads to better results.\n0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75\nnumber of model FLOPs 1e8\n0\n20000\n40000\n60000\n80000\n100000\n120000\n140000frequency\nHistogram for model FLOPs on NASBench-301\nFigure 7.Histogram of model FLOPs on the sampled 1,000,000\narchitectures of NAS-Bench-301.\nδ\nK\n1 2 4 8\n5 × 10\n6\n5.28 5.29 5.30 5.30\n1 × 10\n7\n5.30 5.28 5.29 5.31\n2 × 10\n7\n5.30 5.30 5.31 5.32\nTable 8.Effects of δand Kon architecture pair sampling on NAS-\nBench-301. We report the ﬁnal NAS test error [%] given 100\nqueried architectures on NAS-Bench-301. The result is averaged\nover 200 independent runs.\nCorruption Rate NAS-Bench-101 NAS-Bench-301\n15% 5.89 5.28\n20% 5.88 5.28\n30% 5.93 5.29\nTable 9.NAS results under different corruption rates.\nC. Corruption Rate\nBy default, we randomly select 20% operations from each\narchitecture within the pair for masking in the pairwise pre-\ntraining. We also experimented corruption rates of 15%\nand 30%. As shown in Table 9, overall, we ﬁnd that the\ncorruption rate has a limited effect on the NAS performance.\nNote that the number of nodes in our search space is much\nsmaller compared to the number of tokens in the sequence\nmodeling tasks. Given that, using larger corruption rate may\nslow down the training convergence and result in degraded\nperformance. Based on these results, we use 20% corruption\nrate for other experiments.\nD. NAS-Bench-301 Benchmark\nNAS-Bench-301 (Siems et al., 2020) is the ﬁrst surrogate\nNAS benchmark to cover the large-scale DARTS search\nspace (Liu et al., 2019a). The DARTS search space consists\nof two cells: a convolutional cell and a reduction cell, each\nwith six nodes. For each cell, the ﬁrst two nodes are the\noutputs from the previous two cells. The next four nodes\ncontain two edges as input, creating a DAG. In total, there\nare roughly 1018 DAGs without considering graph isomor-\nphism, which is a much larger search space compared to\nNAS-Bench-101 (Ying et al., 2019) and NAS-Bench-201\n(Dong & Yang, 2020).\nNAS-Bench-301 is fully trained on around 60k architec-\ntures collected by unbiased architecture sampling using\nrandom search as well as biased and dense architecture\nsampling in high-performance regions using different NAS\nmethods and training hyperparameters (including training\ntime, number of parameters, and number of multiply-adds).\nIt trains various regression models such as Random Forest\n(RF) (Breiman, 2001), Support Vector Regression (SVR)\n(Drucker et al., 1997), Graph Isomorphism Network (GIN)\nCATE: Computation-aware Neural Architecture Encoding with Transformers\n1     0     0     0     0     0     0     0     0     0     0   \n0     1     0     0     0     0     0     0     0     0     0\n0     0     0     0     1     0     0     0     0     0     0\n0     0     0     0     0     0     0     1     0     0     0\n0     0     0     0     0     0     0     0     0     1     0\n0     0     0     0     0     0     0     1     0     0     0\n0     0     0     0     0     1     0     0     0     0     0\n0     0     0     0     0     0     0     0     0     1     0\n0     0     1     0     0     0     0     0     0     0     0\n0     0     0     0     1     0     0     0     0     0     0\n0     0     0     0     0     0     0     0     0     1     0\n0     0     0     0     1     0     0     0     0     0     0\n0     0     0     0     0     0     0     1     0     0     0\n0     0     0     0     0     0     0     0     0     1     0\n0     0     0     0     0     0     0     0     0     0     1\n0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n0   0   1   1   0   0   1   0   0   1   0   0   1   0   0\n0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n0   0   0   0   0   1   0   0   0   0   0   1   0   0   1\n0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n0   0   0   0   0   0   0   0   1   0   0   0   0   0   1\n0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nFigure 8.A cell transformation example in DARTS search space. The top panel shows the cell. The bottom-left and bottom-right panels\nshow its corresponding adjacency matrix and operation matrix respectively.\n(Xu et al., 2019) and Tree-based gradient boosting model\n(e.g. XGBoost (XGB) (Chen & Guestrin, 2016), LGBoost\n(LGB) (Ke et al., 2017)) to predict the accuracies of un-\nseen architectures. The three best-performing models (GIN,\nXGB, LGB) are used to predict the search trajectories in the\nbenchmark API.\nD.1. Cell Transformation\nTo transform the DARTS search space into one with the\nsame input format as NAS-Bench-101, we additionally add\na summation node to make nodes to represent operations\nand edges to represent data ﬂow. For example, if there is an\nedge from node A to node B with operation O, we create an\nadditional node P, remove the edge⟨A,B⟩, and add 2 edges\n⟨A,P⟩and ⟨P,B⟩. The operation on node P is set to be O.\nGiven that, a 15 ×15 upper-triangular binary matrix is used\nto encode edges and a 15 ×11 operation matrix is used to\nencode operations with the order of {ck− 2, ck− 1, 3 ×3 max-\npool, 3 ×3 average-pool, skip connect, 3 ×3 separable\nconv, 5×5 separable conv, 3×3 dilated conv, 5×5 dilated\nconv, sum, ck}. Following NAS-Bench-301 (Siems et al.,\n2020), we do not include zero operator. Following (Liu\net al., 2018a), we use the same cell for both normal and\nreduction cells. An example of cell transformation is shown\nin Figure 8.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7224184274673462
    },
    {
      "name": "Computation",
      "score": 0.7177209258079529
    },
    {
      "name": "Architecture",
      "score": 0.6869269609451294
    },
    {
      "name": "Computer architecture",
      "score": 0.607844352722168
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5986357927322388
    },
    {
      "name": "Computer science",
      "score": 0.568665623664856
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36426106095314026
    },
    {
      "name": "Engineering",
      "score": 0.186492919921875
    },
    {
      "name": "Electrical engineering",
      "score": 0.1474079191684723
    },
    {
      "name": "Programming language",
      "score": 0.14213767647743225
    },
    {
      "name": "Voltage",
      "score": 0.05626863241195679
    },
    {
      "name": "Geography",
      "score": 0.0513172447681427
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87216513",
      "name": "Michigan State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I106165777",
      "name": "University of Central Florida",
      "country": "US"
    }
  ]
}