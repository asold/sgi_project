{
    "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
    "url": "https://openalex.org/W4385570984",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3034229139",
            "name": "Or Honovich",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2932737571",
            "name": "Thomas Scialom",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2250897584",
            "name": "Omer Levy",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2738082409",
            "name": "Timo Schick",
            "affiliations": [
                "Tel Aviv University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3081168214",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4309212061",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4306808908",
        "https://openalex.org/W2977235550",
        "https://openalex.org/W4385574293",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W3128710690",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W2998184481",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W3175603587",
        "https://openalex.org/W3103291112",
        "https://openalex.org/W3035331128",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4320561075",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W3212893438",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3196731672",
        "https://openalex.org/W4298343034",
        "https://openalex.org/W4385573325",
        "https://openalex.org/W4285178342",
        "https://openalex.org/W4206636317",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W3207166518",
        "https://openalex.org/W3171654528"
    ],
    "abstract": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14409–14428\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nUnnatural Instructions:\nTuning Language Models with (Almost) No Human Labor\nOr Honovichτ Thomas Scialomµ Omer Levyτµ Timo Schickµ\nτ Tel Aviv University\nµ Meta AI\nAbstract\nInstruction tuning enables pretrained language\nmodels to perform new tasks from inference-\ntime natural language descriptions. These ap-\nproaches rely on vast amounts of human super-\nvision in the form of crowdsourced datasets or\nuser interactions. In this work, we introduce\nUnnatural Instructions: a large dataset of cre-\native and diverse instructions, collected with\nvirtually no human labor. We collect 64,000\nexamples by prompting a language model with\nthree seed examples of instructions and elic-\niting a fourth. This set is then expanded by\nprompting the model to rephrase each instruc-\ntion, creating a total of approximately 240,000\nexamples of instructions, inputs, and outputs.\nExperiments show that despite containing a\nfair amount of noise, training on Unnatural In-\nstructions rivals the effectiveness of training\non open-source manually-curated datasets, sur-\npassing the performance of models such as\nT0++ and Tk-Instruct across various bench-\nmarks. These results demonstrate the poten-\ntial of model-generated data as a cost-effective\nalternative to crowdsourcing for dataset expan-\nsion and diversification.\n1 Introduction\nInstruction tuning enables pretrained language\nmodels to generalize to unseen tasks in a zero-shot\nsetting (Sanh et al., 2021; Wei et al., 2021). One\nway to collect examples of instructions and their\nexecution is to reformulate existing NLP datasets\nin an explicit instruction-input-output format via\nprompt engineering (Mishra et al., 2022; Wang\net al., 2022). However, the resulting data is limited\nto existing academic benchmarks, even though the\ninstruction paradigm can describe any text-based\ntask (Efrat and Levy, 2020). Alternatively, Ouyang\net al. (2022) collect user-generated prompts and\nmanually annotate their expected outputs, reflect-\ning a different (and arguably more desirable) dis-\ntribution of the instruction space, but requiring a\nlive application with existing users and major in-\nvestments in human annotation. Can we create a\nlarge dataset of instructions that is diverse in tasks,\ncontent, and phrasing, without human labor?\nWe introduce Unnatural Instructions, a dataset\nof natural language instructions and their corre-\nsponding inputs and outputs. Inspired by recent\nwork on utilizing language models for data genera-\ntion (Schick and Schütze, 2021b; Lee et al., 2021;\nLiu et al., 2022a), we collect data in a fully auto-\nmatic manner by prompting a pretrained language\nmodel with three examples from the Super-Natural\nInstructions1 dataset (Mishra et al., 2022; Wang\net al., 2022) and asking the model to generate a\nfourth (Figure 1). We repeat this process with 5\ndifferent seeds – i.e. the entire process requires\nonly 15 instruction examples – to automatically pro-\nduce 64,000 diverse triplets of instructions, inputs,\nand outputs.2 We further diversify the dataset’s\nformat by generating additional natural language\nparaphrases of each instruction, while preserving\nthe contents of any input arguments and outputs,\nexpanding the dataset to approximately 240,000\nexamples. Although the dataset contains noise,\nour analysis reveals that more than 50% of gen-\nerated examples are indeed correct, and that even\nincorrect examples typically contain valuable in-\nformation for instruction tuning. At the same time,\nwe find that Unnatural Instructions contains highly\ncreative tasks – some of which are very different\nfrom “classic” NLP tasks – and has a more diverse\nset of instructions than Super-Natural Instructions.\nExperiments show that fine-tuning an 11B-\nparameter T5 model (Raffel et al., 2020) on Un-\nnatural Instructions can outperform both T0++\n(Sanh et al., 2021) and Tk-Instruct (Wang et al.,\n2022) across several benchmarks, including Super-\nNatural Instructions (Wang et al., 2022), BIG-\n1Also known as Natural Instructions v2.\n2In practice, we collected 68,478 examples, but only used\nsubsets of 64,000 examples for training.\n14409\nbench Hard (Suzgun et al., 2022), and LMentry\n(Efrat et al., 2022). When controlling for all vari-\nables besides the data, we find that a model trained\non Unnatural Instructions performs competitively\nwith a baseline model trained on Super-Natural In-\nstructions. In particular, we observe an 18-point\ngain on BIG-bench Hard (original task formula-\ntion) and a 16-point gain on LMentry, suggesting\nthat Unnatural Instructions is particularly useful for\ngeneralizing to instructions that deviate from the\ndistribution of classic NLP tasks. These improve-\nments become even more pronounced when the\ncost of generating examples is amortized; in this\ncase, training on Unnatural Instructions substan-\ntially outperforms our baseline on all benchmarks.\nWe observe a log-linear relationship between the\nnumber of generated examples and downstream\ntask performance, suggesting that performance of\nmodels trained on Unnatural Instructions can fur-\nther be improved simply by increasing its size.\nBeyond the immediate implications on instruc-\ntion tuning, this work demonstrates the viability of\nautomatic dataset expansion using language mod-\nels as an alternative to crowdsourcing. Unnatural\nInstructions highlights the ability of language mod-\nels to produce creative and diverse data, a trait that\nis difficult to obtain with crowd workers, who lack\nthe intrinsic motivation to create novel examples\nand typically collapse into predictable heuristics to\nform annotation artifacts (Gururangan et al., 2018).\nAt the same time, language models are faster and\ncheaper than human labor, opening up new possi-\nbilities for scaling up data annotation.\n2 Data Collection\nWe introduce Unnatural Instructions, a dataset\nof 240,670 diverse natural language instructions.\nEach example contains a natural language instruc-\ntion as input and its expected execution as output.\nTable 2 displays examples from the dataset.\nUnnatural Instructions is collected in a com-\npletely automatic process, requiring a seed of only\n15 manually-constructed examples, which can be\nproduced in about one hour of human labor. We\nfirst collect a core set of 68,478 examples (§2.1)\nby prompting a pretrained language model M with\na seed of 3 manually-annotated examples to pro-\nduce a new (fourth) example. This phase uses a\nstructured instruction format and filtering heuris-\ntics to ensure data quality. We then expand the core\ndataset by rephrasing the structured instructions in\nExample 1\nInstruction: You are given a science question (easy-level) and four \nanswer options (associated with “A”, “B”, “C”, “D”). Your task is to ﬁnd the \ncorrect answer based on scientiﬁc facts, knowledge, and reasoning. Do \nnot generate anything else apart from one of the following characters: \n‘A’, ‘B, ‘C’, ‘D’. There is only one correct answer for each question.\nInput: Which part of a bicycle BEST moves in a circle? (A) Seat (B) \nFrame (C) Foot pedal (D) Kickstand\nConstraints: The output should be one of the following characters: ‘A’, \n‘B, ‘C’, ‘D’.\nExample 2\nInstruction: You are given a negative review and your task is to convert \nit to a positive review by one or more making minimal changes. Avoid \nchanging the context of the review.\nInput: we stood there in shock, because we never expected this.  \nConstraints: None.\nExample 3\nInstruction: In this task, you are given two sentences taken from a \nconversation, and your job is to classify whether these given sentences \nare sequential or not. We will mark the given sentence pair as ’True’ if \nit’s sequential, otherwise ’False’. The two sentences are spoken by two \ndifferent people.\nInput: Noah: When and where are we meeting? :), Madison: I thought \nyou were busy...? \nConstraints: None.\nExample 4\nInstruction: In this task, you will be given a proﬁle of someone and your \njob is to generate a set of interesting questions that can lead to a \nconversation with the person.\nInput: Yvonne has been playing the violin since she was four years old. \nShe loves all kinds of music, but her favorite composer is Bach.\nConstraints: None.\nFigure 1: Our data generation prompt. Blue: The meta-\nprompt, which contains the number of the in-context\nexample, as well as the constant fields of each exam-\nple: instruction, input, and constraints. Black: The\nin-context examples. We show here one of our 5 in-\ncontext seeds. Pink: One of the model’s generations for\nthe given prompt.\nfree-form natural language (§2.2). This expansion\nis performed automatically by prompting a lan-\nguage model with manually-constructed examples,\nscaling up the dataset more than 3-fold. Through-\nout this section, we use OpenAI’s text-davinci-002\nas M. See §6 for experiments with other models.\n2.1 Core Dataset Generation\nThe core dataset consists of examples in a struc-\ntured format, making it easier for the generating\nmodel M to predict and for us to filter automati-\ncally. We use stochastic decoding to generate ex-\nample inputs (to promote creativity), followed by\ndeterministic decoding to generate their outputs\n(for accuracy). Figure 2 illustrates the process.\nFormat Each example in the core dataset con-\ntains four fields: (1) An instruction describing the\ntask. The instruction can be a generic template (e.g.\n“Write whether the following review is positive or\n14410\nx1\nx2\nx3\nInstruction: …\nInput: …\nConstraints: …\nM M\nOutput: …\nﬁnetune Instruction-\nTuned Modelnucleus\nsampling\ngreedy\ndecoding\nFigure 2: The core Unnatural Instructions generation pipeline. We use a seed of three in-context demonstrations\nx1, x2, x3 to create a large dataset of NLP tasks with instructions, inputs and outputs. As a first step, we sample\ninstructions, inputs, and constraints from a language model M. In the next step, we use M to deterministically\ngenerate the corresponding outputs. Finally, the data can be used for instruction tuning.\nnegative”) that can be instantiated by a particular in-\nput argument (e.g. the review itself). (2) The input\nargument that instantiates the instruction, creating a\nspecific example of the task. (3) Output space con-\nstraints, which detail the restrictions on the task’s\noutput space. Constraints are mainly relevant for\nclassification tasks; for tasks with no specific out-\nput space constraints, this field is “None.” (4) A\ntextual output reflecting a correct execution of the\ninstruction given the input arguments and output\nspace constraints. The first three fields (instruc-\ntion, input argument, constraints) are the model’s\ninput, and the output field acts as the reference for\ntraining and/or evaluation. The constraints field is\nmeant to guide M during output generation and is\ndiscarded after generating the outputs (see next).\nIn Appendix D we provide data-driven evidence\nfor selecting this particular format.\nInput Generation We first generate examples of\ninstruction-input-constraints by prompting a model\nwith three task demonstrations x1, x2, x3, each pre-\nsented in the structured format (without outputs).\nThese demonstrations are wrapped by a simple\nmeta-prompt that incentivizes the model to create\na fourth example x4, as illustrated in Figure 1.\nWe use 5 seeds of 3 demonstrations each to gen-\nerate the core dataset; i.e., the whole process re-\nquires only 15 examples. Demonstrations are taken\nfrom the Super-Natural Instructions (Wang et al.,\n2022) train set. To obtain various examples us-\ning the same prompt, decoding is done by nucleus\nsampling with p = 0.99 (Holtzman et al., 2020).\nFiltering We apply three automatic filters to the\ngenerated examples to remove: (1) model gener-\nations that do not include the three input fields\n(instruction, input argument, and constraints), (2)\ninstructions and inputs that are identical to those\ndemonstrated in the prompt, (3) duplicate exam-\nples, i.e. two different examples that have the same\ninstruction and input argument.\nOutput Generation Given a generated example\nx, we generate the corresponding output y by con-\nditioning a pretrained language model with the in-\nstruction, input argument, and constraints (if not\nnone), followed by an “Output:” prompt. Here\nwe apply greedy decoding to prioritize correctness\nover creativity. We ignore examples for which the\ngenerated output is an empty string.\n2.2 Template Expansion\nExamples in our core dataset have a strict\ninstruction-input-output format. To increase the\nformat diversity and obtain tasks phrased in free-\nform natural language (Schick and Schütze, 2021a;\nSanh et al., 2021), we collect alternative formu-\nlations that preserve the content of the original\ninstructions. Specifically, we prompt a language\nmodel to reformulate the core dataset tasks and col-\nlect two alternative formulations for each generated\ntask.3 Alternative formulations are often shorter\nand less formal than the original instructions. The\nrephrasing prompt contains two examples of in-\nstructions and their alternative formulation. We do\nnot include inputs, constraints, and outputs in the\nrephrasing prompt; instead, we utilize the already-\ngenerated inputs and outputs to complement the\nrephrased instruction. Unlike the examples in the\ncore dataset, the input is often embedded into the\ntask description. We achieve that by adding an\n“{INPUT}” placeholder, which marks the position\nfor input insertion (Figure 3).\nIn some cases, the model generates two identical\nreformulations, or it copies the original instruction.\nSome alternative formulations may also have an\ninvalid format - e.g., not containing the “{INPUT}”\nplaceholder. When such failures occur we continue\nto sample reformulations, stopping after five unsuc-\ncessful attempts. Consequently, some instructions\nhave only one alternative formulation, while oth-\ners have none. Overall, more than 97.5% of the\n3The seed reformulations in each prompt are inspired and\npartially taken from PromptSource (Bach et al., 2022).\n14411\nExample 1\nInstruction: In this task, you are given an article. Your task \nis to summarize the article in a sentence.    \nInput: {INPUT}\nAlternative formulation: My college roommate asked me \nwhat this article means: “{INPUT}”. So I recapped it in \nlayman’s terms:\nExample 2\nInstruction: This task is about writing a correct answer for \nthe reading comprehension task. Based on the information \nprovided in a given passage…\nInput: {INPUT}   \nAlternative formulation: {INPUT} Based on the given \ncontext, the answer to the question is\nExample 3\nInstruction: In this task, you are asked to determine \nwhether the given recipe is for a savory or sweet dish. If it \nis for a savory dish, output “SAVORY”. If the recipe is for a \nsweet dish, output “SWEET”.\nInput: {INPUT}   \nAlternative formulation: Given the following recipe, \n{INPUT}, is the dish savory or sweet? Your output should \nbe “SAVORY” or “SWEET”\nFigure 3: Our template expansion prompt. Black: Few-\nshot demonstrations of instructions and alternative for-\nmulations. Blue: The instruction we wish to paraphrase.\nPink: Model-generated task reformulation.\ninstructions have two distinct, valid reformulations.\nIn fact, some instructions end up with more than\ntwo paraphrases because we generate two para-\nphrases per example (i.e. instruction-input-output\npair) and the core dataset contains examples that\nshare the exact same instruction but not the same\ninput argument. Therefore, by cross-referencing\neach instruction’s alternative phrasings with all of\nits input arguments, we can extend the data even\nfurther and arrive at a total of 240,670 examples\nwithout additional cost.\n3 Data Analysis\nWe first demonstrate the creativity of Unnatu-\nral Instructions, and then manually analyze 200\nrandomly-sampled examples from our core dataset,\nfocusing on correctness and diversity. We also\ncompare our data’s distribution to Super-Natural\nInstructions, and find our inputs to be more diverse.\nCreativity A major challenge when creating an\ninstruction dataset is task creativity. Crowd work-\ners typically collapse into predictable heuristics to\nform annotation artifacts (Gururangan et al., 2018).\nWhile the high performance of models trained on\nUnnatural Instructions (see §5) suggests that it is\nindeed diverse and creative, we also present in Ta-\nble 1 some cherry-picked examples, providing a\nglimpse at their creativity.\nCorrectness When evaluating correctness, we\ntest whether (1) the generated instructions are logi-\ncal and executable, (2) the input arguments corre-\nspond to the task described in the instruction, and\n(3) the outputs are correct, given the instruction and\ninput. Although our data filtering process is mini-\nmal, 113 of the 200 analyzed examples (56.5%) are\ncorrect. Of the 87 incorrect examples, 9 (4.5%) had\nincomprehensible instructions, 35 (17.5%) had an\ninput that did not match the task description, and 43\n(21.5%) had incorrect outputs. Table 2 shows some\ncorrect and incorrect examples from our analysis.\nWhile the amount of noise in the data may raise\nconcerns regarding its usability, many of the exam-\nples that were marked as incorrect can still be con-\nsidered informative. For example, one erroneous\nexample had the instruction “In this task, you will\nbe provided with a list of countries and their corre-\nsponding capital cities. You are also given a list of\nclues...For each clue, determine which country it is\nreferring to and write down that country’s name... ”\nThe input argument was “Clue 1: This capital city\nis on two different continents. ” This example is\nincorrect since the input does not conform with the\nformat described by the instruction – a list of coun-\ntries and their capitals is not provided, only a clue.\nHowever, the output is Istanbul, Turkey, which in-\ndeed lies in both Europe and Asia and therefore\ncorresponds with the input clue. In §5 we show\nthat, despite being noisy, Unnatural Instructions\nprovides a highly informative training signal.\nDiversity We manually cluster the instructions\ninto tasks and measure the number of unique types.\nOut of the 200 examples tested, we identify 117\ndistinct tasks. While many tasks are classical NLP\ntasks, such as sentiment analysis, question answer-\ning, and summarization, others are not quite canon-\nical, and some are very specific, such as detecting\na recipe given a list of ingredients. Table 3 shows\nthe most commonly generated tasks from the set\nof 200 analyzed examples. Other tasks appeared 3\ntimes or less, with 85 tasks appearing only once.\nWe also analyze how similar each pair of exam-\nples is, as a general proxy for diversity. Specifi-\ncally, we sample 10,000 pairs of examples from\nUnnatural Instructions, and compute the similar-\nity of their inputs using BERTScore (Zhang et al.,\n14412\nInstruction Category\nYou need to answer the question ’Is this a good experiment design?’, given an experiment\nscenario. A good experiment should have a single independent variable and multiple dependent\nvariables. In addition, all other variables should be controlled so that they do not affect the results\nof the experiment.\nExperiment Verification\nYou are given a recipe for baking muffins that contains some errors. Your task is to correct the\nerrors in the instructions by replacing each underlined word with the correct one from the options\nprovided.\nRecipe Correction\nYou will be given a piece of text that contains characters, places, and objects. For each character\nin the text, you need to determine whether they are static or dynamic. A static character is\nsomeone who does not change over time, while a dynamic character is someone who undergoes\nsignificant internal changes.\nCharacter Categorization\nIn this task, you are asked to generate a limerick given two rhyming words. A limerick is a\nfive-line poem with the following rhyme scheme: AABBA. The first, second and fifth lines must\nbe of three beats, while the third and fourth lines must be of two beats each. Additionally, all\npoems should have the same meter (e.g., iambic pentameter)\nPoem Generation\nI’m not sure what this idiom means: “{INPUT}”. Could you give me an example? Idiom Explanation\n{INPUT} By analyzing the writing styles of the two passages, do you think they were written by\nthe same author?\nAuthor Classification\nI need to invent a new word by combining parts of the following words: {INPUT}. In what order\nshould I put the parts together?\nWord Invention\nWhat is the punchline to the following joke? {INPUT} Humor Understanding\nTable 1: Examples of eight interesting generated instructions and their corresponding category. The first four\nexamples are taken from the core dataset, while the last four were generated during the template expansion phase.\n0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSimilarity (BERTScore)\n0\n200\n400\n600\n800Count\nSuper-Natural\nUnnatural\nFigure 4: Similarity scores distribution for Super-\nNatural Instructions and for Unnatural Instructions, ob-\ntained by sampling 10,000 pairs of examples from each\ndataset and computing their similarity.\n2020). We repeat this process for Super-Natural\nInstructions, producing two empirical distributions.\nFigure 4 shows that the inputs of Unnatural Instruc-\ntions tend to be less similar to each other than the\ninputs of Super-Natural Instructions. This result\ncomes as a surprise considering the fact that the en-\ntire Unnatural Instructions dataset was constructed\nby conditioning only on 15 original examples.\n4 Experimental Setup\nWe describe model fine-tuning on Unnatural In-\nstructions and our evaluation protocol.\n4.1 Fine-Tuning on Unnatural Instructions\nWe fine-tune T5-LM, the language-model-adapted\nvariant of T5-11B (Raffel et al., 2020; Lester et al.,\n2021). We follow standard practice for fine-tuning,\nusing a batch size of 16 examples over 3 epochs.\nFor training on our core dataset, we use the same\ntemplate as Wang et al. (2022) for formatting in-\nstructions and inputs. Our full set of training hyper-\nparameters is available in Appendix A. We create a\nsmall validation set of 1,000 examples for model\nselection following the methodology proposed by\nWang et al. (2022): we randomly select 10 exam-\nples from 100 random tasks of the Super-Natural\nInstructions training set.\n4.2 Baselines\nWe measure the relative utility of Unnatural Instruc-\ntions by comparing it to a variety of models, all\nbased on T5-11B, which were fine-tuned with dif-\nferent types and quantities of manually-annotated\ninstruction data.\nT0++ (Sanh et al., 2021) is an instruction-tuned\nvariant of T5-LM, trained on tasks in the Prompt-\nSource (Bach et al., 2022) prompt formats.\nTk-Instruct Wang et al. (2022) fine-tune T5 v1.1\non Super-Natural Instructions, using a subsample\n14413\nExample\nInstruction: In this task, you are asked to come up with a\ndesign for an office lobby. The client has provided specific\ninstructions on what they want in the lobby, and it is your job\nto create a floor plan that meets all of their requirements. (✓)\nInput: Client wants a welcoming environment with space for\npeople to wait comfortably without feeling cramped. They\nalso would like some sort of area where people can do work if\nthey need to kill some time before their next meeting. (✓)\nOutput: [...] Based on these requirements, a possible floor\nplan [...] The lobby would have a reception area [...] with\naccess to outlets, and a coffee station. (✓)\nInstruction: You will be given several pieces of information\nabout an event, and you have to determine whether or not it\nis a cause-and-effect relationship. If the given statements are\nrelated by cause and effect, then output ‘True’. Otherwise,\noutput ‘False’. (✓)\nInput: Statements: [‘The tornado damaged the city’, ‘Many\npeople were left homeless’]. (✓)\nOutput: True (✓)\nInstruction: You are to determine if the statement is true or\nfalse. (✓)\nInput: Text: Santa Claus lives at the North Pole. (✓)\nOutput: False (✗)\nInstruction: You are given a scientific experiment scenario\nand your job is to determine which variable(s) the scientist\nshould control in order to test the hypothesis. (✓)\nInput: The hypothesis is “If a plant receives more sunlight,\nthen it will grow taller.” Which variable(s) should the scientist\ncontrol? (✗)\nOutput: A (–)\nTable 2: Examples of generated instructions, inputs and\noutputs in our core dataset. For the first two examples,\nthe entire pair of instruction, input and output is valid.\nThe third example has an incorrect output; in the fourth\nexample, the experiment is not described in the input.\nof 757 tasks with 100 examples each. Tk-Instruct\nis trained with a batch size of 1,024 examples\nfor 1,000 steps. Since our evaluation focuses on\nzero-shot instruction understanding, we use the\ndefinition-only version of Tk-Instruct.\nFLAN-T5 Chung et al. (2022) fine-tune T5 on a\ncollection of tasks phrased as instructions in multi-\nple prompting setups (zero-shot, few-shot, Chain-\nof-Thought (Wei et al., 2022)), achieving impres-\nsive zero-shot generalization capabilities.\nT5-LM on Natural InstructionsOur main point\nof comparison is the utility of the original manually-\ncurated instructions in Super-Natural Instructions.\nWe therefore train a model which is identical to\nours in all aspects but data. Specifically, we fine-\ntune the LM-adapted variant of T5-11B on a sub-\nsample of 64,000 examples from Super-Natural\nInstructions training set, excluding examples from\nTask #Examples\nQuestion Answering 11\nSentiment Analysis 10\nArithmetic 8\nGeometry 8\nEvent Ordering 7\nFact Verification 5\nFill-in-the-Blank 5\nGeneral Math Puzzles 4\nIdentifying Overlapping Strings 4\nArray Manipulations and Puzzles 4\nTable 3: Top 10 tasks by #examples, out of the 200\nmanually-analyzed Unnatural Instructions examples.\nany task that participates in the validation set. This\nmodel differs from Tk-Instruct along three aspects:\nthe dataset subsample, the base model (T5-LM),\nand some training hyperparameters (batch size 16\nfor 3 epochs).\n4.3 Evaluation\nWe evaluate models on four different benchmarks,\nmeasuring a range of capabilities. All evaluations\nare carried out in a zero-shot setting, without few-\nshot demonstrations, unless explicitly provided in\nthe instructions. See the full evaluation details in\nAppendix B.\nNatural Instructions We evaluate models on the\ntest set of Super-Natural Instructions (Mishra et al.,\n2022; Wang et al., 2022). As in the original papers,\noutputs are generated using greedy decoding, and\nperformance is measured using Rouge-L.\nT0: Zero-Shot We evaluate models on the held-\nout set of T0 (Sanh et al., 2021), using rank clas-\nsification for decoding and accuracy as a metric.\nFor fair comparison, we remove tasks supersets of\nwhich are present in the Tk-Instruct training set.\nThe final set contains six tasks: ANLI R1-R3, CB,\nCOPA and RTE. We refer to this evaluation set as\nT0: Zero-Shot. Unlike Super-Natural Instructions,\nT0: Zero-Shot tasks do not have a strict format\nand are phrased in a rather free-form manner, in-\ncluding inputs that can be embedded into the task\ndescription. We therefore expect models trained on\nour core dataset (without instruction paraphrases)\nto perform poorly under these conditions, while\nadding the task reformulation data should boost\nperformance on T0: Zero-Shot.\nBIG-bench: Hard The “hard” subset of BIG-\nbench (Suzgun et al., 2022) contains 23 challeng-\ning tasks from BIG-Bench (Srivastava et al., 2022).\nWe investigate two different formats for all tasks:\n14414\ntheir original format in BIG-bench, and the for-\nmat of Suzgun et al. (2022), who reformulate each\ntask as question answering with manually added\ninstructions; for the latter, we remove all few-shot\ndemonstrations. For both formats, we use greedy\ndecoding and exact match with the reference for\nevaluation.\nLMentry LMentry (Efrat et al., 2022) is a bench-\nmark that tests basic language abilities, designed\nto complement common approaches for evaluating\nlarge language models. Outputs are generated by\napplying greedy decoding and evaluated using high-\naccuracy regular expressions. The benchmark’s\nmetric is the LMentry score, which combines accu-\nracy with multiple aspects of robustness.\n5 Results\nOur main results are shown in Table 4, which\nreports the performance of each model on every\nbenchmark. Remarkably, T5-LM finetuned on\nUnnatural Instructions outperforms several strong\ninstruction-tuned baselines such as T0++ and Tk-\nInstruct; the only exception to this is BIG-bench:\nHard (Orig), where T0++ performs better. Re-\ntraining a model on Super-Natural Instructions\nusing our exact setup reveals a significantly bet-\nter baseline than Tk-Instruct, using the same data.\nHowever, even in this direct comparison, Unnatu-\nral Instructions leads to stronger or equal perfor-\nmance for every dataset except Super-Natural In-\nstructions itself. While T5-LM finetuned on Unnat-\nural Instructions is outperformed by FLAN-T5, that\nmodel was trained on approximately 60 times more\ndata. These results demonstrate that automated\ndata generation with pretrained LMs is a viable and\ncost-effective alternative to human-curated data.\n5.1 Performance with Template Expansion\nWe evaluate the contribution of template expansion\n(§2.2) to the performance of models trained on\nUnnatural Instructions. To this end, we finetune a\nsingle model on our full dataset with paraphrases;\nresults are shown in the bottom row of Table 4.\nAdding instruction paraphrases boosts perfor-\nmance on T0: Zero-Shot (+3.3), Big-bench: Hard\nin its original format (+12.1) and LMentry (+8.7).\nWe surmise that this improvement is largely be-\ncause examples in our core dataset were generated\nbased on demonstrations from Super-Natural In-\nstructions only and therefore have their exact for-\nmat and style. Accordingly, models trained on\nour core dataset rely too much on this specific for-\nmat and cannot generalize well to different for-\nmats found in other benchmarks. Obtaining more\nformat diversity through template expansion suc-\ncessfully addresses this issue. On the other hand,\nover-reliance on the format of Super-Natural In-\nstructions is probably preferable when testing on\nthis dataset itself, which explains the performance\ndrop when adding paraphrases compared to the\nboost in performance on other benchmarks.\nWhile some of the performance gains observed\nmay also be attributed to the fact that adding para-\nphrases simply increases the data, in §5.2 we show\nthat template expansion is helpful even when con-\ntrolling for dataset size.\n5.2 Performance Scaling by Dataset Size\nAs all of our data is generated from the same\nmodel using the same set of prompts, scaling up\nthe amount of generated examples might lead to\nnumerous repetitions and, as a consequence, di-\nminishing returns in terms of downstream task per-\nformance. To investigate whether this is an issue,\nwe analyze how the amount of training examples\naffects the performance of our finetuned models.\nTo this end, we train models on subsets of both\nSuper-Natural Instructions and Unnatural Instruc-\ntions, ranging from 250 to 64,000 examples. As\nshown in Figure 5, our core and full data as well\nas Super-Natural Instructions all exhibit log-linear\nscaling laws, suggesting that even for subsets of\nUnnatural Instructions containing thousands of ex-\namples, simply generating more examples still adds\na valuable signal to our training data.\nResults for LMentry (Figure 5) show that our\ntemplate expansion process is still beneficial when\ncontrolling for dataset size. The added value of\nthe paraphrases is therefore likely to be in terms of\nformat diversity rather than solely as a method for\nincreasing the amount of data.\n5.3 Performance Scaling by Cost\nIn practical scenarios with fixed annotation budgets,\nthe actual cost associated with a certain level of per-\nformance is even more relevant than the number of\nrequired examples. We therefore measure model\nperformance as a function of the cost for obtaining\nthe training data. Based on OpenAI’s pricing as of\nDecember 2022, the cost for generating an exam-\nple is estimated at $0.02 for our core dataset, and\n$0.01 for the expanded dataset. Kiela et al. (2021)\nestimate human annotation cost at $0.50–$1.00 per\n14415\nModel #Examples Super-Natural T0: BIG-bench: LMentryInstructions Zero-Shot Hard (Orig/QA)\nPrior Work\nT5-LM 0 24.3 40.2 0.0 / 0.7 20.6\nT0++ 12,492,800 40.3 NHO 20.2 / 13.9 38.3\nTk-Instruct 75,417 45.6 41.4 5.8 / 11.8 35.7\nFLAN-T5 14,336,000 NHO NHO 39.3 / 40.0 52.2\nDirect Comparison Baseline\nT5-LM on Super-Natural Instructions 64,000 54.0 44.0 10.2 / 29.7 34.6\nOur Approach\nT5-LM on Unnatural Instructions 64,000 51.9 45.7 16.0 / 29.5 42.0\n+ Instruction Paraphrases 240,670 49.3 49.0 28.1 / 29.4 50.7\nTable 4: Model performance on four benchmarks. Best results in our direct comparison setup are bold, best results\noverall are underlined. NHO indicates that a benchmark’s data is not held out because it was used for training.\nexample, excluding indirect costs such as task de-\nsign and UX development; for comparison with\nour automatic data collection method, we assume\nthe lower-bound human annotation cost of $0.50.\nAs shown in Figure 5, Unnatural Instructions is\nclearly more cost-efficient than manually curated\ndata. This is true even for the Super-Natural In-\nstructions test set, where a model trained on Un-\nnatural Instructions is weaker than a model trained\non Super-Natural Instructions for a fixed number\nof examples, but better when controlling for cost,\nshowing that our automatic approach outperforms\ncrowdsourcing for a fixed annotation budget.\n6 Generative Model Ablations\nAs a data generation model, we used text-davinci-\n002, an instruction-tuned variant of GPT-3 (Brown\net al., 2020). However, our approach is not limited\nto this specific model. We experiment with original\n(untuned) GPT-3 model by using it as the modelM\nin both the input generation and output generation\nphases (see §2). We train models for 1,500 steps us-\ning 2,000 examples and evaluate the Super-Natural\nInstructions validation set performance as a proxy,\naveraged across three different random seeds.\nTable 5 shows how replacing an instruction-\ntuned model with a vanilla model affects the quality\nof the data. We observe that while the quality of\ngenerated inputs does drop by 4.5 points, it is well\nwithin the range of other prompt ablations (see Ap-\npendix D). In other words, informative and diverse\ninstructions can be generated by untuned language\nmodels. However, generating outputs does seem\nto require instruction tuning. A manual analysis\nreveals that outputs generated by GPT-3 mainly suf-\nfer from the model’s inability to stop, often starting\nwith the correct answer, but then degenerating into\nrepetitions or tangents. While this may be reme-\nModel Used to Generate Super-Natural\nInput Output Instructions\ntext-davinci-002 text-davinci-002 48.7 ± 0.3\nGPT-3 text-davinci-002 44.2 ± 0.7\nGPT-3 GPT-3 4.1 ± 0.1\nTable 5: Performance of 11B T5-LM models trained on\n2,000 examples, generated with different models, on the\nSuper-Natural Instructions validation set.\ndied through various post-processing heuristics, we\nleave exploration of such methods to future work.\n7 Related Work\nInstruction Tuning Efrat and Levy (2020) pro-\npose the Instruction Paradigm, where models learn\nnew tasks from natural language instructions alone.\nMishra et al. (2022); Wang et al. (2022) construct\nthe first large-scale instruction benchmarks by col-\nlecting crowdsourcing instructions used to create\nNLP datasets and converting them into a uniform\nformat. Sanh et al. (2021); Wei et al. (2021) fur-\nther extend the usability of instructions by suggest-\ning instruction tuning, where a language model is\ntrained on many natural language instructions in the\nhope that it will generalize to new, unseen instruc-\ntion tasks. Chung et al. (2022) advance instruction\ntuning by scaling the number of tasks, scaling the\nmodel size, and adding chain-of-thought (Wei et al.,\n2022), while Ouyang et al. (2022) propose a rein-\nforcement learning approach for instruction tuning\nfrom comparative human judgements.\nAutomatic Data Generation Obtaining large-\nscale supervised data can be expensive and time-\nconsuming, making automatic data generation ap-\npealing. A common approach is to automati-\ncally augment existing datasets (Anaby-Tavor et al.,\n2020; Andreas, 2020; Yang et al., 2020; Kaushik\n14416\n103 104 105\n#Examples\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0Super-Natural Instruction (T est)\nSuper-Natural\nUnnatural\n+Paraphrases\n103 104 105\n#Examples\n20\n25\n30\n35\n40\n45\n50LMentry Score\nSuper-Natural\nUnnatural\n+Paraphrases\n101 102 103 104\nCost (USD)\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0Super-Natural Instruction (T est)\nSuper-Natural\nUnnatural\n+Paraphrases\n101 102 103 104\nCost (USD)\n20\n25\n30\n35\n40\n45\n50LMentry Score\nSuper-Natural\nUnnatural\n+Paraphrases\nFigure 5: Scaling experiments comparing Unnatural Instructions with Super-Natural Instructions. Top row:Model\nperformance when controlling for dataset size, tested on Super-Natural Instructions (left) and LMentry (right).\nBottom row: Model performance when controlling for the cost of obtaining data , tested on Super-Natural\nInstructions (left) and LMentry (right).\net al., 2020; Lee et al., 2021, inter alia). Kiela\net al. (2021) suggest a human-and-model-in-the-\nloop dataset creation; In the same manner, Nie et al.\n(2020) apply a process to create training data for\nthe task of NLI (Dagan et al., 2006; Bowman et al.,\n2015). Liu et al. (2022a) combine human annota-\ntors and GPT-3, create challenging NLI examples.\nOther work suggested creating datasets entirely\nautomatically, without the need for labeled data.\nSchick and Schütze (2021b) and Ye et al. (2022)\npropose to leverage pretrained language models to\ngenerate entire labeled datasets from scratch, for\na given, predefined task. Agrawal et al. (2022)\nuse pretrained language models to automatically\nconstruct multilingual QA data using only five ex-\namples per language.\n8 Conclusion\nWe introduce Unnatural Instructions, an automat-\nically generated dataset of natural language in-\nstructions and their corresponding inputs and out-\nputs. To the best of our knowledge, this is the first\ngeneral-purpose NLP dataset that was automati-\ncally generated. Our experiments show that models\ntrained on Unnatural Instructions outperforms mod-\nels trained on manually annotated datasets across\nseveral benchmarks. Unnatural Instructions is not\nonly cost-effective, we also provide evidence of\nenhanced diversity in the instructions produced and\na high level of creativity in the tasks devised, a trait\ndifficult to obtain with crowd workers. Ablations\nshow that even weaker models without instruc-\ntion tuning can generate useful instructions, though\nthey may struggle with producing the correspond-\ning outputs. However, coming up with interesting\ntasks and writing diverse instructions is arguably\nthe main challenge of the data collection process,\nwhereas given instructions and inputs, outputs are\noften far easier to annotate through crowdsourc-\ning. Our findings incentivize utilizing models for\ngeneral-purpose data generation, which we view as\nan intriguing direction for future research.\n14417\n9 Limitations\nWe point at some directions for future improve-\nments in automatic instruction generation.\nFirst, as shown in §3, Unnatural Instructions\ncontains noisy examples, in which either the in-\nstruction, input, or output are invalid. Future work\nmay focus on developing better filters for such ex-\namples - e.g., by annotating a subset of examples\nas either valid or not and training a classifier for\ndetermining the correctness of generated instances\n(West et al., 2022; Liu et al., 2022a).\nSecond, future work may employ a human-in-\nthe-loop approach, where humans should recognize\nchallenging patterns, encouraging models to gener-\nate more complex examples (Liu et al., 2022a).\nIn another human-in-the-loop scenario, models\ntrained on Unnatural Instructions can be queried\nby humans to find examples on which these mod-\nels fail, thus collecting harder examples (Nie et al.,\n2020).\nFinally, language models are known to some-\ntimes reflect undesirable biases present in their\ntraining data. Automatically generated data may\ntherefore contain such content. We note that during\nour manual analysis, we did not notice any harmful\nexamples. Still, future work may consider applying\na filtering mechanism to reduce the risk of having\nbiased content.\nReferences\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\nMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\nDipanjan Das, and Mirella Lapata. 2022. Qameleon:\nMultilingual qa with only 5 examples. arXiv preprint\narXiv:2211.08264.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, N. Tep-\nper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In AAAI\nConference on Artificial Intelligence.\nJacob Andreas. 2020. Good-enough compositional data\naugmentation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7556–7566, Online. Association for\nComputational Linguistics.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert\nWebson, Colin Raffel, Nihal V . Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,\nZhiqing Sun, Srulik Ben-david, Canwen Xu, Gun-\njan Chhablani, Han Wang, Jason Fries, Maged Al-\nshaibani, Shanya Sharma, Urmish Thakker, Khalid\nAlmubarak, Xiangru Tang, Dragomir Radev, Mike\nTian-jian Jiang, and Alexander Rush. 2022. Prompt-\nSource: An integrated development environment and\nrepository for natural language prompts. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstra-\ntions, pages 93–104, Dublin, Ireland. Association for\nComputational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classification,\nand Recognising Tectual Entailment, pages 177–190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nAvia Efrat, Or Honovich, and Omer Levy. 2022. Lmen-\ntry: A language model benchmark of elementary\nlanguage tasks.\nAvia Efrat and Omer Levy. 2020. The turking test: Can\nlanguage models understand instructions?\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\n14418\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107–112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\n2020. Learning the difference that makes a differ-\nence with counterfactually-augmented data. In Inter-\nnational Conference on Learning Representations.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,\nZeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit\nBansal, Christopher Potts, and Adina Williams. 2021.\nDynabench: Rethinking benchmarking in NLP. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4110–4124, Online. Association for Computa-\ntional Linguistics.\nSawan Kumar and Partha Talukdar. 2021. Reorder-\ning examples helps during priming-based few-shot\nlearning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4507–4518, Online. Association for Computational\nLinguistics.\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and\nHyung Won Chung. 2021. Neural data augmentation\nvia example extrapolation.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022a. Wanli: Worker and ai collabora-\ntion for natural language inference dataset creation.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022b. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901, Online. Association for Computa-\ntional Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining , KDD ’20,\npage 3505–3506, New York, NY , USA. Association\nfor Computing Machinery.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nTimo Schick and Hinrich Schütze. 2021a. Few-shot\ntext generation with natural language instructions. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 390–\n402, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\n14419\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2022. Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al. 2022.\nSuper-naturalinstructions:generalization via declara-\ntive instructions on 1600+ tasks. In EMNLP.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602–4625, Seat-\ntle, United States. Association for Computational\nLinguistics.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. Generative data augmentation for common-\nsense reasoning. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n1008–1025, Online. Association for Computational\nLinguistics.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022. ZeroGen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11653–11669, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with bert. In ICLR 2020.\n14420\nA Fine-Tuning Hyperparameters\nWe use the same set of hyperparameters for fine-\ntuning experiments with T5-LM (Raffel et al.,\n2020; Lester et al., 2021). All models are trained\nfor up to max(3 epochs, 3000 steps) and the final\nmodel is chosen based on Rouge-L on our valida-\ntion set, where we evaluate every 100 steps. We\nuse a batch size of 16, a maximum learning rate of\n1 · 10−5 with warm-up for the first 10% of training\nand a weight decay of 0.01. We truncate inputs at\n1,024 tokens and outputs at 128 tokens. All mod-\nels are trained using DeepSpeed’s ZeRO-3 (Rasley\net al., 2020). Training on up to 64,000 examples is\nperformed on 32 NVIDIA Tesla V100 16GB V olta\nGPUs using FP32; for bigger training datasets, we\nused 8 NVIDIA A100 40GB GPUs with BF16. For\ncomputing Rouge-L and exact match scores, we\nuse the implementation of Wang et al. (2022).\nB Evaluation Details\nFor evaluating model performance on Super-\nNatural Instructions, T0: Zero-Shot and LMEntry,\nwe use their official evaluation scripts. For evalua-\ntion on BIG-bench: Hard, we lowercase outputs, re-\nmove punctuation characters and trim extra whites-\npace before computing exact match scores. The\nonly exception to this is the task dyck_languages,\nwhere the target output consists entirely of punctu-\nation characters.\nC Data Generation Prompts\nTable 6 presents the in-context demonstrations we\nused, taken from Wang et al. (2022).\n14421\nIn-Context Demonstrations\nSeed 1\nExample 1\nInstruction: In this task, you’re given passages that contain mentions of names of people, places, or things. Some of these\nmentions refer to the same person, place, or thing. Your job is to write questions that evaluate one’s understanding of such\nreferences. Good questions are expected to link pronouns (she, her, him, his, their, etc.) or other mentions to people, places, or\nthings to which they may refer. Do not ask questions that can be answered correctly without understanding the paragraph or\nhaving multiple answers. Avoid questions that do not link phrases referring to the same entity. For each of your questions, the\nanswer should be one or more phrases in the paragraph, and it should be unambiguous.\nInput: Passage: Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname\nthe \"Artful Dodger\", and his sidekick, a boy of a humorous nature named Charley Bates, but Oliver’s innocent and trusting\nnature fails to see any dishonesty in their actions. The Dodger provides Oliver with a free meal and tells him of a gentleman\nin London who will \"give him lodgings for nothing, and never ask for change\". Grateful for the unexpected assistance, Oliver\nfollows the Dodger to the \"old gentleman’s\" residence. In this way Oliver unwittingly falls in with an infamous Jewish criminal\nknown as Fagin, the gentleman of whom the Artful Dodger spoke. Ensnared, Oliver lives with Fagin and his gang of juvenile\npickpockets in their lair at Saffron Hill for some time, unaware of their criminal occupations. He believes they make wallets and\nhandkerchiefs.\nConstraints: None.\nExample 2\nInstruction: You will be given a piece of text either about an everyday event, or a general statement. If the event seems a plausible\nevent to you, or the general statement makes sense matches your commonsense, output ’True’, otherwise output ’False’.\nInput: Text: The glass fell of a three-story building, so it broke into pieces.\nConstraints: The output should be one of the two: ‘True’ or ‘False’.\nExample 3\nInstruction: You need to answer the question ’Are the given steps in order?’, given a set of steps describing a process. Your\nanswer must be either Yes or No. If the answer is No, that means the steps are out of order and do not make sense in the order\nthey are in. If the answer is Yes, that means the steps are in order and make sense in the order that they are in. A set of steps are\nnot in order if the steps reference information that is introduced in a later step.\nInput: Steps: [‘The seeds are dispersed by wind, animals, etc’, ‘The seeds reach the ground’, ‘Grow into new trees’, ‘The process\nrepeats itself over and over’, ‘A tree produces seeds’,‘These new trees produce seeds’]\nConstraints: The output should be one of the two: ‘Yes’ or ‘No’.\nExample 4\nSeed 2\nExample 1\nInstruction: In this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are\nshort phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX,\nPersonY , PersonZ). PersonX is always the subject of the event. You have to determine whether the Head is used for the Tail or\nnot. The usage describes everyday affordances or uses of objects and includes both typical and atypical uses. For example, a\npopcorn bucket can typically be used to hold popcorn, but it could also serve as a hat in atypical situations. Classify your answers\ninto “Yes” and “No”. The phrase may also contain “-”, a placeholder that can be an object, a person, and/or an action.\nInput: Head: floor mats<sep>Tail: wipe off one’s boots\nConstraints: The output should be ‘Yes’ or ‘No’.\nExample 2\nInstruction: In this task, you’re given a short story of five sentences written in natural language. However, the order of the given\nstory is not correct. Your job is to return the correct order for the given five sentences to create a coherent short story with the\nnew order that has the correct flow. Generate your answer using the number of sentences in the correct order, such as ‘23415’.\nInput: Sentence1: He is happy now. Sentence2: Rick grew up in a troubled household. Sentence3: It wasn’t long before Rick got\nshot in a robbery. Sentence4: The incident caused him to turn a new leaf. Sentence5: He never found good support in family, and\nturned to gangs.\nConstraints: The output must consist of the numbers representing the sentences.\nExample 3\nInstruction: In this task, you are given Yelp reviews. The task is to classify a review as “POSITIVE” if the overall sentiment of\nthe review is positive or as “NEGATIVE” if the overall sentiment of the review is negative.\nInput: This is my go to place to get the best chicken Pad Thai! Also the price is super good, considering most places have high\nprices and poor quality. Love this place, its definitely a top 5 fav for take out.\nConstraints: The output should be ‘POSITIVE’ or ‘NEGATIVE’.\nExample 4\n14422\nSeed 3\nExample 1\nInstruction: You are given a question title. You need to classify it into one of the 10 topics: 0 - ‘Society & Culture’, 1 - ‘Science\n& Mathematics’, 2 - ‘Health’, 3 - ‘Education & Reference’, 4 - ‘Computers & Internet’, 5 - ‘Sports’, 6 - ‘Business & Finance’, 7\n- ‘Entertainment & Music’, 8 - ‘Family & Relationships’, 9 - ’Politics & Government’.\nInput :What’s the best way to clean a keyboard?\nConstraints: The output should be a number between 0 and 9.\nExample 2\nInstruction: You are given a target person and two sentences. Your task is to determine whether the second sentence conveys\neither a stereotype, anti-stereotype, or is unrelated with respect to the first sentence and the target. A stereotype is a widely held\nbut oversimplified notion of a group of people. An anti-stereotype is an idea that goes against the widely held but oversimplified\nnotion of a group of people. Sentences and targets are unrelated if there is no relevance between them.\nInput: Target: mother. The mother walked into her kitchen. She pulled a beer out of the fridge and leaned against the doorway,\nsipping it slowly.\nConstraints: The output should be one of the three: ‘stereotype’, ‘anti-stereotype’ or ‘unrelated’.\nExample 3\nInstruction: In this task, you are given an article. Your task is to summarize the article in a sentence.\nInput: Ms Bourne, who became Sussex’s first PCC in 2012, was declared winner after the vote went to a second count. Three\ncandidates were eliminated in the first count, leaving only Ms Bourne and Labour’s Michael Jones. In the first count, Ms Bourne\nhad 114,570 votes and Mr Jones had 61,017. The second count brought their totals to 139,335 and 86,392 respectively. She said:\n“I’m absolutely honoured and very privileged to be elected.” She said she needed to “find extra savings because budgets are still\nreducing” and “to invest in frontline policing because I know that is really important to people in Sussex”. V oter turnout was\n22.5% compared with 15.3% in 2012. The three eliminated in the first count were Green Party candidate James Doyle, UKIP’s\nPatrick Lowe and James Walsh from the Liberal Democrats. Results listed alphabetically by surname are as follows. BBC News\nApp users: tap here to see the results.\nConstraints: None.\nExample 4\nSeed 4\nExample 1\nInstruction: In this task, you are given Wikipedia articles on a range of topics as passages and a question from the passage. We\nask you to answer the question by classifying the answer as 0 (False) or 1 (True).\nInput: Passage: Property tax – Property tax or ‘house tax’ is a local tax on buildings, along with appurtenant land. It is and\nimposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type\nwealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies,\nspecifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based\nrating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a\npercentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a\n‘service charge’ is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring\nreciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting\ntax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is\nmildly progressive with about 80% of assessments falling in the first two brackets. Question: is house tax and property tax are\nsame.\nConstraints: The output should be 0 or 1.\nExample 2\nInstruction: Rewrite each original sentence in order to make it easier to understand by non-native speakers of English. You can do\nso by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression),\nand/or splitting a long complex sentence into several simpler ones. The final simplified sentences need to be grammatical, fluent,\nand retain the main ideas of their original counterparts without altering their meanings.\nInput: From its inception, it was designated a duty-free port and vied with the neighboring Sultanate of Pattani for trade.\nConstraints: None.\nExample 3\nInstruction: You are provided with an arithmetic question. Your task is to compute the solution using the given arithmetic\noperations. The only arithmetic operators needed to answer the questions are’+’(addition) and’-’(subtraction). The answer\nshould be correct to one decimal place.\nInput: Joan found 70 seashells on the beach. She gave Sam some of her seashells, after which she has 27 seashell left. How\nmany seashells did she give to Sam?\nConstraints: None.\nExample 4\n14423\nSeed 5\nExample 1\nInstruction: You are given a science question (easy-level) and four answer options (associated with “A”, “B”, “C”, “D”). Your\ntask is to find the correct answer based on scientific facts, knowledge, and reasoning. Do not generate anything else apart from\none of the following characters: ‘A’, ‘B, ‘C’, ‘D’. There is only one correct answer for each question.\nInput: Which part of a bicycle BEST moves in a circle? (A) Seat (B) Frame (C) Foot pedal (D) Kickstand\nConstraints: The output should be one of the following characters: ‘A’, ‘B, ‘C’, ‘D’.\nExample 2\nInstruction: You are given a negative review and your task is to convert it to a positive review by one or more making minimal\nchanges. Avoid changing the context of the review.\nInput: we stood there in shock, because we never expected this.\nConstraints: None.\nExample 3\nInstruction: In this task, you are given two sentences taken from a conversation, and your job is to classify whether these given\nsentences are sequential or not. We will mark the given sentence pair as ‘True’ if it’s sequential, otherwise ‘False’. The two\nsentences are spoken by two different people.\nInput: Noah: When and where are we meeting? :), Madison: I thought you were busy...?\nConstraints: The output should be ‘True’ or ‘False’.\nExample 4\nTable 6: The in-context demonstrations used in our experiments.\n14424\nExample 1\nInstruction: …\nInput: …\nConstraints: …\nExample 2\nInstruction: …\nInput: …\nConstraints: …\nExample 3\nInstruction: …\nInput: …\nConstraints: …\nExample 4\nInstruction: …\nInput: …\nConstraints: …\nInstruction: …\nInput: …\nConstraints: …\nInstruction: …\nInput: …\nConstraints: …\nInstruction: …\nBelow are examples of \ninstructions describing a \ndiverse set of textual \ntasks and their inputs.\nInstruction: …\nInput: …\nConstraints: …\nInstruction: …\nInput: …\nConstraints: …\nInstruction: …\nInput: …\nConstraints: …\nWrite instructions and \ninputs for other textual \ntasks.\nMinimal Enumeration Verbose\nFigure 6: The meta-prompts used in our ablations.\nD Structural Prompt Ablations\nWe explore the effect of the different components\nof our data collection pipeline by conducting struc-\ntural prompt ablations. Throughout this section,\nwe train models for 1,500 steps using 2,000 exam-\nples and evaluate the Super-Natural Instructions\nvalidation set performance, averaged across three\ndifferent random seeds.\nD.1 Meta-Prompts\nLanguage models are known to be sensitive to the\nmeta-prompt – i.e., the text wrapping the in-context\ndemonstrations, which can include task description\nor additional guidance regarding the desired output.\nWe therefore experiment with three different meta-\nprompt styles: minimal, enumeration, and verbose\n(Figure 6).\nTable 7 presents the results obtained from fine-\ntuning on datasets generated with different meta-\nprompts. We observe that the simple enumeration\napproach elicits more informative examples than\neither the minimalistic or verbose approaches. Per-\nhaps surprisingly, the verbose meta-prompt per-\nforms worse than the minimalistic one, possibly\nbecause the last line (the command) interrupts the\npattern, and does not align well with patterns in the\npretraining corpus.4\n4While our core dataset was created using the enumera-\ntion meta-prompt, the remaining ablation experiments in this\nsection were run using the verbose meta-prompt.\nMeta-Prompt Super-Natural Instructions\nMinimal 47.5 ± 0.6\nEnumeration 48.7 ± 0.3\nVerbose 46.9 ± 0.3\nTable 7: Performance of 11B T5-LM models trained on\n2,000 examples, generated with each meta-prompt, on\nthe Super-Natural Instructions validation set.\nSeed Demonstrations Super-Natural Instructions\n1 46.9 ± 0.3\n2 46.1 ± 0.3\n3 46.8 ± 0.4\n4 41.9 ± 1.0\n5 46.0 ± 0.2\nMix 46.1 ± 0.3\nTable 8: Performance of 11B T5-LM models trained\non 2,000 examples, generated with various sets of three\nin-context demonstrations (seeds), on the Super-Natural\nInstructions validation set. Mix samples 400 examples\nfrom each of the five single-seed datasets.\nD.2 In-Context Examples\nModels such as GPT-3 are known to be sensitive\nto slight variations in prompt content, resulting\nin performance differences when provided with\ndifferent demonstrations sampled from the same\ndataset (Liu et al., 2022b) and when permuting the\nin-context demonstrations (Kumar and Talukdar,\n2021; Lu et al., 2022). To account for the effect\nof the provided demonstrations on the quality of\nthe generated data, we experiment with each of our\nfive demonstration sets separately.5 Table 8 shows\nthat the data generation pipeline is largely robust\nto variations in the in-context demonstrations, with\none outlier (seed 4). Inspecting the differences\nbetween these groups, we find that seed 4 led to\nless constrained instructions: 1,376 out of 2,000\nexamples do not have constraints, whereas that\nnumber is between 28 and 880 for all other sets.\nIndeed, in seed 4, only one out of three prompt\ndemonstrations had constraints, while in other sets,\nat least two demonstrations had constraints.\nD.3 Constraints\nAs mentioned in §2, each instruction-input demon-\nstration is accompanied by an additional con-\nstraints field, which details the task’s output space\nrestrictions (e.g., “entailment”, “contradiction” or\n“neutral” for NLI). We note that, in all demonstra-\ntions, the instruction itself lists the output space\n5See Appendix C for all demonstration sets.\n14425\nUse “Constraints:” for Super-Natural\nInput Gen Output Gen Instructions\n✓ ✓ 46.9 ± 0.3\n✓ 43.9 ± 0.7\n41.7 ± 0.2\nTable 9: Performance of 11B T5-LM models trained on\n2,000 examples, generated with and without the con-\nstraints field, on the Super-Natural Instructions valida-\ntion set.\nconstraints. We hypothesize that adding the con-\nstraints field may emphasize these restrictions, ul-\ntimately steering the output generation model to\nproduce outputs in the correct format. We verify\nour hypothesis by conducting two ablation exper-\niments. First, we keep the constraints field when\ngenerating the instructions and inputs, but only use\ninstructions and input arguments for the output gen-\neration step (i.e., without concatenating generated\nconstraints). Second, we completely remove the\nconstraints field from the data generation pipeline,\nleaving the instruction field as the only source of\ninformation for output space constraints. Table 9\nshows that the constraints field has a positive ef-\nfect both on the quality of the generated outputs\nand inputs. Removing constraints from the output\ngeneration step reduces performance by 3 points,\nand removing the field from the instructions-inputs\ngeneration phase decreases performance by an ad-\nditional 2.2 points.\nD.4 Two-Step Process\nAn alternative to our two-step pipeline is to gen-\nerate instruction-input-output triplets in one pass.\nTo test this approach, we provide the model with\nthe same prompt used for the instruction-input-\nconstraints generation, only with an additional out-\nput field, added after the constraints field. As Ta-\nble 9 shows, one-step generation obtains a score\nthat is lower by 1.7 than the default two-step pro-\ncess. We suspect that this gap is a result of using\nstochastic decoding in the unified input-output gen-\neration phase, which is critical for obtaining diverse\ninputs. In contrast, when generating outputs in a\nseparate phase, we can use deterministic decoding\nalgorithms to maximize accuracy.\nData Generation Process Super-Natural Instructions\nSeparate I/O Steps 46.9 ± 0.3\nUnified I/O Step 45.2 ± 0.6\nTable 10: Performance of 11B T5-LM models trained\non 2,000 examples, generated either using separate input\nand output steps or a single unified step, on the Super-\nNatural Instructions validation set.\n14426\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n9\n□\u0013 A2. Did you discuss any potential risks of your work?\n9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n2\n□\u0013 B1. Did you cite the creators of artifacts you used?\n1, 2, 4\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe veriﬁed that all the data and code used is publicly open - we veriﬁed license details for each, and\nwe provided citation to all relevant resources, where license details can also be found.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nAs for existing datasets we used, we didn’t discuss that, but other than the fact that we used published\ndatasets that are already used by the research community - we also sampled examples and manually\nveriﬁed their content. As for data we collected, we did discuss that in section 9, and additionally\nprovided data analysis in section 3.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14427\nC □\u0013 Did you run computational experiments?\n4, 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4, Appendix A\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, Appendix A\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5, 6, Appendix D\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4, Appendix A, Appendix B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14428"
}