{
  "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding",
  "url": "https://openalex.org/W3094085917",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2139648367",
      "name": "Young Jin Kim",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2133447107",
      "name": "HANY HASSAN",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W3033737024",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W3014568172",
    "https://openalex.org/W3045818398",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3005957694",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W3024171804",
    "https://openalex.org/W2947946877",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2997710335",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2613065256",
    "https://openalex.org/W2948130861",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2978017171"
  ],
  "abstract": "Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.",
  "full_text": "Proceedings of SustaiNLP: Workshop on Simple and Efﬁcient Natural Language Processing, pages 149–158\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\n149\nFastFormers: Highly Efﬁcient Transformer Models\nfor Natural Language Understanding\nYoung Jin Kim\nMicrosoft\nOne Microsoft Way\nRedmond, W A 98052, USA\nyouki@microsoft.com\nHany Hassan Awadalla\nMicrosoft\nOne Microsoft Way\nRedmond, W A 98052, USA\nhanyh@microsoft.com\nAbstract\nTransformer-based models are the state-of-\nthe-art for Natural Language Understanding\n(NLU) applications. Models are getting bigger\nand better on various tasks. However, Trans-\nformer models remain computationally chal-\nlenging since they are not efﬁcient at inference-\ntime compared to traditional approaches. In\nthis paper, we present FastFormers, a set of\nrecipes to achieve efﬁcient inference-time per-\nformance for Transformer-based models on\nvarious NLU tasks. We show how care-\nfully utilizing knowledge distillation, struc-\ntured pruning and numerical optimization can\nlead to drastic improvements on inference efﬁ-\nciency. We provide effective recipes that can\nguide practitioners to choose the best settings\nfor various NLU tasks and pretrained models.\nApplying the proposed recipes to the Super-\nGLUE benchmark, we achieve from 9.8x up to\n233.9x speed-up compared to out-of-the-box\nmodels on CPU. On GPU, we also achieve\nup to 12.4x speed-up with the presented meth-\nods. We show thatFastFormerscan drastically\nreduce cost of serving 100 million requests\nfrom 4,223 USD to just 18 USD on an Azure\nF16s v21 instance. This translates to a sustain-\nable runtime by reducing energy consumption\n6.9x - 125.8x according to the metrics used in\nthe SustaiNLP 2020 shared task.\n1 Introduction\nSince BERT (Devlin et al., 2018) has been in-\ntroduced, Transformer (Vaswani et al., 2017)-\nbased pretrained language models have dominated\nthe Natural Language Understanding (NLU) ﬁeld.\nTransformer models have provided unprecedented\naccuracy improvement compared to traditional\nmodels (Devlin et al., 2018; Liu et al., 2019).\nHowever, the models’ computational cost at infer-\nence time is prohibitively challenging to be widely\n1https://docs.microsoft.com/en-us/\nazure/virtual-machines/fsv2-series\nadopted in real world production scenarios which\nrequires low latency, fast inference and low serv-\ning costs. In this work, we present FastFormers,\na set of methods and recipes that provides highly\nefﬁcient inference for Transformer models which\nenables deployment in large scale production sce-\nnarios. We speciﬁcally focus on the inference time\nefﬁciency since it mostly dominates the cost of\nproduction deployment.\nMainly, we utilize three methods: Knowledge\nDistillation, Structured Pruning and Model Quanti-\nzation. First, we investigate the efﬁcacy of various\nKnowledge Distillation techniques to signiﬁcantly\nreduce the size of the models with respect to the\ndepth and hidden state sizes while preserving the\naccuracy. Second, we explore Structured Pruning\nthat further reduces the size of the models by re-\nducing the number of self-attention heads and the\nnumber of intermediate hidden states in the feed-\nforward layers to achieve more efﬁciency while\ntrying to preserve the accuracy as well. Finally, we\nexplore Model Quantization which enables faster\nmodel executions by optimally utilizing hardware\nacceleration capabilities. On CPU, 8-bit integer\nquantization method is applied to utilize the most\nefﬁcient CPU instructions available: Vector Neu-\nral Network Instructions (VNNI). On GPU, all the\nmodel parameters are converted into 16-bit ﬂoat-\ning point data type to maximally utilize efﬁcient\nTensor Cores. Furthermore, computational graph\noptimizations which fuse multiple graph nodes are\nperformed by utilizing onnxruntime2 library. In\naddition, we explore optimal settings of allocating\nCPU and GPU resources to achieve better utiliza-\ntion.\nThe proposed methods are optimized and eval-\nuated on both CPUs and GPUs which are the\nmost commonly available hardware platforms. Per-\n2https://github.com/microsoft/\nonnxruntime\n150\nformance evaluations are conducted on Super-\nGLUE (Wang et al., 2019) which is one of the\ngeneral purpose open domain NLU benchmarks.\nFor the efﬁciency measurement, wall clock times\nand energy efﬁciency are measured while per-\nforming inferences on the test sets of BoolQ, CB,\nCOPA, MultiRC, ReCoRD, RTE and WiC tasks\nfrom SuperGLUE. The energy efﬁciency is mea-\nsured by an open source python library called\nexperiment-impact-tracker proposed in (Henderson\net al., 2020). To make sure the optimized models\npreserve similar accuracy, the accuracy of all the\nmodels is measured together.\nThe contributions of this paper are presenting a\nset of recipes for efﬁcient inference of Transformer\nNLU models, analyzing the effect of various opti-\nmization techniques and ﬁnally making the code\npublicly available 3 to facilitate utilizing FastForm-\ners for efﬁcient inference of Transformers models.\nThe rest of the paper is organized as follows:\nSection 2 presents Knowledge Distillation tech-\nniques, Section 3 presents Structured Pruning tech-\nniques, Section 4 discusses Model Quantization ap-\nproaches, Section 5 presents runtime optimization\ntechniques, Section 6 presents results on various\ntasks and ﬁnally Section 7 concludes the ﬁndings\nand future directions.\n2 Knowledge Distillation\nKnowledge distillation (Hinton et al., 2015) is a\nwell known model compression technique where\nthe large model is used as a teacher for a smaller\nstudent model. The knowledge distillation is the\nprocess of training the student model to mimic\nthe behaviour of the larger teacher model. Knowl-\nedge distillation has been shown to improve the\nefﬁciency of the Transformer-based architectures\nfor the NLU tasks (Sanh et al., 2019; Jiao et al.,\n2019) as well as natural language generation tasks\nsuch as machine translation (Kim et al., 2019).\nKnowledge distillation methods: We utilize\ntwo different distillation approaches, namely task-\nspeciﬁc and task-agnostic distillation. In the task-\nspeciﬁc distillation, we distill ﬁne-tuned teacher\nmodels into smaller student architectures following\nthe procedure proposed by TinyBERT (Jiao et al.,\n2019). In the task-agnostic distillation approach,\nwe directly apply ﬁne-tuning on general distilled\n3https://github.com/microsoft/\nfastformers\nmodels to tune for a speciﬁc task. These two meth-\nods are illustrated in Figure 1. In the illustration,\nthe preceding number attached to each arrow means\nthe order of distillation steps. We use soft cross-\nentropy function as the knowledge distillation loss\nfunction in all our experiments as used in (Sanh\net al., 2019; Jiao et al., 2019).\nAs teacher models, we choose 12 stacked layer\nBERT(Devlin et al., 2018) and RoBERTa(Liu et al.,\n2019) models with 768 hidden state dimension and\n12 self-attention heads. This is referred asBase size\nfrom the original BERT paper. In particular, we use\nHuggingFace’s pretrained BERT and RoBERTa4\nmodels.\nIn task-speciﬁc scenario, we observe that the ini-\ntialization of the student models affects the ﬁnal\naccuracy of the distilled models. We utilize open\ndomain pre-distilled models, namely distilroberta-\nbase5 and TinyBERT6 as the initializers for the\ncorresponding student models. Those models\nhave been distilled with one of the original BERT\nmodel’s pretraining objective which is masked lan-\nguage model. So, they are not speciﬁc to any task\nand can be considered as smaller generic pretrained\nmodels.\nKnowledge distillation results: The main goal\nof the knowledge distillation process is to acquire\nthe smallest possible student model while preserv-\ning the accuracy of the teacher model. Since we\nare experimenting with various NLU tasks, the ca-\npacity of the optimal student model that preserves\naccuracy may vary with varying level of task’s difﬁ-\nculty. Therefore, we experiment with distilling vari-\nous sized student models; then, we pick the smaller\nmodel among the distilled models that can offer\nhigher accuracy than the original BERT model for\neach task.\nIn our experiments, we have observed that dis-\ntilled models do not work well when distilled to a\ndifferent model type. Therefore, we restricted our\nsetup to avoid distilling RoBERTa model to BERT\nor vice versa. The major difference between the\ntwo model groups is the input token (sub-word) em-\nbedding. We think that different input embedding\nspaces result in different output embedding spaces,\n4https://github.com/huggingface/\nTransformers\n5https://huggingface.co/\ndistilroberta-base\n6https://github.com/huawei-noah/\nPretrained-Language-Model/tree/master/\nTinyBERT\n151\n(a) Task speciﬁc distillation to general distill models\n (b) Fine-tuning of general distilled models\nFigure 1: Knowledge distillation methods\nModel BoolQ CB COPA MultiRC ReCoRD RTE WiC\nBERT† (Reference, 12L, 768) 72.7 80.7 57.0 41.8 54.9 65.7 65.6\nBERT (Teacher, 12L, 768) 75.99 87.96 64.00 42.00 64.81 69.68 72.41\nBERT (Student, 6L, 768) 76.06 90.12 69.00 37.47 64.47 68.59 71.79\nBERT (Student, 4L, 312) 72.63 87.63 64.00 36.71 33.68 64.62 65.20\nRoBERTa (Teacher, 12L, 768) 81.59 89.34 56.00 50.30 79.66 79.06 71.63\nRoBERTa (Student, 6L, 768) 75.19 90.68 57.00 42.90 67.33 66.43 65.83\nTable 1: Accuracy of teacher and student models on the validation data set for each task of SuperGLUE bench-\nmark with knowledge distillation. Model marked with † represents accuracy numbers on the test set provided by\nSustaiNLP organizers.\nand knowledge transfer with different spaces does\nnot work well.\nThe result of the knowledge distillation on the\ntasks are summarized in Table 1 together with the\nteacher models’ accuracy numbers on the valida-\ntion data sets. It also includes the accuracy values\non test data set for BERT model presented by Sus-\ntaiNLP 2020 organizers7. We train both cased and\nuncased models using both task-speciﬁc and task-\nagnostic approaches, and present the model with\nhigher accuracy values. For the more challenging\ntasks such as MultiRC and ReCoRD, we observe\nthat RoBERTa based models provide better accu-\nracy than BERT based models.\n3 Structured Pruning\nThere has been a signiﬁcant amount of research on\nmodel’s weights pruning approaches inspired by\nThe Lottery Ticket Hypothesis (Frankle and Carbin,\n2018). Furthermore, there are various papers pub-\nlished to apply this pruning strategy to Transformer\nmodels including: (Yu et al., 2019; Sanh et al.,\n2020; Gordon et al., 2020). Most of such ap-\nproaches focused on random pruning to reduce the\nnumber of parameters following the lottery ticket\n7https://sites.google.com/view/\nsustainlp2020/shared-task?authuser=0\nhypothesis. While this can reduce the size of the\nmodel on the computer storage, it may not improve\nthe inference performance since it is not focusing\non better utilization of the computing resources.\nSince our main focus in FastFormersis to improve\ninference efﬁciency, randomly pruning a subset\nof the model’s parameters may not improve per-\nformance. In this work, we focus on structured\npruning which directly reduces the computation\nrequirements.\nV oita et al. (2019); Michel et al. (2019); Hou\net al. (2020) proposed methods to prune some of\nthe heads of Multi-Head Attention (MHA) in Trans-\nformer architecture. DynaBERT (Hou et al., 2020)\nadditionally proposed pruning intermediate hidden\nstates in feed-forward layer of Transformer archi-\ntecture together with rewiring of these pruned atten-\ntion module and feed-forward layers. In the paper,\nwe deﬁne a target model size in terms of the number\nof heads and the hidden state size of feed-forward\nnetwork, and use the pruning/distillation approach\nproposed in DynaBERT as a model compression\nmethod. This effectively reduces the dimensions of\nTransformer models.\nStructured pruning methods: The ﬁrst step of\nour structured pruning method is to identify the\nleast important heads in MHA and the least im-\n152\n6h 1280\n8h 1024\n6h 768\n6h 512\n12h 3072\n40.5\n41\n41.5\n42\n42.5\n43\n43.5\n20 40 60 80 100\nValidation accuracy\n(a) MultiRC task\n6h 2048\n6h 1536\n6h 1280 8h 1024\n9h 1024\n4h 3072\n6h 1024\n51\n52\n53\n54\n55\n56\n57\n58\n59\n70 90 110 130 150\nValidation accuracy\n(b) ReCoRD task\nFigure 2: Structured pruning - inference time versus accuracy. Each data point indicates a pruned model with the\nnumber of remaining heads and the number of remaining intermediate hidden states of feed-forward layers.\nportant hidden states in the feed-forward layers.\nWe use a ﬁrst order method for computing the im-\nportance score, which utilizes the ﬁrst order gradi-\nent information proposed by Michel et al. (2019);\nSanh et al. (2020); Hou et al. (2020) instead of us-\ning magnitude based pruning (Frankle and Carbin,\n2018; Gordon et al., 2020). Before doing the impor-\ntance score computation, we add a mask variable\nto each attention head for the gradient computation\nof the heads. Next, we run forward and backward\npasses of the model on the entire validation data\nset, then the absolute values of the gradients are\naccumulated. These accumulated values are used\nas importance scores which we use to sort the im-\nportance of the heads and the intermediate hidden\nstates. Based on the target model size, we select a\ngiven number of top heads and top hidden states\nfrom the network. Once the sorting and selection\nsteps are done, we re-group and reconnect the re-\nmaining heads and hidden states which result in a\nsmaller sized model. When heads and hidden states\nare pruned, we use the same pruning ratio across\ndifferent layers. This enables further optimizations\nto work seamlessly with the pruned models. In our\nexperiments, we observed that the pruned model\ncan get better accuracy when it goes through an-\nother round of knowledge distillation; this has also\nbeen noted in Hou et al. (2020). Therefore, we do\none more knowledge distillation by using the non-\npruned model as a teacher model and the pruned\nmodel as an initializer of student model.\nStructured pruning results: We apply the struc-\ntured pruning method mainly to MultiRC and\nReCoRD tasks. For the other tasks, the test sets are\nnot that big; therefore, knowledge distillation and\nother optimizations could make the models quite\nefﬁcient. In both MultiRC and ReCoRD tasks, the\nbase model is RoBERTa based distilRoberta-base8\nwhich has 6 stacked layers with 768 hidden states,\n12 self-attention heads and 3072 hidden states in\nfeed-forward layer. The structured pruning method\nbrings better efﬁciency by trading off the accuracy.\nSo, it is important to pick a reasonable model size\nthat doesn’t compromise the accuracy of the task.\nFigure 2 presents trade-off between inference speed\nand accuracy on MultiRC and ReCoRD validation\ndata sets. For the MultiRC task, we could get 2.97x\nspeed-up while losing 1.9 point of accuracy by\npruning 50% of heads and 75% of intermediate\nhidden states from 12 heads and 3072 hidden sizes.\nFor the ReCoRD task, we got 1.95x speed-up while\ntrading off 12.1 point of accuracy by pruning 50%\nof heads and 50% of hidden states in feed-froward\nlayer. In both cases, they are still exceeding the\nteacher sized BERT model’s accuracy.\n4 Low Precision Inference\nAfter knowledge distillation and structured prun-\ning compression, models can beneﬁt form more\nefﬁcient numerical computations by quantizing the\nmodel parameters on CPU and GPU (Rodriguez\net al., 2018). It has been shown that the accuracy of\nTransformer models doesn’t get severely compro-\nmised when utilizing 8-bit or 16-bit lower precision\narithmetic (Devlin, 2017; Kim et al., 2019; Bhan-\ndare et al., 2019; Zafrir et al., 2019; Shen et al.,\n2020; Fan et al., 2020; Aji and Heaﬁeld, 2020).\nModern CPUs and GPU accelerators are capable\nof computing those lower numerical arithmetic ef-\nﬁciently. For example, Cascade Lake CPUs have a\nspecial 8-bit vector instruction set called A VX (Ad-\nvanced Vector eXtensions)-512-VNNI and V100\n8https://huggingface.co/\ndistilRoberta-base\n153\nGPUs can utilize its efﬁcient Tensor Cores with\n16-bit ﬂoating point data.\n8-bit quantized matrix multiplications on the\nCPU: 8-bit quantized matrix multiplication\nbrings a signiﬁcant amount of speed-up compared\nto 32-bit ﬂoating point arithmetic, thanks to the re-\nlieved memory bandwidth bottleneck and reduced\nnumber of CPU instructions. Efﬁcient utilization\nof the vector registers and CPU cache memories re-\nquires that the parameter weight matrix to be tiled\nand transposed in a cache efﬁcient layout. This is\nreferred as packing operation in the matrix multipli-\ncation. Packing itself is a non-negligible operation\nand repeated packing operation could cancel out all\nthe beneﬁts from the quantized matrix multiplica-\ntions. Therefore, the result of the packing operation\nneeds to be properly cached to get efﬁcient perfor-\nmance. Moreover, some of the matrix products\nshould stay 32-bit ﬂoating point to avoid repeated\npacking operations. Therefore, we do not use 8-bit\nmatrix product for the Q, K inner product because\nboth matrices are not constant. All the other ma-\ntrix products have constant weight matrix, so we\nutilize 8-bit matrix products for them with cached\nweight packing. For the 8-bit quantized matrix\nproduct API, we utilize an open source quantized\nmatrix multiplication library FBGEMM9 which we\nhave integrated into the onnxruntime framework.\nIt provides various quantization methods and a sep-\narate matrix packing functionality explicitly. We\nuse dynamic quantization method which decides\nthe quantization range of input matrix dynamically\nevery time10. This enables the quantized values\nto effectively represent all the values in the input\nmatrix. The weight matrix’ quantization range is\nselected for each column separately and the quan-\ntization range for the input matrix is selected for\nentire input tensor. In our experiments, this 8-bit\nquantization brings up to around 3.0x speed-up on\nCascade Lake CPUs for the Transformer models\nby trading off small amount of accuracy loss.\n16-bit model conversion for the GPU: V100\nGPU supports full 16-bit operations for the Trans-\nformer architecture. Also, 16-bit ﬂoating point\noperations do not require special handling of in-\nputs and outputs except for having smaller value\nranges. The impact of the numerical overﬂow due\n9https://github.com/pytorch/FBGEMM\n10https://pytorch.org/tutorials/\nrecipes/recipes/dynamic_quantization.\nhtml\nto the smaller range in 16-bit ﬂoat points is min-\nimal at inference time, so we have not observed\nany differences in accuracy. Therefore, the model\ncan be fully converted into 16-bit ﬂoating point\ndata type before the model is utilized for inference.\nThis 16-bit model conversion brings quite signiﬁ-\ncant speed gain, since the Transformer models are\nmemory bandwidth bound workload. We observe\nup to 3.53x speed-up depending on the model set-\ntings. V100 GPU also supports 8-bit quantized\narithmetic, but it is not supported with its efﬁcient\nTensor cores; hence we do not utilize 8-bit quanti-\nzation on GPUs.\n5 Runtime Optimization\nOn top of the structural and numerical optimiza-\ntions applied, we can utilize various ways to further\noptimize the computations. In particular, we focus\non multi-processing optimizations and computa-\ntional graph optimization.\nMulti-processing optimization: The evaluation\nmachine for the SustaiNLP 2020 shared task has\ntwo Cascade Lake 6248 CPUs with 40 physical\ncores. The default execution engines for Hugging-\nFace’s transformers including PyTorch11 and Ten-\nsorFlow 12 usually use all available CPU cores for\na single operator. This is not the optimal way of uti-\nlizing available CPU cores for several reasons. The\noperators in compressed Transformer architectures\nare not big enough to fully utilize the parallelism\nof 40 CPU cores. Therefore, the overheads of par-\nallelizing the operation signiﬁcantly overshadow\nthe actual gains from the parallelism. Another im-\nportant factor is that the parallelization to all cores\nreduces the cache locality and degrades the overall\nefﬁciency of CPU utilization. Therefore, we imple-\nment a multiple instance inference by leveraging\nmultiprocessing module of python13 programming\nlanguage. It is preferable to use multi-threading in-\nstead of multi-processing to avoid additional copy\nof program memory, but python’s multi-threading\ncannot really utilize multiple threads due to the\nGlobal Interpreter Lock (GIL) 14. Even with this\nlimitation, multi-instance inference with multipro-\ncessing brings much more efﬁcient computation.\n11https://github.com/pytorch/pytorch\n12https://github.com/tensorflow/\ntensorflow\n13https://docs.python.org/3/library/\nmultiprocessing.html\n14https://wiki.python.org/moin/\nGlobalInterpreterLock\n154\nNumber of inference instances Time (sec) Speed-up\nBaseline (no thread control) 433 1.00x\n1 instance (20 threads/instance) 319 1.36x\n2 instances (10 threads/instance) 243 1.78x\n4 instance (5 threads/instance) 247 1.75x\n5 instance (4 threads/instance) 255 1.70x\n10 instance (2 threads/instance) 300 1.44x\n20 instance (1 thread/instance) 351 1.23x\nTable 2: Speed comparison of different number of in-\nference instances with thread control - time to perform\ninference on 1,000 ReCoRD validation data samples.\nTo maximize the cache locality, each inference in-\nstance is pinned to speciﬁc physical cores using\nLinux’taskset command. Utilizing hyper-threading\nharms the cache utilization, so we always keep the\ntotal number of utilized threads (the number of\nthreads per one inference instance multiplied by\nthe number of instances) not exceeding the num-\nber of physical cores in the machine. The optimal\nnumber of multiple processes for the best efﬁciency\nvaries by the model, hardware settings and the data\nset. We conduct experiment with all target tasks\nand investigate the best setting for each task. Table\n2 shows one example of the speed-up achieved by\noptimized multi-instance inference on 1,000 sam-\nples of validation data set of ReCoRD task.\nComputational graph optimizations: Compu-\ntational graph optimization can further improve\nthe efﬁciency of the neural network inference by\npruning unused graph nodes and fusing multiple op-\nerations together. In particular, graph node fusion\nreduces additional memory allocation and copy\nwhich potentially degrade the efﬁciency. Also,\ngraph node fusion potentially improves parallelism\nby increasing the size of individual operators. We\nreplace Gaussian Error Linear Units (GELU) with\nRectiﬁed Linear Units (ReLU) for the computa-\ntional efﬁciency while model is distilled without\nlosing any accuracy. We fuse ReLU and bias ad-\ndition operations followed by matrix multiplica-\ntions into FBGEMM’s fused post processing op-\neration. Moreover, we utilize multi-head attention\nnode fusion provided by onnxruntime. We use a\ncustomized onnxruntime based on v1.3.1.\n6 Results\n6.1 Combined results\nTable 3 and Figure 3 present how the proposed\nmethods work together on BoolQ task using CPU.\nFor the ablation study, an Azure F16s v2 instance\nwhich has Intel(R) Xeon(R) Platinum 8168 CPUs\n(8 physical cores) is utilized. When all the opti-\nmizations are applied, it could achieve around 233x\nspeed-up while only losing 1.2 of accuracy. First,\nit is worth noting that the batch generation code in\nmany frameworks including HuggingFace’s trans-\nformers uses ﬁxed sequence length for the input.\nWhenever there comes a shorter sentence than the\nﬁxed length for the inference, additional zeros are\npadded at the end of each input sentence. This\ndegrades CPU performance greatly which has rel-\natively small parallelism than GPU. We modify\nthe batch generation to support dynamic sequence\nlength for each batch to avoid such redundant com-\nputation. By doing this dynamic shape batching,\nwe could get around 3.51x speed-up on the test\nCPU. On top of the dynamic shape batching, all\nthe optimization techniques introduced are applied.\nAs described in Section 2, knowledge distillation\nbrings a signiﬁcant speed-up which is more than 9\ntimes faster than the original teacher model while\npreserving the accuracy. One thing to note is that\nthe compressible student model size varies with\nthe task. In this BoolQ example, 4 stacked lay-\ners with smaller hidden size (312) model could\nlearn original teacher model’s knowledge without\nlosing any accuracy score on the validation data\nset. 8-bit quantization together with onnxruntime\ngraph optimization brings around 2.26x speed-up\non the 4 layer distilled model. This is lower than\n3.0x speed-up acquired when the same methods\nis applied to 12 layer base size models mentioned\nin Section 4, because the system is already com-\npressed into a smaller size. By using 8 concurrent\nand independent instances for inference instead of\none inference instance with 8 threads, more than\n1.7x additional speed-up could be achieved. Fi-\nnally, by pruning heads in multi-head attentions\nand intermediate hidden states in feed-forward lay-\ners, another speed-up from 1.38x to 1.81x could\nbe accomplished while trading off the accuracy.\nOne interesting observation is that the structured\npruning approach brings better speed-up when it is\ncombined with multi-instance inference which is\nup to 1.81x. The same pruned model brings at most\n1.26x speed-up when it is used with one inference\ninstance. This indicates that multi-instance infer-\nence gets more performance beneﬁts when each\nindividual model size gets smaller.\n155\nOptimization methods added Time (sec) Cumulative Speed-up Accuracy USD for\nspeed-up 100 M queries\nBaseline (PyTorch out-of-the-box, 12L, 768) 734.35 1.00x - 74.01 $4,223\n+ dynamic sequence length 209.29 3.51x 3.51x 74.01 $1,204\n+ knowledge distillation (4L, 312) 22.5 32.64x 9.30x 74.04 $129\n+ 8-bit quantization + graph optimization 9.97 73.66x 2.26x 73.43 $57\n+ multi-instance inference 5.68 129.29x 1.76x 73.43 $33\n+ structured pruning\n25% heads and 25% hidden states pruned 4.11 178.67x 1.38x 73.36 $24\n33% heads and 50% hidden states pruned 3.14 233.87x 1.81x 72.81 $18\nTable 3: An ablation study of CPU inference speed-up on BoolQ validation data set with batch size 1. All perfor-\nmance numbers are measured on an Azure F16s-v2 instance.\n6.2 Shared task submissions\nWe submit our optimized FastFormerssystems to\nSustaiNLP 2020 shared task; four systems for track\n1 and two systems for track 3. For track 1, we\nhave GPU-only submissions and hybrid submis-\nsions which utilize both CPUs and GPUs. We\nobserve that the inference time for ReCoRD only\nexceeds the inference time of the other tasks all\ntogether. Therefore, for the hybrid systems, we use\nGPUs for ReCoRD task inference and CPUs for all\nthe other tasks. Those CPU and GPU inferences\ncan be executed in parallel for the best throughput.\nFor track 3, we only use CPUs for all tasks as indi-\ncated in the shared task description. For the GPU\ninference, we always limit the number of GPUs\nwe utilize to one. Our highly optimized and com-\npressed models can be executed on a single GPU\nfast enough. And, the scaling of multiple GPUs\nis sub-linear (3.0x with 4 GPUs and 1.8x with 2\nGPUs) which indicates a single GPU inference is\nmost energy efﬁcient. We apply different types\nof optimization depending on the time consumed\nto run the task. For most time consuming tasks,\nMultiRC and ReCoRD, all compression and opti-\nmization techniques mentioned above are applied.\nAll GPU inference uses batch size of 256 which\ngives the highest throughput and the best efﬁciency.\nOn the other hand, a single batch works better for\nmost of the cases on CPUs. We use batch size of\n1 for all tasks except for CB (batch size of 4) and\nCOPA (batch size of 8).\nTable 4 summarizes the accuracy, speed-up num-\nbers and energy savings of FastFormerssystems\nprepared for the shared task. For GPUs, CB, COPA\nand WiC data sets are quite small, so the initial\nperformance without any optimization took 1 or 2\nseconds close to the resolution (1 second) of the\nwall clock time measurement. Therefore, it was\nhard to observe big speed improvements for those\ndata sets. For the other tasks, we acquire a good\namount of speed-up ranging from 5.8x to 12.4x\nby our optimization. On CPUs, model compres-\nsion gives more beneﬁts all across the board in\nterms of speed. The smallest gain is 9.8x for WiC\ndata set which could not utilize onnxruntime opti-\nmization due to the control ‘for’ loop in the output\nlayer. This is currently not supported in onnxrun-\ntime. For the other tasks, we observe up to 40.3X\nspeed-up. On the other hand, the energy savings\nwhile preserving BERT model accuracy measured\nby the shared task organizers range from 6.9X up to\n125.8X. When combining all the tasks, our best sys-\ntems save 22.1x energy on CPUs and 21.6x energy\non GPUs.\n7 Conclusion\nIn this paper, we have introduced FastFormers,\nwhich achieves efﬁcient inference-time perfor-\nmance for Transformer-based models on various\nNLU tasks. We showed that utilizing knowledge\ndistillation, structured pruning and numerical op-\ntimization can lead to drastic improvements on in-\nference efﬁciency. We showed that the improve-\nments can be up to 200X speed-up and results in\nmore than 200X inference cost saving with 22X\nenergy saving. We open source FastFormersfor\nthe community hoping it can drive more sustain-\nable optimizations for Transformers models. For\nfuture directions, some other methods such as early\nexiting (Xin et al., 2020; Liu et al., 2020; Schwartz\net al., 2020; Zhou et al., 2020) and linear time com-\nplexity self-attention models (Shen et al., 2018;\nWang et al., 2020) could be added to the proposed\nrecipes and possibly improve the efﬁciency further.\n156\n4.45 15.62\n145.33\n327.98\n575.70\n795.62\n1041.40\n74.01 74.01 74.04\n73.43\n73.43\n73.36\n72.81\n60\n62\n64\n66\n68\n70\n72\n74\n76\n0\n200\n400\n600\n800\n1000\n1200\nBaseline\n(Out-of-the-\nbox)\nDynamic\nsequence\nlength\nKnowledge\ndistillation\n(4L, 312)\n8-bit\nquantization\n+ graph\noptimization\nMulti-\ninstance\ninference\nStructured\npruning\n(9h, 900)\nStructured\npruning\n(8h, 600)\nAccuracy\nQueries / second\nFigure 3: Accuracy versus queries per second with various optimizations on CPU.\nSubmitted systems BoolQ CB COPA MultiRC ReCoRD RTE WiC Overall\nBERT† (Reference, 12L, 768) 72.7 80.7 57.0 41.8 54.9 65.7 65.6 62.6\nInference time speed-up 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x\nEnergy savings 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x\nSystem 1 (GPU only) 74.0 82.7 58.0 41.8 56.2 66.4 66.0 63.6\nInference time speed-up 5.8x 2.0x 2.0x 9.1x 12.4x 9.0x 2.0x 11.3x\nEnergy savings 11.0x 57.9x 125.8x 6.9x 23.9x 20.5x 28.7x 20.3x\nSystem 2 (GPU only) 74.0 82.7 58.0 43.2 56.2 66.4 66.0 63.8\nInference time speed-up 5.8x 2.0x 2.0x 9.1x 12.4x 9.0x 2.0x 11.3x\nEnergy savings 11.9x 58.3x 80.1x 8.1x 25.0x 15.9x 22.4x 21.6x\nSystem 3 (CPU/GPU hybrid) 73.7 82.7 58.0 43.0 56.2 66.9 65.9 63.8\nInference time speed-up 40.3x 22.0x 38.0x 25.0x 12.4x 22.4x 9.8x 14.5x\nEnergy savings 13.4x 49.4x 92.8x 13.8x 23.0x 32.7x 30.4x 16.5x\nSystem 4 (CPU/GPU hybrid) 73.7 82.7 58.0 43.1 56.2 66.9 65.9 63.8\nInference time speed-up 40.3x 22.0x 38.0x 17.5x 12.4x 22.4x 9.8x 14.5x\nEnergy savings 13.2x 45.8x 75.4x 11.8x 23.5x 34.0x 28.4x 16.1x\nSystem 5 (CPU only) 73.7 82.7 58.0 43.0 56.6 66.9 65.9 63.8\nInference time speed-up 40.3x 22.0x 38.0x 25.0x 15.5x 22.4x 9.8x 16.6x\nEnergy savings 14.0x 41.1x 75.0x 15.3x 22.5x 34.7x 26.2x 22.1x\nSystem 6 (CPU only) 73.7 82.7 58.0 43.1 56.6 66.9 65.9 63.8\nInference time speed-up 40.3x 22.0x 38.0x 17.5x 15.5x 22.4x 9.8x 16.1x\nEnergy savings 11.4x 22.4x 27.2x 9.2x 16.0x 85.5x 13.5x 15.6x\nTable 4: Accuracy numbers with speed-up and energy savings on the test data set of submitted systems to the\nSustaiNLP 2020 shared task. Model marked with † was accuracy numbers on the test set provided by organizers.\n157\nAcknowledgments\nWe would like to thank the organizers of SustaiNLP\n2020 for their various efforts and the reviewers for\nthe thoughtful and helpful suggestions.\nReferences\nAlham Fikri Aji and Kenneth Heaﬁeld. 2020. Com-\npressing neural machine translation models with 4-\nbit precision. In Proceedings of the Fourth Work-\nshop on Neural Generation and Translation , pages\n35–42.\nAishwarya Bhandare, Vamsi Sripathi, Deepthi\nKarkada, Vivek Menon, Sun Choi, Kushal Datta,\nand Vikram Saletore. 2019. Efﬁcient 8-bit quantiza-\ntion of transformer neural machine language trans-\nlation model. arXiv preprint arXiv:1906.00532.\nJacob Devlin. 2017. Sharp models on dull hardware:\nFast and accurate neural machine translation decod-\ning on the cpu. arXiv preprint arXiv:1705.01991.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAngela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, R ´emi Gribonval, Herv ´e J ´egou, and Ar-\nmand Joulin. 2020. Training with quantization\nnoise for extreme model compression. arXiv Prepr.\narXiv2004, 7320:1–18.\nJonathan Frankle and Michael Carbin. 2018. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. arXiv preprint arXiv:1803.03635.\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307.\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma\nBrunskill, Dan Jurafsky, and Joelle Pineau. 2020.\nTowards the systematic reporting of the energy\nand carbon footprints of machine learning. arXiv\npreprint arXiv:2002.05651.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn NIPS Deep Learning and Representation Learn-\ning Workshop.\nLu Hou, Lifeng Shang, Xin Jiang, and Qun Liu. 2020.\nDynabert: Dynamic bert with adaptive width and\ndepth. arXiv preprint arXiv:2004.04037.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heaﬁeld, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\nneural machine translation. In Proceedings of the\n3rd Workshop on Neural Generation and Transla-\ntion, pages 280–288.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nHaotang Deng, and Qi Ju. 2020. Fastbert: a self-\ndistilling bert with adaptive inference time. arXiv\npreprint arXiv:2004.02178.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems ,\npages 14014–14024.\nAndres Rodriguez, Eden Segal, Etay Meiri, Evarist\nFomenko, Y Jim Kim, Haihao Shen, and Barukh Ziv.\n2018. Lower numerical precision deep learning in-\nference and training. Intel White Paper, 3.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander M Rush.\n2020. Movement pruning: Adaptive sparsity by ﬁne-\ntuning. arXiv preprint arXiv:2005.07683.\nRoy Schwartz, Gabi Stanovsky, Swabha Swayamdipta,\nJesse Dodge, and Noah A Smith. 2020. The right\ntool for the job: Matching model and instance com-\nplexities. arXiv preprint arXiv:2004.07453.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low pre-\ncision quantization of bert. In AAAI, pages 8815–\n8821.\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai\nYi, and Hongsheng Li. 2018. Efﬁcient attention:\nAttention with linear complexities. arXiv preprint\narXiv:1812.01243.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\n158\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3266–3280.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exit-\ning for accelerating bert inference. arXiv preprint\narXiv:2004.12993.\nHaonan Yu, Sergey Edunov, Yuandong Tian, and Ari S\nMorcos. 2019. Playing the lottery with rewards\nand multiple languages: lottery tickets in rl and nlp.\narXiv preprint arXiv:1906.02768.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\narXiv preprint arXiv:1910.06188.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit.\narXiv preprint arXiv:2006.04152.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8273253440856934
    },
    {
      "name": "Computer science",
      "score": 0.8188687562942505
    },
    {
      "name": "Inference",
      "score": 0.7623584270477295
    },
    {
      "name": "Natural language understanding",
      "score": 0.5990845561027527
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.545457124710083
    },
    {
      "name": "Machine learning",
      "score": 0.4461268484592438
    },
    {
      "name": "Language model",
      "score": 0.444646954536438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42800575494766235
    },
    {
      "name": "Natural language",
      "score": 0.3460618257522583
    },
    {
      "name": "Voltage",
      "score": 0.09283134341239929
    },
    {
      "name": "Engineering",
      "score": 0.08263418078422546
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}