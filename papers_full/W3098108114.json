{
  "title": "Transformers for One-Shot Visual Imitation",
  "url": "https://openalex.org/W3098108114",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5069471785",
      "name": "Sudeep Dasari",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5101761266",
      "name": "Abhinav Gupta",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2769112066",
    "https://openalex.org/W2948199445",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963516265",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2166302491",
    "https://openalex.org/W3038245394",
    "https://openalex.org/W2963094133",
    "https://openalex.org/W3115293622",
    "https://openalex.org/W2012204020",
    "https://openalex.org/W2964161785",
    "https://openalex.org/W2158782408",
    "https://openalex.org/W2970003882",
    "https://openalex.org/W3041404693",
    "https://openalex.org/W2996086858",
    "https://openalex.org/W2990505274",
    "https://openalex.org/W2952191002",
    "https://openalex.org/W2963669336",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2970705309",
    "https://openalex.org/W2342840547",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2583137229",
    "https://openalex.org/W2805623918",
    "https://openalex.org/W2964157221",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2167224731",
    "https://openalex.org/W2937206389",
    "https://openalex.org/W3021708257",
    "https://openalex.org/W2174803659",
    "https://openalex.org/W2802726207",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W3036843665",
    "https://openalex.org/W2964122153",
    "https://openalex.org/W2970377754",
    "https://openalex.org/W1684361744",
    "https://openalex.org/W2962732055",
    "https://openalex.org/W2986616324",
    "https://openalex.org/W1998649346",
    "https://openalex.org/W2139734816",
    "https://openalex.org/W2787053496",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2952991114",
    "https://openalex.org/W3031840745",
    "https://openalex.org/W3025552214",
    "https://openalex.org/W2963937498",
    "https://openalex.org/W2160609165",
    "https://openalex.org/W3023640063",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W1986014385",
    "https://openalex.org/W2898634286",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a $\\sim 2$x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.",
  "full_text": "Transformers for One-Shot Visual Imitation\nSudeep Dasari\nRobotics Institute\nCarnegie Mellon University, USA\nsdasari@andrew.cmu.edu\nAbhinav Gupta\nRobotics Institute\nCarnegie Mellon University, USA\ngabhinav@andrew.cmu.edu\nAbstract: Humans are able to seamlessly visually imitate others, by inferring\ntheir intentions and using past experience to achieve the same end goal. In other\nwords, we can parse complex semantic knowledge from raw video and efﬁciently\ntranslate that into concrete motor control. Is it possible to give a robot this same\ncapability? Prior research in robot imitation learning has created agents which can\nacquire diverse skills from expert human operators. However, expanding these\ntechniques to work with a single positive example during test time is still an open\nchallenge. Apart from control, the difﬁculty stems from mismatches between the\ndemonstrator and robot domains. For example, objects may be placed in differ-\nent locations (e.g. kitchen layouts are different in every house). Additionally, the\ndemonstration may come from an agent with different morphology and physical\nappearance (e.g. human), so one-to-one action correspondences are not available.\nThis paper investigates techniques which allow robots to partially bridge these\ndomain gaps, using their past experience. A neural network is trained to mimic\nground truth robot actions given context video from another agent, and must gen-\neralize to unseen task instances when prompted with new videos during test time.\nWe hypothesize that our policy representations must be both context driven and\ndynamics aware in order to perform these tasks. These assumptions are baked\ninto the neural network using the Transformers attention mechanism and a self-\nsupervised inverse dynamics loss. Finally, we experimentally determine that our\nmethod accomplishes a ∼2x improvement in terms of task success rate over prior\nbaselines in a suite of one-shot manipulation tasks.1\nKeywords: Representation Learning, One-Shot Visual Imitation\n1 Introduction\nImitation is one of the most important cornerstones of intelligence. Watching other humans act,\ninferring their intentions, and attempting the same actions in our own home environments allows us\nto expand our skill set and enhance our representations of the world [1]. On the other hand, robots -\nwhile capable of imitating skills like table tennis [2] and driving [3] – are much less ﬂexible when\nit comes to visual imitation. Most prior work in robotic imitation assumes that the agent is trying\nto acquire a single skill from demonstration(s) collected kinesthetically [4] (i.e. a human manually\nguides a robot) or via tele-operation [5]. These approaches can work so long as the target test-time\ntask and environment are do not signiﬁcantly differ from those seen during training. Is it possible to\ndevelop a robotic agent which can learn to imitate without these restrictions?\nVisual imitation requires extracting a higher level goal from the visual demonstration and using the\ninferred goal to predict actions from pixels. But how does one represent goal/intention and how can\nthis contextual information be incorporated into the policy function itself? There are three primary\napproaches in prior work: the ﬁrst approach is to represent goals/intentions as pixels by generating\ngoal images, and then inferring actions given current observations and inferred goals [6, 7]. While\nthis approach is intuitive and interpretable, it is difﬁcult to generate pixels, in a way that respects\nstructural differences in the image. Figure 1 shows an example with well deﬁned task semantics,\nbut where a change in object positions makes it difﬁcult to visually map the human state to the\n1For code and project video please check our website: https://oneshotfeatures.github.io/\n4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA.\narXiv:2011.05970v1  [cs.LG]  11 Nov 2020\nFigure 1: What should the robot do given video from another demonstration agent? A human would\nimmediately know to place the red triangle on the blue square, and can use their past experience to\nexecute the task. Is it possible to teach a robot to do the same?\nrobot environment. The second approach has been to model visual imitation as a one-shot learning\nproblem [8], which can be solved with meta-learning algorithms [9]. Here, a robot is given a single\nexample, in the form of a video or demonstration (e.g. video + control telemetry), and must use\nthat information to perform new instances of the same task. The demonstration is used to update the\nparameters of a policy function and the updated policy is executed on the robot. Domain gaps can\nbe addressed with a learned adaptive los function [10]. While the one-shot formalism is very useful,\nestimating policy parameters from a single example can be an extremely difﬁcult problem and prone\nto over-ﬁtting.\nIn this paper, we explore a third alternative: task-driven features for one-shot learning. We process\nboth observations from the target agent and demonstrations frames from a ”teacher” agent in order\nto extract context-conditioned state representations. What neural network architectures can create\ntask-driven features? While in the past, approaches such as LSTMs have been used, in this work, we\nfocus on self-attention architectures. In particular, the Transformers architecture - while simple - has\nseen broad success in NLP [11] and Vision [12] tasks. Furthermore, using attention for control tasks\nhas has basis in biology and psychology. Indeed, humans use attention mechanisms to create context\ndriven representations [13], and directly supervising policies with human attention can dramatically\nincrease task performance [14].\nIn this paper, we propose using transformers [11] (or non-local self-attention modules [12]) to ex-\ntract relational features which act as input state vectors for the policy function. Our transformers\ntake as input both the spatial ResNet Features from teacher demonstration and the target agent. This\nallows the policy to automatically adapt its features to the task at hand, by using context frames to\nfocus only on important task-speciﬁc details. For example, in Figure 1 the robot could use human\ncontext frames to focus only on relevant details like the red block’s location, and entirely ignore dis-\ntracting elements like the table’s leg. However, transformer features could easily end up improperly\nweighting important details during test time. We propose to solve this issue by further supervising\nthe state representation learning with an unsupervised inverse dynamics loss. This loss constrains\nthe learning problem and ensures the ﬁnal representations can model the underlying dynamics, as\nwell as task speciﬁc details. Ultimately, our method achieves signiﬁcant improvements over one-\nshot imitation learning baselines on a suite of pick and place tasks: our ﬁnal policies demonstrate a\n2x performance gain and can match baseline performance with 3x fewer data-points.\n2 Related Work\nLearning from Demonstration (LfD) is a rich and diverse ﬁeld of study which focuses on enabling\nrobots to learn skills from human or other expert demonstrations. A thorough review is out of scope\nfor this paper, so we gladly refer the reader to survey articles [15, 16, 17]. Of prior work, Behavior\nCloning (BC) [18, 19], a common formulation of LfD, is most related to our project. BC involves\nimitating an expert agent given a set of trajectories (a.k.a time series of observations and actions),\nby ﬁtting a function which approximates the expert’s action in a given state. This simple formulae\nhas proven successful in imitating a wide range of behaviors from visual inputs, including robotic\nmanipulation tasks [20] and driving [21]. These methods have been extended to situations where\n2\nexpert observations are present without action labels [22], including prior work which linked this\nproblem to inverse dynamics minimization [23]. However, both of these approaches require the\ndemonstration agent match the imitator.\nBC algorithms often assume that they are approximating a single state conditioned policy. In an\nenvironment with multiple tasks or multiple variations of the same task, this constraint can be lim-\niting. Work on goal conditioned imitation learning seeks to relax these assumptions by allowing\nfor policies which condition on a goal variable alongside the current state, and adjust their behav-\nior accordingly. There are myriad ways to introduce goal conditioning, including with the robot’s\nstate [24], ”goal” images of the ﬁnal state [25, 26, 27], natural language [28], and video or images\nof humans [29, 30]. In our project, we assume the robot has a single video of another agent (be it\nanother robot or a human) doing a task, and must complete that same task itself using past expe-\nrience. This is a speciﬁc instance of the one-shot learning problem [8], and has been investigated\nbefore previously using meta-learning with an adaptive loss [10]. Instead of using meta-learning,\nwe propose to attack this problem with an attention mechanism over image frames.\nA challenge in this line of work is learning visual representations which can enable the robot to\ndeduce the task from video of another agent and perform the task itself. Work in computer vision\ndemonstrated that deep neural networks are capable of learning such ﬂexible representations for ac-\ntion recognition [31] and state estimation [32], but often require large image datasets to fully train.\nUnfortunately, collecting ImageNet [33] scale datasets on robotics platforms is prohibitively expen-\nsive, due to the cost of continuous robot operation and hardware fragility. Work in self-supervised\nlearning [34, 35, 36] offers a glimmer of hope, by showing how large and (relatively) cheap sets\nof unlabelled images can be used to learn expressive and useful representations for other down-\nstream tasks. These representations could be used directly as reward functions [37, 38], but it can\nbe very difﬁcult to deﬁne rewards for a suite of tasks. Instead, unsupervised learning techniques\nalongside simple data augmentation can be used to increase data efﬁciency when directly acquiring\npolicies with reinforcement learning [39, 40, 41]. Even simpler self-supervised losses - like inverse\nmodelling (i.e. predicting action between two sequential states) - can be used to learn robust poli-\ncies which adapt to new environments [42]. Our goal in this project is to apply these insights in\nrepresentation learning to the one-shot imitation learning problem.\n3 Our Method\n3.1 Problem Deﬁnition\nOur method follows prior work [9, 10], and formalizes the one-shot imitation learning problem as\nsupervised behavior cloning on a data-set of tasks. For each task T (e.g. place blue bottle in bin),\nwe have several demonstration videos and target trajectories. Note that the demonstration videos\nand target trajectories are semantically similar tasks but could have different starting/end states. We\nrepresent each demonstration video as vi and each target trajectory, ti, as a temporal sequence of\nobservations (o) and actions (a). Hence, ti = {(o(1)\ni ,a(1)\ni ),..., (o(k)\ni ,a(k)\ni )}.\nModels are trained on a dataset of tasks D= {T1,..., Tn}. During test time, new test tasks - Ttest\n- are sampled which the model must successfully control the imitator agent to perform. Thus, all\nmethods are evaluated on task success rates in held out environments. Our setup is challenging\nbecause: (a) morphological differences between demonstration and target agent (e.g. one is hu-\nman and other is robot arm); (b) missing correspondence between demonstration videos and target\ntrajectories.\n3.2 Feature Learning with Transformers\nGiven video context from a demonstrator agent and image frames from the test environment, our\nrepresentation module must deduce relevant features and efﬁciently pass them on to later stages of\nthe pipeline for action prediction. For example, when given a video of a green bottle being dropped\nin a bin, the vision module should detect and represent the green bottle in its own environment\nwhile ignoring other distracting objects. We propose to learn this mechanism end-to-end using self-\nattention Transformer modules [11], in the hope that this powerful inductive bias helps the policy\nperform tasks successfully.\n3\nFigure 2: Our method uses a Transformer neural network to create task-speciﬁc representations,\ngiven context and observation features computed with ResNet-18 (w/ added positional encoding).\nThe attention network is trained end-to-end with a behavior cloning loss, an inverse modelling loss,\nand an optional point loss supervising the robot’s future pixel location in the image.\nBefore the attention module, individual images from both the context video and current state are\npassed through a ResNet-18 architecture [43], and spatial features (size[512,T,H,W ]) are collected\nbefore the average pooling step. At this stage, the features are ﬂattened (size [512,T ∗H ∗W])\nand sinusoidal positional encodings [11] are added to the tensor (i.e. time and space treated as\nsingle dimension). These embeddings can allow neural networks to represent higher frequency\nfunctions [44], and we empirically found that they were crucial to preserving spatial and temporal\ninformation in the attention module. After adding positional encodings, the features are reshaped to\ntheir original size.\nNext, the non-local multi-head attention operator is applied to the input tensor. We adopt a speciﬁc\nimplementation of the Transformers self-attention module presented in Wang et al. [12], which we\naugment with multi-headed self-attention [11]. First, the module generates Key, Query, and Value\ntensors by applying three separate 3D spatio-temporal convolutions (we use kernel size k = 1 )\nwith ReLU activation to the input tensor. To be clear, each convolution layer’s input and output\nare [d,T,H,W ] tensors, where dis the Transformer’s embedding size. These generated key, query,\nand value tensors are then ﬂattened and projected down nseparate times - once for each attention\n“head” - before attention is applied (ﬁnal shape per head[d,T ∗H∗W]). The self-attention operator\nis applied to each head individually. Considering attention head j, temperature parameter τ, and\nprojected tensors Kj,Qj,Vj, this amounts to:\nAj = softmax(KT\nj Qj/τ) V(out)\nj = VjAj\nThe individual attention heads are then concatenated together channel-wise, and then pro-\njected back to the original 512 dimension size with another 3D convolution ( O =\nConv3D(concat[V(out)\n1 ,...,V (out)\nn ])). Note that this multi-head attention operator can be im-\nplemented with little overhead using batched matrix multiplication. Dropout [45], then a resid-\nual connection, and ﬁnally batch normalization [46] are applied to get the ﬁnal output f(x) =\nbatchnorm(x+ dropout(O)), with ﬁnal size [512,T,H,W ]. In order to appropriately apply this\nto behavior cloning (where ot+1 is not known during test time), we make this operation causal by\nappropriately padding the 3D convolution operators and masking the attention.\n3.3 Goal Conditioned Behavior Cloning\nAs discussed previously, our objective is to learn a policy π(at|o1:t,v) which ingests the current\n(or optionally all previous) state observations alongside a context video, and predicts a distribution\nover possible actions the expert policy would select. We process the input video stream with stacked\n4\nattention modules to yield ﬁxed size spatial features, with one feature map per time-step. The fea-\ntures are projected down to a ﬁxed size representation vector using a spatial softmax operator [47],\nfollowed by a multi-layer perceptron with ReLU activations, and ﬁnally L2 normalization to unit\nlength. This representation φt = F(o1:T,v) is used for action prediction.\nMulti-Modal Action Prediction: One of the most naive ways to predict π(at|o1:t,v) from φt is\nto simply parameterize the policy as a normal distribution π(at|o1:t,v) = N(µ(φt),σ(φt)), and\nto sample actions from that. However, this approach can run into severe limitations when the real\nexpert distribution is multi-modal. Consider a robot attempting to top-down lift a cup by its handle.\nRotating the gripper by 90 ◦or -90◦, but not rotating at all (i.e. the mean action) would result in\ntask failure since the gripper would close on top of the handle. Prior work [20, 48, 26] showed\nthis limitation matters in practice, and rectiﬁes the situation by predicting a mixture of uni-modal\ndistributions. We adopt the same solution used by Lynch et al. [26]. First, we discretize the action\nspace (discussed in detail in Section 4.1) and then parameterize the policy as a discretized logistic\nmixture distribution [49]. For each timestep, we predict klogistic distributions with separate mean\nand scale, and form a mixture by convexly weighting them with vector α. The behavior cloning\ntraining loss is simply negative log-likelihood for this distribution:\nLBC(D,θ) = −ln(Σk\ni=0 αk(φt) P(at,µi(φt),σi(φt))\nWhere, P(at,µi(φt),σi(φt)) = F(at+0.5−µi(φt)\nσi(φt) )−F(at−0.5−µi(φt)\nσi(φt) ) and F(·) is the logistic CDF.\nDuring test time, actions are simply sampled from the distribution and executed on the robot with-\nout rounding. For most of our experiments, the model performed best when using two mixture\ncomponents and learned constant variance parameters per action dimension.\n3.4 Inverse Model Regularizer\nOur method also adds a self-supervised inverse modeling objective to act as a regularizer to the be-\nhavior cloning loss during training. Context and trajectory snippets are sampled from the dataset,\nand images in them are randomized with sampled translations, color shifts, and crops. This random-\nization is applied consistently to frames from the context video, whereas images from the agent’s\nobservation stream (a.k.a trajectory images) are randomized individually. This randomized image\nstream is passed through the attention and representation modules to generate ˜φt. The representa-\ntions ˜φt and ˜φt+1 are used to predict a discretized logistic mixture distribution over intermediate\nactions. Thus, the inverse loss is:\nLINV (D,θ) = −ln(Σk\ni=0 αk(˜φt,˜φt+1) logistic(µi(˜φt,˜φt+1),σi(˜φt,˜φt+1)))\nWe share parameters between the behavior cloning and inverse modeling objectives for the attention\nmodule, representation module, and distribution prediction heads (i.e. after ﬁrst layer). In practice,\nwe use the randomized image stream for both tasks as well, in order to minimize memory consump-\ntion.\n3.5 Point Prediction Auxiliary Loss\nFinally, our model uses φt to predict a 2D keypoint location corresponding to the location of the\ngripper in the image H timesteps in the future. Ground truth for this auxiliary loss is easy to ac-\nquire given either a calibrated camera matrix or object detector trained on the robot gripper. One\ncould instead predict the 3D gripper position in world coordinates if neither is available. While\nnot strictly needed for control, this loss is very valuable during debugging, since it lets us visu-\nally check during training if the model understand where the robot ought to be H timesteps in the\nfuture. The point prediction is parameterized with a simple multi-variate 2D normal distribution\nˆpt+H ∼N(µ(φt),Σ(φt)) with loss Lpnt(D,θ) = −ln(likelihood(pt+H,ˆpt+H)). Thus, the overall\nloss for our method is:\nL(D,θ) = λBC LBC(D,θ) + λINV LINV (D,θ) + λpnt Lpnt(D,θ)\n4 Experimental Results\nOur model is evaluated on robotic manipulation tasks - namely pick and place tasks - in simulation\nusing multi-agent MuJoCo [50] environments. Our evaluations investigate the following questions:\n5\nModel Reaching Success Picking Success Placing/Overall Success\nOur Method 99.4% ± 1.2% 92.5% ± 4.1% 88.8% ± 5.0%\nContextual-LSTM 38.8% ± 7.6% 26.3% ± 6.9% 23.8% ± 6.7%\nDAML [10] 36.9% ± 7.6% 10.6% ± 4.8% 6.9% ± 4.0%\nDAML Auxiliary 47.8% ± 7.4% 17.8% ± 5.6% 13.3% ± 5.0%\nTable 1: Comparison between our method and baselines in 16 pick and place tasks. Values indicate\nsuccess rates and 95% conﬁdence intervals for “stages”2 in the overall pick and place task.\n(1) can our model perform new task instances (deﬁned in 4.1) previously unseen during training?\nAnd (2) what components (e.g. inverse loss, etc.) are most crucial for successful control?\n4.1 Simulation Environment and Tasks\nFigure 3: Our base environment is\nadopted from RoboTurk [51]. The\n16 tasks consist of taking an object\n(a-b) to a bin (1-4). Top robot is\nagent and bottom is demonstrator.\nEnvironment Description: The environments we use are\nmodiﬁed variants of those originally presented in Robo-\nTurk [51]. Visually, thebase environment- shown in Figure 3 -\nis the exact same as the original from RoboTurk, except the ob-\nject meshes are replaced with primitive geometric types (e.g.\nboxes and cylinders) in order to improve simulation contact\nstability and run-time. This modiﬁcation results in only mi-\nnor visual differences. In order to investigate visual imitation\nacross agent morphology, we use duplicate versions of the en-\nvironment with two visually distinct robots. The Sawyer robot\n(red robot in Figure 3) provides demonstration videos and the\nPanda robot (white robot in Figure 3) acts as the agent which\nour model must control. Both environment’s action spaces are\nmodiﬁed to support end-effector control. Given a target x,y,z\nposition, rotation in axis-angle form, and gripper joint angle\nthe environment solves for desired robot joint angles with in-\nverse kinematics and sends joint velocities to the robot using\na simple PD controller. Thus, the ﬁnal action space consists\nof a target pose discretized into 256 independent bins per di-\nmension in order to support our behavior cloning loss. It’s\nimportant to note that the demonstrations we train on do not cover the whole state space, so the\nrobot is mostly constrained to 3-DOF movement.\nTask Deﬁnition: A “task instance” consists of picking an object from a speciﬁc start location -\nuniformly distributed on the table in Fig. 3 - and placing the object in one of the four bins on the right.\nTask instances are grouped into “tasks” based on shared properties. For example, picking a milk\ncarton (from Fig. 3) and placing it into bin 1 is a task, and different task instances are constructed by\nchanging the carton’s start position. This precise deﬁnition allows us to collect a suite of train task\ninstances, train models on that data, and test generalization to new task instances.\nData Collection Methodology: Training data is collected using an expert pick-place policy (built\nusing privileged information from the simulator) in the target environment(s). For each task (T) we\nrepeatedly, sample a demonstration video ( vi) by executing the expert policy on the Sawyer robot,\nthen shufﬂe the objects, and sample an expert trajectory ( ti) by executing the expert policy on the\nPanda robot. This way a dataset of tasks is formed from individual task instances.\n4.2 Baseline Comparisons\nOur investigation begins by evaluating our method’s performance in16 tasks in the base environment\n(Figure 3). We seek to determine the robot’s physical competency at manipulating all four objects,\nas well as its ability to deduce which task it should perform from context video. A natural way\nto quantify this is by breaking down the 16 pick and place tasks into “reach,” “pick,” and “place”\n6\nstages2, and reporting success rates on each stage individually. Failure modes can be successfully\ndeduced from these rates. For example, since reaching is a physically easy task, if the robot does\nnot reach the object then it is likely unable to deduce the target object from the context frames.\nFurthermore, if the robot reaches the object but is unable to pick it up, its physical dexterity (or lack\nthereof) is likely to blame.\nWe collect 100 train task instances using the methodology described previously for each of the 16\ntasks. That amounts to 1600 total demonstration videos alongside 1600 expert robot trajectories.\nWe train our method on the dataset and compare against the following baselines:\n• Contextual-LSTM: This baseline utilizes a standard Encoder-Decoder LSTM [52, 53]\n(augmented with self-attention [54, 55]), to ﬁrst consume the context video, and then pre-\ndict actions from encoded observations. It uses the same mixture distribution our model\nuses. Before LSTM processing, images frames are embedded using a pre-trained ResNet-\n18 [43] neural net combined with spatial-softmax [47] and fully-connected layers. The\nwhole network is trained end-to-end with a behavior cloning loss.\n• Domain Adaptive Meta-Learning: DAML [10] uses a learned loss function to adapt a\nneural network’s parameters to perform the desired task. We used a wider version of the\nnetwork used in the original paper, since we found that using deeper models (like ResNet-\n18) resulted in overﬁtting on this task. To increase performance, the same discrete logistic\naction distribution is used. DAML is trained end-to-end with the MAML meta-learning\nalgorithm [56] using a behavior cloning loss, along with explicit supervision of the pick\nand drop locations.\n• DAML-Auxiliary: This method uses the same meta-learning model described above, ex-\ncept only the predicted pick and place locations are used during test time. Given this\nprediction, a grasp motion is executed in the environment using a hard coded grasp policy.\nFor each of the 16 tasks, the models are prompted to perform new task instances (unseen during\ntraining) using freshly generated context videos. Success rates for our method and baselines (av-\neraged across tasks) are shown in Table 1. As you can see, our method is the only one which can\nreliably perform new task instances. Its overall success rate is double the competing models’ reach-\ning success rate, including the DAML-auxiliary model which makes strong task assumptions, and\nthe LSTM model which uses embedding level attention. The LSTM baseline’s (which uses standard\nattention) relative failure supports our hypothesis that the Transformer architecture uniquely enables\ndifﬁcult visual processing. For additional experiments testing generalization to new objects (i.e. new\ntasks instead of new task instances) refer to Appendix A.1.\n4.3 Architecture Ablation\nWhile the our model clearly outperforms the other baselines, it is unclear if the Transformers archi-\ntecture or additional losses deserve more credit. To test this thoroughly, the Transformers model is\ntested against an ablated version of itself without the attention mechanism (i.e. just temporal-spatial\nconvolutions) using the same base environment comparison described before. Furthermore, models\nare trained with various versions of the baseline neural network architectures, alongside the addi-\ntional loss terms. Speciﬁcally, 4 baseline architectures are considered: 2 of them adopt the small\nconvolutional network used in prior work [10, 57] either with or without an additional LSTM [52] on\ntop, and the other 2 use ResNet features [43] (again with or without LSTM). Note all architectures\nwere tuned to maximize their own test performance rather than to match some other metric (e.g.\nnumber of parameters), since doing so often led to worse results for the baseline (e.g. larger LSTMs\noverﬁt more than Transformers). Results are presented in Figure 4. The key takeaways are encour-\naging. First, the Transformers architecture (w/ attention) outperforms a library of other architectures\nfor this task by large margins, even using the same losses. Furthermore, the baselines perform better\nwhen trained with the additional losses compared to being trained purely with a behavior cloning\nloss as done before (contextual-LSTM’s success rate improves20% →40%).\n2Reaching is deﬁned as placing the gripper within < 0.03 units from the target object, picking requires\nstably lifting the object > 0.05 units off ground, and placing requires putting the object in the correct bin (a.k.a\ntask completion)\n7\nFigure 4: Our Transformer model is compared\nagainst other neural networks (all trained w/ our\nlosses and code) to determine how useful the at-\ntention mechanism really is. The Transformer\narchitecture outperforms all others, including a\nversion of itself w/out attention.\nFigure 5: We compute success rate v.s number\nof train samples for our method and versions\nwith one loss excluded (all w/ Transformer).\nNote the model without inverse loss is usu-\nally outperformed when compared to its peers\ntrained on the same data.\n4.4 Loss Function and Method Ablations\nGiven that our training losses/code boosted baseline architecture performance compared to using\njust behavior cloning, we now seek to test exactly which component was most useful. It’s entirely\npossible that some of the additional parts offer more utility in the ”low-data” regime where over-\nﬁtting is more likely, and thus are less useful when more data is present. Thus, we collect two more\nversions of the base environment dataset with fewer samples (480 and 800 samples pairs), and train\nthree ablations - one model without the inverse loss, one without the point loss, and one without data\naugmentation - alongside our base model on all three datasets (two new sets + original). That results\nin a total of 12 models, all of which we evaluate in the same manner as before. Overall success rates\nfor all models are in Figure 5. Note that the model without the inverse loss is outperformed by its\ncounterparts in two out of three datasets, whereas the point loss only makes a signiﬁcant difference\nin the smallest dataset. Indeed as the number of datapoints increases, so does the importance of\nthe inverse loss: the model without inverse loss is more than 25% worse than its counterparts in\nthe N = 1600 case! While the inverse loss clearly makes a difference, this cannot be observed\nas “positive transfer” in the behavior cloning train/test loss (see Appendix A.2). This suggests\ninverse loss regularization helps test time performance in ways not captured in the training objective.\nFinally, conditioning our policy on context video proved to be more effective than just feeding it the\nlast frame, which indicates the demonstration helps our model determine which task to perform\ncompared to using a “goal image” frame. For more check Appendix A.3.\n5 Discussion\nIn this project we explore the one-shot visual imitation learning problem. Our experiments highlight\ntwo technical contributions - applying the Transformers architecture to one-shot imitation tasks and\na self-supervised inverse modelling objective - which both result in large performance gains over\nbaseline one-shot imitation learning approaches. More speciﬁcally, our ablations show that our\nmodel trained without the self-supervised inverse loss performs signiﬁcantly worse when compared\nto other versions with the inverse loss, and all of our Tansformers models (even without inverse loss)\noutperform a Seq2Seq LSTM trained with traditional “embedding level” attention mechanisms by\nroughly 2x.\nThe main takeaway here is that injecting the right biases - both in terms of network design and the\nloss function - can help policies perform better during test-time. We believe that the Transformer’s\nattention mechanism provides such a bias by allowing for task conditioned representations, whereas\nthe inverse model forces the policy to preserve information which is needed for robust control during\ntest time. We hope that these ﬁndings prove useful to others working on one-shot imitation learning\nand goal conditioned reinforcement learning in general.\n8\nAcknowledgments\nWe’d like to begin by acknowledging the students and collaborators at CMU who gave valuable\nfeedback which made the ﬁnal paper much stronger. In particular, we’d like to recognize Sam\nPowers, Devendra Chaplot, Kenneth Marino, Adithya Murali, and Shubham Tulsiani. Finally, this\nresearch was funded by ONR MURI, the ONR Young Investigator Award to Abhinav Gupta and the\nDAPRA MCS award.\nReferences\n[1] S. V ogt and R. Thomaschke. From visuo-motor interactions to imitation learning: behavioural and brain\nimaging studies. Journal of Sports Sciences, 25(5):497–517, 2007.\n[2] K. M ¨ulling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in\nrobot table tennis. The International Journal of Robotics Research, 32(3):263–279, 2013.\n[3] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural\ninformation processing systems, pages 305–313, 1989.\n[4] P. Pastor, L. Righetti, M. Kalakrishnan, and S. Schaal. Online movement adaptation based on previous\nsensor experiences. In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n365–371. IEEE, 2011.\n[5] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imitation learning for\ncomplex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 1–8. IEEE, 2018.\n[6] P. Sharma, D. Pathak, and A. Gupta. Third-person visual imitation learning via decoupled hierarchical\ncontroller. In Advances in Neural Information Processing Systems, pages 2597–2607, 2019.\n[7] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine. Avid: Learning multi-stage tasks via pixel-\nlevel translation of human videos. arXiv preprint arXiv:1912.04443, 2019.\n[8] Y . Duan, M. Andrychowicz, B. Stadie, O. J. Ho, J. Schneider, I. Sutskever, P. Abbeel, and W. Zaremba.\nOne-shot imitation learning. In Advances in neural information processing systems , pages 1087–1098,\n2017.\n[9] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via meta-learning.\nIn Conference on Robot Learning, pages 357–368, 2017.\n[10] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation from observing\nhumans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\n[12] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[13] C. A. Rothkopf, D. H. Ballard, and M. M. Hayhoe. Task and context determine where you look. Journal\nof vision, 7(14):16–16, 2007.\n[14] R. Zhang, Z. Liu, L. Zhang, J. A. Whritner, K. S. Muller, M. M. Hayhoe, and D. H. Ballard. Agil:\nLearning attention from human for visuomotor tasks. In Proceedings of the european conference on\ncomputer vision (eccv), pages 663–679, 2018.\n[15] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration.\nRobotics and autonomous systems, 57(5):469–483, 2009.\n[16] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot programming by demonstration.\nHandbook of robotics, 59(BOOK CHAP), 2008.\n[17] S. Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3(6):233–242,\n1999.\n[18] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-\nregret online learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence\nand statistics, pages 627–635, 2011.\n[19] M. Bain. A framework for behavioural cloning. In Machine Intelligence 15, pages 103–129, 1995.\n[20] R. Rahmatizadeh, P. Abolghasemi, L. B ¨ol¨oni, and S. Levine. Vision-based multi-task manipulation for\ninexpensive robots using end-to-end learning from demonstration. In2018 IEEE International Conference\non Robotics and Automation (ICRA), pages 3758–3765. IEEE, 2018.\n[21] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort,\nU. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316,\n2016.\n[22] F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. arXiv preprint\narXiv:1805.01954, 2018.\n[23] C. Yang, X. Ma, W. Huang, F. Sun, H. Liu, J. Huang, and C. Gan. Imitation learning from observations\nby minimizing inverse dynamics disagreement. In Advances in Neural Information Processing Systems,\npages 239–249, 2019.\n9\n[24] Y . Ding, C. Florensa, P. Abbeel, and M. Phielipp. Goal-conditioned imitation learning. In Advances in\nNeural Information Processing Systems, pages 15324–15335, 2019.\n[25] A. Mandlekar, F. Ramos, B. Boots, L. Fei-Fei, A. Garg, and D. Fox. Iris: Implicit reinforcement\nwithout interaction at scale for learning control from ofﬂine robot manipulation data. arXiv preprint\narXiv:1911.05321, 2019.\n[26] C. Lynch, M. Khansari, T. Xiao, V . Kumar, J. Tompson, S. Levine, and P. Sermanet. Learning latent plans\nfrom play. In Conference on Robot Learning, pages 1113–1132, 2020.\n[27] J. Fu, A. Singh, D. Ghosh, L. Yang, and S. Levine. Variational inverse control with events: A general\nframework for data-driven reward deﬁnition. In Advances in Neural Information Processing Systems ,\npages 8538–8547, 2018.\n[28] C. Lynch and P. Sermanet. Grounding language in play. arXiv preprint arXiv:2005.07648, 2020.\n[29] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg. Concept2robot: Learning manipulation concepts\nfrom instructions and human demonstrations. In Proceedings of Robotics: Science and Systems (RSS) ,\nJuly 2020.\n[30] A. Xie, F. Ebert, S. Levine, and C. Finn. Improvisation through physical understanding: Using novel\nobjects as tools with visual foresight. arXiv preprint arXiv:1904.05538, 2019.\n[31] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In\nAdvances in neural information processing systems, pages 568–576, 2014.\n[32] T. E. Lee, J. Tremblay, T. To, J. Cheng, T. Mosier, O. Kroemer, D. Fox, and S. Birchﬁeld. Camera-to-robot\npose estimation from a single image. arXiv preprint arXiv:1911.09231, 2019.\n[33] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee,\n2009.\n[34] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 9729–9738, 2020.\n[35] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. arXiv preprint arXiv:2002.05709, 2020.\n[36] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D.\nGuo, M. G. Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv\npreprint arXiv:2006.07733, 2020.\n[37] P. Sermanet, K. Xu, and S. Levine. Unsupervised perceptual rewards for imitation learning.arXiv preprint\narXiv:1612.06699, 2016.\n[38] P. Sermanet, C. Lynch, Y . Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain. Time-contrastive\nnetworks: Self-supervised learning from video. In 2018 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 1134–1141. IEEE, 2018.\n[39] A. Srinivas, M. Laskin, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforcement\nlearning. arXiv preprint arXiv:2004.04136, 2020.\n[40] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with aug-\nmented data. arXiv preprint arXiv:2004.14990, 2020.\n[41] I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforce-\nment learning from pixels. arXiv preprint arXiv:2004.13649, 2020.\n[42] N. Hansen, Y . Sun, P. Abbeel, A. A. Efros, L. Pinto, and X. Wang. Self-supervised policy adaptation\nduring deployment. arXiv preprint arXiv:2007.04309, 2020.\n[43] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[44] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi,\nJ. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional\ndomains. arXiv preprint arXiv:2006.10739, 2020.\n[45] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to\nprevent neural networks from overﬁtting. The journal of machine learning research , 15(1):1929–1958,\n2014.\n[46] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[47] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The\nJournal of Machine Learning Research, 17(1):1334–1373, 2016.\n[48] R. Rahmatizadeh, P. Abolghasemi, A. Behal, and L. B ¨ol¨oni. From virtual demonstration to real-world\nmanipulation using lstm and mdn. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[49] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma. Pixelcnn++: Improving the pixelcnn with dis-\ncretized logistic mixture likelihood and other modiﬁcations. arXiv preprint arXiv:1701.05517, 2017.\n[50] E. Todorov, T. Erez, and Y . Tassa. Mujoco: A physics engine for model-based control. In2012 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012.\n[51] A. Mandlekar, Y . Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, E. Orbay,\net al. Roboturk: A crowdsourcing platform for robotic skill learning through imitation. arXiv preprint\narXiv:1811.02790, 2018.\n10\n[52] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n[53] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks. InAdvances\nin neural information processing systems, pages 3104–3112, 2014.\n[54] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473, 2014.\n[55] J. Cheng, L. Dong, and M. Lapata. Long short-term memory-networks for machine reading. arXiv\npreprint arXiv:1601.06733, 2016.\n[56] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.\narXiv preprint arXiv:1703.03400, 2017.\n[57] A. Zhou, E. Jang, D. Kappler, A. Herzog, M. Khansari, P. Wohlhart, Y . Bai, M. Kalakrishnan, S. Levine,\nand C. Finn. Watch, try, learn: Meta-learning from demonstrations and reward. arXiv preprint\narXiv:1906.03352, 2019.\n11\nA Appendix\nA.1 Baseline Comparisons: Multi-Object Environments\nWhile the prior experiments showed our model could successfully generalize to new task instances,\ncan it also generalize to new tasks including unseen objects? To answer this question the baseline\ncomparisons (described in Section 4.2) are repeated in environments with multiple objects. Impor-\ntantly, the objects used during test time are unseen during training.\nEnvironment Description: The multi-object environmentis cloned from the base environment (pre-\nsented in Section 4.1) and modiﬁed to include more objects with different shapes and textures. Note\nthat while object appearance and shape is randomized, dynamical properties - like friction - are kept\nconstant since they cannot be visually judged. The simulator has 30 unique objects, 26 of which are\nseen during training and 4 are only used during test time.\nData Collection Process: To collect train tasks, 4 objects are sampled from the 26 train objects,\nwhich results in an environment with 16 tasks. For each task, multiple task instances composed of\nexpert demonstration videos (vi) and imitator trajectories (ti) are collected using the same method-\nology as before (refer to Section 4.2 and Section 4.1). In total, the train dataset is composed of\n1200 tasks (2400 task instances total). Test tasks are also sampled in the same fashion as before,\nexcept using the 4 new objects. Our method is able to succeed at the object picking stage of the tasks\n50 ±9.9% of the time which is ∼2x better than the best baseline (contextual-LSTM) which only\npicks 23 ±8.4% of the time. Unfortunately, all methods (including ours) often place objects in the\nwrong bin resulting in ﬁnal success rates of 23 ±8.4% for our method and 22 ±8.3% for the best\nbaseline. In practice, this failure mode is easy to rectify since a hard coded policy will always place\nthe object in the right bin. Encouragingly, our policy is best at grasping and picking unseen objects\nwhich is the hardest part of this task. Nonetheless, this failure mode shows more improvements are\nneeded for this method to work in broader settings.\n12\nFigure 6: One hypothesis is that the ablated models fail at test time because they cannot optimize the\nbehavior cloning loss. Comparing train and val loss for models trained on the same data (N=1600)\neliminates this possibility.\nA.2 Regularization Effect on Behavior Cloning Loss\nWhile the inverse model regularization term clearly changed test time performance for the better\n(shown in Section 4.4), can this be explained by positive transfer to the behavior cloning task? In\nother words, it is possible the inverse modelling loss merely prevents over-ﬁtting in the behavior\ncloning loss, and thus some other regularization term could achieve the same effect.\nTo test this theory, we plot behavior cloning loss (both training and validation) vs train iteration\nfor both the base model, and ablation models from Section 4.4. Note that behavior cloning train\nperformance is nearly identical, whereas ﬁnal success rates are dramatically different. We believe\nthese facts in tandem conﬁrm that self-supervised inverse modeling forces our representation to\ncapture information which is useful for robust test performance, but not necessary to minimize the\ncloning loss.\n13\nA.3 Time-Step Ablation\nInstead of using a context video from the demonstrator agent to infer the task, our model could\njust use the last frame from the demonstration video. After all, the last frame should uniquely\nspecify which object should go in which bin, and prior work [27] has successfully used goal image\nconditioning. To test this, we train a version of our model which conditions just on the ﬁnal frame\nfrom the context video, and compare its performance on the benchmarks from Section 4.2. This\nmodiﬁed model achieves a ﬁnal success rate of 61 ±9.7% which is signiﬁcantly less than the 88 ±\n5.0% our model (which ingests more frames from context) can achieve. This effect holds even if the\nbase model uses just one extra context frames (i.e. both beginning and end frame). We hypothesize\nthat these frames, while not strictly necessary, help the infer which task it needs to perform, thus\nresulting in a performance boost.\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7280296087265015
    },
    {
      "name": "Robot",
      "score": 0.6338455080986023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6241745352745056
    },
    {
      "name": "Transformer",
      "score": 0.5291932821273804
    },
    {
      "name": "Imitation",
      "score": 0.45740842819213867
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4177750051021576
    },
    {
      "name": "Task (project management)",
      "score": 0.4107345938682556
    },
    {
      "name": "Artificial neural network",
      "score": 0.4100201725959778
    },
    {
      "name": "Machine learning",
      "score": 0.35781314969062805
    },
    {
      "name": "Engineering",
      "score": 0.1218702495098114
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 7
}