{
  "title": "Pula: Training Large Language Models for Setswana",
  "url": "https://openalex.org/W4411119481",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2025004176",
      "name": "Nathan Brown",
      "affiliations": [
        "Impact"
      ]
    },
    {
      "id": "https://openalex.org/A2308997331",
      "name": "Vukosi Marivate",
      "affiliations": [
        "Impact"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3193636580"
  ],
  "abstract": null,
  "full_text": "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 6634–6656\nApril 29 - May 4, 2025 ©2025 Association for Computational Linguistics\nPula: Training Large Language Models for Setswana\nNathan Brown\nData Science for Social Impact\nUniversity of Pretoria\nSchool of Computing\nClemson University\nnbrown9@clemson.edu\nVukosi Marivate\nData Science for Social Impact\nDepartment of Computer Science\nUniversity of Pretoria\nLelapa AI\nvukosi.marivate@cs.up.ac.za\nAbstract\nIn this work we present Pula, a suite of\nbilingual language models proﬁcient in both\nSetswana and English. Leveraging recent ad-\nvancements in data availability and efﬁcient\nﬁne-tuning, Pula 8B and Pula 14B outper-\nform GPT-4o and Gemini 1.5 Pro on English-\nSetswana translation tasks and achieve state-\nof-the-art performance on Setswana reason-\ning tasks for their size. We release the\nweights for Pula 1B, 3B, 8B, and 14B as well\nas training logs and training and evaluation\ncode. Alongside Pula, we release the largest-\never Setswana text corpus, Marothodi, and\nthe ﬁrst comprehensive Setswana instruction-\ntuning dataset, Medupi, consisting of refor-\nmatted datasets, translated corpora, and syn-\nthetic LLM-generated text. To accompany this\ndata, we release the code used for dataset con-\nstruction, formatting, ﬁltering, and scraping.\nLast, we release two Setswana LLM-translated\nbenchmarks, MMLU-tsn and GSM8K-tsn, to\nmeasure Setswana knowledge and reasoning\ncapabilities.\n1 Introduction\nSetswana, also known as Tswana, is a Bantu lan-\nguage spoken by an estimated ﬁve to ten million\npeople worldwide ( Bennett et al. , 2016). Closely\nrelated to Northern Sotho and Southern Sotho,\nSetswana holds ofﬁcial status in Botswana ( Gov-\nernment of Botswana , 2024), South Africa ( The\nRepublic of South Africa , 1996), and Zimbabwe\n(The Parliament of Zimbabwe , 2013), and is also\nused in countries like Namibia, often interchange-\nably with English ( Norris, 2017). Despite its sig-\nniﬁcance in the lives of millions, Setswana has\nbeen largely overlooked in natural language pro-\ncessing (NLP) research, despite being classiﬁed by\nsome works as a high-resource language ( NLLB\nTeam et al. , 2022). This work aims to bridge\nthe gap between Setswana and other high-resource\nlanguages by making open generative large lan-\nguage models capable of high-quality Setswana\navailable to the community for the ﬁrst time, sig-\nniﬁcantly increasing data availability, and laying\nthe groundwork for future Setswana-centric re-\nsearch.\nLarge Language Models (LLMs) have demon-\nstrated powerful capabilities across various do-\nmains after training on web and synthetic data\n(OpenAI et al. , 2023; Dubey et al. , 2024; An-\nthropic, 2024; Gunasekar et al. , 2023), excelling\nin areas such as mathematics ( Mistral AI , 2024),\nprogramming ( Guo et al. , 2024), creative writing\n(Wang et al. , 2024), and translation tasks ( Vaswani\net al. , 2017). However, developers continue to\nprimarily target English and certain high-resource\nlanguages in training and evaluation. While exist-\ning approaches yield impressive capabilities, they\nmay produce models which lack knowledge of cer-\ntain cultures, limit production use-cases outside\nmajority demographics, or prevent a signiﬁcant\nportion of the global population from utilizing lan-\nguage models effectively. African languages like\nSetswana, with little textual data available com-\npared to languages such as English or French, sub-\nsequently suffer in performance and are underuti-\nlized in research.\nRecent progress has been made to address the\nlack of language diversity in language models. Re-\nleases such as mBART ( Liu et al. , 2020), XLM-\nRoBERTa ( Conneau et al. , 2019), and BLOOM\n(BigScience Workshop et al. , 2022) were among\nthe earliest and most inﬂuential advancements in\nmultilingual language models. Building upon\nthese technologies, newer models including GPT-\n4 ( OpenAI et al. , 2023), Claude ( Anthropic, 2024),\nGemini ( Gemini Team et al. , 2023), Llama ( Dubey\net al. , 2024), and Gemma ( Gemma Team et al. ,\n2024) have also found success in multilingual do-\nmains, often demonstrating reasoning and trans-\nlation capabilities in languages not ofﬁcially sup-\n6634\nported. Releases such as Aya 101 ( Üstün et al. ,\n2024) and Aya 23 ( Aryabumi et al. , 2024) have\ncontinued to improve language coverage for trans-\nlation and generative tasks, and open corpora\nsuch as ROOTS ( Laurençon et al. , 2023), OSCAR\n(Suárez et al. , 2019), and mC4 ( Caswell et al. ,\n2021) have made multilingual pre-training data\nreadily available. However, Setswana comprises\nonly a small fraction of these datasets. For in-\nstance, just 0.0002% of the ROOTS corpus is writ-\nten in Setswana. Moreover, much of the avail-\nable Setswana text in open multilingual corpora\nis of lower quality or predominantly religious in\nnature, resulting in signiﬁcantly worse conversa-\ntional, translation, and reasoning capabilities in\ncurrent open models.\nTo help address this issue, we introduce the\nPula series of language models. This series con-\nsists of LoRA ( Hu et al. , 2021) and QLoRA\n(Dettmers et al. , 2023) ﬁne-tuned versions of\nLlama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B, and\nQwen 2.5 14B ( Yang et al. , 2024; Dubey et al. ,\n2024). By training a range of models across\ndifferent parameter counts, we aim to provide\nthe research community and millions of Setswana\nspeakers with models that are both highly perfor-\nmant and capable of running on various hardware\nconﬁgurations ranging from data centers to con-\nsumer laptops and mobile phones. To fuel the\ntraining behind these models and to provide the\nresearch community with resources to further im-\nprove future models and research, we hand-curate\nthe largest-ever corpus of Setswana text. In doing\nso, we merge several existing corpora which have\nnot yet been consolidated, restore document-level\ntext in certain data subsets, locate new sources of\nSetswana text which to our knowledge have not\nyet been utilized in NLP research, develop sev-\neral dedicated scrapers and parsers to obtain this\nnew data, reformat existing datasets to function as\ninstruction-tuning corpora, translate existing En-\nglish instruction-tuning datasets to Setswana using\nstate-of-the-art large language models and trans-\nlation models, and utilize GPT-4o for the genera-\ntion of entirely new synthetic Setswana text. We\nname our pre-training dataset Marothodi and our\ninstruction-tuning dataset Medupi.\nFollowing the open approach of projects like\nOLMo ( Groeneveld et al. , 2024) and Dolma ( Sol-\ndaini et al. , 2024), we perform a fully open re-\nlease. This includes model weights, all train-\ning data, metadata on data sources, training logs,\nand code for training, evaluation, dataset curation,\ndataset translation, synthetic data generation, and\nweb scraping. All model weights and data can be\naccessed on Hugging Face 1. All code and training\nlogs can be accessed on GitHub 2.\n2 Related Work\nAlthough much of current NLP research is\nEnglish-centric, there have been signiﬁcant re-\ncent advancements in Setswana-centric language\nmodels and data access. TswanaBERT ( Mot-\nsoehli, 2020) represents one of the earliest exam-\nples, trained on over ten thousand Setswana sen-\ntences from the Leipzig Corpora Collection ( Gold-\nhahn et al. , 2012), SABC news headlines ( Mari-\nvate et al. , 2020), and various blogs and websites.\nMore recently, the NCHLT Setswana RoBERTa\nmodel ( Eiselen et al. , 2023) was released, hav-\ning been trained on over fourteen million tokens\nof Setswana text from the NCHLT ( Eiselen and\nPuttkammer, 2014), Autshumato ( McKellar et al. ,\n2016), Leipzig ( Goldhahn et al. , 2012), and Com-\nmon Crawl corpora, and an internal CTexT cor-\npus. PuoBERTa ( Marivate et al. , 2023a) marked\na signiﬁcant step forward in masked language\nmodeling, achieving state-of-the-art performance\nwhile being the ﬁrst language model trained from\nscratch. PuoBERTa was released alongside Puo-\nData, the largest collection of curated Setswana\ntext at the time, totaling 4.5 million PuoBERTa to-\nkens excluding JW300 ( Agi´c and Vuli ´c, 2019).\nMuch of the literature on African languages\nhas targeted massively multilingual developments\nwith a focus on machine translation and transfer\nlearning. Corpora such as OPUS ( Tiedemann,\n2012a) and MADLAD-400 ( Kudugunta et al. ,\n2023) provide access to large volumes of paral-\nlel web text data, while work such as MAFAND-\nMT ( Adelani et al. , 2022a) has enabled improved\ntranslation performance across many African lan-\nguages through additional human-generated data.\nMeta’s No Language Left Behind (NLLB) ( NLLB\nTeam et al. , 2022) has facilitated high-quality ma-\nchine translation between over 200 languages, in-\ncluding Setswana, although it is noted as one of\nthe 21 languages with the lowest accuracy on\nFLORES-200 ( Goyal et al. , 2022). MADLAD-\n400 ( Kudugunta et al. , 2023) has allowed for in-\n1https://huggingface.co/collections/\nOxxoCodes/pula-66af1106ccc0fb38839f39da\n2https://github.com/OxxoCodes/Pula\n6635\ncreased multilinguality, but we ﬁnd suffers from\nworse performance on Setswana-English transla-\ntions as seen in Table 5. Furthermore, models such\nas AfriBERTa ( Ogueji et al. , 2021) and AfroX-\nLMR ( Alabi et al. , 2022) have seen success in\ntraining masked language models across multiple\nAfrican and low-resource languages, and LlamaX\n(Lu et al. , 2024) has demonstrated high degrees of\ntranslation performance across over 100 languages\nas a LLM while retaining generalization.\nThanks to the cumulative improvements made\nby the research community in Setswana-centric\nNLP and increased levels of multilinguality, we be-\nlieve there are signiﬁcant opportunities to enhance\nexisting Setswana NLP systems. Through our in-\nvestigations we ﬁnd several smaller corpora of\nSetswana text are often underutilized in the litera-\nture, potentially due to differing distribution meth-\nods and vastly differing formats, making data dif-\nﬁcult to locate and utilize. For example, the South\nAfrican Center for Digital Language Resources 3\n(SADiLaR, 2024) has made publicly available sev-\neral datasets of Setswana text and audio. However,\nthese texts are not distributed in a standardized for-\nmat and are not typically available on commonly\nused external platforms such as Hugging Face and\nKaggle. As such, much of this data is left out\nof existing corpora. We also ﬁnd many websites\nhosting content written in Setswana are excluded\nfrom existing training datasets, meaning a signiﬁ-\ncant portion of the available high-quality, and es-\npecially educational, Setswana data is not being\nutilized. Through the development and release of\nMarothodi and Medupi, we aim to reduce this bur-\nden and make data signiﬁcantly easier to access.\n3 Data\nOne key consideration in Pula’s design and sub-\nsequent training data selection is identifying tar-\nget languages. There are several parallel corpora\navailable for Setswana, and some works such as\nOPUS contain Setswana text paired alongside mul-\ntiple other languages. However, signiﬁcantly more\nEnglish-Setswana parallel text is available. For ex-\nample, OPUS contains over four times more par-\nallel Setswana-English sentences than Setswana-\nFrench sentences 4. In addition, many publicly\navailable, commonly used, high-quality, educa-\n3https://sadilar.org\n4https://opus.nlpl.eu/results/tn&en/\ncorpus-result-table\nMedupiOPUS\nMarothodi\nMADLAD-400\nPuoDataLeipzig\nGlotCC V1.0\n100\n101\n102\nTotal Tokens (Millions)\nFigure 1: Logarithmic comparison of total tokens per\ncorpus, as measured using Llama 3.1’s tokenizer.\ntional, and instruction-tuning datasets are written\npredominantly in English. Consequently, many\nstate-of-the-art LLMs already excel in English\ntasks, including the Llama and Qwen series of\nmodels which we select for continued pre-training.\nAdditionally, native speakers in regions such as\nBotswana and South Africa often utilize English\nin legal, ofﬁcial, and government documents, with\nEnglish sometimes being spoken interchangeably\nwith Setswana ( Parliament of Botswana , 2022).\nWe observe this trend continued in web text doc-\numents, where many data sources are written ei-\nther exclusively in Setswana or in both Setswana\nand English, either incorporating code-switching\nor providing direct translations ( TRK, 2021).\nDue to the strong link between these two lan-\nguages and in an effort to reduce scope and compu-\ntational requirements, we focus our efforts primar-\nily on curating a high-quality dataset consisting of\nSetswana and English texts. We refer to our cor-\npus of raw Setswana text as Marothodi, and our\ninstruction-tuning dataset as Medupi.\nOur datasets build upon several prior works in\nthe African NLP research community. We mea-\nsure token counts using the Llama 3.1 tokenizer\nthroughout this paper for consistency. We release\nboth Marothodi and Medupi in their entirety on\nHugging Face 5, as well as per-sequence identiﬁ-\ncation metadata including a corpus identiﬁer and\nan exact source URL where available.\n5https://huggingface.co/collections/\nOxxoCodes/pula-66af1106ccc0fb38839f39da\n6636\nSource Data Category Tokens Description\nOther Corpora Existing Datasets 21,783,242 Bloom-lm, GlotCC, MADLAD-400, Vuk’uzenzele, etc.\nEducational Material Web Scraping 21,567,283 Exams, Quizzes, Books\nTinyStories-tsn Translated 13,327,017 Setswana translated subset of TinyStories\nSADiLaR Existing Datasets 4,770,707 Transcripts, ASR/TTS\nSetswana Rare Books Web Scraping/OCR 4,428,724 Old books, high quality\nGovernmental Documents Web Scraping 4,222,174 Legal texts\nSetswana Bible Web Scraping 1,712,827 Religious texts\nSetswana Wikipedia Restored 1,043,992 Broad knowledge, high quality\nMiscellaneous Websites Web Scraping 565,562 iAfrika, Setswana Mo Botswana, Tlhalefang, Unisa\nMiscellaneous Documents Web Scraping 327,429 Declaration of Human Rights, Intro to Setswana, etc.\nNalibali Restored, PDF Extraction 203,685 Children’s stories, educational\nTable 1: Key Data Sources in Marothodi.\n3.1 Marothodi\nMotivation. Marothodi is built with two goals in\nmind:\n1. To dramatically increase the amount of pub-\nlicly available Setswana text through manual\ncuration and ﬁltering.\n2. To signiﬁcantly increase ease of access for\nthe African NLP research community and the\ngeneral public.\nWe believe a signiﬁcant limitation in Setswana-\ncentric NLP is the sparsity of published data: re-\nsources are scattered across various publishers\nsuch as Hugging Face, SADiLaR, various mas-\nsive web crawls, public websites, and cloud stor-\nage providers. As a result, researchers who are\nnot deeply familiar with the current Setswana\ndata landscape or do not have ample time to al-\nlocate toward data collection are severely limited\nin data diversity, quality, and quantity. To assist\nin resolving this problem, we develop Marothodi:\na Setswana pre-training dataset pulling together\ndata from 33 different sources totaling 74 mil-\nlion tokens. Marothodi contains nearly 9 times\nas many tokens as are present in PuoData, mak-\ning Marothodi the largest available single corpus\nof Setswana text.\nData Sources. We develop Marothodi by selec-\ntively including various underutilized data sources.\nWe prioritize document integrity and rich con-\ntext by utilizing complete documents from vari-\nous sources. To aid in this effort, we restore the\noriginal documents of certain subsets of PuoData,\nincluding Wikipedia ( Foundation), Nalibali ( Nal-\nibali, 2024), and the Setswana Bible. In addi-\ntion, we further increase the available Nalibali data\nby performing PDF text extraction on previously\nunutilized children’s stories and educational mate-\nrials.\nData Sources. Prior works predominantly fo-\ncus on certain commonly utilized Setswana cor-\npora, limiting downstream model generalization.\nMarothodi addresses this by directly targeting a\nrich selection of diverse, manually curated sources.\nWe identify web sources containing Setswana text\nand develop individual programs to scrape the data\nas needed. We found many of these sources con-\ntain previously underutilized Setswana text in the\nform of PDFs, making text extraction an impor-\ntant part of Marothodi. We extract text from less\nreadily accessible collections which include edu-\ncational material ( National Education Collabora-\ntion Trust , 2024; Thutong South African Educa-\ntion Portal , 2024; Lingua, 2024), governmental\ndocuments 6, and rare books ( Rahlao et al. , 2021).\nFurthermore, we include miscellaneous individ-\nual documents such as the Setswana Universal\nDeclaration of Human Rights ( Nations, 1998) and\nthe United States Peace Corps’ Intro to Spoken\nSetswana ( Mistry and Gare , 1987), the latter of\nwhich required additional image processing, opti-\ncal character recognition (OCR) with Florence-2\n(Xiao et al. , 2023), and textual reformatting using\nLlama 3.1 70B.\nWe scrape the contents of various websites\ncontaining either Setswana or code-switched\nSetswana-English text, such as iAfrika ( iAfrika,\n2024), Setswana Mo Botswana ( TRK, 2021), Tl-\nhalefang Communications ( Tlhalefang, 2009), and\nthe University of South Africa ( Unisa, 2023). We\nalso include a small corpus of parallel text con-\n6https://www.parliament.gov.bw/index.\nphp?\n6637\nData Category Sources Tokens Description\nParallel Texts OPUS, Autshumato, MAFAND-MT, PolyNews,\nCaLMQA, SIB-200, MSFT Terms, xP3x, SADiLaR\n178,017,061 Setswana-English parallel texts for\ntranslation; OPUS heavily ﬁltered\nTranslated Corpora OpenHermes 2.5, WildChat 1M, Dolly, Magpie Ul-\ntra, UltraChat 200k, The Tome, MURI-IT\n50,844,875 Machine Translated, LLM Translated\nAugmented Tasks MasakhaNER, NCHLT, Daily News Dikgang15,607436 NER, POS, News Classiﬁcation, and\nLemmatization\nSynthetic Data GPT-4o ( FineWeb-seeded) 2,030,883 Synthetic LLM-Generated Text\nTable 2: Key Data Types and Curation Methods in Medupi.\nsisting of monolingual English mathematical text\ntranslated into code-mixed English and Setswana\n(Mokoka, 2024).\nMoreover, we include ﬁve separate corpora\nfrom SADiLaR in Marothodi. This includes a\ncorpus of multilingual code-switched soap opera\nspeech ( van der Westhuizen and Niesler , n.d.),\ntranscripts from a high-quality corpus of Setswana\ntext-to-speech data ( Google and University , 2017),\ntranscripts from the Lwazi Setswana ASR and\nTTS corpora ( van Heerden et al. , n.d.; van Niek-\nerk et al. , n.d.; van Niekerk and Schlünz , n.d.),\nand transcripts from the NCHLT Setswana Aux-\niliary Speech Corpus ( de Wet et al. , n.d.). We\nalso incorporate smaller existing corpora into\nMarothodi including Bloom-lm ( SIL Global - AI ,\n2022), GlotCC ( Kargaran et al. , 2024), HPLT\n(Aulamo et al. , 2023), Vuk’uzenzele ( Lastrucci\net al. , 2023; Marivate et al. , 2023b), OpenSLR\nSLR32 ( van Niekerk et al. , 2017), and MADLAD-\n400 ( Kudugunta et al. , 2023)\nFinally, to further increase the size of our cor-\npus, we employ machine translation using Meta’s\nNLLB-200-3.3B (NLLB Team et al. , 2022) to\ntranslate subsets of TinyStories ( Eldan and Li ,\n2023) to Setswana. We found through evaluat-\ning round-trip translations ( Moon et al. , 2020) that\nTinyStories’ simplistic vocabulary made for gener-\nally higher-quality translations compared to more\neducational text such as FineWeb-edu ( Penedo\net al. , 2024).\n3.2 Medupi\nMotivation To our knowledge, no comprehen-\nsive instruction-tuning or chat-style dataset exists\nwritten in Setswana. One notable exception is\nMURI-IT ( Köksal et al. , 2024), although it is com-\nparatively small with content largely focusing on\ntranslation-only tasks. Given the lack of available\ndata to pull from and the costs associated with\ncreating human-written corpora, Medupi places\na strong focus on data augmentation. In doing\nso, we demonstrate that meaningful performance\nimprovements can be made for certain languages\nwithout substantial ﬁnancial costs.\nMedupi consists of data sourced in three ways:\n1. Augmenting existing corpora such as paral-\nlel texts, Named Entity Recognition (NER),\nand Part-of-Speech tagging (POS) to ﬁt the\nexpected user-assistant format.\n2. Translating existing corpora using NLLB 200\n3.3B, GPT-4o, Gemini 1.5 Pro, and quantized\nLlama 3.1 405B.\n3. Synthetic text generation with GPT-4o.\nMedupi’s largest source of Setswana text is\nthe OPUS corpus ( Tiedemann, 2012a), which in\ntotal includes over six million parallel English-\nSetswana sentences across ﬁve corpora ( Schwenk\net al. , 2021; Fan et al. , 2021; El-Kishky et al. , 2020,\n2021; Tiedemann, 2012b,b; Tatoeba, 2024). How-\never, we ﬁnd including this corpus in its entirety in\nMedupi tends to yield catastrophic forgetting. We\nattribute this to OPUS’s large size relative to the\nrest of Medupi, where training a model on such\nan imbalanced dataset hinders its generalization\ncapabilities. We also identify many low-quality\nEnglish-Setswana parallel sentences, which may\nfurther contribute toward catastrophic forgetting.\nTo mitigate these issues, we translate every En-\nglish sequence to Setswana using NLLB 3.3B and\ncalculate the CHRF score, comparing the trans-\nlated Setswana text and the OPUS Setswana se-\nquence. We then ﬁlter the entirety of this corpus\nto only include sequences with the top 33% of\nCHRF scores. In doing so, we yield a subset of\nOPUS whose parallel sequences have greater lev-\nels of agreement with existing translation systems\nand which we ﬁnd tend to be of greater quality.\nDue to data availability, much of Medupi’s\ndata sources target translation tasks. In addi-\n6638\ntion to OPUS, we include Autshumato ( Groe-\nnewald and du Plooy , 2010), MAFAND-MT’s\ntraining split ( Adelani et al. , 2022a), PolyNews-\nParallel ( Iana et al. , 2024), CaLMQA ( Arora\net al. , 2024), SIB-200 ( Adelani et al. , 2023), Mi-\ncrosoft Terms ( Microsoft, 2022), xP3x ( Muen-\nnighoff et al. , 2022), MURI-IT ( Köksal et al. ,\n2024), and various SADiLaR corpora ( City of\nTshwane et al. , 2021; van Dyk , 2021; Puttkam-\nmer and Hocking , 2021). Each example is for-\nmatted to mimic a user-assistant interaction (e.g.\n\"Can you translate the following from English\nto Setswana...\"). To further discourage overﬁt-\nting and to ensure prompt diversity, we randomize\nthe language and verbiage in the system prompt,\nwhether the source text is provided before or af-\nter the user query, the translation direction, and\nthe language provided in the user query for all\ndatasets where applicable. We found through pre-\nliminary results these efforts to be effective in\nmitigating catastrophic forgetting while improving\ntranslation performance.\nWe perform data augmentation on a variety of\nother data sources. This includes Daily News\nDikgang for news classiﬁcation ( Marivate et al. ,\n2023a), MasakhaNER 2.0 for NER ( Adelani et al. ,\n2022b), and the SADiLaR NCHLT Setswana An-\nnotated Text Corpora for lemmatization and POS\ntagging ( Puttkammer et al. , 2021). We similarly\nrandomize aspects of the system and user prompt\nwhere applicable to ensure data diversity.\nWhile data augmentation is important for data\ndiversity in low-resource scenarios, there are still\nfew non-translation sources we could easily adapt\nfor Medupi. As such, we look toward machine\ntranslation - a process which has seen prior suc-\ncess in training language models ( Doshi et al. ,\n2024). We utilize NLLB 200 3.3B to translate\n15,200 examples from OpenHermes-2.5 ( Teknium,\n2023) and 5,200 examples from WildChat-1M\n(Zhao et al. , 2024). To encourage multi-turn con-\nversations, we ﬁlter WildChat-1M to include ex-\namples with at least three turns, and exclude toxic\nand non-English content. Both datasets are further\nﬁltered after translation to remove sequences with\nsigns of low-quality translations such as repetitive\ntext or certain non-Latin characters.\nWhile machine translation models are useful for\ntranslating simpler text, we note some limitations\nwith existing models. First, machine translation\nmodels do not preserve the original formatting of\nthe text. Second, we ﬁnd these models are prone\nto output the input text verbatim when the input\ntext contains code or other technical jargon, indi-\ncating formats such as these may be outside the\noriginal training distribution. Together, these lim-\nitations make it difﬁcult to take advantage of ma-\nchine translation models when the input text con-\ntains code, bulleted lists, mathematical equations,\ntabular data, and data of highly technical nature.\nTo obtain additional high-quality Setswana chat-\nstyle data while avoiding these problems, we rely\non translation using GPT-4o, Gemini 1.5 Pro, and\nLlama 3.1 405B. We avoid using a single oracle\nmodel and instead opt for several teachers to in-\ncrease diversity with the hope of increased perfor-\nmance ( Odumakinde et al. , 2024). Speciﬁcally, we\nuse GPT-4o to translate a subset of Dolly ( Conover\net al. , 2023), Magpie ( Xu et al. , 2024), and Ultra-\nChat 200k ( Ding et al. , 2023; Tunstall et al. , 2023),\nGemini 1.5 Pro for a subset of OpenHermes 2.5\n(Teknium, 2023), and we use both Gemini 1.5 Pro\nand AWQ INT4 quantized Llama 3.1 405B for sep-\narate subsets of The Tome ( Arcee AI , 2024). We\nutilize the same ﬁlters as previously discussed for\nNLLB to remove low-quality translations. In addi-\ntion, we remove instances where the LLM corrupts\nthe format of the translated conversation, such as\nadditional hallucinated system prompts and incor-\nrect user-assistant turn order. This was signiﬁ-\ncantly more common with Llama 3.1 405B, which\nwe attribute to the high degree of quantization.\nTo further improve the writing quality of the\nPula models and experiment with purely synthetic\nSetswana text, we utilize gpt-4o-2024-05-13 (Ope-\nnAI, 2024) to generate 7,860 pieces of text cov-\nering diverse topics and styles. To promote data\ndiversity, we seed this process with a random sub-\nset of FineWeb and FineWeb-edu ( Penedo et al. ,\n2024) and prompt Llama 3 70B to suggest ﬁve\nsynopses of related writings while identifying the\nﬁve most unique words from each seed text. We\nthen cross-reference these words with Google Re-\nsearch’s Setswana-English GATITOS ( Google Re-\nsearch, 2024) to ﬁlter for quality, and construct\nGPT-4o prompts that combine a system instruc-\ntion, a randomly selected synopsis, and the cor-\nresponding word pairs with a requirement that\neach Setswana word appears in the output. We\nﬁnd this methodology allows GPT-4o to generate\nhigh-quality Setswana writings while maintaining\na high degree of diversity between texts.\n6639\nParameter Pula 1B Pula 3B Pula 8B Pula 14B\nGPUs 8 8 8 8\nMax Seq Length 4096 tokens 4096 tokens 4096 tokens 2048 tokens\nLoRA Alpha 32 32 32 16\nLoRA Dropout 0.2 0.2 0.2 0.2\nLoRA Rank 64 64 64 32\nBias None None None None\nPrecision bf16 bf16 bf16 bf16\nOptimizer AdamW 8bit AdamW 8bit AdamW 8bit AdamW 8bit\nWeight Decay 0.0 0.0 0.0 0.0\nWarmup Ratio 0.05 0.05 0.05 0.05\nLearning Rate 2e-05 2e-05 2e-05 2e-05\nEmbed Learning Rate 8e-06 8e-06 8e-06 8e-06\nLR Scheduler Cosine Cosine Cosine Cosine\nEpochs 3.0 3.0 3.0 3.0\nPacking ✓ ✓ ✓ ✓\nPer-Device Batch Size 1 1 1 1\nAccumulation Steps 8 8 8 1\nEffective Batch Size 64 (260k toks) 64 (260k toks) 64 (260k toks) 8 (16k toks)\nTable 3: Training Hyperparameters\n4 Training\nTraining Setup. We train all Pula models using\nDeepSpeed ( Rasley et al. , 2020) with ZeRO Stage\n3 ( Rajbhandari et al. , 2020) and eight NVIDIA\nH100 GPUs for three epochs on an altered combi-\nnation of Marothodi and Medupi, totaling approx-\nimately 2.3 billion tokens. Our training procedure\nleverages the Hugging Face transformers, trl, and\npeft libraries. We develop Pula via continued pre-\ntraining, where each model is trained with a large\nwarmup ratio and a lower embedding learning rate\nto ensure robust learning without catastrophic for-\ngetting.\nTo train the Pula suite of LLMs effectively\nwhile reducing computational demands, we apply\nLow Rank Adaptation (LoRA) ( Hu et al. , 2021)\nand QLoRA ( Dettmers et al. , 2023) across mul-\ntiple projection matrices including Query, Key,\nValue, Output, Gate, Up, and Down. In addi-\ntion, we maintain full-precision training on the\nlanguage modeling head and embedding layers to\nachieve improved performance when adapting our\nmodels to Setswana. This approach allows Pula\nto efﬁciently learn Setswana without prohibitive\ncomputational or memory costs, and it helps to re-\nduce the risk of overﬁtting and catastrophic forget-\nting ( Biderman et al. , 2024). We train Pula 1B,\n3B, and 8B using LoRA on sequences up to 4096\ntokens, whereas Pula 14B is trained using 4-bit\nQLoRA on sequences up to 2048 tokens in length.\nUniﬁed Data Mixture. Typical LLM training\ninvolves a two-phase pre- and post-training ap-\nproach, where pre-training or continued pre-\ntraining is followed by supervised ﬁne-tuning\n(SFT) ( Ouyang et al. , 2022). Instead, we train\non a mixture of raw text and instruction data by\ncombining Marothodi, our webtext corpus, with\nMedupi, our instruction-tuning dataset. This dual\ntraining strategy not only mitigates the increased\ncomputational overhead associated with separate\npre-training and post-training phases, but also al-\nlows Pula to beneﬁt from the SFT and reinforce-\nment learning beneﬁts already present in the post-\ntrained Llama and Qwen models.\nAugmenting Setswana Reasoning and Multilin-\ngual Capabilities. We found through prelimi-\nnary results Pula’s reasoning performance to be\nsubpar, presumably given the lack of available rea-\nsoning data in Medupi. To further increase the rep-\nresentation of Setswana reasoning text in our train-\ning data, we selectively duplicate our synthetic\nLLM datasets (Dolly, Magpie, Ultrachat, Open-\nHermes, The Tome) three times. To further encour-\nage reasoning, to maintain English capabilities,\nand to encourage cross-lingual transfer across En-\nglish and other languages, we incorporate a 15%\nmixture of additional datasets, including OpenHer-\nmes 2.5 ( Teknium, 2023), Magpie Pro MT 300k\n(Xu et al. , 2024), Aya Dataset ( Singh et al. , 2024),\nand Inkuba Instruct ( Tonja et al. , 2024).\n6640\nModel Avg.\nMAFAND-MT Lego-MT FLORES-200\neng-tsn tsn-eng eng-tsn tsn-eng eng-tsn tsn-eng\nCHRF BLEUCHRF BLEUCHRF BLEUCHRF BLEUCHRF BLEUCHRF BLEU\nLlama 3 Instruct (8B) 11.76 22.90 2.60 29.52 5.73 11.02 1.69 11.61 1.70 20.78 2.37 27.04 4.15\nLlama 3 Instruct (70B)15.48 34.97 7.26 37.33 6.42 11.37 0.58 11.93 0.75 31.47 5.50 33.10 5.07\nAya 23 (8B) 7.31 14.85 0.74 17.17 1.82 8.49 0.94 9.52 1.25 14.19 0.86 16.56 1.33\nAya 23 (35B) 9.77 14.95 0.88 5.96 28.88 8.37 0.89 10.16 1.19 13.05 0.63 27.09 5.13\nLLaMAX3 (8B) 11.59 23.39 2.00 27.55 5.62 11.57 1.29 13.21 1.98 21.51 1.64 25.12 4.14\nMADLAD-400 MT (10B)17.16 22.06 6.07 34.86 14.06 16.85 9.66 22.33 14.60 19.39 3.51 31.40 11.14\nNLLB-200 (3.3B) 28.46 57.64 28.15 46.99 20.66 23.76 13.53 17.10 4.36 50.15 21.73 41.30 16.16\nGPT-4o 30.64 51.07 23.08 60.91 35.28 20.62 6.33 19.57 8.22 45.11 17.41 52.71 27.40\nGPT-4o Mini 25.33 40.12 14.20 53.11 26.18 21.04 9.00 21.16 8.22 35.29 10.05 45.98 19.59\nGemini 1.5 Pro 30.67 55.71 26.29 58.87 34.10 18.10 3.99 17.20 4.72 49.81 21.59 51.16 26.49\nGemini 1.5 Flash 26.50 46.98 16.20 55.01 29.66 18.23 4.42 17.42 3.65 42.41 11.87 48.51 23.60\nPula-1B 16.35 28.06 5.65 38.21 13.69 15.25 3.04 16.21 4.40 28.18 4.65 31.03 7.84\nPula-3B 23.05 38.80 11.41 50.41 25.15 17.71 3.80 19.35 7.17 36.00 8.90 41.57 16.31\nPula-8B 32.24 53.79 26.81 57.06 32.43 23.92 12.02 23.33 14.91 47.38 20.14 49.55 25.55\nPula-14B 33.07 55.57 28.48 57.31 31.94 24.90 12.07 24.34 17.40 47.78 21.00 49.98 26.11\nTable 4: Translation performance across open and closed models on the MAFAND-MT, Lego-MT, and\nFLORES-200 benchmarks. We report CHRF and BLEU scores for English-Setswana and Setswana-English\ntranslation, as well as average overall score.\n5 Evaluation\nWe evaluate our models on a variety of tasks in\nboth Setswana and English, including translation,\nnatural language understanding, multiple choice\nquestion-answering, and mathematical reasoning.\nAll local evaluations are performed using bf16 pre-\ncision using vLLM ( Kwon et al. , 2023).\nTranslation. To measure translation perfor-\nmance, we evaluate on the MAFAND-MT ( Ade-\nlani et al. , 2022a), Lego-MT ( Yuan et al. , 2023),\nand FLORES-200 ( NLLB Team et al. , 2022)\nbenchmarks. These benchmarks cover a variety\nof translation domains, including news headlines,\nweb articles, and miscellaneous web documents.\nTo gauge performance, we utilize the BLEU ( Pap-\nineni et al. , 2002) and CHRF ( Popovi´c, 2015) met-\nrics. Translation performance results are presented\nin Table 4.\nTo evaluate Pula’s knowledge and reasoning\ncapabilities in Setswana we develop MMLU-tsn\nand GSM8K-tsn - Setswana translations of the\nentirety of the test splits of the original Mas-\nsive Multitask Language Understanding (MMLU)\n(Hendrycks et al. , 2021b,a) and Grade School\nMath 8K (GSM8K) ( Cobbe et al. , 2021) bench-\nmarks. We translate using GPT-4o and Gemini 1.5\nPro, respectively. We acknowledge this method’s\nreliance on translation systems to translate this\ntask, especially given its technical nature, is likely\nto suffer from \"translationese\" and other errors\n(Plaza et al. , 2024). Many sequences may be in-\ncorrectly translated, biased, or otherwise impossi-\nble to solve without lucky guessing. However, we\nﬁnd these translated benchmarks to still be a use-\nful proxy for a model’s performance in reasoning,\nknowledge, and instruction-following capabilities.\nThese results are provided in Table 5.\nWe ﬁnd the Pula series excels at translating\nbetween English and Setswana, with both Pula\n14B and Pula 8B on average outperforming sig-\nniﬁcantly larger frontier models such as GPT-4o\nand Gemini 1.5 Pro. These models exceed the per-\nformance of all tested open LLMs as well as the\nNLLB 200 and MADLAD 400 machine transla-\ntion models. We note the Pula series tends to yield\nhigher quality translations when translating from\nSetswana to English - a trend we see in other tested\nLLMs but not in machine translation models.\nReasoning. We evaluate reading comprehen-\nsion using Meta’s Belebele benchmark, a cor-\npus of multiple-choice questions regarding pas-\nsages sourced from FLORES-200. We evaluate\nworld knowledge and question-answering via the\nMMLU and MMLU-tsn benchmarks, containing\nmultiple-choice questions on topics such as mathe-\nmatics, computer science, law, and more. Last, we\nevaluate mathematical reasoning via GSM8K and\nGSM8K-tsn, which consist of open-ended grade-\nschool math word problems.\n6641\nModel Avg. Belebele MMLU GSM8K\ntsn eng tsn eng tsn eng\nLlama 3.1 Instruct 8B 50.32 31.04 89.67 27.73 63.75 6.61 83.09\nLlama 3.1 Instruct 70B 67.69 50.69 96.00 39.95 84.28 40.24 95.00\nAya 23 8B 32.23 30.33 62.00 26.09 44.58 2.05 28.35\nAya 23 35B 39.99 29.16 80.85 29.29 58.42 2.81 39.42\nLLaMAX3 8B 27.67 30.53 68.53 25.53 41.13 0.23 0.07\nGPT-4o 80.59 75.33 96.67 56.32 87.48 72.59 95.14\nGPT-4o Mini 66.42 46.78 94.78 40.97 80.85 41.84 93.32\nGemini 1.5 Pro 81.26 75.78 96.11 60.33 86.35 73.80 95.21\nGemini 1.5 Flash 74.52 64.67 94.11 49.37 80.21 65.22 93.55\nPula 1B 21.06 27.03 36.58 24.38 28.58 3.49 6.29\nPula 3B 35.35 30.93 50.73 30.76 44.03 11.39 44.28\nPula 8B 59.81 51.24 85.43 38.66 61.86 43.28 78.39\nPula 14B 69.40 66.02 91.50 46.15 74.27 57.36 81.12\nTable 5: Performance comparison of different large language models on the Belebele, MMLU, and GSM8K\nbenchmarks. For Setswana, we evaluate using the Setswana Belebele subset, MMLU-tsn, and GSM8K-tsn. For\nEnglish, we evaluate using the English Belebele split, MMLU, and GSM8K.\nWe ﬁnd Pula 14B to, on average, outperform\nGPT-4o Mini as well as all tested open models\nacross Setswana and English reasoning tasks. Pula\n8B outperforms or is competitive with Llama 3.1\n70B on all three benchmarks, and Pula 14B signiﬁ-\ncantly outperforms Llama 3.1 70B on all Setswana\nbenchmarks. In addition, we ﬁnd Pula’s perfor-\nmance steadily grows with scale in both Setswana\nand English tasks; a trend displayed in the perfor-\nmance of existing Llama and Aya models.\nHowever, we do note the roughly equivalent per-\nformance of Aya 23 8B and 35B on Setswana Bele-\nbele. Given the lack of intentional Setswana data\npresent in Aya 23’s training corpus, this indicates\nreading comprehension may be a less transferable\nskill for LLMs when working with largely un-\nseen languages compared to question-answering\nand mathematical reasoning. We also note large\ndiscrepancies between Setswana and English ver-\nsions of benchmarks, such as MMLU and MMLU-\ntsn. We attribute these differences to accumulative\nerrors during translation such as ambiguous or in-\ncorrect wording, impossible questions, or damag-\ning modiﬁcations to the correct answer.\n6 Conclusion\nIn this work we introduce Pula, the ﬁrst series of\nlarge language models tailored for Setswana. Our\nmodels demonstrate signiﬁcantly improved trans-\nlation and reasoning performance, rivaling models\nmuch larger than themselves on Setswana reading\ncomprehension, question-answering, and mathe-\nmatical reasoning tasks while retaining existing\nperformance on English tasks. Pula exceeds in\ntranslating between Setswana and English, with\nPula 8B and 14B on average outperforming GPT-\n4o and Gemini 1.5 Pro, with Pula 14B also out-\nperforming GPT-4o-Mini in Setswana reasoning\ntasks. We introduce Marothodi, the largest-ever\nsingle corpus of raw Setswana text, and Medupi,\nthe ﬁrst-ever comprehensive Setswana instruction-\ntuning dataset. We develop and release MMLU-\ntsn and GSM8K-tsn, Setswana translations of the\nMMLU and GSM8K benchmarks translated us-\ning GPT-4o and Gemini 1.5 Pro. Our results in-\ndicate there may be signiﬁcant performance gains\nnot yet reached in other languages which may be\navailable using existing underutilized data and syn-\nthetic data generation. To support future NLP re-\nsearch and production use cases, we release model\nweights, data, data curation code, benchmarks,\ntraining and evaluation code, and training logs.\n6642\n7 Limitations\nA foundational source for Medupi is translated\nSetswana instructions and synthetic data using\nNLLB 200 3.3B, GPT-4o, Gemini 1.5 Pro, and\nLlama 3.1 405B. The quality of these translations\ndirectly inﬂuences the quality of much of Medupi\nand Pula’s downstream performance on Setswana\ntasks. Any inaccuracies, biases, or nuances lost\nduring translation may propagate into the training\ndata and even become more pronounced ( Gallegos\net al. , 2023).\nOn a similar note, utilizing translations for\nbenchmarking may introduce \"translationese\",\nsuch as direct translations rather than natural lan-\nguage ( Doshi et al. , 2024). These errors may dis-\ntort the benchmark’s authenticity and reduce the\nnumber of answerable questions with correspond-\ning correct answers. These errors may be es-\npecially the case for MMLU-tsn and GSM8K-ts,\nwhere certain domain-speciﬁc vocabulary may not\nhave direct Setswana equivalents.\nWhile Pula demonstrates improved perfor-\nmance compared to existing open source models\nof its size, there is still signiﬁcant room for im-\nprovement. Further experiments involving addi-\ntional data translation and ﬁltering at scale, cu-\nrating human-made chat data, incorporating ad-\nditional languages, and incorporating additional\ntraining methodologies such as multi-stage train-\ning, annealing, model merging, and reinforcement\nlearning may allow for increased performance. We\nhope Pula lays the groundwork for this future work\nand actively encourage research in these direc-\ntions.\nLast, despite extensive efforts to curate com-\nprehensive corpora of Setswana text, certain cul-\ntural and contextual elements may be underrepre-\nsented. Dialectical variations, cultural narratives,\nor region-speciﬁc terms, phrases, or other termi-\nnology may be comparatively sparse in these cor-\npora. This may lead to models that are less ef-\nfective in certain situations that require additional\ncultural insight or sensitivity ( Mousi et al. , 2024).\nAddressing these cultural nuances remains an on-\ngoing challenge and an area for future research\nto ensure language models are properly cultur-\nally knowledgeable and conﬁgurable for Setswana\nspeakers.\n8 Acknowledgements\nWe would like to extend our gratitude to the Ope-\nnAI team for their invaluable support and for grant-\ning us the opportunity to utilize their models. Our\nappreciation also goes to Trelis Research for their\ngenerous ﬁnancial backing. Additionally, we are\ndeeply thankful to Dr. Jacob Sorber and Professor\nCarrie Russell of Clemson University, and Dr. Sri-\nnath Doss of Botho University. This work would\nnot be possible without your guidance and support.\nThis work is the result of a collaboration that was\nfacilitated by the National Science Foundation un-\nder Award CNS 1453607. Any opinions, ﬁndings,\nand conclusions or recommendations expressed in\nthis material are those of the authors and do not\nnecessarily reﬂect the views of the National Sci-\nence Foundation.\n9 References\nReferences\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia\nKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\nDietrich Klakow, Peter Nabende, Ernie Chang,\nTajuddeen Gwadabe, Freshia Sackey, Bonaventure\nF. P. Dossou, Chris Emezue, Colin Leong, Michael\nBeukman, Shamsuddeen Muhammad, Guyo Jarso,\nOreen Yousuf, Andre Niyongabo Rubungo, Gilles\nHacheme, Eric Peter Wairagala, Muhammad Umair\nNasir, Benjamin Ajibade, Tunde Ajayi, Yvonne\nGitau, Jade Abbott, Mohamed Ahmed, Milli-\ncent Ochieng, Anuoluwapo Aremu, Perez Ogayo,\nJonathan Mukiibi, Fatoumata Ouoba Kabore, God-\nson Kalipe, Derguene Mbaye, Allahsera Au-\nguste Tapo, Victoire Memdjokam Koagne, Edwin\nMunkoh-Buabeng, Valencia Wagner, Idris Abdul-\nmumin, Ayodele Awokoya, Happy Buzaaba, Bless-\ning Sibanda, Andiswa Bukula, and Sam Manthalu.\n2022a. A few thousand translations go a long\nway! leveraging pre-trained models for African\nnews translation . In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies , pages 3053–3070, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nDavid Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen,\nNikita Vassilyev, Jesujoba O. Alabi, Yanke Mao,\nHaonan Gao, and Annie En-Shiun Lee. 2023. Sib-\n200: A simple, inclusive, and big evaluation dataset\nfor topic classiﬁcation in 200+ languages and di-\nalects. Preprint, arXiv:2309.07445.\nDavid Ifeoluwa Adelani, Graham Neubig, Sebas-\ntian Ruder, Shruti Rijhwani, Michael Beukman,\nChester Palen-Michel, Constantine Lignos, Je-\nsujoba Oluwadara Alabi, Shamsuddeen Hassan\nMuhammad, Peter Nabende, Cheikh M. Bamba\n6643\nDione, Andiswa Bukula, Rooweither Mabuya,\nBonaventure F. P. Dossou, Blessing K. Sibanda,\nHappy Buzaaba, Jonathan Mukiibi, Godson Kalipe,\nDerguene Mbaye, Amelia Taylor, Fatoumata Ka-\nbore, Chris C. Emezue, Anuoluwapo Aremu,\nPerez Ogayo, Catherine W. Gitau, Edwin Munkoh-\nBuabeng, Victoire Memdjokam Koagne, Allah-\nsera Auguste Tapo, Tebogo Macucwa, Vukosi\nMarivate, Elvis Mboning, Tajuddeen R. Gwad-\nabe, Tosin P. Adewumi, Orevaoghene Ahia, Joyce\nNakatumba-Nabende, Neo L. Mokono, Ignatius M\nEzeani, Chiamaka Ijeoma Chukwuneke, Mofe-\ntoluwa Adeyemi, Gilles Hacheme, Idris Abdul-\nmumin, Odunayo Ogundepo, Oreen Yousuf, Ta-\ntiana Moteu Ngoli, and Dietrich Klakow. 2022b.\nMasakhaner 2.0: Africa-centric transfer learning for\nnamed entity recognition. ArXiv, abs/2210.12391.\nŽeljko Agi ´c and Ivan Vuli ´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages .\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n3204–3210, Florence, Italy. Association for Compu-\ntational Linguistics.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive ﬁne-tuning . In Proceedings\nof the 29th International Conference on Computa-\ntional Linguistics , pages 4336–4349, Gyeongju, Re-\npublic of Korea. International Committee on Com-\nputational Linguistics.\nAnthropic. 2024. Claude 3.5 sonnet .\nArcee AI. 2024. The tome. https://\nhuggingface.co/datasets/arcee-ai/\nThe-Tome.\nShane Arora, Marzena Karpinska, Hung-Ting Chen, Ip-\nsita Bhattacharjee, Mohit Iyyer, and Eunsol Choi.\n2024. Calmqa: Exploring culturally speciﬁc long-\nform question answering across 23 languages .\nViraat Aryabumi, John Dang, Dwarak Talupuru,\nSaurabh Dash, David Cairuz, Hangyu Lin, Bharat\nVenkitesh, Madeline Smith, Jon Ander Campos,\nYi Chern Tan, Kelly Marchisio, Max Bartolo, Se-\nbastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick\nFrosst, Aidan Gomez, Phil Blunsom, Marzieh\nFadaee, Ahmet Üstün, and Sara Hooker. 2024. Aya\n23: Open weight releases to further multilingual\nprogress.\nMikko Aulamo, Nikolay Bogoychev, Shaoxiong Ji,\nGraeme Nail, Gema Ramírez-Sánchez, Jörg Tiede-\nmann, Jelmer van der Linde, and Jaume Zaragoza.\n2023. HPLT: High performance language technolo-\ngies. In Proceedings of the 24th Annual Conference\nof the European Association for Machine Transla-\ntion, pages 517–518, Tampere, Finland. European\nAssociation for Machine Translation.\nWm. G. Bennett, Maxine Diemer, Justine Kerford,\nTracy Probert, and Tsholofelo Wesi. 2016. Setswana\n(south african) . Journal of the International Pho-\nnetic Association , 46(2):235246.\nDan Biderman, Jacob Portes, Jose Javier Gonzalez Or-\ntiz, Mansheej Paul, Philip Greengard, Connor Jen-\nnings, Daniel King, Sam Havens, Vitaliy Chiley,\nJonathan Frankle, Cody Blakeney, and John P. Cun-\nningham. 2024. Lora learns less and forgets less .\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, Matthias Gallé, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Al-\nbert Villanova del Moral, Olatunji Ruwase, Rachel\nBawden, Stas Bekman, Angelina McMillan-Major,\nIz Beltagy, Huu Nguyen, Lucile Saulnier, Sam-\nson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo\nLaurençon, Yacine Jernite, Julien Launay, Mar-\ngaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi\nSimhi, Aitor Soroa, Alham Fikri Aji, Amit Al-\nfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen\nXu, Chenghao Mou, Chris Emezue, Christopher\nKlamm, Colin Leong, Daniel van Strien, David Ife-\noluwa Adelani, Dragomir Radev, Eduardo González\nPonferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar\nNatan, Francesco De Toni, Gérard Dupont, Germán\nKruszewski, Giada Pistilli, Hady Elsahar, Hamza\nBenyamina, Hieu Tran, Ian Yu, Idris Abdulmu-\nmin, Isaac Johnson, Itziar Gonzalez-Dios, Javier\nde la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, Jörg Frohberg, Joseph Tobing,\nJoydeep Bhattacharjee, Khalid Almubarak, Kimbo\nChen, Kyle Lo, Leandro V on Werra, Leon We-\nber, Long Phan, Loubna Ben allal, Ludovic Tanguy,\nManan Dey, Manuel Romero Muñoz, Maraim Ma-\nsoud, María Grandury, Mario ako, Max Huang, Max-\nimin Coavoux, Mayank Singh, Mike Tian-Jian Jiang,\nMinh Chien Vu, Mohammad A. Jauhar, Mustafa\nGhaleb, Nishant Subramani, Nora Kassner, Nuru-\nlaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona\nde Gibert, Paulo Villegas, Peter Henderson, Pierre\nColombo, Priscilla Amuok, Quentin Lhoest, Rheza\nHarliman, Rishi Bommasani, Roberto Luis López,\nRui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-\nbastian Nagel, Shamik Bose, Shamsuddeen Has-\nsan Muhammad, Shanya Sharma, Shayne Long-\npre, Somaieh Nikpoor, Stanislav Silberberg, Suhas\nPai, Sydney Zink, Tiago Timponi Torrent, Timo\nSchick, Tristan Thrush, Valentin Danchev, Vas-\nsilina Nikoulina, Veronika Laippala, Violette Lep-\nercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei\nSi, Davut Emre Taar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y . Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chafﬁn, Arnaud Stiegler, Deba-\njyoti Datta, Eliza Szczechla, Gunjan Chhablani,\nHan Wang, Harshit Pandey, Hendrik Strobelt, Ja-\nson Alan Fries, Jos Rozen, Leo Gao, Lintang\nSutawika, M Saiful Bari, Maged S. Al-shaibani,\nMatteo Manica, Nihal Nayak, Ryan Teehan, Samuel\nAlbanie, Sheng Shen, Srulik Ben-David, Stephen H.\n6644\nBach, Taewoon Kim, Tali Bers, Thibault Fevry, Tr-\nishala Neeraj, Urmish Thakker, Vikas Raunak, Xi-\nangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked\nBrody, Yallow Uri, Hadar Tojarieh, Adam Roberts,\nHyung Won Chung, Jaesung Tae, Jason Phang,\nOﬁr Press, Conglong Li, Deepak Narayanan, Ha-\ntim Bourfoune, Jared Casper, Jeff Rasley, Max\nRyabinin, Mayank Mishra, Minjia Zhang, Mo-\nhammad Shoeybi, Myriam Peyrounette, Nicolas\nPatry, Nouamane Tazi, Omar Sanseviero, Patrick\nvon Platen, Pierre Cornette, Pierre François Laval-\nlée, Rémi Lacroix, Samyam Rajbhandari, Sanchit\nGandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina V oloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenk Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Am-\nmar Khan, Amy Faranak, Ana Santos, Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Arezoo\nAbdollahi, Aycha Tammour, Azadeh HajiHosseini,\nBahareh Behroozi, Benjamin Ajibade, Bharat Sax-\nena, Carlos Muñoz Ferrandis, Daniel McDuff, Dan-\nish Contractor, David Lansky, Davis David, Douwe\nKiela, Duong A. Nguyen, Edward Tan, Emi Bay-\nlor, Ezinwanne Ozoani, Fatima Mirza, Frankline\nOnoniwu, Habib Rezanejad, Hessie Jones, In-\ndrani Bhattacharya, Irene Solaiman, Irina Sedenko,\nIsar Nejadgholi, Jesse Passmore, Josh Seltzer,\nJulio Bonis Sanz, Livia Dutra, Mairon Samagaio,\nMaraim Elbadri, Margot Mieskes, Marissa Ger-\nchick, Martha Akinlolu, Michael McKenna, Mike\nQiu, Muhammed Ghauri, Mykola Burynok, Naﬁs\nAbrar, Nazneen Rajani, Nour Elkott, Nour Fahmy,\nOlanrewaju Samuel, Ran An, Rasmus Kromann,\nRyan Hao, Samira Alizadeh, Sarmad Shubber,\nSilas Wang, Sourav Roy, Sylvain Viguier, Thanh\nLe, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach\nNguyen, Abhinav Ramesh Kashyap, Alfredo Palas-\nciano, Alison Callahan, Anima Shukla, Antonio\nMiranda-Escalada, Ayush Singh, Benjamin Beil-\nharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel León\nPeriñán, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, Maria A Castillo, Mari-\nanna Nezhurina, Mario Sänger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Ros-\naline Su, Ruisi Su, Samuel Cahyawijaya, Samuele\nGarda, Shlok S Deshmukh, Shubhanshu Mishra,\nSid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Sr-\nishti Kumar, Stefan Schweter, Sushil Bharati, Tan-\nmay Laud, Théo Gigant, Tomoya Kainuma, Woj-\nciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash\nVenkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe\nTan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes\nBelkada, and Thomas Wolf. 2022. Bloom: A\n176b-parameter open-access multilingual language\nmodel.\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wa-\nhab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-\nlahsera Tapo, Nishant Subramani, Artem Sokolov,\nClaytone Sikasote, Monang Setyawan, Supheak-\nmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara\nRivera, Annette Rios, Isabel Papadimitriou, Sa-\nlomey Osei, Pedro Javier Ortiz Suárez, Iroro\nOrife, Kelechi Ogueji, Rubungo Andre Niyongabo,\nToan Q. Nguyen, Mathias Müller, André Müller,\nShamsuddeen Hassan Muhammad, Nanda Muham-\nmad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov,\nTapiwanashe Matangira, Colin Leong, Nze Lawson,\nSneha Kudugunta, Yacine Jernite, Mathias Jenny,\nOrhan Firat, Bonaventure F. P. Dossou, Sakhile\nDlamini, Nisansa de Silva, Sakine Çabuk Ballı,\nStella Biderman, Alessia Battisti, Ahmed Baruwa,\nAnkur Bapna, Pallavi Baljekar, Israel Abebe Azime,\nAyodele Awokoya, Duygu Ataman, Orevaoghene\nAhia, Oghenefego Ahia, Sweta Agrawal, and Mofe-\ntoluwa Adeyemi. 2021. Quality at a Glance: An Au-\ndit of Web-Crawled Multilingual Datasets . arXiv e-\nprints, arXiv:2103.12028.\nCity of Tshwane, South African Centre for Digital Lan-\nguage Resources (SADiLaR), Department of Sci-\nence and Innovation (DSI), and Pan South African\nLanguage Board (PanSALB). 2021. Covid-19 mul-\ntilingual terminology . Sponsored by the South\nAfrican Centre for Digital Language Resources and\nthe Department of Science and Innovation.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training veriﬁers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168 .\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale .\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\n6645\nIntroducing the world’s ﬁrst truly open instruction-\ntuned llm .\nFebe de Wet, Laura Martinus, and Jaco Badenhorst.\nn.d. Nchlt setswana auxiliary speech corpus . Ortho-\ngraphically transcribed broadband speech in each of\nSouth Africa’s eleven ofﬁcial languages. Transcrip-\ntions are provided in XML format.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efﬁcient ﬁnetuning\nof quantized llms . In Advances in Neural Informa-\ntion Processing Systems , volume 36, pages 10088–\n10115. Curran Associates, Inc.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi\nZheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,\nand Bowen Zhou. 2023. Enhancing chat language\nmodels by scaling high-quality instructional conver-\nsations. Preprint, arXiv:2305.14233.\nMeet Doshi, Raj Dabre, and Pushpak Bhattacharyya.\n2024. Do not worry if you do not have data: Build-\ning pretrained language models using translationese .\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Let-\nman, Akhil Mathur, Alan Schelten, Amy Yang,\nAngela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar,\nArtem Korenev, Arthur Hinsvark, Arun Rao, As-\nton Zhang, Aurelien Rodriguez, Austen Gregerson,\nAva Spataru, Baptiste Roziere, Bethany Biron,\nBinh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris Mc-\nConnell, Christian Keller, Christophe Touret, Chun-\nyang Wu, Corinne Wong, Cristian Canton Fer-\nrer, Cyrus Nikolaidis, Damien Allonsius, Daniel\nSong, Danielle Pintz, Danny Livshits, David Es-\niobu, Dhruv Choudhary, Dhruv Mahajan, Diego\nGarcia-Olano, Diego Perino, Dieuwke Hupkes, Egor\nLakomkin, Ehab AlBadawy, Elina Lobanova, Emily\nDinan, Eric Michael Smith, Filip Radenovic, Frank\nZhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan\nZarov, Imanol Arrieta Ibarra, Isabel Kloumann, Is-\nhan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee,\nJan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jen-\nnifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,\nJiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Jun-\nteng Jia, Kalyan Vasuden Alwala, Kartikeya Up-\nasani, Kate Plawiak, Ke Li, Kenneth Heaﬁeld, Kevin\nStone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik,\nKuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary,\nLaurens van der Maaten, Lawrence Chen, Liang\nTan, Liz Jenkins, Louis Martin, Lovish Madaan,\nLubo Malo, Lukas Blecher, Lukas Landzaat, Luke\nde Oliveira, Madeline Muzzi, Mahesh Pasupuleti,\nMannat Singh, Manohar Paluri, Marcin Kardas,\nMathew Oldham, Mathieu Rita, Maya Pavlova,\nMelanie Kambadur, Mike Lewis, Min Si, Mitesh Ku-\nmar Singh, Mona Hassan, Naman Goyal, Nar-\njes Torabi, Nikolay Bashlykov, Nikolay Bogoy-\nchev, Niladri Chatterji, Olivier Duchenne, Onur\nÇelebi, Patrick Alrassy, Pengchuan Zhang, Peng-\nwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava,\nPratik Dubal, Praveen Krishnan, Punit Singh Koura,\nPuxin Xu, Qing He, Qingxiao Dong, Ragavan Srini-\nvasan, Raj Ganapathy, Ramon Calderer, Ricardo Sil-\nveira Cabral, Robert Stojnic, Roberta Raileanu, Ro-\nhit Girdhar, Rohit Patel, Romain Sauvestre, Ron-\nnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sa-\nhana Chennabasappa, Sanjay Singh, Sean Bell, Seo-\nhyun Sonia Kim, Sergey Edunov, Shaoliang Nie,\nSharan Narang, Sharath Raparthy, Sheng Shen,\nShengye Wan, Shruti Bhosale, Shun Zhang, Simon\nVandenhende, Soumya Batra, Spencer Whitman,\nSten Sootla, Stephane Collot, Suchin Gururangan,\nSydney Borodinsky, Tamar Herman, Tara Fowler,\nTarek Sheasha, Thomas Georgiou, Thomas Scialom,\nTobias Speckbacher, Todor Mihaylov, Tong Xiao,\nUjjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vig-\nnesh Ramanathan, Viktor Kerkez, Vincent Gonguet,\nVirginie Do, Vish V ogeti, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers,\nXavier Martinet, Xiaodong Wang, Xiaoqing Ellen\nTan, Xinfeng Xie, Xuchao Jia, Xuewei Wang,\nYaelle Goldschlag, Yashesh Gaur, Yasmine Babaei,\nYi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yun-\ning Mao, Zacharie Delpierre Coudert, Zheng Yan,\nZhengxing Chen, Zoe Papakipos, Aaditya Singh,\nAaron Grattaﬁori, Abha Jain, Adam Kelsey, Adam\nShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva\nGoldstand, Ajay Menon, Ajay Sharma, Alex Boe-\nsenberg, Alex Vaughan, Alexei Baevski, Allie Fein-\nstein, Amanda Kallet, Amit Sangani, Anam Yunus,\nAndrei Lupu, Andres Alvarado, Andrew Caples, An-\ndrew Gu, Andrew Ho, Andrew Poulton, Andrew\nRyan, Ankit Ramchandani, Annie Franco, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yaz-\ndan, Beau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhar-\ngavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden\nHancock, Bram Wasti, Brandon Spence, Brani Sto-\njkovic, Brian Gamido, Britt Montalvo, Carl Parker,\nCarly Burton, Catalina Mejia, Changhan Wang,\nChangkyu Kim, Chao Zhou, Chester Hu, Ching-\nHsiang Chu, Chris Cai, Chris Tindal, Christoph\nFeichtenhofer, Damon Civin, Dana Beaty, Daniel\nKreymer, Daniel Li, Danny Wyatt, David Adkins,\nDavid Xu, Davide Testuggine, Delia David, Devi\nParikh, Diana Liskovich, Didem Foss, Dingkang\nWang, Duc Le, Dustin Holland, Edward Dowling,\nEissa Jamil, Elaine Montgomery, Eleonora Presani,\nEmily Hahn, Emily Wood, Erik Brinkman, Este-\nban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,\nFelix Kreuk, Feng Tian, Firat Ozgenel, Francesco\nCaggioni, Francisco Guzmán, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Govind\nThattai, Grant Herman, Grigory Sizov, Guangyi,\nZhang, Guna Lakshminarayanan, Hamid Shojanaz-\n6646\neri, Han Zou, Hannah Wang, Hanwen Zha, Haroun\nHabeeb, Harrison Rudolph, Helen Suk, Henry Aspe-\ngren, Hunter Goldman, Igor Molybog, Igor Tufanov,\nIrina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Japhet Asher, Jean-Baptiste\nGaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny\nZhen, Jeremy Reizenstein, Jeremy Teboul, Jessica\nZhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon\nCarvill, Jon Shepard, Jonathan McPhie, Jonathan\nTorres, Josh Ginsburg, Junjie Wang, Kai Wu,\nKam Hou U, Karan Saxena, Karthik Prasad, Kar-\ntikay Khandelwal, Katayoun Zand, Kathy Matosich,\nKaushik Veeraraghavan, Kelly Michelena, Keqian\nLi, Kun Huang, Kunal Chawla, Kushal Lakhotia,\nKyle Huang, Lailin Chen, Lakshya Garg, Laven-\nder A, Leandro Silva, Lee Bell, Lei Zhang, Liang-\npeng Guo, Licheng Yu, Liron Moshkovich, Luca\nWehrstedt, Madian Khabsa, Manav Avalani, Man-\nish Bhatt, Maria Tsimpoukelli, Martynas Mankus,\nMatan Hasson, Matthew Lennie, Matthias Reso,\nMaxim Groshev, Maxim Naumov, Maya Lathi,\nMeghan Keneally, Michael L. Seltzer, Michal Valko,\nMichelle Restrepo, Mihir Patel, Mik Vyatskov,\nMikayel Samvelyan, Mike Clark, Mike Macey,\nMike Wang, Miquel Jubert Hermoso, Mo Metanat,\nMohammad Rastegari, Munish Bansal, Nandhini\nSanthanam, Natascha Parks, Natasha White, Navy-\nata Bawa, Nayan Singhal, Nick Egebo, Nicolas\nUsunier, Nikolay Pavlovich Laptev, Ning Dong,\nNing Zhang, Norman Cheng, Oleg Chernoguz,\nOlivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin\nKent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-\ndro Rittner, Philip Bontrager, Pierre Roux, Pi-\notr Dollar, Polina Zvyagina, Prashant Ratanchan-\ndani, Pritish Yuvraj, Qian Liang, Rachad Alao,\nRachel Rodriguez, Raﬁ Ayub, Raghotham Murthy,\nRaghu Nayani, Rahul Mitra, Raymond Li, Re-\nbekkah Hogan, Robin Battey, Rocky Wang, Rohan\nMaheswari, Russ Howes, Ruty Rinott, Sai Jayesh\nBondu, Samyak Datta, Sara Chugh, Sara Hunt, Sar-\ngun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh\nVerma, Seiji Yamamoto, Sharadh Ramaswamy,\nShaun Lindsay, Shaun Lindsay, Sheng Feng, Sheng-\nhao Lin, Shengxin Cindy Zha, Shiva Shankar,\nShuqiang Zhang, Shuqiang Zhang, Sinong Wang,\nSneha Agarwal, Soji Sajuyigbe, Soumith Chin-\ntala, Stephanie Max, Stephen Chen, Steve Kehoe,\nSteve Satterﬁeld, Sudarshan Govindaprasad, Sumit\nGupta, Sungmin Cho, Sunny Virk, Suraj Subrama-\nnian, Sy Choudhury, Sydney Goldman, Tal Remez,\nTamar Glaser, Tamara Best, Thilo Kohler, Thomas\nRobinson, Tianhe Li, Tianjun Zhang, Tim Matthews,\nTimothy Chou, Tzook Shaked, Varun V ontimitta,\nVictoria Ajayi, Victoria Montanez, Vijai Mohan,\nVinay Satish Kumar, Vishal Mangla, Vlad Ionescu,\nVlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir\nIvanov, Wei Li, Wenchen Wang, Wenwen Jiang,\nWes Bouaziz, Will Constable, Xiaocheng Tang, Xi-\naofang Wang, Xiaojian Wu, Xiaolan Wang, Xide\nXia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu,\nYe Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang,\nYossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao,\nYundi Qian, Yuzi He, Zach Rait, Zachary DeVito,\nZef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and\nZhiwei Zhao. 2024. The llama 3 herd of models .\nRoald Eiselen, Rico Koen, Albertus Kruger, and\nJacques van Heerden. 2023. Nchlt setswana roberta\nlanguage model .\nRoald Eiselen and Martin Puttkammer. 2014. De-\nveloping text resources for ten South African lan-\nguages. In Proceedings of the Ninth International\nConference on Language Resources and Evalua-\ntion (LREC’14) , pages 3698–3703, Reykjavik, Ice-\nland. European Language Resources Association\n(ELRA).\nAhmed El-Kishky, Vishrav Chaudhary, Francisco\nGuzmÃ ˛ an, and Philipp Koehn. 2020. CCAligned:\nA massive collection of cross-lingual web-document\npairs. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2020) , pages 5960–5969, Online. Associa-\ntion for Computational Linguistics.\nAhmed El-Kishky, Adithya Renduchintala, James\nCross, Francisco Guzmán, and Philipp Koehn. 2021.\nXlent: Mining a large cross-lingual entity dataset\nwith lexical-semantic-phonetic word alignment. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n10424–10430.\nRonen Eldan and Yuanzhi Li. 2023. Tinystories: How\nsmall can language models be and still speak coher-\nent english?\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Michael Auli, and Ar-\nmand Joulin. 2021. Beyond english-centric multilin-\ngual machine translation . Journal of Machine Learn-\ning Research , 22(107):1–48.\nWikimedia Foundation. Wikimedia downloads .\nIsabel O. Gallegos, Ryan A. Rossi, Joe Barrow,\nMd Mehrab Tanjim, Sungchul Kim, Franck Der-\nnoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K.\nAhmed. 2023. Bias and fairness in large language\nmodels: A survey .\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M. Dai, Anja Hauth, Katie\nMillican, David Silver, Melvin Johnson, Ioannis\nAntonoglou, Julian Schrittwieser, Amelia Glaese,\nJilin Chen, Emily Pitler, Timothy Lillicrap, Ange-\nliki Lazaridou, Orhan Firat, James Molloy, Michael\nIsard, Paul R. Barham, Tom Hennigan, Benjamin\nLee, Fabio Viola, Malcolm Reynolds, Yuanzhong\nXu, Ryan Doherty, Eli Collins, Clemens Meyer,\nEliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, Jack Krawczyk, Cosmo Du, Ed Chi,\nHeng-Tze Cheng, Eric Ni, Purvi Shah, Patrick\nKane, Betty Chan, Manaal Faruqui, Aliaksei Sev-\neryn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe\n6647\nIttycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun,\nDustin Tran, Sumit Bagri, Balaji Lakshminarayanan,\nJeremiah Liu, Andras Orban, Fabian Güra, Hao\nZhou, Xinying Song, Aurelien Boffy, Harish Gana-\npathy, Steven Zheng, HyunJeong Choe, Ágoston\nWeisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jar-\nrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah,\nEmanuel Taropa, Majd Al Merey, Martin Baeuml,\nZhifeng Chen, Laurent El Shafey, Yujing Zhang, Ol-\ncan Sercinoglu, George Tucker, Enrique Piqueras,\nMaxim Krikun, Iain Barr, Nikolay Savinov, Ivo\nDanihelka, Becca Roelofs, Anaïs White, Anders\nAndreassen, Tamara von Glehn, Lakshman Yagati,\nMehran Kazemi, Lucas Gonzalez, Misha Khalman,\nJakub Sygnowski, Alexandre Frechette, Charlotte\nSmith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen,\nJames Lottes, Nathan Schucher, Federico Lebron,\nAlban Rrustemi, Natalie Clay, Phil Crone, Tomas\nKocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi\nHoward, Adam Bloniarz, Jack W. Rae, Han Lu,\nLaurent Sifre, Marcello Maggioni, Fred Alcober,\nDan Garrette, Megan Barnes, Shantanu Thakoor, Ja-\ncob Austin, Gabriel Barth-Maron, William Wong,\nRishabh Joshi, Rahma Chaabouni, Deeni Fatiha,\nArun Ahuja, Gaurav Singh Tomar, Evan Senter,\nMartin Chadwick, Ilya Kornakov, Nithya Attaluri,\nIñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan,\nJeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,\nJordan Grimstad, Ale Jakse Hartman, Xavier Gar-\ncia, Thanumalayan Sankaranarayana Pillai, Jacob\nDevlin, Michael Laskin, Diego de Las Casas, Dasha\nValter, Connie Tao, Lorenzo Blanco, Adrià Puig-\ndomènech Badia, David Reitter, Mianna Chen,\nJenny Brennan, Clara Rivera, Sergey Brin, Shariq\nIqbal, Gabriela Surita, Jane Labanowski, Abhi Rao,\nStephanie Winkler, Emilio Parisotto, Yiming Gu,\nKate Olszewska, Ravi Addanki, Antoine Miech, An-\nnie Louis, Denis Teplyashin, Geoff Brown, Elliot\nCatt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe\nAshwood, Anton Briukhov, Albert Webson, San-\njay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-\nWei Chang, Axel Stjerngren, Josip Djolonga, Yut-\ning Sun, Ankur Bapna, Matthew Aitchison, Pedram\nPejman, Henryk Michalewski, Tianhe Yu, Cindy\nWang, Juliette Love, Junwhan Ahn, Dawn Bloxwich,\nKehang Han, Peter Humphreys, Thibault Sellam,\nJames Bradbury, Varun Godbole, Sina Samangooei,\nBogdan Damoc, Alex Kaskasoli, Sébastien M. R.\nArnold, Vijay Vasudevan, Shubham Agrawal, Jason\nRiesa, Dmitry Lepikhin, Richard Tanburn, Srivat-\nsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson,\nPranav Shyam, Johan Ferret, Steven Hand, Ankush\nGarg, Tom Le Paine, Jian Li, Yujia Li, Minh Gi-\nang, Alexander Neitz, Zaheer Abbas, Sarah York,\nMachel Reid, Elizabeth Cole, Aakanksha Chowd-\nhery, Dipanjan Das, Dominika Rogoziska, Vitaliy\nNikolaev, Pablo Sprechmann, Zachary Nado, Lukas\nZilka, Flavien Prost, Luheng He, Marianne Mon-\nteiro, Gaurav Mishra, Chris Welty, Josh Newlan,\nDawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,\nRaoul de Liedekerke, Justin Gilmer, Carl Sarouﬁm,\nShruti Rijhwani, Shaobo Hou, Disha Shrivastava,\nAnirudh Baddepudi, Alex Goldin, Adnan Ozturel,\nAlbin Cassirer, Yunhan Xu, Daniel Sohn, Deven-\ndra Sachan, Reinald Kim Amplayo, Craig Swan-\nson, Dessie Petrova, Shashi Narayan, Arthur Guez,\nSiddhartha Brahma, Jessica Landon, Miteyan Pa-\ntel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wen-\nhao Jia, Matthew Rahtz, Mai Giménez, Legg Ye-\nung, James Keeling, Petko Georgiev, Diana Mincu,\nBoxi Wu, Salem Haykal, Rachel Saputro, Kiran\nV odrahalli, James Qin, Zeynep Cankara, Abhanshu\nSharma, Nick Fernando, Will Hawkins, Behnam\nNeyshabur, Solomon Kim, Adrian Hutter, Priyanka\nAgrawal, Alex Castro-Ros, George van den Driess-\nche, Tao Wang, Fan Yang, Shuo yiin Chang,\nPaul Komarek, Ross McIlroy, Mario Lui, Guodong\nZhang, Wael Farhan, Michael Sharman, Paul Nat-\nsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris\nCao, Siamak Shakeri, Christina Butterﬁeld, Justin\nChung, Paul Kishan Rubenstein, Shivani Agrawal,\nArthur Mensch, Kedar Soparkar, Karel Lenc, Tim-\nothy Chung, Aedan Pope, Loren Maggiore, Jackie\nKay, Priya Jhakra, Shibo Wang, Joshua Maynez,\nMary Phuong, Taylor Tobin, Andrea Tacchetti, Maja\nTrebacz, Kevin Robinson, Yash Katariya, Sebas-\ntian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghe-\nlani, Lora Aroyo, Ambrose Slone, Neil Houlsby,\nXuehan Xiong, Zhen Yang, Elena Gribovskaya,\nJonas Adler, Mateo Wirth, Lisa Lee, Music Li,\nThais Kagohara, Jay Pavagadhi, Sophie Bridgers,\nAnna Bortsova, Sanjay Ghemawat, Zafarali Ahmed,\nTianqi Liu, Richard Powell, Vijay Bolina, Mariko\nIinuma, Polina Zablotskaia, James Besley, Da-Woon\nChung, Timothy Dozat, Ramona Comanescu, Xi-\nance Si, Jeremy Greer, Guolong Su, Martin Polacek,\nRaphaël Lopez Kaufman, Simon Tokumine, Hex-\niang Hu, Elena Buchatskaya, Yingjie Miao, Mo-\nhamed Elhawaty, Aditya Siddhant, Nenad Tomasev,\nJinwei Xing, Christina Greer, Helen Miller, Shereen\nAshraf, Aurko Roy, Zizhao Zhang, Ada Ma, Ange-\nlos Filos, Milos Besta, Rory Blevins, Ted Klimenko,\nChih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Os-\ncar Chang, Mantas Pajarskas, Carrie Muir, Vered\nCohen, Charline Le Lan, Krishna Haridasan, Amit\nMarathe, Steven Hansen, Sholto Douglas, Rajkumar\nSamuel, Mingqiu Wang, Sophia Austin, Chang Lan,\nJiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo,\nLars Lowe Sjösund, Sébastien Cevey, Zach Gle-\nicher, Thi Avrahami, Anudhyan Boral, Hansa Srini-\nvasan, Vittorio Selo, Rhys May, Konstantinos Aiso-\npos, Léonard Hussenot, Livio Baldini Soares, Kate\nBaumli, Michael B. Chang, Adrià Recasens, Ben\nCaine, Alexander Pritzel, Filip Pavetic, Fabio Pardo,\nAnita Gergely, Justin Frye, Vinay Ramasesh, Dan\nHorgan, Kartikeya Badola, Nora Kassner, Subhra-\njit Roy, Ethan Dyer, Víctor Campos Campos, Alex\nTomala, Yunhao Tang, Dalia El Badawy, Elspeth\nWhite, Basil Mustafa, Oran Lang, Abhishek Jin-\ndal, Sharad Vikram, Zhitao Gong, Sergi Caelles,\nRoss Hemsley, Gregory Thornton, Fangxiaoyu Feng,\nWojciech Stokowiec, Ce Zheng, Phoebe Thacker,\nÇalar Ünlü, Zhishuai Zhang, Mohammad Saleh,\nJames Svensson, Max Bileschi, Piyush Patil, Ankesh\nAnand, Roman Ring, Katerina Tsihlas, Arpi Vezer,\nMarco Selvi, Toby Shevlane, Mikel Rodriguez, Tom\nKwiatkowski, Samira Daruki, Keran Rong, Allan\n6648\nDafoe, Nicholas FitzGerald, Keren Gu-Lemberg,\nMina Khan, Lisa Anne Hendricks, Marie Pellat,\nVladimir Feinberg, James Cobon-Kerr, Tara Sainath,\nMaribeth Rauh, Sayed Hadi Hashemi, Richard Ives,\nYana Hasson, Eric Noland, Yuan Cao, Nathan Byrd,\nLe Hou, Qingze Wang, Thibault Sottiaux, Michela\nPaganini, Jean-Baptiste Lespiau, Alexandre Mou-\nfarek, Samer Hassan, Kaushik Shivakumar, Joost\nvan Amersfoort, Amol Mandhane, Pratik Joshi,\nAnirudh Goyal, Matthew Tung, Andrew Brock, Han-\nnah Sheahan, Vedant Misra, Cheng Li, Nemanja\nRakievi, Mostafa Dehghani, Fangyu Liu, Sid Mit-\ntal, Junhyuk Oh, Seb Noury, Eren Sezener, Fan-\ntine Huot, Matthew Lamm, Nicola De Cao, Char-\nlie Chen, Sidharth Mudgal, Romina Stella, Kevin\nBrooks, Gautam Vasudevan, Chenxi Liu, Mainak\nChain, Nivedita Melinkeri, Aaron Cohen, Venus\nWang, Kristie Seymore, Sergey Zubkov, Rahul\nGoel, Summer Yue, Sai Krishnakumaran, Brian\nAlbert, Nate Hurley, Motoki Sano, Anhad Mo-\nhananey, Jonah Joughin, Egor Filonov, Tomasz Kpa,\nYomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin\nBadiezadegan, Taylor Bos, Jerry Chang, Sanil Jain,\nSri Gayatri Sundara Padmanabhan, Subha Putta-\ngunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb,\nVamsi Bedapudi, Adam Kurzrok, Shuntong Lei, An-\nthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu,\nSam Sobell, Andrea Siciliano, Alan Papir, Robby\nNeale, Jonas Bragagnolo, Tej Toor, Tina Chen,\nValentin Anklin, Feiran Wang, Richie Feng, Mi-\nlad Gholami, Kevin Ling, Lijuan Liu, Jules Walter,\nHamid Moghaddam, Arun Kishore, Jakub Adamek,\nTyler Mercado, Jonathan Mallinson, Siddhinita Wan-\ndekar, Stephen Cagle, Eran Ofek, Guillermo Gar-\nrido, Clemens Lombriser, Maksim Mukha, Botu\nSun, Hafeezul Rahman Mohammad, Josip Matak,\nYadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan,\nLeif Schelin, Oana David, Ankur Garg, Yifan He,\nOleksii Duzhyi, Anton Älgmyr, Timothée Lottaz,\nQi Li, Vikas Yadav, Luyao Xu, Alex Chinien,\nRakesh Shivanna, Aleksandr Chuklin, Josie Li,\nCarrie Spadine, Travis Wolfe, Kareem Mohamed,\nSubhabrata Das, Zihang Dai, Kyle He, Daniel\nvon Dincklage, Shyam Upadhyay, Akanksha Mau-\nrya, Luyan Chi, Sebastian Krause, Khalid Salama,\nPam G Rabinovitch, Pavan Kumar Reddy M, Aarush\nSelvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem\nGuven, Himanshu Gupta, Boyi Liu, Deepak Sharma,\nIdan Heimlich Shtacher, Shachi Paul, Oscar Aker-\nlund, François-Xavier Aubet, Terry Huang, Chen\nZhu, Eric Zhu, Elico Teixeira, Matthew Fritze,\nFrancesco Bertolini, Liana-Eleonora Marinescu,\nMartin Bölle, Dominik Paulus, Khyatti Gupta, Te-\njasi Latkar, Max Chang, Jason Sanders, Roopa Wil-\nson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet,\nTulsee Doshi, Sid Lall, Swaroop Mishra, Wanming\nChen, Thang Luong, Seth Benjamin, Jasmine Lee,\nEwa Andrejczuk, Dominik Rabiej, Vipul Ranjan,\nKrzysztof Styrc, Pengcheng Yin, Jon Simon, Mal-\ncolm Rose Harriott, Mudit Bansal, Alexei Rob-\nsky, Geoff Bacon, David Greene, Daniil Mirylenka,\nChen Zhou, Obaid Sarvana, Abhimanyu Goyal,\nSamuel Andermatt, Patrick Siegler, Ben Horn, Assaf\nIsrael, Francesco Pongetti, Chih-Wei \"Louis\" Chen,\nMarco Selvatici, Pedro Silva, Kathie Wang, Jack-\nson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai,\nAlessandro Agostini, Maulik Shah, Hung Nguyen,\nNoah Ó Donnaile, Sébastien Pereira, Linda Friso,\nAdam Stambler, Adam Kurzrok, Chenkai Kuang,\nYan Romanikhin, Mark Geller, ZJ Yan, Kane Jang,\nCheng-Chun Lee, Wojciech Fica, Eric Malmi, Qi-\njun Tan, Dan Banica, Daniel Balle, Ryan Pham,\nYanping Huang, Diana Avram, Hongzhi Shi, Jasjot\nSingh, Chris Hidey, Niharika Ahuja, Pranab Sax-\nena, Dan Dooley, Srividya Pranavi Potharaju, Eileen\nO’Neill, Anand Gokulchandran, Ryan Foley, Kai\nZhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta,\nRagha Kotikalapudi, Chalence Safranek-Shrader,\nAndrew Goodman, Joshua Kessinger, Eran Globen,\nPrateek Kolhar, Chris Gorgolewski, Ali Ibrahim,\nYang Song, Ali Eichenbaum, Thomas Brovelli,\nSahitya Potluri, Preethi Lahoti, Cip Baetu, Ali\nGhorbani, Charles Chen, Andy Crawford, Shalini\nPal, Mukund Sridhar, Petru Gurita, Asier Mujika,\nIgor Petrovski, Pierre-Louis Cedoz, Chenmei Li,\nShiyuan Chen, Niccolò Dal Santo, Siddharth Goyal,\nJitesh Punjabi, Karthik Kappaganthu, Chester Kwak,\nPallavi LV , Sarmishta Velury, Himadri Choudhury,\nJamie Hall, Premal Shah, Ricardo Figueira, Matt\nThomas, Minjie Lu, Ting Zhou, Chintu Kumar,\nThomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams\nYu, Soo Kwak, Victor Ähdel, Sujeevan Rajayo-\ngam, Travis Choma, Fei Liu, Aditya Barua, Colin\nJi, Ji Ho Park, Vincent Hellendoorn, Alex Bai-\nley, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir,\nCharles Sutton, Wojciech Rzadkowski, Fiona Mac-\nintosh, Konstantin Shagin, Paul Medina, Chen\nLiang, Jinjing Zhou, Pararth Shah, Yingying Bi,\nAttila Dankovics, Shipra Banga, Sabine Lehmann,\nMarissa Bredesen, Zifan Lin, John Eric Hoff-\nmann, Jonathan Lai, Raynald Chung, Kai Yang,\nNihal Balani, Arthur Brainskas, Andrei Sozanschi,\nMatthew Hayes, Héctor Fernández Alcalde, Peter\nMakarov, Will Chen, Antonio Stella, Liselotte Sni-\njders, Michael Mandl, Ante Kärrman, Pawe Nowak,\nXinyi Wu, Alex Dyck, Krishnan Vaidyanathan,\nRaghavender R, Jessica Mallet, Mitch Rudominer,\nEric Johnston, Sushil Mittal, Akhil Udathu, Ja-\nnara Christensen, Vishal Verma, Zach Irving, An-\ndreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi,\nMarin Georgiev, Ian Tenney, Nan Hua, Geoffrey\nCideron, Edouard Leurent, Mahmoud Alnahlawi,\nIonut Georgescu, Nan Wei, Ivy Zheng, Dylan Scan-\ndinaro, Heinrich Jiang, Jasper Snoek, Mukund Sun-\ndararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo,\nJeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal\nBen-David, Rishub Jain, Jonathan Uesato, Romina\nDatta, Oskar Bunyan, Shimu Wu, John Zhang, Pi-\notr Stanczyk, Ye Zhang, David Steiner, Subhajit\nNaskar, Michael Azzam, Matthew Johnson, Adam\nPaszke, Chung-Cheng Chiu, Jaume Sanchez Elias,\nAfroz Mohiuddin, Faizan Muhammad, Jin Miao,\nAndrew Lee, Nino Vieillard, Jane Park, Jiageng\nZhang, Jeff Stanway, Drew Garmon, Abhijit Kar-\nmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Lu-\nowei Zhou, Jonathan Evens, William Isaac, Geoffrey\nIrving, Edward Loper, Michael Fink, Isha Arkatkar,\n6649\nNanxin Chen, Izhak Shafran, Ivan Petrychenko,\nZhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai\nZhu, Peter Grabowski, Yu Mao, Alberto Magni,\nKaisheng Yao, Javier Snaider, Norman Casagrande,\nEvan Palmer, Paul Suganthan, Alfonso Castaño,\nIrene Giannoumis, Wooyeol Kim, Mikoaj Rybiski,\nAshwin Sreevatsa, Jennifer Prendki, David Soergel,\nAdrian Goedeckemeyer, Willi Gierke, Mohsen Ja-\nfari, Meenu Gaba, Jeremy Wiesner, Diana Gage\nWright, Yawen Wei, Harsha Vashisht, Yana Kulizh-\nskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie\nIwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khor-\nlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo\nAguilar, Keith Pallo, Abhishek Chakladar, Gin-\nger Perng, Elena Allica Abellan, Mingyang Zhang,\nIshita Dasgupta, Nate Kushman, Ivo Penchev, Alena\nRepina, Xihui Wu, Tom van der Weide, Priya Pon-\nnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng\nLi, Olivier Dousse, Fan Yang, Jeff Piper, Nathan\nIe, Rama Pasumarthi, Nathan Lintz, Anitha Vi-\njayakumar, Daniel Andor, Pedro Valenzuela, Minnie\nLui, Cosmin Paduraru, Daiyi Peng, Katherine Lee,\nShuyuan Zhang, Somer Greene, Duc Dung Nguyen,\nPaula Kurylowicz, Cassidy Hardin, Lucas Dixon,\nLili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang,\nAchintya Singhal, Dayou Du, Dan McKinnon,\nNatasha Antropova, Tolga Bolukbasi, Orgad Keller,\nDavid Reid, Daniel Finchelstein, Maria Abi Raad,\nRemi Crocker, Peter Hawkins, Robert Dadashi,\nColin Gaffney, Ken Franko, Anna Bulanova, Rémi\nLeblond, Shirley Chung, Harry Askham, Luis C.\nCobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina\nSorokin, Chris Alberti, Chu-Cheng Lin, Colin\nEvans, Alek Dimitriev, Hannah Forbes, Dylan Ba-\nnarse, Zora Tung, Mark Omernick, Colton Bishop,\nRachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan\nAmid, Francesco Piccinno, Xingyu Wang, Praseem\nBanzal, Daniel J. Mankowitz, Alex Polozov, Vic-\ntoria Krakovna, Sasha Brown, MohammadHossein\nBateni, Dennis Duan, Vlad Firoiu, Meghana Tho-\ntakuri, Tom Natan, Matthieu Geist, Ser tan Girgin,\nHui Li, Jiayu Ye, Oﬁr Roval, Reiko Tojo, Michael\nKwong, James Lee-Thorp, Christopher Yew, Danila\nSinopalnikov, Sabela Ramos, John Mellor, Abhishek\nSharma, Kathy Wu, David Miller, Nicolas Son-\nnerat, Denis Vnukov, Rory Greig, Jennifer Beat-\ntie, Emily Caveness, Libin Bai, Julian Eisensch-\nlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic,\nWeize Kong, Phuong Dao, Zeyu Zheng, Frederick\nLiu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason\nSanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel\nToyama, Evan Rosen, Sasan Tavakkol, Linting Xue,\nChen Elkind, Oliver Woodman, John Carpenter,\nGeorge Papamakarios, Rupert Kemp, Sushant Kaﬂe,\nTanya Grunina, Rishika Sinha, Alice Talbert, Di-\nane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe\nThornton, Jordi Pont-Tuset, Pradyumna Narayana,\nJing Li, Saaber Fatehi, John Wieting, Omar Ajmeri,\nBenigno Uria, Yeongil Ko, Laura Knight, Amélie\nHéliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing\nLi, Nir Levine, Ariel Stolovich, Rebeca Santamaria-\nFernandez, Sonam Goenka, Wenny Yustalim, Robin\nStrudel, Ali Elqursh, Charlie Deck, Hyo Lee,\nZonglin Li, Kyle Levin, Raphael Hoffmann, Dan\nHoltmann-Rice, Olivier Bachem, Sho Arora, Christy\nKoh, Soheil Hassas Yeganeh, Siim Põder, Mukar-\nram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba\nSeyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol\nGulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown,\nShreya Singh, Wei Fan, Aaron Parisi, Joe Stan-\nton, Vinod Koverkathu, Christopher A. Choquette-\nChoo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash\nShroff, Mani Varadarajan, Sanaz Bahargam, Rob\nWilloughby, David Gaddy, Guillaume Desjardins,\nMarco Cornero, Brona Robenek, Bhavishya Mittal,\nBen Albrecht, Ashish Shenoy, Fedor Moiseev, Hen-\nrik Jacobsson, Alireza Ghaffarkhah, Morgane Riv-\nière, Alanna Walton, Clément Crepy, Alicia Par-\nrish, Zongwei Zhou, Clement Farabet, Carey Rade-\nbaugh, Praveen Srinivasan, Claudia van der Salm,\nAndreas Fidjeland, Salvatore Scellato, Eri Latorre-\nChimoto, Hanna Klimczak-Pluciska, David Bridson,\nDario de Cesare, Tom Hudson, Piermaria Mendolic-\nchio, Lexi Walker, Alex Morris, Matthew Mauger,\nAlexey Guseynov, Alison Reid, Seth Odoom, Lu-\ncia Loher, Victor Cotruta, Madhavi Yenugula, Do-\nminik Grewe, Anastasia Petrushkina, Tom Duerig,\nAntonio Sanchez, Steve Yadlowsky, Amy Shen,\nAmir Globerson, Lynette Webb, Sahil Dua, Dong\nLi, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi,\nAnanth Agarwal, Tomer Shani, Matan Eyal, Anuj\nKhare, Shreyas Rammohan Belle, Lei Wang, Chetan\nTekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin\nSang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao\nZhao, Stephan Lee, Pandu Nayak, Doug Fritz, Man-\nish Reddy Vuyyuru, John Aslanides, Nidhi Vyas,\nMartin Wicke, Xiao Ma, Evgenii Eltyshev, Nina\nMartin, Hardie Cate, James Manyika, Keyvan Amiri,\nYelin Kim, Xi Xiong, Kai Kang, Florian Luisier,\nNilesh Tripuraneni, David Madras, Mandy Guo,\nAustin Waters, Oliver Wang, Joshua Ainslie, Ja-\nson Baldridge, Han Zhang, Garima Pruthi, Jakob\nBauer, Feng Yang, Riham Mansour, Jason Gel-\nman, Yang Xu, George Polovets, Ji Liu, Hong-\nlong Cai, Warren Chen, XiangHai Sheng, Emily\nXue, Sherjil Ozair, Christof Angermueller, Xiaowei\nLi, Anoop Sinha, Weiren Wang, Julia Wiesinger,\nEmmanouil Koukoumidis, Yuan Tian, Anand Iyer,\nMadhu Gurumurthy, Mark Goldenson, Parashar\nShah, MK Blake, Hongkun Yu, Anthony Urbanow-\nicz, Jennimaria Palomaki, Chrisantha Fernando, Ken\nDurden, Harsh Mehta, Nikola Momchev, Elahe\nRahimtoroghi, Maria Georgaki, Amit Raul, Sebas-\ntian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny\nZhou, Komal Jalan, Dinghua Li, Blake Hecht-\nman, Parker Schuh, Milad Nasr, Kieran Milan,\nVladimir Mikulik, Juliana Franco, Tim Green, Nam\nNguyen, Joe Kelley, Aroma Mahendru, Andrea Hu,\nJoshua Howland, Ben Vargas, Jeffrey Hui, Kshi-\ntij Bansal, Vikram Rao, Rakesh Ghiya, Emma\nWang, Ke Ye, Jean Michel Sarr, Melanie Moran-\nski Preston, Madeleine Elish, Steve Li, Aakash\nKaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan,\nMilan Someswar, Tejvi M., Xinyun Chen, Aida\nAmini, Alex Fabrikant, Eric Chu, Xuanyi Dong,\nAmruta Muthal, Senaka Buthpitiya, Sarthak Jauhari,\nNan Hua, Urvashi Khandelwal, Ayal Hitron, Jie\n6650\nRen, Larissa Rinaldi, Shahar Drath, Avigail Dabush,\nNan-Jiang Jiang, Harshal Godhia, Uli Sachs, An-\nthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila\nNoga, Zhuyun Dai, James Wang, Chen Liang, Jenny\nHamer, Chun-Sung Ferng, Chenel Elkind, Aviel\nAtias, Paulina Lee, Vít Listík, Mathias Carlen, Jan\nvan de Kerkhof, Marcin Pikus, Krunoslav Zaher,\nPaul Müller, Sasha Zykova, Richard Stefanec, Vi-\ntaly Gatsko, Christoph Hirnschall, Ashwin Sethi,\nXingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca\nStefanoiu, Bo Feng, Keshav Dhandhania, Manish\nKatyal, Akshay Gupta, Atharva Parulekar, Divya\nPitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhav-\nnani, Omar Alhadlaq, Xiaolin Li, Peter Danen-\nberg, Dennis Tu, Alex Pine, Vera Filippova, Ab-\nhipso Ghosh, Ben Limonchik, Bhargava Urala, Chai-\ntanya Krishna Lanka, Derik Clive, Yi Sun, Ed-\nward Li, Hao Wu, Kevin Hongtongsak, Ianna Li,\nKalind Thakkar, Kuanysh Omarov, Kushal Maj-\nmundar, Michael Alverson, Michael Kucharski, Mo-\nhak Patel, Mudit Jain, Maksim Zabelin, Paolo Pela-\ngatti, Rohan Kohli, Saurabh Kumar, Joseph Kim,\nSwetha Sankar, Vineet Shah, Lakshmi Ramachan-\ndruni, Xiangkai Zeng, Ben Bariach, Laura Wei-\ndinger, Tu Vu, Alek Andreev, Antoine He, Kevin\nHui, Sheleem Kashem, Amar Subramanya, Sissie\nHsiao, Demis Hassabis, Koray Kavukcuoglu, Adam\nSadovsky, Quoc Le, Trevor Strohman, Yonghui Wu,\nSlav Petrov, Jeffrey Dean, and Oriol Vinyals. 2023.\nGemini: A family of highly capable multimodal\nmodels.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivière, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, Léonard Hussenot,\nPier Giuseppe Sessa, Aakanksha Chowdhery, Adam\nRoberts, Aditya Barua, Alex Botev, Alex Castro-\nRos, Ambrose Slone, Amélie Héliou, Andrea Tac-\nchetti, Anna Bulanova, Antonia Paterson, Beth Tsai,\nBobak Shahriari, Charline Le Lan, Christopher A.\nChoquette-Choo, Clément Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya,\nEric Ni, Eric Noland, Geng Yan, George Tucker,\nGeorge-Christian Muraru, Grigory Rozhdestvenskiy,\nHenryk Michalewski, Ian Tenney, Ivan Grishchenko,\nJacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-\nnan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin\nMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-\ncan, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,\nMachel Reid, Maciej Mikua, Mateo Wirth, Michael\nSharman, Nikolai Chinaev, Nithum Thain, Olivier\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-\nley, Paul Michel, Petko Yotov, Rahma Chaabouni,\nRamona Comanescu, Reena Jana, Rohan Anil, Ross\nMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang,\nClément Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,\nDouglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Sen-\nter, Alek Andreev, and Kathleen Kenealy. 2024.\nGemma: Open models based on gemini research and\ntechnology.\nDirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.\n2012. Building large monolingual dictionaries at\nthe Leipzig corpora collection: From 100 to 200 lan-\nguages. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation\n(LREC’12), pages 759–765, Istanbul, Turkey. Euro-\npean Language Resources Association (ELRA).\nGoogle and North-West University. 2017. High quality\ntts data for four south african languages (af, st, tn,\nxh). Multi-speaker TTS high quality transcribed au-\ndio data for Afrikaans, Sesotho, Setswana, and isiX-\nhosa.\nGoogle Research. 2024. Gatitos.\nGovernment of Botswana. 2024. About\nour country. https://www.gov.bw/\nabout-our-country. Accessed: 2024-07-\n30.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation . Transactions of the Association\nfor Computational Linguistics , 10:522–538.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-\ngia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh\nJha, Hamish Ivison, Ian Magnusson, Yizhong\nWang, Shane Arora, David Atkinson, Russell Au-\nthur, Khyathi Raghavi Chandu, Arman Cohan, Jen-\nnifer Dumas, Yanai Elazar, Yuling Gu, Jack Hes-\nsel, Tushar Khot, William Merrill, Jacob Morrison,\nNiklas Muennighoff, Aakanksha Naik, Crystal Nam,\nMatthew E. Peters, Valentina Pyatkin, Abhilasha\nRavichander, Dustin Schwenk, Saurabh Shah, Will\nSmith, Emma Strubell, Nishant Subramani, Mitchell\nWortsman, Pradeep Dasigi, Nathan Lambert, Kyle\nRichardson, Luke Zettlemoyer, Jesse Dodge, Kyle\nLo, Luca Soldaini, Noah A. Smith, and Hannaneh\nHajishirzi. 2024. Olmo: Accelerating the science of\nlanguage models .\nHendrik J Groenewald and Liza du Plooy. 2010. Pro-\ncessing parallel text corpora for three south african\nlanguage pairs in the autshumato project. AfLaT\n2010, page 27.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nCésar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nHarkirat Singh Behl, Xin Wang, Sébastien Bubeck,\nRonen Eldan, Adam Tauman Kalai, Yin Tat Lee, and\nYuanzhi Li. 2023. Textbooks are all you need .\n6651\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY . Wu, Y . K. Li, Fuli Luo, Yingfei Xiong, and Wen-\nfeng Liang. 2024. Deepseek-coder: When the large\nlanguage model meets programming – the rise of\ncode intelligence .\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021a. Aligning ai with shared human values. Pro-\nceedings of the International Conference on Learn-\ning Representations (ICLR) .\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021b. Measuring massive multitask lan-\nguage understanding. Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR).\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models .\niAfrika. 2024. iafrika.\nAndreea Iana, Fabian David Schmidt, Goran Glava,\nand Heiko Paulheim. 2024. News without bor-\nders: Domain adaptation of multilingual sentence\nembeddings for cross-lingual news recommendation .\nPreprint, arXiv:2406.12634.\nAmir Hossein Kargaran, François Yvon, and Hinrich\nSchütze. 2024. GlotCC: An open broad-coverage\ncommoncrawl corpus and pipeline for minority lan-\nguages. Advances in Neural Information Processing\nSystems.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier\nGarcia, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, and Orhan Firat. 2023. Madlad-400:\nA multilingual and document-level large audited\ndataset. In Advances in Neural Information Process-\ning Systems , volume 36, pages 67284–67296. Cur-\nran Associates, Inc.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Efﬁ-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Sys-\ntems Principles .\nAbdullatif Köksal, Marion Thaler, Ayyoob Imani, Ah-\nmet Üstün, Anna Korhonen, and Hinrich Schütze.\n2024. Muri: High-quality instruction tuning\ndatasets for low-resource languages via reverse in-\nstructions. Preprint, arXiv:2409.12958.\nRichard Lastrucci, Isheanesu Dzingirai, Jenalea Rajab,\nAndani Madodonga, Matimba Shingange, Daniel\nNjini, and Vukosi Marivate. 2023. Preparing\nthe vuk’uzenzele and ZA-gov-multilingual South\nAfrican multilingual corpora . In Proceedings of\nthe Fourth Workshop on Resources for African In-\ndigenous Languages (RAIL 2023) , pages 18–25,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro V on Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, Jörg\nFrohberg, Mario ako, Quentin Lhoest, Angelina\nMcMillan-Major, Gerard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Muñoz, Jian\nZhu, Daniel Van Strien, Zaid Alyafeai, Khalid Al-\nmubarak, Minh Chien Vu, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ade-\nlani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai,\nJenny Chim, Violette Lepercq, Suzana Ilic, Margaret\nMitchell, Sasha Alexandra Luccioni, and Yacine Jer-\nnite. 2023. The bigscience roots corpus: A 1.6tb\ncomposite multilingual dataset .\nLive Lingua. 2024. Learn setswana .\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation .\nYinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei\nYuan. 2024. Llamax: Scaling linguistic horizons of\nllm by enhancing translation capabilities beyond 100\nlanguages. arXiv preprint arXiv:2407.05975 .\nVukosi Marivate, Moseli Mots’Oehli, Valencia Wagner,\nRichard Lastrucci, and Isheanesu Dzingirai. 2023a.\nPuoberta: Training and evaluation of a curated lan-\nguage model for setswana. In SACAIR 2023 (To Ap-\npear).\nVukosi Marivate, Daniel Njini, Andani Madodonga,\nRichard Lastrucci, Isheanesu Dzingirai, and Jenalea\nRajab. 2023b. The vuk’uzenzele south african mul-\ntilingual corpus .\nVukosi Marivate, Tshephisho Sefara, V ongani Chabal-\nala, Keamogetswe Makhaya, Tumisho Mokgonyane,\nRethabile Mokoena, and Abiodun Modupe. 2020.\nInvestigating an approach for low resource lan-\nguage dataset creation, curation and classiﬁcation:\nSetswana and sepedi . In Proceedings of the ﬁrst\nworkshop on Resources for African Indigenous Lan-\nguages, pages 15–20, Marseille, France. European\nLanguage Resources Association (ELRA).\nCindy McKellar, Roald Eiselen, and Wikus Pienaar.\n2016. Autshumato english-setswana parallel cor-\npora.\nMicrosoft. 2022. Ms terms. https://\nhuggingface.co/datasets/microsoft/\nms_terms/commits/main.\n6652\nMistral AI. 2024. Mathstral.\nKaren S. Mistry and Grace Gare. 1987. An introduc-\ntion to spoken setswana .\nKeneilwe Mokoka. 2024. Exploring machine trans-\nlation for code-switching between english and\nsetswana in south african classrooms .\nJihyung Moon, Hyunchang Cho, and Eunjeong L. Park.\n2020. Revisiting round-trip translation for quality\nestimation. In Proceedings of the 22nd Annual Con-\nference of the European Association for Machine\nTranslation, pages 91–104, Lisboa, Portugal. Euro-\npean Association for Machine Translation.\nMoseli Motsoehli. 2020. Tswanabert.\nBasel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid\nHasan, Maram Hasanain, Tameem Kabbani, Fahim\nDalvi, Shammur Absar Chowdhury, and Firoj Alam.\n2024. AraDiCE: Benchmarks for dialectal and cul-\ntural capabilities in llms .\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, et al. 2022. Crosslingual general-\nization through multitask ﬁnetuning. arXiv preprint\narXiv:2211.01786.\nNalibali. 2024. Nalibali. https://nalibali.\norg/.\nNational Education Collaboration Trust. 2024. Na-\ntional education collaboration trust. https://\nnect.org.za.\nUnited Nations. 1998. Universal declaration of human\nrights - western sotho/tswana/setswana .\nNLLB Team, Marta R. Costa-jussà, James Cross,\nOnur Çelebi, Maha Elbayad, Kenneth Heaﬁeld,\nKevin Heffernan, Elahe Kalbassi, Janice Lam,\nDaniel Licht, Jean Maillard, Anna Sun, Skyler\nWang, Guillaume Wenzek, Al Youngblood, Bapi\nAkula, Loic Barrault, Gabriel Mejia Gonzalez,\nPrangthip Hansanti, John Hoffman, Semarley Jar-\nrett, Kaushik Ram Sadagopan, Dirk Rowe, Shan-\nnon Spruit, Chau Tran, Pierre Andrews, Necip Fazil\nAyan, Shruti Bhosale, Sergey Edunov, Angela Fan,\nCynthia Gao, Vedanuj Goswami, Francisco Guzmán,\nPhilipp Koehn, Alexandre Mourachko, Christophe\nRopers, Saﬁyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022. No language left behind: Scaling\nhuman-centered machine translation .\nKelsey Norris. 2017. Sound and silence . Oxford Amer-\nican. A conversation with Gothataone Moeng.\nAyomide Odumakinde, Daniel D’souza, Pat Verga,\nBeyza Ermis, and Sara Hooker. 2024. Multilingual\narbitrage: Optimizing data pools to accelerate multi-\nlingual progress .\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data? no problem! exploring the viabil-\nity of pretrained multilingual language models for\nlow-resourced languages . In Proceedings of the 1st\nWorkshop on Multilingual Representation Learning ,\npages 116–126, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nOpenAI. 2024. Hello gpt-4o .\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt,\nSam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul\nBaltescu, Haiming Bao, Mohammad Bavarian,\nJeff Belgum, Irwan Bello, Jake Berdine, Gabriel\nBernadett-Shapiro, Christopher Berner, Lenny Bog-\ndonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa\nBrakman, Greg Brockman, Tim Brooks, Miles\nBrundage, Kevin Button, Trevor Cai, Rosie Camp-\nbell, Andrew Cann, Brittany Carey, Chelsea Carl-\nson, Rory Carmichael, Brooke Chan, Che Chang,\nFotis Chantzis, Derek Chen, Sully Chen, Ruby\nChen, Jason Chen, Mark Chen, Ben Chess, Chester\nCho, Casey Chu, Hyung Won Chung, Dave Cum-\nmings, Jeremiah Currier, Yunxing Dai, Cory De-\ncareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling,\nSheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna\nEloundou, David Farhi, Liam Fedus, Niko Felix,\nSimón Posada Fishman, Juston Forte, Isabella Ful-\nford, Leo Gao, Elie Georges, Christian Gibson, Vik\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-\nLopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Hei-\ndecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu,\nShengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain,\nShawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,\nHaozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,\nHeewoo Jun, Tomer Kaftan, ukasz Kaiser, Ali Ka-\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Jan Hendrik Kirchner,\nJamie Kiros, Matt Knight, Daniel Kokotajlo, ukasz\nKondraciuk, Andrew Kondrich, Aris Konstantinidis,\nKyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\nLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Le-\nung, Daniel Levy, Chak Ming Li, Rachel Lim,\nMolly Lin, Stephanie Lin, Mateusz Litwin, Theresa\nLopez, Ryan Lowe, Patricia Lue, Anna Makanju,\nKim Malfacini, Sam Manning, Todor Markov, Yaniv\nMarkovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney,\nChristine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke\nMetz, Andrey Mishchenko, Pamela Mishkin, Vin-\nnie Monaco, Evan Morikawa, Daniel Mossing, Tong\nMu, Mira Murati, Oleg Murk, David Mély, Ashvin\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind\nNeelakantan, Richard Ngo, Hyeonwoo Noh, Long\n6653\nOuyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rim-\nbach, Carl Ross, Bob Rotsted, Henri Roussez,\nNick Ryder, Mario Saltarelli, Ted Sanders, Shibani\nSanturkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Shep-\npard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie\nSimens, Jordan Sitkin, Katarina Slama, Ian Sohl,\nBenjamin Sokolowsky, Yang Song, Natalie Stau-\ndacher, Felipe Petroski Such, Natalie Summers, Ilya\nSutskever, Jie Tang, Nikolas Tezak, Madeleine B.\nThompson, Phil Tillet, Amin Tootoonchian, Eliz-\nabeth Tseng, Preston Tuggle, Nick Turley, Jerry\nTworek, Juan Felipe Cerón Uribe, Andrea Vallone,\nArun Vijayvergiya, Chelsea V oss, Carroll Wain-\nwright, Justin Jay Wang, Alvin Wang, Ben Wang,\nJonathan Ward, Jason Wei, CJ Weinmann, Ak-\nila Welihinda, Peter Welinder, Jiayi Weng, Lilian\nWeng, Matt Wiethoff, Dave Willner, Clemens Win-\nter, Samuel Wolrich, Hannah Wong, Lauren Work-\nman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Woj-\nciech Zaremba, Rowan Zellers, Chong Zhang, Mar-\nvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang\nZhuang, William Zhuk, and Barret Zoph. 2023. Gpt-\n4 technical report .\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow\ninstructions with human feedback. In Proceedings\nof the 36th International Conference on Neural In-\nformation Processing Systems , NIPS ’22, Red Hook,\nNY , USA. Curran Associates Inc.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics , pages 311–318.\nParliament of Botswana. 2022. Hansard.\nGuilherme Penedo, Hynek Kydlíek, Loubna Ben allal,\nAnton Lozhkov, Margaret Mitchell, Colin Raffel, Le-\nandro V on Werra, and Thomas Wolf. 2024. The\nﬁneweb datasets: Decanting the web for the ﬁnest\ntext data at scale . Preprint, arXiv:2406.17557.\nIrene Plaza, Nina Melero, Cristina del Pozo, Javier\nConde, Pedro Reviriego, Marina Mayor-Rocher, and\nMaría Grandury. 2024. Spanish and llm bench-\nmarks: is mmlu lost in translation?\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation . In Proceedings of the\nTenth Workshop on Statistical Machine Translation ,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMartin Puttkammer and Justin Hocking. 2021. Aut-\nshumato multilingual word and phrase translations .\nWord and phrase lists aligned from English to the\nother ofﬁcial South African languages.\nMartin Puttkammer, Martin Schlemmer, and Ruan\nBekker. 2021. Nchlt setswana annotated text cor-\npora. Lemmatized, part of speech tagged, and mor-\nphologically analyzed corpora developed during the\nNCHLT Text project.\nMalebogo Rahlao, Nina Lewin, and Taariq Surtee.\n2021. New uses for old books: Description of digi-\ntised corpora-based on the setswana language col-\nlection in the wits cullen africana collection. In\nProceedings of the International Conference of the\nDigital Humanities Association of Southern Africa\n(DHASA), Johannesburg, South Africa. The Univer-\nsity of the Witwatersrand, Johannesburg.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: memory optimiza-\ntions toward training trillion parameter models. In\nProceedings of the International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis , SC ’20. IEEE Press.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters . In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining , KDD ’20,\npage 35053506, New York, NY , USA. Association\nfor Computing Machinery.\nSADiLaR. 2024. South african centre for digital lan-\nguage resources (sadilar) .\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov,\nEdouard Grave, Armand Joulin, and Angela Fan.\n2021. CCMatrix: Mining billions of high-quality\nparallel sentences on the web . In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 6490–6500, Online. As-\nsociation for Computational Linguistics.\nSIL Global - AI. 2022. Bloom-lm dataset.\nhttps://huggingface.co/datasets/\nsil-ai/bloom-lm. Dataset available on\nHugging Face.\nShivalika Singh, Freddie Vargus, Daniel Dsouza,\nBörje F. Karlsson, Abinaya Mahendiran, Wei-Yin\n6654\nKo, Herumb Shandilya, Jay Patel, Deividas Mat-\naciunas, Laura OMahony, Mike Zhang, Ramith\nHettiarachchi, Joseph Wilson, Marina Machado,\nLuisa Souza Moura, Dominik Krzemiski, Hakimeh\nFadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib,\nOshan Mudannayake, Zaid Alyafeai, Vu Minh\nChien, Sebastian Ruder, Surya Guthikonda, Emad A.\nAlghamdi, Sebastian Gehrmann, Niklas Muen-\nnighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün,\nMarzieh Fadaee, and Sara Hooker. 2024. Aya\ndataset: An open-access collection for multilingual\ninstruction tuning . Preprint, arXiv:2402.06619.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin\nSchwenk, David Atkinson, Russell Authur, Ben Bo-\ngin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,\nValentin Hofmann, Ananya Harsh Jha, Sachin Ku-\nmar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian\nMagnusson, Jacob Morrison, Niklas Muennighoff,\nAakanksha Naik, Crystal Nam, Matthew E. Peters,\nAbhilasha Ravichander, Kyle Richardson, Zejiang\nShen, Emma Strubell, Nishant Subramani, Oyvind\nTafjord, Pete Walsh, Luke Zettlemoyer, Noah A.\nSmith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groen-\neveld, Jesse Dodge, and Kyle Lo. 2024. Dolma:\nan open corpus of three trillion tokens for language\nmodel pretraining research .\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low resource in-\nfrastructures. In Proceedings of the Workshop on\nChallenges in the Management of Large Corpora\n(CMLC-7) 2019. Cardiff, 22nd July 2019 , Proceed-\nings of the Workshop on Challenges in the Man-\nagement of Large Corpora (CMLC-7) 2019. Cardiff,\n22nd July 2019, pages 9 – 16, Mannheim. Leibniz-\nInstitut für Deutsche Sprache.\nTatoeba. 2024. Tatoeba.\nTeknium. 2023. Openhermes 2.5: An open dataset of\nsynthetic data for generalist llm assistants .\nThe Parliament of Zimbabwe. 2013. Con-\nstitution of zimbabwe. https:\n//parlzim.gov.zw/download/\nconstitution-of-zimbabwe-amendment-no-20-14-05-2013/ .\nAccessed: 2024-07-30.\nThe Republic of South Africa. 1996. The\nsouth african constitution. https://\nwww.justice.gov.za/constitution/\nSAConstitution-web-eng.pdf. Accessed:\n2024-07-30.\nThutong South African Education Portal. 2024. Thu-\ntong south african education portal. https://\nwww.thutong.doe.gov.za/.\nJörg Tiedemann. 2012a. Parallel data, tools and inter-\nfaces in OPUS . In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12) , pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nJörg Tiedemann. 2012b. Parallel data, tools and inter-\nfaces in OPUS . In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12) , pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nTlhalefang. 2009. Tlhalefang communications .\nAtnafu Lambebo Tonja, Bonaventure FP Dossou, Jes-\nsica Ojo, Jenalea Rajab, Fadel Thior, Eric Pe-\nter Wairagala, Aremu Anuoluwapo, Pelonomi\nMoiloa, Jade Abbott, Vukosi Marivate, et al.\n2024. Inkubalm: A small language model for\nlow-resource african languages. arXiv preprint\narXiv:2408.17024.\nTRK. 2021. Setswana mo botswana .\nLewis Tunstall, Edward Beeching, Nathan Lambert,\nNazneen Rajani, Kashif Rasul, Younes Belkada,\nShengyi Huang, Leandro von Werra, Clémentine\nFourrier, Nathan Habib, Nathan Sarrazin, Omar San-\nseviero, Alexander M. Rush, and Thomas Wolf.\n2023. Zephyr: Direct distillation of lm alignment .\nPreprint, arXiv:2310.16944.\nUnisa. 2023. Learn to speak an african language .\nEwald van der Westhuizen and Thomas Niesler. n.d.\nCorpus of multilingual code-switched soap opera\nspeech. 26.9 hours of annotated multilingual speech\nfeaturing examples of code-switching in isiZulu,\nisiXhosa, Setswana, Sesotho, and English.\nTobie van Dyk. 2021. Generic multilingual academic\nwordlists with deﬁnitions . The resource contains\n2,427 terms with part of speech and usage exam-\nples. Developed for students to assist with vocabu-\nlary building and decoding academic texts.\nCharl van Heerden, Etienne Barnard, Jaco Badenhorst,\nand Marelie Davel. n.d. Lwazi setswana asr cor-\npus. Complete audio recordings and orthographic\ntranscriptions used for Lwazi speech recognition sys-\ntems.\nDaniel van Niekerk, Etienne Barnard, Marelie Davel,\nAby Louw, and Alta de Waal. n.d. Lwazi setswana\ntts corpus . Orthographic and phonemically aligned\ntranscriptions.\nDaniel van Niekerk and Georg Schlünz. n.d. Lwazi ii\nsetswana tts corpus . Orthographic and phonemically\naligned transcriptions.\nDaniel van Niekerk, Charl van Heerden, Marelie Davel,\nNeil Kleynhans, Oddur Kjartansson, Martin Jansche,\nand Linne Ha. 2017. Rapid development of TTS cor-\npora for four South African languages . In Proc. In-\nterspeech 2017 , pages 2178–2182, Stockholm, Swe-\nden.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need .\n6655\nTiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang,\nRuoyu Fang, Huilin Wang, Zhaowei Gao, Chun-\nzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jia-\nlong Wu, Shengwei Ding, Long Li, Zhiwei Huang,\nXinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin\nChen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu,\nYi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang\nHuang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan\nGao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei\nWan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule\nSong, Xiangru Tang, Xiaohua Xu, Ningyu Zhang,\nHuajun Chen, Yuchen Eleanor Jiang, and Wangchun-\nshu Zhou. 2024. Weaver: Foundation models for\ncreative writing .\nBin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai,\nHoudong Hu, Yumao Lu, Michael Zeng, Ce Liu, and\nLu Yuan. 2023. Florence-2: Advancing a uniﬁed\nrepresentation for a variety of vision tasks .\nZhangchen Xu, Fengqing Jiang, Luyao Niu, Yun-\ntian Deng, Radha Poovendran, Yejin Choi, and\nBill Yuchen Lin. 2024. Magpie: Alignment data\nsynthesis from scratch by prompting aligned llms\nwith nothing .\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nMa, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze\nBai, Jinzheng He, Junyang Lin, Kai Dang, Kem-\ning Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng\nXue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui\nMen, Ruize Gao, Runji Lin, Shijie Wang, Shuai\nBai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu\nLiu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou,\nXingzhang Ren, Xinyu Zhang, Xipin Wei, Xu-\nancheng Ren, Xuejing Liu, Yang Fan, Yang Yao,\nYichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,\nZeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao\nFan. 2024. Qwen2 technical report .\nFei Yuan, Yinquan Lu, Wenhao Zhu, Lingpeng Kong,\nLei Li, Yu Qiao, and Jingjing Xu. 2023. Lego-MT:\nLearning detachable models for massively multilin-\ngual machine translation . In Findings of the Asso-\nciation for Computational Linguistics: ACL 2023 ,\npages 11518–11533, Toronto, Canada. Association\nfor Computational Linguistics.\nWenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie,\nYejin Choi, and Yuntian Deng. 2024. Wildchat: 1m\nchatgpt interaction logs in the wild .\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong,\nWei-Yin Ko, Daniel D’souza, Gbemileke Onilude,\nNeel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr\nKayid, Freddie Vargus, Phil Blunsom, Shayne Long-\npre, Niklas Muennighoff, Marzieh Fadaee, Julia\nKreutzer, and Sara Hooker. 2024. Aya model: An\ninstruction ﬁnetuned open-access multilingual lan-\nguage model .\n6656",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7470198273658752
    },
    {
      "name": "Training (meteorology)",
      "score": 0.7271164655685425
    },
    {
      "name": "Natural language processing",
      "score": 0.3490879535675049
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}