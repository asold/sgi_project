{
  "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
  "url": "https://openalex.org/W4389520768",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2106510227",
      "name": "Hanlin Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2314199375",
      "name": "Yifu Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105459538",
      "name": "Decheng Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976211542",
      "name": "Kai Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228808771",
      "name": "Jianchen Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227302744",
      "name": "Zhanhui Kang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4297948009",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3044934451",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963091133",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W1674493795",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W2981910001",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4295262505",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4318751318",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2964200805",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W3127829048",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4281651027"
  ],
  "abstract": "Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9119‚Äì9128\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nEasyQuant: An Efficient Data-free Quantization Algorithm for LLMs\nHanlin Tang\nranchotang@tencent.com\nYifu Sun\nyifusun@tencent.com\nDecheng Wu\nwoodchenwu@tencent.com\nKai Liu\nraccoonliu@tencent.com\nJianchen Zhu\ndickzhu@tencent.com\nZhanhui Kang\nkegokang@tencent.com\nAbstract\nLarge language models (LLMs) have proven\nto be very superior to conventional methods in\nvarious tasks. However, their expensive com-\nputations and high memory requirements are\nprohibitive for deployment. Model quantiza-\ntion is an effective method for reducing this\noverhead. The problem is that in most previous\nworks, the quantized model was calibrated us-\ning a few samples from the training data, which\nmight affect the generalization of the quantized\nLLMs to unknown cases and tasks. Hence in\nthis work, we explore an important question:\nCan we design a data-free quantization method\nfor LLMs to guarantee its generalization per-\nformance?\nIn this work, we propose EasyQuant, a training-\nfree and data-free weight-only quantization al-\ngorithm for LLMs. Our observation indicates\nthat two factors: outliers in the weight and\nquantization ranges, are essential for reducing\nthe quantization error. Therefore, in EasyQuant,\nwe leave the outliers (less than 1%) unchanged\nand optimize the quantization range to reduce\nthe reconstruction error. With these methods,\nwe surprisingly find that EasyQuant achieves\ncomparable performance to the original model.\nSince EasyQuant does not depend on any train-\ning data, the generalization performance of\nquantized LLMs are safely guaranteed. More-\nover, EasyQuant can be implemented in parallel\nso that the quantized model could be attained\nin a few minutes even for LLMs over 100B. To\nour best knowledge, we are the first work that\nachieves comparable performance with data-\ndependent algorithms under a data-free setting\nand our algorithm runs over 10 times faster than\nthe data-dependent methods.\n1 Introduction\nRecent work has already proved the superior per-\nformance of Transformer (Vaswani et al., 2017)\nbased LLMs (Workshop, 2023; Zhang et al., 2022;\nTouvron et al., 2023; Brown et al., 2020; Rae et al.,\n2021; Smith et al., 2022; Chowdhery et al., 2022;\nZeng et al., 2022) on various tasks over traditional\nmethods, and has attracted massive interest in how\nto improve and utilize those LLMs. However, the\nmodel size also grows dramatically along with im-\nproved performance. Hence the memory footprint\nand computational cost become the bottleneck for\ndeploying those models. One promising solution to\nalleviate this overhead is model quantization (Fran-\ntar et al., 2023a; Xiao et al., 2023), where we quan-\ntize weight only or weight and activation both i\norder to reduce memory consumption and compu-\ntational cost.\nAlthough model quantization is a well-studied\narea for normal-sized models, such as BERT (De-\nvlin et al., 2018) and GPT-2 (Radford et al., 2019),\nit is still a quite challenging task for LLMs. One\nmajor reason is that previous lossless model quan-\ntization algorithms require retraining for the quan-\ntized model, which is too expensive for models\nover billions of parameters. Beyond this, previous\nmodels are usually designed for specific domain\ntasks, which means the training data are sampled\nfrom limited task domains. However, recent LLMs\nare usually trained on various domains of data cor-\npus, and they have shown to be quite effective for\nmulti-domain zero-shot tasks. In this case, if we\nonly retrain the quantized LLMs using partial do-\nmain corpus, the generalization ability of LLMs\nmight get worse. Therefore both efficiency and\ngeneralization guarantees are very important for\ndesigning LLMs quantization algorithms. To date,\nfor low-bits weight-only quantization, several post-\ntraining algorithms have been proposed (Frantar\net al., 2023a; Yao et al., 2022). However, those\nmethods also require a small calibration set sam-\npled from training data, which still takes at least\nseveral hours. Moreover, the use of those calibra-\ntion data also brings the risk of making the model\noverfit to the calibration set.\n9119\nùê™ùê´ùêöùêßùê†ùêû=ùüèùüéùüé\nùê™ùê´ùêöùêßùê†ùêû=ùüë\nOutlierisolation\nOptimizingùííùíìùíÇùíèùíàùíÜ\nùê™ùê´ùêöùêßùê†ùêû=ùüè.ùüì\nAddEasyQuant\nFigure 1: Pipeline of EasyQuant. We first find all the outliers in weight and keep them in full precision\n(fp32/fp16/bf16). Afterward, we optimize the quantization range (denoted as qrange) in order to approximate\nthe normal values more precisely. In the end, the normal values are quantized into lower bits (denoted as Q[¬∑]) with\noptimized quantization ranges and we set the outliers unchanged in weight.\nOur Contribution: In this work, we propose\na novel data-free model quantization algorithm,\nnamely EasyQuant, that potentially improves the\nperformance of low-bits quantized LLMs. The gen-\neralization ability of LLMs is inherently guaran-\nteed since EasyQuant does not need any input data.\nBy running EasyQuant for only a few minutes, we\ncan quantize public-available OPT-176B, BLOOM-\n176B, and LLAMA-65B into lower bits without\nsignificant loss on various benchmarks. To our best\nknowledge, this is the first data-free LLM quan-\ntization algorithm for LLM quantization without\nnotable system overhead.\nMoreover, our work reveals the essential fac-\ntors that cause the performance degradation of the\nquantized LLMs. We show that the outliers in\nweights are more critical to the model‚Äôs perfor-\nmance compared to the normal elements. Beyond\nthis, we propose to use a gradient-based method\nfor optimizing the quantization range. These two\nstrategies can also be used in other scenarios, such\nas weight-activation quantization and quantization-\naware training (QAT).\nLast but not least, we develop efficient CUDA\nkernels for outlier isolation in dequantization, and\nproved that hold 1% outliers in weights unquan-\ntized brings negligible (less than 0.1%) overhead\nw.r.t to overall latency. We also propose to im-\nplement EasyQuant in parallel for quantizing each\nweight in the model, which means a 175B-sized\nmodel can be quantized into 4-bits within 10 min-\nutes.\n2 Background and Motivation\nThe most widely used quantization method, namely\nrounding to nearest-number ( RTN), quantizes a\ntensor x into k-bits representation according to\nQ[x] = s√ó\n‚åä\nclamp\n(x\ns,lmin,lmax\n)‚åâ\n. (1)\nHere sis the quantization scale, lmin and lmax are\nthe lower and upper bound for clipping, and ‚åä¬∑‚åâ\nis the rounding operator. Usually we set lmin =(\n‚àí2k‚àí1 + 1\n)\nand lmax = 2k‚àí1 and set sto be the\nmaximum absolute value in x.\nThere are two major directions for finding the best\nconfiguration in weight-only LLM quantization.\nThe first is to minimize the reconstruction error\nof the weight parameter (denoted as W), which is\ndefined as\nr(W) := ‚à•Q[W] ‚àíW‚à•2.\nNotice that in this case we only need to have access\nto the weight itself, therefore it is data-free.\nBeyond this, recent studies (Frantar et al., 2023a;\nYao et al., 2022) propose to use the output error,\ndefined as\ne(W) =\n‚àë\nX‚ààD\n‚à•Q[W]X‚àíWX‚à•2 ,\nwhere Dis a calibration set sampled from the orig-\ninal training data, for optimization. This regulation\ntries to mimic the outputs from the original model\ndirectly hence achieving a more promising result\nthan reconstruction-based methods.\nData-dependent calibration might weaken the\ngeneralization ability of LLMs However, the\nperformance gain from using calibration data might\njeopardize the generalization of the quantized\nmodel, because it brings the risk of making the\nmodel overfit to the calibration set. For example,\n9120\n0\n100 200 300 400 500\nOptimizing step\n30000\n35000\n40000\n45000Reconstruction Error\nreconstruction error\n13\n14\n15\n16\n17\n18\n19\n20\nPerplexity on WikiT ext2\nReconstruction Error v.s. Model's Performance\nperplexity\n0\n100 200 300 400 500\nOptimizing step\n20000\n25000\n30000\n35000\n40000\n45000Reconstruction Error\nreconstruction error\n11.6\n11.7\n11.8\n11.9\n12.0\n12.1\nPerplexity on WikiT ext2\nReconstruction Error v.s. Model's Performance Without Outlier\nperplexity\nFigure 2: Smaller reconstruction error cannot guarantee a better model performance. Straightforwardly shrinking\nthe quantization ranges will clip most of the outliers to be very small, hence the perplexity increases severely\nsince those outliers are critical for preserving the model‚Äôs performance. However, when keeping those outliers\nunquantized, the quantized model achieves a better performance as the reconstruction error decreases continuously.\nThis result clearly suggests that the outliers are more important than the normal values in weight, and optimizing the\nquantization ranges using gradient defined in (2) can significantly increase the accuracy of quantized models. More\ndetails about the experiment can be found in Section 5.\nboth ZeroQuant and GPTQ involve changing the\noriginal weight by training or OBS in order to min-\nimize the output error, therefore the distribution\nof the weight‚Äôs parameters might deviate from the\noriginal. Since the calibration data is usually sam-\npled from a few specific domains, the performance\nof the calibrated model on other tasks may not be\nguaranteed.\nData-free quantization is challenging, but very\nimportant Although it‚Äôs more challenging to use\nthe reconstruction error as a regulation because it\ncan only optimize the quantized model indirectly,\nstill it is a very important direction for researching\nbecause the generalization ability of the model is\ninherently guaranteed when using data-free quanti-\nzation since it uses no training data. Therefore in\nthis paper, we aim to answer the following ques-\ntion:\nHow can we efficiently recover the performance\nof the quantized model without using any input\ndata?\nIn this work we propose EasyQuant, a data-free fast\nalgorithm that could significantly improve the per-\nformance of quantized LLMs in a data-free setting,\nand more importantly, even outperforms the results\nfrom data-dependent quantization algorithms. Our\nexperiments reveal that the performance gap of the\nlower bits (e.g. 4-bits) quantized LLMs origins\nfrom two factors:\n1. Setting the quantization range as the maxi-\nmum absolute value of the weight induces a\nlarge reconstruction error for low-bits quanti-\nzation.\n2. The outliers in the weight matrix, which ac-\ncount for less than0.1% of the parameters, im-\npose a very important influence on the model‚Äôs\nperformance.\nIn EasyQuant, we use quantization range mini-\nmization and outlier isolation to address these two\nchallenges, and our results prove that EasyQuant\nachieves a significant improvement over RTN.\n3 Insight behind EasyQuant\nAs mentioned above, the weight‚Äôs outliers and\nquantization ranges are essential to the quantized\nmodel‚Äôs performance. Below we present the sup-\nporting experiments in detail.\n3.1 The quantization range can be efficiently\noptimized using gradient\nAlthough the quantization operation itself is non-\ndifferentiable, the gradient of the reconstruction\nerror (‚à•Q[x] ‚àíx‚à•2) w.r.t. the quantization range\nsis differentiable in most cases. We proved that\nthe gradient of the quantization range sadmits (see\nSection 4 for more details)\n‚àÇ‚à•Q[x] ‚àíx‚à•2\n‚àÇs = 2\n‚àë\ni\n(\n(Q[xi] ‚àíxi)\n‚åäxi\ns\n‚åâ)\n.\n(2)\nWith this gradient, the reconstruction error can be\nquickly minimized within hundreds of steps (see\nFigure 2 for more details). This result indicates that\nby shrinking the quantization range, most of the\nparameters in weight can be approximated more\nprecisely. However, as shown in Figure 2, the per-\nformance of the quantized weight gets even worse\n9121\nas the reconstruction error decreases. This is a very\ncounter-intuitive result.\nThrough in-depth analysis, we realized that\nwhen decreasing the quantization range, more\nsalient parameters outside the quantization range\nwould be clipped out. Although most of the weights\nget approximated more precisely as indicated by\nthe decreased reconstruction error, the salient pa-\nrameters are poorly represented. As the model per-\nformance drops severely in this case, we realized\nthat those outliers are way more important than the\nnormal elements for the model‚Äôs performance.\n3.2 Outliers in weight are very important, but\nnot sufficient\nBefore we further discuss the influence of those\noutliers, we first provide a (nœÉ) criterion for defin-\ning the outliers in weight. For any weight W, we\nsay its (i,j)-th number Wi,j is an (nœÉ) outlier if\n|Wi,j ‚àímean(W)|‚â• n‚àóvar(W), (3)\nwhere mean(W) and var(W) are the mean and\nvariance of W.\nNow the question is: Can we hold those out-\nliers unchanged and straightforwardly compress\nthe normal elements into lower bits?Unfortunately,\nour result suggests that excluding the outliers from\nquantization solely is not enough. As shown in\nTable 1, the performance gap still exists even when\nwe hold 1% numbers in fp16. The problem is that\nif we keep too many numbers in fp16, the overhead\nof the dequantization kernel would also increase\nand result in a decreased overall throughput.\n3.3 EasyQuant potentially improve the\nperformance\nAs shown in Section 3.1 and Section 3.2, optimiz-\ning the quantization ranges directly reduces the\nmodel‚Äôs performance drops severely because of the\nclipped outliers. These key observations inspire\nus to design EasyQuant, in which we isolate the\noutliers from quantization first and then optimizing\nthe quantization range for the remaining elements.\nAs shown in the right part of Figure 2, with outliers\nbeing kept unquantized, the performance of the\nquantized model increases continuously under de-\ncreased reconstruction. This clearly proves we can\npotentially improve the performance of quantized\nLLMs with this strategy.\n4 Methodology\n4.1 Driving of the gradient in (2)\nLet‚Äôs say the original scalesgets an infinitely small\nvariation ‚àÜs, which means\n‚åä x\ns+ ‚àÜs\n‚åâ\n=\n‚åäx\ns\n‚åâ\n, if x\ns ‚àí\n‚åä x\ns+ ‚àÜs\n‚åâ\nÃ∏= 0.5.\nTherefore we get\nQs+‚àÜs[x] =(s+ ‚àÜs)\n‚åä x\ns+ ‚àÜs\n‚åâ\n=(s+ ‚àÜs)\n‚åäx\ns\n‚åâ\n,\nthis leads to\n‚àÇQ[x]\n‚àÇs = Qs+‚àÜs[x] ‚àíQs[x]\n‚àÜs =\n‚åäx\ns\n‚åâ\n.\nThis gives us\n‚àÇ‚à•Q[x] ‚àíx‚à•2\n‚àÇs\n=2\n‚ü®\nQ[x] ‚àíx,‚àÇQ[x]\n‚àÇs\n‚ü©\n=2\n‚£®\nQ[x] ‚àíx,\n‚åäxi\ns\n‚åâ‚ü©\n=2\n‚àë\ni\n(\n(Q[xi] ‚àíxi)\n‚åäxi\ns\n‚åâ)\n.\n4.2 Algorithm description\nIn EasyQuant, for each weightW, we first select all\n(nœÉ) outliers (using (3)) and store its index Io(W).\nAfterward, for the normal elements, we optimize\nthe per-channel quantization range using an opti-\nmizer (in our case we use Adam for example) with\ngradients defined in (2). The final quantized weight\nfrom EasyQuant can be formulated as\nQEasyQuant[W]\n=Masko(W) ‚àóW + (1 ‚àíMasko(W)) ‚àóQ[W],\n(4)\nwhere Masko is a mask tensor defined as\nMasko\ni,j(W) =\n{ 1 if (i,j) ‚ààIo(W),\n0 if (i,j) /‚ààIo(W). (5)\nThe detailed description of EasyQuant is in Algo-\nrithm 1.\n9122\nThreshold n(BLOOM-7B) Baseline 1 2 4 6\nPPL on WikiText2 11.37 12 .153 12 .495 12 .518 12 .536\nTable 1: Isolating outliers in weight from quantization can increase the model‚Äôs performance. Here nrefers to the\nhyper-parameter in the outlier criterion (nœÉ) as defined in (3) and baseline is the result from unquantized model.\nNotice that even with 10%(n= 1) numbers being held unquantized, there is still a large gap to the baseline. This\nmeans isolating the outliers is not enough to fully recover the accuracy of quantized models.\nAlgorithm 1 EasyQuant\n1: Initialize: outlier threshold n, hyper-parameters for opti-\nmizer A, original weight W.\n2: Quantize:\n3: According to (3), compute the index Io(W) of the\n(nœÉ) outliers in W.\n4: Optimizing the quantization range susing optimizer\nAwith gradient defined in (2).\n5: Quantize W into Q[W].\n6: Dequantize:\nQEasyQuant[W] = Masko(W) ‚àó W +\n(1 ‚àíMasko(W) ‚àó Q[W], where Masko(W) is\ndefined in (5).\n5 Experiment\nBaselines: We compare EasyQuant with several\nbaselines in the INT4 quantization setting below:\n‚Ä¢ RTN: The model‚Äôs weights are naively quan-\ntized according to (1).\n‚Ä¢ ZeroQuant: The algorithm proposed in Yao\net al. (2022). Authors treat each layer as\na small neural network and use the origi-\nnal as the teacher model to distill the quan-\ntized one. This is equivalently minimizing‚àë\nx‚ààD‚à•f(WT ; x) ‚àíf(WS; x)‚à•2 where x\nare the input activations, WT is the weight\nof the original model and WS is the quantized\nmodel.\n‚Ä¢ GPTQ: This algorithm is proposed in Frantar\net al. (2023a). Authors use the same objective\nfunction ‚àë\nx‚ààD‚à•f(WT ; x)‚àíf(WS; x)‚à•2 as\nin ZeroQuant. But they utilize OBS for min-\nimizing the loss function instead of using a\ngradient-based optimizer.\nExperiment Setup. For all models, we set the\noutlier threshold n‚àà[2.5,3] in order to ensure that\nthe outliers account less than 1% of all numbers.\nFor BLOOM and LLAMA, we use n= 3. When\noptimizing the quantization ranges, we use Adam\nas the optimizer and set the learning rate 1e‚àí3\nfor BLOOM and 1e‚àí4 for LLAMA. We choose\nthe quantization ranges from step 100 for BLOOM\nand 500 for LLAMA. We use symmetric quanti-\nzation since the normal values are symmetrically\ndistributed with the outliers being excluded. For a\nfair comparison, we use per-channel quantization\nfor weight in all algorithms (which means each\ncolumn shares one common quantization range).\nEvaluation Tasks. As for the evaluation tasks,\nwe mainly focus on perplexity-based tasks, as they\nare known to be particularly sensitive to model\nquantization Frantar et al. (2023b). The perplex-\nity tasks we include are WikiText2 (Merity et al.,\n2016), Penn Treebank (Marcus et al., 1994) and C4\n(Raffel et al., 2020). The zero-shot tasks‚Äô results\nare also provided, such as PIQA (Tata and Patel,\n2003), ARC (Boratko et al., 2018) and StoryCloze\n(Mostafazadeh et al., 2017).\nImplementation. Since each weight can be quan-\ntized in parallel, therefore we use 8‚àóA100 for run-\nning EasyQuant, and we finish the quantization in\n1 ‚àº10 mins for all models. We store the index and\nvalue for all outliers together with the quantized\nnormal values. Our dequantization kernel is built\nusing CUDA.\n5.1 Experiment Analysis\nWe focus our study on LLM by quantizing the\nentire BLOOM, and LLAMA model families to\n4-bit.\nPerplexity-base tasks. We first study perplexity-\nbased tasks. On LLaMA models, Table 2 shows\nthat EasyQuant outperforms GPTQ in most cases.\nFor LLaMA-65B, GPTQ drops 4.21 points on\nPTB, performing worse than the 9 √ó smaller\nfull-precision 7B model, while EasyQuant still\nperforms well on this task. On the other tasks,\nEasyQuant losing only 0.4‚Äì0.7 points. BLOOM\nshows a similar pattern (see Table 10 in ap-\npendix): EasyQuant drops only 0.1-0.16 points\non perplexity-based tasks. Notice that we observe\na smaller gap between our method and GPTQ on\nC4. It is mostly because, as a data-calibrated quan-\ntization method, GPTQ uses C4 dataset for calibra-\n9123\nPerplexity-based Task Perplexity-based Task\nWikiText2 PTB C4 WikiText2 PTB C4\nLLAMA‚Äì7B\nfp16 5.68 8 .80 7 .08\nLLAMA‚Äì33B\nfp16 4.10 7 .30 5 .98\nRTN 6.29 11 .25 8 .12 RTN 4.54 8 .65 6 .54\nGPTQ 6.09 11 .56 7 .78 GPTQ 4.45 8.44 6.40\nEasyQuant 6.01 10.72 7.71 EasyQuant 4.34 8.45 6.37\nLLAMA‚Äì13B\nfp16 5.09 8 .07 6 .61\nLLAMA‚Äì65B\nfp16 3.53 6 .91 5 .62\nRTN 5.53 9 .77 7 .23 RTN 3.99 10 .67 6 .45\nGPTQ 5.36 9 .49 7 .07 GPTQ 4.13 11 .12 6 .38\nEasyQuant 5.29 9.37 6.97 EasyQuant 3.98 9.61 6.30\nTable 2: Perplexity results for LLAMA model family\ntions.\nZeroshot tasks. For most zero-shot tasks,\nEasyQuant achieves harmless performance with\nonly 0.1 %-0.52% accuracy drops as shown in\nTable 10 in appendix and outperforms GPTQ on\nmost cases. Here we simply use the implementa-\ntion of GPTQ on LLAMA from its git.1 We note\nthat EasyQuant can be further improved via finer-\ngranularity grouping. However, we will not include\nthis overhead in this paper.\noutlier ratio overhead\n0.01% 0.027ms\n0.10% 0.055ms\n0.50% 0.093ms\n1% 0.117ms\n5% 0.186ms\n10% 0.212ms\nTable 3: Overhead of outlier isolation on A100\nPractical Latency. We evaluate the overhead\nof EasyQuant by comparing the overhead of out-\nlier isolation, int4 dequantization, and matrix mul-\ntiplication with batch size 1, sequence length\n1024, on a single A100 GPU. The matrix size is\n14336 √ó53746 which is the same as the first FFN\nlayer in 176B BLOOM. For outlier isolation, we\ntest the latency of outliers ratio (fraction of outliers\nwithin the weight) in 6 settings: (0.01%, 0.10%,\n0.50%, 1%, 5%, 10%). The matrix multiplication\ntakes 83ms and dequantization takes 5ms. There-\nfore from Table 3 we can see that recovering the\noutliers in weight brings almost no overhead to the\noverall latency.\nAblation study. To understand the effect of un-\nstructured outliers, we show the perplexity result of\nEasyQuant without outlier isolation or quantization\n1https://github.com/qwopqwop200/GPTQ-for-LLaMa\nrange optimization. As discussed in Section 3, both\nstrategies impose a very important influence on the\nfinal model performance.\nWe further conduct experiments proving whether\nthe performance gain mainly comes from the out-\nlier isolation: Actually, outlier isolation is a very\nimportant component of EasyQuant, but still not\nenough to fully recover the performance loss from\nquantization. Keeping even 10% of weights as fp16\noutliers still admits about 8% ppl increase while\nEasyQuant admits only 1 % ppl increase. Below\nwe present the result of 4-bit quantized BLLOM-\n7B when we just keep 1% outliers in fp16 without\nquantization range optimization on various bench-\nmarks.\nBenchmark EasyQuant 1% fp16 outlier\nWikiText2(PPL) 11.66 12.52\nPTB (PPL) 21.42 23.32\nC4(PPL) 15.46 16.44\nPIQA (ACC) 73.61% 72.74%\nTable 4: Using outlier isolation solely is not enough to\nfully recover the performance loss. EasyQuant consis-\ntently outperforms outlier isolation in all benchmarks.\nOutlier influence. The outlier isolation is a key\ncomponent in EasyQuant, but it can only impose\nan indirect influence on the model accuracy. The\ninteresting phenomenon we find is that the outliers\nbehave like a gating mechanism: without outlier\nisolation, the model achieves a much worse perfor-\nmance under a small reconstruction error; however,\nwhen keeping those outliers in fp16, the quantized\nLLM attains a continuously decreased ppl under\nsmaller reconstruction error:\nMoreover, we have also conducted a comple-\nmentary experiment testing the direct influence of\nthe weight outlier: We prune 1% of the values (\naccording to its magnitude) in weights into 0 and\nsee the ppl results (as shown in Table 6). It has\n9124\nreconstruction error int4 outlier fp16 outlier\n4.8E4 12.65 12.50\n3.5E4 14.73 11.61\n2.7E4 19.71 11.25\n2.3E4 NA 11.10\n1.9E4 NA 11.02\nTable 5: ppl results on Wikitext2 of BLOOM-7B with\nand without outlier isolation.\nshown that the largest value (outliers) imposes the\nsame influence on the model performance as the\nnormal values (median), which means those out-\nliers share the same direct influence on the model\naccuracy with normal values. Therefore outlier\nisolation imposes a key influence on the model\naccuracy indirectly.\npruned weights PPL\nsmallest (top-0% 1%) 11.66\nmedian (top-49% 50%) 19.16\nlargest (top-99% 100%) 19.17\nTable 6: ppl results after pruning 1% weight with differ-\nent magnitude\nOutlier distribution. We also explore the outlier\ndistribution along different modules and layers. It\nshows that the fraction of outliers shares different\npatterns in different modules and layers (as shown\nin Table 7 and 8). FFN.2 has a significantly higher\nfraction of outliers. However, it shows no pattern\nalong the layer index.\nmodule name outlier fraction (%)\nAtt.qkv 0.2993\nAtt.output 0.5036\nFFN.1 0.288\nFFN.2 0.7560\nTable 7: Outlier fraction distribution in different mod-\nules in BLOOM-7B under 3-sigma threshold\nQuantization range. The dynamic of the quanti-\nzation range is shown in Table 9. Roughly speak-\ning, this range decreases fast in the early stage of\ntraining, which means a smaller quantization range\nwill make most of the parameters to be quantized\nmore precisely. After certain steps of training, the\nquantization range becomes stable, this means we\nhave already achieved the optimal range.\nLayer index outlier fraction (%)\n1 0.3187\n5 0.8579\n10 0.3953\n15 0.3975\n20 0.3962\n25 0.4399\n30 0.3954\nTable 8: Outlier fraction distribution in different layer\nindex in BLOOM-7B under 3-sigma threshold\nsteps quantization range\n0 0.078\n10 0.069\n50 0.052\n100 0.048\n150 0.047\n200 0.047\nTable 9: The dynamic quantization range of different\noptimization steps. Here we take the quantization range\nof the Att.qkv module in layer 1 as an example.\n6 Related Work\nModel Quantization Traditional model quanti-\nzation algorithms mainly focus on the cases where\nboth parameters and activations of the model are\nquantized (Lin et al., 2015; Hubara et al., 2016;\nTailor et al., 2021; Ni et al., 2020). However, di-\nrectly quantizing the model will greatly decrease\nthe accuracy of the models, and one important tech-\nnique to improve the performance is Quantization\nAware Training (QAT) (Jacob et al., 2018), where\nit simulates the quantization procedure in training\nto improve the accuracy of the quantized model\nfurther. For Transformer based models, the bound-\nary of the compression level has been continuously\nadvanced. For example, 8-bits quantized transform-\ners as in FullyQT (Prato et al., 2019) and Q8BERT\n(Zafrir et al., 2019), 4-bits quantized BERT in Wu\net al. (2023) and tenary case as in TernaryBERT\n(Zhang et al., 2020).\nModel Quantization for LLMs. For quantizing\nLLMs, due to their prohibitive training expense,\nwe can only use a few training data for calibration.\nThere are two major directions: 1) weight-only\nquantization, where the weights are quantized into\nlower bits. In Frantar et al. (2023a); Yao et al.\n(2022), authors optimize the output error on the\ncalibration set using OBS and gradient descent. 2)\n9125\nActivation and weight quantization, where both ac-\ntivations and weights are quantized into lower bits.\nIn this case, the major obstacle is the outliers in\nactivations. LLM.int8() (Dettmers et al., 2022) ad-\ndresses this problem by isolating those outliers in\nfp16/bf16. However, such implementation leads to\nlarge latency overhead and is even slower than fp16\ninference. Recent studies (Wei et al., 2023; Xiao\net al., 2023) found that the outliers only exist in cer-\ntain channels, and use the LayerNorm weights (Wei\net al., 2023) and calibrated scales (Xiao et al., 2023)\nto smooth those channels. Xiao et al. (2023) has\nalready proved that we can achieve almost loss-\nless W8A8 quantized LLMs using a few calibra-\ntion data, without manipulating the original model\nweights.\n7 Conclusion and Limitations\nIn this paper, we propose a data-free fast weight-\nonly quantization algorithm, namely EasyQuant,\nfor LLMs, that potentially improves the quantized\nmodel‚Äôs performance without using any training\ndata. Our analysis reveals the intrinsic origins of\nthe performance loss when quantizing the model\nweights into lower bits. We show that by isolat-\ning the outliers from quantization, the accuracy\nof the quantized LLM increases accordingly with\ndecreased reconstruction error. Our experiment\nproved that EasyQuant significantly outperforms\nRTN in a data-free setting, and also behaves bet-\nter than data-dependent algorithms. EasyQuant\ncan finish the quantization for a 176B-sized model\nwithin 10 minutes and the overhead of dequantiza-\ntion in EasyQuant is negligible.\nHowever, we also point out some limitations of\nour work: The outlier recovery functionality in\nEasyQuant requires extra CUDA kernels for im-\nplementation. Moreover, weight-only quantization\ncan only reduce the memory footprint without any\ncomputation cost reduction, hence the latency of\nour model cannot be minimized. In addition, this\noutlier isolation will make the weight/activation\nquantization more challenging because the weight\nincludes numbers under different precision. We\nhave also noticed that EasyQuantcannot outper-\nform the data-dependent methods in all tasks, this\nmotivates us to investigate more effective algo-\nrithms in future studies.\nReferences\nMichael Boratko, Harshit Padigela, Divyendra Mikki-\nlineni, Pritish Yuvraj, Rajarshi Das, Andrew McCal-\nlum, Maria Chang, Achille Fokoue-Nkoutche, Pa-\nvan Kapanipathi, Nicholas Mattei, et al. 2018. A\nsystematic classification of knowledge, reasoning,\nand context within the arc dataset. arXiv preprint\narXiv:1806.00358.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\ncation for transformers at scale.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. Cite arxiv:1810.04805Comment: 13 pages.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023a. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023b. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. 2016. Binarized neu-\nral networks. In Advances in neural information\nprocessing systems, pages 4107‚Äì4115.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018. Quanti-\nzation and training of neural networks for efficient\ninteger-arithmetic-only inference. pages 2704‚Äì2713.\nZhouhan Lin, Matthieu Courbariaux, Roland Memi-\nsevic, and Yoshua Bengio. 2015. Neural net-\nworks with few multiplications. arXiv preprint\narXiv:1510.03009.\nMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz,\nRobert MacIntyre, Ann Bies, Mark Ferguson, Karen\nKatz, and Britta Schasberger. 1994. The penn tree-\nbank: Annotating predicate argument structure. In\nHuman Language Technology: Proceedings of a\nWorkshop held at Plainsboro, New Jersey, March\n8-11, 1994.\n9126\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels.\nNasrin Mostafazadeh, Michael Roth, Annie Louis,\nNathanael Chambers, and James Allen. 2017. Ls-\ndsem 2017 shared task: The story cloze test. In\nProceedings of the 2nd Workshop on Linking Models\nof Lexical, Sentential and Discourse-level Semantics,\npages 46‚Äì51.\nRenkun Ni, Hong-min Chu, Oscar Casta√±eda, Ping-\nyeh Chiang, Christoph Studer, and Tom Gold-\nstein. 2020. Wrapnet: Neural net inference with\nultra-low-resolution arithmetic. arXiv preprint\narXiv:2007.13242.\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh. 2019. Fully quantized trans-\nformer for improved translation. arXiv preprint\narXiv:1910.10485.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485‚Äì5551.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nShyam A Tailor, Javier Fernandez-Marques, and\nNicholas D Lane. 2021. Degree-quant: Quantization-\naware training for graph neural networks. Interna-\ntional Conference on Learning Representations.\nSandeep Tata and Jignesh M Patel. 2003. Piqa: An\nalgebra for querying protein data sets. In 15th In-\nternational Conference on Scientific and Statistical\nDatabase Management, 2003., pages 141‚Äì150. IEEE.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao\nGong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and\nXianglong Liu. 2023. Outlier suppression: Pushing\nthe limit of low-bit transformer language models.\nBigScience Workshop. 2023. Bloom: A 176b-\nparameter open-access multilingual language model.\nXiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi,\nZhewei Yao, and Yuxiong He. 2023. Understanding\nint4 quantization for transformer models: Latency\nspeedup, composability, and failure cases.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,\nJulien Demouth, and Song Han. 2023. Smoothquant:\nAccurate and efficient post-training quantization for\nlarge language models.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. arXiv\npreprint arXiv:1910.06188.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. arXiv preprint\narXiv:2009.12812.\nA Appendix\n9127\nPerplexity-based Task Zero-shot Task\nWikiText2 PTB C4 PIQA ARC-easy ARC-Challenge StoryCloze\nBLOOM fp16 22.42 43 .69 26 .6 65 .07% 41 .71% 24 .15% 61 .94%\nRTN 25.90 51 .10 29 .89 63 .11% 39 .40% 23 .89% 60 .15%\n560M GPTQ 24.03 46 .97 28 64.31 % 40 .24% 23 .46% 61.17%\nEasyQuant 23.74 46.86 28.03 63 .06% 40.32% 24.15% 59 .64%\nBLOOM fp16 17.69 57 .96 22 .05 67 .14% 45 .41% 25 .68% 63 .27%\nRTN 22.00 66 .85 24 .44 65 .29% 42 .51% 23 .34% 60 .66%\n1.1B GPTQ 19.05 62 .48 23 .25 66 .05% 44.49% 25 .51% 62.32%\nEasyQuant 18.51 61.83 22.94 66.65 % 43 .73% 25 .51% 62 .06%\nBLOOM fp16 15.39 30 .00 19 .49 69 .97% 48 .11% 26.79 % 65.44%\nRTN 16.97 33 .58 21 .26 67 .74% 44 .70% 26.45 % 62.95%\n1.7B GPTQ 16.48 31 .84 20 .55 68 .77% 44 .49% 25 .94% 64 .48%\nEasyQuant 16.01 31.50 20.15 68.99 % 46.89% 26.19% 65.37%\nBLOOM fp16 13.48 25 .34 17 .49 70 .51% 53 .24% 30.55 % 67.79%\nRTN 14.76 27 .68 18 .76 69 .86% 51 .35% 29 .52% 67 .09%\n3B GPTQ 14.2 26 .49 18 .1 69 .42% 52.82% 28.92% 67 .22%\nEasyQuant 14.01 26.12 17.96 69.80 % 50 .72% 28 .58% 67.35%\nBLOOM fp16 11.37 20 .83 15 .20 73 .72% 57 .37% 33.45 % 71.99%\nRTN 12.10 22 .42 16 .06 72 .69% 56 .14% 32.17 % 70.72%\n7.1B GPTQ 11.73 21 .67 15 .6 72 .96% 56.14% 32 .25% 71.36%\nEasyQuant 11.66 21.47 15.52 73.23 % 55 .72% 32.51 % 71 .10%\nBLOOM fp16 8.11 14 .59 11 .71 79 .16% 67 .47% 44.97 % 76.89%\nRTN 8.37 15 .00 12 .04 79 .00% 66 .33% 43.17 % 76.00%\n176B GPTQ 8.21 14 .75 11.81 79.00% 67 .42% 44 .10% 76 .32%\nEasyQuant 8.21 14 .75 11 .87 79.05% 67.8% 44.45% 77.28%\nTable 10: Perplexity and zershot results for BLOOM model family\n9128",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.8561221361160278
    },
    {
      "name": "Linde‚ÄìBuzo‚ÄìGray algorithm",
      "score": 0.6102613806724548
    },
    {
      "name": "Computer science",
      "score": 0.6031457185745239
    },
    {
      "name": "Algorithm",
      "score": 0.5596531629562378
    },
    {
      "name": "Outlier",
      "score": 0.5005218982696533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2112804353237152
    }
  ],
  "cited_by": 4
}