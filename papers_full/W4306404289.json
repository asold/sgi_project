{
  "title": "iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations",
  "url": "https://openalex.org/W4306404289",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3201668921",
      "name": "Junru Jin",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2100829483",
      "name": "Yingying Yu",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2143871633",
      "name": "Ruheng Wang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2021422979",
      "name": "Xin Zeng",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2042434844",
      "name": "Chao Pang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2014579076",
      "name": "Yi Jiang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2199657526",
      "name": "Zhongshen Li",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2224469997",
      "name": "Yutong Dai",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2109743289",
      "name": "Ran Su",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2181331778",
      "name": "Quan Zou",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1976656464",
      "name": "Kenta Nakai",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2112711241",
      "name": "Leyi Wei",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A3201668921",
      "name": "Junru Jin",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2100829483",
      "name": "Yingying Yu",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2143871633",
      "name": "Ruheng Wang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2021422979",
      "name": "Xin Zeng",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2042434844",
      "name": "Chao Pang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2014579076",
      "name": "Yi Jiang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2199657526",
      "name": "Zhongshen Li",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2224469997",
      "name": "Yutong Dai",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2109743289",
      "name": "Ran Su",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2181331778",
      "name": "Quan Zou",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1976656464",
      "name": "Kenta Nakai",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2112711241",
      "name": "Leyi Wei",
      "affiliations": [
        "Shandong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1987557775",
    "https://openalex.org/W2080324830",
    "https://openalex.org/W2149585679",
    "https://openalex.org/W1970145665",
    "https://openalex.org/W2968658941",
    "https://openalex.org/W1987735941",
    "https://openalex.org/W3086030569",
    "https://openalex.org/W2059099579",
    "https://openalex.org/W2106340111",
    "https://openalex.org/W1984783889",
    "https://openalex.org/W2046646392",
    "https://openalex.org/W2046137036",
    "https://openalex.org/W3007033512",
    "https://openalex.org/W3034527519",
    "https://openalex.org/W2960629037",
    "https://openalex.org/W2979999916",
    "https://openalex.org/W3130800274",
    "https://openalex.org/W4210371902",
    "https://openalex.org/W3045813658",
    "https://openalex.org/W4200281017",
    "https://openalex.org/W3011537067",
    "https://openalex.org/W3204213738",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2565541933",
    "https://openalex.org/W2951959332",
    "https://openalex.org/W3139265997",
    "https://openalex.org/W2103777723",
    "https://openalex.org/W2952479320",
    "https://openalex.org/W2120889911",
    "https://openalex.org/W2955023201",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W2945038365",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W2988126442",
    "https://openalex.org/W4293580221",
    "https://openalex.org/W4300996741",
    "https://openalex.org/W2082893943",
    "https://openalex.org/W2944860976",
    "https://openalex.org/W2901134781",
    "https://openalex.org/W2746990521",
    "https://openalex.org/W2464063823",
    "https://openalex.org/W3045759143",
    "https://openalex.org/W4306404289",
    "https://openalex.org/W6894097108",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2786672974"
  ],
  "abstract": "Abstract In this study, we propose iDNA-ABF, a multi-scale deep biological language learning model that enables the interpretable prediction of DNA methylations based on genomic sequences only. Benchmarking comparisons show that our iDNA-ABF outperforms state-of-the-art methods for different methylation predictions. Importantly, we show the power of deep language learning in capturing both sequential and functional semantics information from background genomes. Moreover, by integrating the interpretable analysis mechanism, we well explain what the model learns, helping us build the mapping from the discovery of important sequential determinants to the in-depth analysis of their biological functions.",
  "full_text": "iDNA‑ABF: multi‑scale deep biological \nlanguage learning model for the interpretable \nprediction of DNA methylations\nJunru Jin1,2, Yingying Yu1,2, Ruheng Wang1,2, Xin Zeng3,4, Chao Pang1,2, Yi Jiang1,2, Zhongshen Li1,2, \nYutong Dai3,4, Ran Su5, Quan Zou6, Kenta Nakai3,4* and Leyi Wei1,2* \nBackground\nDNA methylation is significant for the development and plays an important role in gene \nsilencing, protection against spurious repetitive element activity, genomic stability dur -\ning mitosis, and parent-of-origin imprinting [1]. Moreover, alteration of the DNA meth -\nylation pattern caused by the environment and aging may contribute to the development \nof disease, especially cancer [2, 3]. Currently, 5-methylcytosine (5mC), N6-methyl -\nadenosine (6mA), and 4-methylcytosine (4mC) are three main DNA methylation types, \nnamed according to the type of nucleotide, the type of molecule added, and the position \nof modification within the nucleotide [4]. Different methylations have diverse functional \nmechanisms. For example, among them, 5mC is generated by binding methyl groups at \nthe fifth site of cytosine (C). It is associated with transcriptional inhibition, and thus with \nclassical epigenetic phenomena such as genomic imprinting and X chromosome inacti -\nvation [5]. 6mA, usually with methylation at the sixth position in adenosine (A), plays a \nAbstract \nIn this study, we propose iDNA-ABF, a multi-scale deep biological language learn-\ning model that enables the interpretable prediction of DNA methylations based \non genomic sequences only. Benchmarking comparisons show that our iDNA-ABF \noutperforms state-of-the-art methods for different methylation predictions. Impor-\ntantly, we show the power of deep language learning in capturing both sequential and \nfunctional semantics information from background genomes. Moreover, by integrating \nthe interpretable analysis mechanism, we well explain what the model learns, helping \nus build the mapping from the discovery of important sequential determinants to the \nin-depth analysis of their biological functions.\nKeywords: DNA methylation, Deep learning, Multi-scale information processing, \nInterpretable analysis\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nMETHOD\nJin et al. Genome Biology          (2022) 23:219  \nhttps://doi.org/10.1186/s13059‑022‑02780‑1\nGenome Biology\n*Correspondence:   \nknakai@ims.u-tokyo.ac.jp; \nweileyi@sdu.edu.cn\n1 School of Software, Shandong \nUniversity, Jinan 250101, China\n2 Joint SDU-NTU Centre \nfor Artificial Intelligence Research \n(C-FAIR), Shandong University, \nJinan 250101, China\n3 Human Genome Center, The \nInstitute of Medical Science, \nThe University of Tokyo, \nTokyo 108-8639, Japan\n4 Department of Computational \nBiology and Medical Sciences, \nThe University of Tokyo, \nKashiwa 277-8563, Japan\n5 College of Intelligence \nand Computing, Tianjin \nUniversity, Tianjin 300350, China\n6 Institute of Fundamental \nand Frontier Sciences, \nUniversity of Electronic Science \nand Technology of China, \nChengdu 610054, China\nPage 2 of 23Jin et al. Genome Biology          (2022) 23:219 \ncrucial role in chromosome replication, cell defense, cell-cycle regulation, and transcrip-\ntion [6]. It has been extensively detected in viruses, bacteria, protists, fungi, algae, etc. \nAs the other important epigenetic modification, 4mC protects host DNA from the deg -\nradation of restriction enzymes and corrects prokaryotic DNA replication errors, and \ncontrols the DNA replication and cell cycle of prokaryotes [7]. Therefore, DNA methyla-\ntion identification is fundamentally essential for revealing the functional mechanisms.\nDNA methylation can be determined experimentally through next-generation \nsequencing (NGS) approaches such as whole-genome bisulfite sequencing (WGBS) [8] \nor reduced-representation bisulfite sequencing (RRBS) [9]. The techniques can deter -\nmine the global genomic distribution of DNA methylations at the nucleotide level and \nprovide golden standard datasets for DNA methylation-related downstream task anal -\nysis. However, the detection of DNA methylation using traditional experimental tech -\nniques is often costly and time-consuming [10]. In addition, bisulfite sequencing cannot \nprofile DNA methylation in repetitive genomic areas due to short-read sequencing [11, \n12]. Thus, recent research is more focused on developing computational approaches, \nparticularly machine learning-based approaches, to detect DNA methylations directly \nusing genomic sequences. These methods formulate DNA methylation identification as \na binary prediction task and train machine learning models to distinguish true methyla -\ntion sites from non-methylation sites.\nOver the last few decades, a series of sequence-based approaches using either tradi -\ntional machine learning or deep learning are well developed for the prediction of DNA \nmethylations. Taking 4mC methylation prediction as an example, Tang et al. proposed \nDNA4mC-LIP , an ensemble learning method by combining six existing predictors \nthrough a linear integration strategy to make predictions [13]. DeepTorrent [14] is a \ndeep learning-based predictor that integrates inception module, attention module, and \ntransfer learning to improve the predictive performance of 4mC sites. As the predic -\ntion of 6mA sites, MM-6mAPred [15] makes use of the transition probability between \nadjacent nucleotides based on a Markov model. To simplify the model construction, \nSNNRice6mA [16] builds a simple and lightweight deep learning model using Convo -\nlutional Neural Network (CNN) to identify 6mA sites in the rice genome. Later on, Li \net al. proposed Deep6mA [17], a hybrid deep learning network of CNN and Long Short-\nTerm Memory (LSTM), with more accurate 6mA prediction. BERT6mA [18] is a similar \nmodel but uses transformer to build predictive models, demonstrating the effectiveness \nof natural language processing techniques with applications in 6mA prediction. As for \n5mC site detection, iPromoter-5mC fuses the results of several models that predict the \none-hot encoded sequence through full connection layers [19]. BiLSTM-5mC mainly \nuses Bidirectional Long Short-Term Memory (BiLSTM) to extract features of sequences \nencoded by nucleotide property and frequency for the 5mC prediction [20]. However, \nmost existing approaches can only distinguish one single type of DNA methylation. \nThey are difficult to generalize to other methylation types. iDNA-MS [21] is the first \nmachine learning predictor, which is designed for generic detection of different meth -\nylations across different species. The iDNA-MS utilizes manual features such as K-tuple \nnucleotide frequency component and mono-nucleotide binary encoding with traditional \nmachine learning algorithms like support vector machine (SVM) and random forest \n(RF). The shortcoming of iDNA-MS is that the feature design highly requires a lot of \nPage 3 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nprior knowledge and meanwhile lacks adaptability among different methylation predic -\ntion tasks. To address this problem, in our previous work, we designed a deep learning \nmodel, namely iDNA-ABT [22] that uses the architecture of Bidirectional Encoder Rep -\nresentations from Transformers [23] (BERT) to automatically and adaptively learn dis -\ntinguishable features and make relatively accurate predictions for different methylation \ntypes in different species.\nAs seen above, more and more research efforts attempt to explore the potential of \ndeep learning in the prediction of DNA methylations, and certain progress has been \nmade in the improvement of predictive performance. However, existing deep learning \npredictors have not fully explored the power of feature representation learning, espe -\ncially in the discovery of key sequential patterns that are important for elucidating the \nDNA methylation mechanisms. This also results in the deep learning models with poor \ninterpretation and not being able to dig out the important influence of sequence-based \nmodels in DNA methylation prediction. On the other hand, existing approaches fail to \nanswer other important questions: (1) whether background genomic sequences contain \nextra distinguishable information that can guide the development of DNA methylations, \nand (2) whether DNA methylation occurring exists the conservation and specificity of \nsequential patterns across species or cell lines from computational perspectives is also a \nkey problem.\nWith the development of natural language processing, there are some advanced tech -\nniques such as BERT [24] that are capable of sufficiently exploring and learning high-\nlatent contextual information in natural language texts. Inspired by this, we here consider \ngenomic sequences as “biological texts” and take different-scale sequential determinants \nas different “biological words” . Therefore, we propose iDNA-ABF, a multi-scale biologi -\ncal language learning model to successfully build the mapping from natural language to \nbiological language, and the mapping from methylation-related sequential determinants \nto their functions. Specifically, we introduce a model well pretrained with large-scale \ngenomic sequences to learn biological contextual semantics and propose a multi-scale \nprocessing strategy to capture discriminative methylation information from different \nscales. We further utilize adversarial training and transfer learning to improve the pre -\ndictive performance and enhance the robustness of our model. Benchmarking results on \nseventeen datasets across different methylations and species show that our model sig -\nnificantly outperforms the state-of-the-art sequence-based methods. Importantly, our \nmodel provides interpretable prediction and analysis at sequence level by exploring the \nlocal sequential characteristics based on attention mechanisms. The results reveal that \nour model can accurately and adaptively locate the sequential regions that are closely \nassociated with methylations, demonstrating that there might exist “biological language \ngrammars” that are participating in functional regulations in cellular progress.\nResults\nThe proposed iDNA‑ABF outperforms the state‑of‑the‑art methods\nTo evaluate the performance of our proposed iDNA-ABF, we compared it with four \nstate-of-the-art predictors, including iDNA-ABT, iDNA-MS, BERT6mA, and Deep6mA. \nOf the four predictors, the former two (iDNA-ABT and iDNA-MS) are generic predic -\ntors for different methylation predictions while the other two (BERT6mA and Deep6mA) \nPage 4 of 23Jin et al. Genome Biology          (2022) 23:219 \nare originally designed for 6mA site prediction. The reason to include the two 6mA pre -\ndictors for performance comparison is that they are the state-of-the-art predictors based \non deep learning. Moreover, their models are flexible and can be well extended for other \nmethylation predictions like 5hmC and 4mC, not only for 6mA. All the compared pre -\ndictors were respectively trained on seventeen training datasets across different species \nand different methylation types, and evaluated on the corresponding independent test -\ning datasets (see “Datasets ” section for details). The evaluation results in terms of ACC \nand MCC are shown in Fig.  1A and B, respectively. The detailed results in other metrics \nsuch as SN and SP are presented in Additional file (Additional file 1: Table S1). As clearly \nseen in Fig. 1A and B, our model outperforms the four existing predictors on 15 out of 17 \ndatasets (Additional file 1: Table S1), with only two exceptions—5hmC_M.musculus and \n6mA_A.thaliana, in which our model is actually comparable with the best predictors as \nwell. To be specific, the average ACC of our model on all datasets is higher than that of \ntwo runner-up predictors iDNA-ABT by 1.34% and BERT6mA by 3.73%, respectively. \nIn particular, on the three datasets (4mC_C.equisetifolia, 4mC_S.cerevisiae, and 6mA_S.\ncerevisiae), our iDNA-ABF performs better than the existing predictors with a relatively \nlarge margin, leading by 3.28–14.75%, 1.88–3.59%, and 1.48–4.23% in ACC, respectively. \nSimilar results are observed in terms of MCC. To this end, the results demonstrate that \nour iDNA-ABF is superior to the state-of-the-art approaches for the generic prediction \nof DNA methylations. More importantly, it shows robust performance across species \nunder the three methylation types.\nTo validate the robustness of our model, we further illustrated the ROC and PR curves \nof the predictors on four datasets (4mC_C.equisetifolia, 5hmC_M.musculus, 6mA_C.\nequisetifolia, and 6mA_F.vesca) as presented in Fig.  1C–F, respectively. We can see that \nour iDNA-ABF has the highest AUC and AP in all four datasets. Specifically, the average \nAUC and AP values of our model on the four datasets increase by about 1.39–2.81% and \n0.1–13.8% as compared to the other predictors, respectively. The results further dem -\nonstrate the robust performance of our model in DNA methylation prediction tasks. \nThe ROC and PR curves on the other datasets can be found in Additional file (Addi -\ntional file  1: Fig. S1 and Fig. S2). To intuitively discuss why our iDNA-ABF performs \nbetter than the other approaches, we further visualized the distribution of feature rep -\nresentation space of our iDNA-ABF and the second-best predictor iDNA-ABT on the \nabove four datasets (4mC_C.equisetifolia, 5hmC_M.musculus, 6mA_C.equisetifolia, and \n6mA_F.vesca) using Uniform Manifold Approximation and Projection (UMAP) [25], \na widely used visualization tool that reveals the essential data characteristics through \ndimensionality reduction. Note that the UMAP visualization results on the other data -\nsets can be found in Additional file (Additional file  1: Fig. S3). Figure 1G and H illustrate \nthe feature space distribution of our iDNA-ABF and iDNA-ABT, respectively, in which \neach point represents each sample; methylation sites (positive samples) are annotated \nwith red color while non-methylation sites (negative samples) with blue color. As seen \nfrom Fig.  1G, our model separates the positive and negative samples clearly and every \nclass clusters together rather than disperse, while in Fig.  1H, the positive and negative \nsamples in the feature space of the iDNA-ABT are distributed almost connected, which \nis not easy to circle the boundary for each class. By comparing Fig.  1G and H, we found \nthat the two classes are distributed more clearly in the feature space of our iDNA-ABF \nPage 5 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \n0.70\n0.80\n0.90\n0.40\n0.50\n0.60\n0.80\n0.90\niDNA_ABF\niDNA_ABT\niDNA_MS\n4mC_C.equis\netif\nolia\n4mC_F\n.vesc\na\n5hmC_H.sapiens4mC\n_S\n.cerevisiae\n4mC_\nToly\npo\ncladium\n6mA_A.t\nhal\niana\n6mA_C.\nelegans\n6mA_C.eq\nuis\netifolia\n6mA_D.melanogaster\n6mA_\nF.vesc\na\n6mA_H.\nsapiens\n6mA_R.\nchinensis\n6mA_S.\ncerev\nisiae\n6mA_T\n.th\nermophile\n6mA_\nTolypocladium\n6mA_Xoc BLS256\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0\niDNA_ABF (AP = 0.978)\niDNA_ABT (AP = 0.975)\niDNA_MS (AP = 0.980)\nDeep6mA (AP = 0.964)\n5hmC_M.musculus\nRecallFalse positive rate\nPrecision\nTrue positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.5 0.6 0.7 0.8 0.9 1.0\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.5 0.6 0.7 0.8 0.9 1.0\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.2\n0.3\n0.4\n0.5\n0.6\n02 04 06 08 0 100 12002 04 06 08 0 100\nWithout adversarial trainingWithout adversarial training StepStep\nWith adversarial training\nTest Loss\nTest Loss\n6mA_F.vesca5hmC_M.musculus\nBaseline\nAdversarial\nBaseline\nAdversarial\n5hmC_M.musculus\nAB\nCD\nEF\nG\nH\nIJ\n4mC_C.equisetifolia\n4mC_F\n.vesca\n5hmC_H.sapiens4mC_S.cerevisiae4mC_T\nolypocladium\n6mA_A.thaliana6mA_C.elegans\n6mA_C.equisetifolia6mA_D.melanogaster\n6mA_F\n.vesca\n6mA_H.sapiens6mA_R.chinensis 6mA_S.cerevisia\ne\n6mA_T\n.thermophile\n6mA_T\nolypocladium\n6mA_Xoc BLS2565hmC_M.musculus\nACC\nMCC\nDeep6mA\nBERT6mAiDNA_ABF\niDNA_ABT\niDNA_MS\nDeep6mA\nBERT6mA\nRecallFalse positive rate\nPrecision\nTrue positive rate\nRecallFalse positive rate\nPrecision\nTrue positive rate\nRecallFalse positive rate\nPrecision\nTrue positive rate\nBERT6mA (AP = 0.976)\niDNA_ABF (AP = 0.911)\niDNA_ABT (AP = 0.863)\niDNA_MS (AP = 0.773)\nDeep6mA (AP = 0.874)\nBERT6mA (AP = 0.824)\n4mC_C.equisetifolia\niDNA_ABF (AP = 0.825)\niDNA_ABT (AP = 0.797)\niDNA_MS (AP = 0.792)\nDeep6mA (AP = 0.812)\nBERT6mA (AP = 0.777)\n6mA_C.equisetifolia\niDNA_ABF (AP = 0.981)\niDNA_ABT (AP = 0.954)\niDNA_MS (AP = 0.976)\nDeep6mA (AP = 0.973)\nBERT6mA (AP = 0.955)\n6mA_F.vesca\niDNA_ABF (AUC = 0.980)\niDNA_ABT (AUC = 0.976)\niDNA_MS (AUC = 0.984)\nDeep6mA (AUC = 0.974)\n5hmC_M.musculus\nBERT6mA (AUC = 0.975)\niDNA_ABF (AUC = 0.909)\niDNA_ABT (AUC = 0.855)\niDNA_MS (AUC = 0.780)\nDeep6mA (AUC = 0.886)\nBERT6mA (AUC = 0.798)\n4mC_C.equisetifolia\niDNA_ABF (AUC = 0.810)\niDNA_ABT (AUC = 0.791)\niDNA_MS (AUC = 0.785)\nDeep6mA (AUC = 0.798)\nBERT6mA (AUC = 0.777)\n6mA_C.equisetifolia\niDNA_ABF (AUC = 0.980)\niDNA_ABT (AUC = 0.956)\niDNA_MS (AUC = 0.976)\nDeep6mA (AUC = 0.972)\nBERT6mA (AUC = 0.962)\n6mA_F.vesca\n0.70\nACC MCC\nWith adversarial training\n-2\n0\n2\n4\n6\n8\n10\n-5\n0\n5\n10\n15\n-7.5\n-5.0\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n-2.5\n15.0\n-5.0\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n-2.5\niDNA-ABFiDNA-ABF iDNA-ABFi DNA-ABF\n-2 0 24 6 8 1005 10 15 05 10 15-5 05 10 15\n4mC_C.equisetifolia5hmC_M.musculus 6mA_C.equisetifolia 6mA_F.vesca\nneg\npos\niDNA-ABT iDNA-ABTiDNA-ABTiDNA-ABT\n-2\n0\n2\n4\n6\n8\n10\n12\n1202468 10 14\n-5\n0\n5\n10\n15\n05 10 15-5\n-5\n0\n5\n10\n15\n05 10 15-5\n-5\n0\n5\n10\n15\n05 10 15-5\nneg\npos\n4mC_C.equisetifolia5hmC_M.musculus 6mA_C.equisetifolia 6mA_F.vesca\nFig. 1 Performance comparison between iDNA-ABF and other existing methods. A and B represent the \nACC and MCC values of our proposed iDNA-ABF and other existing methods including iDNA-ABT, iDNA-MS, \nBERT6mA, and Deep6mA on 17 benchmark independent datasets, respectively. C The ROC and PR curves \nof our proposed iDNA-ABF and other existing methods in 5hmC_M.musculus. D The ROC and PR curves of \nour proposed method and other existing methods in 4mC_C.equisetifolia. E The ROC and PR curves of our \nproposed iDNA-ABF and other existing methods in 6mA_C.equisetifolia. F The ROC and PR curves of our \niDNA-ABF and other existing methods in 6mA_F .vesca. G and H represent the feature space distribution \n(with UMAP visualization) of iDNA-ABF and iDNA-ABT in 5hmC_M.musculus, 4mC_C.equisetifolia, 6mA_C.\nequisetifolia, and 6mA_F .vesca, respectively. Negative (in red color) and positive (in blue color) represent \nnon-methylation and true methylation samples, respectively. I The MCCs and ACCs of the models with and \nwithout adversarial training on 17 benchmark independent datasets, respectively; each point in the figure \nrepresents each dataset. J Learning curves of the model with and without the use of adversarial training on \n5hmC_M.musculus, and 6mA_F .vesca \nPage 6 of 23Jin et al. Genome Biology          (2022) 23:219 \nas compared to the state-of-the-art iDNA-ABT. This demonstrates that our model learns \nbetter feature representations from different class samples, possibly due to the well pre -\ntrained model in our model construction, helping us capture more high-latent contex -\ntual semantics information from millions of background genomic sequences.\nAdversarial training enhances the predictive performance and the robustness of iDNA‑ABF\nAdversarial training is an important component of our iDNA-ABF. To investigate the \neffectiveness of the adversarial training, we compared our original iDNA-ABF with the \nmodel without the use of adversarial training. The results of the 17 independent datasets \nare illustrated in Fig.  1I where each dot represents each dataset. As seen, our original \niDNA-ABF (with adversarial training) generally achieves better performance than that \nwithout adversarial training. To be specific, by introducing adversarial training, the per -\nformance improvement in ACC and MCC can be observed on 14 out of 17 datasets, and \n15 out of 17 datasets, respectively. This indicates that adversarial training can enhance \nprediction performance. The results on other metrics (SN, SP , and AUC) can be found in \nAdditional file (Additional file  1: Fig. S4). What is more, to intuitively show the impor -\ntance of adversarial training in model optimization, we further analyzed the learning \ncurves during the training process. Figure  1J shows the curves of the models with and \nwithout adversarial training on two datasets (5hmC_M.musculus and 6mA_F.vesca), ran-\ndomly selected from the datasets. From Fig.  1J, we can see that the models with adver -\nsarial training achieve lower test loss than that without adversarial training although the \nloss reduction rate decreases more slowly than the models without adversarial training. \nFurthermore, using adversarial training the models maintain lower test loss in the later \nperiod of the training process while the models without adversarial training gradually \nbegin to overfit, demonstrating that adversarial training enhances the robustness of our \nmodel in the DNA methylation prediction.\nOur iDNA‑ABF reveals the methylation conservation across species at sequential level\nTo investigate whether the methylated sequential patterns across different species are \nconserved or not, we firstly constructed the evolutionary tree for different species in the \nsame methylation type using Lifemap [26]. As for 4mC methylation, Fig.  2A illustrates \nthe evolutionary relationship of four species. It can be clearly seen that Fragaria vesca \nand Casuarina equisetifolia are evolutionary taxonomies, belonging to the common \nFabids, while the other two species belong to Saccharomyces. An interesting observa -\ntion is that our model exhibited similar performance in the species with evolutionary \ntaxonomies. In F. vesca and C. equisetifolia, the ACCs of our model are 0.852 and 0.858, \nrespectively; while in the other, their ACCs are 0.743 and 0.723. Next, we further ana -\nlyzed the methylation sequential patterns of the four species using the probability-based \nmotif visualization tool—kpLogo [27]. Figure  2B illustrates the sequential patterns in \ntwo evolutionarily close species (F. vesca and C. equisetifolia) while Fig.  2C shows that \nin the other two species. From Fig. 2B, we can see that the methylated sequential regions \nin the species are very similar, particularly enriched with CG content. From Fig.  2C, the \nsimilar results in the other two species can be observed. As for the 6mA methylation, \nwe also found the similar conclusion with 4mC methylation (Additional file  1: Fig. S5). \nOverall, the results demonstrate that the methylated sequential patterns in species with \nPage 7 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nevolutionary taxonomies might be conserved, thus contributing to the similar predictive \nperformance; on the other hand, the methylation patterns in the species with far evolu -\ntionary relationship would be quite different.\nNext, we further investigated the cross-species performance of our model to study \ntheir interrelationships between species; that is, we trained our model in one species and \ntested on the other. To avoid the problem of insufficient learning, we only trained our \nmodels on large datasets and tested on small datasets. The cross-species performances \nare illustrated in Fig. 2D, from which we can see that the performances within evolution-\nary taxonomies are significantly better than that without evolutionary taxonomies. The \nresults further demonstrate that the methylation conservation at sequential level is posi -\ntively correlated with evolutionary taxonomies.\nMulti‑scale sequential design choice is more appropriate to elucidate methylation \nmechanisms\nIn our model, we proposed a multi-scale information processing strategy via using differ-\nent k-mers to represent different “biological words” for feature representation learning. \nTherefore, we firstly validated how single-scale k-mers impact the predictive perfor -\nmance of our model. We compared different k-mers, ranging from 3-mer to 6-mer. The \ncomparative results are illustrated in Fig.  3A, in which we can see that different k-mers \nindeed have their advantages on different datasets, respectively. There is no consistent \nresult observed. It might be that the methylated sequential regions vary across species \nand methylation types in length. Therefore, using single-scale sequential patterns for fea-\nture representations cannot adaptively and sufficiently capture the inherent characteris -\ntics of methylations. To address this problem, we integrated different scales of k-mers as \nour model input, such as 3-mer + 6-mer, 4-mer + 6-mer, and 5-mer + 6-mer, and com-\npared their performance as illustrated in Fig.  3B. It can be observed that the multi-scale \nFig. 2 The relationship between methylation conservation and model accuracy across species. A Taxonomy \ntree and accuracy for four species in 4mC dataset. Two species (i.e., F . vesca and C. equisetifolia) with higher \naccuracy are grouped in red, while the other two species (i.e., S. cerevisiae and Tolypocladium) with lower \naccuracy are grouped in blue. B The motif logo analysis on F . vesca and C. equisetifolia. C The motif logo \nanalysis on S. cerevisiae and Tolypocladium. D The accuracy heatmap of cross-species validation on 4mC \ndataset\nPage 8 of 23Jin et al. Genome Biology          (2022) 23:219 \nk-mer integration (i.e., 3-mer + 6-mer) improves the model performance as compared \nto the single-scale k-mers (i.e., 3-mer, and 6-mer). To be specific, the model using the \nintegration of 3-mer and 6-mer achieved the highest performance with the average ACC \nof 85.95% on all the datasets, which is 2.53 and 1.01% higher than that using 3-mer and \nFig. 3 Interpretable analysis of multi-scale information processing. A The comparison of single scales \nincluding 3-mer, 4-mer, 5-mer, and 6-mer, respectively. B The comparison of multi-scale combinations. C The \nattention map to illustrate the information captured at 3-mer scale on one randomly selected sequence. \nTwo sub-figures visualize the change of information captured before and after training, respectively. D The \nattention map to illustrate the information captured at 6-mer scale. E–G Interpretable illustrations of the \nmotifs learnt by our model in three species covering three methylation types, including 4mC_Tolypocladium, \n5hmC_H.sapiens, and 6mA_C.equisetifolia, respectively. The left part figure clearly shows which region the \nmodel is more focused on by using heatmap from 0 to 1. The closer the score is to 1, the darker the color \nand the more important the region considered by the model. The p-value was calculated using TOMTOM \nby comparing our iDNA-ABF learnt motifs with STREME motifs. The p-value in STREME was calculated by a \none-sided binomial test. The motifs within the gray dashed anchor boxes were extracted for pair comparisons\nPage 9 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \n6-mer, respectively. This demonstrates that the information from different scales is com-\nplementary to each other for learning better feature representations.\nNext, we further investigated why using multi-scale k-mer integration is more appro -\npriate for discriminative information capturing. For this, we utilized attention mecha -\nnism to intuitively interpret the information our model learnt from two sequential \nscales—3-mer and 6-mer. We visualized the attention heatmap of the two scales in \nFig. 3C and D, respectively. Note that the element in the heatmap represents the correla-\ntion degree of two positions along the sequences. Figure  3C shows the information our \nmodel learnt before and after training at 3-mer scale. As we can see, as compared to the \ninitial model, the attention mechanism is more focused on the diagonal of the heatmap \nafter training. This indicates that our model learns more local discriminative informa -\ntion as compared to that before training. Similarly, Fig.  3D illustrates the information \nour model learnt before and after training on the other sequential scale—6-mer. In con -\ntrast, this scale is more focused on global information after training. To this end, we can \nconclude that different scales of sequential patterns learn both local and global informa -\ntion, which might be complementary for the performance improvement.\nIn order to clearly demonstrate which sequential region is the most important for \nmethylation prediction, we randomly selected three sequences from three species with \ndifferent DNA methylation types, and applied the attention mechanism to identify key \nregions from these sequences. As can be seen in Fig.  3E–G (in left), for each sequence, \nour model identified different regions under different sequential scales. This further con-\nfirms that different scales capture different important information. For those identified \nregions, we further extracted and visualized the corresponding motifs using attention \nscores. Figure  3E–G (in right) shows the motifs learnt by our iDNA-ABF and that dis -\ncovered by the conventional tool—STREME [28], respectively. As seen, our learnt motifs \n(highlighted with a gray-color window) almost match the STREME’s motifs in each \nspecies. To quantitatively compare the motif similarity, we adopted TOMTOM [29] to \ncalculate the similarity degree of two motifs, which is measured by p-value. The lower \np-value indicates a higher degree of motif consistency. As can be seen in Fig.  3E–G, our \nmotifs are highly similar to the STREME’s motifs, suggesting that our model can learn \nconserved sequential characteristics.\nOur iDNA‑ABF sufficiently explores genomic information in 5mC prediction across human \ncell lines\nIn this section, we analyzed how well our iDNA-ABF performs the methylation predic -\ntion across human cell lines. Since 5mC is one of the most well-studied methylation \ntypes in human genome, we selected the 5mC methylation to perform our method. We \ntherefore constructed three new 5mC datasets corresponding to three human cell lines, \nincluding GM12878, K562, and HepG2, respectively. The details of the datasets can be \nseen in section “Datasets”.\nFirst, we discussed the impact of the length of methylated sequential regions for the \n5mC methylation prediction. Therefore, for each cell line, we constructed four 5mC \ndatasets, in each of which the 5mC sequences are 11, 41, 71, and 101 bp (base pairs) \nlong, respectively. The details of the datasets are summarized in Additional file (Addi -\ntional file 1: Table S2, Table S3, and Table S4). Figure 4A shows the model performance \nPage 10 of 23Jin et al. Genome Biology          (2022) 23:219 \nvaried with different sequence lengths in the three cell lines. In the beginning, the model \nperformance significantly improves as the sequence length increases, demonstrating that \na longer sequence brings the model extra genomic contextual information. The peak is \nreached when the length is 71 bp. After that, the model performance gradually declines. \nNotably, the model trained with the sequences 11 bp long exhibits extremely poor per -\nformance, with the ACC of around 55%. The reason is that methylation-centered regions \nwith the range of 11 bases are very similar between negative and positive samples. This \nfurther demonstrates that the methylations are strongly correlated with the upstream \nand downstream from the methylated regions.\nAs well known, 5mC methylation is one of the well-studied methylation types, back -\ning supported by many NGS data, such as ChIP-seq data, and ATAC-seq data, etc. [30]. \nAn interesting question is whether integrating the NGS data with sequence data can \ncontribute to more accurate prediction. For this, we chose two histone modifications \n(HM) data, H3k4me3 and H3k36me3, which are reported to be closely associated with \nFig. 4 The 5mC prediction performance of our model on three human cell lines. A The ACC and AUC results \nof different human cell line datasets with different sequence lengths. B–D Performance of sequence data, \nChIP-seq data, and integration of sequence and ChIP-seq data under different sequence lengths in three \nhuman cell lines, respectively. E The 5mC distributions predicted by our model and annotated by WGBS in \na randomly selected genomic region (Chr1: 187000 - 192000, GRCh38). Note that the 5mC distribution is \nderived from HepG2 cell line\nPage 11 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \n5mC [31]. We trained and tested the models using (1) Sequence data only, (2) ChIP-\nseq data only, and (3) Sequence + ChIP-seq data on the three cell lines, respectively. \nThe comparative results are shown in Fig.  4B–D. As we can see, the model trained with \nsequence data achieved remarkably better performance as compared to that trained with \nChIP-seq data, leading by 10.4, 10.9, and 21.1% in the average ACC, AUC, and MCC in \nthree cell lines under different sequence lengths. When combining ChIP-seq data with \nsequence data for model training, all the performance metrics are further improved, \nachieving the highest scores, with the improvement of 3.8, 5.2, and 8.1% on the average \nACC, AUC, and MCC over the model trained with sequence data, demonstrating that \nthe ChIP-seq data and sequence data are complementary to each other for the improved \n5mC prediction.\nApplication of iDNA‑ABF for the 5mC methylation detection at genome scale\nConsidering real application scenario, it is important to measure the performance of our \niDNA-ABF in detecting the 5mC distribution from the whole-genome scale. Thus, we \npredicted the methylation probability on a 5k-bp-long genomic region (Chr1: 187,000–\n192,000) from human genome (GRCh38) based on our iDNA-ABF model trained on \nHepG2. The prediction procedure is as follows. Firstly, we used a 71-bp-long window to \nscreen the region. Secondly, the sequences that meet the following two requirements: (1) \ncentered with base C and (2) centered with CPG patterns were picked out. Ultimately, \nthe resulting sequences were submitted to our iDNA-ABF for prediction. Our model \ngives the predicted confidence of each site candidate.\nFigure 4E illustrates two HM data distributions, the 5mC distribution predicted by our \nmodel, and the true 5mC distribution annotated by WGBS, respectively. As we can see \nfrom Fig. 4E, our predicted 5mC distribution is generally overlapped with the true 5mC \nsite distribution. Moreover, the predicted 5mCs basically match with the two HM data, \ndemonstrating that our predictions have the functional significance. Notably, we found \nthat our model identified some regions (with blue frame, Fig.  4E) that are not identified \nby WGBS, but they matched well with the signal of H3K4me3 data. This implies that \nour model might discover potentially novel functional regions. Although our model also \nproduces some false positives, from the perspective of sequential bins (here, we consid -\nered 100-bp region as a bin), the predicted 5mC region distribution is almost the same \nwith the true 5mC region distribution. The results at least demonstrate that our model \ncan perform well in locating 5mC regions. This could also be helpful for methylation \nresearch.\nOur iDNA‑ABF has robust performance in 5mC prediction on unseen human cell lines\nTo analyze the predictive performance of iDNA-ABF in unseen cell lines, we conducted \nthe cross-cell line validation. To be specific, we trained our model on one cell line and \nevaluated it on the other. Figure  5A shows the heatmap results in terms of four met -\nrics, including ACC, MCC, SN, and SP , respectively. The vertical axis denotes training \ncell lines, while horizontal axis shows testing cell lines. As shown in Fig.  5A, our model \nachieved relatively stable ACC and MCC under the cross-cell line validation. Moreover, \nwe can also see that when evaluated on the K562, our model trained on the GM12878 \nachieved the highest SN, yielding a relative improvement of 16% compared to the model \nPage 12 of 23Jin et al. Genome Biology          (2022) 23:219 \ntrained on the HepG2. For better explanation, we introduced the probability distribu -\ntion analysis in methylated central regions of three human cell lines. Figure  5B and C \nshow the probability distribution in the positive and negative samples in three cell lines, \nrespectively. On the one hand, it can be seen from Fig.  5B that the positive motif logos \nof K562 are more similar to the GM12878 than HepG2 in position from −1 to 1. On the \nother hand, we observed from Fig. 5C that the negative motif logos of K562 are the same \nas the positive motif logos of GM12878 in position from −1 to 1, which can explain the \nlowest SP of our model trained on GM12878 while tested on K562. Furthermore, we \nfound in Fig. 5A that our model in the heatmap of cross-cell line validation performs not \nthat well in terms of SP . This might be that the negative motif logos among all three cell \nlines are quite different. To this end, via the cross-cell line validation results, we can con-\nclude that our model has robust performance even for the unseen cell lines. This further \nexplores the application value of our model.\nOur iDNA‑ABF has good transfer learning ability to capture the specificity of methylated \nsequential patterns\nThe 5mC methylations mainly occur within the sequences with CpG patterns in human \ngenome; actually in a few cases, the methylations are also detected within the CHH and \nCHG patterns (where H = A, C, or T). In order to find out whether different methyl -\nated sequential patterns are correlated with each other, we constructed extra CHG and \nCHH datasets for the three cell lines, respectively. It is worth noting that the number of \nsequences in CpG dataset is far more than that in CHG or CHH datasets. The details of \nthe datasets are presented in Additional file (Additional file 1: Table S5).\nFig. 5 Performance in 5mC prediction on unseen cell lines. A The heatmap of cross-cell line validation in \nterms of different metrics, including ACC, MCC, SN, and SP , respectively. B Motif logos in central sequential \nregions of the positive datasets in three human cell lines, respectively. C Motif logos in central sequential \nregions of the negative datasets in three human cell lines, respectively\nPage 13 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nTo see whether our model has good transfer learning ability in detection of different \nmethylation patterns, we firstly pretrained a model on the CpG dataset and fine-tuned \nit on the CHG or CHH datasets, yielding another model denoted as “transfer learn -\ning model” . Moreover, we also trained a model directly with the CHG or CHH datasets \nfor comparison, denoted as “baseline model” . Both models were then evaluated with \nthe same testing datasets of the CHG or CHH datasets. The performance on the two \ndatasets is shown in Fig.  6A and B, respectively. As we can see, the performance of the \n“transfer learning model” is always superior to the baseline model, with the average AUC \nand AP increasing by 3.1 and 3.3% in three cell lines. The results demonstrate that our \nmodel has a good transfer learning ability; the pre-training mechanism can bring extra \ndiscriminative information from one specific pattern to benefit the prediction of the tar -\nget patterns, thus improving the predictive performance.\nFig. 6 Transfer learning results and analysis of our model. A The ROC and PR curves of the baseline model \nand the transfer learning model on CHG dataset in HepG2 cell line. Note that the baseline model is trained \nwith the CHG dataset while the transfer learning model is pretrained with CpG dataset and fine-tuned on the \nCHG dataset. B The ROC and PR curves of the baseline model and the transfer learning model in CHH dataset \nin HepG2 cell line. Note that the baseline model is trained with the CHH dataset while the transfer learning \nmodel is pretrained with the CpG dataset and fine-tuned on the CHH dataset. C The motifs learnt from \nthree models, including baseline model, pretrained model, and transfer learning model, respectively. D The \nsearching results using our learnt motifs against the topEnriched.MM database\nPage 14 of 23Jin et al. Genome Biology          (2022) 23:219 \nTo in-depth explain the possible reason regarding the performance improvement \nusing transfer learning, we further analyzed and compared the motifs that were learned \nfrom the three models, including baseline model, transfer learning model, and pre -\ntrained model which is trained on CpG dataset only, respectively. The motif comparison \nresults in HepG2 cell line are illustrated in Fig.  6C. Taking the 1st, 5th, and 9th motif \nfigures as an example, we observed that the transfer learning model not only keeps some \nCpG patterns inherited from the pretrained model but also captures the specificity of \nthe CHG patterns learned by baseline model. In addition, the transfer learning model \ncan also discover some new patterns such as the 7th motif figure, which does not share \nsimilar patterns from the baseline model in the 3th motif figure.\nThe motifs learnt from our models are biologically meaningful\nNext, we further explored whether the motifs (or sequential patterns) learnt from our \nabove three models (baseline model, pretrained model, and transfer learning model) are \nbiologically meaningful. Accordingly, we searched the learnt motifs against topEnriched.\nMM, a public methylation database [32]. Interestingly, from Fig.  6D, we found that the \nmotifs learnt by our models can significantly match with some functional motifs in the \ndatabase, which were previously reported to be closely associated with methylation \nmechanisms. The results demonstrate that our model can accurately mine functional \nsequential characteristics; on the other hand, the newly discovered sequential motifs are \nalso biologically meaningful, indicating the strong ability of our model in learning func -\ntional semantics between different sequential patterns.\nDiscussions\nWe presented iDNA-ABF, a novel method for identifying DNA methylation by biological \nlanguage learning solely based on genomic sequences. Our iDNA-ABF not only enables \nrelatively accurate methylation prediction across species and across cell lines, but also \nbuilds the mapping from the sequential level to the functional level using explainable \nattention mechanism to study the in-depth DNA methylation mechanisms.\nFirst, we investigated the predictive performance of our model to see how well and \nhow stable it performs. Experimental results in 17 benchmark datasets covering three \nmethylation types (4mC, 5hmC, and 6mA) in multiple species show that our model \nexhibits the consistently superior and robust performance as compared with the state-\nof-the-art sequence-based approaches. The ablation analyses reveal the importance of \nthe adversarial training in the model performance. Particularly, the adversarial training \nin our training process alleviates the impact of large-scale parameters particularly on \nsome small datasets and improves the generalization ability of our model across differ -\nent species and methylation types. In addition, we also studied the impact of sequence \nlength on the methylation prediction. The results show that the model performance \ngenerally improves with the increase of the input sequence length, demonstrating the \nupstream and downstream surrounding the methylated sequential regions are crucial \nto identifying DNA methylation sites. They might contain some degree of specificity \ninformation from the sequential perspective to help our model distinguish the meth -\nylation sites from non-methylation sites; on the other hand, we adopted DNABERT [33] \nfor model construction, a powerful natural language learning model that was pretrained \nPage 15 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nwith million-scale genomic sequence data. It enables our model to capture more sequen-\ntial semantics from background genomes. The feature space visualization analysis results \nprove that our model learns more distinguishable feature representations as compared \nwith existing predictors. Interestingly, by integrating ChIP-seq data such as histone \nmodification data (e.g., H3K4me3 and H3K36me3) into our model, we observed that the \nperformances are further improved, indicating that biological signals and sequence data \nare complementary for the improved prediction. The result could show the great poten -\ntial of sequence data for DNA methylation prediction and other genomic functional \nanalysis. We can imagine that, as for new cell line data, the NGS data is limited so that \nwe cannot train an effective model. At least, the analyses provide a new way to build a \nmore accurate and robust model by integrating the sequence data.\nSecondly, the main feature of our iDNA-ABF is that we provide the interpretable \nanalysis for DNA methylation prediction. The major problem of existing deep learning-\nbased approaches is that they cannot well explain why their models are effective for \nthe methylation prediction, since deep learning works as “black box” . To address this \nproblem, we did two major improvements for model construction. One is proposing \nthe multi-scale sequence processing strategy for model training, and the other is intro -\nducing the attention mechanism for model analysis. Inspired by word segmentation in \nnatural language learning, we utilized the multi-scale sequence processing strategy by \nsegmenting DNA sequences with different scales (3mer and 6mer) of sequential pat -\nterns to represent “biological words” . Furthermore, we adopted the attention mechanism \nto interpret what information our model learned from different scales of “biological \nwords” . Analyses demonstrate that the multi-scale strategy is capable of bringing more \ndiscriminative semantics information from both local and global levels, effectively over -\ncoming the information lack at one single-scale and the information over-redundancy \nat all scales. Importantly, different sequential scales lead our model to learn different \nmotifs. The results show that our learnt motifs from different scales are highly consist -\nent with that by the conventional motif finding tool—STREME, demonstrating that our \nmodel is capable of discovering conserved sequential patterns. Next, the natural ques -\ntion is whether the sequential patterns learnt by our model are biologically meaningful \nor correlated with the methylations. To answer this question, we applied our model to \nthe prediction of 5mC methylation across human cell lines. The reason to choose human \n5mC methylation is that it has conserved methylated sequential patterns, such as CpG, \nCHH, and CHG; on the other hand, it is back supported by many NGS data, facilitat -\ning further functional validation analysis. We investigated the transfer learning ability \nof our model and the experimental results show that learning the knowledge from CpG \nmethylation patterns can help the improved prediction of the other two methylation \npatterns (i.e., CHH, and CHG). This demonstrates that our model has  a strong ability \nin learning the specificity of different methylation patterns. The results also imply the \npotential of our model in the discovery of other rarely occurred methylation patterns. \nImportantly, we found that by using transfer learning, our model can learn some new \nmotifs and meanwhile keep the motifs in original methylation patterns. By searching our \nlearnt motifs against a well-known methylation database—topEnriched.MM, we found \nthat our motifs are significantly similar to some known methylation-related functional \nPage 16 of 23Jin et al. Genome Biology          (2022) 23:219 \nmotifs (see Fig. 6D). This also demonstrates that our model can learn different biological \nsemantic information under different methylation patterns.\nUltimately, to verify the performance of our model in real application scenarios, we \nfurther applied our model to the detection of 5mC methylation within human genome. \nThe experimental results in a randomly selected genomic region show that our model is \ncapable of accurately detecting true DNA methylation regions (annotated by WGBS). \nImportantly, our model discovered some potential methylation regions, which are not \ndetected by WGBS but are highly overlapped with the methylation-related histone mod-\nification data (e.g., H3K4me3). This demonstrates the strong ability of our model in the \ndiscovery of biologically meaningful sequential regions. It might be that the deep pre -\ntrained model helps us learn functional semantics from millions of background genome \nsequences.\nConclusion\nAltogether, our proposed deep biological language learning model achieves satisfactory \nperformances in DNA methylation prediction. Importantly, we show the power of deep \nlanguage learning in capturing both sequential and functional semantics information \nfrom background genomes. Moreover, by integrating the interpretable analysis mecha -\nnism, we have well explained what we learned, helping us build the mapping from the \ndiscovery of important sequential determinants to the in-depth analysis of their biologi -\ncal functions. However, there is still much room to improve. For example, in the con -\nstruction of methylation prediction models, we only considered local sequential regions \nsurrounding the methylation sites, in which the discriminative information might be \nlimited to some extent. Studies [34] have demonstrated that there is a long-range inter -\nactive impact of gene regulations in genome, such as enhancer-promoter interaction. \nTherefore, exploring how the long-range sequence integrative information affects DNA \nmethylation levels could be an important direction in future work.\nMethods\nDatasets\nDifferent species datasets\nA stringent dataset is fundamentally crucial for training effective and promising predic -\ntors. To further evaluate our proposed method with state-of-the-art methods, we choose \nthe same benchmark datasets originally proposed by iDNA-MS [21]. The datasets con -\nsist of three main DNA methylation types, including seventeen datasets totally. Among \nseventeen datasets, C. equisetifolia (4mC_C.equisetifolia), F. vesca (4mC_F.vesca), S. cer-\nevisiae (4mC_S.cerevisiae), and Ts. SUP5-1 (4mC_Ts.SUP5-1) belong to 4mC. The 6mA \ncontains Arabidopsis thaliana (6mA_A.thaliana), Caenorhabditis elegans (6mA_C.ele -\ngans), Casuarina equisetifolia (6mA_C.equisetifolia), Drosophila melanogaster (6mA_D.\nmelanogaster), Fragaria vesca (6mA_F.vesca), Homo sapiens (6mA_H.sapiens), Rosa \nchinensis (6mA_R.chinensis), Saccharomyces cerevisiae (6mA_S.cerevisiae), Tolypocla-\ndium sp SUP5-1 (6mA_Tolypocladium), Tetrahymena thermophile (6mA_T.thermo -\nphile), and Xanthomonas oryzae PV. Oryzicola (Xoc ) BLS256 (6mA_Xoc.BLS256). What \nis more, there are two 5hmC datasets from two species, including H. sapiens (5hmC_H.\nPage 17 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nsapiens) and M. musculus (5hmC_M.musculus). It should be noted that both positives \nand negatives are 41-base pair (bp) long and the sequence identity of the datasets is less \nthan 80% using the CD-HIT [35] program, which is shown in Fig.  7A. The details of the \ntraining dataset and the validation dataset from seventeen species are given in Addi -\ntional file (Additional file 1: Table S6).\nHuman cell lines datasets\nThe 5mC methylation data of three human cell lines (K562, GM12878, hepG2) \nwere collected from ENCODE portal (ENCSR765JPC, ENCSR890UQO, and ENC -\nSR786DCL) [36], which provides the location information of three methylation pat -\nterns (CpG, CHG, and CHH) experimented by whole-genome bisulfite sequencing \n(WGBS). To construct a high-quality dataset, methylation sites with 100% methylated \nand 10–200×  sequencing coverage were kept for positive samples, whereas methyla -\ntion sites with 0% methylated and 0 sequencing coverage were selected as negative \nsamples. The processed methylation sites located in promoter and gene body region \nwere further mapped using annotation from GENECODE GRCh38. A promoter \nregion is defined as the 1000-bp region upstream from the transcription start site \nFig. 7 Overview of the proposed iDNA-ABF. A shows the DNA methylation dataset collection where different \ndatasets belonging to three main DNA methylation types are reorganized into their training datasets and \nindependent datasets. The overall architecture of our iDNA-ABF is presented in B–E. B Multi-scale information \nprocessing module, exploiting two scales (3-mer and 6-mer) of tokenizers separately to process the input \nsequence and adaptively obtain corresponding embeddings. C BERT encode module, using BERT encoders \nto extract high-latent feature representations. D Multi-scale extraction module, generating final output \nfeature representations based on multi-scale embeddings. E Classification module, integrating binary \nclassification probability values to make prediction. F The workflow of the interpretable analysis. In brief, our \nmodel uses attention mechanisms to extract and learn sequential motifs from query sequences\nPage 18 of 23Jin et al. Genome Biology          (2022) 23:219 \n(TSS) of a gene. The number of each cell line processed dataset is shown in Addi -\ntional file (Additional file  1: Table S7, Table S8, and Table S9). To evaluate the impact \nof sequence length on the prediction of methylations, the DNA sequences that 11, \n41, 71, and 101-bp-long flanking the methylation sites were extracted from GRCh38, \nrespectively. Similar to the different species datasets, the sequence identity of the \nhuman cell line datasets is also less than 80% with using the CD-HIT program.\nDescription of the proposed iDNA‑ABF\nFigure 7 illustrates the overall architecture of our iDNA-ABF. Figure  7A shows the data \nset collection procedure, which is described in “Datasets ” section. The workflow of \niDNA-ABF is clearly seen in Fig.  7B–E, mainly consisting of four modules: (B) Multi-\nscale data processing module, (C) BERT encoder module, (D) Feature fusion module, \nand (E) Classification module. The prediction procedure is described as follows. In mod -\nule B (see Fig.  7B), we exploit two scales of tokenizers (3-mer and 6-mer) separately to \nprocess the input sequence and adaptively learn corresponding embeddings. Due to the \ninput sequences containing multifaceted features of various scales, we design a multi-\nscale architecture rather than using a single simple tokenizer, which may result in infor -\nmation loss. Afterwards, in module C (see Fig.  7C), the iDNA-ABF uses BERT encoders \nindividually to extract different embeddings processed by tokenization. The iDNA-ABF \nthen combines multi-scale embeddings based on BERT output in module D to generate \nthe final evolutionary output feature. After that, in module E (see Fig.  7E), the model \nuses fully connected layers to predict whether the input sequence is methylated or not. \nNotably, we adopt adversarial training to enhance the robustness of the model and pre -\nvent early overfitting, which can be separated into two components: (1) adversarial per -\nturbation, using cross-entropy loss from this propagation as the adversarial perturbation \nback to the network, and (2) adversarial optimization, obtaining the adversarial loss \nwhich is used to make backpropagation and optimize our model. Note that we describe \nthe details of the four modules as follows.\nMulti‑scale information processing module\nIn our model, we tokenize a DNA sequence with k-mer representations. In this way, \neach token is represented by k bases, thus integrating richer contextual information for \neach nucleotide. For example, a given DNA sequence “ ATG GCT G” can be tokenized to a \nsequence of two 6-mers: ATG  GCT  and TGG CTG . Different k results in different token \nrepresentations. In our work, we set k as 3 or 6, and thus obtain two scales of token \nrepresentations. The whole token table has  4k+5 tokens, consisting of all the permuta -\ntions of k-mer as well as 5 special tokens: [CLS], [PAD], [UNK], [SEP], and [MASK], \nwhich stand for classification token, padding token, unknown token, separation token, \nand masked token, respectively.\nBERT‑based encoder module\nPre‑training of the BERT model\nBERT is the first bidirectional language representation model based on the transformer \nproposed by [24]. Due to its powerful performance in language understanding against \nPage 19 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nmany kinds of large corpus, BERT has been widely used in lots of NLP tasks. To bet -\nter play the role of BERT, it generally will first be trained on a large background-related \ncorpus with two pre-training tasks namely the masked language model and the next \nsentence prediction. Here we use a pretrained BERT model namely DNABERT [ 33], \nusing the same architecture as the BERT base, which consists of 12 Transformer lay -\ners with 768 hidden units and 12 attention heads in each layer. Notably, since there is \nno direct semantic logic between DNA sequences, this domain pre-training adjusts the \nsequence length and enables the model to predict contiguous k tokens adapting to a \nDNA sequence. Also, it uses the masked language model technique similar to the origi -\nnal BERT.\nEncoding process of the BERT\nBERT is a transformer-based contextualized language representation model, which has \nbeen applied to many aspects of biology and has achieved many outstanding perfor -\nmances. The basic component of BERT consists of a multi-head attention mechanism, \na feed-forward network, and the residual connection technique. To capture contextual \ninformation, BERT performs the multi-head attention mechanism based on the self-\nattention mechanism, which is described as follows:\nwhere X ∈ RL×dm is the output of the sequence embedding module. What is more, \naccording to respectively linear layers W Q,W K,W V ∈ R dm ×dk , X is transformed to the \nquery matrix Q ∈ RL×dk , key matrix K ∈ RL×dk , and value matrix V ∈ RL×dk , in which L \nis the length of the input protein sequence, dm is the initial embedding dimension, and \ndk is the dimension of matrix Q, K, and V.\nFrom the above base unit, the multi-head attention mechanism can be expressed as \nfollows:\nwhere W Q\ni , W K\ni , W V\ni ∈ Rdm ×dk are the query, key, and value linear transformation layers \nof the ith head while h is the number of heads. Then multi-head concatenates results of h \nindependent head with different sets of W Q\ni , W K\ni , W V\ni  and use a linear conversion \nlayer WO to map the output dimension of the multi-head attention to the initial embed -\nding dimension of the embedding module. The entire procedure is performed L times, \nwhere L represents the number of layers.\n(1)\n\n\n\nQ = XW Q\nK = XW K\nV = XW V\n(2)Self− Attention(Q , K , V ) = softmax\n(\nQK T\n√\ndk\n)\nV\n(3)\n{\nHeadi = Self− Attention\n(\nXW Q\ni ,XW K\ni ,XW V\ni\n)\n,i= 1, ... ,h\nMultiHead− Attention(Q ,K ,V ) = [head1 ,head2 ,··· ,headh] W O\nPage 20 of 23Jin et al. Genome Biology          (2022) 23:219 \nFeature fusion module\nIn order to obtain the final output h M of two BERT parts, we combine the output h kmer1 \nfrom the first scale input format layer and h kmer2 from the second scale input format \nlayer through a dimensional-wise fusion gate F. F is accomplished by the sigmoid activa-\ntion function to encode two parts of representation:\nwhere W1 and W2 are trainable parameters of the fusion gate. Then the final vector rep -\nresentation output of a specific molecule hM is generated through F:\nClassification module\nAdversarial training [37] is a novel regularization method for classifiers to improve \nrobustness to small, approximately worst-case perturbations. Here, because of the large \nparameters that BERT has, we use this strategy to prevent models from overfitting. \nAmong lots of adversarial training methods, we use a variant of Fast Gradient Method \n(FGM) specific for text classification [38]. The cross-entropy loss function LCE is used to \ntrain the output module to improve the prediction performance as our base loss func -\ntion. We define p as the prediction probability, y as the true label, x as the input, θ as the \nparameters of the model, and ε as one additional parameter. When applying this method, \nadversarial training adds the following term to the cost function:\nwhere r is a perturbation on the input and ˆθ is a constant set to the current parameters \nof the model. Backpropagation algorithm should not be used to propagate gradients \nthrough the adversarial example construction process which means ˆθ is not consistent \nwith θ in the Eq. (6). Then, in the training process, we minimize Eq. (6) for θ to obtain \nthe worst-case perturbations radv against the current model.\nIn the FGM method, we apply the adversarial perturbation to the extracted sequence \nembedding, rather than directly to the input. To define adversarial perturbation on the \nword embeddings, we denote relevant embedding of k-mer as s. Then we define the \nadversarial perturbation radv on s as\nTo train a robust model, we define a new adversarial loss based on the adversarial per -\nturbation defined in Eq. (6), which is formulated as follows:\n(4)F = sigmoid( W 1 · hkmer1 + W 2 · hkmer2)\n(5)hM = F · hkmer1 + (1 − F) · hkmer2\n(6)\n{ LCE\n(\np,y|x,θ\n)\n=− ylogp −\n(\n1 − y\n)\nlog(1 − p)\nLCE\n(\np,y|x+ radv,θ\n)\nwhere radv = arg min\nr,||r||≼ε\nLCE\n(\np,y|x,ˆθ\n)\n(7)radv =− ε g⏐⏐⏐⏐g\n⏐⏐⏐⏐\n2\nwhere g =∇ s LCE\n(\np,y|s,θ\n)\n(8)Ladv(θ ) =− 1\nN\nN∑\nn=1\nLCE\n(\npn,y|sn + radv,n,θ\n)\nPage 21 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \nwhere N is the number of batch size. In our work, adversarial training is to minimize the \nLadv based on cross-entropy loss with stochastic gradient descent.\nPerformance metrics\nIn this study, we evaluate the performance of our iDNA-ABF and other existing methods \nwith the following four commonly used metrics: Accuracy (ACC), Matthews’ correlation \ncoefficient (MCC), Sensitivity (SN), and Specificity (SP). The formulas of these metrics \nare described as follows:\nwhere TP , FN, TN, and FP represent the number of true positive, false negative, true \nnegative, and false positive samples, respectively. ACC and MCC are both used to meas -\nure the overall performance of the model. SN refers to the proportion of true methylated \nsamples correctly predicted by a predictive model, and SP measures the proportion of \nnon-methylated samples correctly predicted by the model. Moreover, the ROC (receiver \noperating characteristic) curve and PR (precision-recall) curve [39] are used to intui -\ntively evaluate the overall predictive performance of the model. AUC and AP denote the \narea under ROC curve and that under the PR curve, respectively [39]. They are further \nused to quantitatively measure the overall performance of the model. Altogether, the \nhigher these metrics are, the better the model is.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s13059- 022- 02780-1.\nAdditional file 1: Table S1. Performances of iDNA-ABF and the state-of-the-art methods on 17 benchmark datasets \nacross species and methylation types. Table S2. Performance of our iDNA-ABF under various methylation patterns \nvaried with different sequence lengths in three human cell lines. Table S3. Performance of our iDNA-ABF using ChIP-\nseq data varied with different sequence lengths in three human cell lines. Table S4. Performance of iDNA-ABF using \nChIP-seq data + sequence data varied with different sequence lengths in three human cell lines. Table S5. The trans-\nfer learning performance of our model varied with different sequence lengths in three human cell lines. Table S6. \nThe statistics of 17 benchmark datasets with three methylation types in various species. Table S7. The statistics of raw \ndata with different methylation patterns in three human cell lines. Table S8. The statistics of the data in three human \ncell lines after sequence similarity reduction using CD-HIT. Table S9. The statistics of training and testing data under \ndifferent methylation patterns in three human cell lines. Table S10. Performance comparison of different scales as the \ninput to train the model in various species. Table S11. Performance comparison with the 5mC methods on cancer \ncell line Encyclopedia (CCLE). Table S12. Performance of iPromoter-5mC in three human cell lines. Table S13. Training \nparameters of our model on 17 benchmark datasets. Table S14. Performance of our iDNA-ABF for the SNP clas-\nsification. Table S15. Performance of the multi-task model. Table S16. Performance of the regression model built on \nChIP-seq data. Figure S1. The ROC curves on benchmark datasets. Figure S2. The PR curves on benchmark datasets. \nFigure S3. The UMAP visualization results on benchmark datasets. Figure S4. The SN, SP , and AUC of the models with \nand without adversarial training on 17 benchmark datasets with the independent test. Figure S5. Taxonomy tree and \naccuracy for eleven species in 6mA dataset. Figure S6. The ROC and PR curves of our baseline model and our model \ntransferring from CpG pattern on the other two pattern datasets (i.e., CHG, CHH) in the cell line GM12878 and K562. \nFigure S7. The regression result of signal prediction in the regression model built on ChIP-seq data. Supplementary \nmethods.\nAdditional file 2. Review history.\nReview history\nThe review history is available as Additional file 2.\n(9)\n\n\n\nACC = TP +TN\nTP +FN +TN +FP\nMCC = TP ×TN −FP×FN√(TP +FP)(TP +FN )(TN +FP)(TN +FN )\nSensitivity(SN ) = TP\nTP +FN\nSpeciﬁcity(SP) = TN\nTN +FP\nPage 22 of 23Jin et al. Genome Biology          (2022) 23:219 \nPeer review information\nAndrew Cosgrove was the primary editor of this article and managed its editorial process and peer review in collabora-\ntion with the rest of the editorial team.\nAuthors’ contributions\nJ.J. conceived iDNA-ABF. J.J., Y.Y., and Z.L. performed the experiments and data analysis. X.Z. and Y.D. constructed the cell \nline datasets. Y.J. constructed the web server. J.J., R.W., Y.Y., C.P ., R.S., Q.Z., K.N., and L.W. wrote, revised, and contributed to \nthe final manuscript. L.W. and K.N. designed the study and supervised the project. All authors read and approved the \nfinal manuscript.\nFunding\nThe work was supported by the Natural Science Foundation of China (Nos. 62071278, and 62072329). K.N. is supported \nby grants-in-aid for scientific research (22K06189), JSPS.\nAvailability of data and materials\nThe 5hmC site containing sequences for H. sapiens and M. musculus were collected from NCBI Gene Expression Omni-\nbus (GEO) database under accession number GSE127906 [40].\nThe 6mA site data for 11 species (Arabidopsis thaliana (A. thaliana), Caenorhabditis elegans (C. elegans), Casuarina \nequisetifolia (C. equisetifolia), Drosophila melanogaster (D. melanogaster), Fragaria vesca (F. vesca), H. sapiens, Rosa \nchinensis (R. chinensis), Saccharomyces cerevisiae (S. cerevisiae), Tolypocladium sp SUP5-1 (Ts. SUP5-1), Tetrahymena \nthermophile (T. thermophile), and Xanthomonas oryzae pv. Oryzicola (Xoc) BLS256 (Xoc. BLS256)) were obtained from \nthe MethSMRT database (http:// sysbio. gzzoc. com/ meths mrt/) with accession numbers SRP145409 [41] under BioProject \nPRJNA450482, MDR database (http:// mdr. xiesl ab. org), GEO database under the accession number GSE104475 [42] and \nNCBI Genome database SRA: SRX1424851 and SRX1423750 in NCBI project SRA: PRJNA301527 [43], respectively.\nThe 4mC site data for 4 species (C. equisetifolia, F. vesca, S. cerevisiae, and Ts. SUP5-1) were obtained from the MDR \ndatabase (http:// mdr. xiesl ab. org) and MethSMRT database (http:// sysbio. gzzoc. com/ meths mrt/).\nThe human cell line dataset GM12878, K562, and HepG2 were obtained from ENCODE portal (ENCSR765JPC, ENCS-\nR890UQO, and ENCSR786DCL) [36, 44]. The ChIP-seq data H3K4me3 and H3K36me3 were also obtained from ENCODE \nportal (ENCSR668LDD, ENCSR000DWB, ENCSR057BWO, ENCSR000DRW, ENCSR575RRX, ENCSR000DUD) [36].\nTo facilitate the use of our method, we established a code-free, interactive, and non-programmatic web interface of \niDNA-ABF at https:// server. wei- group. net/ idnaa bf, which can lessen the programming burden biological and biomedical \nresearchers. Besides, the benchmarking datasets and our source code were also available at this server. In addition, our \nsource code is also available at https:// github. com/ FakeE nd/ iDNA_ ABF under MIT license and at Zenodo [45, 46].\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 22 February 2022   Accepted: 3 October 2022\nReferences\n 1. Smith ZD, Meissner A. DNA methylation: roles in mammalian development. Nat Rev Genet. 2013;14:204–20.\n 2. Bergman Y, Cedar H. DNA methylation dynamics in health and disease. Nat Struct Mol Biol. 2013;20:274–81.\n 3. Maegawa S, Hinkal G, Kim HS, Shen L, Zhang L, Zhang J, et al. Widespread and tissue specific age-related DNA \nmethylation changes in mice. Genome Res. 2010;20:332–40.\n 4. Yang H, Liu Y, Bai F, Zhang JY, Ma SH, Liu J, et al. Tumor development is associated with decrease of TET gene expres-\nsion and 5-methylcytosine hydroxylation. Oncogene. 2013;32:663–9.\n 5. Greenberg MV, Bourc’his D. The diverse roles of DNA methylation in mammalian development and disease. Nat Rev \nMol Cell Biol. 2019;20:590–607.\n 6. Fu Y, Luo GZ, Chen K, Deng X, Yu M, Han D, et al. N6-methyldeoxyadenosine marks active transcription start sites in \nChlamydomonas. Cell. 2015;161:879–92.\n 7. Zhao Z, Zhang X, Chen F, Fang L, Li J. Accurate prediction of DNA N 4-methylcytosine sites via boost-learning vari-\nous types of sequence features. BMC Genomics. 2020;21:1–11.\n 8. Lister R, Pelizzola M, Dowen RH, Hawkins RD, Hon G, Tonti-Filippini J, et al. Human DNA methylomes at base resolu-\ntion show widespread epigenomic differences. Nature. 2009;462:315–22.\n 9. Meissner A, Gnirke A, Bell GW, Ramsahoye B, Lander ES, Jaenisch R. Reduced representation bisulfite sequencing for \ncomparative high-resolution DNA methylation analysis. Nucleic Acids Res. 2005;33:5868–77.\n 10. Flusberg BA, Webster DR, Lee JH, Travers KJ, Olivares EC, Clark TA, et al. Direct detection of DNA methylation during \nsingle-molecule, real-time sequencing. Nat Methods. 2010;7:461–5.\n 11. Landan G, Cohen NM, Mukamel Z, Bar A, Molchadsky A, Brosh R, et al. Epigenetic polymorphism and the stochastic \nformation of differentially methylated regions in normal and cancerous tissues. Nat Genet. 2012;44:1207–14.\nPage 23 of 23\nJin et al. Genome Biology          (2022) 23:219 \n \n 12. Treangen TJ, Salzberg SL. Repetitive DNA and next-generation sequencing: computational challenges and solutions. \nNat Rev Genet. 2012;13:36–46.\n 13. Tang Q, Kang J, Yuan J, Tang H, Li X, Lin H, et al. DNA4mC-LIP: a linear integration method to identify N4-methylcyto-\nsine site in multiple species. Bioinformatics. 2020;36:3327–35.\n 14. Liu Q, Chen J, Wang Y, Li S, Jia C, Song J, et al. DeepTorrent: a deep learning-based approach for predicting DNA \nN4-methylcytosine sites. Brief Bioinform. 2021. https:// doi. org/ 10. 1093/ bib/ bbaa1 24.\n 15. Pian C, Zhang G, Li F, Fan X. MM-6mAPred: identifying DNA N6-methyladenine sites based on Markov model. Bioin-\nformatics. 2020;36:388–92.\n 16. Yu H, Dai Z. SNNRice6mA: a deep learning method for predicting DNA N6-methyladenine sites in rice genome. \nFront Genet. 2019;10:1071.\n 17. Li Z, Jiang H, Kong L, Chen Y, Lang K, Fan X, et al. Deep6mA: a deep learning framework for exploring similar patterns \nin DNA N6-methyladenine sites across different species. PLoS Comput Biol. 2021;17:e1008767.\n 18. Tsukiyama S, Hasan MM, Deng H-W, Kurata H. BERT6mA: prediction of DNA N6-methyladenine site using deep \nlearning-based approaches. Brief Bioinform. 2022.https:// doi. org/ 10. 1093/ bib/ bbac0 53.\n 19. Zhang L, Xiao X, Xu ZC. iPromoter-5mC: a novel fusion decision predictor for the identification of 5-methylcytosine \nsites in genome-wide DNA promoters. Front Cell Dev Biol. 2020;8:614.\n 20. Cheng X, Wang J, Li Q, Liu T. BiLSTM-5mC: A Bidirectional Long Short-Term Memory-Based Approach for Predicting \n5-Methylcytosine Sites in Genome-Wide DNA Promoters. Molecules. 2021;26:7414.\n 21. Lv H, Dao F-Y, Zhang D, Guan Z-X, Yang H, Su W, et al. iDNA-MS: an integrated computational tool for detecting DNA \nmodification sites in multiple genomes. Iscience. 2020;23:100991.\n 22. Yu Y, He W, Jin J, Xiao G, Cui L, Zeng R, Wei L. iDNA-ABT: advanced deep learning model for detecting DNA methyla-\ntion with adaptive features and transductive information maximization. Bioinformatics. 2021;37:4603-10.\n 23. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In:  Advances in \nneural information processing systems; 2017. p. 5998–6008.\n 24. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: pre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint. 2018. https:// doi. org/ 10. 48550/ arXiv. 1810. 04805.\n 25. McInnes L, Healy J, Melville J. UMAP: uniform manifold approximation and projection for dimension reduction; 2020.\n 26. de Vienne DM. Lifemap: exploring the entire tree of life. PLoS Biol. 2016;14:e2001624.\n 27. Wu X, Bartel DP . kpLogo: positional k-mer analysis reveals hidden specificity in biological sequences. Nucleic Acids \nRes. 2017;45:W534–8.\n 28. Bailey TL. STREME: accurate and versatile sequence motif discovery. Bioinformatics. 2021;37:2834–40.\n 29. Gupta S, Stamatoyannopoulos JA, Bailey TL, Noble WS. Quantifying similarity between motifs. Genome Biol. 2007;8:1–9.\n 30. Spektor R, Tippens ND, Mimoso CA, Soloway PD. methyl-ATAC-seq measures DNA methylation at accessible chro-\nmatin. Genome Res. 2019;29:969–77.\n 31. Putiri EL, Tiedemann RL, Thompson JJ, Liu C, Ho T, Choi J-H, et al. Distinct and overlapping control of 5-methylcyto-\nsine and 5-hydroxymethylcytosine by the TET proteins in human cancer cells. Genome Biol. 2014;15:1–20.\n 32. Wang M, Zhang K, Ngo V, Liu C, Fan S, Whitaker JW, et al. Identification of DNA motifs that regulate DNA methyla-\ntion. Nucleic Acids Res. 2019;47:6753–68.\n 33. Ji Y, Zhou Z, Liu H, Davuluri RV. DNABERT: pre-trained bidirectional encoder representations from transformers \nmodel for DNA-language in genome. Bioinformatics. 2021;37:2112–20.\n 34. Schoenfelder S, Fraser P . Long-range enhancer–promoter contacts in gene expression control. Nat Rev Genet. \n2019;20:437–55.\n 35. Li W, Godzik A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences. \nBioinformatics. 2006;22:1658–9.\n 36. Luo Y, Hitz BC, Gabdank I, Hilton JA, Kagda MS, Lam B, et al. New developments on the Encyclopedia of DNA Ele-\nments (ENCODE) data portal. Nucleic Acids Res. 2020;48:D882–9.\n 37. Goodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. arXiv preprint. 2014. https:// doi. \norg/ 10. 48550/ arXiv. 1412. 6572.\n 38. Miyato T, Dai AM, Goodfellow I. Adversarial training methods for semi-supervised text classification. arXiv preprint. \n2016. https:// doi. org/ 10. 48550/ arXiv. 1605. 07725.\n 39. Kumar R, Indrayan A. Receiver operating characteristic (ROC) curve for medical researchers. Indian Pediatr. 2011;48:277–87.\n 40. Hu L, Liu Y, Han S, Yang L, Cui X, Gao Y, Dai Q, Lu X, Kou X, Zhao Y, et al. Jump-seq: Genome-Wide Capture and Ampli-\nfication of 5-Hydroxymethylcytosine Sites. Journal of the American Chemical Society. 2019;141:8694-7.\n 41. Ye G, Zhang H, Chen B, Nie S, Liu H, Gao W, Wang H, Gao Y, Gu L. De novo genome assembly of the stress tolerant \nforest species Casuarina equisetifolia provides insight into secondary growth. The Plant Journal. 2019;97:779-94.\n 42. Xiao C-L, Zhu S, He M-H, Chen Y, Yu G-L, De Chen S-QX, et al. N6-methyladenine DNA modification in human \ngenome. Gene Expression Omnibus. 2018. https:// doi. org/ 10. 1101/ 176958.\n 43. Shi L, Guo Y, Dong C, Huddleston J, Yang H, Han X, et al. Long-read sequencing and de novo assembly of a Chinese \ngenome. Datasets Gene Expression Omnibus. 2016. https:// doi. org/ 10. 1038/ ncomm s12065.\n 44. Jing Zhang, Donghoon Lee, Vineet Dhiman, Peng Jiang, Jie Xu, Patrick McGillivray, Hongbo Yang et al. An integrative ENCODE \nresource for cancer genomics. Datasets. ENCODE portal. 2020. https:// doi. org/ doi: 10. 1038% 2Fs41 467- 020- 14743-w.\n 45. Jin J, Yu Y, Wang R, Zeng X, Pang C, Jiang Y, et al. iDNA-ABF: multi-scale deep biological language learning model for \nthe interpretable prediction of DNA methylations. Github; 2022. https:// github. com/ FakeE nd/ iDNA_ ABF.\n 46. Jin J, Yu Y, Wang R, Zeng X, Pang C, Jiang Y, et al. FakeEnd/iDNA_ABF: V0.0.0. Zenodo. 2022. https:// doi. org/ 10. 5281/ \nZENODO. 70182 76.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.6573124527931213
    },
    {
      "name": "Deep learning",
      "score": 0.5873006582260132
    },
    {
      "name": "Benchmarking",
      "score": 0.572977602481842
    },
    {
      "name": "Computer science",
      "score": 0.5332555174827576
    },
    {
      "name": "DNA methylation",
      "score": 0.4895718991756439
    },
    {
      "name": "Scale (ratio)",
      "score": 0.45351970195770264
    },
    {
      "name": "Machine learning",
      "score": 0.4233749210834503
    },
    {
      "name": "Biology",
      "score": 0.418806254863739
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.41828033328056335
    },
    {
      "name": "Computational biology",
      "score": 0.3853611350059509
    },
    {
      "name": "Natural language processing",
      "score": 0.3772760331630707
    },
    {
      "name": "Genetics",
      "score": 0.131000816822052
    },
    {
      "name": "Gene",
      "score": 0.11071234941482544
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Gene expression",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154099455",
      "name": "Shandong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    }
  ]
}