{
  "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation",
  "url": "https://openalex.org/W3181946646",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2349177014",
      "name": "Gao, Yunhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224628821",
      "name": "Zhou Mu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750248134",
      "name": "Metaxas Dimitris",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963542740",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2981899103",
    "https://openalex.org/W3092530369",
    "https://openalex.org/W3210384128",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2979375128",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2944712585",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3017153481",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3014795415",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2732063980",
    "https://openalex.org/W2963354015",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3112701542"
  ],
  "abstract": "Transformer architecture has emerged to be successful in a number of natural language processing tasks. However, its applications to medical vision remain largely unexplored. In this study, we present UTNet, a simple yet powerful hybrid Transformer architecture that integrates self-attention into a convolutional neural network for enhancing medical image segmentation. UTNet applies self-attention modules in both encoder and decoder for capturing long-range dependency at different scales with minimal overhead. To this end, we propose an efficient self-attention mechanism along with relative position encoding that reduces the complexity of self-attention operation significantly from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also proposed to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the dilemma that Transformer requires huge amounts of data to learn vision inductive bias. Our hybrid layer design allows the initialization of Transformer into convolutional networks without a need of pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac magnetic resonance imaging cohort. UTNet demonstrates superior segmentation performance and robustness against the state-of-the-art approaches, holding the promise to generalize well on other medical image segmentations.",
  "full_text": "UTNet: A Hybrid Transformer Architecture for\nMedical Image Segmentation\nYunhe Gao1, Mu Zhou1,2, and Dimitris Metaxas 1\n1 Department of Computer Science, Rutgers University\n2 SenseBrain and Shanghai AI Laboratory and Centre for Perceptual and Interactive\nIntelligence\nAbstract. Transformer architecture has emerged to be successful in a\nnumber of natural language processing tasks. However, its applications\nto medical vision remain largely unexplored. In this study, we present\nUTNet, a simple yet powerful hybrid Transformer architecture that in-\ntegrates self-attention into a convolutional neural network for enhancing\nmedical image segmentation. UTNet applies self-attention modules in\nboth encoder and decoder for capturing long-range dependency at dif-\nferent scales with minimal overhead. To this end, we propose an eﬃcient\nself-attention mechanism along with relative position encoding that re-\nduces the complexity of self-attention operation signiﬁcantly from O(n2)\nto approximate O(n). A new self-attention decoder is also proposed to\nrecover ﬁne-grained details from the skipped connections in the encoder.\nOur approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer de-\nsign allows the initialization of Transformer into convolutional networks\nwithout a need of pre-training. We have evaluated UTNet on the multi-\nlabel, multi-vendor cardiac magnetic resonance imaging cohort. UTNet\ndemonstrates superior segmentation performance and robustness against\nthe state-of-the-art approaches, holding the promise to generalize well on\nother medical image segmentations. Code is available 3.\n1 Introduction\nConvolutional networks have revolutionized the computer vision ﬁeld with out-\nstanding feature representation capability. Currently, the convolutional encoder-\ndecoder architectures have made substantial progress in position-sensitive tasks,\nlike semantic segmentation [14,11,20,17,6]. The used convolutional operation\ncaptures texture features by gathering local information from neighborhood pix-\nels. To aggregate the local ﬁlter responses globally, these models stack multiple\nconvolutional layers and expand the receptive ﬁeld through down-samplings. De-\nspite the advances, there are two inherent limitations of this paradigm. First,\nthe convolution only gathers information from neighborhood pixels and lacks\nthe ability to capture long-range (global) dependency explicitly [26,25,5]. Sec-\nond, the size and shape of convolution kernels are typically ﬁxed thus they can\nnot adapt to the input content [15].\n3 https://github.com/yhygao/UTNet\narXiv:2107.00781v2  [cs.CV]  28 Sep 2021\n2 Y. Gao et al.\nResidual Basic Block\nTrasformer Encoder\nTrasformer Decoder\nMax Pool 2x2\nUp-Conv 2x2\nSkip Connection\n(a) UTNet Structure\nBN\nMHSA\nBN\nReLU\nConv\naddition\nXl\nXl+1\n(c) Transformer Encoder Block\naddition\nBN\nConv\nBN\nReLU\nConv\nXl\nXl+1\nReLU\n(b) Residual Basic Block\naddition\nFig. 1. (a)The hybrid architecture of the proposed UTNet. The proposed eﬃcient self-\nattention mechanism and relative positional encoding allow us to apply Transformer to\naggregate global context information from multiple scales in both encoder and decoder.\n(b) Pre-activation residual basic block.(c) The structure of Transformer encoder block.\nTransformer architecture using the self-attention mechanism has emerged to\nbe successful in natural language processing (NLP) [18] with its capability of cap-\nturing long-range dependency. Self-attention is a computational primitive that\nimplements pairwise entity interactions with a context aggregation mechanism,\nwhich has the ability to capture long-range associative features. It allows the\nnetwork to aggregate relevant features dynamically based on the input content.\nPreliminary studies with simple forms of self-attention have shown its usefulness\nin segmentation [4,16], detection [24] and reconstruction [9].\nAlthough the application of image-based Transformer is promising, training\nand deploying of Transformer architecture has several daunting challenges. First,\nthe self-attention mechanism has O(n2) time and space complexity with respect\nto sequence length, resulting in substantial overheads of training and inference.\nPrevious works attempt to reduce the complexity of self-attention [10,28], but are\nstill far from perfection. Due to the time complexity, the standard self-attention\ncan be only applied patch-wise, e.g. [3,27] encode images using 16 ×16 ﬂattened\nimage patches as input sequences, or on top of feature maps from CNN backbone,\nwhich are already down-sampled into low-resolution [4,22]. However, for position-\nsensitive tasks like medical image segmentation, high-resolution feature plays a\nvital role since most mis-segmented areas are located around the boundary of the\nregion-of-interest. Second, Transformers do not have inductive bias for images\nand can not perform well on a small-scale dataset [3]. For example, Transformer\ncan be beneﬁcial from pre-training through a large-scale dataset like full JFT-\n300M [3]. But even with pre-training on ImageNet, Transformer is still worse\nUTNet: A Hybrid Transformer Architecture for Medical Image Segmentation 3\nthan the ResNet [12,7], not to mention medical image datasets with much less\navailable amounts of medical data.\nIn this paper, we propose a U-shape hybrid Transformer Network: UTNet,\nintegrating the strength of convolution and self-attention strategies for medical\nimage segmentation. The major goal is to apply convolution layers to extract\nlocal intensity features to avoid large-scale pretraining of Transformer, while\nusing self-attention to capture long-range associative information. We follow the\nstandard design of UNet, but replace the last convolution of the building block in\neach resolution (except for the highest one) to the proposed Transformer module.\nTowards enhanced quality of segmentation, we seek to apply self-attention to\nextract detailed long-range relationships on high-resolution feature maps. To\nthis end, we propose an eﬃcient self-attention mechanism, which reduces the\noverall complexity signiﬁcantly from O(n2) to approximate O(n) in both time\nand space. Furthermore, we use a relative position encoding in the self-attention\nmodule to learn content-position relationships in medical images. Our UTNet\ndemonstrates superior segmentation performance and robustness in the multi-\nlabel, multi-vendor cardiac magnetic resonance imaging cohort. Given the design\nof UTNet, our framework holds the promise to generalize well on other medical\nimage segmentations.\n2 Method\n2.1 Revisiting Self-attention Mechanism\nThe Transformer is built upon the multi-head self-attention (MHSA) module\n[18], which allows the model to jointly infer attention from diﬀerent represen-\ntation subspaces. The results from multiple heads are concatenated and then\ntransformed with a feed-forward network. In this study, we use 4 heads and\nthe dimension of multi-head is not presented for simplicity in the following for-\nmulation and in the ﬁgure. Consider an input feature map X ∈ RC×H×W ,\nwhere H,W are the spatial height, width and C is the number of channels.\nThree 1 ×1 convolutions are used to project X to query, key, value embeddings:\nQ, K, V ∈Rd×H×W , where d is the dimension of embedding in each head. The\nQ, K, V is then ﬂatten and transposed into sequences with size n ×d, where\nn = HW . The output of the self-attention is a scaled dot-product:\nAttention(Q, K, V) = softmax(QKT\n√\nd\n)\n  \nP\nV (1)\nNote that P ∈ Rn×n is named context aggregating matrix, or similarity\nmatrix. To be speciﬁc, the i-th query’s context aggregating matrix is Pi =\nsoftmax(qiKT\n√\nd ), Pi ∈R1×n, which computes the normalized pair-wise dot pro-\nduction between qi and each element in the keys. The context aggregating matrix\nis then used as the weights to gather context information from the values. In\nthis way, self-attention intrinsically has the global receptive ﬁeld and is good at\n4 Y. Gao et al.\nX\n11 Conv\nValue\ndHW\nKey\n11 Conv\ndHW\nQuery\n11 Conv\ndHW\nHWd hwd hwd\nSoftmax\nSub-Sample Sub-Sample\nA\nHWhw\nHWd\nRh Rw\nh1d 1wd\nhwd\nContent-Position Content-Content\nRh\nLow-Res\nFeature\n11 Conv\nValue\ndHlWl\nKey\n11 Conv\ndHlWl\nQuery\n11 Conv\ndHhWh\nHhWhd hwd hwd\nSoftmax\nSub-Sample Sub-Sample\nA\nHhWhhw\nHhWhd\nRw\nh1d 1wd\nhwd\nContent-Position Content-Content\nHigh-Res Encoder\nFeature\n(a) Multi-Head Self-Attention Encoder (b) Multi-Head Self-Attention Decoder\nR Q K  V  \nQR⊺ QK ⊺  QR⊺ QK ⊺  \nR Q K V  \nFig. 2.The proposed eﬃcient multi-head self-attention (MHSA). (a) The MHSA used\nin the Transformer encoder. (b) The MHSA used in the Transformer decoder. They\nshare similar concepts, but (b) takes two inputs, including the high-resolution features\nfrom skip connections of the encoder, and the low-resolution features from the decoder.\ncapturing long-range dependence. Also, the context aggregating matrix is adap-\ntive to input content for better feature aggregation. However, the dot-product\nof n ×d matrices leads to O(n2d) complexity. Typically,n is much larger than d\nwhen the resolution of feature map is large, thus the sequence length dominates\nthe self-attention computation and makes it infeasible to apply self-attention\nin high-resolution feature maps, e.g. n = 256 for 16 ×16 feature maps, and\nn = 16384 for 128 ×128 feature maps.\n2.2 Eﬃcient Self-attention Mechanism\nAs images are highly structured data, most pixels in high-resolution feature\nmaps within local footprint share similar features except for the boundary re-\ngions. Therefore, the pair-wise attention computation among all pixels is highly\nineﬃcient and redundant. From a theoretical perspective, self-attention is essen-\ntially low rank for long sequences [21], which indicates that most information is\nconcentrated in the largest singular values. Inspired by this ﬁnding, we propose\nan eﬃcient self-attention mechanism for our task as seen in Fig. 2.\nThe main idea is to use two projections to project key and value: K, V ∈\nRn×d into low-dimensional embedding: K, V ∈Rk×d, where k = hw ≪n, h and\nw are the reduced size of feature map after sub-sampling. The proposed eﬃcient\nself-attention is now:\nAttention(Q, K, V) = softmax(QK\nT\n√\nd\n)\n  \nP:n×k\nV\nk×d\n(2)\nBy doing so, the computational complexity is reduced to O(nkd). Notably,\nthe projection to low-dimensional embedding can be any down-sampling opera-\ntions, such as average/max pooling, or strided convolutions. In our implementa-\ntion, we use 1×1 convolution followed by a bilinear interpolation to down-sample\nthe feature map, and the reduced size is 8.\nUTNet: A Hybrid Transformer Architecture for Medical Image Segmentation 5\n2.3 Relative Positional Encoding\nStandard self-attention module totally discards the position information and is\nperturbation equivariant [1], making it ineﬀective for modeling image contents\nthat are highly structured. The sinusoidal embedding in previous works [13]\ndoes not have the property of translation equivariance in convolutional layers.\nTherefore, we use the 2-dimensional relative position encoding by adding relative\nheight and width information [1]. The pair-wise attention logit before softmax\nusing relative position encoding between pixel i = (ix, iy) and pixel j = (jx, jy)\n:\nli,j = qT\ni√\nd\n(kj + rW\njx−ix + rH\njy−iy ) (3)\nwhere qi is the query vector of pixel i, ki is the key vector for pixel j, rW\njx−ix\nand rH\njy−iy are learnable embeddings for relative width jx −ix and relative height\njy −iy respectively. Similar to the eﬃcient self-attention, the relative width and\nheight are computed after low-dimensional projection. The eﬃcient self-attention\nincluding relative position embedding is:\nAttention(Q, K, V) = softmax(QK\nT\n+ Srel\nH + Srel\nW√\nd\n)\n  \nP:n×k\nV\nk×d\n(4)\nwhere Srel\nH , Srel\nW ∈ RHW ×hw are matrics of relative position logits along\nheight and width dimensions that satisfySrel\nH [i, j] = qT\ni rH\njy−iy , Srel\nW [i, j] = qT\ni rW\njx−ix .\n2.4 Network Architecture\nFig. 1 highlights the architecture of UTNet. We seek to combine the strength\nfrom both convolution and self-attention mechanism. Therefore, the hybrid ar-\nchitecture can leverage the inductive bias of image from convolution to avoid\nlarge-scale pretraining, as well as the capability of Transformer to capture long-\nrange relationships. Because the mis-segmented region usually locates at the\nboundary of region-of-interest, the high-resolution context information could\nplay a vital role in segmentation. As a result, our focus is placed on the pro-\nposed self-attention module, making it feasible to handle large-size feature maps\neﬃciently. Instead of naively integrating the self-attention module on top of the\nfeature maps from the CNN backbone, we apply the Transformer module to each\nlevel of the encoder and decoder to collect long-range dependency from multiple\nscales. Note that we do not apply Transformer on the original resolution, as\nadding Transformer module in the very shallow layers of the network does not\nhelp in experiments but introduces additional computation. A possible reason\nis that the shallow layers of the network focus more on detailed textures, where\ngathering global context may not be informative. The building block of UTNet\nis shown in Fig. 1 (b) and (c), including residual basic block and Transformer\nblock. For both blocks, we use the pre-activation setting for identity mapping in\n6 Y. Gao et al.\nthe short cut. This identity mapping has been proven to be eﬀective in vision\n[8] and NLP tasks [19].\n3 Experiment\n3.1 Experiment Setup\nWe systematically evaluate the UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging (MRI) challenge cohort [2], including the segmenta-\ntion of left ventricle (LV), right ventricle (RV), and left ventricular myocardium\n(MYO). In the training set, we have 150 annotated images from two diﬀerent\nMRI vendors (75 images of each vendor), including A: Siemens; B: Philips. In the\ntesting set, we have 200 images from 4 diﬀerent MRI vendors (50 images of each\nvendor), including A: Siemens; B: Philips; C: GE; D: Canon, where vendor C\nand D are completely absent in the training set (we discard the unlabeled data).\nThe MRI scans from diﬀerent vendors have marked diﬀerences in appearance,\nallowing us to measure model robustness and compare with other models under\ndiﬀerent settings. Speciﬁcally, we have performed two experiments to highlight\nthe performance and robustness of UTNet. First, we report primary results with\ntraining and testing data are both from the same vendor A. Second, we further\nmeasure the cross-vendor robustness of models. Such setting is more challenging\nsince the training and testing data are from independent vendors. We report\nDice score and Hausdorﬀ distance of each model to compare the performance.\n3.2 Implementation Detail\nFor data preprocessing, we resample the in-plane spacing to 1.2 ×1.2 mm, while\nkeeping the spacing along the z-axis unchanged. We train all models from scratch\nfor 150 epochs. We use the exponentially learning rate scheduler with a base\nlearning rate of 0.05. We use the SGD optimizer with a batch size of 16 on\none GPU, momentum and weight decay are set to 0.9 and 1 e −4 respectively.\nData augmentation is applied on the ﬂy during model training, including ran-\ndom rotation, scaling, translation, additive noise and gamma transformation.\nAll images are randomly cropped to 256 ×256 before entering the models. We\nuse the combine of Dice loss and cross-entropy loss to train all networks.\n3.3 Segmentation Results\nWe compare the performance of UTNet with multiple state-of-the-art segmen-\ntation models. UNet [14] builds on top of the fully convolutional networks with\na U-shaped architecture to capture context information. The ResUNet is similar\nto UNet in architecture, but it uses residual blocks as the building block. CBAM\n[23] uses two sequential convolutional modules to infer channel and spatial at-\ntention to reﬁne intermediate feature maps adaptively. Dual attention network\n[4] uses two kinds of self-attention to model the semantic inter-dependencies in\nUTNet: A Hybrid Transformer Architecture for Medical Image Segmentation 7\nTable 1.Segmentation performance in term of Dice score and eﬃciency comparison.\nAll models are trained and tested using data from vendor A. The Hausdorﬀ distant\nresult is reported in the supplementary.\nUNet ResUNet CBAM Dual-Attn UTNet\nLV 91.8 92.2 92.2 92.4 93.1\nMYO 81.7 82.5 82.1 82.3 83.5\nRV 85.6 86.2 87.7 86.4 88.2\nAverage 86.4 86.9 87.3 87.0 88.3\nParams/M 7.07 9.35 9.41 9.69 9.53\nInference Time/s 0.085 0.115 0.149 0.118 0.145\n(a) (b) (c)\nFig. 3.Ablation study. (a) Eﬀect of diﬀerent self-attention position. (b) Eﬀect of\nreduce size and projection of eﬃcient self-attention. (c) Eﬀect of Transformer encoder,\nTransformer decoder, and the relative positional encoding.\nspatial and channel dimensions, respectively. We have implemented CBAM and\ndual attention in ResUNet backbone for better comparison. The dual attention\nis only applied in the feature maps after 4 down-samplings due to its quadratic\ncomplexity.\nAs seen in Table 1, UTNet demonstrates leading performance in all seg-\nmentation outcomes (LV, MYO and RV). By introducing residual connections,\nResUNet is slightly improved than the original UNet. The spatial and channel\nattention from CBAM are inferred from convolutional layers, it still suﬀers from\nlimited receptive ﬁeld. Thus CBAM only has limited improvement compared\nwith ResUNet. We also recognize that dual-attention approach was almost the\nsame as ResUNet, as it suﬀers from quadratic complexity that can not process\nhigher resolution feature maps to ﬁx errors in the segmentation boundary. Mean-\nwhile, our UTNet presents less parameters than dual-attention approach and it\ncan capture global context information from high-resolution feature maps.\nAblation study . Fig. 3 (a) shows the performance of diﬀerent self-attention\npositions. The number in the x-axis indicates the level where self-attention is\nplaces, e.g., ’34’ means the level where 3 and 4 times down-samplings are per-\nformed. As the level goes up, the self-attention can gather more ﬁne-grained\ndetail information with increased performance. However, the curve saturates\nwhen adding to the original resolution. We reason this as the very shallow layer\ntends to be more focused on local texture, where global context information is\nnot informative anymore. Fig. 3 (b) shows the result of eﬃcient self-attention’s\n8 Y. Gao et al.\nTable 2.Robustness comparison, measured with Dice score. All models are trained\non data from vendor A,B, and are tested on data from vendor A,B,C,D. The number\nin brackets of C and D indicates the performance drop compared with the average of\nA and B.\nResUNet CBAM UTNet\nVendor A B C D A B C D A B C D\nLV 92.5 90.1 88.7 (↓2.6) 87.2 (↓4.1) 93.3 91.0 89.4 (↓2.8) 88.8 (↓3.4) 93.1 91.4 89.8 (↓2.5) 90.5 (↓1.8)\nMYO 83.6 85.3 82.8 (↓1.7) 80.2 (↓4.3) 83.9 85.8 82.6 (↓2.3) 80.8 (↓4.1) 83.7 85.9 83.7 (↓1.1) 82.6 (↓2.2)\nRV 87.4 87.5 85.9 (↓1.6) 85.3 (↓2.2) 88.4 88.4 85.3 (↓3.1) 86.4 (↓2.0) 89.4 88.8 86.3 (↓2.8) 87.3 (↓1.8)\nAVG 87.9 87.6 85.7 (↓2.0) 84.2 (↓3.5) 88.5 88.4 85.5 (↓2.7) 85.3 (↓3.2) 88.7 88.7 86.6 (↓2.1) 86.2 (↓2.5)\nreduced size of 4, 8, 16. The reduced size 8 results in the best performance. The\ninterpolation down-sampling is slightly better than using max-pooling. Fig. 3 (c)\nshows the eﬀect of the Transformer encoder, decoder, and the relative positional\nencoding using the optimal hyper-parameter from (a) and (b). The combination\nof the Transformer encoder and decoder gives the optimal performance. The\nrelative positional encoding also plays a vital role, as removing it causes a large\nperformance drop.\nFor a head-to-head comparison with standard self-attention on space and\ntime complexity, we further apply dual attention in four resolutions (1, 2, 3, 4,\nsame as UTNet), and use the same input image size and batch size (256 ×256 ×\n16) to test the inference time and memory consumption. UTNet gains superior\nadvantage over dual attention with quadratic complexity, where GPU memory:\n3.8 GB vs 36.9 GB and time: 0.146 s vs 0.243 s.\nRobustness analysis. Table 2 shows results on training models with data from\nvendor A and B, and then test the models on vendor A, B, C, and D, respec-\ntively. When viewing results on C and D vendors, competing approaches suﬀer\nfrom vendor diﬀerences while UTNet retains competitive performance. This ob-\nservation can probably be attributed to the design of self-attention on multiple\nlevels of feature maps and the content-position attention, allowing UTNet to be\nbetter focused on global context information instead of only local textures. Fig.\n4 further shows that UTNet displays the most consistent results of boundaries,\nwhile the other three methods are unable to capture subtle characteristics of\nboundaries, especially for RV and MYO regions in cardiac MRI.\n4 Conclusion\nWe have proposed a U-shape hybrid Transformer network (UTNet) to merge\nadvances of convolutional layers and self-attention mechanism for medical im-\nage segmentation. Our hybrid layer design allows the initialization of Trans-\nformer into convolutional networks without a need of pre-training. The novel\nself-attention allows us to extend operations at diﬀerent levels of the network\nin both encoder and decoder for better capturing long-range dependencies. We\nbelieve that this design will help richly-parameterized Transformer models be-\ncome more accessible in medical vision applications. Also, the ability to handle\nlong sequences eﬃciently opens up new possibilities for the use of the UTNet on\nmore downstream medical image tasks.\nUTNet: A Hybrid Transformer Architecture for Medical Image Segmentation 9\nInput Image ResUNet CBAM Dual Attention UTNet\nFig. 4.Hard cases visualization on unseen testing data from vendor C and D. First\ntwo rows and the bottom two rows present the results and a zoom-in view of vendor\nC and D, respectively. The outline indicates the ground-truth annotation. Best viewed\nin color with LV(green), MYO(yellow), and RV(red). The test case from vendor C is\nblur due to motion artifacts, while the test case from vendor D is noisy and has low\ncontrast in the boundary. Only UTNet provides consistent segmentation, which demon-\nstrates its robustness. More visualization of segmentation outcomes are presented in\nthe supplementary.\nAcknowledgement This research was supported in part by NSF: IIS 1703883,\nNSF IUCRC CNS-1747778 and funding from SenseBrain, CCF-1733843, IIS-\n1763523, IIS-1849238, MURI- Z8424104 -440149 and NIH: 1R01HL127661-01\nand R01HL127661-05. and in part by Centre for Perceptual and Interactive\nIntellgience (CPII) Limited, Hong Kong SAR.\nReferences\n1. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Attention augmented convo-\nlutional networks. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 3286–3295 (2019)\n2. Campello, V.M., Palomares, J.F.R., Guala, A., Marakas, M., Friedrich, M.,\nLekadir, K.: Multi-Centre, Multi-Vendor & Multi-Disease Cardiac Image Segmen-\ntation Challenge (Mar 2020)\n3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\n10 Y. Gao et al.\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n4. Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual attention network for\nscene segmentation. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 3146–3154 (2019)\n5. Gao, Y., Huang, R., Yang, Y., Zhang, J., Shao, K., Tao, C., Chen, Y., Metaxas,\nD.N., Li, H., Chen, M.: Focusnetv2: Imbalanced large and small organ segmentation\nwith adversarial shape constraint for head and neck ct images. Medical Image\nAnalysis 67, 101831 (2021)\n6. Gao, Y., Liu, C., Zhao, L.: Multi-resolution path cnn with deep supervision for\nintervertebral disc localization and segmentation. In: International Conference\non Medical Image Computing and Computer-Assisted Intervention. pp. 309–317.\nSpringer (2019)\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\nIn: European conference on computer vision. pp. 630–645. Springer (2016)\n9. Huang, Q., Yang, D., Wu, P., Qu, H., Yi, J., Metaxas, D.: Mri reconstruction\nvia cascaded channel-wise attention network. In: 2019 IEEE 16th International\nSymposium on Biomedical Imaging (ISBI 2019). pp. 1622–1626. IEEE (2019)\n10. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross\nattention for semantic segmentation. In: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision. pp. 603–612 (2019)\n11. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a\nself-conﬁguring method for deep learning-based biomedical image segmentation.\nNature methods 18(2), 203–211 (2021)\n12. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., Houlsby,\nN.: Big transfer (bit): General visual representation learning. arXiv preprint\narXiv:1912.11370 6(2), 8 (2019)\n13. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: International Conference on Machine Learning. pp. 4055–\n4064. PMLR (2018)\n14. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n15. Schlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker, B., Rueck-\nert, D.: Attention gated networks: Learning to leverage salient regions in medical\nimages. Medical image analysis 53, 197–207 (2019)\n16. Sinha, A., Dolz, J.: Multi-scale self-guided attention for medical image segmenta-\ntion. IEEE journal of biomedical and health informatics (2020)\n17. Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N., Wu, Z., Ding, X.: Embracing\nimperfect datasets: A review of deep learning solutions for medical image segmen-\ntation. Medical Image Analysis 63, 101693 (2020)\n18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: NIPS (2017)\n19. Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.F., Chao, L.S.: Learning\ndeep transformer models for machine translation. arXiv preprint arXiv:1906.01787\n(2019)\nUTNet: A Hybrid Transformer Architecture for Medical Image Segmentation 11\n20. Wang, S., Zhou, M., Liu, Z., Liu, Z., Gu, D., Zang, Y., Dong, D., Gevaert, O.,\nTian, J.: Central focused convolutional neural networks: Developing a data-driven\nmodel for lung nodule segmentation. Medical image analysis 40, 172–183 (2017)\n21. Wang, S., Li, B., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768 (2020)\n22. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. pp.\n7794–7803 (2018)\n23. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: Convolutional block attention\nmodule. In: Proceedings of the European conference on computer vision (ECCV).\npp. 3–19 (2018)\n24. Yi, J., Wu, P., Jiang, M., Huang, Q., Hoeppner, D.J., Metaxas, D.N.: Attentive\nneural cell instance segmentation. Medical Image Analysis 55, 228–240 (2019).\nhttps://doi.org/https://doi.org/10.1016/j.media.2019.05.004\n25. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv\npreprint arXiv:1511.07122 (2015)\n26. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 2881–2890 (2017)\n27. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840 (2020)\n28. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural net-\nworks for semantic segmentation. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision. pp. 593–602 (2019)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.70633465051651
    },
    {
      "name": "Segmentation",
      "score": 0.5689423680305481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5376253128051758
    },
    {
      "name": "Initialization",
      "score": 0.4985048770904541
    },
    {
      "name": "Encoder",
      "score": 0.4883868098258972
    },
    {
      "name": "Convolutional neural network",
      "score": 0.48418715596199036
    },
    {
      "name": "Transformer",
      "score": 0.4732173979282379
    },
    {
      "name": "Architecture",
      "score": 0.45920756459236145
    },
    {
      "name": "Image segmentation",
      "score": 0.44826894998550415
    },
    {
      "name": "Computer engineering",
      "score": 0.33973413705825806
    },
    {
      "name": "Computer vision",
      "score": 0.32586026191711426
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32290786504745483
    },
    {
      "name": "Engineering",
      "score": 0.13991889357566833
    },
    {
      "name": "Programming language",
      "score": 0.09421837329864502
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 49
}