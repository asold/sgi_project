{
  "title": "SpectralFormer: Rethinking Hyperspectral Image Classification With Transformers",
  "url": "https://openalex.org/W3180637455",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2588029396",
      "name": "Danfeng Hong",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1969263688",
      "name": "Zhu Han",
      "affiliations": [
        "Aerospace Information Research Institute",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2096951688",
      "name": "Jing Yao",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2168598862",
      "name": "Lianru Gao",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2080333607",
      "name": "Bing Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2147799597",
      "name": "Antonio Plaza",
      "affiliations": [
        "Universidad de Extremadura"
      ]
    },
    {
      "id": "https://openalex.org/A1995330977",
      "name": "Jocelyn Chanussot",
      "affiliations": [
        "Grenoble Images Parole Signal Automatique",
        "Chinese Academy of Sciences",
        "Aerospace Information Research Institute",
        "Université Grenoble Alpes",
        "Centre National de la Recherche Scientifique",
        "Institut polytechnique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A2588029396",
      "name": "Danfeng Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969263688",
      "name": "Zhu Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096951688",
      "name": "Jing Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168598862",
      "name": "Lianru Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2080333607",
      "name": "Bing Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147799597",
      "name": "Antonio Plaza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995330977",
      "name": "Jocelyn Chanussot",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3133902755",
    "https://openalex.org/W2964179170",
    "https://openalex.org/W2891915458",
    "https://openalex.org/W3131515870",
    "https://openalex.org/W3168931281",
    "https://openalex.org/W3100714546",
    "https://openalex.org/W3044873739",
    "https://openalex.org/W2910655660",
    "https://openalex.org/W2755992512",
    "https://openalex.org/W3009562877",
    "https://openalex.org/W3137191419",
    "https://openalex.org/W3165729427",
    "https://openalex.org/W2977355106",
    "https://openalex.org/W2889773939",
    "https://openalex.org/W2994639710",
    "https://openalex.org/W3081287597",
    "https://openalex.org/W3046027728",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3016244469",
    "https://openalex.org/W2890133123",
    "https://openalex.org/W2029316659",
    "https://openalex.org/W2500751094",
    "https://openalex.org/W2609880332",
    "https://openalex.org/W2791006446",
    "https://openalex.org/W2898381489",
    "https://openalex.org/W3047443805",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2942454403",
    "https://openalex.org/W2128728535",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W6779902714",
    "https://openalex.org/W6791776128",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3048631361",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3122774149",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3101640299",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W2810170362",
    "https://openalex.org/W2588117332",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3140885850",
    "https://openalex.org/W3100011500",
    "https://openalex.org/W3101012758"
  ],
  "abstract": "Hyperspectral (HS) images are characterized by approximately contiguous spectral information, enabling the fine identification of materials by capturing subtle spectral discrepancies. Owing to their excellent locally contextual modeling ability, convolutional neural networks (CNNs) have been proven to be a powerful feature extractor in HS image classification. However, CNNs fail to mine and represent the sequence attributes of spectral signatures well due to the limitations of their inherent network backbone. To solve this issue, we rethink HS image classification from a sequential perspective with transformers, and propose a novel backbone network called \\ul{SpectralFormer}. Beyond band-wise representations in classic transformers, SpectralFormer is capable of learning spectrally local sequence information from neighboring bands of HS images, yielding group-wise spectral embeddings. More significantly, to reduce the possibility of losing valuable information in the layer-wise propagation process, we devise a cross-layer skip connection to convey memory-like components from shallow to deep layers by adaptively learning to fuse \"soft\" residuals across layers. It is worth noting that the proposed SpectralFormer is a highly flexible backbone network, which can be applicable to both pixel- and patch-wise inputs. We evaluate the classification performance of the proposed SpectralFormer on three HS datasets by conducting extensive experiments, showing the superiority over classic transformers and achieving a significant improvement in comparison with state-of-the-art backbone networks. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_SpectralFormer for the sake of reproducibility.",
  "full_text": "SUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 1\nSpectralFormer: Rethinking Hyperspectral Image\nClassiﬁcation with Transformers\nDanfeng Hong, Senior Member, IEEE, Zhu Han, Student Member, IEEE, Jing Yao, Lianru Gao, Senior\nMember, IEEE, Bing Zhang, Fellow, IEEE, Antonio Plaza, Fellow, IEEE, and Jocelyn Chanussot, Fellow, IEEE\nAbstract—Hyperspectral (HS) images are characterized by\napproximately contiguous spectral information, enabling the ﬁne\nidentiﬁcation of materials by capturing subtle spectral discrepan-\ncies. Owing to their excellent locally contextual modeling ability,\nconvolutional neural networks (CNNs) have been proven to be a\npowerful feature extractor in HS image classiﬁcation. However,\nCNNs fail to mine and represent the sequence attributes of\nspectral signatures well due to the limitations of their inherent\nnetwork backbone. To solve this issue, we rethink HS image\nclassiﬁcation from a sequential perspective with transformers,\nand propose a novel backbone network called SpectralFormer.\nBeyond band-wise representations in classic transformers, Spec-\ntralFormer is capable of learning spectrally local sequence\ninformation from neighboring bands of HS images, yielding\ngroup-wise spectral embeddings. More signiﬁcantly, to reduce\nthe possibility of losing valuable information in the layer-wise\npropagation process, we devise a cross-layer skip connection to\nconvey memory-like components from shallow to deep layers\nby adaptively learning to fuse “soft” residuals across layers. It\nis worth noting that the proposed SpectralFormer is a highly\nﬂexible backbone network, which can be applicable to both\npixel- and patch-wise inputs. We evaluate the classiﬁcation perfor-\nmance of the proposed SpectralFormer on three HS datasets by\nconducting extensive experiments, showing the superiority over\nclassic transformers and achieving a signiﬁcant improvement in\ncomparison with state-of-the-art backbone networks. The codes\nof this work will be available at https://github.com/danfenghong/\nIEEE TGRS SpectralFormer for the sake of reproducibility.\nIndex Terms—Hyperspectral image classiﬁcation, convolu-\ntional neural networks, deep learning, local contextual informa-\ntion, remote sensing, sequence data, skip fusion, transformer.\nI. I NTRODUCTION\nI\nN hyperspectral (HS) imaging, hundreds of (narrow) wave-\nlength bands are collected at each pixel across the complete\nThis work was supported in part by the National Natural Science Foun-\ndation of China under Grant 42030111 and Grant 41722108, and by the\nMIAI@Grenoble Alpes (ANR-19-P3IA-0003) and the AXA Research Fund.\n(Corresponding author: Lianru Gao )\nD. Hong, J. Yao and L. Gao are with the Key Laboratory of Digital\nEarth Science, Aerospace Information Research Institute, Chinese Academy\nof Sciences, 100094 Beijing, China. (e-mail: hongdf@aircas.ac.cn; yao-\njing@aircas.ac.cn; gaolr@aircas.ac.cn)\nZ. Han and B. Zhang are with the Key Laboratory of Digital Earth\nScience, Aerospace Information Research Institute, Chinese Academy of\nSciences, Beijing 100094, China, and also with the College of Resources and\nEnvironment, University of Chinese Academy of Sciences, Beijing 100049,\nChina. (e-mail: hanzhu19@mails.ucas.ac.cn; zb@radi.ac.cn)\nA. Plaza is with the Hyperspectral Computing Laboratory, Department\nof Technology of Computers and Communications, Escuela Polit ´ecnica,\nUniversity of Extremadura, 10003 C ´aceres, Spain. (e-mail: aplaza@unex.es).\nJ. Chanussot is with the Univ. Grenoble Alpes, INRIA, CNRS, Grenoble\nINP, LJK, F-38000 Grenoble, France, and also with the Aerospace Information\nResearch Institute, Chinese Academy of Sciences, Beijing 100094, China. (e-\nmail: jocelyn@hi.is)\nelectromagnetic spectrum, which enables the identiﬁcation or\ndetection of materials at a ﬁne-grained level, particularly for\nthose that have extremely similar spectral signatures in visual\ncues (e.g., RGB) [1]. This provides a great potential in a\nvariety of high-level Earth observation (EO) missions, such\nas accurate land cover mapping, precision agriculture, target\n/ object detection, urban planning, tree species classiﬁcation,\nmineral exploration, and so on.\nA general sequential process in a HS image classiﬁca-\ntion system consists of image restoration (e.g., denoising,\nmissing data recovery) [2]–[5], dimensionality reduction [6],\n[7], spectral unmixing [8]–[12], and feature extraction [13]–\n[16]. Among them, feature extraction is a crucial step in HS\nimage classiﬁcation, which has received increasing attention\nby researchers. Over the past decade, a large number of\nadvanced hand-crafted and subspace learning-based feature\nextraction approaches have been proposed for HS image\nclassiﬁcation [17]. These methods are capable of performing\nwell in small-sample classiﬁcation problems. However, they\ntend to meet the performance bottleneck, when the training\nsize gradually increases and the training set becomes more\ncomplex. The possible reason is due to the limited data ﬁtting\nand representation ability of these traditional methods. Inspired\nby the great success of deep learning (DL), that is capable\nof ﬁnding out connotative, intrinsic, and potentially valuable\nknowledge from the vast amounts of pluralistic data [18],\nenormous efforts have been made in designing and adding\nadvanced modules in networks to extract more diagnostic\nfeatures from remote sensing data. For example, Zhao et al.\n[19] developed a joint classiﬁcation framework using HS and\nlight detection and ranging (LiDAR) data, which has been\nshown to be excellent at extracting features from multisource\nRS data. Zhang et al. [20] designed an extraordinary patch-to-\npatch convolutional neural network (CNN), which has yielded\nsigniﬁcantly better results than other techniques.\nIn recent years, many well-recognized backbone networks\nhave been widely and successfully applied in the HS image\nclassiﬁcation task [28], such as autoencoders (AEs), CNNs,\nrecurrent neural networks (RNNs), generative adversarial net-\nworks (GANs), capsule networks (CapsNet), graph convolu-\ntional networks (GCNs). Chen et al. [21] stacked multiple\nautoencoder networks to extract deep feature representations\nfrom dimension-reduced HS images – generated by principal\ncomponent analysis (PCA) [29] – with the application to HS\nimage classiﬁcation. Chen et al. [22] employed CNNs instead\nof stacked AEs to semantically extract spatial-spectral features\nby considering local contextual information of HS images,\narXiv:2107.02988v2  [cs.CV]  20 Nov 2021\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 2\nFeature \nEmbedding\nEncoder Decoder\n. . . \n. . . \nPixel Reconstructed Pixel\nUnfold \nCategory\nFla/g372ening \nPatch\nConvolution Batch Normalization Activation Function\nAutoencoders CNNs RNNs \n... \nGraph Data\nGCN ReLU Soﬅmax\nCategory\nGCNs \nGraph Embedding\n. . . \nReal Spectral Signatures\nG\nGenerator G\nFake Spectral Signatures\n. . . \nInput Noise\nD\nDiscriminator D\nSigmoid Real/Fake\nSoﬅmax Category\nGANs \nConv+ReLU Primary Capsule Digit Capsule\n16 \n10 \nCategory\nCapsNet \nMulti-Head \nA/g372ention \nInput Output \nPositional \nEncoding \nAdd & Norm \nFeed Forward \nAdd & Norm \nPositional \nEncoding \nMulti-Head \nA/g372ention \nAdd & Norm \nFeed Forward \nMulti-Head \nA/g372ention \nAdd & Norm \nLinear \nSoﬅmax\nAdd & Norm \nOutput \nProbabilities Transformers \nFig. 1. An overview of currently well-recognized backbone networks for the HS image classiﬁcation task, such as autoencoders [21], CNNs [22], RNNs [23],\nGANs [24], CapsNet [25], GCNs [26], and Transformers [27].\nachieving higher classiﬁcation accuracies. Hang et al. [23]\ndesigned a cascaded RNN for HS image classiﬁcation by\ntaking advantage of RNNs that can model the sequentiality\nto represent the relations of neighboring spectral bands effec-\ntively. In [24], GANs were improved, making them applicable\nto the HS image classiﬁcation task, with the input of three PCA\ncomponents and random noises. Paoletti et al. [25] extended a\nCNN-based model by deﬁning a novel spatial-spectral capsule\nunit, yielding a high-performance classiﬁcation framework\nof HS images and meanwhile reducing the complexity of\nthe network design. Hong et al. [26] made a comprehensive\ncomparison between CNNs and GCNs on the classiﬁcation of\nHS images both qualitatively and quantitatively, and proposed\na mini-batch GCN (miniGCNs), providing a feasible solution\nfor addressing the issue of large graphs in GCNs, for state-of-\nthe-art HS image classiﬁcation.\nAlthough these backbone networks and their variants have\nbeen able to obtain promising classiﬁcation results, their ability\nin characterizing spectral series information (particularly in\ncapturing subtle spectral discrepancies along spectral dimen-\nsion) remains inadequate. Fig. 1 gives an overview illustration\nof these state-of-the-art backbone networks in the HS image\nclassiﬁcation task. The speciﬁc limitations can be roughly\nsummarized as follows.\n• As a mainstream backbone architecture, CNNs have\nshown their powerful ability in extracting spatially struc-\ntural information and locally contextual information from\nHS images. Nevertheless, on the one hand, CNNs can\nhardly be capable of capturing the sequence attributes\nwell, particularly middle- and long-term dependencies.\nThis unavoidably meets the performance bottleneck in the\nHS image classiﬁcation task, especially when the to-be-\nclassiﬁed categories are of a great variety and extremely\nsimilar in spectral signatures. On the other hand, CNNs\nare overly concerned with spatial content information,\nwhich spectrally distorts sequential information in the\nlearned features. This, to great extent, enables more\ndifﬁculties of mining diagnostic spectral attributes.\n• Unlike CNNs, RNNs are designed for sequence data,\nwhich accumulatively learns spectral features band-by-\nband from HS images in an orderly fashion. Such a\nmode extremely depends on the order of spectral bands\nand tends to generate gradient vanishing, thereby being\nhard to learn long-term dependencies [30]. This might\nfurther lead to the difﬁculty of capturing spectrally salient\nchanges in time series. More importantly, there are usu-\nally tons of HS samples (or pixels) available in real\nHS image scenes, yet RNNs fail to train the models\nin parallel, limiting the classiﬁcation performance in\npractical applications.\n• For other backbone networks, i.e., GANs, CapsNet,\nGCNs, despite their respective advantages in learning\nspectral representations (e.g., robustness, equivalence,\nlong-range relevance between samples), one thing in\ncommon is that almost all of them could be inherently\nincompetent for modeling sequential information effec-\ntively. That is, the utilization of spectral information is\ninsufﬁcient (being a critical bottleneck in ﬁne land cover\nclassiﬁcation or mapping using HS data).\nBeing aimed at the aforementioned limitations, we rethink\nthe HS image classiﬁcation process from a sequence data\nperspective with current state-of-the-art transformers [27].\nTotally different from CNNs and RNNs, transformers are (at\npresent) one of the cutting-edge backbone networks, owing to\nthe use of self-attention techniques, which are well-designed\nfor the sake of processing and analyzing sequential (or time\nseries) data more effectively. This will provide a good ﬁt for\nHS data processing and analysis, e.g., HS image classiﬁcation.\nIt is well-known that the self-attention block in transformers\nenables to capture globally sequential information by the\nmeans of positional encoding [31]. However, there also exist\nsome drawbacks in transformers that hinder its performance\nto be further improved. For example,\n1) although transformers perform outstandingly in solving\nthe problem of long-term dependencies of spectral signa-\ntures, they lose the power of capturing locally contextual\nor semantic characteristics ( cf. CNNs or RNNs);\n2) as mentioned in [32], the skip connection plays a crucial\nrole in transformers. This might be explained well by\neither using “residuals” to make the gradients better\npropagated or enhancing “memories” to reduce the for-\ngetting or loss of key information. But unfortunately, the\nsimple additive skip connection operation only occurs\nwithin each transformer block, weakening the connec-\ntivity across different layers or blocks.\nFor these reasons, we aim to develop a novel transformers-\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 3\nbased network architecture, SpectralFormer for short, enabling\nthe high-performance HS image classiﬁcation task. Spec-\ntralFormer provides point-to-point solutions corresponding to\nthe above two drawbacks. More speciﬁcally, SpectralFormer\nis capable of learning locally spectral representations from\nmultiple neighboring bands rather than single band (in orig-\ninal transformers) in each encoded position, e.g., group-wise\nversus band-wise embeddings. Furthermore, a cross-layer skip\nconnection is designed in SpectralFormer to progressively\nconvey memory-like components from shallow to deep layers\nby adaptively learning to fuse their “soft” residuals. The main\ncontributions of this paper can be summarized as follows.\n• We revisit the HS image classiﬁcation problem from a\nsequential perspective and propose a new transformers-\nbased backbone network, called SpectralFormer, in order\nto substitute for CNNs or RNNs-based architectures. To\nthe best of our knowledge, this is the ﬁrst time that\ntransformers (without any pre-processing operation, e.g.,\nfeature extraction using convolution and recurrent units\nor other transformation techniques) are purely applied to\nthe HS image classiﬁcation task.\n• We devise two simple but effective modules in Spec-\ntralFormer, i.e., group-wise spectral embedding (GSE)\nand cross-layer adaptive fusion (CAF), to learn locally\ndetailed spectral representations and convey memory-like\ncomponents from shallow to deep layers, respectively.\n• We qualitatively and quantitatively evaluate the classiﬁ-\ncation performance of the proposed SpectralFormer on\nthree representative HS datasets, i.e., Indian Pines, Pavia\nUniversity, and University of Houston, with extensive\nablation studies. The experimental results demonstrate\na signiﬁcant superiority over classic transformers (with\nan increase of approximately 10% OA) and other state-\nof-the-art backbone networks (at least 2% OA improve-\nment).\nThe remaining of the paper is organized as follows. Section\nII ﬁrst reviews classic transformers-related literature and then\ndetails the proposed SpectralFormer with two well-designed\nmodules for HS image classiﬁcation. Extensive experiments\nare conducted with ablation studies and discussions in Section\nIII. Section IV draws comprehensive conclusions and a brief\noutlook on future possible research directions.\nII. S PECTRAL FORMER\nIn this section, we start to review some preliminaries of\nclassic transformers. On this basis, we propose the Spec-\ntralFormer with two well-designed modules, i.e., GSE and\nCAF, making it more applicable to the HS image classiﬁcation\ntask. Finally, we also investigate the ability that the proposed\nSpectralFormer models the spatially contextual information\nwith the input of image patches.\nA. A Brief Review of Transformers\nAs is known to all, transformers [27] have cut a conspic-\nuous ﬁgure in processing sequence-to-sequence problems in\nnatural language processing (NLP), e.g., machine translation.\nSince they abandon the sequence dependence characteristic in\nMatMul \nScale \nMask (opt.) \nSoﬅMax \nMatMul \n(a) Self-attention\nLinear Linear Linear \nScaled Dot-Product A/g372ention \nScale \nLinear (b) Multi-head attention\nFig. 2. An illustration of the attention mechanism in transformers. (a) Self-\nattention module; (b) Multi-head attention.\nRNNs and alternatively introduce a brand-new self-attention\nmechanism. This enables the global information (long-term\ndependencies) capture for the units at any position, which\ngreatly promotes the development of time series data process-\ning models. Even not only limiting in NLP, image processing\nand computer vision ﬁelds have also begun exploring the trans-\nformer architecture. Very recently, the vision transformer (ViT)\n[33] seems to have achieved or approached CNNs-based state-\nof-the-art effects on various vision domain tasks, providing\nnew insight, inspiration, and creative space on vision-related\ntasks.\nThe success of transformers, to a great extent, depends on\nthe use of multi-head attention, where multiple self-attention\n(SA) [34] layers are stacked and integrated. As the name\nsuggests, SA mechanism is better at capturing the internal cor-\nrelation of data or features, thereby reducing the dependence\non external information. Fig. 2(a) illustrates the process of the\nSA module in transformers. More speciﬁcally, the SA layer\ncan be performed according to the following six steps:\nStep 1. Input the sequence data x with the length of m,\nwhere xi,i = 1,...,m denotes either a scalar or a vector.\nStep 2. The feature embedding, denoted as ai, is obtained\non each scalar or vector xi by a shared matrix W.\nStep 3. Each embedding multiplies by three different trans-\nformation matrices Wq, Wk, Wv, respectively, yielding three\nvectors, i.e., Query ( Q= [q1,...qm]), Key ( K = [k1,...km]),\nValue (V = [v1,..., vm]).\nStep 4. Compute the attention score s between each Q\nvector and each K vector in the form of inner product, e.g.,\nqi ·kj, and to stabilize the gradients, the scaled score is\nobtained by normalization, i.e., si,j = qi ·kj/\n√\nd, where d\nis the dimension of qi or kj.\nStep 5. The Softmax activation function is performed on\ns; we then have in the position-1 for instance: ˆs1,i =\nes1,i/∑\nj es1,j .\nStep 6. Generate attention representations z = [z1,..., zm],\ne.g., z1 = ∑\ni ˆs1,ivi.\nTo sum up, the SA layer can be integrally formulated as\nfollows\nz = Attention(Q,K,V ) = softmax(QK⊤\n√\nd\n)V. (1)\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 4\n . . . . . . \n . . . . . . \n . . . . . . \nLinear Projections of Locally Neighboring Spectral Bands \n1 2 3 4 n  . . .   . . .0 *Positional Embedding\n* Extra Learnable [Class] Emebdding\nGroup-wise Spectral Embedding (GSE)\nSpectral Signatures (Pixel or Patch)\nSpectral Bands: e.g., /g352om 1 to n\nTransformer Encoders with Cross-layer Adaptive Fusion (CAF)\nMLP \nHead\nCategory\nBuliding\nTree\nRoad\nGrass\n  . . .\nEmbedded Spectrum\nMulti-Head \nA/g372ention \nNorm \nNorm \nMLP \nTransformer Encoder \nL \nEncoder 1 Encoder 2 Encoder 3 Encoder 4  . . . . . . \nSpectralFromer for Hyperspectral Image Classiﬁcation \nL CAF CAF\nFig. 3. An overview illustration of the proposed SpectralFormer network for the HS image classiﬁcation task. SpectralFormer consists of two well-designed\nmodules, i.e., GSE and CAF, making it (the transformers-based backbone) better applicable to HS images.\nWith Eq. (1), multiple different SA layers can be assembled\nto be a multi-head attention, as shown in Fig. 2(b). In detail,\nwe ﬁrst obtain multiple attention representations (e.g., h= 8),\ndenoted as zh,h = 1,..., 8, and concatenate them to be a larger\nfeature matrix. A linear transformation matrix (e.g., Wo) is\nﬁnally used to make the feature dimension identical to the\ninput data.\nIt should be noted, however, that there is no position\ninformation in the SA layer, which fails to make use of\nsequence information. For this reason, the position information\nis encoded into the feature embedding. The embedding with\ntime signal can be thus formulated as ai+ei, where ei denotes\na unique positional vector given manually.\nB. Overview of SpectralFormer\nWe aim at developing a novel and generic ViT-based\nbaseline network (i.e., SpectralFormer) with a focus on the\nspectrometric characteristics, making it well-applicable to the\nhighly accurate and ﬁne classiﬁcation of HS images. To\nthis end, we devise two key modules, i.e., GSE and CAF,\nand integrate them into the transformer framework, in order\nto improve the detail-capturing capacity of subtle spectral\ndiscrepancies and enhance the information transitivity (or\nconnectivity) between layers (i.e., reduce the information loss\nwith the gradual deepening of layers), respectively. Moreover,\nthe proposed SpectralFormer is not only applied to the pixel-\nwise HS image classiﬁcation, but also extensible to the spatial-\nspectral classiﬁcation with the batch-wise input, yielding the\nspatial-spectral SpectralFormer version. Fig. 3 illustrates an\noverview of the proposed SpectralFormer in the HS image\nclassiﬁcation task, while Table I details the deﬁnition of\nnotations used in the proposed SpectralFormer.\nC. Group-wise Spectral Embedding (GSE)\nUnlike the discrete sequentiality in classic transformers or\nViT (e.g., image patches), hundreds or thousands of spectral\nchannels in HS images are densely sampled from the electro-\nmagnetic spectrum at a subtle interval (e.g., 10nm), yielding\napproximately continuous spectral signatures. The spectral\ninformation in different position reﬂects different absorption\ncharacteristics corresponding to different wavelength. This to\na great extent shows the physical properties of the current\nmaterial. Capturing locally detailed absorption (or changes) of\nsuch spectral signatures is a crucial factor to accurately and\nﬁnely classify the materials lying in the HS scene. For this\npurpose, we propose to learn group-wise spectral embeddings,\ni.e., GSE, instead of band-wise input and representations.\nGiven a spectral signature (a pixel in the HS image) x =\n[x1,x2,...,x m] ∈R1×m, the feature embeddings a obtained\nby classic transformers are formulated by\nA = wx, (2)\nwhere w ∈ Rd×1 denotes the linear transformation that is\nequivalently used for all bands in spectral signatures and A ∈\nRd×m collects the output features. Whereas the proposed GSE\nlearns the feature embeddings from locally spectral proﬁles (or\nneighboring bands). Thus, the GSE can be modeled as\n˙A = WX = Wg(x), (3)\nwhere W ∈ Rd×n and X ∈ Rn×m correspond to the\ngrouped representations with respect to the variables w and\nx, respectively, nrepresents the number of neighboring bands.\nThe variable W can be simply seen as one layer of network,\nwhich can be optimized by updating the whole network. The\nfunction g(·) denotes the overlapping grouping operation in\nregard to the variable x, i.e.,\nX = g(x) = [x1,..., xq,..., xm], (4)\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 5\nTABLE I\nDEFINITION OF NOTATIONS USED IN THE PROPOSED SPECTRAL FORMER\nNotation Deﬁnition Type Size\nm the number of spectral bands scalar 1 ×1\nx spectral signature of a HS pixel vector 1 ×m\nxi reﬂectance or radiance value in the i-th band location scalar 1 ×1\nd the dimension of the feature embeddings scalar 1 ×1\nw the linear transformation w.r.t a pixel (sample) vector d×1\nA the feature embeddings matrix d×m\nn the number of considered HS pixels scalar 1 ×1\nW the linear transformations w.r.t. n pixels (samples) matrix d×n\nX spectral signatures of n pixels matrix n×m\ng(·) the overlapping grouping operation w.r.t. the variable (·) – –\n˙A the feature embeddings of GSE matrix d×m\n⌊·⌋ the round operation – –\nz(l) the feature representations in the (l)-th layer vector 1 ×dz\n¨w the learnable network parameter for adaptive fusion vector 1 ×2\nˆz(l) the fused representations in the (l)-th layer with the CAF vector 1 ×dz\nwhere xq = [ xq−⌊n\n2 ⌋,...,x q,...,x q+⌊n\n2 ⌋]⊤ ∈ Rn×1, ⌊•⌋\ndenotes the round operation. Fig. 4 illustrates the differences\nbetween band-wise and group-wise spectral embeddings in\ntransformers-based backbone networks, i.e., BSE versus GSE.\nD. Cross-layer Adaptive Fusion (CAF)\nThe skip connection (SC) mechanism has been proven to\nbe an effective strategy in deep networks, which can enhance\ninformation exchange between layers and reduce information\nloss in the network learning process. The use of SC has\nrecently gained tremendous success in image recognition and\nsegmentation, e.g., short SC for ResNet [35] and long SC for\nU-Net [36]. It should be noted, however, that the information\n“memory” ability of the short SC remains limited, while the\nlong SC tends to yield insufﬁcient fusion due to a big gap be-\ntween high- and low-level features. This is also a key problem\nexisted in transformers, which will pose a new challenge to the\narchitecture design of transformers. To this end, we devise a\nmiddle-range SC in SpectralFormer to adaptively learn cross-\nlayer feature fusion (i.e., CAF, see Fig. 5).\nLet z(l−2) ∈R1×dz and z(l) ∈R1×dz be the outputs (or\nrepresentations) in the (l−2)-th and (l)-th layers, respectively,\nCAF can be then expressed by\nˆz(l) ← ¨w\n[ z(l)\nz(l−2)\n]\n, (5)\nwhere ˆz(l) denotes the fused representations in the (l)-th layer\nwith the proposed CAF, and ¨w ∈ R1×2 is the learnable\nnetwork parameter for adaptive fusion. It should be noted\nthat our CAF only skips one encoder, e.g., from z(l−2) (the\noutput of Encoder 1) to z(l) (the output of Encoder 3) in\nFig. 5. The reasons for this setting are two-fold. On the one\nhand, there is a big semantic gap between low- and deep-\nlevel features obtained from the shallow and deep layers,\nrespectively. If the relatively long SC, e.g., two, three, and even\nmore encoders, is used, this then might lead to an insufﬁcient\nfusion and potential information loss. On the other hand, the\nHS image classiﬁcation can be often regarded as a small-\nsample problem, due to the limited available training (need\n . . . . . . \nPixel- or Patch-wise Spectral Bands: e.g., /g352om 1 to n\nLinear Projections of Spectral Bands \n(a) Band-wise Spectral Embedding (BSE)\n . . . . . . \n . . . . . . \n . . . . . . \nLinear Projections of Locally Neighboring Spectral Bands \nPixel- or Patch-wise Spectral Bands: e.g., /g352om 1 to n\nLocal Grouping\n(b) Group-wise Spectral Embedding (GSE)\nFig. 4. An illustrative comparison of band-wise and group-wise spectral\nembeddings (BSE versus GSE) in transformers.\nto be labeled manually) samples. A “small” or “shallow”\nbackbone network, e.g., 4 or 5 layers, may already be a good\nﬁt for the HS image classiﬁcation task. As a result, this, to\nsome extent, can explain why we propose to skip only one\nencoder in the CAF module (since a 4 or 5-layered shallow\nnetwork is small, which is not capable of adding multiple CAF\nmodules).\nE. Spatial-Spectral SpectralFormer\nBeyond the pixel-wise HS image classiﬁcation, we similarly\ninvestigate the patch-wise input (inspired by CNNs), yielding\nthe spatial-spectral SpectralFormer version, i.e., patch-wise\nSpectralFormer. Different from CNNs, that directly input a\n3-D patch cube, we unfold the 2-D patch of each band to the\ncorresponding 1-D vector representations. Given a 3-D cube\nX∈ Rm×w×h (wand hare the width and length of the patch),\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 6\nEncoder 1 Encoder 2 Encoder 3 Encoder 4  . . . . . . \nConv\nCAF\nCAF\nCAF\nOutput of encoder\nFig. 5. A diagram of cross-layer adaptive fusion (CAF) module in the\nproposed SpectralFormer.\nwhich can be unfolded along with the spatial direction, we then\nhave\nˆX = [⃗ x1,..., ⃗ xi,..., ⃗ xm], (6)\nwhere ⃗ xi ∈ Rwh×1 denotes the unfolded patch for the i-\nth band. Such an input way can to a great extent preserve\nthe spectrally sequential information in network learning and\nmeanwhile consider spatially contextual information.\nIII. E XPERIMENTS\nIn this section, three well-known HS datasets are ﬁrstly de-\nscribed, the implementation details and compared state-of-the-\nart methods are then introduced. Finally, extensive experiments\nare conducted with ablation analysis to assess the HS image\nclassiﬁcation performance of the proposed SpectralFormer\nboth quantitatively and qualitatively.\nA. Data Description\n1) Indian Pines Data: The ﬁrst HS data was collected in\n1992 by using the Airborne Visible/Infrared Imaging Spec-\ntrometer (A VIRIS) sensor over North-Western Indiana, USA.\nThe HS image consists of 145 ×145 pixels at a ground\nsampling distance (GSD) of 20m, and 220 spectral bands\ncovering the wavelength range of 400nm to 2500nm with a\n10m spectral resolution. After removing 20 noisy and water\nabsorption bands, 200 spectral bands are retained, i.e., 1-103,\n109-149, 164-219. There are 16 mainly-investigated categories\nin this studied scene. The class name and the number of\nsamples used for training and testing in the classiﬁcation task\nare listed in Table II, while the spatial distribution of training\nand testing sets are also given in Fig. 7 to reproduce the\nclassiﬁcation results.\n2) Pavia University Data: The second HS scene was ac-\nquired by the Reﬂective Optics System Imaging Spectrometer\n(ROSIS) sensor over Pavia University and its surroundings,\nPavia, Italy. The sensor can capture 103 spectral bands ranging\nfrom 430nm to 860nm, and the image consists of 610 ×340\npixels at a GSD of 1.3m. This scene includes 9 land cover\nclasses, where the ﬁxed number of training and testing samples\nare detailed in Table III and spatially visualized in Fig. 8.\nTABLE II\nLAND -COVER CLASSES OF THE INDIAN PINES DATASET , WITH THE\nSTANDARD TRAINING AND TESTING SETS FOR EACH CLASS .\nClass No. Class Name Training Testing\n1 Corn Notill 50 1384\n2 Corn Mintill 50 784\n3 Corn 50 184\n4 Grass Pasture 50 447\n5 Grass Trees 50 697\n6 Hay Windrowed 50 439\n7 Soybean Notill 50 918\n8 Soybean Mintill 50 2418\n9 Soybean Clean 50 564\n10 Wheat 50 162\n11 Woods 50 1244\n12 Buildings Grass Trees Drives 50 330\n13 Stone Steel Towers 50 45\n14 Alfalfa 15 39\n15 Grass Pasture Mowed 15 11\n16 Oats 15 5\nTotal 695 9671\nTABLE III\nLAND -COVER CLASSES OF THE PAVIA UNIVERSITY DATASET , WITH THE\nSTANDARD TRAINING AND TESTING SETS FOR EACH CLASS .\nClass No. Class Name Training Testing\n1 Asphalt 548 6304\n2 Meadows 540 18146\n3 Gravel 392 1815\n4 Trees 524 2912\n5 Metal Sheets 265 1113\n6 Bare Soil 532 4572\n7 Bitumen 375 981\n8 Bricks 514 3364\n9 Shadows 231 795\nTotal 3921 40002\n3) Houston2013 Data: The third dataset was acquired\nby the ITRES CASI-1500 sensor over the campus of the\nUniversity of Houston and its neighboring rural areas, Texas,\nUSA, which, as a benchmark dataset, has been widely used\nfor evaluating the performance of land cover classiﬁcation\n[37]. The HS cube comprises of 349 ×1905 pixels with 144\nwavelength bands in the range of 364nm to 1046nm at 10nm\nintervals. It is worth noting, however, that the Houston2013\ndataset we used is a cloud-free version, which is processed\nto recover the missing data or remove occlusions by gener-\nating illumination-related threshold maps 1. Table IV lists 15\nchallenging land-cover and land-use categories as well as the\ncorresponding sample number of training and testing sets.\nSimilarly, the visualization results with respect to the false-\ncolor HS image and the released training and testing samples\nprovided by the 2013 IEEE GRSS data fusion contest 2 are\ngiven in Fig. 10.\n1The data were provided by Prof. N. Yokoya from the University of Tokyo\nand RIKEN AIP.\n2http://www.grss-ieee.org/community/technical-committees/data-\nfusion/2013-ieee-grss-data-fusion-contest/\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 7\nTABLE IV\nLAND -COVER CLASSES OF THE HOUSTON 2013 DATASET, WITH THE\nSTANDARD TRAINING AND TESTING SETS FOR EACH CLASS .\nClass No. Class Name Training Testing\n1 Healthy Grass 198 1053\n2 Stressed Grass 190 1064\n3 Synthetic Grass 192 505\n4 Tree 188 1056\n5 Soil 186 1056\n6 Water 182 143\n7 Residential 196 1072\n8 Commercial 191 1053\n9 Road 193 1059\n10 Highway 191 1036\n11 Railway 181 1054\n12 Parking Lot1 192 1041\n13 Parking Lot2 184 285\n14 Tennis Court 181 247\n15 Running Track 187 473\nTotal 2832 12197\nB. Experimental Setup\n1) Evaluation Metrics: We evaluate the classiﬁcation per-\nformance of each model quantitatively in terms of three\ncommonly-used indices, i.e., Overall Accuracy (OA), Average\nAccuracy (AA) , Kappa Coefﬁcient ( κ). Moreover, the classi-\nﬁcation maps obtained by different models are visualized to\nmake a qualitative comparison.\n2) Comparison with State-of-the-art Backbone Networks:\nSeveral representative baselines and backbone networks are\nselected for the following comparison experiments. They are\nK-nearest neighbor (KNN), support vector machine (SVM),\nrandom forest (RF), 1-D CNN [17], 2-D CNN [22], RNN\n[23], miniGCN [26], transformers [27], and the proposed Spec-\ntralFormer. The parameter conﬁgurations of these compared\nmethods are detailed below:\n• For the KNN, the number of nearest neighbors ( K) is an\nimportant factor, which greatly impacts the classiﬁcation\nperformance. We set it to 10.\n• For the RF, 200 decision trees are used in our experi-\nments.\n• For the SVM, the libsvm toolbox 3 is selected for the\nimplementation of the HS image classiﬁcation task. The\nSVM is performed by using the radial basis function\n(RBF) kernel. In RBF, two hyperparameters σ and λcan\nbe optimally determined by ﬁve-fold cross validation on\nthe training set in the range of σ = [2−3,2−2,..., 24]\nand λ= [10−2,10−1,..., 104], respectively.\n• For the 1-D CNN, one convolutional block is deﬁned as\nthe basic network unit, including a set of 1-D convolu-\ntional ﬁlters with the output size of 128, a batch normal-\nization (BN) layer, and a ReLU activation function. A\nsoftmax function is ﬁnally added on the top layer of the\n1-D CNN.\n• The 2-D CNN architecture has three 2-D convolutional\nblocks and a softmax layer. Similar to the 1-D CNN,\neach convolutional block of 2-D CNN consists of a 2-D\nconventional layer, a BN layer, a max-pooling layer, and\n3https://www.csie.ntu.edu.tw/∼cjlin/libsvm/\na ReLU activation function. Moreover, the spatially and\nspectrally receptive ﬁelds in each 2-D convolutional layer\nare 3 ×3 ×32, 3 ×3 ×64, and 1 ×1 ×128, respectively.\n• For the RNN, there are two recurrent layers with the gated\nrecurrent unit (GRU). Each of them has 128 neuron units.\nThe used codes are openly available from https://github.\ncom/danfenghong/HyFTech.\n• For the miniGCN, the network block successively con-\ntains a BN layer, a graph convolutional layer with 128\nneuron units, and a ReLU layer. Note that the adjacency\nmatrix in GCN can be generated using a KNN-based\ngraph (the number of K is the same with the KNN clas-\nsiﬁer, i.e., K = 10). The miniGCN and 1-D CNN share\nthe same network architecture (for a fair comparison).\nAs the name suggests, the miniGCN can be trained in a\nmini-batch fashion. We refer to [26] for more details and\nthe codes4 for the sake of reproducibility.\n• For the transformers 5, we follow the ViT network ar-\nchitecture [33], i.e., only including transformer encoders.\nIn detail, ﬁve encoder blocks are used in the ViT-based\nnetwork for HS image classiﬁcation.\n• For the proposed SpectralFormer, we adopt the same\nbackbone architecture as the above transformers for a fair\ncomparison. More speciﬁcally, the embedded spectrum\nwith 64 units are fed into 5 cascaded transformer en-\ncoder blocks for HS image classiﬁcation. Each encoder\nblock consists of a four-head SA layer, a MLP with 8\nhidden dimensions, and a GELU [38] nonlinear activation\nlayer. The dropout layer is employed after encoding\npositional embeddings and in MLPs for inhibiting 10%\nneurons. Considering the fact that the parameter size is\nevidently increased from the pixel-wise SpectralFormer\nto the patch-wise one (the patch size is empirically set\nas 7 ×7), we additionally employ an ℓ2 weight decay\nregularization [39] parameterized by 5e−3 to prevent a\npotential overﬁtting risk for the latter.\n3) Implementation Details: Our proposed SpectralFormer\nwas implemented on the PyTorch platform using a workstation\nwith i7-6850K CPU, 128GB RAM, and an NVIDIA GTX\n1080Ti 11GB GPU. We adopt the Adam optimizer [40] with\na mini-batch size of 64. The learning rate is initialized with\n5e−4 and decayed by multiplying a factor of 0.9 after each\none-tenth of total epochs. We roughly set the epochs on the\nthree datasets to 10006. It is worth noting, however, that we\nfound in practice our SpectralFormer with the CAF module\nis capable of embracing an evident efﬁciency improvement\nby convergence using much less epochs, i.e., 300 epochs for\nthe Indian Pines dataset, and 600 epochs for the other two\ndatasets.\n4) Computational Complexity Analysis: For a given HS\nimage with spectral length m, the per-layer computational\ncomplexity of the proposed SpectralFormer is mainly dom-\ninated by self-attention and multi-head operations that require\n4https://github.com/danfenghong/IEEE TGRS GCN\n5The codes of transformers will be provided by authors, making it appli-\ncable to the HS image classiﬁcation task.\n6The epochs might be slightly adjusted for different algorithms and different\ndatasets, which will be speciﬁcally provided in our codes.\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 8\nTABLE V\nABLATION ANALYSIS OF THE PROPOSED SPECTRAL FORMER WITH A\nCOMBINATION OF DIFFERENT MODULES ON THE INDIAN PINES DATASET .\nMethod Input Module Metric\nGSE CAF OA (%) AA (%) κ\nTransformers (ViT)pixel-wise \u0017 \u0017 71.86 78.97 0.6804\nSpectralFormer pixel-wise \u0013 \u0017 75.05 82.91 0.7175\nSpectralFormer pixel-wise \u0017 \u0013 74.37 80.87 0.7084\nSpectralFormer pixel-wise \u0013 \u0013 78.55 84.68 0.7554\nSpectralFormer patch-wise \u0013 \u0013 81.76 87.81 0.7919\nTABLE VI\nSENSITIVITY ANALYSIS OF THE NUMBER OF NEIGHBORING BANDS IN\nSPECTRAL FORMER (ONLY GSE AND GSE + CAF) IN TERMS OF OA, AA,\nAND κON THE INDIAN PINES DATASET .\nSpectralFormer Metric The number of neighboring bands\n1 3 5 7 9 11\nOnly GSE\nOA (%) 71.86 74.21 72.77 75.05 74.13 74.11\nAA (%) 78.97 83.68 82.02 82.91 83.54 82.48\nκ 0.6804 0.7088 0.6918 0.7175 0.7074 0.7067\nGSE + CAF\nOA (%) 74.37 78.55 77.11 77.41 76.46 77.65\nAA (%) 80.87 84.68 84.55 84.20 83.45 83.85\nκ 0.7084 0.7554 0.7394 0.7426 0.7319 0.7457\nTABLE VII\nCLASSIFICATION PERFORMANCE ANALYSIS BETWEEN DIFFERENT SCS IN\nTRANSFORMERS ON THE INDIAN PINES DATASET .\nMethod Input SC Metric\nOA (%) AA (%) κ\nTransformers (ViT)\npixel-wise\nshort-range 71.86 78.97 0.6804\nTransformers (ViT) long-range 67.70 73.12 0.6315\nTransformers (ViT) CAF 74.37 80.87 0.7084\nan overall O(m2d+ md2), where d is the size of the hidden\nfeatures that are also used in deep competitors for a fair\ntheoretical comparison. The RNN yields the complexity of\nO(md2) after m times sequential operations, while a CNN\nwith the kernel width kincreases it considerably to O(kmd2).\nThe GCN (i.e., miniGCN) requires O(bmd+ b2m) owing to\nits batch-wise graph sampling, where b denotes the size of\nmini-batches.\nC. Model Analysis\n1) Ablation Study: We investigate the performance gain\nof the proposed SpectralFormer in terms of classiﬁcation\naccuracies by adding (in stepwise fashion) different modules\n(i.e., GSE and CAF) in networks. For that, we conduct\nextensive ablation experiments on the Indian Pines dataset to\nverify the effectiveness of these components (or modules) in\nSpectralFormer for HS image classiﬁcation applications, as\nlisted in Table V.\nIn detail, the classic transformers (ViT) without GSE and\nCAF modules yield the lowest classiﬁcation accuracies, which\nto some extent indicates that the ViT architecture might not be\na good ﬁt for the HS image classiﬁcation. By plugging either\nGSE or CAF into ViT, the classiﬁcation results of pixel-wise\nSpectralFormer are better than that of ViT (beyond around\n4% and 3% OAs, respectively). What is better still, the joint\nexploitation of GSE and CAF can further bring a dramatic\nperformance improvement (more than 4% OA). More remark-\nably, our SpectralFormer is also capable of capturing the\nlocally spatial semantics of HS images by simply unfolding the\npatch-wise input. As a result, the patch-wise SpectralFormer\nperforms observably better than pixel-wise ones at an increase\nat least 3% OA (compared to the second record, i.e., 78.55%).\nInterestingly, there is a noteworthy trend in Table V. That\nis, the joint use of GSE and CAF tends to obtain the best\nperformance when only using a smaller number of neighboring\nbands in GSE, compared to that of only using GSE. This\nmight be well explained by that after adding CAF, the spectral\ninformation is capable of being learned more efﬁciently and\neasier. In other words, the less overlap between spectral bands\n(i.e., the number of neighboring bands is smaller in GSE)\ncould be enough to obtain a better classiﬁcation performance,\nafter the CAF module is activated.\n2) Parameter Sensitivity Analysis: Apart from the learnable\nparameters in networks and hyper-parameters required in the\ntraining process, the number of neighboring bands in GES\nplays a vital role in the ﬁnal classiﬁcation performance. It\nis, therefore, indispensable to explore the proper parameter\nrange. Similarly, we investigate the parameter sensitivity on the\nIndian pines dataset in an ablation manner, i.e., only with GSE\nand the joint use of GSE and CAF. Table VI lists the changing\ntrend of the classiﬁcation accuracies with the gradual increase\nof the number of grouped bands in terms of OA, AA, and κ.\nA common conclusion is that GSE can better excavate subtle\nspectral discrepancies by effectively capturing locally spectral\nembeddings from neighboring bands. Within a certain range,\nthe parameter is not sensitive to the classiﬁcation performance.\nThis provides great potentials of the proposed model in the\npractical applications. In other words, the parameter can be\nsimply and directly used for other datasets.\nWe also make a quantitative comparison among short-range\nSC (e.g., in ResNet), long-range SC (e.g., in U-Net), and\nmiddle-range (i.e., our CAF module) SC in transformers,\nin order to verify the effectiveness of the proposed Spec-\ntralFormer in processing spectral data. Table VII quantiﬁes\nthe classiﬁcation performance comparison of using short-,\nmiddle-, and long-range SCs, respectively, on the Indian Pines\ndataset. In general, the ViT with long-range SC yields a\npoor performance, possibly since such an SC strategy might\nfail to sufﬁciently fuse and convey long-range cross-layer\nfeatures, tending to lose partially “important” information.\nThis demonstrates the superiority of the CAF module that can\nexchange information across different layers more effectively\n(cf. short-range SC) and reduce the information loss ( cf. long-\nrange SC).\nFurthermore, we randomly selected varying number of\ntraining samples from the given training set on the Indian\nPines dataset out of 10 runs in the proportion of [ 10%, 20%,\n... , 100%] at intervals of 10%. The average results with\nstandard deviation values in terms of the OA obtained by\nthe proposed SpectralFormer (including pixel-wise and patch-\nwise versions) are reported in Fig. 6. There is a basically\nreasonable trend in OA’s results (see Fig. 6). That is, with\nthe increase of the percentage of used training samples, the\nclassiﬁcation performance gradually improves. Note that the\nOAs are tending towards stability when more training samples\n(e.g., 80%, 90%, and 100%) are involved, showing the stability\nof the proposed SpectralFormer to a great extent. Moreover,\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 9\nTABLE VIII\nQUANTITATIVE PERFORMANCE OF DIFFERENT CLASSIFICATION METHODS IN TERMS OF OA, AA, AND κAS WELL AS THE ACCURACIES FOR EACH\nCLASS ON THE INDIAN PINES DATASET . THE BEST ONE IS SHOWN IN BOLD .\nClass No. Conventional Classiﬁers Classic Backbone Networks Transformers (ViT) SpectralFormer\nKNN RF SVM 1-D CNN 2-D CNN RNN miniGCN pixel-wise patch-wise\n1 45.45 57.80 67.34 47.83 65.90 69.00 72.54 53.25 62.64 70.52\n2 46.94 56.51 67.86 42.35 76.66 58.93 55.99 66.20 66.20 81.89\n3 77.72 80.98 93.48 60.87 92.39 77.17 92.93 86.41 88.59 91.30\n4 84.56 85.68 94.63 89.49 93.96 82.33 92.62 89.71 90.16 95.53\n5 80.06 79.34 88.52 92.40 87.23 67.72 94.98 87.66 89.24 85.51\n6 97.49 95.44 94.76 97.04 97.27 89.07 98.63 89.98 95.90 99.32\n7 64.81 77.56 73.86 59.69 77.23 69.06 64.71 72.22 85.19 81.81\n8 48.68 58.85 52.07 65.38 57.03 63.56 68.78 66.00 74.48 75.48\n9 44.33 62.23 72.70 93.44 72.87 65.07 69.33 57.09 72.34 73.76\n10 96.30 95.06 98.77 99.38 100.00 95.06 98.77 97.53 98.15 98.77\n11 74.28 88.75 86.17 84.00 92.85 88.67 87.78 87.62 93.01 93.17\n12 15.45 54.24 71.82 86.06 88.18 50.00 50.00 63.94 60.91 78.48\n13 91.11 97.78 95.56 91.11 100.00 97.78 100.00 95.56 100.00 100.00\n14 33.33 56.41 82.05 84.62 84.62 66.67 48.72 79.49 87.18 79.49\n15 81.82 81.82 90.91 100.00 100.00 81.82 72.73 90.91 90.91 100.00\n16 40.00 100.00 100.00 80.00 100.00 100.00 80.00 80.00 100.00 100.00\nOA (%) 59.17 69.80 72.36 70.43 75.89 70.66 75.11 71.86 78.55 81.76\nAA (%) 63.90 76.78 83.16 79.60 86.64 76.37 78.03 78.97 84.68 87.81\nκ 0.5395 0.6591 0.6888 0.6642 0.7281 0.6673 0.7164 0.6804 0.7554 0.7919\nTABLE IX\nQUANTITATIVE PERFORMANCE OF DIFFERENT CLASSIFICATION METHODS IN TERMS OF OA, AA, AND κ, AS WELL AS THE ACCURACIES FOR EACH\nCLASS ON THE PAVIA UNIVERSITY DATASET . THE BEST ONE IS SHOWN IN BOLD .\nClass No. Conventional Classiﬁers Classic Backbone Networks Transformers (ViT) SpectralFormer\nKNN RF SVM 1-D CNN 2-D CNN RNN miniGCN pixel-wise patch-wise\n1 73.86 79.81 74.22 88.90 80.98 84.01 96.35 71.51 82.95 82.73\n2 64.31 54.90 52.79 58.81 81.70 66.95 89.43 76.82 95.23 94.03\n3 55.10 46.34 65.45 73.11 67.99 58.46 87.01 46.39 78.18 73.66\n4 94.95 98.73 97.42 82.07 97.36 97.70 94.26 96.39 87.95 93.75\n5 99.19 99.01 99.46 99.46 99.64 99.10 99.82 99.19 99.46 99.28\n6 65.16 75.94 93.48 97.92 97.59 83.18 43.12 83.18 65.84 90.75\n7 84.30 78.70 87.87 88.07 82.47 83.08 90.96 83.08 92.35 87.56\n8 84.10 90.22 89.39 88.14 97.62 89.63 77.42 89.63 85.26 95.81\n9 98.36 97.99 99.87 99.87 95.60 96.48 87.27 96.48 100.00 94.21\nOA (%) 70.53 69.67 70.82 75.50 86.05 77.13 79.79 76.99 87.94 91.07\nAA (%) 79.68 80.18 84.44 86.26 88.99 84.29 85.07 80.22 87.47 90.20\nκ 0.6268 0.6237 0.6423 0.6948 0.8187 0.7101 0.7367 0.7010 0.8358 0.8805\nthe patch-wise SpectralFormer observably outperforms the\npixel-wise one, as expected.\nD. Quantitative Results and Analysis\nQuantitative classiﬁcation results in terms of three overall\nindices, i.e., OA, AA, and κ, and the accuracies for each class\nare reported in Tables VIII, IX, and X for Indian Pines, Pavia\nUniversity, and Houston2013 HS datasets, respectively.\nOverall, the conventional classiﬁers, e.g., KNN, RF, SVM,\nachieve similar classiﬁcation performance on all three data\nsets, except the accuracies in terms of OA, AA, and κ with\nKNN on the Indian Pines dataset (which are far inferior to\nthose using RF and SVM). Owing to the powerful learning\nability of DL techniques, classic backbone networks, e.g., 1-D\nCNN, 2-D CNN, RNN, miniGCN, observably perform better\nthan aforementioned conventional classiﬁers, i.e., KNN, RF,\nSVM. The results, to a great extent, demonstrate the value\nand practicality of DL-based approaches in HS image classiﬁ-\ncation. Without any convolutional and recurrent operations,\ntransformers extract ﬁner spectral representations from the\nsequence perspectives, yielding a comparable performance to\nCNNs-, RNNs-, or GCNs-based models.\nAlthough transformers are capable of capturing globally\nsequential information, the ability in modeling some key\nfactors – locally spectral discrepancies – remains limited,\nleads to a performance bottleneck. To overcome this prob-\nlem, the proposed SpectralFormer fully extracts local spectral\ninformation from neighboring bands, dramatically improv-\ning the classiﬁcation performance. In particular, the pixel-\nwise SpectralFormer unexpectedly outperforms others, even\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 10\nTABLE X\nQUANTITATIVE PERFORMANCE OF DIFFERENT CLASSIFICATION METHODS IN TERMS OF OA, AA, AND κAS WELL AS THE ACCURACIES FOR EACH\nCLASS ON THE HOUSTON 2013 DATASET. THE BEST ONE IS SHOWN IN BOLD .\nClass No. Conventional Classiﬁers Classic Backbone Networks Transformers (ViT) SpectralFormer\nKNN RF SVM 1-D CNN 2-D CNN RNN miniGCN pixel-wise patch-wise\n1 83.19 83.38 83.00 87.27 85.09 82.34 98.39 82.81 83.48 81.86\n2 95.68 98.40 98.40 98.21 99.91 94.27 92.11 96.62 95.58 100.00\n3 99.41 98.02 99.60 100.00 77.23 99.60 99.60 99.80 99.60 95.25\n4 97.92 97.54 98.48 92.99 97.73 97.54 96.78 99.24 99.15 96.12\n5 96.12 96.40 97.82 97.35 99.53 93.28 97.73 97.73 97.44 99.53\n6 92.31 97.20 90.91 95.10 92.31 95.10 95.10 95.10 95.10 94.41\n7 80.88 82.09 90.39 77.33 92.16 83.77 57.28 76.77 88.99 83.12\n8 48.62 40.65 40.46 51.38 79.39 56.03 68.09 55.65 73.31 76.73\n9 72.05 69.78 41.93 27.95 86.31 72.14 53.92 67.42 71.86 79.32\n10 53.19 57.63 62.64 90.83 43.73 84.17 77.41 68.05 87.93 78.86\n11 86.24 76.09 75.43 79.32 87.00 82.83 84.91 82.35 80.36 88.71\n12 44.48 49.38 60.04 76.56 66.28 70.61 77.23 58.50 70.70 87.32\n13 28.42 61.40 49.47 69.47 90.18 69.12 50.88 60.00 71.23 72.63\n14 97.57 99.60 98.79 99.19 90.69 98.79 98.38 98.79 98.79 100.00\n15 98.10 97.67 97.46 98.10 77.80 95.98 98.52 98.73 98.73 99.79\nOA (%) 77.30 77.48 76.91 80.04 83.72 83.23 81.71 80.41 86.14 88.01\nAA (%) 78.28 80.35 78.99 82.74 84.35 85.04 83.09 82.50 87.48 88.91\nκ 0.7538 0.7564 74.94 0.7835 0.8231 0.8183 0.8018 0.7876 0.8497 0.8699\n10% 20% 30% 40% 50% 60% 70% 80% 90% \nThe percentage of used training samples \n50 \n55 \n60 \n65 \n70 \n75 \n80 \n85 \n90 \nOA(%) \nSpectralFormer (pixel-wise) \nSpectralFormer (patch-wise) \n100% \n54.54%\n58.90%\n61.80%\n62.55% 62.02%\n66.43%\n73.08%\n75.43%\n77.21%\n78.55%\n56.44%\n59.88%\n59.80%\n62.63%\n65.99%\n70.07%\n77.08% 78.76%\n80.52% 81.76%\n\t0.011 )\n\t0.024 )\n\t0.021 )\n\t0.032 ) \t0.012 )\n\t0.003 )\n\t0.022 )\n\t0.020 )\n\t0.036 )\n\t0)\n\t0.030 )\n\t0.014 )\n\t0.032 )\n\t0.008 )\n\t0.031 )\n\t0.024 )\n\t0.012 )\n\t0.016 )\n\t0.022 )\n\t0)\nFig. 6. Classiﬁcation results (OA) obtained by our proposed SpectralFormer\n(pixel-wise and patch-wise) with the varying number of used training samples\non the Indian Pines dataset.\nthough comparing to CNNs-based methods considering spatial\ncontents. Undoubtedly, the patch-wise SpectralFormer obtains\nhigher classiﬁcation accuracies which are far superior to those\nobtained by other competitors, since the spatial-contextual\ninformation is jointly considered in the sequential feature\nextraction process.\nIn addition, for those challenging classes that have limited\ntraining samples (e.g., Grass Pasture Mowed, Oats) and imbal-\nanced (or noisy) samples (e.g., Corn Mintill , Grass Pasture,\nHay Windrowed ) on the Indian Pines data, SpectralFormer,\neither pixel-wise or patch-wise input, tends to obtain better\nclassiﬁcation performance by focusing on particular absorption\npositions of spectral proﬁles. By contrary, despite the excellent\nrepresentation ability for spectral sequence data, transformers\n(ViT) fail to accurately capture the detailed spectral absorption\nor change due to the weak modeling ability in locally spectral\ndiscrepancies.\nE. Visual Evaluation\nWe make a qualitative evaluation by visualizing the classi-\nﬁcation maps obtained by different methods. Figs. 7, 8, and\n10 provide the obtained results for the Indian Pines, Pavia\nUniversity, and Houston2013 datasets, respectively. Roughly,\nconventional classiﬁcation models (e.g., KNN, RF, SVM) tend\nto generate salt and pepper noises in classiﬁcation maps of\nthree considered datasets. This indirectly indicates that these\nclassiﬁers fail to accurately identifying the materials of objects.\nNot surprisingly, DL-based models, e.g., CNNs, RNNs, GCNs,\nobtain relatively smooth classiﬁcation maps, owing to their\npowerful nonlinear data ﬁtting ability. As an emerging network\narchitecture, transformers (ViT used in our case) can extract\nhighly sequential representations from HS images, leading\nto visualized classiﬁcation maps that are comparable to the\nabove classic backbone networks. By enhancing spectrally\nneighboring information and conveying “memory” information\nacross layers more effectively, our proposed SpectralFormer\nobtains highly desirable classiﬁcation maps, especially in\nterms of texture and edge details. Furthermore, we selected\na region of interest (ROI) (from Figs. 7, 8, and 10) zoomed-\nin 2 times to highlight the differences in classiﬁcation maps\nbetween different models, further evaluating their classiﬁcation\nperformance more intuitively. As can be seen from these ROIs,\na remarkable phenomenon is that our methods, i.e., pixel-wise\nand patch-wise SpectralFormers, show more realistic and ﬁner\ndetails. In particular, the results of our methods have less noisy\npoints compared to those pixel-wise methods, e.g., KNN, RF,\nSVM, 1-D CNN, RNN, miniGCN, and ViT, but also avoid\nover-smoothness in edges or some small semantic objects\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 11\nTraining\nTesting\nRF 1-D CNN\n 2-D CNN\nminiGCN Transformers (ViT) SpectralFormer (pixel)\nCornNotill CornMintill Corn GrassPature GrassTrees HayWindrow SoybeanNotill SoybeanMintill\nSoybeanClean Wheat Woods BuilGraTrDri StoSteTower Alfalfa GrassPastMow Oats\nSVM\nRNN SpectralFormer (patch)\nKNN\nFig. 7. Spatial distribution of training and testing sets, and the classiﬁcation maps obtained by different models on the Indian Pines dataset, where a ROI\nzoomed in 2 times is highlighted for more detailed observation.\nTraining KNN RF 1-D CNN 2-D CNN\nminiGCN \nAsphalt Meadows Gravel Trees Metal Sheets Bare Soil Bitumen Bricks Shadows \nTesting\nSVM\nTransformers (ViT) RNN SpectralFormer (pixel) SpectralFormer (patch)\nFig. 8. Spatial distribution of training and testing sets, and the classiﬁcation maps obtained by different models on the Pavia University dataset, where a ROI\nzoomed in 2 times is highlighted for more detailed observation.\nCAF \nViT \nHSI Patch \nFig. 9. Visualization of selected encoder output features obtained with only CAF and without CAF (i.e., the original ViT).\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 12\nTraining Testing\nKNN RF \nSVM 1-D CNN\n2-D CNN RNN\nminiGCN Transformers (ViT)\nSpectralFormer (pixel) SpectralFormer (patch)\nHealthy Grass Stressed Grass Synthetic Grass Trees Soil Water Residential Comme rcial Road Highway Railway Parkinig Lot1 Parkinig Lot2 Tennis Court Running Track \nFig. 10. Spatial distribution of training and testing sets, and the classiﬁcation maps obtained by different models on the Houston2013 dataset, where a ROI\nzoomed in 2 times is highlighted for more detailed observation.\n(cf. 2-D CNN), which yields more accurate classiﬁcation\nperformance.\nFeature Visualization. Fig. 9 visualizes selected encoder\noutput features using the proposed SpectralFormer framework\nwith only CAF and without CAF (i.e., ViT). We selectively\npick out some representative feature maps for visual compari-\nson, where the visualization results of using the CAF module\nhave ﬁner appearance (e.g., the edge or outline of objects,\ntextural structure, etc.) than those without CAF. This also\ndemonstrates the effectiveness and superiority of the designed\nCAF module from the visual perspective.\nIV. C ONCLUSION\nHS images are typically collected (or represented) as a data\ncube with spatial-spectral information, which can be generally\nregarded as a sequence of data along the spectral dimension.\nUnlike CNNs, that focus mainly on contextual information\nmodeling, transformers have been proven to be a powerful\narchitecture in characterizing the sequential properties glob-\nally. However, the classic transformers-based vision networks,\ne.g., ViT, inevitably suffer from performance degradation when\nprocessing HS-like data. This might be explained well by\nthe fact that ViT fails to model locally detailed spectral\ndiscrepancies and convey “memory”-like components (from\nshallow to deep layers) effectively. To this end, in this paper we\npropose a new transformers-based backbone network, called\nSpectralFormer, which is more focused on extracting spectral\ninformation. Without using any convolution or recurrent units,\nthe proposed SpectralFormer can achieve state-of-the-art clas-\nsiﬁcation results for HS images.\nIn the future, we will investigate strategies to further im-\nprove the transformers-based architecture by utilizing more\nadvanced techniques, e.g., attention, self-supervised learning,\nmaking it more applicable to the HS image classiﬁcation task,\nand also attempt to establish a lightweight transformers-based\nnetwork to reduce the network complexity while maintaining\nSUBMISSION TO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. XX, NO. XX, XXXX, 2021 13\nits performance. Moreover, we would also like to embed more\nphysical characteristics of spectral bands and prior knowledge\nof HS images into the proposed framework, yielding more\ninterpretable deep models. Furthermore, the number of skipped\nand connected encoders in the CAF module is an important\nfactor that might be capable of improving the classiﬁcation\nperformance of the proposed SpectralFormer, which should\nbe paid more attention to in future work.\nREFERENCES\n[1] D. Hong, W. He, N. Yokoya, J. Yao, L. Gao, L. Zhang, J. Chanussot, and\nX. X. Zhu, “Interpretable hyperspectral artiﬁcial intelligence: When non-\nconvex modeling meets hyperspectral remote sensing,” IEEE Geosci.\nRemote Sens. Mag. , vol. 9, no. 2, pp. 52–87, 2021.\n[2] Y . Wang, J. Peng, Q. Zhao, Y . Leung, X.-L. Zhao, and D. Meng,\n“Hyperspectral image restoration via total variation regularized low-rank\ntensor decomposition,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.,\nvol. 11, no. 4, pp. 1227–1243, 2017.\n[3] W. Cao, K. Wang, G. Han, J. Yao, and A. Cichocki, “A robust pca\napproach with noise structure learning and spatial–spectral low-rank\nmodeling for hyperspectral image restoration,” IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens. , vol. 11, no. 10, pp. 3863–3879, 2018.\n[4] M. Wang, Q. Wang, J. Chanussot, and D. Hong, “ l0-l1 hybrid total\nvariation regularization and its applications on hyperspectral image\nmixed noise removal and compressed sensing,” IEEE Trans. Geosci.\nRemote Sens., 2021. DOI: 10.1109/TGRS.2021.3055516.\n[5] J. Peng, W. Sun, H.-C. Li, W. Li, X. Meng, C. Ge, and Q. Du, “Low-\nrank and sparse representation for hyperspectral image processing: A\nreview,”IEEE Geosci. Remote Sens. Mag. , 2021.\n[6] D. Hong, N. Yokoya, J. Chanussot, J. Xu, and X. X. Zhu, “Joint\nand progressive subspace analysis (jpsa) with spatial-spectral manifold\nalignment for semi-supervised hyperspectral dimensionality reduction,”\nIEEE Trans. Cybern. , vol. 51, no. 7, pp. 3602–3615, 2021.\n[7] F. Luo, T. Guo, Z. Lin, J. Ren, and X. Zhou, “Semisupervised hypergraph\ndiscriminant learning for dimensionality reduction of hyperspectral\nimage,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 13,\npp. 4242–4256, 2020.\n[8] J. Yao, D. Meng, Q. Zhao, W. Cao, and Z. Xu, “Nonconvex-sparsity and\nnonlocal-smoothness-based blind hyperspectral unmixing,” IEEE Trans.\nImage Process., vol. 28, no. 6, pp. 2991–3006, 2019.\n[9] D. Hong, N. Yokoya, J. Chanussot, and X. Zhu, “An augmented linear\nmixing model to address spectral variability for hyperspectral unmixing,”\nIEEE Trans. Image Process. , vol. 28, no. 4, pp. 1923–1938, 2019.\n[10] Y . Yuan, Z. Zhang, and Q. Wang, “Improved collaborative non-negative\nmatrix factorization and total variation for hyperspectral unmixing,”\nIEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 13, pp. 998–\n1010, 2020.\n[11] L. Gao, Z. Han, D. Hong, B. Zhang, and J. Chanussot, “Cycu-\nnet: Cycle-consistency unmixing network by learning cascaded au-\ntoencoders,” IEEE Trans. Geosci. Remote Sens. , 2021. DOI:\n10.1109/TGRS.2021.3064958.\n[12] D. Hong, L. Gao, J. Yao, N. Yokoya, J. Chanussot, U. Heiden,\nand B. Zhang, “Endmember-guided unmixing network (egu-net): A\ngeneral deep learning framework for self-supervised hyperspectral un-\nmixing,” IEEE Trans. Neural Netw. Learn. Syst. , May 2021. DOI:\n10.1109/TNNLS.2021.3082289.\n[13] D. Hong, N. Yokoya, J. Chanussot, J. Xu, and X. Zhu, “Learning to\npropagate labels on graphs: An iterative multitask regression framework\nfor semi-supervised hyperspectral dimensionality reduction,” ISPRS J.\nPhotogramm. Remote Sens. , vol. 158, pp. 35–49, 2019.\n[14] J. Peng, W. Sun, and Q. Du, “Self-paced joint sparse representation for\nthe classiﬁcation of hyperspectral images,” IEEE Trans. Geosci. Remote\nSens., vol. 57, no. 2, pp. 1183–1194, 2018.\n[15] D. Hong, X. Wu, P. Ghamisi, J. Chanussot, N. Yokoya, and X. X. Zhu,\n“Invariant attribute proﬁles: A spatial-frequency joint feature extractor\nfor hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote\nSens., vol. 58, no. 6, pp. 3791–3808, 2020.\n[16] Q. Li, B. Zheng, B. Tu, J. Wang, and C. Zhou, “Ensemble emd-based\nspectral-spatial feature extraction for hyperspectral image classiﬁcation,”\nIEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 13, pp. 5134–\n5148, 2020.\n[17] B. Rasti, D. Hong, R. Hang, P. Ghamisi, X. Kang, J. Chanussot, and\nJ. Benediktsson, “Feature extraction for hyperspectral imagery: The\nevolution from shallow to deep: Overview and toolbox,” IEEE Geosci.\nRemote Sens. Mag. , vol. 8, no. 4, pp. 60–88, 2020.\n[18] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[19] X. Zhao, R. Tao, W. Li, H.-C. Li, Q. Du, W. Liao, and W. Philips, “Joint\nclassiﬁcation of hyperspectral and lidar data using hierarchical random\nwalk and deep cnn architecture,” IEEE Trans. Geosci. Remote Sens. ,\nvol. 58, no. 10, pp. 7355–7370, 2020.\n[20] M. Zhang, W. Li, Q. Du, L. Gao, and B. Zhang, “Feature extraction for\nclassiﬁcation of hyperspectral and lidar data using patch-to-patch cnn,”\nIEEE Trans. Cybern. , vol. 50, no. 1, pp. 100–111, 2018.\n[21] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep learning-based\nclassiﬁcation of hyperspectral data,” IEEE J. Sel. Top. Appl. Earth Obs.\nRemote Sens., vol. 7, no. 6, pp. 2094–2107, 2014.\n[22] Y . Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction\nand classiﬁcation of hyperspectral images based on convolutional neural\nnetworks,” IEEE Trans. Geosci. Remote Sens., vol. 54, no. 10, pp. 6232–\n6251, 2016.\n[23] R. Hang, Q. Liu, D. Hong, and P. Ghamisi, “Cascaded recurrent neural\nnetworks for hyperspectral image classiﬁcation,” IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 8, pp. 5384–5394, 2019.\n[24] L. Zhu, Y . Chen, P. Ghamisi, and J. A. Benediktsson, “Generative\nadversarial networks for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens. , vol. 56, no. 9, pp. 5046–5063, 2018.\n[25] M. E. Paoletti, J. M. Haut, R. Fernandez-Beltran, J. Plaza, A. Plaza, J. Li,\nand F. Pla, “Capsule networks for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens., vol. 57, no. 4, pp. 2145–2160, 2018.\n[26] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph\nconvolutional networks for hyperspectral image classiﬁcation,” IEEE\nTrans. Geosci. Remote Sens. , vol. 59, no. 7, pp. 5966–5978, 2021.\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2017.\n[28] S. Li, W. Song, L. Fang, Y . Chen, P. Ghamisi, and J. A. Benediktsson,\n“Deep learning for hyperspectral image classiﬁcation: An overview,”\nIEEE Trans. Geosci. Remote Sens., vol. 57, no. 9, pp. 6690–6709, 2019.\n[29] H. Abdi and L. J. Williams, “Principal component analysis,” Wiley\nInterdiscip. Rev. Comput. Stat. , vol. 2, no. 4, pp. 433–459, 2010.\n[30] Y . Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies\nwith gradient descent is difﬁcult,” IEEE Trans. Neural Netw. , vol. 5,\nno. 2, pp. 157–166, 1994.\n[31] G. Ke, D. He, and T.-Y . Liu, “Rethinking the positional encoding in\nlanguage pre-training,” arXiv preprint arXiv:2006.15595 , 2020.\n[32] Y . Dong, J.-B. Cordonnier, and A. Loukas, “Attention is not all you\nneed: Pure attention loses rank doubly exponentially with depth,” arXiv\npreprint arXiv:2103.03404, 2021.\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\n“An image is worth 16 ×16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[34] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proc. CVPR, pp. 7794–7803, 2018.\n[35] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. CVPR, pp. 770–778, 2016.\n[36] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Proc. MICCAI , pp. 234–241,\nSpringer, 2015.\n[37] D. Hong, L. Gao, N. Yokoya, J. Yao, J. Chanussot, D. Qian, and\nB. Zhang, “More diverse means better: Multimodal deep learning meets\nremote-sensing imagery classiﬁcation,” IEEE Trans. Geosci. Remote\nSens., vol. 59, no. 5, pp. 4340–4354, 2021.\n[38] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[39] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\narXiv preprint arXiv:1711.05101 , 2017.\n[40] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.7492766380310059
    },
    {
      "name": "Computer science",
      "score": 0.7008709907531738
    },
    {
      "name": "Transformer",
      "score": 0.6303365230560303
    },
    {
      "name": "Backbone network",
      "score": 0.6171722412109375
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5977165699005127
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5616656541824341
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5552579164505005
    },
    {
      "name": "Pixel",
      "score": 0.4433594048023224
    },
    {
      "name": "Feature extraction",
      "score": 0.41441112756729126
    },
    {
      "name": "Extractor",
      "score": 0.4134526252746582
    },
    {
      "name": "Data mining",
      "score": 0.387442946434021
    },
    {
      "name": "Telecommunications",
      "score": 0.15402349829673767
    },
    {
      "name": "Engineering",
      "score": 0.11850136518478394
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Process engineering",
      "score": 0.0
    }
  ]
}