{
    "title": "Deep scaffold hopping with multimodal transformer neural networks",
    "url": "https://openalex.org/W3213530673",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2910834915",
            "name": "Shuangjia Zheng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2226258273",
            "name": "Zengrong Lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2511477483",
            "name": "Ai Haitao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103126149",
            "name": "Hongming Chen",
            "affiliations": [
                "Guangzhou Regenerative Medicine and Health Guangdong Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2132719217",
            "name": "Daiguo Deng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130433203",
            "name": "Yuedong Yang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2910834915",
            "name": "Shuangjia Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2226258273",
            "name": "Zengrong Lei",
            "affiliations": [
                "Guangzhou Electronic Technology (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2511477483",
            "name": "Ai Haitao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103126149",
            "name": "Hongming Chen",
            "affiliations": [
                "Guangzhou Regenerative Medicine and Health Guangdong Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2132719217",
            "name": "Daiguo Deng",
            "affiliations": [
                "Guangzhou Electronic Technology (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2130433203",
            "name": "Yuedong Yang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W207689782",
        "https://openalex.org/W2031372687",
        "https://openalex.org/W2562257444",
        "https://openalex.org/W3005769002",
        "https://openalex.org/W2075186867",
        "https://openalex.org/W2107881319",
        "https://openalex.org/W2268755124",
        "https://openalex.org/W2067300585",
        "https://openalex.org/W3012836410",
        "https://openalex.org/W2967052368",
        "https://openalex.org/W1993452491",
        "https://openalex.org/W2899216155",
        "https://openalex.org/W2061937956",
        "https://openalex.org/W2800465968",
        "https://openalex.org/W2003242987",
        "https://openalex.org/W2046911317",
        "https://openalex.org/W2035162463",
        "https://openalex.org/W2063496698",
        "https://openalex.org/W2063021257",
        "https://openalex.org/W3047644501",
        "https://openalex.org/W2114704115",
        "https://openalex.org/W2790808809",
        "https://openalex.org/W2912212024",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2578240541",
        "https://openalex.org/W2916581152",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W2914635984",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2963028280",
        "https://openalex.org/W3111168552",
        "https://openalex.org/W3133989558",
        "https://openalex.org/W2949986955",
        "https://openalex.org/W2947950688",
        "https://openalex.org/W2992613109",
        "https://openalex.org/W3030948478",
        "https://openalex.org/W3011847211",
        "https://openalex.org/W3044640842",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W2074026532",
        "https://openalex.org/W2966702937",
        "https://openalex.org/W2005518172",
        "https://openalex.org/W1988037271",
        "https://openalex.org/W2060531713",
        "https://openalex.org/W2069244634",
        "https://openalex.org/W1970859533",
        "https://openalex.org/W2156125289",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2994678679",
        "https://openalex.org/W3109493217",
        "https://openalex.org/W6604896550",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W6828894009",
        "https://openalex.org/W2970279348",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2621742623",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W2100233978",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W2134967712",
        "https://openalex.org/W2529159351",
        "https://openalex.org/W1968299916",
        "https://openalex.org/W3098269892",
        "https://openalex.org/W2394739053",
        "https://openalex.org/W2095196563",
        "https://openalex.org/W3000478925",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2955905018",
        "https://openalex.org/W2992072991",
        "https://openalex.org/W2971227267",
        "https://openalex.org/W2902415322",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034516664"
    ],
    "abstract": "Abstract Scaffold hopping is a central task of modern medicinal chemistry for rational drug design, which aims to design molecules of novel scaffolds sharing similar target biological activities toward known hit molecules. Traditionally, scaffolding hopping depends on searching databases of available compounds that can't exploit vast chemical space. In this study, we have re-formulated this task as a supervised molecule-to-molecule translation to generate hopped molecules novel in 2D structure but similar in 3D structure, as inspired by the fact that candidate compounds bind with their targets through 3D conformations. To efficiently train the model, we curated over 50 thousand pairs of molecules with increased bioactivity, similar 3D structure, but different 2D structure from public bioactivity database, which spanned 40 kinases commonly investigated by medicinal chemists. Moreover, we have designed a multimodal molecular transformer architecture by integrating molecular 3D conformer through a spatial graph neural network and protein sequence information through Transformer. The trained DeepHop model was shown able to generate around 70% molecules having improved bioactivity together with high 3D similarity but low 2D scaffold similarity to the template molecules. This ratio was 1.9 times higher than other state-of-the-art deep learning methods and rule- and virtual screening-based methods. Furthermore, we demonstrated that the model could generalize to new target proteins through fine-tuning with a small set of active compounds. Case studies have also shown the advantages and usefulness of DeepHop in practical scaffold hopping scenarios.",
    "full_text": "Zheng et al. J Cheminform           (2021) 13:87  \nhttps://doi.org/10.1186/s13321-021-00565-5\nRESEARCH ARTICLE\nDeep scaffold hopping with multimodal \ntransformer neural networks\nShuangjia Zheng1†, Zengrong Lei2†, Haitao Ai2, Hongming Chen3, Daiguo Deng2* and Yuedong Yang1*  \nAbstract \nScaffold hopping is a central task of modern medicinal chemistry for rational drug design, which aims to design mol-\necules of novel scaffolds sharing similar target biological activities toward known hit molecules. Traditionally, scaffold-\ning hopping depends on searching databases of available compounds that can’t exploit vast chemical space. In this \nstudy, we have re-formulated this task as a supervised molecule-to-molecule translation to generate hopped molecules \nnovel in 2D structure but similar in 3D structure, as inspired by the fact that candidate compounds bind with their \ntargets through 3D conformations. To efficiently train the model, we curated over 50 thousand pairs of molecules with \nincreased bioactivity, similar 3D structure, but different 2D structure from public bioactivity database, which spanned \n40 kinases commonly investigated by medicinal chemists. Moreover, we have designed a multimodal molecular \ntransformer architecture by integrating molecular 3D conformer through a spatial graph neural network and protein \nsequence information through Transformer. The trained DeepHop model was shown able to generate around 70% \nmolecules having improved bioactivity together with high 3D similarity but low 2D scaffold similarity to the template \nmolecules. This ratio was 1.9 times higher than other state-of-the-art deep learning methods and rule- and virtual \nscreening-based methods. Furthermore, we demonstrated that the model could generalize to new target proteins \nthrough fine-tuning with a small set of active compounds. Case studies have also shown the advantages and useful-\nness of DeepHop in practical scaffold hopping scenarios.\nKeywords: Deep learning, Drug design, Scaffold hopping, Molecular optimization, Transformer neural network\n© The Author(s) 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nOver the past decades, the hit identification process of \ndrug discovery has been largely facilitated by the rapid \ndevelopments of both high-throughput screening (HTS) \nand fragment-based screening technologies [1]. These \nscreening strategies, together with the combinato -\nrial compound library, discover extensive collections of \ndiverse chemical series. Though these identified com -\npounds usually have weak potency and do not necessarily \npossess an ideal ADMET profile, they are starting points \n(hits) to identify more potent lead compounds through \nlead optimization or lead identification. [2]\nOne common strategy in the lead optimization is the \nscaffold hopping coined by Schneider and co-workers \n[3], where a given reference compound was modified \nin the backbone to generate structurally distinct com -\npounds while keeping the three-dimensional shape or \nthe pharmacophore in order to preserve the biologi -\ncal activity against its target protein [3, 4]. The strategy \nhas been widely used because such design can result in \nnovel chemotypes that have improved properties and/or \nachieve intellectual property rights. However, \"hop\" to \na hit molecule is not guaranteed to work in an expected \nway due to an incomplete understanding of the protein–\nligand interaction mechanism [5], unfavorable ADMET \nOpen Access\nJournal of Cheminformatics\n*Correspondence:  deco@fulmz.com; yangyd25@mail.sysu.edu.cn\n†Shuangjia Zheng and Zengrong Lei contributed equally to this work.\n1 School of Data and Computer Science, Sun Yat-Sen University, China, \n132 East Circle at University City, Guangzhou 510006, China\n2 Fermion Technology Co., Ltd, 1088 Newport East Road, \nGuangzhou 510335, China\nFull list of author information is available at the end of the article\nPage 2 of 15Zheng et al. J Cheminform           (2021) 13:87 \nproperties of the hopped structure, or activity cliff [6]. \nEmpirical scaffold transformation rules like ring-break/\nopening or bioisostere theory [7] summarized by medici -\nnal chemists are insufficient to overcome the sophisti -\ncated real-world cases.\nTo help chemists find better scaffold hops, there are a \nvariety of computational methods having been proposed, \nincluding 3D shape-based similarity search, fingerprint-\nbased similarity search, pharmacophore matching, and \nfragment replacement techniques [4, 8–19]. These meth-\nods mainly relied on a predefined database to select a \nmolecule or a fragment, with the differences between \napproaches arising from the searching algorithms of the \ndatabase, the ways to define the similarity of compound \npairs or the contents of the scanned database. Notwith -\nstanding the solid performance of existing scaffold hop -\nping methods, there remain three main challenges. First, \nthe number of potential hops for a chemotype is too \nlarge to be memorized and requires considerable creativ -\nity and experience. As an example, the widely-used vir -\ntual VEHICLe database [ 20] has 24,847 scaffolds, but it \nis composed of almost heteroaromatic mono- and bicy -\ncles. It remains intricate to maintain a balance between \ndiversity, size, and computational cost. Secondly, most \nof the currently utilized molecular fingerprints are the \nresult of algorithms involving some degree of knowledge-\nguided or manual feature engineering. While these rep -\nresentations can clearly be successful, they always feature \na trade-off in assigning importance to certain molecular \nfeatures while neglecting others, with this choice hand-\ncoded in the algorithm and not amenable to problem-\nspecific tuning [21]. Lastly, these methods depend on a \npredefined database and can’t cover the vast chemical \nspace estimated to contain 10 [23] and 10 [60] drug-like \nmolecules [22]. Therefore, it’s necessary to develop novel \nschemes that can automatically dig into the prioritized \nchemical space while providing bespoke molecular repre-\nsentation for hops identification.\nIn parallel, upon call for a more exhaustive and intel -\nligent exploration of chemical space, the de novo mole -\ncule design has been advanced by recent breakthroughs \nin deep generative models [23, 24]. Various generative \narchitectures, including RNNs [25– 27], autoencoders \n[28, 29], and generative adversarial networks (GANs) \n[30] have been proven effective for generating desir -\nable molecules by representing molecules with either \nthe simplified molecular input line entry specification \n(SMILES) [31] or molecular graph [32]. Recent works \nalso provide alternatives by combining the reinforce -\nment learning and docking methods to generate com -\npounds that satisfy key residue interactions with target \nprotein [33– 35]. These methods aimed to design struc -\nturally diverse compounds from scratch and thus have \nthe capability to search the whole drug-like space with -\nout relying on any predefined database or rules. Albeit \npowerful, these de novo design approaches are rarely \nconsider molecular optimization based on existing ref -\nerence compounds.\nBased on these observations, two research lines were \nrecently carried out for molecule design under scaffold \nconstraints. The first research line is called scaffold-\nbased molecule design proposed by Lim et al. [36] and \nLi et al. [37], where the graph generative models were \nutilized to extend a given scaffold by sequentially add -\ning atoms and bonds. In this context, the generated \nderivatives are guaranteed to maintain the scaffold with \ncertainty, and their properties can thus be controlled by \nconditioning the generation process on desired prop -\nerties. However, the generated molecules often differ \nsignificantly from the starting points in the 3D level, \nand many of the proposed transformations are R-group \nmodifications [38]. The other line is referred to as frag -\nment linking first proposed by Imrie and co-workers, \nwhere the original idea is to join fragments together \nwith a generated linker while keeping the relative con -\nformations of the fragments [39]. Yang et  al. further \nextended it as a sentence completion problem through \ntransformer neural networks [40]. Although these \napproaches claim their capability in scaffold hopping \nto generate molecules with high 3D similarities to the \noriginal molecule, their generated molecules often have \nhigher 2D similarities than expected due to the nature \nof fragment replacement, resulting in unfavorable intel -\nlectual property issues. Moreover, all these models were \ntrained in a ligand-based paradigm using a large num -\nber of bioactive compounds from the different public \ndatabases without using the information of the specific \ntarget proteins, imposing a limit in applications into the \ntarget-centric drug development process.\nIn this study, for the first time, we re-formulate the \nscaffold hopping task as a supervised molecule-to-mol -\necule translation instead of search problem. Given a \nreference molecule and a specified protein target, our \ngoal is to design scaffold hops incorporating 2D and \n3D structural information, protein target information, \nas well as bioactivity information. To this end, we have \ndeveloped a novel target-based scaffold hopping frame -\nwork, DeepHop, to optimize hit/lead compounds based \non a multimodal deep generative model. The model has \nbeen trained with over 50 K constructed scaffold hop -\nping pairs across 40 kinases. Extensive experiments \nshow that our model is capable of generating isofunc -\ntional molecular structures for seed molecules with \nnovel backbones and improved activity. More impor -\ntantly, our model could be easily extended to new \nPage 3 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \nprotein targets outside the training set, which is essen -\ntial for target-centric drug development.\nMethods\nTask definition\nAn exemplary scaffold hop is shown in Fig.  1. In this \nwork, we broadly define a scaffold hopping process as \nsuch: given an input reference molecule X and a speci -\nfied protein target Z, the model predicts the \"hopped\" \nmolecule Y with the improved pharmaceutical activity \nand similar 3D structure but dissimilar 2D structure.\nData preparation\nThere have only been a limited number of successfully \nreported examples for scaffold hopping. As a proof of \nconcept, we constructed sets of scaffold-hopping pairs \nusing a custom-made similarity scoring function from a \nsubset of ChEMBL20 [41].\nSpecifically, we processed the ChEMBL20 dataset by \nfiltering kinase-related target proteins with at least 300 \nup to 5000 unique bioactivity instances. The scaffold \nhopping application in the kinase family has always been \na topic of interest because the kinase patent literature \nis notoriously complicated and hard to break [42]. We \nfurther filtered out the SMILES strings containing dis -\nconnected ions or fragments. The molecules were then \nnormalized using RDKit, which involved the removal of \nsalt and isotopes, as well as charge neutralization. After \nthe preprocessing, the final data set contained 103,511 \nbioactivity data points across 152 kinases. Note that \nwe used pChEMBL values as the standard activity unit, \nwhich were defined as: -Log(molar  IC50, Ki, and Kd).\nDeep QSAR model\nBefore constructing the scaffold hopping pairs, one \nimportant factor required to assess the performance of \nscaffold hopping is whether the generated molecules \nhave similar bioactivity on the desired targets. To enable \na rapid and accurate profiling of generated molecules, vir-\ntual profiling models were trained on all the data points in \nthe whole kinase datasets. We evaluated the state-of-the-\nart directed messages passing neural networks (DMPNN) \n[43] and multi-task deep neural networks (MTDNN) [44] \nwith molecular graphs or molecular fingerprints as the \nmolecular representations. In particular, MTDNN was \nfound to obviously outperform DMPNN with an aver -\nage  R2 of 0.62 and RMSE of 0.61 (pCHEMBL value) on \ninternal test sets. Thus, the MTDNN model was used as \nthe virtual profiling model in the following studies. The \nmodeling details and results are shown in Supplementary \nfiles. For the quality of the virtual bioactive assessments, \nwe kept only targets that had a fivefold cross-validation  R2 \nhigher than 0.70, resulting in 40 targets in the end.\nConstruction of scaffold hopping pairs\nThe scaffold hopping definition emphasized two key \ncomponents: (i) different core structure and (ii) similar \ntopology and pharmacophore that ensure improved bio -\nlogical activities of the new compounds relative to the \nparent compounds. To mimic the scaffold hopping sce -\nnario, we constructed our data set following the idea of \nmatched molecular pairs (MMPs) proposed by Hussain \net  al. [45]. More specifically, we sampled target-based \nhopping pair ((X; Y)|Z) with a significant bioactivity \nimprovement (pCHEMBL Value ≥ 1) for new compound \nY over original compound X in the context of protein \nZ and a strict molecular similarity condition (2D scaf -\nfold similarity (X; Y) ≤ 0.6) ∩ (3D similarity (X; Y) ≥ 0.6). \nFollowing the recent study by Imrie et al [39], we meas -\nured 2D scaffold similarity through the Tanimoto score \nover Morgan fingerprints [46] of the compound scaffolds \n(here referred specifically to the Bemis and Murcko (BM) \nscaffold [47]), and 3D molecular similarity through the \nshape and color similarity score (SC score) (the pharma -\ncophoric feature similarity [48] and the shape similarity \n[49]). To compute the SC score, we sampled 100 confor -\nmations for each molecule using RDKit MMFF94 force \nfiled and selected only the lowest-energy conformation. \nThe SC score is a float value in the range of [0, 1], with \na higher value representing a higher similarity between \nFig. 1 A typical scaffold hop extracted from tankyrase-2 inhibitors [4]. \nThe two compounds have improved bioactivity  (pIC50 increase of 1.5) \nand similar 3D shapes (3D shape and pharmacophore similarity = 0.6) \nbut different scaffolds (2D Tanimoto scaffold similarity = 0.2)\nPage 4 of 15Zheng et al. J Cheminform           (2021) 13:87 \nmolecule pairs. Scores above 0.6 indicate fair structural \nmatches, and those above 0.8 indicate an excellent match.\nTo avoid redundancy of training pairs, we only allowed \nup to 10 hops for each source molecule. For each target, \nwe first randomly selected 10% bioactive molecules as \nthe test set and used the rest 90% molecules to construct \nscaffold hopping pairs for training and validation by a \nratio of 9:1. These processing steps resulted in a training \nset of 57,537 pairs and a test set of 3656 molecules over \n40 kinases.\nIt should be noted that our method could be adapted \nto other applications by simply constructing molecular \ntraining pairs satisfying the specific requirements. We \nemployed such a definition of scaffold hopping since \nthere is no generally preferred definition of core struc -\ntures or scaffolds or accepted metrics available for evalu -\nating the scaffold hopping potential.\nIndependent test set\nTo explore the generalization ability of proteins that have \nnever been observed during the training process, we \nretrieved six targets from the rest of the curated database \nas the independent test set. Among them, three proteins \n(CHEMBL2208, CHEMBL2147, CHEMBL2523) are \nnon-homologous with sequence identity less than 25% \n(calculated by the CDhit [50]) to any sequence in the \ntraining set, while others (CHEMBL4225, CHEMBL2292, \nCHEMBL2041) are homology to the training set with \nthe highest sequence identities of 59, 63, 76%, respec -\ntively. The compounds in these six proteins have never \nbe observed in the model training, validating, and testing \nprocesses. The details of these six proteins are shown in \nAdditional file 1: Table S3.\nModel architecture\nA novel multimodal graph transformer model was pro -\nposed for generating scaffold hops with inputs of a source \nmolecule and a protein sequence based on the trans -\nformer architecture [51]. As classical encoder-decoder \narchitecture, Transformer has recently shown the state \nof the art performances in many sequence-to-sequence \ntranslation tasks, including machine translation [52], ret -\nrosynthesis [53], and fragment assembly [40]. In previous \nchemical applications like retrosynthesis and fragment \nassembly, chemical structures were often converted into \nSMILES strings that ignored spatial information natu -\nrally embedded in chemical 3D conformers. Also, none \nof them considered the protein target information dur -\ning the transformation of the molecule pairs. Obviously, \nboth of these two features play crucial roles in the scaf -\nfold hopping task that needs to be considered.\nAs shown in Fig.  2, DeepHop comprises three main \ncomponents: (1) a molecular 3D graph neural network \n(GNN) for molecular conformer embedding, (2) a pre-\ntrained encoder for target protein embedding, and (3) a \ntransformer for mapping the scaffold hopping pairs.\nMolecular 3D conformer encoder\nWe adopted a simple 3D spatial GNN as the molecular \nconformer encoder following the strategy of Danel et  al. \n[54], which can learn both the molecular graph representa-\ntion and spatial distances between atoms in the 3D space. \nThe GNN follows the paradigm of message passing neural \nnetworks. The input of the conformer encoder is a 3D \nmolecular graph G = (V , E) , where V = {v1 ,... ,vn} \ndenotes a set of nodes (atoms) and E =[ eij]n\ni,j=1 represents \nedges (bonds) between atoms i and j. Each atom vi is repre-\nsented by a d-dimensional initial feature vector hi contain-\ning the 2D chemical features computed by RDkit (See more \ndetails in Additional file 1: Table S1). The atom is addition-\nally attached with its 3D coordinates p i ∈ R3 obtained by \nthe molecular conformer. The 3D GNN then updates the \natom embedding with message passing operations:\nwhere h (l)\nj  is d-dimension the feature vector of atom \n(node) j at the l-th updating iteration, N i is the set of \nneighbored atoms to atom i, U ∈ Rt×d and b ∈ Rd are \ntrainable network parameters and ⊙ denotes element-\nwise multiplication.\nHerein, the overall atom embeddings of the molecule \n(graph) can be described as H (l) ={ h(l)\n1 ,... h(l)\nn } . In the \nlast iteration of the node embedding updating, inspired \nby a recent molecular representation model [43], we \nintroduced a Gated Recurrent Unit (GRU) network [55] \nto increase the power of the network and obtained the \nfinal atom embeddings, as shown as\nwhere H (l)(v) is the set of atom representations in the \nmolecular graph G.\nProtein encoder\nCompared to the drug molecules, protein molecules \nare much bigger, typically containing more than 1,000 \nheavy atoms. To avoid a bulky model that contains too \nmany parameters, we adopted the Tasks Assessing Pro -\ntein Embeddings (TAPE) [56], a recently proposed semi-\nsupervised protein sequence representation learning \nmethod, to generate the protein pre-trained embeddings. \nh(l+1 )\ni (U ,b) =\n∑\nj∈N i\nReLU\n(\n(U T (pj − pi) + b) ⊙ h(l)\nj\n)\nˆH (v) = GRU (H (l)(v))\nPage 5 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \nTAPE was trained by a large transformer neural net -\nwork in an unsupervised paradigm with millions of \nprotein sequences. After training, it can generate an \ninformation-enriched feature vector for an input protein \nsequence. Formally, a protein can be described as a lin -\near sequence that consists of a list of amino acid residues \nP = (r1 ,... rl) . After processing through the TAPE, a \nvector Hp can be obtained as a k-dimensional pre-trained \nfeature vector.\nTransformer architecture\nThe fundamental architecture of DeepHop is a typi -\ncal Transformer neural network containing multiple \nencoder-decoder modules. Each encoder layer consists of \na multi-head self-attention sub-layer and a position-wise \nfeed-forward network (FFN) sub-layer. Multi-head atten-\ntion has several scaled dot-product attention functions \nworking in parallel, which allows the model to focus on \nmessages from different subspaces at different positions. \nThe attention between query (Q), keys (K), and values \n(V) was computed as\nwhere a scaling factor dk (equal to the size of weight \nmatrices) was introduced to avoid excessive dot prod -\nucts. The FFN sub-layer adopts the ReLU activation [57]. \nThen, layer normalization [58, 59] and a residual connec-\ntion [60] were introduced to link the above two sub-lay -\ners. Each decoder layer has three sub-layers, including an \nFFN sub-layer and two attention sub-layers. The decoder \nself-attention sub-layer utilizes a mask function to hinder \nattending to unseen future tokens. The encoder-decoder \nattention layer helps the decoder to focus on essential \nparts in the source sequence, and to capture the relation -\nship between the encoder and decoder.\nFor a given source molecule, we concatenate the \nlearned 3D graph representations ˆH (v) with SMILES \nsequence embedding M s = (s1, …, s m) in atomic level and \nconvert them through a simple linear transformation. \nAttention(Q , K , V ) = softmax\n(\nQK T\n√\ndk\n)\nV ,\nFig. 2 The basic architecture of the multimodal transformer model DeepHop. The model comprises three main components: (1) a 3D graph neural \nnetwork for molecular conformer embedding, (2) a pre-trained encoder for the target protein embedding, and (3) a transformer for mapping the \nscaffold hopping pairs\nPage 6 of 15Zheng et al. J Cheminform           (2021) 13:87 \nThe combined multimodal molecular representations are \nthen sent to the Transformer encoder to convert into a \nlatent representation L ∈ Rm ×f , where m is the sequence \nlength of molecular SMILES and f is the hidden state \ndimension. Afterward, we concatenate L with target pro-\ntein embedding H p ∈ Rk , resulting in a comprehensive \nrepresentation ˆL ∈ Rm ×(f+k) . Given ˆL , the decoder itera -\ntively generates an output SMILES sequence Y = (y1, …, \nyo) until the ending token \"⟨/s⟩\" is generated.\nDuring training, the model minimizes the cross-\nentropy loss between the target sequence Mt = (t1, …, t k) \nand the output sequence Y.\nBaseline models\nWe compare our approaches with the following baselines:\nConventional methods\n1. Ligand-based virtual screening (LBVS). Here, we \nprepared a ZINC lead-like compound library by \nfollowing the strategy of Moses [60], containing \n1,936,963 molecules with 448,854 unique Bemis-\nMurcko scaffolds. For a fair comparison, we ran -\ndomly selected 50,000 molecules in the library \n(equaling to our training set size) and chose top-10 \nmolecules with the highest 3D similarity to the refer -\nence molecule as the final hops. The molecules with \n2D-similarity higher than 0.6 were pre-excluded from \nthe random selection in the library.\n2. MMPA. MMPA was performed by the implementa -\ntion by Hussain et  al. [45], where molecular trans -\nformation rules were extracted from the kinase \ndataset for corresponding tasks. During the test, we \ntranslated a source molecule 10 times using different \nmatching transformation rules and selected the top-\n10 translations with the highest average bioactivity as \nscored by the virtual profiling model if there are more \nthan 10 matching rules.\nDeep learning methods\n3. Seq2seq. The seq2seq model utilizes SMILES strings \nto encode molecules. It consists of an LSTM encoder \nand an LSTM decoder with an attention mechanism. \nThis architecture has been successfully applied to \nother molecular de novo design and molecule trans -\nformation tasks [61].\nL(Y ,M ) =−\nk∑\ni=1\nyilogti\n4. G2G. The fourth baseline is a Graph-to-Graph \nmodel [62] that extends the junction variational \nautoencoder (VAE) via attention mechanism and \ngenerative adversarial networks (GAN). The model \nis capable of translating the current molecule to a \nsimilar molecule with predefined desired property \n(e.g., logP).\nNotably, these algorithms were not designed for multi-\ntask transformation. We randomly chose four targets as \nrepresentatives to evaluate the effectiveness of the base -\nlines and our model.\nEvaluation metrics\nThe scaffold hopping methods is often not comparable, \nsimilar to many virtual screening studies, partly due to \nthe inconsistent definition of scaffold hop and lack of \naccepted benchmarks. We quantitatively analyze the \nhopping success rate, bioactivity improvement, validity, \nuniqueness, diversity, and novelty of different methods.\n• Success rate is a metric that considers both simi -\nlarity and bioactivity improvement. Since this task \naims to generate a molecule that (i) has a different \nscaffold from the input molecule and (ii) has bioac -\ntivity improvement simultaneously. We design cri -\nteria to judge whether it satisfies these two require -\nments by: the generated molecule Y should (a) meet \nthe structural condition, i.e., (2D scaffold similar -\nity (X; Y) ≤ 0.6)∩(3D similarity (X; Y) ≥ 0.6); (b) has \na positive bioactivity gain, i.e., pBioactivity(Y)—\npBioactivity(X) ≥ 0, where the activity of generated \nmolecules was computed through the deep QSAR \nmodels. A constraint success rate is also accounted \nfor by confining a significant increase of bioactivity \nas: pBioactivity(Y)—pBioactivity(X) ≥ 1.\n• Bioactivity improvement is the average improve -\nment of biological activity between the source mol -\necule and the generated molecule computed as \npBioactivity(Y)—pBioactivity(X)\n• Validity is the percentage of generated molecules \nthat are chemically valid according to RDkit;\n• Uniqueness refers to the number of unique struc -\ntures generated;\n• Novelty refers to the percentage of novel molecules \n(not present in the training set) among the chemi -\ncally validly generated molecules.\nModel training and optimization of hyperparameters\nThe DeepHop model was implemented based on Open -\nNMT [63], and all scripts were written in Python [64] \nPage 7 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \n(version 3.7). The models were trained on four GPU \n(Nvidia 2080Ti) and saved checkpoint per epoch. The \nbest hyperparameters were decided based on the loss of \nthe validation set (See more details in Additional file  1: \nTable S2). We adopted the beam search procedure [65] to \ngenerate multiple candidates with different beam widths. \nAll generated candidates were canonicalized using RDkit \nand compared to the source molecules.\nResults and discussion\nIn this section, we mainly discussed our DeepHop per -\nformance from four parts. First, we evaluated our model \nwith different training paradigms on the whole dataset. \nThen, we compared our methods with the state-of-the-\nart deep learning models as well as conventional meth -\nods on four internal proteins. Subsequently, we tested \nour model in unseen protein sets and performed few-\nshot transfer learning on proteins with low performance. \nLastly, our DeepHop method was applied to several \ncase study examples to demonstrate the capability of the \nmodel for practical scaffold hopping.\nEvaluation of DeeopHop on the multi-kinase dataset\nWe first assessed the performance of methods on the \ninternal test set with different training paradigms, includ-\ning single-task, DeepHop-noGNN, DeepHop-noPro -\ntein, and DeepHop. The top 10 candidate sequences for \neach reference compound were generated. As shown \nin Table  1, by averaging on 40 targets, our multimodal \nDeepHops achieved the best overall performance with \na success rate of 65.2 ± 17.5 and constraint success rate \nof 43.7 ± 21.0. By comparison, the single-task method, \nwhich has separately trained and evaluated 40 models \nfor each target protein, achieved the worst performance \nin most of the metrics with a success rate = 27.5 ± 15.9, \nconstraint success rate = 15.5 ± 14.7. Specifically, the \naverage validity is only 12.9 ± 6.3, much lower than > 90% \nby all other three methods. These should be caused by \nthe relatively small number of data points for each sin -\ngle kinase task, leading to a fragile model that is diffi -\ncult to learn the transformation between scaffold pairs. \nWhen integrating all the pairs from different kinase sets \nfor DeepHop-noProtein, the model can capture key \nstructural information in molecular translation, achiev -\ning a success rate of 58.9% and a constraint success rate \nof 34.6. However, its average bioactivity improvement \nis 0.64, much lower than the 0.97 by DeepHop due to a \nlack of protein target information input to the model. On \nthe other hand, DeepHop-noGNN, an removal of the 3D \nGNN module from DeepHop, also decreases the success \nrate by 3.4%, demonstrating the effectiveness of the 3D \nconformer information of input molecules. The separate \nresults over targets are shown in Additional file  1: Figs. \nS1–S3.\nPerformance comparison with other methods\nWe further compared DeepHop with baseline meth -\nods. Since other baseline methods need to re-train the \nTable 1 Performance comparison of different training settings by the average and standard deviation on the internal test set of 40 \nprotein targets\nThe best performing numbers are in bold\nThe numbers in brackets are the standard deviation\nMetrics Models\nSingle-task DeepHop-noGNN DeepHop-noProtein DeepHop\nSuccess rate (%) 27.5(15.9) 61.8(18.6) 58.9(20.9) 65.2(17.5)\nConstraint success (%) 15.5(14.7) 40.6(20.9) 34.6(19.7) 43.7(21.0)\nImprovement 0.53(0.31) 0.92(0.27) 0.64(0.28) 0.97(0.24)\nValidity (%) 12.9(6.3) 94.4(2.7) 92.7(3.8) 95.7(3.8)\nUniqueness (%) 8.7(5.5) 74.6(11.1) 88.2(8.8) 76.4(9.3)\nNovelty (%) 99.0(0.9) 99.5(0.5) 99.6(0.3) 99.4(0.5)\nTable 2 Performance comparison of five methods on four \ninternal protein targets\nReported are average and standard deviation (numbers in brackets) over six \nmetrics\nMetrics Models\nLBVS MMPA Seq2seq G2G DeepHop\nSuccess Rate (%) 34.6(16.4) 33.9(13.2) 29.1(16.6) 33.4(6.6) 65.1(12.7)\nConstraint Success \n(%)\n10.4(7.2) 13.4(6.1) 15.9(8.5) 14.7(1.5) 33.5(8.8)\nImprovement − 0.94(0.61) 0.31(0.15) 0.77(0.47) 0.88(0.65) 0.81(0.32)\nValidity (%) – 97.3(0.04) 29.2(0.11) 99.7(0.03) 93.9(0.02)\nUniqueness (%) – 99.2(0.01) 17.8(0.03) 17.9(0.15) 70.7(0.05)\nNovelty (%) – – 99.1(0.03) 100(0.00) 99.5(0.01)\nPage 8 of 15Zheng et al. J Cheminform           (2021) 13:87 \nmodel or screen the whole database that is slow to run, \nwe randomly selected four protein targets for compari -\nson. As shown in Table  2, DeepHop achieved the best \naverage success rate (65.1%) and constraint success \nrate (33.5%), consistent with the previous benchmark \nover the whole dataset. The rates by DeepHop are both \naround two times higher than four baseline methods \nand are shown to consistently outperform other meth -\nods in four protein targets (Additional file 1 : Tables S4–\nS7). In contrast, four baseline methods achieved similar \nsuccess rates in the range of [29.1, 34.6] and constraint \nsuccess rates of [10.4, 15.9]. Among these, two deep \nlearning-based methods, Seq2seq and G2G, both suf -\nfer from a low uniqueness of 18%. In addition, Seq2seq \nhas a very low validity of 29.2% due to the internal dif -\nficulty of generating valid SMILE texts. These should be \ncaused by the limited number of training data on single \ntarget data because the half success rate is similar to the \nlevel achieved by DeepHop (Single-task) on the test set. \nDeepHop alleviates this issue by integrating 40 protein \ntarget data sets and thus achieve relatively stable results \nin different targets. On the other hand, in conventional \nmethods, LBVS achieved the lowest constraint success \nrate of 10.4%. This is because LBVS could ensure 100% \nvalidity and uniqueness through the database search, \nbut it is hard to find optimized hits due to the limit \nof available compounds with an average bioactivity \nimprovement of −  0.95. The MMPA improves the con -\nstraint success rate by increasing the average bioactiv -\nity improvement of generated compounds with a slight \nloss in validity and uniqueness.\nWe took CHEMBL267, which has a success rate close \nto the average of four targets, as a representative to ana -\nlyze the performance. We conclude the statistical per -\nformance of different methods for CHEMBL267 over \n(a) success rate, (b) 2D similarity to source compounds, \n(c) bioactivity improvement, (d) validity. To compare the \nphysicochemical properties of the compounds generated \nFig. 3 The statistical performance of different methods for CHEMBL267 over a success rate, b 2D similarity to source compounds, c bioactivity \nimprovement, d validity. We also show e t-SNE projection of physicochemical descriptors of the source compounds and compounds generated by \nDeephop and several baseline models\nPage 9 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \nby various models, we first computed the 200 com -\nmon physicochemical properties of each molecule by \nRDkit following Yang et  al. [66]. We then implemented \ndimensionality reduction to 2D with t-SNE. As a result, \nwe found that MMPA could generate property-aligned \nmolecules (Fig.  3e) that are highly similar in 2D to origi -\nnal ones (Fig.  3b) and cannot provide novel scaffolds. \nThis is unfavorable in scaffold hopping scenarios. LBVS, \nthough generating compounds with the lowest 2D simi -\nlarity on average, is hard to produce appropriate hops \ndue to the decreased bioactivity improvement relative to \nthe source compounds (averagely -0.89), as depicted in \nFig. 3c. Additionally, it’s limited to available compounds \nin the library without the ability to exploiting the whole \nchemical space, as shown in Fig.  3e. In deep learning \nmodels, Seq2seq has low valid rates (Fig.  3d) while G2G \ntends to generate outliers (Fig.  3e), resulting in an unsat -\nisfactory success rate. The relatively high performance of \nthe Seq2seq model indicates that the string-based mod -\nels have the potential to design good hops if the issue of \nlow success rate can be solved. Generally, only DeepHop \ncan efficiently generate high-quality scaffold hops by a \nbalanced performance in three measurements and pro -\nducing similar distributions of chemical property to the \nsource compounds, leading to significantly higher suc -\ncess rates and constraint success rates.\nFigure 4 shows two examples of the top-predicted mol -\necules generated by DeepHop. The modified groups lead \nFig. 4 Example of top-4 successful hops with two test molecules generated by DeepHop for CHEMBL267. The changes in the generated molecules \ncompared with starting molecule are highlighted in red\nPage 10 of 15Zheng et al. J Cheminform           (2021) 13:87 \nto significant changes in 2D while small changes in 3D. \nMore cases are shown in Additional file 1: Figs.S4, S5.\nModel performance on unseen targets\nWe have shown that DeepHop achieves good perfor -\nmance in the internal test set. However, in real-world \ncases, scaffold hopping is often required for target pro -\nteins that have only a few known active compounds, and \nthus it is unable to construct sufficient scaffold hopping \npairs for training. To mimic this scenario, we further \nexamined whether DeepHop can be generalized to exter -\nnal targets that have never been observed in the training \nset. Following the same sampling strategy as above, we \ngenerated ten molecules for each parent molecule on six \nunseen targets.\nAs shown in Table  3, the homogeneous targets per -\nformed very well in the external test set, even if all the \nmolecular structures and protein sequences in these tasks \nhave never been observed by the model. The results are \nexpected as the deep learning models are often capable \nof generalizing similar tasks. It also suggests that when \nthere are only a few known actives for a specific target \nprotein that has over 60% sequence identity similarity \nto the training target proteins, DeepHop can be alterna -\ntively applied to generate scaffold hops directly without \nthe need of re-training from scratch.\nAs expected, the model achieved low success rates on \nthree heterogeneous protein targets that are non-homol -\nogous to our training proteins (sequence ID < 25%). The \nlow rates were mostly caused by the drop in bioactivity \nimprovement. For the heterogeneous target proteins, we \nwonder how many scaffold pairs are required to achieve a \ndecent hopping. To this end, we equipped the model with \nthe scheme of transfer learning and tested how well it can \ndesign inhibitors for unfamiliar proteins. Specifically, the \ntrained DeepHop were fine-tuned with 5, 20, 50, 80% of \nscaffold hopping pairs from each unseen target protein, \nrespectively.\nAs shown in Fig.  5, with transfer learning, only 5% \n(around 40 ~ 200, see more details in Additional file  1: \nTable S8–S12) scaffold pairs can help unseen proteins to \nachieve fair success rates. At this point, the uniqueness \nof the generated molecules is poor because of the overfit -\nting of limited data points. Thereafter, with the increase \nof scaffold hopping pairs, the model can gradually \nachieve a decent level of success rates and uniqueness. \nNote that the improvements are stable after fine-tuning \n5% pairs, suggesting that the bioactivity feature is easy \nto capture compared to structural ones. These results \ndemonstrate that DeepHop can be further generalized to \nnon-homologs proteins with few-shot active compounds.\nScaffold hopping case study\nNext, we chose PIM-1 kinase (CHEMBL2147), a well-\nstudied target for antitumor drugs, as a representative to \nmimic a real-world scaffold hopping process. To search \nfor novel inhibitors of the PIM-1 kinase, Saluste and co-\nworkers once reported a typical fragment hopping by \nreplacing imidazopyridazine scaffold with triazolopyri -\ndine, which maintained the primary activity and signifi -\ncantly improved off-target selectivity as well as ADME \nproperty.\nWe started with one lead inhibitor (seed 1, \n IC50 = 0.024  nM) and two hit inhibitors (seed 2, \n IC50 = 155  nM; seed 3,  IC50 = 130  nM), and aimed to \ngenerate potential scaffold hopping candidates with the \nimproved pharmaceutical property. We used the trained \nmodel to generate 500 candidates for three seed com -\npounds, respectively. All the generated candidates were \nthen carried out with the docking process using Auto -\nDock Vina [67].\nAs shown in Table  4, DeepHops can generate a large \nnumber of novel hops for each molecule by simply \nincreasing the beam search width. The uniqueness val -\nues for seeds 1–3 are 77.4%, 52.8% and 66.4%, respec -\ntively. Among them, there are 51, 66, and 40 structurally \nsuccessful hops generated for seed 1,2, and 3, meeting \nthe requirements of (2D scaffold similarity ≤ 0.6)∩(3D \nsimilarity (X; Y) ≥ 0.6). In terms of bioactivity, we found \nthat 26.4%, 69.7%, and 60.8% of generated hops have a \nbetter docking score than the seed compounds, dem -\nonstrating the effectiveness of our model. It is worth \nnoting that even though seed 1 has extremely high activ -\nity  (IC50 = 0.024  nM), there are 11 molecules to have \nTable 3 The independent tests of three heterogeneous proteins without homologs and three homogeneous proteins with homologs \nto the proteins in the training set\nMetrics ChEMBL Target\nHomologs Non-homologs\nCHEMBL 4225 CHEMBL 2041 CHEMBL 2292 CHEMBL 2208 CHEMBL 4523 CHEMBL 2147\nSuccess Rate 0.765 0.630 0.705 0.024 0.055 0.129\nConstraint Success 0.471 0.519 0.341 0.024 0.009 0.036\nImprovement 0.515 1.259 0.824 − 0.378 − 1.210 − 1.263\nPage 11 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \nbetter-predicted activities and 102 molecules that have \nbetter docking scores, suggesting that DeepHop could \nbe a powerful tool in developing Me-too or Me-better \nmolecules.\nSeveral examples are shown in Fig.  6. All scaffold hops \nmeet the condition of the structure while obtaining simi -\nlar or improved activities compared to the starting seeds.\nDiscussions and conclusion\nIn this study, we have proposed a novel multimodal deep \ngenerative model, DeepHop, for scaffold hopping, which \nis a critical task in rational drug design. The model can \ngenerate large sets of potential hops with novel back -\nbones and improved bioactivities. This can be used in not \nonly early drug discovery phases like hit-to-lead or lead \noptimization but also patent busting for Me-Too and Me-\nBetter molecules. Furthermore, we demonstrated that \nthe model could generalize to new target proteins if fine-\ntuning with a small set of active compounds. This enables \nFig. 5 Transfer learning with different ratios of scaffold hopping pairs on the heterogeneous unseen protein targets\nTable 4 Scaffold hopping case study on PIM-1 kinase with three \nseed compounds\nMetrics Scaffold Hopping\nSeed 1 Seed 2 Seed 3\nUnique structures 387 264 332\nStructurally successful hops 51 66 40\nPredicted activity < Lead 11 138 167\nDocking score < Lead 102 184 202\nPage 12 of 15Zheng et al. J Cheminform           (2021) 13:87 \nthe generation of scaffold hops in low source scenarios. \nThrough several case examples, we have shown that our \nmethod can be applied to practical scaffold hopping \ntasks, where most of the generated molecules have better \ndocking scores than the original seeds while maintaining \n3D similar but 2D dissimilar structure.\nWe see three main advantages of our works. First, it \nprovides an entirely data-driven scaffold hopping strat -\negy. The Transformer model implicitly learns the chem -\nical hopping rules and performs candidate ranking via \nthe beam search decoding procedure, without any pre-\ndefinition of screening databases and hand-encoded \nFig. 6 Overlay of the seed inhibitors (sliver) and top-predicted hops (colors). The 2D structures are shown below, and the structural similarity (2D \nScaffold Tanimoto Similarity and 3D Shape and Color Similarity) and docking scores (kcal/mol) are attached in the upper left. Protein structure is \nretrieved from 5KZI [68]\nPage 13 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \nrules. Second, the DeepHop is easy to train and to use, \nand can be adapted to different datasets without any \nmodifications to the model architecture. Furthermore, \nthe DeepHop scales better to larger training data sets, \ndifferent from rule-based expert systems that need to \ndefine rules in the knowledge base manually and are \nhard to process large training datasets.\nThere are also several weak points in our model. One \nmajor problem is diversity. The diversity of the target \nchemical space is constrained by the limited high bio -\nactive compounds. It can be alleviated by active learn -\ning and iterative learning. Secondly, the evaluation of \nscaffold hopping should also be re-considered. The \ndefinition of what constitutes a scaffold hop is highly \nsubjective and often differs, and currently, there is no \naccepted metric available for the evaluation of the scaf -\nfold hopping potential. Li et al. [69] once introduced a \nmathematical function to quantify the \"chemical dis -\ntance\" between scaffolds, which seems to be a rigid \nmetric. To further advance research activities directed \nat scaffold hopping and to make the performance of dif -\nferent methods comparable, there is a need to establish \ngenerally applicable scaffold definitions and retrieval \nmetrics.\nWe also noted the model couldn’t fully utilize protein \ninformation, especially protein 3D structural informa -\ntion. Although the inclusion of protein sequence ena -\nbles successful hopping for homologous proteins, it can’t \nrecognize the complex protein-drug interactions. Fortu -\nnately, we have shown this can be solved by transferred \nlearning over dozens of known active compounds. In the \nfuture, we may include a pre-trained protein-drug inter -\naction network for a more accurate prediction of protein-\ndrug interactions.\nIn addition, the definition of scaffold hopping in our \nwork can be treated as a conditioned topological trans -\nformation, which is also defined as 4°hopping [9 ]. An \nobvious direction for further exploration is to classify \nthe types of scaffold hopping by analyzing the molecular \ntransformation paradigm. The hopping mode, like hetero-\ncycle replacement, ring-opening, and ring closure should \nbecome a controlled condition that guide the scaffold hop-\nping mode. Another interesting extension to the DeepHop \nmodels would be to use multi-objective reinforcement \nlearning to allow our generated hops to match the compre-\nhensive expectation (e.g., scaffold replacement, ADMET, \nsynthesizability) of medicinal chemists [35].\nIn summary, DeepHop provides a novel method that \ncan perform target-based scaffold hopping and can gen -\neralize to new target proteins through fine-tuning with a \nsmall set of active compounds.We believe that the strate -\ngies described in our work will inpire future hit optimiza-\ntion works.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13321- 021- 00565-5.\nAdditional file 1: Table S1. 3D GNN’s initial atom features calculated \nusing RDKit. Table S2. Key hyperparameters, parameters for the best \nmodel in bold. Table S3. Details of selected targets in internal datasets. \nTable S4. Performance comparison of models on the Tyrosine-protein \nkinase JAK1 (CHEMBL2835). Table S5. Performance comparison of \nmodels on the Tyrosine-protein kinase SRC (CHEMBL267). Table S6. \nPerformance comparison of models on the PI3-kinase p110-gamma \nsubunit (CHEMBL3267). Table S7. Performance comparison of models \non the Ribosomal protein S6 kinase 1 (CHEMBL4501). Table S8. Details \nof selected targets in external datasets. Training Pairs Amount is the total \namount of scaffold hopping pairs in the training sets per target.  R2 and \nRMSE are the performance of their virtual profiling models. Table S9. The \nindependent tests by DeepHop-noGNN. Table S10. The independent \ntests by DeepHop-noProtein. Figure S1. The RMSE results of four models \nover 40 target proteins. Figure S2. The  R2 results of four models over 40 \ntarget proteins. Figure S3. Performance of DeepHop model on the scaf-\nfold hopping datasets over 40 kinases protein compared to the DeepHop-\nnoGraph and DeepHop-noProtein. Table S11. Key hyperparameters for \nthe seq2seq model. Table S12. Key hyperparameters for the G2G model. \nFigure S4. Example of top-4 successful hops ((2D scaffold similarity ≤ \n0.6)∩(3D similarity (X; Y) ≥ 0.6)∩Activity Improvement ≥ 1 ) with three test \nmolecules generated by DeepHop for CHEMBL267. Figure S5. Example of \ntop-4 successful hops ((2D scaffold similarity ≤ 0.6)∩(3D similarity (X; Y) ≥ \n0.6)∩Activity Improvement ≥ 1 ) with three test molecules generated by \nDeepHop for CHEMBL267.\nAcknowledgements\nNone.\nAuthors’ contributions\nSZ, ZL, and HA contributed concept and implementation. SZ, HC and YY wrote \nthe manuscript. All authors contributed to the interpretation of results. All \nauthors read and approved the final manuscript.\nFunding\nThis study has been supported by the National Natural Science Foundation \nof China (61772566, 62041209, and U1611261), National Science Founda-\ntion of Guangdong, China (2019A1515012207), Guangdong Key Field R&D \nPlan (2019B020228001 and 2018B010109006), Introducing Innovative and \nEntrepreneurial Teams (2016ZT06D211), Guangzhou S&T Research Plan \n(202002020047, 202007030010), the Guangdong Provincial Key Lab. of New \nDrug Design and Evaluation (Grant 2011A060901014).\nAvailability of data and materials\nDemo, instructions, and codes for DeepHop are available at https:// github. \ncom/ prokia/ deepH ops.\nDeclarations\nCompeting interests\nZ.L., H.A. and D.D. are employees of Fermion Technology Co., Ltd. S.Z. currently \nworks directly or indirectly for Galixir.\nAuthor details\n1 School of Data and Computer Science, Sun Yat-Sen University, China, 132 East \nCircle at University City, Guangzhou 510006, China. 2 Fermion Technology Co., \nLtd, 1088 Newport East Road, Guangzhou 510335, China. 3 Centre of Chemistry \nand Chemical Biology, Guangzhou Regenerative Medicine and Health Guang-\ndong Laboratory, Guangzhou 510530, China. \nReceived: 7 June 2021   Accepted: 31 October 2021\n\nPage 14 of 15Zheng et al. J Cheminform           (2021) 13:87 \nReferences\n 1. Ecker DJ, Crooke ST (1995) Combinatorial drug discovery: which methods \nwill produce the greatest value? Biotechnology (N Y) 13(4):351–360\n 2. Fattori D, Squarcia A, Bartoli S (2008) Fragment-based approach to drug \nlead discovery: overview and advances in various techniques. Drugs R D \n9(4):217–227\n 3. Schneider G, Neidhart W, Giller T, Schmid G (1999) “Scaffold-Hopping” by \ntopological pharmacophore search: a contribution to virtual screening. \nAngew Chem Int Ed Engl 38(19):2894–2896\n 4. Hu Y, Stumpfe D, Bajorath J (2017) Recent Advances in Scaffold Hopping. \nJ Med Chem 60(4):1238–1246\n 5. Zheng S, Li Y, Chen S, Xu J, Yang Y (2020) Predicting drug–protein \ninteraction using quasi-visual question answering system. Na Mach Intell \n2(2):134–140\n 6. Rush TS, Grant JA, Mosyak L, Nicholls A (2005) A shape-based 3-D scaf-\nfold hopping method and its application to a bacterial protein-protein \ninteraction. J Med Chem 48(5):1489–1495\n 7. Stewart KD, Shiroda M, James CA (2006) Drug Guru: a computer software \nprogram for drug design using medicinal chemistry rules. Bioorg Med \nChem 14(20):7011–7022\n 8. Hu Y, Stumpfe D, Bajorath J (2016) Computational exploration of molecu-\nlar scaffolds in medicinal chemistry. J Med Chem 59(9):4062–4076\n 9. Sun H, Tawa G, Wallqvist A (2012) Classification of scaffold-hopping \napproaches. Drug Discov Today 17(7–8):310–324\n 10. Nakano H, Miyao T, Funatsu K (2020) Exploring topological pharmacoph-\nore graphs for scaffold hopping. J Chem Inf Model 60(4):2073–2081\n 11. Laufkotter O, Sturm N, Bajorath J, Chen H, Engkvist O (2019) Combining \nstructural and bioactivity-based fingerprints improves prediction perfor-\nmance and scaffold hopping capability. J Cheminform 11(1):54\n 12. Renner S, Schneider G (2006) Scaffold-hopping potential of ligand-based \nsimilarity concepts. ChemMedChem 1(2):181–185\n 13. Grisoni F, Merk D, Byrne R, Schneider G (2018) Scaffold-hopping from \nsynthetic drugs by holistic molecular representation. Sci Rep 8(1):16469\n 14. Reutlinger M, Koch CP , Reker D, Todoroff N, Schneider P , Rodrigues T, \nSchneider G (2013) Chemically Advanced Template Search (CATS) for \nscaffold-hopping and prospective target prediction for “orphan” mol-\necules. Mol Inform 32(2):133–138\n 15. Floresta G, Amata E, Dichiara M, Marrazzo A, Salerno L, Romeo G, Prez-\nzavento O, Pittala V, Rescifina A (2018) Identification of potentially potent \nheme oxygenase 1 inhibitors through 3D-QSAR coupled to scaffold-\nhopping analysis. ChemMedChem 13(13):1336–1342\n 16. Saluste G, Albarran MI, Alvarez RM, Rabal O, Ortega MA, Blanco C, Kurz G, \nSalgado A, Pevarello P , Bischoff JR, Pastor J, Oyarzabal J (2012) Fragment-\nhopping-based discovery of a novel chemical series of proto-oncogene \nPIM-1 kinase inhibitors. PLoS ONE 7(10):e45964\n 17. Stahura FL, Xue L, Godden JW, Bajorath J (1999) Molecular scaffold-based \ndesign and comparison of combinatorial libraries focused on the ATP-\nbinding site of protein kinases. J Mol Graph Model 17(1):1–9\n 18. Vainio MJ, Kogej T, Raubacher F, Sadowski J (2013) Scaffold hopping by \nfragment replacement. J Chem Inf Model 53(7):1825–1835\n 19. Rabal O, Amr FI, Oyarzabal J (2015) Novel Scaffold FingerPrint (SFP): \napplications in scaffold hopping and scaffold-based selection of diverse \ncompounds. J Chem Inf Model 55(1):1–18\n 20. Pitt WR, Parry DM, Perry BG, Groom CR (2009) Heteroaromatic rings of the \nfuture. J Med Chem 52(9):2952–2963\n 21. Stojanovic L, Popovic M, Tijanic N, Rakocevic G, Kalinic M (2020) Improved \nScaffold Hopping in Ligand-Based Virtual Screening Using Neural Repre-\nsentation Learning. J Chem Inf Model 60(10):4629–4639\n 22. Ruddigkeit L, van Deursen R, Blum LC, Reymond JL (2012) Enumeration \nof 166 billion organic small molecules in the chemical universe database \nGDB-17. J Chem Inf Model 52(11):2864–2875\n 23. Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T (2018) The rise of \ndeep learning in drug discovery. Drug Discov Today 23(6):1241–1250\n 24. Xu Y, Lin K, Wang S, Wang L, Cai C, Song C, Lai L, Pei J (2019) Deep learn-\ning for molecular generation. Future Med Chem 11(6):567–597\n 25. Mikolov, T.; Karafiát, M.; Burget, L.; Černocký, J.; Khudanpur, S.  (2010) \nRecurrent neural network based language model. Interspeech \n2(3):1045-1048\n 26. Segler MHS, Kogej T, Tyrchan C, Waller MP (2018) Generating Focused \nMolecule Libraries for Drug Discovery with Recurrent Neural Networks. \nACS Cent Sci 4(1):120–131\n 27. Zheng S, Yan X, Gu Q, Yang Y, Du Y, Lu Y, Xu J (2019) QBMG: quasi-biogenic \nmolecule generator with deep recurrent neural network. J Cheminform \n11(1):5\n 28. Gomez-Bombarelli R, Wei JN, Duvenaud D, Hernandez-Lobato JM, \nSanchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, \nAdams RP , Aspuru-Guzik A (2018) Automatic chemical design using \na data-driven continuous representation of molecules. ACS Cent Sci \n4(2):268–276\n 29. Skalic M, Jimenez J, Sabbadin D, De Fabritiis G (2019) Shape-based \ngenerative modeling for de novo drug design. J Chem Inf Model \n59(3):1205–1214\n 30. De Cao, N.; Kipf, T., MolGAN: An implicit generative model for small \nmolecular graphs. arXiv preprint arXiv:1805.11973 2018.\n 31. Weininger D (1988) SMILES, a chemical language and information \nsystem. 1. Introduction to methodology and encoding rules. J Chem \nInform Comput Sci 28(1):31–36\n 32. Li Y, Zhang L, Liu Z (2018) Multi-objective de novo drug design with \nconditional graph generative model. J Cheminform 10(1):33\n 33. Jeon W, Kim D (2020) Autonomous molecule generation using rein-\nforcement learning and docking to develop potential novel inhibitors. \nSci Rep 10(1):22104\n 34. Thomas M, Smith RT, O’Boyle NM, de Graaf C, Bender A (2021) Com-\nparison of structure- and ligand-based scoring functions for deep \ngenerative models: a GPCR case study. J Cheminform 13(1):39\n 35. Stahl N, Falkman G, Karlsson A, Mathiason G, Bostrom J (2019) Deep \nreinforcement learning for multiparameter optimization in de novo \ndrug design. J Chem Inf Model 59(7):3166–3176\n 36. Lim J, Hwang S-Y, Moon S, Kim S, Kim WY (2020) Scaffold-based molec-\nular design with a graph generative model. Chem Sci 11(4):1153–1164\n 37. Li Y, Hu J, Wang Y, Zhou J, Zhang L, Liu Z (2020) DeepScaffold: a com-\nprehensive tool for scaffold-based de novo drug discovery using deep \nlearning. J Chem Inf Model 60(1):77–91\n 38. Arús-Pous J, Patronov A, Bjerrum EJ, Tyrchan C, Reymond J-L, Chen H, \nEngkvist O (2020) SMILES-based deep generative scaffold decorator for \nde-novo drug design. J Cheminformatics 12(1):1–18\n 39. Imrie F, Bradley AR, van der Schaar M, Deane CM (2020) Deep genera-\ntive models for 3D linker design. J Chem Inf Model 60(4):1983–1995\n 40. Yang Y, Zheng S, Su S, Zhao C, Xu J, Chen H (2020) SyntaLinker: \nautomatic fragment linking with deep conditional transformer neural \nnetworks. Chem Sci 11(31):8312–8322\n 41. Gaulton A, Bellis LJ, Bento AP , Chambers J, Davies M, Hersey A, Light \nY, McGlinchey S, Michalovich D, Al-Lazikani B, Overington JP (2012) \nChEMBL: a large-scale bioactivity database for drug discovery. Nucleic \nAcids Res 40:D1100–D1107\n 42. Southall NT (2006) Ajay, Kinase patent space visualization using chemi-\ncal replacements. J Med Chem 49(6):2103–2109\n 43. Song, Y.; Zheng, S.; Niu, Z.; Fu, Z.-H.; Lu, Y.; Yang, Y. (2020) Communica-\ntive representation learning on attributed molecular graphs. IJCAI \n2020:2831-2838\n 44. Li X, Li Z, Wu X, Xiong Z, Yang T, Fu Z, Liu X, Tan X, Zhong F, Wan X, \nWang D, Ding X, Yang R, Hou H, Li C, Liu H, Chen K, Jiang H, Zheng \nM (2020) Deep learning enhancing kinome-wide polypharmacology \nprofiling: model construction and experiment validation. J Med Chem \n63(16):8723–8737\n 45. Hussain J, Rea C (2010) Computationally efficient algorithm to identify \nmatched molecular pairs (MMPs) in large data sets. J Chem Inf Model \n50(3):339–348\n 46. Rogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem \nInf Model 50(5):742–754\n 47. Bemis GW, Murcko MA (1996) The properties of known drugs. 1. \nMolecular frameworks. J Med Chem 39(15):2887–2893\n 48. Landrum GA, Penzotti JE, Putta S (2006) Feature-map vectors: a new \nclass of informative descriptors for computational drug discovery. J \nComput Aided Mol Des 20(12):751–762\n 49. Putta S, Landrum GA, Penzotti JE (2005) Conformation mining: an \nalgorithm for finding biologically relevant conformations. J Med Chem \n48(9):3313–3318\n 50. Li W, Godzik A (2006) Cd-hit: a fast program for clustering and compar -\ning large sets of protein or nucleotide sequences. Bioinformatics \n22(13):1658–1659\nPage 15 of 15\nZheng et al. J Cheminform           (2021) 13:87 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 51. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; \nKaiser, Ł.; Polosukhin, I. In: Attention is all you need, Advances in neural \ninformation processing systems, 2017; pp 5998–6008.\n 52. Wang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; Chao, L. S., Learn-\ning deep transformer models for machine translation. arXiv preprint \narXiv:1906.01787 2019.\n 53. Zheng S, Rao J, Zhang Z, Xu J, Yang Y (2020) Predicting retrosynthetic \nreactions using self-corrected transformer neural networks. J Chem Inf \nModel 60(1):47–55\n 54. Danel, T.; Spurek, P .; Tabor, J.; Śmieja, M.; Struski, Ł.; Słowik, A.; Maziarka, Ł., \nSpatial Graph Convolutional Networks. arXiv preprint arXiv:1909.05310 \n2019.\n 55. Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y., Empirical evaluation of gated \nrecurrent neural networks on sequence modeling. arXiv preprint \narXiv:1412.3555 2014.\n 56. Rao, R.; Bhattacharya, N.; Thomas, N.; Duan, Y.; Chen, P .; Canny, J.; Abbeel, \nP .; Song, Y. In Evaluating protein transfer learning with TAPE, Advances in \nNeural Information Processing Systems, 2019; pp 9689–9701.\n 57. Nair, Vinod, and Geoffrey E. Hinton. \"Rectified linear units improve \nrestricted boltzmann machines.\" Icml. 2010.\n 58. Ba, J.; Kiros, J. R.; Hinton, G. E., Layer Normalization. arXiv: 1607. 06450.\n 59. Barrault, L.; Bojar, O. e.; Costa-jussà, M. R.; Federmann, C.; Fishel, M.; \nGraham, Y.; Haddow, B.; Huck, M.; Koehn, P .; Malmasi, S.; Monz, C.; Müller, \nM.; Pal, S.; Post, M.; Zampieri, M. In: Findings of the 2019 Conference on \nMachine Translation (WMT19), Proceedings of the Fourth Conference on \nMachine Translation (Volume 2: Shared Task Papers, Day 1), Florence, Italy, \naug; Association for Computational Linguistics: Florence, Italy, 2019; pp \n1–61.\n 60. He, K.; Zhang, X.; Ren, S.; Sun, J., Deep Residual Learning for Image Recog-\nnition. CoRR 2015, abs/1512.03385.\n 61. Liu B, Ramsundar B, Kawthekar P , Shi J, Gomes J, Luu Nguyen Q, \nHo S, Sloane J, Wender P , Pande V (2017) Retrosynthetic reaction \nprediction using neural sequence-to-sequence models. ACS Cent Sci \n3(10):1103–1113\n 62. Jin, W.; Yang, K.; Barzilay, R.; Jaakkola, T., Learning Multimodal Graph-to-\nGraph Translation for Molecule Optimization. International Conference \non Learning Representations. 2018.\n 63. Klein, G.; Kim, Y.; Deng, Y.; Senellart, J.; Rush, A. M., OpenNMT: Open-source \ntoolkit for neural machine translation. CoRR 2017, abs/1701.02810.\n 64. Python Core Team. Python: A dynamic, open source programming \nlanguage. Python Software Foundation. https:// www. python. org/. \n 65. Ow PS, Morton TE (1988) Filtered beam search in scheduling†. Int J Prod \nRes 26(1):35–62\n 66. Yang K, Swanson K, Jin W, Coley C, Eiden P , Gao H, Guzman-Perez A, Hop-\nper T, Kelley B, Mathea M (2019) Analyzing learned molecular representa-\ntions for property prediction. J Chem Inf Model 59(8):3370–3388\n 67. Trott O, Olson AJ (2010) AutoDock Vina: improving the speed and accu-\nracy of docking with a new scoring function, efficient optimization, and \nmultithreading. J Comput Chem 31(2):455–461\n 68. Wurz RP , Sastri C, D’Amico DC, Herberich B, Jackson CLM, Pettus LH, Tasker \nAS, Wu B, Guerrero N, Lipford JR, Winston JT, Yang Y, Wang P , Nguyen Y, \nAndrews KL, Huang X, Lee MR, Mohr C, Zhang JD, Reid DL, Xu Y, Zhou \nY, Wang HL (2016) Discovery of imidazopyridazines as potent Pim-1/2 \nkinase inhibitors. Bioorg Med Chem Lett 26(22):5580–5590\n 69. Li R, Stumpfe D, Vogt M, Geppert H, Bajorath J (2011) Development \nof a method to consistently quantify the structural distance between \nscaffolds and to assess scaffold hopping potential. J Chem Inf Model \n51(10):2507–2514\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}