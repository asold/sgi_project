{
  "title": "Let’s Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models",
  "url": "https://openalex.org/W4389518706",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2552103131",
      "name": "Ruida Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2952165275",
      "name": "Wangchunshu Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3201915713",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2735704353",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281790610",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W4228998172",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3105662186",
    "https://openalex.org/W4297399052",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2951751045",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2070493638",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4385571831",
    "https://openalex.org/W4318621294",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W3170224286",
    "https://openalex.org/W4385570051",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3101007570"
  ],
  "abstract": "Data Synthesis is a promising way to train a small model with very little labeled data. One approach for data synthesis is to leverage the rich knowledge from large language models to synthesize pseudo training examples for small models, making it possible to achieve both data and compute efficiency at the same time. However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the real task data distribution. Thus, in this paper, we propose Synthesis Step by Step (S3), a data synthesis framework that shrinks this distribution gap by iteratively extrapolating the errors made by a small model trained on the synthesized dataset on a small real-world validation dataset using a large language model. Extensive experiments on multiple NLP tasks show that our approach improves the performance of a small model by reducing the gap between the synthetic dataset and the real data, resulting in significant improvement compared to several baselines: 9.48% improvement compared to ZeroGen, 2.73% compared to GoldGen, and 15.17% improvement compared to the small model trained on human-annotated data. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11817–11831\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLet’s Synthesize Step by Step: Iterative Dataset Synthesis with Large\nLanguage Models by Extrapolating Errors from Small Models\nRuida Wang∗H Wangchunshu Zhou A Mrinmaya Sachan E\nH HKUST A AIWaves Inc. E ETH Zürich\nrwangbr@connect.ust.hk chunshu@aiwaves.cn msachan@ethz.ch\nAbstract\nData Synthesis is a promising way to train a\nsmall model with very little labeled data. One\napproach for data synthesis is to leverage the\nrich knowledge from large language models to\nsynthesize pseudo training examples for small\nmodels, making it possible to achieve both data\nand compute efficiency at the same time. How-\never, a key challenge in data synthesis is that\nthe synthesized dataset often suffers from a\nlarge distributional discrepancy from the real\ntask data distribution. Thus, in this paper, we\npropose Synthesis Step by Step (S3), a data syn-\nthesis framework that shrinks this distribution\ngap by iteratively extrapolating the errors made\nby a small model trained on the synthesized\ndataset on a small real-world validation dataset\nusing a large language model. Extensive ex-\nperiments on multiple NLP tasks show that our\napproach improves the performance of a small\nmodel by reducing the gap between the syn-\nthetic dataset and the real data, resulting in\nsignificant improvement compared to several\nbaselines: 9.48% improvement compared to\nZeroGen, 2.73% compared to GoldGen, and\n15.17% improvement compared to the small\nmodel trained on human-annotated data.1\n1 Introduction\nLarge Language Models (LLMs) (Brown et al.,\n2020; Chowdhery et al., 2022; Touvron et al., 2023;\nOpenAI, 2023) have shown promising zero-shot\nperformance on a wide range of tasks, demonstrat-\ning their potential of serving as generalist models.\nHowever, LLMs suffer from efficiency issues due\nto large model sizes and high inference latency,\nmaking them hard to deploy in real-world appli-\ncations. Therefore, small models trained on task-\nspecific data are still favored in many resource-\nconstrained scenarios because they have much\n∗ Work done while at exchange at ETH Zürich\n1The code and generated data can be found at\nhttps://github.com/RickySkywalker/Synthesis_Step-by-\nStep_Official\nfewer parameters, are easy to deploy, and perform\nwell in specific downstream tasks (Xu et al., 2021).\nFigure 1: Training and testing accuracy of DistilBert\nwith ZeroGen (Ye et al., 2022b) on the IMDb dataset\nwith 200k training datapoints. Also shown are the train-\ning and testing accuracy of the model trained on Gold-\nData. We can see here that ZeroGen’s training accuracy\nquickly reaches nearly 100%, but testing accuracy re-\nmains low.\nHowever, fitting a small model for a specific\ntask may require large amounts of human-labeled\ndata, which is not available in many downstream\ntasks and is expensive to annotate. This data ineffi-\nciency problem makes it challenging to fine-tune\na small model. Therefore, a number of distinct re-\nsearch approaches attempt to reduce the amount of\ndata required for fine-tuning small models on spe-\ncific tasks, including knowledge distillation (Hin-\nton et al., 2015; Beyer et al., 2022; Hsieh et al.,\n2023; Xu et al., 2020; Zhou et al., 2020; Shrid-\nhar et al., 2023), data augmentation (DeVries and\nTaylor, 2017; Shorten and Khoshgoftaar, 2019; Li\net al., 2022), module replacing (Xu et al., 2020;\nZhou et al., 2023), semi-supervised learning (Chen\net al., 2020; Wang et al., 2021; Smith et al., 2022),\nand data synthesis (Anaby-Tavor et al., 2020; Puri\net al., 2020).\nIn this work, we focus on data synthesis, which\ngenerates data and corresponding labels from\nscratch. Unlike semi-supervised learning, which\n11817\nFigure 2: Both (a) traditional zero-shot dataset synthesis methods and (b) training small models directly on gold\ndata do not leverage feedback from the small model trained on the synthesized dataset. In contrast, (c) our approach,\nS3, first synthesizes a seed dataset in a zero-shot fashion with rationales (left-hand side). Then, we iteratively reduce\nthe gap between the synthesized data distribution and the gold data distribution by extrapolating the errors of a small\nmodel trained on the currently synthesized data on a small gold validation set. The additional synthesized data can,\ntherefore, be considered to be sampled from the difference between the currently synthesized data distribution and\ngold data distribution. By mixing it with the currently synthesized data, we can recover the gold data distribution\nand therefore improve the performance of a small model trained on the data mixture.\nrelies on unlabeled data, this approach is simpler\nand more efficient, especially when unlabeled data\nis scarce. Most existing methods in data synthe-\nsis for NLP utilize LLMs to generate an unlimited\namount of training data for training a small model.\nExisting dataset synthesis methods typically re-\nquire a massive amount of synthesized data to\nachieve relatively good performance with a small\nmodel, like in ZeroGen (Ye et al., 2022b), which\nsometimes needs as much as 1M records of synthe-\nsized data. However, this often results in additional\ndata synthesis cost and computation costs when\ntraining the small task-specific model.\nIntuitively, the quality of the synthesized data, or\nthe extent to which the synthesized data resembles\nthe gold task data, is crucial for the small model’s\nperformance. However, due to the complexity of\nspecific tasks in the real world, the synthesized data\noften suffers from a distribution gap from the real-\nworld data distribution. This can be clearly seen\nin Fig.1. The small model’s training accuracy on\nsynthesized data is close to 100% but the testing\naccuracy on real-world data is still low. In contrast,\nthe gap between training and testing accuracy is\nmuch smaller when trained on human-annotated\ndata.\nTo reduce the distribution gap and improve data\nefficiency in dataset synthesis, we propose Syn-\nthesis Step by Step (S3), a novel dataset synthesis\nframework that reduces the distribution gap in a\ndata-efficient way by dynamically optimizing the\nsynthesized dataset. As illustrated in Fig. 2, S3 first\nsynthesizes a seed dataset with an explain-then-\ngenerate method that first prompts LLMs to gen-\nerate rationales for each label and then combines\nthe generated rationale and task-specific prompts\nto generate data points. S3 then refines the seed\ndataset by iteratively synthesizing more data by\nextrapolating the errors of a model trained on the\nseed dataset made on a small validation set, which\nwe assume is sampled from the real task data dis-\ntribution.\nWe summarize our contribution as follows: (1)\nWe propose a novel point of view for dynamic\ndataset synthesis, which allows for the creation of\ntraining data for smaller models and can be opti-\nmized by adding more data; based on this point of\nview, we propose the S3 framework that can syn-\nthesize and optimize a pseudo dataset using LLM\nthat can efficiently shrink the distribution gap in\ndataset synthesis. (2) We perform a theoretical\nanalysis for the effectiveness of S3 on reducing\nthe distribution gap. (3) We perform extensive ex-\nperiments on three major NLP tasks and obtain\n11818\nan average 9.48% improvement compared to Ze-\nroGen (Ye et al., 2022b), a representative baseline\nfor dataset synthesis, using only 30.43% of data on\naverage.\n2 Methodology\nWe describe the proposedS3 framework in detail in\nthis section. The key idea ofS3 is to first synthesize\na seed dataset by prompting LLMs and then to iter-\natively reduce the distribution gap by extrapolating\nerrors the small model makes on a small validation\nset from the gold data distribution. S3 comprises\nthe following steps:\n1. Seed data generation: We utilize an LLM\nto analyze the task we are working on, then\nsynthesize a list of possible rationales for such\na task. If the task is hard to analyze, we can\nskip this step. Then, we combine the synthe-\nsized rationales, possible context sentences,\nand labels in one prompt to guide the LLM to\nsynthesize the dataset.\n2. Small model training: Train the small model\nwith the synthesized dataset, then validate the\nsmall model on real-world validation data, and\nattain misclassified data of the small model,\nuse them as errors.\n3. Error extrapolation: Use the LLM to extrap-\nolate the errors of the small model and synthe-\nsize additional data using the information in\nerrors.\n4. Combine and Repeat: Combine the addi-\ntional dataset and original dataset as a new\nsynthesized train dataset for the small model,\nthen repeat steps 2 and 3 for multiple rounds\nuntil the performance of the small model con-\nverges.\nWe first introduce some background and key\nnotations in Section 2.1. We then describe the al-\ngorithms for seed data synthesis and iterative error\nextrapolation-based synthesis in Section 2.2 (point\n1. above) and Section 2.3 (points 2, 3, 4 above),\nrespectively. Finally, we give a theoretical interpre-\ntation of the proposed method in Section 2.6.\n2.1 Background\nFollowing Sharp et al. (2017), we denote the dis-\ntribution of human language for the LLM under\nprompt input T as PLLM(·|T). The small model\nis a computationally efficient model that will be\ntrained on our synthesized dataset. In general, the\nsmall model contains much fewer parameters and\nis easy to train and deploy in real-world applica-\ntions. We denote a small model trained by dataset\nDtrain as f(·|Dtrain).\n2.2 Seed Data Synthesis with Rationales\nSeed Data is defined as the basic zero-shot synthe-\nsized dataset for our S3 framework.\nAlgorithm 1: Seed data synthesis with ra-\ntionales\nInput: Y,Tration,T(1)\nquery,PLLM,K,k,N seed\nOutput: Dseed\n1 for each yi ∈Y do\n2 ri ←topK(PLLM(·|Tration(yi))\n3 Dseed ←∅\n4 for i in range(Nseed) do\n5 ycurr ∼U1(Y)\n6 rcurr ∼Uk(ri)\n7 xcurr ∼PLLM(·|T(1)\nquery(rcurr,ycurr))\n8 Dseed ←Dseed ∪{(xcurr,ycurr}\nWe present the algorithm for seed data synthe-\nsis with rationales in Alg. 1. Here, Ydenotes the\nset of all possible labels in the task we are work-\ning on; Tration(y) denotes label and task descrip-\ntive prompt for rationales synthesis; T(1)\nquery(r,y)\nis the data synthesis prompt that wraps the ratio-\nnales in r and the label ytogether to query LLM\nfor a data point; topKmeans top-K sampling from\nthe LLM outputs to obtain the rationale list for\na specific label; Ui(S) means uniformly sample i\nnon-repeating elements in set S. The resulting seed\ndataset is denoted as Dseed = {Xseed,Yseed}.\nFor instance, for the IMDb (Maas et al.,\n2011) dataset, a sentiment analysis dataset on\nmovie reviews, Tration(yi = positive/negative)\nis: \" What is the reason that may lead to\na positive/negative movie review .\" and the\nTquery(rcurr,positive) is: \"Now imagine that you\njust watched a movie that has great acting, intrigu-\ning plot, and beautiful cinematography. Now you\nshould write a positive review about this movie.\"\nWe use the prompt as an input to the LLM and\nobtain the target output as the synthesized pseudo\nexample. This “explain-then-generate” approach\nenables us to generate more diverse, informative,\nand realistic examples.\n11819\n2.3 Dataset Refinement with Error\nExtrapolation\nWe then describe the Error Extrapolation-based\nSynthesis (EES) framework that attempts to itera-\ntively reduce the distribution gap by extrapolating\nthe errors of a small model trained on the currently\nsynthesized dataset on a small validation set. This\nis different from conventional data synthesis meth-\nods, where the synthesized dataset is fixed after\nfinishing the synthesis process and is used for train-\ning the small model. Specifically, the EES process\nextrapolates errors made by small models on the\nreal-world validation datasets to synthesize some\nadditional data to fix the error.\nWe use two different data sources in the EES pro-\ncess: the seed dataset (Dseed), and a small human-\nlabeled, real-world dataset referred to as gold data,\ndenoted as Dgold. In EES, we first divide the gold\ndata into a validation dataset D(val)\ngold and a testing\ndataset D(test)\ngold . We use D(val)\ngold to find and fix the\ndistribution gap and use D(test)\ngold to judge the perfor-\nmance of the small model.\nAlgorithm 2: Algorithm for Error Extrapo-\nlation\nInput: Dseed,D(eval)\ngold ,D(test)\ngold ,f, PLLM,R, T(1)\nmis\nOutput: Dtrain\n1 D(0)\nadd ←∅\n2 for qin range(R) do\n3 init(f); // reinitialize f (clear\nlast round’s train)\n4 D(q)\ntrain ←Dseed ∪(∪q\ni=1D(i)\nadd)\n5 train(f,D(q)\ntrain)\n6 D(q)\nmis ←misclass{f(D(eval)\ngold |D(q)\ntrain)}\n7 D(q+1)\nadd ←∅\n8 for each (xmis,ymis) ∈D(q)\nmis do\n9 xadd ∼PLLM(·|T(1)\nmis(xmis,ymis))\n10 D(q+1)\nadd ←D(q+1)\nadd ∪{(xadd,ymis)}\n11 Dtrain ←Dseed ∪(∪N\ni=1D(i)\nadd)\nWe present the whole process of EES in Alg.\n2. One round in the for-loop beginning at line 2\ndenotes one round of EES. Rdenotes the number\nof rounds of EES we want to perform; in our imple-\nmentation, we typically do 2 rounds of experiments.\nf denotes the small model; D(q)\nmis denotes the set of\nexamples mis-classified by the small model on the\ngold validation dataset in the q-th round of EES.\nT(1)\nmis(xmis,ymis) denotes the prompt used for error\nextrapolation. The prompt asks the LLM to syn-\nthesize a data point similar to xmis with label ymis.\nIn our implementation, we use the prompt: \"Write\na positive movie review like The movie is great.\"\nD(q+1)\nadd denotes the q+ 1-th additional dataset we\nsynthesized on LLM based on extrapolating D(q)\nmis.\nThe key steps of the EES algorithm are to\ntrain the small model with the current synthesized\ndataset (line 6) and utilize the LLM to extrapolate\nthe misclassified data to generate more training\ndata (lines 8-10). This creates a dataset that better\nreflects the underlying truth.\nIn sum, the EES process reduces the distribution\ngap by using the misclassified data to model the\ndistribution gap and using the LLM to sample ad-\nditional data points from it. This idea is similar to\ndoing optimization on the residuals in the gradient\nboosting literature (Friedman, 2002).\n2.4 Special process for multi-sentence task\nFor clarity, we focus on single-sentence tasks in\nour algorithm discussed before. When transition-\ning to multi-sentence tasks, small modifications are\nnecessary. Specifically, for complex tasks such as\nquestion answering, the context sentence can be\nexcessively long, preventing our prompt from fit-\nting LLM’s input limit. Even when the prompt fits,\ngenerating rationales for each context sentence can\nbe prohibitively costly. Hence, for these situations,\nwe resort to a more traditional seed data synthesis\napproach.\nSpecifically, we perform dataset synthesis given\na set of conditional contexts C= c1,··· ,cm (e.g.,\npremise in NLI and context & answer in QA task).\nWe perform dataset synthesis as follows:\n1. Uniformly sample the current context ccurr\nsentence from C, and current target label\nycurr from all possible labels Y. Com-\nbine them into a seed data synthesis prompt\nT(2)\nquery(ccurr,ycurr).\n2. Synthesize the target sentence (e.g., hypothe-\nsis in NLI and question in QA) from LLM by\nT(2)\nquery(ccurr,ycurr). The synthesized data is\ndenoted as (ccurr,xsyn,ycurr).\n3. Repeat the above steps until we have enough\nseed data Dseed = (Cseed,Xseed,Yseed)\n11820\nDataset Prompt\nType\nPrompt Label\nword (Y)\nIMDb\nTration Imagine you are watching a movie; consider <X> reasons that may lead to <Y>\nimpression of the movie.\npositive/\nnegative\nT(1)\nquery Now imagine that you just watched a movie that has <X>. Now you should\nwrite a <Y> review about this movie.\npositive/\nnegative\nT(1)\nmis Write a <Y> movie similar to: \\n <X> positive/\nnegative\nQNLI T(2)\nquery Given an information paragraph: <X> \\n Please ask a question that has answers\n<Y> the information paragraph\nin/ not in\nT(2)\nmis Given a premise: <X[\"premise\"]> \\n And here is a question: <X[\"question\"]>\nthat the answer of question is <Y> the premise.\\nPlease write another question\nsimilar to the given question and have answers <Y> the premise.\nin/ not in\nRTE T(2)\nquery <X> \\nBased on the above description, the following sentence is definitely <Y>:correct/\nwrong\nT(2)\nmis <X[\"premise\"]> \\nBased on the above description, the following sentence:\n<X[\"Hypothesis\"]> is definitely <Y>. Now write a sentence similar to the given\nsentence and is definitely <Y> based on the given description.\ncorrect/\nwrong\nAdQA T(2)\nquery Given a context: <X[\"context\"]> \\nX<[\"answer\"] is the answer to the following\nquestion:\nNA\nT(2)\nmis Given a context: <X[\"context\"]> \\nX<[\"answer\"] is the answer to:\n<X[\"question\"]>.\\nA question that has the same answer in the context is:\nNA\nTable 1: Designed prompts for the four datasets. Tration denotes the prompt for the LLM to generate rationales.\nT(1/2)\nquery denotes the prompt for seed data synthesis, and <X> denotes the rationale list or context sentences for the\ncurrent seed data example. T(1/2)\nmis denotes the prompt for EES, where <X> is the full misclassified example.\nFor the EES process, in multi-sentence tasks,\nwe only need to modify the for-loop beginning at\nline 8 in Alg. 2 to fit the multi-sentence task. The\nchanged version of line 8 is shown in Alg. 3.\nAlgorithm 3: Multi-sentence EES, inner\nfor-loop\n1 for each (cmis,xmis,ymis) ∈D(q)\nmis do\n2 xadd ∼PLLM (·|T(2)\nmis(cmis,xmis,ymis))\n3 D(q+1)\nadd ←D(q+1)\nadd ∪{(cmis,xadd,ymis)}\n2.5 Prompt engineering\nThe design of prompts can have a huge impact on\nthe quality of the synthesized dataset. We present\nthe prompt templates used for generating rationales,\ndata points, and error extrapolation in Table 1.\n2.6 Theoretical Analysis\nIn this section, we give a detailed analysis of why\nour S3 framework can shrink the distribution gap\nbetween zero-shot synthesis and real-world distri-\nbution by first clarifying the analysis setup and then\ngiving an analysis of the distribution gap problem\nand the effectiveness of our S3 framework.\nWe denote the probability space of the data ex-\nample as P= (S,Σ); here, for simplicity, we wrap\nall possible elements in a data example into one\nvariable s ∈S, and the components in scan be\nvaried depending on the specific task, for example,\nin the text classification task, i.e., s= (x,y) where\nxis a piece of text and yis the corresponding label.\nWe assume that the gold dataset (denoted as\n{S(gold)\ni }ngold\ni=1 ) is obtained by i.i.d. sampling ngold\ntimes from a real-world distributionPD∈P. Then,\nwe also assume the process of obtaining a syn-\nthesized data example as an i.i.d sampling from\nPLLM ∈P. In the analysis section, for simplic-\nity, we define PLLM as a distribution over the data\nexample set Sinstead of the space of human lan-\nguage. This distinction is important because while\ntext data is in natural language, for many tasks,\nlabels may not be.\nSimilarly, we assume that the process of attain-\ning the seed dataset (denoted as {Si}n1\ni=1), where\nn1 is the number of seed data points, is to draw\nn1 i.i.d. samples from our seed data distribution\n11821\nP(0)\nLLM.\nLet us first recall the origin of the distribution\ngap problem in dataset synthesis methods: conven-\ntional data synthesis methods, as well as the seed\ndataset synthesis stage in our approach, sample\ndata points from a fixed distribution P(0)\nLLM. Since\nthe distribution is fixed and different from the task\ndata distribution PD, the synthesized dataset suf-\nfers from a fixed distribution gap no matter how\nmuch data we synthesize. Therefore, the testing\nperformance of the small model trained on the syn-\nthesized dataset on real task data is bounded by\nthis gap. Our approach, S3, aims to resolve this\nlimitation.\nLet us assume that the small model perfectly\nlearns the synthesized dataset distribution. In this\ncase, the error that the small model makes on the\nsmall gold validation dataset can represent the dis-\ntribution gap between PDand P(0)\nLLM.\nFinally, we argue that a good LLM can perfectly\nextrapolate from the errors. This means that the\nLLM can synthesize samples from the difference\nbetween two distributions PD−P(0)\nLLM. Formally,\nthe additional data synthesized in each round of the\nEES process follows:\nPadd := PLLM(·|PD−P(0)\nLLM) (1)\nTherefore, by sampling the same number of data\npoints from Padd and combining them with the\noriginal seed data distribution P(0)\nLLM, the mixed\ndataset shall follow the distribution:\nP(1)\nLLM := p·Padd + (1 −p)P(0)\nLLM ≈PD (2)\nwhere p ∈ [0,1] is the ratio of combination, it\ncan be intuitively understood as the portion of the\nadditional dataset and seed dataset. This suggests\nthat, theoretically, we can recover the gold data\ndistribution by simply combining the original seed\ndata and the additional data synthesized via EES.\nHowever, please note that we cannot guarantee\nthe LLM and the training of the small model are\nperfect in real-world scenarios. Therefore, S3 re-\npeats this process iteratively to gradually reduce\nthe distribution gap and optimize the mixed dataset\nuntil convergence.\n3 Experiments\nWe conduct experiments to test the effectiveness\nof our approach across three major NLP tasks over\nfour datasets. We also do a thorough ablation study\n(Section 3.4), a transferability study (Section 3.5)\nfor the S3 framework, and a study on additional\ndata quality (Section 3.6).\n3.1 Setup\n3.1.1 Datasets\nIn this study, we evaluate our S3 on three major\nNLP tasks: text classification, Natural Language\nInference (NLI), and Question Answering (QA).\nFor text classification, we use the IMDb (Maas\net al., 2011) dataset; for the NLI task, we use the\nQNLI (Rajpurkar et al., 2016; Wang et al., 2018)\nand the RTE (Bentivogli et al., 2009; Giampiccolo\net al., 2007; Haim et al., 2006) dataset; for the QA\ntask, we use the Adversarial QA (Bartolo et al.,\n2020) dataset.\n3.2 Baselines\nWe compare our S3 framework with the following\nbaselines:\n1. ZeroGen: ZeroGen is the basic data synthe-\nsis method proposed by Ye et al. (2022b). It\nneither uses rationales for data synthesis nor\nattempts to reduce the distribution gap. Note\nthat ZeroGen also uses the same small valida-\ntion set for tuning hyperparameters.\n2. GoldGen: This baseline extrapolates the en-\ntire gold validation data instead of the errors\nmade by the small model. We further use\nthis baseline to test the effectiveness of the\nerror extrapolation idea in the S3 framework.\nWe keep the scale of synthesized datasets the\nsame in order to make a fair comparison with\nS3.\n3. ProGen: This baseline was proposed by Ye\net al. (2022a), like the EES, it also considers\ntraining feedback. However, this framework\nis only available for text classification tasks,\nand it does not use LLM rationales for data\nsynthesis.\n4. Gold Data: We also include a baseline that\ntrains the small model on the original gold\ndata for reference.\n3.2.1 Implementation details\nThis section gives full implementation details ofS3\nin our experiments. We apply GPT3.5 derived from\n(Brown et al., 2020) as the LLM for all the synthe-\nsis work, and we use nucleus sampling (Holtzman\n11822\nMethod Data Size / Results IMDb QNLI RTE Adversarial QA (EM/F1) Average\nGold Data Data Size 25k 105k 2.5k 30k 40.63k\nResults 87.93 88.05 58.12 18.6/29.85 56.51\nProGen Data Size 100k - - - -\nResults 84.12 - - - -\nZeroGen Data Size 200k 200k 200k 200k 200k\nResults 84.28 71.19 59.93 6.33/9.96 46.34\nGoldGen Data Size 25k 150k 30k 80k 61.25k\nResults 87.93 78.31 64.25 11.63/23.33 53.09\nS3 Data Size 21.2k 168k 33.6k 81.5k 76.08k\nResults 89.00 79.92 73.29 12.50/24.38 55.73\nTable 2: Main experimental results. All compared methods are evaluated by fine-tuning DistilBERT. The perfor-\nmance of fine-tuning the small model on gold data is in gray because it is not directly comparable with other results.\net al., 2019) with a temperature of 0.9 for decod-\ning. We use DistilBERT-base-uncased (Sanh et al.,\n2020) provided by the Hugging Face Transform-\ners library (Wolf et al., 2019) as the small model.\nWe perform hyperparameter tuning on the batch\nsize, learning rate, weight decay, and the number\nof epochs for fine-tuning the small model.\n3.2.2 Evaluation Method\nFor text classification and NLI tasks, we use the\naccuracy rate as the evaluation method. For QA\ntasks, we use Exact Match (EM) and F1 score as\nevaluation methods. To implement the experiment\nof S3 method, we utilize the training data from the\noriginal dataset as the gold evaluation data dataset\nin EES (i.e., D(eval)\ngold ). And we use testing data\nfrom the original dataset to test our model’s perfor-\nmance.\n3.3 Experimental Results\nWe present our main experimental results in Table 2.\nWe can observe that our S3 framework has a huge\nimprovement (an average improvement of 9.48%)\ncompared to ZeroGen. The performance gap is\nespecially large in NLI and QA tasks. Moreover,\nwe only use an average of 30.43% amount of data\ncompared to ZeroGen, which can be considered as\na significant improvement. Such an improvement\nproves the effectiveness of the initial seed data syn-\nthesis method and the idea to keep on optimizing\nthe data in our S3.\nWe then compare S3 with the GoldGen base-\nline to test the effectiveness of extrapolating the\nerrors of the small model on the validation set in-\nstead of the entire validation set. We find that S3\noutperforms GoldGen with an average absolute per-\nformance improvement of 2.73%. This confirms\nthe advantage of error extrapolation over directly\nextrapolating gold data.\nIt is also noteworthy that S3 yields competitive\nresults compared to directly fine-tuning the small\nmodel on the full gold training data. Specifically,\nS3 even outperforms gold data performance on\nIMDB and RTE. This confirms the potential of\napplying S3 in real-world applications.\n3.4 Ablation Study\n3.4.1 Ablation of EES\nWe first ablate the error extrapolation-based syn-\nthesis (EES) framework of S3, using only the seed\ndata synthesized based on Section 2.2. We make\nsure that the scale of the training dataset is approx-\nimately the same for a fair comparison. The result\ncan be seen in Table 3. This result proves the ef-\nfectiveness of our view of the dynamic dataset and\nEES. We find that for more complex tasks like QA\nand NLI, our EES framework can give a larger\nimprovement, which proves the distribution gap\nproblem and our EES framework’s ability to shrink\nthis gap.\n3.4.2 Ablation of Seed Data Synthesis with\nRationales\nWe then ablate the use of rationale for dataset syn-\nthesis in the S3 framework on the IMDb dataset.\nThe results are shown in Table 4. We find that us-\ning rationale for dataset synthesis enables the LLM\nto generate datasets of higher quality that leads to\n11823\nMethod IMDb QNLI RTE Adversarial QA\nS3 89.00 79.92 73.29 12.50/24.38\nw/o EES 86.86 73.70 65.71 8.70/20.03\nTable 3: Ablation test results (%) on iterative error ex-\ntrapolation. The baseline w/o error extrapolation is\nfine-tuned on the same amount of data compared to S3.\nbetter performance of the small model with a lower\nbudget, i.e., fewer synthesized examples.\nwith Rationale w/o Rationale\nDataset Size 15k 40k\nResults (%) 86.86 85.34\nTable 4: Experiment result of ablation of rationales anal-\nysis in seed data synthesis. The section with Rationale\nmeans we synthesize seed data guided by a set of LLM\nsynthesized rationales, and w/o Rationale means the\nseed data is synthesized by the task-descriptive prompt\nwithout rationale.\n3.5 Transferability of EES Data\nWe then test the transferability of the EES-\nsynthesized data. The results are shown in Table 5.\nIn this test, we replace the seed dataset of our frame-\nwork with the data synthesized by Ye et al. (2022b).\nWe do two sets of testing. We compare the variants\nwhere we directly add the EES data synthesized in\nS3 (+ourAdd) and that with the small model trained\non the data synthesized by Ye et al. (2022b). We\ncan see that the two variants both lead to similar\nperformance improvements. This shows that the\nEES synthesized data can effectively transfer to\nother zero-shot synthesized datasets. We believe\nthis is because the distributional gap for different\nzero-shot data synthesis methods is similar. There-\nfore, the data synthesized by the EES method can\nbe universally helpful, which further demonstrates\nthe potential of S3.\nMethod IMDb QNLI AdQA\nZeroGen 84.28 68.60 4.60/9.62\n+ourAdd 87.50 73.51 9.70/20.10\n+synAdd 87.41 72.21 10.27/19.92\nTable 5: Transferability test result (%): where +ourAdd\nis ZeroGen dataset as seed data and S3 synthesized\ndata as additional data, and +synAdd is using EES on\nZeroGen trained small model’s misclassified data\n3.6 Additional data quality study\nWe perform this experiment to check the quality\nof the additional dataset synthesized by EES. Note\nthat for earlier LLMs like GPT2 (Radford et al.,\n2019) or T5 (Raffel et al., 2020), there used to\nbe a tendency to repeat the prompt. If the LLM\njust repeats the misclassified data, then there is no\nextrapolation. Thus, we composed experiments as\nfollows to test the quality of the additional dataset:\nSentence Encoding: For both misclassified\ndata Dmis and additional data Dadd, we use Distil-\nBERT to encode each xmis and xadd. This results\nin encoded sentences represented as zmis and zadd\nrespectively, and each encoded sentence is in Rd\n(with d= 768 in DistilBERT)\nCosine Similarity: Then, by comparing the\ncosine similarity between zmis and zadd, we gauge\ntheir semantic similarity. High cosine similarity\nindicates substantial semantic overlap.\nEdit Distance: Further, to understand textual\ndistinctiveness, we compute the edit distance be-\ntween sentences xmis and xadd. If the edit distance\napproaches the sentence length, we infer that the\ntexts differ significantly in their composition. The\nresults are shown in Table 6.\nLabel IMDb QNLI RTE AdQA\nData Num 6,173 51,100 1,522 51,532\nAvg. Cos Sim 0.9497 0.9537 0.9380 0.9468\nAvg. Edit Dist. 273.92 14.64 16.38 13.99\nAvg. xmis len 288.04 14.17 13.91 13.73\nA VG.xadd len 218.72 19.97 24.61 18.70\nTable 6: Quality study of Additional Data\nThe average misclassified data length (avg xmis\nlen) and average generated data length (avg xadd\nlen) provide context to interpret edit distances. This\nresult shows that while there is high semantic sim-\nilarity among the misclassified data and the ad-\nditional generated data (evidenced by the cosine\nsimilarity scores), the generated sentences are not\nmere copies of the misclassified samples (as their\nedit distance is almost the length of the whole sen-\ntence). This result provides extra evidence in favor\nof the quality of the newly generated data.\n4 Related work\n4.1 Dataset Synthesis\nThe vast quantity of data required by the majority\nof Machine Learning methodologies has prompted\nnumerous researchers to explore the concept of\nDataset Synthesis. This aims to generate a dataset\n11824\nfrom large pre-trained models, such as LLMs, in\norder to transfer rich knowledge from large mod-\nels to small models. Initial attempts to achieve\nthis used fine-tuned generative models to gener-\nate data (Anaby-Tavor et al., 2020; Kumar et al.,\n2020). These efforts involved first fine-tuning the\nLLMs with a small amount of human-annotated\ndata (gold data), then combining the generated data\nwith gold data to train small models. Other re-\nsearchers sought to synthesize copious amounts\nof data for semi-supervised learning (Chen et al.,\n2020; Wang et al., 2021). Nonetheless, these meth-\nods are only suitable for straightforward text classi-\nfication tasks, proving data inefficient and ineffec-\ntive for more complex tasks like NLI or QA.\nThe potential of zero-shot performance offered\nby LLMs has led some researchers to consider\nzero-shot dataset synthesis based on non-finetuned\nLLMs (Meng et al., 2022; Ye et al., 2022b). How-\never, as indicated by Fig1, direct querying of non-\nfine-tuned LLMs often results in data that suffers\nfrom a large distribution gap and is typically in-\nefficient. Thus, some studies have attempted data\nselection (Gao et al., 2023) or data augmentation\n(Ye et al., 2022a). However, their capacity to rectify\nthe distribution gap leaves room for improvement.\n4.2 In-context Learning\nBrown et al. (2020) suggests LLMs can better learn\nthe task they are working on by conditioning on a\nfew examples in the prompt. This paradigm, known\nas In-context learning, is particularly appealing as\nit negates the necessity of updating the parame-\nters of LLM. Subsequent research has focused on\noptimizing the choice of prompt templates and in-\ncontext examples (Liu et al., 2021; Wang et al.,\n2023; Lu et al., 2021), and learning with in-context\nobjective descriptions (Chen et al., 2021). The key\nidea for in-context learning is to learn from analogy\n(Dong et al., 2022), which aligns with our idea of\nextrapolating error to synthesize additional data to\nfill the distribution gap. However, most in-context\nlearning methods are designed for a few-shot set-\nting, whereas in our research, the LLM does not\nneed to be trained. We explore the LLM’s ability to\ndirectly extrapolate from errors, providing a crucial\nexample for creating a more effective dataset.\n5 Conclusion\nThis paper proposes the Synthesis Step by Step (S3)\napproach based on a dynamic dataset viewpoint\nfor dataset synthesis. S3 is a novel dataset syn-\nthesis framework that shrinks the distribution gap\nbetween purely LLMs synthesized datasets and the\nreal underlying data distribution. S3 achieves this\nby first using seed data synthesis with rationales to\nhave a low distribution gap in seed data. It shrinks\nthis distribution gap by iteratively extrapolating\nerrors of the small model on a small amount of real-\nworld data. Extensive experiments on three major\nNLP tasks over four commonly used datasets show\nthat compared with a representative baseline, S3\nsignificantly improves the performance of a small\nmodel with averagely only one-third of synthesized\ndata. S3 has high practical potential in many real-\nworld applications because it can effectively (i.e,\nwith better performance) and efficiently (i.e., with\nimproved data efficiency) transfer knowledge in an\nextremely large model (e.g., GPT 3.5) to a small\nmodel (e.g., DistilBert), achieving data efficiency\nand computation efficiency at the same time.\nAcknowledgments\nWe thank the anonymous reviewers for their feed-\nback on our paper. MS acknowledges support from\nthe Swiss National Science Foundation (Project\nNo. 197155), a Responsible AI grant by the Hasler-\nstiftung; and an ETH Grant (ETH-19 21-1).\nLimitations\nAlthough S3 achieved promising results, there are\nstill several limitations of our work. The first lim-\nitation is that in the experiments, we spotted that\na tiny change in the synthesis prompts can lead\nto a significant performance drop, which means\nour framework is not prompt-stable. A possible\nfuture direction is to develop a systematic way to\ncompose prompts that can perform stably well by\nfine-tuning an LLM using good prompts. The sec-\nond limitation is that S3 assumes that the LLM has\na rich knowledge of the specific task. But in the\nactual application of the approach in the real-world,\nthere is no such guarantee. A possible solution to\nmitigate this limitation is to ask the LLM to divide\nthe previously unseen task into multiple simple\ntasks that the LLM has a good understanding of,\nbut it also requires the LLM to have a good ability\nto understand the subtasks. The third limitation\nis that S3 is task-specific. Future work may try to\nextend the method to cross-task settings to further\nimprove the computational and data efficiency of\nthe method.\n11825\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 7383–7390.\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebas-\ntian Riedel, and Pontus Stenetorp. 2020. Beat the ai:\nInvestigating adversarial human annotation for read-\ning comprehension. Transactions of the Association\nfor Computational Linguistics, 8:662–678.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC. Citeseer.\nLucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Mar-\nkeeva, Rohan Anil, and Alexander Kolesnikov. 2022.\nKnowledge distillation: A good teacher is patient and\nconsistent. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 10925–10934.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTing Chen, Simon Kornblith, Kevin Swersky, Moham-\nmad Norouzi, and Geoffrey E Hinton. 2020. Big\nself-supervised models are strong semi-supervised\nlearners. Advances in neural information processing\nsystems, 33:22243–22255.\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\nand He He. 2021. Meta-learning via language model\nin-context tuning. arXiv preprint arXiv:2110.07814.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nTerrance DeVries and Graham W Taylor. 2017. Dataset\naugmentation in feature space. arXiv preprint\narXiv:1702.05538.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nJerome H Friedman. 2002. Stochastic gradient boosting.\nComputational statistics & data analysis, 38(4):367–\n378.\nJiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng\nYe, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan\nLiang, Zhenguo Li, and Lingpeng Kong. 2023. Self-\nguided noise-free data generation for efficient zero-\nshot learning. In The Eleventh International Confer-\nence on Learning Representations.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n2023. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller\nmodel sizes. arXiv preprint arXiv:2305.02301.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. arXiv preprint arXiv:2003.02245.\nBohan Li, Yutai Hou, and Wanxiang Che. 2022. Data\naugmentation approaches in natural language pro-\ncessing: A survey. AI Open, 3:71–90.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming\nfew-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. arXiv\npreprint arXiv:2202.04538.\nOpenAI. 2023. Gpt-4 technical report.\n11826\nRaul Puri, Ryan Spring, Mostofa Patwary, Mohammad\nShoeybi, and Bryan Catanzaro. 2020. Training ques-\ntion answering models from synthetic data. arXiv\npreprint arXiv:2002.09599.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nBernadette Sharp, Florence Sedes, and Wieslaw\nLubaszewski. 2017. Cognitive approach to natural\nlanguage processing. Elsevier.\nConnor Shorten and Taghi M Khoshgoftaar. 2019. A\nsurvey on image data augmentation for deep learning.\nJournal of big data, 6(1):1–48.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2023. Distilling reasoning capabilities into\nsmaller language models. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages\n7059–7073.\nRyan Smith, Jason A Fries, Braden Hancock, and\nStephen H Bach. 2022. Language models in the\nloop: Incorporating prompting into weak supervision.\narXiv preprint arXiv:2205.02318.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce\nlabeling cost? gpt-3 can help. arXiv preprint\narXiv:2108.13487.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776–5788.\nXinyi Wang, Wanrong Zhu, and William Yang Wang.\n2023. Large language models are implicitly topic\nmodels: Explaining and finding good demon-\nstrations for in-context learning. arXiv preprint\narXiv:2301.11916.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. BERT-of-theseus: Com-\npressing BERT by progressive module replacing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7859–7869, Online. Association for Computa-\ntional Linguistics.\nJingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou,\nand Lei Li. 2021. A survey on green deep learning.\nJiacheng Ye, Jiahui Gao, Jiangtao Feng, Zhiyong Wu,\nTao Yu, and Lingpeng Kong. 2022a. Progen: Pro-\ngressive zero-shot dataset generation via in-context\nfeedback. arXiv preprint arXiv:2210.12329.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022b. Zerogen: Efficient zero-shot learning via\ndataset generation. arXiv preprint arXiv:2202.07922.\nWangchunshu Zhou, Ronan Le Bras, and Yejin Choi.\n2023. Modular transformers: Compressing trans-\nformers into modularized layers for flexible efficient\ninference. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 10452–10465,\nToronto, Canada. Association for Computational Lin-\nguistics.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 18330–18341. Curran Associates,\nInc.\nA Intuitive understanding to EES\nSince the pseudo-code of EES may be somewhat\nnon-intuitive to understand, this part aims to pro-\nvide an intuitive understanding of the EES method\non single-sentence tasks.\n11827\nA.1 Attain Error\nThe first step for EES is to attain the error made\nby the small model on the gold validation dataset,\nwhich is, to a certain extent, the representation of\nthe distribution gap between LLM’s seed data syn-\nthesis distribution and the real-world distribution.\nTo attain the error, we must first train the small\nmodel with currently synthesized data. This in-\ncludes the seed data Dseed, and additional datasets\nD(0)\nadd,··· ,D(q)\nadd, where q is the current round of\niteration. Then we have D(0)\nadd = ∅. Thus, the\ntraining dataset for q-th iteration is:\nD(q)\ntrain = Dseed ∪(∪q\nj=0D(j)\nadd) (3)\nThen, we train the small model with D(q)\ntrain. We\ndenote the fitted small model as f(·|D(q)\ntrain). Then,\nwe evaluate the fitted small model on the gold val-\nidation dataset and obtain the data samples with\nhigh error in the validation dataset:\nD(q)\nmis = misclass{f(D(eval)\ngold |Dtrain)} (4)\nwhere the misclassfunction denotes the function\nthat attains the data samples that have been misclas-\nsified. For instance, for the QA task, this can mean\ndata samples that do not have an exact match with\nthe answer or data samples with low F1 scores. We\nrepresent the distribution gap between the underly-\ning truth and the D(q)\ntrain by the misclassified gold\nevaluation dataset D(q)\nmis, which is the distribution\ngap in q-th round of EES.\nA.2 Synthesis on extrapolating error\nAfter having D(q)\nmis, for all the misclassified data\n(xmis,ymis) ∈ D(q)\nmis, we query the LLM again\nusing a prompt that wraps information of the mis-\nclassified data. The prompt T(1)\nmis(xmis,ymis) intu-\nitively asks the LLM to extrapolate the misclassi-\nfied data and synthesize a new data example. For\nexample, in the movie classification problem, if\nthe current misclassified data is: ( The move is\ngreat, positive); our original f(·|D(q)\ntrain) labeled\nsuch a review as a negative one. In this case,\nT(1)\nmis(xmis,ymis) can be something like Generate\na positive movie review like The move is great.\nWe query the LLM with T(1)\nmis(xmis,ymis), to\nobtain another data example similar to the error.\nThis process is repeated for every misclassified\ndata point. Thus, we obtain the q+ 1-th additional\ndataset D(q+1)\nadd . We repeat the Attain Error and\nSynthesis on extrapolating error steps for multi-\nple rounds until the error converges. With such a\nmethod, we can optimize our synthesized dataset\nstep by step to attain a dataset with a lower distri-\nbution gap by utilizing the information provided by\nextrapolating errors that represent the distribution\ngap.\nB Computation complexity comparison\nbetween S3 and ZeroGen\nThis section studies the total computation cost of\nthe S3 framework. We compare the number of\nfloating-point operations (FLOPs) involved in fine-\ntuning the model with S3 and ZeroGen synthesized\ndataset. For the BERT family of models, accord-\ning to Brown et al. (2020), they cost 6 FLOPs per\ntoken per parameter (i.e., Ftoken,para = 6) in train-\ning. The DistilBERT model (Sanh et al., 2020)\nhas npara = 66 ×106 parameters and the typical\ninput length for one record is num(token)\nrec = 512.\nTherefore, the training FLOPs per record of data\nper epoch is:\nFrec =Ftoken,para ∗num(token)\nrec ∗npara\n=2.03 ×1011\nThe ZeroGen method typically uses 200k\nrecords of data and trains for an average of 10\nepochs to achieve the best results based on our ex-\nperiments. Thus, the total fine-tuning cost in terms\nof FLOPs for ZeroGen is:\nFZeroGen = Frec ∗200k∗10 = 4.06 ∗1017\nIn S3, in the first round of fine-tuning (using only\nthe seed data), the dataset size is 51.2krecords on\naverage (i.e., seed dataset is about 2/3th size of\nfinal dataset). After one round of EES, the total\ndataset size becomes 64.0k(i.e., 5/6th size of the\nfinal dataset). The final round of fine-tuning with\ntwo EES additional datasets and the seed dataset\nthat have a total size of 76.8krecords of data. On\naverage, our method needs 8 epochs to achieve its\nbest result. Therefore, the total number of FLOPs\nof fine-tuning DistilBERT for the 3 iterations (2 for\ngetting misclassified data, 1 for final fine-tuning)\nin our S3 is:\nFS3 =Frec ∗(51.2k+ 64.0k+ 76.8k) ∗8\n=3.11 ∗1017\n11828\nlabel Generated Movie Review\nNegative If you’re looking for a film that will make you cringe in disbelief, look no further than\n\"Zombie Cheerleader Apocalypse.\" This movie is the epitome of a low-budget, poorly-made\nproduction. The acting is atrocious, and the jokes are so bad that they will make you want\nto gouge your eyes out. The story revolves around a group of cheerleaders who are turned\ninto zombies after a failed science experiment. They then proceed to wreak havoc on their\nschool and town, killing anyone in their path. The supposed \"hero\" of the film is a nerdy guy\nwho has a crush on the head cheerleader and tries to save her from the zombie apocalypse.\nThe special effects are laughable, and the makeup for the zombies is so poorly done that it’s\nhard to tell if they’re supposed to be scary or comical. The dialogue is cringeworthy, and\nthe attempts at humor fall completely flat. The film tries to be a parody of horror movies,\nbut instead comes off as a cheap knockoff. Overall, \"Zombie Cheerleader Apocalypse\" is a\ncomplete waste of time. It’s not even bad in a fun way, it’s just plain bad. The only reason to\nwatch this movie is if you want to torture yourself for 90 minutes, otherwise, avoid it at all\ncosts.\nPositive \"The Family Man\" is a thrilling and heartwarming movie that will keep you at the edge of\nyour seat. The movie revolves around Srikant Tiwari (Manoj Bajpayee), a senior analyst in\nthe National Intelligence Agency, who is trying to balance his demanding job and his personal\nlife with his wife Suchitra (Priyamani), and their two children. The movie showcases the\nstruggles of maintaining normalcy in Srikant’s life, while also juggling intense missions. The\naction sequences are brilliantly choreographed, with twists and turns that keep you guessing\ntill the very end. Bajpayee’s performance is outstanding, as he brings a depth and complexity\nto his character. The chemistry between him and Priyamani is natural and heartwarming,\nadding a layer of emotion to the story. The supporting cast is equally impressive, with Sharib\nHashmi and Shreya Dhanwanthary delivering memorable performances. The movie also\nexplores important themes such as the impact of terrorism, family values, and duty towards\none’s country. The director, Raj Nidimoru and Krishna D.K., have done a remarkable job of\nbalancing the action with the human story, making it a compelling watch for all audiences.\nOverall, \"The Family Man\" is a must-watch for anyone looking for a thrilling and entertaining\nride, with a strong message of family and patriotism.\"\nPositive If you’re a rom-com fan, then you’ll love this movie! It’s a fun, light-hearted take on the\nclassic boy-meets-girl trope, with plenty of laughs and heartwarming moments to keep you\nentertained. The chemistry between the two leads is great, and the supporting cast is equally\ncharming. While some may criticize the predictable plot and lack of originality, I found it to\nbe a comforting and enjoyable watch. The soundtrack is also a standout, with catchy songs\nthat perfectly capture the mood of each scene. I highly recommend this movie for anyone\nlooking for a feel-good, easy watch. 8/10.\nNegative This movie is a complete waste of time. The story is nonexistent and feels like the director\njust slapped together random scenes. The characters are forgettable and lack any depth or\ndevelopment. The pacing is so fast that it’s hard to keep up with what’s going on. It’s like\nwatching a poorly made music video on repeat.The only redeeming quality of this film is\nthe cinematography, but even that can’t save it from being a complete mess. It’s clear that\nthe director has some talent behind the camera, but they need to work on their storytelling\nskills.Overall, I wouldn’t recommend this film to anyone. Save your money and go watch\nsomething else. Let’s hope the director can learn from their mistakes and create something\nbetter in the future.\nTable 7: Examples of generated IMDb data\n11829\nFigure 3: t-SNE result for QNLI (left), RTE (center), AdQA (right) for dataset diversity analysis. ZeroGen data’s\npoints are plotted in Yellow,S3’s in Green, and Gold data in Purple.\nDataset S3 Coverage ZeroGen Coverage\nQNLI 76.35 63.03\nRTE 73.59 14.90\nAdQA 51.02 46.00\nTable 8: Coverage rate (%) of S3 and ZeroGen\nMethod IMDb QNLI RTE AdQA\nGold Data 92.30 91.00 71.50 22.97/36.59\nZeroGen 83.66 70.11 72.2 5.07/10.74\nS3 89.55 85.20 76.17 20.50/34.40\nTable 9: Apply S3 framework on MiniLM\nTo conclude, due to fewer rounds of fine-tuning\nepochs and the lower need for data, S3 uses only\n3/4th the number of FLOPs compared to the Zero-\nGen baseline, even though we fine-tuned the model\nmultiple times.\nC Dataset Diversity analysis for S3\nThis section analyzes the diversity of the synthe-\nsized sentences. Such an analysis is necessary as\nthe LLMs may generate sentences with similar\nmeanings, rendering the dataset lacking in diver-\nsity. As there is no universally approved method\nfor analyzing dataset diversity, we use both quan-\ntitative and qualitative methods to analyze dataset\ndiversity:\nC.1 Quantitative Analysis:\nFor short synthesized sentences, such as the QNLI,\nRTE, and AdQA datasets, we approach the dataset\nanalysis quantitatively. Given the high hidden di-\nmension of the sentence encoding (e.g., 768 for Dis-\ntilBERT), direct analysis can be inefficient. Hence,\nwe used t-SNE for dimension reduction (Van der\nMaaten and Hinton, 2008). The final steps of our\nanalysis are as follows:\n1. Uniformly sample a similar amount of data\nfrom gold data, S3 synthesized data, Zero-\nGen synthesized data. We have D′\ngold =\n{x(i)\ngold,y(i)\ngold}n1\ni=1, D′\nS3 = {x(j)\nS3 ,y(j)\nS3 }n2\nj=1,\nand D′\nZeroGen = {x(k)\nZeroGen,y(k)\nZeroGen}n3\nk=1\nwhere n1,n2,n3 should be similar.\n2. Encode the sentences using DistilBERT.\nThen, we have the sentence encodings:\n{z(i)\ngold}n1\ni=1,{z(j)\nS3 }n2\nj=1,{z(k)\nZeroGen}n3\nk=1 ⊆Rd,\nwhere d is the hidden state’s dimension (in\nour case, it is 768).\n3. Perform t-SNE on the encoded data z :=\n{z(i)\ngold}n1\ni=1 ∪{z(j)\nS3 }n2\nj=1 ∪{z(k)\nZeroGen}n3\nk=1 to\nreduce the dimension from dto 2. We have:\nt−SNE(z) = p = {p(i)\ngold}n1\ni=1 ∪{p(j)\nS3 }n2\nj=1 ∪\n{p(k)\nZeroGen}n3\nk=1 ⊆R2\n4. Draw the reduced dimension points on a scat-\nter plot to directly see the overlap of our syn-\nthesized dataset and the Gold data. We show\nthe results in Fig. 3. We can see that the green\nregion significantly aligns with the purple re-\ngion, which indicates that S3 results in similar\ndata diversity as the gold data.\nData diversity can also be quantified by count-\ning how many points of p(k)\ngold are in the area\nof AS3 := ∪n2\nj=1Bγ(p(j)\nS3 ) and AZeroGen :=\n∪n3\nk=1Bγ(p(k)\nZeroGen), where Bγ(p) represents a\nsolid circle with center pand radius γ. The results\nfor QNLI, RTE, and AdQA are shown in Table 8.\n11830\nThe results further demonstrate the superior cover-\nage and diversity of our S3 framework compared\nto ZeroGen.\nC.2 Qualitative Analysis:\nFor tasks that require the generation of longer texts,\nthe text encoding approach is not amenable to t-\nSNE dimension reduction and interpretation. Thus,\nin such settings, we conduct qualitative analysis.\nWe show examples of the generated data for the\ncase of sentiment classification of IMDB reviews\nin Table 7. We can observe that these examples\nexhibit rich contexts and diverse patterns, which\nsupports the superiority of our S3 framework. For\nmore qualitative results, please refer to the dataset\nin our project repository.\nD Additional Results for S3 with MiniLM\nIn addition to DistilBERT, we also evaluated the\nperformance of the Synthesis Step by Step (S3)\nframework using MiniLM (Wang et al., 2020) as\nthe small model. The results of this experiment are\npresented in Table 9. Notably, there is a substantial\nenhancement in performance when compared to\nthe ZeroGen baseline in all the tasks. Moreover, in\ntasks like RTE which lack data, our method even\nsurpasses the performance of the model trained on\ngold data. These results provide robust evidence\nthat the effectiveness of S3 is not limited to a spe-\ncific model. Instead, it offers consistent improve-\nments across different small models, underscoring\nits broad applicability and efficacy.\n11831",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.8266496062278748
    },
    {
      "name": "Computer science",
      "score": 0.8201577067375183
    },
    {
      "name": "Language model",
      "score": 0.6103856563568115
    },
    {
      "name": "Task (project management)",
      "score": 0.5234931707382202
    },
    {
      "name": "Synthetic data",
      "score": 0.519612729549408
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4582147002220154
    },
    {
      "name": "Key (lock)",
      "score": 0.4353374242782593
    },
    {
      "name": "Data modeling",
      "score": 0.43396297097206116
    },
    {
      "name": "Machine learning",
      "score": 0.40103980898857117
    },
    {
      "name": "Data mining",
      "score": 0.3916756510734558
    },
    {
      "name": "Database",
      "score": 0.09625697135925293
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ]
}