{
    "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
    "url": "https://openalex.org/W4389519290",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3081741921",
            "name": "Ekaterina Fadeeva",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A5046928528",
            "name": "Roman Vashurin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287895887",
            "name": "Akim Tsvigun",
            "affiliations": [
                "National Research University Higher School of Economics",
                "National University of Sciences and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4320554972",
            "name": "Artem Vazhentsev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5059760393",
            "name": "Sergey Petrakov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3001712327",
            "name": "Kirill Fedyanin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5012623300",
            "name": "Daniil Vasilev",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2766703714",
            "name": "Elizaveta Goncharova",
            "affiliations": [
                "National University of Sciences and Technology",
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2110658729",
            "name": "Alexander Panchenko",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2040166281",
            "name": "Maxim Panov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096893938",
            "name": "Timothy Baldwin",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": "https://openalex.org/A294125378",
            "name": "Artem Shelmanov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4300980582",
        "https://openalex.org/W4285298706",
        "https://openalex.org/W3173618889",
        "https://openalex.org/W2964348886",
        "https://openalex.org/W4321594373",
        "https://openalex.org/W4285429195",
        "https://openalex.org/W4385565160",
        "https://openalex.org/W4285143611",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2152701363",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4287854593",
        "https://openalex.org/W2964119254",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W2953209111",
        "https://openalex.org/W3128459263",
        "https://openalex.org/W4392669906",
        "https://openalex.org/W3101988982",
        "https://openalex.org/W2964223283",
        "https://openalex.org/W4285228085",
        "https://openalex.org/W3153723500",
        "https://openalex.org/W4385570776",
        "https://openalex.org/W4378945636",
        "https://openalex.org/W4225504384",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W3153712677",
        "https://openalex.org/W4385570226",
        "https://openalex.org/W4385572775",
        "https://openalex.org/W2739575494",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4380995299",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W3084095723",
        "https://openalex.org/W4285266418",
        "https://openalex.org/W2972603547",
        "https://openalex.org/W4226278401"
    ],
    "abstract": "Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 446–461\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLM-Polygraph: Uncertainty Estimation for Language Models\nEkaterina Fadeeva3,5 ♢ Roman Vashurin2 ♢ Akim Tsvigun5,6,7 ♢ Artem Vazhentsev3,4 ♢\nSergey Petrakov3 ♢ Kirill Fedyanin2 Daniil Vasilev5 Elizaveta Goncharova4,5,6\nAlexander Panchenko3,4 Maxim Panov1 Timothy Baldwin1,8 Artem Shelmanov1\n1MBZUAI 2TII 3Center for Artificial Intelligence Technology 4AIRI\n5HSE University 6AI Center NUST MISiS 7Semrush 8The University of Melbourne\n{ekaterina.fadeeva, sergey.petrakov}@skol.tech {roman.vashurin, kirill.fedyanin}@tii.ae\nakim.tsvigun@semrush.com {vazhentsev, panchenko, goncharova}@airi.net\n{maxim.panov, timothy.baldwin, artem.shelmanov}@mbzuai.ac.ae\nAbstract\nRecent advancements in the capabilities of\nlarge language models (LLMs) have paved the\nway for a myriad of groundbreaking applica-\ntions in various fields. However, a significant\nchallenge arises as these models often “hallu-\ncinate”, i.e., fabricate facts without providing\nusers an apparent means to discern the verac-\nity of their statements. Uncertainty estimation\n(UE) methods are one path to safer, more re-\nsponsible, and more effective use of LLMs.\nHowever, to date, research on UE methods for\nLLMs has been focused primarily on theoret-\nical rather than engineering contributions. In\nthis work, we tackle this issue by introducing\nLM-Polygraph, a framework with implementa-\ntions of a battery of state-of-the-art UE meth-\nods for LLMs in text generation tasks, with\nunified program interfaces in Python. 1 Addi-\ntionally, it introduces an extendable benchmark\nfor consistent evaluation of UE techniques by\nresearchers, and a demo web application that\nenriches the standard chat dialog with confi-\ndence scores, empowering end-users to discern\nunreliable responses.2,3 LM-Polygraph is com-\npatible with the most recent LLMs, including\nBLOOMz, LLaMA-2, ChatGPT, and GPT-4,\nand is designed to support future releases of\nsimilarly-styled LMs.\n1 Introduction\nLarge language models (LLMs) have demonstrated\nremarkable performance across a variety of text\ngeneration tasks. Instruction fine-tuning and rein-\nforcement learning from human feedback (RLHF)\nhave brought the zero-shot performance of these\nmodels to a new level (Ouyang et al., 2022). How-\never, the capabilities of LLMs, despite their pro-\nfound power and complexity, are inherently con-\nstrained. Limitations arise from the finite nature\n1http://lm-polygraph.nlpresearch.group\n2http://lm-polygraph-demo.nlpresearch.group\n3http://lm-polygraph-video.nlpresearch.group\n♢ Equal contribution\nof the training data and the model’s intrinsic mem-\norization and reasoning capacities. Hence, their\nutility is bounded by the depth and breadth of the\nknowledge they embed.\nDue to their training objectives, even when the\nembedded knowledge of an LLM on a given topic\nis limited, it tends to be over-eager to respond to\na prompt, sometimes generating misleading or en-\ntirely erroneous output. This dangerous behavior\nof attempting to appease the user with plausible-\nsounding but potentially false information is known\nas “hallucination” (Xiao and Wang, 2021; Dziri\net al., 2022). It poses a significant challenge when\ndeploying LLMs in practical applications.\nThere are several well-known approaches to cen-\nsoring LLM outputs, including: filtering with stop-\nword lists, post-processing with classifiers (Xu\net al., 2023), rewriting of toxic outputs (Logacheva\net al., 2022), and longer fine-tuning with RLHF.\nHowever, these approaches cannot be relied on to\ncompletely resolve hallucinations. Since LMs are\nnatural (if “unintentional”) liars, we propose LM-\nPolygraph — a program framework that, similar to\na human polygraph, leverages various hidden sig-\nnals to reveal when one should not trust the subject.\nIn particular, LM-Polygraph provides a compre-\nhensive collection of uncertainty estimation (UE)\ntechniques for LLMs in text generation tasks.\nUncertainty estimation refers to the process of\nquantifying the degree of confidence in the pre-\ndictions made by a machine learning model. For\nclassification and regression tasks, there is a well-\ndeveloped battery of methods (Gal, 2016). There\nhas also been a surge of work investigating UE,\nparticularly in text classification and regression\nin conjunction with encoder-only LMs such as\nBERT (Zhang et al., 2019; He et al., 2020; Shel-\nmanov et al., 2021; Xin et al., 2021; Vazhentsev\net al., 2022; Kotelevskii et al., 2022; Wang et al.,\n2022; Kuzmin et al., 2023). However, UE for se-\nquence generation tasks, including text generation,\n446\nis a much more complex problem. To quantify\nthe uncertainty of the whole sequence, we have\nto aggregate uncertainties of many individual to-\nken predictions and deal with non-trivial sampling\nand pruning techniques like beam search. Contrary\nto classification tasks where the number of possi-\nble prediction options is finite, in text generation,\nthe number of possible predictions is infinite or\nexponential in vocabulary size, complicating the\nestimation of probabilities and information-based\nscores. Finally, a natural language text is not a\nsimple sum of its tokens; it is a nuanced interpo-\nsition of context, semantics, and grammar, so two\ntexts can have very diverse surface forms but simi-\nlar meanings, which should be taken into account\nduring the UE process.\nSeveral recent studies have delved into devel-\noping UE methods for LMs in text generation\ntasks (Malinin and Gales, 2021; van der Poel et al.,\n2022; Kuhn et al., 2023; Ren et al., 2023; Vazhent-\nsev et al., 2023b; Lin et al., 2023). However, the\ncurrent landscape of this research is quite frag-\nmented with many non-comparable or even concur-\nrent studies, which makes it challenging to consoli-\ndate the findings and draw holistic conclusions.\nIn this work, with the development of LM-\nPolygraph, we strive to bridge these disparate re-\nsearch efforts, fostering more cohesion and synergy\nin the field. We envision a framework that consol-\nidates the scattered UE techniques within unified\nframeworks in Python, provides an extendable eval-\nuation benchmark, and offers tools to integrate un-\ncertainty quantification in standard LLM pipelines\nseamlessly. This endeavor will not only make the\njourney less challenging for individual researchers\nand developers but also set the stage for more ro-\nbust, reliable, and trustworthy LLM deployments\nfor end-users.\nOur contributions are as follows:\n• We provide a comprehensive framework that\nimplement state-of-the-art methods for UE of\nLM predictions. We also provide the ability to\ncombine multiple uncertainty scores together\nas suggested by Ren et al. (2023); Vazhentsev\net al. (2023a).\n• We create a tool that enriches standard LLM\nchat capabilities with uncertainty scores for\nmodel outputs. The tool can potentially be\nused by end-users to determine whether the\nanswers of language models are reliable or\nnot, and by researchers to develop novel UE\nfrom lm_polygraph import estimate_uncertainty\nfrom lm_polygraph import WhiteboxModel\nfrom lm_polygraph . estimators import *\nmodel = WhiteboxModel . from_pretrained (\n\" bigscience / bloomz -3b\",\ndevice =\" cuda :0\",\n)\nue_method = MeanPointwiseMutualInformation ()\ninput_text = \" Who is George Bush ?\"\nestimate_uncertainty ( model , ue_method , input_text )\n# Output :\n# UncertaintyOutput (\n# generation = ' President of the United States ',\n# uncertainty = -6.858096446298684)\nFigure 1: Code example of how LM predictions could be\nenriched with uncertainty scores using LM-Polygraph.\ntechniques for LMs in text generation tasks.\n• We construct an easy-to-extend benchmark\nfor UE methods in text generation tasks and\nprovide reference experimental results for im-\nplemented UE techniques.\n2 Python Library\nLM-Polygraph implements a set of state-of-the-art\nUE techniques for LLMs with unified program in-\nterfaces in Python. It is compatible with models\nfrom the Huggingface library and is tested with re-\ncent public-domain LLMs such as BLOOMz (Scao\net al., 2022; Yong et al., 2023), Dolly v2 (Conover\net al., 2023), Alpaca (Taori et al., 2023), LLaMA-\n2 (Touvron et al., 2023), and Flan-T5 (Chung et al.,\n2022). The framework supports both conditional\nmodels with a seq2seq architecture and uncondi-\ntional decoder-only LMs. Figure 1 contains a code\nexample of LM-Polygraph with BLOOMz-3B for\nUE in open-domain question answering. Some\nmethods that do not require access to the model\nitself or its logits could be used in conjunction with\nweb-hosted LLMs like ChatGPT or GPT-4 through\nAPIs. We provide a program wrapper for integra-\ntion with popular online services.\n3 Uncertainty Estimation Methods\nHere, we summarize UE methods implemented in\nLM-Polygraph, as listed in Table 1.\nThere are two major technique types: white-box\nand black-box. The white-box methods require ac-\ncess to logits, internal layer outputs, or the LM\nitself. The black-box methods require access only\nto the generated texts, and can easily be integrated\nwith third-party online services like OpenAI LM\nAPI. We note that the methods differ by compu-\ntational requirements: some techniques pose high\n447\nUncertainty Estimation Method Type Category Compute Memory\nNeed\nTraining\nData?\nMaximum sequence probability\nWhite-box Information-\nbased\nLow Low No\nPerplexity (Fomicheva et al., 2020) Low Low No\nMean token entropy (Fomicheva et al., 2020) Low Low No\nMonte Carlo sequence entropy (Kuhn et al., 2023) High Low No\nPointwise mutual information (PMI) (Takayama and Arase, 2019) Medium Low No\nConditional PMI (van der Poel et al., 2022) Medium Medium No\nSemantic entropy (Kuhn et al., 2023) White-box Meaning\ndiversity High Low No\nSentence-level ensemble-based measures (Malinin and Gales, 2021)\nWhite-box Ensembling\nHigh High Yes\nToken-level ensemble-based measures (Malinin and Gales, 2021) High High Yes\nMahalanobis distance (MD) (Lee et al., 2018)\nWhite-box Density-\nbased\nLow Low Yes\nRobust density estimation (RDE) (Yoo et al., 2022) Low Low Yes\nRelative Mahalanobis distance (RMD) (Ren et al., 2023) Low Low Yes\nHybrid Uncertainty Quantification (HUQ) (Vazhentsev et al., 2023a) Low Low Yes\np(True) (Kadavath et al., 2022) White-box Reflexive Medium Low No\nNumber of semantic sets (NumSets) (Lin et al., 2023)\nBlack-box Meaning\ndiversity\nHigh Low No\nSum of eigenvalues of the graph Laplacian (EigV) (Lin et al., 2023) High Low No\nDegree matrix (Deg) (Lin et al., 2023) High Low No\nEccentricity (Ecc) (Lin et al., 2023) High Low No\nLexical similarity (LexSim) (Fomicheva et al., 2020) High Low No\nTable 1: UE methods implemented in LM-Polygraph.\ncomputational or memory overheads, e.g., due to\nrepeated inference, making them less suitable for\npractical usage. The application of some methods\nalso can be hindered by the need for access to the\nmodel training data.\nLet us consider the input sequence x and the\noutput sequence y ∈Y of length L, where Yis\na set of all possible output sequences. Then the\nprobability of an output sequence given an input\nsequence for probabilistic autoregressive language\nmodels is given by:\nP(y |x,θ) =\n∏ L\nl=1\nP(yl |y<l,x,θ), (1)\nwhere the distribution of each yl is conditioned\non all the previous tokens in a sequence y<l =\n{y1,...,y l−1}, and θ denotes the parameters of\nthe model.\n3.1 White-box Methods\nWe start the discussion of white-box techniques\nfrom information-based methods. These tech-\nniques are based on token P(yl |y<l,x,θ) and\nsequence P(y |x,θ) probabilities obtained from\na single model prediction. The notable example is\nentropy, which can be calculated on the token or\nsequence level. The benefits of information-based\nmethods are that they are cheap to compute and\nsimple to implement. However, the quality of these\nmethods is usually relatively low, so they are con-\nsidered as baselines. Some domain-specific meth-\nods were recently proposed in an attempt to im-\nprove over standard information-based approaches,\nsuch as semantic entropy (Kuhn et al., 2023).\nThe second category of white-box techniques\nis ensemble-based methods, which leverage the\ndiversity of output predictions made by multiple\nslightly different versions of models under slightly\ndifferent conditions. Let us assume that M models\nare available with parameters θi,i = 1 ,...,M .\nThese parameters can be obtained via indepen-\ndent training of models. Then one can use token\nP(yl |y<l,x,θi) and sequence P(y |x,θi) prob-\nabilities to compute various metrics such as mutual\ninformation that measures the discrepancy between\nmodel predictions.\nDensity-based methodsleverage latent repre-\nsentations of instances and construct a probabil-\nity density on top of them. Usually, these meth-\nods approximate training data distribution with\nthe help of one or multiple Gaussian distributions.\nThey can provide a probability or an unnormalized\nscore that determines how likely instances belong\nto the training data distribution. Therefore, they\nare good at spotting out-of-distribution (OOD) in-\nstances (Vazhentsev et al., 2023b). Several varia-\ntions of these methods have been proposed in the\nliterature (Lee et al., 2018; Yoo et al., 2022; Ren\net al., 2023; Kotelevskii et al., 2022).\nThe primary advantage of these methods is that\nthey are computationally efficient: they do not\nneed much time for additional model inference,\nand memory overhead for storing additional pa-\nrameters is minimal. The drawback is that these\nmethods require access to the model’s training data\nto fit auxiliary models like Gaussians (e.g., the Ma-\nhalanobis Distance method requires constructing\n448\nFigure 2: User interface of the demo. A user can interact with an LLM as with any other chat service, but in\nLM-Polygraph the user also sees the confidence of the model answers. It is possible to specify various UE techniques\nand various models, including ChatGPT.\ndata centroids and covariance matrices). These\nmethods are also known to capture only epistemic\nuncertainty. Therefore, they might not be perfect\nfor selective generation as they cannot be used to\nspot ambiguous in-domain instances.\nFinally, we also combine information-based and\ndensity-based methods as suggested by Vazhent-\nsev et al. (2023a) and Ren et al. (2023). More\nspecifically, we implement the hybrid uncertainty\nquantification (HUQ) method (Vazhentsev et al.,\n2023a) that performs a ranking-based aggrega-\ntion and leverages strengths of both information-\nbased methods that detect ambiguous instances and\ndensity-based methods that detect OOD instances.\nDirectly asking the model to validate its an-\nswer is another option for UE (Kadavath et al.,\n2022). In this method, one asks models first to pro-\npose answers and then to evaluate the probability\nP(True) that their answers are correct. Kadavath\net al. (2022) show that it achieves reasonable per-\nformance on a variety of tasks, including question-\nanswering. We note that this method requires in-\nference of a model twice: the first to generate an\nanswer, and the second for processing its own out-\nput. Even though the second inference is usually\nfaster than the first one, it still takes considerable\ntime for computation.\n3.2 Black-box Methods\nIn contemporary models, there are instances where\nthe model’s architecture and hidden states are un-\navailable or there is no access to logits during re-\nsponse generation. Nevertheless, a whole class\nof black box methods only needs to access the\nmodel’s response. Within the scope of this paper,\nwe consider several approaches of this type that\nhave performed well in other studies (Fomicheva\net al., 2020; Kuhn et al., 2023; Lin et al., 2023).\nWe focus on Lexical Similarity, Number of Seman-\ntic Sets, Sum of Eigenvalues of the Graph Lapla-\ncian, Degree Matrix, and Eccentricity. We use the\nsame methodological approach as the authors of\n449\nthe work (Lin et al., 2023):\n• Obtain Kresponses y1,..., yK for a particu-\nlar input x.\n• Compute K×Ksimilarity matrix Sbetween\nresponses, where Sij = s(yi,yj) for some\nsimilarity score s (Natural Language Infer-\nence score or Jaccard score).\n• Based on the similarity matrix S, we compute\nthe final uncertainty score.\nThus, the idea of the methods is to analyze the\nsimilarity matrix and aggregate the information to\ncompute the uncertainty score.\n4 Demo\nWe constructed a demo application that can be used\nto interact with LLMs and also see confidence\nscores of model answers (see Figure 2). A user\nspecifies a UE method and a language model from a\nnumber of publicly-available LLMs with up to 13B\nparameters, e.g., BLOOMz, Vicuna, and LLaMA-\n2. There is also the ability to communicate with\nLLMs deployed as web services such as ChatGPT\nor GPT-4 and obtain their uncertainty scores based\non black-box techniques. For these models, a user\nshould provide an API key.\nThis demo application is potentially helpful for\nboth end-users and researchers. For end-users, it\nextends the standard AI assistant interface with\ninformation about whether it is reasonable to trust\na model answer. Researchers could use this tool\nfor qualitative analysis of various UE methods and\nLLM responses.\n5 Evaluation Benchmark\nLM-Polygraph provides a vast evaluation bench-\nmark. It contains a script for running one or multi-\nple experiments with UE techniques, implemented\nas Python modules. This feature allows the user to\neasily extend the set of available methods and evalu-\nate novel UE techniques in a unified manner. Using\nthis benchmark, we have conducted experiments\nwith most methods implemented in LM-Polygraph.\nBelow, we provide experimental details.\nDatasets. We experiment with three text genera-\ntion tasks: machine translation (MT), text summa-\nrization (TS), and question answering (QA). For\neach task, we use two widely-used datasets: WMT-\n14 German to English and WMT-14 French to En-\nglish (Bojar et al., 2014) for MT, XSum (Narayan\net al., 2018) and AESLC (Zhang and Tetreault,\n2019) for TS, and CoQA (Reddy et al., 2019) and\nbAbI QA (Dodge et al., 2016) for QA. Dataset\nstatistics are presented in Appendix D.\nModels. We conducted experiments with the\nVicuna-v1.5-7B (Zheng et al., 2023) and Llama-v2-\n7B (Touvron et al., 2023) models. The generation\nhyperparameters are provided in Appendix B.\nMetrics. We focus on the task of selective gener-\nation (Ren et al., 2023) where we “rejecting” gener-\nated sequences due to low quality based on uncer-\ntainty scores. Rejecting means that we do not use\nthe model output, and the corresponding queries\nare processed differently: they could be further\nreprocessed manually or sent to a more advanced\nLLM. Following previous work on UE in text gen-\neration (Malinin and Gales, 2021; Vazhentsev et al.,\n2022), we compare the methods using the Predic-\ntion Rejection Ratio (PRR) metric (Malinin et al.,\n2017).\nConsider a test dataset D = {(xi,yi)}. Let\nf(xi) be the output generated by an LLM and\nU(xi) be the uncertainty score of a prediction. The\nprediction rejection (PR) curve indicates the depen-\ndence of the average quality Q(f(xi),yi) of the\ncovered instances from the uncertainty rate aused\nfor rejection, in ascending order. We use ROUGE-L\nand BERTScore (Zhang et al., 2020) as text qual-\nity metrics Q(f(xi),yi). Finally, PRR computes\nthe ratio of the area AUCPRunc between the PR\ncurve for the uncertainty estimates and random es-\ntimates and the area AUCPRoracle between the\noracle and random estimates:\nPRR = AUCPRunc\nAUCPRoracle\n(2)\nHigher PRR values indicate better quality of selec-\ntive generation.\n6 Experimental Results\nTables 2 and 3 present the results for Vicuna-v1.5-\n7b and LLaMA-v2-7b correspondingly.\nFor both models, the better performance is usu-\nally demonstrated by the white-box methods based\non information-theoretic concepts (first 8 rows of\nthe table). These methods are in general also easy\nto implement and computationally lightweight,\nwith the notable exceptions of Semantic Entropy,\nMonte Carlo Sequence Entropy, and Monte Carlo\nNormalized Sequence Entropy, which require sam-\npling from the model several times to obtain uncer-\ntainty scores.\n450\nUE Method AESLC XSUM CoQA bAbiQA WMT14 De-EnWMT14 Fr-EnROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScore\nMaximum Sequence Probability0.24±0.010.19±0.100.01±0.03-0.18±0.150.35±0.010.29±0.010.66±0.020.82±0.030.32±0.010.39±0.010.25±0.010.31±0.01Perplexity 0.19±0.010.06±0.110.04±0.03-0.13±0.130.11±0.01-0.09±0.020.65±0.020.79±0.030.41±0.010.47±0.010.32±0.010.35±0.01Mean Token Entropy0.21±0.010.08±0.110.05±0.03-0.11±0.130.10±0.01-0.11±0.020.52±0.020.68±0.040.44±0.010.49±0.010.35±0.010.39±0.01Pointwise Mutual Information0.01±0.01-0.01±0.110.14±0.030.06±0.13-0.24±0.01-0.42±0.020.14±0.030.55±0.060.14±0.010.09±0.010.11±0.010.10±0.01Conditional Pointwise Mutual Information0.19±0.010.06±0.110.04±0.03-0.13±0.140.11±0.01-0.09±0.010.65±0.020.79±0.030.41±0.010.47±0.010.32±0.010.35±0.01Monte Carlo Sequence Entropy0.22±0.020.16±0.100.03±0.03-0.14±0.150.33±0.010.26±0.010.65±0.020.80±0.030.32±0.010.39±0.010.25±0.010.31±0.01Monte Carlo Normalized Sequence Entropy0.18±0.010.06±0.100.06±0.03-0.15±0.130.09±0.01-0.10±0.010.62±0.020.68±0.030.41±0.010.47±0.010.31±0.010.34±0.01Semantic Entropy 0.22±0.010.16±0.100.04±0.03-0.11±0.140.32±0.010.25±0.010.65±0.020.79±0.030.32±0.010.39±0.010.25±0.010.31±0.01P(True) -0.02±0.01-0.05±0.110.12±0.030.17±0.130.08±0.010.09±0.020.30±0.030.65±0.05-0.00±0.01-0.05±0.010.04±0.01-0.02±0.01Lexical Similarity ROUGE-10.17±0.010.15±0.110.08±0.030.01±0.130.17±0.010.13±0.020.43±0.030.58±0.040.26±0.010.28±0.010.14±0.010.13±0.01Lexical Similarity ROUGE-L0.17±0.020.15±0.110.09±0.030.00±0.130.17±0.010.13±0.010.43±0.030.58±0.040.25±0.010.28±0.010.14±0.010.15±0.01Lexical Similarity BLEU0.13±0.010.08±0.110.08±0.03-0.02±0.130.14±0.010.10±0.020.43±0.030.56±0.050.23±0.010.31±0.010.13±0.010.16±0.01NumSemSets 0.12±0.010.12±0.110.04±0.030.07±0.150.12±0.010.08±0.010.43±0.030.59±0.050.03±0.010.08±0.01-0.03±0.01-0.00±0.01EigValLaplacian NLI Score entail.0.16±0.010.12±0.110.07±0.030.02±0.130.20±0.010.16±0.010.32±0.030.53±0.050.18±0.010.24±0.010.12±0.010.14±0.01EigValLaplacian NLI Score contra.0.13±0.010.13±0.110.06±0.030.04±0.130.18±0.010.13±0.020.35±0.030.45±0.050.19±0.010.29±0.010.09±0.010.13±0.01EigValLaplacian Jaccard Score0.13±0.010.11±0.110.09±0.03-0.00±0.130.14±0.010.09±0.010.43±0.030.59±0.040.24±0.010.31±0.010.14±0.010.17±0.01DegMat NLI Score entail.0.16±0.020.15±0.110.08±0.030.06±0.130.14±0.010.06±0.010.47±0.030.55±0.050.17±0.010.32±0.010.18±0.010.27±0.01DegMat NLI Score contra.0.12±0.010.10±0.110.13±0.030.19±0.130.04±0.01-0.07±0.010.52±0.020.52±0.040.18±0.010.33±0.010.13±0.010.25±0.01DegMat Jaccard Score0.13±0.020.11±0.110.08±0.03-0.00±0.130.15±0.010.09±0.010.43±0.030.58±0.050.22±0.010.30±0.010.14±0.010.15±0.01Eccentricity NLI Score entail.0.27±0.010.18±0.110.04±0.03-0.02±0.130.35±0.010.26±0.010.43±0.020.63±0.040.27±0.010.38±0.010.18±0.010.24±0.01Eccentricity NLI Score contra.0.21±0.010.16±0.110.07±0.030.15±0.140.19±0.010.05±0.010.46±0.030.58±0.050.21±0.010.34±0.010.16±0.010.26±0.01Eccentricity Jaccard Score0.23±0.010.13±0.100.07±0.03-0.06±0.130.29±0.010.19±0.010.43±0.030.64±0.050.35±0.010.42±0.010.27±0.010.32±0.01Mahalanobis Distance - Decoder0.03±0.01-0.01±0.110.03±0.030.03±0.150.03±0.010.07±0.010.36±0.030.57±0.05-0.02±0.01-0.00±0.01-0.02±0.01-0.01±0.01Relative Mahalanobis Distance - Decoder0.02±0.010.04±0.11-0.03±0.03-0.07±0.120.03±0.010.07±0.02-0.25±0.040.04±0.08-0.09±0.01-0.08±0.01-0.06±0.01-0.05±0.01RDE - Decoder -0.03±0.01-0.05±0.110.07±0.030.09±0.130.04±0.010.09±0.020.29±0.030.42±0.06-0.01±0.01-0.02±0.01-0.01±0.01-0.02±0.01HUQ-MD - Decoder 0.19±0.010.06±0.110.03±0.03-0.10±0.140.09±0.01-0.03±0.020.62±0.020.76±0.030.29±0.010.36±0.010.22±0.010.26±0.01HUQ-RMD - Decoder0.19±0.010.06±0.110.02±0.03-0.13±0.140.09±0.01-0.03±0.010.31±0.030.60±0.050.22±0.010.26±0.010.19±0.010.21±0.01\nTable 2: PRR ↑for the Vicuna model with ROUGE-L and BERTScore as text quality metrics. Darker color indicates\nbetter results.\nUE Method AESLC XSUM CoQA bAbiQA WMT14 De-EnWMT14 Fr-EnROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScoreROUGE-L BERTScore\nMaximum Sequence Probability0.22±0.020.22±0.100.12±0.030.16±0.030.44±0.010.45±0.010.43±0.030.93±0.000.44±0.010.64±0.010.45±0.010.60±0.02Perplexity 0.12±0.020.01±0.100.13±0.03-0.04±0.030.32±0.010.18±0.010.43±0.030.93±0.000.43±0.010.46±0.010.40±0.010.41±0.02Mean Token Entropy0.13±0.010.01±0.100.13±0.04-0.06±0.030.33±0.010.16±0.010.43±0.040.99±0.000.43±0.010.42±0.010.41±0.010.37±0.02Pointwise Mutual Information-0.07±0.01-0.07±0.100.16±0.040.05±0.03-0.18±0.01-0.33±0.02-0.35±0.03-1.93±0.04-0.47±0.01-0.91±0.01-0.59±0.01-0.93±0.04Conditional Pointwise Mutual Information0.12±0.010.01±0.100.13±0.04-0.04±0.030.32±0.010.18±0.010.43±0.030.93±0.000.43±0.010.46±0.010.40±0.010.41±0.02Monte Carlo Sequence Entropy0.21±0.020.20±0.090.13±0.040.16±0.030.43±0.010.44±0.010.42±0.030.84±0.010.41±0.010.59±0.010.40±0.010.52±0.02Monte Carlo Normalized Sequence Entropy0.14±0.020.05±0.090.14±0.03-0.01±0.030.30±0.010.16±0.010.37±0.040.83±0.010.43±0.010.47±0.010.40±0.010.43±0.02Semantic Entropy 0.21±0.020.19±0.090.13±0.040.17±0.030.43±0.010.44±0.010.41±0.040.79±0.010.40±0.010.57±0.010.39±0.010.51±0.02P(True) 0.03±0.010.09±0.09-0.17±0.03-0.26±0.04-0.08±0.01-0.11±0.02-0.13±0.030.98±0.00-0.07±0.01-0.11±0.01-0.02±0.010.01±0.02Lexical Similarity ROUGE-10.18±0.020.15±0.100.16±0.030.13±0.030.29±0.010.33±0.010.15±0.040.51±0.020.39±0.010.52±0.010.38±0.010.45±0.02Lexical Similarity ROUGE-L0.16±0.020.16±0.100.16±0.040.13±0.030.29±0.010.33±0.010.15±0.040.51±0.020.38±0.010.50±0.010.37±0.010.47±0.02Lexical Similarity BLEU0.13±0.020.09±0.100.15±0.040.08±0.030.26±0.010.25±0.010.25±0.030.63±0.010.39±0.010.50±0.010.37±0.010.47±0.02NumSemSets 0.08±0.010.08±0.100.03±0.030.10±0.030.21±0.010.20±0.020.19±0.040.51±0.020.05±0.010.06±0.01-0.02±0.010.01±0.03EigValLaplacian NLI Score entail.0.19±0.010.17±0.090.10±0.030.22±0.030.27±0.010.28±0.010.04±0.030.71±0.010.32±0.010.44±0.010.29±0.010.37±0.02EigValLaplacian NLI Score contra.0.15±0.020.13±0.100.08±0.030.20±0.030.26±0.010.28±0.010.08±0.040.67±0.010.32±0.010.44±0.010.28±0.010.38±0.02EigValLaplacian Jaccard Score0.15±0.020.12±0.100.16±0.040.13±0.030.26±0.010.22±0.010.21±0.030.67±0.020.40±0.010.54±0.010.39±0.010.51±0.02DegMat NLI Score entail.0.16±0.010.16±0.090.11±0.040.23±0.030.12±0.010.00±0.010.06±0.03-0.13±0.030.34±0.010.50±0.010.33±0.010.46±0.02DegMat NLI Score contra.0.07±0.010.06±0.100.09±0.030.23±0.03-0.03±0.01-0.15±0.010.12±0.04-0.17±0.030.33±0.010.53±0.010.34±0.010.50±0.02DegMat Jaccard Score0.15±0.010.11±0.100.16±0.030.12±0.030.27±0.010.24±0.010.25±0.040.63±0.020.42±0.010.55±0.010.39±0.010.50±0.02Eccentricity NLI Score entail.0.21±0.010.18±0.100.09±0.030.22±0.030.43±0.010.42±0.010.11±0.040.74±0.010.30±0.010.41±0.010.23±0.010.29±0.02Eccentricity NLI Score contra.0.15±0.010.11±0.100.06±0.030.13±0.030.36±0.010.32±0.010.38±0.040.32±0.030.22±0.010.33±0.010.19±0.010.28±0.02Eccentricity Jaccard Score0.18±0.020.15±0.090.15±0.030.06±0.030.42±0.010.42±0.010.19±0.040.66±0.010.43±0.010.50±0.010.41±0.010.46±0.02Mahalanobis Distance - Decoder0.00±0.01-0.01±0.100.00±0.030.17±0.03-0.02±0.010.06±0.010.31±0.04-0.30±0.03-0.07±0.01-0.10±0.01-0.14±0.01-0.21±0.03Relative Mahalanobis Distance - Decoder0.03±0.010.05±0.09-0.10±0.03-0.24±0.03-0.04±0.010.05±0.01-0.25±0.030.24±0.020.01±0.010.10±0.010.17±0.010.30±0.02RDE - Decoder -0.05±0.01-0.06±0.100.04±0.030.23±0.03-0.01±0.010.08±0.010.30±0.04-0.29±0.03-0.06±0.01-0.08±0.01-0.08±0.01-0.15±0.03HUQ-MD - Decoder 0.07±0.01-0.01±0.100.13±0.03-0.04±0.030.30±0.010.17±0.010.43±0.040.93±0.000.21±0.010.19±0.010.13±0.010.06±0.03HUQ-RMD - Decoder0.11±0.020.04±0.100.13±0.03-0.04±0.030.30±0.010.17±0.010.43±0.030.93±0.000.25±0.010.30±0.010.35±0.010.42±0.02\nTable 3: PRR ↑for the LLaMA-2 model with ROUGE-L and BERTScore as text quality metrics. Darker color\nindicates better results.\nWhen working with LLMs as web services, usu-\nally there is no access to full posterior distributions\nover tokens, therefore, only black-box methods\ncould be used. Among this group of approaches,\nthe best average performance is achieved by Eccen-\ntricity for Vicuna. For LLaMA, there is no clear\nadvantage for any of the methods considered.\nOverall, we see that absolute values for all eval-\nuated methods, models, and datasets are far away\nfrom perfect. Low performance of current methods\nis especially evident on more complicated tasks\nsuch as XSum and WMT14. Our experimental\nresults demonstrate that the task of selective gener-\nation is not close to be solved. This once again un-\nderlines the importance of further research and de-\nvelopment of efficient uncertainty estimation tech-\nniques for generative language models.\n7 Conclusion\nAs the community strives to advance the potential\nof LLMs, it is critical to be mindful about dan-\ngers of their uncontrolled usage. In this work, we\npropose a tool for making the application of LLMs\nsafer. Enriching model predictions with uncertainty\nscores helps users and developers to be informed\nabout these risks, encouraging healthy skepticism\ntowards certain outputs generated by these models.\nWe plan to further expand our framework with\nimplementations of new UE methods that emerge\nin the future. We hope that our work will foster the\ndevelopment of techniques to detect and mitigate\nLLM hallucinations, which we believe is a key to\nunlocking the safe, responsible, and effective use\nof LLMs in real-world applications.\n451\nLimitations\nWe have tried to be as comprehensive as possible\nwith our collection of UE methods. However, we\nomit several techniques that have not demonstrated\nstrong performance in previous work, do not have\na strong theoretical motivation, or are similar to\nother implemented techniques.\nWe note that comprehensive evaluation of UE\nmethods is an open research question. LM-\nPolygraph makes the first steps to systematize, and\nprovide interfaces and tools for testing UE tech-\nniques in a unified manner. However, we believe\nthat the number of tasks and datasets should be\nextended in the future.\nWhen running the demo, we cannot provide an\naccess to the biggest and the most powerful public\nLLMs, because running them is prohibitively ex-\npensive. Nevertheless, a user can access models\nsuch as ChatGPT by providing an API access key.\nLM-Polygraph supports common application\nprogram interfaces used by modern LLMs. How-\never, it is possible that certain modifications will\nbe required to support future releases of LLMs.\nAt the moment of writing, LM-polygraph pro-\nvides valid uncertainty estimates only for model\noutputs in English language. This is due to the fact\nthat most generation quality metrics implemented\nare based off English-specific implementations and\nnon-multilingual models. We plan to alleviate this\nlimitation by allowing the user to easily employ\ncustom quality metrics and scoring models.\nEthics Statement\nWe conducted all experiments on publicly-available\ndatasets that have been leveraged in various previ-\nous work on uncertainty estimation of LLMs.\nWhile training data for most LLMs, such as\nBLOOMz, was selected to contain little or no abu-\nsive text content, such models can still potentially\noutput harmful textual content. Techniques inves-\ntigated in our work estimate certainty of an LM\noutput to “censor” its output, and model debias-\ning is an orthogonal direction to our line of work.\nThese additional methods can and perhaps should\nbe combined in real production LLM deployments.\nWe hope that our framework contributes to safer\nand more reliable usage of language models.\nAcknowledgements\nWe thank the anonymous reviewers for their in-\nsightful feedback towards improving this paper.\nReferences\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Ale s Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nJesse Dodge, Andreea Gane, Xiang Zhang, Antoine\nBordes, Sumit Chopra, Alexander H. Miller, Arthur\nSzlam, and Jason Weston. 2016. Evaluating prerequi-\nsite qualities for learning end-to-end dialog systems.\nIn 4th International Conference on Learning Repre-\nsentations, ICLR 2016, San Juan, Puerto Rico, May\n2-4, 2016, Conference Track Proceedings.\nNouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and\nSiva Reddy. 2022. On the origin of hallucinations\nin conversational models: Is it the datasets or the\nmodels? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5271–5285, Seattle, United States.\nAssociation for Computational Linguistics.\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya,\nFrédéric Blain, Francisco Guzmán, Mark Fishel,\nNikolaos Aletras, Vishrav Chaudhary, and Lucia Spe-\ncia. 2020. Unsupervised quality estimation for neural\nmachine translation. Transactions of the Association\nfor Computational Linguistics, 8:539–555.\nYarin Gal. 2016. Uncertainty in Deep Learning. Ph.D.\nthesis, University of Cambridge.\nJianfeng He, Xuchao Zhang, Shuo Lei, Zhiqian Chen,\nFanglan Chen, Abdulaziz Alhamadani, Bei Xiao, and\nChang-Tien Lu. 2020. Towards more accurate uncer-\ntainty estimation in text classification. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 8362–8372. Associa-\ntion for Computational Linguistics.\n452\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know. CoRR, abs/2207.05221.\nNikita Kotelevskii, Aleksandr Artemenkov, Kirill\nFedyanin, Fedor Noskov, Alexander Fishkov, Artem\nShelmanov, Artem Vazhentsev, Aleksandr Petiushko,\nand Maxim Panov. 2022. Nonparametric uncertainty\nquantification for single deterministic neural network.\nIn Advances in Neural Information Processing Sys-\ntems, volume 35, pages 36308–36323. Curran Asso-\nciates, Inc.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nIn The Eleventh International Conference on Learn-\ning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net.\nGleb Kuzmin, Artem Vazhentsev, Artem Shelmanov,\nXudong Han, Simon Suster, Maxim Panov, Alexan-\nder Panchenko, and Timothy Baldwin. 2023. Uncer-\ntainty estimation for debiased models: Does fairness\nhurt reliability? In Proceedings of the 13th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, pages 744–770, Nusa Dua, Bali. Association\nfor Computational Linguistics.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple unified framework for detecting out-\nof-distribution samples and adversarial attacks. In\nAdvances in Neural Information Processing Systems\n31: Annual Conference on Neural Information Pro-\ncessing Systems 2018, NeurIPS 2018, December 3-8,\n2018, Montréal, Canada , volume 31, pages 7167–\n7177.\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023.\nGenerating with confidence: Uncertainty quantifica-\ntion for black-box large language models. CoRR,\nabs/2305.19187.\nVarvara Logacheva, Daryna Dementieva, Sergey\nUstyantsev, Daniil Moskovskiy, David Dale, Irina\nKrotova, Nikita Semenov, and Alexander Panchenko.\n2022. ParaDetox: Detoxification with parallel data.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 6804–6818, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAndrey Malinin and Mark J. F. Gales. 2021. Uncertainty\nestimation in autoregressive structured prediction. In\n9th International Conference on Learning Represen-\ntations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net.\nAndrey Malinin, Anton Ragni, Kate Knill, and Mark\nGales. 2017. Incorporating uncertainty into deep\nlearning for spoken language assessment. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 45–50, Vancouver, Canada. Association\nfor Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018 , pages 1797–1807. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-\nhammad Saleh, Balaji Lakshminarayanan, and Pe-\nter J Liu. 2023. Out-of-distribution detection and\nselective generation for conditional language mod-\nels. In The Eleventh International Conference on\nLearning Representations.\nPeter J Rousseeuw. 1984. Least median of squares\nregression. Journal of the American statistical asso-\nciation, 79(388):871–880.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\n453\nMatthias Gallé, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Benoît Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Laurençon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. 2022. BLOOM:\nA 176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nArtem Shelmanov, Evgenii Tsymbalov, Dmitri Puzyrev,\nKirill Fedyanin, Alexander Panchenko, and Maxim\nPanov. 2021. How certain is your Transformer? In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1833–1840, Online.\nAssociation for Computational Linguistics.\nJunya Takayama and Yuki Arase. 2019. Relevant and\ninformative response generation using pointwise mu-\ntual information. In Proceedings of the First Work-\nshop on NLP for Conversational AI, pages 133–138,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. CoRR, abs/2307.09288.\nLiam van der Poel, Ryan Cotterell, and Clara Meis-\nter. 2022. Mutual information alleviates hallucina-\ntions in abstractive summarization. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5956–5965, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nArtem Vazhentsev, Gleb Kuzmin, Artem Shelmanov,\nAkim Tsvigun, Evgenii Tsymbalov, Kirill Fedyanin,\nMaxim Panov, Alexander Panchenko, Gleb Gusev,\nMikhail Burtsev, Manvel Avetisian, and Leonid\nZhukov. 2022. Uncertainty estimation of transformer\npredictions for misclassification detection. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 8237–8252, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nArtem Vazhentsev, Gleb Kuzmin, Akim Tsvigun,\nAlexander Panchenko, Maxim Panov, Mikhail Burt-\nsev, and Artem Shelmanov. 2023a. Hybrid uncer-\ntainty quantification for selective text classification\nin ambiguous tasks. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 11659–\n11681, Toronto, Canada. Association for Computa-\ntional Linguistics.\nArtem Vazhentsev, Akim Tsvigun, Roman Vashurin,\nSergey Petrakov, Daniil Vasilev, Maxim Panov,\nAlexander Panchenko, and Artem Shelmanov. 2023b.\nEfficient out-of-domain detection for sequence to se-\nquence models. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 1430–\n1454, Toronto, Canada. Association for Computa-\ntional Linguistics.\nYuxia Wang, Daniel Beck, Timothy Baldwin, and Karin\nVerspoor. 2022. Uncertainty estimation and reduc-\ntion of pre-trained models for text regression. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:680–696.\nYijun Xiao and William Yang Wang. 2021. On hal-\nlucination and predictive uncertainty in conditional\nlanguage generation. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n2734–2744, Online. Association for Computational\nLinguistics.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. The art of abstention: Selective prediction and\nerror regularization for natural language processing.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1040–1051, Online. Association for Computational\nLinguistics.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Mari-\nanna J. Martindale, and Marine Carpuat. 2023. Un-\nderstanding and detecting hallucinations in neural\nmachine translation via model introspection. Trans-\nactions of the Association for Computational Linguis-\ntics, 11:546–564.\n454\nZheng Xin Yong, Hailey Schoelkopf, Niklas Muen-\nnighoff, Alham Fikri Aji, David Ifeoluwa Adelani,\nKhalid Almubarak, M. Saiful Bari, Lintang Sutawika,\nJungo Kasai, Ahmed Baruwa, Genta Indra Winata,\nStella Biderman, Edward Raff, Dragomir Radev, and\nVassilina Nikoulina. 2023. BLOOM+1: adding lan-\nguage support to BLOOM for zero-shot prompting.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 11682–11703. Association for Computa-\ntional Linguistics.\nKiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak.\n2022. Detection of adversarial examples in text clas-\nsification: Benchmark and baseline via robust density\nestimation. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 3656–3672,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nRui Zhang and Joel R. Tetreault. 2019. This email\ncould save your life: Introducing the task of email\nsubject line generation. In Proceedings of the 57th\nConference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers , pages 446–456.\nAssociation for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nXuchao Zhang, Fanglan Chen, Chang-Tien Lu, and\nNaren Ramakrishnan. 2019. Mitigating uncertainty\nin document classification. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3126–3136, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\nCoRR, abs/2306.05685.\n455\nA Methods Description\nHere, we summarize UE methods implemented in\nLM-Polygraph; see also Table 1.\nA.1 White-box Methods\nA.1.1 Information-based methods\nMaximum sequence probability score simply lever-\nages the probability of the most likely sequence\ngeneration: MSP(y |x,θ) = 1 −P(y |x,θ).\nLength-normalized log probability computes the\naverage negative log probability of generated to-\nkens. If the score is exponentiated it corresponds\nto perplexity. The resulting quantity is computed\nas\nP(y,x; θ) = exp\n{\n−1\nLlog P(y |x,θ)\n}\n,\nwhile it is convenient also to denote length-\nnormalized sequence probability by ¯P(y |x,θ) =\nexp\n{\n1\nL log P(y |x,θ)\n}\n.\nWe also provide the mean token entropy, where\nwe simply average entropy of each individual token\nin the generated sequence:\nHT(y,x; θ) = 1\nL\n∑ L\nl=1\nH(yl |y<l,x,θ),\nwhere H(yl |y<l,x,θ) is an entropy of the token\ndistribution P(yl |y<l,x,θ).\nThe other possibility to compute entropy-based\nuncertainty measure is to compute it on the level of\nwhole sequences via E\n[\n−log P(y |x,θ)\n]\n, where\nexpectation is taken over the sequencesy randomly\ngenerated from the distribution P(y |x,θ). In\npractice, one needs to use Monte-Carlo integration,\ni.e. generate several sequences y(k), k= 1,...,K\nvia randoms sampling and compute the resulting\nMonte Carlo Sequence Entropy:\nHS(x; θ) = −1\nK\n∑ K\nk=1\nlog P(y(k) |x,θ).(3)\nThe same procedure can be done by substituting\nP(y(k) |x,θ) with its length-normalized version\n¯P(y(k) |x,θ) leading to a more reliable uncer-\ntainty measure in some applications.\nAnother entropy-based uncertainty measure is\nSemantic Entropy proposed by Kuhn et al. (2023).\nThe method aims to deal with the generated se-\nquences that have similar meaning while hav-\ning different probabilities according to the model,\nwhich can significantly affect the resulting entropy\nvalue (3). The idea is to cluster generated se-\nquences y(k), k= 1,...,K into several semanti-\ncally homogeneous clusters Cm, m= 1,...,M\nwith M ≤K with bi-directional entailment algo-\nrithm and average the sequence probabilities within\nthe clusters. The resulting estimate of entropy is\ngiven by the following formula:\nSE(x; θ) = −\n∑ M\nm=1\nˆPm(x; θ) log ˆPm(x; θ),\nwhere ˆPm(x; θ) = 1\n|Cm|\n∑\ny∈Cm P(y |x,θ).\nFinally, one can consider negative mean Point-\nwise Mutual Information (PMI; Takayama and\nArase (2019)) which is given by\nPMI(y,x; θ) = 1\nL\n∑ L\nl=1\nlog P(yl |y<l,θ)\nP(yl |y<l,x,θ).\nThis method was extended in (van der Poel et al.,\n2022) by considering only those marginal proba-\nbilities for which the entropy of the conditional\ndistribution is above certain threshold: H(yl |\ny<l,x,θ) ≥ τ. It leads to the negative mean\nConditional Pointwise Mutual Information (CPMI)\nmeasure that is given by:\nCPMI(y,x; θ) = −1\nL\n∑ L\nl=1\nlog P(yl |y<l,x,θ)\n+ λ\nL\n∑\nl: H(yl|y<l,x,θ)≥τ\nlog P(yl |y<l,θ),\nwhere λ> 0 is another tunable parameter.\nA.1.2 Ensemble-based methods\nFor the ensembling on a sequence level, we con-\nsider two uncertainty measures: total uncertainty\nmeasured via average sequence probability ¯P(y |\nx) = 1\nM\n∑M\ni=1 ¯P(y |x,θi):\nMSPS(y,x) = 1 −¯P(y |x) (4)\nand\nMS(y,x) = 1\nM\n∑ M\ni=1\nlog P(y |x)\nP(y |x,θi), (5)\nwhich is known as reverse mutual information\n(RMI).\nNext we discus token level uncertainty measures\nand start with a total uncertainty estimate via en-\ntropy:\nHT(y,x) =\n∑ L\nl=1\nH(yl |y<l,x), (6)\n456\nwhere H(yl |y<l,x) is an entropy of the token\ndistribution P(yl | y<l,x) = 1\nM\n∑M\ni=1 P(yl |\ny<l,x; θi).\nAdditionally, for the ensemble one can compute\nthe variety of other token level uncertainty mea-\nsures including average entropy of ensemble mem-\nbers (also known as Data Uncertainty):\nD(yl |y<l,x) = 1\nM\n∑ M\ni=1\nH(yl |y<l,x,θi),\nMutual Information (MI):\nI(yl |y<l,x) = H(yl |y<l,x) −D(yl |y<l,x)\nand Expected Pairwise KL Divergence (EPKL):\nK(yl |y<l,x) =\n(M\n2\n)−1\n·\n·\n∑\ni̸=j\nKL\n(\nP(yl |y<l,x,θi) ∥P(yl |y<l,x,θj)\n)\n,\nwhere KL(P ∥Q) refers to a KL-divergence be-\ntween distributions P and Q.\nFinally, Reverse Mutual Information (RMI) also\ncan be computed on the token level via a simple\nequation\nM(yl |y<l,x) = K(yl |y<l,x) −I(yl |y<l,x).\nThe resulting token-level uncertainties computed\nvia Data Uncertainty, MI, EPKL and RMI can\nbe plugged-in in equation (6) on the place of en-\ntropy leading to corresponding sequence level un-\ncertainty estimates.\nA.1.3 Density-based Methods\nLet h(x) be a hidden representation of an instance\nx. The Mahalanobis Distance (MD; Lee et al.\n(2018)) method fits a Gaussian centered at the train-\ning data centroid µwith an empirical covariance\nmatrix Σ. The uncertainty score is the Mahalanobis\ndistance between h(x) and µ:\nMD(x) =\n(\nh(x) −µ\n)TΣ−1(\nh(x) −µ\n)\n.\nWe suggest using the last hidden state of the en-\ncoder averaged over non-padding tokens or the last\nhidden state of the decoder averaged over all gen-\nerated tokens as h(x).\nThe Robust Density Estimation (RDE; Yoo et al.\n(2022)) method improves over MD by reducing\nthe dimensionality of h(x) via PCA decomposi-\ntion. Additionally, computing of the covariance\nmatrix Σ for each individual class is done by us-\ning the Minimum Covariance Determinant estima-\ntion (Rousseeuw, 1984). The uncertainty score is\ncomputed as the Mahalanobis distance between but\nin the space of reduced dimensionality.\nRen et al. (2023) showed that it might be use-\nful to adjust the Mahalanobis distance score by\nsubtracting from it the other Mahalanobis distance\nMD0(x) computed for some large general purpose\ndataset covering many domains like C4 (Raffel\net al., 2020). The resulting resulting Relative Ma-\nhalanobis Distance score is\nRMD(x) = MD(x) −MD0(x).\nA.2 Black-box Methods\nIn this work, we follow Lin et al. (2023) and con-\nsider two approaches to compute the similarity for\nthe generated responses. The first one is Jaccard\nsimilarity:\ns(y,y′) = |y ∩y′|\n|y ∪y′|,\nwhere the sequences y and y′are considered just\nas sets of words.\nThe other similarity measure considered is Natu-\nral Language Index (NLI) which employs a classifi-\ncation model to identify whether two responses\nare similar. We follow Kuhn et al. (2023) and\nuse the DeBERTa-large model (He et al., 2021)\nthat, for each pair of input sequences, provides\ntwo probabilities: ˆpentail(y,y′) that measures the\ndegree of entailment between the sequences and\nˆpcontra(y,y′) that measures the contradiction be-\ntween them. Then one can use sentail(y,y′) =\nˆpentail(y,y′) or scontra(y,y′) = 1 −ˆpcontra(y,y′)\nas a measure of similarity between sequences y\nand y′.\nNumber of Semantic Sets illustrates whether\nanswers are semantically equivalent. We adopt\nan iterative approach by sequentially examining\nresponses from the first to the last while mak-\ning pairwise comparisons between them (each\npair has indexes j1 and j2, j2 > j1). The\nnumber of semantic sets initially equals the to-\ntal number of generated answers K. If the con-\ndition ˆpentail(yj1 ,yj2 ) > ˆpcontra(yj1 ,yj2 ) and\nˆpentail(yj2 ,yj1 ) > ˆpcontra(yj2 ,yj1 ) is fulfilled we\nput this two sentences into one cluster. The compu-\ntation is done for all the pairs of answers, and then\nthe resulting number of distinct sets UNumSemSets\n457\nis reported. It is worth noting that a higher number\nof semantic sets corresponds to an increased level\nof uncertainty, as it suggests a higher number of\ndiverse semantic interpretations for the answer.\nNonetheless, it is essential to acknowledge a lim-\nitation of this measure: it can only take integer val-\nues. Additionally, it cannot be assumed that the se-\nmantic equivalence derived from the NLI model is\nalways transitive. Consequently, the authors of (Lin\net al., 2023) suggest the consideration of a contin-\nuous counterpart of this metric. They propose the\nSum of Eigenvalues of the Graph Laplacian as a\npotential alternative approach.\nLet’s consider a similarity matrix Sj1j2 =(\ns(yj1 ,yj2 ) + s(yj2 ,yj1 )\n)\n/2. Averaging is done\nto obtain better consistency. Normalized Graph\nLaplacian of the obtained similarity Matrix Shas\nthe following formula L= I−D−1\n2 SD−1\n2 , where\nD is a diagonal matrix and Dii = ∑K\nj=1 Sij.\nConsequently, the following formula is derived:\nUEigV = ∑K\nk=1 max(0,1 −λk). This value is a\ncontinuous analogue of UNumSemSets. In extreme\ncase if adjacency matrix Sis binary these two mea-\nsures will coincide.\nOf course, from a theoretical and practical point\nof view, UEigV is a much more flexible approach\ncompared to UNumSemSets. Still, they have a com-\nmon disadvantage: they can not provide uncertainty\nfor each answer. However, authors of (Lin et al.,\n2023) demonstrate that we can take it from Degree\nMatrix D computed above. The idea is that the\ntotal uncertainty of the answers might be measured\nas a corrected trace of the diagonal matrix Dbe-\ncause elements on the diagonal of matrix D are\nsums of similarities between the given answer and\nother answers. Thus, it is an average pairwise dis-\ntance between all answers, and a larger value will\nindicate larger uncertainty because of the larger dis-\ntance between answers. The resulting uncertainty\nmeasure becomes UDeg = 1 −trace(D)/K2.\nA drawback of previously considered methods\nis the limited knowledge of the actual embedding\nspace for the different answers since we only have\nmeasures of their similarities. Nevertheless, we can\novercome this limitation by taking advantage of\nthe inferential capabilities of the graph Laplacian,\nwhich makes it easier to obtain the coordinates of\nthe answers. Let us introduce u1,..., uk ∈RK as\nthe eigenvectors of Lthat correspond to ksmall-\nest eigenvalues. We can efficiently construct an\ninformative embedding vj = [u1,j,..., uk,j] for\nan answer yj. Authors of (Lin et al., 2023) demon-\nstrate that this approach allows the usage of the\naverage distance from the center as an uncertainty\nmetric and to consider the distance of each response\nfrom the center as a measure of (negative) confi-\ndence. In mathematical terms, the estimates for\nEccentricity can be defined as follows: UEcc =[˜vT\n1 ,..., ˜vT\nK]\n\n2, where ˜vj = vj −1\nK\n∑K\nℓ=1 vℓ.\nLast but not least,Lexical Similarity is a measure\nproposed by (Fomicheva et al., 2020) that computes\nhow similar two words or phrases are in terms of\ntheir meaning. Since the original article is dedi-\ncated to machine translation, this measure calcu-\nlates the average similarity score between all pairs\nof translation hypotheses in a set, using a similarity\nmeasure based on the overlap of their lexical items.\nDifferent metrics can be used, such as ROUGE-1,\nROUGE-2, ROUGE-L, and BLEU. For our task,\nthis measure iterates over all responses and calcu-\nlates the average score with other answers.\n458\nB Generation Hyperparameters\nDataset Task Max Input LengthGeneration LengthTemperatureTop-pDo SampleBeamsRepetition Penalty\nAESLC ATS\n2048\n31\n1.0 1.0 False 1 1\nXSUM 56\nCoQA QA 20\nbAbiQA 3\nWMT14 De-EnNMT 107\nWMT14 Fr-En 107\nTable 4: Text generation hyperparameters for both LLMs Vicuna-v1.5-7b and Llama-2-7b used in the experiments.\nTable 4 presents the hyperparameters used for experiments with LLMs Vicuna-v1.5-7b and LLaMA-2-\n7b-hf on various datasets and tasks. Maximum length of generated sequence was set for each dataset as\nthe 99th percentile of target sequence length on the respecitve train set.\nC Text Generation Quality Metrics\nAESLC XSUM CoQA bAbiQA WMT14 De-EnWMT14 Fr-En\nRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScore\n0.24 0.83 0.18 0.86 0.29 0.85 0.68 1.0 0.59 0.95 0.64 0.95\nTable 5: Rouge-L ↑and BERTScore↑for Vicuna v1.5 model for various tasks.\nAESLC XSUM CoQA bAbiQA WMT14 De-EnWMT14 Fr-En\nRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScoreRouge-L BERTScore\n0.23 0.84 0.19 0.86 0.51 0.91 0.36 0.98 0.54 0.93 0.56 0.92\nTable 6: Rouge-L ↑and BERTScore↑for the Llama v2 model for various tasks.\nD Dataset Statistics\nTable 7 illustrates the statistics of the datasets that were used in the experiments. Experiments were\nconducted using all examples from the test sets of these datasets, while training density-based methods\nwere performed on a random subset of 1000 elements from the train set.\nHYDRA_CONFIG =/ path /to/ cloned / repo / examples / configs / polygraph_eval_coqa . yaml polygraph_eval model = lmsys /\nvicuna -7b-v1 .5\nFigure 3: Script that reproduces benchmark results for CoQA dataset with Vicuna-v1.5-7b model.\nTo evaluate the performance of considered uncertainty estimation methods, we provide code to retrieve\nbenchmark results. Figure 3 shows an example of starting an experiment with the Vicuna-v1.5-7b model\non the Questions Answering task (CoQA dataset).\nFigure 4 shows an example of a config file used for experiment related to CoQA dataset with Vicuna-\nv1.5-7b model. It contains information about import and parameters. For other datasets and models the\nconfig structure is the same.\nE Normalization of Uncertainty Estimates in Demo App\nTo make uncertainty estimation more intuitive for the end user, directly interacting with the LLM, we\nperform normalization of various uncertainty estimates. After normalization the output UE(x) of any\nuncertainty estimation approach becomes a confidence score C(x) ∈[0,1] ⊂R.\nWe experimented with several ways of achieving this normalization, including quantile-based approach\nand simple linear normalization on maximum value obtained from validation dataset. Eventually we\n459\nDataset Num. instances Av. document len. Av. target len. Language\nNMT\nWMT’14 4.51M / 3000 /3003 19.8 / 18.3 23.0 / 21.3 German-to-English\nWMT’14 40.8M / 3000 /3003 33.5 / 32.1 29.2 / 27.0 French-to-English\nATS\nXSum 204045 / 11332 /11334 454.6 26.1 English\nAESLC 14436 / 1960 /1906 165.5 6.7 English\nQA\nCoQA 7199 / 500 / - 271.4 2.7 English\nbAbiQA 2000 / - / 200 31.1 1.0 English\nTable 7: Quantitative information regarding the datasets from experiments. It includes the count of instances\navailable for the training, validation, and test sets, as well as the mean lengths of both texts and targets (answers /\ntranslations / summaries) measured in terms of tokens. In addition, the languages of the source and target texts are\nalso specified.\nhydra :\nrun :\ndir : ${ cache_path }/${ task }/${ model }/${ dataset }/${ now:%Y -%m -%d}/${ now:%H -%M -%S}\ncache_path : ./ workdir / output\nsave_path : '${ hydra : run . dir }'\ndevice : cpu\ntask : qa\ndataset : coqa\ntext_column : questions\nlabel_column : answers\nprompt : \" Answer a question given a story . Output only the answer .\\ nStory :\\n{ story }\\n\\ nQuestion :\\n{ question }\\\nn\\ nAnswer :\\n\"\ntrain_split : train\neval_split : validation\nmax_new_tokens : 20\nload_from_disk : false\ntrain_dataset : null\ntrain_test_split : false\ntest_split_size : 1\nbackground_train_dataset : allenai /c4\nbackground_train_dataset_text_column : text\nbackground_train_dataset_label_column : url\nbackground_train_dataset_data_files : en/c4 - train .00000 - of -01024. json .gz\nbackground_load_from_disk : false\nsubsample_background_train_dataset : 1000\nsubsample_train_dataset : 1000\nsubsample_eval_dataset : -1\nmodel : lmsys / vicuna -7b-v1 .5\nuse_auth_token :\nuse_density_based_ue : true\nuse_seq_ue : true\nuse_tok_ue : false\nignore_exceptions : false\nbatch_size : 1\ndeberta_batch_size : 10\nseed :\n- 1\nFigure 4: Config Example for Question Answering on CoQA dataset.\nperformed normalization as a calibration procedure, where normalized confidence score represents\nexpected value of generation quality metric of choice (i.e. RougeL) for a given uncertainty estimate.\nThis expectation is estimated by computing sample averages of quality metric over bins of uncertainty\nestimates, calculated for some validation dataset. For RougeL metric, the confidence estimate C(xinput)\nthus becomes:\n460\nC(xinput) = 1\n|B|\n∑\nxi,yi∈B\nrougeL(ˆyi,yi),\nwhere ˆyi is model output for input xi, and\nB= {(x,y) ∈Dcalib |UE(x) ∈[UEmin,UEmax)}\nis the bin to which uncertainty estimate of the input belongs. The bounds of this bin are selected from the\npredetermined set of bin boundaries to be the neighboring pair for which condition\nUEmin ≤UE(xinput) <UEmax\nis satisfied.\nThis dataset Dcalib is constructed to be representative of different modes of operation of a given model.\nFor this purpose it is constructed as a mixture of several different datasets for different tasks, with different\nvalues of relevant statistics, such as input sequence length, typical generated output length etc.\nIt is obvious that quality of this normalized confidence score depends heavily on the size and diversity\nof the calibration dataset. In general we consider the problem of translating opaque uncertainty estimates\ninto intuitive absolute confidence scores, that correctly represent likelihood of the generated output being\ncorrect and relevant, as an important and complicated task. We leave solving this problem in a more\nefficient and universal way to the future work.\n461"
}