{
    "title": "Infrared Small-Dim Target Detection with Transformer under Complex Backgrounds",
    "url": "https://openalex.org/W3202696561",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4365208077",
            "name": "Liu, Fangcen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743694585",
            "name": "Gao Chenqiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2073014071",
            "name": "Chen Fang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2387573070",
            "name": "Meng, Deyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A263648189",
            "name": "Zuo, Wangmeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A923581926",
            "name": "Gao, Xinbo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2947066337",
        "https://openalex.org/W3118249006",
        "https://openalex.org/W3086144474",
        "https://openalex.org/W2058928435",
        "https://openalex.org/W3046064342",
        "https://openalex.org/W2344310038",
        "https://openalex.org/W2947125416",
        "https://openalex.org/W3098726713",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3174377922",
        "https://openalex.org/W3097913527",
        "https://openalex.org/W2918460136",
        "https://openalex.org/W3125738281",
        "https://openalex.org/W2134789565",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3113124545",
        "https://openalex.org/W2780677785",
        "https://openalex.org/W2900684139",
        "https://openalex.org/W3048644861",
        "https://openalex.org/W3118934234",
        "https://openalex.org/W3124866053",
        "https://openalex.org/W2407220925",
        "https://openalex.org/W2341998679",
        "https://openalex.org/W2338318173",
        "https://openalex.org/W2969331247",
        "https://openalex.org/W1992873714",
        "https://openalex.org/W3014054456",
        "https://openalex.org/W3112389165",
        "https://openalex.org/W2999295839",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3170227631",
        "https://openalex.org/W3174320978",
        "https://openalex.org/W3092535672",
        "https://openalex.org/W3010079414",
        "https://openalex.org/W3093164896",
        "https://openalex.org/W3175985340",
        "https://openalex.org/W2061208604",
        "https://openalex.org/W2768489488",
        "https://openalex.org/W3202406646",
        "https://openalex.org/W2950740535",
        "https://openalex.org/W2998409107",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2734434825",
        "https://openalex.org/W3194894013",
        "https://openalex.org/W1978993121",
        "https://openalex.org/W3106753828",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2472688184",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2790021535",
        "https://openalex.org/W3176590546",
        "https://openalex.org/W3034512672",
        "https://openalex.org/W2080197848",
        "https://openalex.org/W3165924482",
        "https://openalex.org/W2986554926",
        "https://openalex.org/W3183676354"
    ],
    "abstract": "The infrared small-dim target detection is one of the key techniques in the infrared search and tracking system. Since the local regions similar to infrared small-dim targets spread over the whole background, exploring the interaction information amongst image features in large-range dependencies to mine the difference between the target and background is crucial for robust detection. However, existing deep learning-based methods are limited by the locality of convolutional neural networks, which impairs the ability to capture large-range dependencies. Additionally, the small-dim appearance of the infrared target makes the detection model highly possible to miss detection. To this end, we propose a robust and general infrared small-dim target detection method with the transformer. We adopt the self-attention mechanism of the transformer to learn the interaction information of image features in a larger range. Moreover, we design a feature enhancement module to learn discriminative features of small-dim targets to avoid miss detection. After that, to avoid the loss of the target information, we adopt a decoder with the U-Net-like skip connection operation to contain more information of small-dim targets. Finally, we get the detection result by a segmentation head. Extensive experiments on two public datasets show the obvious superiority of the proposed method over state-of-the-art methods and the proposed method has stronger cross-scene generalization and anti-noise performance.",
    "full_text": "1\nInfrared Small-Dim Target Detection with\nTransformer under Complex Backgrounds\nFangcen Liu, Chenqiang Gao*, Fang Chen, Deyu Meng, Member, IEEE, Wangmeng Zuo, Senior Member, IEEE,\nXinbo Gao, Senior Member, IEEE\nAbstract‚ÄîThe infrared small-dim target detection is one of the\nkey techniques in the infrared search and tracking system. Since\nthe local regions similar to infrared small-dim targets spread\nover the whole background, exploring the interaction information\namongst image features in large-range dependencies to mine\nthe difference between the target and background is crucial for\nrobust detection. However, existing deep learning-based methods\nare limited by the locality of convolutional neural networks,\nwhich impairs the ability to capture large-range dependencies.\nAdditionally, the small-dim appearance of the infrared target\nmakes the detection model highly possible to miss detection. To\nthis end, we propose a robust and general infrared small-dim\ntarget detection method with the transformer. We adopt the self-\nattention mechanism of the transformer to learn the interaction\ninformation of image features in a larger range. Moreover, we\ndesign a feature enhancement module to learn discriminative\nfeatures of small-dim targets to avoid miss detection. After\nthat, to avoid the loss of the target information, we adopt\na decoder with the U-Net-like skip connection operation to\ncontain more information of small-dim targets. Finally, we get the\ndetection result by a segmentation head. Extensive experiments\non two public datasets show the obvious superiority of the\nproposed method over state-of-the-art methods and the proposed\nmethod has stronger cross-scene generalization and anti-noise\nperformance.\nIndex Terms‚ÄîTransformer, infrared small-dim target, detec-\ntion\nI. I NTRODUCTION\nThe infrared small-dim target detection is one of the key\ntechniques in the infrared search and tracking (IRST) system\nbecause the infrared imaging can capture targets from a long\ndistance and has a strong anti-interference ability [1‚Äì3]. How-\never, this task encounters kinds of challenges [4], as illustrated\nin Fig. 1. Infrared targets in infrared images are small and dim,\nwhile backgrounds are usually complex. As a result, the small-\ndim target is easily submerged in the complex background,\n*Corresponding author: Chenqiang Gao.\nFangcen Liu, Chenqiang Gao, Xinbo Gao are with the School of Com-\nmunication and Information Engineering, Chongqing University of Posts\nand Telecommunications, and also with Chongqing Key Laboratory of\nSignal and Information Processing, Chongqing University of Posts and\nTelecommunications, Chongqing 400065, China (e-mail: liufc67@gmail.com,\ngaocq@cqupt.edu.cn, gaoxb@cqupt.edu.cn).\nFang Chen is with the Viterbi School of Engineering, University of Southern\nCalifornia, California 90089, USA (e-mail: fchen905@usc.edu)\nDeyu Meng is with School of Mathematics and Statistics, Xi‚Äôan Jiaotong\nUniversity, Xi‚Äôan, Shanxi, 710049, China, and also with Macau Institute of\nSystems Engineering, Macau University of Science and Technology, Taipa,\n999078, Macau (e-mail: dymeng@mail.xjtu.edu.cn).\nWangmeng Zuo is with School of Computer Science and Technology,\nHarbin Institute of Techonlogy, 47822 Harbin, Heilongjiang, China (e-mail:\nwmzuo@hit.edu.cn).\n(a) Building background (b) Cloudy background (c) Sea-sky background\nFig. 1: Illustration of challenges of the infrared small-dim tar-\nget detection task under complex backgrounds. These targets\nin different scenes appear pretty small-dim and sparse, which\nmakes the detection model easy to miss detection. From (a)\nto (c), we can also obviously observe that the local regions\nsimilar to infrared small-dim targets spread over the whole\nbackground, which easily leads to high false alarms.\nwith a low Signal-to-Clutter Ratio (SCR). In addition, the\nnumber of target pixels is much fewer than background pixels,\nwhich leads to that the target and background pixels in an\nimage are of extreme imbalance.\nTo address above challenges, model-driven infrared small-\ndim target detection methods routinely design the detection\nmodel for small-dim targets by deeply mining the prior knowl-\nedge of imaging characteristics of small-dim targets, back-\ngrounds, or both of them [5‚Äì7]. However, these approaches\nheavily rely on prior knowledge, which makes their generaliza-\ntion ability limited. Additionally, model-driven approaches can\nnot be fast and easily applied to a new application scene whose\nimaging characteristic does not well match the assumption of\nthe model. In contrast, data-driven models are more feasible\nand can easily adapt to a new application scene through sample\nlearning. In recent years, deep learning methods are adopted\nto detect infrared small-dim targets and have shown stronger\ngeneralization ability and feasibility [4, 8].\nHowever, among existing deep learning methods, fea-\nture learning mainly relies on convolutional neural networks\n(CNNs). The locality of CNNs impairs the ability to capture\nlarge-range dependencies [9], which easily results in high false\nalarms. As can be observed from Fig. 1, the local regions\nsimilar to infrared small-dim targets spread over the whole\nbackground. Thus, it is very important to learn the difference\nbetween the target and the background in a large range.\nCurrently, the transformer structure, from the Natural Lan-\nguage Processing (NLP) Ô¨Åeld [10, 11], has demonstrated\nits powerful ability in non-local feature learning in various\ncomputer vision tasks [12, 13]. Different from CNN architec-\narXiv:2109.14379v2  [cs.CV]  11 Nov 2021\n2\ntures, the transformer architecture contains the self-attention\nmechanism and the feed-forward network. The self-attention\nmechanism enables the transformer to have the ability to\ncapture large-range dependencies of all embedded tokens.\nIn this paper, we adopt the transformer to learn the inter-\naction information amongst all embedded tokens of an image.\nFirstly, we embed an image into a sequence of tokens by\nthe Resnet-50 [14]. After that, the self-attention mechanism\nis adopted to model complex dependencies among different\nembedded tokens, so that the difference between the small-\ndim target and background can be well mined.\nFurthermore, due to the small size and the dim appearance\nof the target, as can be seen from Fig. 1, if we can not\ncapture discriminative information of small-dim targets, it\nhighly possibly lend to miss detection. To this end, we design\na feature enhancement module as the feed-forward network\nto acquire more discriminative features of small-dim targets.\nMoreover, since small-dim target features are easily lost in the\nnetwork, we adopt a U-Net-like [15] upsampling structure to\nget more information of small-dim targets.\nWe summarize the main contributions of the paper as\nfollows:\n‚Ä¢ We propose a novel small-dim target detection method. It\nadopts the self-attention mechanism of the transformer to\nlearn the interaction information amongst all embedded\ntokens so that the network can learn the difference\nbetween the small-dim target and background in a larger\nrange. To our best knowledge, this is the Ô¨Årst work to\nexplore the transformer to detect the infrared small-dim\ntarget.\n‚Ä¢ The designed feature enhancement module can help learn\nmore discriminative features of small-dim targets.\n‚Ä¢ We evaluate the proposed method on two public datasets\nand extensive experimental results show that the proposed\nmethod is effective and signiÔ¨Åcantly outperforms state-of-\nthe-art methods.\nThe remainder of this paper is organized as follows: In\nSection II, related works are brieÔ¨Çy reviewed. In Section III,\nwe present the proposed method in detail. In Section IV, the\nexperimental results are given and discussed. Conclusions are\ndrawn in Section V.\nII. R ELATED WORK\nA. Small target detection\n1) Infrared small-dim target detection:In the early stages,\nthe model-driven infrared small-dim target detection methods\ndesign Ô¨Ålters to enhance the target or suppress the background\n[16, 17]. Zeng et al. [18] and Deshpande et al. [19] proposed\nthe Top-Hat method and max-mean/max-median method to\ndirectly enhance targets by Ô¨Åltering them out from original\nimages, respectively. Then, Deng et al. [17] proposed an\nadaptive M-estimator ring top-hat transformation method to\ndetection the small-dim target. Aghaziyarati et al. [20] and\nMoradi et al.[21] detected small-dim targets by suppressing the\nestimated background as much as possible. However, detection\nperformances of these methods are limited by designed Ô¨Ålters.\nInspired by the human visual system, some efforts are based\non the different local contrast which focus on the saliency of\nthe target to distinguish the target from the background and\nimprove the performance of small-dim infrared target detection\n[22‚Äì24]. Deng et al. [23] and Gao et al. [24] focused on\nthe saliency of the target to distinguish the target from the\nbackground. Cui et al. [25] proposed an infrared small-dim\ntarget detection algorithm with two layers which can balance\nthose detection capabilities. The Ô¨Årst layer was designed to\nselect signiÔ¨Åcant local information. Then the second layer\nleveraged a classiÔ¨Åer to separate targets from background\nclutters. Based on the observation that small-dim infrared\ntargets often present sparse features, while the background\nhas the non-local correlation property [26, 27]. Gao et al.\n[7] Ô¨Årstly proposed the patch-image in infrared small-dim\ntarget detection and the low-rank-based infrared patch-image\n(IPI) model. Dai et al. [28] proposed a column weighted\nIPI model (WIPI), and then proposed a reweighed infrared\npatch-tensor model (RIPT) [29]. However, the model-driven\napproaches heavily rely on prior knowledge, which makes the\ngeneralization ability of these models limited.\nRecently, the generalization ability of the data-driven in-\nfrared small-dim target detection is well promoted by deep\nlearning methods [30‚Äì37]. Fan et al. [38] designed a convo-\nlutional neural network architecture to improve the contrast\nbetween the small-dim target and the background. Zhao et al.\n[39] proposed a TBC-Net which included a target extraction\nmodule and semantic constraint module. The target extraction\nmodule aimed to predict the potential target, while the se-\nmantic constraint module aimed to constrain the number of\nsmall-dim targets. Wang et al. [4] used adversarial generation\nnetworks (GANs) to balance Miss Detection (MD) and False\nAlarm (FA). Shi et al. [40] and Zhao et al. [30] regarded\nsmall-dim targets as noises, and they treated the small-dim\ntargets detection task as a denoising task. Hou et al. [41] com-\nbined handcrafted feature methods and convolutional neural\nnetworks and proposed a robust RISTDnet framework. Dai\net al. [8, 42] combined the global and local information of\nthe infrared image and proposed end-to-end detection methods\nnamed ALC and ACM to solve the problem of the lack of Ô¨Åxed\nfeatures of infrared small-dim targets.\nCompared with above data-driven methods, the proposed\nmethod in this paper overcomes the shortcoming of the limi-\ntation of the CNN to learn the interaction information amongst\nall embedded tokens, which can mine differences between\ntargets and backgrounds in a larger range.\n2) Small target detection in RGB images:Different from\ninfrared small-dim target methods, small target detection meth-\nods for RGB images payed more attention to solving the\nproblem of small target size [43, 44], and usually adopted data\naugmentation [45], multi-scale learning [46] and context infor-\nmation learning [47] strategies to improve the robustness and\ngeneralization of detection. However, using the above methods\ndirectly to detect infrared small-dim targets would make the\nperformance drop sharply, as veriÔ¨Åed in [4]. Compared with\ntargets in RGB images, infrared small-dim targets have high\nsimilarity to the background with low SCR, so it is more\ndifÔ¨Åcult to distinguish small-dim targets from backgrounds.\nMoreover, the max-pooling layer adopted in these methods\n3\nFeature\nEmbedding\nCompound Encoder\n‚Ä¶ FEM\n√ók\nLN MSA LN Decoder\n( )LN aE\nPosition\nQ\nK\nV\nLN :  Layer Normalization  \nFc1, Fc2:   Full connection\nConv:  Convolution\nResultConfidence \nMap y\nSegment\n‚Ä¶\n‚Ä¶\nConv\n‚Ä¶\n‚Ä¶\n‚Ä¶\n2D Tokens aF ( )Reshape convFEnhanced\nFeatures\nconvF\n‚Ä¶\nFc1 Fc2\nEmbedded \nTokensxInput\nInteraction \nTokens aE\n_alF2D Tokens\nCompact\nFeatures F E\nUpsampling\nFeatures\n3DF3D Features\n_alE\nCompound \nTokens\nupF\nFig. 2: The proposed infrared small-dim target detection framework in this paper. It contains three parts: a feature embedding\nmodule, a compound encoder with k encoder layers, and a decoder. The feature embedding module is proposed to obtain\nthe compact feature representation. In the compound encoder, each encoder layer mainly has two parts: the multi-head self-\nattention (MSA) mechanism and the feature enhancement module (FEM). The multi-head self-attention is adopted to learn the\ninteraction information amongst all embedded tokens. The designed feature enhancement module can learn more discriminative\nfeatures of small-dim targets. The decoder is adopted to obtain the conÔ¨Ådence map of the small-dim target. Finally, we obtain\nthe detection result through the adaptive threshold segmentation.\nmay suppress or even eliminate features of infrared small-dim\ntargets [48].\nB. Combining self-attention mechanisms with CNNs\nInspired by the success of the self-attention mechanism\nadopted in transformer architectures in the NLP Ô¨Åeld [10, 11],\nsome works employed them in the computer vision Ô¨Åeld [49‚Äì\n51].\nVision Transformer (ViT) is the pioneering work that di-\nrectly applied a transformer architecture on non-overlapping\nmedium-sized image patches for image classiÔ¨Åcation [13].\nLiu et al. [52] proposed a hierarchical Transformer structure\nnamed swin transformer. Such hierarchical architecture had the\nÔ¨Çexibility to model at various scales and had linear computa-\ntional complexity with respect to image size. The most obvious\nadvantage of the transformer is its ability to capture the image\ninformation through the self-attention mechanism in a large\nrange. However, the self-attention mechanism‚Äôs performance\nin local information learning is relatively weak compared\nwith CNN-based methods. Hence, some methods proposed to\ncombine the strengths of CNNs and self-attention mechanisms\n[53‚Äì55]. Carion et al. [56] adopted Resnet-50 or Resnet-101\n[14] to acquire the compact feature representation, and then\nintroduced this representation into the self-attention mecha-\nnism. Liu et al. [52] proposed a general-purpose transformer\nbackbone for computer vision. D‚ÄôAscoli et al. [9] proposed\nthe gated positional self-attention to mimic the locality of\nconvolutional layers. Chen et al. [57] combined self-attention\nmechanism with U-net [15] for medical segmentation.\nDifferent from these methods, the proposed method in this\npaper focuses on the small size of the target, and designs\na feature enhancement module to learn more discriminative\nfeatures of small-dim targets.\nIII. P ROPOSED METHOD\nA. Overview\nAs depicted in Fig. 2, the proposed method consists of three\nmain modules: (1) A feature embedding module to extract a\ncompact feature representation of an image. (2) A compound\nencoder to learn interaction information amongst all embedded\ntokens and more discriminative features of small-dim targets.\n(3) A decoder to produce conÔ¨Ådence maps.\nGiven an image of size C √óH √óW, we embed it into a\nsequence of tokens by the Resnet-50 [14]. Then the designed\ncompound encoder is used to learn the interaction information\namongst all embedded tokens and capture more discriminative\nfeatures of small-dim targets. After that, with the help of the\nU-Net-like [15] skip connection operation, embedded features\nare concatenated with feature maps obtained by the decoder\nto obtain the conÔ¨Ådence map. Finally, we adopt the adaptive\nthreshold [7] to segment the conÔ¨Ådence map to obtain the\ndetection result.\nB. CNN-based feature embedding module\nIn ViT, an image is divided into a sequence of non-\noverlapping patches of the same size and then use the linear\nmapping to embed these patches to a sequence of tokens [13].\nIn this paper, the proposed method adopts the Resnet-50 [14]\nas the feature embedding module to extract compact features\nof the original image, and then reshapes them into a sequence\nof tokens.\nAfter the input image x‚ààRC√óH√óW passes through the fea-\nture embedding module, compact features F ‚ààRC1√óH‚Ä≤√óW‚Ä≤\nwith local information are obtained. Then we Ô¨Çatten 3D\nfeatures F into 2D tokens Eem ‚ààRH‚Ä≤W‚Ä≤√óC1 , where H‚Ä≤W‚Ä≤\nis the number of tokens. To maintain the spatial information\nof these features, we learn speciÔ¨Åc position embeddings Epos.\n4\nFinally, we obtain embedded tokens E = Eem + Epos, where\nE ‚ààRn√óC1 and E = (E1,E2,¬∑¬∑¬∑ ,En), n is the number of\ntokens, and n= H‚Ä≤W‚Ä≤.\nC. Compound encoder\nThe compound encoder contains k encoder layers, and all\nencoder layers have the same structure. An encoder layer\nincludes a multi-head self-attention module with m heads\nand a feature enhancement module. The multi-head self-\nattention mechanism aims to capture the interaction informa-\ntion amongst ntokens so that differences between targets and\nbackgrounds can be well constructed. The feature enhance-\nment module aims to learn more discriminative features of\nsmall-dim targets.\n1) Multi-head self-attention module: Embedded tokens E\nare divided into m heads E =\n{\nE1,E2,¬∑¬∑¬∑ ,Em}\n, Ej ‚àà\nRn√óC1\nm , and then fed into the multi-head self-attention module\nMSA(¬∑) to obtain interaction tokens Ea, we deÔ¨Åne these\nprocesses as:\nEa = MSA (LN (E)) +E, (1)\nwhere the LN(¬∑) is the layer normalization.\nIn each head, the multi-head self-attention module MSA(¬∑)\ndeÔ¨Ånes three learnable weight matrices to transform Queries\n(WQ ‚ààRn√óC1\nm ), Keys ( WK ‚ààRn√óC1\nm ) and Values ( WV ‚àà\nRn√óC1\nm ). The embedded tokens Ej of a head are Ô¨Årst projected\nonto these weight matrices to get Qj = EjWQ, Kj = EjWK\nand Vj = EjWV . The output Zj ‚àà Rn√óC1\nm of the self-\nattention layer is given by:\nZj = softmax\nÔ£´\nÔ£≠QjKjT\n‚àö\nC1\nm\nÔ£∂\nÔ£∏Vj, (2)\nwhere j is the j-th head of the multi-head self-attention. The\nresult of m heads can be expressed as:\nZ =\n{\nZ1,Z2,¬∑¬∑¬∑ ,Zm}\n,Z ‚ààRn√óC1 (3)\n2) Feature enhancement module: We feed interaction to-\nkens Ea into the designed feature enhancement module to\nobtain compound tokens Ea l.\nSpeciÔ¨Åcally, the feature enhancement module is shown\nin Fig. 2. Firstly, these interaction tokens Ea are fed into\nthe Ô¨Årst full connection layer to obtain 2D tokens Fa =\n(Fa 1,Fa 2,¬∑¬∑¬∑ ,Fa n), Fa ‚ààRn√óC2 . Then we reshape 2D\ntokens into 3D features F3D with the size of n√óP √óP and\nadopt the convolution operation to learn the local information\nof F3D, which helps enhance the features of small-dim targets.\nFinally, enhanced features Fconv are further reshaped back to\nthe size of n√óC2 and then fed into the next full connection\nlayer to learn the next 2D tokens Fa l. After that, with the\nsummation of Fa l and Ea, we obtain compound tokens\nEa l ‚ààRn√óC1 .\nD. Feature decoder with skip connection\nTo obtain conÔ¨Ådence maps of small-dim targets, we adopt\na decoder to upsample reshaped compound tokens Fup ‚àà\nRH‚Ä≤√óW‚Ä≤√óC1 . To prevent the loss of the small-dim target\ncontextual information, all features in the feature embedding\nprocess are concatenated with feature maps obtained by up-\nsampling operation through skip connection operation like U-\nNet structure [15]. Then, the conÔ¨Ådence map y is obtained by\nthe sigmoid function. Finally, we adopt the adaptive threshold\n[7] for target segmentation, and achieve the detection result.\nE. Loss function\nTo handle the class imbalance issue between targets and\nbackgrounds [8] and focus more on small-dim target regions,\nthe Intersection of Union (IoU) loss is adopted to calculate\nthe distance between the conÔ¨Ådence map and the ground truth,\ndeÔ¨Åned by:\nLiou = 1‚àíy‚à©xgt\ny‚à™xgt\n. (4)\nThe yis the conÔ¨Ådence map, and the xgt is the ground truth\nimage.\nIV. E XPERIMENT\nIn this section, we Ô¨Årst introduce datasets, evaluation met-\nrics, and implementation details, respectively. Then we com-\npare the proposed method with state-of-the-art methods. Fi-\nnally, we conduct the ablation study, evaluate the performance\non cross-scene generalization, and verify the proposed method\nhas a stronger anti-noise performance.\nA. Experimental setup\n1) Datasets: We adopt the widely used MFIRST dataset\n[4] and SIRST dataset [42] to evaluate the proposed method.\nThe MFIRST dataset contains 9960 training samples and 100\ntest samples. Among them, all infrared small-dim target image\nsamples are generated by the random combination of real\nbackgrounds and real small-dim targets or simulated targets\nwith Gaussian spatial gray distribution. The SIRST dataset is\na widely-used public dataset that contains 341 training samples\nand 86 test samples.\nFig.3 shows representative images of these two datasets. In\nthese datasets, small-dim targets usually appear in the sea, sky,\nmountains, or buildings background. Compared with MFIRST\ndataset, small-dim targets in SIRST dataset have some shape\ninformation.\n2) Evaluation metrics: As the same as [7], we regard that\nthe detection is correct when the following two conditions are\nmet simultaneously: (1) The output result has some overlap\npixels with the ground truth. (2) The pixel distance between\nthe centers of the detection result and the ground truth is less\nthan a threshold (4 pixels).\nIn this paper, four widely used evaluation metrics [4, 7],\nincluding the Probability of detection ( Pd), False alarm ( Fa)\nrate, target-level F1 measure (Ft\n1) and pixel-level F1 measure\n5\nTABLE I: Comparison on different datasets. ‚Äò-‚Äô means that the method can not get reasonable values under Ô¨Åxed Fa = 0.2 for\nPd or under Fa ‚©Ω 2.0 for AUC.\nMethods MFIRST SIRST Times\n(s/100 images)Pd (%) AUC(%) F t\n1 (%) F p\n1 (%) Pd (%) AUC(%) F t\n1 (%) F p\n1 (%)\nTop-Hat [18] - - 44.62 12.8 85.34 82.38 82.52 44.13 1.78\nMax-Mean/Max-Media [19] - 50.50 58.30 14.44 78.46 77.45 73.49 23.97 1.50\nAAGD [20] 43.66 56.96 65.70 32.42 89.09 88.14 84.69 50.27 3.52\nADMD [21] 59.64 64.09 70.99 31.52 94.13 90.46 88.50 56.69 2.02\nLIG [58] 59.29 64.17 70.87 41.27 90.19 90.00 89.72 59.15 70.44\nIPI [7] 41.59 51.02 60.73 33.58 86.87 84.45 85.32 56.97 424.60\nILCM [59] - - 24.52 0.91 - - 47.26 0.71 1.92\nMPCM [60] 57.86 64.62 72.20 35.43 93.56 90.40 86.96 58.59 4.60\nTLLCM [22] - 46.43 52.63 6.67 61.61 79.14 79.66 7.60 321.91\nLEF [61] 49.49 70.01 72.45 5.87 - - 59.60 2.45 430.22\nGST [24] 56.39 59.69 66.67 24.67 77.01 76.81 80.40 35.32 1.05\nMDvsFA [4] 86.62 81.78 85.27 60.36 - - - - 10.62\nACM [42] 70.07 71.95 82.11 58.05 98.24 91.67 96.78 81.30 1.61\nOurs 90.08 89.34 92.59 64.59 100.00 99.14 98.62 83.16 6.41\nMFIRST [4]\nSIRST [42]\nFig. 3: Representative samples of datasets.\n(Fp\n1 ) are used for performance evaluation. These evaluation\nmetrics are deÔ¨Åned as follows:\nPd = # number of true detections\n# number of real targets , (5)\nFa = #number of false detections\n# number of images . (6)\nAs the same as [7], we adopt Pd with Fa=0.2 and the area\nunder Pd ‚àíFa curve (AUC) with Fa <2.0 to evaluate average\nperformance of the proposed method.\nFt\n1 = 2 √óPrecisiont √óRecallt\nPrecisiont + Recallt\n, (7)\nFp\n1 = 2 √óPrecisionp √óRecallp\nPrecisionp + Recallp\n, (8)\nwhere the precision and recall can be deÔ¨Åned as follows:\nPrecision = TP\nTP + FP , (9)\nRecall= TP\nTP + FN , (10)\nwhere TP is the true positive, FP is the false positive and\nFN is the false negative.\nThe Pd can measure the correct detection rate of the model\nwhen Fa = 0.2. The AUC can reÔ¨Çect the false alarm rate of\nthe model. A model with a low false alarm rate will have a\nhigher AUC value.\n3) Implementation details: The framework of the proposed\nmethod is implemented using Pytorch 1.7.1, and accelerated\nby CUDA 11.2. The whole network is trained with the SGD\nalgorithm with a learning rate of 0.01, momentum 0.9, and\nweight decay 1e-4 on NVIDIA GeForce RTX 3090 GPU. All\ntrainable methods are trained from the scratch and the batch\nsize is set to 24. The input image is resized to 224 √ó224.\nIn terms of the results of the hyperparameter discussion in\nthe ablation study, the head of the MSA is set to 12, and the\nnumber of the encoder layer is set to 12.\nB. Comparison with the state-of-the-art methods\nWe compare the proposed method with following related\nmethods:\n‚Ä¢ Model-driven methods: Top-Hat [18], Max-Mean/Max-\nMedian [19], AAGD [20], ADMD [21], LIG [58], IPI\n[7], ILCM [59], MPCM [60], TLLCM [22], LEF [61],\nGST [24].\n‚Ä¢ Deep learning methods: MDvsFA [4] and ACM [42].\nSince the open-source code of the deep-learning-based\ninfrared small-dim target detection method is limited, we adopt\nthe open-source MDvsFA and ACM methods for performance\ncomparison.\n1) Quantitative evaluation: The results of different meth-\nods on two public datasets [4, 42] are listed in Table I.\nFrom this table, we can observe that deep-learning-based\nmethods are signiÔ¨Åcantly superior to model-driven methods.\nThe proposed method outperforms both model-driven methods\nand deep-learning-based methods.\nOn the MFIRST dataset, compared with the MDvsFA\nmethod, the proposed method outperforms approximately\n3.46% on Pd, 7.56% on AUC, 7.32% on Ft\n1 and 4.23% on Fp\n1 .\nBecause the MDvsFA method is difÔ¨Åcult to balance outputs of\ntwo detection generators (There are two generators G1 and G2.\nG1 aims to minimize MD, G2 aims to minimize FA), and this\n6\nFig. 4: The Ft\n1 and Fp\n1 curves with different thresholds and the Pd ‚àíFa ROC curve on MFIRST dataset.\nmethod does not focus on exploring the difference between the\ntarget and the background. Compared with the ACM method,\nthe proposed method also outperforms a lot. The ACM ap-\nproach can not adequately learn the interaction information\namongst image features, because the locality of CNNs impairs\nthe ability to capture large-range dependencies. The proposed\nmethod not only learns the interaction information amongst\nall embedded tokens, but also extracts more discriminative\nfeatures of small-dim targets from a local view. Consequently,\nthe proposed method obtains the best performance.\nOn the SIRST dataset, compared with the state-of-the-art\nACM method, the proposed method outperforms approxi-\nmately 1.76% on Pd, 7.47% on AUC, 1.84% on Ft\n1 and 1.86%\non Fp\n1 . The ACM method is limited by the locality of convolu-\ntional neural networks, while the proposed method adopts the\nself-attention mechanism which can learn the relationships of\none token with others in a larger region so that the interaction\ninformation amongst all embedded features of an image can\nbe better constructed. It is worth noting that the GANs-based\nMDvsFA method is difÔ¨Åcult to converge on SIRST datasets,\nso we do not show the detection result.\nThe Ft\n1, Fp\n1 and Pd‚àíFa ROC curves of some representative\nmethods on MFIRST dataset are shown in Fig. 4. As we can\nsee from the Ô¨Ågure, the proposed method always has better\nperformance than other methods at different thresholds. These\nresults validate the robustness of the proposed method.\n2) Qualitative evaluation: Fig. 5 shows qualitative evalu-\nation comparisons of Ô¨Åve representative methods. It can be\nseen that all targets in infrared images with different complex\nbackgrounds appear small-dim and sparse, but the proposed\nmethod can achieve the best detection performance. When\ndetecting the small-dim target with a high noise level, a low\nSCR, and with building shelters, most methods, including\nADMA, MDvsFA, and ACM have some false alarms. In these\ncases, the local regions similar to infrared small-dim targets\nspread over the whole background, and these methods do not\nfocus on exploring the difference between the small-dim target\nand the background. When detecting the small-dim target with\nbright clutters, the MDvsFA method fails to detect the right\ntarget. When the infrared target is pretty dim and the contrast\nbetween the background and the target is extremely weak,\nsome methods are difÔ¨Åcult to acquire robust detection. This\ncan be seen in the last row of Fig. 5 where the small-dim\ntarget is hidden in the clouds. These methods can not learn the\nTABLE II: The performance of the module ablation on differ-\nent datasets.\nMethods MFIRST SIRST\nPd (%) AUC(%) Pd (%) AUC(%)\nBaseline 80.00 80.76 99.78 97.98\nBaseline-pool 87.05 84.77 99.90 99.00\nBaseline-pool+FEM\n(Ours) 90.08 89.34 100.00 99.14\ndifference between the small-dim target and the background\nadequately. Besides, they may lose some discriminative feature\nof small-dim targets during feature extraction. In contrast,\nthe proposed method can learn the interaction information\namongst embedded tokens in a larger range, and focus on\nlearning more discriminative features of small-dim targets,\nso it has the best detection results in all kinds of complex\nbackgrounds.\nC. Ablation study\nIn this section, we validate the effect of the compound\nencoder which helps learn the interaction information amongst\nall embedded tokens and more discriminative features of\nsmall-dim targets. The results are shown in Table II. The ‚Äòbase-\nline‚Äô method [57] for comparison consists of three modules: a\nResnet-50 structure with a pooling layer [14], a self-attention\nencoder that has the same structure as the ViT [13] model,\na decoder with skip connection. The ‚Äòbaseline-pool‚Äô method\nremoves the pooling layer from the Resnet-50 structure, and\nonly retains the IoU loss. The ‚Äòbaseline-pool+FEM‚Äô method\nis the proposed method in this paper.\nExperimental results in Table II obviously show the pro-\nmotion effect of each component on infrared small-dim target\ndetection. As can be seen, the baseline method with the normal\ntransformer encoder can achieve reasonable results. When we\nremove the pooling layer, all metrics improve a lot. These\nresults show that the pooling layer can degrade the feature\nlearning of small-dim targets, which is the same as [48].\nWhen we adopt a feature enhancement module (FEM) in\nthe compound encoder with the self-attention mechanism, the\nperformances are further improved. On MFIRST dataset, the\nPd is improved by 3.03%, and the AUC is improved by 4.57%.\nOn SIRST dataset, the Pd is improved by 0.10%, and the\nAUC is improved by 0.14%. It shows that the self-attention\n7\nGround TruthInputs MDvsFA [4] Ours\nIPI [7]\nADMD [21]\n ACM [42]\nDim Target Building Shelter Low SCRBright Clutters Noise\nFig. 5: The representative processed results of different methods. The Ô¨Årst row to the last row shows the results of detecting\na small-dim target with a low SCR, with a high noise level, with bright clutters, with complex building shelters, and with a\ndim appearance, respectively.\nmechanism helps capture the interaction information amongst\nembedded tokens to learn the difference between the target\nand background from a large region. Furthermore, the local\ninformation learned by the feature enhancement module helps\nlearn more discriminative features of small-dim targets.\nD. Hyperparameter discussion\nThe number of encoder layers and the head number of the\nMSA module inÔ¨Çuence the effect of learning the interaction\ninformation amongst all embedded tokens. In this section, we\ninvestigate these important hyperparameters, the results can be\nseen in Table III.\nAs can be observed in Table III, as the number of encoder\nlayers increases, the time consumption of detection increases.\nAs the number of encoder layers decreases, the dependency\nbetween the target and background can not be constructed\nadequately. The head number of MSA has little effect on\nperformance. Finally, based on the best experimental perfor-\nmance, we set the number of the encoder layers to 12, and the\nhead number of the MSA module to 12.\nTABLE III: The performance of the hyperparameter ablation\non the MFIRST dataset\nEncoder\nlayers Head Pd(%) AUC(%) Times\n(s/100 images)\n14 3 84.39 84.38 6.62\n14 12 85.71 85.08 6.61\n12 3 87.14 83.11 6.41\n12 12 90.08 89.34 6.41\n6 3 85.00 84.26 5.43\n6 12 85.71 85.52 5.33\nE. Cross-scene generalization\nIn practice, the well-trained model is likely to be applied to a\nnew scene, so the cross-scene generalization is very important.\nConsequently, we evaluate the generalization of the proposed\nmodel on cross-scene datasets. The experimental results are\nlisted in Table IV.\nAs can be seen from Table IV, these experiments verify\nthe strong generalization of deep learning methods. However,\ncompared with other deep learning methods, the proposed\nmethod achieves better performance in terms of cross-scene\ngeneralization. When the model is trained on the MFIRST\n8\nTABLE IV: The performance of different methods in terms of\ncross-scene generalization. ‚Äò-‚Äô means that the method can not\nget reasonable values.\nMethods\nMFIRST(Train)\nSIRST(Test)\nSIRST(Train)\nMFIRST(Test)\nPd(%) AUC(%) Pd(%) AUC(%)\nMDvsFA [4] 97.22 93.95 - -\nACM [42] 92.13 57.49 67.25 62.05\nours 98.15 96.02 87.77 85.58\ndataset and tested on the SIRST dataset, the proposed method\noutperforms approximately 0.93% on Pd and 2.07% on AUC,\ncompared with the state-of-the-art MDvsFA method. When\nthe model is trained on the SIRST dataset and tested on the\nMFIRST dataset, the proposed method outperforms approxi-\nmately 20.52% on Pd and 23.53% on AUC, compared with\nthe state-of-the-art ACM method. Although the distribution of\nthe dataset is heterogeneous, the differences between infrared\nsmall-dim targets and background in different datasets are sim-\nilar. The self-attention mechanism helps capture the interaction\ninformation amongst embedded tokens to learn the difference\nbetween the target and background from a large region, so it\nhas the best detection results under the cross-scene situation.\nF . Evaluation of anti-noise performance\nIn addition to cross-scene generalization, the anti-noise\nperformance is also very crucial. Thus, we evaluate the anti-\nnoise performance of the proposed model on the MFIRST\ndataset. We set four kinds of Gaussian noise with different\nvariances, which are 5.10, 7.65, 12.75, and 25.50, respectively.\nThe mean of these Gaussian noises is set to 0. It‚Äôs worth noting\nthat we only add the above noise to the test set.\nWhen the variances are 0.00, 5.10, 7.65, and 12.75, the\nexperimental results are showed in Fig. 6, 7, 8, and 9. As\nwe can see from these Ô¨Ågures, the detection performance\ndecreases with the increase of noise. However, compared with\nother deep learning methods, the proposed method has the best\n82.11\n76.09\n64.96\n48.23\n85.27\n72.97\n68.45\n47.11\n92.59\n86.60\n82.78\n74.14\nFig. 6: The Ft\n1 performance of different methods under Gaus-\nsian noises with different variances. ‚Äò œÉ2‚Äô means the variance\nof Gaussian noise.\n58.05\n53.36\n50.09\n42.95\n60.36\n53.8452.64\n40.82\n64.59\n61.8559.90\n54.88\nFig. 7: The Fp\n1 performance of different methods under\nGaussian noises with different variances.\n70.07\n55.47\n48.41\n35.62\n86.62\n69.34\n63.49\n22.86\n90.08\n82.54\n73.55\n63.04\nFig. 8: The Pd performance of different methods under Gaus-\nsian noises with different variances.\n71.95\n60.47\n49.91\n31.65\n81.78\n67.34\n61.64\n36.57\n89.34\n81.16\n73.35\n63.83\nFig. 9: The AUC performance of different methods under\nGaussian noises with different variances.\n9\nGround Truthùúé2 = 25.50Original image\nFig. 10: From left to right, the Ô¨Årst column is the original\nimage, the second column is the image after adding Gaussian\nnoise, and the third column is the ground truth. ‚Äò œÉ2‚Äô means\nthe variance of Gaussian noise.\nperformance under Gaussian noise with different variances.\nMeanwhile, with the increase of noise variance, the perfor-\nmance of the proposed method decreases relatively slowly over\nother methods. These experiments verify the proposed method\nhas stronger anti-noise performance.\nWhen the variances is 25.50, the Fig. 10 illustrates some\nsamples of inputs. As we can see from this Ô¨Ågure, the\nsmall-dim targets are completely submerged in backgrounds,\nand we can hardly Ô¨Ånd out small-dim targets. The Table\nV shows the detection performance of different methods,\nand we can see from the table that the performance of\nall methods deteriorates dramatically. Even so, the proposed\nmethod still has the best experimental performance. Fig. 11\nshows the detection performance of different methods when\nthe variance of Gaussian noise is 25.50. As we can observe\nfrom the Ô¨Ågure, when the small-dim target is not submerged\nin backgrounds completely, the ACM method tends to miss\ndetection, while the MDvsFA network tends to increase false\nalarms. In contrast, the proposed method can robustly detect\nsmall-dim targets.\nV. C ONCLUSION\nIn this paper, we propose a new infrared small-dim target\ndetection framework. We adopt the multi-head self-attention\nTABLE V: The detection performance of different methods\nwhen the variance of the Gaussian noise is 25.50.\nMethods Ft\n1(%) Fp\n1 (%) Pd AUC\nACM [42] 22.76 22.65 13.90 13.85\nMDvsFA [4] 24.05 23.04 18.81 18.37\nOurs 47.40 41.67 34.88 35.67\nOursMDvsFA [4]ACM [42]Inputs\n Ground Truth\nFig. 11: The detection performance when the variance of the\nGaussian noise is 25.50.\nmodule to explore the interaction information amongst all\nembedded tokens, and thus differences between targets and\nbackgrounds can be well learned. In addition, the feature\nenhancement module is designed to learn more discriminative\nfeatures of small-dim targets. Experiments on two public\ndatasets show that compared with state-of-the-art methods, the\nproposed method performs much better on detecting infrared\nsmall-dim targets with complex backgrounds. Additionally,\nexperimental results also show the proposed method has a\nstronger generalization and a better anti-noise performance.\nREFERENCES\n[1] H. Zhu, J. Zhang, G. Xu, and L. Deng, ‚ÄúBalanced ring\ntop-hat transformation for infrared small-target detection\nwith guided Ô¨Ålter kernel,‚Äù IEEE Trans. Aerosp. Electron.\nSyst., vol. 56, no. 5, pp. 3892‚Äì3903, 2020.\n[2] T. Zhang, Z. Peng, H. Wu, Y . He, C. Li, and C. Yang, ‚ÄúIn-\nfrared small target detection via self-regularized weighted\nsparse model,‚Äù Neurocomputing, vol. 420, pp. 124‚Äì148,\n2021.\n[3] R. Lu, X. Yang, W. Li, J. Fan, D. Li, and X. Jing,\n‚ÄúRobust infrared small target detection via multidirec-\ntional derivative-based weighted contrast measure,‚ÄùIEEE\nGeosci. Remote. Sens. Lett., early access, Oct. 06, 2020,\ndoi:10.1109/LGRS.2020.3026546.\n[4] H. Wang, L. Zhou, and L. Wang, ‚ÄúMiss detection vs. false\nalarm: Adversarial learning for small object segmentation\nin infrared images,‚Äù inProc. IEEE Int. Conf. Comput. Vis.\n(ICCV), pp. 8509‚Äì8518, 2019.\n[5] H. Deng, X. Sun, M. Liu, C. Ye, and X. Zhou, ‚ÄúInfrared\nsmall-target detection using multiscale gray difference\nweighted image entropy,‚Äù IEEE Trans. Aerosp. Electron.\nSyst., vol. 52, no. 1, pp. 60‚Äì72, 2016.\n10\n[6] J. Han, K. Liang, B. Zhou, X. Zhu, J. Zhao, and L. Zhao,\n‚ÄúInfrared small target detection utilizing the multiscale\nrelative local contrast measure,‚Äù IEEE Geosci. Remote.\nSens. Lett., vol. 15, no. 4, pp. 612‚Äì616, 2018.\n[7] C. Gao, D. Meng, Y . Yang, Y . Wang, X. Zhou, and A. G.\nHauptmann, ‚ÄúInfrared patch-image model for small target\ndetection in a single image,‚ÄùIEEE Trans. Image Process.,\nvol. 22, no. 12, pp. 4996‚Äì5009, 2013.\n[8] Y . Dai, Y . Wu, F. Zhou, and K. Barnard, ‚ÄúAttentional\nlocal contrast networks for infrared small target detec-\ntion,‚Äù IEEE Trans. Geosci. Remote. Sens., vol. 59, no. 11,\npp. 9813‚Äì9824, 2021.\n[9] S. d‚ÄôAscoli, H. Touvron, M. L. Leavitt, A. S. Mor-\ncos, G. Biroli, and L. Sagun, ‚ÄúConvit: Improving\nvision transformers with soft convolutional inductive\nbiases,‚Äù 2021, arXiv:2103.10697. [Online]. Available:\nhttps://arxiv.org/abs/2103.10697.\n[10] H. Ye, N. Zhang, S. Deng, M. Chen, C. Tan, F. Huang,\nand H. Chen, ‚ÄúContrastive triple extraction with gen-\nerative transformer,‚Äù in Proc. AAAI Conf. Artif. Intell.,\nvol. 35, pp. 14257‚Äì14265, 2021.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin,\n‚ÄúAttention is all you need,‚Äù in Proc. Adv. Neural Inf.\nProcess. Syst., pp. 5998‚Äì6008, 2017.\n[12] H. D. Nguyen, X.-S. Vu, and D.-T. Le, ‚ÄúModular graph\ntransformer networks for multi-label image classiÔ¨Åca-\ntion,‚Äù inProc. AAAI Conf. Artif. Intell., vol. 35, pp. 9092‚Äì\n9100, 2021.\n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al., ‚ÄúAn image is worth\n16x16 words: Transformers for image recognition at\nscale,‚Äù in Proc. Int. Conf. Learn. Represent. (ICLR) ,\n2021.\n[14] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual\nlearning for image recognition,‚Äù in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), pp. 770‚Äì778,\n2016.\n[15] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convo-\nlutional networks for biomedical image segmentation,‚Äù in\nInter. Conf. Medi. image comput. comput. assis. interven.\n(MICCAI), pp. 234‚Äì241, Springer, 2015.\n[16] J. Han, C. Liu, Y . Liu, Z. Luo, X. Zhang, and Q. Niu,\n‚ÄúInfrared small target detection utilizing the enhanced\nclosest-mean background estimation,‚Äù IEEE J. Sel. Top.\nAppl. Earth Obs. Remote Sens., 2020.\n[17] L. Deng, J. Zhang, G. Xu, and H. Zhu, ‚ÄúInfrared small\ntarget detection via adaptive m-estimator ring top-hat\ntransformation,‚Äù Pattern Recognit., vol. 112, p. 107729,\n2021.\n[18] M. Zeng, J. Li, and Z. Peng, ‚ÄúThe design of top-hat\nmorphological Ô¨Ålter and application to infrared target\ndetection,‚ÄùInfrared Phys. Technol., vol. 48, no. 1, pp. 67‚Äì\n76, 2006.\n[19] S. D. Deshpande, M. H. Er, R. Venkateswarlu, and\nP. Chan, ‚ÄúMax-mean and max-median Ô¨Ålters for detection\nof small targets,‚Äù in Signal Data Process. Small Targets\n1999, vol. 3809, pp. 74‚Äì83, 1999.\n[20] S. Aghaziyarati, S. Moradi, and H. Talebi, ‚ÄúSmall in-\nfrared target detection using absolute average difference\nweighted by cumulative directional derivatives,‚Äù Infrared\nPhys. Technol., vol. 101, pp. 78‚Äì87, 2019.\n[21] S. Moradi, P. Moallem, and M. F. Sabahi, ‚ÄúFast and\nrobust small infrared target detection using absolute\ndirectional mean difference algorithm,‚Äù Signal Process.,\nvol. 177, p. 107727, 2020.\n[22] J. Han, S. Liu, G. Qin, Q. Zhao, H. Zhang, and\nN. Li, ‚ÄúA local contrast method combined with adaptive\nbackground estimation for infrared small target detec-\ntion,‚Äù IEEE Geosci. Remote. Sens. Lett., vol. 16, no. 9,\npp. 1442‚Äì1446, 2019.\n[23] H. Deng, X. Sun, M. Liu, C. Ye, and X. Zhou, ‚ÄúSmall in-\nfrared target detection based on weighted local difference\nmeasure,‚Äù IEEE Trans. Geosci. Remote. Sens., vol. 54,\nno. 7, pp. 4204‚Äì4214, 2016.\n[24] C. Q. Gao, J. W. Tian, and P. Wang, ‚ÄúGeneralised-\nstructure-tensor-based infrared small target detection,‚Äù\nElectron. Lett., vol. 44, no. 23, pp. 1349‚Äì1351, 2008.\n[25] Z. Cui, J. Yang, S. Jiang, and J. Li, ‚ÄúAn infrared small\ntarget detection algorithm based on high-speed local con-\ntrast method,‚Äù Infrared Phys. Technol., vol. 76, pp. 474‚Äì\n481, 2016.\n[26] D. Pang, T. Shan, W. Li, P. Ma, R. Tao, and Y . Ma,\n‚ÄúFacet derivative-based multidirectional edge awareness\nand spatial-temporal tensor model for infrared small tar-\nget detection,‚Äù IEEE Trans. Geosci. Remote. Sens., early\naccess, Jul. 28, 2021, doi: 10.1109/TGRS.2021.3098969.\n[27] Y . Sun, J. Yang, and W. An, ‚ÄúInfrared dim and small tar-\nget detection via multiple subspace learning and spatial-\ntemporal patch-tensor model,‚Äù IEEE Trans. Geosci. Re-\nmote. Sens., vol. 59, no. 5, pp. 3737‚Äì3752, 2020.\n[28] Y . Dai, Y . Wu, and Y . Song, ‚ÄúInfrared small target and\nbackground separation via column-wise weighted robust\nprincipal component analysis,‚Äù Infrared Phys. Technol.,\nvol. 77, pp. 421‚Äì430, 2016.\n[29] Y . Dai and Y . Wu, ‚ÄúReweighted infrared patch-tensor\nmodel with both nonlocal and local priors for single-\nframe small target detection,‚Äù IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens., vol. 10, no. 8, pp. 3752‚Äì3767,\n2017.\n[30] B. Zhao, C. Wang, Q. Fu, and Z. Han, ‚ÄúA novel pattern\nfor infrared small target detection with generative ad-\nversarial network,‚Äù IEEE Trans. Geosci. Remote. Sens.,\nvol. 59, no. 5, pp. 4481‚Äì4492, 2021.\n[31] M. Ju, J. Luo, G. Liu, and H. Luo, ‚ÄúIstdet: An efÔ¨Åcient\nend-to-end neural network for infrared small target de-\ntection,‚Äù Infrared Phys. Technol., vol. 114, p. 103659,\n2021.\n[32] J. Ryu and S. Kim, ‚ÄúHeterogeneous gray-temperature\nfusion-based deep learning architecture for far infrared\nsmall target detection,‚Äù J. Sensors, vol. 2019, 2019.\n[33] Z. Gao, J. Dai, and C. Xie, ‚ÄúDim and small target\ndetection based on feature mapping neural networks,‚Äù J.\nVis. Commun. Image Represent., vol. 62, pp. 206‚Äì216,\n2019.\n11\n[34] J. Du, H. Lu, M. Hu, L. Zhang, and X. Shen, ‚ÄúCnn-\nbased infrared dim small target detection algorithm using\ntarget-oriented shallow-deep features and effective small\nanchor,‚Äù IET Image Process., vol. 15, no. 1, pp. 1‚Äì15,\n2021.\n[35] X. Tong, B. Sun, J. Wei, Z. Zuo, and S. Su, ‚ÄúEaau-net:\nEnhanced asymmetric attention u-net for infrared small\ntarget detection,‚Äù Remote. Sens., vol. 13, no. 16, p. 3200,\n2021.\n[36] M. Shi and H. Wang, ‚ÄúInfrared dim and small target\ndetection based on denoising autoencoder network,‚ÄùMob.\nNetworks Appl., vol. 25, no. 4, pp. 1469‚Äì1483, 2020.\n[37] Q. Hou, Z. Wang, F. Tan, Y . Zhao, H. Zheng, and\nW. Zhang, ‚ÄúRistdnet: Robust infrared small target detec-\ntion network,‚Äù IEEE Geosci. Remote. Sens. Lett., 2021.\n[38] Z. Fan, D. Bi, L. Xiong, S. Ma, L. He, and W. Ding,\n‚ÄúDim infrared image enhancement based on convo-\nlutional neural network,‚Äù Neurocomputing, vol. 272,\npp. 396‚Äì404, 2018.\n[39] M. Zhao, L. Cheng, X. Yang, P. Feng, L. Liu, and N. Wu,\n‚ÄúTbc-net: A real-time detector for infrared small tar-\nget detection using semantic constraint,‚Äù arXiv preprint\narXiv:2001.05852, 2020, arXiv:2001.05852. [Online].\nAvailable: https://arxiv.org/abs/2001.05852.\n[40] M. Shi and H. Wang, ‚ÄúInfrared dim and small target\ndetection based on denoising autoencoder network,‚ÄùMob.\nNetworks Appl., pp. 1‚Äì15, 2019.\n[41] Q. Hou, Z. Wang, F. Tan, Y . Zhao, H. Zheng, and\nW. Zhang, ‚ÄúRistdnet: Robust infrared small target detec-\ntion network,‚ÄùIEEE Geosci. Remote. Sens. Lett., pp. 1‚Äì5,\n2021.\n[42] Y . Dai, Y . Wu, F. Zhou, and K. Barnard, ‚ÄúAsymmetric\ncontextual modulation for infrared small target detec-\ntion,‚Äù in Proc. IEEE Wint. Conf. App. Comput. Vis.\n(WACV), pp. 950‚Äì959, 2021.\n[43] M. Uzair, R. S. Brinkworth, and A. Finn, ‚ÄúBio-inspired\nvideo enhancement for small moving target detection,‚Äù\nIEEE Trans. Image Process., vol. 30, pp. 1232‚Äì1244,\n2021.\n[44] S. Zhou, J. Wang, L. Wang, J. Zhang, F. Wang, D. Huang,\nand N. Zheng, ‚ÄúHierarchical and interactive reÔ¨Ånement\nnetwork for edge-preserving salient object detection,‚Äù\nIEEE Trans. Image Process., vol. 30, pp. 1‚Äì14, 2021.\n[45] B. Zoph, E. D. Cubuk, G. Ghiasi, T.-Y . Lin, J. Shlens,\nand Q. V . Le, ‚ÄúLearning data augmentation strategies\nfor object detection,‚Äù in Proc. Eur. Conf. Comput. Vis.\n(ECCV), pp. 566‚Äì583, 2020.\n[46] B. Singh and L. S. Davis, ‚ÄúAn analysis of scale invariance\nin object detection snip,‚Äù in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. (CVPR), pp. 3578‚Äì3587, 2018.\n[47] A. Torralba and P. Sinha, ‚ÄúStatistical context priming for\nobject detection,‚Äù in Proc. IEEE Int. Conf. Comput. Vis.\n(ICCV), vol. 1, pp. 763‚Äì770, 2001.\n[48] L. Liangkui, W. Shaoyou, and T. Zhongxing, ‚ÄúUsing deep\nlearning to detect small targets in infrared oversampling\nimages,‚Äù J. Syst. Eng. Electron., vol. 29, no. 5, pp. 947‚Äì\n952, 2018.\n[49] Y . Luo, J. Ji, X. Sun, L. Cao, Y . Wu, F. Huang, C.-\nW. Lin, and R. Ji, ‚ÄúDual-level collaborative transformer\nfor image captioning,‚Äù in Proc. AAAI Conf. Artif. Intell.,\nvol. 35, pp. 2286‚Äì2293, 2021.\n[50] B. Li, W. Cui, W. Wang, L. Zhang, Z. Chen, and\nM. Wu, ‚ÄúTwo-stream convolution augmented transformer\nfor human activity recognition,‚Äù in Proc. AAAI Conf.\nArtif. Intell., vol. 35, pp. 286‚Äì293, 2021.\n[51] W. Zhang, Y . Ying, P. Lu, and H. Zha, ‚ÄúLearning long-\nand short-term user literal-preference with multimodal\nhierarchical transformer network for personalized image\ncaption,‚Äù in Proc. AAAI Conf. Artif. Intell. , vol. 34,\npp. 9571‚Äì9578, 2020.\n[52] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang,\nS. Lin, and B. Guo, ‚ÄúSwin transformer: Hierarchical\nvision transformer using shifted windows,‚Äù Proc. IEEE\nInt. Conf. Comput. Vis. (ICCV), 2021.\n[53] J. Liang, N. Homayounfar, W.-C. Ma, Y . Xiong, R. Hu,\nand R. Urtasun, ‚ÄúPolytransform: Deep polygon trans-\nformer for instance segmentation,‚Äù in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), pp. 9131‚Äì9140,\n2020.\n[54] Z. Dai, B. Cai, Y . Lin, and J. Chen, ‚ÄúUp-detr: Unsuper-\nvised pre-training for object detection with transformers,‚Äù\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), pp. 1601‚Äì1610, 2021.\n[55] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‚ÄúDe-\nformable detr: Deformable transformers for end-to-end\nobject detection,‚Äù in Proc. Int. Conf. Learn. Represent.\n(ICLR), 2020.\n[56] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kir-\nillov, and S. Zagoruyko, ‚ÄúEnd-to-end object detection\nwith transformers,‚Äù in Proc. Eur. Conf. Comput. Vis.\n(ECCV), pp. 213‚Äì229, 2020.\n[57] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang,\nL. Lu, A. L. Yuille, and Y . Zhou, ‚ÄúTransunet: Trans-\nformers make strong encoders for medical image seg-\nmentation,‚Äù 2021, arXiv:2102.04306. [Online]. Available:\nhttps://arxiv.org/abs/2102.04306.\n[58] H. Zhang, L. Zhang, D. Yuan, and H. Chen, ‚ÄúInfrared\nsmall target detection based on local intensity and gradi-\nent properties,‚Äù Infrared Phys. Technol., vol. 89, pp. 88‚Äì\n96, 2018.\n[59] J. Han, Y . Ma, B. Zhou, F. Fan, K. Liang, and Y . Fang,\n‚ÄúA robust infrared small target detection algorithm based\non human visual system,‚Äù IEEE Geosci. Remote. Sens.\nLett., vol. 11, no. 12, pp. 2168‚Äì2172, 2014.\n[60] Y . Wei, X. You, and H. Li, ‚ÄúMultiscale patch-based\ncontrast measure for small infrared target detection,‚Äù\nPattern Recognit., vol. 58, pp. 216‚Äì226, 2016.\n[61] C. Xia, X. Li, L. Zhao, and R. Shu, ‚ÄúInfrared small target\ndetection based on multiscale local contrast measure\nusing local energy factor,‚Äù IEEE Geosci. Remote. Sens.\nLett., vol. 17, no. 1, pp. 157‚Äì161, 2019."
}