{
  "title": "Diversify Question Generation with Retrieval-Augmented Style Transfer",
  "url": "https://openalex.org/W4389519152",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3160174110",
      "name": "Qi Gou",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2514667039",
      "name": "Zehua Xia",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2114015791",
      "name": "Bowen Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000495298",
      "name": "Haiyang Yu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100492699",
      "name": "Yong-Bin Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2308441588",
      "name": "Nguyen Cam Tu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099590177",
    "https://openalex.org/W4389009366",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2967827612",
    "https://openalex.org/W4212880373",
    "https://openalex.org/W4230563027",
    "https://openalex.org/W4285283725",
    "https://openalex.org/W3177334331",
    "https://openalex.org/W3117289850",
    "https://openalex.org/W2606333299",
    "https://openalex.org/W3197377812",
    "https://openalex.org/W3035621030",
    "https://openalex.org/W4285150129",
    "https://openalex.org/W4285310603",
    "https://openalex.org/W3133056742",
    "https://openalex.org/W2131726681",
    "https://openalex.org/W4287887100",
    "https://openalex.org/W2890166583",
    "https://openalex.org/W2825507547",
    "https://openalex.org/W3034881347",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2963521540",
    "https://openalex.org/W4226099451",
    "https://openalex.org/W3199051863",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385573598",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2917436183",
    "https://openalex.org/W3103420681",
    "https://openalex.org/W2964278185",
    "https://openalex.org/W2970717545",
    "https://openalex.org/W19665345",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3105956114",
    "https://openalex.org/W3039602870",
    "https://openalex.org/W2610891036",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2593833795",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2950513705",
    "https://openalex.org/W2970796366",
    "https://openalex.org/W2911857455",
    "https://openalex.org/W3211867455",
    "https://openalex.org/W3211651820",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3202390784",
    "https://openalex.org/W4287634305",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3201266133",
    "https://openalex.org/W3034531294",
    "https://openalex.org/W4206890427",
    "https://openalex.org/W2786983967",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2963371754",
    "https://openalex.org/W2804292122",
    "https://openalex.org/W4372348172",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3102329310",
    "https://openalex.org/W2963034998",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4224247947",
    "https://openalex.org/W2970579055",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4385570490",
    "https://openalex.org/W2169054943"
  ],
  "abstract": "Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for diverse content planning. These methods, however, have not considered the potential of external knowledge for expression diversity. To bridge this gap, we propose RAST, a framework for Retrieval-Augmented Style Transfer, where the objective is to utilize the style of diverse templates for question generation. For training RAST, we develop a novel Reinforcement Learning (RL) based approach that maximizes a weighted combination of diversity reward and consistency reward. Here, the consistency reward is computed by a Question-Answering (QA) model, whereas the diversity reward measures how much the final output mimics the retrieved template. Experimental results show that our method outperforms previous diversity-driven baselines on diversity while being comparable in terms of consistency scores. Our code is available at https://github.com/gouqi666/RAST.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1677–1690\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDiversify Question Generation with Retrieval-Augmented Style Transfer\nQi Gou1, Zehua Xia1, Bowen Yu2, Haiyang Yu2, Fei Huang2\nYongbin Li2∗ and Cam-Tu Nguyen1∗\n1State Key Laboratory for Novel Software Technology, Nanjing University, China\n2Alibaba Group\n{qi.gou,zehuaxia}@smail.nju.edu.cn\n{yubowen.ybw, yifei.yhy, f.huang}@alibaba-inc.com\nshuide.lyb@alibaba-inc.com ncamtu@nju.edu.cn\nAbstract\nGiven a textual passage and an answer, humans\nare able to ask questions with various expres-\nsions, but this ability is still challenging for\nmost question generation (QG) systems. Ex-\nisting solutions mainly focus on the internal\nknowledge within the given passage or the se-\nmantic word space for diverse content planning.\nThese methods, however, have not considered\nthe potential of external knowledge for expres-\nsion diversity. To bridge this gap, we propose\nRAST, a framework for Retrieval-Augmented\nStyle Transfer, where the objective is to uti-\nlize the style of diverse templates for question\ngeneration. For training RAST, we develop\na novel Reinforcement Learning (RL) based\napproach that maximizes a weighted combina-\ntion of diversity reward and consistency reward.\nHere, the consistency reward is computed by\na Question-Answering (QA) model, whereas\nthe diversity reward measures how much the\nfinal output mimics the retrieved template. Ex-\nperimental results show that our method out-\nperforms previous diversity-driven baselines on\ndiversity while being comparable in terms of\nconsistency scores. Our code is available at\nhttps://github.com/gouqi666/RAST.\n1 Introduction\nQuestion Generation (QG) aims to generate ques-\ntions from a given answer and a grounding para-\ngraph. As a dual task of Question Answering (QA),\nQG can potentially be used for the automatic con-\nstruction of QA datasets, thereby improving QA\nwith little annotation effort (Shakeri et al., 2020;\nAlberti et al., 2019; Cui et al., 2021). Furthermore,\nQG can be utilized for educational purposes (Yao\net al., 2022; Qu et al., 2021), dialog systems (Wu\net al., 2022), and conversational recommendation\nsystems (Montazeralghaem and Allan, 2022).\nQG systems are typically known to suffer from\ntwo major issues, namely inconsistency and lack\n∗Corresponding authors.\n智能服务事业部\nSource context: After the death of Tugh Temur in 1332 and subsequent death \nof Rinchinbal (Emperor Ningzong) the same year, the 13-year-old Tugh Temur \n(Emperor Huizong), the last of the nine successors of Kublai Khan, was \nsummoned back from Guangxi and succeeded to the throne. \nRetrieve 3: What’s the [MASK] of [MASK]’s the Grecian Saloon? \nGenerate 3:What’s the year of Tugh Temur’s death?\nRetrieve 1: Kvarner When was [mask]’s [mask]? \nGenerate 1:When was Tugh Temur’s death?\nRetrieve 2:  In what [MASK] did [MASK] let their [MASK] with [MASK]? \nGenerate 2: In what year did  Tugh Temur die?\nVanilla Generate: When did Tugh Temur die? \nExtractive Template: When did [MASK] die?\nRetrieve 4: The [MASK] to [MASK] on the [MASK] passed away in which [MASK]? \nGenerate 4:The successor to Kublai Khan on the throne passed away in which year? \nFigure 1: Given a passage and an answer (in red, under-\nlined text), a base generation model can only produce\nquestions with a single type of expression. Diversified\nquestions, however, can be produced by rewriting the\nbase question using alternative (retrieved) templates.\nNote that the rewriting model should be robust to the\nnoise existing in the retrieved templates.\nof diversity. The former indicates that QG systems\nmay yield context-irrelevant or answer-irrelevant\nquestions (Zhang and Bansal, 2019; Zhao et al.,\n2018; Liu et al., 2019; Song et al., 2018a). The\nlatter is because QG systems may fail to capture\nthe one-to-many nature of QG tasks; that is, many\nquestions can be asked given the same pair of con-\ntext and answer. Existing solutions mainly exploit\nthe internal knowledge within the context (Narayan\net al., 2022a; Wang et al., 2020b), the language\nmodel (Fan et al., 2018a; Holtzman et al., 2019),\nor the semantic word space (Shen et al., 2019; Cho\net al., 2019) for diverse content planning. Unfortu-\nnately, since these methods rely on obscure factors\nsuch as the black-box language model or the latent\nvariable, they are not as controllable as exploiting\nexternal question templates (Figure 1).\nIn this paper, we aim to improve generation di-\nversity by looking for expression variations in an\nexternal set of question templates. Figure 1 shows\nseveral questions that can be generated with a num-\nber of retrieved templates for a given source context.\n1677\nAlthough external information has been exploited\nfor QG (Cao and Wang, 2021; Deschamps et al.,\n2021), prior methods depend on manually crafted\nset of type-dependent templates (Cao and Wang,\n2021) or paraphrasing samples (Deschamps et al.,\n2021). In contrast, we neither require the annota-\ntion of question types and the relevant templates\n(unlike Cao and Wang (2021)) nor assume that\nquestion rewriting samples are accessible (unlike\nDeschamps et al. (2021)).\nOur framework contains three main components:\n(1) a vanilla generator for initial template planning;\n(2) a style retriever, which filters related style tem-\nplates given the initial one; and (3) a style-based\ngenerator, which robustly combines a style tem-\nplate and the internal context to generate the final\nquestion. Training such a model, however, is non-\ntrivial due to two issues: 1) diversity should not\ncome with the cost of consistency; 2) the lack of\ntemplate-based question rewriting samples. We\naddress these issues with Reinforcement Learning\n(RL), which directly maximizes a balanced combi-\nnation of consistency and diversity rewards. Here,\nthe consistency metric is computed by a Question-\nAnswering (QA) model, whereas the diversity met-\nric measures how much the final output mimics\nthe retrieved template. Unlike the standard maxi-\nmum likelihood approach, we do not need token-\nby-token supervised signals for training with RL,\nthus relaxing the need for question rewriting sam-\nples. Our approach is inspired by the retrieval-and-\nedit methods (Cai et al., 2019a,b), but focuses on\nthe unexplored problem of balancing diversity and\nconsistency by using RL.\nAll in all, our main contributions are three-fold:\n1. We propose RAST, a framework for Retrieval-\nAugmented Style Transfer, which retrieves\nquestion style templates from an external set\nand utilizes them to generate questions with\ndiverse expressions.\n2. We propose a novel RL-based method to\njointly train the retriever and the style-based\ngenerator in RAST. Our method is potentially\nadaptable for other retrieval-augmented tasks\nsuch as document-grounded dialog systems\n(Feng et al., 2020; Fu, 2022).\n3. Experimental results on NewsQA (Trischler\net al., 2017) and two splits of SQuAD datasets\n(Zhou et al., 2017; Du et al., 2017) show that\nRAST achieves superior performance on di-\nversity whereas being comparable in terms of\nconsistency.\n2 Related Work\nQuestion Generation Early attempts on QG\nare rule-based (Kunichika et al., 2004; Mostow\nand Wei, 2009), which are inflexible and labor-\nintensive. In addition, such methods are not able to\ngenerate questions from a larger context. Sequence-\nto-sequence-based methods (Du et al., 2017; Ku-\nmar et al., 2019) are able to overcome such issues,\nleading to better results. Recently, supervised fine-\ntuning pre-trained language models (PLM) have\nshown to achieve significant improvement (Dong\net al., 2019; Qi et al., 2020). These systems, how-\never, mostly focus on consistency, whereas diver-\nsity is also essential for downstream tasks such as\nQA. Prior attempts at diversity can be divided into\ntwo main categories, those that make use of inter-\nnal knowledge such as content selection (Cho et al.,\n2019; Shen et al., 2019; Wang et al., 2020b) and im-\nproved decoding (Fan et al., 2018a; Narayan et al.,\n2022a), and those that exploit external patterns (De-\nschamps et al., 2021; Cao and Wang, 2021). Our\nwork falls into the latter category but attempts to\ndo so without samples for question rewriting.\nRetrieval-Augmented Generation There has\nbeen a growing interest in integrating (external)\nknowledge from a retrieval model into a para-\nmetric language model for text generation. Wu\net al. (2019) propose a retrieve-then-edit paradigm,\nwhere an editor is trained to modify a retrieval re-\nsult to produce a more consistent response. Cai\net al. (2019a,b) exploit skeletons to diversify text\ngeneration outputs, where a skeleton is obtained\nby masking query-specific information in the text.\nThe retrieval-augmented generation approach has\nalso been used for task-oriented dialogs (Feng et al.,\n2020, 2021; Shuster et al., 2021; Fu et al., 2022;\nGou et al., 2023; Zhang et al., 2023). These studies,\nhowever, either exploit surface matching methods\n(e.g. tf.idf) (Song et al., 2018b; Cai et al., 2019a,b;\nWu et al., 2019) or separately train the retrieval\nwith relevant labels (Shuster et al., 2021; Feng et al.,\n2020, 2021; Fu et al., 2022). The retriever, there-\nfore, might not be optimal for generation.\nSeveral studies have jointly trained the retriever\nand the generation model (Lewis et al., 2020; Hos-\nsain et al., 2020; Glass et al., 2022) , but they have\nmostly focused on consistency, not diversity.\n1678\nReinforcement Learning for Generation Re-\ninforcement learning (RL) has been used for text\ngeneration to mitigate the exposure bias issue as-\nsociated with the standard Supervised Learning\n(SL) approach. Here, the exposure bias refers to\nthe fact that generation during inference relies on\npredicted tokens instead of ground-truth tokens as\nin training. Furthermore, instead of optimizing\nproxy losses as in SL approach, RL directly opti-\nmizes the quality of interest via RL rewards, thus\nbridging the evaluation gap between training and\ntesting. Researchers have proposed various RL\nrewards for QG, including answerability (for ques-\ntion generation) (Liu et al., 2020), BLEU-4 and\nWord Mover Distance (WMD) (Wang et al., 2020a;\nChen et al., 2020), naturalness (Fan et al., 2018b),\nconsistency using a Question Paraphrase Proba-\nbility (QPP), and a Question Answering Probabil-\nity (QAP) (Zhang and Bansal, 2019; Hosking and\nRiedel, 2019; Yuan et al., 2017). Previous methods\nhave primarily focused on evaluating the consis-\ntency of generated questions, while we seek to\nevaluate both consistency and diversity. Here, the\ndiversity is achieved by training a retrieval model\nfor a retrieval-augmented generation.\nOur work is closely related to RetGen (Zhang\net al., 2022). This method, however, differs from\nours in several ways: 1) only the retrieval model\nis optimized using RL in RetGen, whereas both\nthe retrieval and the generation model are updated\nend-to-end via our RL framework; 2) it uses the\nlikelihood of ground truth generation outputs as\nreturns to update the retriever while we combine\nconsistency and diversity rewards.\nText Style Transfer Our objective of question\nstyle transfer bears some resemblance to text style\ntransfer studies (Li et al., 2018; Xu et al., 2018; Hu\net al., 2022). The main difference is that we do not\nhave predefined style labels, whereas most studies\nin the style transfer literature rely on given labels\nsuch as positive/negative or formal/informal.\nParaphrase Generation Paraphrasing involves\ntransforming a natural language sentence into a\nnew sentence with the same semantic meaning but\na different syntactic or lexical surface form. Al-\nthough diversity can be obtained by paraphrasing\ngenerated questions, our setting is different from\n(He et al., 2020; Goyal and Durrett, 2020; Hosk-\ning et al., 2022). Specifically, question paraphrase\ndatasets, such as (Fader et al., 2013; Wang et al.,\n2017), do not associate context with each pair of\n(sentence, paraphrased sample). As such, para-\nphrasing in these datasets can only focus on dif-\nferent word choice or syntactic modification of an\ninput question. In contrast, our consistency reward\nallows generating questions as long as the answer is\nthe same with the input question given the context.\nIn other words, our method also pays attention to\ndifferent clues of the context for QG diversity.\n3 Methodology\n3.1 Overview\nQG aims to generate question ygiven a paragraph\nc and answer a, which we combine to form the\ncontext x = {c,a}for convenience. To indicate\nthe position of the answer ain x, we wrap it in a\nspecial tag <HL>. Previous works such as (Narayan\net al., 2022a) model p(y|x) for QG, i.e they rely\non the internal knowledge of the context or the\nlanguage model for diversity. Instead, we model\nour diverse QG as follows:\np(y|x,Z)\n= Ez0,z∈Z[p(z0|x) ×p(z|z0)p(y|z,x)]\n= vanilla QG ×RAST\nwhere Z denotes the external corpus of question\nstyle templates, and z0 indicates the initial ques-\ntion template that can be predicted based on the\ncontext x. The intuition is that we choose the\nstyle templates from the external knowledge (z∈\ntop−kfrom Z) that are close but not the same\nas z0, and utilize them to generate questions with\ndiverse styles. During training, for a given context\nx, we extract z0 from the ground truth question\nyby masking context-sensitive information. Dur-\ning inference, as we do not know the ground truth\nquestion, we rely on a vanilla question generation\np(y|x) (vanilla QG) to generate the best y0 from\nwhich we extract the initial template z0. In other\nwords, we approximate p(z0|x) = 1for z0 being\nthe ground truth z0 during training and the greedy\nz0 of the vanilla QG during inference.\nThe general architecture of our framework is\ndemonstrated in Figure 2, which contains a vanilla\nQG and a Retrieval-Augmented Style Transfer\nmodel (RAST model). It is noteworthy that al-\nthough we apply a base T5 model (Raffel et al.,\n2020), many generation methods can be applied for\nvanilla QG to improve content diversity (Narayan\net al., 2022a; Wang et al., 2020b), and the diversity\n1679\nTemplate \nQuery \nQuestion Style\nCorpus Z\nVanilla QG\nQuestion \nPassage c\nStyle Sampling\nContext x\nQuery\nEncoder\nStyle\nTemplate\nEncoder\nStyle\nTransfer\nStyle\nTransfer\nStyle\nTransfer\nStyle\nTransfer\nReward\nModel\nStyle x\nStyle x\nStyle x\nStyle x\nBack propagation\nBack propagationBack propagation\nBack propagationInference Only\nTraining & Inference\nAnswer a\nFigure 2: The architecture of our framework. During training, we use ground question y0, while at evaluation, we\nuse a vanilla QG model to generate a proxy question.\nof RAST subsequently. The vanilla QG is trained\nusing the standard maximum likelihood method,\nwhich we skip here for brevity. In the following, we\ndetail our RAST model and how to train the model\nwithout samples for rewriting questions based on\nalternative styles (z).\n3.2 Question Style Templates\nTo achieve style diversity in RAST, we use a set\nof question style templates which are constructed\nautomatically through two steps - masking and du-\nplication removal. Firstly, we leverage training data\nas our collected question corpus, allowing cross-\nsample reference for diverse question styles. The\nquestion templates are then obtained from the col-\nlected questions by masking context-sensitive infor-\nmation, making such patterns generalizable across\ncontexts. Specifically, for each question, we keep\nstop and interrogative words, but replace entities\n(NER), noun phrases (NP), and context tokens with\n“[MASK]”. Here, NER and NP are detected using\nSpacy1. Finally, near-duplicate templates are re-\nmoved by measuring pairwise Jaccard similarities.\n3.3 Retrieval-Augmented Style Transfer\nStyle Retrieval Model We apply Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020) as the\nstyle retrieval model. Specifically, query and sam-\nple styles are encoded as the following:\nq(z) = BERT1(z) (1)\nq(z0) = BERT2(z0) (2)\npϕ(z|z0) ∝ exp[q(z)Tq(z0)] (3)\n1https://spacy.io/usage/linguistic-features\nwhere BERT-based encoders (Devlin et al., 2019)\nare used to convert question templates into dense\nembedding vectors for style retrieval. Sub-linear\ntime search can be achieved with a Maximum Inner\nProduct Search (MIPS) (Shrivastava and Li, 2014).\nNote that parameters of two encoders (2 BERT)\nconstitute the parameter set ϕof the style retrieval.\nStyle Transfer Model We use T5 (Raffel et al.,\n2020) as our style transfer model pθ(y|z,x), which\ngenerates questions auto-regressively based on a\nchosen style zand the context x:\npθ(y|x,z) =\nT∏\ni=1\npθ(yt|x,z,y 1:t−1) (4)\nwhere T indicates the question length, and θ de-\nnotes T5 model parameters.\n4 Two Stage Training\nWe train RAST using RL to avoid the exposure\nbias and the evaluation discrepancy between train-\ning and testing, which are often associated with\nsupervised learning objectives (Chen et al., 2021).\nTo accelerate the convergence of RL-based train-\ning, we first use supervised learning to initialize\nthe style transfer model, resulting in a two-stage\ntraining procedure described in the following.\n4.1 Supervised Learning\nThe style transfer model pθ(y|x,z) can theoret-\nically be initialized by the model trained on\n{(x,y0,z0)}, where y0 is the ground truth ques-\ntion with the associated template z0. Unfortunately,\ndoing so results in an over-fitting model that is not\n1680\nadaptable to training with noisy templates in the RL\ntraining phase. To overcome this issue, we actively\ncorrupt z0 to obtain a noisy template ˜z0 using sev-\neral mechanisms, including (1) replacing [MASK]\nby a random entity; (2) adding some nouns; (3)\ndeleting [MASK]; and (4) randomly choosing an-\nother template. Let ˆydenote the predicted sequence\ngiven the input xand the corrupted template ˜z0, the\nmodel is then trained with cross-entropy loss:\nLCE\nθ = −\n∑\ni\nyilog p( ˆyi|x,˜z0) (5)\nwhere yi, ˆyi denote the ground truth label and the\npredicted one at the time step i.\n4.2 Reinforcement Learning\n4.2.1 RL for Style Retrieval and Transfer\nOur style retrieval and transfer problem are cast as a\nRL problem. Our model (RAST) introduced above\ncan be viewed as an “agent” that interacts with\nan external “environment” of words and question\ntemplates. The parameters of the retrieval model\n(ϕ) and the transfer model (θ) define a combined\npolicy that results in an “action” that is the selection\nof one style or the prediction of the next word.\nFor simplicity, we assume that the style is chosen\nat the beginning of the sequence generation and\nkept unchanged throughout the generation process.\nUpon generating the end-of-sequence (EOS) token,\nthe agent observes a “reward” r, which is detailed\nin Section 4.2.2. The goal of training is to minimize\nthe negative expected reward:\nLRL(θ,ϕ) =−Eys∼pθ,zs∼pϕ[r(ys,zs)] (6)\nwhere ys = (ys\n1,...,y s\nT) and ys\nt is the word sampled\nfrom the style transfer model pθ at the time step t;\nzs is the template sampled from the style retrieval\nmodel pϕ. Here, ys and zs are sampled according\nto the algorithm described in Section 4.2.3.\nIn order to compute the gradient ∇LRL(θ,ϕ),\nwe use REINFORCE method (Williams, 1992),\nwhich calculates a non-differential reward:\n∇LRL = −Eys,zs[r(ys,zs)∇log pθ,ϕ]\n= −Eys,zs[r(ys,zs)∇log pϕ(zs|z0) −\nEys,zs[(r(ys,zs))∇log pθ(ys|x,zs)]\n= ∇LRL\nθ + ∇LRL\nϕ (7)\nwhere pθ,ϕ indicates pθ,ϕ(ys,zs|x,z0), which can\nbe decomposed into the product of the style trans-\nfer model pθ(ys|x,zs) and the style retrieval model\npϕ(zs|z0). This subsequently decouples the gradi-\nents of the style transfer model∇LRL\nθ and the style\nretrieval model ∇LRL\nϕ .\nRL with a Baseline and KL Divergence In or-\nder to reduce the variance of reinforcement learning\nfor sequence generation, we modify the reward for\nthe style transfer by referencing a baseline busing\nthe Self-critical sequence training (SCST) method\n(Rennie et al., 2017). Here, we use the reward of\nthe greedy output of the style transfer model as the\nbaseline, hence obtaining:\n∇LRL\nθ = −Eys,zs[(r(ys,zs) −b)∇log pθ] (8)\nKL divergence is additionally used to avoid the\nupdated policy (p∗\nθ) drifting too far away from the\noriginal one (pθ) (Liu et al., 2022; Schulman et al.,\n2017). The total gradient function for the style\ntransfer model, therefore, is:\n∇LRL\nϕ,θ = −Eys,zs[r(ys,zs)∇log pϕ]\n−Eys,zs[(r(ys,zs) −b)∇log pθ]\n+β∇KL(pθ||p∗\nθ) (9)\n4.2.2 Reward Model\nConsistency Reward encourages the model to\ngenerate context-relevant and answer-relevant ques-\ntions. Various strategies for consistency rewards\ncan be used such as answerability (Liu et al., 2020),\nBLEU-4 and Word Mover Distance (WMD) (Wang\net al., 2020a; Chen et al., 2020), naturalness (Fan\net al., 2018b). In this paper, inspired by (Zhu and\nHauff, 2021), we apply a Question Answer (QA)\nloss-based metric as our consistency reward. There\nare two reasons for QA-based metrics to be a good\napproximation for QG consistency: 1) QA is the\ndual task of QG; 2) the performance of QA sys-\ntems, e.g., on SQuAD, has come close to human\nperformance. Unlike (Zhu and Hauff, 2021), which\nuses an extractive QA model, we utilize a genera-\ntive QA model based on T5 (Raffel et al., 2020).\nThe reward is then measured as follows:\nLqa = −1\nTa\nTa∑\ni=1\nlog p(ai|c,ys,a<i) (10)\nrcons(ys,zs) = exp(−Lqa) (11)\nwhere Ta indicates the answer length, and ys is a\nsampled question from pθ(y|z,x).\n1681\nAlgorithm 1: Diversity driven Sampling\ninput : The combination of paragraph and\nanswer, x= {c,a}; the list of\ntemplates retrieved from Zbased\non z0, S; the generation sampling\nprobability, p; and k.\noutput :ksampled questions and styles\n1 clusters←cluster Sinto kclusters based\non Jaccard similarity;\n2 QZs ←empty set ;\n3 for i←1 to kdo\n4 if training then\n5 zs ←randomly choose a style from\nclusters[i];\n6 else\n7 zs ←select top style based on\npϕ(zs|z0);\n8 Sample ys from pθ(y|x,zs) using\nnucleus sampling with probability p;\n9 Add {zs,ys}to QZs;\n10 return QZs\nDiversity Reward promotes the generation of\nquestions that are close to retrieved templates. For\nsimplicity, we use Jaccard Similarity as our diver-\nsity reward as follows:\nrdivs(ys,zs) =zs ∩ys\nzs ∪ys (12)\nTotal Reward tries to trade off between consis-\ntency and diversity. It is obtained by combining the\ntwo rewards with a diverse coefficient λ∈[0,1] :\nr(ys,rs) =rcons + λrdivs (13)\nFor the style transfer model, it is intuitive to see\nhow this reward helps balance consistency and\ndiversity. As for the style retriever, since the re-\nward includes the consistency metric, we can as-\nsign higher scores to templates that can be used to\ngenerate various questions as long as the answer\nis a. By doing so, the style retrieval can go be-\nyond surface matching and assign higher scores to\ntemplates of different styles.\n4.2.3 Diversity-driven Sampling\nOne issue with training an RL model is that the\nmodel may degenerate to a locally optimal one,\nduring which the retrieval puts all the probability\nmass on a small number of templates very close\nto z0 according to surface matching, ignoring all\nDataset Train Valid Test\nSQuAD /1 86635 8965 8964\nSQuAD /2 70484 10570 11877\nNewsQA 92549 5166 5126\nTable 1: Statistic for datasets, where SQuAD /1 is the\ntrain/val/test split from (Zhou et al., 2017), and SQuAD\n/2 is another split from (Du et al., 2017).\nthe other templates. To overcome this, we propose\na diversity-driven sampling procedure as in Algo-\nrithm 1. During training, we first cluster retrieved\ntemplates to group those close to each other ac-\ncording to surface matching (Jaccard similarity),\nthen sample a template randomly from each cluster.\nBy doing so, RL can have better exploration for\nvarious styles, thus avoiding the local optimal. Dur-\ning inference, however, we select the top template\nbased on the retrieval scores from the well-trained\nretrieval model.\n5 Experiments\n5.1 Experiment Settings\nDatasets We conduct experiments on two pub-\nlic datasets, SQuAD (Rajpurkar et al., 2016) and\nNewsQA (Trischler et al., 2017). As for SQuAD,\nsince the test set is not accessible, we use the splits\nof (Zhou et al., 2017)2 and (Du et al., 2017) instead.\nTable 1 provides the statistics of these datasets.\nEvaluation Following (Wang et al., 2020b;\nNarayan et al., 2022b), we adopt several metrics to\nevaluate diversity and consistency: 1) Top-1 BLEU\nmeasures BLEU of the best generated output; 2)\nOracle BLEU reflects the overall consistency by\ncomparing the best hypothesis among top-N out-\nputs with the target question. 3) Pairwise BLEU\n(or Self BLEU) measures the diversity by averag-\ning sentence-level metrics of pairs within top N. A\nlower value of pairwise BLEU indicates a higher\nlevel of diversity. 4) Overall BLEU measures the\noverall performance, which can be calculated by\nTop-1 ×Oracle ÷Pairwise. Note that all the men-\ntioned BLEU indicate BLEU-4.\n5.2 Baselines\nWe compare our method with recent diverse-driven\nQG methods, which include those based on content\n2https://res.qyzhou.me/redistribute.zip\n1682\nModel Top-1 ↑ Oracle↑ P-BLEU↓ Overall↑\nSQuAD /1\nMixture-Decoder (Shen et al., 2019) 15.17 21.97 58.73 5.67\nMixture-Selector (Cho et al., 2019) 15.67 22.45 58.82 5.88\nCV AE (Wang et al., 2020b) 15.34 21.15 54.18 5.99\nComposition (Narayan et al., 2022a) ⋆ 16.5 25.7 58.99 7.21†\nNucleus-T5 (Holtzman et al., 2019) 12.98 23.45† 50.28 † 6.05\nRAST(ours) 19.25 23.23 48.91 9.14\nSQuAD /2\nComposition (Narayan et al., 2022a) ⋆ 15.94† 24.90 60.05 6.61†\nNucleus-T5 (Holtzman et al., 2019) 13.31 24.42 † 55.54 5.85\nRAST(ours) 19.36 22.59 56.42 † 7.75\nNewsQA\nMixture-Decoder (Shen et al., 2019) 10.02 17.04 † 55.07 3.10\nMixture-Selector (Cho et al., 2019) 10.90† 17.51 52.61 3.63\nCV AE (Wang et al., 2020b) 9.90 15.48 41.37 3.70 †\nNucleus (T5) (Holtzman et al., 2019) 5.29 14.63 27.47 † 2.82\nRAST(ours) 11.02 16.26 23.16 7.74\nTable 2: Comparison of different techniques on question generation on NewsQA and two splits of SQuAD. Here, ⋆\ndenotes that the results are re-evaluated by us. The best and runner-up† are marked.\nplanning and those based on sampling.\nContent Planning-based Methods Mixture De-\ncoder (Shen et al., 2019) models a mixture of ex-\nperts (MoE), where a latent variable drawn from\nMoE is used to control the generation and produce\na diverse set of hypotheses. Mixture Selector (Cho\net al., 2019) focuses on different parts of the context\nby modeling a binary variable for token selection.\nCV AE (Wang et al., 2020b) also selects tokens from\nthe context, but uses a continuous latent variable\ninstead of a binary variable like Mixture-Selector.\nSampling-based Methods Nucleus Sampling\n(Holtzman et al., 2019) samples tokens from a trun-\ncated distribution, where the unreliable tail of1−p\nprobability mass is cropped. Composition Sam-\npling (Narayan et al., 2022a) uses nucleus sampling\nto obtain diverse entity chains, then utilizes beam\nsearch to generate the most-likely output.\n5.3 Implementation Details\nWe use the pre-trained DPR (Karpukhin et al.,\n2020) to initialize the retrieval encoders. Pre-\ntrained T5-base 3 is used for vanilla QG and the\nstyle transfer model. During inference, the tem-\nplate of the vanilla QG is used as a query to retrieve\nN−1 more templates. The obtained templates are\nthen used to generate N (N=5) questions for eval-\nuation. We use SacreBLEU 4 to calculate BLEU.\n3https://huggingface.co/t5-base\n4https://github.com/mjpost/sacrebleu\nMore details can be found in Appendix A.\nWe conduct experiments with Nucleus-T5 by\nourself using Transformers5. In addition, the re-\nsults of Composition Sampling are reevaluated,\nwhereas those of other baselines are from (Shen\net al., 2019; Cho et al., 2019; Wang et al., 2020b).\n5.4 Results and Analysis\nTable 2 summarizes our experimental results, for\nwhich detailed analysis are given in the following.\nAmong diverse-promoting baselines, Nucleus-\nT5 promotes diversity with the cost of Top-1 BLEU\nbeing dropped significantly. CV AE and Com-\nposition are better at balancing between consis-\ntency and diversity, resulting in high overall scores.\nFor example, in comparison with Nucleus-T5 on\nSQuAD /2, Composition is more consistent (better\nTop-1 and Oracle-BLEU), despite of being less di-\nverse (lower Pairwise-BLEU). Our result is in line\nwith (Narayan et al., 2022b).\nCompared to the previous methods, RAST\nachieve the best diversity score (the lowest\nPairwise-BLEU) on SQuAD/1 and NewsQA, and\nthe second-best on SQuAD/2. Particularly, our\nmethod outperforms strong baselines (Composi-\ntion Sampling and CV AE) by a large margin in\nterms of diversity, whereas being comparable on\nconsistency scores. Specifically, on NewsQA and\n5https://github.com/huggingface/transformers\n1683\nFigure 3: Results of changing the diverse coefficient λ\non SQuAD /1 (SQuAD v1.1 with split from (Zhou et al.,\n2017)). EM and F1 indicate QA-model based metrics,\nand P-BLEU is short for Pairwise-BLEU.\nSQuAD/1, RAST is better than CV AE on both Top-\n1 and Oracle-BLEU. On SQuAD/1 and SQuAD/2,\nRAST is better than Composition on Top-1 whereas\nbeing comparable on Oracle-BLEU. Regarding the\noverall score, RAST obtains the superior results\non three datasets, showing that its capability in\nbalancing between diversity and consistency.\n5.5 Ablation Study\nWe study the impact of different components of\nRAST on SQuAD/1 (Zhou et al., 2017), where the\nresults are given in Figure 3 and Table 3.\nDiverse Coefficient Figure 3 shows how diver-\nsity and consistency change when increasing λ.\nBesides Oracle-BLEU, we also use Exact Match\n(EM) and F1 (two QA metrics) to measure consis-\ntency (Sultan et al., 2020; Lyu et al., 2021). Here,\nthe QA metrics are calculated by averaging EM\nand F1 scores of the answers, which are generated\nby the QA model for top-N evaluation questions.\nAs observable from Figure 3, increasing λleads\nto higher diversity (lower pairwise-BLEU), but\nlower consistency (lower Oracle and QA metrics).\nThis is the result that we expect. The rate of in-\ncrease in diversity, however, is much higher than\nthe rate of decrease in the consistency metrics.\nSpecifically, when λ changes from 0.05 to 0.25,\npairwise-BLEU drops 39.52 points (from 74.7 to\n35.18), whereas F1 only drops 6.96 points (from\n85.16 to 78.2), showing that our method is able\nto maintain consistency within a reasonable range\nwhile promoting diversity significantly.\nFreeze DPR To study the impact of joint RL\ntraining on the retrieval and generation models, we\nModel Top1 ↑ Oracle↑ P-B↓ Over↑\nRAST 19.25 23.23 48.91 9.14\nw/o e2e 19.94 23.40 51.70 9.02\nw/o cluster 15.58 23.04 61.06 5.88\nw/ question 19.00 23.59 54.09 8.28\nTable 3: Experimental results of different model vari-\nants on SQuAD /1 (Zhou et al., 2017). Here, P-B and\nOver are short for Pairwise-BLEU and Overall-BLEU\nrespectively; EM and F1 indicate the QA-based metrics.\nModel Consistency Diversity Total\nRAST 3.36 2.36 2.86\nNucleus 3.00 1.78 2.39\nTable 4: Human evaluation result on SQuAD /1.\ncompare the performance of RAST and RAST (w/o\ne2e). As observable from Table 3, overall BLEU is\nimproved with end2end training, showing that the\nretrieval model is better optimized for balancing\nbetween diversity and consistency.\nDiversity-driven Sampling We measure the im-\npact of the clustering step in diversity-driven sam-\npling (Algorithm 1) by comparing RAST and\nRAST (w/o cluster) in Table 3. Here, during train-\ning, RAST (w/o cluster) samples templates based\nsolely on the retrieval scores. It is observable that\nclustering allows us to better train RAST, and thus\nresults in better performance across all metrics.\nRetrieval Query The last row of Table 3 shows\nthe performance of RAST when we use the best\nquestion y0 of the vanilla QG (RAST w/ question)\ninstead of the question template z0 for querying\nexternal templates. As we can see, using masked\nquestions (RAST) leads to higher diversity than\nthe alternative. This is intuitive given the fact that\nmasking context-sensitive information can make\ntemplates more generalizable across contexts.\n5.6 Human Evaluation\nWe followed (Wang et al., 2020b) to evaluate the\nconsistency and diversity of RAST and Nucleus\nsample6 on 50 samples of SQuAD /1. Here, the\nconsistency metric ranges from 0 to 5, measuring\nthe proportion of the generated questions being\nanswerable based on the given context (without\n6This is because the source code of the other baselines is\nnot publicly available\n1684\nType N-C N-D R-C R-D\nFleiss’ Kappa 0.61 0.60 0.62 0.75\nTable 5: Our inter-annotator agreement score. Here,\nN and R denote Nucleus and RAST, whereas C and D\nmeans consistency and diversity respectively.\n Source context: J. A. Hobson identifies this justification on general grounds as: “It is desirable that the earth should be peopled, governed, and developed, as far as possible, by the races which can do this work best, i.e. by the races of highest social efficiency”.  Retrieved Template: Which [MASK] to have [MASK] of [MASK]? RAST: Which race does Hobson believe to have the responsibility of human development?  Retrieved Template: Who would be seen as having been [MASK] in the [MASK]? RAST: Who would be seen as having been best in the development of the earth?  Retrieved Template: [MASK] states that [MASK] has the [MASK] to[MASK]? RAST: Hobson states which race has the ability to develop the earth to the best of its ability?  \nFigure 4: The three RAST outputs with different ques-\ntion types. Here the given answer is highlighted with\nred color in the source context.\nany hallucinations on the named entity or intent\nerrors). On the other hand, the diversity metric cal-\nculates the number of distinct questions among the\nconsistent ones, which means the diversity score\nranges from 1 to the consistency score. Specifi-\ncally, each sample has been checked by three anno-\ntators. The results in Table 4 indicate that RAST\nless suffers from hallucination, whereas being more\ndiverse. We also provide our inter-annotator agree-\nment score in Table 5, which indicate moderate to\nsubstantial agreement among our annotators.\n5.7 Case Analysis\nTo better analyze the performance of RAST, we\nprovide a case study in Figure 4. As shown in this\ncase, RAST obtains its diversity by retrieving dif-\nferent templates. Notably, the third output replaces\n“that” in the template with “which,” demonstrat-\ning that our model does not simply copy syntactic\nwords from the template. Interesting, the diversity\nalso results from selecting different clues from the\ncontext that are suitable for the retrieved templates,\nsuch as “Hobson,” “race,” “best,” and “the develop-\nment of the earth.” Please refer to Appendix B for\nmore cases.\n6 Conclusion\nThis paper proposes RAST, a framework that ex-\nploits question templates from an external corpus to\nimprove expression diversity in QG systems. Com-\npared to previous methods, that exploit internal\nknowledge of language models for diversity, RAST\nprovides a more flexible and interpretative way to\ncontrol the generation outputs. To train RAST with-\nout question rewriting samples, we develop a novel\nRL based method, where we directly optimize the\ncombination of the consistency and diversity re-\nwards. In addition, we provide two stage training\nand diversity-driven sampling, which help better\ntrain our RL-based model. Experiment results show\nthat RAST outperforms strong baselines in terms\nof diversity whereas being comparable on consis-\ntency scores. For future studies, we aim at further\nimprovement by developing efficient training with\na small number of paraphasing samples.\nLimitations\nOur study currently suffers from several limita-\ntions: (1) QG evaluation is challenging due to one-\nto-many nature of the task. The best evaluation\nshould be human evaluation. Unfortunately, this is\nnot possible since we do not have access to source\ncode of many previous studies. Although the out-\nputs of Composition Sampling are available, they\nonly come with the paired gold questions. Since\nthe data was shuffled, we do not know the corre-\nsponding passages for human evaluation. As an\nalternative, we have tried to cover as many met-\nrics as possibles, including all of the metrics used\nin previous baselines and QA-based metrics. (2)\nTraining a RL-based method like RAST is typically\nmore difficult and time consuming. This is because\nRL requires many rounds of sampling to converge.\nOur two-stage training is helpful, but there is still\nmore room for improvement. (3) Our model is lim-\nited by the maximum context length like most of\nthe Transformer-based methods.\nEthics Statement\nThis paper uses opensource datasets to construct\nthe external style corpus. One concern is that model\ncan learn to mimic target properties in the training\ndata that are not desirable. Another concern is that\nour work might involve the same biases and toxic\nbehaviors in the pre-trained models.\n1685\nReferences\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin,\nand Michael Collins. 2019. Synthetic QA corpora\ngeneration with roundtrip consistency. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang\nLiu, Wai Lam, and Shuming Shi. 2019a. Skeleton-\nto-response: Dialogue generation guided by retrieval\nmemory. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1219–1228, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\njiang Liu, and Shuming Shi. 2019b. Retrieval-\nguided dialogue response generation via a matching-\nto-generation framework. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1866–1875, Hong Kong,\nChina. Association for Computational Linguistics.\nShuyang Cao and Lu Wang. 2021. Controllable open-\nended question generation with a new question type\nontology. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\nOnline.\nHao Chen, Rui Xia, and Jianfei Yu. 2021. Reinforced\ncounterfactual data augmentation for dual sentiment\nclassification. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 269–278, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020.\nReinforcement learning based graph-to-sequence\nmodel for natural question generation. In Proceed-\nings of the 8th International Conference on Learning\nRepresentations.\nJaemin Cho, Minjoon Seo, and Hannaneh Hajishirzi.\n2019. Mixture content selection for diverse sequence\ngeneration. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nShaobo Cui, Xintong Bao, Xinxing Zu, Yangyang Guo,\nZhongzhou Zhao, Ji Zhang, and Haiqing Chen. 2021.\nOnestop qamaker: Extract question-answer pairs\nfrom text in a one-stop approach.\nArthur Deschamps, Sujatha Das Gollapalli, and\nSee Kiong Ng. 2021. On generating fact-infused\nquestion variations. In Proceedings of the Interna-\ntional Conference on Recent Advances in Natural\nLanguage Processing (RANLP 2021).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT (1).\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1342–1352.\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni.\n2013. Paraphrase-driven learning for open question\nanswering. In Proceedings of the 51st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1608–1618.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018a.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nZhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, and\nXuan-Jing Huang. 2018b. A reinforcement learn-\ning framework for natural question generation using\nbi-discriminators. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics.\nSong Feng, Siva Sankalp Patel, Hui Wan, and Sachindra\nJoshi. 2021. MultiDoc2Dial: Modeling dialogues\ngrounded in multiple documents. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6162–6176, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nSong Feng, Hui Wan, Chulaka Gunasekara, Siva Patel,\nSachindra Joshi, and Luis Lastras. 2020. doc2dial: A\ngoal-oriented document-grounded dialogue dataset.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8118–8128.\nHaomin Fu, Yeqin Zhang, Haiyang Yu, Jian Sun, Fei\nHuang, Luo Si, Yongbin Li, and Cam-Tu Nguyen.\n2022. Doc2bot: Accessing heterogeneous doc-\numents via conversational bots. arXiv preprint\narXiv:2210.11060.\nYingxue Fu. 2022. Towards unification of discourse\nannotation frameworks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics: Student Research Workshop, pages 132–\n142, Dublin, Ireland. Association for Computational\nLinguistics.\n1686\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\nChowdhury, Ankita Naik, Pengshan Cai, and Alfio\nGliozzo. 2022. Re2G: Retrieve, rerank, generate.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2701–2715, Seattle, United States. Association\nfor Computational Linguistics.\nQi Gou, Zehua Xia, and Wenzhe Du. 2023. Cross-\nlingual data augmentation for document-grounded di-\nalog systems in low resource languages. In Proceed-\nings of the Third DialDoc Workshop on Document-\ngrounded Dialogue and Conversational Question An-\nswering.\nTanya Goyal and Greg Durrett. 2020. Neural syntac-\ntic preordering for controlled paraphrase generation.\narXiv preprint arXiv:2005.02013.\nJunxian He, Taylor Berg-Kirkpatrick, and Graham Neu-\nbig. 2020. Learning sparse prototypes for text gen-\neration. Advances in Neural Information Processing\nSystems, 33:14724–14735.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nTom Hosking and Sebastian Riedel. 2019. Evaluating\nrewards for question generation models. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2278–2283, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nTom Hosking, Hao Tang, and Mirella Lapata. 2022. Hi-\nerarchical sketch induction for paraphrase generation.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2489–2501, Dublin, Ireland.\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\nmoyer. 2020. Simple and effective retrieve-edit-\nrerank text generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2532–2538, Online. Association\nfor Computational Linguistics.\nZhiqiang Hu, Roy Ka-Wei Lee, Charu C Aggarwal, and\nAston Zhang. 2022. Text style transfer: A review\nand experimental evaluation. ACM SIGKDD Explo-\nrations Newsletter, 24(1):14–45.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nVishwajeet Kumar, Nitish Joshi, Arijit Mukherjee,\nGanesh Ramakrishnan, and Preethi Jyothi. 2019.\nCross-lingual training for automatic question gen-\neration. arXiv preprint arXiv:1906.02525.\nH. Kunichika, T. Katayama, T. Hirashima, and\nA. Takeuchi. 2004. Automated question generation\nmethods for intelligent english learning systems and\nits evaluation. proc of icce.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to senti-\nment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 1865–1874.\nBang Liu, Mingjun Zhao, Di Niu, Kunfeng Lai,\nYancheng He, Haojie Wei, and Yu Xu. 2019. Learn-\ning to generate questions by learningwhat not to gen-\nerate. In The World Wide Web Conference, WWW\n2019, San Francisco, CA, USA, May 13-17, 2019.\nDayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng\nChen, Jiancheng Lv, Nan Duan, and Ming Zhou.\n2020. Tell me how to ask again: Question data aug-\nmentation with controllable rewriting in continuous\nspace. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5798–5810, Online. Association for\nComputational Linguistics.\nRuibo Liu, Guoqing Zheng, Shashank Gupta, Radhika\nGaonkar, Chongyang Gao, Soroush V osoughi, Milad\nShokouhi, and Ahmed H. Awadallah. 2022. Knowl-\nedge infused decoding. In International Conference\non Learning Representations (ICLR).\nChenyang Lyu, Lifeng Shang, Yvette Graham, Jennifer\nFoster, Xin Jiang, and Qun Liu. 2021. Improving\nunsupervised question answering via summarization-\ninformed question generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4134–4148.\nAli Montazeralghaem and James Allan. 2022. Learning\nrelevant questions for conversational product search\nusing deep reinforcement learning. In Proceedings\nof the Fifteenth ACM International Conference on\nWeb Search and Data Mining, WSDM ’22.\nJ. Mostow and C. Wei. 2009. Generating instruc-\ntion automatically for the reading strategy of self-\nquestioning. In Conference on Artificial Intelligence\nin Education: Building Learning Systems That Care:\nfrom Knowledge Representation to Affective Mod-\nelling.\nShashi Narayan, Gonçalo Simões, Yao Zhao, Joshua\nMaynez, Dipanjan Das, Michael Collins, and Mirella\nLapata. 2022a. A well-composed text is half done!\ncomposition sampling for diverse conditional genera-\ntion. In Proceedings of the 60th Annual Meeting of\n1687\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers).\nShashi Narayan, Gonçalo Simões, Yao Zhao, Joshua\nMaynez, Dipanjan Das, Michael Collins, and Mirella\nLapata. 2022b. A well-composed text is half done!\ncomposition sampling for diverse conditional genera-\ntion. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 1319–1339, Dublin,\nIreland. Association for Computational Linguistics.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\n2020. Prophetnet: Predicting future n-gram for\nsequence-to-sequencepre-training. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020.\nFanyi Qu, Xin Jia, and Yunfang Wu. 2021. Asking ques-\ntions like educational experts: Automatically generat-\ning question-answer pairs on real-world examination\ndata. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nsequence training for image captioning. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 7008–7024.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nSiamak Shakeri, Cicero dos Santos, Henghui Zhu,\nPatrick Ng, Feng Nan, Zhiguo Wang, Ramesh Nal-\nlapati, and Bing Xiang. 2020. End-to-end synthetic\ndata generation for domain adaptation of question\nanswering systems. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5445–5460.\nTianxiao Shen, Myle Ott, Michael Auli, and\nMarc’Aurelio Ranzato. 2019. Mixture models for\ndiverse machine translation: Tricks of the trade. In\nProceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings of\nMachine Learning Research.\nAnshumali Shrivastava and Ping Li. 2014. Asymmetric\nlsh (alsh) for sublinear time maximum inner prod-\nuct search (mips). Advances in neural information\nprocessing systems, 27.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings of\nthe Association for Computational Linguistics.\nLinfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang,\nand Daniel Gildea. 2018a. Leveraging context infor-\nmation for natural question generation. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies.\nYiping Song, Cheng-Te Li, Jian-Yun Nie, Ming Zhang,\nDongyan Zhao, and Rui Yan. 2018b. An ensem-\nble of retrieval-based and generation-based human-\ncomputer conversation systems. In Proceedings of\nthe Twenty-Seventh International Joint Conference\non Artificial Intelligence, IJCAI-18.\nMd Arafat Sultan, Shubham Chandel, Ramón Fernan-\ndez Astudillo, and Vittorio Castelli. 2020. On the\nimportance of diversity in question generation for qa.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris,\nAlessandro Sordoni, Philip Bachman, and Kaheer\nSuleman. 2017. Newsqa: A machine comprehension\ndataset. In Proceedings of the 2nd Workshop on\nRepresentation Learning for NLP, pages 191–200.\nLiuyin Wang, Zihan Xu, Zibo Lin, Haitao Zheng, and\nYing Shen. 2020a. Answer-driven deep question gen-\neration based on reinforcement learning. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5159–5170, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nZhen Wang, Siwei Rao, Jie Zhang, Zhen Qin, Guangjian\nTian, and Jun Wang. 2020b. Diversify question gen-\neration with continuous content selectors and ques-\ntion type modeling. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n2134–2143, Online. Association for Computational\nLinguistics.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017.\nBilateral multi-perspective matching for natural lan-\nguage sentences. arXiv preprint arXiv:1702.03814.\nR. J. Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine Learning, 8(3-4):229–256.\nQingyang Wu, Song Feng, Derek Chen, Sachindra Joshi,\nLuis Lastras, and Zhou Yu. 2022. DG2: Data aug-\nmentation through document grounded dialogue gen-\neration. In Proceedings of the 23rd Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 33.\n1688\nJingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu-\nancheng Ren, Houfeng Wang, and Wenjie Li. 2018.\nUnpaired sentiment-to-sentiment translation: A cy-\ncled reinforcement learning approach. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers).\nBingsheng Yao, Dakuo Wang, Tongshuang Wu, Zheng\nZhang, Toby Li, Mo Yu, and Ying Xu. 2022. It\nis AI’s turn to ask humans a question: Question-\nanswer pair generation for children’s story books.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 731–744, Dublin, Ireland.\nXingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro\nSordoni, Philip Bachman, Saizheng Zhang, Sandeep\nSubramanian, and Adam Trischler. 2017. Machine\ncomprehension by text-to-text neural question gener-\nation. In Proceedings of the 2nd Workshop on Repre-\nsentation Learning for NLP, pages 15–25, Vancouver,\nCanada. Association for Computational Linguistics.\nShiyue Zhang and Mohit Bansal. 2019. Address-\ning semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP).\nYeqin Zhang, Haomin Fu, Cheng Fu, Haiyang Yu, Yong-\nbin Li, and Cam-Tu Nguyen. 2023. Coarse-to-fine\nknowledge selection for document grounded dialogs.\nIn ICASSP 2023 - 2023 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1–5.\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\nBrockett, Michel Galley, Jianfeng Gao, and Bill\nDolan. 2022. Retgen: A joint framework for retrieval\nand grounded text generation modeling. Proceedings\nof the AAAI Conference on Artificial Intelligence.\nYao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa\nKe. 2018. Paragraph-level neural question gener-\nation with maxout pointer and gated self-attention\nnetworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,\nHangbo Bao, and Ming Zhou. 2017. Neural ques-\ntion generation from text: A preliminary study. In\nNational CCF Conference on Natural Language Pro-\ncessing and Chinese Computing. Springer.\nPeide Zhu and Claudia Hauff. 2021. Evaluating bert-\nbased rewards for question generation with reinforce-\nment learning. In Proceedings of the 2021 ACM\nSIGIR International Conference on Theory of Infor-\nmation Retrieval, ICTIR ’21.\nParameters SQuAD/1 SQuAD/2 NewsQA\ng-lr 1e-6 1e-6 1e-6\nd-lr 1e-7 1e-7 1e-7\nmax-len 128 512 1250\nBS 12 8 2\nT-num 3 3 2\nTraining hour 48 70 120\nλ 0.5 0.5 0.4\nβ 0.1 0.1 0.05\nTable 6: Hyper-parameters of RAST at reinforcement\nlearning stage, g-lr means generator learning rate, d-\nlr means DPR learning rate, BS means batch size, T-\nnum means how many templates will be sent to style\ntransfer model for every single training data,λis diverse\ncoefficient and βis coefficient of KL divergence.\nA Technical Details\nA.1 Implementation Details\nOur model is implemented with Pytorch 1.8.1 and\nTransformers 4.23.1. For three datasets, we set max\nlength of input as 128/512/1250 for SQuAD split1,\nSQuAD split2, and NewsQA respectively.\nDuring inference, the template of the vanilla QG\n(z0) is used as a query to achieve N −1 more\ntemplates, which are then combined with z0 to gen-\nerate N questions for top-N evaluation (N=5). The\nz0, however, is not actually be used for generating\nquestions with the style transfer model, instead we\nreplace it with an empty string. In other words, the\ninput the style transfer model contain only context.\nThis is done so that we do not take the advantage\nof the vanilla QG into account.\nA.1.1 Hyperparameters\nDuring SL, we fine-tune the baselines for 5 epochs\nwith learning rate of 5e-4 and 5 epochs. We set the\nsampling parameters with top-p of 0.9 and top-k of\n30. Warmup-ratio and weight-decay are set as 0.1\nfor all three datasets. We set batch size as 64/32/6\nfor SQuAD/1, /2, and NewsQA, respectively.\nFor RL, we train RAST with 7 epochs and\nwarmup-ratio of 0.2. The number of retrieval is set\nas 100 at training and 500 at evaluation. We choose\n5 style templates for style transfer model at evalua-\ntion since we should calculate Oracle BLEU(K=5)\nwith baselines for fair comparison. The final model\nis the one with the highest Oracle BLEU on de-\nvelopment set. Please refer to Table 6 for more\ninformation.\nB Samples of Generation Results\n1689\n \nSQuAD v1.1, split1 \nInput: After the death of Tugh Temür in<HL> 1332 <HL> and subsequent death of Rinchinbal \n(Emperor Ningzong) the same year, the 13-year-old Toghun Temür (Emperor Huizong), the last of \nthe nine successors of Kublai Khan, was summoned back from Guangxi and succeeded to the throne. \nGOLD: When did Tugh Temur die? \nRAST: In what year did Tugh Temür die? \n When did Tugh Temür pass away?  \n When did Tugh Temür die?  \n When did Tugh Temür receive his last death? \n Tugh Temür was killed when?  \nNucleus In what year did Tugh Temür die? \n When did Tugh Temür die? \n When did Tugh Temür die? \n In what year did Tugh Temür die? \n In what year did Tugh Temür die? \nInput: All chloroplasts in a plant are descended from<HL> undifferentiated proplastids <HL> \nfound in the zygote, or fertilized egg. \nGOLD: What do a plant's chloroplasts descend from? \nRAST: What do all chloroplasts in a plant descended from? \n Chloroplasts are typically made up of what type?  \n What do chloroplasts appear as in the zygote? \n What type of organism in zygote usually recognized as descended from? \n What type of organism were seen in a zygote? \nNucleus: What are all chloroplasts in a plant descended from? \n What are all chloroplasts in a plant descended from? \n What are all chloroplasts in a plant descended from? \n What are all chloroplasts in a plant descended from? \n What are all chloroplasts in a plant descended from? \nInput: The merger was suspended, and a complaint was filed by the Department of Justice in July \n1967, with ITT going to trial in October 1967; the merger was officially canceled after the trial's \nconclusion on<HL> January 1, 1968 <HL> \nGOLD: When was the merger between ITT and ABC officially canceled? \nRAST:  When did the trial of ITT end? \n On what date was the merger officially canceled? \n The merger was not officially canceled until when? \n When was the trial where the merger of ITT appeared?  \n The merger became the canceled when? \nNucleus: When was the merger officially canceled? \n When was the trial for ITT's merger officially canceled? \n When did the court's decision about the ITT merger come to an end? \n When was the merger officially canceled? \n When did ITT's merger with ITC officially end? \n \nFigure 5: Examples of input context, ground truth question, and model predictions for SQuAD v1.1 dataset of split\n(Zhou et al., 2017). We also show the result generated by nucleus sampling of which P-BLEU is similar with RAST\nwhile Top-1 BLEU and Oracle BLEU are lower than RAST. Despite similar values of P-BLEU, the interrogative\nwords and syntactic structures generated by nucleus sampling are homogenous, not as diverse and flexible as RAST.\n1690",
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.8240134716033936
    },
    {
      "name": "Computer science",
      "score": 0.7837356328964233
    },
    {
      "name": "Diversity (politics)",
      "score": 0.652656614780426
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5312792062759399
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5154136419296265
    },
    {
      "name": "Style (visual arts)",
      "score": 0.4908314645290375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4788953363895416
    },
    {
      "name": "Focus (optics)",
      "score": 0.46095871925354004
    },
    {
      "name": "Information retrieval",
      "score": 0.456387996673584
    },
    {
      "name": "Code (set theory)",
      "score": 0.44371578097343445
    },
    {
      "name": "Natural language processing",
      "score": 0.42662134766578674
    },
    {
      "name": "Programming language",
      "score": 0.08088356256484985
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 4
}