{
    "title": "Towards Knowledge Enhanced Language Model for Machine Reading Comprehension",
    "url": "https://openalex.org/W3112021549",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3095306701",
            "name": "Peizhu Gong",
            "affiliations": [
                "Shanghai Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A2108491511",
            "name": "Jin Liu",
            "affiliations": [
                "Shanghai Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A2124981515",
            "name": "Yihe Yang",
            "affiliations": [
                "Shanghai Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A2184694232",
            "name": "Huihua He",
            "affiliations": [
                "Shanghai Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A3095306701",
            "name": "Peizhu Gong",
            "affiliations": [
                "Shanghai Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A2108491511",
            "name": "Jin Liu",
            "affiliations": [
                "Shanghai Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A2124981515",
            "name": "Yihe Yang",
            "affiliations": [
                "Shanghai Maritime University"
            ]
        },
        {
            "id": "https://openalex.org/A2184694232",
            "name": "Huihua He",
            "affiliations": [
                "Shanghai Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963336993",
        "https://openalex.org/W2908465515",
        "https://openalex.org/W2963829073",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2741075451",
        "https://openalex.org/W2801540583",
        "https://openalex.org/W3023056542",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2951561177",
        "https://openalex.org/W6763234310",
        "https://openalex.org/W6767905578",
        "https://openalex.org/W2774837955",
        "https://openalex.org/W2970431814",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W6678830454",
        "https://openalex.org/W6724366048",
        "https://openalex.org/W6695596964",
        "https://openalex.org/W2514852614",
        "https://openalex.org/W2250342289",
        "https://openalex.org/W6718437798",
        "https://openalex.org/W2073587810",
        "https://openalex.org/W2798586382",
        "https://openalex.org/W6632455782",
        "https://openalex.org/W2891820987",
        "https://openalex.org/W2963344337",
        "https://openalex.org/W2758430142",
        "https://openalex.org/W2963448850",
        "https://openalex.org/W6755178746",
        "https://openalex.org/W6754189453",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W2022166150",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6686133869",
        "https://openalex.org/W2950438065",
        "https://openalex.org/W3002072934",
        "https://openalex.org/W2962808855",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W6729654139",
        "https://openalex.org/W2964022985",
        "https://openalex.org/W2953044594",
        "https://openalex.org/W6751097180",
        "https://openalex.org/W2764004791",
        "https://openalex.org/W2803609569",
        "https://openalex.org/W2984452801",
        "https://openalex.org/W2513378248",
        "https://openalex.org/W2972167903",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2184957013",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W3099387504",
        "https://openalex.org/W2551396370",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4295253143",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2798858969",
        "https://openalex.org/W2891702339",
        "https://openalex.org/W1793121960",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2433281745",
        "https://openalex.org/W2499696929",
        "https://openalex.org/W3098057198",
        "https://openalex.org/W2283196293",
        "https://openalex.org/W2890042874"
    ],
    "abstract": "Machine reading comprehension is a crucial and challenging task in natural language processing (NLP). Recently, knowledge graph (KG) embedding has gained massive attention as it can effectively provide side information for downstream tasks. However, most previous knowledge-based models do not take into account the structural characteristics of the triples in KGs, and only convert them into vector representations for direct accumulation, leading to deficiencies in knowledge extraction and knowledge fusion. In order to alleviate this problem, we propose a novel deep model KCF-NET, which incorporates knowledge graph representations with context as the basis for predicting answers by leveraging capsule network to encode the intrinsic spatial relationship in triples of KG. In KCF-NET, we fine-tune BERT, a highly performance contextual language representation model, to capture complex linguistic phenomena. Besides, a novel fusion structure based on multi-head attention mechanism is designed to balance the weight of knowledge and context. To evaluate the knowledge expression and reading comprehension ability of our model, we conducted extensive experiments on multiple public datasets such as WN11, FB13, SemEval-2010 Task 8 and SQuAD. Experimental results show that KCF-NET achieves state-of-the-art results in both link prediction and MRC tasks with negligible parameter increase compared to BERT-Base, and gets competitive results in triple classification task with significantly reduced model size.",
    "full_text": null
}