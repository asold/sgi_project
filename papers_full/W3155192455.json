{
  "title": "CLiMP: A Benchmark for Chinese Language Model Evaluation",
  "url": "https://openalex.org/W3155192455",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3037908752",
      "name": "Beilei Xiang",
      "affiliations": [
        "University of Colorado System",
        "University of Colorado Boulder"
      ]
    },
    {
      "id": "https://openalex.org/A2154924913",
      "name": "Changbing Yang",
      "affiliations": [
        "University of Colorado Boulder",
        "University of Colorado System"
      ]
    },
    {
      "id": "https://openalex.org/A2097755322",
      "name": "Yu Li",
      "affiliations": [
        "University of Colorado System",
        "University of Colorado Boulder"
      ]
    },
    {
      "id": "https://openalex.org/A2805277953",
      "name": "Alex Warstadt",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2413443321",
      "name": "Katharina Kann",
      "affiliations": [
        "University of Colorado Boulder",
        "University of Colorado System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W4299542557",
    "https://openalex.org/W2889947987",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035064549",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W566301461",
    "https://openalex.org/W2963025830",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W4298392964",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W3168987555",
    "https://openalex.org/W2888922637"
  ],
  "abstract": "Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in Chinese, covering 9 major Chinese linguistic phenomena. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8%. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier–noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8% average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2784–2790\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n2784\nCLiMP: A Benchmark for Chinese Language Model Evaluation\nBeilei Xiang,1 Changbing Yang,1 Yu Li,1 Alex Warstadt2 and Katharina Kann1\n1University of Colorado Boulder, 2New York University\n{beilei.xiang, changbing.yang, yuli9309}@colorado.edu\nwarstadt@nyu.edu\nkatharina.kann@colorado.edu\nAbstract\nLinguistically informed analyses of language\nmodels (LMs) contribute to the understand-\ning and improvement of these models. Here,\nwe introduce the corpus of Chinese linguistic\nminimal pairs (CLiMP), which can be used\nto investigate what knowledge Chinese LMs\nacquire. CLiMP consists of sets of 1,000\nminimal pairs (MPs) for 16 syntactic con-\ntrasts in Mandarin, covering 9 major Mandarin\nlinguistic phenomena. The MPs are semi-\nautomatically generated, and human agree-\nment with the labels in CLiMP is 95.8%. We\nevaluate 11 different LMs on CLiMP, covering\nn-grams, LSTMs, and Chinese BERT. We ﬁnd\nthat classiﬁer–noun agreement and verb com-\nplement selection are the phenomena that mod-\nels generally perform best at. However, mod-\nels struggle the most with the bˇa construction,\nbinding, and ﬁller-gap dependencies. Over-\nall, Chinese BERT achieves an 81.8% average\naccuracy, while the performances of LSTMs\nand 5-grams are only moderately above chance\nlevel.\n1 Introduction\nLanguage models (LMs) are crucial parts of natu-\nral language processing (NLP) systems for a large\nvariety of tasks, including summarization, machine\ntranslation, and dialog generation. More recently,\nthey have become popular in the form of pretrained\nmodels,1 which are then ﬁne-tuned on downstream\ntasks and often obtain state-of-the-art performance\n(Peters et al., 2018; Devlin et al., 2019; Conneau\net al., 2020). However, which linguistic phenom-\nena language models can or cannot learn is still\npoorly understood for many languages.\nResources for the syntactic evaluation of LMs,\nsuch as BLiMP (Warstadt et al., 2020) have focused\n1Throughout this paper, we adopt a broad deﬁnition of\nLMs, which includes language representation models which\nhave been trained on a masked language modeling objective.\nmainly on English, and non-English resources\ncurrently only cover a small set of phenomena\n(Mueller et al., 2020; Gulordava et al., 2018; Rav-\nfogel et al., 2018). In order to spur the analysis and\nsubsequent improvement of LMs in Chinese, we\nintroduce the corpus of Chinese linguistic minimal\npairs (CLiMP), which can be used to evaluate LMs’\nknowledge of Chinese grammar.\nCLiMP consists of 16 individual datasets that are\nsemi-automatically generated from grammar tem-\nplates. Each set—or paradigm—contains 1,000\nminimal pairs (MPs). Together, they cover 9\ncore linguistic phenomena in Chinese. Human\nagreement on this corpus is 95.8%, conﬁrming\nthat CLiMP represents robust contrasts in Chinese\ngrammar. High performance on CLiMP thus im-\nplies high correlation with human acceptability\njudgments across these phenomena.\nWe use CLiMP to study Chinese BERT (Devlin\net al., 2019),2 6 LSTM (Hochreiter and Schmidhu-\nber, 1997) LMs, and 4 5-gram LMs. We evaluate\nfor each MP whether the LM assigns a higher prob-\nability to the grammatical or the ungrammatical\nsentence. Our results show that Chinese BERT is\nclosest to human performance, achieving an 81.8%\naccuracy on average over all phenomena, while the\nperformances of LSTMs and 5-grams, regardless\nof the training data size, are only moderately above\nchance level. Classiﬁer–noun agreement and verb\ncomplement selection are the phenomena that mod-\nels generally perform best at, suggesting that Chi-\nnese LMs are better at acquiring knowledge of local\nselectional restrictions. The bˇa construction, bind-\ning, and ﬁller-gap dependencies are the phenomena\nmodels have the most difﬁculties with. This indi-\ncates that they struggle to learn hierarchical syntax\nand to identify long-distance dependencies.\n2https://github.com/google-\nresearch/bert/blob/master/multilingual.md\n2785\n2 Related Work\n2.1 Language Models\nLMs assign probabilities to sequences of words (Ju-\nrafsky and Martin, 2009). Recently, they have be-\ncome commonly used as pretrained models, which\ncan be ﬁne-tuned for downstream NLP tasks (Pe-\nters et al., 2018; Devlin et al., 2019; Conneau et al.,\n2020). Strictly speaking, LMs compute the proba-\nbilities of words based only on past context. BERT\n(Devlin et al., 2019), however, is trained using a\nmasked language modeling objective: it predicts\nwords based on past and future tokens. Wang and\nCho (2019) show that BERT is a Markov random\nﬁeld language model that can assign sentences a\npseudo-log-likelihood score, which is computed\nby summing the conditional log probabilities of\nall tokens in the sentence, as well as generate text.\nShin et al. (2019) and Salazar et al. (2020) apply\npseudo-log-likelihood scores to sentence ranking\nand LM evaluation.\n2.2 Evaluation of Linguistic Knowledge\nNumerous methods exist for probing syntactic\nknowledge of neural network models in English\n(Hewitt and Manning, 2019; Tenney et al., 2019),\nand a growing body of work evaluates the syntac-\ntic knowledge of neural models by testing whether\nthey can judge the grammatical acceptability of sen-\ntences. One common version of this task uses MPs\nto evaluate LMs’ linguistic knowledge (Linzen\net al., 2016; Marvin and Linzen, 2018; Warstadt\net al., 2020; Wilcox et al., 2018).\nA MP is a pair of sentences that only differ in\nacceptability due to a single edit, as in (1) and (2).\nNative speakers can be asked to choose which sen-\ntence in each pair sounds more grammatical. Semi-\nautomatically generating MPs can yield a larger set\nof controlled sentences, providing sufﬁcient data\nfor model evaluation (Linzen et al., 2016; Marvin\nand Linzen, 2018; Ettinger et al., 2018).\n(1) 王鑫\nW´angx¯ın\n把\nbˇa\n自行车\nz`ıx´ıngch¯e\n扔\nr¯eng\n了。\nle\nSUBJ. BA. OBJ. V . PST.\n“Xin Wang threw away a bike.”\n(2) 王鑫\nW´angx¯ın\n被\nb`ei\n自行车\nz`ıx´ıngch¯e\n扔\nr¯eng\n了。\nle\nSUBJ. PASS. OBJ. V . PST.\n“Xin Wang was thrown away by a bike.”\nIt is possible to model acceptability in a to-\ntally unsupervised way using LMs. The model\nassigns a probability to each sentence in a MP,\nand the one with the higher score is predicted\nas correct, and the model’s predictions can be\nevaluated against human judgments (Marvin and\nLinzen, 2018; Warstadt et al., 2020). Supervised\napproaches are also possible (Warstadt et al., 2019),\nbut can be less informative on LMs’ linguistic\nknowledge acquisition due to the bias introduced\nby training on acceptability judgment labels.\nSome prior work evaluates the linguistic knowl-\nedge of different non-English models (Ravfogel\net al., 2018; Gulordava et al., 2018; Mueller et al.,\n2020). However, these efforts focus mainly on\nsubject-verb agreement, which is absent in Chi-\nnese, and the knowledge of Chinese LMs has not\nyet been explicitly studied.\nFinally, the linguistic abilities of English BERT\nhave been investigated in a a lot of prior work,\ne.g., Clark et al. (2019); Vig (2019); Hewitt and\nManning (2019). We refer the reader to Rogers\net al. (2021) for an overview.\n3 CLiMP\nOur main contribution is CLiMP, a corpus of\nChinese MPs designed to evaluate Chinese LMs.\nCLiMP consists of 1,000 MPs for each of 16 gram-\nmatical contrasts, covering 9 major Chinese lin-\nguistic phenomena. Example MPs for each phe-\nnomenon are shown in Table 1.\n3.1 Data Generation\nWe generate data from grammar templates for ev-\nery paradigm we incorporate. Our templates set\nlexical, syntactic, and semantic constraints for each\nparadigm, aiming at building robust contrasts and\nkeeping the sentence length the same within each\nMP. We then build an annotated vocabulary, and\ngenerate sentences by sampling words from it. (1)\nand (2) show an MP together with the template 3\nused to create it.\n3.2 Vocabulary\nWe translate Warstadt et al.’s (2020) English vocab-\nulary, containing 3,000 English words with mor-\nphological, syntactical, and semantic annotations.\nWe add words and features speciﬁc to Chinese lin-\nguistic phenomena to our vocabulary, including\nclassiﬁers, verb complements, action verbs, and\n3The template example is only for demonstrative purposes.\nMore information is encoded for the actual data generation.\n2786\ncoverbs. Our ﬁnal vocabulary contains 3,456 words\nand 84 features.\nWe show the frequency of words in CLiMP’s\nvocabulary in the Chinese Internet Corpus4 in Fig-\nure 1. 1,055 of the words in CLiMP are within the\n5,000 most frequent words in the Chinese Internet\nCorpus.\nFigure 1: Comparison of word frequencies in CLiMP\nand the Chinese Internet Corpus.\n3.3 Linguistic Phenomena\nCLiMP covers 9 major linguistic phenomena in\nMandarin Chinese, cf. Table 1. They are picked\nfrom a comprehensive Chinese grammar book by\nPo-Ching and Rimmington (2015). Following Po-\nChing and Rimmington’s discussion, we now ex-\nplain the phenomena not present in English. The\nbˇa construction is an SOV construction involv-\ning the particle bˇa, which precedes the object and\nmoves the object to a position before the main verb.\nIt is only grammatical with a subset of transitive\nverbs. Coverbs are verb-like items that precede the\nmain verb in a serial verb construction. They al-\nmost invariably have to be used in conjunction with\nother verbs in a sentence. They share some prop-\nerties with prepositions, but are not syntactically\ninterchangeable with them. Classiﬁers obligato-\nrily appear with nouns when those are modiﬁed\nby numerals or adjectives. Mandarin has dozens\nof classiﬁers, and nouns select the classiﬁer they\ncombine with. Verb complements follow a verb,\noften expressing a result or manner of an event.\nNot all verbs can be used with all complements,\nmaking certain combinations ungrammatical. NP\nhead ﬁnality is present in Mandarin noun phrases.\nThe relative clause precedes noun phrases.\n3.4 Data Validation\nTo verify whether the MPs in our dataset show clear\ncontrasts, we conduct two rounds of human valida-\n4http://corpus.leeds.ac.uk/frqc/internet-zh.num\ntion with 22 annotators. They are all native speak-\ners of Chinese, 14 females and 8 males, whose ages\nrange from 20 to 48. All of them have at least a\nhigh school degree.\nIn our ﬁrst human validation, each human anno-\ntator is assigned a subset (100 MPs) of a paradigm.\nWe let them perform the same forced-choice task\nas our models: decide for each MP which sentence\nseems more acceptable. We discard one paradigm,\nthe coverb-direction paradigm, after this validation,\nbecause its human validation accuracy is below\n85%. The average human agreement for the re-\nmaining paradigms is 95.8%.\nIn the second human validation, we sample 15\nMPs from each of the remaining paradigm, result-\ning in a dataset consisting of 240 MPs. 16 annota-\ntors complete the same forced-choice task on this\ndataset. We count a MP as valid if more than half\nof the annotators agree with its label. The human\nagreement on this dataset is 97.1%, showing that\nour data creation results in valid examples.\n3.5 Comparison with BLiMP\nBLiMP consists of 67 datasets, each containing\n1,000 MPs and organized by phenomenon into 12\ncategories. CLiMP only contains 16 datasets due to\nthe less inﬂectional nature of Mandarin Chinese. 3\nphenomena are covered by both corpora: anaphor\nagreement, binding, and ﬁller-gap. The human\nagreement for these three phenomena in BLiMP is\n97.5%, 87.3%, and 86.9%, respectively. The cor-\nresponding accuracies in CLiMP are 94.5%, 99%,\nand 100%, respectively. The overall human agree-\nment for BLiMP is 88.6%, which is 7.2% lower\nthan for CLiMP.\n4 Models and Methods\nWe use accuracy for evaluation. A MP in CLiMP is\nclassiﬁed correctly if a LM assigns a higher prob-\nability to the grammatical sentence than to the un-\ngrammatical one. We evaluate statistical and neural\nLMs, including masked LMs. Corpora which con-\ntain 0.4M, 2M, and 21.5M sentences are used for\nfurther exploration. We also investigate the effect\nof different tokenizations.5\nChinese BERT BERT (Devlin et al., 2019) is\na transformer-based neural model (Vaswani et al.,\n2017). Here, we evaluate Chinese BERT. 6 This\n5We use character tokenization and word tokenization\n(https://github.com/fxsjy/jieba).\n6https://github.com/google-\nresearch/bert/blob/master/multilingual.md\n2787\nPhenomenon N Acceptable Example Unacceptable Example\nAnaphor\nagreement 1\n王玉珍 震惊-了 她自己。\nJane.F shock-PST herself.\n’Jane shocked herself. ’\n王玉珍 震惊-了 他 自己。\nJane.F shock-PST himself.\n’Jane shocked himself. ’\nBinding 1\n杨颖 治疗吴宇涛之后佩服-过 她自己。\nYang.F cure Wu.M after admire-PST herself\n’Yang admired herselfafter she cured Wu. ’\n杨颖 治疗吴宇涛之后 佩服-过 他自己。\nYang.F cure Wu.M after admire-PST himself\n’Yang admired himselfafter she cured Wu. ’\nbˇa\nconstruction 1\n王鑫 把 自行车 扔 了。\nWong.M BA bike throw PST\n’Wong threw away the bike. ’\n王鑫 被 自行车 扔 了。\nWong.M PASSbike throw PST\n’Wong was thrown awayby the bike. ’\nCoverb 3\n李文清 乘 卡车 到达-了 咖啡店。\nLee.M ride truck arrive-PST coffee shop\n’Lee went to the coffee shop bytruck. ’\n李文清 于卡车 到达-了 咖啡店。\nLee.M at truck arrive-PST coffee shop\n’Lee went to the coffee shop attruck. ’\nNP head ﬁnality 1\n王梦 正在 卖张红梅 清洗-过-的 推车。\nWong.F PROG sell May.F clean-PRF-ADJtrolley\n‘Wong is selling the trolley that Mel has cleaned. ’\n王梦 正在 卖推车 张红梅 清洗-过-的。\nWong.F PROG sell trolley May.F clean-PRF-ADJ\n‘Wong is selling the trolley that Mel has cleaned. ’\nClassiﬁer 2\n张杰 正在 穿过 一 家 艺术画廊。\nJay.M PROG pass one CL:INSTITUTIONart gallery\n’Jay is passing through anart gallery. ’\n张杰正在 穿过 一 段 艺术画廊。\nJay.M PROG pass one CL:LENGTHart gallery\n’Jay is passing through anart gallery. ’\nFiller gap 1\n图书馆， 我 开车去-过 这个地方。\nThe library, I drive to-PRF this place\n‘The library, I have driven to this place. ’\n图书馆， 我 开车去-过 博物馆。\nThe library, I drive to-PRF the museum\n‘The library, I have driven to the museum. ’\nPassive 1\n这些 患者 被 转移-了。\nThese patient PASS transfer-PST\n’These patients were transferred. ’\n这些 患者 被 下降-了。\nThese patient PASS fall-PST\n’These patients were fell. ’\nVerb\ncomplement 5\n王慧 的 文章 吓 坏 了 包曼玉。\nWong.F POSS article frighten badlyPST Bao.F.\n’Wong’s article frightened Bao badly. ’\n王慧 的 文章 吓 开 了 包曼玉。\nWong.F POSS article frighten openlyPST Bao.F.\n’Wong’s article frightened Bao openly. ’\nTable 1: Nine Chinese linguistic phenomena covered by CLiMP with acceptable and unacceptable sentence ex-\namples. Minimal differences are underlined. The second line of each example shows a gloss, the third line is an\nEnglish translation. N represents how many paradigms (each with 1,000 examples) are within each phenomena.\nmodel has 12 layers, 768 hidden units, 12 attention\nheads, and 110M parameters. The training dataset\ncontains 25M sentences. We assign probabilities to\nsentences with this model by masking the words in\na sentence one by one, computing the probability\nof each masked word, and, ﬁnally, multiplying the\nprobabilities of all words (Wang and Cho, 2019;\nSalazar et al., 2020).7\nLSTM LMs We further evaluate 6 LSTM\n(Hochreiter and Schmidhuber, 1997) LMs. These\nmodel have 2 layers, 200 hidden units, and 2 atten-\ntion heads. We train them using Pytorch’s word\nlanguage model code8 on 3 differently-sized Chi-\nnese Wikipedia corpora: 0.4M, 2M, and 21.5M\nsentences. We further compare word-level and\ncharacter-level models (cf. Table 2). For evalu-\nation, we employ code adapted by Warstadt et al.\n(2020) from Gulordava et al. (2018).9\nn-gram LMs Finally, we experiment with 4 dif-\nferent 5-gram LMs, which have been trained on\n0.4M and 2M sentences from Chinese Wikipedia.\nFor each corpus size, we train one word-based and\none character-based LM. Those models are imple-\n7https://github.com/xu-song/bert-as-language-model\n8https://github.com/pytorch/examples/tree/master/\nword language model\n9https://github.com/sheng-fu/colorlessgreenRNNs\nmented using KenLM.10\n5 Results\nAll results are shown in Table 2.\nPhenomenon-speciﬁc Results Our LMs per-\nform best on classiﬁer–noun agreement and verb\ncomplement selection: Chinese BERT’s accuracy\nis only 6.8% and, respectively, 3% lower than that\nof humans on these two phenomena. LSTMs and 5-\ngrams remain around 30% behind humans, but still\nperform better on these phenomena than on others\nin CLiMP. This indicates that Chinese LMs acquire\nlocal selection knowledge better than the linguistic\nknowledge needed to master other phenomena.\nOur LMs stuggle most with the bˇa construction,\nbinding, and ﬁller-gap dependencies. All models\nperform close to chance level for binding, suggest-\ning that they lack the hierarchical knowledge neces-\nsary to correctly resolve the structural relationship\nbetween a reﬂexive and its binder. Similarly, most\nmodels perform near chance on ﬁller-gap depen-\ndencies. This suggests that they do not robustly\nrepresent long-distance dependencies.11\n10https://kheaﬁeld.com/code/kenlm/\n11A caveats applies: because Mandarin lackswh-movement,\nwe test ﬁller-gap dependencies using a topicalization construc-\n2788\nModel Overall Clsfr. V .Cp. Hd.Fi. The ba. Coverb Ana.Agr. Pass. Bind. Fi.Gap\nHuman 95.8 99.7 96.0 100.0 85.0 92.5 94.5 91.0 99.0 100.0\nChinese BERT 81.8 92.9 93.0 53.1 69.0 87.9 86.2 67.7 50.8 62.4\nLSTM-21.5M-word 62.8 75.7 74.0 81.4 10.0 47.0 63.1 68.4 50.1 41.5\nLSTM-21.5M-char 60.7 56.1 64.9 89.1 32.1 43.2 57.0 67.9 50.0 68.8\nLSTM-2M-word 66.0 77.8 73.8 75.0 48.4 43.4 67.0 68.0 50.0 59.2\nLSTM-2M-char 60.4 68.4 68.1 86.3 29.0 28.5 68.1 68.4 50.1 61.9\nLSTM-2M-word 60.6 69.9 65.4 70.3 41.1 38.8 66.3 72.7 50.0 55.2\nLSTM-2M-char 63.2 68.9 69.7 83.9 25.0 45.6 67.7 74.3 50.0 64.4\n5-gram-2M-word 59.0 70.1 71 55.2 15.6 39.2 67.7 72.0 49.6 40.0\n5-gram-2M-char 65.7 70.6 78.8 68.3 30.6 53.9 65.8 64.8 51.6 57.3\n5-gram-0.4M-word 55.9 66.4 69.5 46.3 6.0 37.0 69.1 77.8 49.1 25.2\n5-gram–0.4M-char 60.0 71.5 65.4 70.5 19.3 46.5 68.8 68.7 50.2 48.4\nTable 2: Percentage accuracy of all humans and models on CLiMP. Random guessing yields an accuracy of 50%.\nBold numbers indicate the phenomenon each model is best at. Numbers in model names (21.5, 2, 0.4) refer to the\nnumber of sentences in the training corpus.\nOn the head-ﬁnal construction, Chinese BERT\nperforms surprisingly poorly as compared to the\nother models: only 53.1% accuracy as compared\nto an average accuracy of 81% by the LSTMs. The\ncoverb construction, in contrast, is easy for Chi-\nnese BERT: it achieves87.9% accuracy, while the\nhighest accuracy among all other models is 47%.\nModel-speciﬁc Results Comparing across mod-\nels, Chinese BERT achieves by far the highest over-\nall accuracy with 81.8%. Our different LSTMs\nall perform worse, but obtain surprisingly similar\nscores: from 60.4% to 66.0%. The performances\nof our 5-grams range from 55.9% to 65.7%. Keep-\ning tokenization and corpus size constant, three out\nof four 5-grams are outperformed by LSTMs. Thus,\nwe overall ﬁnd that neural models have advantages\nas compared to statistical models.\nComparing among the LSTMs, we ﬁnd similarly\nto Hu et al. (2020) that the corpus size does not\nhave much inﬂuence on the overall performance,\nwith the caveat that these models perform close\nto chance. In contrast, a larger corpus size does\nresult in a better performance in 5-grams. We\nalso compare the effect of different tokenizations:\nCharacter-based 5-grams demonstrate better per-\nformance than word-based ones. For LSTMs, how-\never, using characters only results in a better per-\nformance for our smallest corpus size (0.4M).\nCompared to English LMs (Warstadt et al.,\n2020), the human–model gap is much bigger for\nChinese models. While neither models nor datasets\nare directly comparable between our and previous\nwork, this still suggests that more analyses and\ndevelopments are needed for non-English models.\ntion more common in speech, and less likely to appear in the\ntraining corpora.\n6 Conclusion\nWe introduced CLiMP, a suite of diagnostic test\nsets aimed at evaluating which syntactic phenom-\nena Chinese LMs learn, and used it to evaluate\n11 different models. All LMs appeared to have\nlearned local selectional restrictions, but struggled\nwith argument structure alternations, hierarchical\nstructure, and long-distance dependencies. Chi-\nnese BERT performed best on CLiMP overall.\nHowever, it obtained a 14% lower accuracy than\nhumans, suggesting there is still much room for\nimprovement. We hope that CLiMP will serve\nas a linguistically informed resource for bench-\nmarking and analyzing future progress on Chi-\nnese LMs. CLiMP is available at https://nala-\ncub.github.io/resources.\n7 Acknowledgments\nWe would like to thank the students from CU Boul-\nder’s CSCI/LING5832 in Spring 2020 for their\nfeedback on this research. We are also grateful\nfor the feedback of the anonymous reviewers.\nReferences\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\n2789\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1790–1801, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\nDan Jurafsky and James H. Martin. 2009. Speech\nand language processing: An introduction to natu-\nral language processing, computational linguistics,\nand speech recognition. Pearson Prentice Hall, Up-\nper Saddle River, N.J.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAaron Mueller, Garrett Nicolai, Panayiota Petrou-\nZeniou, Natalia Talmina, and Tal Linzen. 2020.\nCross-linguistic syntactic evaluation of word predic-\ntion models. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5523–5539, Online. Association for\nComputational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nYip Po-Ching and Don Rimmington. 2015. Chinese: A\ncomprehensive grammar. Routledge.\nShauli Ravfogel, Yoav Goldberg, and Francis Tyers.\n2018. Can LSTM learn to capture agreement? the\ncase of basque. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP , pages 98–107, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the As-\nsociation for Computational Linguistics , 8(0):842–\n866.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nJoonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019.\nEffective sentence scoring method using BERT for\nspeech recognition. In Asian Conference on Ma-\nchine Learning, pages 1081–1093.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\n2790\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the transformer model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n37–42, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation , pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nnananey, Wei Peng, Sheng-Fu Wang, and Samuel\nBowman. 2020. BLiMP: The Benchmark of Lin-\nguistic Minimal Pairs for English. Transactions\nof the Association for Computational Linguistics ,\n8(0):377–392.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do rnn language mod-\nels learn about ﬁller–gap dependencies? In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 211–221.",
  "topic": "Complement (music)",
  "concepts": [
    {
      "name": "Complement (music)",
      "score": 0.8018333911895752
    },
    {
      "name": "Computer science",
      "score": 0.793968915939331
    },
    {
      "name": "Natural language processing",
      "score": 0.6662930250167847
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6271231174468994
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6234610676765442
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5301303267478943
    },
    {
      "name": "Language model",
      "score": 0.5301195979118347
    },
    {
      "name": "Noun",
      "score": 0.5018491744995117
    },
    {
      "name": "Verb",
      "score": 0.492378830909729
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4474492073059082
    },
    {
      "name": "Chinese language",
      "score": 0.42889973521232605
    },
    {
      "name": "Speech recognition",
      "score": 0.32808130979537964
    },
    {
      "name": "Linguistics",
      "score": 0.2889906167984009
    },
    {
      "name": "Complementation",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802236040",
      "name": "University of Colorado System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}