{
  "title": "Scaling Federated Learning for Fine-Tuning of Large Language Models",
  "url": "https://openalex.org/W3127841762",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2887204899",
      "name": "Agrin Hilmkil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751233114",
      "name": "Sebastian Callh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149947646",
      "name": "Matteo Barbieri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2624279769",
      "name": "Leon René Sütfeld",
      "affiliations": [
        "RISE Research Institutes of Sweden"
      ]
    },
    {
      "id": "https://openalex.org/A2765280503",
      "name": "Edvin Listo Zec",
      "affiliations": [
        "RISE Research Institutes of Sweden"
      ]
    },
    {
      "id": "https://openalex.org/A725773188",
      "name": "Olof Mogren",
      "affiliations": [
        "RISE Research Institutes of Sweden"
      ]
    },
    {
      "id": "https://openalex.org/A2887204899",
      "name": "Agrin Hilmkil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751233114",
      "name": "Sebastian Callh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149947646",
      "name": "Matteo Barbieri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2624279769",
      "name": "Leon René Sütfeld",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2765280503",
      "name": "Edvin Listo Zec",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A725773188",
      "name": "Olof Mogren",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6763692313",
    "https://openalex.org/W6609833400",
    "https://openalex.org/W6755988804",
    "https://openalex.org/W6609821937",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6602046083",
    "https://openalex.org/W6607687417",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6601223237",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2808129629",
    "https://openalex.org/W6600140940",
    "https://openalex.org/W2939507640",
    "https://openalex.org/W2950527268",
    "https://openalex.org/W3012721701",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3035453001",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2116612304",
    "https://openalex.org/W3097188301",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2541884796",
    "https://openalex.org/W2900120080",
    "https://openalex.org/W2923978210",
    "https://openalex.org/W2946193510",
    "https://openalex.org/W2917462349",
    "https://openalex.org/W2807006176",
    "https://openalex.org/W3006555759"
  ],
  "abstract": null,
  "full_text": "Scaling Federated Learning for Fine-tuning of Large Language Models\nAgrin Hilmkil*, Sebastian Callh*, Matteo Barbieri*, Leon Ren´e S¨utfeld+, Edvin Listo Zec+, and Olof Mogren+\n*Peltarion, peltarion.com\n+RISE Research Institutes of Sweden, ri.se\nAbstract\nFederated learning (FL) is a promising ap-\nproach to distributed compute, as well as dis-\ntributed data, and provides a level of privacy\nand compliance to legal frameworks. This\nmakes FL attractive for both consumer and\nhealthcare applications. While the area is ac-\ntively being explored, few studies have exam-\nined FL in the context of larger language mod-\nels and there is a lack of comprehensive re-\nviews of robustness across tasks, architectures,\nnumbers of clients, and other relevant factors.\nIn this paper, we explore the ﬁne-tuning of\nTransformer-based language models in a fed-\nerated learning setting. We evaluate three pop-\nular BERT-variants of different sizes (BERT,\nALBERT, and DistilBERT) on a number of\ntext classiﬁcation tasks such as sentiment anal-\nysis and author identiﬁcation. We perform an\nextensive sweep over the number of clients,\nranging up to 32, to evaluate the impact of dis-\ntributed compute on task performance in the\nfederated averaging setting. While our ﬁnd-\nings suggest that the large sizes of the eval-\nuated models are not generally prohibitive to\nfederated training, we found that the different\nmodels handle federated averaging to a vary-\ning degree. Most notably, DistilBERT con-\nverges signiﬁcantly slower with larger num-\nbers of clients, and under some circumstances,\neven collapses to chance level performance.\nInvestigating this issue presents an interesting\nperspective for future research.\n1 Introduction\nTransformer-based architectures such as BERT\nhave recently lead to breakthroughs in a variety\nof language-related tasks, such as document clas-\nsiﬁcation, sentiment analysis, question answering,\nand various forms of text-mining (Vaswani et al.,\n2017; Devlin et al., 2019; Adhikari et al., 2019; Sun\net al., 2019a; Yang et al., 2019; Lee et al., 2020).\nThese models create semantic representations of\ntext, which can subsequently be used in many\ndownstream tasks (Devlin et al., 2019). The train-\ning process for Transformers typically includes two\nphases: During pre-training, the model learns to\nextract semantic representations from large, task-\nindependent corpora. The pre-training is followed\nby task-speciﬁc ﬁne-tuning on a separate dataset to\noptimize model performance further.\nIn this paper, we study the effects of ﬁne-tuning\nTransformer-based architectures in a federated\nlearning (FL) setting. In FL, models are trained in\na decentralized fashion on a number of local com-\npute instances, called clients, and intermittently\naggregated and synchronized via a central server.\nAs such, FL is a solution for distributed compute,\nas well as distributed data, and provides a level\nof privacy with regards to the sharing of personal\nor otherwise sensitive data. Model aggregation is\ncommonly performed via averaging of the weights\nof the individual client models, called Federated\nAveraging (FEDAVG) (McMahan et al., 2017a).\nDepending on the application, the number of\nclients in an FL setting can differ wildly. In in-\nstances where smartphones are used as clients,\ntheir number can reach into the millions (Hard\net al., 2018), whereas settings with higher com-\npute requirements and more data per client will\noften range between a handful and a few dozens\nof clients. Here, we focus on the latter, as training\nlarge language models requires a lot of compute.\nA potential application of this is the medical ﬁeld,\nin which automated analyses of electronic health\nrecords yield enormous potential for diagnostics\nand treatment-related insights (Zeng et al., 2018).\nOur contribution: We provide a comprehensive\noverview of the applicability of the federated learn-\ning setting to large language models. To this end,\nwe work with a ﬁxed computation budget for each\ntask, and use a ﬁxed total amount of data while\narXiv:2102.00875v1  [cs.LG]  1 Feb 2021\nvarying the number of clients between which the\ndata is split up. This way, we isolate the effects\nof distributing data over several clients for dis-\ntributed compute. We leave comparisons with a\nﬁxed amount of data per client and varying non-\ni.i.d. data distributions between clients for future\nwork. The main contributions of this paper are the\nfollowing: (1) We provide a comparison of three\npopular Transformer-based language models in the\nfederated learning setting, using the IMDB, Yelp F,\nand AG News datasets. (2) We analyze how the\nnumber of clients impacts task performance across\ntasks and model architectures.\n2 Related work\nFederated optimization was ﬁrst introduced by\n(Koneˇcn`y et al., 2015). The key challenges in\nthis paradigm are communication efﬁciency when\nlearning from many clients, privacy concerns with\nrespect to leakage of client data, and variability in\ndata distributions between clients (non-i.i.d. set-\nting). FEDAVG (McMahan et al., 2017a) solves\nthe federated optimization problem by building\na global model based on local stochastic gradi-\nent descent updates and has been shown to work\non non-i.i.d. data. Since then, many adaptations\nhave arisen (Li et al., 2019; Mohri et al., 2019;\nKarimireddy et al., 2019). Guha et al. (2019) pro-\nposes a one-shot FL algorithm, learning a global\nmodel efﬁciently in just one communication round.\nZhao et al. (2018), Hsu et al. (2019) and Listo Zec\net al. (2020) study effects of FEDAVG and non-\ni.i.d. client data. McMahan et al. (2017b) and Hard\net al. (2018) train large recurrent language models\nwith user-level differential privacy guarantees and\nfor mobile keyboard prediction, respectively. Ge\net al. (2020) use federated learning for named entity\nrecognition with heterogeneous medical data.\nRegarding model size, most architectures used in\nFL to date are relatively small (e.g., CIFG for mo-\nbile keyboard prediction: 1.4M parameters (Hard\net al., 2018)), compared to BERT-based language\nmodels with hundreds of millions of parameters.\nHow these very large models behave under FE-\nDAVG remains underexplored. To the best of our\nknowledge, Lin et al. (2020) and Liu and Miller\n(2020) are the ﬁrst ones to train large Transformer\nmodels in a federated setting. Liu and Miller (2020)\ntrained BERT on a medical corpus and showed that\nboth pre-training and ﬁne-tuning could be done in a\nfederated manner with only minor declines in task\nperformance. Nonetheless, the study is mainly a\nproof-of-concept and does not explore many of the\nfactors that can be expected in real-world scenarios.\nFor instance, the authors only used ﬁve clients, and\nevaluated them only on i.i.d. data. Lin et al. (2020)\nintroduces FedDF, an ensemble distillation algo-\nrithm for model fusion. The authors train a central\nmodel through unlabeled data on the client models\noutputs, and perform ﬁne-tuning on a pre-trained\nDistilBERT (Sanh et al., 2019) in a federated set-\nting as a baseline. To the best of our knowledge, no\nsystematic variation of the number of clients and\nother relevant factors has previously been explored\nin this context.\n3 Method\n3.1 Federated learning\nFederated learning aims to solve the optimization\nproblem\nmin\nθ∈Rd\n1\nK\nK∑\nk=1\nFk(θ), (1)\nwhere Fk(θ) = Ex∼Dk [ℓk(θ; x)] is the expected\nloss on client kand Dk is the data distribution of\nclient k. In FEDAVG, a global model fθ is ini-\ntialized on a central server and distributed to all\nKclients, each of which then trains its individual\ncopy of the network using SGD for Elocal epochs\nwith local batch size B. The clients’ updated pa-\nrameters are then averaged on the central server,\nweighted by the local data size at each client. The\naveraged model is distributed to the clients again,\nand the process is repeated for a deﬁned number of\ncommunication rounds.\nWe implement FEDAVG using distributed Py-\nTorch (Paszke et al., 2019). For each experiment\nwe start from a pre-trained model, and ﬁne-tune it\nwith federated averaging on the current task.\n3.2 Models\nWe include BERT with 110M parameters, 12 lay-\ners (Devlin et al., 2019), ALBERT with 11M pa-\nrameters, 12 layers (Lan et al., 2020) and Distil-\nBERT with 65M parameters, 6 layers (Sanh et al.,\n2019). This allows us to study the effect that both\nthe parameter count and the number of layers have\non FEDAVG. All models are the corresponding\nbase models pre-trained on (cased) English. In par-\nticular, it should be noted that while the models\nhave similar architectures, they have some key dif-\nferences. ALBERT introduces factorized embed-\nding parameterization and cross-layer parameter\nsharing, while the DistilBERT model is a student\nnetwork trained with knowledge distillation from\nBERT. We use the weights and implementations\nof the models available in the Huggingface Trans-\nformers library (Wolf et al., 2019).\n3.3 Datasets\nWe performed experiments on three standard\ndatasets to assess the performance of the proposed\napproach on different tasks. All of them pose classi-\nﬁcation problems with a different number of target\ncategories and dataset sizes. For each dataset, we\nuse the test set speciﬁed by the source.\nIMDB. The Large Movie Review Dataset (Maas\net al., 2011) contains of a collection of 50,000\nmovie reviews and their associated binary senti-\nment polarity labels (either “positive” or “nega-\ntive”), which is used to train a sentiment classiﬁer.\nYelp F. This dataset (Zhang et al., 2015) contains\nreviews of local businesses and their associated rat-\ning (1-5). The task is posed as a text classiﬁcation\ntask, from the review text to its associated rating.\nAG News. The AG’s corpus of news articles 1\nconsists of over one million news articles gathered\nfrom more than 2,000 news sources, divided into\na number of categories depending on their content.\nWe used the common subset (Zhang et al., 2015) of\nthe whole dataset, consisting of a total of 120,000\nsamples equally divided in four categories.\n3.4 Experiments and hyperparameters\nWe construct several experiments to evaluate how\nwell Federated Learning scales to an exponentially\nincreasing number of clients. In all experiments,\nthe respective dataset is evenly partitioned into a\nnumber of subsets equal to the number of clients.\nData points are uniformly sampled on each client\n(i.i.d.) like (McMahan et al., 2017a). We do not\nperform any hyperparameter tuning, and instead,\nkeep all other hyperparameters constant for an un-\nbiased comparison. As baselines we run for each\ntask and BERT-variant a non-federated scenario\n(clients = 1) with the same conﬁguration.\nWe run the baselines for a ﬁxed number of\nrounds based on our compute budget. The test set\nperformance for the baselines are then compared\nagainst varying number of participating clients at\n1http://groups.di.unipi.it/˜gulli/AG_\ncorpus_of_news_articles.html\nthe same number of rounds. Finally, since runs with\na larger number of clients converge more slowly,\nwe allow those runs to continue to a second thresh-\nold and report the number of rounds required to\nreach 90% of the baseline performance, similar to\nMcMahan et al. (2017a). Runs not reaching90% of\nthe baseline performance within the second thresh-\nold are reported as failures.\nWe run the baseline for 100 rounds for both\nIMDB and AG News while setting the second\nthreshold to 200 rounds. However, we only run\nYelp F baselines for 50 rounds due to its large size\nand set the second threshold at 100 rounds. Like\nLin et al. (2020), we avoid momentum, weight\ndecay, and dynamic learning rates for simplicity.\nInstead, all experiments are performed with SGD.\nBased on Sun et al. (2019b) we choose the constant\nlearning rate 2 ·10−5, maximum sequence length\n128 and batch size ( B) of 32. Furthermore, the\nnumber of local epochs (E) is set to 2 per round.\n4 Results\n4.1 Fixed compute budget\n1 2 4 8 16 32\nClients\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nAG  News\nmodel\nalbert\nbert\ndistilbert\n1 2 4 8 16 32\nClients\nIMDB\n1 2 4 8 16 32\nClients\nYelp F\nFigure 1: Accuracy at a ﬁxed compute budget of 100\nrounds for AG, IMDB, and 50 rounds for Yelp F. The\nexpected accuracy of a random classiﬁer for each task\nhas been highlighted in the dashed line. Higher is bet-\nter.\nIn Figure 1, we study the effect of increasing the\nnumber of clients. It shows the ﬁnal accuracy after\n100 rounds IMDB and AG News, and 50 rounds\nof the much larger Yelp F., with an exponentially\nincreasing number of clients. Both ALBERT and\nBERT are well behaved and exhibit a gradual de-\ncrease with an increasing number of clients. How-\never, DistilBERT shows a much steeper decline\nwhen moving past 4 clients for all datasets, down\nto the random classiﬁer baseline (IMDB, Yelp F).\n1 2 4 8 16 32\nClients\n0\n50\n100\n150Rounds\nAG  News\nmodel\nalbert\nbert\ndistilbert\n1 2 4 8 16 32\nClients\nIMDB\n1 2 4 8 16 32\nClients\nYelp F\nFigure 2: Number of training rounds required to reach\n90% of the non-federated baseline accuracy. Omittions\noccur when the target is not reached in 100 (Yelp F) or\n200 rounds (AG News, IMDB). Lower is better.\n4.2 Rounds until target performance\nExamining the number of rounds necessary to\nachieve 90% of the non-federated baseline accuracy\n(Figure 2) yields a similar observation. While all\nmodels perform worse with more clients, ALBERT\nand BERT mostly reach the target accuracy within\nthe allocated number of rounds until 32 clients are\nused. DistilBERT on the other is unable to reach\nthe target accuracy at 16 clients for Yelp F, and as\nlow as 4 clients for IMDB.\n4.3 Dynamics of ﬁne-tuning\n0.2\n0.4\n0.6\n0.8IMDB: Accuracy\nBERT ALBERT DistilBERT\n0.2\n0.4\n0.6\n0.8Yelp F: Accuracy\n0 50 100\nRound\n0.2\n0.4\n0.6\n0.8AG News: Accuracy\n0 50 100\nRound\n0 50 100\nRound\nclients\n1 2 4 8 16 32\nFigure 3: Test accuracy (higher is better) over commu-\nnication rounds for our scenarios. The random classi-\nﬁer baseline is shown as a dashed line.\nThe test accuracy during ﬁne-tuning (Figure 3)\nallows a more complete understanding of how well\nFEDAVG scales for language model ﬁne-tuning.\nWhile some scenarios (e.g. Yelp F. with BERT)\nshow a gradual degradation with the number of\nclients, others conﬁgurations are more adversely af-\nfected by the increasing number of clients. In some\ninstances the accuracy stays constant over a large\nperiod, sometimes even at the random classiﬁer\nbaseline for the whole (DistilBERT on IMDB) or\npart (DistilBERT on AG News) of the experiment\nwhen the number of clients is high.\n5 Discussion\nIn this paper, we have evaluated the performance of\nTransformer-based language models ﬁne-tuned in a\nfederated setting. While BERT and ALBERT seem\nto learn each task quickly (Figure 3), DistilBERT\nhas a much slower learning progression in the fed-\nerated setup. A possible explanation is the process\nof distillation during pre-training, but further re-\nsearch is needed to fully understand why this hap-\npens. We demonstrated that BERT and ALBERT\nscale well up to 32 clients with no sharp decline in\nperformance (Figure 1), but found DistilBERT to\nstruggle at 16 clients in the Yelp F and AG News\ntasks, and with as low as 4 clients in the IMDB task,\nwith a substantial drop in performance compared to\nthe baseline. Furthermore, DistilBERT takes more\nrounds to achieve the same performance. These\nresults demonstrate that we have obtained a higher\ncommunication efﬁciency for BERT and ALBERT\nas compared to DistilBERT. Further work is re-\nquired to get a good picture of exactly what affects\nthe communication efﬁciency for federated learn-\ning of Transformer-based language models.\nConversely, the sudden drop in performance in\nsome scenarios indicates that FL can be sensitive to\nthe number of clients. The cause for the instability\nhas not been fully determined. It may be related to\nboth smaller partitions and contradicting models.\nThis highlights the importance of evaluating FL\nwith a varying number of clients at these scales.\nFor all three models, the performance decline\nfrom the baseline is steeper for IMDB compared to\nthe other tasks. This may be related to the variabil-\nity in the movie review data, adding to a larger inter-\nclient difference in data distribution when data is\nput into smaller partitions, resulting in a larger dif-\nference between the client models taking part in\nthe federated averaging.\nIn conclusion, we have demonstrated the applica-\nbility of the federated learning paradigm and evalu-\nated it on a number of Transformer-based models\nup to 32 clients. Our ﬁndings show that the rela-\ntively large sizes of these models are not prohibitive\nfor federated learning.\n6 Acknowledgements\nThis work was funded by VINNOV A (grant\n2019-05156). We would also like to thank\nAI Sweden and CGit for providing us with com-\npute resources.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and\nJimmy Lin. 2019. Docbert: Bert for document clas-\nsiﬁcation. arXiv preprint arXiv:1904.08398.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT (1), pages 4171–4186.\nSuyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi,\nYongfeng Huang, and Xing Xie. 2020. Fedner: Med-\nical named entity recognition with federated learn-\ning. arXiv preprint arXiv:2003.09288.\nNeel Guha, Ameet Talwalkar, and Virginia Smith.\n2019. One-shot federated learning. arXiv preprint\narXiv:1902.11175.\nAndrew Hard, Kanishka Rao, Rajiv Mathews,\nFranc ¸oise Beaufays, Sean Augenstein, Hubert\nEichner, Chlo ´e Kiddon, and Daniel Ramage. 2018.\nFederated learning for mobile keyboard prediction.\nCoRR, abs/1811.03604.\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown.\n2019. Measuring the effects of non-identical data\ndistribution for federated visual classiﬁcation. arXiv\npreprint arXiv:1909.06335.\nSai Praneeth Karimireddy, Satyen Kale, Mehryar\nMohri, Sashank J Reddi, Sebastian U Stich, and\nAnanda Theertha Suresh. 2019. Scaffold: Stochas-\ntic controlled averaging for on-device federated\nlearning. arXiv preprint arXiv:1910.06378.\nJakub Kone ˇcn`y, Brendan McMahan, and Daniel Ra-\nmage. 2015. Federated optimization: Distributed\noptimization beyond the datacenter. arXiv preprint\narXiv:1511.03575.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nTian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia\nSmith. 2019. Fair resource allocation in federated\nlearning. arXiv preprint arXiv:1905.10497.\nTao Lin, Lingjing Kong, Sebastian U Stich, and Mar-\ntin Jaggi. 2020. Ensemble distillation for robust\nmodel fusion in federated learning. arXiv preprint\narXiv:2006.07242.\nEdvin Listo Zec, Olof Mogren, John Martinsson,\nLeon Ren´e S¨utfeld, and Daniel Gillblad. 2020. Fed-\nerated learning using a mixture of experts.\nDianbo Liu and Tim Miller. 2020. Federated pretrain-\ning and ﬁne tuning of bert using clinical notes from\nmultiple silos.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017a.\nCommunication-Efﬁcient Learning of Deep Net-\nworks from Decentralized Data. In Proceedings of\nthe 20th International Conference on Artiﬁcial Intel-\nligence and Statistics, volume 54, pages 1273–1282,\nFort Lauderdale, FL, USA. PMLR.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2017b. Learning differentially pri-\nvate recurrent language models. arXiv preprint\narXiv:1710.06963.\nMehryar Mohri, Gary Sivek, and Ananda Theertha\nSuresh. 2019. Agnostic federated learning. In Pro-\nceedings of Machine Learning Research, volume 97,\npages 4615–4625, Long Beach, California, USA.\nPMLR.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. In EMC 2\nWorkshop at NeurIPS 2019.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019a. Uti-\nlizing bert for aspect-based sentiment analysis via\nconstructing auxiliary sentence. arXiv preprint\narXiv:1903.09588.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019b. How to ﬁne-tune bert for text classiﬁcation?\nIn Chinese Computational Linguistics , pages 194–\n206, Cham. Springer International Publishing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. arXiv preprint arXiv:1902.01718.\nZexian Zeng, Yu Deng, Xiaoyu Li, Tristan Nau-\nmann, and Yuan Luo. 2018. Natural language pro-\ncessing for ehr-based computational phenotyping.\nIEEE/ACM transactions on computational biology\nand bioinformatics, 16(1):139–153.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 28 , pages\n649–657. Curran Associates, Inc.\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda,\nDamon Civin, and Vikas Chandra. 2018. Feder-\nated learning with non-iid data. arXiv preprint\narXiv:1806.00582.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8328863382339478
    },
    {
      "name": "Federated learning",
      "score": 0.7166415452957153
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6356620788574219
    },
    {
      "name": "Language model",
      "score": 0.6201695203781128
    },
    {
      "name": "Scaling",
      "score": 0.5217905044555664
    },
    {
      "name": "Transformer",
      "score": 0.4833519160747528
    },
    {
      "name": "Data science",
      "score": 0.46584901213645935
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.44218504428863525
    },
    {
      "name": "Context (archaeology)",
      "score": 0.44083666801452637
    },
    {
      "name": "Machine learning",
      "score": 0.41800999641418457
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4087231159210205
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800664555",
      "name": "RISE Research Institutes of Sweden",
      "country": "SE"
    }
  ]
}