{
  "title": "Parameter-efficient fine-tuning in large language models: a survey of methodologies",
  "url": "https://openalex.org/W4410061981",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2114639371",
      "name": "Luping Wang",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A1549144106",
      "name": "Sheng Chen",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A2795258781",
      "name": "Linnan Jiang",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A2099392549",
      "name": "Shu Pan",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A2402939261",
      "name": "Runze Cai",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A2095688776",
      "name": "Sen Yang",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A2021846487",
      "name": "Fei Yang",
      "affiliations": [
        "First People's Hospital of Yuhang District"
      ]
    },
    {
      "id": "https://openalex.org/A2114639371",
      "name": "Luping Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1549144106",
      "name": "Sheng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2795258781",
      "name": "Linnan Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099392549",
      "name": "Shu Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2402939261",
      "name": "Runze Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095688776",
      "name": "Sen Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2021846487",
      "name": "Fei Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2473418344",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W4402716390",
    "https://openalex.org/W6600609363",
    "https://openalex.org/W3206907172",
    "https://openalex.org/W4385573776",
    "https://openalex.org/W6600438464",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W6605475740",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W6822342538",
    "https://openalex.org/W4324304837",
    "https://openalex.org/W6780161852",
    "https://openalex.org/W4287888919",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W6600050674",
    "https://openalex.org/W4280529031",
    "https://openalex.org/W6600459194",
    "https://openalex.org/W4386075538",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W4389523915",
    "https://openalex.org/W4389520703",
    "https://openalex.org/W4387848667",
    "https://openalex.org/W4402916137",
    "https://openalex.org/W4386065512",
    "https://openalex.org/W4389523703",
    "https://openalex.org/W4386566605",
    "https://openalex.org/W6604963970",
    "https://openalex.org/W4402671950",
    "https://openalex.org/W4389520674",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W4389524340",
    "https://openalex.org/W6601903828",
    "https://openalex.org/W2914397182",
    "https://openalex.org/W4382202657",
    "https://openalex.org/W4393159845",
    "https://openalex.org/W7035904466",
    "https://openalex.org/W6601899773",
    "https://openalex.org/W2739874095",
    "https://openalex.org/W3199258042",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W6601399257",
    "https://openalex.org/W6607045254",
    "https://openalex.org/W4385573116",
    "https://openalex.org/W4390872341",
    "https://openalex.org/W6603374586",
    "https://openalex.org/W6600407735",
    "https://openalex.org/W4385573520",
    "https://openalex.org/W4390874728",
    "https://openalex.org/W6633624738",
    "https://openalex.org/W6606545560",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W6826116265",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W6601202783",
    "https://openalex.org/W4386075814",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W6603695571",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W6756688054",
    "https://openalex.org/W4382463911",
    "https://openalex.org/W4390873067",
    "https://openalex.org/W6600511658",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W4379140845",
    "https://openalex.org/W4232035701",
    "https://openalex.org/W4385570673",
    "https://openalex.org/W6600208142",
    "https://openalex.org/W4406272066",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3206832494",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W6600103761",
    "https://openalex.org/W6600234944",
    "https://openalex.org/W6600291067",
    "https://openalex.org/W4392560653",
    "https://openalex.org/W4385570815",
    "https://openalex.org/W3103616906",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W6602670149",
    "https://openalex.org/W4385574214",
    "https://openalex.org/W4402727764",
    "https://openalex.org/W4400525231",
    "https://openalex.org/W6600351811",
    "https://openalex.org/W6601289607",
    "https://openalex.org/W6600062020",
    "https://openalex.org/W6600194071",
    "https://openalex.org/W4386187806",
    "https://openalex.org/W4402671248",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W4389520688",
    "https://openalex.org/W4385567113",
    "https://openalex.org/W3206816211",
    "https://openalex.org/W4409263059",
    "https://openalex.org/W6603795979",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4393148714",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W6727391228",
    "https://openalex.org/W3170806096",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W6600487154",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W6603143895",
    "https://openalex.org/W4404783357",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W4386072096",
    "https://openalex.org/W6814003322",
    "https://openalex.org/W6602341811",
    "https://openalex.org/W6601241631",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W6601804787",
    "https://openalex.org/W6603806335",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W6606322225",
    "https://openalex.org/W4389524426",
    "https://openalex.org/W4389518686",
    "https://openalex.org/W6600966031",
    "https://openalex.org/W4386072470",
    "https://openalex.org/W4386566659",
    "https://openalex.org/W6922522607",
    "https://openalex.org/W4385537492",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W4312868944",
    "https://openalex.org/W4386482362",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W4389520746",
    "https://openalex.org/W6600339457",
    "https://openalex.org/W6600577311",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W4385572883",
    "https://openalex.org/W4393148183",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W4409263168",
    "https://openalex.org/W4224115290",
    "https://openalex.org/W6600281192",
    "https://openalex.org/W4399458167",
    "https://openalex.org/W4390872006",
    "https://openalex.org/W6822711580",
    "https://openalex.org/W4401545735",
    "https://openalex.org/W4392910564",
    "https://openalex.org/W3199761064",
    "https://openalex.org/W6601375367",
    "https://openalex.org/W4386076397",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4386566869",
    "https://openalex.org/W6600480908",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W6603717528",
    "https://openalex.org/W6607733003",
    "https://openalex.org/W6632164465",
    "https://openalex.org/W4386083036",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4385571686",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W6600195515",
    "https://openalex.org/W4388994251",
    "https://openalex.org/W6600514347",
    "https://openalex.org/W4390873054",
    "https://openalex.org/W6600237248",
    "https://openalex.org/W4402670757",
    "https://openalex.org/W6603054696",
    "https://openalex.org/W6604801084",
    "https://openalex.org/W6600020652",
    "https://openalex.org/W4385570973",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W4385573646",
    "https://openalex.org/W4389518748",
    "https://openalex.org/W4401042689",
    "https://openalex.org/W4389519349",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3200396895",
    "https://openalex.org/W4401042993",
    "https://openalex.org/W3100560913",
    "https://openalex.org/W3099366427"
  ],
  "abstract": null,
  "full_text": "Accepted: 10 April 2025 / Published online: 3 May 2025\n© The Author(s) 2025\nExtended author information available on the last page of the article\nParameter-efficient fine-tuning in large language models: a \nsurvey of methodologies\nLuping Wang1 · Sheng Chen1 · Linnan Jiang1 · Shu Pan1 · Runze Cai1 · Sen Yang1 · \nFei Yang1\nArtificial Intelligence Review (2025) 58:227\nhttps://doi.org/10.1007/s10462-025-11236-4\nAbstract\nThe large language models, as predicted by scaling law forecasts, have made groundbreak-\ning progress in many fields, particularly in natural language generation tasks, where they \nhave approached or even surpassed human levels. However, the unprecedented scale of \ntheir parameters brings significant computational and storage costs. These large language \nmodels require substantial computational resources and GPU memory to operate. When \nadapting large language models to specific downstream tasks, their massive parameter \nscale poses a significant challenge in fine-tuning on hardware platforms with limited com -\nputational power and GPU memory. To address this issue, parameter-efficient fine-tuning \n(PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-\ntrained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters \nof pre-trained large language models to adapt to specific tasks or domains, minimizing \nthe introduction of additional parameters and the computational resources required. This \nreview mainly introduces the preliminary knowledge of PEFT, the core ideas and prin -\nciples of various PEFT algorithms, the applications of PEFT, and potential future research \ndirections. By reading this review, we believe that interested parties can quickly grasp the \nPEFT methodology, thereby accelerating its development and innovation.\nKeywords Fine-tuning · Parameter-efficient · Large language model · Deep learning · \nArtificial intelligence\n1 Introduction\nIn recent years, large pre-trained models, commonly referred to as “large language models”, \nhave emerged as a significant advancement in the field of artificial intelligence. Due to their \noutstanding performance and versatility in various application contexts, these models have \nattracted plenty of attention and provoked much discussion. These models have impressive \ncomputing capabilities and extensive data resources, allowing them to excel in tackling \nintricate jobs. Within the field of natural language processing (NLP), notable interest is \ngiven to large language models (LLMs). These models demonstrate remarkable ingenuity \n et al. [full author details at the end of the article]\n1 3\nL. Wang et al.\nin text generation (Wu et al. 2023; Li et al. 2024b), machine translation (Zhu et al. 2023; \nWang et al. 2023a), personalized chatbots (Zheng et al. 2023; Kim et al. 2023; Dan et al. \n2023), text summarization (Zhang et al. 2019), sentiment analysis (Zhang et al. 2023a), and \nquestion-answering systems (Pan et al. 2024).\nNevertheless, the development of LLMs faces significant challenges and controver -\nsies. These models require substantial computational resources and data support, which \ncan potentially jeopardize the environment and compromise privacy protection (Yao et al. \n2024). Despite their impressive performance in specific tasks, these models still have limi -\ntations and error rates that need continuous optimization and improvement (Huang et al. \n2024a; Huang and Chang 2022; Saparov and He 2022). When directly using LLMs for \nspecific tasks, their performance often falls below desired levels. Consequently, fine-tuning \nLLMs has become a crucial method for enhancing model performance.\nParameter-efficient fine-tuning (PEFT) is a transfer learning method specifically devel -\noped to adapt the parameters of the large pre-trained models to suit new tasks and scenarios. \nThis approach involves dynamically adjusting the model to enhance its effectiveness in per-\nforming certain tasks, taking into account the distinct features and requirements of the target \ntask. The fine-tuning process typically entails improving the model architecture (Houlsby \net al. 2019), optimizing parameters (Li and Liang 2021; Lester et al. 2021), and adapting \nlearning strategies (Chen et al. 2020), among other considerations, to achieve better per -\nformance in new tasks. As the field of deep learning continues to evolve, techniques for \noptimizing and fine-tuning LLMs have also made significant advancements. Notable PEFT \napproaches include LoRA (Hu et al. 2021), adapter tuning (Zhang et al. 2023f), prefix tun-\ning (Li and Liang 2021), prompt tuning (Lester et al. 2021), P tuning (Liu et al. 2024), Bit-\nFit (Zaken et al. 2021), and others. However, despite the significant achievements of large \nmodel fine-tuning techniques across several fields, there are always challenges and difficul-\nties that need to be resolved. Overfitting mitigation, optimizing fine-tuning efficiency, and \nstriking a learning balance between pre-training and fine-tuning tasks are a few examples of \nissues that need more investigation.\nIn recent years, hundreds of articles on PEFT have been published, with some studies \noffering informative overviews of the most prevalent approaches. A comparative analysis of \nthese surveys in terms of taxonomy and application is shown in Table 1.\nDing et al. (2022) introduce a theoretical abstraction for Delta Tuning, which is analyzed \nfrom the viewpoints of optimization and optimum control. This abstraction offers a uni -\nfied approach to describe the current PEFT methods which provides a distinct perspective \nfor future investigations. Nonetheless, while the study predominantly concentrates on NLP \napplications, the generalizability and efficacy of these methods in diverse domains merit \nTable 1 A comparative analysis of survey methodologies: taxonomy and application domains\nSurvey Taxonomy Application\nAdd Sel Rep Hybrid Unified NLP Vision Multi Diffusion\nDing et al. (2022) ✓ ✓ ✓ ✓\nLialin et al. (2023) ✓ ✓ ✓ ✓\nXu et al. (2023a) ✓ ✓ ✓ ✓ ✓ ✓ ✓\nXin et al. (2024) ✓ ✓ ✓ ✓\nHan et al. (2024) ✓ ✓ ✓ ✓ ✓ ✓ ✓\nOurs ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\nAdd. Additive, Sel., selective, Rep. reparameterized , Multi. multi-task, Diffusion diffusion model\n1 3\n227 Page 2 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nadditional investigation. Lialin et al. ( 2023) provide a comprehensive analysis and clas -\nsification that covers a broad range of methods and compares approximately 30 approaches \nacross five dimensions: storage efficiency, memory efficiency, computational efficiency, \naccuracy, and inference overhead. However, while the article primarily focuses on detailed \nmethods with practical efficiency for fine-tuning multibillion-scale language models, the \nexploration of real-world application scenarios is relatively limited. Xu et al. ( 2023a) \nprovide a thorough evaluation and analysis of current PEFT approaches, assessing their \nperformance, parameter efficiency, and memory utilization within a range of NLP tasks. \nNonetheless, the paper does not fully expound on the practical applications of these meth -\nodologies in actual operational environments, nor does it deeply investigate their adapt -\nability and the domain-specific challenges they might encounter. Xin et al. (2024) offer a \ncomprehensive overview and future directions for visual PEFT, with a systematic review of \nthe latest advancements. While the article spans multiple visual tasks, the experiments are \nprimarily focused on several common tasks and do not fully encompass the broader range of \npotential application scenarios. Han et al. (2024) provide a detailed classification of PEFT \napproaches and explores the application of PEFT techniques across various model architec-\ntures and downstream tasks, as well as the systematic design challenges of PEFT methods. It \noffers researchers and engineers a comprehensive overview of PEFT approaches, but there \nis still room for improvement in terms of practical application coverage.\nOur contributions are as follows:\n ● This survey comprehensively reviews the latest literature PEFT, covering cutting-edge \nmethods and related research. It establishes a theoretical framework and offers a solid \nknowledge base for future research.\n ● We make extensive use of intuitive schematic diagrams and structured tables to elabo -\nrate on PEFT methodologies. By means of visualization, we demonstrate the complex \nprinciples of these methods, carry out comparative analyses of different approaches, and \norganically combine intuitiveness with systematicness, which significantly enhances the \nreadability and academic value of the research content.\n ● Breaking traditional boundaries, this survey explores PEFT in natural language process-\ning, computer vision, multimodal fusion, and diffusion models. It uncovers application \npotential, offers practical guidelines, and broadens the application scope of fine-tuning \ntechnology.\nThis survey aims to comprehensively review the recent advancements in large model fine-\ntuning techniques. By conducting a thorough examination of existing research, our objec -\ntive is to identify and fill the gaps in our current knowledge system. This will result in \nthe development of a comprehensive and systematic framework of knowledge, which will \nprovide researchers with a concise perspective on the topic and guide their future research. \nIn conclusion, our work offers valuable resources and perspectives that can be utilized for \nboth academic and practical purposes in related domains. The remainer of this survey is \nstructured in the following manner:\nIn Sect. 2, we offer a succinct summary of the fundamental components of LLMs, includ-\ning their past development, emerging capabilities, and the scaling laws that govern their size. \nSubsequently, we offer a brief overview of the dominant classifications of comprehensive \nlanguage models and introduce the fundamental principles and framework of multi-modal \n1 3\nPage 3 of 64 227\nL. Wang et al.\ncomprehensive models. Furthermore, we investigate the primary methodologies employed \nin the fine-tuning domain of extensive language models, including instruction fine-tuning, \nalignment, and Reinforcement Learning from Human Feedback (RLHF). Ultimately, we \npresent a brief summary of the most used benchmarks and assessment datasets in the field \nof big model fine-tuning.\nIn Sect. 3, we offer a comprehensive analysis and summary of PEFT approaches, present-\ning a cohesive framework for classifying current PEFT methodologies, encompassing over \n100 research articles published from June 2019 to July 2024. Expanding on the conventional \ntripartite classification of additive, reparameterized, and subtractive PEFT, we incorporate \nsummaries of hybrid, quantization, and multi-task categorization PEFT approaches.\nIn Sect. 4, we present a comprehensive analysis and description of the prevailing PEFT \napproaches in the fields of multimodal, visual, and diffusion models. Our objective is to \nprovide a deep understanding and recommendations for choosing and improving PEFT in \ndifferent application scenarios.\nIn Sect. 5, we encapsulate our extensive survey and put forward multiple promising \navenues for future advancements, encompassing both algorithmic refinements and task sce-\nnarios, hoping to provide valuable insights for further research and development in this \nburgeoning field.\n2 Preliminary\n2.1 Large language models: foundations and variants\n2.1.1 Large language models\n2.1.1.1 Background LLMs refer to neural language models with a large number of param-\neters, typically over billions of parameters. These models are built on the transformer archi-\ntecture (Vaswani et al. 2017) and are pre-trained on vast text corpora (Devlin et al. 2018). \nPrior to the emergence of LLMs, the advent of transformers revolutionized the develop -\nment approach for neural language models, shifting from end-to-end training to a pre-train \nthen fine-tune paradigm. Under the pre-train fine-tune paradigm, pre-trained models can \nbe repeatedly utilized, significantly enhancing the scalability of neural language models. \nConsequently, the scale of parameters is continuously growing larger. For instance, Ope -\nnAI’s GPT-1 possessed 120 million parameters, while GPT-2 boasted 1.5 billion param -\neters. This number surged to 175 billion for GPT-3 and soared to 1.76 trillion for the latest \nGPT-4 (Achiam et al. 2023).\n2.1.1.2 Emergent abilities Research suggests that the rapid expansion of the parameter \nscale may lead to emergent abilities (Wei et al. 2022), which are formally defined as abili -\nties that are not present in small models but arise in LLMs, constituting one of the most \n1 3\n227 Page 4 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nprominent characteristics distinguishing LLM from previous PLM. In conclusion, emerging \nabilities can be categorized into threefolds.\nIn-context learning In-context learning (Wei et al. 2022; Sanh et al. 2021), known as \nICL defined in GPT-3 (Brown et al. 2020), illustrates the ability of LLMs to acquire new \ntask capabilities based on a small set of examples in context. Importantly, this process does \nnot require additional training or gradient updates, indicating that the LLM is capable of \ncompleting new tasks with only prompts. In addition, Wei et al. ( 2022) reveals that ICL is \nassociated with both the LLM and the downstream task.\nInstruction following. Natural language descriptions, known as instructions, are essen-\ntial for fine-tuning LLMs. Instruction tuning organizes fine-tuning datasets in the format of \nnatural language descriptions (instructions). Research (Ouyang et al. 2022) shows that with \ninstruction tuning, LLMs are enabled to follow task instructions for new tasks without using \nexplicit examples, demonstrating better generalization capability across inputs of various \ntasks. Chung et al. ( 2024) discovered that to achieve evident efficacy, instruction tuning \nshould be conducted on a relatively large-scale LLM, e.g., over 60B parameters.\nStep-by-step reasoning. Constrained by parameter size, PLMs often struggle to solve \ntasks requiring intricate reasoning. In contrast, scaling up in parameter size equips language \nmodels with the Chain-of-Thought (CoT) (Wei et al. 2022). CoT enhances language models’ \nperformance on tasks involving logic, calculation, and decision making by structuring the \ninput prompt to human reasoning. Thanks to CoT, LLMs are enabled to tackle tasks that \ndemand intermediate reasoning steps to derive the final answer, akin to constructing a step-\nby-step prompt that invokes a thinking and inference process within the model.\nEmergent abilities in LLMs have significantly boosted various real-world applications, \nacross fields such as natural language (Kalla et al. 2023; Team et al. 2023; Liu et al. 2024b), \nhealthcare (Wang 2023; Biswas 2023), legal (Li et al. 2024a), financial (Xing 2024) and \nmultiple scientific disciplines (Ahn et al. 2024; Wang et al. 2023d). Despite the promising \nemergent capabilities, there are three main limitations that restrict the further and deeper \napplications of LLMs. Firstly, the inconsistency across models and tasks. LLMs trained on \ndifferent architectures or datasets may demonstrate emergent behavior to varying degrees. \nSome models might excel in certain tasks while failing to exhibit the same level of ability in \nothers, resulting in unpredictable performance when applied to diverse real-world scenarios \n(Bommasani et al. 2021). Secondly, the hallucinations and factual errors. LLMs often gener-\nate text that is fluent and coherent. However, they can also produce hallucinations, outputs \nthat seem plausible but contain factual inaccuracies or misleading information (Bender et al. \n2021; Lin et al. 2021). This tendency is particularly problematic in contexts where precise \nand reliable information is crucial, such as legal, medical, or scientific applications. Finally, \nthe deficiency in deep understanding. The performance of LLMs largely stems from recog-\nnizing statistical patterns in vast datasets rather than a genuine semantic understanding of \nthe content (Bender et al. 2021). This superficial grasp of language limits their effectiveness \nin tasks requiring in-depth logical reasoning and nuanced comprehension across models \nand tasks.\nIn conclusion, emergent abilities grant LLMs remarkable problem-solving capabilities, \nthough they remain imperfect. To bridge the gap between LLMs and real-world applica -\ntions, integrating traditional algorithms, expert systems, or hybrid models may be necessary \nto enhance reliability, accuracy, and domain-specific expertise.\n1 3\nPage 5 of 64 227\nL. Wang et al.\n2.1.1.3 Scaling laws of LLMs Thanks to the exceptional scalability of the transformer \narchitecture (Vaswani et al. 2017), language models also exhibit high scalability. The scal-\ning laws for LLM describe how the model grows and performs as the volume of training \ndata increases.\nIn general, a scaling law includes four parameters, which also characterize a language \nmodel: (1) Parameters count N. The number of parameters of an LLM is often associated \nwith the number of transformer layers and the hidden size, except for some MoE LLMs. (2) \nData size D. In LLM, this refers to the number of tokens for training. (3) Computation cost \nC. This is typically measured in terms of time and computational resources. (4) Loss L. The \nperformance of training is usually evaluated by the training loss. There are two representa -\ntive scaling laws for transformer LLMs.\nThe Kaplan scaling law Proposed by Kaplan et al. (2020), the law examines the statisti-\ncal relations between the parameters C, N, D and L over a wide range of values, models and \ndata tokens. The relationships can be expressed through the following equations:\n \nL(N)=\n(Nc\nN\n)αN\n,α N ∼ 0.076,N c ∼ 8.8 × 1013  (1)\n \nL(D)=\n(Dc\nD\n)αD\n,α D ∼ 0.095,D c ∼ 5.4 × 1013  (2)\n \nL(C)=\n(Cc\nC\n)αC\n,α C ∼ 0.050,N c ∼ 3.1 × 108,  (3)\nwhere the loss L is influenced by parameters N, D, and C, shedding light on decision-making \nprocesses when computational resources are limited.\nThe Chinchilla scaling law Proposed by DeepMind (Hoffmann et al. 2022), the law pro-\nvides guidelines for compute-optimal training of LLMs, specifically when computational \nresources are limited. Through rigorous experiments spanning a wide range of model sizes \nfrom 70 M to 16B and dataset sizes from 5B to 500B tokens, they derived a scaling law with \ndifferent coefficients compared to Kaplan’s, as shown below:\n L(N,D )= E + A\nNα + B\nDβ , (4)\nwhere E denotes the loss of an ideal generative process on the test data. Fur -\nthermore, claimed by the research, the constants in this formula are \nα =0 .34,β =0 .28,A = 406.4,B = 410.7,L 0 =1 .69. Moreover, there is a general con -\nstraint that model the relationship between C and (N, D): C =6 ND, which means that it \ncosts six FLOPs per parameter to train one token. Thus, the optimal selection of model size \nand data size can be determined and expressed as:\n Nopt =0 .6 C0.45  (5)\n1 3\n227 Page 6 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\n Dopt =0 .3 C0.55  (6)\n Lopt = 1070C−0.154 +1 .7.  (7)\nFrom the equations, scaling laws can guide decisions regarding model size. Given a fixed \ncompute budget (e.g., 100K GPU hours), they enable predictions on whether a smaller \nmodel trained for a longer duration or a larger model trained for a shorter time would yield \nbetter performance. Additionally, scaling laws provide insight into the benefits of continued \ntraining. The diminishing returns they imply suggest that beyond a certain point, increas -\ning compute resources may not lead to a substantial enough performance gain to justify the \nadditional cost.\nIn addition, based on the statistical modeling illustrated by Eq. 4, one approximate esti-\nmation for Chinchilla efficient model size and training dataset size can be denoted as:\n Nopt =0 .1 C0.5  (8)\n Dopt =1 .7 C0.5.  (9)\nThis suggests that the model size and training data volume should be scaled in accordance \nwith the available computational budget. The expected ratio of training tokens to model \nparameters is approximately 17:1. However, in real-world applications, this ratio is often \nslightly higher, as additional training data beyond the 17× scaling rule can still contribute \nto performance improvements when sufficient computational resources are available. For \ninstance, GPT-2 was trained on 40B tokens with 1.5B parameters, LLaMA was trained \non 1.4T tokens with 65B parameters, and DeepSeek-V3 was trained on 14.8T tokens with \n0.671T parameters. While all these ratios exceed 17, they remain close to this scaling \nguideline.\nPEFT and Sustainability of AI Research Training large models from scratch is highly \nenergy-intensive. For example, training LLaMA-3.1 405B can demand 40 million GPU \nhours on H100, resulting in a substantial carbon footprint. While fully Supervised Fine-\nTuning (SFT) can enhance an existing LLM using a relatively smaller set of training sam -\nples, it still requires updating the entire parameter network. In contrast, Parameter-Efficient \nFine-Tuning (PEFT) methods-such as adapters or low-rank adaptations-enable fine-tuning \na large pre-trained model for specific tasks by updating only a small subset of parameters \n(typically just 1–2% of the total). As a result, PEFT significantly reduces computational \ncosts; for instance, a full SFT process that requires 4 million GPU hours can be reduced to \n400K GPU hours or less with PEFT.\nBy lowering GPU usage, PEFT not only decreases energy consumption but also miti -\ngates the environmental impact. Moreover, this reduction in compute requirements is cru -\ncial for sustainable AI research, as PEFT provides a cost-effective and efficient approach for \nthe AI community and researchers to conduct experiments and develop new models.\n2.1.2 Prevalent LLMs\n2.1.2.1 The GPT family Generative Pre-trained Transformers (GPT) constitute a series \nof decoder-only Transformer-based language models, pioneered by OpenAI. This family \n1 3\nPage 7 of 64 227\nL. Wang et al.\nencompasses GPT-1 (Radford et al. 2018), GPT-2 (Radford et al. 2019), GPT-3, Instruc-\nGPT (Ouyang et al. 2022), ChatGPT, GPT-4, GPT-4o, CODEX (Chen et al. 2021), and \nWebGPT (Nakano et al. 2021). GPT-1 and GPT-2 belong to PLMs, while following GPT-3, \nall subsequent models in this family are classified as LLMs.\nGPT-3 (Brown et al. 2020) is widely recognized as the first LLM due to its significantly \nlarger size compared to previous PLMs, showcasing emergent abilities not observed in \nsmaller PLMs before. A key emergent ability demonstrated by GPT-3 is in-context learn -\ning (Kojima et al. 2022), enabling the model to solve various downstream tasks without the \nneed for fine-tuning. Distinct with other GPT-family LLMs, GPT-4 and GPT-4o are both \nmulti-modal LLMs. GPT-4 (Achiam et al. 2023) is one of the most powerful LLM reported \nto train on a transformer network of 1.8 trillion parameters which exhibits great capabilities \nin image understanding and reasoning. GPT-4o, while inheriting the powerful intelligence \nof GPT-4, has further enhanced its capabilities in text, image, and speech processing. Com-\npared to existing models, it particularly excels in visual and audio comprehension.\n2.1.2.2 The LLaMA family LLaMA stands as a series of open-source LLMs developed by \nMeta. To date, the official release includes: LLaMA, LLaMA-2, and LLaMA-3.x, spanning \nparameter scales from 1 billion to 405 billion. Beyond the weights provided by Meta, the \nqualities of these LLMs are further extended through supervised fine-tuning and parameter-\nefficient fine-tuning.\nLLaMA-1 (Touvron et al. 2023) was released in February 2023. Although LLaMA is \nopen-sourced and possesses fewer parameters, LLaMA-13B demonstrates significant \nimprovements over GPT-3 (175 billion parameters) across various benchmarks. As a con -\nsequence, LLaMA has emerged as a widely adopted and exemplary base model for LLM \nresearch. LLaMA-2 (Touvron et al. 2023) was developed in partnership with Microsoft \nand released half a year later. The model maintains the same architecture as the LLaMA-1 \nbut is trained with 40% more data. LLaMA-3 was released by Meta in April 2024, offering \ntwo parameter sizes: 8B and 70B. These models underwent pre-training on approximately \n15 trillion tokens of text sourced from publicly available data and are fine-tuned over 10 \nmillion human-annotated examples. Subsequently, Meta released LLaMA-3.1 (Vavekanand \nand Sam 2024), a 405B open-sourced LLM, which focuses on improving text generation \ncapabilities and achieves performance comparable to leading models like GPT-4. Then, in \nSeptember 2024, LLaMA-3.2 was released, introducing both vision models (11B and 90B) \nand lightweight text-only models (1B and 3B) for mobile device use. LLaMA-3.2 marked \nMeta’s first open-source AI model capable of processing both images and text, broadening \nthe scope of potential applications. The smaller models were designed for efficient perfor -\nmance on mobile devices, promoting wider adoption in edge computing scenarios.\n2.1.2.3 The OpenAI o1 family In September 2024, a new series of LLM, OpenAI-o1 1 \n(Jaech et al. 2024), excels in complex reasoning tasks, using Chain-of-Thought (CoT) rea -\nsoning to outperform GPT-4o in areas like math, coding, and science. The release includes \ntwo versions: o1-preview and o1-mini. The o1-preview is an early iteration of the full model, \nwhile the o1-mini is a lightweight version optimized for size and speed. When solving prob-\n1  h t t p s :  / / o p e  n a i . c o  m / i n  d e x / i  n t r o d  u c i n g -  o p e n  a i - o 1 - p r e v i e w /.\n1 3\n227 Page 8 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nlems, o1 uses the CoT 2 strategy like human deep thinking. Reinforcement learning helps \no1 refine its thinking and strategies, find and correct errors, break down complex steps, and \nchange approaches when necessary, improving reasoning. The reward model combines text \nand number scores for evaluation.\nThen previewed in December 2024, OpenAI o3-mini, 3 the newest, most cost-efficient \nmodel was offically released in January 2025, which provides a specialized alternative for \ntechnical domains requiring precision and speedwhich. It delivers exceptional STEM capa-\nbilities-with particular strength in science, math, and coding-all while maintaining the low \ncost and reduced latency of OpenAI o1-mini.\n2.1.2.4 The DeepSeek family DeepSeek-LLM is a newly established LLM series that has \ngarnered significant attention from both academia and industry. Developed by the com -\npany DeepSeek, the first version, DeepSeek-V1 (Bi et al. 2024), was trained on 2 trillion \ntokens and released in January 2024, featuring two core models: 7B and 67B, along with \ntheir respective chat variants. In the same month, DeepSeek introduced DeepSeek-MoE \n(Mixture of Experts) (Dai et al. 2024a) 16B, which delivers performance comparable to \nLLaMA 2 7B while requiring only 40% of the computational cost. This model introduces \nan innovative Mixture of Experts (MoE) architecture, integrating shared expert isolation \nwith fine-grained expert segmentation. Additionally, it incorporates a novel load-balancing \nstrategy that optimizes both expert and device balance, enhancing computational efficiency. \nThey made significant progress with DeepSeek-V2 (Liu et al. 2024a), a large MoE-LLM \ntrained on 8.1 trillion tokens, featuring 2 shared experts, 160 routed experts, and 236 bil -\nlion parameters. This version introduced Multi-head Latent Attention (MLA), which signifi-\ncantly reduces GPU memory consumption while maintaining the same level of precision. It \noutperforms the widely used Grouped-Query Attention (GQA) strategy adopted by LLaMA \n3. Subsequently, they released DeepSeek-V3 (Liu et al. 2024b) in December 2024. Build -\ning upon V2, the V3 model introduces its Multi-Token Prediction (MTP) approach and \nan Auxiliary-Free Load Balancing strategy to further enhance efficiency. Additionally, it \nintegrates DualPipe (Li and Hoefler 2021), cross-node all-to-all communication techniques, \nand a minimal-overhead memory-saving strategy, achieving a groundbreaking industrial \nmilestone-training a 671B-parameter MoE-LLM with FP8 precision. The performance of \nthe DeepSeek-V3 model is remarkable, achieving state-of-the-art (SOTA) results among all \nopen-source LLMs and demonstrating performance comparable to GPT-4o and Claude 3.5 \nSonnet. Moreover, it offers significant advantages in training and inference costs, requiring \nless than 10% of the training cost of LLaMA 3-405B and only 9% of the inference cost \nof Claude 3.5 Sonnet, revolutionizing the development of industrial LLMs. Then, Deep -\nSeek released R1 (Guo et al. 2025), a reinforcement learning-focused model leveraging \nthe Group Relative Policy Optimization (GRPO) (Shao et al. 2024) algorithm. R1 delivers \nperformance comparable to OpenAI-o1 in mathematical and logical reasoning tasks, while \n2  h t t p s :  / / o p e  n a i . c o  m / i n  d e x / l  e a r n i  n g - t o -  r e a s  o n - w i t h - l l m s /.\n3 https://openai.com/index/openai-o3-mini/.\n1 3\nPage 9 of 64 227\nL. Wang et al.\nrequiring only 2% of the computational cost, marking a major breakthrough in efficiency \nand scalability.\n2.1.2.5 The Claude family Anthropic (2025) represents a series of conversational AI mod-\nels developed by Anthropic, designed with a focus on safety, helpfulness, and natural lan -\nguage understanding. This family includes Claude 1, Claude 2, Claude 2.1, Claude 3 Opus, \nClaude 3 Sonnet, and Claude 3 Haiku.\nClaude 1 marked the initial release of Anthropic’s conversational AI, introducing the \nconcept of Constitutional AI to the field. Claude 2 and its subsequent update, Claude \n2.1, brought significant improvements in language understanding, context retention, and \nresponse coherence. These versions demonstrated enhanced capabilities in handling com -\nplex queries and maintaining longer, more contextually rich conversations.\nClaude 3 models (Opus, Sonnet, and Haiku) represent the latest advancements in the \nClaude family, each tailored for distinct applications. Opus, the most advanced model, inte-\ngrates cutting-edge multimodal capabilities, enabling it to process both textual and visual \ninputs with deep reasoning and high-level comprehension, excelling in complex problem-\nsolving tasks. Sonnet, optimized for efficiency and speed, is ideal for scenarios requiring \nrapid, precise, and contextually appropriate replies. Haiku prioritizes simplicity and ele -\ngance, delivering concise, poetic, and highly relevant responses, making it particularly well-\nsuited for creative and literary applications. Together, these models set new benchmarks for \nAI-driven interaction and analytical reasoning.\nEach model in the Claude family is continuously refined to improve performance, safety, \nand alignment with user needs, ensuring that they remain at the forefront of conversational \nAI technology.\n2.1.2.6 The Gemini family Gemini (Anil et al. 2023) constitutes a series of multimodal \nTransformer-based language models, developed by Google DeepMind. This family includes \nGemini 1, Gemini 1.5, and Gemini 2, each introducing significant advancements in multi-\nmodal understanding, long-context reasoning, and integration with Google’s ecosystem. \nUnlike GPT family models, which initially focused on text generation, Gemini models were \ndesigned from the ground up to be native multimodal models, enabling seamless processing \nof text, images, audio, and video. Gemini 1 marked Google’s transition from its Bard chatbot \nto a more advanced multimodal LLM, introducing cross-modal reasoning and excelling in \nmathematical problem-solving, coding, and knowledge retrieval, though it faced limitations \nin real-world usability. Gemini 1.5 introduced a 1 million-token context window, signifi -\ncantly improving long-document processing, dialogue coherence, and complex multi-step \nreasoning. Additionally, it implemented memory capabilities, allowing it to retain user-spe-\ncific context across interactions. The latest version, Gemini 2 further enhanced reasoning, \ntool integration, and inference speed, introducing a “Flash Thinking” mode that enables \n1 3\n227 Page 10 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nintermediate reasoning steps for improved transparency. It also deepened integration with \nGoogle Search, Docs, and other productivity tools, optimizing it for real-world applications.\n2.1.2.7 Other representative LLMs Mistral Series (Jiang et al. 2023) is an open-sourced \nLLM developed by Mistral AI. The basic Mistral-7B demonstrates superior performance \nacross all evaluated benchmarks, surpassing all open-sourced 13B LLMs and even outper -\nforming LLaMA-34B in reasoning, mathematics, and code generation tasks. Mistral 7B \nemploys Grouped Query Attention (GQA) to enable faster inference and Sliding Window \nAttention (SWA) to handle longer text sequences efficiently. Subsequently, Mistral AI intro-\nduced two additional models: Mixtral 8×7B and Mixtral 8×22B. These models utilize the \nSparse Mixture of Experts (SMoE) technique (Riquelme et al. 2021), which selectively acti-\nvates a subset of experts for each input, thereby significantly reducing computational load.\nThe PaLM (Chowdhery et al. 2023) (Pathwaysutilized Language Models) is developed \nby Google as a collection of decoder-only LLMs. The first PaLM model was trained on a \nhigh-quality text corpus of 780 billion tokens, boasting a remarkable 540 billion parameters. \nUnlike prevalent LLMs which primarily utilize GPUs for training, PaLM is pre-trained with \nthe Pathways system on 6144 TPU v4 chips to facilitate rapid and efficient training. In the \nfollowing days, U-PaLM (Tay et al. 2022), FlAN-PaLM (Chung et al. 2024) and PaLM-2 \nwere released.\n2.1.3 Multimodal large language models\n2.1.3.1 MLLM: background Multimodal large language model (MLLM), is an extension \nof LLM which adopts multimodal information as input such as text, sound, video, etc. to \nenable multiple dimensional reasoning and text generation.\nBefore the emergence of MLLM, significant research efforts were dedicated to multi-\nmodality. These efforts can generally be categorized into representative and generative para-\ndigms. An exemplary work in the representative paradigm is CLIP  (Radford et al. 2021), \nwhich serves as a foundational contribution.\nThis process yields a visual encoder (Cherti et al. 2023; Sun et al. 2023) and a text \nencoder, effectively establishing a bridge for downstream multimodal tasks. In contrast, \ngenerative frameworks (Wang et al. 2022a; Cho et al. 2021) approach multimodal tasks \nby transforming them into sequence-to-sequence tasks. MLLM distinguishes itself from \nprevious multimodal research in two key aspects. (1) Composition: MLLM is comprised \nof at least one LLM with billion-scale parameters. (2) Training techniques: MLLM intro -\nduces and incorporates novel training techniques derived from LLM to enhance multimodal \nperformance.\n2.1.3.2 MLLM: architecture Figure 1 illustrates the mainstream architecture of MLLMs, \ntypically composed of three modules: a multimodal encoder, an LLM, and a modal connector.\nMultimodal Encoder . This module incorporates non-text inputs, such as images or \naudio, and encoding the raw information into a more compact representation. It is notewor-\n1 3\nPage 11 of 64 227\nL. Wang et al.\nthy that the encoder is aligned with one or several encoders in advance to ensure associated \nmeanings are preserved. It is more advisable to directly adopt and fine-tune a pre-trained \nmultimodal encoder, such as CLIP  (Radford et al. 2021), EV A-CLIP (Sun et al. 2023), or \nViT-G (Zhai et al. 2022), rather than starting from scratch to train a new encoder for gener-\nalized data.\nLLM. It is also more efficient to adopt a pre-trained LLM instead of training from the \nstart. Through tremendous pre-training on web corpus, LLMs have been embedded with \nrich world knowledge, and demonstrate strong generalization and reasoning capabilities.\nModal Connector. This module serves as a crucial bridge between different modalities, \nallowing efficient communication with the LLM. It accomplishes this by projecting infor -\nmation into a space that the LLM can readily comprehend. Through training the connector, \nthe encoded multimodal tokens can be transformed to LLM prompt tokens that illustrate \nthe content presented by the image, video, etc. Consequently, the LLM will generate the \nexpected content based on the request and prompt.\n2.2 Optimization, datasets, and evaluation of large language models\n2.2.1 Instruction tuning\nInstruction tuning in LLMs has undergone significant development, evolving from initial \nefforts in multi-task fine-tuning without explicit instruction prompts to sophisticated tech -\nniques leveraging diverse tasks and templates. Early work focused on improving down -\nstream task performance through large-scale multi-task fine-tuning (Raffel et al. 2020; Liu \net al. 2019; Aghajanyan et al. 2021; Aribandi et al. 2021), while other efforts (Khashabi \net al. 2020; McCann et al. 2018; Keskar et al. 2019) converted a range of NLP tasks into a \nsingle generative question answering format using prompt instructions. The instruction tun-\nFig. 1 Architecture of MLLM: this figure shows a common architecture and workflow of an MLLM\n \n1 3\n227 Page 12 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\ning began in 2020 with the release of several task collections, including Natural Instructions \n(Mishra et al. 2021), Flan 2021 (Wei et al. 2021), and PromptSource (Bach et al. 2022). \nThese collections aggregated large NLP datasets and provided templatized instructions for \nzero-shot prompting, enabling models to generalize to unseen instructions. MetaICL (Min \net al. 2021) emphasized few-shot prompting without explicit instructions, using input–out -\nput examples to teach tasks in-context. Research confirmed the benefits of task and template \ndiversity, with some studies highlighting the advantages of inverting inputs and outputs to \ncreate new tasks (Min et al. 2021). The subsequent phase saw the expansion and combina -\ntion of resources, with collections like SuperNatural Instructions (Wang et al. 2022c) and \nOPT-IML (Iyer et al. 2022) integrating more datasets and tasks. This phase also introduced \nmultilingual instruction tuning, as seen in xP3 (Muennighoff et al. 2022), and incorporated \nChain-of-Thought training prompts in Flan 2022 (Chung et al. 2022). These expanded col-\nlections included most tasks from previous resources, establishing a strong foundation for \nfuture open-source work. Current and future research is exploring new directions, such as \nsynthetic data generation for creative and open-ended dialogue tasks (Wang et al. 2022b; \nHonovich et al. 2022; Ye et al. 2022; Gupta et al. 2022) and integrating human feedback \non model responses (Ouyang et al. 2022; Glaese et al. 2022; Nakano et al. 2021; Bai et al. \n2022b). These approaches are viewed as complementary to foundational instruction tuning \nmethods, driving further advancements in the field.\nA recent advance in instruction tuning is the potential to complement or replace few-\nshot in-context learning with PEFT. Compared to instruction tuning, PEFT can achieve \nperformance comparable to full parameter tuning while being computationally more cost-\neffective. Previous studies (Liu et al. 2022c; Wei et al. 2021; Vu et al. 2021; Singhal et al. \n2023) have demonstrated that PEFT can be effectively integrated with instruction tuning, \neither before or after the instruction tuning process. Additionally, this body of research high-\nlights that PEFT can enhance the performance and applicability of instruction tuning across \ndifferent domains.\n2.2.2 Alignment tuning and RLHF\nDespite the emergent abilities brought by increasing parameters of language models, hallu-\ncination exhibit to become a challenge for LLMs to produce satisfying response. To address \nthis issue, alignment tuning is applied to align the models with specific human preferences. \nThere are three primary targets for alignment tuning, respectively presented as helpfulness, \nhonesty and harmlessness. From the targets’ names, it can be concluded that the alignment \ncriteria are closely associated with human’s recognition, making it difficult to formulate \nthem as optimization objectives for LLMs. Therefore, human feedback is widely adopted as \nan assistance to reinforce LLMs’ performance.\nRLHF (Knox and Stone 2008; Christiano et al. 2017) emerged as a method to fine-tune \nlanguage models using human feedback, aiming to align the LLMs with human preferences, \nand consequently enhancing alignment performance.\nGenerally, an RLHF system (Ouyang et al. 2022) comprises three key components: a pre-\ntrained language model, a reward model learned from human feedback, and a reinforcement \nlearning algorithm to train the language model. Figure 2 shows the three key steps.\n ● Supervised Fine-Tuning (SFT):  Initially, a supervised dataset consisting of input \n1 3\nPage 13 of 64 227\nL. Wang et al.\nprompts and desired outputs is applied to fine-tune the language model. These prompts \nand outputs can be written by human labelers for some specific tasks while ensuring the \ndiversity of tasks. This step helps the model learn expected behaviors.\n ● Reward Model Training: A reward model is trained using human feedback data. The \nLLM is employed to generate a certain number of output texts using sampled prompts \nas input. Then human labelers rank these output pairs based on their preferences. Given \nhuman predictions, the reward model is trained to predict these rankings, effectively \nlearning human preferences. Notably, Lee et al. (2023) proposes an approach, namely \nReinforcement Learning from AI Feedback (RLAIF), the annotation of preference on \nresponse pairs can be generated by an AI agent, increasing the automatic ability of the \nreinforcement process.\n ● Reinforcement Learning Fine-Tuning: The final step involves formalizing the align -\nment process as a reinforcement learning problem. Here, the pre-trained language mod-\nel acts as a policy generating text, with the reward model providing feedback scores. \nTo prevent the model from deviating too far from its initial state, a penalty term is \noften included in the reward function. The language model is then optimized using \nalgorithms like SARSA (Sutton 1995), DQN (Fan et al. 2020), PPO (Schulman et al. \n2017), DPO (Rafailov et al. 2024), and GRPO (Shao et al. 2024), iteratively improving \nits performance based on human-aligned rewards.\n2.2.3 Datasets for LLM\nA critical component of the development and deployment of LLM is the datasets used at \nvarious stages of their lifecycle, which significantly influence their capabilities and perfor -\nmance. In this section, we delve into the datasets that are instrumental in the Pre-training, \nSFT, and RLHF. The Pre-training phase is where an LLM absorbs the foundational knowl-\nedge from a diverse array of textual data. This stage is pivotal, as it sets the stage for the \nmodel’s general understanding of language. The datasets used in Pre-training are vast and \nvaried, encompassing everything from the sprawling expanse of the internet to curated col-\nlections of literature and encyclopedias. SFT is the process where the LLM is fine-tuned \nFig. 2 RLHF workflow: this figure is from InstructGPT, which interprets the RL process\n \n1 3\n227 Page 14 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\non specific tasks or domains. This phase refines the model’s abilities, enabling it to per -\nform with greater precision and relevance in targeted applications. SFT datasets are often \nmore specialized and may include annotated examples that guide the model towards desired \nbehaviors and outputs. RLHF is the stage where the LLM is further optimized based on \nhuman feedback. This phase enhances the model’s alignment with human preferences and \nvalues, ensuring that its outputs are more aligned with user expectations. RLHF datasets \ntypically consist of human-labeled examples and feedback, which help the model learn to \nprioritize high-quality and contextually appropriate responses.\n2.2.3.1 Commonly used datasets for pre-training In the realm of LLM, the pre-training \nphase is instrumental in establishing a robust foundation upon which the model’s linguis -\ntic prowess is built. LLM, with their exponentially larger parameter counts, necessitate an \nextensive and diverse corpus of training data that spans a multitude of topics and linguistic \nexpressions. This data not only serves as the bedrock for the model’s comprehension of \nlanguage but also influences its ability to generalize and adapt to new contexts and tasks. \nTo meet these requirements, a variety of comprehensive and accessible datasets have been \ncurated and made available for the research community.\nIn this section, we embark on an overview of the datasets that are pivotal in the pre-train-\ning of LLM. We categorize these datasets based on the type of content they provide, which \ncan be broadly divided into seven distinct groups: Webpages, Books, Code, Social Media, \nWikipedia, and a diverse array of other sources. Each of these categories contributes unique \nelements to the model’s knowledge base, ensuring a well-rounded understanding of human \nlanguage and its myriad uses. Here are 2 typical Pre-training Datasets and their importance \nin evaluating PEFT Methods:\n ● Common Crawl: The Common Crawl corpus is an extensive, unstructured, multilin -\ngual dataset of webpages, encompassing over eight years of web crawler data. This \ndataset is available in various formats, including web archive, web archive transfor -\nmation, and web-extracted text. Many pre-training corpora are obtained through data \npreprocessing based on this corpus, which provides a vast and diverse source of text \nfor language models. Its unstructured nature and multilingual content make it an ideal \nresource for training models that need to handle a wide variety of text types and lan -\nguages. Importantly, the Common Crawl corpus plays a crucial role in evaluating PEFT \nmethods. Its vast and varied content provides a comprehensive base for pre-training \nmodels that can then be fine-tuned using PEFT techniques. This allows researchers to \nassess how effectively PEFT methods can enhance model performance across diverse \nlinguistic contexts.\n ● The Pile: The Pile is a large-scale, diverse language modeling dataset consisting of 22 \ndata subsets, designed to capture text in as many forms as possible and cover a wide \nrange of textual content. The corpus includes academic papers, code, legal materials, \npatents, subtitles, chat content, parallel corpora, and more. This diversity ensures that \nmodels trained on The Pile are exposed to a broad spectrum of language use cases, \nmaking them more adaptable to various downstream tasks. In the context of evaluating \nPEFT methods, The Pile offers a robust testbed. Its rich diversity of text types allows re-\nsearchers to evaluate how well these fine-tuning methods can adapt models to different \n1 3\nPage 15 of 64 227\nL. Wang et al.\ndomains and tasks, thereby enhancing their understanding of the effectiveness of PEFT \nmethods in various applications (Table 2).\n2.2.3.2 Commonly used datasets for SFT and RLHF Two critical stages in LLM are SFT \nand RLHF. These stages are designed to enhance the model’s performance on specific tasks \nand align its outputs with human preferences. This section provides an overview of these \ntwo stages, highlighting their significance and the datasets used to support them.\nSFT is a process where LLM are trained on specialized datasets to improve their perfor-\nmance on specific tasks. This stage is crucial for adapting the model to particular domains \nor applications. SFT involves using annotated datasets that provide examples of desired \noutputs for given inputs. By training on these datasets, the model learns to generate more \naccurate and contextually relevant responses. RLHF is particularly effective in enhancing \nthe model’s ability to follow human instructions. These datasets provide a comprehensive \nset of examples that help the model learn to discern correct answers from plausible alterna-\ntives (Table 3).\n2.2.4 LLM evaluation\nThe burgeoning field of LLM research has necessitated the development of robust evalua -\ntion frameworks to accurately gauge the capabilities and limitations of these sophisticated \nAI systems. Evaluation serves multiple critical functions: it benchmarks model performance \nacross a spectrum of tasks, identifies areas for improvement, and ensures that advancements \nin LLM technology align with ethical and practical standards. In the academic and profes -\nsional realms of LLM evaluation, it is widely recognized that a multifaceted approach is \nessential to gauge the capabilities and limitations of these advanced AI systems comprehen-\nsively. The Qwen blog’s evaluation of the Qwen2.5 base language model, 4 underscore the \nimportance of using multiple benchmarks to assess the model’s performance across various \ndomains thoroughly.\nPlatforms such as Hugging Face offer a suite of datasets for this purpose, 5 including \nIFEval, BBH, MATH (Hendrycks et al. 2021b), GPQA (Rein et al. 2024), and MUSR \n(Sprague et al. 2023). These datasets encompass a broad spectrum of tasks, ranging from \nlanguage modeling to problem-solving in mathematics, ensuring a comprehensive evalu -\nation of model competencies. Models like Qwen2.5 is evaluated using a diverse array of \ndatasets that cover general tasks such as MMLU, and HellaSwag, as well as specialized \ntasks in math and science with datasets like GPQA and MATH, and coding tasks including \nHumanEval and MBPP. Additionally, multilingual capabilities are assessed through datasets \nlike Multi-Exam and Multi-Translation.\nTo achieve a comprehensive evaluation of a language model’s performance, it is often \nnecessary to employ a combination of benchmarks. These benchmarks should be represen-\ntative of real-world scenarios and cover diverse domains and linguistic complexities. The \nevaluations include a variety of tests that measure the model’s ability to handle extended \ndialogues and manage a variety of tasks. By leveraging these diverse datasets and assess -\n4 https://qwenlm.github.io/blog/qwen2.5-llm/.\n5  h t t p s :  / / h u g  g i n g f a  c e . c  o / s p a  c e s / o  p e n - l l  m - l e  a d e r b  o a r d /  o p e n _ l  l m _ l  e a d e r b o a r d.\n1 3\n227 Page 16 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nTable 2 A curated list of datasets for pre-training\nCollections Categories Publication time Size URL\nCommon Crawla Webpages 2023 400 TB https://commoncrawl.org/\nWuDaoCorpora-Text (Yuan et al. 2021) Webpages 2023 5 TB  h t t p s :  / / d a t  a . b a a i  . a c .  c n / d e  t a i l s  / W u D a o  C o r p  o r a T e x t\nBookCorpusOpen  (Zhu et al. 2015) Books 2015 9.05 GB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / d e f u n  c t - d  a t a s e t s / b o o k c o r p u s o p e n\nPG-19  (Rae et al. 2020) Books 2020 11.74 GB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / d e e p m  i n d /  p g 1 9\nThe Stack (Kocetkov et al. 2023) Code 2022 3 TB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / b i g c o  d e / t  h e - s t a c k\nOpenWebText (Gokaslan and Cohen 2019) Social Media 2019 38 GB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / S k y l i  o n 0 0  7 / o p e n w e b t e x t\nPushshift Reddit (Baumgartner et al. 2020) Social Media 2020 89.1 GB https://zenodo.org/records/3608135\nWikipediab Wikipedia 2023 71.8 GB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / w i k i m  e d i a  / w i k i p e d i a\nThe Pile (Gao et al. 2020) Others 2020 800 GB https://pile.eleuther.ai/\nS2ORC (Lo et al. 2020) Others 2020 80.5 GB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / s e n t e  n c e -  t r a n s f o r m e r s / s 2 o r c\nMultiUN (Eisele and Chen 2010) Others 2010 31.8 GB  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / H e l s i  n k i -  N L P / m u l t i u n\nThis table provides a comprehensive overview of various datasets used for pre-training purposes in natural language processing tasks. It includes details such as the \ncollection name, the corpus it belongs to, publication year, size in terms of tokens, and the URL for accessing the dataset. The datasets listed cover a range of sources from \nweb pages to books, offering a diverse set of data for training models in different domains\nahttps://commoncrawl.org/\nbhttps://www.wikipedia.org/\n1 3\nPage 17 of 64 227\nL. Wang et al.\nments, researchers can effectively benchmark LLM and guide their development towards \npractical applications, ensuring alignment with ethical and practical standards. This section \nexplores benchmarking in two parts: general tasks and specialized tasks.\n ● General Tasks: General tasks are designed to assess the broad capabilities of LLM \nacross a wide range of subjects and skills. These benchmarks are essential for evaluat -\ning the foundational knowledge and general reasoning abilities of LLM. These bench -\nmarks help determine how well models can understand and generate text in various \ncontexts, ensuring that they possess a solid understanding of language fundamen -\ntals. Datasets such as MMLU, ARC, and HellaSwag are commonly used for general \nevaluations.Specialized Tasks: Specialized tasks focus on evaluating LLM in specific \ndomains, such as mathematics, coding, and natural language understanding. These \nbenchmarks are designed to assess the model’s proficiency in particular areas, providing \na deeper understanding of their specialized skills. Specialized tasks are crucial for iden-\ntifying domain-specific strengths and weaknesses, ensuring that models can effectively \napply their knowledge in practical scenarios (Table 4).\n3 PEFT taxonomy\nPEFT techniques are typically divided into three primary categories: Additive PEFT (Sect. \n3.1), which introduces additional trainable components or parameters into the pre-exist -\ning model; Reparameterized PEFT  (Sect. 3.2), a method that restructures the model’s \nparameters during the training phase and then reverts to the original form for inference; \nand Selective PEFT (Sect. 3.3), which focuses on optimizing a specific subset of the mod-\nel’s parameters. Besides these, there is the Hybrid PEFT (Sect. 3.4), which combines the \nstrengths of various PEFT approaches. Additionally, there are specialized adaptations such \nas Quantization PEFT (Sect. 3.5) designed for the quantization process, and Multi-task \nPEFT (Sect. 3.6) aimed at enhancing multi-task learning capabilities. A conceptual illustra-\ntion of the core principles underlying these PEFT methodologies is presented in Fig. 3. A \ncomprehensive classification of PEFT methods is depicted in Fig. 4. The main ideas, num-\nber of trainable parameters, applications, and limitations of different types of PEFT methods \nare summarized in Table 5. To facilitate a more intuitive understanding of the performance \ndifferences among various PEFT methods, Table 6 presents the performance results of rep-\nresentative PEFT methods of different types across various base models and tasks.\n3.1 Additive PEFT\nFull-parameter fine-tuning is computationally expensive and could adversely affect the \nmodel’s capacity to generalize. To address this, additive PEFT methods add a small set of \ntrainable parameters to a pre-trained model, carefully integrated into its architecture. When \nfine-tuning for particular downstream tasks, it is only these extra components or parameters \nare adjusted, keeping the original pre-trained model parameters unchanged. This approach \nsignificantly reduces the need for storage, memory, and computation. Based on where and \nhow these additional trainable parameters are incorporated into the model’s architecture, \n1 3\n227 Page 18 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nTable 3 A curated List of datasets for SFT and RLHF\nCollections Categories Publication time Examples URL\nE2E NLG (Dušek et al. 2020) NLP Task 2020 50,000  h t t p s :  / / s i t  e s . g o o  g l e .  c o m / s  i t e / h  w i n t e r  a c t i  o n l a b / E 2 E /\nWikiSQL (Zhong et al. 2017) NLP Task 2017 80,654  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / S a l e s  f o r c  e / w i k i s q l\nWebNLG (Gardent et al. 2017) NLP Task 2017 27,731 https://huggingface.co/datasets/web_nlg\nSAMSum (Gliwa et al. 2019) Daily Chat 2019 16,369  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / S a m s u  n g / s  a m s u m\nOASST1 (Wang et al. 2024a) Daily Chat 2023 161,443  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / O p e n A  s s i s  t a n t / o a s s t 1\nWMTa Others 2019 124,448,248 https://huggingface.co/datasets/wmt/wmt19\nXSUM (Narayan et al. 2018) Others 2018 200,000  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / E d i n b  u r g h  N L P / x s u m\nDART (Nan et al. 2021) Text generation 2021 82,000 https://github.com/Yale-LILY/dart\nHH-rlhf (Bai et al. 2022a) Dialogue and preference 2022 169,000  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / A n t h r  o p i c  / h h - r l h f\nPKU-SafeRLHF (Ji et al. 2023) Dialogue and preference 2023 362,000  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / P K U - A  l i g n  m e n t / P K U - S a f e R L H F\nHotpotQA (Yang et al. 2018) Question–Answering 2018 113,000  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / h o t p o  t q a /  h o t p o t _ q a\nSHP (Ethayarajh et al. 2022) Community preference 2022 385,000  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / s t a n f  o r d n  l p / S H P\nThis table provides an overview of the datasets used in SFT and RLHF phases, categorized by their primary purposes and characteristics. This categorization helps in \nunderstanding the diversity and scope of data used to train and fine-tune models in different phases of development. The URLs provided allow researchers and practitioners \nto access these datasets for further analysis and experimentation\nahttps://www.statmt.org/wmt19/\n1 3\nPage 19 of 64 227\nL. Wang et al.\nthere are primarily three types of additive PEFT techniques: Adapter, Soft Prompt, and \nScale and Shift. We will delve into some of the principal studies on these techniques.\n3.1.1 Adapter\nAdapter methods enable PEFT by inserting small adapter layers into pre-trained models, \nwhich learn task-specific transformations while keeping the base model frozen. These \nadapters, typically consisting of a down-projection, a non-linear activation function and \nan up-projection layer (the standard adapter shown in Fig. 5a), adapt the representations to \ndownstream tasks with minimal overhead. For example, in Sequential Adapter (Houlsby \net al. 2019), two serial adapters are inserted after the attention layer and the feed-forward \nlayer in transformer blocks. Residual Adapter (Lin et al. 2020) dynamically adapts a pre-\ntrained language model, such as GPT-2, to various downstream tasks using low-rank resid-\nual adapters and task embeddings, with the adapter module formulated as:\n Adapter(Hi)=( ReLU(LN(Hi)WE\ni ))WD\ni + Hi, (10)\nwhere Hi is the hidden representation of the ith layer, WE\ni  and WD\ni  are the adapter param-\neters, and LN denotes layer normalization. AdapterDrop (Rücklé et al. 2020) dynamically \nremoves adapters from the lower layers of a transformer during training and inference, \nwhich significantly enhances inference speed in multi-task settings with minimal impact \non task performance. Tiny-Attn Adapter (Zhao et al. 2022) applies a multi-head atten -\ntion mechanism with tiny per-head dimension the intermediate embeddings of each token \nto obtain the modified embeddings, and employs parameter-averaging technique to reduce \ninference cost during deployment. Parallel Adapter (He et al. 2021) integrates the adapter \nnetwork to both the attention and feed-forward layers of the transformer in a parallel man -\nner, facilitating a more efficient incorporation of the module. CIAT (Counter-Interference \nAdapter for Multilingual Machine Translation) (Zhu et al. 2021) employs an embedding \nadapter to refine multilingual word embeddings and parallel layer adapters to de-noise the \nmultilingual interference in intermediate layers, improving the translation performance \nwith a small parameter overhead. CoDA (Condition Adapter) (Lei et al. 2024) enhances \ninference efficiency by selectively activating computations on a subset of input tokens, \ndetermined by a soft top-k operation, thus balancing model expressivity and computational \nefficiency. Hadamard Adapter (Chen et al. 2023e) (shown in Fig. 5b) employs a weight \nvector and a bias vector, applying the Hadamard product (element-wise multiplication) and \nelement-wise addition to the self-attention outputs, resulting in new self-attention outputs. \nCompacter (Karimi Mahabadi et al. 2021) incorporates concepts from adapters, low-\nrank methods, and hypercomplex multiplication layers. It introduces task-specific weight \nmatrices by combining shared “slow” weights with “fast” rank-one matrices computed \nthrough Kronecker products, tailored to each COMPACTER layer’s requirements. Sparse-\nAdapter (He et al. 2022) prunes a significant portion of parameters at initialization, using a \nsparsity-inducing method to maintain performance while reducing computational overhead, \nand further improving capacity through a “Large-Sparse” configuration that scales up the \nbottleneck dimension with an increased sparsity ratio.\n1 3\n227 Page 20 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nTable 4 A typical list of available datasets for LLM evaluation\nCollections Task Publication time Examples URL\nMMLU (Hendrycks et al. 2021a) General 2021 15,908 https://huggingface.co/datasets/cais/mmlu\nARC (Clark et al. 2018) General 2018 7787  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / a l l e n  a i / a  i 2 _ a r c\nHellaSwag (Zellers et al. 2019) General 2019 59,950  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / R o w a n  / h e l  l a s w a g\nGLUE (Wang et al. 2019b) Natural language understanding 2018 1,485,043 https://huggingface.co/datasets/nyu-mll/glue\nSuperGLUE (Wang et al. 2019a) Natural language understanding 2019 196,309  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / a p s / s  u p e r  _ g l u e\nGSM8K (Cobbe et al. 2021) Science and mathematics 2021 17,584 https://huggingface.co/datasets/openai/gsm8k\nTheoremqa (Chen et al. 2023d) Science and mathematics 2023 800  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / T I G E R  - L a b  / T h e o r e m Q A\nHumaneval (Chen et al. 2021) Code 2021 164  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / o p e n a  i / o p  e n a i _ h u m a n e v a l\nMBPP (Austin et al. 2021) Code 2021 1401  h t t p s :  / / h u g  g i n g f a  c e . c  o / d a t  a s e t s  / g o o g l  e - r e  s e a r c h - d a t a s e t s / m b p p\nAGIEval (Zhong et al. 2024) Exam 2023 8062 https://github.com/ruixiangcui/AGIEval\nGAOKAO-Bench (Zhang et al. 2023h) Exam 2023 2811 https://github.com/OpenLMLab/GAOKAO-Bench\nTruthfulQA (Lin et al. 2021) Other 2021 1634  h t t p s :   /  / h u g g i n g f a c  e .  c  o / d a  t a s e  t  s / t r u  t h f u  l  q a / t r u t h  f u l _ q a\nBBH (Suzgun et al. 2023) Other 2022 6511 https://huggingface.co/datasets/lukaemon/bbh\nThis table provides an exhaustive compilation of datasets pertinent to the evaluation of LLM. These datasets span a diverse array of tasks, from general to domain-specific, \naiming to holistically assess the performance of LLM across various scenarios. The table delineates the publication timeline, the number of examples, and the access points \n(URLs) for each dataset, facilitating researchers in procuring and utilizing these resources\n1 3\nPage 21 of 64 227\nL. Wang et al.\n3.1.2 Soft prompt\nSoft prompt methods involve appending a sequence of trainable continuous vectors, known \nas soft prompts, to the input of pre-trained language models. These soft prompts act as addi-\ntional context that guides the model towards the desired output for a specific task. During \ntraining, the soft prompts are optimized to facilitate the model’s adaptation to the new task, \nwhile the rest of the model remains largely unchanged, making the approach parameter-\nefficient. Based on the intuition that a properly optimized context, in the form of continuous \nword embeddings, can guide the language model towards performing an NLG task without \naltering its parameters, Prefix-tuning (Li and Liang 2021) and prompt-tuning (Lester et al. \n2021) involve prepending a prefix Pθ of trainable vectors θ to the input. The activations for \nthese prefix indices are treated as free parameters. To stabilize the optimization process, \nPθ is parametrized by reparameterizing it through a smaller matrix P′\nθ, which is then com-\nposed with a feedforward neural network (MLP), i.e., Pθ = MLP(P′\nθ). p-tuning (Liu et al. \n2021b) leverages trainable continuous prompt embeddings, which are concatenated with \ndiscrete prompts to form an input sequence for a pretrained language model. This sequence \nis then mapped to a hidden representation through an embedding function parameterized by \na prompt encoder, such as an LSTM or MLP, and is optimized via backpropagation to mini-\nmize a task-specific loss function. p-tuning v2 (Liu et al. 2021a) is an optimized prompt \ntuning method that universally matches the performance of fine-tuning across various model \nscales and NLU tasks by applying trainable continuous embeddings to every layer of the \npre-trained model as prefix tokens, thus increasing the capacity of continuous prompts and \nreducing the gap to fine-tuning, especially for smaller models and more challenging tasks. \nSMoP (Sparse Mixture-of-Prompts) (Choi et al. 2023) utilizes a gating mechanism to route \neach input instance to one of multiple short soft prompts, which are specialized in handling \ndifferent subsets of the data, thereby achieving efficient training and inference while main -\ntaining performance gains typically induced by longer soft prompts. The routing probabil -\nity for the j-th prompt is calculated as pj(X)= softmax(Lµ( ¯X))]j, where Lµ is a small \nlinear router model, ¯X is the average of input embeddings, and μ are the parameters of the \nrouter model. APT (Adaptive Prefix Tuning) (Zhang et al. 2023i) dynamically customizes \nthe prefix at each layer of a Transformer model through a gate mechanism. It utilizes both \nfine-grained gated weight assignment and coarse-grained scaled weight specification. The \npseudo prefix tokens ˆPi in the ith layer are updated as follows:\n ˆPi = λi ⊙ αi · [Pik,P iv], (11)\nwhere [Pik,P iv] represents the keys-values pair of the original pseudo prefix tokens, λi is a \nlearnable scaled weight, ⊙ denotes element-wise multiplication, and αi represents the gated \nweights, which are calculated as:\n αi = sigmoid(hi−1Wi), (12)\nwhere hi−1 represents the hidden states from the previous layer, and Wi are the parameters \nto be learned. IDPG (Instance Dependent Prompt Generation) (Wu et al. 2022) works on \nthe principle of generating prompts for each input instance using a lightweight model G that \ntakes the instance representation x and task T as inputs to produce a task-specific prompt \n1 3\n227 Page 22 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nFig. 4 Taxonomy of PEFT methods\n \nFig. 3 Illustration of the main idea of different types of PEFT methods\n \n1 3\nPage 23 of 64 227\nL. Wang et al.\nTable 5 An overview of different types of PEFT methods: main idea, number of trainable parameters, applications, and limitations\nCategory Main idea Representative methods # Trainable \nparameters\nApplications Advantages Limitations\nAdditive Add trainable compo-\nnents, freeze original.\nSequential Adapter (Houlsby et al. 2019), \nPrefix-tuning (Li and Liang 2021), \n(IA)3 (Liu et al. 2022c), IPA (Lu et al. \n2023)\n#Params of addi-\ntional modules\nSingle-task, rapid \nadaptation.\nMinimal updates, \nflexible insertion.\nComputa-\ntional overhead, \ndesign-sensitive.\nReparameterized Low-rank decomposition, \ntune low-rank matrices.\nLoRA (Hu et al. 2021), AdaLoRA (Zhang \net al. 2023f), DoRA (Liu et al. 2024e)\n#Params of low-\nrank matrices\nLarge-scale, ef-\nficient updates.\nFewer parameters, \nno inference \nlatency.\nLow-rank con-\nstraints, hyperpa-\nrameter tuning.\nSelective Update subsets (e.g., \nbiases, masked params).\nU-Bitfit (Lawton et al. 2023), \nFAR (Vucetic et al. 2022)\n#Params of select-\ned subsets (e.g., \nbiases, masked \nparams)\nResource-\nconstrained \nenvironments.\nCritical updates, \nlow memory.\nTask-sensitive, \nparameter \nselection.\nHybrid Combine multiple PEFT \nmethods dynamically.\nUniPELT (Mao et al. 2021), MAM-\nAdapter (He et al. 2021)\n#Params of used \nPEFT modules\nComplex tasks, \nmultimodal.\nTask flex-\nibility, improved \nperformance.\nHigh complexity, \nsearch overhead.\nQuantization Quantize model, enable \nefficient tuning.\nQLoRA (Dettmers et al. 2024), Bit-\nDelta (Liu et al. 2024d)\n#Params of used \nPEFT modules\nEdge devices, \nlow resources.\nLow storage, \nlow-precision \ninference.\nPrecision loss, \nquantization \nbalance.\nMulti-task Share parameters and \ndynamic adapters for \nmulti-task.\nAdapterFusion (Pfeiffer et al. 2020), \nSPoT (Vu et al. 2022), MOELoRA (Liu \net al. 2023b)\n#Params of shared \nand task-specific \nmodules\nMulti-task, cross-\ntask knowledge.\nRedundant reduc-\ntion, task transfer.\nTask con-\nflicts, routing \ncomplexity.\n1 3\n227 Page 24 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nWp(T,x ), which is then inserted into the input sequence x for fine-tuning the pre-trained \nlanguage model M with a unified template, as denoted by the equations:\n \nWp(T,x )= G(M(x),T ),x ∈ Dtrain,\nh[CLS]= M(concat[x, Wp(T,x )]).  (13)\nLPT (Late Prompt Tuning) (Liu et al. 2022b) is a method that inserts a “late prompt” into \na pre-trained model (PTM) at an intermediate layer. This late prompt is created by a neural \nprompt generator (NPG) which uses the hidden states from the model layer just before the \nprompt insertion. This process generates a prompt that is tailored to each specific instance, \nenhancing the model’s performance and efficiency. The generation of this instance-aware \nprompt involves a series of steps that include transformations and combinations of various \nelements derived from the model’s hidden states. Once created, the prompt is reshaped to \nbe integrated into the model’s processing workflow. SPT (Selective Prompt Tuning) (Zhu \nand Tan 2023) initializes a prompt hyper-network where each intermediate layer of the \npre-trained model (PTM) has a prompt generation layer controlled by a learnable proba -\nbilistic gate αi, which is optimized to determine the importance of each layer for the task \nat hand, using the formulation ai = σ(αi), where σ is the sigmoid function, and pi, the \nprompt at layer I, is calculated as pi = (1− τ · ai) · pprev,i + τ · ai · pnew,i, with τ  being \na hyper-parameter that decides whether to discard the previous layer’s prompt when a new \none is generated. APrompt (Wang et al. 2023b) introduces trainable query, key, and value \nprompts, denoted as Pq,P k, and Pv, into the self-attention mechanism of a Transformer \nencoder layer, which are integrated into the respective matrices to guide the attention com-\nputation during fine-tuning, while keeping the majority of the model parameters frozen. The \nnew attention computations are formulated as:\n \nL(·)= MLP(LN(MSA(·))),\nMSA(·)= softmax\n(QT\nnewKnew√\nd\n)\nVnew, (14)\nwhere MLP and LN represent the frozen multi-layer perceptron and layer norm, MSA is \nthe multi-head self-attention module, Qnew is the new query matrix, Knew and Vnew are the \nnew key and value matrices augmented with attention prompts, and d is the dimension of \nthe embeddings. DePT (Decomposed Prompt Tuning) (Shi and Lipani 2023) decomposes a \ntrainable soft prompt matrix P ∈ Rl×d into a shorter trainable prompt matrix Ps ∈ Rm×d \nand a pair of low-rank matrices A ∈ Rs×r and B ∈ Rr×d, where the rank r ≪ min(s, d)\n. These components are optimized with different learning rates α1 and α2 respectively. \nThe updated word embedding matrix for the ith sample is given by W′\ni = Wi + BA, \nwhere Wi is the original word embedding matrix. The loss function to be optimized is \nLDePT = − ∑N\ni=1 log P(yi|[Ps,W ′\ni ]; Θ), where Θ represents the frozen pretrained model \nweights. Xprompt (Ma et al. 2022) operates on the principle of hierarchical structured \npruning to identify and retain only the most effective soft prompt tokens, denoted as pi, and \ntheir components, denoted as qi,e, by calculating their importance scores Ipi  and Iqi,e  using \nthe following expressions:\n1 3\nPage 25 of 64 227\nL. Wang et al.\nTable 6 Performance evaluation across various PEFT methods for fine-tuning common base models (RoBERTa-base, RoBERTa-large, and DeBERTaV3-base) on the GLUE \nbenchmark\nModel PEFT type PEFT method #TPs CoLA SST2 MRPC STS-B QQP MNLI QNLI RTE\nRoBERTa-base FT 124.6M 59.07 92.89 88.24/91.58 90.87/90.61 90.81/87.72 86.27 91.07 72.2\nAdditive AdapterS 7.41M 63.32 94.31 90.44/93.18 91.25/90.94 90.81/86.55 87.33 92.06 73.56\nPrefix-tuning 0.96M 59.31 93.81 87.25/91.03 88.48/88.32 87.75/84.09 85.21 90.77 54.51\n(IA)3 0.66M 59.58 93.92 87.00/90.52 90.30/90.32 87.99/84.10 83.95 90.88 71.12\nReparameterized LoRA 0.89M 62.09 94.04 87.50/90.68 90.66/90.83 88.83/85.21 86.54 92.02 72.92\nAdaLoRA 1.03M 59.82 93.92 87.99/91.33 90.83/90.73 88.58/84.98 86.26 91.43 70.04\nSelective BitFit 0.69M 61.32 94.72 89.22/92.41 90.34/90.27 88.12/84.11 84.64 91.09 77.98\nChild-Tuning / 60.33 93.58 89.22/92.20 91.14/90.93 90.98/88.04 87.4 92.2 77.62\nHybrid MAM Adapter 46.78M 61.42 94.87 89.31/92.21 90.74/90.42 88.31/83.20 86.63 90.19 72.62\nRoBERTa-large FT 355.3M 65.78 95.54 89.22/92.28 91.74/91.76 89.30/86.68 89.42 93.61 81.23\nAdditive AdapterS 19.77M 67.03 96.37 89.94/92.54 92.58/92.42 92.19/88.50 91 94.31 85.25\nPrefix-tuning 2.03M 59.01 95.76 88.24/91.37 90.92/91.07 88.88/85.45 89.3 93.32 74.01\n(IA)3 1.22M 61.15 94.61 86.52/90.33 92.22/92.03 89.45/86.25 88.63 94.25 81.23\nReparameterized LoRA 1.84M 64.47 96.67 87.50/91.19 91.66/91.44 90.15/86.91 90.76 95 79.78\nAdaLoRA 2.23M 65.85 94.95 89.46/92.34 92.05/91.80 89.60/86.30 90.36 94.62 77.98\nSelective BitFit 1.32M 68.01 96.1 90.93/93.38 91.93/91.77 89.48/86.43 89.98 94.47 87.73\nChild-Tuning / 63.08 95.07 90.69/93.43 92.36/92.18 91.52/88.75 35.45 93.15 86.25\nHybrid MAM Adapter 122.2M 67.39 95.81 90.12/92.77 92.44/92.18 90.87/86.65 90.62 94.31 86.62\nDeBERTaV3-base FT / 69.2 95.3 89.5/93.3 91.6/91.1 92.4/89.8 90.5 94 82\nQuantization QLoRA / N.A 86.5 73.8/82.8 83.0/82.8 86.8/82.3 75.4 82.4 55.9\nLoftQ / 37.4 90.2 83.8/88.6 87.1/86.9 90.3/86.9 84.7 86.6 61.4\nAll performance metrics are cited from prior published works (Xu et al. 2023a; Li et al. 2023b). Metrics may vary by task: Matthews correlation for COLA, accuracy/F1 score \nfor MRPC and QQP, Pearson/Spearman correlation for STS-B, average matched accuracy for MNLI, and accuracy for the remaining tasks. Higher metric values indicate \nsuperior performance. #TP  denotes the number of trainable parameters for each method\n1 3\n227 Page 26 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\n \nIpi = Ex∼Dx\n⏐⏐⏐⏐\n∂L(x)\n∂γi\n⏐⏐⏐⏐ ,\nIqi,e = Ex∼Dx\n⏐⏐⏐⏐\n∂L(x)\n∂ζi\n⏐⏐⏐⏐ ,\n (15)\nwhere L is the loss function, Dx is the training data distribution, γi and ζi are mask variables \nfor token-level and piece-level pruning respectively, and the importance scores determine the \ncontribution of each prompt token and piece to the model’s performance. InfoPrompt (Wu \net al. 2024b) maximizes the mutual information between the prompt P and the parameters of \nthe classification head θ, denoted as I(P; θ|X), and between the prompt P and the encoded \nrepresentation from the pretrained language model Z = Φ(P,X ), denoted as I(P; Z|X), \nby optimizing two novel loss functions, referred to as the head loss and the representation \nloss, respectively. PTP (Prompt Tuning with Perturbation-based Regularizer) (Chen et al. \n2023c) introduces perturbation-based regularizers to stabilize prompt tuning by smoothing \nthe loss landscape. This can be formulated as:\n min\nθ\nE(s,y)∼D [L (M (θ, s+ δ, y))] , (16)\nwhere δ is the perturbation sampled from either a Gaussian distribution ( δ ∼N  for PTP-\nRN) or generated by an adversarial attack algorithm ( δ = argmax∥δ∥≤ϵL (θ, s+ δ, y) for \nPTP-ADV). s is the input sequence, y is its label, M is the LLM, θ represents the trainable \nprompt parameters, and L is the loss function.\n3.1.3 Scale and shift\n(IA)3 (Infused Adapter by Inhibiting and Amplifying Inner Activations) (Liu et al. 2022c) \nshown in Fig. 6a is a PEFT method for scaling inner activations of a model by learned \nvectors. For a decoder with L layers, (IA)3 adds scaling vectors lk,l v, and lff  (initialized \nas ones) to scale key, value, and feed-forward activations, respectively. This allows for \ntask-specific adaptations while updating a tiny fraction ( ≤ 0.01%) of the model’s param -\neters, facilitating mixed-task batches. The method can be applied permanently to weight \nmatrices if the model is dedicated to a single task, avoiding extra computations. MoV (Mix-\nture of Vectors) (Zadouri et al. 2023) introduces a parameter-efficient Mixture of Experts \n(MoE) architecture that updates only lightweight experts, less than 1% of an 11B param -\neter model. It generalizes well to unseen tasks. Computation is routed with soft merging: \nEmix = ∑n\ni=1 si · Ei; y = Emix(x), where Ei represents each expert, si is the gating \nweight for each expert, and x is the input. This approach ensures robust performance under \nstrict parameter constraints. SSF (Lian et al. 2022) shown in Fig. 6b modifies deep features \nextracted by a pre-trained model through linear transformations to match the distribution of \nthe target dataset. Given an input x ∈ R(N2+1)×d, the output y is computed as:\n y =[ γ ⊙ x + β]T , (17)\nwhere γ and β are learnable scale and shift parameters, respectively, and ⊙ denotes ele -\nment-wise multiplication. This approach requires tuning far fewer parameters than full fine-\ntuning. PASTA (PArameter-efficient tuning with Special Token Adaptation) (Yang et al. \n2023b), as illustrated in Fig. 6c, modifies special token representations in pretrained mod -\n1 3\nPage 27 of 64 227\nL. Wang et al.\nels. For the lth Transformer layer, given input H(l) = {h(l)\ni }N\ni=1, where h(l)\ni ∈ Rd, PASTA \nupdates the input as H(l)\nmod = {h(l)\ni + m(l)\ni }N\ni=1, where m(l)\ni  is defined as:\n \nm(l)\ni =\n{ 0 if i is not a special token\ne(v(l)\np ) if i is the pth special token , (18)\nwith e(v(l)\np ) ∈ Rd being the trainable vector for the pth special token at layer I.\n3.1.4 Others\nIPA (Inference-time Policy Adapters) (Lu et al. 2023) tailors LLMs to specific objectives \nwithout fine-tuning. IPA combines the output distribution of a base LLM with a smaller, \ntrainable adapter policy. The adapter is optimized via reinforcement learning (RL) to align \nthe LLM’s output with user-defined goals. At inference, the base model’s distribution and \nthe trained adapter’s distribution are merged for decoding as follows:\n pcombined(output | input)= αpbase(output | input)+( 1− α)padapter(output | input), (19)\nwhere pbase is the base model’s probability distribution, padapter is the adapter’s distribu -\ntion, and α controls their mixture. LST (Ladder Side-Tuning) (Sung et al. 2022) introduces \na side network that predicts outputs using shortcuts (ladders) from a pre-trained back -\nbone, avoiding backpropagation through the entire backbone. Formally, given a backbone \nfN(fN−1(...f 2(f1(x)) ... )), the side network g takes intermediate activations zi as inputs, \nwhere zi = fi(x). The final output ˆy is computed by g(zi; θg), significantly reducing mem-\nFig. 6 Illustration of three representative scale and shift algorithms\n \nFig. 5 Illustration of three representative types of adapter\n \n1 3\n227 Page 28 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nory cost. Here, x is the input, fi represents the i-th layer function, and θg are the parameters \nof the side network. Attention-Fusion (Cao et al. 2022) aggregates intermediate layer rep-\nresentations from a pre-trained model to compute task-specific token representations. This \nmodule trains only 0.0009% of total parameters and achieves competitive performance to \nfull fine-tuning. Formally, given a pre-trained model with L layers, the output h(l)\ni  of each \nlayer l for token i is used to compute a weighted sum ri = ∑L\nl=1 α(l)\ni h(l)\ni , where α(l)\ni  repre-\nsents the attention weight for layer l on token i.\n3.2 Reparameterized PEFT\nReparameterization is a technique for improving the training efficiency and performance of \na model by transforming its parameters. In the context of PEFT, the transformation involves \nlow-rank parameterization, which entails constructing a low-rank learnable parameter \nmatrix to adapt to specific downstream tasks. During training, only the low-rank parameter \nmatrix is fine-tuned, and at inference time, the learned matrix is combined with the pre-\ntrained parameters to ensure that inference speed is not affected.\n3.2.1 Low-rank decomposition\nLoRA (Low-rank Adaptation) (Hu et al. 2021) introduces low-rank trainable matri -\nces A ∈ Rd×r and B ∈ Rr×k to update the pre-trained weight matrix W0 ∈ Rd×k via \n∆W = BA, where W = W0 +∆ W is used for inference without additional latency. \nKronA (Edalati et al. 2022) is a Kronecker product-based adapter module for efficient fine-\ntuning of Transformer-based pre-trained language models (PLMs). The tuned weight matrix \nWtuned is computed as the original PLM weight matrix W plus a scaled Kronecker product \nof two learnable matrices Ak and Bk:\n Wtuned = W + s[Ak ⊗ Bk], (20)\nwhere s is a scaling factor, and ⊗ denotes the Kronecker product operator.\n3.2.2 LoRA derivatives\n3.2.2.1  Dynamic rank\nDyLoRA (Valipour et al. 2023) shown in Fig. 7a introduces a dynamic low-rank adap -\ntation technique by training Low-Rank Adapter (LoRA) blocks for a range of ranks dur -\ning training, where the representation learned by the adapter module is sorted at different \nranks, enabling the model to be flexible and perform well across a wider range of ranks \nwithout additional training time or the need for rank selection. AdaLoRA (Zhang et al. \n2023f) illustrated in Fig. 7b dynamically allocates the budget among weight matrices based \non their importance scores, where incremental updates are parameterized in the form of \na singular value decomposition as W = W0 + PΛQ, with P ∈ Rd1×r, Q ∈ Rr×d2 , and \nΛ ∈ Rr×r being the left singular vectors, right singular vectors and singular values, respec-\ntively. IncreLoRA (Zhang et al. 2023b) presented in Fig. 7c incrementally allocates train-\nable parameters during the training process based on the importance scores of each module, \nwhich is formulated as follows:\n1 3\nPage 29 of 64 227\nL. Wang et al.\n \nW = W0 +\nr∑\ni=1\nλiwi = W0 +\nr∑\ni=1\nλibiai, (21)\nwhere W0 is the pretrained weight matrix, r ≪ min(in, out), wi is a rank-1 matrix, ai ∈ Rin\n, bi ∈ Rout, and λi is a scaling factor updated through backpropagation, with λi initialized \nto zero to ensure the initial update matrix is zero. SoRA (Sparse low-rank Adaption) (Ding \net al. 2023) introduces a gate unit, optimized with a proximal gradient method to control the \nsparsity of the LoRA’s low-rank matrices. The gate unit enables dynamic adjustment of the \nrank of LoRA during training, enhancing representation power while maintaining parameter \nefficiency. During inference, blocks corresponding to zero entries in the gate unit are elimi-\nnated, reducing the SoRA module to a concise, rank-optimal LoRA.\n3.2.2.2  LoRA improvement LoRA+ (Hayou et al. 2024) introduces a novel technique by \napplying different learning rates to the down- and up-projection matrices A and B: ηB = ληA\n, where λ is a fixed value greater than 1, focusing on tuning ηA for enhanced model adapt-\nability. Designed to mitigate the significant memory requirements for activations that are \nintrinsic to LoRA, LoRA-FA (Low-Rank Adaptation with Frozen-A) (Zhang et al. 2023d) \nfreezes the pre-trained weight W and the projection-down weight A, and only update the \nprojection-up weight B during the fine-tuning process, which results in a model weight \nchange ∆W that resides in a low-rank space defined by the column space of A. The method \nis designed to reduce the activation memory footprint without incurring additional com -\nputational overhead. DoRA (Weight-Decomposed Low-Rank Adaption) (Liu et al. 2024e) \naims to bridge the gap in performance between LoRA and full fine-tuning (FT) by leverag-\ning a novel weight decomposition approach. It decomposes the pre-trained weight matrix \nW0 ∈ Rd×k into magnitude and direction. During fine-tuning, only the direction component \nis updated using a low-rank approximation ∆W = BA, where B ∈ Rd×r and A ∈ Rr×k\n, and r ≪ min(d, k). Here, r denotes the rank of the low-rank approximation, d and k rep-\nresent the dimensions of the weight matrix. This allows for efficient parameter updates \nwhile preserving the original weight’s magnitude, enhancing learning capacity and stability. \nLaplace-LoRA (Yang et al. 2023a) introduces a Bayesian approach to LoRA for fine-tuning \nLLMs. It addresses the issue of overconfidence in fine-tuned LLMs by estimating predictive \nuncertainty. Laplace-LoRA approximates the posterior distribution over LoRA parameters \nusing a Laplace approximation, leading to better-calibrated models. Mathematically, given \na maximum a posteriori (MAP) estimate θMAP, the predictive distribution for a new input \nx∗ is approximated as:\nFig. 7 Illustration of three representative dynamic rank methods in LoRA\n \n1 3\n227 Page 30 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\n fθ(x∗) ∼N (fθMAP(x∗), Λ) , (22)\nwhere Λ=( ∇θfθ(x∗)|θ=θMAP)Σ(∇θfθ(x∗)|θ=θMAP)⊤. Here, ∇θfθ(x∗) represents the \ngradient of the prediction with respect to the parameters, and Σ is the covariance matrix of \nthe Laplace approximation. The prior precision λ is optimized using the Laplace marginal \nlikelihood on the training dataset:\n P(y|X) ≈ exp(L(y,X ; θMAP))(2π)D/2|Σ|1/2, (23)\nSamples from the predictive distribution are obtained by:\n ˜fθ(x∗)= fθMAP(x∗)+Lξ,  (24)\nwhere L is the Cholesky factor of Λ and ξ is a vector of independent standard normal \nrandom variables. This method improves calibration without requiring a separate valida -\ntion set, making it suitable for small datasets. PeriodicLoRA (PLoRA) (Meng et al. 2024) \nenhances LoRA’s learning capacity by periodically accumulating low-rank updates to \nform a higher-rank matrix. During each stage, only LoRA weights WLoRA are updated. \nAt the end of each stage, WLoRA is unloaded into the backbone parameters Wbackbone, \ni.e., Wbackbone ← Wbackbone +∆ WLoRA, and then WLoRA is reinitialized. This increases \nthe effective update rank without additional memory cost. HydraLoRA (Tian et al. 2024) \nenhances LoRA by adopting an asymmetric structure for efficient fine-tuning. It segments \nthe LoRA into multiple “intrinsic components,” each with a distinct matrix Bk, sharing a \ncommon matrix A. The update formula is given by:\n \nαW = W0 + r\nN∑\nk=1\nABk, (25)\nwhere W0 is the original weight matrix, r is a scaling factor, A and Bk are low-rank matrices, \nand N is the number of components. A trainable MoE router dynamically allocates samples \nto these components for fine-tuning. AFLoRA (Liu et al. 2024g) incrementally freezing \ntrainable low-rank matrices based on a novel freezing score, computed using smoothed \ngradient ¯I(t)Al , uncertainty tensor ¯U(t)Al , and their Hadamard product to determine the \nstability of weights throughout training, as described by the equations:\n \nI(t)Al = |∇L(θ)|,\n¯I(t)Al = β1 ¯I(t − 1)Al + (1− β1)I(t)Al ,\nU(t)Al = |I(t)Al − ¯I(t)Al |,\n¯U(t)Al = β2 ¯U(t − 1)Al + (1− β2)U(t)Al ,\ns(t)Al = mean(¯I(t)Al ⊙ ¯U(t)Al ),\n (26)\nwhere Al represents the low-rank tensor, L(θ) is the loss function, β1 and β2 are smooth-\ning factors, and t denotes the current training step. LoRA-SP (Wu et al. 2024c) selectively \nfreezes half of the parameters in the matrices A and B during fine-tuning, with the adapted \n1 3\nPage 31 of 64 227\nL. Wang et al.\nweight matrix ∆W calculated as ∆W =( A ⊙ S)(B ⊙ S)⊤, where S is a binary selec -\ntion matrix that determines which parameters to update or freeze, and ⊙ denotes element-\nwise multiplication. SuperLoRA (Chen et al. 2024) generalizes LoRA approach by jointly \nadapting all weight updates ∆W across layers through a high-order tensor decomposition, \nwhere ∆Wgroupg  is computed as\n \nF(∆Wlorag )= F\n( K⊗\nk=1\n(\nCgk\nM∏\nm=1\n×mAgkm\n))\n, (27)\nwith F being a projection function, M the order of tensor modes, K the number of Kro -\nnecker splits, Cgk the core tensor, Agkm the plane factors, ∏M\nm=1 ×m the tensor products \nfrom model-1 to model-M, and ⊗ the Kronecker product.\n3.3 Selective PEFT\nContrary to Additive PEFT, Selective PEFT selects a very small subset of the pre-trained \nmodel’s parameters for fine-tuning to adapt to specific downstream tasks through a param -\neter masking matrix. Depending on the way the parameters are masked, Selective PEFT can \nbe divided into unstructured masking and structured masking.\n3.3.1 Unstructural masking\nU-Diff pruning (Guo et al. 2020) introduces a task-specific “diff” vector δτ  that is added \nto pretrained model parameters θ. The task-specific parameters are defined as θτ = θ + δτ\n. During training, δτ  is adaptively pruned using a differentiable L0-norm approximation to \nencourage sparsity. θ remains fixed. This method enables efficient transfer learning, modify-\ning only a small fraction of the parameters per task. U-Bitfit (Lawton et al. 2023) determines \nwhich components of the bias update vector ∆b should be zero or non-zero, based on a \nfirst-order approximation of the change in training loss from pruning a bias parameter θ, \ncalculated as −θ · ∂L\n∂θ . PaFi (Liao et al. 2023) generates a universal sparse mask for param-\neter selection without training. PaFi identifies the least significant pre-trained parameters by \ntheir magnitude and fine-tuning only those, represented as selecting parameters θi where \n|θi|≤ sort(|θ|)k for the mask m. FishMask (Sung et al. 2021) precomputes a fixed sparse \nmask for neural network parameters, selecting the top k parameters based on their Fisher \ninformation to be updated during training. This “FISH (Fisher-Induced Sparse uncHanging) \nmask” enables efficient training by updating only a subset of parameters, which reduces \nmemory and communication costs compared to full model updates. k represents the number \nof parameters to be selected for updates, and Fisher information measures parameter impor-\ntance for the given task. Fish-Dip (Das et al. 2023) dynamically updates the importance \nof model parameters for fine-tuning based on feedback from the most regressing samples, \nusing the empirical Fisher information to create a sparsity mask that focuses training on a \nsubset of parameters, as denoted by the equation:\n \nˆFθ ≈ 1\nn\n∑\n{(xi,yi)|Ltr(xi,yi)∈topn}\n(∂ log pθ(yi|xi)\n∂θ\n)2\n, (28)\n1 3\n227 Page 32 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nwhere ˆFθ represents the empirical Fisher information, n is the number of most regressing \ntraining examples, pθ(yi|xi) is the output probability for the given input xi and parameters \nθ, and the sum is taken over the top n regressing examples as determined by their loss Ltr \nduring training. LT-SFT (see Fig. 8c) (Ansell et al. 2021) introduces a composable sparse \nfine-tuning method for cross-lingual transfer learning. It learns sparse, real-valued masks \nbased on a variant of the Lottery Ticket Hypothesis (LTH). Task-specific masks are derived \nfrom supervised data in the source language, while language-specific masks are obtained \nthrough masked language modeling in the target language. These masks are composed \nwith the pre-trained model to enable zero-shot cross-lingual transfer. The sparsity of the \nmasks reduces parameter overlap and interference, improving modularity and preventing \noverfitting. SAM (Second-order Approximation Method) (Fu et al. 2023) approximates the \noriginal optimization problem using a second-order Taylor expansion to make it analytically \nsolvable, and directly determines the parameters to optimize by solving the approximation \nfunction, which is formulated as:\n \nmin\n∆θ\n[\nL(θ0)+∇L (θ0)T M∆θ + 1\n2(M∆θ)T HM ∆θ\n]\n, (29)\nsubject to ∥M∥0 = ⌊mp⌋; Mij =0 , ∀i ̸= j; Mii ∈{ 0, 1}, where θ0 are the pre-trained \nparameters, ∆θ is the difference vector, M is the parameter mask matrix, L is the loss func-\ntion, ∇L(θ0) is the gradient of the loss function at θ0, and H is an approximated diagonal \nHessian matrix. Child-tuning (see Fig. 8b) (Xu et al. 2021) updates only a subset of param-\neters, referred to as the child network, during fine-tuning while masking out the gradients of \nthe remaining parameters in the backward pass, which can be formulated as:\n \nwt+1 = wt − η ⊙ ∂L(wt)\n∂wt\n⊙ Mt, (30)\nwhere wt represents the model parameters at the tth iteration, η is the learning rate, L(wt) is \nthe loss function, and Mt is a 0–1 mask indicating the child network. U-MAM (Lawton et al. \n2023) is an unstructured neural architecture search approach for parameter-efficient tuning \nof large pre-trained language models. It involves pruning a dense low-rank update from \nan initial parameter-efficient tuning architecture to find an efficient subset of parameters to \nfine-tune. Threshold-Mask (Zhao et al. 2020) learns selective binary masks for pre-trained \nlanguage model weights without fine-tuning, where each linear layer Wl is associated with \na real-valued matrix Ml initialized randomly, and a binary mask Mbin\nl  is obtained by apply-\ning a thresholding function, used to select important weights: (mbin\nl )i,j = 1(ml,i,j ≥ τ) \nwith ml,i,j ∈ Ml and the global thresholding hyperparamter τ , and the masked weights are \nFig. 8 Illustration of three representative unstructural masking methods\n \n1 3\nPage 33 of 64 227\nL. Wang et al.\ncomputed as ˆWl = Wl ⊙ Mbin\nl , with Ml updated during training via the straight-through \nestimator: Ml ← Ml −η∂L( ˆWl)\n∂Mbin\nl\n. LoRAPrune (see Fig. 8a) (Zhang et al. 2023e) approxi-\nmates the importance of each parameter in the pre-trained model weights W0 by utilizing \nthe gradients of the low-rank matrices A and B, which are then used to perform structured \npruning in an iterative and progressive manner, efficiently reducing the model’s size while \nmaintaining performance.\n3.3.2 Structural masking\nS-Diff pruning (Guo et al. 2020) introduces a structured pruning strategy by dividing the \nweight parameters into local groups and strategically removing them collectively. S-Bit-\nfit (Lawton et al. 2023) selects whether to update each bias parameter b with a learned update \n∆b, where the decision is based on a pruning criterion that sums the first-order approxi -\nmation of the loss change over the entire bias update ∆b, expressed as − ∑\nθ∈∆b θ · ∂L\n∂θ\n. FAR (Freeze And Reconfigure) (Vucetic et al. 2022) leverages overparameterization in \nBERT-like models to efficiently fine-tune them on resource-constrained devices. FAR selec-\ntively updates parameters based on their importance, determined through priming, while \nfreezing others. This reduces memory usage and fine-tuning time, with minimal impact on \nperformance. Notation-wise, if P represents the total parameters, Pfrozen ⊂ P denotes fro-\nzen parameters, and Pactive = P\\Pfrozen are active parameters updated during fine-tuning. \nPfrozen is selected using priming to ensure optimal performance. BitFit (Zaken et al. 2021) \nmodifies only the bias terms of a pre-trained BERT model, demonstrating competitive per-\nformance with full fine-tuning on small to medium datasets and practical utility for deploy-\ning multi-task models in memory-constrained environments. Xattn Tuning (Gheini et al. \n2021) updates only cross-attention parameters in Transformer models for machine transla -\ntion, showing it can achieve near-equivalent performance to fine-tuning the entire model, \nwhile also leading to crosslingually aligned embeddings that can mitigate catastrophic for -\ngetting and enable zero-shot translation capabilities. SPT (He et al. 2023a) identifies task-\nspecific sensitive parameters by measuring their impact on loss reduction, denoted as sn, \nand then adaptively allocates trainable parameters to these positions under a given budget τ , \nutilizing both unstructured tuning for individual parameters and structured tuning for weight \nmatrices with a high number of sensitive parameters, as indicated by σopt. S-MAM (Law-\nton et al. 2023) is a structured neural architecture search approach for parameter-efficient \ntuning of large pre-trained language models. It selects and fine-tunes a fixed rank of param-\neters within the model’s attention mechanisms and feed-forward networks.\n3.4 Hybrid PEFT\nDue to the significant performance differences of different types of PEFT methods on vari-\nous tasks, many studies aim to enhance model performance by combining the advantages \nof different types of PEFT methods. These research efforts are summarized as Hybrid PEFT \nmethods. A representative hybrid PEFT method, known as MAM-Adapter, is illustrated in \nFig. 9.\nUniPELT (Mao et al. 2021) operates on the principle of dynamically activating the most \nsuitable parameter-efficient language model tuning (PELT) submodules for a given task \n1 3\n227 Page 34 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nthrough a gating mechanism, which is mathematically represented as h′\nA = GAhA + hF\n, where h′\nA is the final output, hA is the output of the adapter submodule, hF  is the direct \ninput to the adapter, and GA is the gating function that modulates the contribution of the \nadapter submodule based on the specific data and task setup. S4 (Chen et al. 2023b) discov-\ners design patterns by grouping layers in a spindle pattern, uniformly allocating trainable \nparameters, tuning all groups, and assigning tailored strategies to different groups, consis -\ntently outperforming existing fine-tuning strategies across various NLP tasks and models. \nMAM Adapter (He et al. 2021) is a unified framework for parameter-efficient transfer \nlearning methods by reframing them as modifications to specific hidden states in pretrained \nmodels, which can be mathematically represented as h ← (1 −λ(x))h + λ(x)∆h, where \nh is the original hidden representation, λ(x) is a gating scalar, and ∆h is the modifica -\ntion vector computed by a function f applied to the input x. LLM-Adapters (Hu et al. \n2023) discusses the use of different adapters such as Series Adapters, Parallel Adapters, \nand LoRA (Low-Rank Adaptation), which are incorporated into the model’s architecture \nat optimal locations. NOAH (Zhang et al. 2022) employs neural architecture search to \nautomatically design optimal “prompt modules” for large vision models, tailored to each \ndownstream dataset, enhancing transfer learning, few-shot learning, and domain general -\nization. AUTOPEFT (Zhou et al. 2024) automates the configuration selection for PEFT \nof large pre-trained language models. It employs a multi-objective Bayesian optimization \napproach to discover a set of Pareto-optimal configurations that balance task performance \nwith parameter efficiency, significantly outperforming existing PEFT methods with minimal \ntraining costs. S3Delta-M (Hu et al. 2022) automatically searches for an optimal trainable \nstructure within pre-trained models by using a unified framework of various Delta Tuning \nmethods. It employs bi-level optimization and a shifted global sigmoid function to control \nsparsity, achieving high performance with minimal trainable parameters. ProPETL (Zeng \net al. 2023) enables the sharing of a single prototype network across different layers and \nFig. 9 Illustration the principles of MAM-\nAdapter, a representative hybrid PEFT \nmethod\n \n1 3\nPage 35 of 64 227\nL. Wang et al.\ntasks, with binary masks learned to prune sub-networks, significantly reducing parameter \nstorage while improving efficiency and performance over other methods.\n3.5 Quantization PEFT\nQuantization is another widely used and studied technique aimed at improving computa -\ntional efficiency and reducing memory usage. We summarize the PEFT methods that use \nand research quantization technology, as Quantization PEFT.\nBI-Adapter (Jie et al. 2023) introduces a novel method for low-precision adapter train-\ning in vision models. It utilizes the observation that adapter parameters converge to flat \nminima, suggesting robustness to precision reduction. The method employs a quantization-\naware training strategy, minimizing the quantization error by clustering weight parameters \ninto Gaussian distributions. Specifically, weights w are standardized w′ = w−µ\nσ , quantized, \nand then de-standardized to backpropagate gradients effectively. This approach significantly \nreduces model size with minimal impact on performance, addressing storage and transmis-\nsion inefficiencies in multi-task learning. PEQA (Kim et al. 2024) involves a two-step pro-\ncess: first, decomposing the parameter matrix of each fully-connected layer into a low-bit \ninteger matrix and quantization scales, and second, fine-tuning only the quantization scale \nwhile keeping the integer matrix frozen, which can be mathematically represented as:\n \n˜W =( s0 +∆ s) ·\n(\nclamp\n(\n⌊ W0\ns0\n⌉ + z0, 0, 2b − 1\n)\n− z0\n)\n, (31)\nwhere the notation A · B denotes the element-wise product of matrices A and B. The sym-\nbol ⌊·⌉ represents the rounding function, which rounds its argument to the nearest integer. \nThe function clamp(·, a, b) signifies the clamping operation that constrains its input within \nthe range [a, b]. Here, W0 denotes the original weight matrix, s0 represents the initial scale \nfactor, and z0 is the zero-point value. The variable ∆s ∈ Rn×1 signifies the gradient update \nof s0, obtained through adaptation to a downstream task, and b indicates the bit-width. \nQLORA (Dettmers et al. 2024), a quantized version of LoRA, utilizes 4-bit NormalFloat \n(NF4) precision for quantizing pretrained models, enhanced by double quantization and a \npaged optimizer to prevent the gradient checkpointing memory spikes. The NF4 is an infor-\nmation theoretically optimal quantization data type for normally distributed data, deliver -\ning enhanced empirical performance over 4-bit Integer and Float representations. While \nQLoRA converts the FP16 pretrained weights W to the NF4 precision to enable LLM fine -\ntuning on a reduced number of GPUs, the auxiliary weights of the LoRA matrix re-quantize \nthe final weights back to FP16 post-finetuning. Therefore, QA-LoRA (Quantization-Aware \nLow-Rank Adaptation) (Xu et al. 2023c) addresses the imbalance between quantization and \nadaptation by employing group-wise operations, which increase the flexibility of low-bit \nquantization while reducing that of the adaptation process. The algorithm is straightfor -\nward to implement and provides two key benefits: during fine-tuning, LLM weights are \nquantized (e.g., to INT4) to conserve time and memory; post fine-tuning, the LLM and aux-\niliary weights are seamlessly integrated into a quantized model without accuracy loss. The \ncomparative analysis and conceptual distinctions among LoRA, QLoRA, and QA-LoRA \nmethodologies are visually illustrated in Fig. 10. LoftQ (Li et al. 2023b) introduces a simul-\ntaneous process of quantizing an LLM and initializing LoRA with low-rank matrices to \nmitigate performance gaps. The algorithm approximates the original weights W ∈ Rd1×d2  \nwith a quantized version Q ∈ Rd1×d2\nN  and low-rank matrices A ∈ Rd1×r and B ∈ Rd2×r\n1 3\n227 Page 36 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\n, minimizing the Frobenius norm ∥W − Q − AB⊤∥F . LoftQ alternates between quantiza-\ntion and SVD, efficiently approximating the original weights for improved downstream task \nperformance, especially in 2-bit and 2/4-bit mixed precision scenarios. LQ-LoRA (Guo \net al. 2023) iteratively decomposes a pretrained matrix W into a quantized component Q \nand a low-rank component L1L2 by solving the optimization problem:\n \narg min\nQ,L1,L2\n∥W − (Q + L1L2)∥F , (32)\nwhere Q is fixed during finetuning and only L1 and L2 are updated. QDyLoRA (Rajabza-\ndeh et al. 2024) is a quantized dynamic low-rank adaptation technique for efficient tuning \nof LLMs. It builds upon the DyLoRA  (Valipour et al. 2023) method, which enables train -\ning across a spectrum of ranks dynamically, and combines it with quantization techniques \nfrom QLoRA (Dettmers et al. 2024). The core principle is to allow the model to finetune \non a set of predefined ranks and then select the optimal rank for inference, achieving effi -\nciency without compromising performance. Mathematically, the forward pass is given by \nh = WDDequant\nNF4 x + α∑r\nb=1(Wup):,b(Wdw)b,:x, where WDDequant\nNF4  is the dequantized \npretrained weight, x is the input, α is the LoRA scalar, r is the sampled rank, and Wup \nand Wdw are the up- and down-projection matrices, respectively. This approach reduces \nmemory usage during training and inference, making it suitable for large-scale LLMs. Bit-\nDelta (Liu et al. 2024d) is an efficient post-training quantization method for compressing \nlarge language models after fine-tuning. The core idea is to represent the fine-tuning induced \nweight delta, ∆= Wﬁne − Wbase, where Wﬁne is the weight matrix of the fine-tuned model \nand Wbase is the base pre-trained model’s weight, using only 1 bit. This is achieved by \nquantizing ∆ to its sign bits and a trainable scaling factor α, resulting in ˆ∆= α ⊙ Sign(∆)\n. The scaling factor is initialized to minimize the L2 norm of the error and further refined \nthrough distillation to align the quantized model’s output with the original fine-tuned model. \nThis approach dramatically reduces memory requirements and can enhance inference speed, \nwith minimal impact on performance.\n3.6 Multi-task PEFT\nThe previously introduced PEFT methods were mainly designed for single downstream \ntask. This section focuses on PEFT for multi-task learning. Figure 11 illustrates three multi-\ntask PEFT approaches: AdaMix (Adapter-based), ATTEMPT (Soft Prompt-based), and \nMOELoRA (LoRA-based).\nFig. 10 Illustration of the difference among LoRA, QLoRA and QA-LoRA\n \n1 3\nPage 37 of 64 227\nL. Wang et al.\n3.6.1 Adapter-based\nAdapterFusion (Pfeiffer et al. 2020) employs a two-stage approach to transfer learning, \nwhere it first extracts knowledge into task-specific adapters and then composes this knowl-\nedge in a separate step to exploit multi-task representations without destructive interference. \nAdaMix (Wang et al. 2022d) integrates multiple adaptation modules within each Trans -\nformer layer of a pre-trained language model, enabling efficient tuning with a mixture of \nthese modules while maintaining most of the model’s weights unaltered. PHA Zhao et al. \n(2023) leverages an instance-dense retriever and a prototypical hypernetwork to efficiently \ngenerate task-specific adapter layers by retrieving prototype embeddings and feeding them \ninto the hypernetwork, enabling sample-efficient multi-task learning and new task general -\nization. AdapterSoup (Chronopoulou et al. 2023) improves the generalization of pretrained \nlanguage models to new domains by averaging the weights of adapters trained on different \ndomains, without the need for additional training or increasing inference cost. MerA (He \net al. 2023b) efficiently incorporates pretrained adapters into a single model through model \nfusion, aligning the parameters via optimal transport based on weights and activations to \nenhance performance in few-shot learning scenarios. Hyperformer (Mahabadi et al. 2021) \nintegrates hypernetwork-based adapter layers into a transformer model, enabling the model \nto share knowledge across tasks while adapting to each individual task through task-specific \nadapters generated by shared hypernetworks.\n3.6.2 Soft prompt-based\nSPoT (Soft Prompt Transfer) (Vu et al. 2022) leverages soft prompts to adapt pre-trained \nlanguage models efficiently. It first trains a soft prompt p on one or more source tasks, where \np ∈ Rd represents a sequence of continuous vectors with dimensionality d. This learned \nprompt is then used to initialize the prompt for a target task, facilitating transfer learning. \nSPoT significantly improves upon the performance of prompt tuning and matches or out -\nperforms full model fine-tuning while using significantly fewer task-specific parameters. \nATTEMPT (ATTEntional Mixtures of Prompt Tuning) (Asai et al. 2022) leverages pre-\ntrained soft prompts P1,...,P t for different high-resource tasks and a new target prompt \nPtarget. An attention module G computes attention scores between input X and each prompt \ntoken to produce an instance-wise prompt Pinstance = ∑t+1\nj=1 aj Pj , where aj  represents the \nattention weight for prompt Pj . Only Ptarget and G are updated during training, keeping the \noriginal language model frozen. This approach is parameter-efficient and flexible for multi-\ntask learning. MPT (Multitask Prompt Tuning) (Wang et al. 2022e) is a method for efficient \ntransfer learning of LLMs across multiple downstream tasks. The core idea is to distill \nFig. 11  Illustration of three representative multi-task PEFT methods: AdaMix (Adapter-based), AT -\nTEMPT (Soft Prompt-based), and MOELoRA (LoRA-based)\n \n1 3\n227 Page 38 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nknowledge from multiple task-specific source prompts into a single transferable prompt, \nP∗, which is then adapted to each target task with minimal additional parameters. The \nprompt for each source task is decomposed into a shared matrix P∗ and a low-rank task-\nspecific matrix Wk = uk ⊗ vT\nk , where uk and vk are task-specific vectors. This decomposi-\ntion is learned through a knowledge distillation process that minimizes the KL-divergence \nbetween teacher and student prompts, LLogits, and an additional mean squared loss on the \nhidden states, LHidden. The total training loss is LTotal = LPLM + λ(LLogits + LHidden)\n, where LPLM is the task loss and λ balances the distillation impact. The innovation lies \nin leveraging cross-task knowledge within a parameter-efficient framework, which outper -\nforms full finetuning with far fewer task-specific parameters. IPT (Intrinsic Prompt Tun -\ning) (Qin et al. 2021) is a method to reparameterize the adaptation of pre-trained language \nmodels to various tasks within a low-dimensional intrinsic task subspace. The key idea is \nto decompose the soft prompts P for multiple NLP tasks into a shared, lower-dimensional \nspace using an auto-encoder with projection Proj(·) and back-projection Projb(·) functions. \nThe auto-encoder is trained to minimize the reconstruction loss LAE = ||P∗ − P||2\n2, where \nP∗ = Projb(Proj(P)). The intrinsic dimension dI  determines the size of this subspace. \nAfter finding the subspace, IPT tunes only dI  parameters to adapt PLMs to new tasks or \ndata, suggesting that the adaptations can be generalized across tasks by optimizing a small \nset of free parameters in a unified subspace. TPT (transferable prompt tuning) (Su et al. \n2021) investigates transferring soft prompts across tasks and models to improve prompt \ntuning (PT) efficiency. Soft prompts P = {p1,p 2,...,p l}, where pi ∈ Rd and d is the input \ndimension, are prepended to input sequences X = {x1,x 2,...,x n}. The objective is to \nmaximize the likelihood L = p(y|P, x1,...,x n) of generating desired outputs y, with P \nbeing the only trainable component. Transferability is explored through initializing with \nsimilar tasks’ prompts and using a cross-model projector. The overlapping rate of activated \nneurons is found to be a strong indicator of transferability.\n3.6.3 LoRA-based\nLoRAHub (Huang et al. 2023a) is a dynamic composition of multiple LoRA modules, \nrepresented as ˆm =( w1A1 + w2A2 + ... + wNAN)(w1B1 + w2B2 + ... + wNBN), \nfollowed by a gradient-free optimization to determine the coefficients wi that best adapt the \ncombined module for performance on new, unseen tasks. MOELoRA (Liu et al. 2023b) \nintegrates a Mixture-of-Experts (MOE) model with trainable experts {Ei}N\ni=1, each consist-\ning of a pair of low-rank matrices Bi ∈ Rdin×r and Ai ∈ Rr×dout , along with a task-moti-\nvated gate function that outputs expert weights ωji  for task Tj , to efficiently fine-tune LLMs \nfor multi-task medical applications while maintaining a compact set of trainable param -\neters. L-LoRA (Linearized LoRA) (Tang et al. 2023) is a novel partial linearization method \nfor PEFT models, which enhances weight disentanglement and improves multi-task fusion \ncapability with a low computational cost overhead by linearizing only the adapter modules \nand applying model fusion algorithms over the linearized adapters. MTLoRA (Agiza et al. \n2024) revolves around the use of Task-Agnostic and Task-Specific Low-Rank Adaptation \nmodules to efficiently adapt a shared transformer backbone for multiple downstream tasks \nin a Multi-Task Learning architecture, balancing between learning shared features and those \nspecific to individual tasks.\n1 3\nPage 39 of 64 227\nL. Wang et al.\n4 Applications of PEFT\nThis section presents a comprehensive overview of PEFT methodologies specifically devel-\noped for several prominent applications, categorized as follows: PEFT in Vision Models \n(Sect. 4.1), which primarily focuses on adapting pretrained vision models to specialized \ncomputer vision tasks (e.g., image classification, image segmentation, object detection, and \ndepth estimation); PEFT in Diffusion Models (Sect. 4.2), which addresses the adaptation \nof diffusion models for vision generation tasks; and PEFT in MLLM  (Sect. 4.3), which \nemphasizes training model connectors on domain-specific datasets to bridge multimodal \ndata discrepancies while maintaining input consistency for LLMs. For a structured over -\nview of these applications and their corresponding recommended PEFT techniques, refer \nto Fig. 12.\n4.1 PEFT in vision models\nOver the past decade, deep learning has achieved significant advancements in the field of \ncomputer vision, particularly with the introduction of the ImageNet dataset and the wide -\nspread adoption of the pre-training-fine-tuning paradigm based on pretrained vision models \n(PVMs). Numerous studies have shown that better ImageNet pre-training performance typi-\ncally leads to improved performance on downstream tasks. As visual pre-trained models \ncontinue to evolve, especially with the introduction of Vision Transformer (ViT) architec -\ntures, the scale of model parameters has increased significantly, highlighting the inefficien-\ncies of traditional full fine-tuning methods in terms of parameter efficiency. To address these \nissues and improve parameter efficiency during the fine-tuning process of PVMs, various \nPEFT methods have emerged. These methods have demonstrated their advantages across \nmultiple domains, including image classification, dense prediction, video analysis, and 3D \npoint cloud analysis. This section will focus on the application of PEFT methods in image \nclassification and dense prediction tasks.\nFig. 12 Taxonomy of PEFT methods for vision models, diffusion models and MLLM\n \n1 3\n227 Page 40 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\n4.1.1 Image classification\nIn this subsection, we introduce PEFT methods for image classification tasks in vision mod-\nels. Figure 13 illustrates the principles of three representative PEFT methods discussed in \nthis subsection.\nVP (Bahng et al. 2022) investigates visual prompting as a means to adapt large-scale \npre-trained models for new tasks without updating model parameters. A single image per -\nturbation ( δ) is learned such that when added to input images ( x), the prompted image \n(x′ = x + δ) steers the model’s prediction towards a target task. This method is akin to \nadversarial reprogramming, but it aims for constructive task adaptation. Its effectiveness is \ndemonstrated through experiments, which show competitive performance compared to lin-\near probes. Notably, the approach is input-agnostic and dataset-wide. VPT (Visual Prompt \nTuning) (Jia et al. 2022) adapts pre-trained vision Transformers for downstream tasks by \nintroducing task-specific, learnable parameters ( P = {pk ∈ Rd|k ∈ N, 1 ≤ k ≤ m}) into \nthe input sequence, while keeping the backbone of the model frozen. Here, d represents the \ndimensionality of the input features, while m signifies the total number of prompts. These \nprompts P are prepended to the input sequence of each Transformer layer and learned along-\nside a linear classification head during fine-tuning. NOAH (Neural prOmpt seArcH) (Zhang \net al. 2022) automatically searches for the optimal design of prompt modules for large vision \nmodels through Neural Architecture Search (NAS). NOAH encompasses three prompt \nmodules: Adapter, LoRA, and VPT, each inserted into Transformer blocks. The search \nspace includes parameters like embedding dimensions D = {5, 10, 50, 100} and depths \nFig. 13 Illustration of the principles of three PEFT methods for image classification: VPT (Soft Prompt-\nbased), AdapterFormer (Adapter-based), and FacT (LoRA-based). ×i in FacT is mode-i product\n \n1 3\nPage 41 of 64 227\nL. Wang et al.\nL = {3, 6, 9, 12}, determining the range of applications. An AutoFormer-based one-shot \nNAS algorithm is employed to select the best configuration for each downstream dataset. \nConvpass (Jie and Deng 2022), convolutional bypasses for ViTs, to serve as adaptation \nmodules during finetuning. Convpass, introduced as a parallel convolutional bottleneck \nblock to the Multi-Head Self-Attention (MHSA) or MLP blocks, “bypasses” the original \nViT block. For a ViT layer, the input sequence X ∈ RN×d is processed through Convpass, \nreconstructing the spatial structure of the token sequence. During finetuning, only Con -\nvpass modules and the classification head are updated. Convpass leverages the inductive \nbias of convolutional layers, enhancing its suitability for visual tasks, particularly in low-\ndata scenarios. AdaptFormer (Chen et al. 2022a) is a lightweight module designed for \nefficient fine-tuning of pre-trained ViTs on diverse visual recognition tasks. It introduces \nadditional trainable parameters, consisting of two fully connected layers FC1, FC2, a non-\nlinear activation function ( σ), and a scaling factor ( α). These components are placed in \nparallel with the feed-forward network (FFN) of the original ViT. The learnable parameters \nof AdaptFormer are updated during the fine-tuning phase, while the pre-trained ViT param-\neters remain frozen. This design enables AdaptFormer to enhance the transferability of ViTs \nwith minimal parameter updates, thereby improving scalability and performance on various \nvisual tasks. DAM-VP (Diversity-Aware Meta Visual Prompting) (Huang et al. 2023b) par-\ntitions a dataset into homogeneous subsets based on diversity, optimizing a unique prompt \nfor each subset. Prompts are initialized with a meta-prompt learned across multiple datasets, \nimproving convergence speed and performance. During inference, the appropriate prompt is \nselected based on the feature distance between input and subset prototypes. Formally, for a \ndataset D divided into K subsets D1,D 2, ..., DK, the optimal prompts p∗\n1, ..., p∗\nK are found \nby minimizing the cross-entropy loss:\n \np∗\n1, ..., p∗\nK = arg min\np1,...,pK\nK∑\nk=1\n∑\nx∈Dk\nLCE(M(x + pk),y ), (33)\nwhere pk is the prompt for subset Dk, M is the pre-trained model, x is an input image, \ny is the ground truth label, and LCE is the cross-entropy loss function. ILM-VP (Chen \net al. 2023a) is an iterative label mapping-based visual prompting method. It optimizes the \nmapping between source and target labels to improve the accuracy of reprogramming pre-\ntrained models for new tasks. The key equation is:\n \nmin\nδ\n∑\nyt∈T tr\nmin\nys∈Ss\nL(fθ(x + δ), ys; yt), (34)\nwhere δ is the visual prompt, L is the cross-entropy loss, fθ is the pre-trained model, x is \nthe input image, Ttr is the target training set, Ss is the set of source labels, and ys and yt are \nthe source and target labels, respectively. ILM-VP enhances interpretability by providing \nmeaningful mappings. EVP (Enhanced Visual Prompting) (Wu et al. 2024a) is a method \nfor adapting pre-trained models to downstream tasks without substantial parameter updates. \nInstead of directly combining the prompt P and the image I, they shrink I and pad P around \nit, ensuring independence. They also reintroduce input diversity and gradient normalization \ntechniques, originally used in adversarial example generation, to improve the optimization \n1 3\n227 Page 42 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nand generalizability of the prompt. This approach outperforms linear probing and matches \nfully fine-tuning in some cases, with significantly fewer parameters. VQT (Visual Query \nTuning) (Tu et al. 2023) leverages learnable “query” tokens in each Transformer layer to \nsummarize intermediate features effectively. VQT introduces a set Q = {q1,q 2,...,q n} \nwhere qi ∈ Rd represents the i-th query token with d being the feature dimension. These \nqueries interact with the intermediate features X ∈ RN×d through the attention mechanism, \nwhere N is the number of tokens. The output Z = {z1,z 2,...,z n} summarizes the layer’s \ninformation, with zi denoting the summary for qi. This enables efficient transfer learning \nwith memory and parameter savings. FacT (Jie and Deng 2023) is a method for efficient \nfine-tuning of pre-trained ViTs by updating only a fraction of parameters. The key idea is \nto tensorize the weights of ViT into a 3D tensor and decompose the weight increments into \nlightweight factors. During fine-tuning, only these factors are updated and stored. Math -\nematically, if ∆W represents the increment of a weight matrix W, then ∆W is approxi -\nmated as ∆W ≈ A × B, where A and B are the decomposed factors. A and B are learned \nduring fine-tuning, reducing storage requirements. DTL (Disentangled Transfer Learn -\ning) (Fu et al. 2024) addresses the inefficiency of Parameter-Efficient Transfer Learning \n(PETL) methods in GPU memory usage. DTL employs a Compact Side Network (CSN) to \ndisentangle trainable parameters from the backbone. CSN uses low-rank linear mappings to \nextract and reintegrate task-specific information. Formally, given a backbone with N blocks, \nthe output zi+1 of the i-th block is updated as z′\ni+1 = zi+1 + θ(hi+1) for i ≥ M, where θ is \na non-linear activation function, and hi+1 captures the task-specific information extracted \nby CSN. This disentanglement significantly reduces GPU memory footprint and trainable \nparameters while maintaining or improving accuracy. LION (impLicit vIsion prOmpt tuN-\ning) (Wang et al. 2024b) inserts two equilibrium implicit layers (P1, P2) at the start and end \nof a frozen pre-trained backbone (θ). P1 and P2 are defined as:\n P1 = f(1)\neq (x; ϕ1),P 2 = f(2)\neq (z; ϕ2), (35)\nwhere x is the input, z is the output of the backbone, and ϕ1, ϕ2 are parameters of the \nimplicit layers. feq denotes the equilibrium function. To reduce computational burden, \nparameters are pruned based on the lottery ticket hypothesis. LION adapts the backbone to \ndownstream tasks efficiently with minimal parameter updates.\n4.1.2 Dense prediction\nDense prediction, encompassing tasks such as image segmentation, object detection, depth \nestimation, etc., is another crucial task in the field of 2D vision. Unlike image classification \ntasks, which typically generate a single prediction label for an entire image, dense predic -\ntion tasks require making predictions for every pixel in the image, usually resulting in an \noutput image with the same resolution as the input image. Fine-tuning pre-trained models \nfrom image classification is a common approach for dense prediction tasks. With the appli-\ncation of PEFT methods in vision tasks, various PEFT methods tailored for dense prediction \ntasks have been proposed. Figure 14 illustrates a representative PEFT method for dense \nprediction.\nPolyhistor (Liu et al. 2022a) employs a strategy of hypernetworks that are broken down \ninto components, along with scaling kernels applied at each layer, to facilitate the sharing \n1 3\nPage 43 of 64 227\nL. Wang et al.\nof information across various tasks efficiently and with a minimal number of parameters. In \nthis approach, the weight matrix of each adapter, denoted as W, is decomposed into two dis-\ntinct elements: a template kernel T and a scaling kernel S. The weight matrix is then recon-\nstructed through the Kronecker product of these two kernels, represented as W = T ⊗ S\n. This method effectively reduces the number of parameters required while still preserving \nthe level of accuracy in the system. ViT-Adapter (Chen et al. 2022c) leverages the inherent \nrepresentation power of a plain ViT backbone and augments it with an adapter that incorpo-\nrates image-specific inductive biases during fine-tuning. This enables the model to capture \nhigh-frequency details crucial for tasks like object detection and segmentation. SAN (Side \nAdapter Network) (Xu et al. 2023b) decouples mask proposal generation and class recogni-\ntion for open-vocabulary semantic segmentation. A lightweight side network is attached to \na frozen CLIP model, predicting mask proposals and attention bias to guide CLIP’s recogni-\ntion of the mask’s class. This design leverages CLIP’s robustness while minimizing addi -\ntional parameters and computational cost. The attention bias is applied in CLIP’s attention \nmechanism Attention(Q, K, V,bias), where Q, K, and V  represent query, key, and value \nvectors, enhancing CLIP’s awareness of the proposed regions. LoRand (Yin et al. 2023) \nadds lightweight, low-rank adapter modules to a pre-trained vision model, such as the Swin \nTransformer, without updating the original model’s parameters. These adapters consist of \nmulti-branch low-rank projections and non-linearities, enabling them to capture complex \nrepresentations with minimal parameters. Specifically, for a backbone with parameters θ, \nLoRand trains a small subset ϕ (1% − 3%) of θ, where ϕ ⊂ θ, achieving competitive perfor-\nmance with full fine-tuning while significantly reducing the number of trainable parameters.\n4.2 PEFT in diffusion models\nAs diffusion models evolve, these models have now surpassed GANs as the mainstream \nmethod in the image generation domain. Given their success in image generation, their \npotential applications in video generation, 3D content generation, and speech synthesis are \nalso becoming increasingly apparent. Additionally, many application domains involve fine-\nFig. 14 Illustration of a representative PEFT method for dense prediction: Polyhistor\n \n1 3\n227 Page 44 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\ntuning diffusion models, including embedding personalized concepts in image generation, \ncustomizing generated images based on reference images, and training multi-view image \ngeneration capabilities based on pre-trained text-to-image diffusion models in the 3D con -\ntent generation domain. Compared to the NLP field, research on PEFT for diffusion models \nis relatively scarce. Current research mainly focuses on two areas: generation by few-shot \nfinetuning and controllable generation in image generation:\n4.2.1 Generation by few-shot finetuning\nGeneration by few-shot finetuning involves providing a few images (or even just one) of an \nobject or style, and fine-tuning the model on these images. This process allows the model to \ngenerate new images that reflect the unique characteristics of the provided examples.\nDreamBooth (Ruiz et al. 2023) is a method for personalizing text-to-image diffusion \nmodels using just a few images of a subject. The technique fine-tunes a pre-trained model \nwith a novel autogenous class-specific prior preservation loss, to bind a unique identifier \nto the subject and preserve class diversity. This enables generating photorealistic images \nof the subject in various scenes while maintaining key features. The fine-tuning process \ninvolves adjusting the model parameters based on input images and text prompts, leverag -\ning the model’s semantic prior and the new loss function to enhance subject fidelity and \nversatility in image synthesis. Textual Inversion (Gal et al. 2022) is a method that personal-\nizes text-to-image generation by embedding unique concepts as new “pseudo-words” in the \nlatent space of a pre-trained model. This allows intuitive composition into sentences guid -\ning image creation, capturing both semantics and details without retraining the model. The \ninnovation lies in optimizing a single word embedding to represent a concept through recon-\nstruction, balancing distortion and editability. The method’s strength is its simplicity and \ncompatibility with existing models, while its limitation is the potential for less precise shape \nretention. DreamArtist (Dong et al. 2022) leverages positive–negative prompt-tuning to \nenable one-shot text-to-image generation. Given a reference image I, it learns a positive \nembedding S∗\np  that captures the image’s characteristics and a negative embedding S∗\nn that \nrectifies deficiencies. S∗\np  drives diverse generation, while S∗\nn ensures corrections, improving \ncontrollability. The embeddings are combined through a fusion function fm(zp,z n) where \nzp and zn represent the latent representations of positive and negative prompts, respectively. \nThis approach facilitates the synthesis of high-quality, diverse, and controllable images \nfrom a single reference. In paper (V oynov et al. 2023b), an Extended Textual Conditioning \n(P+) space is introduced for text-to-image generation, allowing for more granular control \nover image synthesis through per-layer textual prompts. The innovation, Extended Textual \nInversion, inverts images into P+ space using a set of token embeddings, enhancing expres-\nsiveness and precision without compromising editability. This method is advantageous due \nto its faster convergence and the ability to achieve finer control over image attributes by \nleveraging the distinct sensitivities of U-net layers to shape or appearance. The downside \nincludes imperfect concept reconstruction and the relatively slow inversion process. Dif-\nfFit (Xie et al. 2023) fine-tunes only the bias terms and introduces scaling factors γ in \nspecific layers, initialized to 1.0, to adapt to new domains quickly. The method achieves \nsignificant training efficiency and reduced storage costs, with γ enhancing feature scaling \nfor better adaptation. The efficacy is theoretically justified by analyzing the shift in distri -\nbutions caused by the scaling factors. SVDiff (Han et al. 2023) is a method for fine-tuning \n1 3\nPage 45 of 64 227\nL. Wang et al.\ntext-to-image diffusion models by adjusting the singular values (σi) of weight matrices (W\n), represented as W = ∑\ni σiuiv⊤\ni , where ui and vi are the left and right singular vec -\ntors, respectively. This approach leads to a compact parameter space, reducing overfitting \nand model size (≈ 2, 200× fewer parameters than DreamBooth). They also introduce Cut-\nMix-Unmix for improved multi-subject generation and a single-image editing framework. \nLyCORIS (Yeh et al. 2023) is an open-source library for fine-tuning Stable Diffusion mod-\nels. It implements methods like LoRA, LoHa, LoKr, GLoRA, and (IA)3. The library aims \nto simplify the integration and evaluation of these methods. A comprehensive evaluation \nframework is proposed, using metrics for concept fidelity, text-image alignment, diversity, \nand style preservation. Experiments highlight the nuanced impacts of hyperparameters and \nthe suitability of different methods for specific tasks. DiffuseKronA (Marjit et al. 2024) \nutilizes a Kronecker product-based adaptation mechanism to efficiently fine-tune large dif -\nfusion models for personalized text-to-image generation. The method reduces the param -\neter count by applying truncated singular value decomposition on critical model layers, \nenabling subject-specific image synthesis with enhanced stability, interpretability, and text \nalignment. The approach offers a ≥ 50% parameter reduction compared to state-of-the-art \nmethods, with comparable or superior image quality. OFT (Orthogonal Finetuning) (Qin \net al. 2024) is a method to adapt text-to-image diffusion models for downstream tasks with-\nout losing generative performance. OFT preserves the hyperspherical energy which char -\nacterizes neuron relationships by applying a layer-shared orthogonal transformation R to \nthe pretrained weights W0. This maintains the pairwise angles among neurons, crucial for \nsemantic information. The transformation is constrained as RT R = RRT = I, ensuring \nminimal deviation from the original model. A variant, Constrained Orthogonal Finetuning \n(COFT), further limits angular deviation with ∥R − I∥≤ ϵ. The method aims to balance \nflexibility and stability in finetuning.\n4.2.2 Controllable generation\nControllable generation primarily involves adding control sources beyond the prompt to \nguide the image generation. These control sources can include sketches, keypoints, or other \nforms of guidance to shape the generated output more precisely. A representative implemen-\ntation of controllable generation method is shown in Fig. 15\nSketch-guided diffusion (V oynov et al. 2023a) is a method to guide pre-trained text-to-\nimage diffusion models using spatial maps like sketches. It involves training a lightweight \nper-pixel multi-layer perceptron (MLP), named the latent guidance predictor (LGP), to map \nnoisy image features to spatial maps. The LGP is trained on a small dataset, predicting \nspatial layouts from latent features F (zt|c,t ) extracted from a denoising diffusion probabi-\nlistic model (DDPM) network, where zt is a noisy image at timestep t, and c presents the \nconditioning text prompt. ControlNet (Zhang et al. 2023c) enhances pretrained text-to-\nimage diffusion models by adding spatially localized conditions. For a neural block F(x; Θ) \ntransforming input x to output y, ControlNet freezes Θ and introduces a trainable copy. \nConditions c are injected through zero-initialized convolution layers (zero convolutions) \nensuring no initial noise. yc = F (x, c;Θ′) represents the output with conditions, where Θ′ \ndenotes the updated parameters. This approach facilitates robust finetuning and sudden con-\nvergence. T2I-Adapter (Mou et al. 2023) enhances controllability of pre-trained text-to-\nimage (T2I) models by learning lightweight adapter models that align the model’s internal \n1 3\n227 Page 46 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nknowledge with external control signals. This is achieved without modifying the original \nT2I model, allowing for granular control over generated images’ structure and color. Math-\nematically, let M denote the pre-trained T2I model, A the adapter, and xc the control signal \n(e.g., sketches, masks). The adapted model generates images x from text prompts t and \ncontrol signals xc as follows:\n x = Madapted(t, xc)= M(t)+ω ·A (xc), (36)\nwhere ω is a weighting factor balancing the influence of the control signal. The adapter A \nis trained to translate xc into a form that can steer M towards desired outputs, enabling \nprecise control. Uni-ControlNet (Zhao et al. 2024) integrates diverse control signals into \npre-trained text-to-image (T2I) diffusion models through two lightweight adapters, facilitat-\ning efficient and composable control. It employs a multi-scale condition injection strategy, \nusing Feature Denormalization (FDN) to modulate noise features with local conditions:\n FDNr(Zr,c l)= norm(Zr) · (1 +convγ (zero(hr(cl)))) +convβ(zero(hr(cl))), (37)\nFig. 15 Illustration of the principle of ControlNet, a representative implementation of a controllable gen-\neration method\n \n1 3\nPage 47 of 64 227\nL. Wang et al.\nwhere Zr are noise features at resolution r, cl are concatenated local conditions, hr extracts \nfeatures at resolution r, and convγ converts features into modulation coefficients. Global con-\ntrols are aligned with text embeddings via a condition encoder. hg(cg) → K global tokens \nHere, cg is the global condition, and K is the number of global tokens. IP-Adapter (Ye \net al. 2023) enables pretrained text-to-image models to utilize image prompts effectively. It \nintroduces a decoupled cross-attention mechanism, adding extra layers dedicated to image \nfeatures while keeping the original text-focused layers intact. During training, these new \nlayers learn to process image embeddings extracted by a CLIP encoder. At inference, the \nimage and text features are processed separately then combined, improving controllability \nand fidelity of generated images. The core equation is:\n ˆϵθ(xt, c, t)=wϵθ(xt, c, t)+( 1− w)ϵθ(xt,t ), (38)\nwhere ˆϵθ(xt, c, t) is the predicted noise, w is the guidance scale adjusting the influence of \ncondition c, ϵθ(xt, c, t) is the conditional noise prediction, and ϵθ(xt,t ) is the unconditional \nprediction.\n4.3 PEFT in MLLM\nThe PEFT of MLLM primarily focuses on the model connector. It is because maintain \nconsistency for both multimodal and textual data is challenging. As a consequence, a modal \nconnector is serially connected right before the LLM, converting multimodal embeddings \ninto understandable text prompt tokens for the LLM. Training the model connector on PEFT \ndataset bridges the gap between different modal data while ensuring consistency in the input \nto the LLM. As a representative PEFT approach within the MLLM framework, the sche -\nmatic diagram of LLaMA-Adapter (Zhang et al. 2023g) is illustrated in Fig. 16.\nGenerally, the parameter scale of the model connector will not be very large, much \nsmaller than the prevalent LLMs. Therefore, full-parameter training instead of PEFT is \nmore prevalent for model connector. Studies of the model connector primarily focus on the \nstructural design, which will be dedicated to improving the training performance. A classic \ndesign of the modal connector involves employing a set of learnable query tokens to extract \ninformation in a query-based manner, a technique first introduced in BLIP-2 (Li et al. \n2023a) and subsequently adopted by various projects (Dai et al. 2024b). These query-based \napproaches, reminiscent of Q-Former-style methods, condense visual tokens into a smaller \nFig. 16 Illustration of the principle of LLaMA-Adapter, which is a representative PEFT method in MLLM\n \n1 3\n227 Page 48 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nset of representation vectors. In the meantime, some methods utilize an MLP-based inter -\nface to bridge the modality gap. For instance, the LLaV A series  (Liu et al. 2023a, 2024c) \nemploys one or two linear MLPs to project visual tokens and align feature dimensions \nwith word embeddings. In feature-level fusion, additional modules facilitate deep interac -\ntion and fusion between text features and visual features. For example, Flamingo (Alayrac \net al. 2022) introduces extra cross-attention layers between the frozen Transformer layers \nof LLMs, enhancing language features with external visual cues. In addition, adapters and \nprompt embedding are also applied to add learnable parameters to fill the gap, such as \nLLaMA Adapter (Zhang et al. 2023g) and CogVLM (Wang et al. 2023c).\nFigure 17 illustrates the concrete structures of the two designs. The first one, pioneered \nby the LLaV A series, is characterized by its simplicity. As highlighted by Liu et al. (2024c), \nan MLP composed of basic linear layers is adept at transforming multimodal embeddings \ninto LLM prompt tokens.\nIn contrast, the second paradigm, known as the Q-Former (Li et al. 2023a; Dai et al. \n2024b), introduces a transformer neural network for modal information conversion. Unlike \ntraditional approaches of directly applying self-attention on input embeddings, Q-Former \nemploys a set of trainable query tokens. This approach bears resemblance to LLM PEFT \nmethods such as prefix-tuning and p-tuning, which incorporate external trainable embed -\nding tokens. However, the key distinction lies in how these methods handle the tokens: \nprefix-tuning and p-tuning append them to the input text tokens to form a comprehensive \nLLM input, while Q-Former accepts the query tokens as the primary input.\nFrom both the structural design and training intricacies, it becomes evident that Q-Former \nis considerably more complicated compared to the MLP-based LLaV A. However, this com-\nplexity comes with its advantages. A comprehensive transformer network like Q-Former \nenables the execution of numerous pre-trained tasks, facilitating explicit alignment between \nnon-textual and textual modalities. This, in turn, reduces the quality requirements on the \nmultimodal data. Nevertheless, LLaV A, as detailed by Liu et al. (2024c), which incorpo -\nrates GPT-4 (Achiam et al. 2023) as the LLM, reports a slight performance improvement \nFig. 17 Modal connector design: this figure shows two different mainstream design of the modal connec-\ntor in MLLM. The first one, a simple MLP for converting the modal. The second one, layers with cross \nattention and query tokens for training\n \n1 3\nPage 49 of 64 227\nL. Wang et al.\nover BLIP-2. This is largely attributed to the inherent superiority of GPT-4 over BLIP-2’s \nFlan-T5 across various aspects. Specifically, GPT-4 possesses innate multimodal reasoning \ncapabilities, a feature lacking in Flan-T5. This observation underscores the fact that a com-\nprehensive modal connector design may not be necessary when the LLM itself possesses \nsignificant power and capabilities.\nTo further quantify the performance of different PEFT methods in specific applications, \nwe present Table 7, which compares various methods based on key metrics such as accuracy \nand the number of trainable parameters across multiple benchmark tasks. Since existing \nliterature does not provide detailed computational cost analysis, we use the number of train-\nable parameters as an approximate measure of computational efficiency, serving as a practi-\ncal proxy for resource consumption across different PEFT methods. As shown in Table 7, \ncompared to full fine-tuning, PEFT methods in specific applications significantly reduce the \nnumber of trainable parameters while maintaining competitive performance. These results \nhighlight the advantage of PEFT methods in various applications, where they enable effi -\ncient adaptation of large models with lower computational and storage costs while preserv-\ning task-specific performance.\n5 Future directions\nIn this section, focusing on potential issues with existing PEFT techniques and aspects that \nhave not received sufficient attention, we propose a series of possible research directions. \nThese directions encompass task, data, model, learning mechanisms, and fundamental \nflaws. \n1. PEFT methods for multi-objective tasks: Current PEFT methods mainly focus on opti-\nmizing for single objectives (e.g., task accuracy), but real-world applications often \nrequire balancing multiple objectives (e.g., privacy, fairness, latency). For example, in \nhealthcare, models must preserve patient privacy while maintaining diagnostic accu -\nracy. Existing methods like LoRA or Adapters lack explicit mechanisms to handle such \ntrade-offs. In recent work (Yang et al. 2024), the authors addressed the program repair \ntask by incorporating a dual-objective optimization framework, wherein the two objec-\ntives were combined through linear weighting with manually predefined coefficients to \nformulate the model’s loss function. Although this study presents a straightforward and \neffective approach to PEFT for multi-objective tasks, determining the optimal weight -\ning coefficients remains non-trivial. This limitation highlights the need for developing \nmore flexible and task-adaptive methodologies to enhance the robustness and generaliz-\nability of such approaches.\n2. PEFT methods in multimodal learning:  Multimodal models (e.g., vision-language \nmodels) face unique challenges in aligning heterogeneous data streams (text, images, \naudio). Current PEFT methods (e.g., adapters) are primarily designed for unimodal \nLLMs, leading to suboptimal performance in tasks like visual question answering. \nRecent work on CLIP adaptations  (Zavras et al. 2024) highlights the need for modal -\nity-specific parameter-efficient tuning to bridge domain gaps. Multimodal learning \nhas emerged as one of the most prominent research topics in contemporary machine \nlearning. However, significant challenges persist in effectively integrating cross-modal \n1 3\n227 Page 50 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\ninformation through PEFT approaches, particularly in achieving optimal inter-modal \nalignment and representation learning while maintaining computational efficiency.\n3. Automated design of adapter modules:  Adapter architectures (e.g., bottleneck layers) \nrely on manually tuned hyperparameters (e.g., dimension, placement), which limits scal-\nability. Neural Architecture Search (NAS) techniques (Xu and Wen 2024) could auto-\nmate adapter design, optimizing for both parameter efficiency and task performance. \nHowever, the extensive design space of adapter modules significantly compromises the \nefficiency of NAS approaches. This limitation necessitates further investigation into \nmore efficient and flexible automated design methodologies that can navigate the com-\nplex parameter space effectively while maintaining architectural optimality.\n4. Heuristic search strategies for hybrid PEFT methods:  Hybrid methods (e.g., combin -\ning LoRA and adapters) often rely on trial-and-error combinations, lacking principled \nstrategies. For example, in paper (Chen et al. 2023b), the authors, under a predefined \ndesign space, conduct numerous experiments to determine an ideal hybrid strategy. \nHowever, the optimal hybrid strategy may not be included within this artificially pre -\ndefined design space. Therefore, introducing heuristic search strategies to find the best \nhybrid strategy is a promising direction for future research.\n5. Continual learning for PEFT methods:  Deployed models must adapt to evolving data \ndistributions (e.g., user preferences in chatbots). Traditional PEFT lacks mechanisms \nto prevent catastrophic forgetting. Current work (Wei et al. 2024) proposed a method \nfor task-free online continual learning that dynamically adapts pretrained Vision Trans-\nformer models by adding new low-rank adaptation parameters when the loss surface \nplateaus, indicating data distribution shifts, and uses online weight regularization to \nmitigate catastrophic forgetting. The experimental results presented in this paper dem -\nonstrate significant performance improvements through the application of LoRA, estab-\nlishing a valuable reference framework for investigating continual learning paradigms \nin other types of PEFT methodologies.\n6. Improving the calibration of fine-tuned LLMs: To date, numerous PEFT approaches \ndeveloped for the purpose of adeptly tailoring LLMs to downstream tasks have achieved \nnotable advancements in computational and storage efficiency. Nonetheless, when sub-\njected to fine-tuning on modest datasets, LLMs are often prone to overconfidence in \ntheir predictions (Jiang et al. 2021; Tian et al. 2023; Achiam et al. 2023). This phe -\nnomenon is especially pernicious for decision-making processes within safety-critical \napplications or domains where data is scarce, such as medical diagnostics, financial \nservices, and experimental design (Singhal et al. 2023; Lee et al. 2024; Huang et al. \n2024b). Hence, there exists an exigent demand for the formulation of strategies aimed \nat refining the calibration of fine-tuned LLMs, ensuring that their predictive outputs are \nnot only dependable but also robust.\n7. Differential privacy for PEFT methods: Different downstream tasks often involve vary-\ning levels of sensitve and personal data, which further emphasizes the need for pri -\nvacy in LLM fine-tuning, particularly with PEFT methods. The integration of LLM \nfine-tuning and differential privacy holds significant promise for future research. How-\never, existing differential privacy techniques, such as DP-SGD (Abadi et al. 2016) \nand DP-AdamW (Li et al. 2021), often result in limited performance and substantial \ncomputaitional cost. Therefore, future reasearch should focus on developing methods \nthat preserve privacy while simultaneously optimizing performance and minimizing \n1 3\nPage 51 of 64 227\nL. Wang et al.\nTask Model PEFT method #TPs (M) Result\nCIFAR 100 CIFAR 10 Flowers Food EuroSAT SUN DMLab SVHN Pets DTD RESISC CLEVR\nImage Classification CLIP FT 151.28 82.1 95.8 97.4 87.8 99 79 63.5 95.7 88.5 72.3 98.1 94.4\nVP 0.07 75.3 94.2 62 83.2 95.6 68.4 41.9 88.4 86.5 57.1 84.1 81.4\nVPT 0.064 76.6 95 76.2 84.7 94.6 69.3 48.4 86.1 92.1 61.6 84.3 58.6\nEVP 0.062 81.2 96.6 82.3 84.1 97.6 71 62.3 90.5 90 68.4 89.7 75.9\nTask Model PEFT method #TPs (M) Seg. H.Part Sal. Normals.\nDense Prediction Swin Transform-\ner -Tiny\nSingle-task FT 112.62 67.21 61.93 62.35 17.97\nMulti-task FT 30.06 68.71 62.13 64.18 17.35\nBitfit 2.85 68.57 55.99 60.64 19.42\nRelative bias 2.64 63.51 52.35 57.74 21.07\nVPT-shallow 2.57 62.96 52.27 58.31 20.9\nVPT-deep 3.43 64.35 55.24 58.15 21.07\nPHM layer 3.14 68.55 56.28 60.35 19.23\nCompacter 2.78 68.38 56.69 59.47 19.54\nCompacter++ 2.66 67.26 55.69 59.47 19.54\nLoRA 2.87 67.26 55.69 59.47 19.54\nAdapter 11.24 69.21 57.38 61.28 18.83\nLow-rank adapter 2.89 68.31 56.53 60.29 19.36\nShared Adapter 4.74 70.21 59.15 62.29 19.26\nHyperformer 75.32 71.43 60.73 65.54 17.77\nPolyhistor 8.96 70.87 59.54 65.47 17.47\nPolyhistor-Lite 2.96 70.24 59.12 64.75 17.4\nTable 7 Performance of PEFT methods in specific applications\n1 3\n227 Page 52 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nTask Model PEFT method #TPs \n(M)\nFood SUN DF- 20 M Caltech CUB- \nBird\nArtBench Oxford \nFlowers\nStandard \nCars\nAver-\nage \nFID\nGeneration-\nby Few-shot \nFinetuning\nDiT-XL-2 FT 673.8 10.46 7.96 17.26 35.25 5.68 25.31 21.05 9.79 16.59\nAdapt-Parallel 4.28 13.67 11.47 22.38 35.76 7.73 38.43 21.24 10.73 20.17\nAdapt-Sequential 4.28 11.93 10.68 19.01 34.17 7 35.04 21.36 10.45 18.7\nBitFit 0.61 9.17 9.11 17.78 34.21 8.81 24.53 20.31 10.64 16.82\nVPT-Deep 2.81 18.47 14.54 32.89 42.78 17.29 40.74 25.59 22.12 26.8\nLoRA-R8 1.15 33.75 32.33 120.25 86.05 56.03 80.99 164.13 76.24 81.31\nLoRA-R16 2.18 34.34 32.15 121.51 86.51 58.25 80.72 161.68 75.35 81.31\nDiffFit 0.83 6.96 8.55 17.35 33.84 5.48 20.87 20.18 9.9 15.39\nTask Model PEFT method #TPs (M) CLIP-T CLIP-I\nControllable Generation CLIP ViT-L/14 Uni-ControlNet (Global Control) 47 0.51 0.74\nT2I-Adapter (Style) 39 0.49 0.65\nControlNet Shuffle 361 0.42 0.62\nIP-Adapter 22 0.59 0.83\nAll performance metrics are cited from prior published work (Wu et al. 2024a; Liu et al. 2022a; Xie et al. 2023; Mou et al. 2023). Metrics vary by task: 1. Image Classification: \n12 datasets with CLIP. 2. Dense Prediction: 4 datasets with Swim Transformer-Tiny. 3. Generation by Few-shot Finetuning: 9 datasets with DiT-XL-2. 4. Controllable \nGeneration: 2 datasets with CLIP ViT-L/14\nTable 7 (continued)\n \n1 3\nPage 53 of 64 227\nL. Wang et al.\ncomputational costs. Additionally, exploring scalable, privacy preserving methods tai -\nlored to PEFT methods is essential. These advancements will enable secure and effi -\ncient fine-tuning of LLMs, ensuring robust privacy protections.\n6 Conclusions\nLLMs have garnered widespread attention due to their exceptional performance across \na broad spectrum of natural language tasks, beginning with the release of ChatGPT in \nNovember 2022. These models have acquired the capability for general-purpose language \nunderstanding and generation by training billions of parameters on vast amounts of textual \ndata, as predicted by scaling laws. Traditional full-parameter fine-tuning methods pose sig-\nnificant challenges when customizing these models for specific downstream tasks, particu -\nlarly on hardware platforms with limited computational capabilities, due to their enormous \nparameter scale and computational demands. PEFT has emerged as an efficient method for \nadapting to various downstream tasks, minimizing the number of additional parameters \nintroduced or the computational resources required, thereby enabling the fine-tuned model’s \nperformance to approach or even surpass that of full-parameter fine-tuning methods. This \nsurvey provides a systematic overview of the latest advancements in PEFT, encompassing \nintroductions to classic pre-trained large models, classification and principle explanation of \nPEFT algorithms, applications of PEFT methods, and prospects for future research direc -\ntions in PEFT. This survey not only offers readers a comprehensive and systematic orga -\nnization of PEFT work but also inspires researchers in various fields to identify potential \nresearch directions in PEFT research, accelerating the research process of PEFT methods.\nAuthor contributions I have read the Nature Portfolio journal policies on author responsibilities and submit \nthis manuscript in accordance with those policies.\nFunding This work was supported by the National Key Research and Development Program of China (Grant \nNo. 2023YFE0108600), National Natural Science Foundation of China (Grant No. U22A6001), Shanghai \nArtificial Intelligence Laboratory (Grant No. P22KN00581) and “Pioneer” and “Leading Goose” R&D Pro-\ngram of Zhejiang (Grant No. 2024SSYS0002).\nData availability No, I do not have any research data outside the submitted manuscript file.\nDeclarations\nConflict of interest Yes, the authors have Conflict of interest as defined by Springer, or other interests that \nmight be perceived to influence the results and/or discussion reported in this paper.\nOpen Access I confirm that I understand Artificial Intelligence Review is an open access journal that levies \nan article processing charge per articles accepted for publication. By submitting my article I agree to pay this \ncharge in full if my article is accepted for publication.\nConsent to publication The results/data/figures in this manuscript have not been published elsewhere, nor are \nthey under consideration (from you or one of your Contributing Authors) by another publisher.\nThird party material All of the material is owned by the authors and/or no permissions are required.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n1 3\n227 Page 54 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction \nin any medium or format, as long as you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do \nnot have permission under this licence to share adapted material derived from this article or parts of it. The \nimages or other third party material in this article are included in the article’s Creative Commons licence, \nunless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative \nCommons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, \nyou will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h \nt t p : /  / c r e a  t i v e c o  m m o n  s . o r g  / l i c e  n s e s / b  y - n c  - n d / 4 . 0 /.\nReferences\nAbadi M, Chu A, Goodfellow I et al (2016) Deep learning with differential privacy. In: Proceedings of the \n2016 ACM SIGSAC conference on computer and communications security, pp 308–318\nAchiam J, Adler S, Agarwal S et al (2023) GPT-4 technical report. arXiv preprint. arXiv:2303.08774\nAghajanyan A, Zettlemoyer L, Gupta S (2020) Intrinsic dimensionality explains the effectiveness of language \nmodel fine-tuning. arXiv preprint. arXiv:2012.13255\nAghajanyan A, Gupta A, Shrivastava A et al (2021) MUPPET: massive multi-task representations with pre-\nfinetuning. arXiv preprint. arXiv:2101.11038\nAgiza A, Neseem M, Reda S (2024) MTLORA: a low-rank adaptation approach for efficient multi-task learn-\ning. arXiv preprint. arXiv:2403.20320\nAhn J, Verma R, Lou R et al (2024) Large language models for mathematical reasoning: progresses and chal-\nlenges. arXiv preprint. arXiv:2402.00157\nAlayrac JB, Donahue J, Luc P et al (2022) FLAMINGO: a visual language model for few-shot learning. Adv \nNeural Inf Process Syst 35:23716–23736\nAnil R, Borgeaud S, Wu Y et al (2023) Gemini: A family of highly capable multimodal models. arXiv pre -\nprint. arXiv:2312.11805 1\nAnsell A, Ponti EM, Korhonen A et al (2021) Composable sparse fine-tuning for cross-lingual transfer. arXiv \npreprint. arXiv:2110.07560\nAnthropic (Online) Claude. https://www.anthropic.com/claude. Accessed 11 Feb 2025\nAribandi V , Tay Y , Schuster T et al (2021) EXT5: towards extreme multi-task scaling for transfer learning. \narXiv preprint. arXiv:2111.10952\nAsai A, Salehi M, Peters ME et al (2022) Attempt: parameter-efficient multi-task tuning via attentional mix-\ntures of soft prompts. In: Proceedings of the 2022 conference on empirical methods in natural language \nprocessing, pp 6655–6672\nAustin J, Odena A, Nye M et al (2021) Program synthesis with large language models. arXiv preprint. \narXiv:2108.07732\nBach SH, Sanh V , Yong ZX et al (2022) Promptsource: an integrated development environment and reposi -\ntory for natural language prompts. arXiv preprint. arXiv:2202.01279\nBahng H, Jahanian A, Sankaranarayanan S et al (2022) Exploring visual prompts for adapting large-scale \nmodels. arXiv preprint. arXiv:2203.17274\nBai Y , Jones A, Ndousse K et al (2022a) Training a helpful and harmless assistant with reinforcement learning \nfrom human feedback. arXiv preprint. arXiv:2204.05862\nBai Y , Kadavath S, Kundu S et al (2022b) Constitutional ai: Harmlessness from ai feedback. arXiv preprint. \narXiv:2212.08073\nBaumgartner J, Zannettou S, Keegan B et al (2020) The pushshift reddit dataset. In: ICWSM. AAAI Press, \npp 830–839\nBender EM, Gebru T, McMillan-Major A et al (2021) On the dangers of stochastic parrots: Can language \nmodels be too big? In: Proceedings of the 2021 ACM conference on fairness, accountability, and trans-\nparency, pp 610–623\nBi X, Chen D, Chen G et al (2024) Deepseek llm: Scaling open-source language models with longtermism. \narXiv preprint. arXiv:2401.02954\nBiswas SS (2023) Role of Chat GPT in public health. Ann Biomed Eng 51(5):868–869\nBommasani R, Hudson DA, Adeli E et al (2021) On the opportunities and risks of foundation models. arXiv \npreprint. arXiv:2108.07258\nBrown T, Mann B, Ryder N et al (2020) Language models are few-shot learners. Adv Neural Inf Process \nSyst 33:1877–1901\n1 3\nPage 55 of 64 227\nL. Wang et al.\nCao J, Prakash CS, Hamza W (2022) Attention fusion: a light yet efficient late fusion mechanism for task \nadaptation in NLU. In: Findings of the Association for Computational Linguistics: NAACL 2022, pp \n857–866\nChen S, Hou Y , Cui Y et al (2020) Recall and learn: fine-tuning deep pretrained language models with less \nforgetting. arXiv preprint. arXiv:2004.12651\nChen M, Tworek J, Jun H et al (2021) Evaluating large language models trained on code. arXiv preprint. \narXiv:2107.03374\nChen S, Ge C, Tong Z et al (2022a) ADAPTFORMER: adapting vision transformers for scalable visual rec-\nognition. Adv Neural Inf Process Syst 35:16664–16678\nChen Y , Hazarika D, Namazifar M et al (2022b) Empowering parameter-efficient transfer learning by recog-\nnizing the kernel structure in self-attention. In: Findings of the Association for Computational Linguis-\ntics: NAACL 2022, pp 1375–1388\nChen Z, Duan Y , Wang W et al (2022c) Vision transformer adapter for dense predictions. arXiv preprint. \narXiv:2205.08534\nChen A, Yao Y , Chen PY et al (2023a) Understanding and improving visual prompting: a label-mapping \nperspective. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, \npp 19133–19143\nChen J, Zhang A, Shi X et al (2023b) Parameter-efficient fine-tuning design spaces. arXiv preprint. \narXiv:2301.01821\nChen L, Huang H, Cheng M (2023c) PTP: boosting stability and performance of prompt tuning with pertur-\nbation-based regularizer. arXiv preprint. arXiv:2305.02423\nChen W, Yin M, Ku M et al (2023d) THEOREMQA: a theorem-driven question answering dataset. In: \nEMNLP. Association for Computational Linguistics, pp 7889–7901\nChen Y , Fu Q, Fan G et al (2023e) Hadamard adapter: an extreme parameter-efficient adapter tuning method \nfor pre-trained language models. In: Proceedings of the 32nd ACM international conference on infor -\nmation and knowledge management, pp 276–285\nChen X, Liu J, Wang Y et al (2024) SUPERLORA: parameter-efficient unified adaptation of multi-layer \nattention modules. arXiv preprint. arXiv:2403.11887\nCherti M, Beaumont R, Wightman R et al (2023) Reproducible scaling laws for contrastive language-image \nlearning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, \npp 2818–2829\nCho J, Lei J, Tan H et al (2021) Unifying vision-and-language tasks via text generation. In: International \nconference on machine learning, PMLR, pp 1931–1942\nChoi JY , Kim J, Park JH et al (2023) SMOP: towards efficient and effective prompt tuning with sparse \nmixture-of-prompts. In: The 2023 conference on empirical methods in natural language processing\nChowdhery A, Narang S, Devlin J et al (2023) PALM: scaling language modeling with pathways. J Mach \nLearn Res 24(240):1–113\nChristiano PF, Leike J, Brown T et al (2017) Deep reinforcement learning from human preferences. In: \nAdvances in neural information processing systems, vol 30\nChronopoulou A, Peters ME, Fraser A et al (2023) Adaptersoup: Weight averaging to improve generalization \nof pretrained language models. arXiv preprint. arXiv:2302.07027\nChung HW, Hou L, Longpre S et al (2022) Scaling instruction-finetuned language models. arXiv preprint. \narXiv:2210.11416\nChung HW, Hou L, Longpre S et al (2024) Scaling instruction-finetuned language models. J Mach Learn \nRes 25(70):1–53\nClark P, Cowhey I, Etzioni O et al (2018) Think you have solved question answering? Try arc, the AI2 reason-\ning challenge. arXiv preprint. arXiv:1803.05457v1\nCobbe K, Kosaraju V , Bavarian M et al (2021) Training verifiers to solve math word problems. arXiv pre -\nprint. arXiv:2110.14168\nDai D, Deng C, Zhao C et al (2024a) DeepSeekMoE: towards ultimate expert specialization in mixture-of-\nexperts language models. arXiv preprint. arXiv:2401.06066\nDai W, Li J, Li D et al (2024b) InstructBLIP: towards general-purpose vision-language models with instruc-\ntion tuning. In: Advances in Neural Information Processing Systems, vol 36\nDan Y , Lei Z, Gu Y et al (2023) Educhat: a large-scale language model-based chatbot system for intelligent \neducation. arXiv preprint. arXiv:2308.02773\nDas SSS, Zhang RH, Shi P et al (2023) Unified low-resource sequence labeling by sample-aware dynamic \nsparse finetuning. arXiv preprint. arXiv:2311.03748\nDettmers T, Pagnoni A, Holtzman A et al (2024) QLORA: efficient finetuning of quantized LLMS. In: \nAdvances in neural information processing systems. vol 36\nDevlin J, Chang MW, Lee K et al (2018) BERT: pre-training of deep bidirectional transformers for language \nunderstanding. arXiv preprint. arXiv:1810.04805\n1 3\n227 Page 56 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nDing N, Qin Y , Yang G et al (2022) Delta tuning: a comprehensive study of parameter efficient methods for \npre-trained language models. arXiv preprint. arXiv:2203.06904\nDing N, Lv X, Wang Q et al (2023) Sparse low-rank adaptation of pre-trained language models. In: Proceed-\nings of the 2023 conference on empirical methods in natural language processing, pp 4133–4145\nDong Z, Wei P, Lin L (2022) DreamArtist: towards controllable one-shot text-to-image generation via posi-\ntive-negative prompt-tuning. arXiv preprint. arXiv:2211.11337\nDušek O, Novikova J, Rieser V (2020) Evaluating the state-of-the-art of end-to-end natural language genera-\ntion: the E2E NLG challenge. Comput Speech Lang 59:123–156\nEdalati A, Tahaei M, Kobyzev I et al (2022) Krona: parameter efficient tuning with kronecker adapter. arXiv \npreprint. arXiv:2212.10650\nEisele A, Chen Y (2010) Multiun: a multilingual corpus from united nation documents. In: LREC\nEthayarajh K, Choi Y , Swayamdipta S (2022) Understanding dataset difficulty with V-usable information. In: \nChaudhuri K, Jegelka S, Song L et al (eds) Proceedings of the 39th international conference on machine \nlearning, proceedings of machine learning research, vol 162. PMLR, pp 5988–6008\nFan J, Wang Z, Xie Y et al (2020) A theoretical analysis of deep Q-learning. In: Learning for dynamics and \ncontrol, PMLR, pp 486–489\nFu Z, Yang H, So AMC et al (2023) On the effectiveness of parameter-efficient fine-tuning. In: Proceedings \nof the AAAI Conference on Artificial Intelligence, pp 12799–12807\nFu M, Zhu K, Wu J (2024) DTL: disentangled transfer learning for visual recognition. In: Proceedings of the \nAAAI conference on artificial intelligence, pp 12082–12090\nG Team, Anil R, Borgeaud S et al (2023) Gemini: a family of highly capable multimodal models. arXiv \npreprint. arXiv:2312.11805\nGal R, Alaluf Y , Atzmon Y et al (2022) An image is worth one word: personalizing text-to-image generation \nusing textual inversion. arXiv preprint. arXiv:2208.01618\nGao L, Biderman S, Black S et al (2020) The Pile: ann 800gb dataset of diverse text for language modeling. \narXiv preprint. arXiv:2101.00027\nGardent C, Shimorina A, Narayan S et al (2017) Creating training corpora for NLG micro-planning. In: 55th \nAnnual meeting of the association for computational linguistics, ACL 2017. Association for Computa-\ntional Linguistics (ACL), pp 179–188\nGheini M, Ren X, May J (2021) Cross-attention is all you need: adapting pretrained transformers for machine \ntranslation. arXiv preprint. arXiv:2104.08771\nGlaese A, McAleese N, Trbacz M et al (2022) Improving alignment of dialogue agents via targeted human \njudgements. arXiv preprint. arXiv:2209.14375\nGliwa B, Mochol I, Biesek M et al (2019) Samsum corpus: a human-annotated dialogue dataset for abstrac-\ntive summarization. arXiv preprint. arXiv:1911.12237\nGokaslan A, Cohen V (2019) Openwebtext corpus.  h t t p : /  / S k y l  i o n 0 0 7  . g i t  h u b . i  o / O p e  n W e b T e  x t C o  r p u s\nGuo D, Rush AM, Kim Y (2020) Parameter-efficient transfer learning with diff pruning. arXiv preprint. \narXiv:2012.07463\nGuo D, Yang D, Zhang H et al (2025) Deepseek-r1: incentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint. arXiv:2501.12948\nGuo H, Greengard P, Xing EP et al (2023) LQ-LORA: low-rank plus quantized matrix decomposition for \nefficient language model finetuning. arXiv preprint. arXiv:2311.12023\nGupta P, Jiao C, Yeh YT et al (2022) Instructdial: Improving zero and few-shot generalization in dialogue \nthrough instruction tuning. arXiv preprint. arXiv:2205.12673\nHan L, Li Y , Zhang H et al (2023) SVDIFF: compact parameter space for diffusion fine-tuning. In: Proceed-\nings of the IEEE/CVF international conference on computer vision, pp 7323–7334\nHan Z, Gao C, Liu J et al (2024) Parameter-efficient fine-tuning for large models: A comprehensive survey. \narXiv preprint. arXiv:2403.14608\nHayou S, Ghosh N, Yu B (2024) Lora+: efficient low rank adaptation of large models. arXiv preprint. \narXiv:2402.12354\nHe J, Zhou C, Ma X et al (2021) Towards a unified view of parameter-efficient transfer learning. arXiv pre-\nprint. arXiv:2110.04366\nHe S, Ding L, Dong D et al (2022) Sparseadapter: an easy approach for improving the parameter-efficiency \nof adapters. arXiv preprint. arXiv:2210.04284\nHe H, Cai J, Zhang J et al (2023a) Sensitivity-aware visual parameter-efficient fine-tuning. In: Proceedings \nof the IEEE/CVF international conference on computer vision, pp 11825–11835\nHe S, Fan RZ, Ding L et al (2023b) MERA: merging pretrained adapters for few-shot learning. arXiv pre -\nprint. arXiv:2308.15982\nHendrycks D, Burns C, Basart S et al (2021a) Measuring massive multitask language understanding. In: \nICLR. OpenReview.net\n1 3\nPage 57 of 64 227\nL. Wang et al.\nHendrycks D, Burns C, Kadavath S et al (2021b) Measuring mathematical problem solving with the math \ndataset. In: Thirty-fifth conference on neural information processing systems datasets and benchmarks \ntrack (Round 2), pp 1–11\nHoffmann J, Borgeaud S, Mensch A et al (2022) Training compute-optimal large language models. arXiv \npreprint. arXiv:2203.15556\nHonovich O, Scialom T, Levy O et al (2022) Unnatural instructions: tuning language models with (almost) \nno human labor. arXiv preprint. arXiv:2212.09689\nHoulsby N, Giurgiu A, Jastrzebski S et al (2019) Parameter-efficient transfer learning for nlp. In: Interna -\ntional conference on machine learning, PMLR, pp 2790–2799\nHu EJ, Shen Y , Wallis P et al (2021) LORA: low-rank adaptation of large language models. arXiv preprint. \narXiv:2106.09685\nHu S, Zhang Z, Ding N et al (2022) Sparse structure search for delta tuning. Adv Neural Inf Process Syst \n35:9853–9865\nHu Z, Wang L, Lan Y et al (2023) LLM-ADAPTERS: an adapter family for parameter-efficient fine-tuning \nof large language models. arXiv preprint. arXiv:2304.01933\nHuang J, Chang KCC (2022) Towards reasoning in large language models: A survey. arXiv preprint. \narXiv:2212.10403\nHuang C, Liu Q, Lin BY et al (2023a) LORAHUB: efficient cross-task generalization via dynamic lora com-\nposition. arXiv preprint. arXiv:2307.13269\nHuang Q, Dong X, Chen D et al (2023b) Diversity-aware meta visual prompting. In: Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, pp 10878–10887\nHuang K, Mo F, Li H et al (2024a) A survey on large language models with multilingualism: Recent advances \nand new frontiers. arXiv preprint. arXiv:2405.10936\nHuang K, Qu Y , Cousins H et al (2024b) CRISPR-GPT: an llm agent for automated design of gene-editing \nexperiments. arXiv preprint. arXiv:2404.18021\nIyer S, Lin XV , Pasunuru R et al (2022) OPT-IML: scaling language model instruction meta learning through \nthe lens of generalization. arXiv preprint. arXiv:2212.12017\nJaech A, Kalai A, Lerer A et al (2024) OPENAI O1 system card. arXiv preprint. arXiv:2412.16720\nJi J, Liu M, Dai J et al (2023) BeaverTails: towards improved safety alignment of LLM via a human-prefer-\nence dataset. Adv Neural Inf Process Syst 36:24678–24704\nJia M, Tang L, Chen BC et al (2022) Visual prompt tuning. In: European conference on computer vision. \nSpringer, pp 709–727\nJiang Z, Araki J, Ding H et al (2021) How can we know when language models know? On the calibration of \nlanguage models for question answering. Trans Assoc Comput Ling 9:962–977\nJiang AQ, Sablayrolles A, Mensch A et al (2023) MISTRAL 7B. arXiv preprint. arXiv:2310.06825\nJie S, Deng ZH (2022) Convolutional bypasses are better vision transformer adapters. arXiv preprint. \narXiv:2207.07039\nJie S, Deng ZH (2023) Fact: Factor-tuning for lightweight adaptation on vision transformer. In: Proceedings \nof the AAAI conference on artificial intelligence, pp 1060–1068\nJie S, Wang H, Deng ZH (2023) Revisiting the parameter efficiency of adapters from the perspective of \nprecision redundancy. In: Proceedings of the IEEE/CVf international conference on computer vision, \npp 17217–17226\nKalla D, Smith N, Samaah F et al (2023) Study and analysis of ChatGPT and its impact on different fields of \nstudy. Int J Innov Sci Res Technol 8(3):827\nKaplan J, McCandlish S, Henighan T et al (2020) Scaling laws for neural language models. arXiv preprint. \narXiv:2001.08361\nKarimi Mahabadi R, Henderson J, Ruder S (2021) Compacter: efficient low-rank hypercomplex adapter lay-\ners. Adv Neural Inf Process Syst 34:1022–1035\nKeskar NS, McCann B, Xiong C et al (2019) Unifying question answering, text classification, and regression \nvia span extraction. arXiv preprint. arXiv:1904.09286\nKhashabi D, Min S, Khot T et al (2020) UNIFIEDQA: crossing format boundaries with a single qa system. \narXiv preprint. arXiv:2005.00700\nKim JK, Chua M, Rickard M et al (2023) Chatgpt and large language model (LLM) chatbots: The current \nstate of acceptability and a proposal for guidelines on utilization in academic medicine. J Pediatr Urol \n19(5):598–604\nKim J, Lee JH, Kim S et al (2024) Memory-efficient fine-tuning of compressed large language models via \nsub-4-bit integer quantization. In: Advances in neural information processing systems, vol 36\nKnox WB, Stone P (2008) TAMER: training an agent manually via evaluative reinforcement. In: 2008 7th \nIEEE international conference on development and learning. IEEE, pp 292–297\nKocetkov D, Li R, Allal L et al (2022) The stack: 3 tb of permissively licensed source code. arXiv preprint. \nhttps://arxiv.org/abs/2211.15533\n1 3\n227 Page 58 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nKojima T, Gu SS, Reid M et al (2022) Large language models are zero-shot reasoners. Adv Neural Inf Pro -\ncess Syst 35:22199–22213\nLawton N, Kumar A, Thattai G et al (2023) Neural architecture search for parameter-efficient fine-tuning of \nlarge pre-trained language models. arXiv preprint. arXiv:2305.16597\nLee H, Phatale S, Mansoor H et al (2023) RLAIF: scaling reinforcement learning from human feedback with \nai feedback. arXiv preprint. arXiv:2309.00267\nLee J, Stevens N, Han SC et al (2024) A survey of large language models in finance (FINLLMS). arXiv \npreprint. arXiv:2402.02315\nLei T, Bai J, Brahma S et al (2024) Conditional adapters: parameter-efficient transfer learning with fast infer-\nence. In: Advances in neural information processing systems, vol 36\nLester B, Al-Rfou R, Constant N (2021) The power of scale for parameter-efficient prompt tuning. arXiv \npreprint. arXiv:2104.08691\nLi S, Hoefler T (2021) Chimera: efficiently training large-scale neural networks with bidirectional pipelines. \nIn: Proceedings of the international conference for high performance computing, networking, storage \nand analysis, pp 1–14\nLi XL, Liang P (2021) Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint. \narXiv:2101.00190\nLi X, Tramer F, Liang P et al (2021) Large language models can be strong differentially private learners. \narXiv preprint. arXiv:2110.05679\nLi J, Li D, Savarese S et al (2023a) BLIP-2: bootstrapping language-image pre-training with frozen image \nencoders and large language models. arXiv preprint. arXiv:2301.12597\nLi Y , Yu Y , Liang C et al (2023b) LOFTQ: lora-fine-tuning-aware quantization for large language models. In: \nThe Twelfth international conference on learning representations\nLi H, Chen J, Yang J et al (2024a) LegalAgentBench: evaluating LLM agents in legal domain. arXiv preprint. \narXiv:2412.17259\nLi J, Tang T, Zhao WX et al (2024b) Pre-trained language models for text generation: a survey. ACM Comput \nSurv 56(9):1–39\nLialin V , Deshpande V , Rumshisky A (2023) Scaling down to scale up: a guide to parameter-efficient fine-\ntuning. arXiv preprint. arXiv:2303.15647\nLian D, Zhou D, Feng J et al (2022) Scaling & shifting your features: a new baseline for efficient model tun-\ning. Adv Neural Inf Process Syst 35:109–123\nLiao B, Meng Y , Monz C (2023) Parameter-efficient fine-tuning without introducing new latency. arXiv \npreprint. arXiv:2305.16742\nLin Z, Madotto A, Fung P (2020) Exploring versatile generative language model via parameter-efficient \ntransfer learning. arXiv preprint. arXiv:2004.03829\nLin S, Hilton J, Evans O (2021) TRUTHFULQA: measuring how models mimic human falsehoods. arXiv \npreprint. arXiv:2109.07958\nLiu X, He P, Chen W et al (2019) Multi-task deep neural networks for natural language understanding. arXiv \npreprint. arXiv:1901.11504\nLiu X, Ji K, Fu Y et al (2021a) P-tuning v2: prompt tuning can be comparable to fine-tuning universally \nacross scales and tasks. arXiv preprint. arXiv:2110.07602\nLiu X, Zheng Y , Du Z et al (2021b) GPT understands, too. arXiv preprint. arXiv:2103.10385\nLiu YC, Ma CY , Tian J et al (2022a) POLYHISTOR: parameter-efficient multi-task adaptation for dense \nvision tasks. Adv Neural Inf Process Syst 35:36889–36901\nLiu X, Sun T, Huang X et al (2022b) Late prompt tuning: a late prompt could be better than many prompts. \narXiv preprint. arXiv:2210.11292\nLiu H, Tam D, Muqeeth M et al (2022c) Few-shot parameter-efficient fine-tuning is better and cheaper than \nin-context learning. Adv Neural Inf Process Syst 35:1950–1965\nLiu H, Li C, Li Y et al (2023a) Improved baselines with visual instruction tuning. arXiv preprint. \narXiv:2310.03744\nLiu Q, Wu X, Zhao X et al (2023b) MOELORA: an MOE-based parameter efficient fine-tuning method for \nmulti-task medical applications. arXiv preprint. arXiv:2310.18339\nLiu Z, Feng R, Zhu K et al (2023c) Cones: Concept neurons in diffusion models for customized generation. \narXiv preprint. arXiv:2303.05125\nLiu A, Feng B, Wang B et al (2024a) DEEPSEEK-V2: a strong, economical, and efficient mixture-of-experts \nlanguage model. arXiv preprint. arXiv:2405.04434\nLiu A, Feng B, Xue B et al (2024b) DEEPSEEK-V3 technical report. arXiv preprint. arXiv:2412.19437\nLiu H, Li C, Wu Q et al (2024c) Visual instruction tuning. In: Advances in neural information processing \nsystems, vol 36\nLiu J, Xiao G, Li K et al (2024d) BITDELTA: your fine-tune may only be worth one bit. arXiv preprint. \narXiv:2402.10193\n1 3\nPage 59 of 64 227\nL. Wang et al.\nLiu SY , Wang CY , Yin H et al (2024e) DORA: weight-decomposed low-rank adaptation. arXiv preprint. \narXiv:2402.09353\nLiu X, Zheng Y , Du Z et al (2024f) GPT understands, too. AI Open 5:208–215\nLiu Z, Kundu S, Li A et al (2024g) AFLORA: adaptive freezing of low rank adaptation in parameter efficient \nfine-tuning of large models. arXiv preprint. arXiv:2403.13269\nLo K, Wang LL, Neumann M et al (2020) S2ORC: the semantic scholar open research corpus. In: ACL. \nAssociation for Computational Linguistics, pp 4969–4983\nLu X, Brahman F, West P et al (2023) Inference-time policy adapters (IPA): tailoring extreme-scale lms \nwithout fine-tuning. In: Proceedings of the 2023 conference on empirical methods in natural language \nprocessing, pp 6863–6883\nMa F, Zhang C, Ren L et al (2022) XPROMPT: exploring the extreme of prompt tuning. arXiv preprint. \narXiv:2210.04457\nMahabadi RK, Ruder S, Dehghani M et al (2021) Parameter-efficient multi-task fine-tuning for transformers \nvia shared hypernetworks. arXiv preprint. arXiv:2106.04489\nMao Y , Mathias L, Hou R et al (2021) UNIPELT: a unified framework for parameter-efficient language model \ntuning. arXiv preprint. arXiv:2110.07577\nMarjit S, Singh H, Mathur N et al (2024) DIFFUSEKRONA: a parameter efficient fine-tuning method for \npersonalized diffusion model. arXiv preprint. arXiv:2402.17412\nMcCann B, Keskar NS, Xiong C et al (2018) The natural language decathlon: multitask learning as question \nanswering. arxiv preprint. arXiv:1806.08730\nMeng X, Dai D, Luo W et al (2024) PERIODICLORA: breaking the low-rank bottleneck in lora optimiza -\ntion. arXiv preprint. arXiv:2402.16141\nMin S, Lewis M, Zettlemoyer L et al (2021) METAICL: learning to learn in context. arXiv preprint. \narXiv:2110.15943\nMishra S, Khashabi D, Baral C et al (2021) Cross-task generalization via natural language crowdsourcing \ninstructions. arXiv preprint. arXiv:2104.08773\nMou C, Wang X, Xie L et al (2023) T2I-ADAPTER: learning adapters to dig out more controllable ability for \ntext-to-image diffusion models. arXiv preprint. arXiv:2302.08453\nMuennighoff N, Wang T, Sutawika L et al (2022) Crosslingual generalization through multitask finetuning. \narXiv preprint. arXiv:2211.01786\nNakano R, Hilton J, Balaji S et al (2021) WEBGPT: browser-assisted question-answering with human feed-\nback. arXiv preprint. arXiv:2112.09332\nNan L, Radev DR, Zhang R et al (2021) DART: open-domain structured data record to text generation. In: \nNAACL-HLT. Association for Computational Linguistics, pp 432–447\nNarayan S, Cohen SB, Lapata M (2018) Don’t give me the details, just the summary! topic-aware con -\nvolutional neural networks for extreme summarization. In: EMNLP. Association for Computational \nLinguistics, pp 1797–1807\nOuyang L, Wu J, Jiang X et al (2022) Training language models to follow instructions with human feedback. \nAdv Neural Inf Process Syst 35:27730–27744\nPan Z, Luo H, Li M et al (2024) CONV-COA: improving open-domain question answering in large language \nmodels via conversational chain-of-action. arXiv preprint. arXiv:2405.17822\nPfeiffer J, Kamath A, Rücklé A et al (2020) ADAPTERFUSION: non-destructive task composition for trans-\nfer learning. arXiv preprint. arXiv:2005.00247\nQin Y , Wang X, Su Y et al (2021) Exploring universal intrinsic task subspace via prompt tuning. arXiv pre -\nprint. arXiv:2110.07867\nQiu Z, Liu W, Feng H et al (2024) Controlling text-to-image diffusion by orthogonal finetuning. In: Advances \nin neural information processing systems, vol 36\nRadford A, Narasimhan K, Salimans T et al (2018) Improving language understanding by generative pre-\ntraining. Technical Report, OpenAI\nRadford A, Wu J, Child R et al (2019) Language models are unsupervised multitask learners. OpenAI Blog \n1(8):9\nRadford A, Kim JW, Hallacy C et al (2021) Learning transferable visual models from natural language super-\nvision. In: International conference on machine learning, PMLR, pp 8748–8763\nRae JW, Potapenko A, Jayakumar SM et al (2020) Compressive transformers for long-range sequence model-\nling. In: ICLR. OpenReview.net\nRafailov R, Sharma A, Mitchell E et al (2024) Direct preference optimization: your language model is \nsecretly a reward model. In: Advances in neural information processing systems, vol 36\nRaffel C, Shazeer N, Roberts A et al (2020) Exploring the limits of transfer learning with a unified text-to-text \ntransformer. J Mach Learn Res 21(140):1–67\nRajabzadeh H, Valipour M, Zhu T et al (2024) QDYLORA: quantized dynamic low-rank adaptation for \nefficient large language model tuning. arXiv preprint. arXiv:2402.10462\n1 3\n227 Page 60 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nRein D, Hou BL, Stickland AC et al (2024) GPQA: a graduate-level google-proof q &a benchmark. In: First \nconference on language modeling\nRiquelme C, Puigcerver J, Mustafa B et al (2021) Scaling vision with sparse mixture of experts. Adv Neural \nInf Process Syst 34:8583–8595\nRücklé A, Geigle G, Glockner M et al (2020) ADAPTERDROP: on the efficiency of adapters in transformers. \narXiv preprint. arXiv:2010.11918\nRuiz N, Li Y , Jampani V et al (2023) DREAMBOOTH: fine tuning text-to-image diffusion models for sub -\nject-driven generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern \nrecognition, pp 22500–22510\nSanh V , Webson A, Raffel C et al (2021) Multitask prompted training enables zero-shot task generalization. \narXiv preprint. arXiv:2110.08207\nSaparov A, He H (2022) Language models are greedy reasoners: a systematic formal analysis of chain-of-\nthought. arXiv preprint. arXiv:2210.01240\nSchulman J, Wolski F, Dhariwal P et al (2017) Proximal policy optimization algorithms. arXiv preprint. \narXiv:1707.06347\nShao Z, Wang P, Zhu Q et al (2024) DEEPSEEKMATH: pushing the limits of mathematical reasoning in \nopen language models. arXiv preprint. arXiv:2402.03300\nShi Z, Lipani A (2023) DEPT: decomposed prompt tuning for parameter-efficient fine-tuning. arXiv preprint. \narXiv:2309.05173\nSinghal K, Azizi S, Tu T et al (2023) Large language models encode clinical knowledge. Nature \n620(7972):172–180\nSprague Z, Ye X, Bostrom K et al (2023) MUSR: testing the limits of chain-of-thought with multistep soft \nreasoning. arXiv preprint. arXiv:2310.16049\nSu Y , Wang X, Qin Y et al (2021) On transferability of prompt tuning for natural language processing. arXiv \npreprint. arXiv:2111.06719\nSun Q, Fang Y , Wu L et al (2023) EV A-CLIP: improved training techniques for clip at scale. arXiv preprint. \narXiv:2303.15389\nSung YL, Nair V , Raffel CA (2021) Training neural networks with fixed sparse masks. Adv Neural Inf Pro-\ncess Syst 34:24193–24205\nSung YL, Cho J, Bansal M (2022) LST: ladder side-tuning for parameter and memory efficient transfer learn-\ning. Adv Neural Inf Process Syst 35:12991–13005\nSutton RS (1995) Generalization in reinforcement learning: Successful examples using sparse coarse coding. \nIn: Advances in neural information processing systems, vol 8\nSuzgun M, Scales N, Schärli N et al (2023) Challenging big-bench tasks and whether chain-of-thought can \nsolve them. In: ACL (findings). Association for Computational Linguistics, pp 13003–13051\nTang A, Shen L, Luo Y et al (2023) Parameter efficient multi-task model fusion with partial linearization. \narXiv preprint. arXiv:2310.04742\nTay Y , Wei J, Chung HW et al (2022) Transcending scaling laws with 0.1% extra compute. arXiv preprint. \narXiv:2210.11399\nTian K, Mitchell E, Zhou A et al (2023) Just ask for calibration: Strategies for eliciting calibrated confidence \nscores from language models fine-tuned with human feedback. arXiv preprint. arXiv:2305.14975\nTian C, Shi Z, Guo Z et al (2024) HYDRALORA: an asymmetric lora architecture for efficient fine-tuning. \narXiv preprint. arXiv:2404.19245\nTouvron H, Lavril T, Izacard G et al (2023) Llama: Open and efficient foundation language models. arXiv \npreprint. arXiv:2302.13971\nTu CH, Mai Z, Chao WL (2023) Visual query tuning: towards effective usage of intermediate representations \nfor parameter and memory efficient transfer learning. In: Proceedings of the IEEE/CVF conference on \ncomputer vision and pattern recognition, pp 7725–7735\nValipour M, Rezagholizadeh M, Kobyzev I et al (2023) DYLORA: parameter-efficient tuning of pre-trained \nmodels using dynamic search-free low-rank adaptation. In: Proceedings of the 17th conference of the \nEuropean chapter of the Association for Computational Linguistics, pp 3274–3287\nVaswani A, Shazeer N, Parmar N et al (2017) Attention is all you need. In: Advances in neural information \nprocessing systems, vol 30\nVavekanand R, Sam K (2024) LLAMA 3.1: an in-depth analysis of the next-generation large language model. \nhttps://doi.org/10.13140/RG.2.2.10628.74882\nV oynov A, Aberman K, Cohen-Or D (2023a) Sketch-guided text-to-image diffusion models. In: ACM SIG-\nGRAPH 2023 conference proceedings, pp 1–11\nV oynov A, Chu Q, Cohen-Or D et al (2023b) p+[CDATA[ p+ ]]: extended textual conditioning in text-to-\nimage generation. arXiv preprint. arXiv:2303.09522\nVu T, Lester B, Constant N et al (2021) SPoT: better frozen model adaptation through soft prompt transfer. \narXiv preprint. arXiv:2110.07904\n1 3\nPage 61 of 64 227\nL. Wang et al.\nVu T, Lester B, Constant N et al (2022) SPoT: better frozen model adaptation through soft prompt transfer. \nIn: Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: \nlong papers), pp 5039–5059\nVucetic D, Tayaranian M, Ziaeefard M et al (2022) Efficient fine-tuning of bert models on the edge. In: 2022 \nIEEE international symposium on circuits and systems (ISCAS). IEEE, pp 1838–1842\nWang J (2023) The power of ai-assisted diagnosis. EAI Endorsed Transactions on e-Learning 8(4)\nWang A, Pruksachatkun Y , Nangia N et al (2019a) SuperGLUE: a stickier benchmark for general-purpose \nlanguage understanding systems. In: NeurIPS, pp 3261–3275\nWang A, Singh A, Michael J et al (2019b) GLUE: a multi-task benchmark and analysis platform for natural \nlanguage understanding. In: ICLR (Poster). OpenReview.net\nWang P, Yang A, Men R et al (2022a) OFA: unifying architectures, tasks, and modalities through a simple \nsequence-to-sequence learning framework. In: International conference on machine learning, PMLR, \npp 23318–23340\nWang Y , Kordi Y , Mishra S et al (2022b) Self-instruct: aligning language models with self-generated instruc-\ntions. arXiv preprint. arXiv:2212.10560\nWang L, Lyu C, Ji T et al (2023a) Document-level machine translation with large language models. arXiv \npreprint. arXiv:2304.02210\nWang Q, Mao Y , Wang J et al (2023b) APROMPT: attention prompt tuning for efficient adaptation of pre-\ntrained language models. In: Proceedings of the 2023 conference on empirical methods in natural lan -\nguage processing, pp 9147–9160\nWang W, Lv Q, Yu W et al (2023c) COGVLM: visual expert for pretrained language models. arXiv preprint. \narXiv:2311.03079\nWang X, Hu Z, Lu P et al (2023d) SCIBENCH: evaluating college-level scientific problem-solving abilities \nof large language models. arXiv preprint. arXiv:2307.10635\nWang Y , Mishra S, Alipoormolabashi P et al (2022c) Benchmarking generalization via in-context instructions \non 1,600+ language tasks. arXiv preprint. arXiv:2204.07705 2\nWang Y , Mukherjee S, Liu X et al (2022d) Adamix: mixture-of-adapter for parameter-efficient tuning of large \nlanguage models. arXiv preprint. arXiv:2205.12410 1(2):4\nWang Z, Panda R, Karlinsky L et al (2022e) Multitask prompt tuning enables parameter-efficient transfer \nlearning. In: The Eleventh international conference on learning representations\nWang G, Cheng S, Zhan X et al (2024a) OpenChat: advancing open-source language models with mixed-\nquality data. In: ICLR. OpenReview.net\nWang H, Chang J, Zhai Y et al (2024b) LION: implicit vision prompt tuning. In: Proceedings of the AAAI \nconference on artificial intelligence, pp 5372–5380\nWei J, Bosma M, Zhao VY et al (2021) Finetuned language models are zero-shot learners. arXiv preprint. \narXiv:2109.01652\nWei J, Tay Y , Bommasani R et al (2022) Emergent abilities of large language models. arXiv preprint. \narXiv:2206.07682\nWei X, Li G, Marculescu R (2024) ONLINE-LORA: task-free online continual learning via low rank adapta-\ntion. arXiv preprint. arXiv:2411.05663\nWu Z, Wang S, Gu J et al (2022) IDPG: an instance-dependent prompt generation method. arXiv preprint. \narXiv:2204.04497\nWu S, Fei H, Qu L et al (2023) NEXT-GPT: any-to-any multimodal llm. arXiv preprint. arXiv:2309.05519\nWu J, Li X, Wei C et al (2024a) Unleashing the power of visual prompting at the pixel level. In: TMLR\nWu J, Yu T, Wang R et al (2024b) Infoprompt: Information-theoretic soft prompt tuning for natural language \nunderstanding. In: Advances in neural information processing systems, vol 36\nWu Y , Xiang Y , Huo S et al (2024c) LORA-SP: streamlined partial parameter adaptation for resource-effi -\ncient fine-tuning of large language models. arXiv preprint. arXiv:2403.08822\nXie E, Yao L, Shi H et al (2023) Difffit: Unlocking transferability of large diffusion models via simple \nparameter-efficient fine-tuning. In: Proceedings of the IEEE/CVF International Conference on Com -\nputer Vision, pp 4230–4239\nXin Y , Luo S, Zhou H et al (2024) Parameter-efficient fine-tuning for pre-trained vision models: a survey. \narXiv preprint. arXiv:2402.02242\nXing F (2024) Designing heterogeneous llm agents for financial sentiment analysis. ACM Trans Manag Inf \nSyst 16(1):1–24\nXu S, Wen X (2024) Automatic design of adapter architectures for enhanced parameter-efficient fine-tun -\ning. In: ICASSP 2024–2024 IEEE international conference on acoustics, speech and signal processing \n(ICASSP), pp 12536–12540\nXu R, Luo F, Zhang Z et al (2021) Raise a child in large language model: towards effective and generalizable \nfine-tuning. arXiv preprint. arXiv:2109.05687\n1 3\n227 Page 62 of 64\nParameter-efficient fine-tuning in large language models: a survey of…\nXu L, Xie H, Qin SZJ et al (2023a) Parameter-efficient fine-tuning methods for pretrained language models: \na critical review and assessment. arXiv preprint. arXiv:2312.12148\nXu M, Zhang Z, Wei F et al (2023b) Side adapter network for open-vocabulary semantic segmentation. In: \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 2945–2954\nXu Y , Xie L, Gu X et al (2023c) QA-LORA: quantization-aware low-rank adaptation of large language mod-\nels. In: The Twelfth international conference on learning representations\nYang Z, Qi P, Zhang S et al (2018) HOTPOTQA: a dataset for diverse, explainable multi-hop question \nanswering. In: EMNLP. Association for Computational Linguistics, pp 2369–2380\nYang AX, Robeyns M, Wang X et al (2023a) Bayesian low-rank adaptation for large language models. In: \nThe Twelfth international conference on learning representations\nYang X, Huang JY , Zhou W et al (2023b) Parameter-efficient tuning with special token adaptation. In: Pro -\nceedings of the 17th conference of the European chapter of the association for computational linguistics, \npp 865–872\nYang B, Tian H, Ren J et al (2024) Multi-objective fine-tuning for enhanced program repair with LLMS. \narXiv preprint. arXiv:2404.12636\nYao Y , Duan J, Xu K et al (2024) A survey on large language model (LLM) security and privacy: the good, \nthe bad, and the ugly. High-Confidence Computing p 100211\nYe S, Kim D, Jang J et al (2022) Guess the instruction! making language models stronger zero-shot learners. \narXiv preprint. arXiv:2210.02969\nYe H, Zhang J, Liu S et al (2023) IP-ADAPTER: text compatible image prompt adapter for text-to-image \ndiffusion models. arXiv preprint. arXiv:2308.06721\nYeh SY , Hsieh YG, Gao Z et al (2023) Navigating text-to-image customization: from lycoris fine-tuning to \nmodel evaluation. arXiv preprint. arXiv:2309.14859\nYin D, Yang Y , Wang Z et al (2023) 1% vs 100%: parameter-efficient low rank adapter for dense predic -\ntions. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp \n20116–20126\nYuan S, Zhao H, Du Z et al (2021) Wudaocorpora: a super large-scale Chinese corpora for pre-training lan -\nguage models. AI Open 2:65–68\nZadouri T, Üstün A, Ahmadian A et al (2023) Pushing mixture of experts to the limit: extremely parameter \nefficient moe for instruction tuning. arXiv preprint. arXiv:2309.05444\nZaken EB, Ravfogel S, Goldberg Y (2021) Bitfit: simple parameter-efficient fine-tuning for transformer-\nbased masked language-models. arXiv preprint. arXiv:2106.10199\nZavras A, Michail D, Demir B et al (2024) Mind the modality gap: towards a remote sensing vision-language \nmodel via cross-modal alignment. arXiv preprint. arXiv:2402.09816\nZellers R, Holtzman A, Bisk Y et al (2019) Hellaswag: can a machine really finish your sentence? In: Pro -\nceedings of the 57th annual meeting of the Association for Computational Linguistics\nZeng G, Zhang P, Lu W (2023) One network, many masks: towards more parameter-efficient transfer learn-\ning. arXiv preprint. arXiv:2305.17682\nZhai X, Kolesnikov A, Houlsby N et al (2022) Scaling vision transformers. In: Proceedings of the IEEE/CVF \nconference on computer vision and pattern recognition, pp 12104–12113\nZhang H, Xu J, Wang J (2019) Pretraining-based natural language generation for text summarization. arXiv \npreprint. arXiv:1902.09243\nZhang Y , Zhou K, Liu Z (2022) Neural prompt search. arXiv preprint. arXiv:2206.04673\nZhang B, Yang H, Zhou T et al (2023a) Enhancing financial sentiment analysis via retrieval augmented \nlarge language models. In: Proceedings of the fourth ACM international conference on AI in finance, \npp 349–356\nZhang F, Li L, Chen J et al (2023b) Increlora: Incremental parameter allocation method for parameter-effi -\ncient fine-tuning. arXiv preprint. arXiv:2308.12043\nZhang L, Rao A, Agrawala M (2023c) Adding conditional control to text-to-image diffusion models. In: Pro-\nceedings of the IEEE/CVF international conference on computer vision, pp 3836–3847\nZhang L, Zhang L, Shi S et al (2023d) LORA-FA: memory-efficient low-rank adaptation for large language \nmodels fine-tuning. arXiv preprint. arXiv:2308.03303\nZhang M, Shen C, Yang Z et al (2023e) Pruning meets low-rank parameter-efficient fine-tuning. arXiv pre -\nprint. arXiv:2305.18403\nZhang Q, Chen M, Bukharin A et al (2023f) ADALORA: adaptive budget allocation for parameter-efficient \nfine-tuning. arXiv preprint. arXiv:2303.10512\nZhang R, Han J, Liu C et al (2023g) LLAMA-ADAPTER: efficient fine-tuning of language models with zero-\ninit attention. arXiv preprint. arXiv:2303.16199\nZhang X, Li C, Zong Y et al (2023h) Evaluating the performance of large language models on gaokao bench-\nmark. arXiv preprint. arXiv:2305.12474\n1 3\nPage 63 of 64 227\nL. Wang et al.\nZhang ZR, Tan C, Xu H et al (2023i) Towards adaptive prefix tuning for parameter-efficient language model \nfine-tuning. arXiv preprint. arXiv:2305.15212\nZhao M, Lin T, Mi F et al (2020) Masking as an efficient alternative to finetuning for pretrained language \nmodels. arXiv preprint. arXiv:2004.12406\nZhao H, Tan H, Mei H (2022) Tiny-attention adapter: contexts are more important than the number of param-\neters. arXiv preprint. arXiv:2211.01979\nZhao H, Fu J, He Z (2023) Prototype-based hyperadapter for sample-efficient multi-task tuning. arXiv pre -\nprint. arXiv:2310.11670\nZhao S, Chen D, Chen YC et al (2024) UNI-CONTROLNET: all-in-one control to text-to-image diffusion \nmodels. In: Advances in neural information processing systems, vol 36\nZheng L, Chiang WL, Sheng Y et al (2023) Judging llm-as-a-judge with mt-bench and chatbot arena. Adv \nNeural Inf Process Syst 36:46595–46623\nZhong V , Xiong C, Socher R (2017) SEQ2SQL: generating structured queries from natural language using \nreinforcement learning. arXiv preprint. arXiv:1709.00103\nZhong W, Cui R, Guo Y et al (2024) AGIEV AL: a human-centric benchmark for evaluating foundation mod-\nels. In: NAACL-HLT (Findings). Association for Computational Linguistics, pp 2299–2314\nZhou H, Wan X, Vulić I et al (2024) AUTOPEFT: automatic configuration search for parameter-efficient fine-\ntuning. Trans Assoc Comput Ling 12:525–542\nZhu W, Tan M (2023) SPT: learning to selectively insert prompts for better prompt tuning. In: The 2023 \nconference on empirical methods in natural language processing\nZhu Y , Kiros R, Zemel RS et al (2015) Aligning books and movies: towards story-like visual explanations by \nwatching movies and reading books. In: ICCV . IEEE Computer Society, pp 19–27\nZhu Y , Feng J, Zhao C et al (2021) Counter-interference adapter for multilingual machine translation. arXiv \npreprint. arXiv:2104.08154\nZhu W, Liu H, Dong Q et al (2023) Multilingual machine translation with large language models: empirical \nresults and analysis. arXiv preprint. arXiv:2304.04675\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nAuthors and Affiliations\nLuping Wang1 · Sheng Chen1 · Linnan Jiang1 · Shu Pan1 · Runze Cai1 · Sen Yang1 · \nFei Yang1\n \r Fei Yang\nyangf@zhejianglab.org\nLuping Wang\nwangluping@zhejianglab.org\nSheng Chen\nscucs@zhejianglab.org\nLinnan Jiang\njianglinnan@zhejianglab.org\nShu Pan\nshu.pan@zhejianglab.org\nRunze Cai\ncairz@zhejianglab.org\nSen Yang\nyangsen@zhejianglab.org\n1 Zhejiang Laboratory, Kechuang Avenue, Zhongtai Sub-district, Yuhang District,  \nHangzhou 311121, Zhejiang Province, China\n1 3\n227 Page 64 of 64",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7897654175758362
    }
  ],
  "institutions": [],
  "cited_by": 20
}