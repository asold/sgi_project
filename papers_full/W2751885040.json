{
  "title": "Cynical Selection of Language Model Training Data",
  "url": "https://openalex.org/W2751885040",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288529097",
      "name": "Axelrod, Amittai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2291208696",
    "https://openalex.org/W2277863734",
    "https://openalex.org/W1984985727",
    "https://openalex.org/W148400104",
    "https://openalex.org/W2106596127"
  ],
  "abstract": "The Moore-Lewis method of \"intelligent selection of language model training data\" is very effective, cheap, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the in-domain and the out-of-domain (or data pool) corpora against each other. This powerful idea-- which we set out to preserve-- treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems. We present a greedy, lazy, approximate, and generally efficient information-theoretic method of accomplishing the same goal using only vocabulary counts. The method has the following properties: (1) Is responsive to the extent to which two corpora differ. (2) Quickly reaches near-optimal vocabulary coverage. (3) Takes into account what has already been selected. (4) Does not involve defining any kind of domain, nor any kind of classifier. (6) Knows approximately when to stop. This method can be used as an inherently-meaningful measure of similarity, as it measures the bits of information to be gained by adding one text to another.",
  "full_text": "arXiv:1709.02279v1  [cs.CL]  7 Sep 2017\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\namittai axelrod\namittai@\nb0rked.org :metal: conform and obey. in theory, you are copying my email address. in pra ctice, you are not.\nalum.mit.edu\nAbstract. The Moore-Lewis method of “intelligent selection of langua ge model training\ndata” is very eﬀective, cheap, eﬃcient... and also has struc tural problems.\n(1) The method deﬁnes relevance by playing language models t rained on the in-domain\nand the out-of-domain (or data pool) corpora against each ot her. This powerful\nidea – which we set out to preserve – treats the two corpora as t he opposing ends\nof a single spectrum. This lack of nuance does not allow for th e two corpora to be\nvery similar. In the extreme case where the come from the same distribution, all of\nthe sentences have a Moore-Lewis score of zero, so there is no resulting ranking.\n(2) The selected sentences are not guaranteed to be able to mo del the in-domain data,\nnor to even cover the in-domain data. They are simply well-li ked by the in-domain\nmodel; this is necessary, but not suﬃcient.\n(3) There is no way to tell what is the optimal number of senten ces to select, short of\npicking various thresholds and building the systems.\nWe present “cynical selection of training data”: a greedy, l azy, approximate, and generally\neﬃcient method of accomplishing the same goal. It has the fol lowing properties:\n(1) Is responsive to the extent to which two corpora diﬀer.\n(2) Quickly reaches near-optimal vocabulary coverage.\n(3) Takes into account what has already been selected.\n(4) Does not involve deﬁning any kind of domain, nor any kind o f classiﬁer.\n(5) Has real units.\n(6) Knows approximately when to stop.\n1. Scenario\nWe have a number of translation tasks, each deﬁned as “a ﬂavor of data we want to be able\nto translate well”. Perhaps it’s a speciﬁc client’s data, or a kind of data such as “customer\nsupport chat logs”, or just a system for a language arc (“Mexi can Spanish to American\nEnglish’). Machine Translation (MT) tasks often have one of these two problems:\n(1) Web-scale data, or too much data than can be used readily t o train or run an\nMT system. We want to know what data we can exclude from training without\nsacriﬁcing performance.\n(2) Not enough parallel data to train an MT system. We want to k now what data we\ncan add to improve performance, and how much improvement we might ex pect.\nWork unaﬃliated with amazon.com.\n1\n2 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nWe can add data from known parallel data, or by paying to have m onolingual data\ntranslated.\nWe present a single method for handling both:\n(1) Given a too-large parallel corpus, we identify the best s ubset to use for training\na system that is at least as good as the full system. This is the traditional data\nselection scenario, and we improve upon the standard Moore- Lewis cross-entropy\ndiﬀerence method (Moore and Lewis, 2010).\n(2) Given a representative monolingual corpus for a transla tion task, and an optional\nparallel corpus in the language pair, identify the monoling ual sentences that should\nbe manually translated and added to the parallel corpus in or der to improve trans-\nlation for the speciﬁc task.\nTranslating monolingual data is expensive, and training on bilingual data is expensive. By\n“expense” we mean eﬀort, computation, time, and dollars. As a rule, we want to spend as\nlittle as possible to get as much out of the available data as w e can.\n2. Context\n2.1. Language Model Size\nSeymore and Rosenfeld (1996) decided that pruning a model is better than incrementally\ngrowing it:\nUsing more training data, up to at least 25 - 30 million words i nitially,\nand then pruning it down is a better approach than just starti ng with a\nsmall amount of training data [...] Beyond 25 million words, the amount of\ntraining data does not have a noticeable eﬀect.\nStolcke (1998) showed how to produce an eﬃciently-sized lan guage model (LM) by building\nan LM and then pruning entries in the model that do not help mod el the training set. This\nis determined by whether removing the entries changes the en tropy of the training set, or\nnot. They “assume that the n-grams aﬀect the relative entropy roughly independently,\nand compute [the change in entropy] due to each individual n-gram [...] then rank the\nn-grams by their eﬀect on the model entropy” and focus on those t hat aﬀect the model\nentropy the least. The relative entropy score of the sentenc e actually can be decomposed\ninto the sum of the word scores (see derivation by Sethy et al. (2006b)), so there is no need\nto assume.\nSiivola and Pellom (2005) approach from the other direction : incrementally growing an\nn-gram LM, adding entries that decrease the number of bits re quired to model the training\ndata. They compute gains on the data coding length, along the lines of the Minimum\nDescription Length principle which minimizes the size of th e model plus the training data\nas encoded by the model (Rissanen, 1983). Sethy et al. (2006b ) also take the growing\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 3\napproach, using an information-gain -based score. However , due to computational cost,\nthey did not update the estimate after every new selection.\n2.2. Corpus Similarity\nKilgarriﬀ (2001) posited:\nCorpus similarity is complex, and there is no absolute answe r to “is Corpus\n1 more like Corpus 2 than Corpus 3?”. All there are are possibl e measures\nwhich serve particular purposes more or less well.\nPerhaps similarity cannot be deﬁned absolutely, but the uni ts of measurement can. Bits\nof information are ideal for this.\n2.3. Data Selection\nData selection – the deliberate reduction in size of the trai ning corpus – is a roundabout\nway of achieving domain adaptation. Each of the training dat a sentences is scored by its\nrelevance to the data to be modeled, and only the best-scorin g sentences are kept. The stan-\ndard approach for data selection is the same for both languag e modeling (Moore and Lewis,\n2010) and machine translation (Axelrod et al., 2011). Calle d “cross-entropy diﬀerence”, it\npreferentially selects data that is like the translation task and unlike the rest of the data\npool.1\n2.4. Cynically Quantifying Relevance\nOne major advantage to the above-mentioned model-growing m ethods is that there is\nno need to train a huge language model in order to ﬁgure out how to build a smaller\none. An advantage of information-theoretic similarity mea surements is that the focus\nis on quantifying the relationship between sentences and co rpora, so the output can be\nused for a variety of scenarios and not just language modelin g (or, by extension, machine\ntranslation).\nHere we propose a method to incrementally construct an eﬃcie ntly-sized downstream model\nby incrementally constructing an eﬃciently-sized trainin g corpus. We score data by how\nuseful\nit would be to add it to a corpus that is used to model a particul ar translation task.\nIn particular, we score sentences by how much we’d learn if we added it to our existing\ndata, right now. Sethy et al. (2006b) describe the general id ea as “an incremental greedy\nselection scheme based on relative entropy, which selects a sentence if adding it to the\nalready selected set of sentences reduces the relative entr opy with respect to the in-domain\ndata distribution”.\n1 More complete explanations can be found in previous work: Ax elrod et al. (2011) and Axelrod (2014).\n4 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\n3. Inputs\nThe algorithm requires four inputs to be speciﬁed, twice as m any as the Moore-Lewis cross-\nentropy diﬀerence method. which only had two: the in-domain a nd the general-pool data\nﬁles. We separate the idea of the vocabulary distributions o f the kinds of language that we\nhave (or want) from the actual corpora that we have (or want). These additional inputs\nallow the cynical selection method to be used in a wider range of applications.\n3.1. REPRESENTATIVE corpus distribution\nA corpus that is representative of what we want to translate. For example: all the data we\nhave for a task, or a sample of a data stream from the domain, or whatever. We will evaluate\nthe selected data by its ability to model the Representative corpus (speciﬁcally: by\ncomputing the perplexity of the Representative corpus using a language model trained\non the selected data). We only use the Representative ﬁle to compute some vocabulary\nstatistics (described later). This means a client with conﬁ dential data they can’t share\ncould just share the derived statistics and not the text itse lf.\n3.2. UNADAPTED corpus distribution\nA corpus that reﬂects our currently-available data. This wa s called the “data pool” in\nprior work, but we’ve changed the name to make some distincti ons clearer. We only need\nthis corpus to compute some corpus statistics, and then comp are the statistics of this\ncorpus with that of the Representative corpus. Perhaps the Unadapted is generic\ndata, perhaps it is just all the data that we have in the langua ge of interest. By default\nthe Unadapted corpus distribution would come from the set of sentences to s elect from\n(i.e. the data pool is the unadapted corpus is the set of sentences t o be scored).\nThe Representative corpus gives us the vocabulary distribution of the data we want,\nand Unadapted gives us the vocabulary distribution of the data we have. We need these\ntwo distributions in order to quantify how these two sets of l anguage diﬀer.\n3.3. SEED text ﬁle\nContains any headway that has been made towards assembling a corpus that meets the\ngoal of being able to train a system to translate Representat ive-type data. It might be that\nthe client has already sent oﬀ 1000 hand-picked sentences to be translated, or we have 100k\nsentences we are certain come from Spanish-speaking custom ers in Mexico or whatever the\nunifying theme of the Representative corpus is. The Seed is assumed to be empty by\ndefault.\n3.4. A V AILABLE text ﬁle\nThe set of candidate sentences we can choose from. We will score and rank these sentences\naccording to their usefulness in modeling the Representativedistribution. In most cases,\nthe A vailablecorpus is the same one that is used to compute the Unadapted corpus\ndistribution.\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 5\nHowever, if the goal is to pick monolingual sentences to send oﬀ for manual translation,\nthen the A vailablecorpus is probably the Representative corpus! In this case, the\nRepresentative corpus would be monolingual client data: things we want to kn ow how\nto translate, but have no translations for. The Unadapted is probably our existing\nparallel data, and we want to know how much parallel Representative data we need to\npurchase and add to the Unadapted in order to build a system to translate the rest of\nthe Representative data.\n3.5. JADED text ﬁle\nThis is the output ﬁle. It is a re-ranked subset of the A vailablecorpus with a few special\n(cynical) properties. Sentences in the ﬁle are ordered (and scored) by the amount of useful\ninformation they contain about the Representativecorpus. Speciﬁcally, we measure how\nmuch we can improve (decrease) the perplexity of the Representative corpus evaluated\nwith a language model trained on the Seed+Jaded data. Other practical properties: if\nwe read the ﬁle line by line, everything already read is more u seful than everything we\nhave not yet seen. Furthermore, the next line in the ﬁle is alw ays the most informative\nsentence to add to what we have already seen. Every sentence i n the ﬁle is ranked strictly\naccording to its utility.\n4. Notation\n• The lowercase w variable is for word tokens.\nAny capital W is some set of tokens in a corpus.\n• The lowercase v variable is for word types (lexical / vocabulary items).\nAny capital V is a set of vocabulary items in a corpus.\n• The lowercase c variable is for the count of a single word type.\nAny capital C variable is the count of some word type in a corpus.\n• wn is the set of tokens in the nth selected line.\nvn is the set of word types in the nth selected line.\ncn(v) is the count of word type v in the nth line.\n• Wn and Vn are the total number of word tokens and word types, respectiv ely, in\nthe ﬁrst n selected lines. Cn(v) is the count of word v in the ﬁrst n selected lines.\n• Wrepr and Vrepr are the total number of word tokens and word types, respectiv ely,\nin the representative corpus. Guess what Wunadapt and Vunadapt are?\n5. Math\n5.1. Cross-Entropy: What It Is\nLet Q be the probability distribution from a language model LM. The general deﬁnition\n6 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nof the cross-entropy H of the Repr corpus using Q is:\nHLM (repr) = −\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Q(v)(1)\nH represents how well the language model’s training corpus ca n be used to model the\nRepresentative corpus. Perplexity is normally used to evaluate language models. The\ngeneral deﬁnition is:\npplLM (repr) = 2 H(repr)\nEverything that follows could be rewritten in terms of perpl exity, but we won’t.\nThe stupidest kind of language model we can build is an unsmoo thed unigram LM (the\nmaximum likelihood estimate LM). That kind of LM makes Q be the empirical probability\nof each word in the corpus:\nH(repr) = −\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog C(v)\n|W |(2)\nH is always positive, and smaller (lower entropy) is better.\nWe will be selecting sentences to incrementally grow a corpu s. After selecting the nth line,\nthe cross-entropy between the n-sentence corpus and the Representative one is:\nHn(repr) = −\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nWn\n(3)\nCall that Hn for short. We will be computing H1, H2, . . . , Hn−1, Hn as we select lines.\n5.2. Cross-Entropy: What It Is Not\nEntropy, cross-entropy, and relative entropy are easily co nfused in the literature. They\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 7\nare three diﬀerent things. Compounding the situation, there are also alternate names: ‘En-\ntropy’ speciﬁcally means ‘Shannon entropy’, and ‘Relative Entropy’ means the Kullback-\nLeibler (KL) divergence. The relationship between the kind s of (Shannon) entropy regard-\ning two probability distributions P and Q is:\nEntropy = Cross Entropy − Relative Entropy\n= H(P, Q) − DKL (P ||Q)\n= −\n∑\nx\np(x) log q(x) −\n∑\nx\np(x) log p(x)\nq(x)\n= −\n∑\nx\np(x) log q(x) −\n∑\nx\np(x)(log p(x) − log q(x))\n= −\n∑\nx\np(x) log q(x) −\n∑\nx\np(x) log p(x) +\n∑\nx\np(x) log q(x)\nH(P ) = −\n∑\nx\np(x) log p(x)\nAn intuitive explanation 2: The Shannon entropy of P is the minimum number of bits\nrequired to perfectly encode all events drawn from distribu tion P . Cross entropy is the\nnumber of bits required to encode all events drawn from P , but using an imperfect code\nbased on distribution Q. Relative entropy is the number of bits that are wasted by usi ng\nQ instead of the true distribution P . Then: Number of bits used - wasted bits = optimal\nnumber of bits.\n5.3. Deﬁning Cross-Entropy Greedily\nSuppose we have selected n sentences, and have computed the current subset’s score Hn.\nWe want to know what is the next sentence we should select to ac hieve our goal of ﬁnding\nthe subset of sentences that minimize H. After the n + 1th step we wish to have the best\n(lowest) possible Hn+1 score. We cannot un-select a sentence, so Hn+1 can be deﬁned as:\nHn+1 = Hn + ∆ H\nn→n+1\n(4)\nIt is suﬃcient to ﬁnd the n+1th sentence that minimizes 3 the negative quantity ∆ H\nn→n+1\n.\n2 Expanded from anonymous Stanford NLP notes on KL Divergence :\nhttps://nlp.stanford.edu/fsnlp/mathfound/fsnlp-slides-kl.pdf\n3 Sign-switching is hard: H ideally decreases over time, making Hn+1 < Hn. Then [ Hn+1 − Hn] is\nnegative, and ideally as negative as possible. It could be le ss complicated to deﬁne Hn+1 = Hn − | ∆ H\nn→n+1\n|\nand then maximize the positive quantity |∆ H|.\n8 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nWe will shortly derive the following:\n∆ H\nn→n+1\n= Penalty\nn→n+1\n+ Gain\nn→n+1\n(5)\nso of course:\nHn+1 = Hn + Penalty\nn→n+1\n+ Gain\nn→n+1\n(6)\nThe penalty term is positive, and the gain term is a negative n umber. Adding any new\nline to the corpus incurs an entropy-increasing penalty. If the candidate line is a good\none to add to the corpus, then it adds useful information to th e model and will lower the\ncross entropy. A suﬃciently good candidate sentence will pr ovide a gain that outweighs\nthe penalty. We would like to minimize the (positive) penalt y term, and also minimize the\ngain term (as negative as possible).\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 9\n5.4. Deriving the Greedy Cross-Entropy Delta\nExpanding the deﬁnition of ∆ H (Equation 4) using Equation 3:\n∆ H\nn→n+1\n= Hn+1 − Hn\n=\n(\n−\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn+1(v)\nWn+1\n)\n−\n(\n−\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nWn\n)\n=\n( ∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nWn\n)\n−\n( ∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn+1(v)\nWn+1\n)\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\n(\nlog Cn(v)\nWn\n− log Cn+1(v)\nWn+1\n)\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\n\nlog\nCn(v)\nWn\nCn+1(v)\nWn+1\n\n\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog\n( Cn(v)\nWn\n· Wn+1\nCn+1(v)\n)\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog\n( Wn+1\nWn\n· Cn(v)\nCn+1(v)\n)\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\n(\nlog Wn+1\nWn\n+ log Cn(v)\nCn+1(v)\n)\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Wn+1\nWn\n+\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn+1(v)\n= log Wn+1\nWn\n( ∑\nv∈Vrepr\nCrepr(v)\nWrepr\n)\n+\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn+1(v)\n= log Wn+1\nWn\n+\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn+1(v)\n∆ H\nn→n+1\n= log Wn + wn+1\nWn  \nP enalty\n+\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v)\n  \nGain\nWe have derived Equation 5, the greedy cross -entropy delta. Sethy et al. (2006a) and\n(2006b) derived instead the greedy relative -entropy delta. Interestingly, both kinds of\nentropy have the same deltas because the extra p(x) terms cancel out. Our Penalty term\nis the same as their T 1 term, and our Gain term is their T 2 term. 4\n4 Actually, our Gain = − (T 2), because we gather the negative sign inside the term and th ey do not.\n10 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nThe penalty term biases the delta score towards selecting sh ort sentences. Adding any new\nline to the corpus incurs an entropy-increasing penalty pro portional to the number of tokens\nin the new line, regardless of what the words are. The penalty term is always positive,\nand decreases asymptotically to zero: adding one more sente nce makes less and less of a\ndiﬀerence as the selected corpus grows larger. A speciﬁc sent ence’s penalty decreases over\ntime, too, for the same reason.\nThe gain term biases the delta score towards longer sentence s that contain many words\nwhich are high in probability in the Repr distribution but do not appear many times in\nthe sentences selected so far. There is an entropy-lowering beneﬁt to adding information,\nso the gain of adding any sentence is always a negative number 5 that gets subtracted\nfrom Hn. Adding more data can be net good (the magnitude of the gain te rm is larger\nthan the penalty, so H decreases) or net bad (the magnitude of the gain is less than t he\npenalty, increasing H). A suﬃciently good candidate sentence will provide a gain t hat\noutweighs the penalty: it is adding useful information to th e model, and will lower the\ncross-entropy.\nThe length biases of the penalty and the gain terms counterac t each other, guarding the\nalgorithm from the Moore-Lewis method’s ﬁxation on one-wor d sentences with a very\ncommon token, or long sentences full of OOVs.\nWe can plug Equation 5 into Equation 6 to get an iterative meth od for computing cross-\nentropy:\nHn+1 = Hn + Penalty\nn→n+1\n+ Gain\nn→n+1\n= Hn + log Wn + wn+1\nWn  \nP enalty\n+\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v)\n  \nGain\nAgain: the log terms force the penalty to be positive and the g ain to be negative.\n5.5. Algorithmic Complexity of Greedy Cross-Entropy Delta Method\nSuppose we have already selected the n most useful sentences from A vailable, removing\nthem from their original corpus and placing them into Jaded. The theoretically optimal\nalgorithm picks sentence n + 1 as follows:\n(1) Compute the ∆ H\nn→n+1\nscore for each sentence remaining in A vailable. This requires\ngoing word by word through the sentences in A vailableto compute the Penalty\nand Gain terms based on the current count of each word in the se ntence.\n5 Negative or zero, so... non-positive?\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 11\n(2) Sort the sentences in A vailableby ∆ H\nn→n+1\nscore, and choose the sentence with the\nbest (lowest) score to be sentence n + 1. Remove it from A vailableand add it to\nJaded.\nThe steps are repeated for each subsequent sentence to be cho sen.\nThe cost appears brutally prohibitive: O(N) iterations, each requiring an update of O(Wavail)\nwords and then a sort of O(N log N) lines. The number of words in the corpus is roughly\nlinear with respect to the number of lines ( N = 15-ish W ), so the cost is O(N3 log N).\nWe will not quite do that.\n6. Hulk Smash Math\n6.1. Good-Enough Sentences\nThe Gain term for a particular sentence after selecting n other ones is always:\nGain\nn→n+1\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v)\nThe total lexicon Vrepr can be divided into two parts: vocabulary items vn+1 that do\nappear in the ( n + 1)st sentence, and those that do not ( v /∈ vn+1 = Vrepr \\ vn+1) :\nGain\nn→n+1\n=\n∑\nv∈Vrepr\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v)\n=\n∑\nv∈vn+1\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v) +\n∑\nv /∈vn+1\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + 0\n=\n∑\nv∈vn+1\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v) +\n∑\nv /∈vn+1\nCrepr(v)\nWrepr\n·0\n=\n∑\nv∈vn+1\nCrepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v)\n=\n∑\nv∈vn+1\nGain\nn→n+1\n(v)\nThe words that do not appear in the sentence do not aﬀect the ∆ H score. The gain for\nthe sentence is the sum of the gains for each word (type) in the sentence. The empirical\nprobability Crepr(v)\nWrepr\nhas the usual long-tail distribution, so we hypothesize tha t the sentence\ngain will often be dominated by only one or a few of the word (ty pe) gain terms. Those\ndominant words will be ones that the Jaded corpus needs to see more times in order to\naccurately model the repr distribution.\nIn that case, it would always be useful to select a sentence th at contains the word with the\nbest word gain. The best-gain sentence that contains the bes t-gain word may not be the\n12 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\noverall best-gain sentence, but it will still be pretty good . It does contain the single most\nuseful word, so even if would ideally not choose to pick this s entence right now, we would\nprobably want to pick it before too long.\nThe exact gain for a sentence from containing a speciﬁc word t ype depends on cn+1(v), the\nnumber of times the word type appears in the sentence. We hypo thesize that most words\nwill appear once per sentence. The obvious exception are the closed-class words, and they\nare few. As such, we can estimate the word gain as:\nGain\nn→n+1\n(v) = Crepr(v)\nWrepr\nlog Cn(v)\nCn(v) + cn+1(v) ≈ Crepr(v)\nWrepr\nlog Cn(v)\nCn(v) + 1\nThis estimate is an upper bound on the true value of the word gain, because the gain term\nis negative and log A\nA+1 > log A\nA+2 . This means we are selecting the best word based on\na lower bound estimate of the magnitude of its goodness, whic h is reasonable. The true\nword gain for any word is at least as good (more negative) as it s estimate, so the best word\nis at least as good as we think it is. We now select sentence n + 1 as follows:\n(1) Estimate the Gain\nn→n+1\n(v) ≈ Crepr(v)\nWrepr log Cn(v)\nCn(v)+1 for all word types v ∈ V .\n(2) Sort the word types in V by their estimated gain, and note the best word v′\n(3) Let A vail(v′) be the set of lines still in A vailablewhich contain v′.\nCompute the ∆ H\nn→n+1\nscore for each sentence in A vail(v′).\nThis requires going word by word through each of those senten ces to compute the\nPenalty and Gain terms based on the current count of each word in the sentence.\n(4) Sort A vail(v′) by the ∆ H\nn→n+1\nscore, and choose the sentence with the best (lowest)\nscore to be sentence n + 1. Remove it from A vailableand add it to Jaded.\nThis algorithm takes O(N) iterations, each requiring:\nan update of V word gains and a sort of O(V log V ) words to get the best word v′,\nand then an update of O(WA vail(v′)) words and a sort of O(|A vail(v′)| ·log |A vail(v′))|\nlines to get the best line containing v′.\nThe number of words in those lines WA vail(v′) is again linear w.r.t. the number of lines, or\nO(|A vail(v′)|). The size of |A vail(v′)| can be estimated by the average number of lines\nthat a speciﬁc word appears in. Empirically, it’s a little le ss than\n3√\nN = N\n1\n3 , as well as\nsmaller than both log 2 N and log 2 V . We’ll use the N\n1\n3 term for clarity. This reduces\nthe complexity to O(N\n(\nV 2 log V + N\n1\n3 + N\n1\n3 log(N\n1\n3 )\n)\n), which is mostly O(NV 2 log V )\nbecause large-set vocabularies have over a million word typ es.\n6.2. Good-Enough Words\nWe have reduced the time complexity of the optimal algorithm to be heavily dependent on\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 13\nthe size of the vocabulary rather than the number of lines in t he training data. We now fo-\ncus on reducing the size of the vocabulary itself, with the fo llowing intuition (Axelrod et al.,\n2015):\n“Where the frequencies of words diﬀer, the corpora diﬀer. Wher e the fre-\nquencies do not diﬀer, neither do the corpora. [...] We use the ratio of the\nword’s probabilities in the corpora to determine how much th e two spe-\nciﬁc corpora diﬀer with respect to a word. [...]. This can also be readily\ncomputed using unigram LMs trained on each of the corpora.”\nConsider the unigram frequency ratio Prepr(v)\nPunadapt(v) of an arbitrary word v. There are four\ncases of interest to us:\n(1) Crepr(v) < 3 and Cunadapt(v) < 3:\nBarely-seen words do not have reliable empirical statistic s. A word appearing twice\nin one corpus and once in another is not necessarily twice as l ikely, nor is a singleton\nthat does not appear in the other corpus inﬁnitely more likel y. The unigram ratio\nis therefore also not reliable.\nThe threshold of 3 is not important; it was picked because LM s moothing is often\napplied to terms appearing < 3 times. Choose a threshold x based on the question:\n“If I saw a word x − 1 times, would I trust its probability?”. It does not need to\nbe the same threshold for both corpora, either.\n(2) Prepr(v)\nPunadapt(v) << 1\nWhen the ratio is considerably less than 6 1, the word is strongly indicative of the\nUnadapted distribution. These words (by deﬁnition) appear relativel y rarely in\nthe Repr corpus, so cannot comprise a signiﬁcant chunk of the cross-e ntropy H.\n(3) Prepr(v)\nPunadapt(v) ≈ 1\nThese words appear roughly as often (within an order of magni tude) in one distri-\nbution as in the other. A sample of the A vailcorpus can be expected to contain\nthese words in a reasonable proportion.\n(4) Prepr(v)\nPunadapt(v) >> 1\nWhen the ratio is considerably greater than 7 1, the word is strongly indicative\nof the Representative distribution. These words are the most important ones:\nThey comprise much of the probability mass of the Repr distribution (according to\nhow much Repr and Unadapt diverge), and they are relatively rare in the A vail\ncorpus.\nWe assert that the words in the fourth category are most impor tant, and that it suﬃces\nto use only them for selecting sentences. We collapse each of the ﬁrst three categories\n6 An order of magnitude less or so: < 1\ne , < 1\n2 , or < 1\n10 depending on base.\n7 An order of magnitude more or so: > e, > 2, or > 10 depending on base.\n14 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nto a single token, again following Axelrod et al. (2015) and A xelrod (2014). Category 1\nwords become ‘dubious’, category 2 are ‘bad’, and category 3 are ‘meh’. The words\nin category 4 remain unchanged. We further diﬀerentiate word s in Repr but not A vail\nas ‘impossible’, and words in A vail+Unadapt but not in Repr as ‘useless’. This\nreplacement strategy consolidates probability mass for ne gative and neutral events, letting\nthe distribution focus on words whose presence is meaningfu l and relevant.\nEmpirically, the reduces the number of words in the vocabula ry to around 10,000 word\ntypes: words V+REP R with heavy bias towards the repr distribution. The complexity\ncalculation in Section 6.1 becomes\n(1) Estimate the Gain\nn→n+1\n(v) ≈ Crepr(v)\nWrepr log Cn(v)\nCn(v)+1 for the ≈ 10,000 word types in V+REP R.\n(2) Sort the word types in V+REP R by their estimated gain, and note the best word v′\n(3) Let A vail(v′) be the set of lines still in A vailablethat contain v′.\nCompute the ∆ H\nn→n+1\nscore for each sentence in A vail(v′).\n(4) Sort A vail(v′) by ∆ H\nn→n+1\nscore, and choose the sentence with the best (lowest) score\nto be sentence n + 1. Remove it from A vailableand add it to Jaded.\nThis algorithm takes O(N) iterations, each requiring:\nan update and then sort of V+REP R word gains, but O(10,000) is now constant: O(C).\nAn update of O(WA vail(v′)) words and a sort of O(|A vail(v′)| ·log |A vail(v′))|lines to get\nthe best line containing v′.\nThe number of words in those lines WA vail(v′) is again < N\n1\n3 . The complexity is\nO(N\n(\nC + N\n1\n3 + N\n1\n3 log N\n1\n3\n)\n) ≈ O(NN\n1\n3 log N\n1\n3 ) = O(N\n4\n3 ·1\n3 log N) = O(N\n4\n3 log N)\nThis value compares very favorably with the original O(N3 log N).\n6.3. Good-Enough Scoring\nThe bulk of the processing time in the algorithmic block is in the rescoring and re-sorting\nof the sentences that contain the best word for that particul ar timestep. We assert we do\nnot need to score all the sentences every time.\nThe sentence ∆ H score estimate term is the sum of the penalty and gain terms. W hile\nboth the penalty and the gain decrease over time, their sum is not monotonic. The sentence\ngain estimate\n( ∑\nv∈Vrepr\nCrepr(v)\nWrepr\n)\nfor a particular sentence always increases over time (the\nterm becomes less negative, so the sentence has less of an eﬀec t on the entropy score). A\nsentence’s current gain estimate is thus a lower bound on the sentence’s gain estimate at\nsome future iteration.\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 15\nThe sentence penalty\n(\nlog Wn+wn+1\nWn\n)\nfor a particular sentence always decreases over time.\nHowever, if we compare the magnitude of the derivatives, the penalty term d\ndn P enalty\nappears to be smaller than d\ndn Gain as n → ∞ .8 The penalty term changes mostly as a\nfunction of time: each selected sentence increases the size of the selected (cynical) corpus\nby roughly the average number of words per sentence. By contr ast, the sentence gain\nestimate changes as a function of which particular sentence s have been chosen. There is no\nequivalent notion of “average vocabulary content of a sente nce”, so the gain terms continue\nto vary signiﬁcantly according to the sentence.\nThe converging penalty terms mean that we can get away with so rting and storing sentences\nby their estimated gain rather than their estimated ∆ H score, and compute the penalty\non the ﬂy. There may be a few cases where ordering by ∆ H score would not match the\nordering by estimated gain, but on the whole it is good enough . A sentence with a pleasing\ngain estimate is probably useful, even if it is long enough (o r the current n + 1th iteration\nis early enough) to have a large penalty term.\n(1) Estimate the Gain\nn→n+1\n(v) ≈ Crepr(v)\nWrepr\nlog Cn(v)\nCn(v)+1 for the ≈ 10,000 word types in V+REP R.\n(2) Sort the word types in V+REP R by their estimated gain, and note the best word v′\n(3) Let A vail(v′) be the set of lines still in A vailablethat contain v′.\nThe previous iteration left this list sorted by estimated sentence gain. We can up-\ndate the sentence gain score for only the ﬁrst sentence on the list, and then note\nwhere in the list it would be resorted to. Let the formerly-be st sentence’s new index\nin the list be i. Thanks to the monotonicity of the sentence gain estimate, o nly\nsentences with index < ineed to have their scores updated.\nCompute the ∆ H\nn→n+1\nscore for each such sentence in A vail(v′).\n(4) Sort A vail(v′) by ∆ H\nn→n+1\nscore, and choose the sentence with the best (lowest) score\nto be sentence n + 1. Remove it from A vailableand add it to Jaded.\nIt does not seem like the complexity cost changes appreciabl y.\n6.4. Good-Enough Code\nThe process of removing the chosen sentence from A vailableis not eﬃcient. Recall that\nwe are only looking at lines that contain our best word v′. For each of the 10,000 or so\nwords in V+REP R, we maintain a list of all sentences that contain it. These li sts enable\n8 The following argument might hold, but we are not sure:\nUsing standard identities, d\ndx\n(\nlog x+k\nx\n)\n= −k\nx(x+k) and d\ndy\n(\nlog y\ny+i\n)\n= i\ny(y+i) , so d\ndn P enalty≈ −k\nWn(Wn +k)\nand d\ndn Gain ≈ ∑\nv\ni\nCn(v)(Cn(v)+i) In general, k (avg number of words per sentence) > i (avg number of\noccurrences of a word within a sentence), and Wn = ∑\nv Cn(v), so the magnitude of the derivative of the\npenalty should be less than the magnitude of the derivative o f the gain, at least in the limit.\n16 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nthe signiﬁcant complexity reduction in Section 6.1. Every s entence thus appears as many\ntimes in the full mapping as there are words types in the sente nce.\nWhen we remove the best sentence from A vailableat each iteration, we are simply using\nshift to remove the ﬁrst element from the list of sentences for the s ingle best word. The\nother mentions of that sentence, in the sentence-lists for t he other words in the sentence,\nare not\nremoved. Each sentence-list is kept sorted by the estimated gain of the sentences\nin it. Therefore there is not a cheap way to ﬁnd and remove a sen tence without checking\nevery sentence in the list.\nInstead, we maintain a small hash table in which the sentence IDs are the keys, and the\nvalue is “true”. When we remove the best sentence from A vailable, we delete its key from\nthe hash. The entries for that sentence in the other lists are unchanged, but they are now\nghosts of sentences already-selected. We eliminate these g hosts only when we happen to\nsee them, namely when we (try to) update the sentence’s score or select the same sentence\nagain.\nWhen updating a sentence’s score, it is simple enough to chec k whether the sentence ID\nis still in the hash table. This operation is O(1), and we do it as many times as there\nare sentences to check. If we checked every sentence, we woul d not save any time. In\nSection 6.3 we mentioned using the change in index after sort ing as a way to reduce the\nnumber of sentences to check– here is why this reduction matt ers. The worst case scenario\nis that the sentences always get resorted to the end of the lis t, so the complexity cost does\nnot change even though in practice it is faster.\n7. Batch Mode = Fast Mode\nFinding the single next best sentence to add is the most eﬃcie nt strategy for the case where\nwe really want to pick the absolute best sentences for manual processing (transcription,\ntranslation, etc), because the external cost for using each sentence is relatively high.\nHowever, it is common to just want a\nrelevant subcorpus for automatically training a\nlanguage model, an MT system, or some other downstream syste m. Here the cost for using\neach sentence is relatively low, and it only matters whether a particular sentence is selected\n(or not). It does not matter much whether the sentence is chos en on the thirtieth or three\nmillionth step, and certainly not if it was number 235,442 ve rsus number 235,443. As such,\nthere is little point to worrying about getting the best sent ence at the optimal iteration.\nAny iteration will do, much like any port in a storm.\nWe enable Batch Mode to hurry the process along a bit. Let A(v′) = |A vail(v′)| as the\nnumber of available sentences still containing the best wor d v′. At every iteration, we now\nalways update the scores of the top\n√\nA(v′) lines (not counting ghosts that we prune). We\nthen always select (and remove) the top 1\n2\n√\nA(v′) lines.\nThis reduces the complexity even further. We previously est imated A(v′) ≈ N\n1\n3 , so:\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 17\nO( 1\n1\n2\n√\nA(v′)\nN\n4\n3 log N) ≈ O(2 ·N\n4\n3\nN\n1\n3\nlog N) ≈ O(N log N)\nThis is about as good as one can hope for.\nAs string-identical sentences have identical estimated ga ins and penalties, Batch Mode will\noften select multiple copies of the same sentence in a single batch (the one-at-a-time version\nwill never do this). Picking a sentence twice (or twenty time s) is not a bad thing, as long\nas the sentence contains at least one word that needs to be see n many more times in order\nto better match the Representative distribution. As the sentences are selected based on\nthe presence of such a word, the condition is satisﬁed by cons truction.\nHowever, it greatly bothers human users to see The Algorithm make such an obvious\n‘mistake’, and it leads to bikeshedding. We are tired of expl aining how statistical methods\nwork to people who think that a second copy of a sentence inval idates the entire selection\nmethod. Therefore, we unique all sentences within a batch, a nd un-select any copies. The\nun-selected identical sentences go back onto the word’s lis t of un-selected sentences ( e.g.\nA(v′)). Thus it is still both possible and probable for 500 occurr ences of something as\nbanal as ‘‘Thank you .’’ to be selected. Mercifully, these copies will be sprinkled a cross\n500 batches in the ﬁnal selected corpus, so fewer humans will notice.\n8. Wrapping Up\nAll experimental results will be published separately, for entirely amiable legal reasons.\nThe code for cynical data selection is going through proper c orporate/legal channels for\nrelease. It will be on github with a permissive license as soo n as that is completed (ex-\npected September 2017). In a pinch, this work contains enoug h detail to implement the\nalgorithm.\nThe method presented in the preceding sections has the follo wing properties:\n• Is responsive to the extent to which two corpora diﬀer.\nIt retains the core concept from cross-entropy diﬀerence of p laying the two distri-\nbutions against each other, but only for the parts that actua lly diﬀer between the\ntwo distributions.\n• Quickly reaches near-optimal vocabulary coverage.\nThe cynical selection method selects sentences by ﬁrst look ing for word whose\nprobability estimate can be improves the most. It happens th at the greatest possible\nimprovement is to go from zero – unseen – to a bad estimate (see n once). Because\nof this, it winds up selecting almost every word in the Representative corpus–\nexactly once! – very early in the process. This minimizes the number of unknown\nwords in the training corpus, meaning the cynical subset pre serves the breadth of\nuseful\nvocabulary found in the much larger A vailablecorpus. This is also an\n18 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA\nimprovement over Moore-Lewis: empirically, the cynical su bsets have about 80%\nfewer OOV (out of vocabulary) tokens than the Moore-Lewis su bsets.\n• Takes into account what has already been selected.\nThis is the key beneﬁt of the submodular data selection metho ds of Bilmes and Kirchhoﬀ\n(2014) and Wei et al. (2014). Our cynical method seems to sati sfy the deﬁnition\nof submodularity 9 However, we are able to avoid much of their heavy lifting by\nalways using the same feature function (entropy gain) and no t needing to specify\na budget. We can automatically pick a good cutoﬀ point by sele ct sentences until\nthe estimated gain becomes (and stays) positive.\n• Does not involve deﬁning any kind of domain.\nMore importantly, it also does not deﬁne what it means to not\nbelong to the domain.\nA sentence could be moved from one corpus to another, or delet ed, or cloned into\nboth, without changing the deﬁnition of the Representative and Unadapted\ndistributions. Those distributions represent the two corp ora, regardless of what\nthey may contain. Compare with the Moore-Lewis method, wher e the Pool data\nhad to diﬀer signiﬁcantly from the Task corpus, and subsequen t classiﬁer-based\nextensions where “not-in-domain” data had to be deﬁned. Sim ilarly, our method\ndoes not involve any kind of classiﬁer. We can quantify relev ance without needing\nto label anything.\n• Has real units.\nA sentence’s relevance score is the change it has, in bits of e ntropy, on how well\na model can represent the Representative data. As they are real units, they\ncan be compared across systems, methods, etc. Cross-entrop y diﬀerence scores are\nalso in bits, but they are relative: “by how many more bits doe s the in-domain\nLM like the sentence than the Pool LM does?”. Changing either of the models\nrenders the scores meaningless. Other methods use abstract measures like cosine\nsimilarity in higher-dimensional spaces, or other normali zed scores that lack units\nentirely. Three may be the number of the count 10, but it makes little sense to say\n“this sentence is 3 close to the in-domain model”.\n• Knows approximately when to stop.\nData selection methods often require gridsearch to determi ne the best amount of\ndata to select. First try 1%, then 5%, then 10% and so on, build systems on each\nof them, and take the one that works the best. The optimal valu e does not carry\nover between systems nor language arcs. The cynical method k nows it has selected\nenough data for the most good enough (if not best) subset: whe n ∆ H\nn→n+1\nbecomes\n(and stays) positive. No tuning nor searching is needed.\n9 from Wei et al. (2014): “...an equivalent deﬁnition of submo dularity is f(j|S) ≥ f(j|T ),∀S?T . That\nis, the incre- mental gain of adding item j to the set decreases when the set in which j is considered grows\nfrom S to T .” In our case, the gain of adding a sentence decreases as the a lready-selected set grows.\n10 “...and the number of the counting shall be three.”\nCYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA 19\nReferences\nAxelrod, A. (2014). Data Selection for Statistical Machine Translation. PhD thesis, Uni-\nversity of Washington.\nAxelrod, A., He, X., and Gao, J. (2011). Domain Adaptation Vi a Pseudo In-Domain Data\nSelection. EMNLP (Empirical Methods in Natural Language Processing), pages 355–362.\nAxelrod, A., Vyas, Y., Martindale, M., and Carpuat, M. (2015 ). Class-Based N-gram\nLanguage Diﬀerence Models for Data Selection. IWSLT (International Workshop on\nSpoken Language Translation).\nBilmes, J. and Kirchhoﬀ, K. (2014). Submodularity for Data Se lection in Statistical Ma-\nchine Translation. EMNLP (Empirical Methods in Natural Language Processing).\nKilgarriﬀ, A. (2001). Comparing Corpora. International Journal of Corpus Linguistics.\nMoore, R. C. and Lewis, W. D. (2010). Intelligent Selection o f Language Model Training\nData. ACL (Association for Computational Linguistics).\nRissanen, J. (1983). A Universal Prior for Integers and Esti mation by Minimum Description\nLength. The Annals of Statistics, 11(2):416–431.\nSethy, A., Georgiou, P. G., and Narayanan, S. (2006a). Selec ting Relevant Text Subsets\nfrom Web-Data for Building Topic Speciﬁc Language Models. NAACL (North American\nAssociation for Computational Linguistics).\nSethy, A., Georgiou, P. G., and Narayanan, S. (2006b). Text D ata Acquisition for Domain-\nSpeciﬁc Language Models. EMNLP (Empirical Methods in Natural Language Process-\ning), (July):382–389.\nSeymore, K. and Rosenfeld, R. (1996). Scalable Backoﬀ Langu age Models. ICLSP (Inter-\nnational Conference on Spoken Language Processing).\nSiivola, V. and Pellom, B. L. (2005). Growing an N-gram Langu age Model. INTER-\nSPEECH.\nStolcke, A. (1998). Entropy-Based Pruning of Backoﬀ Langua ge Models. DARPA Broadcast\nNews Transcription and Understanding Workshop.\nWei, K., Liu, Y., Kirchhoﬀ, K., Bartels, C., and Bilmes, J. (20 14). Submodular Subset\nSelection for Large-Scale Speech Training Data. ICASSP (International Conference on\nAcoustics, Speech, and Signal Processing).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.818585991859436
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6894481778144836
    },
    {
      "name": "Language model",
      "score": 0.6224582195281982
    },
    {
      "name": "Vocabulary",
      "score": 0.5913612246513367
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5839095115661621
    },
    {
      "name": "Relevance (law)",
      "score": 0.5445762276649475
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5215939283370972
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5062664747238159
    },
    {
      "name": "Training set",
      "score": 0.5039731860160828
    },
    {
      "name": "Natural language processing",
      "score": 0.5001847743988037
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4974348843097687
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.47156211733818054
    },
    {
      "name": "Cover (algebra)",
      "score": 0.4244646430015564
    },
    {
      "name": "Machine learning",
      "score": 0.3593929409980774
    },
    {
      "name": "Linguistics",
      "score": 0.16160404682159424
    },
    {
      "name": "Mathematics",
      "score": 0.11617174744606018
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}