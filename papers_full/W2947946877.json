{
    "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model",
    "url": "https://openalex.org/W2947946877",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4302747491",
            "name": "Bhandare, Aishwarya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302660633",
            "name": "Sripathi, Vamsi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224553011",
            "name": "Karkada, Deepthi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744233420",
            "name": "Menon, Vivek",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2513767736",
            "name": "choi sun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302747496",
            "name": "Datta, Kushal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4294641914",
            "name": "Saletore, Vikram",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963206030",
        "https://openalex.org/W2613681825",
        "https://openalex.org/W2233116163",
        "https://openalex.org/W2962761403",
        "https://openalex.org/W2793950911",
        "https://openalex.org/W2610592929",
        "https://openalex.org/W2964164125",
        "https://openalex.org/W1965555277",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2756540778",
        "https://openalex.org/W2260663238",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2787027558",
        "https://openalex.org/W2775304348"
    ],
    "abstract": "In this work, we quantize a trained Transformer machine language translation model leveraging INT8/VNNI instructions in the latest Intel$^\\circledR$ Xeon$^\\circledR$ Cascade Lake processors to improve inference performance while maintaining less than 0.5$\\%$ drop in accuracy. To the best of our knowledge, this is the first attempt in the industry to quantize the Transformer model. This has high impact as it clearly demonstrates the various complexities of quantizing the language translation model. We present novel quantization techniques directly in TensorFlow to opportunistically replace 32-bit floating point (FP32) computations with 8-bit integers (INT8) and transform the FP32 computational graph. We also present a bin-packing parallel batching technique to maximize CPU utilization. Overall, our optimizations with INT8/VNNI deliver 1.5X improvement over the best FP32 performance. Furthermore, it reveals the opportunities and challenges to boost performance of quantized deep learning inference and establishes best practices to run inference with high efficiency on Intel CPUs.",
    "full_text": "Efﬁcient 8-Bit Quantization of Transformer Neural Machine Language\nTranslation Model\nAishwarya Bhandare 1 Vamsi Sripathi1 Deepthi Karkada 1 Vivek Menon2 Sun Choi 1 Kushal Datta 1\nVikram Saletore1\nAbstract\nIn this work, we quantize a trained Transformer\nmachine language translation model to lower\nprecision 8-bit integers. We leverage the\nhigh performance Intel R⃝ Math Kernel Library\nmartix multiplication kernels optimized with\nINT8/VNNI instructions in the latest Intel R⃝\nXeon R⃝ Cascade Lake processors to improve\ninference efﬁciency while maintaining less than\n0.5 drop in BLEU score accuracy. To the\nbest of our knowledge, this is the ﬁrst attempt\nin the industry to quantize the Transformer\nmodel. We present novel quantization techniques\ndirectly in TensorFlow to opportunistically\nreplace 32-bit ﬂoating point (FP32) computations\nwith 8-bit integers (INT8) and transform the FP32\ncomputational graph. We also present a parallel\nbatching technique to maximize CPU utilization\nduring inference. Our optimizations improved\nperformance of both FP32 and INT8-quantized\nmodel resulting in a net improvement of\n1.5X of the best quantized model over the\nbest FP32 performance. Furthermore, we\nreveal opportunities and challenges of quantizing\nemerging deep learning model inference on Intel\nCPUs and establish best practices to do so.\n1. Introduction\nThe Transformer model using self-attention mechanism\nhas recently achieved the state of the art accuracy in\nlanguage translation (Vaswani et al., 2017). Compared\nto its predecessors, this sequence transduction model\n1Artiﬁcial intelligence products group, Intel Corporation\n2Work done while at Intel corporation. Correspondence\nto: Kushal Datta <kushal.datta@intel.com>, Vikram\nSaletore <vikram.saletore@intel.com>, Deepthi Karkada\n<deepthi.karkada@intel.com>.\nTo be presented at the Joint Workshop on On-Device Machine\nLearning & Compact Deep Neural Network Representations,36 th\nInternational Conference on Machine Learning, Long Beach,\nCalifornia, 2019. Copyright 2019 by the author(s).\ncircumvents recurrent or long-short memory (LSTM) neural\ncells and exploits multi-headed attention mechanism to\ncapture global dependencies between input and output word\nsequences. Since its ﬁrst use in machine translation, the\nmulti-headed attention mechanism has shown tremendous\npromise in speech recognition (Chiu et al., 2017), generative\nlanguage modeling (Guu et al., 2017), machine reading\ncomprehension (Hu et al., 2017), Language representation\nmodels (Devlin et al., 2018) and other natural language\nprocessing workloads.\nThe growing presence of machine language translation\nservices and tools (Microsoft, 2018), (Google, 2018), (AWS,\n2018) and (LingoTek, 2019) to name a few, clearly shows\nthat machine translation inference is an important workload.\nQuantization is a technique to improve the performance of\ninference workloads by using lower precision data types\n(8-bit, 4-bit or 2-bit integers) in place of 32-bit ﬂoating\npoint. The latest Intel R⃝ Xeon R⃝ Cascade Lake processors\ninclude specialized vectorized neural network instructions\n(VNNI) to expedite quantized inference by fusing 64 8-bit\nmultiply and add (FMA) operations into a single instruction\n(Fomenko, 2018). This means that the vectorized FMAs can\nbe completed in fewer clock cycles than previous generation\nIntel R⃝ Xeon R⃝ processors. As a result, 8-bit matrix\nmultiplications (MatMuls) or quantized MatMuls execute\nfaster on these platforms. This motivated us to explore\nthe impact of VNNI on the performance of Transformer\nmodel inference. To the best of our knowledge, the\nTransformer model has not been quantized before. However,\nthe impact of quantized MatMuls on the overall performance\nof Transformer inference was not known before this work\nas speedup between INT8 and FP32 MatMuls depend on\nthe shape and size of the matrices involved.\nAdditionally, we want to minimize the drop in translation\naccuracy which can result due to the usage of reduced\nprecision data types. In this work, our contributions include\nthe following:\n1. Quantized a trained FP32 Transformer model to INT8\nto achieve <0.5 drop in state-of-the-art (SOTA) BLEU\nscore.\narXiv:1906.00532v2  [cs.LG]  7 Jun 2019\nINT8 Quantization of TransformerLT Model\n2. Improve inference performance by:\n(a) Optimizing quantized MatMuls for tensor shapes\nand sizes in the Transformer model\n(b) Reducing overhead due to quantization operations\nin the Transformer model compute graph\n(c) Optimizing input pipeline by ordering sentences\nby token length\n(d) Implementing parallel execution of batches with\nincreased inference throughput\nThe rest of the paper is organized as follows. In section 2,\nwe describe prior work on quantization techniques for deep\nlearning models. In section 3, we provide a brief description\nof the Transformer translation model. In section 4, we\ndescribe how we ﬁrst quantize the graph to maintain\naccuracy and then in section 5 detail strategies to improve\ninference efﬁciency. In section 6, we present the overall\nimprovement in inference performance.\n2. Related work\nVarious techniques including quantization i.e. using lower\nprecision data types with smaller bit-widths have been\nproposed to compress deep neural network models. Wu\net al used quantization to compressed and accelarate neural\nnetworks, but their work was restricted to only CNNs\n(Jiaxiang Wu & Cheng, 2016). Other related work on\nquantization proposed by (Courbariaux & Bengio, 2016),\n(Gysel et al., 2018), (Park et al., 2018) and (Migacz,\n2017) all have similar restrictions. (Hou & Kwok, 2018)\nproposed a ternerization scheme for quantization of weights\nto compress the neural network model, but didn’t deal with\nquantization of activations. The authors in (Mellempudi\net al., 2017) quantized both weights and activations using a\nsub 8-bit inference pipeline where weights were constrained\nto +1, 0, -1 and activations were quantized using either 8 or\n4-bits. Although, they reduced memory consumption, their\ntechnique failed to improve inference performance.\nBinary, ternary and quaternany quantization techniques for\nrecurrent methods including LSTM, GRU and ConvLSTM\nwere proposed by the authors in (Alom et al., 2018) .\nAlthough they observed promising performance results,\nthey did not explore the applicability of their technique\nto the Transformer model with self-attention layers. To\nthe best of our knowledge, this is the ﬁrst work where\nquantization has been applied to the Transformer language\ntranslation network. Transformer network does not\ncontain convolutions or recurrent layers, only the attention\nmechanism making it unique. This work is also the ﬁrst to\ndemonstrate the speedup observed in inference performance\nby leveraging VNNI.\n3. Model Description\nThe architecture of the Transformer translation model with\nmulti-headed attention is shown in ﬁgure 1 of (Vaswani et al.,\n2017). It has an encoder-decoder structure. The encoder\nmaps the input sequence of tokens in source language to a\nsequence of latent representations. The decoder generates\nthe translated sequence of tokens in target language from\nthese representations. The decoder is auto-regressive which\nmeans that previously generated tokens are used to decode\nthe next token using a while loop.\nThe model uses the scaled dot-product attention and\nmulti-headed attention mechanisms. The equations\nEquation 1 and Equation 2 from (Vaswani et al., 2017)\ndescribe the fundamental computation in the model. It can\nbe inferred from the equations that the primary operation\nin this model is a Matrix Multiplication (MatMul). It is\nalso clear that the model contains the Softmax operation in\nbetween MatMuls. The Softmax operation in Equation 3\ninvolves mapping the input to a probability distribution, and\nhas a division operation. This would mean that computing a\nSoftmax in a lower precision datatype would result in high\naccuracy loss as compared to computing it in full-precision\nFP32 datatype. In addition to Softmax, the graph has Layer\nNormalization(Lei Ba et al., 2016) layer in between any\ntwo layers(Vaswani et al., 2017). The Layer Normalization\nlayer involves calculating the mean and variance of each\nlayer, and normalizing the values of that layer. This involves\noperations like division, square and square root, which again\nare more accurate with a full-precision FP32 datatype over\nINT8 datatype. Thus, the entire computation graph of this\nmodel doesn’t support low precision INT8 datatype. Parts\nof the graph need to be selectively transformed to work in\nlow-precision, while keeping the remainder of the graph in\nFP32.\nAttention(Q,K,V ) =Softmax(QKT\n√dK\n)V (1)\nMultiHead(Q,K,V ) =concatenate(head1,\nhead2,...,head h)WO (2)\nwhere,headi = Attention(QW1,KW2,VW 3)\nSoftmax(φk) = exp(φk)∑c\nj exp(φj), (3)\nTransformer model has two variations – base and big. The\ndifferences are in the number of weight dimensions and\nnumber of layers. For English to German translation, the\nbase and big model achieve BLEU scores of 27.3 and 28.4\nrespectively. In this study, we retrained the base model and\nstart with a BLEU score of 27.68, which is close to the\nstate-of-the-art. (Wu, 2018).\nINT8 Quantization of TransformerLT Model\nInput A (FP32)Input B (FP32)\nOutput (FP32)\nMatMul\nInput A (FP32)Input B (FP32)\nQuantizeV2MinMax\nOutput (FP32)\nQuantizedMatMul\nDequantize\nINT32MaxMin\nQuantizeV2MinMax\nINT8MaxMinINT8MaxMin\nRequantizeINT8 MaxMin\nRequantizationRangeINT32MinMax\nINT32MinMax\nFigure 1.Schematic of Na ¨ıve Quantization for MatMuls in\nTensorFlow\n4. Quantization with accuracy\nQuantizing a model entails converting the FP32 tensors\nto a target lower precision integer data type as described\nin Equation 5 and then replace the FP32 operations with\ncorresponding quantized operations.\nscale= target\nMax −Min (4)\nAquantized = round((Afloat −zerooffset ) ·scale) (5)\nFor example, to convert an FP32 tensor following Equation 5\nto an unsigned INT8 tensor the simplest way is to map\nminimum (Min) and maximum (Max) values in the tensor to\n0 and 255 respectively. Values between 0 and 255 represent\nthe numerical distribution between Min and Max. This way\nwe can represent an arbitrary magnitude of ranges to a lower\nprecision datatype. The overhead of quantization is O(N)\nas it requires linear scans to calculate Min and Max, and\nthen transform the entire tensor. N here signiﬁes the size of\nthe tensor.\n4.1. Na¨ıve Quantization\nThe primary compute in neural networks is in the form of\nconvolution and matrix multiplication operations. These\noperations beneﬁt from quantization due to speed-up\nobtained from INT8/VNNI instructions. The Transformer\nLT model has MatMuls as seen in section 3, and MatMuls\ntake up the largest portion of the computation as shown in\nFigure 10(a). Thus, MatMul is the ﬁrst operation that must\nbe quantized.\nThe na¨ıve way of quantization is mapping the entire FP32\ndynamic range of a tensor to INT8. The modiﬁcation\nin the computation graph in TensorFlow as shown in\nFigure 1 involves replacing a quantizable operation\n(MatMul) with the corresponding quantized version of\nthat operation(QuantizedMatMul) in the model graph, and\npassing quantized input tensors to that operation. The\nquantized tensors are obtained by passing the FP32 tensor\nthrough a QuantizeV2 operation to convert it to INT8.\nOperations to calculate Min and Max values of the tensor are\nalso inserted before the QuantizeV2 node. The difference\nbetween quantized kernel of an operation and its FP32\nversion is that it executes ISA optimized INT8 instructions\n(including VNNI). For example, the QuantizedMatMul\noperation accepts the A matrix (signed INT8), the B\nmatrix (unsigned INT8) and their corresponding Min/Max\nvalues. The result of the multiplication is accumulated as\nsigned INT32 value. The RequantizationRange operation\ncalculates the output Min and Max values from the INT32\nresult. This is fed into a Requantize operation which maps\nthe INT32 result into an INT8 representation with a new\nMin and Max value. Since operations downstream in the\ncomputational graph may not accept INT8 datatype values,\na Dequantize operation is required to convert the INT32\nvalue back into an FP32 value as shown in Equation 6.\nDequantization is also O(N) in complexity.\nAdequantized = (Max−Min)·(Aquantized −zerooffset )\n(6)\nThis transform was done to all the MatMul operations in\nthe graph. On running inference on the graph transformed\nusing this method, it failed to emit a stop token at all, and\nconsequently failed to achieve the less than 0.5 drop in the\nFP32 BLEU score accuracy. This ended up deeming the\nna¨ıve quantization approach inappropriate for this model. A\ncloser investigation by visualizing the histograms of input\ntensor values show that most of them have a long-tailed\nnumerical distribution.\nIf the entire range of the FP32 tensor was to be preserved,\nthere would be a loss of precision due to multiple values\nbeing mapped to the same bin. Hence, quantization\nusing absolute Min and Max can result in signiﬁcant\nloss in accuracy. As a result na ¨ıve quantization is not\na viable approach. In the next subsection, we explore\nother approximation methods based on divergence between\nprobability distributions.\n4.2. KL-Divergence for optimal saturation thresholds\nOne solution to preserve precision when quantizing tensors\nis to reduce the range of the representation. This relies on\nthe assumption that maintaining small differences between\ntensor values that are close together is more important than\nrepresenting the absolute extreme values or the outliers.\nIdeally, the numerical distribution of values in the mapped\nINT8 tensor representations should be as close as possible\nto the distribution of values for FP32 tensors. One way\nto measure this “closeness“ is to use the Kullback-Leibler\nINT8 Quantization of TransformerLT Model\nFigure 2.Tensors with sparse, narrow and Gaussian histograms in Transformer model\nDivergence (KL-Divergence) (Kullback & Leibler, 1951)\nmetric between the histograms of FP32 and INT8 tensors.\nBy iteratively choosing different Min and Max threshold\nvalues and mapping them to their respective extrema in\nthe INT8 representation, we are able to ﬁnd optimal Min\nand Max values that minimize the KL divergence between\nthe INT8 and FP32 tensors. We refer to this process as\ncalibration within the quantization workﬂow. This idea was\nﬁrst introduced in (Migacz, 2017).\nWe chose 600 random length samples out of 3003 sentences\nin the validation dataset as calibration data for quantization.\nThe MatMul input tensors in Transformer LT graph\ncome from three types of distributions, as classiﬁed\nfrom inspection of histogram values from inferring on\nthe calibration dataset shown in Figure 2. The sparse\ntensors, when quantized, result in unacceptable accuracy\ndegradation, and are kept unquantized i.e. with FP32 data\ntype. For the other two types of tensors, they can be\nthresholded to get a reasonable accuracy degradation. The\ninputs to the MatMul in this model are both signed FP32, as\nopposed to the expected case of one signed weight and an\nunsigned activation. Thus, there is a need to quantize one of\nthe tensors to unsigned INT8, which is described in detail in\nsubsection 5.1. The way to generate the thresholds for this\nconversion of a signed FP32 tensor to unsigned INT8 tensor\naffects the overall accuracy of the model. We determined\nthe positive and negative thresholds using three separate\nways, referred to as quantization modes:\n1. Symmetric calculates the KL-divergence on the entire\ndistribution. Here,\nThresholdMin = −ThresholdMax\n2. Independent separates the distribution about value zero\nTable 1.Effects of calibration modes on the accuracy\nMode BLEU score Drop in Accuracy\nNa¨ıve quantization NA NA\nSymmetric 27.30 0.38\nIndependent 27.33 0.35\nConjugate 27.26 0.421\nand calculates ThresholdMin and ThresholdMax\nindependently\n3. Conjugate separates the distribution about zero and\ncalculates thresholds independently, but reports\n{\nThresholdMax = max(|Max|,|Min|)\nThresholdMin = −ThresholdMax\nTable 1 shows the effect of different quantization modes\non the ﬁnal acuracy (BLEU score). One test of the need\nfor early thresholding using KL-divergence is to perform\nna¨ıve quantization on all the MatMuls that do not have a\nsparse input. The graph generated by this method could not\nemit a STOP token during inference, giving out garbage\ntranslations. The BLEU score is unavailable, marked as NA\nin the table. This proves the need to use early thresholding.\nIt can also be observed that independently calculating the\nthresholds for positive and negative halves of the histogram\nresults in the least drop in accuracy. In this case, the min and\nmax thresholds might not be symmetric about zero, causing\nthe quantized tensors to have a non-zero value for the offset.\nThis results in the QuantizedMatMul kernel being slightly\nslower than the case where the offsets are zero. Thus, we\nuse the symmetric mode for threshold calculation at a small\ncost of 0.03 drop in accuracy. Note that we ended up not\nquantizing the tensors with Sparse histogram (appearing in\nINT8 Quantization of TransformerLT Model\n42%\n22%\n10%\n4%\n4%\nMatMul\nGatherNd\nTranspose\n_MklConcatV2\nBatchMatMul\n_MklSub\n_MklAdd\nMul\nAd d\n_MklSoftmax\nMean\n18%\nFigure 3.Operation times of FP32 Transformer Model\nFigure 4.MKL GEMM Speedup with INT8/VNNI compared to\nA VX512 with micro-benchmarks\n12 out of 97 MatMuls). Tensors with narrow and Gaussian\ndistributions are quantized.\n5. Improving Performance\nOur objective here is to not leave any performance on\nthe table; exploit every opportunity to improve efﬁciency\nincluding (but not limited to) leverage MatMul kernels\noptimized with VNNI instructions, reduce operation times\nin the model, fuse operations, optimize input pipeline,\nimprove resource utilizing with parallelization and other\nstrategies. In the process, we reduced total number of\noperations in the graph by removing redundant operations\nand reordering operations maintaining correctness. We\nintroduced new optimizations in the Intel R⃝ MKL kernel\nimplementations, found a new way of ordering input\nsentences and parallelized execution of batches. These\ntechniques are described in the following subsections.\n5.1. Optimizing Quantized MatMuls\nEarlier generation (before 2019) Intel CPUs designed\nwith 512-bit Advanced Vector Extensions instructions\n(A VX512) helped vectorize both 16 FP32 and 64 INT8\nmultiply-and-add operations. 2nd Generation Xeon R⃝\nScalable codenamed Cascade Lake CPUs designed with\nFigure 5.Comparison of speedups of MKL GEMM INT8 vs FP32\nfor different matrix shapes\nINT8/VNNI instructions further optimized them. The ﬁgure\nFigure 4 shows INT8 MatMuls using VNNI provides a\nspeed-up of 3.7X over FP32 MatMuls using A VX512. And,\nthe speed-up provided by VNNI over A VX512 for INT8\nMatMul is 2.3X. In this context, as seen in the operation\nsplits in Figure 3, MatMuls take 42% of time in the FP32\nmodel. This data strongly motivates us to use VNNI\ninstructions to expedite MatMuls.\nThe default TensorFlow version 1.12 use open-source\nGEMMLOWP library for the integer matrix-matrix\nmultiplication (Team, 2016). When we started this work,\nGEMMLOWP kernel did not use INT8/VNNI instructions.\nAdditionally, GEMMLOWP required format conversions\nof the input matrices prohibitively reducing its efﬁciency\non our platform. Quantized MatMul kernel in Intel R⃝\nMKL BLAS, on the other hand, is IA optimized and\nexploit INT8/VNNI goodness. Hence, as our ﬁrst order\nof business we integrated MKL integer GEMM (General\nMatrix Multiply) kernel to TensorFlow INT8 quantized\nMatMul which accepts one signed and one unsigned INT8\nmatrix and accumulates results into a signed INT32 tensor.\nHowever, our ﬁrst implementation resulted in lower than\nexpected performance boost. Further analysis showed that\nthe MatMul shapes used in the Transformer model were\nrunning with lower efﬁciency due to non-zero offsets. To\nelaborate, Equations 7 and 8 below shows how a single\ncall to GEMM S8U8S32: A (INT8), B(UINT8), C(INT32)\nimplicitly executes 6 MatMuls with non-zero offsets. In\nthe equations, αand βare scalar values, oa, oband ocare\noffsets for A, B and C respectively. While the ﬁrst operation\ni.e. Op(A)Op(B) is optimized in the MKL kernel, the\nother multiplications were not. Fixing this gap resulted\nin an average 2.4X speedup across all the MatMuls in the\nmodel as shown in Figure 5. We consider this one of our\nkey achievements.\n(7)C = α(Op(A) +oa)(Op(B) +ob) +βC + oc\nINT8 Quantization of TransformerLT Model\nFigure 6.Operations in the while loop of the Transformer model\n(8)C = α(Op(A)Op(B) +Op(A)ob+ Op(B)oa+ oa\n∗ob) +βC + oc\n5.2. Optimizing GatherNd\nThe GatherNd operation on N-dimensional tensors uses\nthe input indices to perform a Gather on the input tensor\nto form the output tensor. In total, there are 40 such\nGatherNd operations in the Transformer model which occur\nin the decoder while loop. These operations involve copy\non large tensors, making it one of the most expensive\noperations due to beam search. There is no obvious\ncompute beneﬁt for quantizing GatherND. Looking into\nthe GatherNd implementation in TensorFlow, we found that\nmemory copy time dominates the operation time. Hence,\nwe reduced amount of data copied by changing the data type\nof the tensors to INT8 via quantizing GatherNd.\nIn the Transformer computational graph, the decoder while\nloop contains GatherNd operations as shown in Figure 6.\nThe na¨ıve way of quantizing GatherNd involves adding\na Quantize and Dequantize node before and after the\nGatherNd node. This, however adds an overhead, which\nreduces the overall speedup. We however managed to\nminimize the extra cost of this Dequantize by repositioning\nthe existing the Quantizes and Dequantizes in the graph due\nto QuantizedMatMul. With these changes we reduced the\ncopy size by 3.8X for the validation dataset. The execution\ntime of the GatherNd operation alone was reduced by 5X.\n5.3. Sorting Input Sentences\nIn machine translation, the inputs to the network have\nvarying sequence lengths. When input sentences are batched\ntogether, all the sentences except the longest sentence in\nthe batch are padded to the sequence length of the longest\nsentence in each batch. This adds an overhead per batch\nin terms of wasted computations for the pad tokens. To\nwork around this issue, it is important to sort the input\nFigure 7.Schematic of Optimized Quantization for MatMuls in\nTensorFlow\nvalidation dataset to keep the padding added to each batch\nto a minimum. There are different sorting methodologies\nsuch as sorting based on the number of words in each input\nsentence or token sorting based on the number of tokens\nin each input sentences. We have found that inference\nperformance with sorting based on the number of tokens\ngives us an improvement of 28% over inference performance\nwith sorting based on the word count of the input sentence.\n5.4. Eliminating Operations from Graph\nAs discussed in subsection 4.2, ﬁnding thresholds using\nKL-divergence method eliminated the need for computing\nabsolute Min and Max of tensors in the graph. These\nthreshold values are inserted as Const operations in the\ngraph. This further removed some of the Reshape operations.\nWe also eliminated Requantize and RequantizationRange\noperations for tensors which were feeding in to unquantized\noperations. We used a Dequantize operation to convert\nINT32 to FP32 directly as shown in Figure 7. These\nremovals contributed to reducing the total number of\noperations in the quantized compute graph. Additional\nquantize/dequantize operations were also removed in the\nGatherNd quantization.\n5.5. Parallel Batching\nThe execution time of inference varies depending on the\nlength of the sentences in the batch. This occurs due to larger\nnumber of operations (such as Matmuls) with increased\nsentence length and decode steps. Our measurements\nrepeatedly showing that CPU utilization signiﬁcantly drops\nINT8 Quantization of TransformerLT Model\nFigure 8.Comparison of the serial and parallel execution\ntechniques\ndepending on sentence size led us to investigate how we\ncould further exploit this to expedite performance. Since\nbatches of longer sentences more efﬁcient use CPU cores,\nserially executing batches in-order seemed inefﬁcient. We\ncould pack one or more batches of longer sentences with\nbatches of shorter sentences and effectively increase CPU\nutilization.\nTo achieve this, we create a parent TensorFlow session\nwhich in turn creates a FIFO batch queue. This parent\nprocess creates children processes which are afﬁnitized to\nspeciﬁc subset of CPU cores and local memory (NUMA)\ndomain. The children processes pick batches of input\nsentences from the batch queue and perform inference.\nThe input sentences are ordered by decreasing token count\nbefore being added to the batch queue. Note that the\nchild processes dequeue batches asynchronously from\nthe batch queue. This means batches of long and short\nsentences are processed in parallel utilizing the cores more\nefﬁciently. As a result of this optimization, we observe\na 1.4X improvement in throughput as shown in Figure 8.\nWe use multiple inference streams per node as described in\n(Saletore et al., 2018).\n6. Throughput Performance Results\nExperimental Setup: FP32 and INT8 performance are\nevaluated on 2S Intel R⃝ Xeon R⃝ Platinum 8168 (24 cores per\nsocket) processors and Intel R⃝ Xeon R⃝ Platinum 8268 (24\ncores per socket) processors, respectively. Both are tested\nwith TensorFlow 1.12.0 built with VNNI, Intel R⃝ MKL\nmklml lnx 2019.0.3.20190119, Python 2.7 on CentOS 7.5.\nThe inference performance of Transformer model for both\nINT8 and FP32 graphs are evaluated with newstest2014\ndataset. A mini batch size of 64 is used in all experiments.\nIn Figure 10, percentage of operation times are shown\nin different colors. MatMul is the major operation that\naccounts for 43% of the FP32 execution time. In case of\nINT8/VNNI quantized graph, some of these MatMuls are\nreplaced with QuantizedMatMuls reducing the percentage\nof time spent in matrix multiplications. However, the\nquantization of MatMuls resulted in overheads such as\nDequantize and QuantizeV2 in the INT8 graph. GatherND,\nanother operation that took up a signiﬁcant portion of\nthe FP32 computation, also signiﬁcantly reduced its\nINT8 percentage through the optimization described in\nsubsection 5.2.\nFigure 9 compares the overall inference throughput of the\nTransformer model with INT8/VNNI optimizations on 2S\nXeon R⃝ 8268 with optimized FP32 on 2S Xeon R⃝ 8168\nplatforms. The ﬁrst two bars in Figure 9 are throughput\nresults obtained by using default word-sorted input data\nmeasured with 1 and 2 streams per node respectively. The\nnext two sets use token-sorted input data on 2 and 4\nstreams per node respectively. The last three sets use both\ntoken-sorted and parallel batching with 2, 4, and 8 streams\nper node respectively. Figure 9a shows that we were able\nto achieve up to 4.5X throughput performance scaling with\nINT8/VNNI quantization relative to the out-of-the-box FP32\nperformance with all of our optimizations. However, our\ninput pipeline and system level optimizations effectively\nimproved FP32 performance by 3X. Figure 9b shows\nthroughput performance scaling using INT8/VNNI relative\nto best FP32 SKX-8168 with the best system conﬁguration.\nThe highest INT8 throughput is achieved with 2 inference\nstreams/socket with token sorting and parallel batching\nresulting in a scaling of 1.51X.\n7. Conclusion\nIn this work we have quantized the Transformer machine\nlanguage translation model in TensorFlow and maintained\nless than 0.5 drop in BLEU score accuracy. The key learning\nis that models using non-linear layers like Softmax and\nLayer Normalization appearing between layers like MatMul\nmake the quantization process effort-intensive. For our\nreaders who wish to attempt the same, our recommendation\nis to analyze distributions of FP32 values in the tensors\nand selectively quantize to achieve both high accuracy and\nspeedup. Although this proof-point has been developed on\nIntel CPUs, we profess that the method can be generally\napplicable to all hardware platforms and all models that use\nself-attention or multi-head attention layers. It is essential\nto ensure that accuracy target is met while trying to improve\nthe performance. We optimized the compute graph by\nreducing number of operations, improved kernels of key\noperations such as MatMuls and GatherNd, optimized order\nof sentences in the input pipeline and ﬁnally used parallel\nbatching to achieve the highest throughput gains of 1.5X.\nAcknowledgements\nWe acknowledge the contributions from the Intel R⃝\nTensorFlow Direct Optimization team and the Intel R⃝ Math\nKernel Library team.\nINT8 Quantization of TransformerLT Model\n(a)\n(b)\nFigure 9.Throughput Performance of INT8/VNNI on 2S Xeon\nR⃝ 8268 vs FP32 2S Xeon\nR⃝ 8168\nINT8 Quantization of TransformerLT Model\nFigure 10.Distribution of percentage operation times in FP32 vs INT8 graph; INT8/VNNI avg time per batch is lower than FP32\nReferences\nAlom, M. Z., Moody, A. T., Maruyama, N., Essen, B. C. V .,\nand Taha, T. M. Effective quantization approaches for\nrecurrent neural networks. CoRR, abs/1802.02615, 2018.\nURL http://arxiv.org/abs/1802.02615.\nAWS. How amazon translate works, 2018. URL\nhttps://docs.aws.amazon.com/translate/\nlatest/dg/how-it-works.html.\nChiu, C., Sainath, T. N., Wu, Y ., Prabhavalkar, R.,\nNguyen, P., Chen, Z., Kannan, A., Weiss, R. J.,\nRao, K., Gonina, K., Jaitly, N., Li, B., Chorowski,\nJ., and Bacchiani, M. State-of-the-art speech\nrecognition with sequence-to-sequence models. CoRR,\nabs/1712.01769, 2017. URL http://arxiv.org/\nabs/1712.01769.\nCourbariaux, M. and Bengio, Y . Binarynet: Training deep\nneural networks with weights and activations constrained\nto +1 or -1. CoRR, abs/1602.02830, 2016. URL http:\n//arxiv.org/abs/1602.02830.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K.\nBERT: pre-training of deep bidirectional transformers for\nlanguage understanding. CoRR, abs/1810.04805, 2018.\nURL http://arxiv.org/abs/1810.04805.\nFomenko, E. Understanding new vector neural\nnetwork instructions (vnni), 2018. URL https:\n//aidc.gallery.video/detail/video/\n5790616836001/understanding-new-\nvector-neural-network-instructions-\nvnni.\nGoogle, A. . M. L. P. Translating text, 2018.\nURL https://cloud.google.com/translate/\ndocs/translating-text.\nGuu, K., Hashimoto, T. B., Oren, Y ., and Liang, P.\nGenerating sentences by editing prototypes. CoRR,\nabs/1709.08878, 2017. URL http://arxiv.org/\nabs/1709.08878.\nGysel, P., Pimentel, J., Motamedi, M., and Ghiasi,\nS. Ristretto: A framework for empirical study\nof resource-efﬁcient inference in convolutional neural\nnetworks. IEEE Transactions on Neural Networks and\nLearning Systems, PP:1–6, 03 2018. doi: 10 .1109/\nTNNLS.2018.2808319.\nHou, L. and Kwok, J. T. Loss-aware weight quantization\nof deep networks. CoRR, abs/1802.08635, 2018. URL\nhttp://arxiv.org/abs/1802.08635.\nHu, M., Peng, Y ., and Qiu, X. Mnemonic reader for machine\ncomprehension. CoRR, abs/1705.02798, 2017. URL\nhttp://arxiv.org/abs/1705.02798.\nJiaxiang Wu, Cong Leng, Y . W. Q. H. and Cheng, J.\nQuantized convolutional neural networks for mobile\ndevices. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR). 2016.\nKullback, S. and Leibler, R. A. On information and\nsufﬁciency. The Annals of Mathematical Statistics, 22\n(1):79–86, 1951. ISSN 00034851. URL http://\nwww.jstor.org/stable/2236703.\nLei Ba, J., Kiros, J. R., and Hinton, G. E. Layer\nnormalization. arXiv preprint arXiv:1607.06450, 2016.\nINT8 Quantization of TransformerLT Model\nLingoTek. Api 5 basic integration, 2019. URL\nhttps://devzone.lingotek.com/basic-\nintegration-5.\nMellempudi, N., Kundu, A., Mudigere, D., Das, D.,\nKaul, B., and Dubey, P. Ternary neural networks\nwith ﬁne-grained quantization, 2017. URL https:\n//arxiv.org/pdf/1705.01462.pdf.\nMicrosoft, C. S. Machine translation, 2018. URL https:\n//www.microsoft.com/en-us/translator/\nbusiness/machine-translation/.\nMigacz, S. 8-bit inference with tensorrt, 2017. URL\nhttp://on-demand.gputechconf.com/\ngtc/2017/presentation/s7310-8-bit-\ninference-with-tensorrt.pdf.\nPark, E., Yoo, S., and Vajda, P. Value-aware quantization\nfor training and inference of neural networks. In\nThe European Conference on Computer Vision (ECCV),\nSeptember 2018.\nSaletore, V ., Karkada, D., and Datta, K. Boosting\ndeep learning training and inference performance\non intel R⃝xeon R⃝and intel R⃝xeon phi TMprocessors,\n2018. URL https://software.intel.com/en-\nus/articles/boosting-deep-learning-\ntraining-inference-performance-on-\nxeon-and-xeon-phi .\nTeam, G. G. Overview of gemmlowp design,\n2016. URL https://github.com/google/\ngemmlowp/blob/master/doc/design.md.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. June 2017.\nWu, K. Tensorﬂow transformer ofﬁcial model, 2018. URL\nhttps://github.com/tensorflow/models/\nblob/master/official/transformer/\ntestdata/newstest2014.en.\nNotices and Disclaimers\nPerformance results are based on testing as of dates shown\nin conﬁguration and may not reﬂect all publicly available\nsecurity updates. No product can be absolutely secure. See\nconﬁguration disclosure for details. Optimization Notice:\nIntel’s compilers may or may not optimize to the same\ndegree for non-Intel microprocessors for optimizations\nthat are not unique to Intel microprocessors. These\noptimizations include SSE2, SSE3, and SSSE3 instruction\nsets and other optimizations. Intel does not guarantee\nthe availability, functionality, or effectiveness of any\noptimization on microprocessors not manufactured by Intel.\nMicroprocessor-dependent optimizations in this product\nare intended for use with Intel microprocessors. Certain\noptimizations not speciﬁc to Intel micro-architecture are\nreserved for Intel microprocessors. Please refer to the\napplicable product User and Reference Guides for more\ninformation regarding the speciﬁc instruction sets covered\nby this notice. Software and workloads used in performance\ntests may have been optimized for performance only on\nIntel microprocessors. Performance tests, such as SYSmark\nand MobileMark, are measured using speciﬁc computer\nsystems, components, software, operations and functions.\nAny change to any of those factors may cause the results to\nvary. You should consult other information and performance\ntests to assist you in fully evaluating your contemplated\npurchases, including the performance of that product\nwhen combined with other products. For more complete\ninformation visit: http://www.intel.com/performance."
}