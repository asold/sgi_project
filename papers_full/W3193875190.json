{
    "title": "Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens",
    "url": "https://openalex.org/W3193875190",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3195393084",
            "name": "Itay Itzhak",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2250897584",
            "name": "Omer Levy",
            "affiliations": [
                "Tel Aviv University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3155183406",
        "https://openalex.org/W1647671624",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3115462295",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2143017621",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4206136559",
        "https://openalex.org/W3088592174",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2575598244",
        "https://openalex.org/W3008110149"
    ],
    "abstract": "Standard pretrained language models operateon sequences of subword tokens without direct access to the characters that compose eachtoken’s string representation. We probe theembedding layer of pretrained language models and show that models learn the internalcharacter composition of whole word and subword tokens to a surprising extent, withoutever seeing the characters coupled with the tokens. Our results show that the embedding layers of RoBERTa and GPT2 each hold enoughinformation to accurately spell up to a thirdof the vocabulary and reach high characterngram overlap across all token types. We further test whether enriching subword modelswith character information can improve language modeling, and observe that this methodhas a near-identical learning curve as training without spelling-based enrichment. Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesnot appear to enhance its performance on suchtasks.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5061 - 5068\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nModels In a Spelling Bee:\nLanguage Models Implicitly Learn the Character Composition of Tokens\nItay Itzhak Omer Levy\nThe Blavatnik School of Computer Science\nTel Aviv University\n{itay1itzhak,omerlevy}@gmail.com\nAbstract\nStandard pretrained language models operate\non sequences of subword tokens without di-\nrect access to the characters that compose each\ntoken’s string representation. We probe the\nembedding layer of pretrained language mod-\nels and show that models learn the internal\ncharacter composition of whole word and sub-\nword tokens to a surprising extent, without\never seeing the characters coupled with the to-\nkens. Our results show that the embedding lay-\ners of RoBERTa and GPT2 each hold enough\ninformation to accurately spell up to a third\nof the vocabulary and reach high character\nngram overlap across all token types. We fur-\nther test whether enriching subword models\nwith character information can improve lan-\nguage modeling, and observe that this method\nhas a near-identical learning curve as train-\ning without spelling-based enrichment. Over-\nall, our results suggest that language model-\ning objectives incentivize the model to implic-\nitly learn some notion of spelling, and that ex-\nplicitly teaching the model how to spell does\nnot appear to enhance its performance on such\ntasks.1\n1 Introduction\nContemporary subword tokenization algorithms\nsuch as BPE (Sennrich et al., 2016) partition a\nstring into contiguous spans of characters. Each\nspan represents a frequent character ngram, from\nindividual characters ( a), through preﬁxes ( uni)\nand sufﬁxes (tion), and even complete words (cats).\nThe tokenizer then converts each such span into\na discrete symbol (a token) with no internal struc-\nture, effectively discarding the token’s orthographic\ninformation. Therefore, a model operating over se-\nquences of subword tokens should be oblivious to\nthe spelling of each token. In this work, we show\nthat despite having no direct access to the subwords’\n1Our code is available at: https://github.com/\nitay1itzhak/SpellingBee\ninternal character composition, pretrained language\nmodels do learn some notion of spelling.\nTo examine what pretrained language models\nlearn about spelling, we present the SpellingBee\nprobe. SpellingBee is a generative language model\nthat predicts the character composition of a token\ngiven only its (uncontextualized) vector representa-\ntion from the pretrained model’s embeddings ma-\ntrix. SpellingBee is trained on part of the model’s\nvocabulary, and then tested by spelling unseen to-\nken types. If the probe can successfully reconstruct\nthe correct character sequence from an unseen to-\nken’s embedding, then there must be signiﬁcant\northographic information encoded in the vector.\nWe ﬁnd that the embedding layers of several\npretrained language models contain surprising\namounts of character information. SpellingBee\naccurately spells 31.8% of the held-out vocabu-\nlary for RoBERTa-Large (Liu et al., 2019), 32.9%\nfor GPT2-Medium (Radford et al., 2019), and\n40.9% for the Arabic language model AraBERT-\nLarge (Antoun et al., 2020). A softer metric that\nis sensitive to partially-correct spellings (chrF)\n(Popovi´c, 2015) shows a similar trend, with 48.7\nfor RoBERTa-Large and 62.3 for AraBERT-Large.\nThese results are much higher than the baseline\nof applying SpellingBee to randomly-initialized\nvectors, which fails to spell a single token.\nGiven that subword models learn some notion\nof character composition to fulﬁll language mod-\neling objectives, could they perhaps beneﬁt from\nknowing the exact spelling of each token a priori?\nTo that end, we reverse SpellingBee’s role and use\nit to pretrain the embedding layer of a randomly-\ninitialized model, thus imbuing each token repre-\nsentation with its orthographic information before\ntraining the whole model on the masked language\nmodeling objective. We compare the pretraining\nprocess of the character-infused model to that of\nan identical model whose embedding layer is ran-\ndomly initialized (and not pretrained), and ﬁnd that\n5061\nboth learning curves converge to virtually identi-\ncal values within the ﬁrst 1,000 gradient updates,\na fraction of the total optimization process. This\nexperiment suggests that while language models\nmay need to learn some notion of spelling to op-\ntimize their objectives, they might also be able to\nquickly acquire most of the character-level informa-\ntion they need from plain token sequences without\ndirectly observing the composition of each token.\n2 Spelling Bee\nTo measure how much a model knows the character\ncomposition of its tokens, we introduce Spelling-\nBee, a generative probe that tries to spell out a to-\nken character-by-character. Speciﬁcally, Spelling-\nBee probes the original model’sembedding matrix,\nsince spelling is a property of token types, invari-\nant to context. For example, given the embedding\nof the token cats, SpellingBee will try to generate\nthe sequence [ c, a, t, s]. We do so by modeling\nSpellingBee as a character-based language model,2\nwhere the ﬁrst token is a vector representation of\nthe vocabulary item.3\nTraining We split the vocabulary to train and test\nsets,4 and use teacher forcing to train SpellingBee.\nIn the example of cats, SpellingBee will compute\nthe following probabilities:\nP(x1 = c | x0 = cats)\nP(x2 = a | x0 = cats, x1 = c)\nP(x3 = t | x0 = cats, x1 = c, x2 = a)\n...\nAll of SpellingBee’s parameters are randomly ini-\ntialized. The only parameters that are pretrained\nare the token embeddings (e.g. the representation\nof cats or a), which are taken from the original\npretrained language model we intend to probe,\nand treated as constants; i.e. kept frozen during\nSpellingBee’s training.\nInference & Evaluation Once SpellingBee is\ntrained, we apply it to the test set using greedy de-\ncoding. For each vocabulary item w in the test set,\n2Implemented using the transformer decoder architecture,\nfollowing standard practice in language modeling.\n3Some vocabularies have symbols for indicating preceding\nwhitespaces (_) or that the next token is part of the same word\n(##). SpellingBee learns to predict these symbols too.\n4We test various train/test splits to ensure the robustness\nof our ﬁndings. See Section 3 for more detail.\nSpellingBee is given only the corresponding em-\nbedding vector ew, and is expected to generate the\ncharacter sequence w1, . . . , wn that deﬁnes w. We\nmeasure success on the test set using two metrics:\nexact match(EM), and character ngram overlap\nscore using chrF (Popovi´c, 2015). While EM is\nstrict, chrF allows us to measure partial success.\nWe also report edit distance using Levenshtein dis-\ntance ratio in Appendix A.\nSpellingBee for Pretraining Embeddings\nWhile we mainly use SpellingBee as a probe, a\nvariation of our method could potentially imbue\nthe embedding layer with character information\nbefore training a language model. We could train\na probe with randomly-initialized embeddings\n(instead of pretrained embeddings from another\nmodel) to predict the spelling of all vocabulary\nitems, and use these trained probe embeddings\nto initialize any target model’s embedding layer\n(instead of random initialization). We experiment\nwith this method in Section 5, but ﬁnd that it does\nnot have any signiﬁcant impact on the convergence\nof language models.\n3 Experiment Setup\nWe begin with a series of probing experiments,\nwhere we apply SpellingBee to the embedding\nlayer of various pretrained models.5\nPretrained Models We probe four pretrained\nmodels: RoBERTa-Base and Large (Liu et al.,\n2019), GPT2-Medium (Radford et al., 2019), and\nAraBERT-Large (Antoun et al., 2020). This set\nintroduces some diversity in vocabulary, objective,\nand scale: the ﬁrst three models are trained on En-\nglish corpora, while AraBERT is trained on text\nin Arabic; GPT2 is an autoregressive language\nmodel, while the rest are masked language mod-\nels; RoBERTa-Base consists of 125M parameters\n(with 768 dimensions per embedding), while the\nother models have approximately 350M parameters\n(with 1024 dimensions per embedding).\nControl Since SpellingBee is atrained probe, we\nwish to establish the probe’s baseline performance\nwhen provided with inputs with no orthographic\ninformation. As an empirical control, we train and\ntest SpellingBee on randomly-initialized vectors, in\naddition to the main experiments where we utilize\nthe pretrained embedding layers.\n5Hyperparameters are detailed in Appendix E.\n5062\nFilter RoBERTa GPT2 AraBERT\nBase Large Medium Large\nEM\nNone 27.3 31.8 32.9 40.9\nSimilarity 15.7 18.2 17.9 21.9\nLemma 15.7 17.7 16.5 19.5\nControl 0.0 0.0 0.0 0.0\nchrF\nNone 44.7 48.7 51.6 62.3\nSimilarity 32.7 35.1 36.4 46.0\nLemma 32.6 34.8 35.2 43.9\nControl 7.0 7.0 7.0 7.0\nTable 1: The percent of token types that can be spelled\nout exactly (EM) from their embeddings by Spelling-\nBee, and the ngram overlap between SpellingBee’s re-\nproductions and the token types’ true spellings (chrF).\nThe ﬁrst three rows reﬂect different methods for ﬁlter-\ning the training data, and the fourth represents the con-\ntrol experiment, which uses randomly initialized em-\nbeddings. All SpellingBee instances in this table are\ntrained on 32,000 examples.\nTraining & Testing Data We split the vocabu-\nlary into training and testing data using the fol-\nlowing protocol. First, we randomly sample 1000\ntoken types as test. We then ﬁlter the remaining\nvocabulary to eliminate tokens that may be too\nsimilar to the test tokens, and randomly sample\n32000 training examples.We experiment with three\nﬁlters: none, which do not remove tokens beyond\nthe test-set tokens; similarity, which removes the\ntop 20 most similar tokens for every token in test,\naccording to the cosine similarity induced by the\nembedding vectors; lemma, which removes any to-\nken type that shares a lemma with a test-set token\n(e.g. if diving is in the test set, then diver cannot\nbe in the training set). 6 The lemma ﬁlter always\napplies the similarity ﬁlter ﬁrst, providing an even\nmore adversarial approach for splitting the data.\nTo control for variance, we create 10 such splits\nfor each model and ﬁlter, and report the averaged\nevaluation metrics over all 10 test sets.\n4 Results\nMain Result Table 1 shows how well Spelling-\nBee can spell a vocabulary token using only its\nfrozen pretrained embedding. We observe that\nSpellingBee is able to accurately recover the\nspelling of up to 40.9% of the test set, while the\ncontrol is unable to spell even a single word cor-\nrectly. A similar trend can be seen when consider-\ning the ﬁner character ngram metric (chrF). Manu-\n6We lemmatize using NLTK’s WordNet lemmatizer (Loper\nand Bird, 2002) for English and Farasa’s Stemmer (Darwish\nand Mubarak, 2016) for Arabic.\nally analyzing the predictions of the control base-\nlines (see Appendix D) indicate that it primarily\ngenerates combinations of frequent character se-\nquences, which mildly contributes to the chrF score,\nbut does not affect EM. These results are persistent\nacross different models and ﬁlters, strongly indicat-\ning that the embedding layer of pretrained models\ncontains signiﬁcant amounts of information about\neach token’s character composition.\nOne may suggest that training SpellingBee over\n32,000 examples may leak information from the\ntest set; for example, if dog was seen during train-\ning, then spelling out dogs might be easy. We thus\nconsider the similarity and lemma ﬁlters, which\nremove such near-neighbors from the training set.\nWhile results are indeed lower (and probably do\naccount for some level of information leakage),\nthey are still considerably higher than the control,\nboth in terms of EM and chrF. Results using the\nsimilarity and lemma ﬁlters are rather similar, sug-\ngesting that embedding-space similarity captures\nsome information about each token’s lemma.\nFinally, we ﬁnd that the properties of pretrained\nmodels also seem to have a signiﬁcant effect on\nthe amount of spelling information SpellingBee\ncan extract. Larger models tend to score higher in\nthe probe, and the model trained on text in Ara-\nbic appears to have substantially higher EM and\nchrF scores than those trained on English corpora.\nOne possibility is that Arabic’s rich morphology\nincentivizes the model to store more information\nabout each token’s character composition; however,\nit is also possible that AraBERT’s different vocab-\nulary, which allocates shorter character sequences\nto each token type, might explain this difference\n(we discuss the link between sequence length and\naccuracy in Appendix C).\nOverall, our probing experiments show that even\nthough subword-based language models do not\nhave direct access to spelling, they can and do\nlearn a surprising amount of information about the\ncharacter composition of each vocabulary token.\nCharacter-Aware Models Some models are pro-\nvided with the raw character sequence of each to-\nken. To test whether the embedding layers of such\nmodels are indeed more informed about each to-\nken’s spelling, we apply SpellingBee to Character-\nBERT (El Boukkouri et al., 2020), a BERT-style\nmodel whose layer-zero word embeddings are de-\nrived from a character CNN, following ELMo (Pe-\nters et al., 2018).\n5063\nFilter RoBERTa CharacterBERT GloVe\nBase Base 300D\nEM\nNone 43.0 28.2 2.0\nSimilarity 9.6 12.9 1.6\nLemma 9.9 12.9 1.6\nControl 0.0 0.0 0.0\nchrF\nNone 58.8 53.3 13.6\nSimilarity 27.0 37.5 13.2\nLemma 27.3 37.5 13.0\nControl 7.9 8.0 8.0\nTable 2: The percent ofwhole wordsthat can be spelled\nout exactly (EM) from their embeddings by Spelling-\nBee, and the ngram overlap between SpellingBee’s re-\nproductions and the token types’ true spellings (chrF).\nAll SpellingBee instances in this table are trained on\n32,000 examples of whole words.\nTable 2 shows that the spelling-aware embed-\ndings of CharacterBERT score higher on the\nSpellingBee probe when the similarity and lemma\nﬁlters are applied. However, when no ﬁlter is ap-\nplied, RoBERTa’s character-oblivious but highly-\ntuned training process produces embeddings that\nscore higher on SpellingBee, presumably by lever-\naging implicit similarity functions in the embed-\nding space.\nAlthough CharacterBERT’s embedding layer\nis better at reconstructing original words (when\nsimilarity ﬁlters are applied), this does not mean\nthat character-aware models are necessarily better\ndownstream. El Boukkouri et al. (2020) report per-\nformance increases only on the medical domain.\nIn Section 5, we demonstrate that initializing a\nmasked language model’s embedding layer with\ncharacter information has a negligible effect on its\nperplexity.\nContext-Oblivious Models The ﬁrst generation\nof neural word representations (Mikolov et al.,\n2013a,b) contained only embedding layers, with-\nout any contextualization mechanism. We thus\nuse GloVe (Pennington et al., 2014) to estimate a\nlower bound on character information that can be\nobtained by simple context-oblivious models. We\nprobe the ﬁrst 50K words in GloVe’s vocabulary\nwith SpellingBee. Table 2 shows that GloVe embed-\ndings do contain a weak orthographic signal, better\nthan random embeddings, but substantially weaker\nthan the information stored in the embedding layer\nof large transformer-based language models.\nProbing with Less Training Data We further\nexamine whether SpellingBee can extract informa-\nTraining Examples\nEM\n0\n10\n20\n30\n40\n1000 2000 4000 8000 16000 32000\nNone Similarity Lemma Control\nTraining Examples\nchrF\n0\n10\n20\n30\n40\n50\n1000 2000 4000 8000 16000 32000\nNone Similarity Lemma Control\nFigure 1: The amount of character information\nSpellingBee is able to extract from RoBERTa-Large, as\nmeasured by EM (top) and chrF (bottom), given differ-\nent quantities of training examples.\ntion when trained on less examples. Figure 1 shows\nhow well SpellingBee can spell RoBERTa-Large’s\nvocabulary when trained on varying amounts of\ndata, across all ﬁlters. We ﬁnd that more data\nmakes for a better probe, but that even a few thou-\nsand examples are enough to train SpellingBee to\nextract signiﬁcant character information from the\nembeddings, which cannot be extracted from ran-\ndomized vectors (the control).7\n5 Pretraining Language Models to Spell\nOur probing experiments reveal that language mod-\nels learn some partial notion of spelling, despite\nthe lack of direct access to characters. Therefore,\nwe hypothesize that learning to spell is beneﬁcial\nfor language models, and propose pretraining the\nembedding layer using a variant of the SpellingBee\nprobe described in Section 2. Here, the goal is to\nimbue each embedding with enough information\nfor SpellingBee to accurately generate its surface\nform, and then initialize the language model with\nthe pretrained embeddings before it starts training\non the language modeling objective.\nWe apply this process to RoBERTa-Large, train-\n7We provide additional analysis on spelling accuracy by\nsubword frequency and length in Appendices B and C.\n5064\n0 1000 2000 3000 4000 5000 6000\nTraining Steps\n2\n4\n6\n8\n10\n12Training Loss\nControl\nPretrained\n101\n102\n103\nTraining Steps\n2\n4\n6\n8\n10\n12\n14\n16Training Loss\nControl\nPretrained\n0 1000 2000 3000 4000 5000 6000\nTraining Steps\n2\n3\n4\n5\n6\n7Validation Loss\nControl\nPretrained\nFigure 2: The overall training loss (left), ﬁrst steps of training loss (center), and validation loss (right) of RoBERTa-\nLarge, when training on the masked language modeling objective with embeddings pretrained by SpellingBee\n(pretrained) and randomly-initialized embeddings (control).\ning the model’s embedding layer with Spelling-\nBee using the same hyperparameter settings from\nAppendix E, with the key difference being that\nthe embeddings are now tunable parameters (not\nfrozen).8 We train RoBERTa-Large on English\nWikipedia using the hyperparameter conﬁguration\nof 24hBERT (Izsak et al., 2021), and cease training\nafter 24 hours (approximately 16,000 steps). For\ncomparison, we train exactly the same model with\na randomly-initialized embedding layer.\nFigure 2 shows the masked language modeling\nloss with and without pretrained embeddings. We\nsee that the curves quickly converge into one. After\nonly 1000 training steps, the difference between the\nvalidation losses never exceeds 0.01. This result\nindicates that in this scenario, the model does not\nutilize the character information injected into the\ntokens’ embeddings.\nAlthough there are many possible ways to ex-\nplicitly add orthographic information to tokens em-\nbeddings, our method is relatively straightforward\nas it gives the model a chance to utilize pre-stored\ncharacter information. Along with the results from\nSection 4, we hypothesize that the implicit notion\nof spelling that the model learns during pretraining\nmight be sufﬁcient for masked language modeling.\n6 Conclusion\nThis work reveals that pretrained language models\nlearn, to some extent, the character composition\nof subword tokens. We show that our Spelling-\nBee probe can spell many vocabulary items using\ntheir uncontextualized embedding-layer represen-\ntations alone. Trying to explicitly infuse character\ninformation into the model appears to have a min-\nimal effect on the model’s ability to optimize its\n8To verify that this process does indeed encode the to-\nkens’ spellings into the embeddings, we apply a SpellingBee\nprobe (using a different random initialization) to the learned\nembeddings, which yields 93.5% EM on held-out token types.\nlanguage modeling objective, suggesting that the\nmodel can independently learn all the character-\nlevel information it needs for the task.\nAcknowledgements\nThis work was supported by the Tel Aviv University\nData Science Center, Len Blavatnik and the Blavat-\nnik Family foundation, the Alon Scholarship, Intel\nCorporation, and the Yandex Initiative for Machine\nLearning. We thank Avia Efrat for his valuable\nfeedback.\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nKareem Darwish and Hamdy Mubarak. 2016. Farasa:\nA new fast and accurate Arabic word segmenter. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 1070–1074, Portorož, Slovenia. European\nLanguage Resources Association (ELRA).\nHicham El Boukkouri, Olivier Ferret, Thomas\nLavergne, Hiroshi Noji, Pierre Zweigenbaum, and\nJun’ichi Tsujii. 2020. CharacterBERT: Reconciling\nELMo and BERT for word-level open-vocabulary\nrepresentations from characters. In Proceedings\nof the 28th International Conference on Compu-\ntational Linguistics, pages 6903–6915, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nPeter Izsak, Moshe Berchansky, and Omer Levy. 2021.\nHow to train bert with an academic budget. arXiv\npreprint arXiv:2104.07705.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\n5065\nVladimir I Levenshtein et al. 1966. Binary codes capa-\nble of correcting deletions, insertions, and reversals.\nIn Soviet physics doklady, volume 10, pages 707–\n710. Soviet Union.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nEdward Loper and Steven Bird. 2002. NLTK: The nat-\nural language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Com-\nputational Linguistics, pages 63–70, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013b. Distributed rep-\nresentations of words and phrases and their compo-\nsitionality. In NIPS.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\n5066\nA Levenshtein Distance\nLevenshtein distance (Levenshtein et al., 1966) is\nan edit distance metric that, given two strings, cal-\nculates the minimal number of changes needed to\nbe done in order to make the two strings identical.\nLevenshtein distance ratio is the length-normalized\nversion, which is computed by adding the sum of\nlengths of both strings to the edit distance and divid-\ning by the same sum of lengths. We report the main\nexperiment’s results using this ratio in Table 3.\nFilter RoBERTa GPT2 AraBERT\nBase Large Medium Large\nNone 69.7 72.7 74.4 83.6\nSimilarity 61.5 63.7 64.5 75.8\nLemma 61.4 63.3 63.7 74.8\nControl 25.6 26.4 27.0 25.7\nTable 3: Levenshtein distance ratio. The ﬁrst three rows\nreﬂect different methods for ﬁltering the training data,\nand the fourth represents the control experiment, which\nuses randomly initialized embeddings. All SpellingBee\ninstances in this table are trained on 32000 examples.\nB Spelling Accuracy by Frequency\nWe test whether pretrained models tend to\nstore more spelling-related information in higher-\nfrequency token types. We focus on RoBERTa-\nLarge, and assign each token in the test set to\nits frequency quintile according to the number of\ntimes it appeared in the pretraining corpus – from\nthe 10000 most frequent token types (top 20%) to\nthose ranked 40000-50000 in the vocabulary (bot-\ntom 20%) – and measure the average performance\nof SpellingBee within each quintile. Figures 3 and\n4 shows the results with and without the similarity\nﬁlter. We observe that SpellingBee is indeed able\nto extract more information from higher-frequency\ntoken types, suggesting that the pretrained model\nhas more information about their character compo-\nsition.\nC Spelling Accuracy by Length\nWe analyze the effect of token length on the probe’s\nability to spell. A priori, it is reasonable to assume\nthat it is easier for the probe to spell shorter to-\nkens, since less information needs to be extracted\nfrom the embedding and there are less discrete de-\ncisions to be made while decoding. Indeed, Figure\n5 shows that with the none ﬁlter most vocabulary\n0-10K 10K-20K 20K-30K 30K-40K 40K-50K\nFrequency\n0\n20\n40\n60EM\nnone Filter\n0-10K 10K-20K 20K-30K 30K-40K 40K-50K\nFrequency\n0\n20\n40\n60EM\nsimilarity Filter\nFigure 3: The EM scores of SpellingBee on RoBERTa-\nLarge for each frequency quintile with the none ﬁlter\n(top) and the similarity ﬁlter (bottom).\n0-10K 10K-20K 20K-30K 30K-40K 40K-50K\nFrequency\n0\n20\n40\n60\n80chrF\nnone Filter\n0-10K 10K-20K 20K-30K 30K-40K 40K-50K\nFrequency\n0\n20\n40\n60\n80chrF\nsimilarity Filter\nFigure 4: The chrF scores of SpellingBee on RoBERTa-\nLarge for each frequency quintile with the none ﬁlter\n(top) and the similarity ﬁlter (bottom).\ntokens with 2-4 characters can be accurately re-\nproduced from their vector representations, while\nlonger tokens are harder to replicate. This trend\nis particularly sharp when the similarity ﬁlter is\napplied, as the probe is hardly able to spell tokens\nwith 6 or more characters accurately; having said\nthat, the probe is able to generate many partially\ncorrect spellings, as measured by chrF (Figure 6).\nPerhaps a less intuitive result is the probe’s fail-\nure to spell single-character tokens. A closer look\nreveals that many of these examples are rare or\nnon-alphanumeric characters (e.g. ç and $), which\nare probably very difﬁcult for the probe to gener-\nate if it had not seen them during training. While\nthese results show strong trends with respect to\nlength, token length is also highly correlated with\nfrequency, and it is not necessarily clear which of\nthe two factors has a stronger impact on the amount\nand resolution of character-level information stored\nin the embedding layer of pretrained models.\n5067\n1 2 3 4 5 6 7 8 9 10 11+\nToken Length\n0\n20\n40\n60EM\nnone Filter\n1 2 3 4 5 6 7 8 9 10 11+\nToken Length\n0\n20\n40\n60EM\nsimilarity Filter\nFigure 5: The EM scores of SpellingBee on RoBERTa-\nLarge for each token length with the none ﬁlter (top)\nand the similarity ﬁlter (bottom). The rightmost col-\numn groups together tokens with length of 11 or above.\n1 2 3 4 5 6 7 8 9 10 11+\nToken Length\n0\n20\n40\n60\n80chrF\nnone Filter\n1 2 3 4 5 6 7 8 9 10 11+\nToken Length\n0\n20\n40\n60\n80chrF\nsimilarity Filter\nFigure 6: The chrF scores of SpellingBee on RoBERTa-\nLarge for each token length with the none ﬁlter (top)\nand the similarity ﬁlter (bottom). The rightmost col-\numn groups together tokens with length of 11 or above.\nD Manual Error Analysis\nWe manually analyze 100 random tokens that\nSpellingBee spelled incorrectly with the lemma ﬁl-\nter to understand the nature of the spelling mistakes.\nOut of those 100 we display 20 mistakes in Table\n4 alongside the spelling prediction of the control\nbaseline. SpellingBee’s mistakes vary from single-\ncharacter typos to completely different words. Hav-\ning said that, the vast majority of mistakes have\nsigniﬁcant overlap with the correct spelling, such\nas shared preﬁxes and capitalization.\nE Hyperparameters\nWe implement SpellingBee with a 6-layer encoder-\ndecoder model, with 512 model dimensions.\nThe model parameters are optimized with Adam\n(Kingma and Ba, 2015) for 1000 steps with up to\n1024 tokens per batch, a learning rate of 5e-4, and a\ndropout rate of 0.1. These are the default hyperpa-\nToken SpellingBee Control\n_Issa _Asey _kinston\n_Rhod _Rob _hoedn\nMemory Mathinge _entically\n_metals _metrys _leaved\n_Reed _Redd _fomparing\n_break _breach _promoters\n_summit _mosump _seasons\nCatholic Cravital _tonversal\n_cleanup _lamed _paclus\n_Winner _Womer _purden\n_LIM _LUM _Send\nCopy Cople _providers\n_voicing _relicing _walking\n_Stab _Stamb _hoviders\n_356 _353 _budiance\nﬁnd wive _malding\n_Psychic _Syptanc _joacter\n_Looking _Lowing parging\nCLOSE DEFIC _tuldence\n_proliﬁc _promistic _complexement\nTable 4: Sampled SpellingBee errors with the lemma\nﬁlter alongside the control baseline’s spelling for the\nsame tokens. The underscore (_) represents a preceding\nwhitespace.\nrameters for training a transformer language model\nin Fairseq (Ott et al., 2019).\n5068"
}