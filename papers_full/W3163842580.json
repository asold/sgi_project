{
  "title": "Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents",
  "url": "https://openalex.org/W3163842580",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222555875",
      "name": "Xiao, Chaojun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2298002479",
      "name": "Hu Xueyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1860873322",
      "name": "Liu Zhi-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222555879",
      "name": "Tu, Cunchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381112680",
      "name": "Sun, Maosong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3010261130",
    "https://openalex.org/W3135190223",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962940365",
    "https://openalex.org/W3103181983",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3136275640",
    "https://openalex.org/W2009751122",
    "https://openalex.org/W3117737817",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2164386086",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035668167",
    "https://openalex.org/W2890026792",
    "https://openalex.org/W1979482500",
    "https://openalex.org/W2985237994",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3108610837",
    "https://openalex.org/W3105134823",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2858159822",
    "https://openalex.org/W3099641295",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3082274269"
  ],
  "abstract": "Legal artificial intelligence (LegalAI) aims to benefit legal systems with the technology of artificial intelligence, especially natural language processing (NLP). Recently, inspired by the success of pre-trained language models (PLMs) in the generic domain, many LegalAI researchers devote their effort to apply PLMs to legal tasks. However, utilizing PLMs to address legal tasks is still challenging, as the legal documents usually consist of thousands of tokens, which is far longer than the length that mainstream PLMs can process. In this paper, we release the Longformer-based pre-trained language model, named as Lawformer, for Chinese legal long documents understanding. We evaluate Lawformer on a variety of LegalAI tasks, including judgment prediction, similar case retrieval, legal reading comprehension, and legal question answering. The experimental results demonstrate that our model can achieve promising improvement on tasks with long documents as inputs.",
  "full_text": "Lawformer: A Pre-trained Language Model for\nChinese Legal Long Documents\nChaojun Xiao1∗, Xueyu Hu2∗, Zhiyuan Liu1†, Cunchao Tu3, Maosong Sun1\n1Department of Computer Science and Technology\nInstitute for Artiﬁcial Intelligence, Tsinghua University, Beijing, China\nBeijing National Research Center for Information Science and Technology, China\n2 Beihang University, Beijing, China\n3 Beijing Powerlaw Intelligent Technology Co., Ltd., China\nxcjthu@gmail.com, huxueyu@buaa.edu.cn\ntucunchao@gmail.com {liuzy,sms}@tsinghua.edu.cn\nAbstract\nLegal artiﬁcial intelligence (LegalAI) aims to\nbeneﬁt legal systems with the technology of\nartiﬁcial intelligence, especially natural lan-\nguage processing (NLP). Recently, inspired\nby the success of pre-trained language models\n(PLMs) in the generic domain, many LegalAI\nresearchers devote their effort to apply PLMs\nto legal tasks. However, utilizing PLMs to\naddress legal tasks is still challenging, as the\nlegal documents usually consist of thousands\nof tokens, which is far longer than the length\nthat mainstream PLMs can process. In this\npaper, we release the Longformer-based pre-\ntrained language model, named as Lawformer,\nfor Chinese legal long documents understand-\ning. We evaluate Lawformer on a variety of\nLegalAI tasks, including judgment prediction,\nsimilar case retrieval, legal reading compre-\nhension, and legal question answering. The ex-\nperimental results demonstrate that our model\ncan achieve promising improvement on tasks\nwith long documents as inputs. The code and\nparameters are available athttps://github.\ncom/thunlp/LegalPLMs.\n1 Introduction\nLegal artiﬁcial intelligence (LegalAI) focuses on\napplying methods of artiﬁcial intelligence to bene-\nﬁt legal tasks (Zhong et al., 2020a), which can help\nimprove the work efﬁciency of legal practitioners\nand provide timely aid for those who are not fa-\nmiliar with legal knowledge. Thus, LegalAI has\nreceived great attention from both natural language\nprocessing (NLP) researchers and legal profession-\nals (Zhong et al., 2018, 2020b; Wu et al., 2020;\nChalkidis et al., 2020; Hendrycks et al., 2021).\nIn recent years, pre-trained language mod-\nels (PLMs) (Peters et al., 2018; Devlin et al., 2019;\nLiu et al., 2019; Raffel et al., 2020; Brown et al.,\n∗Indicates equal contribution.\n†Corresponding author.\nFigure 1: The length distribution of the fact descrip-\ntion in criminal and civil cases. The criminal and civil\ncases consist of 1260.2 tokens on average. The length\nof 64.12% cases is over 512.\n2020) have proven effective in capturing rich lan-\nguage knowledge from large-scale unlabelled cor-\npora and achieved promising performance improve-\nment on various downstream tasks. Inspired by\nthe great success of PLMs in the generic domain,\nconsiderable efforts have been devoted to employ-\ning powerful PLMs to promote the development\nof LegalAI (Shaghaghian et al., 2020; Shao et al.,\n2020; Chalkidis et al., 2020).\nSome researchers attempt to transfer the contex-\ntualized language models pre-trained on the generic\ndomain, such as Wikipedia, Children’s Books, to\ntasks in the legal domain (Shaghaghian et al., 2020;\nZhong et al., 2020b; Shao et al., 2020; Elwany\net al., 2019). Besides, some works conduct con-\ntinued pre-training on legal documents to bridge\nthe gap between the generic domain and the legal\ndomain (Zhong et al., 2019; Chalkidis et al., 2020).\nHowever, most of these works adopt the full\nself-attention mechanism to encode the documents\nand cannot process long documents due to the\nhigh computational complexity. As shown in Fig-\nure 1, the average length of the criminal cases and\narXiv:2105.03887v1  [cs.CL]  9 May 2021\ncivil cases is 1260.2, which is far longer than the\nmaximum length that mainstream PLMs (BERT,\nRoBERTa, etc.) can handle. With the limited capac-\nity to process long sequences, these PLMs cannot\nachieve satisfactory performance in representing\nthe legal documents (Zhong et al., 2020a; Shao\net al., 2020). Hence, how to utilize PLMs to pro-\ncess legal long sequences needs more exploration.\nIn this work, we release Lawformer, which is pre-\ntrained on large-scale Chinese legal long case docu-\nments. Lawformer is a Longformer-based (Beltagy\net al., 2020) language model, which can encode\ndocuments with thousands of tokens. Instead of em-\nploying the standard full self-attention, we combine\nthe local sliding window attention and the global\ntask motivated full attention to capture the long-\ndistance dependency. To the best of our knowledge,\nLawformer is the ﬁrst legal pre-trained language\nmodel, which can process the legal documents with\nthousands of tokens.\nBesides, we evaluate Lawformer on a collection\nof typical legal tasks including legal judgment pre-\ndiction, similar case retrieval, legal reading com-\nprehension, and legal question answering. Solving\nthese tasks requires the model to understand do-\nmain knowledge and concepts in legal texts, and\nbe able to analyze complicated case scenarios and\nlegal provisions.\nNotably, the data distribution of existing datasets\nfor legal judgment prediction is quite different from\nthe real-world data distribution. And these datasets\nonly contain criminal cases and omit civil cases.\nTherefore, we construct new datasets from scratch\nfor legal judgment prediction tasks, which consist\nof hundreds of thousands of criminal cases and\ncivil cases. As for the other tasks, we rely on the\npre-existing datasets. Experimental results on these\nvarious tasks demonstrate that the proposed Law-\nformer can achieve strong performance in legal\ndocuments understanding.\nThe main contributions of this paper are summa-\nrized as follows:\n• We release a Chinese legal pre-trained language\nmodel, which can process documents with thou-\nsands of tokens, named as Lawformer. To the\nbest of our knowledge, Lawformer is the ﬁrst\npre-trained language model for legal long docu-\nments.\n• We evaluate Lawformer for legal documents un-\nderstanding, with high coverage of existing typi-\ncal LegalAI tasks. In this benchmark, we propose\nnew legal judgment prediction datasets for both\ncriminal and civil cases.\n• Extensive experiments demonstrate the proposed\nLawformer can achieve strong performance on\nvarious LegalAI tasks that require the models are\nable to process the long documents. In terms of\nthe tasks with short inputs, Lawformer can also\nachieve comparable results with RoBERTa (Liu\net al., 2019) pre-trained on the legal corpora.\n2 Related Work\n2.1 Legal Artiﬁcial Intelligence\nLegal artiﬁcial intelligence aims to proﬁt the tasks\nin the legal domain (Zhong et al., 2020a). Due to\nthe amount of textual legal resources, LegalAI has\ndrawn great attention from NLP researchers in re-\ncent years (Luo et al., 2017; Ye et al., 2018; Duan\net al., 2019b; Shao et al., 2020; Wu et al., 2020).\nEarly works attempt to analyze legal documents\nwith hand-crafted features and statistical meth-\nods (Kort, 1957; Nagel, 1963; Segal, 1984). With\nthe development of deep learning, many efforts\nhave been devoted to solving various legal tasks,\nsuch as legal charge prediction (Luo et al., 2017;\nZhong et al., 2018; Xiao et al., 2018; Chalkidis\net al., 2019; Yang et al., 2019), relevant law ar-\nticle retrieval (Chen et al., 2013; Raghav et al.,\n2016), court view generation (Ye et al., 2018; Wu\net al., 2020), reading comprehension (Duan et al.,\n2019a), question answering (Zhong et al., 2020b;\nKien et al., 2020), and case retrieval (Raghav et al.,\n2016; Shao et al., 2020). Besides, inspired by the\ngreat success of PLMs in the generic domain, there\nare also some researchers conducting pre-training\non legal corpora (Zhong et al., 2019; Chalkidis\net al., 2020). However, these models adopt the\nBERT as the basic encoder, which cannot process\ndocuments longer than 512. To the best of our\nknowledge, Lawformer is the ﬁrst pre-trained lan-\nguage model for legal long documents.\n2.2 Pre-trained Language Model\nPre-trained language models (PLMs), which are\ntrained on amounts of unlabelled corpora, are able\nto beneﬁt a variety of downstream NLP tasks (De-\nvlin et al., 2019; Liu et al., 2019; Brown et al.,\n2020). Then we will introduce previous works\nrelated to ours from the following two aspects:\ndomain-adaptive pre-training and long-document\npre-training.\n[CLS]   …   重   伤   ，  经   医   院   抢   救   无  …  [SEP]\nDilated\nSliding Window\nSliding Window Global\n…\nFigure 2: An example of the combination of the three types of attention mechanism in Lawformer. The size of the\nsliding window attention is 2. The size of the dilated sliding window attention is 2 and the gap is 1. The token,\n[CLS], is selected to perform the global attention.\nDomain-Adaptive Pre-training. Many re-\nsearchers attempt to achieve performance gain by\ndomain-adaptive pre-training on various domains,\nincluding the biomedical domain (Lee et al., 2020),\nthe clinical domain (Alsentzer et al., 2019), the sci-\nentiﬁc domain (Beltagy et al., 2019), and the legal\ndomain (Zhong et al., 2019; Chalkidis et al., 2020).\nThese works further pre-train the BERT (Devlin\net al., 2019) on the speciﬁc domain texts and have\nshown that continued pre-training on the target do-\nmain corpora can consistently achieve performance\nimprovement (Gururangan et al., 2020).\nLong-Document Pre-training. Due to the\nhigh computational complexity of the full self-\nattention mechanism, the traditional transformer-\nbased PLMs are limited in processing long docu-\nments (Beltagy et al., 2020). Some works propose\nto utilize the left-to-right auto-regressive objective\nto pre-train the language models (Dai et al., 2019;\nSukhbaatar et al., 2019). And some works attempt\nto reduce the computational complexity with slid-\ning window based self-attention (Beltagy et al.,\n2020; Qiu et al., 2020; Zaheer et al., 2020). In this\nwork, we adopt the widely-used Longformer (Belt-\nagy et al., 2020) as our basic encoder, which com-\nbines the sliding window attention and the global\nfull attention to process the documents.\n3 Our Approach\n3.1 Lawformer\nOur current model utilizes Longformer (Beltagy\net al., 2020) as our basic encoder. Instead of utiliz-\ning the full self-attention mechanism, Longformer\ncombines the sliding window attention, dilated\nsliding window attention, and the global attention\nmechanism to encode the long sequence. We will\nintroduce the three types of attention patterns in the\nsection. Figure 2 gives an example of the combina-\ntion of three types of attention mechanisms.\nSliding Window Attention. In this attention\npattern, we only compute the attention scores be-\ntween the surrounding tokens. Speciﬁcally, given\nthe size of the sliding window w, each token only\nattends to the 1\n2 w tokens on each side. While the to-\nkens will only aggregate information around them\nin each layer, with the number of layers increases,\nthe global information can also be integrated into\nthe hidden representations of each token.\nDilated Sliding Window Attention. Similar\nto dilated CNNs (van den Oord et al., 2016), the\nsliding window attention can be dilated to reach a\nlonger context. In this attention mechanism, each\nwindow is not continuous but has a gap d between\neach attended token. Notably, in the multi-head at-\ntention, the gap in different heads can be different,\nwhich can promote the model performance.\nGlobal Attention. In some speciﬁc tasks, we\nneed some tokens to attend to the whole sequence\nto obtain sufﬁcient information. For example, in the\ntext classiﬁcation task, the special token [CLS]\nshould be used to attend to the whole document.\nIn the question answering task, the questions are\nsupposed to attend the whole sequence to generate\nexpressive representations. Therefore, we apply\nglobal attention for some pre-selected tokens for\ntask-speciﬁc representations. That is, instead of\nattending to the surrounding tokens, the selected to-\nkens will attend to the whole sequence to generate\nthe hidden representations. Notably, the parameters\nin the global attention and sliding window attention\nare different.\nWith the three types of attention mechanisms,\nwe can process the long sequences with linear com-\nplexity.\ncriminal civil\nModel Mic@c Mac@c Mic@l Mac@l Dis@t Mic@c Mac@c Mic@l Mac@l\nBERT 94.8 68.2 81.5 52.9 1.286 80.6 47.6 61.7 31.6\nRoBERTa 94.7 69.3 81.1 53.5 1.291 80.0 47.2 60.2 29.9\nL-RoBERTa 94.9 70.8 81.1 53.4 1.280 80.8 49.4 61.2 31.3\nLawformer 95.4 72.1 82.0 54.3 1.264 81.1 50.0 63.0 33.0\nTable 1: The results on the legal judgment prediction dataset, CAIL-Long. For criminal cases, we evaluate the\nmodels on charge prediction task (Mic@c, Mac@c), relevant law prediction task (Mic@l, Mac@l), and term of\npenalty prediction task (Dis@t). For civil cases, we evalute the models on cause of actions prediction task (Mic@c,\nMac@c), and relevant law prediction task (Mic@l, Mac@l).\n3.2 Data Processing\nWe collect tens of millions of case documents pub-\nlished by the Chinese government from China judg-\nment Online1. As the downstream tasks are mainly\nin the areas of criminal and civil cases, we only\nkeep the documents of criminal cases and civil\ncases. We divide each document into four parts:\nthe information about the parties, the fact descrip-\ntion, the court views, and the judgment results. We\nonly keep the documents with the fact description\nlonger than 50 tokens. After the data processing,\nthe rest of the data are used for pre-training. The\ndetailed statistics is listed in Table 2.\n# Doc. Len. Size\ncriminal cases 5,428,717 962.84 17 G\ncivil cases 17,387,874 1,353.03 67 G\nTable 2: The statistics of the pre-training data. # Doc.\nrefers to the number of documents. Len. refers to the\naverage length of the documents. Size refers to the size\nof the pre-training data.\n3.3 Pre-training Details\nFollowing the previous work (Beltagy et al., 2020),\nwe pre-train Lawformer with MLM objective, con-\ntinuing from the checkpoint, RoBERTa-wwm-ext,\nreleased in Cui et al. (2019). We set the learning\nrate as 5 ×10−5, the sequence length as 4, 096,\nand the batch size as 32. As the length of le-\ngal documents is usually smaller than 4, 096, we\nconcatenate different documents together to make\nfull use of the input length. We pre-train Law-\nformer for 200, 000 steps, and the ﬁrst 3, 000 steps\nare for warm-up. We utilize Adam (Kingma and\nBa, 2015) to optimize the model. The rest of the\nhyper-parameters are the same as Longformer. We\n1https://wenshu.court.gov.cn/\npre-train Lawformer with 8 ×32G NVIDIA V100\nGPUs.\nIn the ﬁne-tuning stage, we select different to-\nkens to conduct the global attention mechanism.\nFor the classiﬁcation task, we select the token,\n[CLS], to perform the global attention. And for\nthe reading comprehension task and the question\nanswering task, we perform the global attention\non the whole questions. For the speciﬁc details of\neach task, please refer to the next section.\n4 Experiments\n4.1 Baseline Models\nTo verify the effectiveness of the proposed model,\nwe compare Lawformer with following competitive\nbaseline models:\n• BERT (Devlin et al., 2019): we simply ﬁne-tune\nthe published checkpoint, BERT-base-chinese,\nwhich is pre-trained on Chinese wikipedia docu-\nments2, on the following downstream datasets.\n• RoBERTa-wwm-ext (RoBERTa) (Cui et al.,\n2019): it is pre-trained with the whole word\nmasking strategy, in which the tokens that belong\nto the same word will be masked simultaneously.\nNotably, Lawformer is pre-trained continuously\nfrom the RoBERTa.\n• Legal RoBERTa (L-RoBERTa): we pre-train a\nRoBERTa (Liu et al., 2019) on the same legal\ncorpus, continuing from the released RoBERTa-\nwwm-ext checkpoint.\nAs these baseline models can only process docu-\nments with less than 512 tokens, we only truncate\nthe documents to 512 tokens for these models.\n4.2 Legal judgment Prediction\nDataset Construction: Legal judgment predic-\ntion aims to predict the judgment results given\n2https://zh.wikipedia.org/\nModel P@5 P@10 P@20 P@30 NDCG@5 NDCG@10 NDCG@20 NDCG@30 MAP\nBERT 44.27 41.83 36.73 33.49 78.18 80.06 84.43 91.46 50.65\nRoBERTa 45.93 41.71 36.53 33.40 79.93 80.57 84.99 91.82 50.77\nL-RoBERTa 45.75 42.85 37.79 33.58 78.90 81.01 85.26 91.70 50.17\nLawformer 51.91 46.44 37.95 33.99 83.11 84.05 87.06 93.22 57.36\nTable 3: The results on the legal case retrieval dataset, LeCaRD. The dataset contains long cases with thousands of\ntokens as candidates.\nthe fact description. It is a critical application\nin the LegalAI ﬁeld and has received great atten-\ntion recently. To facilitate the development of this\ntask, Xiao et al. (2018) have proposed a large-scale\ncriminal judgment prediction dataset, CAIL2018.\nHowever, the average case length of CAIL2018\nis much shorter than that of real-world cases. Be-\nsides, CAIL2018 only focuses on criminal cases\nand omits civil cases. In this paper, we propose\na new judgment prediction dataset, CAIL-Long,\nwhich contains both civil and criminal cases with\nthe same length distribution as in the real world.\nCAIL-Long consists of 1, 129, 053 criminal\ncases and 1, 099, 605 civil cases. For both crim-\ninal and civil cases, we take the fact description as\ninputs and extract the judgment annotations with\nregular expressions. Speciﬁcally, each criminal\ncase is annotated with the charges, the relevant\nlaws, and the term of penalty. Each civil case is\nannotated with the causes of actions and the rele-\nvant laws. The detailed statistics of the dataset are\nshown in Table 4.\n# Case Len. # C. # Law prison\ncriminal 115,849 916.57 201 244 0-180\ncivil 113,656 1,286.88 257 330 –\nTable 4: The statistics of the judgment prediction datset,\nCAIL-Long. # Case denotes the number of cases. Len.\ndenotes the average length of the fact description. #\nC. denotes the number of charges/cause of actions. #\nLaw denotes the number of relevant laws. And prison\ndenotes the range of term of penalty (unit: month).\nImplementation Details: Following previous\nwork (Zhong et al., 2018), we train the models\nin the multi-task paradigm. For criminal cases,\nthe charge prediction and the relevant law predic-\ntion are formalized as multi-label text classiﬁcation\ntasks. The term of penalty prediction task is for-\nmalized as the regression task. For civil cases, the\ncause of actions prediction is formalized as a single-\nlabel text classiﬁcation task, and the relevant law\nprediction is formalized as a multi-label text clas-\nsiﬁcation task. The models for criminal and civil\ncases are trained separately.\nWe adopt micro-F1 scores (Mic@ {c,l}) and\nmacro-F1 scores (Mac@{c,l}) as metrics for clas-\nsiﬁcation tasks, and the log distance (Dis@t) as\nmetric for the regression task. We set the learning\nrate as 5 ×10−5, and batch size as 32.\nResult: We present the results in Table 1. As\nshown in the table, Lawformer achieves the best\nperformance among the four models in both the\nmicro-F1 and macro-F1 scores. The improvement\nindicates that Lawformer can accurately capture the\nkey information from the long fact description. Be-\nsides, the case numbers of different labels (charges,\ncause of actions, and laws) are highly imbalanced,\nand Lawformer can also achieve signiﬁcant im-\nprovement in macro-F1 scores, which indicates\nthat Lawformer is able to handle the labels with\nlimited cases. However, the overall results are still\nunsatisfactory. It still needs more exploration to\npredict the judgment results more accurately and\nrobustly.\n4.3 Legal Case Retrieval\nDataset: Legal case retrieval is the specialized in-\nformation retrieval task in the legal domain, which\naims to retrieve similar cases given the query fact\ndescription. For this task, we adopt the Legal Case\nRetrieval Dataset (LeCaRD)3 as our benchmark,\nwhich contains 107 query cases and 10, 716 can-\ndidate cases. Notably, the length of the candidate\ncases is extremely long. As shown in the Table 5,\nthe average length of the cases is 6, 319.14, which\nis a great challenge for the existing models.\nImplementation Details: We train the models\nwith the binary classiﬁcation task, which requires\nthe models to judge whether the given candidate\ncase is relevant to the query case. We set the ﬁne-\ntuning batch size as 32, the learning rate as 10−5\nfor all models. For the baseline models, which can\nonly process sequences with lengths no more than\n3https://github.com/myx666/LeCaRD\n# Query # Cand. Q-Len. C-Len. # Pair\n107 10,716 444.55 6,319.14 1,094\nTable 5: The statistics of LeCaRD dataset. # Query\nand # Cand. denote the number of query cases and can-\ndidate cases. Q-Len. and C-Len. denote the average\nlength of the fact description of the query cases and\ncandidate cases. # Pair denotes the number of positive\nquery-candidate case pairs.\n512, we set the maximum length of the query and\nthe candidates as 100 and 409, respectively. And\nfor the Lawformer, we set the maximum length of\nthe query and the candidates as 509 and 3, 072, re-\nspectively. All tokens of the query case are selected\nto perform the global attention mechanism.\nFollowing the previous works, we adopt 5-fold\ncross-validation for the dataset. We employ the\ntop-k Normalized Discounted Cumulative Gain\n(NDCG@k), Precision (P@k), and Mean Average\nPrecision (MAP) as evaluation metrics. For each\nmodel, we report the performance of the check-\npoint with the highest average score on P@ 10,\nNDCG@10, and MAP.\nResult: The results are shown in Table 3. We can\nobserve that Lawformer can signiﬁcantly outper-\nform all baseline models. For example, Lawformer\nimproves upon baselines by 6.59 points in terms of\nmean average precision. The similar case retrieval\ntask requires the models to compare the query case\nand candidate case in detail. The baseline models\neven cannot read the complete documents, and thus\nperform unsatisfactorily. Lawformer can process\nsequences with thousands of tokens and achieve\npromising results. However, the average length of\ncandidate cases in LeCaRD is 6, 319.14, which is\nalso beyond the capacity of Lawformer. We argue\nthat it still needs further exploration to retrieve the\nlong cases.\n4.4 Legal Reading Comprehension\nDataset: We utilize the Chinese judicial reading\ncomprehension (CJRC) as our benchmark dataset\nfor legal reading comprehension (Duan et al.,\n2019a). CJRC is published in the Chinese AI and\nLaw Challenge contest. We adopt the dataset pub-\nlished in 2020 as the benchmark4. CJRC consists\nof 9, 532 question-answer pairs with correspond-\ning supporting sentences. There are three types\nof answers for these questions, including the span\nof words, yes/no, and unanswerable. The detailed\n4https://github.com/china-ai-law-challenge/CAIL2020\nstatistics of CJRC are shown in Table 6. It is worth\nmentioning that the average length of the docu-\nments in CJRC is only 441.04.\n# Doc Len. # Que. # S-Que. # YN-Que. # U-Que.\n9,532 441.04 9,532 6,692 1,892 948\nTable 6: The statistics of CJRC dataset. # Doc denotes\nthe number of case documents. Len. denotes the aver-\nage length of the documents. # Que., # S-Que., # YN-\nQue., and # U-Que. denotes the total number of ques-\ntions, questions with span of words as answers, ques-\ntions with yes/no answers, unanswerable questions, re-\nspectively.\nImplementation Details: We implement the mod-\nels following the source code provided in Duan\net al. (2019a). We train the models to predict the\nstart positions and end positions. And the support-\ning sentence prediction is formalized as the binary\nclassiﬁcation for all the sentences. For the Law-\nformer, we select the whole question to perform the\nglobal attention. We adopt the exact match score\n(EM) and F1 score as evaluation metrics.\nResult: The results are shown in Table 7. From the\nresults, we can observe that (1) L-RoBERTa and\nLawformer, which are pre-trained on the in-domain\ncorpus, can signiﬁcantly outperform the BERT and\nRoBERTa model, which are pre-trained on the out-\nof-domain corpus. (2) L-RoBERTa can achieve\ncomparable performance with Lawformer, as the\nlong documents are ﬁltered out when constructing\nthe dataset. We argue that with the improvement of\nthe ability to process long sequences, Lawformer\ncan achieve better performance in the long docu-\nment reading comprehension.\nAnswer Support Joint\nModel EM F1 EM F1 EM F1\nBERT 50.70 66.53 31.45 71.80 21.19 52.15\nRoBERTa 53.81 68.86 34.60 73.63 23.76 55.62\nL-RoBERTa 54.12 69.76 35.73 74.44 24.42 56.40\nLawformer 55.02 69.98 35.15 74.28 24.18 56.62\nTable 7: The results on the legal reading comprehen-\nsion task. Answer refers to the question answering task.\nSupport refers to the supporting sentences prediction\ntask. Joint refers to the overall performance.\n4.5 Legal Question Answering\nDataset: Legal question answering requires the\nmodel to understand legal knowledge and answer\nthe given questions. A high-quality legal ques-\ntion answering system can provide an accurate le-\ngal consult service. For this task, we select JEC-\nQA (Zhong et al., 2020b) to evaluate the perfor-\nmance of the proposed model. JEC-QA consists\nof 28, 641 multiple-choice questions from the Chi-\nnese national bar exam, which is quite challenging\nfor existing models (Zhong et al., 2020b).\nImplementation Details: We formalize the task\nas a text classiﬁcation task. First, we concatenate\nthe questions and candidate choices together to\nform the inputs of the models. Then a linear layer\nis applied to compute the matching scores of the\ncandidates. Previous works also take the reading\nmaterials retrieved by statistical methods (BM25,\nTFIDF) as inputs (Zhong et al., 2020b,a). We ig-\nnore these reading materials in our experiments, as\nthe retrieval results are unsatisfactory and would\nharm the performance. We set the learning rate as\n2×10−5 and the batch size as32. The positions are\nset as 0 for questions, and 1 for candidate choices.\ndev test\nModel single all single all\nBERT 42.78 25.77 41.23 24.50\nRoBERTa 44.46 28.18 43.18 27.09\nL-RoBERTa 45.17 27.54 43.29 27.81\nLawformer 45.81 27.21 45.81 27.43\nTable 8: The results on the legal question answering\ntask. We adopt accuracy as the evaluation metric. Here,\nsingle denotes the single-answer questions and all de-\nnotes all questions.\nResult: The results are shown in Table 8. As the in-\nputs in the dataset do not require long-distance un-\nderstanding, L-RoBERTa can achieve comparable\nresults with Lawformer. However, as the task needs\nthe models to perform complex reasoning (Zhong\net al., 2020b), all the models cannot achieve promis-\ning results. Therefore, it remains a great challenge\nfor future works to enhance the models with legal\nknowledge and the logic reasoning capacity.\n5 Conclusion and Future Work\nIn this paper, we pre-train a Longformer-based\nlanguage model with tens of millions of criminal\nand civil case documents, which is named as Law-\nformer. Then we evaluate Lawformer on several\ntypical LegalAI tasks, including legal judgment pre-\ndiction, similar case retrieval, legal reading compre-\nhension, and legal question answering. The results\ndemonstrate that Lawformer can achieve signiﬁ-\ncant performance improvement on tasks with long\nsequence inputs.\nThough Lawformer can achieve performance im-\nprovement for legal documents understanding, the\nexperimental results also show that the challenges\nstill exist. In the future, we will further explore the\nlegal knowledge augmented pre-training. It is an\nestablished fact that enhancing the models with le-\ngal knowledge is quite necessary for many LegalAI\ntasks (Zhong et al., 2020a).\nMeanwhile, we will also explore the generative\nlegal pre-trained model. In real-world legal prac-\ntice, the practitioners need to conduct heavy and\nredundant paper writing works. A powerful gen-\nerative legal pre-trained language model can be\nbeneﬁcial for legal professionals to improve work\nefﬁciency.\nTo summarize, we release Lawformer for legal\nlong document understanding in this paper. In the\nfuture, we will attempt to integrate legal knowledge\ninto the legal pre-trained language models, and pre-\ntrain generative models for LegalAI tasks.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of Clinical Nat-\nural Language Processing Workshop, pages 72–78.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientiﬁc text. In\nProceedings of EMNLP-IJCNLP, pages 3606–3611.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Proceedings of NeurIPS.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos\nAletras. 2019. Neural legal judgment prediction in\nEnglish. In Proceedings of ACL, pages 4317–4323.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert:“preparing the muppets for court’”.\nIn Proceedings of EMNLP: Findings , pages 2898–\n2904.\nYen-Liang Chen, Yi-Hung Liu, and Wu-Liang Ho.\n2013. A text mining approach to assist the general\npublic in the retrieval of legal documents. Journal\nof the American Society for Information Science and\nTechnology, 64(2):280–290.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for Chinese\nbert. arXiv preprint arXiv:1906.08101.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proceedings of ACL ,\npages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT , pages 4171–\n4186.\nXingyi Duan, Baoxin Wang, Ziyue Wang, Wentao Ma,\nYiming Cui, Dayong Wu, Shijin Wang, Ting Liu,\nTianxiang Huo, Zhen Hu, et al. 2019a. Cjrc: A re-\nliable human-annotated benchmark dataset for Chi-\nnese judicial reading comprehension. In Proceed-\nings of CCL, pages 439–451. Springer.\nXinyu Duan, Yating Zhang, Lin Yuan, Xin Zhou, Xi-\naozhong Liu, Tianyi Wang, Ruocheng Wang, Qiong\nZhang, Changlong Sun, and Fei Wu. 2019b. Le-\ngal summarization for multi-role debate dialogue via\ncontroversy focus mining and multi-task learning.\nIn Proceedings of CIKM, pages 1361–1370.\nEmad Elwany, Dave Moore, and Gaurav Oberoi. 2019.\nBert goes to law school: Quantifying the compet-\nitive advantage of access to large legal corpora in\ncontract understanding. In Proceedings of NeurIPS\nWorkshop on Document Intelligence.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL, pages 8342–8360.\nDan Hendrycks, Collin Burns, Anya Chen, and\nSpencer Ball. 2021. Cuad: An expert-annotated\nnlp dataset for legal contract review. arXiv preprint\narXiv:2103.06268.\nPhi Manh Kien, Ha-Thanh Nguyen, Ngo Xuan Bach,\nVu Tran, Minh Le Nguyen, and Tu Minh Phuong.\n2020. Answering legal questions by learning neu-\nral attentive text representation. In Proceedings of\nCOLING, pages 988–998.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nFred Kort. 1957. Predicting supreme court decisions\nmathematically: A quantitative analysis of the” right\nto counsel” cases. The American Political Science\nReview, 51(1):1–12.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinform., 36(4):1234–\n1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nBingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang,\nand Dongyan Zhao. 2017. Learning to predict\ncharges for criminal cases with legal basis. In Pro-\nceedings of EMNLP, pages 2727–2736.\nStuart S Nagel. 1963. Applying correlation analysis to\ncase prediction. Tex. L. Rev., 42:1006.\nA¨aron van den Oord, Sander Dieleman, Heiga Zen,\nKaren Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. 2016. Wavenet: A generative model\nfor raw audio. In Proceedings of ISCA Speech Syn-\nthesis Workshop, pages 125–125.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise self-\nattention for long document understanding. In Pro-\nceedings of EMNLP: Findings, pages 2555–2565.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research ,\n21:1–67.\nK Raghav, P Krishna Reddy, and V Balakista Reddy.\n2016. Analyzing the extraction of relevant legal\njudgments using paragraph-level and citation infor-\nmation. AI4JCArtiﬁcial Intelligence for Justice ,\npage 30.\nJeffrey A Segal. 1984. Predicting supreme court\ncases probabilistically: The search and seizure cases,\n1962-1981. The American political science review ,\npages 891–900.\nShohreh Shaghaghian, Luna Yue Feng, Borna Jafar-\npour, and Nicolai Pogrebnyakov. 2020. Customiz-\ning contextualized language models for legal docu-\nment reviews. In Proceedings of IEEE Big Data ,\npages 2139–2148. IEEE.\nYunqiu Shao, Jiaxin Mao, Yiqun Liu, Weizhi Ma, Ken\nSatoh, Min Zhang, and Shaoping Ma. 2020. Bert-\npli: Modeling paragraph-level interactions for legal\ncase retrieval. In Proceedings of IJCAI, pages 3501–\n3507.\nSainbayar Sukhbaatar, ´Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive atten-\ntion span in transformers. In Proceedings of ACL ,\npages 331–335.\nYiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu,\nChanglong Sun, Jun Xiao, Yueting Zhuang, Luo Si,\nand Fei Wu. 2020. De-biased court’s view gener-\nation with causality. In Proceedings of EMNLP ,\npages 763–780.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Yansong Feng,\nXianpei Han, Zhen Hu, Heng Wang, et al. 2018.\nCail2018: A large-scale legal dataset for judgment\nprediction. arXiv preprint arXiv:1807.02478.\nWenmian Yang, Weijia Jia, XIaojie Zhou, and Yutao\nLuo. 2019. Legal judgment prediction via multi-\nperspective bi-feedback network. In Proceedings of\nIJCAI.\nHai Ye, Xin Jiang, Zhunchen Luo, and Wenhan Chao.\n2018. Interpretable charge predictions for criminal\ncases: Learning to generate court views from fact\ndescriptions. In Proceedings of NAACL-HLT, pages\n1854–1864.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta˜n´on, Philip Pham, Anirudh Ravula, Qifan\nWang, Li Yang, and Amr Ahmed. 2020. Big bird:\nTransformers for longer sequences. In Proceedings\nof NeurIPS.\nHaoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun\nXiao, Zhiyuan Liu, and Maosong Sun. 2018. Le-\ngal judgment prediction via topological learning. In\nProceedings of EMNLP, pages 3540–3549.\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020a.\nHow does NLP beneﬁt legal system: A summary of\nlegal artiﬁcial intelligence. In Proceedings of ACL,\npages 5218–5230.\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020b. Jec-\nqa: A legal-domain question answering dataset. In\nProceedings of the AAAI , volume 34, pages 9701–\n9708.\nHaoxi Zhong, Zhengyan Zhang, Zhiyuan Liu, and\nMaosong Sun. 2019. Open Chinese language pre-\ntrained model zoo. Technical report.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7638362646102905
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.673069953918457
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6303853988647461
    },
    {
      "name": "Question answering",
      "score": 0.583380937576294
    },
    {
      "name": "Natural language processing",
      "score": 0.5760820508003235
    },
    {
      "name": "Process (computing)",
      "score": 0.5381764769554138
    },
    {
      "name": "Mainstream",
      "score": 0.48636725544929504
    },
    {
      "name": "Reading (process)",
      "score": 0.46634382009506226
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4610874056816101
    },
    {
      "name": "Natural language understanding",
      "score": 0.45041558146476746
    },
    {
      "name": "Language model",
      "score": 0.4260205030441284
    },
    {
      "name": "Natural language",
      "score": 0.35074448585510254
    },
    {
      "name": "Linguistics",
      "score": 0.19033923745155334
    },
    {
      "name": "Political science",
      "score": 0.07474496960639954
    },
    {
      "name": "Law",
      "score": 0.07336995005607605
    },
    {
      "name": "Programming language",
      "score": 0.06492078304290771
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    }
  ],
  "cited_by": 15
}