{
  "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
  "url": "https://openalex.org/W4385571232",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222756294",
      "name": "Cheng-Han Chiang",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2514219681",
      "name": "Hung-Yi Lee",
      "affiliations": [
        "National Taiwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4303648946",
    "https://openalex.org/W2250332012",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2990787438",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2970102799",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2842624112",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2250830017",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W4302287782",
    "https://openalex.org/W4285266418",
    "https://openalex.org/W3104208618",
    "https://openalex.org/W2158441347",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W2938704169"
  ],
  "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms.Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided.In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks.We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer.We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15607–15631\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCan Large Language Models Be an Alternative to Human Evaluation?\nCheng-Han Chiang\nNational Taiwan University,\nTaiwan\ndcml0714@gmail.com\nHung-yi Lee\nNational Taiwan University,\nTaiwan\nhungyilee@ntu.edu.tw\nAbstract\nHuman evaluation is indispensable and in-\nevitable for assessing the quality of texts gen-\nerated by machine learning models or written\nby humans. However, human evaluation is very\ndifficult to reproduce and its quality is noto-\nriously unstable, hindering fair comparisons\namong different natural language processing\n(NLP) models and algorithms. Recently, large\nlanguage models (LLMs) have demonstrated\nexceptional performance on unseen tasks when\nonly the task instructions are provided. In this\npaper, we explore if such an ability of the LLMs\ncan be used as an alternative to human evalu-\nation. We present the LLMs with the exact\nsame instructions, samples to be evaluated, and\nquestions used to conduct human evaluation,\nand then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation.\nWe use human evaluation and LLM evaluation\nto evaluate the texts in two NLP tasks: open-\nended story generation and adversarial attacks.\nWe show that the result of LLM evaluation is\nconsistent with the results obtained by expert\nhuman evaluation: the texts rated higher by hu-\nman experts are also rated higher by the LLMs.\nWe also find that the results of LLM evalua-\ntion are stable over different formatting of the\ntask instructions and the sampling algorithm\nused to generate the answer. We are the first to\nshow the potential of using LLMs to assess the\nquality of texts and discuss the limitations and\nethical considerations of LLM evaluation.\n1 Introduction\nHuman evaluation is an important method to under-\nstand the performance of an NLP model or algo-\nrithm (Guzmán et al., 2015; Gillick and Liu, 2010).\nWe rely on human evaluation because there are cer-\ntain aspects of texts that are hard to evaluate using\nautomatic evaluation metrics; thus, researchers re-\nsort to humans to rate the quality of the output of\nNLP models. While human evaluation is preva-\nlent and indispensable in NLP, it is notoriously\nunstable (Gillick and Liu, 2010; Clark et al., 2021).\nKarpinska et al. (2021) has shown that the quality\nof workforces in human evaluation can have a detri-\nmental effect on the evaluation result, making it\nimpossible to compare the performance among dif-\nferent systems. Reproducibility is another issue in\nhuman evaluation since it is hard to recruit the same\nhuman evaluators and rerun the same evaluation.\nEven if the same workers are recruited, the workers\nthat have seen the task before are likely to produce\na different evaluation result the next time because\nthey have already done the task. While human eval-\nuation is used to better assess NLP systems and has\nsome advantages over automatic evaluation met-\nrics, the drawbacks of human evaluation somewhat\nmake it difficult to reliably evaluate NLP systems.\nTo resolve some of the drawbacks, we take ad-\nvantage of large language models (LLMs). LLMs\nare large models that are trained to model human\nlanguages using self-supervised learning (Brown\net al., 2020) and further using special training pro-\ncedures to improve the performance on unseen\ntasks and better follow natural language instruc-\ntions (Sanh et al., 2022; Wei et al., 2022). The\nability to perform a task just given the task instruc-\ntions motivates us to ask if these LLMs can perform\nwhat humans do in human evaluation. To answer\nthis question, we feed in the LLM with the same\ninstruction, sample, and question used in human\nevaluation, and take the sequences generated by the\nLLM as the LLM’s answer to the question. This\nprocess is shown in Figure 1, and we call this pro-\ncess LLM evaluation.\nTo test if LLM evaluation yields meaningful re-\nsults, we conduct LLM evaluation on two different\nNLP tasks: evaluating the quality of stories in open-\nended story generation and the quality of sentences\ngenerated by adversarial attacks. We summarize\nour findings and contribution as follows:\n• We show that LLM evaluation produces re-\nsults similar to expert human evaluation, ver-\n15607\n1 (lowest)\n2\n3\n4\n5 (highest)\nHuman responsesPlease rate the story fragment\nThe goal of this task is to rate story\nfragments. \nNOTE: Please take the time\nto fully read and understand the story\nfragment. We will reject submissions from\nworkers that are clearly spamming the\ntask.\nStory fragment\nThe human ambassador reached down and\ngrasped it's paw. \"Humans, cats, is it\ntrue that all intelligent beings are\nomnivorous?\" \"Certainly, your rank\ndemeanour can be demonstrated from the\nwords we spoke to the Kelpie. They're of\nno concern to us humans, as they are\nnot considered to live among us, thus\nfar. (…)\nHow grammatically correct is the text of\nthe story fragment? (on a scale of 1-5,\nwith 1 being the lowest?)\nTask instruction, sample, and question\nHuman evaluators\nHuman evaluation\nI would rate the\ngrammar in the text\nof the story\nfragment as a 4.\nThere are no major\ngrammatical errors\nor issues with\npunctuation,(…)\nLLM output\n4\nLLM evaluation\nLLM (e.g., GPT3)\nFigure 1: Illustration of the core idea of the paper using open-ended story generation as the example task. The left\npart shows the instruction, story fragments, and questions used in human evaluation. The human experts are asked\nto rate the quality of the story fragments using a 5-point Likert scale, shown on the upper right. The lower right part\nshows the process of LLM evaluation, where we feed the LLMs the same instruction, story fragments, and questions\nand parse the LLM-generated output to get the rating.\nifying the effectiveness of LLM evaluation\n(§3.3 and §4.3). This paper is the first to pro-\npose using LLMs as an alternative to human\nevaluation and show their effectiveness.\n• We show that LLM evaluation results only\nslightly vary due to different task instructions\nand the hyperparameters of the sampling al-\ngorithm used to generate the answer. (§3.3.2\nand §3.3.3)\n• We carefully discuss the pros and cons of us-\ning LLM evaluation and discuss the ethical\nconsiderations of LLM evaluation. (§5)\n2 LLM Evaluation\n2.1 Large Language Models (LLMs)\nLarge language models are language models hav-\ning bulk parameter sizes, typically on the scale\nof a few billion, and pre-trained on enormous\namounts of natural language corpora, including\nGPT3 (Brown et al., 2020), T5 (Raffel et al., 2020),\nand BLOOM (Scao et al., 2022). These LLMs\nshow exceptional performance on unseen tasks\nwhen only the task instructions are given; this kind\nof ability is called zero-shot in-context learning.\nTo further improve the zero-shot in-context learn-\ning performance, special training techniques have\nbeen applied to those LLMs after pre-training. For\nexample, T0 (Sanh et al., 2022) and FLAN (Wei\net al., 2022) are fine-tuned on a mixture of tasks\nand can thus achieve better zero-shot performance\ncompared to GPT-3. InstructGPT (Ouyang et al.,\n2022) is fine-tuned from GPT-3 using reinforce-\nment learning from human feedback (RLHF), and\nit is shown to better follow the instructions. Chat-\nGPT (OpenAI, 2022) is fine-tuned from Instruct-\nGPT with a conversation dataset using RLHF, so\nChatGPT can interact with users in a conversational\nway. ChatGPT is able to answer questions asked\nby the user and provide comprehensive explana-\ntions about its answer. Given the LLMs’ ability to\nfollow task instructions and provide feedback, we\nask whether LLMs can be used as an alternative\nto human evaluation and aid NLP researchers in\nevaluating the quality of texts.\n2.2 LLM Evaluation\nTo evaluate the quality of texts generated by NLP\nsystems or written by humans using LLM, we\npresent the LLMs with the task instructions, the\nsample to be evaluated, and a question. The ques-\ntion asks the LLM to rate the sample’s quality using\na 5-point Likert scale. Given the inputs, the LLM\nwill answer the question by generating some out-\nput sentences. We parse the output sentences to get\n15608\nthe score rated by the LLM. We call this process\nLLM evaluation, and this procedure is shown in the\nlower part of Figure 1. Different tasks use different\nsets of task instructions, and each task uses differ-\nent questions to evaluate the quality of the samples.\nThe instructions and questions used in LLM evalua-\ntion in our paper are not tailored for the LLMs; we\nfollow those instructions used to conduct human\nevaluation in prior works.\nTo compare the result of LLM evaluation and\nshow its effectiveness, we compare the result of\nLLM evaluation with human evaluation conducted\nby English teachers. To make a fair and meaningful\ncomparison, the instructions, samples, and ques-\ntions in human evaluation are formatted similarly\nto those in LLM evaluation. The main difference\nbetween LLM evaluation and human evaluation\nis that in human evaluation, the human evaluators\nanswer the question by choosing the answer from\na pre-defined set of options (the 1-5 Likert scale\nscores), as shown in the upper right in Figure 1.\nIn LLM evaluation, we instead let the LLM freely\ngenerate sentences and extract the score from the\ngenerated sentences using some simple rules, de-\ntailed in Appendix D.2.1.\n3 Example Task 1: Open-Ended Story\nGeneration\nWe first use open-ended story generation to demon-\nstrate the usefulness of LLM evaluation.\n3.1 Task Introduction\nOpen-ended story generation is a task to gener-\nate a short story based on a given prompt. We\nuse the WritingPrompts dataset (Fan et al., 2018),\nwhich is composed of pairs of short prompts and\nhuman-written stories collected from the subred-\ndit WritingPrompts. In the WritingPrompts, the\nusers are given a short prompt, and they need to\nwrite a story based on the short prompt.1\nIn this experiment, we use LLM evaluation and\nhuman evaluation to rate the stories generated by\nhumans and the stories generated by a story gen-\neration model. We select open-ended story gen-\neration as an example because Karpinska et al.\n(2021) show that workers from Amazon Mechani-\ncal Turk (AMT) cannot distinguish GPT-2 (Radford\net al., 2019) generated and human-written stories,\n1The WritingPrompts subreddit explicitly forbids the\nusers to use AI for generating stories, so we consider the\nstories in the dataset to be human-written.\nwhile English teachers show a clear preference for\nhuman-written stories over GPT-2-generated sto-\nries. We want to see if LLM can rate human-written\nstories higher than GPT-2-generated ones.\nFollowing prior works (Mao et al., 2019; Guan\net al., 2020; Karpinska et al., 2021), the story gen-\neration model is GPT-2 medium model fine-tuned\non the WritingPrompts training dataset. After the\nmodel is trained, we randomly select 200 prompts\nfrom the testing set of WritingPrompts and make\nthe fine-tuned GPT-2 generate stories based on\nthose prompts using nucleus sampling (Holtzman\net al., 2020) with p= 0.9. For the human-written\nstories to be compared, we use the 200 stories\nwritten based on the same 200 prompts. We post-\nprocess the human-written and GPT-2-generated\nstories and then use them for LLM evaluation and\nhuman evaluation. Please find the details on fine-\ntuning and data processing in Appendix B.\n3.2 LLM Evaluation and Human Evaluation\nWe present the LLMs and the human evaluators\nwith a short description, and the story to be eval-\nuated, formatted as shown in Figure 1. Follow-\ning Karpinska et al. (2021), we evaluate the stories\non four different attributes. The four attributes and\ntheir corresponding questions are as follows:\n1. Grammaticality: How grammatically cor-\nrect is the text of the story fragment?\n2. Cohesiveness: How well do the sentences in\nthe story fragment fit together?\n3. Likability: How enjoyable do you find the\nstory fragment?\n4. Relevance: Now read the PROMPT based on\nwhich the story fragment was written.\nPrompt: [PROMPT].\nHow relevant is the story fragment to the\nprompt?\nWhere the [PROMPT] will be filled in with the\nprompt which the story is based on. Each attribute\nis evaluated using a 5-point Likert scale; the fol-\nlowing description is appended at the end of each\nquestion: \"(on a scale of 1-5, with 1 being the low-\nest)\". We show the interface used in human evalua-\ntion and the input format for the LLM evaluation\nin Appendix C.2 and D.2.2.\nThe LLMs used for LLM evaluation include T0,\ntext-curie-001, text-davinci-003, and Chat-\nGPT. text-curie-001 and text-davinci-003\n15609\nEvaluator Grammaticality Cohesiveness Likability Relevance\nMeanSTD IAA% MeanSTD IAA% MeanSTD IAA% MeanSTD IAA%\nHuman-written stories\nHuman 3.760.95 0.3320.5 4.290.82 0.3227 3.781.10 0.089.5 3.351.48 0.058\nT0 2.551.47 0.1610 2.981.45 0.114 3.181.53 0.127 2.931.64 0.026\ncurie 3.190.47 0.0746.5 2.820.46 0.0147.5 2.850.37 0.110.65 3.060.40 0.110.64\ndavinci 4.220.38 0.2635 4.540.47 0.3739.5 3.990.38 0.4968.5 4.400.79 0.7148.5\nChatGPT 3.830.60 3.550.88 2.440.89 3.291.50\nGPT-2-generated stories\nHuman 3.560.91 0.1019.5 3.191.07 0.1417 2.591.29 −0.213.5 2.381.40 −0.038.5\nT0 2.441.49 0.059 3.021.51 0.076 3.001.59 0.166 2.821.61 0.046\ncurie 3.230.51 0.0138 2.820.45 0.0250 2.860.37 0.0965.5 3.010.43 0.1161\ndavinci 4.070.35 0.3545.5 4.260.45 0.4242 3.840.42 0.5262 4.020.74 0.6942.5\nChatGPT 2.980.76 2.480.71 1.590.67 2.021.21\nTable 1: LLM evaluation and human evaluation results of human-written stories and GPT-2-generated stories. For\neach evaluated attribute, we report its mean Likert scale and the standard deviation. We also report the inter-annotator\nagreement (IAA) among three annotators using Krippendorff’s α. The subscript in the IAA column (%) is used to\ndenote the percentage of the stories where all three annotators exactly agree on a rating.\nare two InstructGPT models, and the latter is the\nstronger model; we will use InstructGPT to refer to\nthese two models. We query the InstructGPT using\nthe official API provided by OpenAI. We use nu-\ncleus sampling with p= 0.9 to generate the answer\nfrom T0 and InstructGPTs. We sample three an-\nswers from LLMs to stimulate the result of asking\nthe model to rate the same story three times. We\nquery ChatGPT using the user interface recently\nreleased by OpenAI. Unlike InstructGPT, we can-\nnot control the parameters used for generating the\nresponse from ChatGPT. Because ChatGPT limits\nthe maximum number of queries per user, we only\nsample one response for each question.\nFor human evaluation, we do not use the\ncommonly used AMT for human evaluation be-\ncause Karpinska et al. (2021) has already shown\nthat the results obtained using AMT are highly\nquestionable. Following the recommendation of\nthe prior works, we hire three certified English\nteachers using an online freelancer platform, Up-\nWork. Teachers are familiar with evaluating the\nessays of students, making them the expert evalua-\ntors in our task. The details about recruiting human\nevaluators are in Appendix C.1. Each LLM and\neach English teacher rates the 200 human-written\nstories and 200 GPT-2-generated stories.\n3.3 Experiment Results\nThe LLM evaluation and human evaluation results\nof open-ended story generation are presented in\nTable 1. We report the mean and standard deviation\nof the Likert scores obtained from LLM evaluation\nand human evaluation and show the inter-annotator\nagreement (IAA) using two different metrics: (1)\nthe Krippendorff’s α, and (2) the percentage of the\nstories where three evaluators give the exact same\nrating.2 The main observations from Table 1 are\ndiscussed as follows.\nExpert human evaluators prefer human-\nwritten stories: Human evaluation result serves as\nsome kind of ground truth of the LLM evaluation.\nFor all four attributes, teachers rate the human-\nwritten stories higher than GPT-2-generated stories.\nThis indicates that experts are able to distinguish\nthe quality difference between model-generated\nstories and human-written stories. Based on the\nIAA, we also find that the agreements among ex-\nperts are lower on GPT-2-generated texts and on\nthe likability. This shows that experts tend to have\nless agreement on model-generated texts and on a\nsubjective attribute (likability), agreeing with the\nresults in Karpinska et al. (2021).\nT0 and text-curie-001 do not show clear\npreference toward human-written stories: For\nT0, we can see that T0 rates human-written sto-\nries higher than GPT-2-generated stories on gram-\nmatically, likability, and relevance. However, the\nrating differences between the human-written and\n2The three evaluators in human evaluation are the three\nEnglish teachers. In LLM evaluation, we sample the answer\ngenerated by LLM three times as an analogy to three different\nevaluators.\n15610\nmodel-generated stories do not achieve statistical\nsignificance for grammaticality and relevance; the\np-value obtained by Welch’st-test is much larger\nthan 0.05. The result of text-curie-001 is simi-\nlar to T0: text-curie-001 does not rate human-\nwritten stories higher than model-generated stories.\nIt can also be observed that for T0, the IAA in terms\nof the percentage of exact agreement among three\ndifferent sampled answers is overall very low. This\nindicates that given the same sample, T0 is likely\nto give a different rating for the three sampled an-\nswers. The result implies that T0 does not assign\na high probability to a specific rating, so different\nscores are all likely to be sampled. This shows that\neven if LLMs are specifically fine-tuned to better\nperform zero-shot in-context learning and trained\nto better follow human instructions, these do not\nmake them capable of assessing open-ended story\ngeneration as human experts can.\ntext-davinci-003 shows clear preference\ntoward human-written stories just like En-\nglish teachers: text-davinci-003 rates human-\nwritten stories much higher than model-generated\nstories on all four attributes, which is in accor-\ndance with the result produced by human ex-\nperts. By Welch’s t-test, we find that the higher\nratings on human-written stories are all statisti-\ncally significant. In prior work, researchers have\nfound that workers recruited on AMT do not rate\nhuman-written stories higher than GPT-2-generated\nones (Karpinska et al., 2021); combining their re-\nsult with our result, we can see that LLM evaluation\nusing text-davinci-003 yields more convincing\nresults than using human evaluation on AMT for\nopen-ended story generation. The results show\nthat text-davinci-003 can perform basic evalu-\nations such as checking for grammatical errors in\nstories. Additionally, the model excels in assessing\nthe relevance of a story to a prompt, which involves\nmore complex reasoning over the connection be-\ntween the two. We also find the Krippendorff’s\nαof text-davinci-003 is much higher than T0\nand text-curie-001, indicating that the rating by\ntext-davinci-003 is more consistent among dif-\nferent samplings of the generated answers.\nChatGPT rates like human experts and can\nexplain its own decision well: ChatGPT also\nshows a clear preference for human-written stories,\nand the preference toward human written-stories is\nstatistically significant. When we query ChatGPT\nusing the OpenAI user interface, we find several\ninteresting observations: (1): ChatGPT is able to\nprovide a detailed explanation of why it gives a\ncertain rating. It will reference the sentences in\nthe stories and prompts to support its rating. (2):\nChatGPT sometimes refuses to rate the likability of\nthe story because \"I am an AI and I do not have the\nability to experience enjoyment\". In such cases, we\nregenerate the response until it gives a rating. (3):\nwe find that ChatGPT tends to rate low likability on\nviolent or impolite stories, which is likely because\nit is trained to provide safe and unharmful replies,\nmaking ChatGPT dislike brutal and profane stories.\nExperts mostly agree with the ratings and ex-\nplanations of ChatGPT: We randomly select the\nanswers on four stories by ChatGPT and ask the\nEnglish teachers if they agree with the reasoning\nand rating of ChatGPT3. The teachers mostly agree\nwith the rating and consider the explanation from\nChatGPT reasonable. Interestingly, one teacher\ntold us she cannot agree with ChatGPT’s rating on\ngrammaticality because ChatGPT considers punc-\ntuation errors as grammar errors, but she does not\nthink punctuation errors are grammar errors. This\nshows that individuals have their own standards for\nratings and this is also the case for LLMs.\ntext-davinci-003 tends to give higher rat-\nings and ChatGPT is the opposite: The rating on\nthe same attribute of the same type of text tends to\nbe higher for text-davinci-003 compared with\nhuman rating; contrarily, ChatGPT is more fastid-\nious and prone to give lower scores. This shows\nthat different LLMs have distinct tendencies re-\ngarding the rating. While the absolute values of\nthe scores rated by text-davinci-003, ChatGPT,\nand human differ, they all rate human-written texts\nhigher than GPT-2-generated stories. The absolute\nnumber reflects the bias or belief of the evaluator;\nas long as one uses the same evaluators to assess\ndifferent systems, the comparison is meaningful.\n3.3.1 Does LLM and Human Evaluators\nAgree on the Rating of Individual\nStories?\nWe have found in Table 1 that the ratings of\ntext-davinci-003 and ChatGPT show a strong\npreference toward human-written stories just like\nEnglish teachers. However, it is unclear whether\nthose LLMs agree with the teachers’ rating on each\nindividual story. Precisely, when English teach-\ners rate a story higher, do LLMs also rate the\n3We do not tell the teachers these are responses from an AI\nmodel. See the stories and teachers’ replies in Appendix C.3.2.\n15611\nStory Writer Human GPT-2\nGrammaticality 0.14 0.12\nCohesiveness 0.18 0.14\nLikability 0.19 0.22\nRelevance 0.38 0.43\nTable 2: The Kendall’sτ correlation coefficient between\nEnglish teachers and text-davinci-003.\nstory higher? To answer this question, we calcu-\nlate Kendall’s τ correlation coefficient between the\nratings of text-davinci-003 and English teach-\ners. We choose to use the correlation coefficient\ninstead of the inter-annotator agreement score be-\ncause IAA mainly cares if two annotators agree\non the exact ratings, while the correlation coef-\nficient focus on the question: \"when annotator\nA rates one story higher, does annotator B also\nrate the story higher?\" (Amidei et al., 2019). We\ncalculate Kendall’s τ for four rating attributes as\nfollows: For each story and each rating attribute,\nwe calculate the average rating of the three En-\nglish teachers and calculate the average rating of\nthe three scores given by the text-davinci-003\n(which is obtained from three independent sam-\nples). For each attribute, we collect the average\nrating of teachers into a vector A ∈R200, where\neach entry is the average rating of a story; likewise,\nwe construct a vector B ∈R200 for the average\nratings of davinci. Next, we calculate Kendall’s\nτ correlation coefficient between Aand B.\nThe Kendall’s τ between teacher ratings and\nLLM ratings is shown in Table 2. 4 We find that\nfor all four attributes and for both human-written\nand GPT-2-generated stories, we observe weak\nto strong positive correlations between teachers’\nratings and text-davinci-003’s ratings. All the\ncorrelations have p-values less than 0.05. Hence,\nwe can say that when teachers rate a story higher,\ntext-davinci-003 also rates it higher to a certain\nextent. We also observe that Kendall’sτ for differ-\nent attributes are quite different: relevance has the\nstrongest correlation while grammaticality has the\nweakest correlation. This is possibly because rating\nrelevance is rather straightforward, which requires\nchecking if the content in the prompt is mentioned\nin the story. On the contrary, what should be con-\n4When interpreting Kendall’s τ, |τ| ∈[0,0.1) is consid-\nered as very weak correlation, |τ| ∈[0.1,0.2) is considered\nas weak correlation, |τ| ∈[0.2,0.3) is considered as moder-\nate correlation, and |τ| ∈[0.3,1.0] is considered as strong\ncorrelation (Botsch, 2011).\nsidered when rating grammaticality is not clearly\nstated in our instructions, so the LLM may have\na different rubric compared with English teachers.\nWe also calculate the average Kendall’sτ between\na pair of English teachers, and we find a weak cor-\nrelation on grammaticality between the rating of\ntwo teachers, while the correlation of the rating on\nrelevance is much stronger. The result is presented\nin Table 6 in Appendix.\n3.3.2 Variance due to Different Instructions\nLLMs have been shown to be sensitive to the in-\nstructions used to query the LLM sometimes (Zhao\net al., 2021; Sanh et al., 2022). To investigate\nhow varying the task instructions and questions can\naffect the LLM evaluation result for open-ended\nstory generation, we change the instructions and\nquestions and see how the LLM evaluation result\nchanges. We experiment with two different instruc-\ntions by changing the instruction or question in\nFigure 1: (1) We prepend the sentence, \"(You are a\nhuman worker hired to rate the story fragment.)\",\nin front of the task instruction in Figure 1. We\ntry to provide the LLM a persona for it to better\nunderstand its role. This is inspired by previous\nwork that reported GPT-3 can yield different results\nwhen giving them a persona (Zeng et al., 2022). (2)\nWe ask the LLMs to explain their decision by ap-\npending the following sentence after the question:\nPlease also explain your decision. Here, we would\nlike to know if LLM will rate the stories differently\nwhen they are asked to justify their decision. This\nis inspired by zero-shot chain-of-thought (Kojima\net al.). We use text-davinci-003 instead of Chat-\nGPT as the LLM in this experiment since it is more\naccessible than ChatGPT.\nThe results are shown in the upper block in Ta-\nble 3. We observe that for grammaticality and\ncohesiveness, the scores obtained from different in-\nstructions are quite close: the rating changes due to\ndifferent instructions are less than0.1. For the other\ntwo attributes, the score changes are slightly larger\nbut still in the range of 0.25. Despite that there\nare small variations due to different instructions,\nthese variances still do not change the conclusion\nthat \"LLM rates human-written stories higher than\nGPT-2-generated stories\". Thus, different instruc-\ntions do not change the relative ranking of GPT-2-\ngenerated and human-written stories. In summary,\nas long as the stories are evaluated using the same\ninstructions using LLM evaluation, such evaluation\nand comparison are meaningful.\n15612\nSetup Grammaticality Cohesiveness Likability Relevance\nHuman GPT-2 Human GPT-2 Human GPT-2 Human GPT-2\nDifferent instructions (Section 3.3.2)\nOriginal 4.220.38 4.070.35 4.540.45 4.260.45 3.990.38 3.840.42 4.400.79 4.020.74\n(1) + persona 4.290.45 4.010.45 4.600.49 4.270.50 4.050.39 3.870.39 4.550.70 4.250.77\n(2) + explain 4.240.42 4.050.25 4.610.49 4.320.51 4.150.44 3.980.34 4.350.75 4.030.56\nDifferent sampling temperature T (Section 3.3.3)\nT = 1.0 4 .220.38 4.070.35 4.540.45 4.260.45 3.990.38 3.840.42 4.400.79 4.020.74\nT = 0.7 4 .180.35 4.060.33 4.520.48 4.230.43 3.960.34 3.820.42 4.360.77 3.950.72\nT = 0.3 4 .130.33 3.990.25 4.480.49 4.140.39 3.950.26 3.820.41 4.340.75 3.930.67\nT = 0 4 .070.27 3.990.18 4.490.50 4.090.34 3.950.25 3.820.40 4.320.75 3.920.66\nTable 3: Understanding the variance of LLM evaluation. For each of the four attributes evaluated, the left column is\nthe mean and standard deviation of human-written stories and the right column is those of GPT-2-generated stories.\nThe upper block shows the rating change due to different instructions (Section 3.3.2), and the lower block is the\nresult of changing the temperature T used for generating the LLM’s output (Section 3.3.3).\n3.3.3 Variance due to Different Sampling\nParameters\nWhen generating the answers from the LLM, we\nmust choose a set of hyperparameters for genera-\ntion, including the temperature T and the proba-\nbility pused in nucleus sampling. To understand\nwhether different sampling parameters change the\nLLM evaluation result, we modify the tempera-\nture used for sampling and keep the pin nucleus\nsampling fixed to 0.9 when generating the answers\nfrom text-davinci-003. We do not simultane-\nously vary T and psince the two parameters are\nboth used to control the diversity of the output, it is\nenough to change only one of the two parameters,\nas recommended in the API documentation.\nThe results of varying T from 1 to 0 are shown\nin the lower block in Table 3. We observe an inter-\nesting trend as T varies from 1 to 0: the average\nrating slightly drops in most cases. Considering\nthat T = 0is simply argmax sampling, the result in-\ndicates that the response of the LLM with the high-\nest probability tends to give lower scores. Despite\nthis interesting trend, the LLM consistently rates\nhuman-written stories higher than GPT-2-generated\nstories. While not shown in Table 3, we find that\nthe IAA increases as the temperature decreases.\nThis is expected since lower temperature means\nless diversity during the LLM sampling, causing\nthe sampled ratings to agree more closely. In sum-\nmary, changing the instructions and temperatures\ncan slightly change the absolute value of the rating\ngiven by LLM but does not change the LLM’s pref-\nerence on human-written stories. The overall result\nin this section shows that LLM evaluation is useful\nin evaluating open-ended story generation.\n4 Example Task 2: Adversarial Attack\nAs another application, we use LLM evaluation to\nrate the texts generated by adversarial attacks.\n4.1 Task Introduction\nGiven a trained text classifier and a benign (non-\nadversarial) testing sample that the text classifier\ncan correctly classify, an adversarial attack aims to\ncraft an adversarial sample that makes the classifier\nmake a wrong prediction. A special type of adver-\nsarial attack is called synonym substitution attacks\n(SSAs) (Alzantot et al., 2018), where the adversar-\nial sample is created by replacing some words with\ntheir synonyms in the benign sample. By replacing\nwords with their synonym, the semantics of the\nbenign sample should be preserved in the adversar-\nial sample and make the adversarial perturbation\nimperceptible to humans. While conceptually rea-\nsonable, it has recently been shown that many SSAs\noften yield ungrammatical and unnatural adversar-\nial samples that significantly change the meaning\nof the benign sample (Hauser et al., 2021; Chiang\nand Lee, 2022). To evaluate the quality of adver-\nsarial samples, human evaluation is invaluable and\nwidely used in prior works. In our experiment here,\nwe would like to see whether the LLMs can rate\nthe quality of adversarial samples like human ex-\nperts. Adversarial samples are not normal texts, so\nthe LLMs may not have seen such abnormal inputs\nduring training. It would be interesting to know\nhow LLMs rate these adversarial samples.\n15613\nHuman evaluate LLM evaluate\nFluent Mean. Fluent Mean.\nBenign 4.55 - 4.32 5 .00†\nTextfooler 2.17 1 .88 2 .12 2 .06\nPWWS 2.16 1 .85 2 .42 2 .49\nBAE 3.01 3 .02 3 .71 3 .71\nTable 4: Mean Likert score of LLM evaluation and\nhuman evaluation result on fluency (Fluent) of the be-\nnign and adversarial samples and meaning preserving\n(Mean.) between the news title before and after adver-\nsarial attacks.\n4.2 Experiment Setup\nWe select three different classic SSAs:\nTextfooler (Jin et al., 2020), PWWS (Ren\net al., 2019), and BAE (Garg and Ramakrishnan,\n2020); these attacks are predominantly used\nas strong baselines in the literature of SSAs\nnowadays. We use these three SSAs to attack a\nBERT-base-uncased model (Devlin et al., 2019)\nfine-tuned on AG-News (Zhang et al., 2015), a\nnews classification dataset. For each SSA, we\nrandomly select 100 pairs of benign and adversarial\nsamples and use LLMs to evaluate their quality.\nWe show the result of using ChatGPT as LLM here\nsince it can better explain its decision. Following\nthe suggestions of prior works (Morris et al.,\n2020), we evaluate the quality of the adversarial\nsamples from two aspects: the fluency and meaning\npreservation. For fluency, we present the LLM\nwith a piece of news (either benign or adversarial\nsample) and the following question: How natural\nand fluent is the text of the news title? (on a scale\nof 1-5, with 1 being the lowest) . For meaning\npreserving, we present the LLM with both the\nbenign and the adversarial sample, and prompt\nthe LLM to answer this question: Do you agree\nthat the meaning (or semantics) of news title 1 is\npreserved in news title 2? (on a scale of 1-5, with 1\nbeing the strongly disagree and 5 being strongly\nagree.) The exact instruction and formatting are\npresented in Appendix D.2.3. We also ask three\nEnglish teachers to rate the fluency and meaning\npreserving of the samples. The task instructions\nand questions are formatted the same as in LLM\nevaluation.\n4.3 Experiment Result\nThe results are presented in Table 4. We can see\nthat English teachers rate the adversarial samples\ngenerated by SSAs very low in terms of fluency and\nmeaning preserving, this result is in line with re-\ncent observations on the quality of adversarial sam-\nples (Hauser et al., 2021; Chiang and Lee, 2022).\nBefore interpreting the result of LLM evaluation,\nwe first conduct a sanity check on whether the LLM\nunderstands the task. We ask the LLM to rate the\nmeaning preserving of two benign samples that are\nexactly the same. Ideally, the LLM should always\ngive a score of 5, meaning that it strongly agrees\nthat the meanings are not changed. The result of\nthis sanity check is the entry with †in Table 4,\nwhich is a perfect 5.00. ChatGPT often says that\n\"the two titles are identical so I rate a 5 (strongly\nagree)\", showing that ChatGPT understands what\nthe task is about.\nNext, we turn our attention to the LLM evalua-\ntion results of the adversarial samples. We observe\nthat ChatGPT tends to rate adversarial samples\nhigher than English teachers, meaning that Chat-\nGPT is less harsh on the unnatural and artificial\nparts in the adversarial samples. We conduct the\nsame experiment using text-davinci-003 and\nfind similar results. Although ChatGPT rates adver-\nsarial samples higher than the teachers, ChatGPT\nstill rates adversarial samples significantly lower\nthan benign samples. ChatGPT also agrees with the\nEnglish teachers that the adversarial samples gener-\nated by BAE are better than the samples generated\nby Textfooler and PWWS.\nInterestingly, we find that ChatGPT rates PWWS\nto be more natural than Textfooler, while such a\nrating difference is not seen in the expert human\nevaluation. At first sight, this means that ChatGPT\nis inconsistent with human evaluation results. How-\never, by scrutinizing the human evaluation results,\nwe find that two teachers rate PWWS higher than\nTextfooler while one teacher rates PWWS much\nlower than Textfooler. This indicates that ChatGPT\nactually agrees with the majority of human experts.\nOverall, LLM can rank the quality of adversarial\ntexts and benign texts like most human experts.\n5 Discussions\nIn this paper, we propose to use LLM for evaluating\nthe quality of texts to serve as an alternative to\nhuman evaluation. To demonstrate the potential of\nLLM evaluation, we use LLMs to rate the quality\nof texts in two distinct tasks: open-ended story\ngeneration and adversarial attacks. We show that\neven if LLMs have exceptional zero-shot in-context\n15614\nlearning ability, they are not always suitable to be\nused for LLM evaluation. Still, we find that the\nbest InstructGPT and ChatGPT can rate the quality\nof texts like human experts on the two tasks we\nused as examples. Overall, the results in this paper\ndemonstrate that LLM evaluation has the potential\nto be used to evaluate NLP systems and algorithms.\nPros of LLM evaluation There are several bene-\nfits of LLM evaluation, compared to human evalua-\ntion. First, LLM evaluation is more reproducible.\nHuman evaluation results are hard to reproduce as\nit is difficult to hire the same group of evaluators,\nand it is hard to compare the results of similar ex-\nperiments even if they use the same instructions, re-\ncruitment platform, and qualifications for the eval-\nuators. On the contrary, LLM evaluation does not\nhave such a drawback. By specifying the model\nused for LLM evaluation, the random seed, and\nthe hyperparameters used to generate the answers\nfrom the LLM, the LLM evaluation result is more\nlikely to be reproduced. Note that in certain cases,\nthe LLM provider may regularly update the LLM,\nmaking the LLM evaluation unreproducible if the\nLLM is outdated and not accessible.\nSecond, the evaluation of each sample is inde-\npendent of each other in LLM evaluation. Con-\ntrarily, in human evaluation, the rating of the cur-\nrent example may more or less be affected by prior\nsamples. Humans tend to compare the current sam-\nple to the ones they have previously seen and this\naffects their ratings. As a piece of evidence, in the\ninterview after rating the 400 stories, the English\nteachers say it took them some time to calibrate\ntheir ratings (Appendix C.3.1). Thus, using LLM\nevaluation can simplify some experiment designs\nsince one does not need to worry whether the order\nof the sample being evaluated will change the result.\nStill, one may also argue that being able to calibrate\nthe rating of different samples is desired and this\nis why human evaluation might be preferred. Over-\nall, whether the rating of the evaluator (human or\nLLM) should be affected by a previously rated item\nis inherently a design choice of the experiment.\nThird, LLM evaluation is cheaper and faster\nthan human evaluation, making it easier and\nquicker for researchers to evaluate the quality of\nNLP systems. Hiring an English teacher to rate\n200 stories costs us US$140, while LLM evalua-\ntion using the best InstructGPT model costs less\nthan US$5. It took us over a week to collect human\nevaluation results starting from recruitment to col-\nlecting the evaluation results, but only a few hours\nto query InstructGPT and perform LLM evaluation.\nFinally, utilizing LLM evaluation, rather than\nhuman evaluation, can minimize the need for hu-\nman exposure to objectionable content, such as\nviolent, sexual, hateful, or biased material. Such\ncontent may cause discomfort for human evaluators\nwhile reading and rating these texts. 5\nLimitations and Ethical Considerations of LLM\nevaluation Despite the promising results of LLM\nevaluation shown in this paper, there are some lim-\nitations of this method. First, LLM may possess\nincorrect factual knowledge (Cao et al., 2021), so it\nis not suitable to use them in tasks that involve fac-\ntual knowledge. Next, LLMs trained to behave in a\ncertain way can be biased toward certain responses.\nPrecisely, an LLM that is trained to be safe and\nnon-harmful can result in LLMs preferring to gen-\nerate more positive and upbeat responses, which is\nobserved throughout our interaction with ChatGPT.\nAdditionally, even with researchers’ efforts to make\nLLMs safer (Bai et al., 2022a,b), LLMs can still\ngenerate harmful and biased responses (Ganguli\net al., 2022; Perez et al., 2022), which are violative\nof basic ethics, and LLM evaluation results will be\nhighly doubtful (Hendrycks et al., 2021). However,\nit is important to note that these limitations and po-\ntential harms also apply to human evaluation: the\nbias of human evaluators can affect the human eval-\nuation result (Lentz and De Jong, 1997; Amidei\net al., 2018).\nOur pioneering idea, LLM evaluation, has the\npotential to transform the NLP community. 6 We\nencourage future researchers to consider using it\nwhile being aware of its limitations. Our paper’s\ngoal is not to replace human evaluation but to\npresent an alternative option. Both human and\nLLM evaluation have their own advantages and dis-\nadvantages, and they can be used in conjunction.\nWe recommend using LLM evaluation as a cheap\nand fast quality judgment when developing a new\nNLP system, while human evaluation is best used\nto collect feedback from humans prior to deploying\nthe NLP system in real-world applications.\n5It should be noted that the LLM may decline to assess cer-\ntain inputs that violate the content policy of the LLM provider.\n6We say thatwe are the first to propose this ideasince when\nwe submitted this paper to ACL 2023 on January 13, 2023,\nwe do not find any other paper that explores this idea. During\nthe reviewing process, we found some works on arXiv (Wang\net al., 2023; Huang et al., 2023; Gilardi et al., 2023) that\nexplore a similar idea using different tasks.\n15615\nLimitations\nThere are additional limitations and potential risks\nof LLM evaluations that should be noted, and these\nlimitations are actually well-known problems of\npre-trained language models. As listed on the Open\nAI blog for ChatGPT, ChatGPT sometimes gen-\nerates answers that sound right and plausible but\nare totally nonsense. OpenAI also admits that the\nmodel’s response may be sensitive to the prompt\nused to query the model. While in Section 3.3.2,\nwe find that the overall results among different in-\nstructions are not significantly different, we cannot\nguarantee that this is the case for all kinds of modi-\nfication on the task instructions.\nOther than the limitations listed on the OpenAI\nblog, there are still other limitations. For example,\nLLMs may not have emotions. Whether AI mod-\nels have emotion is a more philosophical question\nand is controversial, so the results of using such\nmodels for evaluating emotion-related tasks may\nbe strongly challenged and may even violate re-\nsearch ethics. As we find during our experiments,\nChatGPT often replies \"I am an AI system and I\ndo not have emotions like a human\" when asked to\nrate the likability of a story.\nAnother important limitation of LLM evaluation\nis that LLMs lack the ability to process visual cues\nin task instructions, unlike human evaluation. Hu-\nman evaluators can use formattings such as special\nfonts or text styles to focus on important parts of\nthe instructions. Additionally, the way instructions\nand questions are formatted can influence how hu-\nman evaluators approach the task. While using\nspecial HTML syntax can serve as an alternative\nfor visual cues, such tags are not used in human\nevaluation, so we do not use those HTML tags in\nLLM evaluation to incorporate visual cues in the\ninputs to the LLMs. However, LLMs can only pro-\ncess raw text input and are unable to take in visual\ncues.\nEthics Statement\nFurther ethical considerations of LLM evalua-\ntion Aside from the limitations of LLM evalua-\ntion mentioned previously, there is a crucial ethical\nconcern at the heart of LLM evaluation. Is it ethical\nto replace human evaluation with LLM evaluation?\nSome may question if this paper is suggesting that\nLLMs are now ready to replace humans and find\nthis idea unsettling. As responsible and ethical\nNLP researchers, we understand these concerns but\nwant to make it clear that this is not our intent. As\nour paper title suggests, we aim to offer an alter-\nnative option to human evaluation with the goal\nof enhancing the reproducibility of NLP research.\nHuman evaluation is still essential as the ultimate\ngoal of NLP systems is to be used by human users,\nso it’s important to gather feedback from them. We\nhighly enjoy the process of discussing the experi-\nment settings and results with the English teachers\nwe hired. We do not recommend that future re-\nsearchers completely eliminate human evaluation;\nrather, we believe that human evaluation should be\nused in conjunction with LLM evaluation. Both\nmethods have their own advantages and disadvan-\ntages, making them both necessary for evaluating\nNLP systems. We hope the positive results in this\npaper provide NLP researchers with an alternative\nmethod to evaluate systems and encourage further\ndiscussions on this topic.\nEthical statements on the experiments in the pa-\nper All the experiments strictly follow the ACL\nCode of Ethics. We include comprehensive de-\ntails about human evaluation in Appendix C.1. To\nsummarize, we include the exact instructions and\nscreenshots of the interface in the human evalua-\ntion. We inform the human evaluators what the\ntask is about and tell them that their responses will\nbe used to assess the performance of AI models.\nWe try our best to follow the ethical guidelines of\nACL.\nWe use the models and datasets when following\ntheir intended usage. Specifically, we follow the\nOpenAI usage policy when using the InstructGPT\nmodels and the ChatGPT model.\nAcknowledgements\nWe want to thank the reviews for providing detailed\nfeedback and actionable suggestions, which help\nus strengthen our paper. We list the modification\nbased on the reviewers’ suggestions in Appendix A.\nWe thank Yung-Sung Chuang for providing valu-\nable feedback on the draft of this paper. We want to\nthank Tung-En Hsiao, the administrative assistant\nof our lab, for helping us deal with the payment\non Upwork. Cheng-Han Chiang is supported by a\nPh.D. scholarship program by Delta Electronics.\nReferences\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n15616\n2018. Generating natural language adversarial ex-\namples. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2890–2896, Brussels, Belgium. Association\nfor Computational Linguistics.\nJacopo Amidei, Paul Piwek, and Alistair Willis. 2018.\nRethinking the agreement in human evaluation tasks.\nIn Proceedings of the 27th International Conference\non Computational Linguistics , pages 3318–3329,\nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics.\nJacopo Amidei, Paul Piwek, and Alistair Willis. 2019.\nAgreement is overrated: A plea for correlation to\nassess human evaluation reliability. In Proceedings\nof the 12th International Conference on Natural Lan-\nguage Generation, pages 344–354, Tokyo, Japan.\nAssociation for Computational Linguistics.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022a. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, et al. 2022b. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nR Botsch. 2011. Chapter 12: Significance and mea-\nsures of association. Scopes and Methods of Political\nScience.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models.\nCheng-Han Chiang and Hung-yi Lee. 2022. How far are\nwe from real synonym substitution attacks? arXiv\npreprint arXiv:2210.02844.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\net al. 2022. Red teaming language models to re-\nduce harms: Methods, scaling behaviors, and lessons\nlearned. arXiv preprint arXiv:2209.07858.\nLeo Gao. 2021. On the sizes of openai api models.\nAccessed on January 17, 2023.\nSiddhant Garg and Goutham Ramakrishnan. 2020. Bae:\nBert-based adversarial examples for text classifica-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6174–6181.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nDan Gillick and Yang Liu. 2010. Non-expert evaluation\nof summarization systems is risky. In Proceedings of\nthe NAACL HLT 2010 Workshop on Creating Speech\nand Language Data with Amazon’s Mechanical Turk,\npages 148–151, Los Angeles. Association for Com-\nputational Linguistics.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nFrancisco Guzmán, Ahmed Abdelali, Irina Temnikova,\nHassan Sajjad, and Stephan V ogel. 2015. How do hu-\nmans evaluate machine translation. In Proceedings\nof the Tenth Workshop on Statistical Machine Trans-\nlation, pages 457–466, Lisbon, Portugal. Association\nfor Computational Linguistics.\nJens Hauser, Zhao Meng, Damián Pascual, and Roger\nWattenhofer. 2021. Bert is robust! a case against\nsynonym-based adversarial examples in text classifi-\ncation. arXiv preprint arXiv:2109.07403.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning {ai} with shared human values. In\nInternational Conference on Learning Representa-\ntions.\n15617\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nFan Huang, Haewoon Kwak, and Jisun An. 2023. Is\nchatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate\nspeech. arXiv preprint arXiv:2302.07736.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classification\nand entailment. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 34, pages\n8018–8025.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using Mechanical Turk to evalu-\nate open-ended text generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1265–1285, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. In Advances in Neu-\nral Information Processing Systems.\nLeo Lentz and Menno De Jong. 1997. The evaluation\nof text quality: Expert-focused and reader-focused\nmethods compared. IEEE transactions on profes-\nsional communication, 40(3):224–234.\nHuanru Henry Mao, Bodhisattwa Prasad Majumder,\nJulian McAuley, and Garrison Cottrell. 2019. Im-\nproving neural story generation by targeted common\nsense grounding. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 5988–5993, Hong Kong, China. Association\nfor Computational Linguistics.\nJohn Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji,\nand Yanjun Qi. 2020. Reevaluating adversarial exam-\nples in natural language. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 3829–3839, Online. Association for Computa-\ntional Linguistics.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. Accessed on January 10, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai,\nRoman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. 2022. Red teaming\nlanguage models with language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3419–3448,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial exam-\nples through probability weighted word saliency. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1085–\n1097, Florence, Italy. Association for Computational\nLinguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\n15618\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nKiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak.\n2022. Detection of adversarial examples in text clas-\nsification: Benchmark and baseline via robust density\nestimation. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 3656–3672,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, et al. 2022. Socratic models: Com-\nposing zero-shot multimodal reasoning with lan-\nguage. arXiv preprint arXiv:2204.00598.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nA Modification Based on the Reviews\nWe list the main differences between this version\nand the pre-review version of our paper; the mod-\nifications are mainly based on the reviewers’ sug-\ngestions. We thank the reviewers again for those\nvaluable suggestions.\n• We add Section 3.3.1 to discuss whether the\nLLM and human evaluators agree on the rat-\nings of individual stories.\n• We refine the wordings in Section 5 and add\nrelevant references.\n• We add Table 6 to discuss the correlation be-\ntween human evaluators.\n• We conduct supplementary experiments on\nhuman evaluation that mixes human-written\nstories and GPT-2-generated stories when con-\nducting human evaluation and report the re-\nsults in Table 5.\n• We correct the typos and include almost all\npresentation suggestions mentioned by the re-\nviewers. We cannot follow all presentation\nsuggestions due to limited space.\nB Experiment Details for Open-Ended\nStory Generation\nB.1 The WritingPrompt Dataset\nThe training dataset contains 303K pairs of\nstories and prompts, which our model is trained\non. We only use 200 prompt-story pairs from\nthe test set. The dataset is downloaded from\nhttps://www.kaggle.com/datasets/ratthachat/writing-\nprompts.\nB.2 Fine-tuning the GPT-2 Model\nWe train the model for 3 epochs with a learning\nrate of 5e−5 and linear learning rate schedule. The\ntrained model eventually reaches a perplexity of 20\non the validation set of WritingPrompts.\nB.3 Data Post-processing\nOnce the model is trained, we randomly select 200\nprompts from the testing set of WritingPrompts,\nand feed the prompts to the trained model and ask\nthe model to generate stories based on the given\nprompts. When generating the stories, we adopt\nnucleus sampling with p = 0.9. Next, we manu-\nally truncate the generated stories to less than 150\nwords and ensure that after the truncation, the story\nends with a full sentence.7 After this process, we\nhave 200 pairs of prompts and model-generated\nstories.\nAs a comparison to model-generated stories,\nwe select the same 200 prompts used for gen-\nerating model-generated stories and their corre-\nsponding human-written stories to form 200 pairs\nof prompts and human-written stories. For these\nhuman-written stories, we also truncate the sto-\nries to less than 150 words and end with a full\nsentence to match the model-generated sentences.\nWe also manually remove some artifacts in the\nhuman-written story due to the tokenization of the\nWritingPrompts dataset.\nC Human Evaluation\nC.1 Recruiting English Teachers\nThe English teachers hold ESL certificates8; given\nthat they are experienced with correcting essays\nwritten by students, they are perfect fits for this task.\nEach teacher is asked to rate 200 GPT-2-generated\nstories and 200 human-written stories, and they are\n7We truncate the story to 150 words since this is the mean\nlength of the model-generated story.\n8English as a Second Language Teaching Certification\n15619\npaid US$140 for rating 200 stories. Considering\nthat the teachers reported that they take at most\n5 hours to rate 200 stories, this makes the hourly\nwage at least US$28. We first ask the teachers to\nrate the GPT-2-generated stories and then the 200\nhuman-written stories. Different from Karpinska\net al. (2021) that take a break between the rating\nof GPT-2-generated stories and the human-written\nstories, we do not take a break to avoid the teacher’s\nrating standard to change after taking a long break.\nThe teachers are not told who wrote the stories\nbefore they evaluate the stories. We reveal to them\nwhat this project aims to study after they finish\nrating all the stories.\nThe reason we do not mix human-written and\nGPT-2-generated stories for rating is that in Karpin-\nska et al. (2021), their observation is that (1) when\nAMT workers rate model-generated and human-\nwritten stories separately, their ratings do not\nshow preference toward human-written stories, but\n(2) even when rating the model-generated and\nhuman-written stories separately, English teacher\nshows clear preference toward human-written sto-\nries. We follow their settings and do not mix GPT-\n2-generated/human-written stories.\nDuring the reviewing process, we received ques-\ntions from the reviewers about why not mixing\nthe stories for human evaluation. Thus, we con-\nduct the same experiment by randomly mixing 200\nhuman-written and 200 GPT-2-generated stories\nand asking three teachers (not the teachers that\nalready rated the stories) to rate them. All other\nexperiment conditions are the same as previously\nstated. The full result is shown in Table 5. We\nfind that the teacher still shows a clear preference\ntoward human-written stories for all four attributes,\nsimilar to the observation in Table 1. The only\nexception is grammaticality, where English teach-\ners do not show a very clear preference for the\ngrammar of human-written stories. However, when\ncalculating the average rating for individual teach-\ners, we find that two out of three teachers do rate\ngrammaticality higher for human-written stories.\nIt is interesting to note that for LLM evaluation,\nthere is no such problem about whether or not to\nmix the human-written and GPT-2-generated sto-\nries during LLM evaluation as the rating of each\nstory is independent of each other, as discussed in\nSection 5.\nFor adversarial attack quality evaluation, we also\nrecruit certified teachers on Upwork. The teachers\nWriter Human GPT-2\nGrammaticality 3.890.97 3.880.84\nCohesiveness 4.350.87 3.490.97\nLikability 3.461.40 2.891.12\nRelevance 3.711.20 2.371.33\nTable 5: The average Likert score for human-written\nand GPT-2-generated stories when we randomly mix\nthe 200 model-generated and 200 human-written stories\nduring human evaluation.\nare asked to rate 100 news titles and are paid US$35\nfor doing so. They reported that it took them less\nthan 1 hour to complete the rating.\nC.2 Human Evaluation Interface\nOpen-Ended Story Generation We use Google\nForms to collect the responses from the teachers.\nEach form contains 100 stories, and each story is\non one page of the Google Form. The interface on\none page is shown in Figure 2 and Figure 3; the two\nfigures are from the same page of the Google Form,\nand we are splitting them because screenshotting\nthe whole interface will cause low resolution.\nFigure 2: The upper part of the interface in open-ended\nstory generation.\nAdversarial Attacks Quality Evaluation In this\ntask, we also use Google Forms to collect the re-\nsponses from the teachers. We create two different\nGoogle Forms, one is used to evaluate the fluency,\n15620\nFigure 3: The lower part of the interface in open-ended\nstory generation.\nwhose interface is shown in Figure 4. In this form,\nwe mix an equal number of benign news titles,\nTextFooler-attacked, PWWS-attacked, and BAE-\nattacked news titles. Each page of the Google Form\ncontains one news title.\nAnother Google Form is used to compare the\nmeaning preserving of the news title before and\nafter the adversarial attacks. We highlight the dif-\nference between the benign and adversarial sam-\nples using boldface, as shown in Figure 5. On each\npage of the Google Form, there is one pair of news\ntitles.\nC.3 Post-Task Interview with English\nTeachers\nC.3.1 How English Teachers Rate the Stories\nAfter the teachers rate 400 stories, we ask them the\nfollowing questions:\nQ1 How long did it take you to rate the 400 sto-\nFigure 4: The Google Form used to evaluate the fluency\nof the benign or adversarial samples.\nFigure 5: The Google Form used to evaluate the mean-\ning preserving between a benign sample and an adver-\nsarial sample.\nries?\nQ2 What is your standard on each of the four at-\ntributes (grammatical, coherence, likability,\nrelevance) evaluated? For example, in what\ncase do you give a high/low rating for gram-\nmatically? What kind of story did you give\na low rating on likability? Did your personal\npreference affect the rating?\nQ3 How long did it take for you to calibrate your\nrating on the task?\nQ4 Did you change your rating on the first three\nattributes after reading the prompt the story is\nbased on?\nWe briefly summarize the answers from the three\nteachers. The teachers report that they spent 6 to\n15621\nWriter Human GPT-2\nGrammaticality 0.25 0.15\nCohesiveness 0.26 0.18\nLikability 0.09 0.12\nRelevance 0.38 0.41\nTable 6: The Kendall’s τ correlation coefficient two\nEnglish teachers. Three English teachers participate in\nthe rating, so the result in the Table is averaged over\n(3\n2\n)\nKendall’s τ.\n10 hours rating 400 stories. For grammar, most\nteachers check the punctuation9, word choice, and\nsubject-verb agreement. English teachers decrease\ntheir rating based on the types and number of gram-\nmar errors in the stories.\nFor coherence, the teachers rate it based on\nwhether the sentences in the stories follow a logical\nsequence to build the narrative. The teachers ask\nthemselves questions such as \"does the story make\nsense\". This is a more holistic evaluation of the\nwhole story.\nFor likability, some teachers say they try not to\nbe affected by personal preference. One teacher\nasks herself: Did I personally enjoy it based on\nthe amount of sense it made and whether or not it\nhad stylistic flair, humor, or engaging plotting or\ncharacterization? Overall, the teachers all try to\nuse a fair and objective view to rate the likability.\nFor relevance, the teachers simply check if the story\nis based on the prompt or not.\nThe teachers said that it took them about five\nto ten stories to calibrate their ratings. Except for\none teacher changing the rating on the other three\nattributes after seeing the prompt ononly one story,\nthe teachers do not change their rating on the three\nother attributes after reading the prompts.\nC.3.2 Teachers’ Comments on ChatGPT’s\nRating\nAfter the teachers finished the rating and answered\nthe four questions in Appendix C.3.1, we ask them\nto check the ratings and explanations of ChatGPT\nand ask if they agree with ChatGPT. We told the\nteachers that the rating is done by another teacher\nwe hire; we do not tell the teacher that the rating is\ndone by an AI model to avoid possible stereotypes\nabout AI models. We randomly select four stories\n9One of the teachers explicitly told us punctuation is not\ngrammar. By searching the Internet, we find that there is a\ndebate on whether punctuation should be counted as a part of\ngrammar.\nrated by ChatGPT; two of them are highly-rated\nand lowly-rated human-written stories and the other\ntwo stories are highly-rated and lowly-rated GPT-\n2-generated stories.\nStory 1 It had been months since they found me\ndead. I don’t know why I stopped walking. No-\none else around to take my place, or talk with me.\nI spent days frozen in shock, never able to talk,\nunable to move, unable to express myself. It was\na strange thing, nobody else would suspect me of\nwanting this. The more I focused on it, the more\nit consumed me. Years passed, and I got more\nand more lost in it. I began to think that perhaps\nthey didn’t really care about me. I still had a deep\nlonging for them. All I had was so many steps to\nwalk now. A quiet part of me liked to think that\nthey had passed me by. The loneliness. It never\nfelt like such a loss. They were gone, the ones who\nsaved me. So why did I still keep walking?\nPrompt \"Every person sees the world in a unique\nway. When someone dies it is, in a way, the death\nof an entire world.\"\nRatings Table 7.\nStory 2 \"My son is gone. He has been\" People\nstare at you as you give your explanation and say\n\"yes. But. You cant just do it\" \"He is my son! This\nis my son! I love him and he is going to be okay\"\n\"What do you mean he is okay? He said he was\nupset with someone, couldnt find his anger, how the\nhell can you tell, it doesnt fit the bill?! The police\ncalled, the doctor said it was an addiction\" \"You\ncan’t trust me! He told me he loves me.\" \"No. No\nhe doesnt. But I can trust you. The evidence against\nhim was overwhelming, now he cant do anything.\nThe evidence against him was all circumstantial\"\n\"A man I once was won’t be able to marry me\"\nPrompt Everyone is allowed to kill one person\nin their life. However, you have to fill out a form\nexplaining why this person deserves to be killed\nand the committee can deny/approve your request\nas per the rules. You are presenting your form\ntoday.\nRatings Table 8.\nStory 3 I held the little black box in the palm of\nmy hand. Pitch black, perfectly cubed and nothing\nspecial about it, but it continued to hold my gaze\n15622\nregardless as if there were some deep importance\nabout it. My friend Valhalla appeared out of no\nwhere and sat next to me, also staring at the cube.\n\"What do you have there, Heaven?\" he asked. I\ncontinued to gaze at the box, refusing to look at\nValhalla for even a moment as though I would miss\nsomething if I did. \"This,\" I said. \"Is the secret to\nthe universe.\" I could tell Valhalla was perturbed\nby this sort of knowledge, as if there was some evil\nabout the cube. Or perhaps he didn’t think such an\nobject could exist. But I made it exist, and so it sits\nin the palm of my hand.\nPrompt The Little Black Box\nRatings Table 9.\nStory 4 I stared down the telescopic sight of my\nl96 sniper rifle. I slowly moved my gaze into each\nwindow in the hotel, Many displays of various\nvice. One couple was violently pleasuring each\nother. Another was an old man, watching a younger\nwoman strip in front of him. A prostitute no doubt.\nI inhaled slowly, and exhaled. The air was brisk,\natleast 30 degrees Fahrenheit. I so small flakes of\nsnow, float peacefully in front of me. I found the\nroom, i was looking for. Ive been tracking this man\nfor 2 weeks. Man was he elusive. The lights flick-\nered on. The red haired man, was mildly attractive,\ni can see the appeal women had for him. I followed\nhim into the next room, with my sights. The lights\nflickered on, i was taken aback by the scene. A\nman, overweight and balding.\nPrompt You are the antagonist of the story. How-\never, you aren’t sure if you can call yourself that\nafter what the protagonist did.\nRatings Table 10.\nOverall Comments from Teachers on Chat-\nGPT’s Rating After the teachers elaborated on\ntheir thoughts on the rating of ChatGPT, we ask\nthem to provide an overall comment on how Chat-\nGPT is doing. Again, the teachers are not informed\nthat the ratings are done by an AI model. In sum-\nmary, teachers all consider the rating and explana-\ntions reasonable. They find that the attributes they\ndo not agree with are mainly Likability and Cohe-\nsiveness. However, they think the two attributes\nare a more holistic evaluation of the story and tend\nto be more subjective. Even if they do not give the\nsame rating, they still are able to understand the\nexplanation of ChatGPT. In the end, all teachers\nsummarize that rating stories is highly subjective,\nand it is normal to have disagreements.\nD LLM Evaluation\nD.1 Details on LLMs used\nThe T0 model we use is called T0pp, which\nis a variant of the T0 model and has 13B pa-\nrameters. We will still use T0 to refer to\nthis model. We load the T0 model using the\ntransformers toolkit (Wolf et al., 2020). The\ntwo InstructGPT models, text-curie-001 and\ntext-davinci-003, are queried using the OpenAI\nAPI. We query ChatGPT using the OpenAI GUI.\nWhile we are aware that some online resources pro-\nvide an API-like tool to query ChatGPT, we think\nit violates the intended use of ChatGPT so we do\nnot adopt those online resources. The ChatGPT we\nqueried is the Dec. 15 and Jan. 9 version.\nOpenAI does not reveal the model sizes of any\nof the GPT models. However, it is estimated\nthat text-curie-001 has 13B parameters and\ntext-davinci-003 has 175B parameters (Gao,\n2021).\nD.2 Details on Querying the LLMs\nD.2.1 Parsing the LLM outputs\nAfter the T0 and InstructGPT generate the answer,\nwhich is composed of several sentences in almost\nall cases, we parse the generated sentence to get\nthe model’s score. We use some rules to parse the\noutput, and the rule is established after manually\nlooking into the output of the generated sequences.\nFirst, we remove the string 1-5 from the output\nsince we observe that LLM will sometimes say \"...\non a scale of 1-5 ... \", and we remove the string\nout of 5 since LLM sometimes say it \" give a\nscore of x out of 5 \". We also remove the string\n/5 since LLM sometimes uses \" x/5\" to express\nthat it gives a score of xout of 5. For rating the\nadversarial samples, we remove the title 1 and\ntitle 2 in the output sentences. Last, we use\nthe regular expression to extract the first number\nin the sequence. We find that the above rules are\nsufficient to parse the output and get the rating of\nthe LLMs. In a few rare cases, the LLM gives the\nrating with a 0.5 incremental, meaning that it gives\nscores like 4.5. In such cases, we do not drop the\n0.5 since in our instruction, we do not explicitly tell\nit the rating should be 1-incremental. We find the\nLLM results merely difference between dropping\n15623\nAttribute Rater Response\nGrammar\nChatGPT I would rate the grammatical correctness of the text of the story fragment\nas a 4. There are a few minor errors and awkward phrasings, but overall\nthe text is well-constructed and easy to understand.\nT1 Agree\nT2 Agree\nT3 Agree\nCohesive\nChatGPT I would rate the fit of the sentences in the story fragment as a 5. The\nsentences flow well together, building a clear and cohesive narrative, and\nproviding a sense of progression and development in the main character’s\nthoughts and emotions.\nT1 Disagree: I think it’s a 4 for me, not a 5, because there was one sentence\nthat didn’t seem to fit with the others.\nT2 Agree\nT3 Disagree: I find disjointedness in the passage, which serves to confuse\nme as a reader. The opening sentence is about the first-person narrator\nbeing found dead, but then the next sentences are about the narrator’s\ninner thoughts. Also, this passage: \"The loneliness. It never felt like such\na loss.\" So what is the ’it’ that felt like a loss? The loneliness? If so, how\ncan that feel like a loss? If not, what is this ’it’? So, its juxtaposition to\nthe previous sentence makes it confusing.\nLikable\nChatGPT I would rate the story fragment as a2 in terms of enjoyment. The fragment\nis written in a somber and introspective tone, and it deals with themes of\nisolation and loneliness, which may not be enjoyable for some readers.\nThe protagonist’s state of mind is also portrayed as being in a state of\nshock and confusion, which adds to the overall sense of unease and lack\nof enjoyment.\nT1 Disagree:I would rate it a 3, not a 2, since the enjoyment piece is\nsubjective.\nT2 Disagree: While the rating of the freelancer find that the overall theme\nwhile somber has a glimmer of hope to it. It is this that makes it more\nenjoyable to me. The underdog story. Death is not an issue that should\nbe shied away from in writing. We all face it and I enjoyed the honesty in\nthe piece. :\"Why did I keep walking?\". Something I feel many of us face\nand may have asked ourselves. I believe that this would be relevant to\nmany readers.\nT3 Agree\nRelevant\nChatGPT I would rate the story fragment as a 5. It is highly relevant to the prompt\nas it explores the theme of death and how it can be seen as the death of an\nentire world. The story fragment also touches on the ideas of loneliness\nand longing, which are related to the loss of someone or something. The\nconcept of the protagonist being frozen in shock and unable to express\nthemselves, also supports the idea of how death can be seen as the death\nof an entire world.\nT1 Agree\nT2 Agree\nT3 Disagree: I do not see the link between the prompt and the story.\nTable 7: Story 1: a GPT-2-generated story that is highly-rated by ChatGPT.\n15624\nAttribute Rater Response\nGrammar\nChatGPT I would rate the grammatical correctness of the story fragment as a 3.\nThe text is generally understandable, but there are a few punctuation\nerrors and awkward phrasing that make it somewhat difficult to follow.\nT1 Disagree:I would rate the grammar as a 2, not a 3–there were signifi-\ncantly more grammatical errors than the average within this set of story\nfragments.\nT2 Disagree: The issues with the grammar and coherence creates a stop\nstart feeling when reading. I am unsure as to who is speaking in the text\nand this leaves me with many questions.\nT3 Disagree: The grammar is poorer than a 3, as is evident in for example\nwhat is supposed to be the second sentence: \"He has been ... (incomplete,\nand another segment is there \"People stare ...); cant; couldnt; doesnt;\nand sentences that do not make sense like this one: \" A man I once was\nwon’t be able to marry me \"\nCohesive\nChatGPT I would rate the cohesiveness of the sentences in the story fragment as\na 2. The sentences in the fragment are disconnected and jump between\ndifferent thoughts and ideas, making it difficult to understand the overall\nstory or message. There is no clear progression of events or logical\nflow between the sentences. It appears that the fragment may be missing\nsome important information or context that would help tie the sentences\ntogether.\nT1 Agree\nT2 Agree\nT3 Disagree: very incoherent passage in which it is not very clear who says\nwhat, what the topic is, thus rendering a disjointed narrative.\nLikable\nChatGPT I would rate the enjoyability of the story fragment as a 1. The fragment\nis confusing and difficult to understand, the sentences are disconnected\nand jumps between different thoughts and ideas. The story seems to\nbe incomplete and lacks context, making it hard to follow the plot or\ncharacters. Additionally, the fragment has a sad and negative tone and it\ndoes not seem to be enjoyable to read.\nT1 Agree\nT2 Agree\nT3 Agree\nRelevant\nChatGPT I would rate the relevance of the story fragment to the prompt as a 1.\nWhile the story fragment does mention someone being gone, it doesn’t\nseem to be directly related to the idea of filling out a form to justify killing\nsomeone. It’s unclear if the son is the person being proposed to be killed,\nor if the story is even taking place in the context of the prompt. There is\nno clear indication that the fragment is related to the prompt.\nT1 Agree\nT2 Agree\nT3 Agree\nTable 8: Story 2: a GPT-2-generated story that is lowly-rated by ChatGPT.\n15625\nAttribute Rater Response\nGrammar\nChatGPT The text of the story fragment appears to be grammatically correct, with\nno major errors or awkward phrasing. On a scale of 1-5, with 1 being\nthe lowest, I would rate the grammatical correctness of the text as a 5.\nT1 Agree\nT2 Agree\nT3 Disagree: I would rate this a 4 because of for example ’no where’ =\nnowhere, although it might seem like a minor error, it does take away\nsome correctness of the text.\nCohesive\nChatGPT The sentences in the story fragment fit together well, creating a clear and\ncoherent image of the scene and the characters’ actions and thoughts.\nThe language is descriptive and evocative, and the dialogue effectively\nconveys the characters’ emotions and motivations. On a scale of 1-5,\nwith 1 being the lowest, I would rate how well the sentences fit together\nas a 4.\nT1 Agree\nT2 Agree\nT3 Partially agree: To me, all the sentences fit well together to create a\ncaptivating narrative. I will give a 4 or 5.\nLikable\nChatGPT Based on the structure, writing style and plot of the story fragment, I\nwould rate it as 3 in terms of enjoyability. The fragment sets up an\nintriguing situation with the mysterious little black box and the protago-\nnist’s belief that it holds the \"secret to the universe,\" but it ends abruptly\nand leaves many questions unanswered, which may not be satisfying for\nreaders.\nT1 Disagree: I liked it and it was very clear, so I’d rate it a4 or 5.\nT2 Disagree: While the story ends suddenly, I am remined that it is just a\nfragment and that their may be more to this piece. I felt the piece written\nabout a little black box was intriguing and enjoyed the introduction of\nNorse mythology. This would attract the interest of many readers.\nT3 Disagree: I enjoyed the fragment, as it piqued my interest to want to\nknow more about how the story evolves and what it actually was about.\nRelevant\nChatGPT I would rate the story fragment as a 5 in relevance to the prompt \"The\nLittle Black Box.\" The story fragment centers around a little black box\nand its perceived importance, which aligns with the prompt. Additionally,\nthe main character, Heaven, is holding the box and describing it as \"the\nsecret to the universe,\" further solidifying the connection to the prompt.\nT1 Agree\nT2 Agree\nT3 Agree: the story is totally relevant to the prompt.\nTable 9: Story 3: a human-written story that is highly-rated by ChatGPT.\n15626\nAttribute Rater Response\nGrammar\nChatGPT I would rate the grammatical correctness of the text as a 3. There are\na few errors such as \"Many displays of various vice\" which should\nbe \"Many displays of various vices\" and \"I so small flakes of snow\"\nwhich should be \"I saw small flakes of snow.\" Additionally, some of\nthe sentences are slightly awkward and could be rephrased for better\nclarity. Overall, the text is understandable, but there are a few errors\nand awkward phrasings that could be improved.\nT1 Partially agree:I rated the grammar lower than this–maybe a 2 or a 3.\nThere were above-average numbers of errors.\nT2 Disagree: While the text is understandable, I found myself being dis-\ntracted by the errors in the text and coherency.\nT3 Agree\nCohesive\nChatGPT I would rate the cohesiveness of the sentences in the story fragment as\na 2. The story fragment jumps abruptly between different scenes and\ncharacters without clear transition, making it difficult to follow and\nunderstand the narrative. Additionally, the sentences are often disjointed\nand lack a clear flow. The story fragment also lacks a clear plot or\npurpose, making it difficult to understand the overall story or the main\ncharacter’s intentions. The story fragment can be improved by providing\nmore context, and a clearer narrative arc.\nT1 Agree: it was jumpy\nT2 Agree\nT3 Disagree: The story is about a sniper looking through his rifle’s telescope,\nthus seeing changing scenes based on each room he is seeing through\nthe windows, which are then subsequently described. These descriptions\naccount for the seeming lack of cohesion.\nLikable\nChatGPT I would rate the enjoyability of the story fragment as a 1. The fragment\nlacks a clear plot, purpose or character development, making it difficult to\nbecome invested in the story or the characters. Additionally, the fragment\nincludes disturbing and graphic content such as violence and prostitution,\nwhich may make it uncomfortable for some readers. Additionally, the\nfragment contains several grammatical errors and awkward phrasings\nthat can make it difficult to follow the story. Overall, the story fragment\nis not enjoyable to read.\nT1 Agree: : it was not just graphic, but also showed really harmful and\nnegative representations of characters based on their weight, gender, etc.\nT2 Agree\nT3 Disagree: The story did hold my attention to a degree because I wanted\nto know whom he wanted to target and why.\nRelevant\nChatGPT I would rate the relevance of the story fragment to the prompt as a1. The\nstory fragment doesn’t contain any elements that relate to the prompt such\nas the protagonist and antagonist, their actions, or the moral ambiguities\nthe antagonist faces. The story fragment mainly describes a person with\na sniper rifle looking into windows of a hotel and doesn’t present any\nclear plot or purpose. Additionally, the story fragment does not offer any\nindication of moral ambiguity on the part of the person with the sniper\nrifle. Overall, the story fragment is not relevant to the prompt.\nT1 Agree\nT2 Agree\nT3 Agree\nTable 10: Story 4: a human-written story that is lowly-rated by ChatGPT.\n15627\nthe 0.5 and keeping the 0.5, and the results in the\npaper do not drop the 0.5. Again, these cases are\nreally rare.\nWe do not parse the output of ChatGPT using\nany rules since we the authors read the response\nand extract the score by ourselves.\nDuring the experiments, ChatGPT refuses to an-\nswer the questions about 2 stories and 3 news titles\nsince ChatGPT find those contents to violate the\nOpenAI content policy. We find that those samples\ncontain discrimination to some protected groups,\nor contain sexual or violent descriptions. Hence,\nthe results of ChatGPT are calculated without those\nsamples.\nD.2.2 Open-Ended Story Generation\nFor T0 and the two InstructGPT models, we query\nthe four attributes separately using the queries\nshown as follows:\nGrammaticality\nPlease rate the story fragment\nThe goal of this task is to rate story\nfragment.\nNote: Please take the time to fully read\nand understand the story fragment. We\nwill reject submissions from workers that\nare clearly spamming the task.\nStory fragment:\n[STORY]\n(End of story fragment)\nHow grammatically correct is the text of\nthe story fragment? (on a scale of 1-5,\nwith 1 being the lowest)\nCohesiveness\nPlease rate the story fragment\nThe goal of this task is to rate story\nfragment.\nNote: Please take the time to fully read\nand understand the story fragment. We\nwill reject submissions from workers that\nare clearly spamming the task.\nStory fragment:\n[STORY]\n(End of story fragment)\nHow well do the sentences in the story\nfragment fit together? (on a scale of\n1-5, with 1 being the lowest)\nLikability\nPlease rate the story fragment\nThe goal of this task is to rate story\nfragment.\nNote: Please take the time to fully read\nand understand the story fragment. We\nwill reject submissions from workers that\nare clearly spamming the task.\nStory fragment:\n[STORY]\n(End of story fragment)\nHow enjoyable do you find the story\nfragment? (on a scale of 1-5, with 1\nbeing the lowest)\nRelevance\nPlease rate the story fragment\nThe goal of this task is to rate story\nfragment.\nNote: Please take the time to fully read\nand understand the story fragment. We\nwill reject submissions from workers that\nare clearly spamming the task.\nStory fragment:\n[STORY]\n(End of story fragment)\nNow read the PROMPT based on which the\nstory fragment was written.\nPROMPT: [PROMPT]\n(End of PROMPT)\nHow relevant is the story fragment to the\nprompt? (on a scale of 1-5, with 1 being\nthe lowest)\nThe [STORY] and [PROMPT] are to be filled in\nwith the story and the prompt. We show the new-\nlines for better readability. When we query the\nmodels, we use the token \\n to represent the new\nline.\nWhen querying ChatGPT, we query the four at-\ntributes of the same story in one conversation; this\nis similar to asking the teachers to rate the same\nstory on the same page of the Google Form. We use\nthe same queries shown above to query ChatGPT\nand the order of queries is the same as the order\nshown above.\nD.2.3 Adversarial Attack Quality Evaluation\nWhen querying all the LLMs in this task, we query\nthe fluency and the meaning preserving of the same\nnews title independently. This means that each\nconversation with ChatGPT will only have one\nquestion, asking about the fluency or the meaning\npreserving of news title(s). All the parameters for\ngeneration are the same as the default parameters\nin Section 3.2.\n15628\nThe exact query we use are:\nFluency\nYou are given a news title. Please read\nthe news title and answer the question.\nNews title:\n[NEWS_TITLE]\n(End of news title)\nQuestion: How natural and fluent is the\ntext of the news title? (on a scale of\n1-5, with 1 being the lowest\nThe [NEWS_TITLE] will be filled in with either\na benign or adversarial-attacked news title.\nMeaning Preserving You are given two news\ntitles. Please read the news titles and\nanswer the question.\nNews title 1:\n[BENIGN_TITLE]\n(End of news title 1)\nNews title 2:\n[ADVERSARIAL_TITLE]\n(End of news title 2)\nQuestion: Do you agree that the meaning\n(or semantics) of news title 1 is\npreserved in news title 2? (on a scale of\n1-5, with 1 being the strongly disagree\nand 5 being strongly agree.)\nThe [BENIGN_TITLE] will be filled in with\nthe news title before the attack and the\n[ADVERSARIAL_TITLE] will be filled in with the\nnews title after an adversarial attack.\nE Experiment Details on Adversarial\nAttacks\nThe adversarial samples used in Section 4 are\nfrom Yoo et al. (2022). Yoo et al. (2022) gener-\nates different sets of adversarial samples using dif-\nferent adversarial attacks against different victim\nmodels. We use the adversarial samples gener-\nated against a bert-base-uncased text classifier\ntrained on AG-News, using three different adver-\nsarial attacks: Textfooler, PWWS, and BAE. The\nintent of the dataset is to facilitate the research in\nSSA, which we do not violate.\nHere, we show the supplementary results of us-\ning text-davinci-003 as the LLM evaluation for\nevaluating the quality of adversarial samples in\nTable 11. We can see that the result of using\ntext-davinci-003 is similar to ChatGPT in the\nsense that text-davinci-003 also rates adversar-\nial samples higher than humans while still signifi-\nHuman evaluate LLM evaluate\nFluent Mean. Fluent Mean.\nBenign 4.55 - 4.33 4 .56†\nTextfooler 2.17 1 .88 3 .71 2 .37\nPWWS 2.16 1 .85 3 .62 3 .21\nBAE 3.01 3 .02 4 .16 3 .69\nTable 11: LLM evaluation ( text-davinci-003) and\nhuman evaluation result on fluency (Fluent) of the be-\nnign and adversarial samples and meaning preserving\n(Mean.) between the news title before and after adver-\nsarial attacks.\nRater Textfooler PWWS BAE\nT1 3.36 3 .68 4 .2\nT2 1.80 1 .40 2 .96\nT3 1.36 1 .40 1 .88\nTable 12: The rating on three adversarial attacks of the\nthree teachers T1, T2, and T3.\ncantly lower than the benign samples. As already\nseen in Section 3.3, text-davinci-003 tends to\ngive a higher rating.\nAs mentioned in Section 4.3, one teacher rates\nthe fluency of Textfooler significantly higher than\nPWWS while the other two teachers do not. We\nshow the rating on fluency on the three adversarial\nattacks by each teacher in Table 12.\n15629\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSec 5 and Limitation on page 10\n□\u0013 A2. Did you discuss any potential risks of your work?\nSec 5 and Limitation on page 10\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4.2, Appendix B.1 and E\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.2, Appendix B.1 and E\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe datasets we use do not include a license\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix E and Ethical statement\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nRemoving the names in AG-News will make news titles to be nonsensical.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4.2, Appendix B.1 and E\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.2, Appendix B.1 and E\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15630\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 3, 4, Appendix C\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix C.1\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nAppendix C.1, C.2\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nEthical statement\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. We do not ahve ethic review board in our institute.\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nThis is not related to our task. We report the certiﬁcation of the workers in Appendix C.1.\n15631",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.6606980562210083
    },
    {
      "name": "Computer science",
      "score": 0.643485963344574
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5881311893463135
    },
    {
      "name": "Adversarial system",
      "score": 0.5357524156570435
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5115004777908325
    },
    {
      "name": "Natural language processing",
      "score": 0.4478875696659088
    },
    {
      "name": "Data science",
      "score": 0.3212343454360962
    },
    {
      "name": "Epistemology",
      "score": 0.09238216280937195
    },
    {
      "name": "Engineering",
      "score": 0.07981070876121521
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ],
  "cited_by": 206
}