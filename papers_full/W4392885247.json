{
  "title": "Using protein language models for protein interaction hot spot prediction with limited data",
  "url": "https://openalex.org/W4392885247",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5077764895",
      "name": "Karen Sargsyan",
      "affiliations": [
        "Institute of Biomedical Sciences, Academia Sinica"
      ]
    },
    {
      "id": "https://openalex.org/A5010018908",
      "name": "Carmay Lim",
      "affiliations": [
        "Institute of Biomedical Sciences, Academia Sinica"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4382501959",
    "https://openalex.org/W4211084060",
    "https://openalex.org/W4387303685",
    "https://openalex.org/W4223581484",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W4306404538",
    "https://openalex.org/W4286669150",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4386709418",
    "https://openalex.org/W2999481648",
    "https://openalex.org/W4366823175",
    "https://openalex.org/W4312198087",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W2168164063",
    "https://openalex.org/W3025314715",
    "https://openalex.org/W4387698451",
    "https://openalex.org/W2157735020",
    "https://openalex.org/W2953103783",
    "https://openalex.org/W3136918052",
    "https://openalex.org/W4200632439",
    "https://openalex.org/W4313454290",
    "https://openalex.org/W4388033280",
    "https://openalex.org/W2150444353",
    "https://openalex.org/W2069663555",
    "https://openalex.org/W2008708467",
    "https://openalex.org/W2253581743",
    "https://openalex.org/W4387440676",
    "https://openalex.org/W4399082902",
    "https://openalex.org/W3211728297",
    "https://openalex.org/W2995514860"
  ],
  "abstract": "Abstract Background Protein language models, inspired by the success of large language models in deciphering human language, have emerged as powerful tools for unraveling the intricate code of life inscribed within protein sequences. They have gained significant attention for their promising applications across various areas, including the sequence-based prediction of secondary and tertiary protein structure, the discovery of new functional protein sequences/folds, and the assessment of mutational impact on protein fitness. However, their utility in learning to predict protein residue properties based on scant datasets, such as protein–protein interaction (PPI)-hotspots whose mutations significantly impair PPIs, remained unclear. Here, we explore the feasibility of using protein language-learned representations as features for machine learning to predict PPI-hotspots using a dataset containing 414 experimentally confirmed PPI-hotspots and 504 PPI-nonhot spots. Results Our findings showcase the capacity of unsupervised learning with protein language models in capturing critical functional attributes of protein residues derived from the evolutionary information encoded within amino acid sequences. We show that methods relying on protein language models can compete with methods employing sequence and structure-based features to predict PPI-hotspots from the free protein structure. We observed an optimal number of features for model precision, suggesting a balance between information and overfitting. Conclusions This study underscores the potential of transformer-based protein language models to extract critical knowledge from sparse datasets, exemplified here by the challenging realm of predicting PPI-hotspots. These models offer a cost-effective and time-efficient alternative to traditional experimental methods for predicting certain residue properties. However, the challenge of explaining why specific features are important for determining certain residue properties remains.",
  "full_text": "Using protein language models for protein \ninteraction hot spot prediction with limited data\nKaren Sargsyan1* and Carmay Lim1* \nBackground\nLarge language models [1] are a remarkable advancement in artificial intelligence, dem -\nonstrating exceptional capabilities in understanding and generating human language. \nTheir applications span diverse domains, including software development, education, \nAbstract \nBackground: Protein language models, inspired by the success of large language \nmodels in deciphering human language, have emerged as powerful tools for unrave-\nling the intricate code of life inscribed within protein sequences. They have gained \nsignificant attention for their promising applications across various areas, includ-\ning the sequence-based prediction of secondary and tertiary protein structure, the dis-\ncovery of new functional protein sequences/folds, and the assessment of mutational \nimpact on protein fitness. However, their utility in learning to predict protein residue \nproperties based on scant datasets, such as protein–protein interaction (PPI)-hotspots \nwhose mutations significantly impair PPIs, remained unclear. Here, we explore the feasi-\nbility of using protein language-learned representations as features for machine learn-\ning to predict PPI-hotspots using a dataset containing 414 experimentally confirmed \nPPI-hotspots and 504 PPI-nonhot spots.\nResults: Our findings showcase the capacity of unsupervised learning with pro-\ntein language models in capturing critical functional attributes of protein residues \nderived from the evolutionary information encoded within amino acid sequences. \nWe show that methods relying on protein language models can compete with meth-\nods employing sequence and structure-based features to predict PPI-hotspots \nfrom the free protein structure. We observed an optimal number of features for model \nprecision, suggesting a balance between information and overfitting.\nConclusions: This study underscores the potential of transformer-based protein \nlanguage models to extract critical knowledge from sparse datasets, exemplified here \nby the challenging realm of predicting PPI-hotspots. These models offer a cost-effec-\ntive and time-efficient alternative to traditional experimental methods for predicting \ncertain residue properties. However, the challenge of explaining why specific features \nare important for determining certain residue properties remains.\nKeywords: Protein language models, ESM-2, Protein–protein interaction, PPI-hotspot, \nSmall datasets, Feature selection\nOpen Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdo-\nmain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nSargsyan and Lim  BMC Bioinformatics          (2024) 25:115  \nhttps://doi.org/10.1186/s12859-024-05737-2\nBMC Bioinformatics\n*Correspondence:   \nkaren.sarkisyan@gmail.com; \ncarmay@gate.sinica.edu.tw\n1 Institute of Biomedical \nSciences, Academia Sinica, \nTaipei 115, Taiwan\nPage 2 of 10Sargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \nscientific research, healthcare, finance, and law [1]. Analogous to how these models \ndecode human language, deep-learning models known as protein language models have \nemerged to decipher the intricate language of life by learning the patterns embedded \nin protein sequences over evolutionary time [2–6]. The key differences between large \nlanguage models and protein language models lie in their input data and intended appli -\ncations. Whereas large language models are trained on massive textual datasets, protein \nlanguage models harness vast protein sequence databases containing millions of amino \nacid (aa) sequences from various organisms [5]. However, certain data types such as \nper-residue binding free energy contributions [7] are experimentally arduous to collect, \nyielding limited datasets. Thus, machine learning algorithms may not extract meaningful \ninsights from such sparse data. This study aims to explore the potential of protein lan -\nguage models in extracting subtle information from sparse datasets, exemplified by the \nlimited dataset of protein–protein interaction (PPI) hotspots, defined as residues whose \nmutations significantly impair/abolish PPIs [7].\nProtein language models, like large language models, are built upon transformer archi-\ntectures for representation learning and generative modeling [8]. In the transformer \narchitecture, the encoder component encodes aa sequences, mapping each residue in \nthe input sequence to an N-dimensional vector. The value of N is determined by various \nfactors, including the model size (i.e., number of parameters), the training dataset size, \nand available computational resources [4]. Each vector encapsulates the aa type and its \nsurrounding sequence context. Protein language models learn patterns and relationships \nwithin input protein sequences during pre-training through self-supervised masking \ntasks. In this process, aa residues in the input protein sequence are randomly masked, \nand the training objective is to predict the identity of these masked residues based on \ncontextual clues from the surrounding residues. The goal is to minimize an objective \nfunction, which represents the negative logarithm likelihood of the true aa residue given \nthe masked sequence [4, 5, 9]. Currently, one of the largest protein language models is \nESM-2 (Evolutionary Scale Model-2) with 15 billion parameters, trained on ~ 65 million \nunique protein sequences [5].\nProtein language models offer several advantages: The encoder-generated vectors \ninherently encode a spectrum of features, encompassing biochemical aa properties, \nspecies information, structural homology at the superfamily and fold level, sequence \nalignment within a protein family, secondary and tertiary structures, long-range con -\ntacts, and protein design principles [4]. These learned representations can be indepen -\ndently utilized to train deep neural networks for various classification tasks, including \nthe sequence-based prediction of secondary structures and long-range contacts [4], ter -\ntiary structures [5], inverse folding [10], and the impact of mutations on protein func -\ntion [11–13]. For example, the representations learned ESM-2 have been used to train \nEMSFold [5], a model capable of predicting 3D structure using only a single sequence as \ninput. In contrast, AlphaFold2 [14] requires time-consuming multiple sequence align -\nment. Thus, protein language model representations eliminated the need to search for \nevolutionarily related sequences to construct a multiple sequence alignment, enhancing \nprediction speed. This enabled proteome-level predictions and the discovery of new pro-\ntein structures and functions [5, 6, 15]. Additionally, protein language models have often \noutperformed current prediction methods across various classification tasks [4, 5, 16]. \nPage 3 of 10\nSargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \n \nThey have shown the ability to transfer knowledge learned from sequences to improve \nantibody binding affinity by suggesting high-fitness aa substitutions based solely on \nthe wild-type antibody sequence [17]. They can also design proteins by generating new \nprotein sequences and corresponding predicted structures that fulfill user-defined con -\nstraints such as secondary structure, solvent accessibility, fold and active/binding-sites \n[18]. For example, the protein language model, ProGen, can generate artificial functional \nproteins across protein families [19].\nWhile protein language models can capture information representing different levels \nof protein organization from sequence data alone, their ability to learn and extract spe -\ncific residue properties from limited data remains uncertain. This is of particular interest \nfor two key reasons. First, gathering experimental data for specific aa residue properties \nsuch as PPI-hotspots can be time-consuming and resource-intensive, resulting in limited \ndata. Second, large language models have shown the capability to adapt effectively to \nnew language tasks with minimal examples (so-called few-shot learners) [20]. Thus, we \naim to assess the possibility of using protein language models and sparse data of a certain \nproperty to predict that property accurately. Specifically, we assessed whether protein \nlanguage models can discern intricate details from a limited dataset of experimentally \nknown PPI-hotspots that make substantial contributions to PPIs [7, 21]. Since a resi -\ndue’s functional role is influenced by its local environment [22], we harnessed informa -\ntion within encoder-generated vectors encompassing residue types and their sequence \ncontext. We propose using these vector elements as features/descriptors for machine-\nlearning algorithms to identify PPI-hotspots solely from protein sequences. For train -\ning and validation, we employed a dataset comprising 414 experimentally determined \nPPI-hotspots and 504 nonhot spots. This dataset had been previously used to train an \nensemble of classifiers to identify PPI-hot spots using the free protein structure [23]. The \nresults show that a subset of randomly selected features suffices for robust PPI-hot spot \nprediction, with performance comparable to models using all elements of encoder-gen -\nerated vectors as features. The performance of our approach is also comparable to that \nof a model trained on 10 residue features, which requires the free protein structure as \ninput. Hence, protein language models can discern the few PPI-hot spots from sequence \nalone, underscoring their potential in deciphering protein intricacies, even when data \nare sparse. By following our strategy, one can train new predictors to classify protein \nresidues into desired classes (exemplified herein with hot spots or nonhot spots) using \nonly sequence data, thereby avoiding the need for dedicated feature engineering, even \nwith a limited training dataset.\nMethods\nDataset\nFor training and validation, we employed a dataset consisting of 414 experimentally \nconfirmed PPI-hot spots and 504 PPI-nonhot spots. The 414 PPI-hot spots are found \nin 158 nonredundant proteins with free structures and were obtained from the updated \nPPI-Hotspot +  PDBBM(1.1) [7, 23]. These PPI-hot spots were derived from two sources: \n(i) mutations in the ASEdb [24] and SKEMPI 2.0 [25] database that resulted in a reduc -\ntion of protein binding free energy by ≥ 2 kcal/mol, and (ii) mutations in UniProtKB [26] \nthat were manually curated to significantly impair/disrupt PPIs. The 504 experimentally \nPage 4 of 10Sargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \nconfirmed PPI-nonhot spots are found in 75 nonredundant proteins with free struc -\ntures. They were chosen based on (i) mutations in the ASEdb [24] and SKEMPI 2.0 [25] \ndatabase that did not alter the protein binding free energy by ≥ 0.5  kcal/mol, and (ii) \nmutations in UniProtKB [26] curated not to perturb PPIs.\nRepresentations learned by the ESM‑2 protein language model\nTo capture the local environment surrounding a target residue, we considered a 101-aa \nsequence, spanning 50 residues on each side of the target residue. If the target residue \nwas located near the N- or C-terminus, it was positioned at the N- or C-terminus within \nthe 101-aa sequence. This process generated sequences with an average of 34 ± 7% simi-\nlarity. The sequence length was chosen to strike a balance—it is not overly long to bur -\nden memory during computations, yet not too short to overlook information from the \nlocal protein environment. These 101-aa sequences were then fed into the ESM-2 pro -\ntein language model. Among the various pretrained ESM-2 models, we chose the esm2_\nt33_650M_UR50D trained model as a representative due to its transformer architecture \nusing 33 layers and 650 million parameters [5], and extensive pre-training on UniRef50 \n[7]. We direct the reader to reference [5] for details about the model’s training, valida -\ntion, and performance metrics. The final layer of this model provided an N-dimensional \n(N = 1280) embedding vector for the target residue, yielding 1280 features as input for \nmachine-learning algorithms to identify if the target residue is a PPI-hot spot or not.\nModel training and validation with full and subset sequence features\nWe conducted model training and validation using all features derived from the learned \nrepresentations and another using only a subset of these features. For model train -\ning and validation, we use an automatic machine-learning (AutoML) framework, viz., \nAutoGluon (https:// auto. gluon. ai/). We chose AutoGluon due to its robustness and \nuser-friendliness, enabling us to explore various machine-learning approaches and \ntheir combinations simultaneously and automatically. It has been validated in differ -\nent applications [27, 28]. Furthermore, passing transformer-learned representations to \ndownstream machine-learning approaches for making predictions has been success -\nfully demonstrated [28, 29]. Specifically, we employed AutoGluon’s AutoTabular module, \nwhich automates the training and validation process via a stacked ensemble comprising \ndiverse models, including XGBoost, CatBoost, GBM, random forests, and neural net -\nworks [30].\nWe chose the F1 score as a single evaluation metric for model training because it bal -\nances precision and recall. The dataset was randomly split into three sets with propor -\ntions of 20%, 10%, and 70%, and models were trained on each set using the F1 score \nalong with the best-quality preset. Using the first split dataset (20%), we initially trained \na model using all 1280 features. To assess the relative importance of the 1,280 encoder-\nprovided features, we employed the smallest split dataset (10%) for permutation test -\ning, as implemented in the AutoGluon package: In this feature importance test, values \nfor a given feature (column) were randomly shuffled across different residues (rows) and \nsupplied as input to the model. The importance score for each feature is computed by \ncomparing the model’s performance on the original dataset with its performance on \nthe permuted datasets. Based on the importance scores derived from the permutation \nPage 5 of 10\nSargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \n \ntest results of 20 shuffled sets, features were ranked in order of their contribution to the \nmodel’s performance. We selected the top k-ranked features ( k = 10, 20, 30, 40, 50, 100, \n200, 300, 400, 500, 700, 1000) or all 1280 features for retraining the model on the largest \nsplit dataset (70%). The resulting PPI-hot spot prediction model using k selected fea -\ntures is referred to as PPI-HotspotPLM,k where the superscript PLM denotes Protein Lan-\nguage Model and the superscript k is the number of selected features chosen. To assess \nwhether prediction quality depends on the selection of specific features or the mere \nnumber of features, we randomly selected k features, instead of selecting them based \non permutation testing, and trained the model on the same 70% split dataset used to \nderive PPI-HotspotPLM,k. The resulting PPI-hot spot prediction model using k randomly \nselected features is referred to as PPI-HotspotPLM,k-random.\nTo mitigate the influence of outlier values and ensure the stability of our results, the \nentire procedure depicted in Fig.  1 was repeated a sufficient number of times. For each \nrepetition, a different random seed was used to randomly split the original database into \nthree sets. Across the repetitions, we calculated the average F1 score for each model and \ncompared the differences in averages for the different models to gauge the consistency \nand stability of the results. We found that 20 repetitions allowed us to detect a statisti -\ncally significant difference of 0.01 in the F1 score.\nModel training and validation with sequence and structure‑based features\nInstead of using features from protein language models as input, we performed model \ntraining and validation on the same 70% split dataset as PPI-Hotspot PLM,k using a set of \n10 features/residue and the free protein structure as input (Fig.  2). These residue fea -\ntures encompass sequence, structural, and stability attributes including the aa type, con -\nservation score, secondary structure, solvent-accessible surface area (SASA), gas-phase \nenergy and its components, as well as the polar and nonpolar solvation free energy. For \neach residue, the conservation score was derived from ConSurf [31, 32], which requires \nFig. 1 Model training and validation using features derived from the representations learned by the ESM-2 \nprotein language model. The target residue, together with its sequence neighbor aa residues, is passed to \nthe ESM-2 encoder, which produces an N-dimensional (N = 1280) embedding vector for each residue in the \nsequence. The 1,280 elements of each vector were supplied as a set of input features for training a model on \na 20% split dataset. All features or a reduced set of k (k < N) features were selected either randomly or based \non AutoGluon’s feature importance test for training a model on a 70% split dataset\nPage 6 of 10Sargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \nsearch and selection of sequences for multiple sequence alignment. Using the free pro -\ntein structure, the residue’s secondary structure was based on the DSSP program [33], its \nSASA was computed using FreeSasa [34], and the per-residue energy/free energy contri-\nbutions were estimated using the MMPBSA (Molecular Mechanics Poisson-Boltzmann \nSurface Area) module in AmberTools [35]. For details on the calculations, we direct the \nreader to the work of Chen et  al. [23] The final PPI-hot spot prediction model based \non these 10 features is named PPI-Hotspot ID,10, where PPI-HotspotID is the name of the \nmethod for identifying PPI-hot spots using the free protein structure in reference [23] \nand the superscript 10 indicates that 10 residue features were employed.\nResults\nPerformance of PPI‑hotspotPLM,1280 versus PPI‑hotspotID,10\nIdentifying PPI-hot spots is important for understanding protein function, engineer -\ning proteins, and designing PPI modulators. To identify PPI-hot spots, we trained PPI-\nHotspotPLM,k models using k sequence-based features as well as PPI-Hotspot ID,10 using \n10 residue features on the same 70% split dataset. The mean validation F1 score of the \nPPI-HotspotPLM,1280 models using all 1280 input features was 0.69 ± 0.018, which is sim-\nilar to the validation F1 score of 0.71 ± 0.002 achieved by the PPI-Hotspot ID,10 (Table 1). \nHowever, PPI-HotspotID,10 required as input the free protein structure to compute the \nper-residue energy/free energy contributions as well as multiple sequence alignment \nto compute the conservation score, which may take up to 30 min [23]. In contrast, the \nFig. 2 Model training and validation using sequence, structural, and stability attributes. For each residue, we \ncomputed the aa type, conservation score, secondary structure, SASA, gas-phase energy and the respective \ncomponents, as well as the polar and nonpolar solvation free energy. All 10 features were used as input for \nthe AutoGluon training and validation procedure\nTable 1 Performance of PPI-Hotspot ID,10, PPI-Hotspot PLM,1280, PPI-Hotspot PLM,300, and PPI-\nHotspotPLM,300-random on the same 70% split datasets\nMethod PPI‑HotspotID,10 PPI‑HotspotPLM,1280 PPI‑HotspotPLM,300 PPI‑HotspotPLM,300‑random\nArchitecture Ensemble Ensemble + ESM2 Ensemble + ESM2 Ensemble + ESM2\n# of features 10 1280 300 300\nInput Structure Sequence Sequence Sequence\nValidation F1-score 0.71 ± 0.002 0.69 ± 0.018 0.71 ± 0.02 0.70 ± 0.016\nPage 7 of 10\nSargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \n \nencoder generated features in less than a minute. The similarity in validation F1 scores \nsuggests that unsupervised learning can capture the functional properties of residues \nencoded within aa sequences during evolution.\nPerformance of PPI‑hotspotPLM,k as a function of the number of features, k\nWe examined the effect of selecting the top k-ranked features on the performance of the \nPPI-HotspotPLM,k models with increasing k. The results in Fig.  3a show that increasing k \nled to an overall improvement in performance (increase in the F1-score) up to k ~ 300. \nBeyond this threshold, further increases in k resulted in a slight decline in the F1 score. \nWe hypothesize that initially, the inclusion of additional important features contributes \nvaluable information, thereby improving the training of the model, but once the number \nof features greatly exceeds the size of the training dataset, AutoGluon’s training routine \nadopts a more conservative strategy to prevent overfitting, resulting in a slight reduction \nin final validation precision. We observed a similar trend for randomly selected features, \nas shown in Fig. 3b.\nPerformance of PPI‑hotspotPLM,k versus PPI‑hotspotPLM,k‑random\nComparison of the F1 scores from PPI-Hotspot PLM,k models using k  importance-based \nfeatures and PPI-HotspotPLM,k-random models using k  randomly selected features models \n(Additional file 1: Table S1) indicates that the number of features k plays a crucial role in \ndetermining the final model precision. To assess whether the results from importance-\nbased feature selection differ significantly from those of random feature selection we \ncompared the means of F1 scores for a given number of features, k , using the statistical \nt-test. No observable difference was found; e.g., for k  = 300, the mean F1 scores for PPI-\nHotspotPLM,300 (0.71 ± 0.02) and PPI-HotspotPLM,k-random (0.70 ± 0.016) are nearly identical \n(Table 1). This is consistent with the importance scores obtained from the feature impor-\ntance test: The features deemed most important contribute no more than 0.03 to the final \nF1 score. We also attempted to enhance the importance feature test by increasing the num-\nber of shuffles in the permutation test. This led to a significant increase in computing time \nFig. 3 Validation F1 scores for the models trained as a function of the number of features, k. Features were \nselected using either feature importance test (a) or randomly (b). The dot in the figure corresponds to the \nmean, whereas the error bar denotes the standard deviation\nPage 8 of 10Sargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \nfor the importance feature test without any significant improvement in performance com-\npared to random feature selection.\nDiscussion\nOur primary objective herein was not to develop a highly accurate sequence-based PPI-hot \nspot prediction method, but rather to showcase the potential of transformer-based protein \nlanguage models in extracting critical information from sparse datasets whose total number \nof entries is comparable to or less than the number of elements (N = 1280) in the encoder-\ngenerated vector. This is important as experimental collection of certain types of data, \nsuch as PPI-hot spots, remains challenging yielding insufficient data for machine learning \nalgorithms to extract meaningful representations. As exemplified herein with elusive PPI-\nhot spots, protein language models can offer a solution, as their encoder-generated vec -\ntors, which encode protein residues and their contexts, provide valuable input features for \nsubsequent machine learning training to predict certain aa residue properties. These input \nfeatures can be substantially reduced in number through feature importance ranking or \nrandom sampling. Our proposed approach is practical for real-world applications: Model \ntraining using  AutoGluon  has proven successful even without GPU support. Although \nGPU usage is preferred for efficiency, ESM models exhibit sufficient speed to run on CPUs \nalone, requiring as little as 16 GB of memory. Furthermore, to facilitate ease of use, we pro-\nvide a notebook in the GitHub repository that runs on Google Colab, enabling users to \ninput their sequences and obtain predicted PPI-hot spots. Note that we sought to provide \na fair comparison by using the same dataset to compare the PPI-hot spot predictions based \non protein language-derived features and those based on features requiring the free protein \nstructure. As other sequence-based PPI-hot spot prediction methods have been trained on \ndifferent datasets, they could not be fairly compared with our PPI-HotspotPLM,1280 model. \nDiscrepancies in predictions may arise from differences in underlying training datasets, \nspecific features, or machine-learning methodologies.\nLimitations and future work\nAn evident drawback of the approach outlined here is the lack of a clear explanation as to \nwhy specific features are crucial for determining certain residue properties and how they \ncontribute to PPI-hot spot predictions. This limitation aligns with the inherent lack of \ninterpretability of large language models; currently, understanding the inner workings of \nthese models remains elusive. Another limitation is the representativeness of the dataset. \nEven though our dataset includes data not only from the ASEdb [24] and SKEMPI 2.0 [25] \ndatabase, but also UniProtKB [26], it is still not comprehensive as experimental studies do \nnot sample all representative protein interactions. Future improvements in model architec-\nture, dataset size (including more experimentally confirmed PPI-hot spots and PPI-nonhot \nspots), and computational resources may enhance the accuracy of sequence-based PPI-hot \nspot predictions.\nConclusions\nWe have presented a general, robust, and straightforward approach for the train -\ning and validation of predictive models using protein language models that can effec -\ntively extract valuable information from sparse protein datasets. Specifically, the ESM-2 \nPage 9 of 10\nSargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \n \nmodel showed promising results in predicting PPI-hot spots using all 1,280 features as \nwell as using a subset of features fewer than the number of entries in the dataset. Future \nimprovements in model architecture and increased dataset sizes may further enhance \nthe accuracy of PPI-hot spot predictions, which would aid in understanding protein \nfunction and drug design. The ability to do this from just the sequence alone would save \ntime and costs compared to traditional experimental methods. This study further dem -\nonstrates that even with sparse datasets, encoder-generated vectors, which encompass \nresidue information and their contexts, offer valuable input features for machine learn -\ning to make reliable predictions. In addition, we provide a notebook as part of our source \nrepository, allowing users to run PPI-hot-spot predictions on their protein sequences.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 024- 05737-2.\nAdditional file 1: Supplementary Table S1. Performance of PPI-HotspotPLM,k and PPI-HotspotPLM,k-random on the \nsame 70% split datasets for all k.\nAcknowledgements\nNot applicable.\nAuthor contributions\nKS performed the algorithm design, implementation, result evaluation, and produced the first draft. CL wrote and edited \nthe manuscript. All authors read and approved the final manuscript.\nFunding\nThis research was supported by Academia Sinica (AS-IA-107-L03) and the Ministry of Science and Technology (MOST-98-\n2113-M-001-011), Taiwan.\nAvailability of data and materials\nSupporting code and datasets are available at https:// github. com/ karsar/ PPI_ hotsp ot_ seq.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 3 January 2024   Accepted: 11 March 2024\nReferences\n 1. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z et al. A survey of large language \nmodels; 2023. http:// arxiv. org/ abs/ 2303. 18223\n 2. Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM. Unified rational protein engineering with sequence-based \ndeep representation learning. Nat Methods. 2019;16(12):1315–22.\n 3. Heinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes F, Rost B. Modeling aspects of the language of life \nthrough transfer-learning protein sequences. Bmc Bioinform. 2019;20(723):1–17.\n 4. Rives A, Meier J, Sercu T, Goyal S, Lin ZM, Liu JS, Guo DM, Ott M, Zitnick CL, Ma J, et al. Biological structure and \nfunction emerge from scaling unsupervised learning to 250 million protein sequences. P Natl Acad Sci USA. \n2021;118(15):e2016239118.\n 5. Lin ZM, Akin H, Rao RS, Hie B, Zhu ZK, Lu WT, Smetanin N, Verkuil R, Kabeli O, Shmueli Y, et al. Evolutionary-scale \nprediction of atomic-level protein structure with a language model. Science. 2023;379(6637):1123–30.\n 6. Le NQK. Leveraging transformers-based language models in proteome bioinformatics. Proteomics. \n2023;23(23–24):e2300011.\n 7. Chen YC, Chen YH, Wright JD, Lim C. PPI-HotspotDB: database of protein-protein interaction hot spots. J Chem Inf \nModel. 2022;62(4):1052–60.\nPage 10 of 10Sargsyan and Lim  BMC Bioinformatics          (2024) 25:115 \n 8. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I. Attention is all you need; \n2017. http:// arxiv. org/ abs/ 1706. 03762\n 9. Su J, Han C, Zhou Y, Shan J, Zhou X, Yuan F: SaProt: protein language modeling with structure-aware vocabulary. \nbioRxiv 2023:2023.2010.2001.560349\n 10. Hsu C, Verkuil R, Liu J, Lin Z, Hie B, Sercu T, Lerer A, Rives A: Learning inverse folding from millions of predicted \nstructures. In: Kamalika C, Stefanie J, Le S, Csaba S, Gang N, Sivan S, editors Proceedings of the 39th international \nconference on machine learning; proceedings of machine learning research, PMLR 2022; pp. 8946–8970\n 11. Meier J, Rao R, Verkuil R, Liu J, Sercu T, Rives A. Language models enable zero-shot prediction of the effects of muta-\ntions on protein function. bioRxiv 2021:2021.2007.2009.450648\n 12. Chowdhury R, Bouatta N, Biswas S, Floristean C, Kharkar A, Roy K, Rochereau C, Ahdritz G, Zhang JN, Church GM, \net al. Single-sequence protein structure prediction using a language model and deep learning. Nat Biotechnol. \n2022;40(11):1692–1692.\n 13. Wu R, Ding F, Wang R, Shen R, Zhang X, Luo S, Su C, Wu Z, Xie Q, Berger B et al. High-resolution de novo structure \nprediction from primary sequence. bioRxiv 2022:2022.2007.2021.500999\n 14. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Zidek A, Potapenko A, \net al. Highly accurate protein structure prediction with AlphaFold. Nature. 2021;596(7873):583–9.\n 15. Durairaj J, Waterhouse AM, Mets T, Brodiazhenko T, Abdullah M, Studer G, Tauriello G, Akdel M, Andreeva A, Bateman \nA, et al. Uncovering new families and folds in the natural protein universe. Nature. 2023;622:646–53.\n 16. Strodthoff N, Wagner P , Wenzel M, Samek W. UDSMProt: universal deep sequence models for protein classification. \nBioinformatics. 2020;36(8):2401–9.\n 17. Hie BL, Shanker VR, Xu D, Bruun TUJ, Weidenbacher PA, Tang SG, Wu WS, Pak JE, Kim PS. Efficient evolution of human \nantibodies from general protein language models. Nat Biotechnol. 2023;42:275–83.\n 18. Hie B, Candido S, Lin Z, Kabeli O, Rao R, Smetanin N, Sercu T, Rives A: A high-level programming language for gen-\nerative protein design. bioRxiv 2022:2022.2012.2021.521526\n 19. Madani A, Krause B, Greene ER, Subramanian S, Mohr BP , Holton JM, Olmos JL, Xiong CM, Sun ZZ, Socher R, \net al. Large language models generate functional protein sequences across diverse families. Nat Biotechnol. \n2023;41(8):1099–106.\n 20. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P , Neelakantan A, Shyam P , Sastry G, Askell A et al. Lan-\nguage models are few-shot learners; 2020. http:// arxiv. org/ abs/ 2005. 14165\n 21. Fischer TB, Arunachalam KV, Bailey D, Mangual V, Bakhru S, Russo R, Huang D, Paczkowski M, Lalchandani V, \nRamachandra C, et al. The binding interface database (BID): a compilation of amino acid hot spots in protein inter-\nfaces. Bioinformatics. 2003;19(11):1453–4.\n 22. Mazmanian K, Sargsyan K, Lim C. How the local environment of functional sites regulates protein function. J Am \nChem Soc. 2020;142(22):9861–71.\n 23. Chen Y, Sargsyan K, Wright J, Chen Y, Huang Y, Lim C: PPI-hotspotID: a method for detecting protein–protein interac-\ntion hot spots from the free protein structure; 2023. https:// doi. org/ 10. 21203/ rs.3. rs- 34001 69/ v1\n 24. Thorn KS, Bogan AA. ASEdb: a database of alanine mutations and their effects on the free energy of binding in \nprotein interactions. Bioinformatics. 2001;17(3):284–5.\n 25. Jankauskaite J, Jiménez-García B, Dapkunas J, Fernández-Recio J, Moal IH. SKEMPI 2.0: an updated benchmark \nof changes in protein-protein binding energy, kinetics and thermodynamics upon mutation. Bioinformatics. \n2019;35(3):462–9.\n 26. Bateman A, Martin MJ, Orchard S, Magrane M, Alpi E, Bely B, Bingley M, Britto R, Bursteinas B, Busiello G, et al. UniProt: \na worldwide hub of protein knowledge. Nucleic Acids Res. 2019;47(D1):D506–15.\n 27. Schwen LO, Schacherer D, Geißler C, Homeyer A. Evaluating generic AutoML tools for computational pathology. \nInform Med Unlock. 2022;29:100853.\n 28. Cheng Y, Wang H, Xu H, et al. Co-evolution-based prediction of metal-binding sites in proteomes by machine learn-\ning. Nat Chem Biol. 2023;19:548–55.\n 29. Raza A, Uddin J, Almuhaimeed A, Akbar S, Zou Q, Ahmad A. AIPs-SnTCN: predicting anti-inflammatory peptides \nusing fasttext and transformer encoder-based hybrid word embedding with self-normalized temporal convolu-\ntional networks. J Chem Inf Model. 2023;63(21):6537–54.\n 30. Erickson N, Mueller J, Shirkov A, Zhang H, Larroy P , Li M, Smola A. AutoGluon-tabular: robust and accurate AutoML \nfor structured data; 2020. http:// arxiv. org/ abs/ 2003. 06505\n 31. Glaser F, Pupko T, Paz I, Bell RE, Bechor-Shental D, Martz E, Ben-Tal N. ConSurf: identification of functional regions in \nproteins by surface-mapping of phylogenetic information. Bioinformatics. 2003;19(1):163–4.\n 32. Landau M, Mayrose I, Rosenberg Y, Glaser F, Martz E, Pupko T, Ben-Tal N. ConSurf 2005: the projection of evolutionary \nconservation scores of residues on protein structures. Nucleic Acids Res. 2005;33:W299–302.\n 33. Kabsch W, Sander C. Dictionary of protein secondary structure: pattern-recognition of hydrogen-bonded and geo-\nmetrical features. Biopolymers. 1983;22(12):2577–637.\n 34. Mitternacht S. FreeSASA: an open source C library for solvent accessible surface area calculations. F1000Res. \n2016;5:189.\n 35. Case DA, Aktulga HM, Belfon K, Cerutti DS, Cisneros GA, Cruzeiro VWD, Forouzesh N, Giese TJ, Götz AW, Gohlke H, \net al. The AmberTools. J Chem Inf Model. 2023;63(20):6183–91.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.7922704815864563
    },
    {
      "name": "Computer science",
      "score": 0.6630822420120239
    },
    {
      "name": "Machine learning",
      "score": 0.5339288711547852
    },
    {
      "name": "Protein sequencing",
      "score": 0.5234239101409912
    },
    {
      "name": "Protein structure prediction",
      "score": 0.5190611481666565
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5158159136772156
    },
    {
      "name": "Language model",
      "score": 0.49621158838272095
    },
    {
      "name": "Protein structure",
      "score": 0.44296497106552124
    },
    {
      "name": "CASP",
      "score": 0.4329177737236023
    },
    {
      "name": "Protein–protein interaction",
      "score": 0.4198916256427765
    },
    {
      "name": "Computational biology",
      "score": 0.41246312856674194
    },
    {
      "name": "Bioinformatics",
      "score": 0.32106804847717285
    },
    {
      "name": "Biology",
      "score": 0.21936804056167603
    },
    {
      "name": "Peptide sequence",
      "score": 0.18590053915977478
    },
    {
      "name": "Genetics",
      "score": 0.10777893662452698
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096666",
      "name": "Institute of Biomedical Sciences, Academia Sinica",
      "country": "TW"
    }
  ]
}