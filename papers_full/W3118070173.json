{
  "title": "KS@LTH at SemEval-2020 Task 12: Fine-tuning Multi- and Monolingual Transformer Models for Offensive Language Detection",
  "url": "https://openalex.org/W3118070173",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3114379776",
      "name": "Kasper Socha",
      "affiliations": [
        "Lund University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2954034987",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3032261270",
    "https://openalex.org/W3032237992",
    "https://openalex.org/W3030332779",
    "https://openalex.org/W2954479967",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W4381683870",
    "https://openalex.org/W2936832793",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2983342160",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2805551365",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3014459433",
    "https://openalex.org/W3013027210",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2912102236",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3012507282",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2889438178",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962993339",
    "https://openalex.org/W2972463128",
    "https://openalex.org/W3154956759",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W3022992164",
    "https://openalex.org/W3115903740",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4360886147",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "This paper describes the KS@LTH system for SemEval-2020 Task 12 OffensEval2: Multilingual Offensive Language Identification in Social Media. We compare mono- and multilingual models based on fine-tuning pre-trained transformer models for offensive language identification in Arabic, Greek, English and Turkish. For Danish, we explore the possibility of fine-tuning a model pre-trained on a similar language, Swedish, and additionally also cross-lingual training together with English.",
  "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2045–2053\nBarcelona, Spain (Online), December 12, 2020.\n2045\nKS@LTH at SemEval-2020 Task 12: Fine-tuning multi- and monolingual\ntransformer models for offensive language detection\nKasper Socha\nLund University / Lund, Sweden\nfte10kso@student.lu.se\nAbstract\nThis paper describes the KS@LTH system for SemEval-2020 Task 12 OffensEval2: Multilingual\nOffensive Language Identiﬁcation in Social Media. We compare mono- and multilingual models\nbased on ﬁne-tuning pre-trained transformer models for offensive language identiﬁcation in\nArabic, Greek, English and Turkish. For Danish, we explore the possibility of ﬁne-tuning a model\npre-trained on a similar language, Swedish, and additionally also cross-lingual training together\nwith English. Overall we ﬁnd that monolingual models achieve higher macro-averaged F1 score.\nWith cross-lingual training of Danish together with English, we achieve better results than by\ntraining on the small Danish dataset alone. For Arabic, Danish, English, Greek, and Turkish, we\nobtained macro-averaged F1 scores of 0.890, 0.775, 0.916, 0.848, and 0.810 ranking 6th, 5th, 6th,\n3rd and 4th for each language, respectively.\n1 Introduction\nOffensive language is a prevalent phenomenon in many online communities and social media platforms.\nDue to the vast amount of content, it is often infeasible to manually moderate all user submitted content.\nComputational methods for identifying this type of content is one possible way to help mitigate the\nproblem. Different aspects of the problem such as aggression (Kumar et al., 2018), cyber bulling\n(Sprugnoli et al., 2018) and hate speech (Malmasi and Zampieri, 2017) have been studied in recent work.\nOffensEval 2019 used a new three-level hierarchical annotation schema to capture multiple aspects of\noffensive language in one framework (Zampieri et al., 2019a).\nWhile much of the previous work is focused on English, offensive language detection is a multilingual\nproblem. Apart from country speciﬁc communities, large social media platforms such as Facebook\nand Twitter have many users interacting in their native tongue. Recently, offensive language detection\naddressed different languages such as German (Wiegand et al., 2018), Arabic (Mulki et al., 2019), Italian\n(Sanguinetti et al., 2018), and Spanish (Fersini et al., 2018). In OffensEval 2020, the ﬁrst level task of\noffensive language detection has been expanded to cover ﬁve languages, Arabic, Danish, English, Greek,\nand Turkish.\nTransfer learning is nothing new in NLP but over time, the pre-training has become more complex,\nincorporating more context. In recent years, language models based on the transformer architecture\npre-trained on large amounts of unlabeled text and then ﬁne-tuned on downstream tasks have been used\nto achieve state-of-the-art (SOTA) results on many natural language benchmarks (Devlin et al., 2018;\nLiu et al., 2019; Yang et al., 2019). In OffensEval 2019, seven of the top ten models used BERT in\nsome way (Zampieri et al., 2019b). One of the advantages of transfer learning is that it can potentially\nreduce the amount of labeled data that is needed. The model can learn general features of language from a\nlarge unannotated corpus during pre-training. Task speciﬁc features can then be learned from a smaller\nannotated corpus. On some datasets, using a pre-trained language model has shown to match the results\nof models trained from scratch on ten times more data. Adding language model ﬁne-tuning on unlabeled\ndomain speciﬁc text can potentially reduce the need for labeled data even more (Howard and Ruder, 2018).\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n2046\nOne obstacle to using large transformer models is that the pre-training step is expensive. The Megatron-\nLM has 8.3 billion parameters and was trained over 9 days on 512 GPUs (Shoeybi et al., 2019). In\ncomparison, the ﬁne-tuning step is relatively inexpensive. This makes model sharing an important part\nof applying large transformer models to many tasks. The HuggingFace Transformerslibrary provides a\nplatform for sharing models developed by researchers and the community, and a uniﬁed API for using\nthem (Wolf et al., 2019).\nOne additional challenge with multilingual offensive language detection is low resource languages.\nSuch languages might lack both unlabeled data for pre-training and labeled data for ﬁne-tuning. One\npossible solution in such cases is to use multilingual models. Such models can achieve lower perplexity\nthan monolingual models for language modeling of low resource languages (Conneau and Lample, 2019).\nIn some contexts, multilingual models can even outperform monolingual models on downstream tasks\n(Conneau et al., 2019). In the case of lacking labeled data, they have also shown to perform well on\nzero-shot cross-lingual classiﬁcation tasks. This type of transfer works best between typologically similar\nlanguages. However, transfer is possible to some extent even between languages with different scripts\n(Pires et al., 2019).\nThis paper describes our system for OffensEval 2020 (Zampieri et al., 2020). We participated in\nSub-task A: Offensive language identiﬁcation for all language tracks. Based on the recent success of\nthe transformer architecture, we compared monolingual BERT models for Arabic, English, Greek, and\nTurkish with the XLM-R multilingual model (Conneau et al., 2019). We found that the monolingual\nmodels outperform the multilingual models for all languages on the development data. We used models\navailable through the HuggingFace Transformers library. Since no monolingual models were available\nfor Danish, we initially compared a Swedish BERT model with multilingual XLM-R. We found that\nthe Swedish model worked reasonably well on the development data, while XLM-R only predicted the\nmajority class for most runs. We hypothesized that this is due to the small and imbalanced Danish dataset;\nsimilar high variance results have been seen for BERT in Devlin et al. (2018) and Phang et al. (2019). To\nget around the problem of the small dataset, we tried cross-lingual training of Danish and English using\nXLM-R which outperformed the Swedish BERT model.\nIn Section 2 we give a short description of the task and data used. Section 3 presents our approach,\ndescribing data preprocessing, models and training approach. Section 4 shows our results on the test data\nfor OffensEval 2020.\n2 Task and Data\nOffensEval 2020 uses a multilingual dataset of posts from Twitter, tweets, with annotations following the\nhierarchical annotation schema proposed by Zampieri et al. (2019a). Only the ﬁrst level of annotation is\nprovided for all languages. This level discriminates between two kinds of tweets:\n• Offensive (OFF):Tweets containing any form of offensive language. This includes insults, threats,\nand profanity.\n• Not Offensive (NOT):Tweets not containing any form of unacceptable language.\nThe goal of the task is to distinguish between offensive and not offensive tweets. Macro-averaged\nF1-score is used as evaluation metric.\nTable 1 shows a summary of the labeled training datasets for each language. All the datasets are\nimbalanced to some extent, with the majority of tweets being labeled as not offensive. Danish is the most\nextreme in this regard, having only 13% of tweets labeled as offensive. We can also see that the size of the\ndatasets varies quite a bit, with Turkish having about ten times as many labeled instances as Danish.\nFor English the labeled dataset is the same as for OffensEval 2019. In addition to the manually annotated\ndata, about nine million additional tweets labeled using unsupervised methods are provided for English\n(Rosenthal et al., 2020). These tweets have a conﬁdence score and a standard deviation. For English, we\nalso use additional publicly available data from Kaggle1 and Davidson et al. (2017)2.\n1https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n2https://github.com/t-davidson/hate-speech-and-offensive-language\n2047\nLanguage OFF NOT Total\nTurkish (C ¸¨oltekin, 2020) 6131 (19%) 25625 (81%) 31756\nEnglish (Zampieri et al., 2019a) 4640 (33%) 9460 (67%) 14100\nGreek (Pitenis et al., 2020) 2486 (28%) 6257 (72%) 8743\nArabic (Mubarak et al., 2020) 1589 (20%) 6411 (80%) 8000\nDanish (Sigurbergsson and Derczynski, 2020) 384 (13%) 2576 (87%) 2960\nTable 1: Training dataset size and class distribution for each language.\n3 Approach\n3.1 Data Preprocessing\nA minimal amount of preprocessing was done. We applied only two operations to all languages:\n1. Multiple consecutive user mentions were replaced with a single @User to reduce sequence length\nand noise.\n2. All tweets were truncated or padded to a common length. This length was chosen separately for each\nlanguage to be the smallest length longer than 95% of all tweets in the training set.\nAdditional processing was done on the external datasets for English. We sampled about 10,000\nadditional tweets from Davidson et al. (2017). Samples were chosen such that the complete labeled tweet\ndataset became balanced. Tweets with at least 3 annotators labeling it as either offensive language or\nhate speech were labeled as OFF. Tweets with all annotators agreeing on theneither-class were labeled\nas NOT. A balanced dataset of 13,000 Wikipedia comments, from the Kaggle dataset, were also added.\nTo be consistent with the Twitter data, all comments were at most 280 characters. Any comment having\nat least two of the labels toxic, severe toxic, obscene, threat, insult, or identity hate was labeled as OFF.\nComments with no negative labels were labeled as NOT. For both datasets, we replaced URLs with a URL\ntoken, and for the tweet dataset, we replaced user mentions with @User.\nAdditionally we sampled 400,000 tweets from the English silver standard data using conﬁdence scores\nas weights. These were then ﬁltered down further to the 40,000 tweets with highest conﬁdence using our\nmodel as described in section 3.3.\n3.2 Models\nVaswani et al. (2017) initially introduced the transformer architecture in the context of machine translation.\nWhile previous approaches relied on convolutional and recurrent neural networks, they showed that a\nrelatively simple architecture based on feed-forward neural networks and attention mechanisms could\nprovide better results while being more parallelizable and faster to train. Like previous sequence-to-\nsequence models the transformer consists of two main components: an encoder component and a decoder\ncomponent.\nRadford et al. (2018) trained a left-to-right language model, GPT, using only the decoder part of the\ntransformer and ﬁne-tuned it on multiple downstream tasks with minimal task speciﬁc changes. Devlin et\nal. (2018) showed the importance of bi-directional pre-training for certain types of tasks by obtaining new\nSOTA results on 11 NLP benchmarks, including an almost 8 point improvement on GLUE. Their model\narchitecture, named BERT (Bidirectional Encoder Representations from Transformers), is the architecture\nwe used for all monolingual models apart from English.\nSince the decoder component of the transformer already does masking of subsequent positions, it is\na natural choice for the next word prediction language modeling task used by GPT. To be able to train\na bidirectional language model, BERT instead uses the encoder part of the transformer. Apart from\nincreasing the size, it is almost identical to the initial transformer implementation. BERT consists of a\nstack of encoders, 12 for BERTBASE and 24 for BERTLARGE, compared to 6 in the original transformer.\nEach encoder, in turn, consists of two main parts: a self-attention layer followed by a feed-forward\nneural network. Self-attention is the mechanism which allows the transformer to consider other words\n2048\nin the sequence when encoding the current word. BERT increases the number of attention heads from 8\nin the original Transformer to 12 for BERTBASE and 16 for BERTLARGE. Finally the number of hidden\nunits in the feed-forward neural networks is also increased from 512 to 758 and 1024 for BERTBASE and\nBERTLARGE, respectively.\nWe used pre-trained BERT language models without changes to the base architecture. For the ﬁne-\ntuning step, we followed the approach for single sentence classiﬁcation suggested by Devlin et al. (2018).\nA single fully connected classiﬁcation layer was added to the base model. A special [CLS] token was\nprepended to all inputs. The contextual representation of this token was used as an embedding for the\ncomplete sentence, and passed to the classiﬁcation head. The complete base model was ﬁne-tuned during\ntraining.\nLiu et al. (2019) showed that BERT is undertrained. Their model, RoBERTa, uses exactly the same\narchitecture as BERT. RoBERTa outperforms BERT simply by training on more data, with larger batches,\nfor a longer time. Some additional simple changes in the pre-training approach, such as removing one of\nthe pre-training objectives and training on longer sequences, improved the results even further. This is the\nmonolingual model we used for English. There were no pre-trained RoBERTa models available for the\nother languages. The ﬁne-tuning approach is identical to the one used for BERT.\nSimilarly, in the multilingual context, the XLM-RoBERTa (XLM-R) model we used achieves much\nof its improvement over previous multilingual models by using several orders of magnitude more data\n(Conneau et al., 2019). Conneau et al. (2019) also ﬁnd that vocabulary size has a large impact when many\nlanguages are used. Again XLM-R uses the same model architecture as BERT. However, the increase of\nvocabulary size from 30K to 250K leads to an increase of the total number of parameters from 110M and\n335M to 270M and 550M for the BASE and LARGE models, respectively. All ﬁve languages are present\namong the 100 languages used during pre-training of XLM-R. The ﬁne-tuning approach is identical to the\none used for the previous models.\nA summary of the different pre-trained models that we used for each language is provided below:\nArabic\n• Monolingual: Arabic BERTBASE3\n• Multilingual: XLM-RBASE (Arabic corpus of 28.0GB).\nDanish\n• Monolingual: Swedish BERTBASE4\n• Multilingual: XLM-RBASE (Danish corpus of 45.6GB).\nEnglish\n• Monolingual: English RoBERTaLARGE (Liu et al., 2019).\n• Multilingual: XLM-RLARGE (English corpus of 300.8GB).\nGreek\n• Monolingual: Greek BERTBASE5\n• Multilingual: XLM-RBASE (Greek corpus of 46.9GB).\nTurkish\n• Monolingual: Turkish BERTBASE6\n• Multilingual: XLM-RBASE (Turkish corpus of 20.9GB).\n3https://huggingface.co/asafaya/bert-base-arabic\n4https://huggingface.co/af-ai-center/bert-base-swedish-uncased , https://github.com/\naf-ai-center/SweBERT\n5https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1\n6https://huggingface.co/dbmdz/bert-base-turkish-cased\n2049\nLanguage Model Mean F1 Max F1\nArabic BERT 0.864 0.868\nXLM-R 0.709 0.833\nDanish BERT Swedish 0.745 0.777\nXLM-RDanish 0.464 0.464\nXLM-RDanish+English 0.795 0.813\nEnglish RoBERTa 0.913 0.917\nXLM-R 0.892 0.901\nGreek BERT 0.820 0.845\nXLM-R 0.766 0.798\nTurkish BERT 0.814 0.820\nXLM-R 0.805 0.814\nTable 2: Mean and maximum F1 macro on the development sets for ﬁve random restarts on each language\nand model.\n3.3 Experiments\nWe carried out the initial experimentation and the hyperparameter selection using the English data from\nOffensEval 2019. We followed the ﬁne-tuning procedure recommended for BERT by Devlin et al. (2018).\nWe tested the following parameters, where the best performing values are underlined:\n• Batch size: 16, 32\n• Learning rate (Adam): 5e-5, 3e-5, 2e-5\n• Epochs: 2, 3, 4\nThe dropout was kept constant at 0.1 for all layers. Overall we found that ﬁne-tuning was relatively\ninsensitive to batch size and learning rate. However, most random restarts seemed to overﬁt when using\nmore than 2 epochs. The same hyperparameters were then used for all further experiments.\nFor each language, 20% of the data was set aside as a development set and used for model selection. For\neach model, we ran ﬁve random restarts with different data shufﬂing and classiﬁer head layer initialization.\nThe model with the best macro-averaged F1-score on the development set was then used for submission.\nTable 2 summarizes the results we obtained.\nFor English, the training was done in two steps. Initially, we trained the model using only the labeled\ndata. We then used this model to label 400,000 samples from the silver standard data. We labeled the\n20,000 instances with the highest scores as OFF and the 20,000 instances with the lowest scores as NOT.\nWe ﬁnally added these 40,000 tweets to the training set used to train the ﬁnal model.\nFor Danish, we initially failed to train XLM-R to predict anything other than the majority class. Since\nXLM-R has shown promising cross-lingual transfer results, we tried training Danish together with English.\nWe did this by shufﬂing the Danish training data with the English data from OffensEval 2019. We\nevaluated the models only on the Danish development dataset.\n4 Results\nTable 3 shows our results on the ofﬁcial test data. The ﬁgures are similar to those we obtained on the\ndevelopment dataset. Danish shows the largest drop in performance, going from 0.813 on the development\ndataset to 0.775 on the test dataset. Nonetheless, since the development set was rather small, it might be\ndifﬁcult to conclude on the generalization performance.\n4.1 Impact of external and silver standard data\nPrevious work has shown that models for offensive language detection often generalize poorly to other\ndatasets (Karan and ˇSnajder, 2018; Swamy et al., 2019; Arango et al., 2019). This is especially true when\n2050\nLanguage F1 Rank\nArabic 0.890 6\nDanish 0.775 5\nEnglish 0.916 6\nGreek 0.848 3\nTurkish 0.810 4\nTable 3: Macro averaged F1-score on the test data and competition placement.\nTraining Data F1\nOffensEval19 0.906\nWikipedia 0.909\nDavidson 0.848\nOffensEval19 + Davidson 0.917\nOffensEval19 + Davidson + Wikipedia 0.917\nOffensEval19 + Davidson + Wikipedia + Silver 0.916\nOffensEval19 + Silver 0.915\nTable 4: Results on the English test set using different subsets of the training data. For the combination\nOffensEval19 + Silver, the silver standard data was processed using the approach described previously,\nbut only using OffensEval19 for the initial training.\nevaluating across domains, e.g. between Twitter and Wikipedia, but also within the same domain. Some\nfeatures are likely platform speciﬁc and some datasets focus on speciﬁc aspects of offensive language.\nThe data collection process can also lead to some types of content being overrepresented.\nWe tried to determine the impact of the different English datasets we used by retraining the model on\ndifferent subsets of the data. The results on the test set are shown in table 4. All the labeled datasets\nperform reasonably well on their own. Surprisingly the sampled Wikipedia data performs just as well as\nthe OffensEval 2019 data. The sampled data from (Davidson et al., 2017) performs worse. This might\nbe due to it being smaller and oversampled to contain more offensive tweets. This hypothesis is also\nsupported by the fact that when used with the OffensEval 2019 data, the results are comparable with\nthe submitted model. Finally, the silver standard data seems to be most useful when the original labeled\ndataset is small.\n4.2 Error analysis\nTo get a better understanding of the kind of mistakes the system makes we studied some of the misclassiﬁed\ninstances. To get some indications of what words are important for the classiﬁcation of a given sentence,\nwe applied LIME (Ribeiro et al., 2016). In short, LIME estimates the importance of a word by:\n1. Generating many distorted versions of the original tweet.\n2. Applying the original classiﬁers to the distorted tweets.\n3. Training a white-box model to predict the output of the original classiﬁer given a version of the\ntweet.\nTable 5 shows ﬁve instances from the English OffensEval 2019 dataset, where the classiﬁer assigned a\nhigh conﬁdence to the wrong class. Examples 1 and 2 are very short and the profanity dominates the other\nwords. Both examples look like reasonable classiﬁcations. However, the same thing seems to happen\nin Example 3. The word shit dominates the otherwise inoffensive sentence. Example 4 has no direct\nprofanity. Looking at bigrams using LIME, stinking cute is correctly identiﬁed as inoffensive. Example 5\ndoesn’t seem to have any offensive language. It is possible that it could be considered offensive given\nexternal knowledge about the people mentioned. Given only the tweet, the classiﬁcation looks reasonable.\n2051\n# Tweet Prediction Label\n1 Are you fucking + serious? URL OFF NOT\n2 And dicks +. URL OFF NOT\n3 #Room25 is actually incredible, Noname is the shit+, always has been, and\nI’m seein her in like 5 days in Melbourne. Life is good. Have a nice day.\nOFF NOT\n4 @User Aw she is so stinking – cute+! How old is she now? NOT OFF\n5 #ChristineBlaseyFord is your #Kavanaugh accuser. #Liberals try this EVERY\ntime. #ConﬁrmJudgeKavanaugh URL\nNOT OFF\nTable 5: Examples of misclassiﬁcations for English. Using LIME, we marked words that have a\nlarge impact on the classiﬁcation. A + indicates agreement with the predicted label and a – indicates\ndisagreement.\n5 Conclusions\nIn the context of offensive language detection for multiple languages, we found that ﬁne-tuning transformer\nmodels works well. Monolingual models outperform multilingual models for all languages studied.\nHowever, multilingual models can still be a viable alternative when no monolingual models are available.\nWhen the amount of labeled data is small, they can also be used for cross-lingual transfer. We showed the\npositive effect of cross lingual transfer when augmenting Danish with English.\nAcknowledgments\nI would like to thank Pierre Nugues at Lund University for helpful discussions and feedback on this paper.\nReferences\nAym´e Arango, Jorge P´erez, and Barbara Poblete. 2019. Hate speech detection is not as easy as you may think: A\ncloser look at model validation. In Proceedings of the 42nd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages 45–54.\nC ¸ a˘grı C ¸¨oltekin. 2020. A Corpus of Turkish Offensive Language on Social Media. In Proceedings of the 12th\nInternational Conference on Language Resources and Evaluation. ELRA.\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information\nProcessing Systems 32, pages 7059–7069. Curran Associates, Inc.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an,\nEdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual repre-\nsentation learning at scale. arXiv preprint arXiv:1911.02116.\nThomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection\nand the problem of offensive language. In Eleventh international aaai conference on web and social media.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding.\nElisabetta Fersini, Paolo Rosso, and Maria Anzovino. 2018. Overview of the task on automatic misogyny identiﬁ-\ncation at ibereval 2018. In IberEval@ SEPLN, pages 214–228.\nJeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 328–339, Melbourne, Australia, July. Association for Computational Linguistics.\nMladen Karan and Jan ˇSnajder. 2018. Cross-domain detection of abusive language online. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2), pages 132–137, Brussels, Belgium, October. Association\nfor Computational Linguistics.\n2052\nRitesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and Marcos Zampieri. 2018. Benchmarking Aggression Iden-\ntiﬁcation in Social Media. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying\n(TRAC-2018), pages 1–11, Santa Fe, New Mexico, USA, August. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.\nShervin Malmasi and Marcos Zampieri. 2017. Detecting hate speech in social media. In Proceedings of the\nInternational Conference Recent Advances in Natural Language Processing, RANLP 2017 , pages 467–472,\nVarna, Bulgaria, September. INCOMA Ltd.\nHamdy Mubarak, Ammar Rashed, Kareem Darwish, Younes Samih, and Ahmed Abdelali. 2020. Arabic offensive\nlanguage on twitter: Analysis and experiments. arXiv preprint arXiv:2004.02192.\nHala Mulki, Hatem Haddad, Chedi Bechikh Ali, and Halima Alshabani. 2019. L-HSAB: A Levantine twitter\ndataset for hate speech and abusive language. In Proceedings of the Third Workshop on Abusive Language\nOnline, pages 111–118, Florence, Italy, August. Association for Computational Linguistics.\nJason Phang, Thibault F ´evry, and Samuel R. Bowman. 2019. Sentence Encoders on STILTs: Supplementary\nTraining on Intermediate Labeled-data Tasks. arXiv:1811.01088 [cs], February. arXiv: 1811.01088.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics.\nZeses Pitenis, Marcos Zampieri, and Tharindu Ranasinghe. 2020. Offensive Language Identiﬁcation in Greek. In\nProceedings of the 12th Language Resources and Evaluation Conference. ELRA.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “why should i trust you?”. Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16.\nSara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav Nakov. 2020. A large-scale\nsemi-supervised dataset for offensive language identiﬁcation. arXiv preprint arXiv:2004.14454.\nManuela Sanguinetti, Fabio Poletto, Cristina Bosco, Viviana Patti, and Marco Stranisci. 2018. An Italian twit-\nter corpus of hate speech against immigrants. In Proceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018), Miyazaki, Japan, May. European Language Resources As-\nsociation (ELRA).\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\nMegatron-lm: Training multi-billion parameter language models using model parallelism.\nGudbjartur Ingi Sigurbergsson and Leon Derczynski. 2020. Offensive Language and Hate Speech Detection for\nDanish. In Proceedings of the 12th Language Resources and Evaluation Conference. ELRA.\nRachele Sprugnoli, Stefano Menini, Sara Tonelli, Filippo Oncini, and Enrico Piras. 2018. Creating a WhatsApp\nDataset to Study Pre-teen Cyberbullying. In Proceedings of the 2nd Workshop on Abusive Language Online\n(ALW2), pages 51–59, Brussels, Belgium, October. Association for Computational Linguistics.\nSteve Durairaj Swamy, Anupam Jamatia, and Bj¨orn Gamb¨ack. 2019. Studying generalisability across abusive lan-\nguage detection datasets. In Proceedings of the 23rd Conference on Computational Natural Language Learning\n(CoNLL), pages 940–950, Hong Kong, China, November. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need.\nMichael Wiegand, Melanie Siegel, and Josef Ruppenhofer. 2018. Overview of the germeval 2018 shared task on\nthe identiﬁcation of offensive language. In Proceedings of GermEval.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art\nnatural language processing.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding.\n2053\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019a. Pre-\ndicting the type and target of offensive posts in social media. InProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 1415–1420, Minneapolis, Minnesota, June. Association for Computational\nLinguistics.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019b.\nSemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval). In Pro-\nceedings of the 13th International Workshop on Semantic Evaluation , pages 75–86, Minneapolis, Minnesota,\nUSA, June. Association for Computational Linguistics.\nMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon\nDerczynski, Zeses Pitenis, and C ¸ a˘grı C ¸¨oltekin. 2020. SemEval-2020 Task 12: Multilingual Offensive Language\nIdentiﬁcation in Social Media (OffensEval 2020). In Proceedings of SemEval.",
  "topic": "Offensive",
  "concepts": [
    {
      "name": "Offensive",
      "score": 0.9119760394096375
    },
    {
      "name": "SemEval",
      "score": 0.8476409316062927
    },
    {
      "name": "Computer science",
      "score": 0.8225431442260742
    },
    {
      "name": "Transformer",
      "score": 0.7655209302902222
    },
    {
      "name": "Language identification",
      "score": 0.664797306060791
    },
    {
      "name": "Turkish",
      "score": 0.6460734605789185
    },
    {
      "name": "Natural language processing",
      "score": 0.5818941593170166
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5717740654945374
    },
    {
      "name": "Arabic",
      "score": 0.5369111895561218
    },
    {
      "name": "Language model",
      "score": 0.4959021508693695
    },
    {
      "name": "Task (project management)",
      "score": 0.4517703056335449
    },
    {
      "name": "Speech recognition",
      "score": 0.37159326672554016
    },
    {
      "name": "Linguistics",
      "score": 0.28612327575683594
    },
    {
      "name": "Natural language",
      "score": 0.19669732451438904
    },
    {
      "name": "Voltage",
      "score": 0.17182618379592896
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I187531555",
      "name": "Lund University",
      "country": "SE"
    }
  ],
  "cited_by": 11
}