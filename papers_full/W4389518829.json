{
    "title": "Active Learning Principles for In-Context Learning with Large Language Models",
    "url": "https://openalex.org/W4389518829",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3089003676",
            "name": "Katerina Margatina",
            "affiliations": [
                "Fairfield University"
            ]
        },
        {
            "id": "https://openalex.org/A2738082409",
            "name": "Timo Schick",
            "affiliations": [
                "Fairfield University"
            ]
        },
        {
            "id": "https://openalex.org/A93365683",
            "name": "Nikolaos Aletras",
            "affiliations": [
                "Fairfield University"
            ]
        },
        {
            "id": "https://openalex.org/A4308012672",
            "name": "Jane Dwivedi-Yu",
            "affiliations": [
                "Fairfield University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3160638507",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2963368301",
        "https://openalex.org/W4385571671",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4221160826",
        "https://openalex.org/W3015777882",
        "https://openalex.org/W3026404337",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W3100509098",
        "https://openalex.org/W3215013410",
        "https://openalex.org/W2956090150",
        "https://openalex.org/W4385571045",
        "https://openalex.org/W4386798146",
        "https://openalex.org/W4385573504",
        "https://openalex.org/W4389518888",
        "https://openalex.org/W2785787385",
        "https://openalex.org/W4320516905",
        "https://openalex.org/W2959026463",
        "https://openalex.org/W3152515526",
        "https://openalex.org/W95183648",
        "https://openalex.org/W2774918944",
        "https://openalex.org/W4221138760",
        "https://openalex.org/W4389520255",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4297899309",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W4312091426",
        "https://openalex.org/W4385571445",
        "https://openalex.org/W4378474102",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4385571505",
        "https://openalex.org/W4297412056",
        "https://openalex.org/W4385570095",
        "https://openalex.org/W4206648492",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W4206500717",
        "https://openalex.org/W2951911250",
        "https://openalex.org/W2951786554",
        "https://openalex.org/W4376654497",
        "https://openalex.org/W2971068072",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4310738704",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4384211302",
        "https://openalex.org/W4323697341",
        "https://openalex.org/W4285123725",
        "https://openalex.org/W4295601746",
        "https://openalex.org/W4285129823",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W131533222",
        "https://openalex.org/W3105522431",
        "https://openalex.org/W4385570957",
        "https://openalex.org/W4287692594",
        "https://openalex.org/W4319323306",
        "https://openalex.org/W4310831983",
        "https://openalex.org/W4389524190",
        "https://openalex.org/W2903158431",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4386566646",
        "https://openalex.org/W4286769130",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3184553750",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4313483544",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W3118384492",
        "https://openalex.org/W4283078127",
        "https://openalex.org/W4385573636",
        "https://openalex.org/W4307005828",
        "https://openalex.org/W4224211508",
        "https://openalex.org/W4367628401",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2996848635",
        "https://openalex.org/W4296415404",
        "https://openalex.org/W2467669267",
        "https://openalex.org/W4281613493",
        "https://openalex.org/W4320839455"
    ],
    "abstract": "The remarkable advancements in large language models (LLMs) have significantly enhanced predictive performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively perform the task at hand through in-context learning. However, the process of selecting demonstrations for maximizing performance has received limited attention in prior work. This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration. We compare standard AL algorithms based on uncertainty, diversity, and similarity, and consistently observe that the latter outperforms all other methods, including random sampling. Our extensive experimentation involving a diverse range of GPT and OPT models across 24 classification and multi-choice tasks, coupled with thorough analysis, unambiguously demonstrates the importance of using demonstrations that are semantically similar to the domain of the test examples. In fact, we show higher average classification performance using \"similar\" demonstrations with GPT-2 (124M) than random demonstrations with GPT-Neox (20B). Notably, while diversity sampling shows promise, uncertainty sampling, despite its success in conventional supervised learning AL scenarios, performs poorly in in-context learning.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5011–5034\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nActive Learning Principles for In-Context Learning\nwith Large Language Models\nKaterina Margatina♢∗ Timo Schick† Nikolaos Aletras♢ Jane Dwivedi-Yu†\n♢University of Sheffield †FAIR, Meta\n{k.margatina, n.aletras}@sheffield.ac.uk\njaneyu@meta.com\nAbstract\nThe remarkable advancements in large lan-\nguage models (LLMs) have significantly en-\nhanced predictive performance in few-shot\nlearning settings. By using only a small number\nof labeled examples, referred to as demonstra-\ntions, LLMs can effectively perform the task at\nhand through in-context learning. However, the\nprocess of selecting demonstrations for maxi-\nmizing performance has received limited atten-\ntion in prior work. This paper addresses the is-\nsue of identifying the most informative demon-\nstrations for few-shot learning by approaching\nit as a pool-based Active Learning (AL) prob-\nlem over a single iteration. We compare stan-\ndard AL algorithms based on uncertainty, di-\nversity, and similarity, and consistently observe\nthat the latter outperforms all other methods,\nincluding random sampling. Our extensive ex-\nperimentation involving a diverse range of GPT\nand OPT models across 24 classification and\nmulti-choice tasks, coupled with thorough anal-\nysis, unambiguously demonstrates the impor-\ntance of using demonstrations that are semanti-\ncally similar to the domain of the test examples.\nIn fact, we show higher average classification\nperformance using “similar” demonstrations\nwith GPT-2 (124M) than random demonstra-\ntions with GPT-Neox ( 20B). Notably, while\ndiversity sampling shows promise, uncertainty\nsampling, despite its success in conventional\nsupervised learning AL scenarios, performs\npoorly in in-context learning.\n1 Introduction\nThe field of Natural Language Processing (NLP)\nhas recently witnessed a remarkable paradigm shift\nwith the emergence of in-context learning with\nlarge language models (LLMs), also referred to\nas few-shot learning (Brown et al., 2020). Tradi-\ntionally, NLP systems heavily relied on supervised\nlearning approaches, where large amounts of la-\nbeled training data were necessary to achieve high\n∗ Work done during an internship at FAIR, Meta.\nFigure 1: Performance of different in-context selection\nalgorithms in classification and multi-choice tasks.\npredictive performance. However, in-context learn-\ning has changed this status-quo by enabling LLMs\nto learn from limited, context-specific examples\nand adapt to new tasks and domains with remark-\nable proficiency (Zhao et al., 2021; Chowdhery\net al., 2022; García et al., 2023; Wei et al., 2023b;\nTouvron et al., 2023; Bubeck et al., 2023). Unlike\nmore traditional approaches, which require exten-\nsive retraining or fine-tuning for every new task,\nin-context learning empowers LLMs to general-\nize from a few examples that are fed to the model\nthrough prompting to learn a new task at hand,\nwithout any weight updates.\nThe data efficiency of few-shot in-context learn-\ning of LLMs is indeed remarkable with only a small\nnumber of demonstrations.1 Still, such demonstra-\ntions constitute labeled data examples, raising two\nkey questions: (1) When faced with tasks where\nthere is only unlabeled data available, how can we\nselect the most appropriate samples to label and\nthen use as in-context demonstrations? (2) When\nwe have labeled data for a given task, how can\n1We use the terms in-context examples, few-shot examples,\ndemonstrations, descriptors and exemplars interchangeably\nthroughout the paper.\n5011\nwe efficiently identify the most informative combi-\nnation of demonstrations for in-context learning?\nAnswering these questions is essential to ensure ef-\nfective and efficient few-shot learning using LLMs.\nA growing line of work has investigated how in-\ncontext learning works (Reynolds and McDonell,\n2021; Razeghi et al., 2022; Xie et al., 2022; Ye\net al., 2023b), which demonstrations to use (Liu\net al., 2022; Zhang et al., 2022b; Wu et al., 2022;\nKim et al., 2022), how to form the prompt (Zhao\net al., 2021; Lu et al., 2022; Yang et al., 2023) and\nwhether ground truth labels matter (Webson and\nPavlick, 2022; Min et al., 2022; Yoo et al., 2022;\nWang et al., 2022; Wei et al., 2023b). Still, to the\nbest of our knowledge, no prior work has explored\nthe problem of in-context demonstration selection\nexplicitly through the lens of active learning (AL).\nBased on the core principle that not all data\npoints are equally useful, AL (Cohn et al., 1996;\nSettles, 2009) aims to identify the most informa-\ntive instances from a pool of unlabeled data for\nannotation. Iterating through model training, data\nacquisition and human annotation, the goal is to\nachieve data efficiency. A data-efficient AL al-\ngorithm ensures that a model achieves satisfactory\nperformance on a withheld test set by selecting only\na small fraction of the unlabeled data for annotation\nthat typically is better than randomly selecting and\nannotating data of equal size.\nIn this paper, our main aim is to redefine the\nconcept of data efficiency within the framework of\nin-context learning inspired by conventional active\nlearning settings. For this purpose, we assume\nthat given a pool of labeled or unlabeled data, the\nobjective is to identify a set of k examples that will\nserve as demonstrations to an LLM, resulting in\noptimal performance on a held-out test set. Given\nthis formulation of data efficiency, we explore the\neffectiveness of the most prevalent AL approaches\nbased on uncertainty (Lewis and Gale, 1994; Cohn\net al., 1996; Gal et al., 2017), diversity (Brinker,\n2003; Bodó et al., 2011; Sener and Savarese, 2018)\nand similarity (Margatina et al., 2021; Kirsch et al.,\n2021; Liu et al., 2022), as demonstration selection\nmethods for in-context learning (Figure 1).\nOur key contributions are as follows:\n• We formulate the selection of in-context ex-\namples as a single iteration AL problem and\nexplore the effectiveness of four standard ap-\nproaches: uncertainty, diversity, similarity\nand random sampling.\n• We evaluate 15 models, between 125M and\n30B parameters, from the GPT (Radford et al.,\n2019; Brown et al., 2020; Black et al., 2022)\nand OPT (Zhang et al., 2022a) families in 15\nclassification and 9 multi-choice tasks, using\ndifferent AL sampling techniques to select\ndemonstrations for few-shot learning.\n• We demonstrate that while diversity and uncer-\ntainty sampling perform slightly better than\nrandom sampling, choosing in-context exam-\nples that are semantically similar to the in-\nput test examples outperforms consistently all\nother methods by a large margin across model\nfamilies and sizes in all tasks.\n• We show that while uncertainty sampling is\none of the strongest AL approaches in super-\nvised learning, this does not generalize to in-\ncontext learning, where interestingly it under-\nperforms. Our analysis, however, shows that\nlarger models might perform better with uncer-\ntain demonstrations, hinting that uncertainty\nmight be an emerging LLM ability.\n2 Active In-context Learning\n2.1 Problem Formulation\nTo build our in-context learning framework with\nactively acquired demonstrations, depicted in Fig-\nure 2, we borrow the formulation from the standard\npool-based active learning paradigm. We consider\nan AL setting where we have a large pool of unla-\nbeled data from which we want to sample a batch\nof k data points using a data acquisition algorithm.\nWe assume that these k are subsequently labeled\nby humans (Figure 2, top). Instead of following the\nstandard approach that involves multiple iterations\nof data selection and model training, we only per-\nform a single iteration (Longpre et al., 2022), since\nwe do not train or perform any model-in-the-loop\nupdates. We use the acquired set of k examples\nas demonstrations for in-context learning with an\nLLM (i.e., as part of the prompt). We assume the\nexisting datasets as the pool from which to select\nthese k examples. The goal is to find the most\ninformative examples from the pool, which are ex-\npected to yield improved performance on the test\nset when employed as a few-shot prompt, com-\npared to demonstrations randomly sampled from\nthe same pool. The resulting prompt consists of\nthe concatenation of the k acquired examples (text\n5012\nFigure 2: Top: Active data collection (single iteration). Bottom: Prompt construction and model inference.\ninputs and labels with standard verbalizers), along-\nside the test example, repeated for all data instances\nin the test set (Figure 2, bottom).\n2.2 Few-shot Data Acquisition Algorithms\nWe build few-shot data acquisition algorithms in-\nspired by the most prevalent AL algorithmic fami-\nlies that are uncertainty sampling, diversity\nsampling and similarity (also known as test-\naware sampling) (Zhang et al., 2022c). We ac-\nknowledge that there are more elaborate demon-\nstration selection methods for in-context learning\nthat are not considered in our experiments, such\nas Q-learning (Zhang et al., 2022b), Self Adaptive\n(Wu et al., 2022), SG-ICL (Kim et al., 2022), MI\n(Sorensen et al., 2022), inter alia. These methods\nfall beyond the scope of our analysis, as our ob-\njective is to gain insights into AL principles for\nin-context learning, rather than benchmarking all\navailable demonstration sampling algorithms. Ad-\nditionally, there are techniques, complementary to\nthe aforementioned few-shot data selection meth-\nods, such as calibration (Zhao et al., 2021) and\nprompt re-ordering (Lu et al., 2022), which can fur-\nther enhance few-shot learning performance, while\nalso being out of the scope of our work.\nRandom The overarching objective of any data se-\nlection method, like AL algorithms, is to identify\ndata points that, however used, yield superior mod-\nels compared to randomly sampled data from the\nsame pool which we consider as a baseline method.\nDiversity The first data selection method that\nwe use as a representative for the diversity family\nof methods is a simple clustering technique, similar\nto Yu et al. (2022). Specifically, we first encode\nall data points in the pool of unlabeled data with\nSentence-BERT (Reimers and Gurevych, 2019)\nembeddings and then we perform k-means cluster-\ning.2 We choose the number of clusters to bek and\nselect one data point from each cluster. The under-\nlying principle of this approach is that leveraging a\ndiverse set of in-context examples can offer greater\nadvantages compared to random sampling. This\nselection strategy ensures that the chosen demon-\nstrations are likely to encompass a broad range of\ninformation, enhancing the overall effectiveness of\nthe learning process.\nUncertainty The second approach is an\nuncertainty-based sampling algorithm that is based\non SPELL, proposed by Gonen et al. (2022). Since\nwe use an off-the-shelf LLM that does not have a\nfine-tuned classification layer, we cannot compute\nthe model probabilities associated with each class\n(for a classification or multi-choice task). This\nessentially means that we cannot use standard AL\nuncertainty baselines such as maximum entropy\nor least confidence. Instead, we can use the\nloss, i.e., perplexity, of the LLM to score each\ncandidate example from the pool. Gonen et al.\n(2022) define perplexity of the prompt as the\nperplexity of the full prompt sequence, including\nthe input itself, and without the label, averaged\nover 1, 000 examples. Our approach is different\nsince we want to evaluate the perplexity of each\nin-context example individually. We also do not\ndo the averaging over a thousand examples as we\nwanted to make the method more general, without\n2We use the implementation from https://www.sbert.\nnet/examples/applications/clustering/.\n5013\nthe need to assume access to that many examples.\nThe underlying principle guiding this approach is\nthe belief that a high perplexity set of in-context\nexamples can yield greater advantages compared\nto randomly sampling from the dataset (or at least\nfor data efficiency in a supervised learning setting\nthis is proven to enhance the learning process).\nSimilarity Finally, the third AL algorithm we\nconsider is based on KATE a kNN-augmented in-\ncontext example selection method proposed by Liu\net al. (2022). This method retrieves examples from\nthe pool that are semantically-similar to a test query\nsample. We use Sentence-BERT (Reimers and\nGurevych, 2019) representations of both the pool\nand the test set to find the k-nearest neighbours.\nThe rationale behind this approach is that the most\nsimilar demonstrations to the test example will best\nhelp the model answer the query. We have to high-\nlight, however, that by definition each test example\nwill have a different prompt, as the k most similar\ndemonstrations will be different. This is a crucial\nlimitation of this approach compared to the others,\nas it assumes that we are able to acquire labels for\nany in-context example selected from the pool.\n3 Experimental Setup\nModels We evaluate 15 LLMs in total, 8 mod-\nels from the GPT (Radford et al., 2019; Brown\net al., 2020; Black et al., 2022) and 7 from the\nOPT (Zhang et al., 2022a) family. We choose our\nmodels to span from a few million to tens of billions\nparameters, as we want to study how the model size\naffects the effectiveness of in-context example se-\nlection methods. All models considered in this\nwork are publicly available.\nTasks & Datasets Following Min et al. (2022),\nwe evaluate all LLMs in 15 classification and 9\nmulti-choice tasks taken from the Crossfit (Ye\net al., 2021) benchmark. We provide details for all\ntasks and datasets considered in the Appendix A.1.\nIn-context Learning Prompting Unless speci-\nfied otherwise, we sample k=16 demonstrations,\ni.e., labeled data, from the pool with each AL\nmethod. After collecting the k input-label pairs,\nwe concatenate them all together with the test ex-\nample that we want to make a prediction for to form\nthe LLM prompt (Figure 2). Our implementation,\nincluding prompt verbalizers, is based on those by\nMin et al. (2022) and Yoo et al. (2022).\n4 Results\nFigure 3 shows the results on few-shot in-\ncontext learning across all data acquisition meth-\nods ( random, diversity, uncertainty and\nsimilarity), model families (GPT and OPT) and\ntasks (classification and multi-choice question an-\nswering).3 Overall, we observe the anticipated\ntrend of performance enhancement with increas-\ning scale, particularly notable in the multi-choice\ntasks for both OPT and GPT models.\nStill, the most remarkable finding is the sub-\nstantial performance improvement achieved by se-\nlecting similar in-context examples for few-shot\nlearning, particularly in classification tasks. This\nobservation aligns with the findings reported by Liu\net al. (2022), who demonstrated similar patterns in\nsentiment analysis tasks with GPT-3. Our results\nindicate that the selection of appropriate demonstra-\ntions can hold greater significance than the number\nof model parameters, at least within the scope of\nthe models evaluated in this study. In multi-choice\ntasks, similarity is also the top-performing ac-\nquisition method, while the other three approaches\nexhibit closely competitive performance.\nThe data selection method based on diversity\nis consistently the second best approach after\nsimilarity (with very few exceptions in the multi-\nchoice tasks for OPT models). Even though it is\nnot the top performing method, we can consider\nthat consistently outperforming random sampling\nis a strong signal that diversity in the demonstra-\ntions is a characteristic of effective demonstrations.\nLevy et al. (2022) explore the setting of composi-\ntional generalization, where models are tested on\noutputs with structures that are absent from the\ntraining set and thus selecting similar demonstra-\ntions is insufficient. They show that combining\ndiverse demonstrations with in-context learning\nsubstantially improves performance for the task of\ncompositional generalization semantic parsing.\nRemarkably, uncertainty sampling, typically\nregarded as one of the best approaches for tradi-\ntional supervised AL (Shen et al., 2017; Margatina\net al., 2022; Schröder et al., 2023), exhibits the\nlowest performance. This finding contradicts the\nconventional AL principles that suggest selecting\na few highly uncertain labeled data points for data\nefficiency. Similar to our findings, Gonen et al.\n(2022) explore the performance variabilty of dif-\n3We provide the results per dataset and model in the Ap-\npendix A.2, including the majority vote baseline.\n5014\nFigure 3: Results for various GPT (top) and OPT (bottom) models and AL methods averaged over 15 classification\nand 9 multi-choice tasks. Similarity is consistently the best performing approach overall, followed by diversity and\nrandom. Interestingly, we observe that uncertainty sampling underperforms in this setting of in-context learning.\nferent prompts (consisting of randomly sampled\ndemonstrations) for in-context learning using un-\ncertainty, and find that the lower the perplexity of\nthe prompt is, the better the prompt is able to per-\nform the task. Still, in a later analysis we show that\nlarger models might be able to handle high uncer-\ntain prompts better than the smaller ones (§5.4).\n5 Analysis\n5.1 Effect of Model Size\nIn order to gain some intuition on the effect of\nscale, we group together GPT and OPT models\nwith similar number of parameters. We provide\nthe results in Figure 4. Even after aggregating the\nresults from both model families, we do not see any\nspecific pattern as the model parameters increase.\nWe wanted to explore whether the largest models of\nour collection would behave differently under the\nvarying in-context learning settings, thus perhaps\nattributing such a behaviour to potential emergent\nabilities of the bigger LLMs, but we observe the\nsame patterns (in terms of ranking between the\nconsidered data selection methods). We believe\nthat this is an interesting avenue of future research,\nespecially as models grow and, most likely, will\ncontinue to grow exponentially in terms of model\nparameters. Our findings show that the in-context\nlearning ability of models from a few millions to a\nfew billions of parameters follows similar patterns.\nHowever, this might not be the case when studying\neven larger models, as primary results hint (Rae\net al., 2022; Wei et al., 2023b; Chowdhery et al.,\n2022; Touvron et al., 2023).\n5.2 Ground Truth Demonstrations\nWe next delve into the debate of whether ground\ntruth demonstrations, i.e., providing the correct la-\nbel to the in-context examples, is crucial for high\nperforming in-context learning. Various findings\nhave shown mixed results for randomly sampled\ndata, which essentially means that the benefit of\nground truth labels depends on the label space or\nthe distribution of inputs specified by the demon-\nstrations (Min et al., 2022; Yoo et al., 2022). In\nour analysis, we differentiate from prior work by\nexploring the importance of ground truth demon-\nstrations in the case of leveraging similar in-context\nexamples (§2.2). The rationale is that if the find-\n5015\nFigure 4: Results per model size.\nFigure 5: Effect of ground truth labels on in-context\nlearning with with the similarity AL selection method.\nings of Min et al. (2022) ubiquitously hold, then\nthe performance should only marginally drop if we\nreplace ground truth labels with random ones. If\nthe high performance of the similarity acquisi-\ntion method can be retained, we would be able to\nconstruct an efficient and effective in-context se-\nlection algorithm that would be agnostic to correct\nlabels. However, we find that this is not the case.\nWe show in Figure 5 that for almost all datasets\nconsidered in this part of analysis, the performance\nwith random labels drops significantly as expected.\nThere are cases where replacing the original labels\nwith random ones as in Min et al. (2022) retains the\nsame performance (e.g., in the glue-rte dataset),\nbut this is certainly a finding that does not general-\nize overall. In summary, we find that ground truth\ndemonstrations are crucial for high performing, ro-\nbust in-context learning (Yoo et al., 2022).\n5.3 Most vs. Least Similar Demonstrations\nTo investigate the striking effectiveness of thesim-\nilarity-based acquisition strategy, we conduct ad-\nditional experiments where we invert the approach\nand choose the least similar examples from the\npool to form the prompt. This investigation aims\nto ascertain whether the remarkable performance\ngains can be attributed solely to the semantic simi-\nlarity between the demonstrations and the test input.\nThe results depicted in Figure 6 substantiate our hy-\npothesis, demonstrating a significant performance\ndrop when employing opposite examples from the\npool as in-context exemplars. While this pattern is\nparticularly pronounced in the classification tasks,\nit consistently emerges across different model sizes\nand task types. Hence, we can assert that maximiz-\ning semantic similarity between the demonstations\nand the input test sample is an unequivocally vital\nattribute for achieving successful in-context learn-\ning outcomes with LLMs. Future endeavors in the\nfield of building effective in-context learning frame-\nworks should incorporate this principle to enable\ndata-efficient algorithms that can fully harness the\npotential of LLMs.\n5.4 Most vs. Least Uncertain Demonstrations\nAlong these lines, we also opt to examine the dual-\nity between selecting the most or the least uncertain\nin-context examples from the pool. We show the\nresults of these experiments for the GPT models\nin Figure 7. Interestingly, we observe that while\nthe smaller language models (gpt2, gpt2-medium,\ngpt-large) perform better with the least uncertain\nprompts, the larger models seem to start benefit-\ning from the demonstrations with high uncertainty.\nThis is particularly clear in the largest model of our\ncollection, GPT-Neox (20B parameters). This inter-\nesting finding shows that even larger models will\nmost likely perform better with high entropy in-\ncontext examples, similar to their supervised learn-\n5016\nFigure 6: Most vs. least similar in-context examples.\nFigure 7: Most vs. least uncertain in-context examples.\ning counterparts. Such findings open a plethora of\nresearch questions regarding understanding how in-\ncontext learning works (Reynolds and McDonell,\n2021; Razeghi et al., 2022; Xie et al., 2022; Min\net al., 2022), how AL and data acquisition methods\nreshape with larger language models or whether we\ncan properly investigate potential emergent abili-\nties of LLMs acquired by model scaling (Wei et al.,\n2022; Schaeffer et al., 2023).\n5.5 Evaluation with Different Metrics\nFinally, we want to provide a clear overview of our\nexperiments and summary of our findings, while\nmaking some clarifications regarding how we evalu-\nate and compare different approaches to in-context\nlearning. Figure 8 shows the results for in-context\nlearning with random sampling, three data selec-\ntion techniques inspired by AL (§2.2), namely\ndiversity, uncertainty and similarity, and\na zero-shot baseline where no labeled examples are\nincluded in the prompt (no_demo). We show that in-\ncontext learning with k=16 demonstrations consis-\ntently outperform zero-shot learning for an average\nof 15 classification tasks for gpt2-large, gpt-j\nand gpt-neox. Next, we observe that the best\nperforming in-context example selection method\nis by a clear margin similarity, followed by\ndiversity. This finding corroborates the origi-\nnal hypothesis of AL that, indeed, not all data is\nequal and there exist more informative data subsets\nin the pool that can be used as in-context exemplars.\nWe can see that the uncertainty baseline, which\nis usually top performing in supervised AL, gen-\nerally underperforms in the few-shot setting. Still,\nthere is some evidence that this could change with\neven larger and better models (§5.4). Finally, delv-\ning into the debate on whether ground truth labels\nmatter or not (Min et al., 2022; Yoo et al., 2022),\nwe show that replacing original with random in-\ncontext labels hurt significantly the performance of\nsimilarity, the best data selection method (§5.2).\nWe further emphasize the significance of em-\nploying a meticulous evaluation framework, partic-\nularly in the selection of appropriate metrics. In\nFigure 8, we illustrate the same classification ex-\nperiments, but with the F1 score plotted on the\nleft and accuracy on the right. The use of F1,\nthe conventional metric for classification tasks,\nreveals a distinct ranking among the various AL\nmethods, with similarity exhibiting the best per-\nformance, followed by diversity. Conversely,\nwhen employing accuracy to compare the methods,\ndiversity emerges as the top approach, followed\nby similarity and random selection. This dispar-\nity highlights the potential for misconceptions or\nobscured findings, underscoring the need for cau-\ntion when evaluating and comparing different meth-\nods across various models within the in-context\nlearning framework (Dehghani et al., 2021; Min\net al., 2022; Yoo et al., 2022; Tedeschi et al., 2023).\n6 Related Work\n6.1 Understanding In-Context Learning\nFew-shot in-context learning with LLMs has gar-\nnered significant attention in recent NLP research.\nSimply concatenating a few labeled examples to\nform the prompt for the LLM results in high perfor-\nmance gains, even outperforming fine-tuned mod-\nels (Brown et al., 2020; Chung et al., 2022; Ouyang\net al., 2022; Dong et al., 2022). This has naturally\nlead to study its effectiveness with multiple few-\nshot learning benchmarks such as Crossfit (Ye\net al., 2021) and BigBench (Srivastava et al., 2022).\nAnother active area of research is on understand-\ning how in-context learning works (Xie et al., 2022;\nGarg et al., 2022; Akyürek et al., 2022; Xie et al.,\n2022; Pan et al., 2023), and what are its strengths\nand limitations (Webson and Pavlick, 2022; Jang\net al., 2022; Levy et al., 2022; Shi et al., 2022;\nAgrawal et al., 2022; Wei et al., 2023b; Ye et al.,\n2023b). Previous work has explored the effec-\n5017\nFigure 8: The ranking of data selection methods is different depending on the metric used.\ntiveness of the chain-of-thought prompting tech-\nnique (Wei et al., 2023a; Wang et al., 2022; Madaan\nand Yazdanbakhsh, 2022), while other studies try\nto determine the importance of in-context ground\ntruth labels, with Min et al. (2022) showing that\nrandom labels do not hurt performance consider-\nably and Yoo et al. (2022) providing a rebuttal. Wei\net al. (2023b) explain that model size plays an role\nin the effect of ground truth labels, showing that\nsmall LMs ignore flipped labels, while LLMs can\noverride semantic priors learned during pretraining.\nInterestingly, Razeghi et al. (2022) demonstrates\nthat in-context learning performance is highly cor-\nrelated with the prevalence of each instance in the\npretraining corpus, showing that models are more\naccurate on few-shot numerical reasoning on in-\nstances whose terms are more frequent.\n6.2 Selecting Informative Demonstrations\nTypically, work on evaluating LLMs in few-shot\nsettings commonly uses randomly sampled exam-\nples to compose the in-context prompt (Brown\net al., 2020; Zhang et al., 2022a; Chowdhery\net al., 2022; Chung et al., 2022; Touvron et al.,\n2023). Nonetheless, it has been demonstrated that\nthe effectiveness of few-shot performance signif-\nicantly depends on the selection of in-context ex-\namples (Kocielnik et al., 2022; Ye et al., 2023a;\nDiao et al., 2023; Xu et al., 2023). Consequently,\nthere is ongoing research on generating or select-\ning the most informative demonstrations, aiming to\nmaximize the downstream few-shot performance.\nSome approaches are based on a retrieval compo-\nnent that sources the most relevant examples from\na pool. The prompt retriever can be trainable (Ru-\nbin et al., 2022) or based on pretrained embed-\ndings (Liu et al., 2022; Agrawal et al., 2022). Go-\nnen et al. (2022) use uncertainty to evaluate the use-\nfulness of in-context examples and find that the best\nperforming prompts have low perplexity. Zhang\net al. (2022b) formulate example selection for in-\ncontext learning as a sequential decision problem\nand show modest performance improvements by\nacquiring data with their proposed method based\non reinforcement learning. Other previous work,\ninstead of focusing on the part of acquiring data for\nin-context learning, show that demonstration order-\ning (Lu et al., 2022) and model calibration (Zhao\net al., 2021) are additional properties that influence\nthe few-shot learning performance.\n6.3 Active Learning for NLP\nAL has been extensively studied in various NLP\ntasks, including machine translation (Miura et al.,\n2016; Zhao et al., 2020), natural language infer-\nence (Snijders et al., 2023), named entity recog-\nnition (Shen et al., 2017; Wei et al., 2019), and\ntext classification (Ein-Dor et al., 2020; Margatina\net al., 2022; Schröder et al., 2023), among others.\nStill, its importance and potential value is on the\nrise (Zhang et al., 2022c; Rauch et al., 2023), as the\ncurrent language model pretraining paradigm con-\ntinues to advance the state-of-the-art (Tamkin et al.,\n2022). Given the fundamental premise that“not all\ndata is equal” it is reasonable to expect researchers\nto actively seek the “most informative” data for\npretraining or adapting their large language models\n(LLMs), as well as identifying the most valuable\nin-context examples for few-shot learning scenar-\nios. Previous work has explored AL for prompt-\nbased finetuning (Köksal et al., 2022), proposing\na method based in inter-prompt uncertainty sam-\npling with diversity coupled with the PET archi-\ntecture (Schick and Schütze, 2021a,b) that outper-\nforms all AL baselines.\n5018\n7 Conclusion\nIn this study, we have examined the selection of\ndemonstrations, i.e., labeled data that provide ex-\namples of solving a task, for in-context learning\nwith LLMs. We formulated the selection process\nas a single iteration active learning problem and\nevaluated four standard approaches: uncertainty,\ndiversity, similarity, and random sampling.\nOur evaluation involved 15 models of varying\nsize from the GPT and OPT families, encom-\npassing 15 classification tasks and 9 multi-choice\ntasks. Through extensive experimentation, we have\ndemonstrated that selecting demonstrations that are\nsemantically similar to the test input examples con-\nsistently outperforms all other methods by a signif-\nicant margin across all model families, sizes, and\ntasks. This corroborates findings of several previ-\nous and concurrent studies that explore the proper-\nties of “good” in-context examples (Liu et al., 2022;\nShi et al., 2022). Interestingly, our findings reveal\nthat uncertainty sampling, although effective in su-\npervised learning, underperforms in the in-context\nlearning paradigm. This highlights the importance\nof our work in exploring the principles of active\nlearning in the context of few-shot learning.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their suggestions to improve our work. We also\nthank Louis Martin, Patrick Lewis, Fabio Petroni\nand other members of FAIR for their constructive\nfeedback on previous versions of the paper.\nLimitations\nTasks & Datasets We acknowledge that even\nthough we experimented with a well established\nbenchmark, the Crossfit (Ye et al., 2021)\nbenchmark consisting of 15 classification and\n9 multi-choice question answering datasets (Ap-\npendix A.1), it might still not be sufficient to ensure\nthat our findings will generalize to any NLP clas-\nsification or multi-choice application of in-context\nlearning.\nLanguage We also acknowledge that all the\ndatasets and models considered in this work are\nbased on the English language alone. This limits\ngeneralizability of our findings to other languages.\nModel scale We investigated in-context learning\nwith actively acquired demonstrations with15 GPT\nand OPT models that span 125M to 30B param-\neters. Even though our experimentation is thor-\nough, our findings might not generalize to larger\nor smaller transformer-based models, or models\nbased in a different architecture.\nActive learning considerations We explicitly\nnote in the paper that we do a single active learning\niteration, which is different than the common AL\nloop that consists of multiple iterations. As we ex-\nplained, because the model-in-the-loop (the LLM)\nis not updated (no fine-tuning) with new data, per-\nforming multiple iterations does not make sense\nin this context (Figure 2). Still, it would be inter-\nesting for future work to explore how we can per-\nform multiple AL iterations while constructing the\nprompt (i.e., acquiring the demonstrations). The\nupper bound would be to try all the combinations\nof a set of labeled data and find the best performing\nprompt. However, doing this with unlabeled data,\nin an efficient way, is far from trivial. We refer to\nZhang et al. (2022c); Treviso et al. (2023); Mar-\ngatina and Aletras (2023) for in-depth suggestions\nfor future work in this area.\nReferences\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\ncontext examples selection for machine translation.\nEkin Akyürek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2022. What learn-\ning algorithm is in-context learning? investigations\nwith linear models. ArXiv, abs/2211.15661.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. Tweeteval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644–1650.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nZalán Bodó, Zsolt Minier, and Lehel Csató. 2011. Ac-\ntive learning with clustering. In Proceedings of the\nActive Learning and Experimental Design workshop\n5019\nIn conjunction with AISTATS 2010, volume 16, pages\n127–139.\nKlaus Brinker. 2003. Incorporating diversity in active\nlearning with support vector machines. In Proceed-\nings of the International Conference on Machine\nLearning, pages 59–66.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with gpt-4.\nMichael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-\nnandez, and Doug Downey. 2019. CODAH: An\nadversarially-authored question answering dataset\nfor common sense. In Proceedings of the 3rd Work-\nshop on Evaluating Vector Space Representations for\nNLP, pages 63–69, Minneapolis, USA. Association\nfor Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language mod-\nels.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nDavid A. Cohn, Zoubin Ghahramani, and Michael I.\nJordan. 1996. Active learning with statistical mod-\nels. Journal of Artificial Intelligence Research ,\n4(1):129–145.\nOna de Gibert, Naiara Pérez, Aitor García-Pablos, and\nMontse Cuadros. 2018. Hate speech dataset from\na white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2),\npages 11–20.\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Investi-\ngating projection in naturally occurring discourse.\nMostafa Dehghani, Yi Tay, Alexey A. Gritsenko, Zhe\nZhao, Neil Houlsby, Fernando Diaz, Donald Metzler,\nand Oriol Vinyals. 2021. The benchmark lottery.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models.\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\nlian, Massimiliano Ciaramita, and Markus Leippold.\n2020. Climate-fever: A dataset for verification of\nreal-world climate claims.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\nArXiv, abs/2301.00234.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim. 2020.\nActive Learning for BERT: An Empirical Study. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7949–7962, Online. Association for Computa-\ntional Linguistics.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017.\nDeep Bayesian active learning with image data. In\nProceedings of the 34th International Conference\n5020\non Machine Learning , volume 70 of Proceedings\nof Machine Learning Research , pages 1183–1192.\nPMLR.\nXavier García, Yamini Bansal, Colin Cherry, George F.\nFoster, Maxim Krikun, Fan Feng, Melvin Johnson,\nand Orhan Firat. 2023. The unreasonable effective-\nness of few-shot learning for machine translation.\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gre-\ngory Valiant. 2022. What can transformers learn\nin-context? a case study of simple function classes.\nArXiv, abs/2208.01066.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,\nand Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation. ArXiv,\nabs/2212.04037.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roem-\nmele. 2012. SemEval-2012 task 7: Choice of plau-\nsible alternatives: An evaluation of commonsense\ncausal reasoning. In *SEM 2012: The First Joint\nConference on Lexical and Computational Seman-\ntics – Volume 1: Proceedings of the main conference\nand the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Eval-\nuation (SemEval 2012) , pages 394–398, Montréal,\nCanada. Association for Computational Linguistics.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2022. Can\nlarge language models truly understand prompts?\na case study with negated prompts. ArXiv,\nabs/2209.12711.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. Qasc: A\ndataset for question answering via sentence compo-\nsition. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8082–8090.\nHyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk\nKim, Kang Min Yoo, and Sang goo Lee. 2022.\nSelf-generated in-context learning: Leveraging auto-\nregressive language models as a demonstration gen-\nerator. ArXiv, abs/2206.08082.\nAndreas Kirsch, Tom Rainforth, and Yarin Gal. 2021.\nTest distribution-aware active learning: A principled\napproach against distribution shift and outliers.\nRafal Kocielnik, Sara Kangaslahti, Shrimai Prabhu-\nmoye, M Hari, R. Michael Alvarez, and Anima\nAnandkumar. 2022. Can you label less by using\nout-of-domain data? active & transfer learning with\nfew-shot instructions. ArXiv, abs/2211.11798.\nAbdullatif Köksal, Timo Schick, and Hinrich Schutze.\n2022. Meal: Stable and active learning for few-shot\nprompting. ArXiv, abs/2211.08358.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth international conference on the principles of\nknowledge representation and reasoning.\nItay Levy, Ben Bogin, and Jonathan Berant. 2022. Di-\nverse demonstrations improve in-context composi-\ntional generalization.\nDavid D. Lewis and William A. Gale. 1994. A se-\nquential algorithm for training text classifiers. In\nIn Proceedings of the Annual International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nS. Longpre, Julia Reisler, Edward Greg Huang, Yi Lu,\nAndrew J. Frank, Nikhil Ramesh, and Chris DuBois.\n2022. Active learning over multiple domains in natu-\nral language tasks. ArXiv, abs/2202.00254.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\nand patterns: For effective chain of thought, it takes\ntwo to tango. ArXiv, abs/2209.07686.\nKaterina Margatina and Nikolaos Aletras. 2023. On\nthe limitations of simulating active learning. In Find-\nings of the Association for Computational Linguistics:\nACL 2023, pages 4402–4419.\nKaterina Margatina, Loic Barrault, and Nikolaos Ale-\ntras. 2022. On the importance of effectively adapting\npretrained language models for active learning. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 825–836, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021. Active learning by ac-\nquiring contrastive examples. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 650–663, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nClara H. McCreery, Namit Katariya, Anitha Kannan,\nManish Chablani, and Xavier Amatriain. 2020. Effec-\ntive transfer learning for identifying similar questions:\nMatching user questions to covid-19 faqs.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?\n5021\nShervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-\njes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.\n2021. Deep learning–based text classification: a com-\nprehensive review. ACM computing surveys (CSUR),\n54(3):1–40.\nAkiva Miura, Graham Neubig, Michael Paul, and\nSatoshi Nakamura. 2016. Selecting syntactic, non-\nredundant segments in active learning for machine\ntranslation. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 20–29, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\nand Grigorios Tsoumakas. 2022. ETHOS: a multi-\nlabel hate speech detection dataset. Complex Intelli-\ngent Systems, 8(6):4663–4678.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\n2023. What in-context learning \"learns\" in-context:\nDisentangling task recognition and task learning.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nLukas Rauch, Matthias Aßenmacher, Denis Huseljic,\nMoritz Wirth, Bernd Bischl, and Bernhard Sick. 2023.\nActiveglae: A benchmark for deep active learning\nwith transformers.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot numerical reasoning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 840–854, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’21, New York, NY , USA.\nAssociation for Computing Machinery.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo.\n2023. Are emergent abilities of large language mod-\nels a mirage?\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nChristopher Schröder, Lydia Müller, Andreas Niekler,\nand Martin Potthast. 2023. Small-text: Active learn-\ning for text classification in python. In Proceedings\nof the 17th Conference of the European Chapter of\n5022\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 84–95, Dubrovnik, Croa-\ntia. Association for Computational Linguistics.\nOzan Sener and Silvio Savarese. 2018. Active learn-\ning for convolutional neural networks: A core-set\napproach. In International Conference on Learning\nRepresentations.\nBurr Settles. 2009. Active learning literature survey.\nComputer Sciences Technical Report 1648, Univer-\nsity of Wisconsin–Madison.\nYanyao Shen, Hyokun Yun, Zachary Lipton, Yakov\nKronrod, and Animashree Anandkumar. 2017. Deep\nactive learning for named entity recognition. In Pro-\nceedings of the Workshop on Representation Learn-\ning for NLP, pages 252–256.\nEmily Sheng and David C Uthus. 2020. Investigating\nsocietal biases in a poetry composition system. In\nProceedings of the Second Workshop on Gender Bias\nin Natural Language Processing, pages 93–106.\nWeijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman,\nYulia Tsvetkov, and Luke Zettlemoyer. 2022. Toward\nhuman readable prompt tuning: Kubrick’s the shining\nis a good movie, and a good prompt too? ArXiv,\nabs/2212.10539.\nArd Snijders, Douwe Kiela, and Katerina Margatina.\n2023. Investigating multi-source active learning for\nnatural language inference. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 2187–\n2209, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nTaylor Sorensen, Joshua Robinson, Christopher Ryt-\nting, Alexander Shaw, Kyle Rogers, Alexia Delorey,\nMahmoud Khalil, Nancy Fulda, and David Wingate.\n2022. An information-theoretic approach to prompt\nengineering without ground truth labels. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 819–862, Dublin, Ireland. Association\nfor Computational Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-\nmekci, Bill Yuchen Lin, Blake Howald, Cameron\nDiao, Cameron Dour, Catherine Stinson, Cedrick Ar-\ngueta, César Ferri Ramírez, Chandan Singh, Charles\nRathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu,\nChris Callison-Burch, Chris Waites, Christian V oigt,\nChristopher D. Manning, Christopher Potts, Cindy\nRamirez, Clara E. Rivera, Clemencia Siro, Colin Raf-\nfel, Courtney Ashcraft, Cristina Garbacea, Damien\nSileo, Dan Garrette, Dan Hendrycks, Dan Kilman,\nDan Roth, Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam,\nDieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dim-\nitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekate-\nrina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Martínez-Plumed, Francesca\nHappé, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nLópez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Ko-\nco´n, Jana Thompson, Jared Kaplan, Jarema Radom,\nJascha Sohl-Dickstein, Jason Phang, Jason Wei, Ja-\nson Yosinski, Jekaterina Novikova, Jelle Bosscher,\nJennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse En-\ngel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jil-\nlian Tang, Joan Waweru, John Burden, John Miller,\nJohn U. Balis, Jonathan Berant, Jörg Frohberg, Jos\nRozen, Jose Hernandez-Orallo, Joseph Boudeman,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Colón, Luke Metz, Lütfi Kerem\n¸ Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramírez Quintana, Marie Tolkiehn,\n5023\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, Mukund Varma T, Nanyun Peng, Nathan\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas Roberts, Nick Doiron, Nikita\nNangia, Niklas Deckers, Niklas Muennighoff, Ni-\ntish Shirish Keskar, Niveditha S. Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pascale\nFung, Paul Pu Liang, Paul Vicol, Pegah Alipoormo-\nlabashi, Peiyuan Liao, Percy Liang, Peter Chang,\nPeter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel\nHabacker, Ramón Risco Delgado, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Rohan\nSikand, Roman Novak, Roman Sitelew, Ronan Le-\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Rus-\nlan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Sto-\nvall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\nMohammad, Sajant Anand, Sam Dillavou, Sam\nShleifer, Sam Wiseman, Samuel Gruetter, Samuel R.\nBowman, Samuel S. Schoenholz, Sanghyun Han,\nSanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian,\nSayan Ghosh, Sean Casey, Sebastian Bischoff, Sebas-\ntian Gehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-\nantadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Théo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Timothy Telleen-Lawton,\nTitus Tunduny, Tobias Gerstenberg, Trenton Chang,\nTrishala Neeraj, Tushar Khot, Tyler Shultz, Uri Sha-\nham, Vedant Misra, Vera Demberg, Victoria Nyamai,\nVikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fe-\ndus, William Saunders, William Zhang, Wout V ossen,\nXiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu,\nXudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz,\nYangqiu Song, Yasaman Bahri, Yejin Choi, Yichi\nYang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid,\nZhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui\nWang, and Ziyi Wu. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models.\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau\nYih, and Ashish Sabharwal. 2019a. Quarel: A dataset\nand models for answering questions about qualitative\nrelationships. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 33, pages 7063–\n7071.\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\nClark. 2019b. Quartz: An open-domain dataset of\nqualitative relationship questions. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5941–5946.\nAlex Tamkin, Dat Pham Nguyen, Salil Deshpande, Jesse\nMu, and Noah Goodman. 2022. Active learning\nhelps pretrained models learn the intended task. In\nAdvances in Neural Information Processing Systems.\nSimone Tedeschi, Johan Bos, Thierry Declerck, Jan Ha-\njic, Daniel Hershcovich, Eduard H. Hovy, Alexander\nKoller, Simon Krek, Steven Schockaert, Rico Sen-\nnrich, Ekaterina Shutova, and Roberto Navigli. 2023.\nWhat’s the meaning of superhuman performance in\ntoday’s nlu? ArXiv, abs/2305.08414.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nMarcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken,\nQingqing Cao, Manuel R. Ciosici, Michael Hassid,\nKenneth Heafield, Sara Hooker, Colin Raffel, Pe-\ndro H. Martins, André F. T. Martins, Jessica Zosa\nForde, Peter Milder, Edwin Simpson, Noam Slonim,\nJesse Dodge, Emma Strubell, Niranjan Balasubra-\nmanian, Leon Derczynski, Iryna Gurevych, and Roy\nSchwartz. 2023. Efficient Methods for Natural Lan-\nguage Processing: A Survey. Transactions of the\nAssociation for Computational Linguistics, 11:826–\n860.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In International\nConference on Learning Representations.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022.\nTowards understanding chain-of-thought prompt-\ning: An empirical study of what matters. ArXiv,\nabs/2212.10001.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\n5024\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023a. Chain-of-thought prompting\nelicits reasoning in large language models.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert\nWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, and Tengyu Ma. 2023b.\nLarger language models do in-context learning dif-\nferently.\nQiang Wei, Yukun Chen, Mandana Salimi, Joshua C\nDenny, Qiaozhu Mei, Thomas A Lasko, Qingxia\nChen, Stephen Wu, Amy Franklin, Trevor Cohen,\nand Hua Xu. 2019. Cost-aware active learning for\nnamed entity recognition in clinical text. Journal\nof the American Medical Informatics Association ,\n26(11):1314–1322.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-\npeng Kong. 2022. Self-adaptive in-context learning.\nArXiv, abs/2212.10375.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In Interna-\ntional Conference on Learning Representations.\nCanwen Xu, Yichong Xu, Shuohang Wang, Yang Liu,\nChenguang Zhu, and Julian McAuley. 2023. Small\nmodels are valuable plug-ins for large language mod-\nels.\nSohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon\nYe, Hyunji Lee, and Minjoon Seo. 2023. Improving\nprobability-based prompt selection through unified\nevaluation and analysis.\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and\nLingpeng Kong. 2023a. Compositional exemplars\nfor in-context learning.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7163–7189, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nXi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoy-\nanov, Greg Durrett, and Ramakanth Pasunuru. 2023b.\nComplementary explanations for effective in-context\nlearning. In Findings of the Conference of the Asso-\nciation for Computational Linguistics.\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-\nsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\nand Taeuk Kim. 2022. Ground-truth labels matter: A\ndeeper look into input-label demonstrations.\nW. Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingx-\nuan Ju, Soumya Sanyal, Chenguang Zhu, Michael\nZeng, and Meng Jiang. 2022. Generate rather than\nretrieve: Large language models are strong context\ngenerators. ArXiv, abs/2209.10063.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. Opt: Open\npre-trained transformer language models.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Ac-\ntive example selection for in-context learning. InPro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 9134–\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nZhisong Zhang, Emma Strubell, and Eduard Hovy.\n2022c. A survey of active learning for natural lan-\nguage processing. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning.\nTony Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\nICML, abs/2102.09690.\nYuekai Zhao, Haoran Zhang, Shuchang Zhou, and Zhi-\nhua Zhang. 2020. Active learning approaches to\nenhancing neural machine translation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 1796–1806, Online. Association\nfor Computational Linguistics.\n5025\nFigure 9: Results per model family.\nA Experimental Details\nA.1 Tasks & Datasets\nFollowing Min et al. (2022), we evaluate our\nmodels in 15 classification and 9 multi-choice\ntasks taken from the Crossfit (Ye et al., 2021)\nbenchmark. Specifically the tasks we evaluate are\npoem_sentiment (Sheng and Uthus, 2020), glue-\nwnli (Wang et al., 2019; Levesque et al., 2012),\nclimate_fever (Diggelmann et al., 2020), glue-\nrte (Wang et al., 2019), superglue-cb (de Marn-\neffe et al., 2019), sick (Minaee et al., 2021), medi-\ncal_questions_pairs (McCreery et al., 2020), glue-\nmrpc (Wang et al., 2019; Dolan and Brockett,\n2005), hate_speech18 (de Gibert et al., 2018),\nethos-national_origin (Mollas et al., 2022), ethos-\nrace (Mollas et al., 2022), ethos-religion (Mollas\net al., 2022), tweet_eval-stance_atheism (Barbieri\net al., 2020), tweet_eval-stance_feminist (Barbi-\neri et al., 2020) and quarel (Tafjord et al., 2019a),\nopenbookqa,qasc (Khot et al., 2020), common-\nsense_qa, ai2_arc (Clark et al., 2018),codah (Chen\net al., 2019), superglue-copa (Gordon et al., 2012),\nquartz-with_knowledge (Tafjord et al., 2019b),\nquartz-no_knowledge (Tafjord et al., 2019b), for\nclassification and multi-choice respectively.\nA.2 Full results\nWe provide below the full set of results, for each\ndataset, model and active learning acquisition strat-\negy considered. The dashed line depicts the major-\nity vote baseline.\nA.3 Model Family\nWe provide the results on few-shot learning with\nk=16 demonstrations per prompt per model family\nand task type in Figure 9. We observe the same\npatterns for both GPT and OPT models.\n5026\n5027\n5028\n5029\n5030\n5031\n5032\n5033\n5034"
}