{
    "title": "Transformers for modeling physical systems",
    "url": "https://openalex.org/W3216107495",
    "year": 2021,
    "authors": [
        {
            "id": null,
            "name": "Geneva, Nicholas",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A4286942392",
            "name": "Zabaras, Nicholas",
            "affiliations": [
                "University of Notre Dame"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2838828138",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W6704574911",
        "https://openalex.org/W1985926778",
        "https://openalex.org/W2801459707",
        "https://openalex.org/W2994028939",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W6752307458",
        "https://openalex.org/W2103496339",
        "https://openalex.org/W6757998062",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6760979436",
        "https://openalex.org/W6754852571",
        "https://openalex.org/W6601894380",
        "https://openalex.org/W3035079212",
        "https://openalex.org/W2951392159",
        "https://openalex.org/W3033067316",
        "https://openalex.org/W1972005403",
        "https://openalex.org/W2965300934",
        "https://openalex.org/W1988115241",
        "https://openalex.org/W2137983211",
        "https://openalex.org/W6694756022",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W1997054581",
        "https://openalex.org/W2561027863",
        "https://openalex.org/W2964350754",
        "https://openalex.org/W1997273053",
        "https://openalex.org/W2730644873",
        "https://openalex.org/W6769286903",
        "https://openalex.org/W6745764291",
        "https://openalex.org/W2177933918",
        "https://openalex.org/W6639478124",
        "https://openalex.org/W2777417212",
        "https://openalex.org/W6776724805",
        "https://openalex.org/W3004360475",
        "https://openalex.org/W6725080942",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W2811208018",
        "https://openalex.org/W6751845306",
        "https://openalex.org/W2963765797",
        "https://openalex.org/W2006640177",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W6774033856",
        "https://openalex.org/W6628877408",
        "https://openalex.org/W6763355497",
        "https://openalex.org/W6743945476",
        "https://openalex.org/W2887258823",
        "https://openalex.org/W3016309349",
        "https://openalex.org/W2786232134",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2097553780",
        "https://openalex.org/W3033526262",
        "https://openalex.org/W2965742591",
        "https://openalex.org/W2018159038",
        "https://openalex.org/W2998104826",
        "https://openalex.org/W6752378368",
        "https://openalex.org/W2905872298",
        "https://openalex.org/W2784733489",
        "https://openalex.org/W2953118818",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W1576278180",
        "https://openalex.org/W3000514857",
        "https://openalex.org/W2950527759",
        "https://openalex.org/W3033256359",
        "https://openalex.org/W3129637124",
        "https://openalex.org/W3112085823",
        "https://openalex.org/W3101766586",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W2949335953",
        "https://openalex.org/W46679369",
        "https://openalex.org/W4288289156",
        "https://openalex.org/W4287324022",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W3146803896",
        "https://openalex.org/W2913668833",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W4303633609",
        "https://openalex.org/W3084942782",
        "https://openalex.org/W2343765104",
        "https://openalex.org/W2926003515",
        "https://openalex.org/W4241618768",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W3099412592",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2312564560",
        "https://openalex.org/W4247607820",
        "https://openalex.org/W3098172061",
        "https://openalex.org/W2507974895",
        "https://openalex.org/W2559655401",
        "https://openalex.org/W2963302407",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4234552385",
        "https://openalex.org/W2955227499",
        "https://openalex.org/W3100968477",
        "https://openalex.org/W3101392902",
        "https://openalex.org/W2808746463",
        "https://openalex.org/W3135481891",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2604272474",
        "https://openalex.org/W2804632314",
        "https://openalex.org/W2753603185",
        "https://openalex.org/W2953065922",
        "https://openalex.org/W1971947347",
        "https://openalex.org/W3007913393",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W4238634189",
        "https://openalex.org/W4297837594",
        "https://openalex.org/W3132349482",
        "https://openalex.org/W2791552377",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2788305176",
        "https://openalex.org/W2776083643",
        "https://openalex.org/W2599674900",
        "https://openalex.org/W2277172374",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2996064936",
        "https://openalex.org/W2483714367",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2946567085"
    ],
    "abstract": null,
    "full_text": "Transformers for Modeling Physical Systems\nNicholas Genevaa, Nicholas Zabarasa,∗\naScientiﬁc Computing and Artiﬁcial Intelligence (SCAI) Laboratory, University of Notre Dame,\n311 Cushing Hall, Notre Dame, IN 46556, USA\nAbstract\nTransformers are widely used in natural language processing due to their ability to\nmodel longer-term dependencies in text. Although these models achieve state-of-the-\nart performance for many language related tasks, their applicability outside of the\nnatural language processing ﬁeld has been minimal. In this work, we propose the\nuse of transformer models for the prediction of dynamical systems representative of\nphysical phenomena. The use of Koopman based embeddings provide a unique and\npowerful method for projecting any dynamical system into a vector representation\nwhich can then be predicted by a transformer. The proposed model is able to accu-\nrately predict various dynamical systems and outperform classical methods that are\ncommonly used in the scientiﬁc machine learning literature. 1\nKeywords: Transformers, Deep Learning, Self-Attention, Physics, Koopman,\nSurrogate Modeling\n1. Introduction\nThe transformer model [1], built on self-attention, has largely become the state-of-\nthe-art approach for a large set of natural language processing (NLP) tasks including\nlanguage modeling, text classiﬁcation, question answering, etc. Although more recent\ntransformer work is focused on unsupervised pre-training of extremely large mod-\nels [2, 3, 4, 5], the original transformer model garnered attention due to its ability\nto out-perform other state-of-the-art methods by learning longer-term dependencies\nwithout recurrent connections. Given that the transformer model was originally de-\nveloped for NLP, nearly all related work has been rightfully conﬁned within this ﬁeld\nwith only a few exceptions. Here, we focus on the development of transformers to\n∗Corresponding author\n1Code available at: https://github.com/zabaras/transformer-physx.\nPreprint submitted to Neural Networks December 15, 2021\narXiv:2010.03957v6  [cs.LG]  13 Dec 2021\nmodel dynamical systems that can replace otherwise expensive numerical solvers. In\nother words, we are interested in using transformers to learn the language of physics.\nThe surrogate modeling of physical systems is a research ﬁeld that has ex-\nisted for several decades and is a large ongoing eﬀort in scientiﬁc machine learn-\ning. Past literature has explored multiple surrogate approaches including Gaussian\nprocesses [6, 7, 8], polynomial chaos expansions [9], reduced-order models [10, 11],\nreservoir computing [12] and deep neural networks [13, 14, 15]. A surrogate model\nis deﬁned as a computationally inexpensive approximate model of a physical phe-\nnomenon that is designed to replace an expensive computational solver that would\notherwise be needed to resolve the system of interest. An important characteristic\nof surrogate models is their ability to model a distribution of initial or boundary\nconditions rather than learning just one solution. This is arguably essential for the\njustiﬁcation of training a deep learning model versus using a standard numerical\nsolver, particularly in the context of utilizing deep learning methods which tend to\nhave expensive training procedures. The most tangible applications of surrogates are\nfor optimization, design and inverse problems where many repeated simulations are\ntypically needed.\nStandard deep neural network architectures such as auto-regressive [16, 15], resid-\nual/Euler [17, 18], recurrent and LSTM based models [16, 19, 20, 21] have been\nlargely demonstrated to be eﬀective at modeling various physical dynamics. Such\nmodels generally rely on the most recent time-steps to provide complete information\non the current and past state of the system’s evolution. Approaches that meld nu-\nmerical time–integration methods with neural networks have also proven to be fairly\nsuccessful, e.g. [22, 23, 24], but have a ﬁxed temporal window from which information\nis provided. Present machine learning models lack generalizable time cognizant capa-\nbilities to predict multi-time-scale phenomena present in systems including turbulent\nﬂuid ﬂow, multi-scale materials modeling, molecular dynamics, chemical processes,\netc. Much work is needed to scale such deep learning models to complex physical\nsystems that are of scientiﬁc and industrial interest. This work deviates from this\npre-existing literature by investigating the use of transformers for the prediction of\nphysical systems, relying entirely on self-attention to surrogate model dynamics. In\nthe recent work of [25], such self-attention models were tested to learn single solutions\nof several low-dimensional ordinary diﬀerential equations.\nThe novel contributions of this paper are as follows: (a) The application of\nself-attention transformer models for modeling physical dynamics; (b) The use of\nKoopman dynamics for developing physics inspired embeddings of high-dimensional\nsystems with connections to embedding methods seen in NLP; (c) Discussion of\nthe relations between self-attention with traditional numerical time-integration; (d)\n2\nDemonstration of our model on high-dimensional partial diﬀerential equation prob-\nlems that include chaotic dynamics, ﬂuid ﬂows and reaction-diﬀusion systems. To\nthe authors best knowledge, this is the ﬁrst work to explore transformer NLP archi-\ntectures for the surrogate modeling of physical systems. The remainder of this paper\nis as follows: In Section 2, the machine learning methodology is discussed including\nthe transformer decoder model in Section 2.1 and the Koopman embedding model\nin Section 2.2. Following in Section 3, the proposed model is implemented for a set\nof numerical examples of diﬀerent dynamical nature. This includes classical chaotic\ndynamics in Section 3.1, periodic ﬂuid dynamics in Section 3.2 and three-dimensional\nreaction-diﬀusion dynamics in Section 3.3. Lastly, concluding discussion and future\ndirections are given in Section 4.\n2. Methods\nWe are interested in systems that can be described through a dynamical ordinary or\npartial diﬀerential equation of the form:\nφt = F\n(\nx,φ(t,x,η),∇xφ,∇2\nxφ,φ·∇xφ,...\n)\n,\nt∈T ⊂R+, x∈Ω ⊂Rm, (1)\nin which φ ∈Rn is the solution of this diﬀerential equation of n state variables\nwith parameters η, in the time interval T and spatial domain Ω with a boundary\nΓ ⊂ Ω. This general form can embody a vast spectrum of physical phenomena\nincluding ﬂuid ﬂow and transport processes, mechanics and materials physics, and\nmolecular dynamics. In this work, we are interested in learning the set of solutions\nfor a distribution of initial conditions φ0 ∼p(φ0), boundary conditions B(φ) ∼\np(B) ∀x∈Γ or equation parameters η ∼p(η). This accounts for modeling initial\nvalue, boundary value and stochastic problems.\nTo make the modeling of such dynamical systems applicable to the use of modern\nmachine learning architectures, the continuous solution is discretized in both the\nspatial and temporal domains such that the solution of the diﬀerential equation is\nΦ = ( φ0,φ1,... φT) ;φi ∈Rn×d, for which φi has been discretized by d points in\nΩ. We assume an initial state φ0 and that the time interval T is discretized by\nT time-steps with a time-step size ∆ t. Hence, we pose the modeling a dynamical\nsystem as a time-series problem. The proposed machine learning methodology has\ntwo core components: the transformer for modeling dynamics and the embedding\nnetwork for projecting physical states into a vector representation. Similar to NLP,\nthe embedding model is trained prior to the transformer. This embedding model is\nthen frozen and the entire data-set is converted to the embedded space in which the\n3\ntransformer is then trained as illustrated in Fig. 1. During testing, the embedding\ndecoder is used to reconstruct the physical states from the transformer’s predictions.\nFigure 1: The two training stages for modeling physical dynamics using transformers. (Left to right)\nThe embedding model is ﬁrst trained using Koopman based dynamics. The embedding model is\nthen frozen (ﬁxed), all training data is embedded and the transformer is trained in the embedded\nspace.\n2.1. Transformer\nThe transformer model was originally designed with NLP as the sole application with\nword vector embeddings of a passage of text being the primary input [1]. However,\nrecent works have explored using attention mechanisms for diﬀerent machine learning\ntasks [26, 27, 28] and a few investigate the use of transformers for applications outside\nof the NLP ﬁeld [29]. This suggests that self-attention and in particular transformer\nmodels may work well for any problem that can be posed as a sequence of vectors.\n2.1.1. Transformer Decoder\nIn this work, the primary input to the transformer will be an embedded dynamical\nsystem, Ξ = ( ξ0,ξ1,.... ξT), where the embedded state at time-step i is denoted as\nξi ∈Re. Given that we are interested in the prediction of a physical time series,\nthis motivates the usage of a language modeling architecture that is designed for the\nsequential prediction of words in a body of text. We select the transformer decoder\narchitecture used in the Generative Pre-trained Transformer (GPT) models [30, 3].\nOur model follows the GPT-2 architecture based on the implementation in the Hug-\nging Face transformer repository [31], but is signiﬁcantly smaller in size than these\nmodern NLP transformers. This model consists of a stack of transformer decoder\nlayers that use masked attention, as depicted in Fig. 2. The input to the transformer\nis the embedded representation of the physical system from the embedding model and\na sinusoidal positional encoding proposed in the original transformer [1]. Since the\n4\nFigure 2: The transformer decoder model used for the prediction of physical dynamics.\ntransformer does not contain any recurrent or convolutional operation, information\nregarding the relative position of the embedded input sequence must be provided.\nThe positional embedding is deﬁned by the following sine and cosine functions:\nPEpos,2j = sin\n(\npos/100002j/e)\n, PE pos,2j+1 = cos\n(\npos/100002j/e)\n, (2)\nfor the 2 j and 2 j + 1 elements in the embedded vector, ξi. pos is the embedded\nvector’s relative or global position in the input time series. To train the model,\nconsider a data set of D embedded i.i.d. time-series D= {Ξi}\nD\ni=1 for which we can\nuse the standard time-series Markov model (language modeling) log-likelihood:\nLD=\nD∑\ni\nT∑\nj\n−log p\n(\nξi\nj|ξi\nj−k,..., ξi\nj−1,θ\n)\n, (3)\nwhere θ are the model’s parameters and k is the transformer’s context window.\nContrary to the standard NLP approach which poses the likelihood as a softmax over\na dictionary of tokens, the likelihood here is taken as a standard Gaussian between\nthe transformer’s prediction and the target embedded value resulting in a L2 loss.\nThis is due to the fact that the solution to most physical systems cannot be condensed\nto a discrete ﬁnite set. Thus tokenization into a ﬁnite dictionary not possible and a\nsoftmax approach not applicable. Training is the standard auto-regressive method\nused in GPT [30], as opposed to the word masking [2], constrained to the embedded\nspace. The physical states, φi, have the potential to be very high-dimensional and\ntraining the transformer in the lower-dimensional embedded space can signiﬁcantly\nlower training costs.\n2.1.2. Self-Attention\nSelf-attention is the main mechanism that allows the transformer to learn temporal\ndependencies. Prior to the seminal transformer paper, several works had already\n5\nproposed a handful of diﬀerent attention models typically integrated into recurrent\nmodels for language processing [32, 33, 34]. With the popularization of the trans-\nformer model [1], the most commonly used attention model is the scaled-dot product\nattention:\nki = Fk(xi), qi = Fq(xi), vi = Fv(xi) (4)\ncn =\nk∑\ni=1\nαn,ivi, α n,i = exp(qT\nnki/√dk)∑k\nj=1 exp(qT\nnkj/√dk)\n, (5)\nin which we use xi ∈Rd and ci ∈Rdv to denote an arbitrary input and context\noutput, respectively. k∈Rdk, q∈Rdk and v∈Rdv are referred to as the key, query,\nand value vectors, respectively, calculated using neural networksFk, Fq and Fv. The\nattention score, αn,i, is calculated by the soft-max of the dot product between the\nquery and key vectors scaled by the dimension dk. Due to the soft-max calculation\nnote that the attention scores always sum to one, ∑k\ni=1 αn,i = 1, for every input.\nThe attention calculation can be condensed for the entire context length of the\nmodel by a matrix representation C= softmax\n(\nQKT/√dk\n)\nV, where Q∈Rk×dk,\nK∈Rk×dk and V ∈Rk×dv. As illustrated in Fig. 2, self-attention is typically imple-\nmented with a residual connection with multiple independent attention calculations\nreferred to as attention heads [1]. While computationally inexpensive to evaluate,\nthe memory requirement of scaled-dot product attention can become increasingly\ncumbersome as the context length, k, increases. Hence, several methods for approx-\nimating this calculation have been proposed which include the use of kernels and\nrandom projections in an attempt to lower the dimensionality of the self-attention\ncalculation without loss of predictive accuracy [35, 36, 37].\nWhile designed for natural language processing, in the context of dynamical sys-\ntems, self-attention bears a very similar form to numerical time-integration meth-\nods. The use of time-integration methods such as Runge-Kutta schemes or Euler\nmethods can be found in Neural ODEs [38, 39] and Res-Net based models [17, 18].\nSelf-attention oﬀers a much larger learning capacity than using these traditional nu-\nmerical methods for learning a unique latent time-integration method. The function\nspace of a self-attention layer with a residual connection contains the space of ex-\nplicit linear time-integration methods within an arbitrarily small non-zero amount\nof error.\nTheorem 1.Consider a dynamical system of the form, dφ/dt= f(t,φ), φ(t) ∈Rd,\nwhere f : R ×Rd →Rd is Lipschitz continuous with respect to φand continuous in\nt. Let the function Aθ(t,φt−k∆t:t) = ˆφt+∆t be a self-attention layer with a residual\n6\nconnection, of context length kand containing learnable parametersθ∈Rdθ. Suppose\nA :=\n{\nAθ(t,φ) |∀θ∈Rdθ\n}\nbe the set of all possible self-attention calculations. Let\nMi\n(\nt,φt−(i−1)∆t:t\n)\n= φt+∆t be the ith order explicit Adams method time-integrator.\nThe set of functions up to kth order M := {Mi|1 ≤i≤k}is a subset of A such\nthat ∃Aθi ∈A s.t. ||Mi −Aθi||∞<O(ϵ) for any Mi ∈M and ϵ> 0.\nProof. The state variables are discretized w.r.t. time such that φi = φ(i∆t); ti =\ni∆t. The general deﬁnition for linear multi-step methods follows:\nφn+s + as−1 ·φn+s−1 + ...+ a0 ·φn\n= ∆t·(bs ·f(tn+s,φn+s) + bs−1 ·f(tn+s−1,φn+s−1) + ...+ b0 ·f(tn,φn)) , (6)\nin which ai and bi are coeﬃcients determined by the integration method. For explicit\nAdams methods, the φn+s is directly computed with bs = 0, as−1 = −1 and a0:s−2 =\n0. The generalized formula for s-step explicit Adams methods can be represented as\nthe following linear combination:\nMs = φn+s = φn+s−1 + ∆t\ns−1∑\nj=0\nbjf(tn+j,φn+j) , (7)\nwhich encapsulates up to and including sth order time integration [40]. Consider\na residual scaled dot-product self-attention calculation with context length, s, for\noutput prediction ˆφn+s, input states φn:n+s−1 and time t:\nAθi = ˆφn+s = φn+s−1 +\ns−1∑\ni=0\nαn+s−1,ivi, α n+s−1,i = exp(qT\nn+s−1ki/√dk)∑s−1\nj=0 exp(qT\nn+s−1kj/√dk)\n, (8)\nfor which ki = Fk(tn+i,φn+i), qi = Fq(tn+i,φn+i) and vi = Fv(tn+i,φn+i) vectors\nare outputs of diﬀerentiable functions parameterized by neural networks:\nFk : R ×Rd →Rdk, Fq : R ×Rd →Rdk, Fv : R ×Rd →Rdv. (9)\nIt suﬃces to show that there exists a form of Eq. (8) that is equal to Eq. (7) within\na error order O(ϵ) such that\n⏐⏐⏐\n⏐⏐⏐φn+s −ˆφn+s\n⏐⏐⏐\n⏐⏐⏐\n∞\n<O(ϵ). (10)\nTo this end, although Fq and Fk are fully-trainable neural networks, herein we con-\nsider a ﬁxed single-entry query and key vectors resulting in the following simpliﬁed\n7\nself-attention scores:\nqi = Fq(tn+i,φn+i) =\n√\ndkem, ki = Fk(tn+i,φn+i) = log(bi) em,\nαn+s−1,i = exp(log(bi))∑s−1\nj=0 exp(log(bj))\n= bi\n∑s−1\nj=0 bj\n, (11)\nwhere em ∈Rdk is a unit vector with all elements set to zero except them-th element\nthat is set to 1. By the universal approximation theorem [41, 42, 43], the neural\nnetwork, Fv, is assumed to be of suﬃcient capacity such that it can approximate the\nr.h.s. of the governing equation:\nvi = Fv(tn+i,φn+i) = cf(tn+i,φn+i) + O(ϵ), ϵ> 0, (12)\nwhere the constant c is taken as c= ∆t∑s−1\nj=0 bj. Equations (11) and (12) can then\nbe combined leading to the following:\nφn+s−1 +\ns−1∑\ni=0\nαn+s−1,ivi = φn+s−1 +\ns−1∑\ni=0\nbi\n∑s−1\nj=0 bj\ncf(tn+i,φn+i) + O(ϵ),\n= φn+s−1 + ∆t\ns−1∑\ni=0\nbif(tn+i,φn+i) + O(ϵ).\n(13)\nFor a large capacity neural network, we can assume that the error can be made\narbitrarily small (ϵ→0) and Eq. (10) is proved. Therefore, the form of the explicit\nAdams multi-step methods of order ≤scan be captured by a residual self-attention\ntransformer layer.\nA single transformer layer is signiﬁcantly more expressive than any numerical\ntime integration scheme, enabling it to learn more complex temporal dependen-\ncies. However, the linear combination of weighted features from past time-steps\nis a commonality between self-attention and numerical time-integration methods.\nThis mathematical similarity indicates self-attention models may be better suited\nfor learning dynamical systems than alternative deep learning approaches. In par-\nticular traditional Euler time integration present in various models for modeling\ndynamics [17, 18, 44] is a speciﬁc case of this single layer attention calculation.\nRemark. The inclusion of time, t, as an input is required for non-autonomous sys-\ntems with explicit time-dependent terms. This has surprising connections to the\nimplementation of the transformer which uses a positional embedding. Although the\npositional embedding is included for the sake of denoting the order of the input to\nthe transformer, using current time of the given input state can accomplish the same\ngoal.\n8\n2.2. Embedding Model\nThe second major component of the machine learning methodology is the embedding\nmodel responsible for projecting the discretized physical state space into a 1D vector\nrepresentation. In NLP, the standard approach is to tokenize then embed a ﬁnite\nvocabulary of words, syllables or characters using methods such as n-gram models,\nByte Pair Encoding [45], Word2Vec [46, 47], GloVe [48], etc. These methods allow\nlanguage to be represented by a series of 1D vectors that serve as the input to the\ntransformer. Clearly a ﬁnite tokenization and such NLP embeddings are not directly\napplicable to physics, thus we propose our own embedding method designed specif-\nically for dynamical systems. Consider learning the generalized mapping between\nthe system’s state space and embedded space: F: Rn×d →Re and G: Re →Rn×d.\nNaturally, multiple approaches can be used especially if the dimensionality of the\nembedded space is less than that of the state-space but this is not always the case.\nThe primary approach that we will propose is a Koopman observable embedding\nwhich is a technique that can be applied universally to all dynamical systems. Con-\nsidering the discrete time form of the dynamical system in Eq. (1), the evolution of\nthe state variables can be abstracted by φi+1 = F(φi) for which F is the dynamic\nmap from one time-step to the next. The foundation of Koopman theory states\nany dynamical system can be represented in terms of an inﬁnite dimensional linear\noperator acting on an inﬁnite set of state observable functions, g(φi), such that:\nKg(φi) ≜ g◦F(φi) , (14)\nwhere Kis the inﬁnite-dimensional linear operator referred to as the Koopman oper-\nator [49]. This implies that the system of observables can be evolved in time through\nrepeated application of the Koopman operator:\ng(φi+1) = Kg(φi) , g (φi+2) = K2g(φi) , g (φi+3) = K3g(φi) ,... (15)\nin which Kn denotes a n-fold composition, e.g. K3(g) = K(K(K(g))). Modeling the\ndynamics of a system through the linear Koopman space can be attractive due to\nits simpliﬁcation of the dynamics but also the potential physical insights it brings\nalong with it. Spectral analysis of the Koopman operator can reveal fundamental\ndynamical modes that drive the system’s evolution in time.\nKoopman theory can be viewed as a trade oﬀ between lifting the state space\ninto observable space with more complex states but simpler dynamics. In practice,\nthe Koopman operator must be ﬁnitely approximated. This ﬁnite approximation\nrequires the identiﬁcation of the essential measurement functions that govern the\nsystem’s dynamics and the respective approximate Koopman operator. Data-driven\n9\nmachine learning has proven to be an eﬀective approach for learning key Koopman\nobservables for modeling, control and dynamical mode analysis of many physical\nsystems [50, 51, 52, 53]. In recent years, the use of deep neural networks for learning\nKoopman dynamics has proven to be successful [54, 55, 56, 57]. While deep learning\nmethods have enabled greater success with discovering Koopman observables and\noperators such approaches have yet to be demonstrated for the long time prediction\nof high-dimensional systems. This is likely due to the approximation of the ﬁnite-\ndimensional Koopman observables, limited data and complete dependence on the\ndiscovered Koopman operator Kto model the dynamics. Suggesting the prediction\nof a system through a single linear transform clearly has signiﬁcant limitations and\nis fundamentally a naive approach from a machine learning perspective.\nFigure 3: Example of a Koopman embedding for a two-dimensional system using a convolution\nencoder-decoder model. The encoder model, F, projects the physical states into the approximate\nKoopman observable embedding. The decoder model, G recovers the physical states from the\nembedding.\nIn this work, we propose using approximate Koopman dynamics as a methodology\nto develop embeddings for the transformer model such that F(φi) ≜ g(φi). As\nseen in Fig. 3, the embedding model follows a standard encoder-decoder model with\nthe middle latent variables being the Koopman observables. In this model, the\nKoopman operator assumes the form of a learnable banded matrix that is optimized\nwith the auto-encoder. Imposing some level of inductive bias on the form of the\nKoopman matrix is fairly common in similar deep Koopman works [55, 56, 58]. We\nfound that this reduction of learnable parameters helped encourage the model to\ndiscover better dynamical modes, preventing the model from overﬁtting to high-\nfrequency ﬂuctuations. Additionally, this form requires signiﬁcantly less memory to\nstore allowing models with embedding vectors of higher-dimensionality to be trained.\nThis learned Koopman operator is disposed of once training of the embedding model\nis complete. Given the data set of physical state time-series, DΦ = {Φi}\nD\ni , the\n10\nKoopman embedding model is trained with the following loss:\nLDΦ =\nD∑\ni=1\nT∑\nj=0\nλ0 MSE\n(\nφi\nj,G◦F\n(\nφi\nj\n))\n  \nReconstruction\n+λ1 MSE\n(\nφi\nj,G◦K jF\n(\nφi\n0\n))\n  \nDynamics\n+λ2 ∥K∥2\n2\nDecay\n.\n(16)\nThis loss function consists of three components: the ﬁrst is a reconstruction loss\nwhich ensures a consistent mapping to and from the embedded representation. The\nsecond is the Koopman dynamics loss which pushes ξj to follow linear dynamics.\nThe last term decays the Koopman operator’s parameters to help force the model to\ndiscover meaningful dynamical modes and further prevent overﬁtting.\nIn reference to traditional NLP embeddings, we believe our Koopman observ-\nable embedding has a motivation similar to Word2Vec [46] as well as more recent\nembedding methods such as context2vec [59], ELMo [60], etc. These methods are\nbased on word context and association to develop a map where words that are re-\nlated or synonymous to each other have similar embedded vectors. The Koopman\nembedding model has a similar objective encouraging physical realizations contain-\ning similar dynamical modes to also have similar embeddings. This is because the\nKoopman operator is time-invariant which means the embedded states must share\nthe same basis functions that govern their evolution. As a result, the loss func-\ntion of the embedding model rewards time-steps that are near each other in time\nor have the same underlying dynamics to have similar embeddings. Hence, our goal\nwith the embedding model is to not ﬁnd the true Koopman observables or operator,\nbut rather leverage Koopman to enforce physical context and association using the\nlearned dynamical modes.\n3. Experiments and Results\nThe proposed transformer model is implemented for three dynamical systems. The\nclassical Lorenz system is used as a baseline test case in Section 3.1 to compare\nthe proposed model to alternative machine learning approaches due to the Lorenz\nsystem’s numerical sensitivity. Following in Section 3.2, we consider the modeling of\ntwo-dimensional Navier-Stokes ﬂuid ﬂow and compare diﬀerent embedding methods\nfor the transformer. Lastly, to demonstrate the models scalability, we demonstrate\nin Section 3.3 the transformer for the prediction of a three-dimensional reaction-\ndiﬀusion system. These examples encompass surrogate modeling physical systems\nwith stochastic initial conditions and parameters.\n11\n3.1. Chaotic Dynamics\nAs a foundational numerical example to rigorously compare the proposed model to\nother classical machine learning techniques, we will ﬁrst look at surrogate modeling\nof the Lorenz system governed by:\ndx\ndt = σ(y−x) , dy\ndt = x(ρ−z) −y, dz\ndt = xy−βz. (17)\nWe use the classical parameters of ρ = 28 ,σ = 10 ,β = 8 /3. For this numerical\nexample, we wish to develop a surrogate model for predicting the Lorenz system\ngiven a random initial state x0 ∼U(−20,20), y0 ∼U(−20,20) and z0 ∼U(10,40).\nIn other words, we wish to surrogate model various initial value problems for this\nsystem of ODEs. The Lorenz system is used because of its well known chaotic\ndynamics which make it extremely sensitive to numerical perturbations and thus an\nexcellent benchmark for assessing a machine learning model’s accuracy.\nA total of four alternative machine learning models are implemented: a fully-\nconnected auto-regressive model, a fully-connected LSTM model, a deep neural net-\nwork Koopman model and lastly an echo-state model. All of these types of models\nhave been proposed in past literature for predicting various physical systems (e.g.\nauto-regressive [15], fully-connected LSTMs [61], deep Koopman [55, 56] and echo-\nstate models [62, 63]). Each are provided the same training, validation and testing\ndata sets containing 2048, 64 and 256 time-series at a time-step size of ∆ t = 0.01\nsolved using a Runge-Kutta numerical solver, respectively. The training data set\ncontains time-series of 256 time-steps while the validation and testing data sets have\n1024 time-steps. Each model is allowed to train for 500 epochs if applicable. The\nproposed transformer and embedding model are trained for 200 and 300 epochs, re-\nspectively, with an embedding dimension of 32. The embedding model is a simple\nfully-connected encoder-decoder model, F: R3 →R32; G: R32 →R3, illustrated in\nFig. 4. The transformer is trained with a context length of 64 with 4 transformer\ndecoder layers. Both the training and validation data sets were chunked into a set\nof 64 steps for the alternative models, when applicable, to train on the same context\nlength.\nWe plot four separate test cases in Fig. 5 for which only the initial state is\nprovided and the transformer model predicts 320 time-steps. Several test predictions\nfor alternative models are provided in Appendix A. In general, we can see that the\ntransformer model is able to yield extremely accurate predictions even beyond its\ncontext length. Additionally, we plot the Lorenz solution for 25k time-steps from a\nnumerical solver and predicted from the transformer model in Fig. 6. Note that both\nhave the same structure, which qualitatively indicates that the transformer indeed\nmaintains physical dynamics.\n12\nFigure 4: Fully-connected embedding network with ReLU activation functions for the Lorenz sys-\ntem.\nFigure 5: Four test case predictions using the transformer model for 320 time-steps.\nThe proposed transformer and alternative models’ relative mean squared errors\nfor the test set are plotted in Fig. 7a as a function of time and listed in Table 1 seg-\nmented into several intervals based on the transformer’s context length. In general,\nwe can see all deep learning models perform well in the time-series length for which\nthey were trained with the deep Koopman model performing the best followed by\nthe transformer. As we extrapolate our predictions past the trained context range,\nthe beneﬁts of the transformer become apparent with it achieving the best accuracy\nfor later times. To quantify accuracy of the chaotic dynamics of each model, the\nLorenz map is plotted in Fig. 7b which is a well-deﬁned relation between successive\nz local maxima despite the Lorenz’s chaotic nature. Calculated using 25k time-step\npredictions from each model, again we can see that the transformer model agrees\nthe best with the numerical solver indicating that it has learned the best physical\ndynamics of all the tested models.\nAdditionally, we test the transformer’s sensitivity to contaminated data by adding\nwhite noise to the training observations scaled by the magnitude of the state vari-\nables. Each model tested with clean data is retrained with data perturbed by 1%\nand 5% noise. The eﬀects of these two diﬀerent noise levels is qualitatively illus-\n13\n(a) Numerical Solver\n (b) Transformer\nFigure 6: Lorenz solution of 25k time-steps with ∆ t = 0.01.\n(a)\n (b)\nFigure 7: (a) The test relative mean-squared-error (MSE) with respect to time. (b) The Lorenz\nmap produced by each model.\ntrated in Appendix A. The errors are listed in Table 2. In general, we can indeed\nsee that the transformer can still perform adequately with noisy data by being the\nbest performing model for 1% noise and still being competitive, particularly at later\ntime-steps, with 5% noise.\nThe self-attention vectors, αi ∈R64, for a single time-series prediction of 512\nsteps are plotted in Fig. 8. For time-steps over i≥64, the full context length of the\ntransformer is used with the attention weight αi,64 corresponding to the most recent\ntime-step. Note that the transformer has learned multi-scale temporal dependencies\nnot achievable with other machine learning architectures. Additionally, this veriﬁes\nthat each attention head is learning diﬀerent dependencies suggesting that each head\nmay be a particular component of the transformer’s latent dynamics. This aligns\nwith what is observed in NLP when using multi-head attention which increases the\ntransformer’s predictive accuracy [1].\n14\nTable 1: Test set relative mean-squared-error (MSE) for surrogate modeling the Lorenz system at\nseveral time-step intervals.\nRelative MSE\nModel Parameters [0 −64) [64 −128) [128 −192)\nTransformer 36k/54k† 0.0003 0.0060 0.0221\nLSTM 103k 0.0041 0.0175 0.0369\nAutoregressive 92k 0.0057 0.0253 0.0485\nEcho State 7.5k/6.3m‡ 0.1026 0.1917 0.2209\nKoopman 108k 0.0001 0.0962 2.0315\n†Learnable parameters for the embedding/transformer model.\n‡Learnable output parameters/ﬁxed input and reservoir parameters.\nTable 2: Test set relative mean-squared-error (MSE) for surrogate modeling the Lorenz system at\nseveral time-step intervals with noisy data.\nRelative MSE 1% Noise Relative MSE 5% Noise\nModel [0 −64) [64 −128) [128 −192) [0 −64) [64 −128) [128 −192)\nTransformer 0.0021 0.0216 0.0429 0.0210 0.0759 0.1292\nLSTM 0.0045 0.0218 0.0437 0.0212 0.0758 0.1324\nAutoregressive 0.0114 0.0417 0.0901 0.0760 0.2060 0.2065\nEcho State 0.0859 0.1686 0.2102 0.1000 0.1581 0.2051\nKoopman 0.0047 0.1192 0.1597 0.0200 0.0787 0.1906\n3.2. 2D Fluid Dynamics\nThe next dynamical system we will test is transient 2D ﬂuid ﬂow governed by the\nNavier-Stokes equations:\n∂ui\n∂t + uj\n∂ui\n∂xj\n= −1\nρ\n∂p\n∂xi\n+ ν ∂2ui\n∂xj∂xj\n, (18)\nin which ui and p are the velocity and pressure, respectively. ν is the viscosity of\nthe ﬂuid. We consider modeling the classical problem of ﬂow around a cylinder at\nvarious Reynolds number deﬁned by Re = uind/ν in which uin = 1 and d = 2 are\nthe inlet velocity and cylinder diameter, respectively. In this work, we choose to\ndevelop a surrogate model to predict the solution at any Reynolds number between\nRe ∼U (100,750). This problem is a fairly classical ﬂow to investigate in scientiﬁc\nmachine learning with various levels of diﬃculty [64, 65, 55, 66, 67, 21]. Here we\nchoose one of the more diﬃcult forms: model the ﬂow starting at a steady state ﬂow\n15\nFigure 8: Attention vectors of the transformer model for the prediction of a single test case of the\nLorenz system. The top contours illustrate the attention weights at each time-step; the bottom\nshows the respective state variables.\nﬁeld at t = 0. Meaning the model is provided zero information on the structure of\nthe cylinder wake during testing other than the viscosity.\nTraining, validation and testing data is obtained using the OpenFOAM simula-\ntor [68], from which a rectangular structured sub-domain is sampled centered around\nthe cylinder’s wake. Given that the ﬂow is two-dimensional, our model will predict\nthe x-velocity, y-velocity and pressure ﬁelds, ( ux,uy,p) ∈R3×64×128. The training,\nvalidation and test data sets consist of 27, 6 and 7 ﬂuid ﬂow simulations, respec-\ntively with 400 time-steps each at a physical time-step size of ∆ t = 0.5. As a base\nline model, we train a convolutional encoder-decoder model with a stack of three\nconvolutional LSTMs [69] in the center. The input for this model consists of the\nvelocity ﬁelds, pressure ﬁeld and viscosity of the ﬂuid. We note that convolutional\nLSTMs have been used extensively in recent scientiﬁc machine learning literature\nfor modeling various physical systems including ﬂuid dynamics [66, 70, 19, 21, 71],\nthus can be considered a state-of-the-art approach. The convolutional LSTM model\nis trained for 500 epochs.\nAdditionally, three diﬀerent embedding methods are implemented: the ﬁrst is\nthe proposed Koopman observable embedding using a convolutional auto-encoder\n16\nillustrated in Fig. 9. This model encodes the ﬂuid x-velocity, y-velocity, pressure and\nviscosity ﬁelds to an embedded dimension of 128, F: R4×64×128 →R128; G: R128 →\nR3×64×128. The second embedding method is using the same convolutional auto-\nFigure 9: 2D convolutional embedding network with ReLU activation functions for the ﬂow around a\ncylinder system consisting of 5 convolutional encoding/decoding layers. Each convolutional operator\nhas a kernel size of (3 , 3). In the decoder, the feature maps are up-sampled before applying a\nstandard convolution. Additionally, two auxiliary fully-connected networks are used to predict the\ndiagonal and oﬀ-diagonal elements of the Koopman operator for each viscosity ν.\nencoder model but without the enforcement of Koopman dynamics on the embedded\nvariables. The third embedding method tested was principal component analysis\n(PCA) as a classical baseline. For each embedding method an identical transformer\nmodel with a context length of 128 and 4 transformer decoder layers is trained.\nSimilar to the previous example, the embedding models are trained for 300 epochs\nwhen applicable and the transformer is trained for 200. To further isolate the impact\nof the transformer model from the embedding model, we also train a fully-connected\nmodel with 4 LSTM cells using the Koopman embedding as an input for 200 epochs.\nEach trained model is tested on the test set by providing the initial laminar state\nat t = 0 with the ﬂuid viscosity and allowing the model to predict 400 time-steps\ninto the future. Two test predictions using the proposed transformer model with\nKoopman embeddings are plotted in Fig. 10 in which the predicted vorticity ﬁelds\nare in good agreement with the true solution. The test set relative mean square\nerror for each output ﬁeld for each model is plotted in Fig. 11. The errors of each\nﬁeld over the entire time-series are listed in Table 3. Additional results are provided\nin Appendix B.\nFor all alternative models, a rapid error increase can be seen between t= [0,100]\nwhich is due to the transition from the laminar ﬂow into vortex shedding. This\nerror then plateaus since each model is able to produce stable vortex shedding, as\n17\nFigure 10: Vorticity, ω= ∇xuy −∇yux, of two test case predictions using the proposed transformer\nwith Koopman embeddings at Reynolds numbers 233 (top) and 633 (bottom).\nFigure 11: Test set relative mean-squared-error (MSE) of the transformer with Koopman (KM),\nauto-encoder (AE) and PCA embedding methods, the convolutional LSTM model (ConvLSTM)\nand fully-connected LSTM with Koopman embeddings (LSTM-KM).\nillustrated in Figs. B.18 & B.19 in Appendix B. The proposed transformer with\nKoopman embeddings is the only model that can accurately match the instantaneous\nstates from the numerical solution. These results indicate that the performance of\nthe transformer is highly dependent on the embedding method used, which should be\nexpected. However, the transformer with self-attention is equally important to the\nmodel’s success as indicated by the performance of the Koopman embedding LSTM\nmodel. Compared to the widely used ConvLSTM model, the proposed transformer\noﬀers more reliable predictions for this ﬂuid ﬂow with less learnable parameters.\nTo gain a greater understanding of why the Koopman embedding performs better\nthan the alternatives, we perform linear dimensionality reduction of the embedded\nstates using PCA into two principle components, ˜ξ1,˜ξ2, in Fig. 12. For all embed-\nding methods, a circular structure is present reﬂecting the periodic dynamics of the\n18\nTable 3: Test set relative mean-squared-error (MSE) of each output ﬁeld for surrogate modeling\n2D ﬂuid ﬂow past a cylinder. Models listed include the transformer with Koopman (KM), auto-\nencoder (AE) and PCA embedding methods, the convolutional LSTM model (ConvLSTM) and\nfully-connected LSTM with Koopman embeddings (LSTM-KM).\nRelative MSE [0 −400]\nModel Parameters ux uy p\nTransformer-KM 224k/628k† 0.0042 0.0198 0.0021\nTransformer-AE 224k/628k† 0.0288 0.1068 0.0161\nTransformer-PCA 3.1m/628k† 0.0247 0.0984 0.0117\nConvLSTM 934k 0.0240 0.0938 0.0103\nLSTM-KM 224k/759k‡ 0.0266 0.1155 0.0094\n†Learnable parameters for the embedding/ transformer model.\n‡Learnable parameters for the embedding/ LSTM model.\nvortex shedding. Compared to the auto-encoder, we note that the Koopman prin-\nciple subspace has a more consistent structure for later time-steps indicating that\nthe Koopman loss term does indeed encourage the discovery of common dynamical\nmodes. Yet for early time-steps, the Koopman model has a unique trajectory be-\ntween Reynolds numbers. This lack of uniqueness with the PCA embedding at early\ntime-steps results in the transformer not being able to diﬀerentiate between Reynolds\nnumbers, lowering predictive accuracy with this embedding method. The Koopman\napproach strikes a balance between structure but uniqueness between ﬂows for the\ntransformer to learn with.\nFigure 12: The two-dimensional principle subspace of the embedded vectors, ξi, from each tested\nembedding model for two diﬀerent Reynolds numbers.\n19\n3.3. 3D Reaction-Diﬀusion Dynamics\nThe ﬁnal numerical example to demonstrate the proposed transformer model is a\n3D reaction-diﬀusion system governed by the Gray-Scott model:\n∂u\n∂t = ru\n∂2u\n∂x2\ni\n−uv2 + f(1 −u) , ∂v\n∂t = rv\n∂2v\n∂x2\ni\n+ vu2 −(f + k) v, (19)\nin which uand v are the concentration of two species, ru and rv are their respective\ndiﬀusion rates, k is the kill rate and f is the feed rate. This is a classical system\nof particular application to chemical processes as it models the following reaction:\nU+ 2V →3V; V →P. For a set region of feed and kill rates, this seemingly simple\nsystem can yield a wide range of complex dynamics [72, 73]. Hence, under the right\nsettings, this system is an excellent case study to push the proposed methodology to\nits predictive limits. In this work, we will use the parameters: ru = 0.2, rv = 0.1,\nk = 0.055 and f = 0.025 which results in a complex dynamical reaction. Akin to\nthe ﬁrst numerical example, the initial condition of this system is stochastic such\nthat the system is seeded with 3 randomly placed perturbations within the periodic\ndomain.\nTraining, validation and testing data are obtained from a Runge-Kutta ﬁnite\ndiﬀerence simulation on a structured grid, ( u,v) ∈R2×32×32×32. The training, val-\nidation and test data sets consist of 512, 16 and 56 time-series, respectively, with\n200 time-steps each at a physical time-step size of ∆ t = 5. A 3D convolutional\nencoder-decoder is used to embed the two species into a 512 embedding dimension,\nF: R2×32×32×32 →R512; G: R512 →R2×32×32×32, illustrated in Fig. 13.\nTransformer models with varying depth are trained all with a context length\nof 128. All other model and training parameters for the transformer models are\nconsistent. A test prediction using the transformer model is shown in Fig. 14 and\nthe errors for each trained transformer are listed in Table 4. Despite this system\nhaving complex dynamics in 3D space, the transformer is able to produce acceptable\npredictions with very similar structures as the numerical solver. To increase the\nmodel’s predictive accuracy we believe the limitation here is not in the transformer,\nbut rather the number of training data and the inaccuracies of the embedding model\ndue to the dimensionality reduction needed. This is supported by the fact that\nincreasing the transformer’s depth does not yield considerable improvements for the\ntest errors in Table 4. Additional results are provided in Appendix C.\n4. Conclusion\nWhile transformers and self-attention models have been established as a powerful\nframework for NLP tasks, the adoption of such methodologies has yet to fully per-\n20\nFigure 13: 3D convolutional embedding network with leaky ReLU activation functions for the Gray-\nScott system. Batch-normalization used between each of the convolutional layers. In the decoder,\nthe feature maps are up-sampled before applying a standard 3D convolution.\nmute other ﬁelds. In this work, we have demonstrated the potential transformers\nhave for modeling dynamics of physical phenomena. The transformer architecture\nallows the model to learn longer and more complex temporal dependencies than al-\nternative machine learning methods. Such models can be particularly beneﬁcial for\nphysical systems that exhibit dynamics that evolve on multiple time scales or consist\nof multiple phases such as turbulent ﬂuid ﬂow, chemical reactions or molecular dy-\nnamics. This can be attributed to the transformer’s ability to draw information from\nmany past time-steps directly with self-attention, learning accurate time integration\nwithout computationally expensive recurrent connections.\nThe key challenge of using the transformer model is identifying appropriate em-\nbeddings to represent the physical state of the system, for which we propose the use\nof Koopman dynamics to enforce dynamical context. Using the proposed methods,\nour transformer surrogate can outperform alternative models widely seen in recent\nscientiﬁc machine learning literature. The investigation of unsupervised pre-training\n21\n(a) u target (top) and transformer prediction (bottom)\n(b) v target (top) and transformer prediction (bottom)\nFigure 14: Test case volume plots for the Gray-Scott system. Isosurfaces displayed span the range\nu, v= [0.3, 0.5] to show the inner structure.\nof such transformer models as well as gaining a better understanding of what at-\ntention mechanisms imply for physical systems will be the subject of works in the\nfuture.\nAcknowledgments\nThe anonymous reviewers are thanked for their signiﬁcant eﬀort in improving and\nclarifying this manuscript. The work reported here was initiated from the Defense\nAdvanced Research Projects Agency (DARPA) under the Physics of Artiﬁcial Intel-\nligence (PAI) program (contract HR00111890034). The authors acknowledge com-\nputing resources provided by the AFOSR Oﬃce of Scientiﬁc Research through the\nDURIP program and by the University of Notre Dame’s Center for Research Com-\nputing (CRC). The work of NG was supported by the National Science Foundation\n(NSF) Graduate Research Fellowship Program grant No. DGE-1313583.\n22\nTable 4: Test set relative mean-squared-error (MSE) for surrogate modeling 3D Gray-Scott system.\nRelative MSE [0 −200]\nModel Layers Parameters u v\nTransformer 2 6.2m/6.6m† 0.0159 0.0120\nTransformer 4 6.2m/12.9m† 0.0154 0.0130\nTransformer 8 6.2m/25.5m† 0.0125 0.0101\n†Learnable parameters for the embedding/ transformer model.\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, I. Polosukhin, Attention is all you need, in: I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett\n(Eds.), Advances in Neural Information Processing Systems, Vol. 30, Curran\nAssociates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep\nbidirectional transformers for language understanding, in: NAACL-HLT, Asso-\nciation for Computational Linguistics, 2019, pp. 4171–4186. doi:10.18653/v1/\nN19-1423.\nURL https://aclanthology.org/N19-1423\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language\nmodels are unsupervised multitask learners, OpenAI Blog.\nURL https://cdn.openai.com/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf\n[4] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, R. Salakhutdinov,\nTransformer-XL: Attentive language models beyond a ﬁxed-length context, in:\nProceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019, pp. 2978–2988. doi:10.18653/v1/P19-1285.\nURL https://aclanthology.org/P19-1285\n[5] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettle-\nmoyer, V. Stoyanov, Roberta: A robustly optimized bert pretraining approach,\narXiv preprint arXiv:1907.11692.\n23\n[6] I. Bilionis, N. Zabaras, B. A. Konomi, G. Lin, Multi-output separable Gaus-\nsian process: Towards an eﬃcient, fully Bayesian paradigm for uncertainty\nquantiﬁcation, Journal of Computational Physics 241 (2013) 212 – 239.\ndoi:10.1016/j.jcp.2013.01.011.\nURL http://www.sciencedirect.com/science/article/pii/\nS0021999113000417\n[7] I. Bilionis, N. Zabaras, Bayesian Uncertainty Propagation Using Gaussian\nProcesses, Springer International Publishing, 2016, pp. 1–45. doi:10.1007/\n978-3-319-11259-6_16-1 .\nURL https://doi.org/10.1007/978-3-319-11259-6_16-1\n[8] S. Atkinson, N. Zabaras, Structured Bayesian Gaussian process latent variable\nmodel: Applications to data-driven dimensionality reduction and high-\ndimensional inversion, Journal of Computational Physics 383 (2019) 166 – 195.\ndoi:10.1016/j.jcp.2018.12.037.\nURL http://www.sciencedirect.com/science/article/pii/\nS0021999119300397\n[9] D. Xiu, G. E. Karniadakis, The Wiener-Askey polynomial chaos for stochastic\ndiﬀerential equations, SIAM journal on scientiﬁc computing 24 (2) (2002) 619–\n644. doi:10.1137/S1064827501387826.\nURL https://doi.org/10.1137/S1064827501387826\n[10] S. Chakraborty, N. Zabaras, Eﬃcient data-driven reduced-order models for\nhigh-dimensional multiscale dynamical systems, Computer Physics Communi-\ncations 230 (2018) 70 – 88. doi:10.1016/j.cpc.2018.04.007.\nURL http://www.sciencedirect.com/science/article/pii/\nS0010465518301176\n[11] H. Gao, J.-X. Wang, M. J. Zahr, Non-intrusive model reduction of large-scale,\nnonlinear dynamical systems using deep learning, Physica D: Nonlinear\nPhenomena 412 (2020) 132614. doi:10.1016/j.physd.2020.132614.\nURL http://www.sciencedirect.com/science/article/pii/\nS0167278919305573\n[12] G. Tanaka, T. Yamane, J. B. H´ eroux, R. Nakane, N. Kanazawa, S. Takeda,\nH. Numata, D. Nakano, A. Hirose, Recent advances in physical reser-\nvoir computing: A review, Neural Networks 115 (2019) 100 – 123.\ndoi:10.1016/j.neunet.2019.03.005.\n24\nURL http://www.sciencedirect.com/science/article/pii/\nS0893608019300784\n[13] Y. Zhu, N. Zabaras, Bayesian deep convolutional encoder–decoder networks for\nsurrogate modeling and uncertainty quantiﬁcation, Journal of Computational\nPhysics 366 (2018) 415 – 447. doi:10.1016/j.jcp.2018.04.018.\nURL http://www.sciencedirect.com/science/article/pii/\nS0021999118302341\n[14] R. K. Tripathy, I. Bilionis, Deep UQ: Learning deep neural network surrogate\nmodels for high dimensional uncertainty quantiﬁcation, Journal of Computa-\ntional Physics 375 (2018) 565 – 588. doi:10.1016/j.jcp.2018.08.036.\nURL http://www.sciencedirect.com/science/article/pii/\nS0021999118305655\n[15] N. Geneva, N. Zabaras, Modeling the dynamics of PDE systems with physics–\nconstrained deep auto–regressive networks, Journal of Computational Physics\n403 (2020) 109056. doi:10.1016/j.jcp.2019.109056.\nURL https://www.sciencedirect.com/science/article/pii/\nS0021999119307612\n[16] S. Mo, Y. Zhu, N. Zabaras, X. Shi, J. Wu, Deep convolutional encoder-\ndecoder networks for uncertainty quantiﬁcation of dynamic multiphase ﬂow\nin heterogeneous media, Water Resources Research 55 (1) (2019) 703–728.\ndoi:10.1029/2018WR023528.\nURL https://doi.org/10.1029/2018WR023528\n[17] R. Gonz´ alez-Garc´ ıa, R. Rico-Mart´ ınez, I. Kevrekidis, Identiﬁcation of dis-\ntributed parameter systems: A neural net based approach, Computers &\nChemical Engineering 22 (1998) S965 – S968, european Symposium on Com-\nputer Aided Process Engineering-8. doi:10.1016/S0098-1354(98)00191-4.\nURL http://www.sciencedirect.com/science/article/pii/\nS0098135498001914\n[18] A. Sanchez-Gonzalez, J. Godwin, T. Pfaﬀ, R. Ying, J. Leskovec, P. Battaglia,\nLearning to simulate complex physics with graph networks, in: H. D. III,\nA. Singh (Eds.), Proceedings of the 37th International Conference on Machine\nLearning, Vol. 119 of Proceedings of Machine Learning Research, PMLR, 2020,\npp. 8459–8468.\nURL http://proceedings.mlr.press/v119/sanchez-gonzalez20a.html\n25\n[19] M. Tang, Y. Liu, L. J. Durlofsky, A deep-learning-based surrogate model for\ndata assimilation in dynamic subsurface ﬂow problems, Journal of Computa-\ntional Physics 413 (2020) 109456. doi:10.1016/j.jcp.2020.109456.\nURL https://www.sciencedirect.com/science/article/pii/\nS0021999120302308\n[20] R. Maulik, R. Egele, B. Lusch, P. Balaprakash, Recurrent neural network ar-\nchitecture search for geophysical emulation (2020) 1–14doi:10.1109/SC41405.\n2020.00012.\nURL https://doi.org/10.1109/SC41405.2020.00012\n[21] N. Geneva, N. Zabaras, Multi-ﬁdelity generative deep learning turbulent ﬂows,\nFoundations of Data Science 2 (2020) 391. doi:10.3934/fods.2020019.\nURL http://aimsciences.org/article/id/3a9f3d14-3421-4947-a45f-a9cc74edd097\n[22] Yi-Jen Wang, Chin-Teng Lin, Runge-kutta neural network for identiﬁcation of\ndynamical systems in high accuracy, IEEE Transactions on Neural Networks\n9 (2) (1998) 294–307. doi:10.1109/72.661124.\n[23] M. Zhu, B. Chang, C. Fu, Convolutional neural networks combined with Runge-\nKutta methods, arXiv preprint arXiv:1802.08831.\n[24] H. Wessels, C. Weißenfels, P. Wriggers, The neural particle method – an\nupdated Lagrangian physics informed neural network for computational ﬂuid\ndynamics, Computer Methods in Applied Mechanics and Engineering 368\n(2020) 113127. doi:10.1016/j.cma.2020.113127.\nURL https://www.sciencedirect.com/science/article/pii/\nS0045782520303121\n[25] A. Shalova, I. Oseledets, Tensorized transformer for dynamical systems model-\ning, arXiv preprint arXiv:2006.03445.\n[26] P. Veliˇ ckovi´ c, G. Cucurull, A. Casanova, A. Romero, P. Li` o, Y. Bengio, Graph\nattention networks.\nURL https://openreview.net/forum?id=rJXMpikCZ\n[27] H. Zhang, I. Goodfellow, D. Metaxas, A. Odena, Self-attention generative ad-\nversarial networks, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of\nthe 36th International Conference on Machine Learning, Vol. 97 of Proceedings\nof Machine Learning Research, PMLR, 2019, pp. 7354–7363.\nURL http://proceedings.mlr.press/v97/zhang19d.html\n26\n[28] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, H. Lu, Dual attention network\nfor scene segmentation, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2019, pp. 3146–3154.\nURL https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_\nDual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.pdf\n[29] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, I. Sutskever, Genera-\ntive pretraining from pixels, in: H. D. III, A. Singh (Eds.), Proceedings of the\n37th International Conference on Machine Learning, Vol. 119 of Proceedings of\nMachine Learning Research, PMLR, 2020, pp. 1691–1703.\nURL http://proceedings.mlr.press/v119/chen20s.html\n[30] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving language\nunderstanding by generative pre-training, OpenAI Blog.\nURL https://openai-assets.s3.amazonaws.com/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf\n[31] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma,\nY. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, A. M.\nRush, Huggingface’s transformers: State-of-the-art natural language processing,\narXiv preprint arXiv:1910.03771.\n[32] A. Graves, G. Wayne, I. Danihelka, Neural turing machines, arXiv preprint\narXiv:1410.5401.\n[33] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning\nto align and translate, in: 3rd International Conference on Learning Represen-\ntations, 2015. arXiv:1409.0473.\n[34] T. Luong, H. Pham, C. D. Manning, Eﬀective approaches to attention-based\nneural machine translation, in: EMNLP, 2015, pp. 1412–1421. arXiv:508.\n04025.\nURL http://aclweb.org/anthology/D/D15/D15-1166.pdf\n[35] S. Sukhbaatar, E. Grave, P. Bojanowski, A. Joulin, Adaptive attention span in\ntransformers, in: Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, Association for Computational Linguistics, Flo-\nrence, Italy, 2019, pp. 331–335. doi:10.18653/v1/P19-1032.\nURL https://aclanthology.org/P19-1032\n27\n[36] S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, A. Joulin, Augmenting self-\nattention with persistent memory, arXiv preprint arXiv:1907.01470.\n[37] N. Kitaev, L. Kaiser, A. Levskaya, Reformer: The eﬃcient transformer, in:\nInternational Conference on Learning Representations, 2020.\nURL https://openreview.net/forum?id=rkgNKkHtvB\n[38] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, D. K. Duvenaud, Neural ordinary\ndiﬀerential equations, in: S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, R. Garnett (Eds.), Advances in Neural Information Processing\nSystems, Vol. 31, Curran Associates, Inc., 2018, pp. 6571–6583.\nURL https://proceedings.neurips.cc/paper/2018/file/\n69386f6bb1dfed68692a24c8686939b9-Paper.pdf\n[39] E. Dupont, A. Doucet, Y. W. Teh, Augmented neural odes, in: H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d 'Alch´ e-Buc, E. Fox, R. Garnett (Eds.), Ad-\nvances in Neural Information Processing Systems, Vol. 32, Curran Associates,\nInc., 2019, pp. 3140–3150.\nURL https://proceedings.neurips.cc/paper/2019/file/\n21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf\n[40] J. Stoer, R. Bulirsch, Introduction to numerical analysis, Vol. 12, Springer, 2013.\ndoi:10.1007/978-1-4757-2272-7 .\nURL https://doi.org/10.1007/978-1-4757-2272-7\n[41] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\nare universal approximators, Neural Networks 2 (5) (1989) 359–366.\ndoi:10.1016/0893-6080(89)90020-8.\nURL https://www.sciencedirect.com/science/article/pii/\n0893608089900208\n[42] G. Cybenko, Approximation by superpositions of a sigmoidal function, Math-\nematics of control, signals and systems 2 (4) (1989) 303–314. doi:10.1007/\nBF02551274.\nURL https://doi.org/10.1007/BF02551274\n[43] K. Hornik, Approximation capabilities of multilayer feedforward networks,\nNeural Networks 4 (2) (1991) 251–257. doi:10.1016/0893-6080(91)90009-T.\nURL https://www.sciencedirect.com/science/article/pii/\n089360809190009T\n28\n[44] Y. Lu, A. Zhong, Q. Li, B. Dong, Beyond ﬁnite layer neural networks: Bridg-\ning deep architectures and numerical diﬀerential equations, in: International\nConference on Machine Learning, PMLR, 2018, pp. 3276–3285.\n[45] P. Gage, A new algorithm for data compression, Vol. 12, McPherson, KS: R &\nD Publications, c1987-1994., 1994, pp. 23–38.\nURL https://www.derczynski.com/papers/archive/BPE_Gage.pdf\n[46] T. Mikolov, K. Chen, G. Corrado, J. Dean, Eﬃcient estimation of word repre-\nsentations in vector space, in: Workshop Proceedings International Conference\non Learning Representations, 2013.\n[47] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed\nrepresentations of words and phrases and their compositionality, in: Advances\nin neural information processing systems, 2013, pp. 3111–3119.\nURL https://proceedings.neurips.cc/paper/2013/file/\n9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n[48] J. Pennington, R. Socher, C. D. Manning, Glove: Global vectors for word rep-\nresentation, in: Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), 2014, pp. 1532–1543.\n[49] B. O. Koopman, Hamiltonian systems and transformation in Hilbert space,\nProceedings of the national academy of sciences of the United States of America\n17 (5) (1931) 315. doi:10.1073/pnas.17.5.315.\n[50] Q. Li, F. Dietrich, E. M. Bollt, I. G. Kevrekidis, Extended dynamic mode decom-\nposition with dictionary learning: A data-driven adaptive spectral decomposi-\ntion of the Koopman operator, Chaos: An Interdisciplinary Journal of Nonlinear\nScience 27 (10) (2017) 103111. arXiv:https://doi.org/10.1063/1.4993854,\ndoi:10.1063/1.4993854.\nURL https://doi.org/10.1063/1.4993854\n[51] M. Korda, I. Mezi´ c, Linear predictors for nonlinear dynamical systems: Koop-\nman operator meets model predictive control, Automatica 93 (2018) 149 – 160.\ndoi:10.1016/j.automatica.2018.03.046.\nURL http://www.sciencedirect.com/science/article/pii/\nS000510981830133X\n[52] M. Korda, M. Putinar, I. Mezi´ c, Data-driven spectral analysis of the Koopman\noperator, Applied and Computational Harmonic Analysis 48 (2) (2020) 599 –\n29\n629. doi:10.1016/j.acha.2018.08.002.\nURL http://www.sciencedirect.com/science/article/pii/\nS1063520318300988\n[53] I. Mezic, On numerical approximations of the Koopman operator, arXiv preprint\narXiv:2009.05883.\n[54] N. Takeishi, Y. Kawahara, T. Yairi, Learning Koopman invariant subspaces\nfor dynamic mode decomposition, in: I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in\nNeural Information Processing Systems, Vol. 30, Curran Associates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper/2017/file/\n3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf\n[55] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear em-\nbeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 1–10.\ndoi:10.1038/s41467-018-07210-0.\nURL https://doi.org/10.1038/s41467-018-07210-0\n[56] S. E. Otto, C. W. Rowley, Linearly recurrent autoencoder networks for learning\ndynamics, SIAM Journal on Applied Dynamical Systems 18 (1) (2019) 558–593.\ndoi:10.1137/18M1177846.\nURL https://doi.org/10.1137/18M1177846\n[57] S. L. Brunton, M. Budiˇ si´ c, E. Kaiser, J. N. Kutz, Modern Koopman theory for\ndynamical systems, arXiv preprint arXiv:2102.12086.\n[58] Y. Li, H. He, J. Wu, D. Katabi, A. Torralba, Learning compositional Koopman\noperators for model-based control, in: International Conference on Learning\nRepresentations, 2020.\nURL https://openreview.net/forum?id=H1ldzA4tPr\n[59] O. Melamud, J. Goldberger, I. Dagan, context2vec: Learning generic context\nembedding with bidirectional LSTM, in: Proceedings of the 20th SIGNLL con-\nference on computational natural language learning, 2016, pp. 51–61.\nURL https://aclanthology.org/K16-1006.pdf\n[60] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettle-\nmoyer, Deep contextualized word representations, in: Proceedings of the 2018\n30\nConference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long Pa-\npers), Association for Computational Linguistics, 2018, pp. 2227–2237. doi:\n10.18653/v1/N18-1202.\nURL https://aclanthology.org/N18-1202\n[61] J. Zhao, F. Deng, Y. Cai, J. Chen, Long short-term memory - fully connected\n(LSTM-FC) neural network for PM2.5 concentration prediction, Chemosphere\n220 (2019) 486–492. doi:10.1016/j.chemosphere.2018.12.128.\nURL https://www.sciencedirect.com/science/article/pii/\nS0045653518324639\n[62] A. Chattopadhyay, P. Hassanzadeh, D. Subramanian, K. Palem, Data-driven\nprediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep\nlearning methods: Reservoir computing, ANN, and RNN-LSTM, Nonlinear Pro-\ncesses in Geophysics 27 (2020) 373–389. doi:10.5194/npg-27-373-2020.\nURL https://doi.org/10.5194/npg-27-373-2020\n[63] M. Lukoˇ seviˇ cius, A practical guide to applying echo state networks, in: Neural\nnetworks: Tricks of the trade, Springer, 2012, pp. 659–686. doi:10.1007/\n978-3-642-35289-8_36 .\nURL https://doi.org/10.1007/978-3-642-35289-8_36\n[64] S. Lee, D. You, Prediction of laminar vortex shedding over a cylinder using deep\nlearning, arXiv preprint arXiv:1712.07854.\n[65] J. Morton, A. Jameson, M. J. Kochenderfer, F. Witherden, Deep dynamical\nmodeling and control of unsteady ﬂuid ﬂows, in: S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Garnett (Eds.), Advances in\nNeural Information Processing Systems, Vol. 31, Curran Associates, Inc., 2018.\nURL https://proceedings.neurips.cc/paper/2018/file/\n2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf\n[66] R. Han, Y. Wang, Y. Zhang, G. Chen, A novel spatial-temporal prediction\nmethod for unsteady wake ﬂows based on hybrid deep neural network, Physics\nof Fluids 31 (12) (2019) 127101. doi:10.1063/1.5127247.\nURL https://doi.org/10.1063/1.5127247\n[67] J. Xu, K. Duraisamy, Multi-level convolutional autoencoder networks\nfor parametric prediction of spatio-temporal dynamics, Computer\nMethods in Applied Mechanics and Engineering 372 (2020) 113379.\n31\ndoi:10.1016/j.cma.2020.113379.\nURL https://www.sciencedirect.com/science/article/pii/\nS0045782520305648\n[68] H. Jasak, A. Jemcov, Z. Tukovic, et al., Openfoam: A C++ library for complex\nphysics simulations, in: International workshop on coupled methods in numeri-\ncal dynamics, Vol. 1000, IUC Dubrovnik Croatia, 2007, pp. 1–20.\n[69] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-k. Wong, W.-c. WOO, Convolu-\ntional lstm network: A machine learning approach for precipitation nowcasting,\nin: C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances\nin Neural Information Processing Systems, Vol. 28, Curran Associates, Inc.,\n2015.\nURL https://proceedings.neurips.cc/paper/2015/file/\n07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf\n[70] S. Wiewel, M. Becher, N. Thuerey, Latent space physics: Towards learning the\ntemporal evolution of ﬂuid ﬂow, Computer Graphics Forum 38 (2) (2019) 71–82.\ndoi:10.1111/cgf.13620.\nURL https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13620\n[71] R. Maulik, B. Lusch, P. Balaprakash, Reduced-order modeling of advection-\ndominated systems with recurrent neural networks and convolutional autoen-\ncoders, Physics of Fluids 33 (3) (2021) 037106. arXiv:https://doi.org/10.\n1063/5.0039986, doi:10.1063/5.0039986.\nURL https://doi.org/10.1063/5.0039986\n[72] J. E. Pearson, Complex patterns in a simple system, Science 261 (5118) (1993)\n189–192. doi:10.1126/science.261.5118.189.\nURL https://science.sciencemag.org/content/261/5118/189\n[73] K. J. Lee, W. D. McCormick, Q. Ouyang, H. L. Swinney, Pattern formation by\ninteracting chemical fronts, Science 261 (5118) (1993) 192–194. doi:10.1126/\nscience.261.5118.192.\nURL https://science.sciencemag.org/content/261/5118/192\n32\nAppendix A. Lorenz Supplementary Results\nPredictions of the tested models for three test cases are plotted in Fig. A.15. All\nmachine learning methods are accurate for the ﬁrst several time-steps, but begin\nto quickly deviate from the numerical solution in subsequent time-steps. The trans-\nformer model is consistently accurate within the plotted time frame, typically outper-\nforming alternative methods at later time-steps. The predictions of the transformer\nmodel are also compared to the solution from a less accurate Euler time-integration\nmethod in Fig. A.16. Due to the numerical diﬀerences between the Runge-Kutta\nground truth and the Euler methods, the solutions are quick to deviate and the\ntransformer clearly outperforms the Euler numerical simulator. Lastly, in Fig. A.17\nwe illustrate the two diﬀerent noise levels used to contaminate the training data for\nthe results listed in Table 2.\nFigure A.15: Three Lorenz test case predictions using each tested model for 128 time-steps.\nFigure A.16: Three Lorenz test solutions solved using Runge-Kutta and Euler time-integration\nmethods with the prediction of the transformer for 256 time-steps.\n33\nFigure A.17: Comparison between the clean and contaminated (noisy) training data.\nAppendix B. Cylinder Supplementary Results\nPrediction ﬁelds of all the tested models for a single test case are plotted in Figs. B.18\nand B.19. While all models are able to perform adequately with qualitatively good\nresults, the transformer model with Koopman embeddings (Transformer-KM) out-\nperforms alternatives. Additionally, the training and validation errors during training\nare plotted in Fig. B.20 where all models have similar training and validation error\nindicating minimal overﬁtting.\nThe evolution of the ﬂow ﬁeld projected onto the dominant eigenvectors of the\nlearned Koopman operator for two Reynolds numbers is plotted in Fig. B.21. This\nreﬂects the dynamical modes that were learned by the embedding model to impose\nphysical “context” on the embeddings. Given that the eigenvectors are complex, we\nplot both the magnitude, |ψ|, and angle, ∠ψ. For both Reynolds numbers, it is easy\nto see the initial transition region, t <100, before the system enters the periodic\nvortex shedding. Once in the periodic region, we can see that the higher Reynolds\nnumber has a higher frequency which reﬂects the increased vortex shedding speed.\nAppendix C. Gray-Scott Supplementary Results\nVolume plots of two test cases for both species are provided in Figs. C.22 & C.23. In\naddition to better visualize the accuracy of the trained transformer model, contour\nplots for three test cases are also provided in Fig. C.24. The transformer model is\nable to reliably predict the earlier time-steps with reasonable accuracy. As the system\ncontinues to react, the reaction fronts begin to interact resulting in the well-known\ncomplex structures of the Gray-Scott system [72, 73]. During these later times, the\n34\nFigure B.18: Velocity magnitude predictions of a test case at Re = 633 using the transformer with\nKoopman (KM), auto-encoder (AE) and PCA embedding methods, the convolutional LSTM model\n(ConvLSTM) and fully-connected LSTM with Koopman embeddings (LSTM-KM).\ntransformer begins to degrade in accuracy yet does maintain notable consistency\nwith the true solution.\n35\nFigure B.19: Pressure predictions of a test case at Re = 633 using the transformer with Koopman\n(KM), auto-encoder (AE) and PCA embedding methods, the convolutional LSTM model (ConvL-\nSTM) and fully-connected LSTM with Koopman embeddings (LSTM-KM).\nFigure B.20: The training and validation mean square error (MSE) of the transformer with Koop-\nman (KM), auto-encoder (AE) and PCA embedding methods, the convolutional LSTM model\n(ConvLSTM) and fully-connected LSTM with Koopman embeddings (LSTM-KM) during training.\n36\n(a) Re= 233\n(b) Re= 633\nFigure B.21: The dynamics of the ﬂuid ﬂow around a cylinder projected onto the 8 most dominant\neigenvectors of the learned Koopman operator, K, in the embedding model.\n37\n(a) u target (top) and transformer prediction (bottom)\n(b) v target (top) and transformer prediction (bottom)\nFigure C.22: Test case volume plots for the Gray-Scott system. Isosurfaces displayed span the\nrange u, v= [0.3, 0.5] to show the inner structure.\n(a) u target (top) and transformer prediction (bottom)\n(b) v target (top) and transformer prediction (bottom)\nFigure C.23: Test case volume plots for the Gray-Scott system. Isosurfaces displayed span the\nrange u, v= [0.3, 0.5] to show the inner structure.\n38\nFigure C.24: x −y plane contour plots of three Gray-Scott test cases sliced at z = 16.\n39"
}