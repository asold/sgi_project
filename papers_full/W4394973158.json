{
  "title": "Behavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommendation Systems",
  "url": "https://openalex.org/W4394973158",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5102222362",
      "name": "Dayu Yang",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A5046938994",
      "name": "F.L. Chen",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A5101716412",
      "name": "Hui Fang",
      "affiliations": [
        "University of Delaware"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386728933",
    "https://openalex.org/W2970236742",
    "https://openalex.org/W2944069152",
    "https://openalex.org/W3185784178",
    "https://openalex.org/W4392846385",
    "https://openalex.org/W4285091076",
    "https://openalex.org/W4384890810",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4225886166",
    "https://openalex.org/W4283324387",
    "https://openalex.org/W4386729449",
    "https://openalex.org/W3035734059",
    "https://openalex.org/W3080122044",
    "https://openalex.org/W3102862020",
    "https://openalex.org/W3104405162"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated great potential in\\nConversational Recommender Systems (CRS). However, the application of LLMs to\\nCRS has exposed a notable discrepancy in behavior between LLM-based CRS and\\nhuman recommenders: LLMs often appear inflexible and passive, frequently\\nrushing to complete the recommendation task without sufficient inquiry.This\\nbehavior discrepancy can lead to decreased accuracy in recommendations and\\nlower user satisfaction. Despite its importance, existing studies in CRS lack a\\nstudy about how to measure such behavior discrepancy. To fill this gap, we\\npropose Behavior Alignment, a new evaluation metric to measure how well the\\nrecommendation strategies made by a LLM-based CRS are consistent with human\\nrecommenders'. Our experiment results show that the new metric is better\\naligned with human preferences and can better differentiate how systems perform\\nthan existing evaluation metrics. As Behavior Alignment requires explicit and\\ncostly human annotations on the recommendation strategies, we also propose a\\nclassification-based method to implicitly measure the Behavior Alignment based\\non the responses. The evaluation results confirm the robustness of the method.\\n",
  "full_text": "Behavior Alignment: A New Perspective of Evaluating LLM-based\nConversational Recommender Systems\nDayu Yang\nUniversity of Delaware\nNewark, DE, USA\ndayu@udel.edu\nFumian Chen\nUniversity of Delaware\nNewark, DE, USA\nfmchen@udel.edu\nHui Fang\nUniversity of Delaware\nNewark, DE, USA\nhfang@udel.edu\nABSTRACT\nLarge Language Models (LLMs) have demonstrated great poten-\ntial in Conversational Recommender Systems (CRS). However, the\napplication of LLMs to CRS has exposed a notable discrepancy\nin behavior between LLM-based CRS and human recommenders:\nLLMs often appear inflexible and passive, frequently rushing to\ncomplete the recommendation task without sufficient inquiry. This\nbehavior discrepancy can lead to decreased accuracy in recommen-\ndations and lower user satisfaction. Despite its importance, existing\nstudies in CRS lack a study about how to measure such behavior\ndiscrepancy. To fill this gap, we propose Behavior Alignment , a\nnew evaluation metric to measure how well the recommendation\nstrategies made by a LLM-based CRS are consistent with human\nrecommenders‚Äô. Our experiment results show that the new metric is\nbetter aligned with human preferences and can better differentiate\nhow systems perform than existing evaluation metrics. As Behavior\nAlignment requires explicit and costly human annotations on the\nrecommendation strategies, we also propose a classification-based\nmethod to implicitly measure the Behavior Alignment based on\nthe responses. The evaluation results confirm the robustness of the\nmethod.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚Üí Users and interactive retrieval ; ‚Ä¢\nHuman-centered computing ‚Üí HCI design and evaluation\nmethods.\nKEYWORDS\nRecommender Systems, Conversational Systems, Evaluation Metric.\nACM Reference Format:\nDayu Yang, Fumian Chen, and Hui Fang. 2024. Behavior Alignment: A\nNew Perspective of Evaluating LLM-based Conversational Recommender\nSystems. In Proceedings of the 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR ‚Äô24), July 14‚Äì18,\n2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https:\n//doi.org/10.1145/3626772.3657924\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657924\n1 INTRODUCTION\nRecent advancements have highlighted the potential of large lan-\nguage models (LLMs) across a range of tasks. Their impressive\nrecommendation performance [16, 8, 22] and language generation\ncapability [20] have attracted the attention of researchers in the\nconversational recommender system (CRS) community [4, 13, 21,\n7]. For example, Google Research team [4] recently proposed an\nLLM-based CRS that achieved huge success on YouTube. LLMs are\ninherently well-suited to the needs of CRS. Unlike traditional recom-\nmender systems, which depend on user profiles and past activities,\nCRS prioritizes the identification of user preferences through real-\ntime interactions [17, 21, 4], striving to mimic the interactions of\nhuman recommenders [6, 12]. These characteristics require strong\ncapabilities in language understanding and generation [12, 25], ar-\neas where LLMs excel. Consequently, there is an increasing trend\nof adopting LLMs within the CRS domain.\nHowever, existing studies also found a significant weakness of\nLLMs when implemented into conversational recommendation [21,\n13]: the behavior of LLMs lacks proactiveness and adaptivity through-\nout dialogues, therefore leads to the shortage of information for\nunderstanding the users‚Äô preference [13].\nTo validate these findings, we conducted a comparative analysis\ninvolving two popular LLMs and a human recommender across 20\nrandomly selected datapoints from INSPIRED dataset [6], the only\npopular conversational recommendation dataset providing behav-\nior labels. This comparison revealed a noticeable difference in the\nbehavior of LLM-based CRS and human recommenders. Specifically,\nLLM-based CRS systems tend to be passive and inflexible, often\nrushing to make recommendations without conducting any inquiry.\nIn contrast, human recommenders display much greater patience,\ndynamism, and adaptability. They show a wider range of complex\ninformation-seeking strategies contributing to recommendations,\nas illustrated in Figure 1. A more tangible illustration is that human\nrecommenders averagely conduct 2.5 conversational turns before\nmaking their first recommendation, a number far exceeds those\nby GPT3.5[14] or Llama2[18], as shown in Table 1. The user infor-\nmation obtained from initial inquiries helps human recommenders\nmake better recommendations and achieve higher success rates.\nRecently, many studies on LLMs have focused on improving their\nbehavior alignment with humans [14, 10, 1]. The alignment between\nthe generated sentences from CRS and human recommenders is\nalso desirable for LLM-based CRS. First, better behavior alignment\ncan enhance the user experience by creating a more efficient and\nproactively interactive conversation. More importantly, the ability\nto use complex recommendation strategies can help the system re-\nceive more user preference information and better understand user\nprofiles, leading to more accurate recommendations [6]. Despite\narXiv:2404.11773v2  [cs.IR]  17 Oct 2024\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA Dayu Yang, Fumian Chen, & Hui Fang\nFigure 1: Comparing the strategies that LLM typically uses\nand human recommenders use for making conversational\nrecommendations.\n#Turns before Rec Success Rate\nLLM-based CRS\nGPT 3.5 1.158 15.8%\nLlama 2 1.000 5.3%\nReference\nHuman 2.500 57.1%\nTable 1: Comparing the behavior of LLM-based CRSs and hu-\nman‚Äôs behavior including: average length of conversational\nturns before making the first recommendation(#Turns be-\nfore Rec), average success rate.\nthe importance of measuring behavior alignment, the evaluation\nmetrics currently used in CRS have no ability to measure this.\nTo fill this gap, we propose a new evaluation metric: Behavior\nAlignment, a metric that explicitly evaluates how well LLM-based\nCRS‚Äôs recommendation strategies are aligned with humans. We\nalso conduct experiments to demonstrate the effectiveness of the\nmeasure. In particular, Behavior Alignment has high agreement\nwith the human preferences, and can differentiate the performance\nof different LLM-based CRS systems much better than the exist-\ning metrics. However, one limitation of Behavior Alignment is\nthe requirement of having human annotations of recommendation\nstrategies, which can be costly and time-consuming to obtain. To\novercome this limitation, we propose a classification-based method\nto implicitly estimate the behavior alignment without the annota-\ntions of recommendation strategies. Experiment results confirm\nthe robustness of the proposed implicit estimation of the Behavior\nAlignment measure across multiple CRS datasets.\n2 RELATED WORK\nExisting research practices primarily evaluate CRS‚Äôs performance in\ntwo aspects: recommendation accuracy, and the quality of sentence\ngeneration. In terms of recommendation accuracy, most CRS studies\nadopt metrics from traditional recommender systems, like hit rates,\nprecision, and recall, from IR community [24, 5].\nTo evaluate the quality of generated sentences, automatic metrics\nborrowed from dialogue systems, such as Perplexity, BLEU, and\nDIST, are used [2, 26, 24, 19]. However, their alignment withquality\nand user satisfaction in the context of CRS is questioned [9, 23].\nUnder such questions, several manual evaluation metrics are\nproposed. Chen et al. [2] advocated for Consistency. Zhou et al. [25]\nproposed fluency and informativeness as substitutes for BLEU. How-\never, with the introduction of LLM for building CRS [7, 11, 21, 4],\nfluency and informativeness have lost their relevance as LLM can\ngenerate highly fluent sentences and the informativeness can be\nconveniently adjusted by prompting. In addition, manual evalu-\nation‚Äôs inherent subjectivity and cost can pose a challenge. Con-\nsequently, recent works in LLM-based CRS [7, 11] opted to only\nevaluate CRS based on accuracy metrics, and overlooked the quality\nof generated sentences.\n3 BEHAVIOR ALIGNMENT: THE METRIC\nUsers tend to find the conversations more natural and engaging\nwith human recommenders. And those conversations often lead\nto better recommendations compared with automatic conversa-\ntional recommender systems [6]. This is largely because human\nrecommenders usually deploy various sociable and preference elic-\nitation strategies to grab more personal information of users [6].\nIn addition, we and existing studies [13, 21] consistently observed\na substantial disparity between human‚Äôs strategies and those em-\nployed by LLM-based CRS where latter often appears naive and\npassive. Therefore, human recommenders‚Äô behaviors could become\nvaluable references for the LLM-based CRS.\n3.1 Definition\nBased on the assumption that human recommenders‚Äô behaviors\ncan perform as good references for CRS, given a context (e.g., user‚Äôs\npast interactions with a CRS), a CRS generate a response ùê∂. And\na human recommender writes a response ùêª for the same context.\nAssuming we know the recommendation strategy of ùê∂ is ùëÖùê∂ and\nthe recommendation strategy of ùêª is ùëÖùêª, the behavior alignment\nscore of the pair (i.e., ùêµùê¥(ùê∂,ùêª)) can be computed as in Equation\n(1). In a test collection with ùëÅ generated responses, the system‚Äôs\nBehavior Alignment score can be calculated using Equation (2).\nùêµùê¥(ùê∂,ùêª) =\n(\n1 ùëñùëì ùëÖùê∂ = ùëÖùêª\n0 ùëñùëì ùëÖùê∂ ‚â† ùëÖùêª\n(1)\nùêµùëí‚Ñéùëéùë£ùëñùëúùëü ùê¥ùëôùëñùëîùëõùëöùëíùëõùë°=\n√ç\nùëò=2..ùëÅ ùêµùê¥(ùê∂ùëò,ùêªùëò)\nùëÅ (2)\nIn Equation (2), the first interactive turn is not counted into\nBehavior Alignment since the start point of every conversation can\nbe random in practice. After the first turn is finished, the behaviors\nof following turns should conditioned on the existing context.\nFor recommendation strategy ùëÖ, existing CRS studies [6] have\nprovided a comprehensive categorization that includes 13 mutu-\nally exclusive types. These types are: acknowledgment, redibility,\nencouragement, experience inquiry , offer help , opinion inquiry , per-\nsonal experience , personal opinion , preference confirmation, rephrase\npreference, self modeling , similarity, and transparency.\n4 BEHAVIOR ALIGNMENT: EFFECTIVENESS\nAS AN EVALUATION METRIC\nBehavior Alignment can measure the behavior differences between\ngenerated sentences from CRS and human recommenders, but it re-\nmains unclear whether such a metric is more desirable than existing\nmetrics. To answer this question, we design two sets of experiments\nto evaluate (1) whether Behavior Alignment reflects user preferences ;\nand (2) whether it can differentiate well-performed systems from those\npoorly performed ones .\nBehavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommender Systems SIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA\nFigure 2: Agreement with human judgments over \"Is the\nresponse from Falcon-based CRS better than Llama2-based\nCRS ?\", measured by Cohen‚Äôs Kappa.\n4.1 Agreement with Human Preferences\nThe first set of experiments aims to assess whether the evaluation\nresults of Behavior Alignment are closely consistent with human\npreferences. Specifically, responses from two CRS systems are eval-\nuated by Behavior Alignment and humans. A strong agreement\nbetween them would reflect the effectiveness of a metric.\nWe randomly sampled 1,000 instances from INSPIRED dataset.\nThe two CRS systems are built on two LLMs respectively: Falcon-\n7B [15] and Llama2-7B [18]. They were specifically chosen due\nto the distinct training data they utilize, which leads to varying\nresponse styles in reaction to dialog histories. To gauge human\npreferences, we enlisted two annotators who have experience in\nconversational recommendations. They assumed the role of CRS\nusers and annotated preferences on the responses generated by the\ntwo LLM-based CRS systems (i.e., same, A is better than B, or B is\nbetter than A). In addition to preferences between the two systems,\nwe ask an expert trained with linguistic backgrounds to annotate\nthe recommendation strategies for each generated response, which\nis necessary for computing Behavior Alignment.\nAn ideal evaluation metric should yield ratings that highly cor-\nrelate to human preferences [3]. We use Cohen‚Äôs Kappa to measure\nsuch correlation. In addition to Cohen‚Äôs Kappa, we employed the\nBootstrap method to establish the 2.5% to 97.5% confidence interval.\nTo compare how various metrics reveal user preferences, we\nconduct a comparative analysis with two popular automatic metrics\nthat existing CRS studies used to evaluate generation quality [2, 25,\n24]: BLEU@K and DIST@K. The results are depicted in Figure 2.\nWe can see Behavior Alignment demonstrates a considerable\nlevel of agreement with human preference, as evidenced by a Co-\nhen‚Äôs Kappa of 0.74, a value typically interpreted as indicating\n\"Substantial Agreement\". On the contrary, DIST@K and BLEU@K\nexhibit a much lower level of agreement with human preferences.\nOne potential reason for the poor alignment is they are word-level\nmetrics that are directly adopted from Machine Translation and\nSummarization tasks, which do not fit the context and goal of the\nconversational recommendation task very well [3].\n4.2 Differentiation of System Performance\nA desirable evaluation metric should be able to differentiate various\nsystems [3]. Also, the increase/decrease of rating should be consis-\ntent with the increase/decrease of human preference for different\nsystems. However, it is hard to quantify human preference for each\nLLM-based CRS and it is almost impossible to find large numbers\nFigure 3: Metric score changes (y-axis) along with human\npreference changes (x-axis)\nof LLMs covering different levels of human preferences to meet the\nnecessity for conducting evaluations.\nTo address these challenges, we build synthetic LLM-based CRS\nsystems with different levels of user preferences by mixing dif-\nferent ratios of \"good generations\" and \"bad generations\" based\non the annotated sentences generated from two LLM-based CRS\nwe implemented. Specifically, 100 generation pairs were randomly\nchosen from annotated data, with one system‚Äôs response identified\nas superior to the other. We selected the responses preferred by\nhumans to create a hypothetical \"ideal\" system, while the responses\nthat were less preferred were selected to build a \"worst-case\" sys-\ntem. Following this, we crafted synthetic CRS systems of varying\nquality by blending 90%, 80%, ..., 10% of the \"ideal\" system with\n10%, 20%, ..., 90% of the \"worst-case\" system. After the synthesis, we\ncompute each metric including Behavior Alignment to see if they\ncan effectively differentiate between different synthetic systems.\nAs displayed in Figure 3, the x-axis signifies the proportion of\nsamples taken from the \"ideal\" system, which can be interpreted as a\n\"human preference score\". The results show that there is a consistent\nincrease in Behavior Alignment which ranges from 0.11 to 0.88 as\nwe shift from a \"worst-case\" system to an \"ideal\" system. This rise\nis directly proportional to each incremental inclusion of human-\npreferred samples. However, the other two metrics, BLEU@K and\nDIST@K, exhibited minimal variability despite the enhancement\nin human-preferred samples.\n5 ESTIMATION OF IMPLICIT BEHAVIOR\nALIGNMENT\nAlthough the proposed Behavior Alignment is effective, it requires\nreal-time annotations of behavior types, which is not readily avail-\nable. To address this issue, we propose a classification-based method\nto estimate the implicit behavior alignment. Instead of directly pre-\ndicting the behavior categories and then comparing whether the\nbehaviors of two responses are the same, the method focuses on a\nbinary problem: predicting whether the two responses belong to\nthe same category, which would not require the explicit behavior\nlabels anymore1.\n5.1 Methodology\nGiven a human-generated response ùêª and a CRS-generated re-\nsponse ùê∂, we propose estimating the alignment through a binary\nclassifier, which takes ùêª and ùê∂ as input and predicts whether they\n1The source code and the experiments can be found at https://github.com/dayuyang1\n999/Behavior-Alignment\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA Dayu Yang, Fumian Chen, & Hui Fang\nBehavior Type Accuracy 1st Misclassification 2nd Misclassification\npersonal experience 0.60 credibility similarity\nrephrase preference 0.45 preference confirmation personal opinion\nself modeling 0.31 personal experience similarity\nsimilarity 0.53 acknowledgment self modeling\ntransparency 0.65 opinion inquiry offer help\nTable 2: Top 1st and 2nd misclassification for those Behavior classes having accuracy lower than 0.7\nAccuracy on hold-out test set\nOriginal Mixed-hard\nFold0 0.960 0.976\nFold1 0.958 0.978\nFold2 0.958 0.977\nFold3 0.940 0.976\nFold4 0.967 0.973\nAveraged 0.957 0.976\nTable 3: Cross-validation results\ncorrespond to the same category or not. The binary classifier is\nbuilt based on BERT2 and fine-tuned on 100,000 human response\npairs selected from INSPIRED dataset. How to select the pairs? We\nexplore two strategies to construct the training/testing data.\nThe first training strategy is straightforward. We created 50,000\nnegative sentence pairs with different behavior types and another\n50,000 positive pairs with the same behavior type. For each sentence\npair, they are concatenated by a special token [SEP], which allows\nBERT to identify the segment of two sentences. This strategy is\nreferred to as \"Original\".\nThe second strategy is to intentionally include more difficult\nexamples in the training data to improve the robustness of the clas-\nsifier. To achieve this, we first train a multi-class classifier to predict\nthe exact behavior type of each sentence. The prediction results\nare compared with the ground truth labels in order to compute\nthe misclassification rate. Specifically, we define \"hard negatives\"\nas negative pairs that a sentence having a prediction accuracy of\nless than 0.7 and another sentence is from its most misclassified\ncategory as shown in Table 2. Consequently, we created 10,000 hard\nnegative samples across five categories. We then randomly replaced\n10,000 negative examples in the original training data with hard\nnegatives to build the \"mixed-hard\" training set.\n5.2 Evaluation\nThe robustness of a binary classifier is important, as it directly\ninfluences the efficacy and applicability of the proposed metric.\nTo assess the robustness, we employ three methodologies: (1) im-\nplementing cross-validation to ensure consistently high accuracy;\n(2) determining its Cohen‚Äôs Kappa in relation to human annota-\ntions to verify an invariably strong agreement; and (3) evaluating\nits performance on out-of-distribution data to ascertain minimal\nperformance degradation.\nFirst, we employ cross-validation on the binary sentence pairs.\nAs observed in Table 3, the accuracy remains consistently high\nregardless of which strategy is used for constructing training data.\nSecond, we utilized Cohen‚Äôs Kappa to quantify the agreement\nbetween the classifier‚Äôs predictions and human annotations. As\nillustrated in Table 4, our results depict that Cohen‚Äôs Kappa exceeds\n2https://huggingface.co/bert-large-uncased\nAccuracy Cohen‚Äôs kappa\nOriginal 0.957 0.913\nMixed-hard 0.976 0.952\nTable 4: The agreement between predictions and human an-\nnotations on testing samples\nAccuracy Cohen‚Äôs kappa\nOriginal 0.782 0.563\nMixed-hard 0.932 0.865\nTable 5: The agreement between predictions and human\nannotations on out-of-distribution instances from ReDial\ndataset\n0.9 on the testing set, denoting an almost impeccable alignment\nbetween the classifier‚Äôs predictions and human annotations.\nLastly, to verify the generalizability of the binary classifier, we\nselected another widely-used CRS dataset that the model has not\nseen during training, ReDial [12] for testing. Compared with IN-\nSPIRED dataset, ReDial dataset emphasizes more on making rec-\nommendations during chi-chat, rendering its linguistic style more\ncolloquial. The out-of-distribution samples that have differences\non using of language may introduce challenges to the classifier.\nGiven the absence of behavior annotation in the ReDial dataset,\nwe first instructed two annotators to annotate ReDial in a man-\nner similar to INSPIRED, resulting in 42006 sentence pairs with\nground-truth binary labels. As Table 5 showcases, the binary classi-\nfier trained on \"Original\" sentence pairs witnessed a marked decline\nin performance. Conversely, the classifier trained on sentence pairs\naugmented with hard negatives demonstrated superior generaliz-\nability, achieving over 93% accuracy and a Cohen‚Äôs kappa of 0.86\nwith humans. This generally indicates a near-perfect agreement\nbetween the automatic annotator and human recommenders. The\nfindings emphasize the importance of hard negatives, particularly\nfor a dataset-agnostic evaluation metric.\n6 CONCLUSION\nEvaluating the generated sentences is crucial for CRS as they sig-\nnificantly influence user responses. This, in turn, impacts both\ninformation exposure and user profiling. In this paper, we intro-\nduce a novel evaluation metric designed to assess the quality of\nresponses generated by LLM-based CRSs. Our experiments demon-\nstrate this metric‚Äôs effectiveness, particularly in its alignment with\nhuman preferences and its capacity to distinguish LLM-based CRSs\nby their ability to implement complex recommendation strategies.\n7 ACKNOWLEDGEMENTS\nThis research is supported by the graduate fellowship from the Insti-\ntute for Financial Services Analytics at the University of Delaware.\nWe would also like to express our sincere gratitude to the reviewers\nfor their insightful comments and suggestions.\nBehavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommender Systems SIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA\nREFERENCES\n[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. Tallrec: an effective and efficient tuning framework to align large\nlanguage model with recommendation. arXiv preprint arXiv:2305.00447 .\n[2] Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia\nYang, and Jie Tang. 2019. Towards knowledge-based recommender dialog\nsystem. arXiv preprint arXiv:1908.05391 .\n[3] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset,\nEneko Agirre, and Mark Cieliebak. 2021. Survey on evaluation methods for\ndialogue systems. Artificial Intelligence Review , 54, 755‚Äì810.\n[4] Luke Friedman et al. 2023. Leveraging large language models in conversational\nrecommender systems. arXiv preprint arXiv:2305.07961 .\n[5] Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, and Tat-Seng\nChua. 2021. Advances and challenges in conversational recommender systems:\na survey. AI open , 2, 100‚Äì126.\n[6] Shirley Anugrah Hayati, Dongyeop Kang, Qingxiaoyang Zhu, Weiyan Shi,\nand Zhou Yu. 2020. Inspired: toward sociable recommendation dialog systems.\narXiv preprint arXiv:2009.14306 .\n[7] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng,\nBodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large\nlanguage models as zero-shot conversational recommenders. arXiv preprint\narXiv:2308.10053.\n[8] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\nand Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for\nrecommender systems. arXiv preprint arXiv:2305.08845 .\n[9] Dietmar Jannach. 2023. Evaluating conversational recommender systems: a\nlandscape of research. Artificial Intelligence Review , 56, 3, 2365‚Äì2400.\n[10] Andreas K√∂pf et al. 2023. Openassistant conversations‚Äìdemocratizing large\nlanguage model alignment. arXiv preprint arXiv:2304.07327 .\n[11] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large language models\nfor generative recommendation: a survey and visionary discussions. arXiv\npreprint arXiv:2309.01157 .\n[12] Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Lau-\nrent Charlin, and Chris Pal. 2018. Towards deep conversational recommenda-\ntions. Advances in neural information processing systems , 31.\n[13] Lizi Liao, Grace Hui Yang, and Chirag Shah. 2023. Proactive conversational\nagents in the post-chatgpt world. In Proceedings of the 46th International ACM\nSIGIR Conference on Research and Development in Information Retrieval , 3452‚Äì\n3455.\n[14] Long Ouyang et al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Processing Systems , 35,\n27730‚Äì27744.\n[15] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only.arXiv preprint arXiv:2306.01116.\n[16] Damien Sileo, Wout Vossen, and Robbe Raymaekers. 2022. Zero-shot recom-\nmendation as language modeling. In European Conference on Information Re-\ntrieval. Springer, 223‚Äì230.\n[17] Yueming Sun and Yi Zhang. 2018. Conversational recommender system. InThe\n41st international acm sigir conference on research & development in information\nretrieval, 235‚Äì244.\n[18] Hugo Touvron et al. 2023. Llama 2: open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 .\n[19] Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022. Towards\nunified conversational recommender systems via knowledge-enhanced prompt\nlearning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining , 1929‚Äì1937.\n[20] Jason Wei et al. 2022. Emergent abilities of large language models.arXiv preprint\narXiv:2206.07682.\n[21] Gangyi Zhang. 2023. User-centric conversational recommendation: adapting\nthe need of user with large language models. In Proceedings of the 17th ACM\nConference on Recommender Systems , 1349‚Äì1354.\n[22] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-\nRong Wen. 2023. Recommendation as instruction following: a large language\nmodel empowered recommendation approach. arXiv preprint arXiv:2305.07001 .\n[23] Shuo Zhang and Krisztian Balog. 2020. Evaluating conversational recommender\nsystems via user simulation. In Proceedings of the 26th acm sigkdd international\nconference on knowledge discovery & data mining , 1512‚Äì1520.\n[24] Kun Zhou, Xiaolei Wang, Yuanhang Zhou, Chenzhan Shang, Yuan Cheng,\nWayne Xin Zhao, Yaliang Li, and Ji-Rong Wen. 2021. Crslab: an open-source\ntoolkit for building conversational recommender system.arXiv preprint arXiv:2101.00939.\n[25] Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen,\nand Jingsong Yu. 2020. Improving conversational recommender systems via\nknowledge graph based semantic fusion. InProceedings of the 26th ACM SIGKDD\ninternational conference on knowledge discovery & data mining , 1006‚Äì1014.\n[26] Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang, and Ji-Rong\nWen. 2020. Towards topic-guided conversational recommender system.arXiv\npreprint arXiv:2010.04125 .",
  "topic": "Perspective (graphical)",
  "concepts": [
    {
      "name": "Perspective (graphical)",
      "score": 0.781549870967865
    },
    {
      "name": "Computer science",
      "score": 0.734732985496521
    },
    {
      "name": "Recommender system",
      "score": 0.5668736100196838
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.3970586061477661
    },
    {
      "name": "Artificial intelligence",
      "score": 0.28034117817878723
    },
    {
      "name": "Information retrieval",
      "score": 0.24297523498535156
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86501945",
      "name": "University of Delaware",
      "country": "US"
    }
  ],
  "cited_by": 11
}