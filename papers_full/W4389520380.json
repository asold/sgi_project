{
    "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    "url": "https://openalex.org/W4389520380",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2736090994",
            "name": "Mor Geva",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2250490688",
            "name": "Jasmijn Bastings",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1994677000",
            "name": "Katja Filippova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1484279603",
            "name": "Amir Globerson",
            "affiliations": [
                "Google (United States)",
                "Tel Aviv University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385570354",
        "https://openalex.org/W4385572928",
        "https://openalex.org/W4308023630",
        "https://openalex.org/W574030736",
        "https://openalex.org/W4362598605",
        "https://openalex.org/W4315881234",
        "https://openalex.org/W2034132675",
        "https://openalex.org/W1902674502",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W4386566424",
        "https://openalex.org/W2964159778",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W2972854536",
        "https://openalex.org/W2058602429",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W4385573190",
        "https://openalex.org/W4385572504",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3092292656",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W3097252660",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2970820321",
        "https://openalex.org/W4316135772",
        "https://openalex.org/W4385572952",
        "https://openalex.org/W4294955582",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3152884768",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W2603222250",
        "https://openalex.org/W4386566794",
        "https://openalex.org/W2948771346",
        "https://openalex.org/W4385571791",
        "https://openalex.org/W4306313145",
        "https://openalex.org/W4386566901",
        "https://openalex.org/W4281657280"
    ],
    "abstract": "Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation \"queries\" the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216–12235\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDissecting Recall of Factual Associations in\nAuto-Regressive Language Models\nMor Geva1 Jasmijn Bastings1 Katja Filippova1 Amir Globerson2,3\n1Google DeepMind 2Tel Aviv University 3Google Research\n{pipek, bastings, katjaf, amirg}@google.com\nAbstract\nTransformer-based language models (LMs)\nare known to capture factual knowledge in\ntheir parameters. While previous work looked\ninto where factual associations are stored, only\nlittle is known about how they are retrieved in-\nternally during inference. We investigate this\nquestion through the lens of information ﬂow.\nGiven a subject-relation query, we study how\nthe model aggregates information about the\nsubject and relation to predict the correct at-\ntribute. With interventions on attention edges,\nwe ﬁrst identify two critical points where in-\nformation propagates to the prediction: one\nfrom the relation positions followed by another\nfrom the subject positions. Next, by analyz-\ning the information at these points, we unveil a\nthree-step internal mechanism for attribute ex-\ntraction. First, the representation at the last-\nsubject position goes through an enrichment\nprocess, driven by the early MLP sublayers,\nto encode many subject-related attributes. Sec-\nond, information from the relation propagates\nto the prediction. Third, the prediction repre-\nsentation “queries” the enriched subject to ex-\ntract the attribute. Perhaps surprisingly, this\nextraction is typically done via attention heads,\nwhich often encode subject-attribute mappings\nin their parameters. Overall, our ﬁndings in-\ntroduce a comprehensive view of how factual\nassociations are stored and extracted internally\nin LMs, facilitating future research on knowl-\nedge localization and editing.1\n1 Introduction\nTransformer-based language models (LMs) cap-\nture vast amounts of factual knowledge (Roberts\net al., 2020; Jiang et al., 2020), which they en-\ncode in their parameters and recall during inference\n(Petroni et al., 2019; Cohen et al., 2023). While\nrecent works focused on identifying where factual\n1Our code is publicly available at https:\n//github.com/google-research/google-research/\ntree/master/dissecting_factual_predictions\nApple\nApple\nBeats Music is owned by\nMLP ATTN\n(A) Subject \nenrichment\n(B) Relation \npropagation\n(C) Attribute \nExtraction\nFigure 1: Illustration of our ﬁndings: given subject-\nrelation query, a subject representation is constructed\nvia attributes’ enrichment from MLP sublayers (A),\nwhile the relation propagates to the prediction (B). The\nattribute is then extracted by the MHSA sublayers (C).\nknowledge is encoded in the network (Meng et al.,\n2022a; Dai et al., 2022; Wallat et al., 2020), it\nremains unclear how this knowledge is extracted\nfrom the model parameters during inference.\nIn this work, we investigate this question through\nthe lens of information ﬂow, across layers and input\npositions (Elhage et al., 2021). We focus on a ba-\nsic information extraction setting, where a subject\nand a relation are given in a sentence (e.g. “Beats\nMusic is owned by”), and the next token is the cor-\nresponding attribute (i.e. “Apple”). We restrict\nour analysis to cases where the model predicts the\ncorrect attribute as the next token, and set out to un-\nderstand how internal representations evolve across\nthe layers to produce the output.\nFocusing on modern auto-regressive decoder-\nonly LMs, such an extraction process could be\n12216\nimplemented in many different ways. Informally,\nthe model needs to “merge” the subject and relation\nin order to be able to extract the right attribute, and\nthis merger can be conducted at different layers and\npositions. Moreover, the attribute extraction itself\ncould be performed by either or both of the multi-\nhead self-attention (MHSA) and MLP sublayers.\nTo investigate this, we take a reverse-engineering\napproach, inspired by common genetic analysis\nmethods (Grifﬁths et al., 2005; Tymms and Kola,\n2008) and the recent work by Wang et al. (2022).\nNamely, we artiﬁcially block, or “knock out”, spe-\nciﬁc parts in the computation to observe their im-\nportance during inference. To implement this ap-\nproach in LLMs, we intervene on the MHSA sub-\nlayers by blocking the last position from attending\nto other positions at speciﬁc layers. We identify\ntwo consecutive critical points in the computation,\nwhere representations of the relation and then the\nsubject are incorporated into the last position: ﬁrst\nthe relation and then the subject.\nNext, to identify where attribute extraction oc-\ncurs, we analyze the information that propagates\nat these critical points and the representation con-\nstruction process that precedes them. This is done\nthrough additional interventions to the MHSA and\nMLP sublayers and projections to the vocabulary\n(Dar et al., 2022; Geva et al., 2022b; Nostalgebraist,\n2020). We discover an internal mechanism for at-\ntribute extraction that relies on two key components.\nFirst, a subject enrichment process, through which\nthe model constructs a representation at the last\nsubject-position that encodes many subject-related\nattributes. Moreover, we ﬁnd that out of the three\nsources that build a representation (i.e., the MHSA\nand MLP sublayers and the input token embeddings\n(Mickus et al., 2022)), the early MLP sublayers are\nthe primary source for subject enrichment.\nThe second component is an attribute extraction\noperation carried out by the upper MHSA sublay-\ners. For a successful extraction, these sublayers rely\non information from both the subject representation\nand the last position. Moreover, extraction is per-\nformed by attention heads, and our analysis shows\nthat these heads often encode subject-attribute map-\npings in their parameters. We observed this extrac-\ntion behavior in ∼70% of the predictions.\nOur analysis provides a signiﬁcantly improved\nunderstanding of the way factual predictions are\nformed. The mechanism we uncover can be intu-\nitively described as the following three key steps\n(Fig. 1). First, information about the subject is\nenriched in the last subject token, across early lay-\ners of the model. Second, the relation is passed\nto the last token. Third, the last token uses the re-\nlation to extract the corresponding attribute from\nthe subject representation, and this is done via at-\ntention head parameters. Unlike prior works on\nfactual knowledge representation, which focus on\nmid-layer MLPs as the locus of information (e.g.\nMeng et al. (2022b)), our work highlights the key\nrole of lower MLP sublayers and of the MHSA\nparameters. More generally, we make a substan-\ntial step towards increasing model transparency,\nintroducing new research directions for knowledge\nlocalization and model editing.\n2 Background and Notation\nWe start by providing a detailed description of\nthe transformer inference pass, focusing on auto-\nregressive decoder-only LMs. For brevity, bias\nterms and layer normalization (Ba et al., 2016) are\nomitted, as they are nonessential for our analysis.\nA transformer-based LM ﬁrst converts an input\ntext to a sequencet1,...tN of Ntokens. Each token\nti is then embedded as a vector x0\ni ∈Rd using an\nembedding matrix E ∈R|V|×d, over a vocabulary\nV. The input embeddings are then transformed\nthrough a sequence of Ltransformer layers, each\ncomposed of a multi-head self-attention (MHSA)\nsublayer followed by an MLP sublayer (Vaswani\net al., 2017). Formally, the representation xℓ\ni of\ntoken iat layer ℓis obtained by:\nxℓ\ni = xℓ−1\ni + aℓ\ni + mℓ\ni (1)\nwhere aℓ\ni and mℓ\ni are the outputs from the ℓ-th\nMHSA and MLP sublayers (see below), respec-\ntively. An output probability distribution is ob-\ntained from the ﬁnal layer representations via a\nprediction head δ:\npi = softmax\n(\nδ(xL\ni\n)\n), (2)\nthat projects the representation to the vocabulary\nspace, either through multiplying by the embedding\nmatrix (i.e., δ(xL\ni ) = ExL\ni ) or by using a trained\nlinear layer (i.e., δ(xL\ni ) = WxL\ni + u for W ∈\nR|V|×d,u ∈R|V|).\nMHSA Sublayers The MHSA sublayers com-\npute global updates that aggregate information\nfrom all the representations at the previous layer.\nThe ℓ-th MHSA sublayer is deﬁned using four\n12217\nparameter matrices: three projection matrices\nWℓ\nQ,Wℓ\nK,Wℓ\nV ∈ Rd×d and an output matrix\nWℓ\nO ∈Rd×d. Following Elhage et al. (2021); Dar\net al. (2022), the columns of each projection ma-\ntrix and the rows of the output matrix can be split\ninto H equal parts, corresponding to the number\nof attention heads Wℓ,j\nQ ,Wℓ,j\nK ,Wℓ,j\nV ∈Rd×d\nH and\nWℓ,j\nO ∈R\nd\nH ×d for j ∈[1,H]. This allows describ-\ning the MHSA output as a sum of matrices, each\ninduced by a single attention head:\naℓ\ni =\nH∑\nj=1\nAℓ,j\n(\nXℓ−1Wℓ,j\nV\n)\nWℓ,j\nO (3)\n:=\nH∑\nj=1\nAℓ,j\n(\nXℓ−1Wℓ,j\nV O\n)\n(4)\nAℓ,j = γ\n((\nXℓ−1Wℓ,j\nQ\n)(\nXℓ−1Wℓ,j\nK\n)T\n√\nd/H\n+ Mℓ,j\n)\n(5)\nwhere Xℓ ∈RN×d is a matrix with all token rep-\nresentations at layer ℓ, γ is a row-wise softmax\nnormalization, Aℓ,j ∈RN×N encodes the weights\ncomputed by the j-th attention head at layer ℓ, and\nMℓ,j is a mask for Aℓ,j. In auto-regressive LMs,\nAℓ,j is masked to a lower triangular matrix, as each\nposition can only attend to preceding positions (i.e.\nMℓ,j\nrc = −∞∀c > rand zero otherwise). Impor-\ntantly, the cell Aℓ,j\nrc can viewed as a weighted edge\nfrom the r-th to the c-th hidden representations at\nlayer ℓ−1.\nMLP Sublayers Every MLP sublayer computes\na local update for each representation:\nmℓ\ni = Wℓ\nF σ\n(\nWℓ\nI\n(\naℓ\ni + xℓ−1\ni\n))\n(6)\nwhere Wℓ\nI ∈Rdi×d and Wℓ\nF ∈Rd×di are param-\neter matrices with inner-dimension di, and σis a\nnonlinear activation function. Recent works has\nshown that transformer MLP sublayers can be cast\nas key-value memories (Geva et al., 2021) that store\nfactual knowledge (Dai et al., 2022; Meng et al.,\n2022a).\n3 Experimental Setup\nWe focus on the task of factual open-domain\nquestions, where a model needs to predict an at-\ntribute aof a given subject-relation pair (s,r). A\ntriplet (s,r,a) is typically expressed in a question-\nanswering format (e.g. “What instrument did Elvis\nPresley play?”) or as a ﬁll-in-the-blank query (e.g.\n“Elvis Presley played the ____”). While LMs often\nsucceed at predicting the correct attribute for such\nqueries (Roberts et al., 2020; Petroni et al., 2019), it\nis unknown how attributes are extracted internally.\nFor a factual query q that expresses the sub-\nject s and relation r of a triplet (s,r,a), let t =\n(t1,...,t N ) be the representation of qas a sequence\nof tokens, based on some LM. We refer by the\nsubject tokens to the sub-sequence of t that cor-\nresponds to s, and by the subject positionsto the\npositions of the subject tokens int. The non-subject\ntokens in qexpress the relation r.\nData We use queries from COUNTER FACT\n(Meng et al., 2022a). For a given model, we extract\na random sample of queries for which the model\npredicts the correct attribute. In the rest of the pa-\nper, we refer to the token predicted by the model\nfor a given query qas the attribute a, even though\nit could be a sub-word and thus only the preﬁx of\nthe attribute name (e.g. Wash for “Washington”).\nModels We analyze two auto-regressive decoder-\nonly GPT LMs with different layouts:GPT-2 (Rad-\nford et al., 2019) (L = 48, 1.5B parameters) and\nGPT-J (Wang and Komatsuzaki, 2021) (L = 28,\n6B parameters). Both models use a vocabulary\nwith ∼50K tokens. Also, GPT-J employs parallel\nMHSA and MLP sublayers, where the output of the\nℓ-th MLP sublayer for the i-th representation de-\npends on xℓ−1\ni rather than on aℓ\ni + xℓ−1\ni (see Eq. 6).\nWe follow the procedure by Meng et al. (2022a) to\ncreate a data sample for each model, resulting in\n1,209 queries for GPT-2 and 1,199 for GPT-J.\n4 Overview: Experiments & Findings\nWe start by introducing our attention blocking\nmethod and apply it to identify critical information\nﬂow points in factual predictions (§5) – one from\nthe relation, followed by another from the subject.\nThen, we analyze the evolution of the subject repre-\nsentation in the layers preceding this critical point\n(§6), and ﬁnd that it goes through an enrichment\nprocess driven by the MLP sublayers, to encode\nmany subject-related attributes. Last, we investi-\ngate how and where the right attribute is extracted\nfrom this representation (§7), and discover that this\nis typically done by the upper MHSA sublayers, via\nattention heads that often encode a subject-attribute\nmapping in their parameters.\n12218\n0 10 20 30 40 50\n60\n40\n20\n0\n20\nGPT2-xl\nsubject non-subject last\n0 5 10 15 20 25\n60\n40\n20\n0\nGPT-J\nlayer\n% change in prediction probability\nFigure 2: Relative change in the prediction probability\nwhen intervening on attention edges to the last position,\nfor window sizes of 9 layers in GPT-2 and 5 in GPT-J.\n5 Localizing Information Flow via\nAttention Knockout\nFor a successful attribute prediction, a model\nshould process the input subject and relation such\nthat the attribute can be read from the last position.\nWe investigate how this process is done internally\nby “knocking out” parts of the computation and\nmeasuring the effect on the prediction. To this\nend, we propose a ﬁne-grained intervention on the\nMHSA sublayers, as they are the only module that\ncommunicates information between positions, and\nthus any critical information must be transferred by\nthem. We show that factual predictions are built in\nstages where critical information propagates to the\nprediction at speciﬁc layers during inference.\nMethod: Attention Knockout Intuitively, criti-\ncal attention edges are those that, when blocked,\nresult in severe degradation in prediction quality.\nTherefore, we test whether critical information\npropagates between two hidden representations at a\nspeciﬁc layer, by zeroing-out all the attention edges\nbetween them. Formally, let r,c ∈[1,N] such that\nr≤cbe two positions. We block xℓ\nr from attend-\ning to xℓ\nc at a layer ℓ<L by updating the attention\nweights to that layer (Eq. 5):\nMℓ+1,j\nrc = −∞ ∀j ∈[1,H] (7)\nEffectively, this restricts the source position from\nobtaining information from the target position, at\nthat particular layer. Notably, this is different from\ncausal tracing (Meng et al., 2022a), which checks\nwhat hidden representations restore the original\nprediction when given perturbed input tokens; we\ntest where critical information propagates rather\nthan where it is located during inference.\nExperiment We use Attention Knockout to test\nwhether and, if so, where information from the\nsubject and relation positions directly propagates\nto the last position. Let S,R ⊂[1,N) be the\nsubject and non-subject positions for a given input.\nFor each layer ℓ, we block the attention edges from\nthe last position to each of S, Rand the last (N-\nth) position, for a window of klayers around the\nℓ-th layer, and measure the change in prediction\nprobability. We setk= 9 (5) for GPT-2 (GPT-J ).2\nResults Fig. 2 shows the results. For both GPT-\n2 and GPT-J , blocking attention to the subject to-\nkens (solid green lines) in the middle-upper layers\ncauses a dramatic decrease in the prediction proba-\nbility of up to 60%. This suggests that critical infor-\nmation from the subject positions moves directly\nto the last position at these layers. Moreover, an-\nother substantial decrease of 35%-45% is observed\nfor the non-subject positions (dashed purple lines).\nImportantly, critical information from non-subject\npositions precedes the propagation of critical in-\nformation from the subject positions, a trend we\nobserve for different subject-relation orders (§A.1).\nExample interventions are provided in §H.\nOverall, this shows that there are speciﬁc dis-\njointed stages in the computation with peaks of\ncritical information propagating directly to the pre-\ndiction from different positions. In the next section,\nwe investigate the critical information that propa-\ngates from the subject positions to the prediction.\n6 Intermediate Subject Representations\nWe saw that critical subject information is passed\nto the last position in the upper layers. We now\nanalyze what information is contained in the sub-\nject representation at the point of transfer, and how\ndoes this information evolves across layers. To do\nthis, we map hidden representations to vocabulary\ntokens via projection. Our results indicate that the\nsubject representation contains a wealth of infor-\nmation about the subject at the point where it is\ntransferred to the last position.\n2Results for varying values of k are provided in §A.4.\n12219\nSubject Example top-scoring tokens by the subject representation\nGPT-2\nIron Man 3, 2, Marvel, Ultron, Avenger, comics, suit, armor, Tony, Mark, Stark, 2020\nSukarno Indonesia, Buddhist, Thailand, government, Museum, Palace, Bangkok, Jakarta\nRoman Republic Rome, Augustus, circa, conquered, fame, Antiqu, Greece, Athens, AD, Caesar\nGPT-J\nFerruccio Busoni music, wrote, piano, composition, International, plays, manuscript, violin\nJoe Montana career, National, football, NFL, Award, retired, quarterback, throws, Field\nChromecast device, Audio, video, Wireless, HDMI, USB, Google, Android, technology, 2016\nTable 1: Example tokens by subject representations of GPT-2 ( ℓ= 40) and GPT-J ( ℓ= 22).\n0 20 40\nlayer\n0\n20\n40attributes rate\nGPT2-xl\n0 10 20\nlayer\n10\n20\n30\n40\n50 GPT-J\nsubject last\nsubject first\nsubject subseq.\ninput last\nFigure 3: Attributes rate at different positions across\nlayers (starting from layer 1), in GPT-2 and GPT-J.\n6.1 Inspection of Subject Representations\nMotivating Observation To analyze what is en-\ncoded in a representation hℓ\nt, we cast it as a distri-\nbution pℓ\nt over the vocabulary (Geva et al., 2022b;\nNostalgebraist, 2020), using the same projection\napplied to ﬁnal-layer representations (Eq. 2). Then,\nwe inspect the ktokens with the highest probabil-\nities in pℓ\nt. Examining these projections, we ob-\nserved that they are informative and often encode\nseveral subject attributes (Tab. 1 and §D). There-\nfore, we turn to quantitatively evaluate the extent\nto which the representation of a subject encodes\ntokens that are semantically related to it.\nEvaluation Metric: Attributes Rate Semantic\nrelatedness is hard to measure based on human\njudgment, as ratings are typically of low agreement,\nespecially between words of various parts of speech\n(Zesch and Gurevych, 2010; Feng et al., 2017).\nHence, we propose an automatic approximation\nof the subject-attribute relatedness, which is the\nrate of the predicted attributes in a given set of\ntokens known to be highly related to the subject.\nFor a given subject s, we ﬁrst create a set As of\ncandidate attributes, by retrieving paragraphs about\nsfrom Wikipedia using BM25 (Robertson et al.,\n1995), tokenizing each paragraph, and removing\ncommon words and sub-words. The setAs consists\nof non-common tokens that were mentioned in the\ncontext of s, and are thus likely to be its attributes.\nFurther details on the construction of these sets are\nprovided in §C. We deﬁne the attributes ratefor\na subject sin a set of tokens T as the portion of\ntokens in T that appear in As.\nExperiment We measure the attributes rate in\nthe top k = 50 tokens by the subject representa-\ntion, that is, the representation at the last-subject\nposition, in each layer. We focus on this position as\nit is the only subject position that attends to all the\nsubject positions, and thus it is likely to be the most\ncritical (we validate this empirically in §A.3). We\ncompare with the rate at other positions: the ﬁrst\nsubject position, the position after the subject, and\nthe last input position (i.e., the prediction position).\nResults Fig. 3 shows the results for GPT-2 and\nGPT-J . In both models, the attributes rate at the\nlast-subject position is increasing throughout the\nlayers, and is substantially higher than at other po-\nsitions in the intermediate-upper layers, reaching\nclose to 50%. This suggests that, during infer-\nence, the model constructs attribute-rich subject\nrepresentations at the last subject-position. In addi-\ntion, critical information from these representations\npropagates to the prediction, as this range of lay-\ners corresponds to the peak of critical information\nobserved by blocking the attention edges to the\nprediction (§5).\nWe have seen that the representation of the sub-\nject encodes many terms related to it. A natural\nquestion that arises is where these terms are ex-\ntracted from to enrich that representation. In princi-\nple, there are three potential sources (Mickus et al.,\n2022), which we turn to analyze in the next sec-\ntions: the static embeddings of the subject tokens\n(§6.2) and the parameters of the MHSA and MLP\nsublayers (§6.3).\n12220\n1-10 11-20 21-30 31-40\nintervention layers\n10\n20\n30\n40\nattributes rate\nat layer 40\nNo intervention\nMLP sublayers\nMHSA sublayers\nFigure 4: The attributes rate of the subject representa-\ntion with and without canceling updates from the MLP\nand MHSA sublayers in GPT-2.\n6.2 Attribute Rate in Token Embeddings\nWe test whether attributes are already encoded in\nthe static embeddings of the subject tokens, by mea-\nsuring the attributes rate, as in §6.1. Concretely,\nlet t1,...,t |s|be the tokens representing a subject s\n(e.g. Piet, ro, Men, nea for “Pietro Mennea”),\nand denote by ¯e := 1\n|s|\n∑|s|\ni=1 eti their mean em-\nbedding vector, where eti is the embedding of ti.\nWe compute the attributes rate in the top k = 50\ntokens by each of eti and by ¯e. We ﬁnd that the\nhighest attributes rate across the subject’s token\nembeddings is 19.3 on average for GPT-2 and 28.6\nin GPT-J , and the average rate by the mean sub-\nject embedding is 4.1 in GPT-2 and 11.5 in GPT-J .\nThese rates are considerably lower than the rates by\nthe subject representations at higher layers (Fig. 3).\nThis suggests that while static subject-token em-\nbeddings encode some factual associations, other\nmodel components are needed for extraction of\nsubject-related attributes.\n6.3 Subject Representation Enrichment\nWe next assess how different sublayers contribute\nto the construction of subject representations\nthrough causal interventions.\nMethod: Sublayer Knockout To understand\nwhich part of the transformer layer “adds” the in-\nformation about attributes to the representation,\nwe simply zero-out the two key additive elements:\nthe MHSA and MLP sublayers. Concretely, we\nzero-out updates to the last subject position from\neach MHSA and MLP sublayer, for 10 consec-\nutive layers. Formally, when intervening on the\nMHSA (MLP) sublayer at layer ℓ, we set aℓ′\ni = 0\n(mℓ′\ni = 0) for ℓ′= ℓ,..., min{ℓ+9,L}(see Eq. 1).\nFor each intervention, we measure the effect on the\nattributes rate in the subject representation at some\nspeciﬁc layer ¯ℓ, where attribute rate is high.\nResults Results with respect to layer ¯ℓ = 40 in\nGPT-2 are shown in Fig. 4, showing that canceling\nthe early MLP sublayers has a destructive effect on\nthe subject representation’s attributes rate, decreas-\ning it by ∼88% on average. In contrast, canceling\nthe MHSA sublayers has a much smaller effect of\n<30% decrease in the attributes rate, suggesting\nthat the MLP sublayers play a major role in creat-\ning subject representations. Results with respect to\nother layers and for GPT-J show similar trends and\nare provided in §C and §E, respectively. We further\nanalyze this by inspecting the MLP updates, show-\ning they promote subject-related concepts (§F).\nNotably, these ﬁndings are consistent with the\nview of MLP sublayers as key-value memories\n(Geva et al., 2021; Dai et al., 2022) and extend\nrecent observations (Meng et al., 2022a; Wallat\net al., 2020) that factual associations are stored in\nintermediate layers, showing that they are spread\nacross the early MLP sublayers as well.\n7 Attribute Extraction via Attention\nThe previous section showed that the subject repre-\nsentation is enriched with information throughout\nthe early-middle layers. But recall that in our pre-\ndiction task, only one speciﬁc attribute is sought.\nHow is this attribute extracted and at which point?\nWe next show that (a) attribute extraction is typ-\nically carried out by the MHSA sublayers (§7.1)\nwhen the last position attends to the subject, (b) the\nextraction is non-trivial as it reduces the attribute’s\nrank by the subject representation considerably\n(§7.2) and it depends on the subject enrichment\nprocess (§7.3), and (c) the relevant subject-attribute\nmappings are often stored in the MHSA parameters\n(§7.4). This is in contrast to commonly held belief\nthat MLPs hold such information.\n7.1 Attribute Extraction to the Last Position\nRecall that the critical information from the subject\npropagates when its representation encodes many\nterms related to it, and after the critical information\nﬂow from the relation positions (§5). Thus, the last-\nposition representation at this point can be viewed\nas a relation query to the subject representation. We\ntherefore hypothesize that the critical information\nthat ﬂows at this point is the attribute itself.\nExperiment: Extraction Rate To test this hy-\npothesis, we inspect the MHSA updates to the last\nposition in the vocabulary, and check whether the\n12221\n0 10 20 30 40 50\nlayer\n0.0\n0.1\n0.2attribute extraction rate\nMHSA\nMLP\nFigure 5: Attribute extraction rate across layers, for the\nMHSA and MLP sublayers in GPT-2.\nExtraction\nrate\n# of extracting\nlayers\nMHSA 68.2 2.15\n- all but subj. last + last 44.4 1.03\n- all non-subj. but last 42.1 1.04\n- last 39.4 0.97\n- subj. last 37.7 0.82\n- all but last 32.9 0.55\n- subj. last + last 32.8 0.7\n- non-subj. 31.5 0.71\n- subj. 30.2 0.51\n- all but subj. last 23.5 0.44\n- all but ﬁrst 0 0.01\nMLP 31.3 0.38\nTable 2: Per-example extraction statistics across layers,\nfor the MHSA and MLP sublayers, and MHSA with in-\nterventions on positions: (non-)subj. for (non-)subject\npositions, last (ﬁrst) for the last (ﬁrst) input position.\nExtraction rate here refers to the fraction of queries for\nwhich there was an extraction event in at least one layer.\ntop-token by each update matches the attribute pre-\ndicted at the ﬁnal layer. Formally, let\nt∗:= arg max(pL\nN ) ; t′:= arg max(Eaℓ\nN )\nbe the token predicted by the model and the top-\ntoken by the ℓ-th MHSA update to the last position\n(i.e., aℓ\nN ). We check the agreement between t∗and\nt′for every ℓ∈[1,L], and refer to agreement cases\n(i.e. when t′= t∗) as extraction events, since the\nattribute is being extracted by the MHSA. Similarly,\nwe conduct the experiment while blocking the last\nposition from attending to different positions (using\nattention knockout, see §5), and also apply it to the\nMLP updates to the last position.\nResults Fig. 5 shows the extraction rate (namely\nthe fraction of queries for which there was an ex-\ntraction event) for the MHSA and MLP updates\nacross layers in GPT-2 , and Tab. 2 provides per-\nexample extraction statistics (similar results for\nGPT-J are in §E). When attending to all the input\npositions, the upper MHSA sublayers promote the\n0 5 10 15 20\nlayer used for patching\n0.0\n0.2\n0.4\n0.6\n0.8attribute extraction rate\nno patching\npatched positions\nsubject\nnon-subject\nlast\nFigure 6: Extraction rate when patching representa-\ntions from early layers at different positions in GPT-2.\nattribute to the prediction (Fig. 5), with 68.2% of\nthe examples exhibiting agreement events (Tab. 2).\nThe layers at which extraction happens coincide\nwith those where critical subject information prop-\nagates to the last position (Fig. 2), which further\nexplains why this information is critical for the pre-\ndiction.\nConsidering the knockout results in Tab. 2, at-\ntribute extraction is dramatically suppressed when\nblocking the attention to the subject positions\n(30.2%) or non-subject positions (31.5%). More-\nover, this suppression is alleviated when allowing\nthe last position to attend to itself and to the sub-\nject representation (44.4%), overall suggesting that\ncritical information is centered at these positions.\nLast, the extraction rate by the MLP sublayers\nis substantially lower (31.3%) than by the MHSA.\nFurther analysis shows that for 17.4% of these ex-\namples, extraction by the MLP was preceded by an\nextraction by the MHSA, and for another 10.2% no\nextraction was made by the MHSA sublayers. This\nsuggests that both the MHSA and MLP implement\nattribute extraction, but MHSA is the prominent\nmechanism for factual queries.\n7.2 Extraction Signiﬁcance\nA possible scenario is that the attribute is already\nlocated at the top of the projection by the subject\nrepresentation, and the MHSA merely propagates it\n“as-is” rather than extracting it. We show that this\nis not the case, by comparing the attribute’s rank in\nthe subject representation and in the MHSA update.\nFor every extraction event with a subject represen-\ntation hℓ\ns, we check the attribute’s rank in δ(hℓ\ns),\nwhich indicates how prominent the extraction by\nthe MHSA is (recall that, at an extraction event,\nthe attribute’s rank by the MHSA output is 1). We\nobserve an average attribute rank of 999.5, which\nshows that the extraction operation promotes the\nspeciﬁc attribute over many other candidate tokens.\n12222\n7.3 Importance of Subject Enrichment\nAn important question is whether the subject repre-\nsentation enrichment is required for attribute extrac-\ntion by the MHSA. Arguably, the attribute could\nhave been encoded in early-layer representations\nor extracted from non-subject representations.\nExperiment We test this by “patching” early-\nlayer representations at the subject positions and\nmeasuring the effect on the extraction rate. For\neach layer ℓ= 0,1,5,10,20, with 0 being the em-\nbeddings, we feed the representations at the subject\npositions as input to the MHSA at any succeeding\nlayer ℓ′ > ℓ. This simulates the MHSA opera-\ntion at different stages of the enrichment process.\nSimilarly, we patch the representations of the last\nposition and of the other non-subject positions.\nResults Results for GPT-2 are shown in Fig. 6\n(and for GPT-J in §E). Patching early subject rep-\nresentations decreases the extraction rate by up to\n50%, which stresses the importance of attributes en-\nrichment for attribute recall. In contrast, patching\nof non-subject representations has a weaker effect,\nwhich implies that they are “ready” very early in\nthe computation. These observations are further\nsupported by a gradient-based feature attribution\nanalysis (§B), which shows the inﬂuence of the\nearly subject representations on the prediction.\nNotably, for all the positions, a major increase\nin extraction rate is obtained in the ﬁrst layer (e.g.\n0.05 →0.59 for non-subject positions), suggesting\nthat the major overhead is done by the ﬁrst layer.\n7.4 “Knowledge” Attention Heads\nWe further investigate how the attribute is extracted\nby MHSA, by inspecting the attention heads’ pa-\nrameters in the embedding space and analyzing the\nmappings they encode for input subjects, using the\ninterpretation by Dar et al. (2022).\nAnalysis To get the top mappings for a token t\nby the j-th head at layer ℓ, we inspect the matrix\nWℓ,j\nV Oin the embeddings space with\nGℓ,j := ET Wℓ,j\nV OE ∈R|V |×|V |, (8)\nby taking the ktokens with the highest values in\nthe t-th row of Gℓ,j. Notably, this is an approxi-\nmation of the head’s operation, which is applied to\ncontextualized subject representations rather than\nto token embeddings. For every extraction event\nwith a subject sand an attribute a, we then check if\naappears in the top-10 tokens for any ofs’s tokens.\nResults We ﬁnd that for 30.2% (39.3%) of the\nextraction events in GPT-2 (GPT-J ), there is a\nhead that encodes the subject-attribute mapping\nin its parameters (see examples in §G). Moreover,\nthese speciﬁc mappings are spread over 150 atten-\ntion heads in GPT-2 , mostly in the upper layers\n(24-45). Interestingly, further analysis of the fre-\nquent heads show they encode hundreds of such\nmappings, acting as “knowledge hubs” during in-\nference (§G). Overall, this suggests that factual\nassociations are encoded in the MHSA parameters.\n8 Related Work\nRecently, there has been a growing interest in\nknowledge tracing in LMs. A prominent thread\nfocused on locating layers (Meng et al., 2022a;\nWallat et al., 2020) and neurons (Dai et al., 2022)\nthat store factual information, which often informs\nediting approaches (De Cao et al., 2021; Mitchell\net al., 2022; Meng et al., 2022b). Notably, Hase\net al. (2023) showed that it is possible to change an\nencoded fact by editing weights in other locations\nfrom where methods suggest this fact is stored,\nwhich highlights how little we understand about\nhow factual predictions are built. Our work is mo-\ntivated by this discrepancy and focuses on under-\nstanding the recall process of factual associations.\nOur analysis also relates to studies of the pre-\ndiction process in LMs (V oita et al., 2019; Tenney\net al., 2019). Speciﬁcally, Haviv et al. (2023) used\nﬁne-grained interventions to show that early MLP\nsublayers are crucial for memorized predictions.\nAlso, Hernandez et al. (2023) introduced a method\nfor editing knowledge encoded in hidden repre-\nsentations. More broadly, our approach relates to\nstudies of how LMs organize information internally\n(Reif et al., 2019; Hewitt and Manning, 2019).\nMechanistic interpretability (Olah, 2022; Nanda\net al., 2023) is an emerging research area. Re-\ncent works used projections to the vocabulary (Dar\net al., 2022; Geva et al., 2022b; Ram et al., 2022)\nand interventions in the transformer computation\n(Wang et al., 2022; Haviv et al., 2023) to study\nthe inner-workings of LMs. A concurrent work by\nMohebbi et al. (2023) studied contextualization in\nLMs by zeroing-out MHSA values, a method that\neffectively results in the same blocking effect as\nour knockout method. In our work, we leverage\nsuch methods to investigate factual predictions.\n12223\n9 Conclusion\nWe carefully analyze the inner recall process of\nfactual associations in auto-regressive transformer-\nbased LMs, unveiling a core attribute extraction\nmechanism they implement internally. Our exper-\niments show that factual associations are stored\nalready in the lower layers in the network, and\nextracted eminently by the MLP sublayers during\ninference, to form attribute-rich subject represen-\ntations. Upon a given subject-relation query, the\ncorrect attribute is extracted from these represen-\ntations prominently through the MHSA sublayers,\nwhich often encode subject-attribute mappings in\ntheir parameters. These ﬁndings open new research\ndirections for knowledge localization and model\nediting.\nLimitations\nSome of our experiments rely on interpreting in-\ntermediate layer representations and parameters\nthrough projection to the vocabulary space. While\nthis approach has been used widely in recent works\n(Geva et al., 2022b,a; Dar et al., 2022; Ram et al.,\n2022; Nostalgebraist, 2020), it only provides an\napproximation of the information encoded in these\nvectors, especially in early layers. In principle, this\ncould have been an explanation to the increasing\nattributes rate in Fig. 3. However, this clear trend\nis unlikely to be explained only by this, given the\nlow attribute rate at the embedding layer and the\nincrease observed in the last few layers where ap-\nproximation is better (Geva et al., 2021).\nAnother limitation is that our attention knockout\nintervention method does not account for “infor-\nmation leakage” across positions. Namely, if we\nblock attention edges between two positions at a\nspeciﬁc layer, it is still possible that information\npassed across these positions in earlier layers. For\nthis reason, we block a range of layers rather than\na single layer, which alleviates the possibility for\nsuch leakage. Moreover, our primary goal in this\nwork was to identify critical attention edges, which\nare still critical even if such leakage occurs.\nAcknowledgements\nWe thank Asma Ghandeharioun for useful feedback\nand constructive suggestions.\nReferences\nMarco Ancona, Enea Ceolini, Cengiz Öztireli, and\nMarkus Gross. 2019. Gradient-Based Attribution\nMethods, pages 169–191. Springer International\nPublishing, Cham.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use atten-\ntion as explanation when we have saliency methods?\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 149–155, Online. Association for Com-\nputational Linguistics.\nRoi Cohen, Mor Geva, Jonathan Berant, and Amir\nGloberson. 2023. Crawling the internal knowledge-\nbase of language models. In Findings of the Asso-\nciation for Computational Linguistics: EACL 2023,\npages 1856–1869, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons\nin pretrained transformers. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8493–8502, Dublin, Ireland. Association for Com-\nputational Linguistics.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\n2022. Analyzing transformers in embedding space.\narXiv preprint arXiv:2209.02535.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n6491–6506, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMisha Denil, Alban Demiraj, and Nando De Freitas.\n2014. Extraction of salient sentences from labelled\ndocuments. arXiv preprint arXiv:1412.6815.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatﬁeld-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread. Https://transformer-\ncircuits.pub/2021/framework/index.html.\nYue Feng, Ebrahim Bagheri, Faezeh Ensan, and Jelena\nJovanovic. 2017. The state of the art in semantic re-\nlatedness: a framework for comparison. The Knowl-\nedge Engineering Review, 32:e10.\n12224\nMor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval\nSadde, Micah Shlain, Bar Tamir, and Yoav Goldberg.\n2022a. LM-debugger: An interactive tool for inspec-\ntion and intervention in transformer-based language\nmodels. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 12–21, Abu Dhabi,\nUAE. Association for Computational Linguistics.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022b. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 30–45, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5484–5495, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJohn F Grifﬁths, Anthony JF Grifﬁths, Susan R\nWessler, Richard C Lewontin, William M Gelbart,\nDavid T Suzuki, Jeffrey H Miller, et al. 2005. An\nintroduction to genetic analysis. Macmillan.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023. Does localization inform editing?\nsurprising differences in causality-based localization\nvs. knowledge editing in language models. arXiv\npreprint arXiv:2301.04213.\nAdi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster,\nYoav Goldberg, and Mor Geva. 2023. Understand-\ning transformer memorization recall through idioms.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 248–264, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nEvan Hernandez, Belinda Z Li, and Jacob Andreas.\n2023. Measuring and manipulating knowledge rep-\nresentations in language models. arXiv preprint\narXiv:2304.00740.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 681–691, San Diego, California. As-\nsociation for Computational Linguistics.\nKevin Meng, David Bau, Alex J Andonian, and\nYonatan Belinkov. 2022a. Locating and editing fac-\ntual associations in GPT. In Advances in Neural In-\nformation Processing Systems.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass-\nediting memory in a transformer.\nTimothee Mickus, Denis Paperno, and Mathieu Con-\nstant. 2022. How to dissect a Muppet: The struc-\nture of transformer embedding spaces. Transactions\nof the Association for Computational Linguistics,\n10:981–996.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nHosein Mohebbi, Willem Zuidema, Grzegorz Chru-\npała, and Afra Alishahi. 2023. Quantifying context\nmixing in transformers. In Proceedings of the 17th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 3378–\n3400, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nNeel Nanda, Lawrence Chan, Tom Liberum, Jess\nSmith, and Jacob Steinhardt. 2023. Progress mea-\nsures for grokking via mechanistic interpretability.\narXiv preprint arXiv:2301.05217.\nNostalgebraist. 2020. interpreting GPT: the logit lens.\nChris Olah. 2022. Mechanistic interpretability,\nvariables, and the importance of interpretable\nbases. Transformer Circuits Thread(June 27).\nhttp://www. transformer-circuits. pub/2022/mech-\ninterp-essay/index. html.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. .\nOri Ram, Liat Bezalel, Adi Zicher, Yonatan Be-\nlinkov, Jonathan Berant, and Amir Globerson. 2022.\nWhat are you token about? dense retrieval as\ndistributions over the vocabulary. arXiv preprint\narXiv:2212.10380.\n12225\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. Advances in Neural Information Processing\nSystems, 32.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp,\n109:109.\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, and Os-\nkar van der Wal. 2023. Inseq: An interpretabil-\nity toolkit for sequence generation models. arXiv\npreprint arXiv:2302.13942.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nMartin J Tymms and Ismail Kola. 2008.Gene knockout\nprotocols, volume 158. Springer Science & Busi-\nness Media.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nJonas Wallat, Jaspreet Singh, and Avishek Anand.\n2020. BERTnesia: Investigating the capture and for-\ngetting of knowledge in BERT. In Proceedings of\nthe Third BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP, pages 174–\n183, Online. Association for Computational Linguis-\ntics.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022. In-\nterpretability in the wild: a circuit for indirect ob-\nject identiﬁcation in gpt-2 small. arXiv preprint\narXiv:2211.00593.\nKayo Yin and Graham Neubig. 2022. Interpreting lan-\nguage models with contrastive explanations. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 184–\n198, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nTorsten Zesch and Iryna Gurevych. 2010. Wisdom of\ncrowds versus wisdom of linguists–measuring the\nsemantic relatedness of words. Natural Language\nEngineering, 16(1):25–59.\n12226\n0 10 20 30 40 50\n60\n40\n20\n0\n20\nGPT2-xl\nsubject non-subject last\n0 5 10 15 20 25\n75\n50\n25\n0\nGPT-J\nlayer\n% change in prediction probability\nFigure 7: Relative change in the prediction probability\nwhen intervening on attention edges to the last position,\nfor 9 layers in GPT-2 and 5 in GPT-J, for the subset of\nexamples where the subject appears at the ﬁrst position\nin the input.\nA Additional Information Flow Analysis\nA.1 Subject-Relation Order\nWe break down the results in §5 by the subject-\nrelation order in the input query, to evaluate\nwhether we observe the same trends. Since the\nrelation is expressed by all the non-subject tokens,\nwe split the data into two subsets based on the the\nsubject position: (a) examples where the subject\nappears in the ﬁrst position (i.e. the subject ap-\npears before the relation), and (b) examples where\nit appears at later positions (i.e. the subject appears\nafter the relation). We conduct the same attention\nblocking experiment as in §5, and show the results\nfor the two subsets in Fig. 7 (a) and Fig. 8 (b).\nIn both ﬁgures, subject information passes to the\nlast position at the same range of layers, showing\nthat this observation hold in both cases. However,\nwhen the relation appears before the subject, block-\ning the attention to its positions has a more promi-\nnent impact on the prediction probability. Also,\nits effect is more spread-out across all the layers.\nWe suggest that this different behavior is a result\nof a positional bias encoded in the ﬁrst position\nin GPT-like models: regardless of which token ap-\npears in the ﬁrst position, blocking attention to this\nposition typically results in a substantial decrease\nin the prediction probability. The examples in §H\ndemonstrate this. We further verify this in §A.2.\n0 10 20 30 40 50\n60\n40\n20\n0\n20\nGPT2-xl\nsubject non-subject last\n0 5 10 15 20 25\n80\n60\n40\n20\n0\nGPT-J\nlayer\n% change in prediction probability\nFigure 8: Relative change in the prediction probability\nwhen intervening on attention edges to the last position,\nfor 9 layers in GPT-2 and 5 in GPT-J, for the subset\nof examples where the subject appears after the ﬁrst\nposition in the input.\nA.2 First-position Bias\nWe observe that blocking the last position from\nattending to the ﬁrst position, regardless of which\ntoken corresponds to it, has a substantial effect on\nthe prediction probability (see examples in §H).\nWe quantify this observation and show that it\ndoes not change our main ﬁndings in §5. To this\nend, we conduct the same experiment in §5, but\nwithout blocking the attention edges to the ﬁrst po-\nsition. Results are provided in Fig. 9, showing the\nsame trends as observed when blocking the atten-\ntion to the ﬁrst position as well; in both GPT-2 and\nGPT-J , there are clear peaks of critical information\nfrom subject and non-subject positions propagating\nto the last position at different layers, which when\nblocked reduce the prediction probability drasti-\ncally.\nNonetheless, the decrease in probability at these\npeaks is smaller in magnitude when the ﬁrst po-\nsition is not blocked compared to when it is. For\nexample, blocking the subject positions leads to a\ndecrease of up to 40% when the last position can at-\ntend to the ﬁrst position, compared to 60% when it\ncannot (Fig. 2). Likewise, blocking the non-subject\npositions (which correspond to the relation) leads\nto greater impact across the early-intermediate lay-\ners when the ﬁrst position is blocked compared\nto when it is not. For instance, intervening on\nlayers 5-20 in GPT-J constantly decreases the out-\n12227\n0 10 20 30 40 50\n40\n20\n0\nGPT2-xl\nsubject non-subject last\n0 5 10 15 20 25\n40\n20\n0\nGPT-J\nlayer\n% change in prediction probability\nFigure 9: Relative change in the prediction probability\nwhen intervening on attention edges to the last position,\nfor 9 layers in GPT-2 and 5 in GPT-J. Here, we do not\nblock attention edges to the ﬁrst position in the input.\nput probability by ∼20% when the ﬁrst position is\nblocked (Fig. 2) compared to <5% when it is not.\nA.3 Information Flow from Subject Positions\nWe conjecture that, due to auto-regressivity, sub-\nject representations with critical information for\nthe prediction are formed at the last-subject posi-\ntion. To verify that, we reﬁne our interventions in\n§5, and block the attention edges to the last posi-\ntion from all the subject positions except one. We\nthen measure the effect of these interventions on\nthe prediction probability, which indicates from\nwhich position critical information propagates to\nthe prediction.\nFig. 10 depicts the results for GPT-2 , showing\nthat indeed the prediction is typically damaged\nby 50% −100% when the last subject-position is\nblocked (i.e. “ﬁrst” and “before-last”), and usually\nremains intact when this position is not blocked\n(i.e. “last”).\nA.4 Window Size\nWe examine the effect of the window size hyperpa-\nrameter on our information ﬂow analysis, as con-\nducted in §5 for the last position in GPT-2 and\nGPT-J . Results of this analysis with varying win-\ndow sizes of k= 1,5,9,13,17,21 are provided in\nFig. 11 for GPT-2 and Fig. 12 for GPT-J . Over-\nall, the same observations are consistent across\ndifferent window sizes, with two prominent sites\nlast first before last\nnon-blocked subject position\n100\n0\n100\n200\n300\nrelative change (%)\nin prediction probability\nFigure 10: Relative change in the prediction probability\nwhen intervening on attention edges to the last position\nfrom different subject positions in GPT-2.\n0 10 20 30 40 50\n10\n5\n0\n5\nk=1\nsubject non-subject last\n0 10 20 30 40 50\n40\n20\n0\nk=5\n0 10 20 30 40 50\n60\n40\n20\n0\n20\nk=9\n0 10 20 30 40 50\n60\n40\n20\n0\n20\nk=13\n0 10 20 30 40 50\n80\n60\n40\n20\n0\nk=17\n0 10 20 30 40 50\n80\n60\n40\n20\n0\nk=21\nlayer\n% change in prediction probability\nFigure 11: Relative change in the prediction probabil-\nity when intervening on attention edges from different\npositions to the last position, for windows ofklayers in\nGPT-2. The plots show the analysis for varying values\nof k.\nof critical information ﬂow to the last position –\none from the relation positions in the early layers,\nfollowed by another from the subject positions in\nthe upper layers. An exception, however, can be\nobserved when knocking out edges from the re-\nlation positions using just a single-layer window\nin GPT-2 ; in this case, no signiﬁcant change in\nthe prediction probability is apparent. This might\nimply that critical information from the relation\npositions is processed in multiple layers. Moreover,\nthe decrease in the prediction probability becomes\nmore prominent when for larger values of k. This\nis expected, as knocking out more attention edges\nin the computation prevents the model from con-\ntextualizing the input properly.\n12228\n0 5 10 15 20 25\n20\n10\n0\n10\n20 k=1\nsubject non-subject last\n0 5 10 15 20 25\n60\n40\n20\n0\nk=5\n0 5 10 15 20 25\n75\n50\n25\n0\nk=9\n0 5 10 15 20 25\n75\n50\n25\n0\nk=13\n0 5 10 15 20 25\n75\n50\n25\n0\nk=17\n0 5 10 15 20 25\n75\n50\n25\n0\nk=21\nlayer\n% change in prediction probability\nFigure 12: Relative change in the prediction probabil-\nity when intervening on attention edges from different\npositions to the last position, for windows ofklayers in\nGPT-J. The plots show the analysis for varying values\nof k.\nB Gradient-based Analysis\nGradient-based feature attribution methods, also\nknown as saliency methods, are a way to inspect\nwhat happens inside neural models for predictions\nof speciﬁc examples. Typically, they result in\na heatmap over the input (sub)tokens, i.e., high-\nlighted words. We distinguish between sensitivity\nand saliency (Ancona et al., 2019; Bastings and\nFilippova, 2020): methods such as Gradient-L2 (Li\net al., 2016) show to which inputs the model is\nsensitive, i.e., where a small change in the input\nwould make a large change in the output, but a\nmethod such as Gradient-times-Input (Denil et al.,\n2014) reﬂects salience: it shows (approximately)\nhow much each input contributes to this particular\nlogit value. The latter is computed as:\n∇xℓ\ni\nfℓ\nc (xℓ\n1,xℓ\n2,..., xℓ\nN ) ⊙xℓ\ni (9)\nwhere xℓ\ni is the input at position iin layer ℓ(with\nℓ = 0 being the embeddings which are used for\nGradient-times-Input), and fℓ\nc (·) the function that\ntakes the input sequence at layer ℓand returns the\nlogit for the target/predicted token c. To obtain a\nscalar importance score for each token, we take the\nL2 norm of each vector and normalize the scores to\nsum to one. For our analysis we use a generaliza-\ntion of this method3, and apply it not just to the in-\nput (sub)word embeddings (ℓ= 0), but also to each\nintermediate transformer layer (ℓ= 1,...,N ).4\nFig. 13 shows the per-layer gradient-times-input\nanalysis for the input “Beats Music is owned by”\nfor GPT-2 and GPT-J. This analysis supports our\nearlier ﬁndings (§6, Fig. 6) that shows the “readi-\nness” of subject vs. non-subject positions: We\nobserve both for both models that the subject po-\nsitions remain relevant until deep into the com-\nputation, while the subject is being enriched with\nassociations. Moreover, the input for ‘owned’ is\nrelevant for the prediction in the ﬁrst few layers,\nafter which the plot suggests it is incorporated into\nthe ﬁnal position. That ﬁnal position becomes more\nrelevant for the prediction the deeper we get into\nthe network, as it seemingly incorporates informa-\ntion from other positions, before it is used as the\npoint to make the ﬁnal prediction for target ‘Apple’\nat layer 48 (28). At that point, the ﬁnal position has\nvirtually all relevance, in line with what we would\nexpect for an auto-regressive model.\nIn Fig. 14 we go beyond a single example, and\nshow per-layer gradient-times-input aggregated\nover the dataset (a subset of COUNTER FACT, cf.\n§3) for each model. We can see that the observa-\ntions we made for the single example in Fig. 13\nalso hold in general: the subject tokens remain rel-\nevant until about 2\n3 into the network depth, and the\nrelation (indicated by “further tokens”) is relevant\nin the ﬁrst few layers, after which it becomes less\nrelevant to the prediction.\nC Attributes Rate Evaluation\nC.1 Evaluation Details\nWe provide details on the construction process of\ncandidate sets for attributes rate evaluation (§6.1).\nGiven a subject s, ﬁrst we use the BM25 algorithm\n(Robertson et al., 1995) to retrieve 100 paragraphs\nfrom the English Wikipedia5 with sbeing the query.\nFrom the resulting set, we keep only paragraphs for\n3Concurrently, the use of per-layer gradient-times-input,\nalso called gradient-times-activation, was also proposed by\nSarti et al. (2023) for computing contrastive (Yin and Neubig,\n2022) per-layer explanations.\n4We add an Identity layer to the output of each trans-\nformer layer block, and capture the output and gradient there,\nafter the attention and MLP sublayers have been added to the\nresidual. This is important especially for GPT-J, where the\nattention and MLP sublayers are computed in parallel, so cap-\nturing the output and gradient only at the MLP would result\nin incorrect attributions.\n5We use the dump of October 13, 2021.\n12229\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nBe\nats\n Music\n is\n owned\n by\nGradient*Activation for logit \"Apple\"\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) GPT-2\n0\n5\n10\n15\n20\n25\nBe\nats\n Music\n is\n owned\n by\nGradient*Activation for logit \"Apple\"\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) GPT-J\nFigure 13: Gradient-times-Activation analysis for the\nexample “Beats Music is owned by Apple”.\nwhich the subject appears as-is in their content or\nin the title of the page/section they are in. This is to\navoid noisy paragraph that could be obtained from\npartial overlap with the subject (e.g. retrieving a\nparagraph about “2004 Summer Olympics” for the\nsubject “2020 Summer Olympics”). This process\nresults in 58.1 and 54.3 paragraphs per subject on\naverage for the data subsets of GPT-2 and GPT-J ,\nrespectively.\nNext, for each model, we tokenize the sets of\nparagraphs, remove duplicate tokens, and tokens\nwith less than 3 characters (excluding spaces). The\nlater is done to avoid tokens representing frequent\nshort sub-words like S and ’s. For stopwords re-\nmoval, we use the list from the NLTK package. 6\nThis yields the ﬁnal sets As, of 1154.4 and 1073.1\ncandidate tokens on average forGPT-2 and GPT-J ,\nrespectively.\nC.2 Additional Sublayer Knockout Results\nWe extend our analysis in §6.3, where we analyzed\nthe contribution of the MLP and MHSA sublayers\nto the subject enrichment process. Speciﬁcally,\nwe now measure the effect of knocking out these\nsublayers on the attribute rate at any successive\nlayer, rather than on a single upper layer. Results\nfor GPT-2 are presented in Fig. 15 (MLP sublayers\nknockouts) and 16 (MHSA sublayers knockouts),\nshowing similar trends where canceling updates\nfrom the MLP sublayers decreases the attributes\nrate dramatically, while a more benign effect is\n6https://www.nltk.org/\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nFirst input token\nFirst subject token\nLast subject token\nAll subject tokens\nFirst subsequent token\nFurther tokens\nLast input token\nAverage attributions for target attribute\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) GPT-2\n0\n5\n10\n15\n20\n25\nFirst input token\nFirst subject token\nLast subject token\nAll subject tokens\nFirst subsequent token\nFurther tokens\nLast input token\nAverage attributions for target attribute\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) GPT-J\nFigure 14: Gradient-times-Activation averaged over all\nexamples.\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\n33\n35\n37\n39\n41\n43\n45\n47\nsubject representation layer\n1\n4\n7\n10\n13\n16\n19\n22\n25\n28\n31\n34first intervention layer\nGPT2-xl, MLP\n0\n10\n20\n30\n40\nFigure 15: The attributes rate of the subject representa-\ntion at different layers, when intervening on (canceling)\ndifferent 10 consecutive MLP sublayers in GPT-2.\nobserved when canceling MHSA updates.\nD Projection of Subject Representations\nWe provide additional examples for top-scoring\ntokens in the projection of subject representations\nacross layers, in GPT-2 and GPT-J . Tab. 3 (Tab. 4)\nshows the tokens for the subject “Mark Messier”\n(“iPod Classic”) across layers in GPT-2 and GPT-\nJ, excluding stopwords and sub-word tokens.\nE Additional Results for GPT-J\nWe supply here additional results for GPT-J .\nFig. 17 shows the effect of canceling updates from\nthe MHSA and MLP sublayers on the attributes\nrate of the subject representation at layer 22 (§6.3).\nFig. 18 and Tab. 5 provide the extraction rate by\nthe MHSA and MLP across layers and per-example\nextraction statistics, respectively (§7). Last, Fig. 19\n12230\nℓ Top-scoring tokens by the subject representation\nGPT-2\n25 Jr, Sr, era, Era, MP, JR, senior, Clarence, final, stars, Junior, High, Architects\n35 Hockey, hockey, Jr, NHL, Rangers, Canucks, Sr, Islanders, Leafs, Montreal, Oilers\n40 NHL, Hockey, Jr, Canucks, retired, hockey, Toronto, Sr, Islanders, Leafs, jersey, Oilers\nGPT-J\n13 Jr, Archives, ice, ring, International, Jersey, age, Institute, National, aged, career\n17 hockey, NHL, Jr, Hockey, ice, Canadian, Canada, Archives, ice, Toronto, ring, National\n22 NHL, National, hockey, Jr, Foundation, Hockey, Archives, Award, ice, career, http\nTable 3: Top-scoring tokens by the subject representations of “Mark Messier” across intermediate-upper layers (ℓ)\nin GPT-2 and GPT-J.\nℓ Top-scoring tokens by the subject representation\nGPT-2\n25 Edition, Ribbon, Series, Mini, version, Card, CR, XL, MX, XT, Cube, RX, Glow, speakers\n35 iPod, Bluetooth, Apple, Mini, iPhone, iOS, speaker, Edition, Android, Controller\n40 iPod, headphone, Bluetooth, speakers, speaker, Apple, iPhone, iOS, audio, headphones\nGPT-J\n13 Series, device, Apple, iPod, model, Music, iPhone, devices, models, style, music\n17 Series, model, device, style, Music, series, Archives, music, design, interface, models\n22 iPod, Music, music, song, Series, iPhone, Apple, songs, Music, review, series, Audio\nTable 4: Top-scoring tokens by the subject representations of “iPod Classic” across intermediate-upper layers ( ℓ)\nin GPT-2 and GPT-J.\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\n33\n35\n37\n39\n41\n43\n45\n47\nsubject representation layer\n1\n4\n7\n10\n13\n16\n19\n22\n25\n28\n31\n34first intervention layer\nGPT2-xl, MHSA\n0\n10\n20\n30\n40\nFigure 16: The attributes rate of the subject representa-\ntion at different layers, when intervening on (canceling)\ndifferent 10 consecutive MHSA sublayers in GPT-2.\nshows the effect of replacing intermediate represen-\ntations with early layer representations at different\npositions on the attribute extraction rate (§7).\nOverall, these results are consistent with those\nfor GPT-2 described throughout the paper.\nF Analysis of MLP Outputs\nFollowing the observation that the early MLP sub-\nlayers are crucial for attribute enrichment of subject\nrepresentations, we further analyze their updates\nto these representations. To this end, we decom-\npose these updates into sub-updates that, accord-\ning to Geva et al. (2022b), often encode human-\ninterpretable concepts. Concretely, We decompose\n1-10 6-15 11-20 16-22 21-22\nintervention layers\n30\n40\n50\nattributes rate\nat layer 22\nNo intervention\nMLP sublayers\nMHSA sublayers\nFigure 17: The attributes rate of the subject representa-\ntion with and without canceling updates from the MLP\nand MHSA sublayers in GPT-J.\nEq. 6 to a linear combination of parameter vectors\nof the second MLP matrix:\nmℓ\ni =\ndinner∑\nj=1\nσ\n(\nw(ℓ,j)\nI\n(\naℓ\ni + xℓ−1\ni\n))\n·w(ℓ,j)\nF ,\nwhere dinner is the MLP inner dimension, and\nw(ℓ,j)\nI ,w(ℓ,j)\nF are the j-th row and column of\nWℓ\nI ,Wℓ\nF , respectively. We then take the 100 vec-\ntors in Wℓ\nF with the highest contribution to this\nsum, for ℓ = 1,..., 20 in GPT-2 , and inspect the\ntop-scoring tokens in their projections to E.\nFrom manual inspection, we indeed were able to\nidentify cases where concepts related to the input\nsubject are promoted by the dominant sub-updates.\nExamples are provided in Tab. 6. We note that\nquantifying this process is non-trivial (either with\n12231\n0 5 10 15 20 25\nlayer\n0.0\n0.1\n0.2\n0.3\n0.4attribute extraction rate\nMHSA\nMLP\nFigure 18: Attribute extraction rate across layers, for\nthe MHSA and MLP sublayers in GPT-J.\nExtraction\nrate\n# of extracting\nlayers\nMHSA 76.7 1.9\n- last 46.7 0.8\n- non-subj. 45.6 0.8\n- all non-subj. but last 44.4 0.9\n- subj. last 41.1 0.7\n- subj. last + last 40.3 0.7\n- all but subj. last + last 34.8 0.5\n- all but subj. last 31.3 0.5\n- subj. 27.8 0.3\n- all but last 24.8 0.3\n- all but ﬁrst 0 0\nMLP 41 0.6\nTable 5: Per-example extraction statistics across layers,\nfor the MHSA and MLP sublayers, and MHSA with in-\nterventions on positions: (non-)subj. for (non-)subject\npositions, last (ﬁrst) for the last (ﬁrst) input position.\nhuman annotations or automatic methods), and so\nwe leave this for future work.\nG Subject-Attribute Mappings in\nAttention Heads\nIn §7, we showed that for many extraction events,\nit is possible to identify speciﬁc heads that encode\nmappings between the input subject and predicted\nattribute in their parameters. We provide examples\nfor such mappings in Tab. 7.\nAs a further analysis, we inspected the heads\nthat repeatedly extract the attribute for different in-\nput queries in GPT-2 . Speciﬁcally, for the matrix\nWℓ,j\nV Oof the j-th head at the ℓ-th layer, we analyze\nits projection to the vocabulary Gℓ,j (Eq. 8), but\ninstead of looking at the top mappings for a spe-\nciﬁc subject-token (i.e. a row in Gℓ,j), we look at\nthe top mappings across all rows, as done by (Dar\net al., 2022). In our analysis, we focused on the\nheads that extract the attribute for at least 10% of\nthe queries (7 heads in total). We observed that\nthese heads act as “knowledge hubs”, with their\ntop mappings encoding hundreds of factual asso-\n0 2 4 6 8 10\nlayer used for patching\n0.0\n0.2\n0.4\n0.6\n0.8attribute extraction rate\nno patching\npatched positions\nsubject\nnon-subject\nlast\nFigure 19: Extraction rate when patching representa-\ntions from early layers at different positions in GPT-J.\n0 5 10 15 20 25 30 35 40\nL\nud\nwig\n von\n M\nises\n's\n domain\n of\n work\n is\n the*\nIntervening on flow to:  the\nwindow: 10, base probability: 0.1373\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\np(economics)\nFigure 20: Example intervention on information ﬂow\nto the last position of the input Ludwig von Mises’s\ndomain of work is the , using a window size 10, in\nGPT-2.\nciations that cover various relations. Tab. 8 shows\nexample mappings by these heads.\nH Example Interventions on Information\nFlow\nExample interventions in GPT-2 are shown in\nFig. 20, 21, 22, 23, 24, and in GPT-J in\nFig. 25, 26, 27, 28, 29.\n12232\nSubject Description Layer,\nDim.\nTop tokens in the projection\nUnited\nLaunch\nAlliance\nAmerican spacecraft\nlaunch service provider\n1, 5394 jet, flights, aircraft, Cargo, passenger, plane,\nrockets, airspace, carrier, missiles, aerospace\nYakuza 2 Action-adventure video\ngame\n2, 4817 Arcade, pixels, arcade, Wii, Fighters, pixels,\nMinecraft, Sega, Sonic, GPUs, Hardware,\ndownloadable, multiplayer, Developers, livestream\nLeonard Bern-\nstein\nAmerican composer, pi-\nanist, music educator,\nand author\n4, 248 violin, rehearsal, opera, pian, composer, Harmony,\nensemble, Melody, piano, musicians, poets,\nOrchestra, Symphony\nBrett Hull Canadian–American\nformer ice hockey\nplayer\n4, 5536 discipl, athlet, Athletics, League, Hockey, ESPN,\nformer, Sports, NHL, athleticism, hockey, Champions\nBhaktisiddhanta\nSaraswati\nSpiritual master, instruc-\ntor, and revivalist in\nearly 20th century India\n5, 3591 Pradesh, Punjab, Bihar, Gandhi, Laksh, Hindu, Hindi,\nIndian, guru, India, Tamil, Guru, Krishna, Bengal\nMark Walters English former profes-\nsional footballer\n10, 1078 scorer, offence, scoring, defences, backfield,\nmidfielder, midfield, striker, fielder, playoffs,\nrebounds, rushing, touchdowns\nAcura ILX Compact car 19, 179 Highway, bike, truck, automobile, Bicycle,\nmotorists, cycle, Motor, freeway, vehicle, commute,\nRoute, cars, motorcycle, Tire, streetcar, traffic\nBeats Music Online music streaming\nservice owned by Apple\n20, 5488 Technologies, Technology, Apple, iPod, iOS, cloud,\nAppl, engineering, software, platform, proprietary\nTable 6: Top-scoring tokens in the projection of dominant MLP sub-updates to the subject representation in GPT-2.\nMatrix Subject Top mappings\nW(33,13)\nOV Barry Zito Baseball, MLB, infield,\nBethesda, RPGs, pitching,\noutfield, Iw, pitcher\nW(37,3)\nOV iPod Classic Macintosh, iPod, Mac,\nPerse, Apple, MacBook,\nBeck, Philipp\nW(44,16)\nOV Philippus van\nLimborch\nDutch, Netherlands,\nvan, Amsterdam, Holland,\nVanessa\nTable 7: Example mappings between subject-tokens\n(underlined) and attributes (bold) encoded in attention\nparameters (tokens for the same word are not shown).\n0 5 10 15 20 25 30 35 40\nThe\n head\nquarter\n of\n WWE\n is\n in*\nIntervening on flow to:  in\nwindow: 10, base probability: 0.9702\n0.4\n0.6\n0.8\np(Stamford)\nFigure 21: Example intervention on information ﬂow\nto the last position of the input The headquarter of\nWWE is in, using a window size 10, in GPT-2.\n0 5 10 15 20 25 30 35 40\nC\nab\nern\net\n Franc\n,\n which\n is\n named\n for\n the\n region\n of*\nIntervening on flow to:  of\nwindow: 10, base probability: 0.4321\n0.1\n0.2\n0.3\n0.4\n0.5\np(France)\nFigure 22: Example intervention on information ﬂow\nto the last position of the input Cabernet Franc,\nwhich is named for the region of, using a window\nsize 10, in GPT-2.\n12233\nMatrix Top mappings\nW(43,25)\nOV\n(Finnish, Finland)\n(Saskatchewan, Canadian)\n(Finland, Finnish)\n(Saskatchewan, Alberta)\n(Helsinki, Finnish)\n(Illinois, Chicago)\n(Chicago, Illinois)\n(Blackhawks, Chicago)\n(Australia, Australians)\n(NSW, Sydney)\n(Minneapolis, Minnesota)\n(Auckland, NZ)\n(Texas, Texans)\nW(36,20)\nOV\n(Japanese, Tokyo)\n(Koreans, Seoul)\n(Korean, Seoul)\n(Haiti, Haitian)\n(Korea, Seoul)\n(Mexican, Hispanic)\n(Spanish, Spanish)\n(Japanese, Japan)\n(Hawai, Honolulu)\n(Dublin, Irish)\n(Norwegian, Oslo)\n(Israelis, Jewish)\n(Vietnamese, Thai)\n(Israel, Hebrew)\nW(31,9)\nOV\n(oglu, Turkish)\n(Tsuk, Japanese)\n(Sven, Swedish)\n(Tsuk, Tokyo)\n(Yuan, Chinese)\n(stadt, Germany)\n(von, German)\n(Hiro, Japanese)\n(Sven, Norwegian)\n(stadt, Berlin)\n(Yuan, Beijing)\n(Sven, Danish)\n(oglu, Erdogan)\n(Fei, Chinese)\n(Samurai, Japanese)\nTable 8: Example top mappings encoded in the parame-\nters of attention heads that extract the attribute for many\ninput queries.\n0 5 10 15 20 25 30 35 40\nS\niem\niat\ny\nc\nze\n is\n located\n in\n the\n country\n of*\nIntervening on flow to:  of\nwindow: 10, base probability: 0.3715\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\np(Poland)\nFigure 23: Example intervention on information ﬂow\nto the last position of the input Siemiatycze is\nlocated in the country of , using a window size\n10, in GPT-2.\n0 5 10 15 20 25 30 35 40\nThe\n currency\n of\n the\n United\n States\n,\n the*\nIntervening on flow to:  the\nwindow: 10, base probability: 0.6915\n0.2\n0.4\n0.6\n0.8\np(dollar)\nFigure 24: Example intervention on information ﬂow\nto the last position of the input The currency of the\nUnited States, the, using a window size 10, in GPT-\n2.\n0 5 10 15 20 25\nCom\nmer\nzb\nank\n,\n whose\n headquarters\n are\n in*\nIntervening on flow to:  in\nwindow: 5, base probability: 0.6748\n0.2\n0.4\n0.6\n0.8\np(Frankfurt)\nFigure 25: Example intervention on information ﬂow\nto the last position of the input Commerzbank, whose\nheadquarters are in, using a window size 5, in GPT-\nJ.\n0 5 10 15 20 25\nEd\nvard\n Gri\neg\n,\n playing\n the*\nIntervening on flow to:  the\nwindow: 5, base probability: 0.4273\n0.1\n0.2\n0.3\n0.4\np(piano)\nFigure 26: Example intervention on information ﬂow\nto the last position of the input Edvard Grieg,\nplaying the, using a window size 5, in GPT-J.\n0 5 10 15 20 25\nQueen\n Elizabeth\n Land\n belongs\n to\n the\n continent\n of*\nIntervening on flow to:  of\nwindow: 5, base probability: 0.6688\n0.2\n0.4\n0.6\n0.8\np(Antarctica)\nFigure 27: Example intervention on information ﬂow\nto the last position of the inputQueen Elizabeth Land\nbelongs to the continent of, using a window size\n5, in GPT-J.\n12234\n0 5 10 15 20 25\nStat\nistical\n Package\n for\n the\n Social\n Sciences\n was\n created\n by*\nIntervening on flow to:  by\nwindow: 5, base probability: 0.4543\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\np(IBM)\nFigure 28: Example intervention on information ﬂow\nto the last position of the input Statistical Package\nfor the Social Sciences was created by , using\na window size 5, in GPT-J.\n0 5 10 15 20 25\nThe\n mother\n tongue\n of\n Piet\nro\n Men\nnea\n is*\nIntervening on flow to:  is\nwindow: 5, base probability: 0.2988\n0.1\n0.2\n0.3\n0.4\np(Italian)\nFigure 29: Example intervention on information ﬂow\nto the last position of the inputThe mother tongue of\nPietro Mennea is, using a window size 5, in GPT-J.\n12235"
}