{
  "title": "Language Modelling via Learning to Rank",
  "url": "https://openalex.org/W3207058314",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2335500902",
      "name": "Arvid Frydenlund",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2097046519",
      "name": "Gagandeep Singh",
      "affiliations": [
        "Nuance Communications (Austria)"
      ]
    },
    {
      "id": "https://openalex.org/A206911289",
      "name": "Frank Rudzicz",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Vector Institute",
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2335500902",
      "name": "Arvid Frydenlund",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2097046519",
      "name": "Gagandeep Singh",
      "affiliations": [
        "Nuance Communications (Austria)"
      ]
    },
    {
      "id": "https://openalex.org/A206911289",
      "name": "Frank Rudzicz",
      "affiliations": [
        "Vector Institute",
        "University of Toronto",
        "Unity Health Toronto"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2952603081",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2798471433",
    "https://openalex.org/W6751751081",
    "https://openalex.org/W3123816584",
    "https://openalex.org/W6676699771",
    "https://openalex.org/W2964508650",
    "https://openalex.org/W6753056052",
    "https://openalex.org/W6794347191",
    "https://openalex.org/W2404429280",
    "https://openalex.org/W2995460523",
    "https://openalex.org/W3087395956",
    "https://openalex.org/W3092904961",
    "https://openalex.org/W3035018839",
    "https://openalex.org/W3021708827",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W6996569244",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2411934291",
    "https://openalex.org/W2788031431",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W6601741120",
    "https://openalex.org/W6794435261",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6689264180",
    "https://openalex.org/W3204510963",
    "https://openalex.org/W3022579377",
    "https://openalex.org/W3163887420",
    "https://openalex.org/W3169059075",
    "https://openalex.org/W6767057552",
    "https://openalex.org/W6674706990",
    "https://openalex.org/W2903707108",
    "https://openalex.org/W2946536929",
    "https://openalex.org/W3034756453",
    "https://openalex.org/W2758816656",
    "https://openalex.org/W3035512170",
    "https://openalex.org/W3152607317",
    "https://openalex.org/W2798540241",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3101754294",
    "https://openalex.org/W3175265131",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W3159358337",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W4287666709",
    "https://openalex.org/W2097116485",
    "https://openalex.org/W4287828713",
    "https://openalex.org/W2229162816",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W4287124696",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W3159620685",
    "https://openalex.org/W2963824800",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W2905933322",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2803023299",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3004809259",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2983128379",
    "https://openalex.org/W43928053",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3035072529"
  ],
  "abstract": "We consider language modelling (LM) as a multi-label structured prediction task by re-framing training from solely predicting a single ground-truth word to ranking a set of words which could continue a given context. To avoid annotating top-k ranks, we generate them using pre-trained LMs: GPT-2, BERT, and Born-Again models. This leads to a rank-based form of knowledge distillation (KD). We also develop a method using N-grams to create a non-probabilistic teacher which generates the ranks without the need of a pre-trained LM. We confirm the hypotheses: that we can treat LMing as a ranking task and that we can do so without the use of a pre-trained LM. We show that rank-based KD generally gives a modest improvement to perplexity (PPL) -- though often with statistical significance -- when compared to Kullback–Leibler-based KD. Surprisingly, given the naivety of the method, the N-grams act as competitive teachers and achieve similar performance as using either BERT or a Born-Again model teachers. Unsurprisingly, GPT-2 always acts as the best teacher. Using it and a Transformer-XL student on Wiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and against a KL-based KD of 56.70.",
  "full_text": "Language Modelling via Learning to Rank\nArvid Frydenlund1,2, Gagandeep Singh3, Frank Rudzicz1,2,4\n1 Department of Computer Science, University of Toronto\n2 Vector Institute for Artiﬁcial Intelligence\n3 Nuance Communications Inc.\n4 Unity Health Toronto\narvie@cs.toronto.edu, gagandeep.singh1@nuance.com, frank@cs.toronto.edu\nAbstract\nWe consider language modelling (LM) as a multi-label struc-\ntured prediction task by re-framing training from solely pre-\ndicting a single ground-truth word to ranking a set of words\nwhich could continue a given context. To avoid annotating\ntop-k ranks, we generate them using pre-trained LMs: GPT-\n2, BERT, and Born-Again models. This leads to a rank-\nbased form of knowledge distillation (KD). We also develop a\nmethod using N-grams to create a non-probabilistic teacher\nwhich generates the ranks without the need of a pre-trained\nLM.\nWe conﬁrm the hypotheses that we can treat LMing as a rank-\ning task and that we can do so without the use of a pre-trained\nLM. We show that rank-based KD generally improves per-\nplexity (PPL) — often with statistical signiﬁcance — when\ncompared to Kullback–Leibler-based KD. Surprisingly, given\nthe simplicity of the method, the N-grams act as competi-\ntive teachers and achieve similar performance as using either\nBERT or a Born-Again model as teachers. GPT-2 always acts\nas the best teacher, though, and using it and a Transformer-XL\nstudent on Wiki-02, rank-based KD reduces a cross-entropy\nbaseline from 65.27 to 55.94 and against a KL-based KD of\n56.70.\n1 Introduction and Motivation\nMore often than not, there are many ways to say the same\nthing. For example, ‘the cat sat on the mat’ is semanti-\ncally equivalent to ‘the cat sat on the rug’, in most contexts.\nThis basic fact about language is ignored when training lan-\nguage models, where we try to maximize the probability\nof predicting a single ground-truth word for a given con-\ntext and penalize all other words regardless of any potential\nsemantic equivalence. In particular, a word-level language\nmodel (LM) deﬁnes a distribution over T discrete tokens,\nx1;:::;x T , as\np(x1;:::;x T |x0) =\nTY\nt=1\np(xt |x<t)\n=\nTY\nt=1\newgt\nt\nP|V|\nj=1 ewj\nt\n=\nTY\nt=1\newgt\nt\nZt\n;\n(1)\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nwhere we use the softmax function to model a multinomial\ndistribution over a set vocabulary V, wt ∈R|V|are the log-\nits produced by the model at step t, j indexes the score for\nthe jth word-type, and gt is the ground-truth (GT) index at\ntime t. Such models are trained by minimizing the per-word\ncross-entropy (CE or H(y;p)) against the ground-truth text,\ny = y1;:::;y T ∈R|V|which are one-hot representations\nof x1;:::;x T ,\n−1\nT\nTX\nt=1\n|V|X\ni=1\nyi\nt log ewi\nt\nZt\n= 1\nT\nTX\nt=1\n|V|X\ni=1\nyi\nt(log Zt −wi\nt): (2)\nDuring training, we assume that y is a one-hot distribu-\ntion since we do not have access to the true data distribution.\nThis assumption is obviously incorrect, considering that hu-\nmans are able to form a valid continuation of most contexts\nwith multiple potential words. Call these potential words the\nbranching set of a given context. Instead, we may want to\ngive partial credit for predicting words in the branching set,\nlike ‘rug’ in instances when the GT is ‘mat’ (and vice versa)\nbased on the semantic similarity between the words and con-\ntexts. One method for achieving this is via label smoothing.\n1.1 Label Smoothing and Knowledge Distillation\nLabel smoothing (LS) is a method which modiﬁes each yt\nto be soft-targets, instead of one-hot, by redistributing proba-\nbility from the GT label to other labels (Szegedy et al. 2016).\nIt is commonly implemented using a hand-crafted function,\nsuch as setting the GT probability to 0.9 and spreading the\nremaining 0.1 uniformly across the other labels (Pereyra\net al. 2017). LS is a simple method which has different mo-\ntivations depending on the problem it is meant to solve. One\nproblem is noisy labels (Lukasik et al. 2020a). Here one\nviews the GTs as truly one-hot but possibly incorrect due to\nerrors in annotation. A second problem is to regularize over-\nconﬁdent models in situations where we desire that model’s\nprobabilities should not spike to a single prediction (Pereyra\net al. 2017; M ¨uller, Kornblith, and Hinton 2019). Here, one\nagain views the GTs as truly one-hot but they do not want\nthe model to make predictions too close to the one-hot dis-\ntribution due to how the model is to be used. However, in our\ncase, we use label smoothing to make the single-target GTs\nmulti-target, under the view that the true GTs are actually\nmulti-target but that we lack access to this information.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10636\nGiven that the multiple-targets should change given the\ncontext, we desire the smoothing values to depend on the\ncontext. One can have a data-dependent or semantically-\naware LS by using an auxiliary pre-trained model to form\nlabel scores, such as using cosine scores from FastText (El-\nbayad, Besacier, and Verbeek 2018; Li et al. 2019). This can\nbe seen as a form of knowledge distillation (KD), which is\nwhen a pre-trained teacher model is used to train a student\nmodel by minimizing the cross entropy,H(pKD;p), between\nthe teacher’s distribution,pKD and the student’s,p(Hinton,\nVinyals, and Dean 2015; Wang and Yoon 2021). Here, the\nsmoothing scores are derived from the teacher’s distribution\nand thus can be viewed as parametric form of semantic LS\n(Yuan et al. 2020; Tang et al. 2020).\nOne can perform KD either from a weaker language rep-\nresentation model such as FastText or a strong language\nmodel such as GPT-2. However, this raises the question –\nwho teaches the teacher ? That is, in order to train our LM\nwe need to assume the existence of an already established\nmodel, which seems to defeat the purpose. Instead, we set\nour own desideratum that we do not want to use a pre-trained\nneural LM as a teacher.\nWe hypothesize thatN-gram statistics are a rich source of\nsemantic information which can be exploited to train neu-\nral LMs, i.e., introducing global N-gram statistics to our\notherwise local training procedure (Neubig and Dyer 2016;\nZhao et al. 2017; Yang, Wang, and Downey 2019). The na¨ıve\napproach would be to pre-train an N-gram LM to use as\nour teacher; however, this will not work because the N-\ngram model will be a weak teacher. N-gram LMs perform\nworse than neural language models (Mikolov 2012; Joze-\nfowicz et al. 2016). Because of this, if we just try to match\nthe teacher’s distribution, as is done in KD, we will end up\nwith a higher perplexity model than had we forgone the KD\nfrom the N-gram model. To overcome this issue, we need to\nexamine how label smoothing and N-gram LMs work.\n1.2 Ranking in Lieu of Label Smoothing\nWe can break semantic label smoothing into two problems:\na) how to choose which words should get partial credit\nin a given context, and b) how much partial credit they\nshould receive. To solve the ﬁrst problem, we could use the\ncommon ‘distributional hypothesis’ from linguistics to give\npartial credit to words which share similar contexts (Har-\nris 1954; Mikolov et al. 2013). That is, words which have\nsome shared context should get some partial credit. The dis-\ntributional hypothesis underlies N-gram language models\nas well, which employ smoothing algorithms and back-off\nmethods to weigh higher-order N-grams more than lower\norder ones (Goodman 2001). To solve the problem of quan-\ntifying credit assignment, it seems we need a probabilistic\n(even if un-normalized) model to specify the smoothing val-\nues. However, as reasoned above, if we na¨ıvely useN-gram\nLMs for this, we will wind up trying to match the distribu-\ntion of a weaker teacher model. One solution to this would\nbe to modify the N-gram LM teacher itself.\nN-gram LM algorithms were developed with the goal of\nthe model being a valid LM for use in applications. One cri-\nterion of such a model is that it produces a non-zero prob-\nability. This is achieved by including the unigram distribu-\ntion. Another criterion is that the N-gram LM generalizes\nwell to unseen contexts. This is achieved by well-developed\nsmoothing algorithms. If we only intend to use the N-gram\nmodel as a teacher, we may ease or remove these criteria.\nFor example, we might posit the converse of the distribu-\ntional hypothesis and say that we will restrict credit from\nwords which do not share contexts. To achieve this, we could\nforego backing-off to the unigram distribution and smooth-\ning into unseen contexts. One of the main motivations of\nneural LMs over N-grams is their ability to generalize due\ntheir distributed representations (Bengio et al. 2003). This\nimplies that we could forego applying any kind of smooth-\ning algorithm to the N-gram teacher and let the neural stu-\ndent learn to generalize to unseen contexts. If we follow this\napproach, then we would need to make modiﬁcations to ex-\nisting N-gram LM algorithms to determine how to combine\nthe various N-gram probabilities into a (sparse) probabilis-\ntic model. Instead, we propose a more radical approach in\norder to avoid this.\nIf we decompose the problem of specifying label weights,\nwe are specifying not only how to weight each cost term but\nalso, implicitly, an ordering of the weighted terms. Thus,\ngiven label weights, we can convert the problem of LS to\na ranking problem by specifying that the most probable la-\nbel is the most relevant rank and the second most probable\nis the second most relevant rank, etc. The main contribu-\ntion of this paper is therefore the idea that we may not\nneed to specify speciﬁc label weights, so long as we can\nspecify an order to the labels. In particular, we hypothesize\nthat the ordering of labels contains sufﬁcient semantic infor-\nmation to be used for our desired partial credit. To specify\nthe ordering, we again employ the distributional hypothesis\nand consider that increasing the shared context between a\nword and the ground-truth word increases the relevance of\nthat word in the ordering. Note that the original GT word\nwill always retain the most relevant rank, as it will always\nshare the most context with itself. Since ordinal information\nis less strict than interval information, we may overcome the\nissue of having a weak teacher since we are no longer trying\nto directly match a weaker distribution.\nThis approach obviates the need for a modiﬁed probabilis-\ntic N-gram LM since we just need to consider the size of\na shared N-gram context to determine how much credit to\ngive to a set of words which can follow. We describe how\nto construct artiﬁcial ranking GTs from N-gram statistics in\nSection 2.1 and how to use these in training the neural LM\nin Section 2.2.\n2 Methods\n2.1 N-gram Branching Set Construction\nLet a branching set (BS), bt, be a set of words which can\ncontinue a given context ct = (x1;:::;x t−1) at step t. Let\nct(n) be a slice of the context which only includes the previ-\nous n−1 words (assuming t≥n). Intuitively, the construc-\ntion of the BS is simple. We are going to consider a series of\npast N-gram contexts, starting with the largest context until\nthe smallest, and any word that we ﬁnd in the training set\n10637\nAdams River is a tributary to the Thompson and Fraser\nMoro passes Lumber the , Jim sub Sam\nShebelle Lake <unk> located of Harry <unk>\nColorado to thought the Kansas\nMissouri run north in Platte\nPayette ﬂows 298 valleys Missouri\n<unk> begins took Yellowstone\nBack ( by Mickey\nRed valley paymen- Mississippi\nYellowst- Provincial is Cheyenne\nGrand sockeye Columbia\nMississ- area Ohio\n, Osage\nand Kissimmee\nFigure 1: Branching sets created for the ground-truth ‘Adams River is a tributary to the Thompson and Fraser Rivers ...’ using\nthe ground-truths (Ogt) and four orders (O5, O4, O3, O2) from the Wiki02 training partition. The branching sets for ‘tributary’\nand ‘Thompson’ are only the ground-truths since all other orders have been pruned.\nthat continues those contexts will be in the BS. Then, in or-\nder to derive artiﬁcial rank GTs from the BS, we can give\na weak ordering to the words in the BS by specifying that\nwords in the BS with a larger context should have a higher\nrank relevance than those with a smaller context. If, for ex-\nample, we consider bigrams and trigrams then any word that\ncontinues the trigram context will also continue the bigram\ncontext but words which only continue the bigram context\nbut not the trigram context will be less relevant. Unigrams\nare excluded since they use no context.\nMore formally, we construct a tuple of sets referred to as\norders, O = (Ogt;OM ;:::;O 2) which will be merged to\nform the BS. Ogt = {xt}, only contains the original GT.\nThe other orders are made up of all words which continue\na speciﬁc context length, such that we construct each of the\norders as Om = {x|x∈V\\O>m;C(ct(m);x) ≥1}, where\nm ∈ {M;:::; 2}and C(ct(m);x) is the count for how\nmany times x appears in the context ct(m). Note, we ex-\nclude words already included in previous orders. We also use\na cut-off, q, so that we only consider sufﬁciently small order-\nings |Om|<q, since large BSs will not be informative; e.g.,\nall words which follow ‘the’ or ‘is a’. The BS is constructed\nas an ordered tuple where bt = (x|x ∈Oi;Oi ∈O). The\nartiﬁcial GT ranks are then speciﬁed as the weak rank order-\ning of the BS, which is weak since each Om may contain\nmultiple unordered words.\nWe can further resolve the rank within each order by us-\ning future context in addition to past context. This assumes\nthat, had we access to longer past contexts, the ranks would\nhave resolved the same way as using future information.\nNote that we can introduce future information here because\nthe teacher model does not need to form a valid proba-\nbility distribution since it only deﬁnes targets for the stu-\ndent model. Let ct(np;nf ) be the context deﬁned by us-\ning np −1 words in the past and nf −1 words in the fu-\nture. We prioritize past context over future context, since\nthat is what the student model will be able to observe as\ninput. This is done by specifying the orders by a reverse or-\nthographical sorting of past and future context lengths, i.e.,\nct(3;4) ≻ct(3;3) ≻ct(2;4). This does not completely re-\nsolve the weak ordering, though, since two words may share\nboth the same past and future contexts. Figure 1 shows an\nexample BS and Figure 2 shows how it was constructed us-\ning future information.\n2.2 Plackett-Luce Rank Loss\nLet us assume that the BS, bt, is strongly ordered by some\npreferred (partial) rank over k ≤|V|words in V for step t\nand that the GT word has the ﬁrst rank. Let yt be the set of\nwords in V and say that bt(m) indexes the element of rank\nmaccording to bt, i.e, bt deﬁnes a rank permutation. If we\nassume that yt and wt are sorted by rank, we can drop the\nindexing in the following section.\nGiven strongly ordered top-k ground-truths, we train our\nLM by learning to rank with the top-k Plackett-Luce (PL)\nrank loss (Plackett 1975; Cao et al. 2007; Xia, Liu, and Li\n2009). The PL loss models the data as a Plackett-Luce dis-\ntribution, where for any time step t,\npt(y1\nt ≻···≻yk\nt ) =\nkY\ni=1\npt(yi\nt |y1\nt ;:::;y i−1\nt )\n=\nkY\ni=1\newi\nt\nP|V|\nj=i ewj\nt\n:\n(3)\nThe PL distribution represents a generative process of sam-\npling a softmax distribution without replacement, were we\nre-normalize the distribution by excluding previous samples\nat every ranking step. The loss is then deﬁned as the negative\nlog-likelihood of equation 3,\n10638\nAdams River is a tributary to the Thompson and Fraser Rivers\na tributary , the Thompson and Sam\na tributary of the and <unk> Rivers\ntributary the and Kansas Rivers\ntributary in and Platte Rivers\ntributary valleys and Missouri Rivers\ntributary took and Yellowstone Rivers\nFigure 2: BS construction for the example in Figure 1 using four orders, 2p-1f, 2p, 1p-1f, 1p and the GTs. Where ‘ 2p-1f’\nindicates an order which matches two past words and one future word. We show the branching sets for ‘to’ and ‘Fraser’, centred\nin dash lines, and the context that selected those words in solid lines. The table has been truncated.\nkX\ni=1\nlog\n|V|X\nj=i\newj\nt −wi\nt =\nkX\ni=1\nlog\n\u0010\nZt −\nX\nj<i\newj\nt\n\u0011\n−wi\nt: (4)\nWhile there are many surrogate ranking losses, we choose\nPL since, in the case when k = 1, the PL distribution col-\nlapses to the softmax distribution and the loss to the corre-\nsponding CE loss. That is, we revert back to traditional lan-\nguage modelling. This is a beneﬁcial property since a) we\nmight encounter contexts during training without available\nranking data, i.e., only the original GT word is in the branch-\ning set, b) we only consider the probability of the original\nGT during evaluation, and c) we do not wish to radically\nchange the distribution being modelled so that PPL is com-\nparable to previous work. The PL loss can also be efﬁciently\nimplemented for GPUs and does not add signiﬁcant compu-\ntation over a regular softmax (see Appendix C).\nWhen combined with a method for constructing artiﬁcial\nranking ground-truths, we can view the PL loss as a form of\nself-supervision where the ﬁrst ranking term is our original\nloss and all higher ranking terms are auxiliary losses. How-\never, unlike most auxiliary losses used in self-supervised\nlearning, the extra PL terms are a natural extension or gen-\neralization of the original loss.\nThe PL loss can fail to capture the inductive bias that it\nis more important to get the more-preferred ranks correct\nover the lesser-preferred ranks (Lan et al. 2014). This is rel-\nevant since the original GT is the ﬁrst rank and we will have\nless conﬁdence as we descend the ranks, since the amount\nof semantic information in a bigram is generally less than in\na trigram, and so on. Lan et al. (2014) introduced discount\nweights on the PL terms which decreased exponentially, but,\nsince their exact method did not work for our task, we use\na stepped function where a static \u0011 <1 weight is given to\nthe top rank and the remaining 1 −\u0011is partitioned such that\nthere is an equal difference between the weights for con-\nsecutive ranks. In practice, \u0011acts similar to the temperature\nsmoothing hyper-parameter used in KD.\nThe PL loss does not allow for weak orderings. This is\na problem since the words within each individual order,\nOi, have no ordering; i.e., if Oi = {o1;:::;o S}, then the\nS items are of an unknown ordering. This creates a par-\ntitioned preference where all the partitions deﬁne a rank-\ning, Oi ≻ Oi+1, but the ranking within each ordering is\nunknown. One way of handling this would be to consider\nthe marginal distribution across all permutations for each\nordering. However, the likelihood would require calculating\na factorial number of terms (Ma et al. 2021). Instead, we\nconsider a lack of known preference as an inductive bias\nthat words in the weak orders are equally preferred. We\nenforce this by modifying the PL loss such that we revert\nback to regular softmax-cross entropy within the weak or-\nder for rankings i;:::;i + Sas Pi+S\ns=i log Zt;i −ws\nt , where\nZt;i = Zt −P\nj<i ewj\nt , which will optimize to a uniform\ndistribution within the ordering. This is a novel modiﬁcation\nto our knowledge.\n3 Related Work\nPrevious work combined N-gram information with neural\nLMs. Neubig and Dyer (2016); Bakhtin et al. (2018) used\ndynamic mixtures of N-grams and neural LMs. Neubig\nand Dyer (2016) found that their models performed worse\nthan regularly trained neural LMs except on low-frequency\nwords. Noraset, Demeter, and Downey (2018) used soft\nconstraints for text generation with an LSTM LM. They\ntried two sets of constraints: those to reduce repeated words\nduring generation and those to match Kneser-Ney bigram\nprobabilities during generation. Their method estimates the\nmarginal probability of the LSTM LM given some condi-\ntioning constraint and regularizes it using a KL-divergence\nagainst the constraint distribution. These marginal probabili-\nties are based on a pool of generated text that is periodically\nupdated during training. This can be seen as a form of LS\nagainst the actual generative distribution instead of the GT\ndistribution. They showed they could better match the dis-\ntribution of repeated tokens and bigram statistics against a\nbaseline on PTB. Yang, Wang, and Downey (2019) built on\nthis by making computation of the marginals more tractable.\nThey regularized the LM’s whole vocabulary against tri-\ngram statistics, which makes their methods very similar to\nKD against a trigram N-gram model. They also proposed a\nmethod for selecting useful N-grams based on an informa-\ntion criterion. This is important, as they need to do a single\nforward pass for every N-gram context they regularize. So\nselecting which contexts they use signiﬁcantly decreases the\ncomputational cost. They trained an LSTM LM and a tri-\ngram N-gram LM. On Wiki02, their method achieved 69\n10639\nPPL given a baseline of 76.\nVarious works implemented semantic label smoothing\nusing pre-trained probabilistic models (Elbayad, Besacier,\nand Verbeek 2018; Hahn and Choi 2019; Li et al. 2019;\nGhoshal et al. 2020; Liu, Shen, and Lapata 2021; Lukasik\net al. 2020b). These can use various pre-trained models\nsuch as embedding models like Fasttext (Joulin et al. 2017),\nlanguage representation models like BERT (Devlin et al.\n2019) or language models like GPT2 (Radford et al. 2019).\nLukasik et al. (2020b) smoothed over related sequences\nwhich were found using a pre-trained model and then re-\nﬁned using BLEU as a selection criterion. Wang et al. (2020)\nintroduced graduated label smoothing which uses a set of\nbinned label-smoothing values and the conﬁdence of a pre-\ntrained model to determine the bin to which each token be-\nlongs, improving BLEU and calibration for NMT.\nLi et al. (2019) used an auxiliary KL-divergence based\nloss on a semantically-aware or data-dependent Gaussian\nprior which is parameterized using cosine similarity from\nFasttext embeddings. They applied this over a range of con-\nditional language modelling tasks including NMT, text sum-\nmarization, and story telling. For story telling, their method\nlowered test PPL by ≈ 2-3 points. In general, LS can be\nframed as an entropy regularizer and many prior distribu-\ntions can be used (Pereyra et al. 2017; Meister, Salesky, and\nCotterell 2020).\nGhoshal et al. (2020) proposed a method which jointly\ntrains a model with a set of learned smoothing parameters.\nGiven Cclasses, one could implement semantic LS by learn-\ning a similarity C ×C matrix where each row provides\nsmoothing scores for a given target class. Instead, the au-\nthors approximated this with a C×kmatrix with k nC.\nWhile this is a form of semantic label smoothing, it is lim-\nited in that it is based only on the target class and not the full\ncontext. They applied it to semantic parsing and question\nanswering.\nWelleck et al. (2019) introduced an unlikelihood loss\nwhich penalizes words which should have low probability.\nWe make use of the concept of a branching set which is the\nwords that can follow a given context. The complementary\nset of the branching set could be used to select negative sam-\nples for the unlikelihood loss.\nBorn-again distillation or self-distillation is where the stu-\ndent and teacher have the same speciﬁcation (Furlanello\net al. 2018; Hahn and Choi 2019; Yuan et al. 2020).\nFurlanello et al. (2018) applied born-again KD to LMs and\nshowed that a student LSTM LM on PTB could outperform\nthe same teacher model. Yang et al. (2019) used a top-kaux-\niliary loss and trained image classiﬁcation models with a se-\nries of self-distillations. Tang et al. (2020) developed a top-k\nKD using the teacher’s probability for the top-k ranks and\ndistributing the remaining probability uniformly.\nReddi et al. (2021) recently introduced the use of top-k\nrank loss functions for rank-based KD using PL and pairwise\nhinge losses, outperforming traditional KD on a variety of\ntraditional ranking tasks. Other forms of structured KD can\nalso be applied to other NLP sequence tasks (Wang et al.\n2021).\n4 Experiments\nWe use the word-level Penn Treebank (PTB) and WikiText-\n02 (Wiki02) datasets and use ADW-LSTM (LSTM) and\nTransformer-XL (T-XL) students (Taylor, Marcus, and San-\ntorini 2003; Merity et al. 2017; Merity, Keskar, and Socher\n2018; Dai et al. 2019). Our LSTM uses 24.2M parameters on\nPTB and 33.5M on Wiki02, and our Transformer-XL uses\n35.8M on Wiki02.\nWe propose two hypotheses: that we can re-frame the\nproblem of label-smoothing as a form of rank-based knowl-\nedge distillation from a teacher which only provides rank\nground-truths and that we can derive the ranks directly from\na non-probabilistic N-gram teacher. In order to decouple\nthese two hypotheses, we ﬁrst use three different types of\npre-trained LMs to evaluate the PL loss when used for KD\nthen, provided that works, we apply the PL loss with an N-\ngram teacher model.\nWe choose GPT-2, BERT, and Born Again (BA) mod-\nels as teachers for different reasons. GPT-2 and BERT were\nchosen under the assumption that these large LMs will pro-\nduce better ranks than the N-grams. We tried both since the\nformer is an auto-regressive LM which only conditions on\npast context and thus matches our student models, while the\nlatter is an auto-encoding language representation model us-\ning both past and future contexts, allowing us to test if we\ncan distil future information. BA models are also consid-\nered as they will not present data-leakage problems, unlike\nBERT and GPT-2 which were pre-trained on a large amount\nof auxiliary data. The BA models are selected as the highest\nperforming CE baseline models.\nGPT-2 and BERT use sub-word vocabularies instead of\nthe standard word-level ones for PTB and Wiki02. We con-\nvert them by summing the sub-word hidden states to get\nwhole-words and ﬁne-tune them using CE.\nTable 1 shows the validation performance of the teacher\nmodels. The performance of these models is better than the\nbaseline CE models, which warrants their use as teacher\nmodels. BERT out-performs GPT-2 due to using surround-\ning context when predicting a word where GPT-2 is lim-\nited to only past context 1. We provide two other sanity\nchecks; ﬁrst, we provide examples of the top-k predictions\nfor BERT, GPT-2 and theN-grams in Appendix E and, sec-\nond, we plot frequency statistics for BERT and GPT-2 in\nAppendix D.\nThe N-grams used a set of contexts from 5 to 1 past to-\nkens concurrently with 4 to 0 future tokens and a pruning\ncut-off of 10, i.e. 5p-4f, 5p-3f, ::: , 1p-1f, 1p.\nWe compare a CE baseline to four other losses. The ﬁrst\nis a top-k KL-based KD. This has been modiﬁed in three\nways from traditional KD. First, we only use the top-kranks,\nas did Tang et al. (2020), although we forgo placing a uni-\nform distribution over all words not in the top-k . They re-\nported a test PPL of 60.85 using top-k KD on PTB with\nAWD-LSTM using a smaller student with 9.1M parameters.\nSecond, we post-process the predicted ranks by ﬂoating the\n1These PPL results are not directly comparable as BERT does\nnot form a valid distribution over sequences, hence using P-PPL.\nAlso, they use different auxiliary training data.\n10640\nData Model (P-)PPL A ⊆1 A ⊆2 A ⊆3 A ⊆5 A ⊆10\nPTB\nBERT 24-layer *1.205 0.975 0.986 0.989 0.991 0.993\nGTP-2 774M 22.810 0.437 0.525 0.573 0.630 0.701\nBA-LSTM 60.724 0.302 0.394 0.446 0.511 0.593\nWiki02\nBERT 24-layer *1.423 0.930 0.979 0.983 0.985 0.987\nGTP-2 774M 25.277 0.419 0.521 0.575 0.635 0.706\nBA-LSTM 68.437 0.296 0.394 0.448 0.512 0.592\nBA-T-XL 67.468 0.301 0.398 0.452 0.517 0.598\nTable 1: Perplexity (PPL) and accuracy in the top-k (A ⊆k ) for word-level ﬁned-tuned probabilistic teacher models BERT,\nGPT-2, Born-Again (BA) on the PTB and Wiki02 validation sets. *We report apseudo-PPL for BERT.\noriginal GTs to the top rank while keeping the top-k proba-\nbilities the same order i.e. make the GTs the most probable.\nInitial experimentation did not show a signiﬁcant change for\nKL-based KD and we believed this was an important mod-\niﬁcation for rank-based KD. Third, we cycle the interpola-\ntion value between the CE and KD terms, similar to Clark\net al. (2019); Jafari et al. (2021). For PL, we can forego dis-\ncounting (PL), use the teacher’s probabilities (PL-t), or use\nthe stepped function (PL-s). Note that the KL also uses the\nteacher’s probabilities. See Appendix A for further experi-\nmental details.\n5 Discussion\nThe results in Table 2 show we can treat language modelling\nas a structured prediction task using a rank-based KD and\nthat this can be done without the need of a probabilistic\nteacher. We discuss four main results.\nFirst, given the proper form of discounting, either form of\nKD improves PPL over the CE baseline.\nSecond, some form of rank-based KD often signiﬁcantly\noutperforms KL-based KD. This is true of 6/9 test-set\ngroups, with only a single experiment showing the reverse\n(this is a slight conceit, since we are making post-hoc com-\nparisons against a set of PL experiments instead of se-\nlecting the best form of discounting during the validation\nphase). Since, KL-based KD contains both the label order\nand weight information, it acts as a strong baseline. We be-\nlieve that the reason the rank-based KD often does better is\nbecause matching the teacher’s ranks is a less strict crite-\nrion than matching its probabilities. It may also be a better\ncriterion when evaluating PPL, since PL may allow for the\ntop ranks to all increase their probability together, so as long\nas the GT is in the top ranks, then PPL will decrease. Im-\nportantly, these results also indicate that performance of the\nN-grams will be due to their inherent value as a teacher and\nwill not be hindered by the loss function. The results of the\nPL and PL-s experiments support the claim that the ordering\nof the labels provides sufﬁcient semantic information to per-\nform KD and that we can forego label weights (with a caveat\naddressed later).\nThird, the N-grams act as a competitive teacher, with only\nGPT-2 consistently outperforming them. It seems plausible\nthat the N-grams would surpass the BA models, since both\ntypes of KD will restrict how much the BA students can de-\nviate from the baseline CE teacher. With BA and BERT, and\nLSTM students, the raw PL loss often does worse than the\nCE baseline. This is indicative of the PL loss failing when it\nis easier to optimize the lesser-preference ranks over the GT.\nThat the N-grams outperform BERT speaks more to\nBERT failing to act as a good teacher rather than the qual-\nity of the N-gram models. For BERT, it was important to\nuse the teacher probabilities (see the KL and PL-t vs PL and\nPL-s experiments). We believe that reliance on the teacher’s\nprobabilities is indicative of poor ranks where the teacher’s\nsharp probability discounts the lower ranks. A qualitative\nanalysis of the produced ranks shows that the future infor-\nmation can produce ranks that do not generalize well to a\nstudent evaluated on a past-context-only task. For example,\nBERT can see when a sentence ends and so produces ranks\nwhich do not try to continue the sentence where GPT-2 and\nthe N-grams will. Thus, the major caveat mentioned above\nis that label ordering is sufﬁcient, provided those ranks are\ninformative for the student’s task. The N-grams are inher-\nently sparse, since they only derive ranks when supported\nby the context. In the future, we would like to consider ways\nof dynamically selecting the top-k which may ﬁx this dis-\ncounting issue. This also continues the desire to model the\nbranching set, which are differently sized for each context.\nThe fourth result is how well GPT-2 does when paired\nwith the T-XL student, where we see a reduction of almost\n10 PPL points. Compare this with the LSTM student which\nonly sees a reduction of 4 points. Tang et al. (2020) sug-\ngested that certain failure cases in KD could be explained by\na capacity gap, where the teacher’s distribution is too com-\nplex to properly be distilled to the student model. We believe\nthat the GPT-2 results are the inverse of this phenomenon,\nwhere the student falls into a good regime to match the\nteacher instead of out of the regime. This makes sense\nsince, outside of the BA models, which may have limited\nvalue as teachers, GPT-2 and T-XL are the closest student-\nteacher pair in terms of speciﬁcation. However, since the\nT-XL model only has 7% more parameters than the LSTM\nmodel, we believe this should be considered as a capability\ngap where the change in performance between the LSTM\nand T-XL is more likely due to actual model differences in-\nstead of raw capacity. Additionally, the T-XL model using\nPL and no discounting was competitive to the discounted\nforms for BERT and BA and was the best performing model\n10641\nPTB Wiki02\nLSTM Student LSTM Student T-XL Student\nValidation Test Validation Test Validation Test\nTchr Loss PPL SEM PPL SEM PPL SEM PPL SEM PPL SEM PPL SEM\nCE 61.44 0.014 59.11 0.015 69.59 0.018 66.43 0.015 68.66 0.021 65.27 0.020\nGPT-2\nKL 57.43 0.012 55.46 0.010 65.41 0.008 62.79 0.007 59.43 0.009 56.70 0.008\nPL-t 57.28 0.014 55.17 0.013 65.47 0.015 62.69 0.014 59.32 0.007 56.58 0.007\nPL 58.16 0.012 56.25 0.012 65.45 0.011 62.74 0.011 58.61 0.007 55.94 0.007\nPL-s 57.63 0.008 55.67 0.007 65.34 0.008 62.59 0.008 59.48 0.008 56.76 0.007\nBERT\nKL 59.86\n0.018 57.73 0.017 68.63 0.010 65.51 0.009 67.79 0.010 64.16 0.009\nPL-t 59.38 0.009 57.20 0.009 68.04 0.010 64.99 0.009 67.66 0.012 64.18 0.012\nPL 62.23 0.011 60.32 0.011 71.98 0.009 68.68 0.008 67.78 0.009 64.28 0.010\nPL-s 61.13 0.010 59.11 0.010 68.55 0.007 65.53 0.008 67.72 0.008 64.26 0.008\nBA\nKL 60.14 0.011 57.63 0.010 68.90\n0.014 65.75 0.012 67.46 0.014 64.19 0.012\nPL-t 60.80 0.014 58.15 0.014 69.14 0.009 65.74 0.009 66.96 0.012 63.46 0.011\nPL 63.36 0.017 60.54 0.013 69.63 0.007 66.16 0.008 66.93 0.017 63.58 0.014\nPL-s 60.91 0.010 58.22 0.012 67.96 0.007 64.76 0.006 67.10 0.012 63.64 0.011\nN PL-s 59.66\n0.008 57.16 0.009 67.25 0.006 64.44 0.006 66.59 0.015 63.54 0.013\nTable 2: Average PPL (↓) with standard error (SEM) (n = 30) using an LSTM and T-XL student models and GPT-2, BERT,\nBorn again (BA) and Ngram (N) teachers (Tchr). Bolded PL experiments exceed the in-group KL baseline withp<: 001 using\na two-tailed t-test. Bolded KL experiments exceed the best in-group PL experiment with signiﬁcance. Underlined experiments\nare those which perform better than the N-gram model with signiﬁcance.\nfor GPT-2. This is in contrast to the non-discounted PL us-\ning the LSTM student which often did worse than the CE\nbaseline. This again speaks to a difference in capability be-\ntween the two types of student models. We wish to explore\nthis in the future.\nOur results are limited to English and might not general-\nize to other languages. We believe that training a LM via\nranking is language agnostic; however, our N-gram algo-\nrithm may not be as it assumes strong word-order informa-\ntion. Rank-based KD is a general idea which allows one to\nintegrate rule-based systems into a student model. We could\nimprove or generalize the N-grams by incorporating syn-\ntactic rules or enforcing co-references. Other uses may be to\nuse rank-based KD to integrate an LM into a NMT system\nby generating target-side ranks without modifying the NMT\nmodel. It may be useful for black-box KD, where one only\nhas access to the teacher’s hard predictions (Wang 2021) or\nto augment small-data environments.\nIn this work, we offer a novel perspective on language\nmodelling by considering it as a multi-label task and develop\nthe methods necessary to train such a model. We connect\nthe idea of LS to ranking via KD by showing that we can\nincorporate semantic similarity information from an ordered\nset of words into a LM without requiring associated proba-\nbilities. In our desire to forego using a pre-trained LM, we\nre-examined how N-gram statistics should be incorporated\ninto a neural LM. We ﬁnd that this can be done using a sim-\nple method which just considers N-gram contexts, and that\nthis surpasses the CE baseline and is comparable to using\nBA and BERT as teachers. We also ﬁnd that, for language\nmodelling, a rank-based KD is often a superior method for\nKD.\nAcknowledgements\nWe acknowledge the support and resources provided by the\nProvince of Ontario, the companies sponsoring the Vec-\ntor Institute, and the Natural Sciences and Engineering Re-\nsearch Council of Canada (NSERC).\nReferences\nBakhtin, A.; Szlam, A.; Ranzato, M.; and Grave, E. 2018.\nLightweight adaptive mixture of neural and n-gram lan-\nguage models. arXiv preprint arXiv:1804.07705.\nBengio, Y .; Ducharme, R.; Vincent, P.; and Jauvin, C. 2003.\nA neural probabilistic language model. Journal of machine\nlearning research, 3(Feb): 1137–1155.\nCao, Z.; Qin, T.; Liu, T.-Y .; Tsai, M.-F.; and Li, H. 2007.\nLearning to rank: from pairwise approach to listwise ap-\nproach. In Proceedings of the 24th international conference\non Machine learning, 129–136.\nClark, K.; Luong, M.-T.; Khandelwal, U.; Manning, C. D.;\nand Le, Q. V . 2019. BAM! Born-Again Multi-Task Net-\nworks for Natural Language Understanding. In Proceedings\nof the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 5931–5937. Florence, Italy: Association\nfor Computational Linguistics.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\n10642\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978–2988. Florence, Italy: Associ-\nation for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nElbayad, M.; Besacier, L.; and Verbeek, J. 2018. Token-\nlevel and sequence-level loss smoothing for RNN language\nmodels. In ACL-56th Annual Meeting of the Association for\nComputational Linguistics, 2094–2103. ACL.\nFurlanello, T.; Lipton, Z.; Tschannen, M.; Itti, L.; and\nAnandkumar, A. 2018. Born Again Neural Networks. In In-\nternational Conference on Machine Learning, 1607–1616.\nGhoshal, A.; Chen, X.; Gupta, S.; Zettlemoyer, L.; and\nMehdad, Y . 2020. Learning Better Structured Representa-\ntions Using Low-rank Adaptive Label Smoothing. In Inter-\nnational Conference on Learning Representations.\nGoodman, J. T. 2001. A bit of progress in language model-\ning. Computer Speech & Language, 15(4): 403–434.\nHahn, S.; and Choi, H. 2019. Self-Knowledge Distilla-\ntion in Natural Language Processing. In Proceedings of\nthe International Conference on Recent Advances in Nat-\nural Language Processing (RANLP 2019), 423–430. Varna,\nBulgaria: INCOMA Ltd.\nHarris, Z. S. 1954. Distributional structure. Word, 10(2-3):\n146–162.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. stat, 1050: 9.\nJafari, A.; Rezagholizadeh, M.; Sharma, P.; and Ghodsi, A.\n2021. Annealing Knowledge Distillation. In Proceedings of\nthe 16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume, 2493–\n2504. Online: Association for Computational Linguistics.\nJoulin, A.; Grave, ´E.; Bojanowski, P.; and Mikolov, T. 2017.\nBag of Tricks for Efﬁcient Text Classiﬁcation. In Proceed-\nings of the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume 2, Short\nPapers, 427–431.\nJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y . 2016. Exploring the limits of language modeling.\narXiv preprint arXiv:1602.02410.\nLan, Y .; Zhu, Y .; Guo, J.; Niu, S.; and Cheng, X. 2014.\nPosition-Aware ListMLE: A Sequential Learning Process\nfor Ranking. In UAI, 449–458.\nLi, Z.; Wang, R.; Chen, K.; Utiyama, M.; Sumita, E.; Zhang,\nZ.; and Zhao, H. 2019. Data-dependent gaussian prior ob-\njective for language generation. InInternational Conference\non Learning Representations.\nLiu, Y .; Shen, S.; and Lapata, M. 2021. Noisy Self-\nKnowledge Distillation for Text Summarization. In Pro-\nceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 692–703. Online: Associa-\ntion for Computational Linguistics.\nLukasik, M.; Bhojanapalli, S.; Menon, A.; and Kumar, S.\n2020a. Does label smoothing mitigate label noise? In\nIII, H. D.; and Singh, A., eds., Proceedings of the 37th In-\nternational Conference on Machine Learning, volume 119\nof Proceedings of Machine Learning Research, 6448–6458.\nPMLR.\nLukasik, M.; Jain, H.; Menon, A.; Kim, S.; Bhojanapalli,\nS.; Yu, F.; and Kumar, S. 2020b. Semantic Label Smooth-\ning for Sequence to Sequence Problems. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 4992–4998. Online: Associa-\ntion for Computational Linguistics.\nMa, J.; Yi, X.; Tang, W.; Zhao, Z.; Hong, L.; Chi, E.; and\nMei, Q. 2021. Learning-to-Rank with Partitioned Pref-\nerence: Fast Estimation for the Plackett-Luce Model. In\nBanerjee, A.; and Fukumizu, K., eds.,The 24th International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS\n2021, April 13-15, 2021, Virtual Event, volume 130 of Pro-\nceedings of Machine Learning Research, 928–936. PMLR.\nMeister, C.; Salesky, E.; and Cotterell, R. 2020. Generalized\nEntropy Regularization or: There’s Nothing Special about\nLabel Smoothing. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, 6870–\n6886. Online: Association for Computational Linguistics.\nMerity, S.; Keskar, N. S.; and Socher, R. 2018. An Analysis\nof Neural Language Modeling at Multiple Scales. CoRR,\nabs/1803.08240.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer Sentinel Mixture Models. In 5th International Con-\nference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nMikolov, T. 2012.Statistical language models based on neu-\nral networks. Ph.d. thesis, Brno University of Technology,\nFaculty of Information Technology.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013. Distributed representations of words and\nphrases and their compositionality. Advances in neural in-\nformation processing systems, 26: 3111–3119.\nM¨uller, R.; Kornblith, S.; and Hinton, G. E. 2019. When\ndoes label smoothing help? In Advances in Neural Informa-\ntion Processing Systems, 4694–4703.\nNeubig, G.; and Dyer, C. 2016. Generalizing and Hybridiz-\ning Count-based and Neural Language Models. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natu-\nral Language Processing, 1163–1172.\nNoraset, T.; Demeter, D.; and Downey, D. 2018. Controlling\nGlobal Statistics in Recurrent Neural Network Text Genera-\ntion. In AAAI, 5333–5341.\nPereyra, G.; Tucker, G.; Chorowski, J.; Kaiser, L.; and Hin-\nton, G. E. 2017. Regularizing Neural Networks by Pe-\nnalizing Conﬁdent Output Distributions. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Workshop Track Pro-\nceedings. OpenReview.net.\n10643\nPlackett, R. L. 1975. The analysis of permutations. Journal\nof the Royal Statistical Society: Series C (Applied Statistics),\n24(2): 193–202.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI blog 1.8, 9.\nReddi, S.; Kumar Pasumarthi, R.; Menon, A.; Singh Rawat,\nA.; Yu, F.; Kim, S.; Veit, A.; and Kumar, S. 2021. RankDis-\ntil: Knowledge Distillation for Ranking. In Banerjee, A.; and\nFukumizu, K., eds., Proceedings of The 24th International\nConference on Artiﬁcial Intelligence and Statistics, volume\n130 of Proceedings of Machine Learning Research, 2368–\n2376. PMLR.\nSzegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna,\nZ. 2016. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2818–2826.\nTang, J.; Shivanna, R.; Zhao, Z.; Lin, D.; Singh, A.; Chi,\nE. H.; and Jain, S. 2020. Understanding and Improving\nKnowledge Distillation. arXiv preprint arXiv:2002.03532.\nTaylor, A.; Marcus, M.; and Santorini, B. 2003. The Penn\ntreebank: An overview. In Treebanks, 5–22. Springer.\nWang, L.; and Yoon, K.-J. 2021. Knowledge distillation and\nstudent-teacher learning for visual intelligence: A review\nand new outlooks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence.\nWang, S.; Tu, Z.; Shi, S.; and Liu, Y . 2020. On the Infer-\nence Calibration of Neural Machine Translation. In Pro-\nceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 3070–3079. Online: Association\nfor Computational Linguistics.\nWang, X.; Jiang, Y .; Yan, Z.; Jia, Z.; Bach, N.; Wang, T.;\nHuang, Z.; Huang, F.; and Tu, K. 2021. Structural Knowl-\nedge Distillation: Tractably Distilling Information for Struc-\ntured Predictor. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), 550–564. Online: As-\nsociation for Computational Linguistics.\nWang, Z. 2021. Zero-Shot Knowledge Distillation from\na Decision-Based Black-Box Model. In Meila, M.; and\nZhang, T., eds., Proceedings of the 38th International Con-\nference on Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, 10675–10685. PMLR.\nWelleck, S.; Kulikov, I.; Roller, S.; Dinan, E.; Cho, K.; and\nWeston, J. 2019. Neural Text Generation With Unlikelihood\nTraining. In International Conference on Learning Repre-\nsentations.\nXia, F.; Liu, T.-Y .; and Li, H. 2009. Statistical consistency of\ntop-k ranking. Advances in Neural Information Processing\nSystems, 22: 2098–2106.\nYang, C.; Xie, L.; Qiao, S.; and Yuille, A. L. 2019. Train-\ning deep neural networks in generations: A more tolerant\nteacher educates better students. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 33, 5628–\n5635.\nYang, Y .; Wang, J.-P.; and Downey, D. 2019. Using Large\nCorpus N-gram Statistics to Improve Recurrent Neural Lan-\nguage Models. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), 3268–3273.\nYuan, L.; Tay, F. E.; Li, G.; Wang, T.; and Feng, J. 2020. Re-\nvisiting Knowledge Distillation via Label Smoothing Regu-\nlarization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 3903–3911.\nZhao, Z.; Liu, T.; Li, S.; Li, B.; and Du, X. 2017.\nNgram2vec: Learning Improved Word Representations from\nNgram Co-occurrence Statistics. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Lan-\nguage Processing, 244–253. Copenhagen, Denmark: Asso-\nciation for Computational Linguistics.\n10644",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.7198328971862793
    },
    {
      "name": "Computer science",
      "score": 0.701292097568512
    },
    {
      "name": "Language model",
      "score": 0.6916913986206055
    },
    {
      "name": "Transformer",
      "score": 0.5916815400123596
    },
    {
      "name": "Natural language processing",
      "score": 0.5825707912445068
    },
    {
      "name": "Artificial intelligence",
      "score": 0.561337411403656
    },
    {
      "name": "Probabilistic logic",
      "score": 0.48926088213920593
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.46394655108451843
    },
    {
      "name": "Machine learning",
      "score": 0.4530111253261566
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.441652774810791
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4317563772201538
    },
    {
      "name": "Mathematics",
      "score": 0.1926746368408203
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210127509",
      "name": "Vector Institute",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I1292318990",
      "name": "Nuance Communications (Austria)",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ],
  "cited_by": 3
}