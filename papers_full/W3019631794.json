{
  "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting",
  "url": "https://openalex.org/W3019631794",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2632827109",
      "name": "Chen San-Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224908025",
      "name": "Hou, Yutai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349293236",
      "name": "Cui Yiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250687399",
      "name": "Che, Wanxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966340437",
      "name": "Liu Ting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223953452",
      "name": "Yu, Xiangzhan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2473930607",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2964088867",
    "https://openalex.org/W2774373350",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W2962863357",
    "https://openalex.org/W2924984511",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2113839990",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W2964186069",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2989430018",
    "https://openalex.org/W3013325675",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W2963072899",
    "https://openalex.org/W2994415862",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963850662",
    "https://openalex.org/W2974731134",
    "https://openalex.org/W2474280151",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2963588172",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2786446225",
    "https://openalex.org/W2962707369",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3003289092",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2962724315",
    "https://openalex.org/W2963813679",
    "https://openalex.org/W2964352358",
    "https://openalex.org/W2737492962",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2945383715",
    "https://openalex.org/W3127504686",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3103800629",
    "https://openalex.org/W1581755290",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2963788399",
    "https://openalex.org/W2939911019",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W3140968660",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2426267443",
    "https://openalex.org/W2605043629",
    "https://openalex.org/W2979736514",
    "https://openalex.org/W2974317861",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.",
  "full_text": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with\nLess Forgetting\nSanyuan Chen1, Yutai Hou1, Yiming Cui1,2, Wanxiang Che1, Ting Liu1, Xiangzhan Yu1\n1School of Computer Science and Technology, Harbin Institute of Technology, China\n2Joint Laboratory of HIT and iFLYTEK Research (HFL), Beijing, China\n{sychen, ythou, ymcui, car, tliu}@ir.hit.edu.cn, yxz@hit.edu.cn\nAbstract\nDeep pretrained language models have\nachieved great success in the way of pretrain-\ning ﬁrst and then ﬁne-tuning. But such a\nsequential transfer learning paradigm often\nconfronts the catastrophic forgetting problem\nand leads to sub-optimal performance. To\nﬁne-tune with less forgetting, we propose a\nrecall and learn mechanism, which adopts\nthe idea of multi-task learning and jointly\nlearns pretraining tasks and downstream\ntasks. Speciﬁcally, we propose a Pretrain-\ning Simulation mechanism to recall the\nknowledge from pretraining tasks without\ndata, and an Objective Shifting mechanism\nto focus the learning on downstream tasks\ngradually. Experiments show that our method\nachieves state-of-the-art performance on the\nGLUE benchmark. Our method also enables\nBERT-base to achieve better performance than\ndirectly ﬁne-tuning of BERT-large. Further,\nwe provide the open-source R ECADAM\noptimizer, which integrates the proposed\nmechanisms into Adam optimizer, to facility\nthe NLP community.1\n1 Introduction\nDeep Pretrained Language Models (LMs), such\nas ELMo (Peters et al., 2018) and BERT (Devlin\net al., 2019), have signiﬁcantly altered the land-\nscape of Natural Language Processing (NLP) and\na wide range of NLP tasks has been promoted by\nthese pretrained language models. These successes\nare mainly achieved through Sequential Transfer\nLearning (Ruder, 2019): pretrain a language model\non large-scale unlabeled data and then adapt it to\ndownstream tasks. The adaptation step is usually\nconducted in two manners: ﬁne-tuning or freez-\ning pretrained weights. In practice, ﬁne-tuning is\nadopted more widely due to its ﬂexibility (Phang\net al., 2018; Lan et al., 2019; Peters et al., 2019).\n1https://github.com/Sanyuan-Chen/RecAdam\nDespite the great success, sequential transfer\nlearning of deep pretrained LMs tends to suffer\nfrom catastrophic forgetting during the adaptation\nstep. Catastrophic forgetting is a common prob-\nlem for sequential transfer learning, and it happens\nwhen a model forgets previously learned knowl-\nedge and overﬁts to target domains (McCloskey\nand Cohen, 1989; Kirkpatrick et al., 2017). To rem-\nedy the catastrophic forgetting in transferring deep\npretrained LMs, existing efforts mainly explore\nﬁne-tuning tricks to forget less. ULMFiT (Howard\nand Ruder, 2018) introduced discriminative ﬁne-\ntuning, slanted triangular learning rates, and grad-\nual unfreezing for LMs ﬁne-tuning. Lee et al.\n(2019) reduced forgetting in BERT ﬁne-tuning by\nrandomly mixing pretrained parameters to a down-\nstream model in a dropout-style.\nInstead of learning pretraining tasks and down-\nstream tasks in sequence, Multi-task Learning\nlearns both of them simultaneously, thus can in-\nherently avoid the catastrophic forgetting prob-\nlem. Xue et al. (2019) tackled forgetting in au-\ntomatic speech recognition by jointly training the\nmodel with previous and target tasks. Kirkpatrick\net al. (2017) proposed Elastic Weight Consolidation\n(EWC) to overcome catastrophic forgetting when\ncontinuous learning multiple tasks by adopting the\nmulti-task learning paradigm. EWC regularizes\nnew task training by constraining the parameters\nwhich are important for previous tasks and adapt\nmore aggressively on other parameters. Thanks\nto the appealing effects on catastrophic forgetting,\nEWC has been widely applied in various domains,\nsuch as game playing (Ribeiro et al., 2019), neural\nmachine translation (Thompson et al., 2019) and\nreading comprehension (Xu et al., 2019).\nHowever, these multi-task learning methods can-\nnot be directly applied to the sequential transferring\nregime of deep pretrained LMs. Firstly, multi-task\nlearning methods require to use data of pretrain-\narXiv:2004.12651v1  [cs.CL]  27 Apr 2020\ning tasks during adaptation, but pretraining data\nof LMs is often inaccessible or too large for the\nadaptation. Secondly, we only care about the per-\nformance of the downstream task, while multi-task\nlearning also aims to promote performance on pre-\ntraining tasks.\nIn this paper, we propose a recall and learn\nmechanism to cope with the forgetting problem\nof ﬁne-tuning the deep pretrained LMs. To achieve\nthis, we take advantage of multi-task learning by\nadopting LMs pretraining as an auxiliary learning\ntask during ﬁne-tuning. Speciﬁcally, we propose\ntwo mechanisms for the two challenges mentioned\nabove, respectively. As for the challenge of data\nobstacles, we propose the Pretraining Simulation\nto achieve multi-task learning without accessing\nto pretraining data. It helps the model to recall\npreviously learned knowledge by simulating the\npretraining objective using only pretrained param-\neters. As for the challenge of learning objective\ndifference, we propose theObjective Shifting to bal-\nance new task learning and pretrained knowledge\nrecalling. It allows the model to focus gradually\non the new task by shifting the multi-task learning\nobjective to the new task learning.\nWe also provide Recall Adam (RECADAM) op-\ntimizer to integrate the recall and learn mechanism\ninto Adam optimizer (Kingma and Ba, 2015). We\nrelease the source code of the RECADAM opti-\nmizer implemented in Pytorch. It is easy to use and\ncan facilitate the NLP community for better ﬁne-\ntuning of deep pretrained LMs. Experiments on\nGLUE benchmark with the BERT-base model show\nthat the proposed method can signiﬁcantly outper-\nform the vanilla ﬁne-tuning method. Our method\nwith the BERT-base model can even achieve bet-\nter results than directly ﬁne-tuning the BERT-large\nmodel. In addition, thanks to the effectiveness\nof pretrained knowledge recalling, we gain better\nperformance by initializing model with random pa-\nrameters rather than pretrained parameters. Finally,\nwe achieve state-of-the-art performance on GLUE\nbenchmark with the ALBERT-xxlarge model.\nOur contributions can be summarized as follows:\n(1) We propose to tackle the catastrophic forgetting\nproblem of ﬁne-tuning the deep pretrained LMs by\nadopting the idea of multi-task learning and obtain\nstate-of-the-art results on GLUE benchmark. (2)\nWe propose Pretraining Simulation and Objective\nShifting mechanisms to achieve multi-task ﬁne-\ntuning without data of pretraining tasks. (3) We\nprovide the open-source RECADAM optimizer to\nfacilitate deep pretrained LMs ﬁne-tuning with less\nforgetting.\n2 Background\nIn this section, we introduce two transfer learning\nsettings: sequential transfer learning and multi-task\nlearning. They both aim to improve the learning\nperformance by transferring knowledge across mul-\ntiple tasks, but apply to different scenarios.\n2.1 Sequential Transfer Learning\nSequential transfer learninglearns source tasks and\ntarget tasks in sequence, and transfers knowledge\nfrom source tasks to improve the models’ perfor-\nmance on target tasks.\nIt typically consists of two stages: pretraining\nand adaptation. During pretraining, the model\nis trained on source tasks with the loss function\nLossS. During adaptation, the pretrained model is\nfurther trained on target tasks with the loss func-\ntion LossT. The standard adaptation methods in-\ncludes ﬁne-tuning and feature extraction. Fine-\ntuning updates all the parameters of the pretrained\nmodel, while feature extraction regards the pre-\ntrained model as a feature extractor and keeps it\nﬁxed during the adaptation phase.\nSequential transfer learning has been widely\nused recently, and the released deep pretrained\nLMs have achieved great successes on various NLP\ntasks (Peters et al., 2018; Devlin et al., 2019; Lan\net al., 2019). While the adaptation of the deep pre-\ntrained LMs is very efﬁcient, it tends to suffer from\ncatastrophic forgetting, where the model forgets\npreviously learned knowledge from source tasks\nwhen learning new knowledge from target tasks.\n2.2 Multi-task Learning\nMulti-task Learning learns multiple tasks simulta-\nneously, and improves the models’ performance\non all of them by sharing knowledge across these\ntasks (Caruana, 1997; Ruder, 2017).\nUnder the multi-task learning paradigm, the\nmodel is trained on both source tasks and target\ntasks with the loss function:\nLossM = λLossT + (1−λ)LossS (1)\nwhere λ ∈(0,1) is a hyperparameter balancing\nthese two tasks. It can inherently avoid catastrophic\nforgetting problem because the loss on source tasks\nLossS is always part of the optimization objective.\nTo overcome catastrophic forgetting problem\n(discussed in §2.1), can we apply the idea of multi-\ntask learning to the adaptation of the deep pre-\ntrained LMs? There are two challenges in practice:\n1) We cannot get access to the pretraining data to\ncalculate LossS during adaptation.\n2) The optimization objective of adaptation is\nLossT, while multi-task learning aims to op-\ntimize LossM, i.e., the weighted sum of LossT\nand LossS.\n3 Methodology\nIn this section, we introduce Pretraining Simulation\n(§3.1) and Objective Shifting (§3.2) to overcome\nthe two challenges (discussed in §2.2) respectively.\nPretraining Simulation allows the model to learn\nsource tasks without pretraining data, and Objec-\ntive Shifting allows the model to focus on target\ntasks gradually. We also introduce the RECADAM\noptimizer (§3.3) to integrate these two mechanisms\ninto the common-used Adam optimizer.\n3.1 Pretraining Simulation\nAs for the ﬁrst challenge that pretraining data is\nunavailable, we introduce Pretraining Simulation\nto approximate the optimization objective of source\ntasks as a quadratic penalty, which keeps the model\nparameters close to the pretrained parameters.\nFollowing Elastic Weight Consolidation (EWC;\nKirkpatrick et al. 2017; Husz´ar 2017), we approx-\nimate the optimization objective of source tasks\nwith Laplaces Method and independent assumption\namong the model parameters. Since EWC requires\npretraining data, we further introduce a stronger\nindependent assumption and derive a quadratic\npenalty, which is independent with the pretraining\ndata. We introduce the detailed derivation process\nas follows.\nFrom the probabilistic perspective, the learning\nobjective on the source tasks LossS would be opti-\nmizing the negative log posterior probability of the\nmodel parameters θgiven data of source tasks DS:\nLossS = −logp(θ|DS)\nThe pretrained parameters θ∗can be assumed\nas a local minimum of the parameter space, and it\nsatisﬁes the equation:\nθ∗= arg minθ{−logp(θ|DS)}\nDue to the intractability, the optimization objec-\ntive −log p(θ|DS) is locally approximated with\nthe Laplaces Method (MacKay, 2003):\n−logp(θ|DS) ≈−logp(θ∗|DS)\n+ 1\n2(θ−θ∗)⊤H(θ∗)(θ−θ∗)\nwhere H(θ∗) is the Hessian matrix of the opti-\nmization objective w.r.t. θ and evaluated at θ∗.\n−log p(θ∗|DS) is a constant term w.r.t. θ, and\nit can be ignored during optimization.\nSince the pretrained model convergences on the\nsource tasks, H(θ∗) can be approximated with\nthe empirical Fisher information matrix F(θ∗)\n(Martens, 2014):\nF(θ∗) =Ex∼DS[∇θlogpθ(x)∇θlogpθ(x)⊤|θ=θ∗]\nH(θ∗) ≈NF(θ∗) +Hprior(θ∗)\nwhere N is the number of i. i. d. observations in\nDS, Hprior(θ∗) is the Hessian matrix of the nega-\ntive log prior probability −log p(θ).\nBecause of the computational intractability,\nEWC approximate H(θ∗) by using the diagonal\nof F(θ∗) and ignoring the prior Hessian matrix\nHprior(θ∗):\n(θ−θ∗)⊤H(θ∗)(θ−θ∗) ≈N∑\niFi(θi−θ∗i)2\nwhere Fi is the corresponding diagonal Fisher in-\nformation value of the model parameter θi.\nSince the pretraining data is unavailable, we fur-\nther approximate H(θ∗) with a stronger assump-\ntion that each diagonal Fisher information value Fi\nis independent of the corresponding parameter θi:\n(θ−θ∗)⊤H(θ∗)(θ−θ∗) ≈NF∑\ni(θi−θ∗i)2\nThe ﬁnal approximated optimization objective\nof the source tasks is the quadratic penalty between\nthe model parameters and the pretrained parame-\nters:\nLossS=−logp(θ|DS)\n≈1\n2(θ−θ∗)⊤H(θ∗)(θ−θ∗)\n≈1\n2(θ−θ∗)⊤(NF(θ∗) +Hprior(θ∗))(θ−θ∗)\n≈1\n2N∑\ni\nFi(θi−θ∗i)2\n≈1\n2NF∑\ni\n(θi−θ∗i)2\n= 1\n2γ∑\ni\n(θi−θ∗i)2\nwhere 1\n2 γis the coefﬁcient of the quadratic penalty.\nt0\nTimestep\n0.0\n0.5\n1.0(t)\n0 < k <\nk  \nk  0\nFigure 1: Objective Shifting: we replace the coefﬁcient\nλ with the annealing function λ(t). Fine-tuning and\nmulti-task learning can be regarded as the special cases\n(k→∞ and k→0) of our method.\n3.2 Objective Shifting\nAs for the second challenge that the optimization\nobjective of multi-task learning is inconsistent with\nadaptation, we introduce Objective Shifting to al-\nlow the objective function to gradually shift to\nLossT with the annealing coefﬁcient.\nWe replace the coefﬁcient λin the optimization\nobjective of multi-task learning (as shown in Eq. 1)\nwith the annealing function λ(t), where trefers to\nthe update timesteps during ﬁne-tuning. The loss\nfunction of our method is set to multi-task learning\nwith annealing coefﬁcient:\nLoss= λ(t)LossT + (1−λ(t))LossS\nSpeciﬁcally, to better balance the multi-task\nlearning and ﬁne-tuning, λ(t) is calculated as the\nsigmoid annealing function (Bowman et al., 2016):\nλ(t) = 1\n1 + exp(−k·(t−t0))\nwhere kand t0 are the hyperparameters controlling\nthe annealing rate and timesteps.\nAs shown in Figure 1, at the beginning of the\ntraining process, the model mainly learns gen-\neral knowledge by focusing more on pretraining\ntasks. As training progress, the model gradually fo-\ncuses on target tasks and learns more target-speciﬁc\nknowledge while recalling the knowledge of pre-\ntraining tasks. At the end of the training process,\nthe model completely focuses on target tasks, and\nthe ﬁnal optimization objective is LossT.\nFine-tuning and multi-task learning can be re-\ngarded as special cases of our method. When\nk→∞, our method can be regarded as ﬁne-tuning.\nThe model ﬁrstly gets pretrained on source tasks\nwith the LossS, then learns the target tasks with\nthe LossT. When k →0, λ(t) is a constant func-\ntion, then our method can be regarded as the multi-\ntask learning. The model learns source tasks and\ntarget tasks simultaneously with the loss function\n1\n2 (LossT + LossS).\n3.3 RecAdam Optimizer\nAdam optimizer (Kingma and Ba, 2015) is com-\nmonly used for ﬁne-tuning the deep pretrained\nLMs. We introduce Recall Adam ( RECADAM)\noptimizer to integrate the quadratic penalty and the\nannealing coefﬁcient, which are the core factors of\nthe Pretraining Simulation ( §3.1) and Objective\nShifting (§3.2) mechanisms respectively, by de-\ncoupling them from the gradient updates in Adam\noptimizer.\nLoshchilov and Hutter (2019) observed that L2\nregularization and weight decay are not identical\nfor adaptive gradient algorithms such as Adam, and\nconﬁrmed the proposed AdamW optimizer based\non decoupled weight decay could substantially im-\nprove Adam’s performance in both theoretical and\nempirical way.\nSimilarly, it is necessary to decouple the\nquadratic penalty and the annealing coefﬁcient\nwhen ﬁne-tuning the pretrained LMs with Adam\noptimizer. Otherwise, both the quadratic penalty\nand annealing coefﬁcient would be adapted by the\ngradient update rules, resulting in different magni-\ntudes of the quadratic penalty among the model’s\nweights.\nThe comparison between Adam and\nRECADAM are shown in Algorithm 1, where\nSetScheduleMultiplier(t) (Line 11) refers to the\nprocedure (e.g. warm-up technique) to get the\nscaling factor of the step size.\nLine 6 of Algorithm 1 shows how we implement\nthe quadratic penalty and annealing coefﬁcient with\nthe vanilla Adam optimizer. The weighted sum\nof the gradient of target task objective function\n∇f(θ) and the gradient of the quadratic penalty\nγ(θ−θ∗) get adapted by the gradient update rules,\nwhich derives to inequivalent magnitudes of the\nquadratic penalty among the model’s weights, e.g.\nthe weights that tend to have larger gradients∇f(θ)\nwould have the larger second moment v and be\npenalized by the relatively smaller amount than\nother weights.\nAlgorithm 1 Adam and RecAdam\n1: given initial learning rate α ∈R, momentum factors β1 = 0.9,β2 = 0.999,ϵ = 10−8, pretrained parameter vector\nθ∗ ∈Rn, coefﬁcient of quadratic penalty γ ∈R, annealing coefﬁcient in objective function λ(t) = 1/(1 + exp(−k·(t−\nt0)),k ∈R,t0 ∈N\n2: initialize timestep t←0, parameter vector θt=0 ∈Rn, ﬁrst moment vector mt=0 ←0, second moment vector vt=0 ←0,\nschedule multiplier ηt=0 ∈R\n3: repeat\n4: t←t+ 1\n5: ∇ft(θt−1) ←SelectBatch(θt−1) ⊿ select batch and return the corresponding gradient\n6: gt ← λ(t) ∇ft(θt−1) +(1 −λ(t))γ(θt−1 −θ∗)\n7: mt ←β1mt−1 + (1−β1)gt ⊿ here and below all operations are element-wise\n8: vt ←β2vt−1 + (1−β2)g2\nt\n9: ˆmt ←mt/(1 −βt\n1) ⊿ β1 is taken to the power of t\n10: ˆvt ←vt/(1 −βt\n2) ⊿ β2 is taken to the power of t\n11: ηt ←SetScheduleMultiplier(t) ⊿ can be ﬁxed, decay, or also be used for warm restarts\n12: θt ←θt−1 −ηt\n(\nλ(t) αˆmt/(√ˆvt + ϵ) +(1 −λ(t))γ(θt−1 −θ∗)\n)\n13: until stopping criterion is met\n14: return optimized parameters θt\nWith RECADAM optimizer, we decouple the gra-\ndient of the quadratic penalty γ(θ−θ∗) and the an-\nnealing coefﬁcient λ(t) in Line 12 of Algorithm 1.\nIn this way, only the gradient of target task objec-\ntive function ∇f(θ) get adapted during the opti-\nmization steps, and all the weights of the training\nmodel would be more effectively penalized with\nthe same rate (1 −λ(t))γ.\nSince the RECADAM optimizer is only one line\nmodiﬁcation from Adam optimizer, it can be eas-\nily used by feeding the additional parameters, in-\ncluding the pretrained parameters and a few hy-\nperparameters of the Pretraining Simulation and\nObjective Shifting mechanisms.\n4 Experiments\n4.1 Setup\nModel We conduct the experiments with the\ndeep pretrained language model BERT-base (De-\nvlin et al., 2019) and ALBERT-xxlarge (Lan et al.,\n2019).\nBERT is a deep bi-directional pretrained model\nbased on multi-layer Transformer encoders. It is\npretrained on the large-scale corpus with two unsu-\npervised tasks: Masked LM and Next Sentence Pre-\ndiction, and has achieved signiﬁcant improvements\non a wide range of NLP tasks. We use the BERT-\nbase model with 12 layers, 12 attention heads and\n768 hidden dimensions.\nALBERT is the latest deep pretrained LM that\nachieves the state-of-the-art performance on several\nbenchmarks. It improves BERT by the parameter\nreduction techniques and self-supervised loss for\nsentence-order prediction (SOP). The ALBERT-\nxxlarge model with 12 layers, 64 attention heads,\n128 embedding dimension and 4,096 hidden dimen-\nsions is the current state-of-the-art model released\nby Lan et al. (2019).\nData We evaluate our methods on the Gen-\neral Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019).\nGLUE is a well-known benchmark focused on\nevaluating model capabilities for natural language\nunderstanding. It includes 9 tasks: Corpus of\nLinguistic Acceptability (CoLA; Warstadt et al.\n2019), Stanford Sentiment Treebank (SST; Socher\net al. 2013), Microsoft Research Paraphrase Corpus\n(MRPC; Dolan and Brockett 2005), Semantic Tex-\ntual Similarity Benchmark (STS; Cer et al. 2017),\nQuora Question Pairs (QQP; Shankar Iyer and\nCsernai. January 2017), Multi-Genre NLI (MNLI;\nWilliams et al. 2018), Question NLI (QNLI; Ra-\njpurkar et al. 2016), Recognizing Textual Entail-\nment (RTE; Dagan et al. 2005; Roy Bar-Haim\nand Szpektor. 2006; Giampiccolo et al. 2007; Ben-\ntivogli et al. 2009) and Winograd NLI (WNLI;\nLevesque et al. 2012).\nFollowing previous works (Yang et al., 2019; Liu\net al., 2019; Lan et al., 2019), we report our single-\ntask single-model results on the dev set of 8 GLUE\ntasks, excluding the problematic WNLI dataset. 2\nWe report Pearson correlations for STS, Matthew’s\ncorrelations for CoLA, the match condition (MNLI-\nm) for MNLI, and accuracy scores for other tasks.\nImplementation As discussed in §3.3, we im-\nplement the Pretraining Simulation and Objective\n2https://gluebenchmark.com/faq\nShifting techniques with the proposed RECADAM\noptimizer. Our methods use random initialization\nbecause of the pretrained knowledge recalling im-\nplementation, while vanilla ﬁne-tuning initializes\nthe ﬁne-tuning model with pretrained parameters.\nWe ﬁne-tune BERT-base and ALBERT-xxlarge\nmodel with the same hyperparameters following\nDevlin et al. (2019) and Lan et al. (2019), except\nfor the maximum sequence length which we set\nto 128 rather than 512. For the BERT-base model,\nwe set the learning rate to 2e-5 and select the train-\ning step to make sure the convergence of vanilla\nﬁne-tuning on each target task. We note that we\nﬁne-tune for RTE, STS, and MRPC directly using\nthe pretrained LM while the previous works are\nusing an MNLI checkpoint for further performance\nimprovement. As for the hyperparameters of our\nmethods, we set γin the quadratic penalty to 5,000,\nand select the best t0 and k in {100, 250, 500,\n1,000}and {0.05, 0.1, 0.2, 0.5, 1}respectively for\nthe annealing coefﬁcient λ(t). Following previous\nworks (Yang et al., 2019; Liu et al., 2019; Lan et al.,\n2019), we report the score of 5 differently-seeded\nruns for each result.\n4.2 Results on GLUE\nTable 1 shows the single-task single-model results\nof our RECADAM ﬁne-tuning method comparing\nto the vanilla ﬁne-tuning method with BERT-base\nand ALBERT-xxlarge model on the dev set of the\nGLUE benchmark.\nResults with BERT-base With the BERT-base\nmodel, we outperform the vanilla ﬁne-tuning\nmethod on 7 out of 8 tasks of the GLUE benchmark\nand achieve 1.1% improvements on the average me-\ndian performance.\nEspecially for the tasks with smaller training\ndata (<10k), our method can achieve signiﬁcant\nimprovements (+1.7% on average) compared to\nthe vanilla ﬁne-tuning method. Because of the data\nscarcity, vanilla ﬁne-tuning on these tasks are poten-\ntially brittle, and rely on the pretrained parameters\nto be reasonably close to an ideal setting for the\ntarget task (Phang et al., 2018). With the proposed\nRECADAM method, we successfully achieve better\nﬁne-tuning by learning target tasks while recalling\nthe knowledge of pretraining tasks.\nIt is interesting to ﬁnd that compared to the me-\ndian results with BERT-large model, we can also\nachieve better results on more than half of the tasks\n(e.g., +4.0% on RTE, +0.4% on STS, +1.8% on\nCoLA, +0.4% on SST, +0.1% on QQP) and bet-\nter average results (+0.2%) of all the GLUE tasks.\nThanks to the less catastrophic forgetting realized\nby RECADAM, we can get comparable overall per-\nformance with much fewer parameters of the pre-\ntrained model.\nResults with ALBERT-xxlarge With the state-\nof-the-art model ALBERT-xxlarge, we outperform\nthe vanilla ﬁne-tuning method on 5 out of 8 tasks\nof the GLUE benchmark and achieve the state-of-\nthe-art single-task single-model average median\nperformance 90.2% on dev set of the GLUE bench-\nmark.\nSimilar to the results with the BERT-base model,\nWe ﬁnd that our improvements mostly come from\nthe tasks with smaller training data (<10k), and we\ncan improve the ALBERT-xxlarge model’s median\nperformance on these tasks by +1.5% on average.\nAlso, compared to the reported results by Lan et al.\n(2019), we can achieve similar or better median\nresults on RTE (+0.1%), STS (-0.1%), and MRPC\n(+1.0%) tasks without pretraining on MNLI task.\nOverall, we outperform the average median re-\nsults of the baseline with the ALBERT-xxlarge\nmodel by 0.7%, which is lower than the improve-\nment we gain with the BERT-base model (+1.1%).\nWith advanced model design and pretraining tech-\nniques, ALBERT-xxlarge achieves signiﬁcantly\nbetter performance on GLUE benchmark, which\nwould be harder to be further improved.\n4.3 Analysis\nModel Initialization With our RECADAM\nmethod based on Pretraining Simulation and\nObjective Shifting, the model can be initialized\nwith random values, and recall the knowledge of\npretraining tasks while learning the new tasks.\nIt is interesting to see whether the choice of ini-\ntialization strategies would have an impact on the\nperformance of our RECADAM method. Table 2\nshows the performance comparison of different ini-\ntialization strategies for RECADAM obtained by\nthe BERT-base model. It shows that RECADAM,\nwith both initialization strategies, can outperform\nthe vanilla ﬁne-tuning method on all the four tasks.\nFor the target task STS, the model with pretrained\ninitialization can achieve the same result as ran-\ndom initialization. For the other tasks (e.g., CoLA,\nMRPC, RTE), Random initialize the model would\nbe our best choice. It is because the model would\nbeneﬁt from a larger parameter search space with\nModel MNLI QQP QNLI SST Avg CoLA STS MRPC RTE Avg Avg392k 363k 108k 67k >10k 8.5k 5.7k 3.5k 2.5k <10k\nBERT-base (Devlin et al., 2019) 84.4 - 88.4 92.7 - - - 86.7 - - -\nBERT-base (rerun)Median 84.8 91.4 88.6 93.0 89.5 60.6 89.8 86.5 71.1 77.0 83.2\nBERT-base + RecAdamMedian 85.3 91.4 89.1 93.6 89.9 62.4 90.4 87.7 74.4 78.7 84.3\nBERT-base (rerun)Max 85.2 91.4 89.0 93.3 89.7 61.6 89.9 88.7 71.5 77.9 83.8\nBERT-base + RecAdamMax 85.4 91.6 89.4 94.0 90.1 62.6 90.6 88.7 77.3 79.8 85.0\nBERT-large (Devlin et al., 2019) 86.6 91.3 92.3 93.2 90.9 60.6 90.0 88.0 70.4 77.3 84.1\nXLNet-large (Yang et al., 2019) 89.8 91.8 93.9 95.6 92.8 63.6 91.8 89.2 83.8 82.1 87.4\nRoBERTa-large (Liu et al., 2019) 90.2 92.2 94.7 96.4 93.4 68.0 92.4 90.9 86.6 84.5 88.9\nALBERT-xxlarge (Lan et al., 2019) 90.8 92.2 95.3 96.9 93.8 71.4 93.0 90.9 89.2 86.1 90.0\nALBERT-xxlarge (rerun)Median 90.6 92.2 95.4 96.7 93.7 69.5 93.0 91.2 87.4 85.3 89.5\nALBERT-xxlarge + RecAdamMedian 90.5 92.3 95.3 96.8 93.7 72.9 92.9 91.9 89.3 86.8 90.2\nALBERT-xxlarge (rerun)Max 90.7 92.2 95.4 96.8 93.8 72.1 93.2 91.4 89.9 86.7 90.2\nALBERT-xxlarge + RecAdamMax 90.6 92.4 95.5 97.0 93.9 75.1 93.0 93.1 91.7 88.2 91.1\nTable 1: State-of-the-art single-task single-model results on the dev set of the GLUE benchmark. The number\nbelow each task refers to the number of training data. The average scores of the tasks with large training data\n(>10k), the tasks with small training data (<10k), and all the tasks are reported separately. We rerun the baseline\nof vanilla ﬁne-tuning without further pretraining on MNLI. We report median and maximum over 5 runs.\nMethod CoLA STS MRPC RTE Avg\nvanilla ﬁne-tuning 60.6 89.8 86.5 71.1 77.0\nRecAdam + PI 62.0 90.4 87.3 73.6 78.3\nRecAdam + RI 62.4 90.4 87.7 74.4 78.7\nTable 2: Comparison of different model initialization\nstrategies: pretrained initialization (PI) and Random\nInitialization (RI). We report median over 5 runs.\nrandom initialization. In contrast, with pretrained\ninitialization, the search space would be limited to\naround the pretraining model, making it harder for\nthe model to learn the new tasks.\nForgetting Analysis As introduced in §3.2, we\nrealize multi-task ﬁne-tuning with the Objective\nShifting technique, which allows the model’s learn-\ning objective to shift from the source tasks to the\ntarget tasks gradually. The hyperparameter kcon-\ntrols the rate of the objective shifting.\nFigure 2 shows the learning curves of our ﬁne-\ntuning methods with different kvalue obtained by\nBERT-base model trained on CoLA dataset. As dis-\ncussed in §3.2, Fine-tuning and multi-task learning\ncan be regarded as the special cases (k→∞ and\nk→0) of our method.\nAs shown in Figure 2a, with the larger shifting\nrate k, the model can converge quickly on the target\ntask. As kdecreases, it takes a longer time for the\nmodel to converge on the target task because of\nthe slower shifting from the pretrained knowledge\nrecalling to target task learning.\nFigure 2b shows the pretrained knowledge for-\ngetting during the ﬁne-tuning process. We mea-\nsure the pretrained knowledge forgetting by the\nEuclidean distance between the weights of the ﬁne-\ntuning model and the pretrained model. At the\nvery early timesteps, the Euclidean distance drops\nsharply because of the random initialization and\npretrained knowledge recalling. Then the curve\nrises with the growth rate slowing down because of\nthe target task learning. As the objective shifting\nrate kdecreases, we ﬁnd that the model can achieve\nless forgetting from the pretrained model at the end\nof the ﬁne-tuning.\nOverall, our methods provide a bridge between\nﬁne-tuning and multi-task learning. With smaller\nk, the model achieves less knowledge forgetting\nfrom the source tasks but risks not converging com-\npletely on the target task. With a good balance\nbetween the pretrained knowledge recalling and\nnew task learning, our methods can consistently\noutperform the vanilla ﬁne-tuning by not only con-\nverging on target tasks but also less forgetting from\nsource tasks.\n5 Related Works\nCatastrophic forgetting has been observed as a\ngreat challenge issue in sequential transfer learn-\ning, especially in the continuous learning paradigm\n(McCloskey and Cohen, 1989; French, 1999; Good-\nfellow et al., 2013; Lange et al., 2019). Many meth-\nods have been proposed to avoid catastrophic for-\ngetting. Replay-based methods alleviate forgetting\nby relaying the samples of the previous tasks while\n0 2 4 6 8 10 12 14\nTimestep (1e3)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Target loss\n(a) Training loss on the target task\nk = 0\nk = 1e-4\nk = 5e-4\nk = 1e-3\nk = 5e-3\nk = 1e-2\nk = 5e-2\nk = 1e-1\nk = 5e-1\nk = 1\nk = \n0 2 4 6 8 10 12 14\nTimestep (1e3)\n0\n2\n4\n6\n8\n10\n12Euclidean distance (b) Knowledge forgetting from the source tasks\nFigure 2: Learning curves obtained by BERT-base model trained with different objective shifting rate kon CoLA\ntask. We measure the knowledge forgetting by the Euclidean distance between the weights of the ﬁne-tuning\nmodel and the pretrained model. With smaller k, the model achieves less knowledge forgetting from the source\ntasks while it takes more timesteps to converge on the target task.\nlearning the new task (Rebufﬁ et al., 2017; Shin\net al., 2017; Lopez-Paz and Ranzato, 2017; Aljundi\net al., 2019). Parameter isolation-based methods\navoid forgetting by updating a set of parameters\nfor each task and freezing them for the new task\n(Mallya and Lazebnik, 2018; Serr `a et al., 2018;\nRusu et al., 2016; Xu and Zhu, 2018; Rosenfeld and\nTsotsos, 2020). Regularization-based methods pro-\npose to recall the previous knowledge with an extra\nregularization term (Kirkpatrick et al., 2017; Zenke\net al., 2017; Lee et al., 2017; Li et al., 2018; Aljundi\net al., 2018; Liu et al., 2018; Li and Hoiem, 2018;\nJung et al., 2016; Triki et al., 2017; Zhang et al.,\n2019b). We focus on regularization-based methods\nin this paper, because they don’t require the storage\nof the pretraining data, and more ﬂexible compared\nto the parameter isolation-based methods.\nRegularization-based methods can be further di-\nvided into data-focused and prior-focused meth-\nods. Data-focused methods regularize the new task\nlearning by knowledge distillation from the pre-\ntrained model (Hinton et al., 2015; Li and Hoiem,\n2018; Jung et al., 2016; Triki et al., 2017; Zhang\net al., 2019b). Prior-focused methods regard the dis-\ntribution of the pretrained parameters as prior when\nlearning the new task (Kirkpatrick et al., 2017;\nZenke et al., 2017; Lee et al., 2017; Li et al., 2018;\nAljundi et al., 2018; Liu et al., 2018). We adopted\nthe idea of prior-focused methods because they en-\nable the model to learn more general knowledge\nfrom the pretrained model’s parameters more ef-\nﬁciently. While the prior-focused methods, such\nas EWC (Kirkpatrick et al., 2017) and its variants\n(Schwarz et al., 2018; Chaudhry et al., 2018; Liu\net al., 2018), don’t directly access to the pretrain-\ning data, they need some pretraining knowledge\n(e.g., Fisher information matrix of the source tasks),\nwhich is not available in our setting. Therefore, we\nfurther approximate to a quadratic penalty which\nis independent with the pretraining data given the\npretrained parameters.\nCatastrophic forgetting in NLP has raised in-\ncreased attention recently (Mou et al., 2016; Arora\net al., 2019; Chronopoulou et al., 2019). Many ap-\nproaches have been proposed to overcome the for-\ngetting problem in various domains, such as neural\nmachine translation (Barone et al., 2017; Thomp-\nson et al., 2019) and reading comprehension (Xu\net al., 2019). As sequential transfer learning widely\nused for NLP tasks (Howard and Ruder, 2018; De-\nvlin et al., 2019; Liu et al., 2019; Lan et al., 2019;\nHou et al., 2019), previous works explore many\nﬁne-tuning tricks to reduce catastrophic forgetting\nfor adaptation of the deep pretrained LMs (Howard\nand Ruder, 2018; Sun et al., 2019; Lee et al., 2019;\nZhang et al., 2019a; Felbo et al., 2017). In this pa-\nper, we bring the idea of multi-task learning which\ncan inherently avoid catastrophic forgetting, apply\nit to the ﬁne-tuning process with Pretraining Sim-\nulation and Objective Shifting mechanisms, and\nachieve consistent improvement with only the deep\npretrained LMs available.\n6 Conclusion\nIn this paper, we solve the catastrophic forgetting\nin transferring deep pretrained language models by\nbridging two transfer learning paradigm: sequen-\ntial ﬁne-tuning and multi-task learning. To cope\nwith the absence of pretraining data during the joint\nlearning of pretraining task, we propose a Pretrain-\ning Simulation mechanism to learn the pretraining\ntask without data. Then we propose the Objective\nShifting mechanism to better balance the learning\nof the pretraining and downstream task. Experi-\nments demonstrate the superiority of our method in\nthe transferring of deep pretrained language mod-\nels, and we provide the open-source RECADAM\noptimizer by integrating the proposed mechanisms\ninto Adam optimizer to facilitate the better usage\nof deep pretrained language models.\nReferences\nRahaf Aljundi, Francesca Babiloni, Mohamed Elho-\nseiny, Marcus Rohrbach, and Tinne Tuytelaars.\n2018. Memory aware synapses: Learning what (not)\nto forget. In Computer Vision - ECCV 2018 - 15th\nEuropean Conference, Munich, Germany, Septem-\nber 8-14, 2018, Proceedings, Part III, volume 11207\nof Lecture Notes in Computer Science , pages 144–\n161. Springer.\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua\nBengio. 2019. Online continual learning with no\ntask boundaries. CoRR, abs/1903.08671.\nGaurav Arora, Afshin Rahimi, and Timothy Baldwin.\n2019. Does an LSTM forget more than a cnn? an\nempirical study of catastrophic forgetting in NLP.\nIn Proceedings of the The 17th Annual Workshop\nof the Australasian Language Technology Associa-\ntion, ALTA 2019, Sydney, Australia, December 4-6,\n2019, pages 77–86. Australasian Language Technol-\nogy Association.\nAntonio Valerio Miceli Barone, Barry Haddow, Ulrich\nGermann, and Rico Sennrich. 2017. Regularization\ntechniques for ﬁne-tuning in neural machine trans-\nlation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September 9-\n11, 2017, pages 1489–1494. Association for Compu-\ntational Linguistics.\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe ﬁfth PASCAL recognizing textual entailment\nchallenge. In Proceedings of the Second Text Analy-\nsis Conference, TAC 2009, Gaithersburg, Maryland,\nUSA, November 16-17, 2009. NIST.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M. Dai, Rafal J ´ozefowicz, and Samy Ben-\ngio. 2016. Generating sentences from a continuous\nspace. In Proceedings of the 20th SIGNLL Confer-\nence on Computational Natural Language Learning,\nCoNLL 2016, Berlin, Germany, August 11-12, 2016,\npages 10–21. ACL.\nRich Caruana. 1997. Multitask learning. Mach. Learn.,\n28(1):41–75.\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, I ˜nigo\nLopez-Gazpio, and Lucia Specia. 2017. Semeval-\n2017 task 1: Semantic textual similarity multilin-\ngual and crosslingual focused evaluation. In Pro-\nceedings of the 11th International Workshop on Se-\nmantic Evaluation, SemEval@ACL 2017, Vancouver,\nCanada, August 3-4, 2017, pages 1–14. Association\nfor Computational Linguistics.\nArslan Chaudhry, Puneet Kumar Dokania, Tha-\nlaiyasingam Ajanthan, and Philip H. S. Torr. 2018.\nRiemannian walk for incremental learning: Under-\nstanding forgetting and intransigence. In Computer\nVision - ECCV 2018 - 15th European Conference,\nMunich, Germany, September 8-14, 2018, Proceed-\nings, Part XI , volume 11215 of Lecture Notes in\nComputer Science, pages 556–572. Springer.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\n2019 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 2089–2095. Associ-\nation for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing, IWP@IJCNLP 2005, Jeju Island,\nKorea, October 2005, 2005 . Asian Federation of\nNatural Language Processing.\nBjarke Felbo, Alan Mislove, Anders Søgaard, Iyad\nRahwan, and Sune Lehmann. 2017. Using millions\nof emoji occurrences to learn any-domain represen-\ntations for detecting sentiment, emotion and sarcasm.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1615–1625, Copenhagen, Denmark. Association for\nComputational Linguistics.\nRobert M. French. 1999. Catastrophic forgetting in\nconnectionist networks. Trends in Cognitive Sci-\nences, 3(4):128 – 135.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9. Association for Computa-\ntional Linguistics.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2013. An em-\npirical investigation of catastrophic forgetting in\ngradient-based neural networks. arXiv preprint\narXiv:1312.6211.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nYutai Hou, Zhihan Zhou, Yijia Liu, Ning Wang, Wanx-\niang Che, Han Liu, and Ting Liu. 2019. Few-shot\nsequence labeling with label dependency transfer.\narXiv preprint arXiv:1906.08711.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nFerenc Husz ´ar. 2017. On quadratic penalties in\nelastic weight consolidation. arXiv preprint\narXiv:1712.03847.\nHeechul Jung, Jeongwoo Ju, Minju Jung, and Junmo\nKim. 2016. Less-forgetting learning in deep neural\nnetworks. CoRR, abs/1607.00122.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2019. ALBERT: A lite BERT for self-\nsupervised learning of language representations.\nCoRR, abs/1909.11942.\nMatthias De Lange, Rahaf Aljundi, Marc Masana,\nSarah Parisot, Xu Jia, Ales Leonardis, Gregory G.\nSlabaugh, and Tinne Tuytelaars. 2019. Continual\nlearning: A comparative study on how to defy forget-\nting in classiﬁcation tasks. CoRR, abs/1909.08383.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2019. Mixout: Effective regularization to ﬁne-\ntune large-scale pretrained language models. arXiv\npreprint arXiv:1909.11299.\nSang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-\nWoo Ha, and Byoung-Tak Zhang. 2017. Overcom-\ning catastrophic forgetting by incremental moment\nmatching. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 4652–4662.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nPrinciples of Knowledge Representation and Rea-\nsoning: Proceedings of the Thirteenth International\nConference, KR 2012, Rome, Italy, June 10-14, 2012.\nAAAI Press.\nXuhong Li, Yves Grandvalet, and Franck Davoine.\n2018. Explicit inductive bias for transfer learn-\ning with convolutional networks. arXiv preprint\narXiv:1802.01483.\nZhizhong Li and Derek Hoiem. 2018. Learning with-\nout forgetting. IEEE Trans. Pattern Anal. Mach. In-\ntell., 40(12):2935–2947.\nXialei Liu, Marc Masana, Luis Herranz, Joost van de\nWeijer, Antonio M. L ´opez, and Andrew D. Bag-\ndanov. 2018. Rotate your networks: Better weight\nconsolidation and less catastrophic forgetting. In\n24th International Conference on Pattern Recog-\nnition, ICPR 2018, Beijing, China, August 20-24,\n2018, pages 2262–2268. IEEE Computer Society.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. In\nAdvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017, Long\nBeach, CA, USA, pages 6467–6476.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nDavid J. C. MacKay. 2003. Information theory, infer-\nence, and learning algorithms . Cambridge Univer-\nsity Press.\nArun Mallya and Svetlana Lazebnik. 2018. Packnet:\nAdding multiple tasks to a single network by iter-\native pruning. In 2018 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018 , pages\n7765–7773. IEEE Computer Society.\nJames Martens. 2014. New perspectives on the natural\ngradient method. CoRR, abs/1412.1193.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109–165. El-\nsevier.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable\nare neural networks in NLP applications? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016, pages 479–\n489. The Association for Computational Linguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceed-\nings of the 4th Workshop on Representation Learn-\ning for NLP , RepL4NLP@ACL 2019, Florence, Italy,\nAugust 2, 2019, pages 7–14. Association for Compu-\ntational Linguistics.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nSylvestre-Alvise Rebufﬁ, Alexander Kolesnikov,\nGeorg Sperl, and Christoph H. Lampert. 2017. icarl:\nIncremental classiﬁer and representation learning.\nIn 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017 , pages 5533–5542. IEEE\nComputer Society.\nJoao Ribeiro, Francisco S Melo, and Joao Dias. 2019.\nMulti-task learning and catastrophic forgetting in\ncontinual reinforcement learning. arXiv preprint\narXiv:1909.10008.\nAmir Rosenfeld and John K. Tsotsos. 2020. Incremen-\ntal learning through deep adaptation. IEEE Trans.\nPattern Anal. Mach. Intell., 42(3):651–663.\nBill Dolan Lisa Ferro Danilo Giampiccolo\nBernardo Magnini Roy Bar-Haim, Ido Dagan\nand Idan Szpektor. 2006. The second pascal recog-\nnising textual entailment challenge. In Proceedings\nof the second PASCAL challenges workshop on\nrecognising textual entailment, page 64.\nSebastian Ruder. 2017. An overview of multi-\ntask learning in deep neural networks. CoRR,\nabs/1706.05098.\nSebastian Ruder. 2019. Neural transfer learning for\nnatural language processing. Ph.D. thesis, NUI Gal-\nway.\nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume\nDesjardins, Hubert Soyer, James Kirkpatrick, Ko-\nray Kavukcuoglu, Razvan Pascanu, and Raia Had-\nsell. 2016. Progressive neural networks. CoRR,\nabs/1606.04671.\nJonathan Schwarz, Wojciech Czarnecki, Je-\nlena Luketina, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell.\n2018. Progress & compress: A scalable framework\nfor continual learning. In Proceedings of the 35th\nInternational Conference on Machine Learning,\nICML 2018, Stockholmsm ¨assan, Stockholm, Swe-\nden, July 10-15, 2018 , volume 80 of Proceedings\nof Machine Learning Research , pages 4535–4544.\nPMLR.\nJoan Serr `a, Didac Suris, Marius Miron, and Alexan-\ndros Karatzoglou. 2018. Overcoming catastrophic\nforgetting with hard attention to the task. In Pro-\nceedings of the 35th International Conference on\nMachine Learning, ICML 2018, Stockholmsm ¨assan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of\nProceedings of Machine Learning Research , pages\n4555–4564. PMLR.\nNikhil Dandekar Shankar Iyer and Kornl Csernai.\nJanuary 2017. First quora dataset release: Question\npairs. https://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question-Pairs .\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon\nKim. 2017. Continual learning with deep generative\nreplay. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural In-\nformation Processing Systems 2017, 4-9 December\n2017, Long Beach, CA, USA, pages 2990–2999.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2013, 18-21 October 2013, Grand Hy-\natt Seattle, Seattle, Washington, USA, A meeting of\nSIGDAT, a Special Interest Group of the ACL, pages\n1631–1642. ACL.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nBrian Thompson, Jeremy Gwinnup, Huda Khayrallah,\nKevin Duh, and Philipp Koehn. 2019. Overcoming\ncatastrophic forgetting during domain adaptation of\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2062–2068.\nAmal Rannen Triki, Rahaf Aljundi, Matthew B.\nBlaschko, and Tinne Tuytelaars. 2017. Encoder\nbased lifelong learning. In IEEE International Con-\nference on Computer Vision, ICCV 2017, Venice,\nItaly, October 22-29, 2017, pages 1329–1337. IEEE\nComputer Society.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTACL, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers) , pages\n1112–1122. Association for Computational Linguis-\ntics.\nJu Xu and Zhanxing Zhu. 2018. Reinforced contin-\nual learning. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, 3-8 December 2018, Montr´eal, Canada, pages\n907–916.\nYing Xu, Xu Zhong, Antonio Jose Jimeno Yepes, and\nJey Han Lau. 2019. Forget me not: Reducing catas-\ntrophic forgetting for domain adaptation in reading\ncomprehension. arXiv preprint arXiv:1911.00202.\nJiabin Xue, Jiqing Han, Tieran Zheng, Xiang Gao,\nand Jiaxing Guo. 2019. A multi-task learning\nframework for overcoming the catastrophic forget-\nting in automatic speech recognition. arXiv preprint\narXiv:1904.08039.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nFriedemann Zenke, Ben Poole, and Surya Ganguli.\n2017. Continual learning through synaptic intelli-\ngence. In Proceedings of the 34th International Con-\nference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 , volume 70 of\nProceedings of Machine Learning Research , pages\n3987–3995. PMLR.\nJeffrey O. Zhang, Alexander Sax, Amir Roshan Za-\nmir, Leonidas J. Guibas, and Jitendra Malik. 2019a.\nSide-tuning: Network adaptation via additive side\nnetworks. CoRR, abs/1912.13503.\nJunting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li,\nSerafettin Tasci, Larry P. Heck, Heming Zhang,\nand C.-C. Jay Kuo. 2019b. Class-incremental\nlearning via deep model consolidation. CoRR,\nabs/1903.07864.",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.9051368236541748
    },
    {
      "name": "Computer science",
      "score": 0.799677848815918
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7379136085510254
    },
    {
      "name": "Recall",
      "score": 0.6640539169311523
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6313828229904175
    },
    {
      "name": "Task (project management)",
      "score": 0.5717761516571045
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.5354734659194946
    },
    {
      "name": "Language model",
      "score": 0.49457117915153503
    },
    {
      "name": "Transfer of learning",
      "score": 0.4769096374511719
    },
    {
      "name": "Deep learning",
      "score": 0.4630921185016632
    },
    {
      "name": "Focus (optics)",
      "score": 0.4237097501754761
    },
    {
      "name": "Natural language processing",
      "score": 0.3473987281322479
    },
    {
      "name": "Cognitive psychology",
      "score": 0.12699347734451294
    },
    {
      "name": "Psychology",
      "score": 0.07178598642349243
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}