{
  "title": "SCQT-MaxViT: Speech Emotion Recognition With Constant-Q Transform and Multi-Axis Vision Transformer",
  "url": "https://openalex.org/W4381802061",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Ong, Kah Liang",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A3203032335",
      "name": "Lee Chin Poo",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2074006958",
      "name": "Lim Heng Siong",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2528248902",
      "name": "Lim Kian Ming",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": null,
      "name": "Mukaida, Takeki",
      "affiliations": [
        "University of Electro-Communications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2936451900",
    "https://openalex.org/W4316659623",
    "https://openalex.org/W3013363049",
    "https://openalex.org/W3088789516",
    "https://openalex.org/W2803193013",
    "https://openalex.org/W2972640480",
    "https://openalex.org/W6796931752",
    "https://openalex.org/W2959133507",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W3129285739",
    "https://openalex.org/W2994791944",
    "https://openalex.org/W4324382619",
    "https://openalex.org/W4361731443",
    "https://openalex.org/W175750906",
    "https://openalex.org/W4312847199",
    "https://openalex.org/W2766272105",
    "https://openalex.org/W4362579608",
    "https://openalex.org/W2951123823",
    "https://openalex.org/W4210796791",
    "https://openalex.org/W3145643603",
    "https://openalex.org/W4309564859",
    "https://openalex.org/W3208052376",
    "https://openalex.org/W3171087525"
  ],
  "abstract": "Speech emotion recognition presents a significant challenge within the field of affective computing, requiring the analysis and detection of emotions conveyed through speech signals. However, existing approaches often rely on traditional signal processing techniques and handcrafted features, which may not effectively capture the nuanced aspects of emotional expression. In this paper, an approach named &#x201C;SCQT-MaxViT&#x201D; is proposed for speech emotion recognition, combining signal processing, computer vision, and deep learning techniques. The method utilizes the Constant-Q Transform (CQT) to convert speech waveforms into spectrograms, providing high-frequency resolution and enabling the model to capture intricate emotional details. Additionally, the Multi-axis Vision Transformer (MaxViT) is employed for further representation learning and classification of the CQT spectrograms. MaxViT incorporates a multi-axis self-attention mechanism, facilitating both local and global interactions within the network and enhancing the ability of the model to learn meaningful features. Furthermore, the dataset is augmented using random time masking techniques to enhance the generalization capabilities. Achieving accuracies of 88.68&#x0025; on the Emo-DB dataset, 77.54&#x0025; on the RAVDESS dataset, and 62.49&#x0025; on the IEMOCAP dataset, the proposed SCQT-MaxViT method exhibits promising performance in capturing and recognizing emotions in speech signals.",
  "full_text": "Received 17 May 2023, accepted 19 June 2023, date of publication 22 June 2023, date of current version 27 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3288526\nSCQT-MaxViT: Speech Emotion Recognition\nWith Constant-Q Transform and Multi-Axis\nVision Transformer\nKAH LIANG ONG\n 1, CHIN POO LEE\n1, (Senior Member, IEEE),\nHENG SIONG LIM\n 2, (Senior Member, IEEE), KIAN MING LIM\n1, (Senior Member, IEEE),\nAND TAKEKI MUKAIDA3\n1Faculty of Information Science and Technology, Multimedia University, Malacca 75450, Malaysia\n2Faculty of Engineering and Technology, Multimedia University, Malacca 75450, Malaysia\n3School of Informatics and Engineering, University of Electro-Communications, Chofu, Tokyo 182-8585, Japan\nCorresponding author: Chin Poo Lee (cplee@mmu.edu.my)\nThis work was supported in part by the Fundamental Research Grant Scheme of the Ministry of Higher Education under Award\nFRGS/1/2021/ICT02/MMU/02/4, and in part by the Telekom Malaysia Research and Development under Grant RDTC/231084.\nABSTRACT Speech emotion recognition presents a significant challenge within the field of affective\ncomputing, requiring the analysis and detection of emotions conveyed through speech signals. However,\nexisting approaches often rely on traditional signal processing techniques and handcrafted features, which\nmay not effectively capture the nuanced aspects of emotional expression. In this paper, an approach named\n‘‘SCQT-MaxViT’’ is proposed for speech emotion recognition, combining signal processing, computer\nvision, and deep learning techniques. The method utilizes the Constant-Q Transform (CQT) to convert\nspeech waveforms into spectrograms, providing high-frequency resolution and enabling the model to\ncapture intricate emotional details. Additionally, the Multi-axis Vision Transformer (MaxViT) is employed\nfor further representation learning and classification of the CQT spectrograms. MaxViT incorporates a\nmulti-axis self-attention mechanism, facilitating both local and global interactions within the network and\nenhancing the ability of the model to learn meaningful features. Furthermore, the dataset is augmented using\nrandom time masking techniques to enhance the generalization capabilities. Achieving accuracies of 88.68%\non the Emo-DB dataset, 77.54% on the RA VDESS dataset, and 62.49% on the IEMOCAP dataset, the\nproposed SCQT-MaxViT method exhibits promising performance in capturing and recognizing emotions\nin speech signals.\nINDEX TERMSSpeech, speech emotion, speech emotion recognition, spectrogram, constant-Q transform,\nvision transformer, multi-axis vision transformer, Emo-DB, RA VDESS, IEMOCAP.\nI. INTRODUCTION\nSpeech emotion recognition is an interdisciplinary field that\naims to identify and classify emotions in spoken language.\nSpeech emotion recognition plays a crucial role in various\napplications, such as human-computer interaction, mental\nhealth monitoring, customer service, and virtual assistants.\nThe growing interest in speech emotion recognition stems\nfrom the increasing demand for intelligent systems capable\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Ikramullah Lali.\nof understanding and adapting to human emotions, thus pro-\nviding more natural and intuitive user experiences.\nThe rapid advancements in Artificial Intelligence and\nmachine learning have garnered significant attention for\nspeech emotion recognition from both researchers and prac-\ntitioners. Numerous approaches have been proposed, ranging\nfrom traditional machine learning techniques like Support\nVector Machines and Hidden Markov Models to more recent\ndeep learning methods, such as Convolutional Neural Net-\nworks and Recurrent Neural Networks. These approaches\ndepend on extracting relevant acoustic features, including\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 63081\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\npitch, energy, and spectral characteristics, as well as employ-\ning effective classification techniques.\nDespite substantial progress in speech emotion recog-\nnition, several challenges remain unaddressed. One key\nchallenge is the variability and ambiguity of emotions, which\noften complicate the definition and annotation of emotional\nstates in speech data. Moreover, factors such as speaker\nvariability, language, and cultural differences heavily influ-\nence the performance of speech emotion recognition systems.\nConsequently, there is an ongoing need for more robust and\ngeneralizable models to tackle these challenges.\nThis paper presents an approach that leverages the\nConstant-Q Transform (CQT) to convert speech signals into\na time-frequency representation, effectively capturing the\nnon-stationary characteristics of speech. The CQT spectro-\ngram offers several benefits, such as enhanced frequency\nresolution at lower frequencies and improved time resolution\nat higher frequencies, making it an ideal choice for speech\nprocessing tasks. To enhance the model’s robustness and\ngeneralization capabilities, time masking is employed as a\ndata augmentation technique. Time masking operates directly\non the CQT spectrogram by applying masking to segments\nof the spectrogram. This augmentation method enables the\nmodel to learn more diverse and invariant features from the\ninput data, thereby improving its performance on unseen data.\nFinally, a Multi-Axis Vision Transformer (MaxViT) is uti-\nlized for representation learning and classification. MaxViT\ncombines the strengths of both the Vision Transformer and\nthe multi-axis attention mechanism, allowing the model to\ncapture both local and global contextual information from the\nCQT spectrogram. This results in a more expressive repre-\nsentation, leading to superior classification performance and\ngeneralization in the speech emotion recognition task. This\npaper presents the following key contributions:\n• Representation of the speech waveforms as CQT spec-\ntrograms: The CQT spectrograms offer high-frequency\nresolution, facilitating the accurate representation of\nvarious frequency components present in speech wave-\nforms. This capability enables the model to capture\nfine-grained details related to emotional expression,\nincluding variations in pitch and intonation.\n• Augmentation of the diversity of the speech dataset\nthrough time masking: To mitigate overfitting and\nenhance the generalization capabilities of the model, the\nspeech dataset is augmented using time masking. This\ntechnique involves randomly masking segments of the\nCQT spectrogram, thereby introducing variations and\nenhancing the model’s ability to handle diverse audio\nsignals.\n• Utilization of MaxViT for the classification of CQT\nspectrograms: The MaxViT model is employed for the\nclassification of CQT spectrograms in speech emotion\nrecognition. MaxViT incorporates the blocked multi-\naxis self-attention, which enables both global and local\ninteractions within the network, reducing computational\ncomplexity while preserving non-local information. The\nMax-SA module decomposes fully dense attention into\nblock attention and grid attention, providing efficient\nand effective attention across the spatial dimensions of\nCQT spectrograms.\nII. RELATED WORKS\nThe existing works in speech emotion recognition can be\nbroadly categorized into: traditional machine learning models\nand deep learning models.\nA. TRADITIONAL MACHINE LEARNING\nSingh et al. [1] proposed a support vector machine (SVM)\nmodel for speech emotion recognition tasks with acoustic\nfeatures. The researchers used two classifiers, SVM and\nrecurrent neural network, to evaluate the performance of\nspectral features and prosodic features on the Emo-DB and\nRA VDESS datasets. They found that the combination of\nboth feature representations with the SVM classifier achieved\nbetter results than the RNN classifier, with an accuracy of\n86.36% and 64.15% on the Emo-DB and the RA VDESS\ndatasets, respectively.\nIn the study by Liu et al. [2], a novel approach was pre-\nsented to improve the accuracy of speech emotion recognition\nby combining formant characteristics feature extraction and\nphoneme type convergence. They extracted formant charac-\nteristics from speech data and clustered them into different\nphoneme types using a k-means classifier. The resulting\nphoneme types were then used to train random forest,\nk-nearest neighbors, and multi-layer perceptron (MLP) mod-\nels for classification. The MLP classifier achieved the highest\naccuracy of 72.91% on the Emo-DB dataset, followed by the\nsame MLP classifier on the RA VDESS dataset with 61.02%\naccuracy. The RF classifier achieved the best accuracy of\n62.01% on the IEMOCAP dataset.\nAncilin and Milton [3] found that the mel frequency magni-\ntude coefficient (MFMC) is a superior feature representation\ncompared to other spectral features for speech emotion recog-\nnition. They extracted MFMC features from speech data\nby using the magnitude of Fourier transform, excluding the\ndiscrete cosine transformation (DCT) applied in MFCC. The\nproposed SVM method achieved an accuracy of 81.50% and\n75.63% on the Emo-DB and RA VDESS datasets, respec-\ntively, which outperformed traditional MFCC methods.\nSeknedy and Fawzi [4] conducted a comparative study\nof four machine learning classifiers for speech emotion\nrecognition, using three different feature sets: the INTER-\nSPEECH 2009 Emotion Challenge feature set (IS09), time-\ndomain features, and frequency-domain features. They found\nthat the combination of the second feature set with SVM\nclassifier achieved the highest accuracy of 85.97% on the\nEmo-DB dataset. On the RA VDESS dataset, the combination\nof IS09 with SVM classifier resulted in the best accuracy of\n70.56%, followed closely by the second feature set with SVM\nclassifier, which achieved an accuracy of 70.42%.\n63082 VOLUME 11, 2023\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nParra-Gallego and Orozco-Arroyave [5] proposed a\nsoft-margin SVM with a Gaussian kernel for speech emo-\ntion recognition, using different feature approaches. These\nfeatures included i-vectors, x-vectors, the I2010PC feature\nset, and phonation (pho), articulation (art), and prosody (pro)\nfeatures. I-vectors represent low-dimensional vector repre-\nsentations also known as identity vectors. X-vectors represent\nembedding features obtained from deep neural networks,\nwhile the I2010PC feature set comprises 38 low-level descrip-\ntor representations. The phonation features include temporal\nchanges in frequency (jitter), amplitude changes in the sig-\nnal (shimmer), amplitude perturbation quotient (APQ), and\npitch perturbation quotient (PPQ). The articulation features\nencode relevant information by representing the transition\nbetween voiced and unvoiced segments, while the prosody\nfeatures represent prosodic features such as pitch, voice qual-\nity, intonation, accentuation, phrases, and rhythm. Based on\nthe results, the combination of features (I2010PC, x-vector,\nart, pro, pho) achieved the highest accuracy of 80.70% and\n63.80% on the Emo-DB and RA VDESS datasets, respec-\ntively. The best combination of features (I2010PC, x-vector)\nachieved an accuracy of 58.90% on the IEMOCAP dataset.\nSingh et al. [6] proposed an approach for speech emotion\nrecognition using a deep neural network with support vector\nmachine (DNN-SVM) model. The proposed model employed\nthe constant-Q transform based modulation spectral features\n(CQT-MSF) to process the speech data. First, the speech\ndata was converted into CQT spectrogram, and then the CQT\nspectrogram was process to extract the temporal modulations.\nThe DNN model was used to extract temporal feature repre-\nsentations, and the SVM model was used to classify emotions.\nThe proposed DNN-SVM model with CQT-MSF was evalu-\nated on two different datasets, Emo-DB and RA VDESS. The\nresults showed remarkable accuracy of 79.86% and 52.24%\non the Emo-DB and RA VDESS datasets, respectively.\nIn their recent study, Ong et al. [7] conducted speech\nemotion recognition by leveraging both frequency and tem-\nporal domain features. Their approach involved applying\ndata augmentation techniques, specifically pitch shifting and\ntime stretching, to the audio waveforms. Subsequently, the\nauthors extracted seven distinct features from the augmented\nwaveforms, including MFCC, Mel Spectrogram, Wavelet\nTransform, Kurtosis, Root Mean Square, Chroma, and Zero-\nCrossing Rate. These features were then fed into a LightGBM\nclassifier for classification purposes. The experimental find-\nings revealed that the proposed method achieved an accuracy\nof 84.91% on the Emo-DB dataset and 67.72% on the\nRA VDESS dataset.\nB. DEEP LEARNING\nZhang et al. [8] proposed a discriminant temporal pyramid\nmatching (DTPM) strategy with deep CNNs. The DTPM was\ndesigned to identify the most discriminative utterance-level\nrepresentation from the Mel-frequency cepstral coefficients\n(MFCC) features. Experimental results on the Emo-DB\ndataset showed that the DCNN-DTPM method achieved an\naccuracy of 87.31%, demonstrating its effectiveness in speech\nemotion recognition.\nGuo et al. [9] presented an improved speech emotion\nrecognition approach by using a combination of complemen-\ntary features and kernel extreme learning machine (KELM).\nThe feature extraction involved both CNN and deep neural\nnetwork (DNN) models to extract a combined set of features,\nnamely MFCC, pitch, and voice quality representations from\nspeech data. The output of these complementary features was\nthen fed into the KELM for classification. To evaluate the pro-\nposed KELM method, the Emo-DB and IEMOCAP datasets\nwere used. The results indicated that the proposed KELM\napproach achieved an accuracy of 84.49% and 57.10% on the\nEmo-DB and IEMOCAP datasets, respectively.\nJiang et al. [10] introduced a parallelized convolutional\nrecurrent neural network (PCRN) to utilize the learning abil-\nity of neural networks. The PCRN is composed of a CNN\nand an LSTM, which split and process speech waveforms\ninto two distinct feature representations in parallel. The CNN\nmodel learns the time and frequency representation, while\nthe LSTM model learns the temporal features from the Mel-\nspectrograms features. In the end, the final output is obtained\nby combining the outputs of all the parallel branches. The\nproposed PCRN achieved 86.44% accuracy on the Emo-DB\ndataset.\nIn their study, Chatziagapi et al. [11] compared different\ndata augmentation methods, including pitch shifting, time\nstretching, and GANs, to augment speech data for emotion\nrecognition. The researchers evaluated the augmented data\nusing the VGG19 model on the IEMOCAP dataset and found\nthat the GAN-based data augmentation method outperformed\nthe other methods, achieving a 54.60% accuracy.\nChauhan et al. [12] proposed a simple CNN architec-\nture for speech emotion recognition, which takes log-mel\nspectrograms as input features. The model includes mul-\ntiple convolutional layers, followed by pooling and fully\nconnected layers. Each convolutional layer includes 2D\nconvolutional layers, batch normalization, activation, and\nmax-pooling layers. The proposed CNN model achieved\naccuracies of 72.02% and 59.33% on the Emo-DB and\nIEMOCAP datasets, respectively.\nIn a similar approach, Neumann and Vu [13] presented\nan unsupervised representation learning method to enhance\nspeech emotion recognition performance on unlabeled\nspeech data. The authors extracted 26 log mel-filterbanks as\ninput features and applied the auDeep toolkit to learn addi-\ntional feature vectors from the unlabeled speech data. The\nunsupervised representation was then fed into an attention\nconvolutional neural network (ACNN) to learn the unlabeled\nspeech data representation. The proposed method achieved an\naccuracy of 59.54% on the IEMOCAP dataset.\nSeo and Kim [14] proposed a Visual Attention-based Con-\nvolutional Neural Network (V ACNN) for speech emotion\nrecognition, which uses the Bag of Visual Words (BoVW)\ntechnique to extract features from the log-mel spectrogram.\nVOLUME 11, 2023 63083\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nThe V ACNN model incorporates a visual attention mech-\nanism to learn local and global feature representations by\nconstructing a frequency histogram of visual words. This\nenables the model to capture both auditory and visual cues,\nresulting in improved accuracy in emotion recognition. The\nproposed method was evaluated on two datasets, Emo-DB\nand RA VDESS, achieving accuracies of 79.44% and 74.31%,\nrespectively.\nYao et al. [15] employed a multi-task learning-based\napproach for speech emotion recognition, which com-\nbines three classifiers based on DNN, CNN, and RNN\narchitectures. Each classifier is trained independently on\nmel-spectrogram features to recognize emotions, with the\nDNN model learning high-level spectral features, the CNN\nmodel learning mel-spectrogram features, and the RNN\nmodel learning low-level descriptors. The outputs of the\nclassifiers are fused using a weighted sum to obtain the\nfinal emotion classification. The proposed method was eval-\nuated on the IEMOCAP dataset and achieved an accuracy of\n58.30%.\nSingh et al. [16] presented an approach for speech emotion\nrecognition using multidimensional convolutional neural net-\nworks with different feature representations. Three features\nare used namely, MFCC, Chroma MFCC, and Chroma. The\nperformance of one-dimensional convolutional neural net-\nworks (1D-CNN) and two-dimensional convolutional neural\nnetworks (2D-CNN) with and without data augmentation\nwere compared to evaluate the proposed approach. The\ndata augmentation technique involved inserting noise into\nthe speech data. The experiment reported that the 2D-CNN\nmodel with augmented log-mel spectrogram achieved an\naccuracy of 63.00%, while the 2D-CNN model without aug-\nmented MFCC features achieved 64.00% accuracy on the\nRA VDESS dataset.\nSingh et al. [17] proposed a deep learning model that\ncombines a 2D-CNN and a LSTM network enhanced with\na self-attention mechanism. The proposed model leveraged\nMFCC features as input representations from speech data.\nThe researchers evaluated their proposed model on the\nRA VDESS dataset, where the CNN-2D and LSTM with\nself-attention approach achieved an accuracy of 74.44%.\nIII. SPEECH EMOTION RECOGNITION WITH CONSTANT-Q\nTRANSFORM AND MULTI-AXIS VISION TRANSFORMER\nThe proposed method for speech emotion recognition, called\nSCQT-MaxViT, integrates the spectrogram of the constant-Q\ntransform (CQT) with the Multi-Axis Vision Transformer\n(MaxViT). The method comprises three fundamental steps:\nsignal representation, data augmentation, and classification.\nIn the signal representation step, the speech signals are\ntransformed into CQT spectrograms. This representation\ncaptures the frequency content of the signals and provides\nvaluable insights into the underlying emotions. The CQT\nspectrograms serve as the input data for subsequent process-\ning. The CQT spectrograms are then divided into separate\ntraining and testing sets, enabling the model to learn from a\nlabeled dataset and evaluate its performance on unseen data.\nTo enhance the quantity and diversity of the training data,\nthe training set undergoes data augmentation. This augmenta-\ntion process involves applying time masking transformations\nto the CQT spectrograms. Time masking introduces varia-\ntions in the temporal structure of the signals, enriching the\ntraining dataset and improving the model’s ability to general-\nize to different speech samples.\nThe augmented CQT spectrograms are then utilized to train\nthe MaxViT model, which employs a multi-axis attention\nmechanism to learn powerful representations from the input\ndata. This representation learning phase enables the model to\ncapture complex patterns and discriminative features related\nto different emotional states.\nFinally, the performance evaluation is conducted on the\ntrained MaxViT model using the CQT spectrograms from\nthe testing set. This evaluation assesses the model’s abil-\nity to accurately classify and recognize emotions present\nin the speech signals. The workflow of the proposed\nSCQT-MaxViT method is illustrated in Figure 1.\nA. CONSTANT-Q TRANSFORM\nThe Constant-Q Transform (CQT) is a frequency-domain\nanalysis technique widely used in audio signal processing.\nUnlike the traditional Fourier transform or the Short-Time\nFourier Transform (STFT), which use a linear frequency\nscale, the CQT employs a logarithmic frequency scale that\nclosely approximates the nonlinear frequency response of the\nhuman auditory system.\nThe CQT is computed by convolving the input signal with\na set of complex exponential functions that are equally spaced\non a logarithmic frequency scale, with the Q-factor (i.e., the\nratio of the center frequency to bandwidth) being kept con-\nstant for all bins. This results in a representation of the signal\nin the time-frequency domain, where each bin corresponds to\na fixed Q value and a different center frequency. The CQT\ncan be expressed mathematically as follows:\nS =\nN−1∑\nn=0\nx(n) · g∗(t − n1t)e−2πikn/N (1)\nwhere x(n) is the input signal, g∗(t − n1t) is the complex\nconjugate of the CQT kernel function, k is the frequency bin\nindex, N is the total number of samples, t is the time variable,\nand 1t is the time resolution of the kernel function. The CQT\nkernel function is defined as:\ng(t) = 1\n√Qw\n(t\nQ\n)\ne2πif0t (2)\nwhere f0 is the center frequency of the kernel, w(t) is a\nwindow function that localizes the kernel in time, and Q is\nthe Q-factor of the kernel.\nThe CQT produces a time-frequency representation of the\nsignal in the form of a spectrogram, where the amplitude\nof each bin represents the energy or power of the signal at\n63084 VOLUME 11, 2023\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nFIGURE 1. Workflow of SCQT-MaxViT.\nFIGURE 2. Sample of a Constant-Q Transform spectrogram.\nthat particular frequency and time. Since the CQT uses a\nlogarithmic frequency scale, it provides higher resolution at\nlower frequencies and lower resolution at higher frequencies,\nwhich is well-suited for analyzing musical signals that have a\npitch structure. Moreover, the CQT can capture the harmonic\nstructure of the signal even when the fundamental frequency\nvaries over time, making it more robust to pitch variations\nthan the STFT. Not only that, the CQT also provides better\nresolution and robustness than traditional Fourier-based tech-\nniques and can be computed efficiently using fast algorithms\nsuch as the FFT. Figure 2 shows an example of a CQT\nspectrogram.\nB. DATA AUGMENTATION\nData augmentation techniques are widely employed to gen-\nerate additional training samples, thereby enhancing the\nrobustness and generalization capabilities of models. In this\nstudy, time masking is performed on the CQT spectro-\ngrams. Time masking involves masking (i.e., setting to zero)\na continuous time segment of the spectrogram, effectively\nremoving some of the temporal information. The time mask-\ning operation can be represented as follows:\nLet S be the original spectrogram of size F × T , where F\nis the number of frequency bins and T is the number of time\nsteps.\n1) Select a random time step t, where 0 ≤ t < T .\n2) Select a random time mask width τ, where 0 ≤ τ ≤ T\nand τ ≤ Tmax, with Tmax being the maximum allowed\ntime mask width.\n3) Create a time-masked spectrogram S′ by setting the\nvalues in the time range [t , t + τ) to zero.\nMathematically, this can be represented as:\nS′(f , t′) =\n{\n0, if t ≤ t′ < t + τ\nS(f , t′), otherwise (3)\nHere, S′(f , t′) represents the value in the time-masked spec-\ntrogram at frequency bin f and time step t′, and S(f , t′) is the\nvalue in the original spectrogram at frequency bin f and time\nstep t′.\nC. MULTI-AXIS VISION TRANSFORMER\nThis study employs the Multi-Axis Vision Transformer\n(MaxViT) [18] for further representation learning and clas-\nsification. MaxViT incorporates the blocked multi-axis\nself-attention (Max-SA) module, which introduces a novel\nVOLUME 11, 2023 63085\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nFIGURE 3. Sample time masking with random segments and noise\nfactors.\nattention mechanism for enabling global and local interac-\ntions within the network while minimizing computational\ncomplexity. To achieve this, the Max-SA module decom-\nposes fully dense attention mechanisms into two sparse\nforms: block attention and grid attention, inspired by pre-\nvious sparse approaches. This decomposition effectively\nconverts the quadratic complexity of vanilla attention to\nlinear complexity without compromising non-locality. The\nMax-SA module utilizes self-attention to enable spatial mix-\ning across the entire spatial or sequence locations based on\ncontent-dependent weights derived from normalized pairwise\nsimilarity. The pre-normalized relative self-attention, known\nfor incorporating a learned bias, is utilized in this study,\nconsistently outperforming the original attention mechanism\nin various vision tasks.\nTo address the computational challenges associated with\napplying attention across the entire space, Max-SA intro-\nduces a multi-axis approach encompassing block attention\nand grid attention. These two types of attention are sequen-\ntially stacked within a single block to facilitate local and\nglobal interactions. In the block attention, the input feature\nmap, denoted as X ∈ RH×W ×C , undergoes a partition-\ning technique to enable attention mechanisms. Rather than\ndirectly applying attention to the flattened spatial dimension\nHW , the feature map is reshaped into a tensor of shape\n(H\nP × W\nP , P × P, C), resulting in non-overlapping windows\nwithin the feature maps, each with a size of P × P.\nFurthermore, the grid attention mechanism is employed\nby reshaping the tensor into (G × G, H\nG × W\nG , C), gridding\nthe feature maps into G × G partitions. By utilizing fixed\nwindow and grid sizes (P = G = 7), the computational load\nis evenly distributed between local and global operations,\nboth exhibiting linear complexity relative to the spatial size\nor sequence length.\nThe Max-SA module seamlessly integrates into the\nMaxViT block, which incorporates additional components\nsuch as LayerNorm, Feedforward networks (FFNs), skip-\nconnections, and a Mobile Inverted Residual Bottleneck\nTABLE 1. MaxViT-T configurations.\nConvolution (MBConv) block with a squeeze-and-excitation\n(SE) module. The MBConv block enhances the network’s\ngeneralization and trainability, while the depthwise con-\nvolutions serve as conditional position encoding (CPE),\neliminating the need for explicit positional encoding lay-\ners. The hierarchical design of the MaxViT block involves\nstacking alternating layers of Max-SA with MBConv, pro-\nviding global and local receptive fields throughout the entire\nnetwork.\nThe architecture of the MaxViT model is visualized in\nFigure 4. Following the conventional practices of CNN,\na hierarchical backbone is employed, which includes a\ndownsampling step in the initial stem stage (S0) using two\nconvolutional layers with a kernel size of 3 × 3 (Conv3 × 3).\nThe network body consists of four stages (S1-S4), where each\nsubsequent stage has half the resolution of the previous one\nand a doubled number of channels in the hidden dimension.\nIdentical MaxViT blocks are utilized throughout the entire\nbackbone.\nThe downsampling operation is applied in the Depthwise\nConv3×3 layer of the first MobileNetV3-like Convolutional\n(MBConv) block within each stage. The inverted bottleneck\nand squeeze-excitation (SE) mechanisms have expansion\nrates of 4 and shrink rates of 0.25, respectively. The attention\nhead size is set to 32 for all attention blocks. Model scaling is\nachieved by increasing the number of blocks per stage (B) and\nthe channel dimension (C ). In this study, the MaxViT-T vari-\nant is employed, and the specific configurations are presented\nin Table 1. The ‘‘Size’’ column indicates the downsampling\nratio of each stage.\nMaxViT incorporates both global and local receptive\nfields throughout the entire network. By leveraging self-\nattention mechanisms, MaxViT enables the model to capture\nlong-range dependencies and spatial correlations in the CQT\nspectrograms. This allows for a comprehensive understand-\ning of the spectrogram features at different scales, enhancing\nthe model’s ability to recognize subtle patterns and variations\nassociated with different emotions.\nIV. EXPERIMENTAL RESULTS AND DISCUSSION\nThis section provides an analysis of the key aspects in speech\nemotion recognition research. It includes an overview of\nthe datasets utilized, an evaluation of various spectrograms\nas feature representations, a discussion on the classifiers\nemployed, an examination of data augmentation techniques,\nand a comparison of the proposed approach with existing\nworks in the field.\n63086 VOLUME 11, 2023\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nFIGURE 4. Architecture of the MaxViT model.\nA. DATASETS\nThe proposed SCQT-MaxViT method is evaluated on three\nspeech emotion datasets, namely the Berlin Database of Emo-\ntional Speech (Emo-DB), Ryerson Audio-Visual Database\nof Emotional Speech and Song (RA VDESS), and Interactive\nEmotional Dyadic Motion Capture (IEMOCAP). To ensure a\nfair comparison with previous research, each dataset is split\ninto an 80% training set and a 20% testing set.\nThe Berlin Database of Emotional Speech (Emo-DB) [19]\nis a German language speech emotion recognition dataset\nthat includes 535 samples from five male and five female\nprofessional speakers. The dataset contains 535 samples\nwith 7 emotions: 127 anger, 81 boredom, 79 neutral,\n71 happiness, 69 anxiety, 62 sadness, and 46 disgust audio\nsamples. Each utterance is labeled with the correspond-\ning emotion and stored as a separate audio file in WA V\nformat.\nThe Ryerson Audio-Visual Database of Emotional Speech\nand Song (RA VDESS) [20] consists of 1440 samples with\neight emotions: 96 neutral, 192 calm, 192 happy, 192 sad,\n192 angry, 192 fearful, 192 disgust, and 192 surprised audio\nsamples. The dataset was recorded in English by twelve male\nand twelve female professional actors who produced two\nversions of each emotion, one with speech and one with\nsinging, resulting in a total of 1440 recordings. The audio\nrecordings are stored in the WA V format.\nThe Interactive Emotional Dyadic Motion Capture (IEMO-\nCAP) [21] dataset includes 5507 samples produced by five\nmale and five female actors in various conversational tasks.\nThe actors were asked to converse with each other and to\nengage in tasks such as telling stories or solving problems.\nTo have a fair comparison with existing works, this study\nconsidered four emotions: 1704 neutral, 1636 happiness,\n1090 anger, and 1077 sadness audio samples.\nB. EXPERIMENTAL RESULTS OF DIFFERENT\nSPECTROGRAMS\nThe performance analysis of different spectrogram types in\nconjunction with MaxViT for speech emotion recognition is\npresented. The speech signals are initially sampled at a fre-\nquency of 44.1kHz and subsequently transformed into CQT\nspectrograms. To conform to the input size requirements of\nthe MaxViT model, the CQT spectrograms are appropriately\nresized to a resolution of 224 × 224 pixels.\nExperimental results comparing three spectrogram types\n(CQT, Linear-STFT, and MFCC) combined with MaxViT\nare summarized in Table 2. Notably, the accuracy values\nconsistently indicate that CQT spectrograms outperform\nthe other two types. In terms of accuracy, CQT+MaxViT\nyielded the highest scores of 86.79%, 76.14%, and 62.31%\nfor the Emo-DB, RA VDESS, and IEMOCAP datasets,\nrespectively.\nCQT spectrograms are well-regarded for their ability to\nprovide a more comprehensive representation of audio sig-\nnals in the time-frequency domain, enabling them to capture\nintricate details in human speech. Unlike Linear-STFT, CQT\nadopts a logarithmic frequency scale, which closely aligns\nwith the characteristics of the human auditory system. Fur-\nthermore, CQT spectrograms possess enhanced resolution at\nlower frequencies, which is crucial for accurately detecting\nthe fundamental frequency of human speech. Conversely,\nMFCC leverages a mel-scale filterbank to extract features,\npotentially leading to the loss of valuable information embed-\nded within the audio signals.\nC. EXPERIMENTAL RESULTS OF DIFFERENT CLASSIFIERS\nTable 3 provides the experimental results of different\nclassifiers with the MaxViT model. The evaluated meth-\nods involve combining the CQT with various classifiers,\nVOLUME 11, 2023 63087\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nTABLE 2. Experimental results of different spectrograms with MaxViT.\nTABLE 3. Experimental results of different classifiers.\nincluding Random Forest, K-Nearest Neighbors (KNN),\nSVM, MLP, VGG19, CNN, CoAtNet [22], Vision Trans-\nformer (ViT) [23], and MaxViT. Accuracy percentages for\nthree datasets, namely Emo-DB, RA VDESS, and IEMOCAP,\nare reported.\nThe CQT + MaxViT method demonstrates the highest\naccuracy across all three datasets, with values of 86.79% for\nEmo-DB, 76.14% for RA VDESS, and 62.31% for IEMO-\nCAP. This significant performance superiority over other\nmethods highlights the effectiveness of the MaxViT model\nin capturing intricate relationships within the audio data.\nBy leveraging multi-axis self-attention mechanisms, MaxViT\neffectively captures long-range dependencies and spatial cor-\nrelations within the CQT spectrograms. This allows the\nmodel to understand the spectrogram features at different\nscales and recognize subtle patterns and variations associated\nwith different emotions. The combination of MaxViT with\nCQT spectrograms provides a powerful framework for speech\nemotion classification, enabling the model to achieve high\naccuracy by effectively extracting meaningful representations\nfrom the audio data.\nApart from that, several other classifiers, including CQT +\nVGG19, CQT + CNN, CQT + CoAtNet, and CQT + ViT,\nachieve relatively high accuracy percentages. These models\nexhibit accuracy values ranging from 58.49% to 85.85% for\nEmo-DB, 59.95% to 74.39% for RA VDESS, and 66.67%\nto 73.33% for IEMOCAP. These outcomes suggest the effi-\ncacy of pre-trained models, especially those trained on visual\nrecognition tasks, in extracting meaningful representations\nfrom audio data.\nTABLE 4. Experimental results MaxViT with and without data\naugmentation.\nD. EXPERIMENTAL RESULTS OF DATA AUGMENTATION\nThis section presents the experimental results of the pro-\nposed SCQT-MaxViT in the context of data augmentation.\nThe obtained results are shown in Table 4. The findings\ndemonstrate that performing data augmentation significantly\nenhances the performance of SCQT-MaxViT, as compared to\nSCQT-MaxViT without data augmentation, across all three\ndatasets. Particularly noteworthy improvements are observed\nin Emo-DB and RA VDESS, where statistically signifi-\ncant enhancements are observed. Specifically, the accuracy\nin Emo-DB increases from 86.79% to 88.68%, while in\nRA VDESS, it increases from 76.14% to 77.54%. Although\nthe improvement in IEMOCAP is marginal, there is a slight\nincrease from 62.31% to 62.49%.\nThese results underscore the efficacy of data augmentation\nin bolstering the performance of SCQT-MaxViT in speech-\nbased emotion recognition tasks. Data augmentation involves\nthe application of random time masking to the input spec-\ntrograms during training. This technique aids the model in\ndeveloping improved generalization capabilities, enabling it\nto effectively handle unseen data and mitigate overfitting\nissues.\nE. COMPARISON RESULTS WITH THE EXISTING WORKS\nTable 5 compares the proposed SCQT-MaxViT method with\nexisting methods on three emotion speech datasets: Emo-\nDB, RA VDESS, and IEMOCAP. The experimental result\nshows that the proposed method outperforms the existing\nmethods on all three datasets. The dash ‘‘-’’ in the table\nrepresents that the dataset was not used in the existing\nworks. On the Emo-DB dataset, existing methods achieved\naccuracy in the range of 60.05% to 87.31%, while the pro-\nposed SCQT-MaxViT method achieved a significantly higher\naccuracy of 88.68%. Similarly, on the RA VDESS dataset,\nthe best existing method MFCC with CNN [3] achieved\nan accuracy of 75.63%, which is 1.91% lower than the\nproposed SCQT-MaxViT method. However, all methods per-\nformed relatively poorly on the IEMOCAP dataset, likely\ndue to it contains spontaneous, unscripted and sometimes\noverlapping conversations between actors. Existing methods\nachieved an accuracy of 54.60% to 62.01%, while the pro-\nposed SCQT-MaxViT method achieved a higher accuracy of\n62.49%.\nThe remarkable improvement in performance confirms\nthe effectiveness of the proposed SCQT-MaxViT method in\n63088 VOLUME 11, 2023\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nTABLE 5. Comparative results on Emo-DB, RAVDESS, IEMOCAP dataset.\nspeech emotion recognition. The use of CQT, with its loga-\nrithmically spaced frequency bins, provides better frequency\nresolution at lower frequencies and allows for accurate iden-\ntification of relevant features in speech signals. Additionally,\nthe logarithmic frequency spacing of the CQT makes it less\nFIGURE 5. Confusion matrix of the Emo-DB.\nsensitive to pitch variations, which are common in emotional\nspeech, resulting in the extraction of more consistent and\nreliable features for emotion recognition tasks. The variable\nwindow size of the CQT also provides better temporal reso-\nlution for higher frequencies.\nThe application of data augmentation increases the amount\nof training data by applying random transformations to the\noriginal spectrograms. This improves the model’s general-\nization ability and makes it more robust to variations in\nemotional speech patterns. The use of time masking randomly\nremoves a range of segments from the spectrogram, forcing\nthe model to rely on other segments for emotion recognition,\nimproving its ability to recognize emotions even in the pres-\nence of noise or other distortions.\nMaxViT can extract features from different levels of\nabstraction in the input data, enabling the model to cap-\nture both local and global patterns crucial for identifying\nemotional cues in speech signals. The multi-axis attention\nmechanism allows the model to focus on different spatial\npositions and scales in the CQT spectrograms, leading to a\nbetter understanding of the relationships between different\nfrequency components and their temporal variations, thereby\nimproving emotion recognition.\nFigure 5 presents the confusion matrix of the proposed\nSCQT-MaxViT method on the Emo-DB dataset, indicating\nthe model’s performance in classifying different emotions.\nAs seen in the confusion matrix, misclassifications are typ-\nically observed in classes with relatively fewer samples. The\nresults suggest that happiness and boredom are the emotions\nthat are most frequently misclassified, often mistaken for\nanger. The possible reason behind this could be that these\nemotions share certain acoustic characteristics with anger,\nleading to confusion in the classification.\nVOLUME 11, 2023 63089\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\nFIGURE 6. Confusion matrix of the RAVDESS dataset.\nFIGURE 7. Confusion matrix of the IEMOCAP dataset.\nThe confusion matrix depicted in Figure 6 provides\ninsights into the performance of the proposed SCQT-MaxViT\nmodel on the RA VDESS dataset. Notably, certain classes\nexhibit a higher degree of confusion with others, indicat-\ning inherent challenges in accurately distinguishing between\nthem. Specifically, the ‘‘disgust’’ class frequently over-\nlaps with ‘‘fear’’ or ‘‘calm’’ in terms of misclassification,\nwhile the ‘‘neutral’’ class is often mistaken for ‘‘angry’’\nor ‘‘happy’’. These misclassifications could be attributed\nto shared acoustic characteristics between the audio wave-\nforms associated with these emotions. However, individual\ndifferences in emotional expression may also contribute\nto misclassification, as individuals may exhibit unique\nvariations in expressing the same emotion through their audio\nsignals.\nThe confusion matrix presented in Figure 7 provides an\noverview of the performance of the proposed SCQT-MaxViT\nmodel on the demanding IEMOCAP dataset. The majority\nof misclassifications are observed within the happiness and\nsadness classes, highlighting the difficulty in accurately\ndistinguishing between these two emotions. Notably, the\nIEMOCAP dataset poses considerable challenges for emo-\ntion recognition models due to its unique characteristics. The\ndataset encompasses a substantial number of samples, and the\nspeech recordings involve multiple speakers simultaneously,\ncomplicating the isolation of individual speaker character-\nistics. Furthermore, the brevity of some speech segments\npresents a challenge in accurately capturing the underlying\nemotion contained within the audio waveforms.\nV. CONCLUSION\nThis paper presents a novel approach, SCQT-MaxViT,\nwhich enhances speech emotion recognition by combining\nthe Constant-Q Transformer (CQT) spectrogram and the\nMulti-Axis Vision Transformer (MaxViT). The proposed\nmethod involves a series of stages to extract and process\nfeatures from audio samples. Initially, the audio signals are\ntransformed into visual representations using the CQT spec-\ntrogram, which captures the frequency content of the speech.\nTo improve the diversity and robustness of the training\ndata, a data augmentation technique called time masking is\nemployed, introducing random variations in the spectrogram\nby masking specific time bands. This augmentation strat-\negy effectively reduces overfitting and enables the model to\ngeneralize better to unseen data. The augmented CQT spec-\ntrograms are then fed into the Multi-Axis Vision Transformer\nfor speech emotion recognition, allowing both global and\nlocal interactions within the network. The SCQT-MaxViT\napproach achieves impressive accuracy rates of 88.68%,\n77.54%, and 62.49% on the Emo-DB, RA VDESS, and\nIEMOCAP datasets, respectively, demonstrating its effective-\nness as a reliable solution for speech emotion recognition\ntasks. Despite the remarkable performance exhibited by the\nproposed SCQT-MaxViT model on various datasets, it is\nworth noting that the IEMOCAP dataset presents unique\nchallenges due to its dyadic nature, involving interactions\nbetween two individuals. The complexity of capturing and\ninterpreting emotional cues in dialogue scenarios requires\nfurther attention in future research.\nREFERENCES\n[1] R. Singh, H. Puri, N. Aggarwal, and V . Gupta, ‘‘An efficient language-\nindependent acoustic emotion classification system,’’ Arabian J. Sci. Eng.,\nvol. 45, no. 4, pp. 3111–3121, Apr. 2020.\n[2] Z.-T. Liu, A. Rehman, M. Wu, W.-H. Cao, and M. Hao, ‘‘Speech emo-\ntion recognition based on formant characteristics feature extraction and\nphoneme type convergence,’’ Inf. Sci., vol. 563, pp. 309–325, Jul. 2021,\ndoi: 10.1016/j.ins.2021.02.016.\n[3] J. Ancilin and A. Milton, ‘‘Improved speech emotion recognition with\nMel frequency magnitude coefficient,’’ Appl. Acoust., vol. 179, Aug. 2021,\nArt. no. 108046.\n63090 VOLUME 11, 2023\nK. L. Ong et al.: SCQT-MaxViT: Speech Emotion Recognition With CQT and MaxViT\n[4] M. E. Seknedy and S. Fawzi, ‘‘Speech emotion recognition system for\nhuman interaction applications,’’ in Proc. 10th Int. Conf. Intell. Comput.\nInf. Syst. (ICICIS), Dec. 2021, pp. 361–368.\n[5] L. F. Parra-Gallego and J. R. Orozco-Arroyave, ‘‘Classification of emo-\ntions and evaluation of customer satisfaction from speech in real world\nacoustic environments,’’ Digit. Signal Process., vol. 120, Jan. 2022,\nArt. no. 103286.\n[6] P. Singh, M. Sahidullah, and G. Saha, ‘‘Modulation spectral features for\nspeech emotion recognition using deep neural networks,’’ Speech Com-\nmun., vol. 146, pp. 53–69, Jan. 2023.\n[7] K. L. Ong, C. P. Lee, H. S. Lim, and K. M. Lim, ‘‘Speech emotion\nrecognition with light gradient boosting decision trees machine,’’ Int.\nJ. Electr. Comput. Eng., vol. 13, no. 4, p. 4020, Aug. 2023.\n[8] S. Zhang, S. Zhang, T. Huang, and W. Gao, ‘‘Speech emotion recognition\nusing deep convolutional neural network and discriminant temporal pyra-\nmid matching,’’ IEEE Trans. Multimedia, vol. 20, no. 6, pp. 1576–1590,\nJun. 2018.\n[9] L. Guo, L. Wang, J. Dang, Z. Liu, and H. Guan, ‘‘Exploration of comple-\nmentary features for speech emotion recognition based on kernel extreme\nlearning machine,’’ IEEE Access, vol. 7, pp. 75798–75809, 2019.\n[10] P. Jiang, H. Fu, H. Tao, P. Lei, and L. Zhao, ‘‘Parallelized convolutional\nrecurrent neural network with spectral features for speech emotion recog-\nnition,’’IEEE Access, vol. 7, pp. 90368–90377, 2019.\n[11] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopoulos,\nM. Nikandrou, T. Giannakopoulos, A. Katsamanis, A. Potamianos, and\nS. Narayanan, ‘‘Data augmentation using GANs for speech emotion recog-\nnition,’’ in Proc. Interspeech, Sep. 2019, pp. 171–175.\n[12] P. M. Larisa and R. Tapu, ‘‘Speech emotion recognition using 1D/2D\nconvolutional neural networks,’’ in Proc. Int. Symp. Electron. Telecommun.\n(ISETC), Nov. 2022, pp. 1–4.\n[13] M. Neumann and N. T. Vu, ‘‘Improving speech emotion recognition\nwith unsupervised representation learning on unlabeled speech,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2019,\npp. 7390–7394.\n[14] M. Seo and M. Kim, ‘‘Fusing visual attention CNN and bag of visual words\nfor cross-corpus speech emotion recognition,’’ Sensors, vol. 20, no. 19,\np. 5559, Sep. 2020.\n[15] Z. Yao, Z. Wang, W. Liu, Y . Liu, and J. Pan, ‘‘Speech emotion recog-\nnition using fusion of three multi-task learning-based classifiers: HSF-\nDNN, MS-CNN and LLD-RNN,’’ Speech Commun., vol. 120, pp. 11–19,\nJun. 2020.\n[16] S. P. Singh, S. Kumar, S. Verma, and I. Kaur, ‘‘Hybrid approach for human\nemotion recognition from speech,’’ in Proc. 4th Int. Conf. Adv. Comput.,\nCommun. Control Netw. (ICAC3N), Dec. 2022, pp. 1282–1285.\n[17] J. Singh, L. B. Saheer, and O. Faust, ‘‘Speech emotion recognition using\nattention model,’’Int. J. Environ. Res. Public Health, vol. 20, no. 6, p. 5140,\nMar. 2023.\n[18] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y . Li,\n‘‘MaxViT: Multi-axis vision transformer,’’ in Proc. 17th Eur. Conf. Com-\nput. Vis.Cham, Switzerland: Springer, 2022, pp. 459–479.\n[19] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss,\n‘‘A database of German emotional speech,’’ in Proc. Interspeech,\nSep. 2005, pp. 1517–1520, doi: 10.21437/interspeech.2005-446.\n[20] S. R. Livingstone and F. A. Russo, ‘‘The Ryerson audio-visual database\nof emotional speech and song (RA VDESS): A dynamic, multimodal\nset of facial and vocal expressions in North American English,’’ PLoS\nONE, vol. 13, no. 5, May 2018, Art. no. e0196391, doi: 10.1371/jour-\nnal.pone.0196391.\n[21] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, ‘‘IEMOCAP: Interactive emo-\ntional dyadic motion capture database,’’ Lang. Resour. Eval., vol. 42, no. 4,\npp. 335–359, Dec. 2008.\n[22] Z. Dai, H. Liu, Q. V . Le, and M. Tan, ‘‘CoatNet: Marrying convolution and\nattention for all data sizes,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 34,\n2021, pp. 3965–3977.\n[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\nKAH LIANG ONGreceived the bachelor’s degree\n(Hons.) in information technology and in arti-\nficial intelligence from Multimedia University,\nMalaysia, in 2021. He is currently pursuing the\nmaster’s degree. His current research interests\ninclude speech emotion recognition which mainly\ninvolves audio pre-processing, feature extraction,\nand emotion classification.\nCHIN POO LEE(Senior Member, IEEE) received\nthe Master of Science and Ph.D. degrees in\ninformation technology and in abnormal behavior\ndetection and gait recognition. She is currently a\nSenior Lecturer with the Faculty of Information\nScience and Technology, Multimedia University,\nMalaysia. She has been a certified professional\ntechnologist, since 2018, a member of the Interna-\ntional Association of Engineers, since 2020, and\nthe Outcome-Based Education Consultant and a\nTrainer. Her research interests include action recognition, computer vision,\ngait recognition, natural language processing, and deep learning.\nHENG SIONG LIM (Senior Member, IEEE)\nreceived the B.Eng. degree (Hons.) in electrical\nengineering from Universiti Teknologi Malaysia,\nin 1999, and the M.Eng.Sc. and Ph.D. degrees\nin engineering focusing on signal processing for\nwireless communications from Multimedia Uni-\nversity, in 2002 and 2008, respectively. He is\ncurrently a Professor with the Faculty of Engineer-\ning and Technology, Multimedia University. His\ncurrent research interests include signal processing\nfor advanced communication systems, with an emphasis on detection and\nestimation theory and their applications.\nKIAN MING LIM (Senior Member, IEEE)\nreceived the B.IT. (Hons.) degree in information\nsystems engineering and the Master of Engineer-\ning Science (M.Eng.Sc.) and Ph.D. (I.T.) degrees\nfrom Multimedia University. He is currently a Lec-\nturer with the Faculty of Information Science and\nTechnology, Multimedia University. His research\nand teaching interests include machine learn-\ning, deep learning, computer vision, and pattern\nrecognition.\nTAKEKI MUKAIDA received the Associate of\nScience degree in engineering from the Gifu Col-\nlege, National Institute of Technology, in March\n2022. He is currently pursuing the Bachelor of\nScience degree in informatics with the University\nof Electro-Communications.\nVOLUME 11, 2023 63091",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7993134260177612
    },
    {
      "name": "Spectrogram",
      "score": 0.7688443660736084
    },
    {
      "name": "Speech recognition",
      "score": 0.6520588397979736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5621041655540466
    },
    {
      "name": "Speech processing",
      "score": 0.4665652811527252
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43199974298477173
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173029219",
      "name": "Multimedia University",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I20529979",
      "name": "University of Electro-Communications",
      "country": "JP"
    }
  ],
  "cited_by": 16
}