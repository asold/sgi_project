{
    "title": "Choose a Transformer: Fourier or Galerkin",
    "url": "https://openalex.org/W3172643943",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2385589019",
            "name": "Cao, Shuhao",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2127981460",
        "https://openalex.org/W2133619711",
        "https://openalex.org/W2934526567",
        "https://openalex.org/W3100269082",
        "https://openalex.org/W2133560833",
        "https://openalex.org/W2052121881",
        "https://openalex.org/W3098951945",
        "https://openalex.org/W3137474564",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W3123883114",
        "https://openalex.org/W2963088785",
        "https://openalex.org/W1582734821",
        "https://openalex.org/W2983902802",
        "https://openalex.org/W3125045721",
        "https://openalex.org/W3169316583",
        "https://openalex.org/W3046774627",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W1984259047",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3175217541",
        "https://openalex.org/W3161703459",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W1601728065",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2964324019",
        "https://openalex.org/W3045668026",
        "https://openalex.org/W3123615524",
        "https://openalex.org/W3014009018",
        "https://openalex.org/W2767286248",
        "https://openalex.org/W2912389156",
        "https://openalex.org/W2930017973",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2997347790",
        "https://openalex.org/W3150635270",
        "https://openalex.org/W2963587345",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2784733489",
        "https://openalex.org/W3200673624",
        "https://openalex.org/W2521593519",
        "https://openalex.org/W2242464395",
        "https://openalex.org/W2051575485",
        "https://openalex.org/W2963485221",
        "https://openalex.org/W3036157773",
        "https://openalex.org/W1525132831",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W1997221482",
        "https://openalex.org/W1983078963",
        "https://openalex.org/W2979636403",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W3213362119",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W3125056032",
        "https://openalex.org/W1866311589",
        "https://openalex.org/W3099878876",
        "https://openalex.org/W3166702123",
        "https://openalex.org/W3208463764",
        "https://openalex.org/W3101765447",
        "https://openalex.org/W2964183068",
        "https://openalex.org/W2963901342",
        "https://openalex.org/W3140870552",
        "https://openalex.org/W2963149188",
        "https://openalex.org/W3020102814",
        "https://openalex.org/W3134307371",
        "https://openalex.org/W2995273672",
        "https://openalex.org/W2101743932",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W3137898953",
        "https://openalex.org/W2139923370",
        "https://openalex.org/W2110114082",
        "https://openalex.org/W2037028678",
        "https://openalex.org/W3212935550",
        "https://openalex.org/W3163993681",
        "https://openalex.org/W3021748939",
        "https://openalex.org/W2999467285",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3027734040",
        "https://openalex.org/W3200336025",
        "https://openalex.org/W3168992342",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2970423380",
        "https://openalex.org/W2899283552",
        "https://openalex.org/W3118310857",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W2964261857",
        "https://openalex.org/W1961194538",
        "https://openalex.org/W3162090017",
        "https://openalex.org/W2963889991",
        "https://openalex.org/W3202621745",
        "https://openalex.org/W3132430896",
        "https://openalex.org/W2996324165",
        "https://openalex.org/W3120633509",
        "https://openalex.org/W3173365702",
        "https://openalex.org/W2515505748",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3146803896"
    ],
    "abstract": "In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need for the first time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efficacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the first time that the softmax normalization in the scaled dot-product attention is sufficient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers' equation, an interface Darcy flow, and an inverse interface coefficient identification problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows significant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts.",
    "full_text": "Choose a Transformer: Fourier or Galerkin\nShuhao Cao\nDepartment of Mathematics and Statistics\nWashington University in St. Louis\ns.cao@wustl.edu\nAbstract\nIn this paper, we apply the self-attention from the state-of-the-art Transformer in\nAttention Is All You Need [88] for the ﬁrst time to a data-driven operator learning\nproblem related to partial differential equations. An effort is put together to explain\nthe heuristics of, and to improve the efﬁcacy of the attention mechanism. By\nemploying the operator approximation theory in Hilbert spaces, it is demonstrated\nfor the ﬁrst time that the softmax normalization in the scaled dot-product attention\nis sufﬁcient but not necessary. Without softmax, the approximation capacity of a\nlinearized Transformer variant can be proved to be comparable to a Petrov-Galerkin\nprojection layer-wise, and the estimate is independent with respect to the sequence\nlength. A new layer normalization scheme mimicking the Petrov-Galerkin projec-\ntion is proposed to allow a scaling to propagate through attention layers, which\nhelps the model achieve remarkable accuracy in operator learning tasks with unnor-\nmalized data. Finally, we present three operator learning experiments, including\nthe viscid Burgers’ equation, an interface Darcy ﬂow, and an inverse interface\ncoefﬁcient identiﬁcation problem. The newly proposed simple attention-based\noperator learner, Galerkin Transformer, shows signiﬁcant improvements in both\ntraining cost and evaluation accuracy over its softmax-normalized counterparts.\n1 Introduction\nPartial differential equations (PDEs) arise from almost every multiphysics and biological systems,\nfrom the interaction of atoms to the merge of galaxies, from the formation of cells to the change\nof climate. Scientists and engineers have been working on approximating the governing PDEs of\nthese physical systems for centuries. The emergence of the computer-aided simulation facilitates\na cost-friendly way to study these challenging problems. Traditional methods, such as ﬁnite el-\nement/difference [20, 22], spectral methods [ 12], etc., leverage a discrete structure to reduce an\ninﬁnite dimensional operator map to a ﬁnite dimensional approximation problem. Meanwhile, in the\nﬁeld practice of many scientiﬁc disciplines, substantial data for PDE-governed phenomena available\non discrete grids enable modern black-box models like Physics-Informed Neural Network (PINN)\n[71, 62, 49] to exploit measurements on collocation points to approximate PDE solutions.\nNonetheless, for traditional methods or data-driven function learners such as PINN, given a PDE,\nthe focus is to approximate a single instance, for example, solving for an approximated solution\nfor one coefﬁcient with a ﬁxed boundary condition. A slight change to this coefﬁcient invokes a\npotentially expensive re-training of any data-driven function learners. In contrast, an operator learner\naims to learn a map between inﬁnite-dimensional function spaces, which is much more difﬁcult\nyet rewarding. A well-trained operator learner can evaluate many instances without re-training or\ncollocation points, thus saving valuable resources, and poses itself as a more efﬁcient approach in\nthe long run. Data-driven resolution-invariant operator learning is a booming new research direction\n[60, 5, 56, 64, 90, 57, 61, 91, 37, 74]. The pioneering model, DeepONet [60], attributes architecturally\nto a universal approximation theorem for operators [18]. Fourier Neural Operator (FNO) [57] notably\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2105.14995v4  [cs.LG]  1 Nov 2021\nshows an awing state-of-the-art performance outclassing classic models such as the one in [100] by\norders of magnitudes in certain benchmarks.\nUnder a supervised setting, an operator learner is trained with the operator’s input functions and\ntheir responses to the inputs as targets. Since both functions are sampled at discrete grid points,\nthis is a special case of a seq2seq problem [81]. The current state-of-the-art seq2seq model\nis the Transformer ﬁrst introduced in [ 88]. As the heart and soul of the Transformer, the scaled\ndot-product attention mechanism is capable of unearthing the hidden structure of an operator by\ncapturing long-range interactions. Inspired by many insightful pioneering work in Transformers\n[50, 19, 75, 84, 96, 97, 95, 59, 76, 66], we have modiﬁed the attention mechanism minimally yet in a\nmathematically profound manner to better serve the purpose of operator learning.\nAmong our new Hilbert space-inspired adaptations of the scaled dot-product attention, the ﬁrst\nand foremost change is: no softmax, or the approximation thereof. In the vanilla attention [ 88],\nthe softmax succeeding the matrix multiplication convexiﬁes the weights for combining different\npositions’ latent representations, which is regarded as an indispensable ingredient in the positive\nkernel interpretation of the attention mechanism [84]. However, softmax acts globally in the sequence\nlength dimension for each row of the attention matrix, and further adds to the quadratic complexity\nof the attention in the classic Transformer. Theory-wise, instead of viewing “ row ≈word” in the\nNatural Language Processing (NLP) tradition, the columns of the query/keys/values are seen as\nsampling of functions in Hilbert spaces on discretized grids. Thus, taking the softmax away allows\nus to verify a discrete Ladyzhenskaya–Babuška–Brezzi (LBB) condition, which further amounts to\nthe proof that the newly proposed Galerkin-type attention can explicitly represent a Petrov-Galerkin\nprojection, and this approximation capacity is independent of the sequence length (Theorem 4.3).\nNumerically, the softmax-free models save valuable computational resources, outperforming the\nones with the softmax in terms of training FLOP and memory consumption (Section 5). Yet in an\nablation study, the training becomes unstable for softmax-free models (Table 8). To remedy this, a new\nGalerkin projection-type layer normalization scheme is proposed to act as a cheap diagonal alternative\nto the normalizations explicitly derived in the proof of the Petrov-Galerkin interpretation (equation\n(40)). Since a learnable scaling can now be propagated through the encoder layers, the attention-\nbased operator learner with this new layer normalization scheme exhibits better comprehension of\ncertain physical properties associated with the PDEs such as the energy decay. Combining with other\napproximation theory-inspired tricks including a diagonally dominant rescaled initialization for the\nprojection matrices and a layer-wise enrichment of the positional encodings, the evaluation accuracies\nin various operator learning tasks are boosted by a signiﬁcant amount.\nMain contributions. The main contributions of this work are summarized as follows.\n• Attention without softmax. We propose a new simple self-attention operator and its linear\nvariant without the softmax normalization. Two new interpretations are offered, together with the\napproximation capacity of the linear variant proved comparable to a Petrov-Galerkin projection.\n• Operator learner for PDEs.We combine the newly proposed attention operators with the current\nbest state-of-the-art operator learner Fourier Neural Operator (FNO) [57] to signiﬁcantly improve\nits evaluation accuracy in PDE solution operator learning benchmark problems. Moreover, the\nnew model is capable of recovering coefﬁcients based on noisy measurements that traditional\nmethods or FNO cannot accomplish.\n• Experimental results. We present three benchmark problems to show that operator learners\nusing the newly proposed attentions are superior in computational/memory efﬁciency, as well\nas in accuracy versus those with the conventional softmax normalization. The PyTorch codes to\nreproduce our results are available as an open-source software. 1\n2 Related Works\nOperator learners related to PDEs. In [4, 5], certain kernel forms of the solution operator of\nparametric PDEs are approximated using graph neural networks. The other concurrent notable\napproach is DeepONet [60, 61]. [56] further improves the kernel approach by exploiting the multilevel\ngrid structure. [57] proposes a discretization-invariant operator learner to achieve a state-of-the-art\n1https://github.com/scaomath/galerkin-transformer\n2\nperformance in certain benchmark problems. [90, 91] proposed a DeepONet roughly equivalent to an\nadditive attention, similar to the one in the Neural Turing Machine (NMT) in [7]. Model/dimension\nreduction combined with neural nets is another popular approach to learn the solution operator for\nparametric PDEs [10, 64, 55, 24]. Deep convolutional neural networks (DCNN) are widely applied\nto learn the solution maps with a ﬁxed discretization size [1, 9, 40, 36, 35, 100, 86]. Recently, DCNN\nhas been successfully applied in various inverse problems [ 35, 47] such as Electrical Impedance\nTomography (EIT). To our best knowledge, there is no work on data-driven approaches to an inverse\ninterface coefﬁcient identiﬁcation for a class of coefﬁcients with random interface geometries.\nAttention mechanism and variants. Aside from the ground-breaking scaled dot-product attention\nin [88], earlier [7] proposed an additive content-based attention, however, with a vanishing gradient\nproblem due to multiple nonlinearity composition. [25] shows the ﬁrst effort in removing the softmax\nnormalization in [7] after the projection, however, it still uses a Sigmoid nonlinearity before the\nadditive interpolation propagation stage, and performs worse than its softmax counterpart. The\ncurrent prevailing approach to linearize the attention leverages the assumption of the existence of a\nfeature map to approximate the softmax kernel [50, 19, 70]. Another type of linearization exploits\nthe low-rank nature of the matrix product using various methods such as sampling or projection\n[73, 11, 79, 92], or fast multipole decomposition [65]. The conjecture in [75] inspires us to remove\nthe softmax overall. [ 76] ﬁrst proposed the inverse sequence length scaling normalization for a\nlinear complexity attention without the softmax, however, the scaling normalization has not been\nextensively studied in examples and performs worse.\nVarious studies on Transformers. The kernel interpretation in [84] inspires us to reformulate the\nattention using the Galerkin projection. [95, Theorem 2] gives a theoretical foundation of removing\nthe softmax normalization to formulate the Fourier-type attention. The Nyström approximation\n[97] essentially acknowledges the similarity between the attention matrix and an integral kernel.\n[96, 66, 59] inspires us to try different layer normalization and the rescaled diagonally dominant\ninitialization schemes. The practices of enriching the latent representations with the positional\nencoding recurrently in our work trace back to [2, 26], and more recently, contribute to the success\nof AlphaFold 2 [ 48], as it is rewarding to exploit the universal approximation if the target has a\ndependence ansatz in the coordinate frame and/or transformation group but hard to be explicitly\nquantiﬁed. Other studies on adapting the attention mechanisms to conserve important physical\nproperties are in [82, 31, 44].\n3 Operator learning related to PDEs\nClosely following the setup in [56, 57], we consider a data-driven model to approximate a densely-\ndeﬁned operator T : H1 →H2 between two Hilbert spaces with an underlying bounded spacial\ndomain Ω ⊂Rm. The operator T to be learned is usually related to certain physical problems, of\nwhich the formulation is to seek the solution to a PDE of the following two types.\nParametric PDE: given coefﬁcient a∈A, and source f ∈Y, ﬁnd u∈X such that La(u) = f.\n(i) To approximate the nonlinear mapping from the varying parameter ato the solution with a\nﬁxed right-hand side, T : A→X , a↦→u.\n(ii) The inverse coefﬁcient identiﬁcation problem to recover the coefﬁcient from a noisy measure-\nment ˜uof the steady-state solution u, in this case, T : X→A , ˜u↦→a.\nNonlinear initial value problem: given u0 ∈H0, ﬁnd u∈C([0,T]; H) such that ∂tu+ N(u) = 0.\n(iii) Direct inference from the initial condition to the solution. T : H0 →H, u0(·) ↦→u(t1,·) with\nt1 ≫∆twith t1 much greater than the step-size in traditional explicit integrator schemes.\nUsing (i) as an example, based on the given N observations {a(j),u(j)}N\nj=1 and their approximations\n{a(j)\nh ,u(j)\nh }deﬁned at a discrete grid of size h≪1, the goal of our operator learning problem is to\nbuild an approximation Tθ to T, such that Tθ(ah) is a good approximation to u= L−1\na f =: T(a) ≈\nuh independent of the mesh size h, where ah and uh are in ﬁnite dimensional spaces Ah,Xh on this\ngrid. We further assume that a(j) ∼νfor a measure νcompactly supported on A, and the sampled\ndata form a reasonably sized subset of Arepresentative of ﬁeld applications. The loss J(θ) is\nJ(θ) := Ea∼ν\n[\n∥\n(\nTθ(a) −u∥2\nH+ G(a,u; θ)\n]\n(1)\n3\nand in practice is approximated using the sampled observations on a discrete grid\nJ(θ) ≈ 1\nN\nN∑\nj=1\n{(\nTθ\n(\na(j)\nh\n)\n−u(j)\nh\n2\nH+ G\n(\na(j)\nh ,u(j)\nh ; θ\n)}\n. (2)\nIn example (i), ∥·∥His the standard L2-norm, and G(a,u; θ) serves as a regularizer with strength γ\nand is problem-dependent. In Darcy ﬂow where La := −∇·(a∇(·)), it is γ∥a∇(Tθ(a) −u)∥2\nL2(Ω),\nsince u∈H1+α(Ω) (α> 0 depends on the regularity of a) and a∇u∈H(div; Ω) a priori. For the\nevaluation metric, we drop the G(a,u; θ) term, and monitor the minimization of (2) using ∥·∥H.\n4 Attention-based operator learner\nFeature extractor. We assume the functions in both inputs and targets are sampled on a uniform\ngrid. In an operator learning problem on Ω ⊂R1, a simple feedforward neural network (FFN) is used\nas the feature extractor that is shared by every position (grid point).\nInterpolation-based CNN. If Ω ⊂R2, inspired by the multilevel graph kernel network in [ 56],\nwe use two 3-level interpolation-based CNNs (CiNN) as the feature extractor, but also as the\ndownsampling and upsampling layer, respectively, in which we refer to restrictions/prolongations\nbetween the coarse/ﬁne grids both as interpolations. For the full details of the network structure\nplease refer to Appendix B.\nRecurrent enrichment of positional encoding. The Cartesian coordinates of the grid, on which\nthe attention operator’s input latent representation reside, are concatenated as additional feature\ndimension(s) to the input, as well as to each latent representation in every attention head.\nProblem-dependent decoder. The decoder is a problem-dependent admissible network that maps\nthe learned representations from the encoder back to the target dimension. For smooth and regular\nsolutions in H1+α(Ω), we opt for a 2-layer spectral convolution that is the core component in [57].\nA simple pointwise feedforward neural network (FFN) is used for nonsmooth targets in L∞(Ω).\n4.1 Simple self-attention encoder\nmm LN LN mm add FFN add softmax \n=\n=\n=\n(a)\nmm \nLN \nLN \nmm add FFN add \n=\n=\n=\n(b)\nFigure 1: Comparison of the vanilla attention [88] with the Galerkin-type simple self-attention in a\nsingle head; (a) in the standard softmax attention, the softmax is applied row-wise after the matrix\nproduct matmul; (b) a mesh-weighted normalization allows an integration-based interpretation.\nThe encoder contains a stack of identical simple attention-based encoder layers. For simplicity, we\nconsider a single attention head that maps y ∈Rn×d to another element in Rn×d, and deﬁne the\n4\ntrainable projection matrices, and the latent representations Q/K/V as follows.\nWQ,WK,WV ∈Rd×d, and Q:= yWQ, K := yWK, V := yWV. (3)\nWe propose the following simple attention that (i) uses a mesh (inverse sequence length)-weighted\nnormalization without softmax, (ii) allows a scaling to propagate through the encoder layers.\nAttnsp : Rn×d →Rn×d, ˜y ←y + Attn†(y), y ↦→˜y + g(˜y), (4)\nwhere the head-wise normalizations are applied pre-dot-product: for †∈{ f,g},\n(Fourier-type attention) z = Attnf(y) := ( ˜Q˜K⊤)V/n, (5)\n(Galerkin-type attention) z = Attng(y) := Q( ˜K⊤˜V)/n, (6)\nand ˜⋄denotes a trainable non-batch-based normalization. As in the classic Transformer [ 88], and\ninspired by the Galerkin projection interpretation, we choose ˜⋄as the layer normalization Ln(⋄),\nand g(·) as the standard 2-layer FFN identically applied on every position (grid point). In simple\nattentions, the weight for each row of V, or column of Qin the linear variant, is not all positive\nanymore. This can be viewed as a cheap alternative to the cosine similarity-based attention.\nRemark 4.1. If we apply the regular layer normalization rule that eliminates any scaling:\ny ↦→Ln\n(\ny + Attn†(y) + g\n(\nLn(y + Attn†(y))\n))\n, where Attn†(y) := Q(K⊤V)/n, (7)\nthen this reduces to the efﬁcient attention ﬁrst proposed in [76].\n4.1.1 Structure-preserving feature map as a function of positional encodings\nConsider an operator learning problem with an underlying domain Ω ⊂ R1. {xi}n\ni=1 denotes\nthe set of grid points in the discretized Ω such that the weight 1/n = h is the mesh size. Let\nζq(·),φk(·),ψv(·) : Ω →R1×d denote the feature maps of Q,K,V , i.e., the i-th row of Q,K,V\nwritten as qi = ζq(xi), ki = φk(xi), vi = ψv(xi). They are, in the NLP convention, viewed as the\nfeature (embedding) vector at the i-th position, respectively. The inter-position topological structure\nsuch as continuity/differentiability in the same feature dimension is learned thus not explicit. The\nfollowing ansatz for Q/K/V in the same attention head is fundamental to our new interpretations.\nAssumption 4.2. The columns of Q/K/V, respectively, contain the vector representations of the\nlearned basis functions spanning certain subspaces of the latent representation Hilbert spaces.\nUsing V ∈Rn×d with a full column rank as an example, its columns contain potentially a set of bases\n{vj(·)}d\nj=1 evaluated at the grid points (degrees of freedom, or DoFs). Similarly, the learned bases\nwhose DoFs form the columns of Q,K are denoted as {qj(·)}d\nj=1, {kj(·)}d\nj=1, as well as {zj(·)}d\nj=1\nfor the outputs in (5) and (6). To be speciﬁc, the j-th column of V, denoted by vj, then stands for\na vector representation of the j-th basis function evaluated at each grid point, i.e., its l-th position\nstands for (vj)l = vj(xl). Consequently, the row vi = (v1(xi),...,v d(xi)) can be alternatively\nviewed as the evaluation of a vector latent basis function at xi.\n4.1.2 Fourier-type attention of a quadratic complexity\n     \n-th row\n         \n-th row\n-th column\n-th row\n-th column\n-th column\n-th column\n-th column\nFigure 2: A dissection of Fourier-type attention’s output. Bothmatmuls have complexity O(n2d).\nIn the Fourier-type attention (5), Q,K are assumed to be normalized for simplicity, the j-th column\n(1 ≤j ≤d) in the i-th row (1 ≤i≤n) of z is computed by (see Figure 2):\n(zi)j = h(QK⊤)i•vj = h\n(\nqi ·k1,..., qi ·kl,..., qi ·kn\n)⊤\n·vj\n= h\nn∑\nl=1\n(qi ·kl)(vj)l ≈\n∫\nΩ\n(\nζq(xi) ·φk(ξ)\n)\nvj(ξ) dξ,\n(8)\n5\nwhere the h-weight facilitates the numerical quadrature interpretation of the inner product. Concate-\nnating columns 1 ≤j ≤dyields the i-row zi of the output z: zi ≈\n∫\nΩ\n(\nζq(xi) ·φk(ξ)\n)\nψv(ξ) dξ.\nTherefore, without the softmax nonlinearity, the local dot-product attention output at i-th row\ncomputes approximately an integral transform with a non-symmetric learnable kernel function\nκ(x,ξ) := ζq(x)φk(ξ) evaluated at xi, whose approximation property has been studied in [ 95,\nTheorem 2], yet without the logits technicality due to the removal of the softmax normalization.\nAfter the skip-connection, if we further exploit the learnable nature of the method and assume\nWV = diag{δ1,··· ,δd}such that δj ̸= 0 for 1 ≤j ≤d, under Assumption 4.2:\nδ−1\nj vj(x) ≈zj(x) −\n∫\nΩ\nκ(x,ξ)vj(ξ) dξ, for j = 1,··· ,d, and x∈{xi}n\ni=1. (9)\nThis is the forward propagation of the Fredholm equation of the second-kind for each vj(·). When\nusing an explicit orthogonal expansion such as Fourier to solve for {vj(·)}d\nj=1, or to seek for a better\nset of {vj(·)}in our case, it is long known being equivalent to the Nyström’s method with numerical\nintegrations [8] (similar to the h = 1/nweighted sum). Therefore, the successes of the random\nFourier features in [19, 70] and the Nyströmformer’s approximation [97] are not surprising.\nFinally, we name this type of simple attention “Fourier” is due to the striking resemblance between\nthe scaled dot-product attention and a Fourier-type kernel [30] integral transform, since eventually the\ntarget resides in a Hilbert space with an underlying spacial domain Ω, while the latent representation\nspace parallels a “frequency” domain onΩ∗. This also bridges the structural similarity of the scaled\ndot-product attention with the Fourier Neural Operator [57] where the Fast Fourier Transform (FFT)\ncan be viewed as a non-learnable change of basis.\n4.1.3 Galerkin-type attention of a linear complexity\n         \n-th row\n-th row\n \n \n-th column\n-th column\n-th column\n-th column\nFigure 3: A dissection of Galerkin-type attention’s output. Both matmuls have complexity O(nd2).\nFor the Galerkin-type simple attention in (6), K,V are assumed to be normalized for simplicity, we\nﬁrst consider the i-th entry in the j-th column zj of z (see Figure 3):\n(zj)i = hq⊤\ni ·(K⊤V)•j, (10)\nwhich is the inner product of the i-th row of Qand the j-th column of K⊤V. Thus,\nzj = h\n( | | | |\nq1 q2 ··· qn\n| | | |\n)⊤\n(K⊤V)•j = h\n\n(K⊤V)⊤\n•j\n\n\nq1\n...\nqd\n\n\n\n\n⊤\n(11)\nThis reads as: (K⊤V)•j contains the coefﬁcients for the linear combination of the vector represen-\ntations {ql}d\nl=1 of the bases stored in Q’s column space to form the outputz. Meanwhile, the j-th\ncolumn (K⊤V)•j of K⊤V consists the inner product of j-th column of V with every column of K.\nzj = h\nd∑\nl=1\nql(K⊤V)lj, where (K⊤V)•j =\n(\nk1 ·vj,k2 ·vj,··· ,kd ·vj)⊤\n. (12)\nAs a result, using Assumption 4.2, and for simplicity the latent Hilbert spaces Q,K,Vare assumed\nto be deﬁned on the same spacial domain Ω, i.e., kl(·), vj(·) evaluated at every xi are simply their\nvector representations kl (1 ≤l≤d) and vj, we have the functions represented by the columns of\nthe output z can be then compactly written as: rewriting ⟨vj,kl⟩:= (K⊤V)lj\nzj(x) :=\nd∑\nl=1\n⟨vj,kl⟩ql(x), for j = 1,··· ,d, and x∈{xi}n\ni=1, (13)\n6\nwhere the bilinear form ⟨·,·⟩: V×K→ R. (13) can be also written in a componentwise form:\nzj(xi) := (zj)i = h\nd∑\nl=1\n(kl ·vj)(ql)i ≈\nd∑\nl=1\n(∫\nΩ\nvj(ξ)kl(ξ) dξ\n)\nql(xi). (14)\nTherefore, when {⋄j(·)}d\nj=1, ⋄∈{ q,k,v }consist approximations to three sets of bases for potentially\ndifferent subspaces, and if we set the trial spaces as the column spaces of Qand the test space as that\nof K, respectively, the forward propagation of the Galerkin-type attention is a recast of a learnable\nPetrov–Galerkin-type projection (cf. Appendix D.1) for every basis represented by the columns of V.\nWhile the form of (14) suggests the orthonormality of the basis represented by Q,K,V , as well as\nbeing of full column ranks, the learnable nature of the method suggests otherwise (see Appendix D).\nAt last, we have the following strikingly simple yet powerful approximation result.\nTheorem 4.3 (Céa-type lemma, simpliﬁed version) . Consider a Hilbert space Hdeﬁned on a\nbounded domain Ω ⊂Rm discretized by ngrid points, and f ∈H. y ∈Rn×d is the current latent\nrepresentation for n > d > mand full column rank. Qh ⊂Q⊂H and Vh ⊂V⊂H are the\nlatent approximation subspaces spanned by basis functions with the columns of Qand V in (3) as\ndegrees of freedom, respectively, and0 <dim Qh = r≤dim Vh = d. Let b(·,·) : V×Q→ R be\na continuous bilinear form, and if for any ﬁxed q ∈Qh the functional norm of b(·,q) is bounded\nbelow by c> 0, then there exists a learnable map gθ(·) that is the composition of the Galerkin-type\nattention operator with an updated set of projection matrices {WQ,WK,WV}, and a pointwise\nuniversal approximator, such that forfh ∈Qh being the best approximation of f in ∥·∥Hit holds:\n∥f −gθ(y)∥H≤c−1 min\nq∈Qh\nmax\nv∈Vh\n|b(v,fh −q)|\n∥v∥H\n+ ∥f −fh∥H. (15)\nRemarks on and interpretations of the best approximation result. Theorem 4.3 states that the\nGalerkin-type attention has the architectural capacity to represent a quasi-optimal approximation in\n∥·∥Hin the current subspace Qh. For the mathematically rigorous complete set of notations and the\nfull details of the proof we refer the readers to Appendix D.3. Even though Theorem 4.3 is presented\nfor a single instance of f ∈H for simplicity, the proof shows that the attention operator is fully\ncapable of simultaneously approximating a collection of functions (Appendix D.3.4).\nEstimate (15) comes with great scalability with respect to the sequence length in that it all boils down\nto whether cis independent of nin the lower bound of∥b(·,q)∥V′\nh\n. The existence of an n-independent\nlower bound is commonly known as the discrete version of the Ladyzhenskaya–Babuška–Brezzi\n(LBB) condition [21, Chapter 6.12], also referred as the Banach-Neˇcas-Babuška (BNB) condition in\nGalerkin methods on Banach spaces [29, Theorem 2.6].\nAs the cornerstone of the approximation to many PDEs, the discrete LBB condition establishes the\nsurjectivity of a map from Vh to Qh. In a simpliﬁed context (15) above of approximating functions\nusing this linear attention variant (Q: values, query, V: keys), it roughly translates to: for an incoming\n“query” (function f in a Hilbert space), to deliver its best approximator in “value” (trial function\nspace), the “key” (test function space) has to be sufﬁciently rich such that there exists a key to unlock\nevery possible value.\nDynamic basis update. Another perspective is to interpret the Galerkin-type dot-product attention\n(14) as a change of basis: essentially, the new set of basis is the column space of Q, and how to\nlinearly combine the bases in Qis based on the inner product (response) of the corresponding feature\ndimension’s basis in V against every basis in K. From this perspective ( Q: values, K: keys, V:\nquery), we have the following result of a layer-wise dynamical change of basis: through testing\nagainst the “keys”, a latent representation is sought such that “query” (input trial space) and “values”\n(output trial space) can achieve the minimum possible difference under a functional norm; for details\nand the proof please refer to Appendix D.3.4.\nTheorem 4.4 (layer-wise dynamic basis update, simple version) . Under the same assumption as\nTheorem 4.3, it is further assumed that b(·,q) is bounded below on Kh ⊂ K= V ⊂ Hand\na(·,·) : V×K→ R is continuous. Then, there exists a set of projection matrices to update the value\nspace {˜ql(·)}d\nl=1 ⊂Qh = span{ql(·)}d\nl=1, for zj ∈Qh (j = 1,··· ,d) obtained through the basis\nupdate rule (14), it holds\na(vj,·) −b(·,zj)\n\nK′\nh\n≤min\nq∈Qh\nmax\nk∈Kh\n|a(vj,k) −b(k,q)|\n∥k∥K\n. (16)\n7\nThe role of feed-forward networks and positional encodings in the dynamic basis update. Due\nto the presence of the concatenated coordinates x := ∥n\ni=1xi ∈Rn×m to the latent representation\ny, the pointwise subnetwork gs(·) : Rn×m →Rn×d of the nonlinear universal approximator (FFN)\nin each attention block is one among many magics of the attention mechanism. In every attention\nlayer, the basis functions in Qh/Kh/Vh are being constantly enriched by span{wj ∈Xh : wj(xi) =\n(gs(x))ij,1 ≤j ≤d}⊂H , thus being dynamically updated to try to capture how an operator\nof interest responses to the subset of inputs. Despite the fact that the FFNs, when being viewed\nas a class of functions, bear no linear structure within, the basis functions produced this way act\nas a building block to characterize a linear space for a learnable projection. This heuristic shows\nto be effective when the target is assumed to be a function of the (relative) positional encodings\n(coordinates, transformation groups, etc.), in that this is incorporated in many other attention-based\nlearners with applications in physical sciences [82, 31, 44, 48].\n5 Experiments\nIn this section we perform a numerical study the proposed Fourier Transformer ( FT) with the\nFourier-type encoder, and the Galerkin Transformer (GT) with the Galerkin-type encoder, in various\nPDE-related operator learning tasks. The models we compare our newly proposed models with\nare the operator learners with the simple attention replaced by the standard softmax normalized\nscaled dot-product attention ( ST) [88], and a linear variant ( LT) [76] in which two independent\nsoftmax normalizations are applied on Q,K separately.2 The data are obtained courtesy of the PDE\nbenchmark under the MIT license.3 For full details of the training/evaluation and model structures\nplease refer to Appendix C.\nInstead of the standard Xavier uniform initialization [34], inspired by the interpretations of Theorem\n4.3 in Appendix D.3.4, we modify the initialization for the projection matrices slightly as follows\nW⋄\ninit ←ηU + δI, for ⋄∈{ Q,K,V }, (17)\nwhere U = (xij) is a random matrix using the Xavier initialization with gain 1 such that xij ∼\nU([−\n√\n3/d,\n√\n3/d]), and δ is a small positive number. In certain operator learning tasks, we\nfound that this tiny modiﬁcation boosts the evaluation performance of models by up to 50% (see\nAppendix C.2) and improves the training stability acting as a cheap remedy to the lack of a softmax\nnormalization. We note that similar tricks have been discovered concurrently in [23].\nUnsurprisingly, when compared the memory usage and the speed of the networks (Table 1), the\nFourier-type attention features a 40%–50% reduction in memory versus the attention with a softmax\nnormalization. The Galerkin attention-based models have a similar memory proﬁle with the standard\nlinear attention, it offers up to a 120% speed boost over the linear attention in certain tests.\nTable 1: The memory usage/FLOP/complexity comparison of the models. Batch size: 4; the CUDA\nmem (GB): the sum of the self_cuda_memory_usage; GFLOP: Giga FLOP for 1 backpropagation\n(BP); both are from the PyTorch autograd proﬁler for 1 BP averaging from 1000 BPs; the mem\n(GB) is recorded from nvidia-smi of the memory allocated for the active Python process during\nproﬁling; the speed (iteration per second) is measured during training; the exponential operation is\nassumed to have an explicit complexity of ce >1 [14].\nExample 1:n= 8192 Encoders only:n= 8192,d= 128,l= 10 Computational complexity\nof the dot-product per layerMem CUDA Mem Speed GFLOP Mem CUDA Mem Speed GFLOP\nST 18.39 31 .06 5 .02 1393 18 .53 31 .34 4 .12 1876 O(n2ced)\nFT 10.05 22 .92 6 .10 1138 10 .80 22 .32 5 .46 1610 O(n2d)\nLT 2.55 2 .31 12 .70 606 2 .73 2 .66 10 .98 773 O(n(d2+ced))\nGT 2.36 1.93 27.15 275 2.53 2.33 19.20 412 O(nd2)\nThe baseline models for each example are the best operator learner to-date, the state-of-the-art\nFourier Neural Operator (FNO) in [ 57] but without the original built-in batch normalization. All\nattention-based models match the parameter quota of the baseline, and are trained using the loss in\n2https://github.com/lucidrains/linear-attention-transformer\n3https://github.com/zongyi-li/fourier_neural_operator\n8\n(2) with the same 1cycle scheduler [78] for 100 epochs. For fairness, we have also included the\nresults for the standard softmax normalized models (ST and LT) using the new layer normalization\nscheme in (5) and (6). We have retrained the baseline with the same 1cycle scheduler using the\ncode provided in [57], and listed the original baseline results using a step scheduler of 500 epochs of\ntraining from [57] Example 5.1 and Example 5.2, respectively.\n5.1 Example 1: viscous Burgers’ equation\nIn this example, we consider a benchmark problem of the viscous Burgers’ equation with a periodic\nboundary condition on Ω := (0 ,1) in [57]. The nonlinear operator to be learned is the discrete\napproximations to the solution operator T : C0\np(Ω) ∩L2(Ω) →C0\np(Ω) ∩H1(Ω), u0(·) ↦→u(·,1).\nThe initial condition u0(·)’s are sampled following a Gaussian Random Field (GRF).\nThe result can be found in Table 2a. All attention-based operator learners achieve a resolution-\ninvariant performance similar with FNO1d in [57]. The new Galerkin projection-type layer normal-\nization scheme signiﬁcantly outperforms the regular layer normalization rule in this example, in\nwhich both inputs and targets are unnormalized. For full details please refer to Appendix C.2.\n5.2 Example 2: Darcy ﬂow\nIn this example, we consider another well-known benchmark −∇·(a∇u) = f for u∈H1\n0 (Ω) from\n[10, 57, 56, 64], and the operator to be learned is the approximations toT : L∞(Ω) →H1\n0 (Ω),a ↦→u,\nin which ais the coefﬁcient with a random interface geometry, and uis the weak solution. Here\nL∞(Ω) is a Banach space and cannot be compactly embedded in L2(Ω) (a Hilbert space), we choose\nto avoid this technicality as the ﬁnite dimensional approximation space can be embedded in L2(Ω)\ngiven that Ω is compact.\nThe result can be found in Table 2b. As the input/output are normalized, in contrast to Example 5.1,\nthe Galerkin projection-type layer normalization scheme does not signiﬁcantly outperform the regular\nlayer normalization rule in this example. The attention-based operator learners achieve on average\n30% to 50% better evaluation results than the baseline FNO2d (only on the ﬁne grid) using the same\ntrainer. For full details please refer to Appendix C.3.\nTable 2: (a) Evaluation relative error (×10−3) of Burgers’ equation 5.1. (b) Evaluation relative error\n(×10−2) of Darcy interface problem 5.2.\n(a)\nn= 512 n= 2048 n= 8192\nFNO1d [57] 15.8 14.6 13.9\nFNO1d1cycle 4.373 4 .126 4 .151\nFT regular Ln 1.400 1 .477 1 .172\nGT regular Ln 2.181 1 .512 2 .747\nST regular Ln 1.927 2 .307 1 .981\nLT regular Ln 1.813 1 .770 1 .617\nFT Ln onQ,K 1.135 1.123 1.071\nGT Ln onK,V 1.203 1.150 1.025\nST Ln onQ,K 1.271 1 .266 1 .330\nLT Ln onK,V 1.139 1 .149 1 .221\n(b)\nnf,nc= 141,43 nf,nc= 211,61\nFNO2d [57] 1.09 1 .09\nFNO2d1cycle 1.419 1 .424\nFT regular Ln 0.838 0.847\nGT regular Ln 0.894 0 .856\nST regular Ln 1.075 1 .131\nLT regular Ln 1.024 1 .130\nFT Ln onQ,K 0.873 0 .921\nGT Ln onK,V 0.839 0.844\nST Ln onQ,K 0.946 0 .959\nLT Ln onK,V 0.875 0 .970\n5.3 Example 3: inverse coefﬁcient identiﬁcation for Darcy ﬂow\nIn this example, we consider an inverse coefﬁcient identiﬁcation problem based on the same data\nused in Example 5.2. The input (solution) and the target (coefﬁcient) are reversed from Example\n5.2, and the noises are added to the input. The inverse problems in practice are a class of important\ntasks in many scientiﬁc disciplines such as geological sciences and medical imaging but much\nmore difﬁcult due to poor stability [51]. We aim to learn an approximation to an ill-posed operator\nT : H1\n0 (Ω) →L∞(Ω),u + ϵNν(u) ↦→a, where Nν(u) stands for noises related to the sampling\ndistribution and the data. ϵ= 0.01 means 1% of noise added in both training and evaluation, etc.\n9\nThe result can be found in Table 3. It is not surprising that FNO2d, an excellent smoother which\nﬁlters higher modes in the frequency domain, struggles in this example to recover targets consisting of\nhigh-frequency traits (irregular interfaces) from low-frequency prevailing data (smooth solution due\nto ellipticity). We note that, the current state-of-the-art methods [16] for inverse interface coefﬁcient\nidentiﬁcation need to carry numerous iterations to recover a single instance of a simple coefﬁcient\nwith a regular interface, provided that a satisfactory denoising has done beforehand. The attention-\nbased operator learner has capacity to unearth structurally how this inverse operator’s responses on a\nsubset, with various beneﬁts articulated in [56, 57, 5, 64, 10].\nTable 3: Evaluation relative error (×10−2) of the inverse problem 5.3.\nnf,nc= 141,36 nf,nc= 211,71\nϵ= 0 ϵ= 0.01 ϵ= 0.1 ϵ= 0 ϵ= 0.01 ϵ= 0.1\nFNO2d (onlynf) 13.71 13 .78 15 .12 13 .93 13 .96 15 .04\nFNO2d (onlync) 14.17 14 .31 17 .30 13 .60 13 .69 16 .04\nFT regular Ln1.799 2.467 6.814 1 .563 2 .704 8 .110\nGT regular Ln2.026 2 .536 6.659 1.732 2 .775 8 .024\nST regular Ln2.434 3 .106 7 .431 2 .069 3 .365 8 .918\nLT regular Ln2.254 3 .194 9 .056 2 .063 3 .544 9 .874\nFT Ln onQ,K 1.921 2 .717 6 .725 1.523 2.691 8.286\nGT Ln onK,V 1.944 2 .552 6 .689 1 .651 2 .729 7.903\nST Ln onQ,K 2.160 2 .807 6 .995 1 .889 3 .123 8 .788\nLT Ln onK,V 2.360 3 .196 8 .656 2 .136 3 .539 9 .622\n6 Conclusion\nWe propose a general operator learner based on a simple attention mechanism. The network is versatile\nand is able to approximate both the PDE solution operator and the inverse coefﬁcient identiﬁcation\noperator. The evaluation accuracy on the benchmark problems surpasses the current best state-of-\nthe-art operator learner Fourier Neural Operator (FNO) in [ 57]. However, we acknowledge the\nlimitation of this work: (i) similar to other operator learners, the subspace, on which we aim to\nlearn the operator’s responses, may be inﬁnite dimensional, but the operator must exhibit certain\nlow-dimensional attributes (e.g., smoothing property of the higher frequencies in GRF); (ii) it is not\nefﬁcient for the attention operator to be applied at the full resolution for a 2D problem, and this limits\nthe approximation to a nonsmooth subset such as functions in L∞; (iii) due to the order of the matrix\nproduct, the proposed linear variant of the scaled dot-product attention is non-causal thus can only\napply to encoder-only applications.\n7 Broader Impact\nOur work introduces the state-of-the-art self-attention mechanism the ﬁrst time to PDE-related\noperator learning problems. The new interpretations of attentions invite numerical analysts to work\non a more complete and delicate approximation theory of the attention mechanism. We have proved\nthe Galerkin-type attention’s approximation capacity in an ideal Hilbertian setting. Numerically, the\nnew attention-based operator learner has capacity to approximate the difﬁcult inverse coefﬁcient\nidentiﬁcation problem with an extremely noisy measurements, which was not attainable using\ntraditional iterative methods for nonlinear mappings. Thus, our method may pose a huge positive\nimpact in geoscience, medical imaging, etc. Moreover, traditionally the embeddings in Transformer-\nbased NLP models map the words to a high dimensional space, but the topological structure in the\nsame feature dimension between different positions are learned thereby not efﬁcient. Our proof\nprovides a theoretical guide for the search of feature maps that preserve, or even create, structures\nsuch as differentiability or physical invariance. Thus, it may contribute to the removal of the softmax\nnonlinearity to speed up signiﬁcantly the arduous training or pre-training of larger encoder-only\nmodels such as BERT [ 27], etc. However, we do acknowledge that our research may negatively\nimpact on the effort of building a cleaner future for our planet, as inverse problems are widely studied\nin reservoir detection, and we have demonstrated that the attention-based operator learner could\npotentially help to discover new fossil fuel reservoirs due to its capacity to infer the coefﬁcients from\nnoisy measurements.\n10\nAcknowledgments and Disclosure of Funding\nThe hardware to perform this work is kindly donated by Andromeda Saving Fund. The ﬁrst author was\nsupported in part by the National Science Foundation under grants DMS-1913080 and DMS-2136075.\nNo additional revenues are related to this work. We would like to thank the anonymous reviewers and\nthe area chair for the suggestions on improving this article. We would like to thank Dr. Long Chen\n(Univ of California Irvine) for the inspiration of and encouragement on the initial conceiving of this\npaper, as well as numerous constructive advices on revising this paper, not mentioning his persistent\ndedication of making publicly available tutorials [17] on writing beautiful vectorized code. 4 We\nwould like to thank Dr. Ari Stern (Washington Univ in St. Louis) for the help on the relocation during\nthe COVID-19 pandemic. We would like to thank Dr. Likai Chen (Washington Univ in St. Louis) for\nthe invitation to the Stats and Data Sci seminar at WashU that resulted the reboot of this study. 5 We\nwould like to thank Dr. Ruchi Guo (Univ of California Irvine) and Dr. Yuanzhe Xi (Emory Univ) for\nthe invaluable feedbacks on the choice of the numerical experiments. We would like to thank the\nKaggle community, including but not limited to Jean-François Puget (CPMP@Kaggle) for sharing a\nsimple Graph Transformer in TensorFlow,6 Murakami Akira (mrkmakr@Kaggle) for sharing a Graph\nTransformer with a CNN feature extractor in Tensorﬂow,7 and Cher Keng Heng (hengck23@Kaggle)\nfor sharing a Graph Transformer in PyTorch.8 We would like to thank daslab@Stanford, OpenVaccine,\nand Eterna for hosting the COVID-19 mRNA Vaccine competition and Deng Lab (Univ of Georgia)\nfor collaborating in this competition. We would like to thank CHAMPS (Chemistry and Mathematics\nin Phase Space) for hosting the J-coupling quantum chemistry competition and Corey Levinson\n(Eligo Energy, LLC) for collaborating in this competition. We would like to thank Zongyi Li (Caltech)\nfor sharing some early dev code in the updated PyTorch fft interface and the comments on the\nviscosity of the Burgers’ equation. We would like to thank Ziteng Pang (Univ of Michigan) and\nTianyang Lin (Fudan Univ) to update us with various references on Transformers. We would like to\nthank Joel Schlosser (Facebook) to incorporate our change to the PyTorch transformer module to\nsimplify our testing pipeline. We would be grateful to the PyTorch community for selﬂessly code\nsharing, including Phil Wang(lucidrains@github) and Harvard NLP group [52]. We would like to\nthank the chebfun [28] for integrating powerful tools into a simple interface to solve PDEs. We\nwould like to thank Dr. Yannic Kilcher (ykilcher@twitter) and Dr. Hung-yi Lee (National Taiwan\nUniv) for frequently covering the newest research on Transformers in video formats. We would also\nlike to thank the Python community [87, 68] for sharing and developing the tools that enabled this\nwork, including PyTorch [69], NumPy [39], SciPy [89], Plotly [45] Seaborn [93], Matplotlib [43],\nand the Python team for Visual Studio Code. We would like to thank draw.io [46] for providing\nan easy and powerful interface for producing vector format diagrams. For details please refer to the\ndocuments of every function that is not built from the ground up in our open-source software library.9\n4https://github.com/lyc102/ifem\n5Transformer: A Dissection from an Amateur Applied Mathematician\n6https://www.kaggle.com/cpmpml/graph-transfomer\n7https://www.kaggle.com/mrkmakr/covid-ae-pretrain-gnn-attn-cnn\n8https://www.kaggle.com/c/stanford-covid-vaccine/discussion/183518\n9https://github.com/scaomath/galerkin-transformer\n11\nReferences\n[1] Jonas Adler and Ozan Öktem. Solving ill-posed inverse problems using iterative deep neural networks.\nInverse Problems, 33(12):124007, 2017.\n[2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language\nmodeling with deeper self-attention. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 33\n(01):3159–3166, Jul. 2019.\n[3] Giovanni Alessandrini. An identiﬁcation problem for an elliptic equation in two variables. Annali di\nmatematica pura ed applicata, 145(1):265–295, 1986.\n[4] Ferran Alet, Adarsh Keshav Jeewajee, Maria Bauza Villalonga, Alberto Rodriguez, Tomas Lozano-Perez,\nand Leslie Kaelbling. Graph element networks: adaptive, structured computation and memory. In\nInternational Conference on Machine Learning, pages 212–222. PMLR, 2019.\n[5] Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li,\nBurigede Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differential equations.\nIn ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations, 2020.\n[6] Daniel Arndt, Wolfgang Bangerth, Bruno Blais, Thomas C. Clevenger, Marc Fehling, Alexander V .\nGrayver, Timo Heister, Luca Heltai, Martin Kronbichler, Matthias Maier, Peter Munch, Jean-Paul Pelteret,\nReza Rastak, Ignacio Thomas, Bruno Turcksin, Zhuoran Wang, and David Wells. The deal.II library,\nversion 9.2. Journal of Numerical Mathematics, 28(3):131–146, 2020.\n[7] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning\nto align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, 2015.\n[8] Jean-Paul Berrut and Manfred R Trummer. Equivalence of Nyström’s method and Fourier methods for\nthe numerical solution of Fredholm integral equations. Mathematics of computation, 48(178):617–623,\n1987.\n[9] Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Prediction of\naerodynamic ﬂow ﬁelds using convolutional neural networks. Computational Mechanics, 64(2):525–545,\n2019.\n[10] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction\nand neural networks for parametric PDEs. arXiv preprint arXiv:2005.03180, 2020.\n[11] Guy Blanc and Steffen Rendle. Adaptive sampled softmax with kernel based sampling. In International\nConference on Machine Learning, pages 590–599. PMLR, 2018.\n[12] Ronald Newbold Bracewell and Ronald N Bracewell. The Fourier transform and its applications, volume\n31999. McGraw-Hill New York, 1986.\n[13] Susanne C Brenner and Ridgway Scott. The mathematical theory of ﬁnite element methods, volume 15.\nSpringer, 2008.\n[14] Richard P Brent. Multiple-precision zero-ﬁnding methods and the complexity of elementary function\nevaluation. In Analytic computational complexity, pages 151–176. Elsevier, 1976.\n[15] Shuhao Cao, Long Chen, and Ruchi Guo. A virtual ﬁnite element method for two dimensional Maxwell\ninterface problems with a background unﬁtted mesh. Mathematical Models and Methods in Applied\nSciences, to appear, 2021.\n[16] Tony F Chan and Xue-Cheng Tai. Identiﬁcation of discontinuous coefﬁcients in elliptic problems using\ntotal variation regularization. SIAM Journal on Scientiﬁc Computing, 25(3):881–904, 2003.\n[17] Long Chen. iFEM: an integrated ﬁnite element methods package in MATLAB. Technical report, 2008.\n[18] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with\narbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural\nNetworks, 6(4):911–917, 1995.\n[19] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin\nBelanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with Performers. In International\nConference on Learning Representations (ICLR), 2021.\n12\n[20] Philippe G Ciarlet. The ﬁnite element method for elliptic problems. SIAM, 2002.\n[21] Philippe G Ciarlet. Linear and nonlinear functional analysis with applications, volume 130. SIAM, 2013.\n[22] Richard Courant, Kurt Friedrichs, and Hans Lewy. On the partial difference equations of mathematical\nphysics. IBM journal of Research and Development, 11(2):215–234, 1967.\n[23] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks improve\nsystematic generalization of transformers. In Proc. Conf. on Empirical Methods in Natural Language\nProcessing (EMNLP), Punta Cana, Dominican Republic, November 2021.\n[24] Niccolò Dal Santo, Simone Deparis, and Luca Pegolotti. Data driven approximation of parametrized pdes\nby reduced basis and neural networks. Journal of Computational Physics, 416:109550, 2020.\n[25] Alexandre de Brébisson and Pascal Vincent. A cheap linear attention mechanism with fast lookups and\nﬁxed-size representations. arXiv preprint arXiv:1609.05866, 2016.\n[26] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. In International Conference on Learning Representations, 2019.\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1, pages 4171–4186. Association for Computational Linguistics, 2019.\n[28] Tobin A Driscoll, Nicholas Hale, and Lloyd N Trefethen. Chebfun guide, 2014.\n[29] Alexandre Ern and Jean-Luc Guermond. Theory and Practice of Finite Elements. Springer, 2004.\n[30] Charles Fox. The G and H functions as symmetrical Fourier kernels. Transactions of the American\nMathematical Society, 98(3):395–429, 1961.\n[31] Fabian Fuchs, Daniel Worrall, V olker Fischer, and Max Welling. SE(3)-Transformers: 3D Roto-\nTranslation Equivariant Attention Networks. In Advances in Neural Information Processing Systems,\nvolume 33, pages 1970–1981, 2020.\n[32] D. Gilbarg and N.S. Trudinger. Elliptic Partial Differential Equations of Second Order . Classics in\nMathematics. Springer Berlin Heidelberg, 2001. ISBN 9783540411604.\n[33] Vivette Girault and P-A Raviart. Finite element approximation of the Navier-Stokes equations. Lecture\nNotes in Mathematics, Berlin Springer Verlag, 749, 1979.\n[34] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and\nStatistics, volume 9 of Proceedings of Machine Learning Research, pages 249–256, Chia Laguna Resort,\nSardinia, Italy, 13–15 May 2010. PMLR.\n[35] Ruchi Guo and Jiahua Jiang. Construct deep neural networks based on direct sampling methods for\nsolving electrical impedance tomography. SIAM Journal on Scientiﬁc Computing, 43(3):B678–B711,\n2021.\n[36] Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady ﬂow approximation.\nIn Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data\nmining, pages 481–490, 2016.\n[37] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential\nequations. arXiv preprint arXiv:2109.13459, 2021.\n[38] W. Hackbusch. Multi-Grid Methods and Applications. Springer Series in Computational Mathematics.\nSpringer Berlin Heidelberg, 2013. ISBN 9783662024270.\n[39] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,\nStephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río,\nMark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser,\nHameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature,\n585(7825):357–362, September 2020.\n[40] Juncai He and Jinchao Xu. Mgnet: A uniﬁed framework of multigrid and convolutional neural network.\nScience China Mathematics, 62(7):1331–1354, 2019.\n13\n[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.\nIn European conference on computer vision, pages 630–645. Springer, 2016.\n[42] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal\napproximators. Neural networks, 2(5):359–366, 1989.\n[43] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):90–95,\n2007.\n[44] Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and Hyunjik\nKim. LieTransformer: equivariant self-attention for Lie groups. pages 4533–4543, 2021.\n[45] Plotly Technologies Inc. plotly, 2015.\n[46] JGraph. draw.io, 2021.\n[47] Jiahua Jiang, Yi Li, and Ruchi Guo. Learn an index operator by cnn for solving diffusive optical\ntomography: a deep direct sampling method. arXiv preprint arXiv:2104.07703, 2021.\n[48] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure\nprediction with AlphaFold. Nature, 596(7873):583–589, 2021.\n[49] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.\nPhysics-informed machine learning. Nature Reviews Physics, 2021.\n[50] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:\nFast autoregressive transformers with linear attention. In International Conference on Machine Learning,\npages 5156–5165. PMLR, 2020.\n[51] A. Kirsch. An Introduction to the Mathematical Theory of Inverse Problems . Applied Mathematical\nSciences. Springer New York, 2011. ISBN 9781441984746.\n[52] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. OpenNMT: Open-\nSource Toolkit for Neural Machine Translation. In Proc. ACL, 2017.\n[53] Peter D Lax and Robert D Richtmyer. Survey of the stability of linear ﬁnite difference equations.\nCommunications on pure and applied mathematics, 9(2):267–293, 1956.\n[54] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with\nFourier transforms. arXiv preprint arXiv:2105.03824, 2021.\n[55] Sijing Li, Zhiwen Zhang, and Hongkai Zhao. A data-driven approach for multiscale elliptic PDEs with\nrandom coefﬁcients based on intrinsic dimension reduction. Multiscale Modeling & Simulation, 18(3):\n1242–1271, 2020.\n[56] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhat-\ntacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential\nequations. In Advances in Neural Information Processing Systems, volume 33, pages 6755–6766, 2020.\n[57] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya,\nAndrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential\nequations. In International Conference on Learning Representations, 2021.\n[58] Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approximator.\nAdvances in Neural Information Processing Systems (NIPS 2018), 31:6169–6178, 2018.\n[59] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 2021.\n[60] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identi-\nfying differential equations based on the universal approximation theorem of operators. arXiv preprint\narXiv:1910.03193, 2019.\n[61] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear\noperators via deeponet based on the universal approximation theorem of operators. Nature Machine\nIntelligence, 3(3):218–229, 2021.\n[62] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for\nsolving differential equations. SIAM Review, 63(1):208–228, 2021.\n14\n[63] Peter Monk et al. Finite element methods for Maxwell’s equations. Oxford University Press, 2003.\n[64] Nicholas H Nelsen and Andrew M Stuart. The random feature model for input-output maps between\nbanach spaces. SIAM Journal on Scientiﬁc Computing, 43(5):A3212–A3243, 2021.\n[65] Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, and Bao Wang. FMMformer: Efﬁcient\nand Flexible Transformer via Decomposed Near-ﬁeld and Far-ﬁeld Attention. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2021.\n[66] Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-\nattention. arXiv preprint arXiv:1910.05895, 2019.\n[67] Akifumi Okuno, Tetsuya Hada, and Hidetoshi Shimodaira. A probabilistic framework for multi-view\nfeature learning with many-to-many associations via neural networks. In International Conference on\nMachine Learning, pages 3888–3897. PMLR, 2018.\n[68] Travis E. Oliphant. Python for scientiﬁc computing. Computing in Science Engineering, 9(3):10–20,\n2007.\n[69] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In\nAdvances in Neural Information Processing Systems 32 (NeurIPS 2019), pages 8024–8035, 2019.\n[70] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random\nfeature attention. In International Conference on Learning Representations, 2021.\n[71] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning\nframework for solving forward and inverse problems involving nonlinear partial differential equations.\nJournal of Computational Physics, 378:686–707, 2019.\n[72] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint\narXiv:1710.05941, 2017.\n[73] Ankit Singh Rawat, Jiecao Chen, Felix Xinnan X Yu, Ananda Theertha Suresh, and Sanjiv Kumar.\nSampled softmax with random fourier features. In Advances in Neural Information Processing Systems,\nvolume 32, 2019.\n[74] Nicholas Roberts, Mikhail Khodak, Tri Dao, Liam Li, Christopher Ré, and Ameet Talwalkar. Rethinking\nneural operations for diverse tasks. arXiv preprint arXiv:2103.15798, 2021.\n[75] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight\nprogrammers, 2021.\n[76] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efﬁcient attention: Attention\nwith linear complexities. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, pages 3531–3539, 2021.\n[77] Jonathan W Siegel and Jinchao Xu. Approximation rates for neural networks with general activation\nfunctions. Neural Networks, 128:313–321, 2020.\n[78] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large\nlearning rates. In Artiﬁcial Intelligence and Machine Learning for Multi-Domain Operations Applications,\nvolume 11006, page 1100612. International Society for Optics and Photonics, 2019.\n[79] Kyungwoo Song, Yohan Jung, Dongjun Kim, and Il-Chul Moon. Implicit kernel attention. Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, 35(11):9713–9721, May 2021.\n[80] Ari Stern. Banach space projections and Petrov–Galerkin estimates. Numerische Mathematik, 130(1):\n125–133, 2015.\n[81] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural networks. In\nProceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2,\nNIPS’14, page 3104–3112, Cambridge, MA, USA, 2014. MIT Press.\n[82] Kai Sheng Tai, Peter Bailis, and Gregory Valiant. Equivariant transformer networks. In International\nConference on Machine Learning, pages 6086–6095. PMLR, 2019.\n15\n[83] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\nJessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. MLP-mixer: An all-MLP\narchitecture for vision. arXiv preprint arXiv:2105.01601, 2021.\n[84] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.\nTransformer dissection: An uniﬁed understanding for transformer’s attention via the lens of kernel. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4344–4353,\nHong Kong, China, November 2019.\n[85] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient\nfor fast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[86] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian ﬂuid simulation\nwith continuous convolutions. In International Conference on Learning Representations, 2020.\n[87] Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA,\n2009. ISBN 1441412697.\n[88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing\nSystems (NIPS 2017), volume 30, 2017.\n[89] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,\nEvgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew\nBrett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert\nKern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald,\nAntônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:\nFundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020.\n[90] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient ﬂow pathologies in\nphysics-informed neural networks. SIAM Journal on Scientiﬁc Computing, 43(5):A3055–A3081, 2021.\n[91] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial\ndifferential equations with physics-informed DeepOnets. arXiv preprint arXiv:2103.10974, 2021.\n[92] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[93] Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60):\n3021, 2021.\n[94] H. Whitney. Geometric Integration Theory. Princeton Legacy Library. Princeton University Press, 2015.\nISBN 9781400877577.\n[95] Matthew A Wright and Joseph E Gonzalez. Transformers are deep inﬁnite-dimensional non-Mercer\nbinary kernel machines. arXiv preprint arXiv:2106.01506, 2021.\n[96] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. InInternational\nConference on Machine Learning, pages 10524–10533. PMLR, 2020.\n[97] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nyströmformer: A Nyström-based algorithm for approximating self-attention. Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, 35(16):14138–14148, May 2021.\n[98] Jinchao Xu and Ludmil Zikatanov. Algebraic multigrid methods. Acta Numerica, 26:591–721, 2017.\n[99] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are\ntransformers universal approximators of sequence-to-sequence functions? In International Conference on\nLearning Representations, 2020.\n[100] Yinhao Zhu and Nicholas Zabaras. Bayesian deep convolutional encoder–decoder networks for surrogate\nmodeling and uncertainty quantiﬁcation. Journal of Computational Physics, 366:415–447, 2018.\n16\nAppendices of Choose a Transformer: Fourier or Galerkin\nA Table of notations\nTable 4: Notations used in an approximate chronological order and their meaning in this work.\nNotation Meaning\nH,X,Y Hilbert spaces deﬁned on a domainΩ, f∈H: Ω→R\nQ,K,V Latent representation Hilbert spaces, e.g.,v∈V: Ω∗→R\n(H,⟨·,·⟩) Hand its inner-product structure,⟨u,v⟩:= ∫\nΩ u(x)v(x)dxfor simplicity\nH′ the space of the bounded linear functionals deﬁned on a Hilbert space\n∥v∥H The norm deﬁned by the inner product∥v∥H:= ⟨v,v⟩1/2\n∥fu∥H′ The natural induced norm offu(·) :=⟨u,·⟩, ∥fu∥H′:= supv∈H|fu(v)|/∥v∥H\n∥v∥ Theℓ2-norm deﬁned by the inner product∥v∥:= (v·v)1/2 forv∈Rd\nb(·,·) A bilinear form, having two inputs from potentially different subspaces\nLp(Ω) The space of functions with integrablep-th moments inΩ\nL∞(Ω) The space of functions with a bounded essential supremum inΩ\nH1(Ω) Sobolev spaceW1,2(Ω) :={φ∈L2(Ω) :Dφ∈L2(Ω)}\nH10(Ω) {v∈H1(Ω) : Υ(u) = 0on ∂Ω}, whereΥ(·) is the trace operator\nH(div; Ω) Hilbert space with a graph norm{φ∈L2(Ω) : divφ∈L2(Ω)}\nC0p(Ω)≃C0(S1) The space of continuous functions with a periodic boundary condition\n∥u∥L2(Ω) TheL2-norm ofu, ∥u∥2\nL2(Ω) := ∫\nΩ |u|2 dx\n|u|H1(Ω) TheH1-seminorm ofu, |u|2\nH1(Ω) := ∫\nΩ |Du|2 dx\nx∈Ω ⊂Rm A point in the spacial domainΩ of interest\nm The dimension of the underlying spacial domain inRm\nd The dimension of the latent representation approximation subspace\nLa(u) =f The operator form (strong form) of a PDE with coefﬁcienta\nu(·) The solution to the weak form⟨Lu,v⟩= ⟨f,v⟩for any test functionv∈H\na(·) The coefﬁcients in a PDE operator\n∂tu+N(u) = 0 A time-dependent stiff PDE, whereN(·) is nonlinear differential operator\nh The mesh size of a uniform grid\nn≈1/hm The discretization size (sequence length) of data,O(nm) for anRmproblem\nnf,nc The ﬁne grid size, the coarse grid size\nXh,Yh,Ah The discrete function space with degrees of freedom on grid points of mesh sizeh\nQh,Vh Certain subspaces spanned by functions inXh,Yh\nuh,ah The approximation tou,awhose degrees of freedom deﬁned at the grid points\nT The operator to be learned related to a partial differential equation\nTh The approximation toTapplied on functions on a discrete grid with mesh sizeh\ny,z the input of and the output from the attention operator, inRn×d\nqi,ki,vi thei-th row of, or thei-th position’s feature vector in a latent representation\nzi,qi,ki,vi thei-th column of, or thei-th basis’s discrete DoFs in a latent representation\nAi•/A•j thei-th row/j-th column of a matrixA\n{yj(·)}d\nj=1 A set of latent basis whose DoFs form the column space ofY ∈Rn×d\n{χyj(·)}d\nj=1 The set of degrees of freedom associated with the set of bases{yj(·)}d\nj=1\n(v)i thei-th entry/row of a vectorv\nIh The nodal interpolation operator such that(Ihv)(xi) =v(xi)\nΠh The interpolation or projection operator that maps function to a grid with mesh sizeh\nH↪→C0(Ω) His continuously embedded in the space of continuous functions\nB Network structures\nThe network in Figure 4 is used in Example 5.1. The model used in the forward Darcy problem 5.2 is\nin Figure 5. A detailed comparison can be found in Table 5.\n17\nTable 5: The detailed comparison of networks; SC: spectral convolution layer; atorch.cfloat type\nparameter entry counts as two parameters.\nEncoder Decoder\n# paramslayers dmodel nhead# SC dmodel modes activation\nFNO 1D 0 N/A N/A 4 64 16 ReLU 550k\nFT/GT in 5.1 4 96 1 2 48 16 SiLU 523k–530k\nFNO 2D 0 N/A N/A 4 32×32 12 ReLU 2.37m\nFT/GT in 5.2 6 128 4 2 32×32 12 SiLU 2.22m\nFT/GT in 5.3 6 192 4 0 N/A N/A SiLU 2.38m\nWhen the target is smooth, the spectral convolution layer from [ 57] is used in our network as a\nsmoother (decoder), and the original ReLU activation is replaced by the Sigmoid Linear Unit (SiLU)\n[72]. We have removed the batch normalization (BN) from the original spectral convolution layers as\nwell. Whenever the input or the output of certain layers of the network is approximating a non-smooth\nfunction a priori, the activation are changed from SiLU to ReLU.\nInputs \n \n Single head \nSelf-Attention \nwithout Softmax \nFeature extractor \n(Pointwise FFN)\nDecoding\nregressor \n(Spectral\nConv)\nApprox. \nApprox.  \nspacial coordinates \nOutputs \n   \nFigure 4: A simple attention-based operator learner in Example 5.1.\nInputs (variable size) \ninterpolated \nfeature \nMultihead Simple \nSelf-Attention \nwithout Softmax \nConcatenate attn output\ninterpolated \nfeature 2\ninterpolated \nfeature 1 \nConcatenate CiNN output\nDownsample \nCiNN \nFeature\nExtractor\n \n \n \n \nUpsample \nCiNN \n+ \nDecoding\nRegressor\npositional encoding \nOutputs \nFigure 5: An attention-based operator learner on Ω ⊂R2.\nDownsampling CNN. The downsampling interpolation-based CNN (CiNN) is to reduce the size\nof the input to a computationally feasible extent for attention-based encoder layers, in addition to\na channel expansion to match the hidden feature dimension (channels). The full resolution input\nfunction sampled at a ﬁne grid of size nf ×nf is downsampled by CiNN to a collection of latent\nfunctions on an nc ×nc coarse grid. Then, the coarse grid representations are concatenated with\nthe positional encoding (Euclidean coordinates on the coarse grid) to be sent to the attention-based\nencoder layers.\nThe structures of the downsampling CiNN can be found in Figure 6. In CiNN, instead of pooling, the\ndownsampling are performed through a bilinear interpolation with nonmatching coarse/ﬁne grids.\nThe convolution block adopts a simpliﬁed variant from the basic block in [41]. The convolution layer\nis applied only once before the skip-connection, and the batch normalization is removed from the\nblock.\nIn the downsampling CiNN, the ﬁrst convolution layer maps the input data to a tensor of which the\nnumber of channels matches the number of hidden dimension of the attention layers. Then, the full\nresolution representations in all channels are interpolated from the nf ×nf grid to an nm ×nm grid\nof an intermediate size between nf and nc, and nm ≈√nfnc. Next, another three convolution layers\nare applied consecutively together with their outputs stacked in the channel dimension. Finally, this\n18\nstacked tensor is downsampled again by another bilinear interpolation as the output the downsampling\nCiNN.\nThe coarse grid positional encoding of size nc ×nc ×2 is concatenated to the output latent represen-\ntations before ﬂattening and the scaled dot-product attention. As a result, the input/output dimensions\nof an attention-based encoder layer are both n2\nc ×d. Nevertheless, inside an encoder layer, the\npropagation runs for a tensor of size n2\nc ×(d+ m·(nhead)).\nUpsampling CNN. The output from the attention-based encoder layers is ﬁrst reshaped from an\nn2\nc ×dmatrix to an nc ×nc ×dtensor, then upsampled by another CiNN to the full resolution. The\nupsampling CiNN has a simpler structure (Figure 7) than the downsampling CiNN. We have two\ninterpolations that map tensors of nc ×nc ×dto nm ×nm ×d, and nm ×nm ×dto nf ×nf ×d,\nrespectively. A simple convolution layer with matching number of channels is between them. The\npositional encoding of the ﬁne grid is then concatenated with the output from the upsample layer, and\nsent to the decoder layers.\nFlatten output\nInterpolation\n \nConv2d feature  \n \n \nConv2d feature 1\nResBlock \nInterpolated\nfeature \nInterpolated\nfeature 1\nResBlock \nConv2d\nfeature \nConv2d\nfeature 1\nResBlock \nConv2d\nfeature \nConv2d\nfeature 1\nStack output\nInterpolation\nFigure 6: A 2D bilinear interpolation-based CNN (CiNN) for downsampling.\n \n \n \nStacked output\nInterpolation \nInterpolated\nfeature  \n \n \nInterpolated\nfeature 1\nConv2d \nConv2d \nfeature  \n \n \nConv2d \nfeature 1\nInterpolation\nFigure 7: A 2D bilinear interpolation-based CNN (CiNN) for upsampling.\nC Supplemental details of the experiments\nC.1 Training and evaluation setup\nIn training our models, we opt for a standard 1cycle [78] learning rate strategy with a warm-up\nphase for an environmental responsible and a seed-invariant training. We run a mini-batch ADAM\niterations for a total number of 100 epochs (12800 iterations with batch size 8). The learning rate\nstarts and ends with 10−4 ·lrmax, and reaches the maximum of lrmax at the end of the 30-th epoch.\nThe lrmax = 10−3 for all models except being 5 ×10−4 for ST and FT in 2D problems.\nThe batch size is set to 8 in 1D in n= 512,2048, and 4 in n= 8192 as well as in 2D examples. The\ntraining cost of our models are reported in Table 6. It is not surprising that attention-based operator\nlearners can be more efﬁciently trained than traditional MLP-based operator learners. Our model can\nbe trained using merely a fraction of time versus MLP-based operator learners (cf. Burgers’ equation\ntraining time in [91, Appendix C]), thus proven to be a more environment-friendly data-driven model.\nFor the three examples prepared, there are 1024 samples in the training set, and 100 in the testing\nset. Even though the initial conditions or the coefﬁcients for different samples, in Example 5.1\nand Example 5.2 respectively, follow the same distribution constructed based on GRF, there are no\nrepetitions between the functions in the training set and those in the testing set.\nDuring training, there is no regularization applied for the weights of the models. A simple gradient\nclip of 1 is applied. When the target function is known a priori being smooth with anH1+α regularity\n(α> 0), we employ anH1-seminorm regularization between the 2nd order approximation (the central\n19\nTable 6: Environmental impact measured in computational cost of training (in hours).\nExample 1 Example 2 Example 3\nn= 512 n= 2048 n= 8192 nf= 141 nf= 211 nf= 141 nf= 211\nFT 0.063 0 .138 1 .217 0 .615 1 .553 0 .452 2 .638\nGT 0.064 0 .079 0 .245 0 .367 0 .610 0 .248 0 .857\nTable 7: The dropout comparison during training.\nattention FFN downsample upsample decoder\nFT both Lns in 5.1 0.0 0 .05 N/A N/A 0.0\nGT new Ln in 5.1 0.0 0 .0 N/A N/A 0.0\nGT reg Ln in 5.1 0.1 0 .1 N/A N/A 0.0\nFT in 5.2 0.1 0 .1 0 .05 0 .0 0 .0\nGT in 5.2 0.1 0 .05 0 .05 0 .0 0 .0\nFT/GT in 5.3 0.05 0 .05 0 .05 N/A 0.05\ndifference in 1D, and the 5-point stencil in 2D) to derivatives of the targets and those of the outputs\nfrom the model (see Section 3). We choose γ = 0.1hin Example 5.1 and γ = 0.5hin Example 5.2.\nThe dropouts during training are obtained through a simple grid search in {0.0,0.05,0.1}and can be\nfound in Table 7.\nThroughout all the numerical experiments, the result is obtained from setting 1127802 as the random\nnumber generator seed, and the PyTorch cuDNN backend to deterministic. Error bands are reported\nusing 10 different seeds (see Figure 8). All benchmarks are run on a single RTX 3090. Due to the\nnature of our problem and the data pairs, there is no randomness between the input and the output for\na single instance in Example 5.1 and Example 5.2, the only stochastic components are the sampling\nfor optimizations and the initializations of the model, and the impact due to the choice of seeds for\nour models is empirically minimal.\n(a)\n (b)\nFigure 8: Typical training and evaluation convergences of the Galerkin Transformer. (a) example\n5.2: 10 different seeds for the model initialization; (b) example 5.1: 10 different seeds in both\nmodel initializations and the train loader; the discrete norm of a nonlinear operator ∥N∥V′\nh\n:=\nsupv∈Vh ∥N(v)∥H/∥v∥His deﬁned similarly to that of a linear operator. The H1-seminorm part in\nthe H1-norm shown in ﬁgures is weighted by γfrom Section 3.\nC.2 Experimental details for Example 5.1\nData preparation. The following problem is considered in Example 5.1:\n{\n∂tu+ u∂xu= ν∂xxu for (x,t) ∈(0,1) ×(0,1],\nu(x,0) = u0(x) for x∈(0,1),\n(18)\n20\nand it is assumed that u0 ∈C0\np(Ω) ∩L2(Ω). The operator to be learned is:\nT : C0\np(Ω) ∩L2(Ω) →C0\np(Ω) ∩H1(Ω), u 0(·) ↦→u(·,1).\nFollowing [57], the initial data are prepared using a Gaussian Random Field (GRF) simulation\n∼ N(0,252(−∆ + 25I)−2), and the system is solved using the chebfun package [28] with a\nspectral method using a very ﬁne time step δt ≪1 for the viscosity ν = (2 π)−10.1 on a grid\nwith n = 8192 . Solutions and initial conditions in other two resolutions ( n = 512 , 2048) are\ndownsampled from this ﬁnest grid. Therefore, the discrete approximation the model learns is: for\nh∈{2−9,2−11,2−13}\nTh : Xh →Xh, I hu0(·) ↦→Πhuh∗(·,1),\nwhere Xh ≃Rn denotes a function space of which the degrees of freedom are deﬁned on the grid\nwith mesh size h, such as the space of piecewise linear functions. Ih(·) denotes the nodal value\ninterpolation, and Πh(·) denotes the downsampling operator from the space on the ﬁnest grid with\nmesh size h∗= 2−13 to Xh.\nFigure 9: Evaluation results for 4 randomly chosen samples in the test set; the average relative error\n= 1.079 ×10−3.\nEffect of the diagonal initialization. When using the regular layer normalization rule (7) to train\nthe GT models (which is equivalent to the efﬁcient attention proposed in [76]), it fails to converge\nunder the 1cycle learning rate scheduling with the 0.05 dropout in FFN and 0.0 dropout for the\nattention weights. We observe that the evaluation metric peaks after the warmup phase (≈2 ×10−2\naround epoch 30). The GT using the new rule(6) is convergent almost unconditionally under the same\ninitialization, which reaches the common ≈1 ×10−3 range if the diagonal initialization is employed\n(e.g., see Figure 8b). The result reported in Table 2a for GT with the regular layer normalization is\nobtained through imposing a 0.1 dropout for the attention weights. A more detailed comparison can\nbe found in Table 8. The training becomes divergent for a certain model if the best epoch shown in the\ntable is not around 100. We conjecture that the success of the diagonal initialization is due to the input\n(initial conditions, blue curves in Figure 9) being highly correlated spacially with the target (solutions\nat t= 1, green curves in Figure 9), despite the highly nonlinear mapping. Thus, in the encoder layers,\nwhat the attention operator learned is likely to be a perturbation of the identity operator in the latent\nHilbert space, if a suitable basis can be found for Vh (or Qh in the linear variant).\nEffect of the Galerkin projection-type layer normalization scheme. Multiplying uon both sides\nof (18), the energy law of the Burgers’ equation can be obtained through an integration by parts on\nΩ := (0,1) with the periodic boundary condition:\n⟨∂tu,u⟩+ ⟨∂x(u2),u⟩/2 = ν⟨∂xxu,u⟩ =⇒ d\n(\n∥u∥2\nL2(Ω)\n)\n/dt= −ν∥∂xu∥2\nL2(Ω). (19)\n21\nTable 8: Ablation study: evaluation relative error ( ×10−3) of the Galerkin Transformer with the\nregular layer normalization (7) and the new one (6), using various types of initialization; the baseline\nmodel is the GT with the default Xavier uniform initialization. GTR: layer normalization (7);\nGTA: layer normalization (6); δ: the weight added to the diagonal; η: the gain of Xavier uniform\ninitialization; ζ: dropout for the attention weights.\nn= 512 n= 2048 n= 8192(b= 4)\nRel. err Best ep. Rel. err Best ep. Rel. err Best ep.\nGTR(ET in [76]) 200.4 86 208.4 39 217.5 28\nGTR, ζ= 0.1 206 .4 46 205.9 59 207.0 75\nGTR, η= 10−2 1.406 99 21.38 14 17.75 16\nGTR, η= 10−2, ζ= 0.1 16 .85 20 11.19 21 2.571 98\nGTR, η=δ= 10−2 15.14 35 1.512 97 15.98 34\nGTR, η=δ= 10−2, ζ= 0.1 2 .181 100 13.96 19 2.331 92\nGTA 10.06 96 10.12 99 9.129 100\nGTA, η= 10−2 1.927 95 2.453 100 1.689 99\nGTA, η=δ= 10−2 1.203 99 1.150 100 1.025 100\nConsequently, once the initial condition is given, integrating(19) from t= 0 to a ﬁxed future time\nyields how much the energy of a single instance of uhas decayed, and this is a deterministic quantity.\nThis indicates that the scale-preserving property of the Galerkin projection-type layer normalization\nwould potentially learn this decaying property resulted by the operator, thus outperforms the regular\nlayer normalization scheme that normalizes each position’s feature vector. We also note that using an\ninstance normalization [85] may appear to be more sensible as it normalizes ∥vj∥to 1 (1 ≤j ≤d)\nafter the 1/nweight, however, we ﬁnd that opting for the instance normalization deteriorates the\ntraining’s stability and the dropout needs to be further dialed up. For more heuristics of the Galerkin-\ntype layer normalization scheme please refer to Section D.4.\nC.3 Experimental details for Example 5.2\nData preparation. Example 5.2 considers another well-known benchmark problem used in [57, 64,\n56]. The Darcy ﬂow in porous media Ω := (0,1)2, in which the diffusion coefﬁcient a∈L∞(Ω) :\nx↦→R+ represents the permeability of the media and has a sharp contrast within the domain.\n{−∇·(a∇u) = f in Ω,\nu= 0 on ∂Ω. (20)\nFor each sample in the training and validation data, a(x) is generated according to a ∼ν :=\nψ♯N(0,(−∆ + 9I)−2), where within the covariance −∆ is deﬁned on (0,1)2 and has homogeneous\nNeumann boundary conditions. The mapping ψ: R →R is constructed by\nψ(ρ) = 121(0,∞)(ρ) + 31(−∞,0)(ρ).\nThus, the resulting coefﬁcient afollows a pushforward probability measure ν, and takes values of\n12 and 3 almost surely within Ω. The geometry of the interface exhibits a random pattern in Ω (see\nFigure 10c). The forcing f is ﬁxed as f ≡1.\nThe operator to be learned is between the diffusion coefﬁcient and the unique weak solution:\nT : L∞(Ω) →H1\n0 (Ω), a ↦→u.\nThe ﬁnite dimensional approximation uh’s are obtained using a 5-point stencil second-order ﬁnite\ndifference scheme on a 421 ×421 grid. Therefore, the discrete operator Th to be learned is:\nTh : Ah ↦→Vh, a h ↦→Πhuh∗, (21)\nwhere Ah and Vh are function spaces of which the degrees of freedom are deﬁned on the ﬁne grid\npoints with mesh size h, h∗= 1/421 is the ﬁnest grid size, and ah := Ihaand Πh(·) are deﬁned\naccordingly in a similar fashion with Example 5.1 explained in Appendix C.2. Following the practice\nof [57], a non-trainable Gaussian normalizer is built in the network to transform the input and the\ntarget to be ∼N(0,1) pointwisely on each grid point.\n22\n(a)\n (b)\n(c)\n (d)\nFigure 10: Interface Darcy ﬂow in example 5.2: a randomly chosen sample from the test dataset.\n(a) the target being the ﬁnite difference approximation to the solution on a very ﬁne grid; (b) the\ninference approximation by model evaluation (relative L2-error 6.454 ×10−3); (c) the input a(x);\n(d) the L∞-error distribution for the inference solution.\nBy choosing Vh as the standard bilinear Lagrange ﬁnite element on a uniform Cartesian grid on Ω, a\nstandard summation by parts argument for −∆h and the discrete Poincaré inequality guarantees the\nwell-posedness of problem using the Lax-Milgram lemma, i.e., given ah ∈Ah ≃Rnf ×nf , the linear\nsystem of the −ah∆h(·) discretization has a unique solution uh. Even though the inversion of the\nstiffness matrix in resulting linear system is a linear problem, the mapping in (21) is highly nonlinear\nbetween two spaces isomorphic to Rnf ×nf .\nLimitations. We acknowledge that our method, despite surpassing the current best operator\nlearner’s benchmark evaluation accuracy, still does reach the accuracy of traditional discretization-\nbased methods that aim to best the approximation for a single instance. For example, in Figure\n10d, the error is more prominent at the location where the coefﬁcient has sharp contrast. How to\nincorporate the adaptive methods (allocating more degrees of freedom based on the a posteriori local\nerror) to data-driven operator learners will be a future study.\nC.4 Experimental details for Example 5.3\nInverse problems. Playing a central role in many practical applications, the inverse problems are a\nclass of important tasks in many scientiﬁc disciplines [51]. The problem summarizes to using the\nmeasurements to infer the material/physical coefﬁcients. In almost all cases, the inverse problem\nis much harder than solving the forward problem (e.g., solving for uin problem La(u) = f), as\nthe mapping from the solution (measurements) back to the coefﬁcient is much less stable than\nthe forward operator due to a much bigger Lipschitz constant. As a result, the inverse operator\nampliﬁes noises in measurements by a signiﬁcant amount. For example, by [ 3, Theorem 5.1], the\nerror estimate of coefﬁcient reconstruction indicates that in order that coefﬁcient can be recovered,\nthe measurements have to reach an accuracy with an error margin under O(h) where hdenotes the\nmesh size. Meanwhile, standard iterative techniques that construct aϵ to approximate arelies on the\nregularity of the coefﬁcient aitself [3, Section 3]. This regularity assumption is largely violated in\nour problem setting, as the ahas sharp material interfaces (see Figure 10c), has no extra regularity,\nand is only in L∞.\nHaving the measurements on the discrete grid, the ideal goal is to learn the following inverse map Th,\nTh : Xh ↦→Ah, Πhuh∗ ↦→ah, (22)\n23\nHowever, in practice the measurements (solution) could have noise. Therefore, we aim to learn the\nfollowing discrete operator in this example, i.e., to reconstruct the coefﬁcient on the coarse grid based\non a noisy measurement on the ﬁne grid. We note that this operator is not well-posed anymore due to\nnoise.\nTh : Xhf ↦→Ahc , u hf + ϵνhf ↦→Πhc ah, (23)\nwhere hc,hf denotes the mesh size of the coarse grid nc×nc, and the ﬁne grid nf ×nf, respectively.\nΠhc denotes a map that restricts a function Xh deﬁned on nf ×nf to nc ×nc. ϵis the strength of\nthe noise, and νhf (xi) ∼N(0,ci) where ci is the variance of all training samples’ value at xi. If\nϵ= 0.1, we have 10% of noise in the solution measurements for the training and testing data (see\nFigure 11).\n(a)\n (b)\n (c)\n4\n6\n8\n10\n12\n(d)\n4\n6\n8\n10\n12\n (e)\n4\n6\n8\n10\n12\n (f)\nFigure 11: Galerkin transformer evaluation for the inverse interface coefﬁcient identiﬁcation problem\nusing the same sample with Figure 10, model trained and evaluated under the same amount of noises:\n(a)–(c) the input uh(x) with noise level 0, 1%, and 10% on 211 ×211 grid; (d)–(f) the recovered\ncoefﬁcient through evaluation with noise level 0, 1%, and 10% on 71 ×71 grid with relative error\nbeing 0.0160, 0.0292, and 0.0885, respectively.\nWhy not ﬁne grid reconstruction? The reason we can only reconstruct the coarse grid coefﬁcient\nis as follows. Since we use an upsampling interpolation from the coarse to ﬁne grids, a limitation of\nthe 2D operator learner structure in Figure 5 is that it can approximate well if the target is smooth, and\nconsists most combination of basis functions of lower frequencies. The low-frequency part, which\ncan be roughly interpreted as the general trends, of the solution can be well-resolved by the coarse\ngrid, then the operator learner beneﬁts from the smoothing property of the operator a priori, as well\nas the approximation property of the interpolation operator. If the high frequency part of the target\nis prevailing due to low regularity (such as L∞), the model can only resolve the frequency up to of\ngrid size nc ×nc as the upsampling interpolation loses the approximation order (prolongation error\nestimate from coarse to ﬁne grids, see e.g., [38, Chapter 6.3]).\nMore limitations. Moreover, we do acknowledge another limitation of the proposed operator\nlearner: it suffers from a common foe in many applications of deep learning, the instability with\nrespect to the noise during evaluation. If the model is trained with clean data, in evaluation it becomes\noversensitive to noises, especially in Example 5.3 due to the large Lipschitz constant in the original\nproblem itself, which is further ampliﬁed by the black-box model. Therefore, we recommend adding\ncertain amount of noises for inverse coefﬁcient identiﬁcation problems. See Figure 12.\n24\n4\n6\n8\n10\n12\n(a)\n4\n6\n8\n10\n12\n (b)\n4\n6\n8\n10\n12\n (c)\n4\n6\n8\n10\n12\n(d)\n4\n6\n8\n10\n12\n (e)\n4\n6\n8\n10\n12\n (f)\nFigure 12: Effect of noise in the inverse interface coefﬁcient identiﬁcation using the same sample\nwith Figure 10 and 11, ε: relative error in L2-norm. (a)–(b) model trained with no noise, 1% and\n1.5% noises in evaluation, ε = 0.194 and ε = 0.416; (c) model trained with 1% noise, no noise\nin evaluation, ε = 0.0235; (d) model trained with 1% noise, 2% in evaluation, ε = 0.0754. (e)\nmodel trained with 1% noise, 5% in evaluation, ε= 0.403. (f) model trained with 10% noise, 5% in\nevaluation, ε= 0.0691.\nD Proof of The Approximation Capacity of A Linear Attention\nIn this section, we prove Theorem 4.3, which shows that the linear attention variant we proposed in\n(6), Galerkin-type attention operator (nonlinear), is capable of replicating explicitly a Petrov-Galerkin\nprojection (linear) in the current latent representation subspace under a Hilbertian setup. To better\nelaborate our Galerkin projection-inspired modiﬁcations to the attention operator, in Section D.1, the\ntechnical background that bridges (14) to a learnable Petrov-Galerkin projection is presented. Then,\nsome historical contexts are provided in Section D.2 for an overview of Theorem 4.3, connecting the\nsequence-length invariant training of the attention operator to how important a theorem like Theorem\n4.3 is for an operator approximation problem in traditional applied mathematics. In Section D.3, the\nproof of Theorem 4.3 is shown with a full array of mathematically rigorous setting and assumptions.\nThereafter, in Section D.4 some possible generalizations are discussed, together with the role of\nremoving the softmax in obtaining a sequence-length uniform bound, as well as the heuristic behind\nthe choice of the Galerkin projection-type normalization in the scaled dot-product attention. Last but\nnot least, technical lemmata that are needed to show Theorem 4.3 are proved in Section D.5.\nD.1 Background on Galerkin methods\nThe huge success of many Galerkin-type methods in approximating solutions to operator equations\nsuch as PDEs [20] attributes partly to the following two fundamental properties of Hilbert spaces\n(see e.g., [21, Chapter 4]): for (H,⟨·,·⟩)\n• Let Ybe a convex and complete subset of H, and Yis potentially inﬁnite-dimensional. For any\nf ∈H, the projection Πf ∈Y is uniquely determined by\n∥f −Πf∥H= inf\ny∈Y\n∥f −y∥H, (24)\ni.e., the projection recovers the unique element in Ythat is “closest” tof.\n• If His inﬁnitely-dimensional and separable, then there exists a set of orthogonal basis functions\n{ql(·)}∞\nl=1 such that any f ∈H can have its Fourier series expansion:\nf(·) =\n∞∑\nl=1\nalql(·) :=\n∞∑\nl=1\n⟨f,ql⟩\n⟨ql,ql⟩ql(·), (25)\n25\ni.e., Hcan be identiﬁed by ℓ2 (the space that the Fourier coefﬁcients {al}∞\nl=1 are in). Thus, given\nany ﬁxed tolerance under the norm induced by the inner product, any f ∈H can be approximated\nusing a ﬁnite number of basis {ql(·)}d\nl=1 (identiﬁed by a ﬁnite number of coefﬁcients) thanks to\nthe square summability.\nTogether, they rationalize the practice of using a ﬁnite dimensional vector (space) to approximate\nany element in H, or a function in the solution subspace of an operator equation that is compact in\nH. Consider the ﬁnite dimensional approximation space (trial space) Qh := span{˜ql(·)}d\nl=1 (that is\nconvex and closed), where ˜ql(·) := ql(·)/∥ql∥H. The projection Πf onto Qh is the best approximator\nin Qh to f ∈H:\n∥f −Πf∥H= inf\nq∈Qh\n∥f −q∥H. (26)\nExploiting the deﬁnition of ∥·∥H, any perturbationq∈Qh(test space) to the unique (local) minimizer\np:= Πf shall increase the difference in ∥·∥H, thus\n0 = lim\nτ→0\nd\ndτ∥f −(p+ τq)∥2\nH= lim\nτ→0\nd\ndτ⟨f −(p+ τq),f −(p+ τq)⟩,\nChoosing q as ˜ql (l = 1,...,d ), one shall obtain the following if assuming that H= L2(Ω) in a\nsimple case\nΠf(x) =\nd∑\nl=1\n⟨f,˜ql⟩˜ql(x) =\nd∑\nl=1\n(∫\nΩ\nf(ξ)˜ql(ξ) dξ\n)\n˜ql(x), for x∈Ω. (27)\nWhen the set {fj(·)}d\nj=1 is projected onto Qh element by element, (27) carries a resoundingly similar\nform to that of (14). In light of proving the approximation capacity of (14), the differences are:\n(a) The test and trial function spaces are the same in the ideal case above (27), while in a Galerkin-\ntype attention operator (14), they are different and become learnable. This difference brings the\nPetrov-Galerkin projection into the picture. In Section D.3, we shall see that the minimization\nis done for a more general functional (dual) norm ∥·∥V′\nh\n(min-max problem (33)), instead of\n∥·∥H.\n(b) {˜ql(·)}d\nl=1 needs to be orthonormal to yield a compact formula as (27). In the forward\npropagation of the attention operations, there is no such guarantee unless certain orthogonal-\nization/normalization is performed. We shall see in the proof in Section D.3, the Galerkin\nprojection-type layer normalization acts as a cheap learnable alternative to the normalization\nshown in the explicit formula Petrov-Galerkin projection (inverse of the Gram matrices in\n(40)).\nD.2 Overview of Theorem 4.3\nHistorical context. Theorem 4.3 resembles the famous Céa’s lemma (e.g., see [ 13, Theorem\n2.8.1], [ 20, Theorem 2.4.1]). It is one of the most fundamental theorems in approximating an\noperator equation such as a PDE under the Hilbertian framework. Deﬁne the operator norm to be\nthe induced norm from the original Hilbertian norm, the Céa’s “lemma” reads: if the norm of the\noperator associated with the bilinear form is bounded below (either by the Lax-Milgram lemma or\nthe Ladyzhenskaya–Babuška–Brezzi inf-sup condition) in an approximation subspace of the original\nHilbert space, which implies its invertibility, then this mere invertibility implies the quasi-optimality\n(optimal up to a constant) of the Galerkin-type projection to a given function. This says: in the\ncurrent approximation space, measured by the distance of the Hilbertian norm, the Galerkin-type or\nthe Petrov-Galerkin-type projection (see e.g., [80]) is equivalent to the closest possible approximator\nto any given target function (see (26)).\nAs might be expected, whether this approximation space has enough approximation power, such that\nthis closest approximator is actually close to the target, is another story. This closeness, either in terms\nof distance or structure, is usually referred as a part of “consistency” in the context of approximating\na PDE’s solution operator. Any method with a sufﬁcient approximation power together with the\ninvertibility has “convergence”, and this is also known as the Lax equivalence principle [53].\n26\nHeuristic comparison: convergence of traditional numerical methods versus a data-driven\noperator learner. For a traditional numerical method that approximates an operator equation\nto be successful, in that scientists and engineers can trust the computer-aided simulations, the\naforementioned convergence is indispensable. One key difference of a traditional numerical method\nto an attention-based operator learner is how the “convergence” is treated.\n• A traditional numerical method: once the discretization is ﬁxed (ﬁxed degrees of freedom such\nas grids, radial bases, Fourier modes, etc.), the approximation power of this ﬁnite dimensional\napproximation space is ﬁxed. The method seeks the best approximator to a single instance in\nthis ﬁxed space. The convergence refers to the process of error decreasing as one continuously\nenlarges the approximation subspaces (e.g., grid reﬁnement, more Fourier modes).\n• An attention-based operator learner: the approximation space is not ﬁxed, and is constantly\nreplenished with new bases during optimization (e.g., see the remarks in 4.1.3), thus able to\napproximate an operator’s responses in a subset/subspace in a much more dynamic manner. A\npossible exposition of the “convergence” is more problem-dependent as one obtains a better set\nof basis to characterize the operator of interest progressively through the stochastic optimization.\nPositional encoding and a dynamic feature/basis generation. Even though [ 88] opens a new\nera in NLP by introducing the state-of-the-art Transformer, in an operator learning problem, we\nﬁnd that its explanation unsatisfactory on the absolute necessity to include the positional encodings\nin the attention mechanism. From the proof of Theorem 4.3 we see that, if we ought to learn\nthe identity operator from Qh to Qh (with a few other caveats), i.e., the target f itself is in the\napproximation subspace Qh, then the attention mechanism has certainly the capacity to learn this\noperator exactly without any positional encodings. We want to emphasize that in addition to the\nremarks in 4.1.3, in our interpretation, the utmost importance of the positional encoding is to make\nthe approximation subspaces dynamic. Otherwise, the Galerkin-type attention (or a linear attention)\nis simply a linear combination of the current approximation subspace (or a convex combination in\nits softmax-normalized siblings). Consequently, the approximation power of the subspaces cannot\nenjoy this dynamic update mechanism through optimizations. In this sense, we can view the attention\nmechanism a universal “dynamic feature/basis generator” as well. For an empirical evidence of this\nlayer-wise dynamic basis update in our experiments of the Darcy interface ﬂow please refer to Figure\n13.\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\nFigure 13: Extracted latent representation sequences reshaped to nc ×nc from the encoder layers in\nthe Galerkin Transformer using the same sample with Figure 10 and 11 in evaluation; (a)–(b): two\nbasis functions from the ﬁrst encoder layer; (c)–(d): two other basis functions from the fourth encoder\nlayer, and we note that visually all basis functions in the fourth layers are smoother than those in the\nﬁrst; (e)–(f): −∇·(a∇(·))’s ﬁrst two eigenfunction approximations using the bilinear ﬁnite element\non the nc ×nc grid, i.e., the ﬁrst two Fourier bases associated with this self-adjoint operator.\n27\nD.3 Proof of Theorem 4.3\nNotations. Before presenting the proof, to avoid confusion of the notions for various ﬁnite dimen-\nsional function spaces, the settings for different spaces are paraphrased from the condensed versions\nin Table 4. The caveat is that for the same function in the latent approximation space, it has two\nvector representations: (i) nodal values at the grid points which can be used to form the columns\nof Q,K,V , this vector is in Rn; (ii) the vector representation using degrees of freedom (coefﬁcient\nfunctional) in an expansion using certain set of basis, this vector is in Rd or Rr (r≤d<n ).\nWe also note that in the context of using the Galerkin-type attention (or other linear attentions) to\napproximate functions under a Hilbertian framework, in that the output of the attention-based map\ncan represent a Petrov-Galerkin projection, Qstands for values, Kfor query, and V for keys. In the\nproof of Theorem 4.3, we shall refer the discrete approximation space generated by Qas a “value\nspace”, and that ofV as a “key space”. Meanwhile,Ω and Ω∗can be seen as spacial/temporal domain\nand frequency domain, respectively.\nAssumption D.1 (assumptions and settings for Theorem 4.3). The following notations and assump-\ntions are used throughout the proof of the quasi-optimal approximation result in Theorem 4.3:\n(D1) (H,⟨·,·⟩H) is a Hilbert space. For f ∈H, f : Ω →R. H↪→C0(Ω). Ω ⊂Rm is a bounded\ndomain, discretized by {xi}n\ni=1 with a mesh size h.\n(D2) Yh ⊂H is an approximation space associated with {xi}n\ni=1, such that for any y ∈Yh,\ny(·) = ∑n\ni=1 y(xi)φxi (·) where {φxi (·)}n\ni=1 form a set of nodal basis for Yh in the sense that\nφxi (xj) = δij, and the support of every nodal basis φxi (·) is of O(hm).\n(D3) (V,⟨·,·⟩V) is a latent Hilbert space. For v∈V, v: Ω∗→R. V↪→C0(Ω∗). Ω∗≃Ω and is\ndiscretized by {ξi}n\ni=1 with a mesh size h.\n(D4) Wh ⊂V is an approximation space associated with {ξi}n\ni=1, i.e., for any w∈Wh, w(·) =∑n\ni=1 w(ξi)ψξi (·) where {ψξi (·)}n\ni=1 form a set of nodal basis for Wh in the sense that ψξi (ξj) =\nδij, and the support of every nodal basis ψξi (·) is of O(hm).\n(D5) y ∈Rn×d is the current input latent representation. WQ,WK,WV denote the current\nprojection matrices. n>d>m and rank y = d.\n(D6) Qh ⊂Yh ⊂Q is the current value space from Q. Qis a subspace of Hwith the same\ntopology. Qh is the spanned by basis functions whose degrees of freedom associated with {xi}n\ni=1\nform the columns of Q:= yWQ ∈Rn×d, i.e., Qh = span{qj(·) ∈Yh : qj(xi) = Qij, 1 ≤i≤\nn, 1 ≤j ≤d}.\n(D7) Vh ⊂Wh ⊂V is the current key space from V. Vh is the spanned by basis functions\nwhose degrees of freedom associated with {ξi}n\ni=1 form the columns of V := yWV ∈Rn×d, i.e.,\nVh = span{vj(·) ∈Wh : vj(ξi) = Vij, 1 ≤i≤n, 1 ≤j ≤d}.\n(D8) V′\nh is the dual space of Vh consisting of all bounded linear functionals deﬁned on Vh;\n∥g(·)∥V′\nh\n:= supv∈Vh |g(v)|/∥v∥V for g∈V′\nh.\n(D9) dim Qh = r≤dim Vh = d, i.e., the key space is bigger than the value space.\n(D10) For w∈Vh, w(·) = ∑d\nj=1 µvj (w)vj(·) is the expansion in {vj(·)}d\nj=1, where µvj (·) ∈V′\nh\nis the coefﬁcient functional; in this case,wcan be equivalently identiﬁed by its vector representation\nµ(w) := (µv1 (w),··· ,µvd (w))⊤∈Rd.\n(D11) Forp∈Qh, p(·) = ∑r\nj=1 λqj (p)qj(·) is the expansion in {qj(·)}r\nj=1, where λqj (·) ∈Q′\nh is\nthe coefﬁcient functional; in this case, pcan be equivalently identiﬁed by its vector representation\nλ(p) := (λq1 (p),··· ,λqr (q))⊤∈Rr.\n(D12) b(·,·) : V×Q→ R is a continuous bilinear form, i.e., |b(v,q)|≤ c0∥v∥V∥q∥Hfor any\nv∈V, q∈Q. For (w,y) ∈Wh ×Yh ⊂V×Q , b(w,y) := hm∑n\ni=1 w(ξi)y(xi).\n(D13) gθ(·) : Rn×d →Qh,y ↦→z is a learnable map that is the composition of the Galerkin-\ntype attention operator (6) with an updated set of {˜WQ,˜WK,˜WV}and a pointwise universal\napproximator; θdenotes all the trainable parameters within.\n28\nTheorem 4.3 (Céa-type lemma, general version) . For any f ∈ H, under Assumption D.1, for\nfh ∈Qh being the best approximator of f in ∥·∥H, we have:\nmin\nθ\n∥f −gθ(y)∥H≤c−1 min\nq∈Qh\nmax\nv∈Vh\n|b(v,fh −q)|\n∥v∥V\n+ ∥f −fh∥H. (28)\nProof. By triangle inequality, inserting the best approximation fh ∈Qh\n∥f −gθ(y)∥H≤∥fh −gθ(y)∥H+ ∥f −fh∥H. (29)\nfh := argminq∈Qh ∥f −fh∥Hdescribes the approximation capacity of the current value space Qh\nand has nothing to do with θ.\nIn the ﬁrst part of the proof, we focus on bridging ∥fh −gθ(y)∥Hin (29) to the linear problem\nassociated with seeking the Petrov-Galerkin projection of fh. By Lemma D.6, the continuous linear\nfunctional deﬁned by b(·,q) : V→ R, v↦→b(v,q) is bounded below on the current key space, i.e.,\nthere exists c> 0 independent of qor the discretization (mesh size h) such that,\n∥b(·,q)∥V′\nh\n≥c∥q∥H, for any ﬁxed q∈Qh. (30)\nBy the deﬁnition of ∥·∥V′\nh\nin (D8), we have\n∥fh −gθ(y)∥H≤c−1 sup\nv∈Vh\n|b(v,fh −gθ(y))|\n∥v∥V\n= c−1 max\nv∈Vh\n|b(v,fh −gθ(y))|\n∥v∥V\n.\nIn the rest of the proof, the goal is to establish\nmin\nθ\nmax\nv∈Vh\n|b(v,fh −gθ(y))|\n∥v∥V\n≤min\nq∈Qh\nmax\nv∈Vh\n|b(v,fh −q)|\n∥v∥V\n, (31)\ni.e., the best approximation based on the attention operator, if exists, is on par with that offered by a\nPetrov-Galerkin projection.\nBy the continuity of b(·,·) in (D12), given any ﬁxed q∈Qh, v↦→b(v,q) deﬁnes a bounded linear\nfunctional for any v ∈Vh. Moreover, since we use ℓ2-inner product to approximate ⟨·,·⟩on Vh,\ndeﬁne\n⟨u,v⟩h := hm\nn∑\ni=1\nu(ξi)v(ξi) ≈\n∫\nΩ∗\nu(ξ)v(ξ) dξ=: ⟨u,v⟩V, for u,v ∈Vh. (32)\nApplying the Riesz representation theorem (e.g., see [21, Theorem 4.6]), together with Lemma D.3,\nimplies that there exists a value-to-key linear map Φ for fh ∈Qh\nΦ : Qh →Vh, such that b(v,fh) = ⟨Φfh,v⟩h for any v∈Vh.\nThus, we have for the right-hand side of (31)\nmin\nq∈Qh\nmax\nv∈Vh\n|b(v,fh −q)|\n∥v∥V\n= min\nq∈Qh\nmax\nv∈Vh\n|⟨Φfh,v⟩h −b(v,q)|\n∥v∥V\n. (33)\nUsing (D6),(D7),(D10),(D11), and Lemma D.5, the problem on the right-hand side of (33) is\nequivalent to the block matrix form (47) as follows.\n(\nM B ⊤\nB 0\n)(\nµ\nλ\n)\n=\n(\nζ\n0\n)\n. (34)\nIn the system above, (µ,λ) ∈Rd ×Rr are the vector representations of the critical point (w,p) ∈\nVh ×Qh, if exists, under basis sets {vj(·)}d\nj=1 and {qj(·)}r\nj=1, respectively, and\nζ∈Rd, where (ζ)j = ⟨Φfh,vj⟩h ≈\n∫\nΩ∗\n(Φfh)(ξ)vj(ξ) dξ. (35)\nIn the next part of the proof, we shall seek the solution to(34), which is the Petrov-Galerkin projection\nto the function of interest fh. Since {vj(·)}d\nj=1 form a set of basis of Vh, M is invertible, and we can\neliminate µby solving the ﬁrst equation above and plugging it in the second to get\nBM−1B⊤λ= BM−1ζ. (36)\n29\nKnowing that M−1 is symmetric positive deﬁnite, to show\nλ= (BM−1B⊤)−1BM−1ζ, (37)\nit sufﬁces to show that B is surjective (full row rank) if we ought to use Lemma D.4. A simple\nargument by contradiction is as follows: suppose Bis not full row rank, then there exists a linear\ncombination of the rows of B being the 0 vector, i.e., there exists a set of nontrivial coefﬁcients\n˜λ:= (˜λ1,..., ˜λr)⊤such that\nb(v,˜p) = 0, for any v∈Vh where 0 ̸≡˜p(·) :=\nr∑\nl=1\n˜λlql(·).\nThis is contradictory to the lower bound of b(·,˜p) in (30):\n0 <c∥˜p∥H≤∥b(·,˜p)∥V′\nh\n= max\nv∈Vh\n|b(v,˜p)|\n∥v∥V\n= 0.\nThus, (37) holds and the critical point p(or its vector representation) exists and is a local minimizer\ndue to M being positive deﬁnite.\nThe last part of the proof is to show that this p∈Qh is representable by the learnable map gθ(y).\nTo this end, we multiply a permutation matrix U ∈Rd×d to Q, such that QU’s ﬁrst r columns\nQ0 ∈Rn×r form the nodal value vector(qj(x1),··· ,qj(xn))⊤for bases {qj(·)}r\nj=1 of Qh; see (D2).\nThen, multiplying the permuted basis matrix QU with (λ 0)⊤∈Rd yields the best approximator\np’s vector representation at the grid pointsp:= (p(x1),··· ,p(xn))⊤\np= (Q0 Q1)\n(\nλ\n0\n)\n= QU\n(\nλ\n0\n)\n. (38)\nMoreover, since Bij = b(vj,qi) with {qi(·)}r\ni=1 and {vj(·)}d\nj=1 being the sets of basis for Qh and\nVh, it is straightforward to verify that B = hmQ⊤\n0 V; see (D12). Here without loss of generality,\nwe simply assume that the nodal value vector representation (vj(ξ1),··· ,vj(ξn))⊤=: vj forms the\nj-th column of V in a sequential order. Therefore, using (37), (λ 0)⊤∈Rd can be written as the\nfollowing block form:\n(\nλ\n0\n)\n=\n(\n(BM−1B⊤)−1\n0\n)(\nB\n∗\n)\nM−1ζ= hm\n(\n(BM−1B⊤)−1\n0\n)(\nQ⊤\n0\nQ⊤\n1\n)\nVM −1ζ.\n(39)\nConsequently, the ideal updated {˜Q, ˜K, ˜V}that has capacity to obtain the current best approximator\np, or its vector presentation pin (38), are\n˜Q:= y˜WQ ←yWQU,\n˜K := y˜WK ←yWQUΛ,\n˜V := y˜WV ←yWVM−1,\nand p= hm˜Q( ˜KT ˜V)ζ,\n(40)\nwhere Λ := blkdiag\n{\n(BM−1B⊤)−1,0\n}\nand is symmetric. This essentially implies that gθ(·) has\ncapacity to learn this best approximator pusing the updated set of {˜Q, ˜K, ˜V}from (40):\ngθ(·) : Rn×d →Qh,y ↦→p, such that p(xi) =\n(\nhm˜Q( ˜KT ˜V)ζ\n)\ni for i= 1,··· ,n.\nPassing the inequality from minimizing in a bigger set yields the desired inequality (31), which, in\nturn, shows the approximation capacity indicated in the theorem.\nDynamical basis update for a set of functions. Despite the fact that Theorem 4.3 is for a single\ninstance of f ∈H, it is not difﬁcult to see that the form (39) easily extends to a latent representation\nin Rn×d whose columns are {fj,h(·)}d\nj=1 sampled at the grid points {xi}n\ni=1. Simply replacing ζ\nin (35) by a matrix WZ := ( ζ1,··· ,ζd) ∈Rd×d, of which the i-th entry in the j-th column is\n(ζj)i = ⟨Φfj,h,vi⟩h, we can verify that a pointwise FFN gθ(·) is fully capable of representing WZ.\nUsing the argument above, we can further elaborate the dynamical basis update nature of the scaled\ndot-product attention (14) (see also the remarks in 4.1.3 and Appendix D.2.3):\n30\n• Test the columns of V (query) against the columns of K(keys), and use the responses to seek a\n“better” set of basis using the columns ofQ(values).\nTo this end, we modify Assumption D.1 slightly by appending (with a superscript +) or redeﬁning\n(with a superscript ∗) certain entries as follows.\nAssumption D.2 (assumptions and settings for Theorem 4.4). Assumption D.1 are assumed to hold\nwith the following amendments to their respective numbered entries for the proof of Theorem 4.4:\n(D+\n6 ) Kh ⊂Yh ⊂K is the current key space from K and is deﬁned on Ω∗. Deﬁne ⟨s,k⟩h :=\nhm∑n\ni=1 s(ξi)k(ξi) as an inner product for s,k ∈Kh.\n(D∗\n7) Vh ⊂Wh ⊂V⊆Q is the current query space from V deﬁned on Ω.\n(D∗\n12) b(·,·) : K×Q→ R is continuous. b(k,q) := hm∑n\ni=1 k(ξi)q(xi) for (k,q) ∈Yh ×Wh.\n(D+\n12) a(·,·) : V×K→ R is continuous. a(w,k) := hm∑n\ni=1 w(xi)k(ξi) for (w,k) ∈Wh×Yh.\nAfter the introduction of a(·,·), a new continuous functional can be deﬁned as follows:\nℓ(w,q)(·) : K→ R, k ↦→a(w,k) −b(k,q). (41)\nTheorem 4.4 (layer-wise basis update of the scaled dot-product attention). Under Assumption D.2,\nand the columns of V and Qare formed by the DoFs of query and value functions, respectively,\ni.e., Vij = vj(xi) and Qij = qj(xi), 1 ≤i ≤n, 1 ≤j ≤d. Then, there exists an updated set\n{˜ql(·)}d\nl=1 ⊂Qh = span{ql(·)}d\nl=1 of value functions, such that zj(·) ∈Qh satisfying the basis\nupdate rule in (14)\nzj(·) :=\nd∑\nl=1\na(vj,kl) ˜ql(·), for j = 1,··· ,d,\nis the minimizer to the following problem for every j = 1,··· ,d\nℓ(vj,zj)(·)\n\nK′\nh\n= min\nq∈Qh\na(vj,·) −b(·,q)\n\nK′\nh\n= min\nq∈Qh\nmax\nk∈Kh\n|a(vj,k) −b(k,q)|\n∥k∥K\n. (42)\nProof. The proof repeats most parts from that of Theorem 4.3 in Appendix D.3. By Lemma D.5,\nb(·,q) : k ↦→b(k,q) is bounded below on the current key space Kh, i.e., ∥b(·,q)∥K′\nh\n≥c∥q∥Hfor\nany q ∈Qh ﬁxed. Noting that the variational formulation corresponding to the min-max problem\nin (42) have the same form as the one in (45)–(46) in Lemma D.5 with the right-hand side switched\nfrom ⟨·,·⟩h to a(·,·), therefore, we conclude that the following operator equation associated with the\nright-hand side of (42) has a unique solution zj ∈Qh:\n{⟨s,k⟩h + b(k,zj) = a(vj,k), ∀k∈Kh,\nb(s,q) = 0, ∀q∈Qh. (43)\nIt is straightforward to verify that hm(K⊤V)ij = a(vj,ki) thus the matrix associated with a(·,·)\nfor all {vj(·)}d\nj=1 is hm(K⊤V). Consequently, following the last part of the proof of Theorem 4.3\nand writing in the form of scaled dot-product, we let z ∈Rn×d be the latent representation with\ncolumns being {zj(·)}d\nj=1 at the grid points, i.e., zj(xi) = zij, 1 ≤i ≤n, 1 ≤j ≤d, and carry\nover identical deﬁnitions for all matrices involved with the ones used in (40),\nz = hmQU˜WQ(K⊤V), where ˜WQ := (QUΛ)⊤(VM −1) ∈Rd×d.\nThe corollary is proved by setting {˜ql(·)}d\nl=1 as functions with DoFs being columns of QU˜WQ.\nHeuristics on learning the latent representations. In the dynamical basis update interpretation\nabove, the scaled dot-product attention is capable to bring the latent representation of query as\nclose as possible to that of values in a functional distance, which is ∥a(vj,·) −b(·,q)\n\nK′\nh\nin (42).\nHeuristically speaking in each encoder layer, an ideal operator learner could learn a latent subspace\non which the input (query) and the target (values) are “close”, and this closeness is measured by how\nthey respond to a dynamically changing set of basis (keys).\n31\nD.4 Interpretations, remarks, and possible generalizations\nInspirations from ﬁnite element methods. The ﬁrst part of the proof for Theorem 4.3 follows\nclosely to the ﬁnite element methods of the velocity-pressure formulation of a stationary 2D Stokesian\nﬂow [ 13, Lemma 12.2.12], where the key is to establish the solvability of an inf–sup problem\n(min–max in our ﬁnite dimensional setting). The n-independent lower bound of b(·,q) plays a key\nrole in the convergence theory of traditional ﬁnite element methods (see the remarks in Appendix\nD.2.1–D.2.2). In our interpretation of the scaled dot-product attention, this independence is essentially\nconsequent on the diagonalizability of normalizations of the two matrices in this product (cf. the\ndiscussion in Appendix D.4.4; see also the proof in Lemma D.6). The singular values of the attention\nmatrix should be independent of the sequence length if the lower bound of b(·,q) is to be veriﬁed\nwith a constant independent of n. The presence of softmax renders this veriﬁcation impossible (see\nRemark D.7).\nIn traditional ﬁnite element methods [ 13, Chapter 12], the dimension of the approximation sub-\nspaces are usually tied to the geometries (discretization, mesh), for example, dim Vh = 2 n =\n2(#grid points), and dim Qh = #triangles ≃O(n). Inspired by [13, Lemma 12.2.12], the introduc-\ntion of the bilinear form b(·,·) : V×Q→ R here in Theorem 4.3 is to cater the possibility that the\ncolumn spaces of V and Qmay represent drastically different functions.\nIn our proof, we have shown that, using small subspaces (dimension dand r) of these complete ﬁnite\nelement approximation spaces (dimension n ≫d ≥r) sufﬁces to yield the best approximator if\nthis key-to-value map exists. This ﬁts perfectly into our setting to use merely the column space of\nQto build the degrees of freedom (DoF) for the “value” functions: the DoF functional is simply a\nfunction integrating against the function which is discretized at the grid points, i.e., a dot-product in\nthe sequence-length dimension represents an integral. Recently, we also become aware that the CV\nand NLP communities begin to exploit this topological structure in the feature (channel) dimension\nby treating the vector in the same feature dimension as a function to exert operations upon, instead of\nmainly relying on position-wise operations, see e.g., the work in [83, 54] essentially acknowledges\nAssumption 4.2 implicitly.\nIt is also worth noting that, the subscripthin the approximation subspaces is a choice of the exposition\nof the proof to facilitate the notion that “dot product ≈integral”. In computations, unlike ﬁnite\nelement methods where the basis functions are locally supported, the learned basis functions are\nglobally supported, dynamically updated, and not bound to a ﬁxed resolution. As such, numerically\nthe approximation subspaces are essentially mesh-free (e.g., see (14), (27), and Figure 13), how this\nnature attributes to the capability of zero-shot super-resolution of kernelizable architectures (e.g.,\n[57]) will be an interesting future study.\nDifference with universal approximation theorems. In the era of using a nonlinear approximator\nsuch as a deep neural network to approximate functions/operators, following the discussion in Section\nD.2, the “consistency” of an approximation class embodies itself as the Universal Approximation\nTheorem (UAT, e.g., see [42, 18], and [58, 67, 77, 99] for modern expositions). A UAT roughly\ntranslates to “a d-layer (d≥2) neural network with certain nonlinear activation can approximate any\ncompactly supported continuous function”, with the approximation error depending on the width and\nthe depth of a network.\nIn our work, Theorem 4.3 is in its spirit a “stability” result of the attention-based learner. Under\nthis ﬁxed general dot-product attention architecture, Theorem 4.3 tries to bridge the approximation\ncapacity of a nonlinear operator (hard to quantify) to a linear one (easy to ﬁnd). To further give a more\nreﬁned bound in terms of the architectural hyperparameters (number of layers, number of learned\nbasis, etc.), one shall invoke a UAT on top of the result in Theorem 4.3. Theorem 4.3 states that the\nattention mechanism, in terms of architecture, allows practitioners to exploit simple non-learnable\nlinear approximation results as well, for example the number of Fourier bases that one chooses to\nbuild the “value” space.\nPDE-inspired attention architectures. The lower bound of b(·,q) gives a theoretical guideline\non how to design a new dot-product attention between compatible subspaces. For example, we can\nuse the inter-position differences in certain direction (ﬂux) as query to test against the key, which\nmay provides more information on how to choose the best value. This aforementioned “differential\nattention” corresponds to ⟨q,div v⟩in the velocity-pressure formulation of the Stokesian ﬂow.\n32\nIn the proof, we assume nothing such as Qh ⊂Vh but only bridge them through a value-to-key map.\nThe lower bound of this value-to-key map using the simple scaled dot-product is veriﬁed in Lemma\nD.6. In general, we have two guidelines: (1) the scaling shall be chosen such that the lower bound\n(50) is independent of the sequence length; (2) the key space Vh (functions to test the responses\nagainst) is bigger than the value space Qh, which contains functions to approximate the target or to\nform a better set of latent basis. In practice, for example, Q,V ∈Rn×r,Rn×d with d≥rin [19].\nWe remark that the proof is done for dim Vh = d, but in general it applies to dim Vh ≤das the ﬁnal\nblock form matrices (39) can be easily adjusted.\nFrom the proof, it is also straightforward to see that the columns ofQ,K,V merely act as the degrees\nof freedom representations associated with xi or ξi, thus not necessarily the pointwise value at xi or\nξi. The essential requirement is that, there exists two sets of locally supported nodal basis functions\nin Qh and Vh associated with xi or ξi to build the globally supported learned bases. These nodal\nbasis functions are assumed to be supported in anO(hm)-neighborhood of xi or ξi, which implies the\nsparse connectivity of the grids in the discretization. Here the degrees of freedom being the pointwise\nvalues is assumed merely for simplicity, thus imposing H↪→C0(Ω) and V↪→C0(Ω∗) to ensure the\nexistence of the pointwise values is just a technicality and not essential. As a result, this broadens\nthe possibility of a more structure-preserving feature map. For example, the bilinear form b(·,·) can\nincorporate information from the higher derivatives, DoFs for splines, etc.; another example is that a\nsingle entry in a column of Q,K,V may stand for an edge DoF vector representation in a graph, and\nthe edge–edge interaction is extensively studied for the simulation of Maxwell’s equations [63, 15, 6]\non manifolds [94].\nGalerkin-type layer normalization scheme. In the proposed Galerkin-type attention(6), the layer\nnormalization is pre-dot-product but post-projection; cf. the pre-LN scheme in [96] is pre-dot-product\nand pre-projection. From the proof of Theorem 4.3, it is natural to impose such a normalization\nfrom a mathematical perspective. As in (40), the updated ˜Kand ˜V have (BM−1B⊤)−1 and M−1\nas their normalizations, respectively. While M is the Gram matrix for V, the inverse of M’s Schur\ncomplement can be understood as the inverse of the Riesz map from Qto V. Since a given layer\nnormalization module is shared by every position of a latent representation upon which it is exerted,\nthe weight acting as the variance (in Rd) in the layer normalization acts a cheap learnable diagonal\nmatrix approximation to M−1 ∈Rd×d. This offers a reasonable approximation if one assumes the\nself-similarity of the bases ∥vj∥2\nVoutweighs the inter-basis inner product ⟨vi,vj⟩for i̸= j.\nD.5 Auxiliary results\nLemma D.3. Consider Ω∗ ⊂Rm a bounded domain, we assume (V,⟨·,·⟩V) ↪→C0(Ω∗), where\n⟨u,v⟩V:=\n∫\nΩ∗u(ξ)v(ξ) dξ. Ω is discretized by {ξi}n\ni=1. Yh ⊂V is an approximation space deﬁned\non {ξi}n\ni=1 and Yh ≃Rn. Yh = span {φξ1 ,··· ,φξn }such that the degree of freedom for the\ni-th nodal basis is deﬁned as χξi (v) := v(ξi). Vh = span{v1 ··· ,vd}for d < nwith dlinearly\nindependent vj(·) ∈Yh. Then, ⟨·,·⟩h : V×V→ R deﬁnes a continuous inner product in Vh, where\n⟨u,v⟩h := hm\nn∑\ni=1\nu(ξi)v(ξi), for u,v ∈Vh. (44)\nProof. First we show that ⟨v,v⟩h = 0 implies that v ≡0. By (44), obviously this v satisﬁes\nv(ξi) = 0 for i= 1,··· ,n. Now expanding vin {φξi }n\ni=1 we have\nv(·) =\nn∑\ni=1\nχξi (v)φξi (·) =\nn∑\ni=1\nv(ξi)φξi (·) ≡0.\nThus, ∥·∥2\nVh := ⟨·,·⟩h deﬁnes a norm, and it is equivalent to ∥·∥L2(Ω∗) restricted on Vh due to being\nﬁnite dimensional. The desired result follows from applying the Cauchy-Schwarz inequality.\nLemma D.4. If r≤d, B ∈Rr×d has full row rank, G∈Rd×d is symmetric positive deﬁnite, then\nBGB⊤∈Rr×r is invertible.\n33\nProof. With G being symmetric positive deﬁnite, upon applying a Cholesky factorization G =\nCC⊤, we have BGB⊤ = ( BC)(BC)⊤. Now it is straightforward to see that rank(BC) = r\nas rank(BC) ≤min{r,d}and rank(BC) ≥rby the Sylvester’s inequality. Applying the same\nargument on the product of BC with (BC)⊤yields the desired result of rank(BGB⊤) = r.\nLemma D.5. Under the same setting with Theorem 4.3 (Assumption D.1), given a u∈Vh, looking\nfor a saddle point of\nmin\nq∈Qh\nmax\nv∈Vh\n|⟨u,v⟩h −b(v,q)|\n∥v∥V\n(45)\nis equivalent to solving the following operator equation system: ﬁnd p∈Qh,w ∈Vh\n{⟨w,v⟩h + b(v,p) = ⟨u,v⟩h, ∀v∈Vh,\nb(w,q) = 0, ∀q∈Qh. (46)\nIt is further equivalent to solve the following linear system if {vj(·)}d\nj=1 and {qj(·)}r\nj=1 form sets of\nbasis for Vh and Qh, respectively:\n(\nM B ⊤\nB 0\n)(\nµ\nλ\n)\n=\n(\nζ\n0\n)\n, (47)\nwhere M ∈Rd×d with Mij = ⟨vj,vi⟩h. B ∈Rr×d with Bij = b(vj,qi). ζ ∈Rd with (ζ)j =\n⟨u,vj⟩h. For w(·) ∈Vh, its vector representation is Rd ∋µ:= µ(w) = (µv1 (w),...,µ vd (w))⊤\nfor w(·) = ∑d\nj=1 µvj (w)vj(·). Similar notion applies to λ:= λ(p) = (λq1 (p),··· ,λqr (p))⊤∈Rr\nbeing the vector representation for p(·) = ∑r\nj=1 λqj (p)qj(·) where p(·) ∈Qh.\nProof. This lemma is a ﬁnite dimensional rephrasing of the commonly-known variational formulation\nof a saddle point problem in inﬁnite dimensional Hilbert spaces [33, Chapter I §4.1], for completeness\nwe include the proof here for the convenience of readers.\nDeﬁne η(·) : Vh →R such that η(v) := ⟨u,v⟩h −b(v,q), using Lemma D.3 and the assumptions\n(D11) (D12) on b(·,·) in Assumption D.1, then clearly η ∈V′\nh for (Vh,⟨·,·⟩h). By Riesz repre-\nsentation theorem, there exists an isomorphism R : Vh →V′\nh such that w := R−1(η) ∈Vh and\nη(v) = ⟨w,v⟩h = ⟨u,v⟩h −b(v,q). (48)\nThen, (45) is equivalent to ﬁnd the minimizer for the following problem, deﬁne ∥·∥2\nVh := ⟨·,·⟩h:\nmin\nq∈Qh\n∥η(·)∥2\nV′\nh\n= min\nq∈Qh\n∥w∥2\nVh = min\nq∈Qh\n∥R−1(⟨u,·⟩h −b(·,q))∥2\nVh =: min\nq∈Qh\nJ(q).\nTaking the Gateaux derivativelimτ→0 dJ(p+ τq)/dτ in order to ﬁnd the critical point(s) p∈Qh,\nwe have for any perturbation q∈Qh such that p+ τq ∈Qh\n0 = lim\nτ→0\nd\ndτ\n⟨\nR−1(⟨u,·⟩h −b(·,p + τq),R−1(⟨u,·⟩h −b(·,p + τq))\n⟩\nh,\nand applying R−1 on b(·,p) ∈V′\nh, it reads for any q∈Qh\n⟨\nR−1(\n⟨u,·⟩h −b(·,q)\n)\n,R−1(b(·,q))\n⟩\nh =\n⟨\nw,R−1(b(·,q))\n⟩\nh = b(w,q) = 0. (49)\nThus, (48) and (49) form the desired system (46). Lastly, applying an expansion of w and p in\n{vi(·)}d\ni=1 and {qi(·)}r\ni=1 with degrees of freedom µvj (·) and λqj (·) respectively, and choosing the\ntest functions to be each v= vj for j = 1,··· ,d and q= qj for j = 1,··· ,r, yield the system (47)\nwith iand jrepresenting the row index and column index, respectively.\nLemma D.6 (veriﬁcation of the lower bound of b(·,p)). Under the setting of Assumption D.1, for\nany given p∈Qh, we have\nc∥p∥H≤max\nw∈Vh\n|b(w,p)|\n∥w∥V\n, (50)\nwhere the constant c = c−1\nV c−1\nQ minj=1,...,r|σj|. {σj}r\nj=1 are the singular values of the matrix\nB ∈Rr×d with Bij = b(vj,qi) under the sets of basis {vj(·)}d\nj=1 and {qj(·)}r\nj=1. cV, cQ are the\nnorm equivalence constants between functions w(·) ∈Vh, p(·) ∈Qh and their vector representation\n34\nusing the DoF functionals µ(w) and λ(p) deﬁned in Lemma D.5 under the sets of basis {vj(·)}d\nj=1\nand {qj(·)}r\nj=1: for any w∈Vh, and any p∈Qh, it is assumed that the following norm equivalence\nholds\n∥w∥V≤cV∥µ(w)∥ and ∥p∥H≤cQ∥λ(p)∥. (51)\nProof. The proof is straightforward by exploiting the singular value decomposition (SVD) to the\nmatrix representation of the bilinear form b(·,·). When r≤d, rank(B) = r. Consider an SVD of\nB: for D= (diag{σ1,··· ,σr}, 0) with σj ̸= 0, UQ,UV being orthonormal,\nB = UQDU⊤\nV , where UQ ∈Rr×r,D ∈Rr×d, and UV ∈Rd×d.\nHence, b(w,p) can be equivalent written as the following for µ:= µ(w) and λ:= λ(q)\nb(w,p) = b\n\n\nd∑\nj=1\nµvj (w)vj(·),\nr∑\nj=1\nλqj (p)qj(·)\n\n= λ⊤Bµ= (U⊤\nQλ)⊤D(U⊤\nV µ).\nBy the norm equivalence (51) and above,\nmax\nw∈Vh\n|b(w,p)|\n∥w∥V\n≥c−1\nV max\nµ∈Rd\n⏐⏐(U⊤\nQλ)⊤D(U⊤\nV µ)\n⏐⏐\n∥µ∥ = c−1\nV max\nµ∈Rd\n⏐⏐(U⊤\nQλ)⊤D(U⊤\nV µ)\n⏐⏐\nU⊤\nV µ\n .\nSince U⊤\nV is surjective, we choose a speciﬁc U⊤\nV µ∈Rd to pass the lower bound: let the ﬁrst rentries\nof U⊤\nV µbe U⊤\nQλ, we have\nc−1\nV max\nµ∈Rd\n⏐⏐(U⊤\nQλ)⊤D(U⊤\nV µ)\n⏐⏐\nU⊤\nV µ\n ≥c−1\nV\n⏐⏐(U⊤\nQλ)⊤diag{σ1,··· ,σr}(U⊤\nQλ)\n⏐⏐\nU⊤\nQλ\n\n≥c−1\nV min\nj=1,···,r\n|σj|\nU⊤\nQλ\n= c−1\nV min\nj=1,···,r\n|σj|∥λ∥≥ c−1\nV c−1\nQ min\nj=1,···,r\n|σj|∥p∥H.\n(52)\nRemark D.7 (Constants cV,cQ and the potential impact of softmax thereon). The norm equivalence\nconstants bridging the integral-based ∥·∥Hand the ℓ2-norm ∥·∥ depend on the topology of the\napproximation spaces.\nIf the basis functions are globally supported, such as the Fourier-type basis from the eigenfunctions\nof the self-adjoint operator, or the learned bases shown in Figure 13, orthonormal, and deﬁned on the\nsame discretization on the spacial domain, then it is easy to see that cV and cQ are approximately 1\ndue to the Parseval identity, minus the caveat of approximating an integral on a discrete grid.\nEven if the basis functions {vj(·)}and {qj(·)}for Vh and Qh are locally supported, such as the\nnodal basis in (D2) and (D4), the hm-weight in (44) will make cV and cQ be of O(hm/2), or the\ninverse square root of the sequence length1/√n= O(hm/2), see e.g., [98, Section 11]. Nevertheless,\nthe ﬁnal bound (50) will be sequence length-independent because now the minimum singular value of\nBwill scale as O(hm) the same with a mass matrix in the ﬁnite element method (see e.g., [29, Section\n4.4.2]). Consequently, these two constants depend on the number of explicit connections (not learned)\nthat a single position has in the discretization. In our examples, the Euclidean coordinate positional\nencodings yield a sparse connection (tri-diagonal in 1D, 5-point stencil in 2D) thus independent of n.\nIf a softmax normalization S(·), such as the one in [76], is applied on the key matrix in the sequence\nlength dimension, the norm equivalence constant cV is not n-independent anymore. To illustrate\nthis, without loss of generality, let V ⊂ H= L2(Ω), Ω = Ω ∗ = (0 ,1). By the deﬁnition of\nsoftmax, it is straightforward to verify that the test functions essentially change to ˜w := S(w) ≈(\nn\n∫\nΩ ewdx\n)−1\new. Following the argument of Lemma D.6, the lower bound can be proved for\n˜w, i.e., maxw∈Vh |b(w,p)|/∥˜w∥H ≥c∥p∥H. However, by an exponential Sobolev inequality [ 32,\nTheorem 7.21] if we further assume that w has a weak derivative with a bounded L1(Ω)-norm,\n∥w∥H ≤cΩn∥˜w∥H, thus passing the inequality to try to obtain an estimate like (50) yields an\ninevitable constant related to n.\n35"
}