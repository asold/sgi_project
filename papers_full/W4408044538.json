{
    "title": "Evaluation of Large Language Models in Tailoring Educational Content for Cancer Survivors and Their Caregivers: Quality Analysis",
    "url": "https://openalex.org/W4408044538",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2255505624",
            "name": "Darren Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096945033",
            "name": "Xiao Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2084263436",
            "name": "Canhua Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2597733565",
            "name": "Jinbing Bai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3119688920",
            "name": "Zahra A. Barandouzi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1975565157",
            "name": "Stephanie K Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4379349672",
            "name": "Caitlin Webster",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5015107579",
            "name": "La-Urshalar Brock",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101705295",
            "name": "Lindsay Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5093793101",
            "name": "Delgersuren Bold",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100602091",
            "name": "Yu-Fen Lin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3164322802",
        "https://openalex.org/W3159628728",
        "https://openalex.org/W3121958155",
        "https://openalex.org/W4295079418",
        "https://openalex.org/W4291597105",
        "https://openalex.org/W4387498243",
        "https://openalex.org/W2800803175",
        "https://openalex.org/W3094077142",
        "https://openalex.org/W3008821292",
        "https://openalex.org/W2605800802",
        "https://openalex.org/W2906348427",
        "https://openalex.org/W4378471472",
        "https://openalex.org/W3207721040",
        "https://openalex.org/W4221095374",
        "https://openalex.org/W2337007737",
        "https://openalex.org/W2412648145",
        "https://openalex.org/W4387211004",
        "https://openalex.org/W2951141580",
        "https://openalex.org/W31187365",
        "https://openalex.org/W2288089864",
        "https://openalex.org/W1949534580",
        "https://openalex.org/W4322718832",
        "https://openalex.org/W4398143508",
        "https://openalex.org/W4390023570",
        "https://openalex.org/W4367626167",
        "https://openalex.org/W4390889231",
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4386865118",
        "https://openalex.org/W4381469233",
        "https://openalex.org/W4387701515",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4387168090",
        "https://openalex.org/W4392310631",
        "https://openalex.org/W4385573087",
        "https://openalex.org/W4394845301",
        "https://openalex.org/W4385473539",
        "https://openalex.org/W2168949313",
        "https://openalex.org/W4392490371",
        "https://openalex.org/W2019898356",
        "https://openalex.org/W1994256246",
        "https://openalex.org/W4391971084",
        "https://openalex.org/W4404958200",
        "https://openalex.org/W2166031157",
        "https://openalex.org/W1594749578",
        "https://openalex.org/W1966341062",
        "https://openalex.org/W4388022708",
        "https://openalex.org/W3207161555",
        "https://openalex.org/W1947951941",
        "https://openalex.org/W4394948082",
        "https://openalex.org/W4393119004",
        "https://openalex.org/W2087388709",
        "https://openalex.org/W2789519695",
        "https://openalex.org/W3163726243",
        "https://openalex.org/W4386081793",
        "https://openalex.org/W4393299877",
        "https://openalex.org/W4322766882"
    ],
    "abstract": "Abstract Background Cancer survivors and their caregivers, particularly those from disadvantaged backgrounds with limited health literacy or racial and ethnic minorities facing language barriers, are at a disproportionately higher risk of experiencing symptom burdens from cancer and its treatments. Large language models (LLMs) offer a promising avenue for generating concise, linguistically appropriate, and accessible educational materials tailored to these populations. However, there is limited research evaluating how effectively LLMs perform in creating targeted content for individuals with diverse literacy and language needs. Objective This study aimed to evaluate the overall performance of LLMs in generating tailored educational content for cancer survivors and their caregivers with limited health literacy or language barriers, compare the performances of 3 Generative Pretrained Transformer (GPT) models (ie, GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo; OpenAI), and examine how different prompting approaches influence the quality of the generated content. Methods We selected 30 topics from national guidelines on cancer care and education. GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo were used to generate tailored content of up to 250 words at a 6th-grade reading level, with translations into Spanish and Chinese for each topic. Two distinct prompting approaches (textual and bulleted) were applied and evaluated. Nine oncology experts evaluated 360 generated responses based on predetermined criteria: word limit, reading level, and quality assessment (ie, clarity, accuracy, relevance, completeness, and comprehensibility). ANOVA (analysis of variance) or chi-square analyses were used to compare differences among the various GPT models and prompts. Results Overall, LLMs showed excellent performance in tailoring educational content, with 74.2% (267/360) adhering to the specified word limit and achieving an average quality assessment score of 8.933 out of 10. However, LLMs showed moderate performance in reading level, with 41.1% (148/360) of content failing to meet the sixth-grade reading level. LLMs demonstrated strong translation capabilities, achieving an accuracy of 96.7% (87/90) for Spanish and 81.1% (73/90) for Chinese translations. Common errors included imprecise scopes, inaccuracies in definitions, and content that lacked actionable recommendations. The more advanced GPT-4 family models showed better overall performance compared to GPT-3.5 Turbo. Prompting GPTs to produce bulleted-format content was likely to result in better educational content compared with textual-format content. Conclusions All 3 LLMs demonstrated high potential for delivering multilingual, concise, and low health literacy educational content for cancer survivors and caregivers who face limited literacy or language barriers. GPT-4 family models were notably more robust. While further refinement is required to ensure simpler reading levels and fully comprehensive information, these findings highlight LLMs as an emerging tool for bridging gaps in cancer education and advancing health equity. Future research should integrate expert feedback, additional prompt engineering strategies, and specialized training data to optimize content accuracy and accessibility.",
    "full_text": null
}