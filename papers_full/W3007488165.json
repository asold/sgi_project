{
  "title": "Molecule Attention Transformer",
  "url": "https://openalex.org/W3007488165",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Maziarka, {\\L}ukasz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286983795",
      "name": "Danel, Tomasz",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mucha, S{\\l}awomir",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Rataj, Krzysztof",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226501789",
      "name": "Tabor, Jacek",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jastrz\\k{e}bski, Stanis{\\l}aw",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W2952254971",
    "https://openalex.org/W2967158012",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W2972608805",
    "https://openalex.org/W2527189750",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964233714",
    "https://openalex.org/W2991265431",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W2735246657",
    "https://openalex.org/W2951936753",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2941783269",
    "https://openalex.org/W2135732933",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2789816271",
    "https://openalex.org/W2798416089",
    "https://openalex.org/W2766761250",
    "https://openalex.org/W2214665483",
    "https://openalex.org/W2754490690",
    "https://openalex.org/W2911484737",
    "https://openalex.org/W2895884529",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W2806547269",
    "https://openalex.org/W2901003004",
    "https://openalex.org/W2785720803",
    "https://openalex.org/W2794474109",
    "https://openalex.org/W2685808923",
    "https://openalex.org/W2963017945",
    "https://openalex.org/W3202562506",
    "https://openalex.org/W2279490987",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view.",
  "full_text": "Molecule Attention Transformer\nŁukasz Maziarka 1 2 Tomasz Danel1 2 Sławomir Mucha2 Krzysztof Rataj 1 Jacek Tabor2\nStanisław Jastrz˛ ebski3 4\nAbstract\nDesigning a single neural network architec-\nture that performs competitively across a range\nof molecule property prediction tasks remains\nlargely an open challenge, and its solution may un-\nlock a widespread use of deep learning in the drug\ndiscovery industry. To move towards this goal, we\npropose Molecule Attention Transformer (MAT).\nOur key innovation is to augment the attention\nmechanism in Transformer using inter-atomic dis-\ntances and the molecular graph structure. Exper-\niments show that MAT performs competitively\non a diverse set of molecular prediction tasks.\nMost importantly, with a simple self-supervised\npretraining, MAT requires tuning of only a few\nhyperparameter values to achieve state-of-the-art\nperformance on downstream tasks. Finally, we\nshow that attention weights learned by MAT are\ninterpretable from the chemical point of view.\n1. Introduction\nThe task of predicting properties of a molecule lies at the\ncenter of applications such as drug discovery or material\ndesign. In particular, estimated 85% drug candidates fail\nthe clinical trials in the United States after a long and costly\ndevelopment process (Wong et al., 2018). Potentially, many\nof these failures could have been avoided by having correctly\npredicted a clinically relevant property of a molecule such\nas its toxicity or bioactivity.\nFollowing the breakthroughs in image (Krizhevsky et al.,\n2012) and text classiﬁcation (Vaswani et al., 2017), deep\nneural networks (DNNs) are expected to revolutionize other\nﬁelds such as drug discovery or material design (Jr et al.,\n1Ardigen, Cracow, Poland.\n2Jagiellonian University, Cracow, Poland.\n3Molecule.one, Warsaw, Poland.\n4New York University, New York, USA.\nCorrespondence to:\nŁukasz Maziarka <lukasz.maziarka@ardigen.com>,\nStanisław Jastrz˛ ebski <staszek.jastrzebski@gmail.com>.\nPreprint. Work in progress.\n2019). However, on many molecular property prediction\ntasks DNNs are outperformed by shallow models such as\nsupport vector machine or random forest (Korotcov et al.,\n2017; Wu et al., 2018). On the other hand, while DNNs can\noutperform shallow models on some tasks, they tend to be\ndifﬁcult to train (Ishiguro et al., 2019; Hu et al., 2019), and\ncan require tuning of a large number of hyperparameters.\nWe also observe both issues on our benchmark (see Section\n4.2).\nMaking deep networks easier to train has been the central\nforce behind their widespread use. In particular, one of\nthe most important breakthroughs in deep learning was the\ndevelopment of initialization methods that allowed to train\neasily deep networks end-to-end (Goodfellow et al., 2016).\nIn a similar spirit, our aim is to develop a deep model that\nis simple to use out-of-the-box, and achieves strong perfor-\nmance on a wide range of tasks in the ﬁeld of molecule\nproperty prediction.\nIn this paper we propose the Molecule Attention Trans-\nformer (MAT). We adapt Transformer (Devlin et al., 2018)\nto chemical molecules by augmenting the self-attention\nwith inter-atomic distances and molecular graph structure.\nFigure 1 shows the architecture. We demonstrate that\nMAT, in contrast to other tested models, achieves strong\nperformance across a wide range of tasks (see Figure 2).\nNext, we show that self-supervised pre-training further im-\nproves performance, while drastically reducing the time\nneeded for hyperparameter tuning (see Table 3). In these\nexperiments we tuned only the learning rate, testing 7\ndifferent values. Finally, we ﬁnd that MAT has inter-\npretable attention weights. We share pretrained weights\nat https://github.com/gmum/MAT.\n2. Related work\nMolecule property prediction. Predicting properties of\na candidate molecule lies at the heart of many ﬁelds such\nas drug discovery and material design. Broadly speaking,\nthere are two main approaches to predicting molecular prop-\nerties. First, we can use our knowledge of the underlying\nphysics (Lipinski et al., 1997). However, despite recent\nadvances (Schütt et al., 2017), current approaches remain\nprohibitively costly to accurately predict many properties of\narXiv:2002.08264v1  [cs.LG]  19 Feb 2020\nMolecule Attention Transformer\nNH2\nS\nN\nEmbedding Layer\nLayer Norm\nMolecule Multi-Head\nSelf-Attention\nLayer Norm\nPosition-Wise\nFeed Forward × K\nLayer Norm\nGlobal Pooling\nFully-Connected Layer\nPrediction\n× N\nInput\nNH2\nS\nN\nNH2\nS\nN λg\nλd\nλa\nNH2\nS\nN\nself-attention\nadjacency\ndistance\nFigure 1.Molecule Attention Transformer architecture. We largely base our model on the Transformer encoder. In the ﬁrst layer we\nembed each atom using one-hot encoding and atomic features. The main innovation is the Molecule Multi-Head Self-Attention layer\nthat augments attention with distance and graph structure of the molecule. We implement this using a weighted (by λd, λg, and λa)\nelement-wise sum of the corresponding matrices.\ninterest such as bioactivity. The second approach is to use\nexisting data to train a predictive model (Haghighatlari &\nHachmann, 2019). Here the key issue is the lack of large\ndatasets. Even for the most popular drug targets, such as\n5-HT1A (a popular target for depression), only thousands\nof active compounds are known. Promising direction is\nusing hybrid approaches such as Wallach et al. (2015) or\napproaches leveraging domain knowledge and underlying\nphysics to impose a strong prior such as Feinberg et al.\n(2018).\nDeep learning for molecule property prediction. Deep\nlearning has become a valuable tool for modeling molecules.\nDuring the years, the community has progressed from us-\ning handcrafted representations to representing molecules\nas strings of symbols, and ﬁnally to the currently popular\napproaches based on molecular graphs.\nGraph convolutional networks in each subsequent layer\ngather information from adjacent nodes in the graph. In\nthis way after N convolution layers each node has informa-\ntion from its N-edges distant neighbors. Using the graph\nstructure improves performance in a range of molecule mod-\neling tasks (Wu et al., 2018). Some of the most recent works\nimplement more sophisticated generalization methods for\ngathering the neighbor data. Veliˇckovi´c et al. (2017); Shang\net al. (2018) propose to augment GCNs with an attention\nmechanism. Li et al. (2018) introduces a model that dynam-\nically learns neighbourhood function in the graph.\nIn parallel to these advances, using the three-dimensional\nstructure of the molecule is becoming increasingly popu-\nlar. Perhaps the most closely related models are 3D Graph\nConvolutional Neural Network (3DGCN), Message Passing\nNeural Network (MPNN), and Adaptive Graph Convolu-\ntional Network (AGCN) (Cho & Choi, 2018; Gilmer et al.,\n2017; Li et al., 2018). 3DGCN and MPNN integrate graph\nand distance information in a single model, which enables\nthem to achieve strong performance on tasks such as solu-\nbility prediction. In contrast to them, we additionally allow\nfor a ﬂexible neighbourhood based on self-attention.\nTransformer, originally developed for natural language pro-\ncessing (Vaswani et al., 2017), has been recently applied to\nretrosynthesis in Karpov et al. (2019). They represent com-\npounds as sentences using the SMILES notation (Weininger,\n1988). In contrast to them, we represent compounds as a list\nof atoms, and ensure that models understand the structure of\nMolecule Attention Transformer\nthe molecule by augmenting the self-attention mechanism\n(see Figure 1). Our ablation studies show it is a critical\ncomponent of the model.\nTo summarize, methods related to our model have been\nproposed in the literature. Our contribution is unifying\nthese ideas in a single model based on the state-of-the-art\nTransformer architecture that preserves strong performance\nacross many chemical tasks.\nHow easy is it to use deep learning for molecule prop-\nerty prediction? DNNs performance is not always com-\npetitive to methods such as support vector machine or ran-\ndom forest. MoleculeNet is a popular benchmark for meth-\nods for molecule property prediction (Wu et al., 2018) that\ndemonstrates this phenomenon. Similar results can be found\nin Withnall et al. (2019). We reproduce a similar issue on\nour benchmark. This makes using deep learning less appli-\ncable to molecule property prediction because in some cases\npractitioners might actually beneﬁt from using other meth-\nods. Another issue is that graph neural networks, which are\nthe most popular class of models for molecule property pre-\ndiction, can be difﬁcult to train. Ishiguro et al. (2019) show\nand try to address the problem that graph neural networks\ntend to underﬁt the training set. We also reproduce this issue\non our benchmark (see also App. C).\nThere has been a considerable interest in developing easier\nto use deep models for molecule property prediction. Goh\net al. (2017) pretrains a deep network that takes as an input\nan image of a molecule. Another studies highlight the need\nto augment feedforward (Mayr et al., 2018) and graph neural\nnetworks (Yang et al., 2019) with handcrafted representa-\ntions of molecules. Hu et al. (2019) proposes pretraining\nmethods for graph neural networks and shows this largely\nalleviates the problem of underﬁtting, present in these ar-\nchitectures (Ishiguro et al., 2019). We take inspiration from\nHu et al. (2019) and use one of the three pretraining tasks\nproposed therein.\nConcurrently, Wang et al. (2019); Honda et al. (2019) pre-\ntrain a vanilla Transformer (Devlin et al., 2018) that takes as\ninput a text representation (SMILES) of a molecule. Honda\net al. (2019) shows that decoding based approach improves\ndata efﬁciency of the model. A similar approach, special-\nized to the task of drug-target interaction prediction, was\nconcurrently proposed in Shin et al. (2019). In contrast to\nthem, we adapt Transformer to chemical structures, which\nin our opinion is crucial for achieving strong empirical per-\nformance. We also use a domain-speciﬁc pretraining based\non Wu et al. (2018). We further conﬁrm importance of both\napproaches by comparing directly with Honda et al. (2019).\nSelf-attention based models. Arguably, the attention\nmechanism (Bahdanau et al., 2014) has been one of the\nmost important breakthroughs in deep learning. This is per-\nhaps best illustrated by the wide-spread use of Transformer\narchitecture in natural language processing (Vaswani et al.,\n2017; Devlin et al., 2018).\nMultiple prior works have augmented self-attention in Trans-\nformer using domain-speciﬁc knowledge (Chen et al., 2018;\nShaw et al., 2018; Bello et al., 2019; Guo et al., 2019). Guo\net al. (2019) encourages Transformer to attend to adjacent\nwords in a sentence, and Chen et al. (2018) encourages an-\nother attention-based model to focus on pairs of words in a\nsentence that are connected in an external knowledge base.\nOur novelty is applying this successive modeling idea to\nmolecule property prediction.\n3. Molecule Attention Transformer\nAs the rich literature on deep learning for molecule property\nprediction suggests, it is necessary for a model to be ﬂex-\nible enough to represent a range of possible relationships\nbetween atoms of a compound. Inspired by its ﬂexibility\nand strong empirical performance, we base our model on\nthe Transformer encoder (Vaswani et al., 2017; Devlin et al.,\n2018). It is worth noting that natural language processing\nhas inspired important advances in cheminformatics (Segler\net al., 2017; Gómez-Bombarelli et al., 2018), which might\nbe due to similarities between the two domains (Jastrz˛ ebski\net al., 2016).\nTransfomer. We begin by brieﬂy introducing the Trans-\nformer architecture. On a high level, Transformer for classi-\nﬁcations has N attention blocks followed by a pooling and\na classiﬁcation layer. Each attention block is composed of a\nmulti-head self-attention layer, followed by a feed-forward\nblock that includes a residual connection and layer normal-\nization.\nThe multi-head self-attention is composed ofHheads. Head\ni(i= 1,...,H ) takes as input hidden stateHand computes\nﬁrst Qi = HWQ\ni , Ki = HWH\ni , and Vi = HWV\ni . These\nare used in the attention operation as follows:\nA(i) = ρ\n(QiKT\ni√dk\n)\nVi, (1)\nMolecule Self-Attention. Using a naive Transformer ar-\nchitecture would require encoding of chemical molecules\nas sentences. Instead, inspired by Battaglia et al. (2018),\nwe interpret the self-attention as a soft adjacency matrix\nbetween the elements of the input sequence. Following this\nline of thought, it is natural to augment the self-attention\nusing information about the actual structure of the model.\nThis allows us to avoid using linearized (textual) representa-\ntion of molecule as input (Jastrz˛ ebski et al., 2016), which\nwe expect to be a better inductive bias for the model.\nMolecule Attention Transformer\nMore concretely, we propose the Molecule Self-Attention\nlayer, which we describe in Equation 2. We augment the\nself-attention matrix as follows: let A ∈{0,1}Natoms×Natoms\ndenote the graph adjacency matrix, and D ∈RNatoms×Natoms .\ndenote the inter-atomic distances. Let λa, λd, and λg denote\nscalars weighting the self-attention, distance, and adjacency\nmatrices. We modify Equation 1 as follows:\nA(i) =\n(\nλaρ\n(QiKT\ni√dk\n)\n+ λdg(D) +λgA\n)\nVi, (2)\nsee also Figure 1. We denote λa, λd, and λg jointly as λ.\nWe use asgeither softmax (normalized over the rows), or an\nelement-wise g(d) = exp(−d). Finally, the distance matrix\nD is computed using RDKit package (Landrum, 2016).\nNote that while we use only the adjacency and the distance\nmatrices, MAT can be easily extended to include other types\nof information, e.g. forces between the atoms.\nMolecule Attention Transformer. To deﬁne the model,\nwe replace all self-attention layers in the original Trans-\nformer encoder by our Molecular Self Attention layers. We\nembed each atom as a 26 dimensional vector following (Co-\nley et al., 2017), shown in Table 1. In the experiments,\nwe treat λa, λd, and λg as hyperparameters and keep them\nfrozen during training. Figure 1 illustrates the model.\nPretraining. We experiment with one of the two node-\nlevel pretraining tasks proposed in Hu et al. (2019), which\ninvolves predicting the masked input nodes. Consistently\nwith Hu et al. (2019), we found it stabilizes learning (see\nFigure 6) and reduces the need for an extensive hyperparam-\neter search (see Table 3). Given that MAT already achieves\ngood performance using this simple pretraining task, we\nleave for future work exploring the other tasks proposed in\nHu et al. (2019).\nOther details. Inspired by Li et al. (2017); Clark et al.\n(2019), we add an artiﬁcial dummy node to the molecule.\nThe dummy node is not connected by an edge to any other\natom and the distance to any of them is set to 106. Our\nmotivation is to allow the model to skip searching for a\nmolecular pattern if none is to ﬁnd by putting higher atten-\ntion on that distant node, which is similar to how BERT uses\nthe separation token (Devlin et al., 2018; Clark et al., 2019).\nWe conﬁrm this intuition in Section 4.4 and Section 4.5.\nFinally, the distance matrices are calculated from 3D con-\nformers calculated using UFFO PTIMIZE MOLECULE func-\ntion from the RDKit package (Landrum, 2016), and the\ndefault parameters (MAX ITERS =200, VDW THRESH =10.0,\nCONF ID=−1, IGNORE INTERFRAG INTERACTIONS =True).\nFor each compound we use one pre-computed conformation.\nWe experimented with sampling more conformations for\neach compound, but did not observe a consistent boost in\nperformance, however it is possible that using more sophis-\nticated algorithms for compound 3D structure minimization\ncould improve the results. We leave this for future work.\nTable 1.Featurization used to embed atoms in MAT.\nINDICES DESCRIPTION\n0 − 11\nATOMIC IDENTITY AS A ONE -HOT VECTOR OF\nB, N, C, O, F, P, S, C L, BR, I, D UMMY, OTHER\n12 − 17\nNUMBER OF HEAVY NEIGHBORS AS ONE -HOT\nVECTOR OF 0, 1, 2, 3, 4, 5\n18 − 22\nNUMBER OF HYDROGEN ATOMS AS\nONE -HOT VECTOR OF 0, 1, 2, 3, 4\n23 FORMAL CHARGE\n24 IS IN A RING\n25 IS AROMATIC\n4. Experiments\nWe begin by comparing MAT to other popular models in the\nliterature on a wide range of tasks. We ﬁnd that with simple\npretraining MAT outperforms other methods, while using a\nsmall budget for hyperparameter tuning.\nIn the rest of this section we try to develop understanding\nof what makes MAT work well. In particular, we ﬁnd that\nindividual heads in the multi-headed self-attention layers\nlearn chemically interpretable functions.\n4.1. Experimental settings\nComparing different models for molecule property predic-\ntion is challenging. Despite considerable efforts, the com-\nmunity still lacks a standardized way to compare different\nmodels. In our work, we use a similar setting to Molecu-\nleNet (Wu et al., 2018).\nEvaluation. Following recommendations of Wu et al.\n(2018) and the experimental setup of Podlewska & Kafel\n(2018), we use random split for FreeSolv, ESOL, and Met-\nStab. For all the other datasets we use scaffold split, which\nassigns compounds that share the same molecular scaffold-\ning to different subsets of the data (Bemis & Murcko, 1996).\nIn regression tasks, the property value was standardized.\nTest performance is based on the model which gave best\nresults in the validation setting. Each training was repeated\n6 times, on different train/validation/test splits. All the other\nexperimental details are reported in the Supplement.\nDatasets. We run experiments on a wide range of datasets\nthat represent typical tasks encountered in molecule mod-\nMolecule Attention Transformer\neling. Below, we include a short description of these tasks,\nand a more detailed description is moved to App. A.\n• FreeSolv, ESOL. Regression tasks used in Wu et al.\n(2018) for predicting water solubility in terms of the\nhydration free energy (FreeSolv) and log solubility in\nmols per litre (ESOL). The datasets have642 and 1128\nmolecules, respectively.\n• Blood-brain barrier permeability (BBBP). Binary\nclassiﬁcation task used in Wu et al. (2018) for predict-\ning the ability of a molecule to penetrate the blood-\nbrain barrier. The dataset has 2039 molecules.\n• Estrogen Alpha, Estrogen Beta. The tasks are to pre-\ndict whether a compound is active towards a given\ntarget (Estrogen-α, Estrogen-β) based on experimen-\ntal data from the ChEMBL database (Gaulton et al.,\n2011). The datasets have 2398, and 1961 molecules,\nrespectively.\n• MetStabhigh, MetStablow. Binary classiﬁcation tasks\nbased on data from Podlewska & Kafel (2018) to pre-\ndict whether a compound has high (over 2.32 h half-\ntime) or low (lower than 0.6 h half-time) metabolic sta-\nbility. Both datasets contain the same 2127 molecules.\n4.2. Molecule Attention Transformer\nModels. Similarly to Wu et al. (2018), we test a com-\nprehensive set of baselines that span both shallow and\ndeep models. We compare MAT to the following base-\nlines: GCN (Duvenaud et al., 2015), Random Forest (RF)\nand Support Vector Machine with RBF kernel (SVM). We\nalso test the following recently proposed models: Edge\nAttention-based Multi-Relational Graph Convolutional Net-\nwork (EAGCN) (Shang et al., 2018), and Weave (Kearnes\net al., 2016).\nHyperparameter tuning. For each method we ex-\ntensively tune their hyperparameters using random\nsearch (Bergstra & Bengio, 2012). To ensure fair com-\nparison, each model is given the same budget for hyperpa-\nrameter search. We run two sets of experiments with budget\nof 150 and 500 evaluations. We include hyperparameter\nranges in App. B.\nResults. We evaluate models by their average rank accord-\ning to the test set performance on the 7 datasets. Figure 2\nreports ranks of all methods for the two considered hyperpa-\nrameter budgets (150 and 500). Additionally, we report in\nTable 2 detailed scores on all datasets. We make three main\nobservations.\nFirst, graph neural networks (GCN, Weave, EAGCN) on\naverage do not outperform the other models. The best graph\nmodel achieves average rank 3.28 compared to 3.14 by RF.\nOn the whole, performance of the deep models improves\nwith larger hyperparameter search budget. This further cor-\nroborates the original motivation of our study. Indeed, using\ncommon deep learning methods for molecule property pre-\ndiction is challenging in practice. It requires a large compu-\ntational budget, and might still result in poor performance.\nSecond, MAT outperforms the other tested methods in terms\nof the average rank. MAT achieves average rank of 2.71\nand 2.42 for 150 and 500 budgets, compared to 3.14 of RF,\nwhich is the second best performing model. This shows\nthat architecture MAT is ﬂexible enough and has the correct\ninductive bias to perform well on a wide range of tasks.\nExamining performance of MAT across individual datasets,\nwe observe that RF and SVM perform better on Estrogen-β,\nMetStablow, and MetStab high. Both RF and SVM use\nextended-connectivity ﬁngerprint (Rogers & Hahn, 2010)\nas input representation, which encodes substructures in the\nmolecule as features. Metabolic stability of a compound\ndepends on existence of particular moieties, which are rec-\nognized by enzymes. Therefore a simple structure-based ﬁn-\ngerprints perform well in such a setting. Wang et al. (2019);\nMayr et al. (2018) show that using ﬁngerprint as input repre-\nsentation improves performance of deep networks on related\ndatasets. These two observations suggest that MAT could\nbeneﬁt from using ﬁngerprints. Instead, we avoid using\nhandcrafted representations, and investigate pretraining as\nan alternative in the next section. Though ﬁngerprint-based\nmodels show excellent performance in all presented tasks,\nthere are datasets on which they fail to match the perfor-\nmance of graph approaches. We observed this also on an\nenergy prediction task (see the extension of our benchmark\nin App. C).\n4.3. Pretrained Molecule Attention Transformer\nSelf-supervised pretraining has revolutionized natural lan-\nguage processing (Devlin et al., 2018) and has improved\nperformance in molecule property prediction (Hu et al.,\n2019). We apply here node-level self-supervised pretraining\nfrom Hu et al. (2019) to MAT. The task is to predict features\nof masked out nodes. We refer the reader to App. D for\nmore details.\nModels. We compare MAT to the two following baselines.\nFirst, we apply the same pretraining to EAGCN, which we\nwill refer to as “Pretrained EAGCN”. Second, we compare\nto a concurrent work by Honda et al. (2019). They pretrain\na vanilla Transformer by decoding textual representation\n(SMILES) of molecules. We will refer to their method as\n“SMILES Transformer”.\nMolecule Attention Transformer\nSVM RF GC Weave EAGCN MAT\n1\n2\n3\n4\n5\n6Rank\n3.43\n3.14\n3.86 3.86\n4.29\n2.43\n(a) Hyperparameter search budget of 500 combinations.\nSVM RF GC Weave EAGCN MAT\n1\n2\n3\n4\n5\n6Rank 3.14 3.14\n4.0\n3.29\n4.71\n2.71 (b) Hyperparameter search budget of 150 combinations.\nFigure 2.The average rank across the 7 datasets in the benchmark. For each model we test 500 (left) or 150 (right) hyperparameter\ncombinations. We split the data using random or scaffold split (according to the dataset description) 6 times into train/validation/test folds\nand use the mean metrics across the test folds to obtain the ranklists of models. Interestingly, shallow models (RF and SVM) outperform\ngraph models (GCN, EAGCN and Weave).\nTable 2.Test performances in the benchmark. For each model we test 500 (top) and 150 (bottom) hyperparameter combinations. On\nESOL and FreeSolv we report RMSE (lower is better). The other tasks are evaluated using ROC AUC (higher is better). Experiments are\nrepeated 6 times.\n(a) Hyperparameter search budget of 500 combinations.\nBBBP ESOL F REE SOLV ESTROGEN -α ESTROGEN -β METSTABLOW METSTABHIGH\nSVM .707 ± .000 .478 ± .054 .461 ± .077 .973 ± .000 .778 ± .000 .893 ± .030 .890 ± .029\nRF .725 ± .006 .534 ± .073 .523 ± .097 .977 ± .001 .797 ± .007 .885 ± .029 .888 ± .030\nGCN .712 ± .010 .357 ± .032 .271 ± .048 .975 ± .003 .730 ± .006 .881 ± .031 .875 ± .036\nWEAVE .701 ± .016 .311 ± .023 .311 ± .072 .974 ± .003 .769 ± .023 .863 ± .028 .882 ± .043\nEAGCN .680 ± .014 .316 ± .024 .345 ± .051 .961 ± .011 .781 ± .012 .883 ± .024 .868 ± .034\nMAT (OURS ) .728 ± .008 .285 ± .022 .263 ± .046 .979 ± .003 .765 ± .007 .862 ± .038 .888 ± .027\n(b) Hyperparameter search budget of 150 combinations.\nBBBP ESOL F REE SOLV ESTROGEN -α ESTROGEN -β METSTABLOW METSTABHIGH\nSVM .723 ± .000 .479 ± .055 .461 ± .077 .973 ± .000 .772 ± .000 .893 ± .030 .890 ± .029\nRF .721 ± .003 .534 ± .073 .524 ± .098 .977 ± .001 .791 ± .012 .892 ± .026 .888 ± .030\nGCN .695 ± .013 .369 ± .032 .299 ± .068 .975 ± .003 .730 ± .006 .884 ± .033 .875 ± .036\nWEAVE .702 ± .009 .298 ± .025 .298 ± .049 .974 ± .003 .769 ± .023 .863 ± .028 .885 ± .042\nEAGCN .680 ± .014 .322 ± .052 .337 ± .042 .961 ± .011 .781 ± .012 .859 ± .024 .844 ± .037\nMAT (OURS ) .727 ± .006 .290 ± .019 .289 ± .047 .979 ± .003 .765 ± .007 .861 ± .029 .844 ± .052\nHyperparameters. For all methods that use pre-\ntraining we reduce the hyperparameter grid to a\nminimum. We tune only the learning rate in\n{1e−3,5e−4,1e−4,5e−5,1e−5,5e−6,1e−6}. We set\nthe other hyperparameters to reasonable defaults based on\nresults from Section 4.2. For MAT and EAGCN, we fol-\nlow (Devlin et al., 2018) and use the largest model that still\nﬁts the GPU memory. For SMILES Transformer we use\npretrained weights provided by Honda et al. (2019).\nResults. As in previous section, we compare the models\nbased on their average rank on our benchmark. Figure 3\nand Table 3 summarize the results.\nWe observe that Pretrained MAT achieves average rank of\n1.57 and outperforms MAT (average rank of2.14). Impor-\ntantly, for Pretrained MAT we only tuned the learning rate\nby evaluating 7 different values. This is in stark contrast to\nthe 500 hyperparameter combinations tested for MAT and\nEAGCN. To visualize this, in Figure 4 we plot the average\ntest performance of all models as a function of the number\nof tested hyperparameter combinations. We also note that\nMolecule Attention Transformer\nPretrained\nSMILES\nEAGCN Pretrained\nEAGCN\nMAT Pretrained\nMAT\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0Rank\n4.29\n3.0\n4.0\n2.14\n1.57\nFigure 3.The average ranks across the 7 datasets in the benchmark.\nPretrained MAT outperforms the other methods, despite a drasti-\ncally smaller number of tested hyperparameters (7) compared to\nMAT and EAGCN (500).\nPretrained MAT is more competitive on the three datasets\nmentioned in the previous section.\nWe also ﬁnd that Pretrained MAT outperforms the other two\npretrained methods. Pretraining degrades the performance\nof EAGCN (average rank of4.0), and SMILES Transformer\nachieves the worst average rank (average rank of4.29). This\nsuggests that both the architecture, and the choice of the\npretraining task are important for the overall performance\nof the model.\n4.4. Ablation studies\nTo better understand what contributes to the performance of\nMAT, we run a series of ablation studies on three representa-\ntive datasets from our benchmark. We leave understanding\nhow these choices interact with pretraining for future work.\nFor experiments in this section we generated additional\nsplits for ESOL, FreeSolv and BBBP datasets (different\nthan in Section 4.2). For each conﬁguration we select the\nbest hyperparameters settings using random search under\na budget of 100 evaluations. Experiments are repeated 3\ntimes.\nDummy node is not so dummy. MAT uses a dummy\nnode that is disconnected from other atoms in the graph (Li\net al., 2017). Our intuition is that such functionality can\nbe useful to automatically adapt capacity on small datasets.\nBy attending to the dummy node, the model can effectively\nchoose to avoid changing the internal representation in a\ngiven layer. To examine this architectural choice, in Table 4\nwe compare MAT to a variant that does not include the\ndummy node. Results show that dummy node improves\nperformance of the model.\nKnowing molecular graph and distances between atoms\nimproves performance. Our key architectural innovation\nis integrating the molecule graph and inter-atomic distances\nwith the self-attention layer in Transformer, as shown in\nFigure 1. To probe the importance of each of these sources\nof information, we removed them individually during train-\ning. Results in Table 5 suggest that keeping all sources of\ninformation results in the most stable performance across\nthe three tasks, which is our primary goal. We also show\nthat MAT can effectively use distance information in a toy\ntask involving 3-dimensional distances between functional\ngroups (see App.F).\nUsing a more complex featurization does not improve\nperformance. Many models for predicting molecule prop-\nerties use additional edge features (Coley et al., 2017; Shang\net al., 2018; Gilmer et al., 2017). In Table 6 we show that\nadding additional edge features does not improve MAT per-\nformance. This is certainly possible that a more compre-\nhensive set of edge features or a better method to integrate\nthem would improve performance, which we leave for fu-\nture work. Procedure of using edge features is described in\ndetail in App. E.\n4.5. Analysis.\nTo understand MAT better, we investigate attention weights\nof the model, and the effect of pretraining on the learning\ndynamics.\nWhat is MAT looking at? In natural language process-\ning, it has been shown that heads in Transformer seem to\nimplement interpretable functions (Htut et al., 2019; Clark\net al., 2019). Similarly, we investigate here the chemical\nfunction implemented by self-attention heads in MAT. We\nshow patterns found in the model that was pretrained with\nthe atom masking strategy (Hu et al., 2019), and then we\nverify our ﬁndings on a set of molecules extracted from the\nBBBP testing dataset.\nBased on a manual inspection of attention matrices of MAT,\nwe ﬁnd two broad patterns: (1) many attention heads are\nalmost fully focused on the dummy node, (2) many attention\nheads focus only on a few atoms. This seems consistent\nwith observations about Transformer in Clark et al. (2019).\nWe also notice that initial self-attention layers learn simple\nand easily interpretable chemical patterns, while subsequent\nlayers capture more complex arrangements of atoms. In Fig-\nure 5 we exemplify attention patterns on a random molecule\nfrom the BBBP dataset.\nTo quantify the above ﬁndings, we select six heads from the\nﬁrst layer that ﬁt the second category and seem to implement\nsix patterns: (i) focuses on 2-neighboured aromatic carbons\n(not substituted); (ii) focuses on sulfurs; (iii) focuses on non-\nMolecule Attention Transformer\nTable 3.Test set performances of methods that use pretraining. Experiments are repeated 6 times. SMILES refers to SMILES Transformer\nfrom Honda et al. (2019).\nBBBP ESOL F REE SOLV ESTROGEN -α ESTROGEN -β METSTABLOW METSTABHIGH\nMAT .737 ± .009 .278 ± .020 .265 ± .042 .998 ± .000 .773 ± .012 .862 ± .025 .884 ± .030\nEAGCN .687 ± .023 .323 ± .031 1.244 ± .341 .994 ± .002 .770 ± .010 .861 ± .029 .839 ± .038\nSMILES .717 ± .008 .356 ± .017 .393 ± .032 .953 ± .002 .757 ± .002 .860 ± .038 .881 ± .036\n100 101 102\nHyperparameters tested\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2Test RMSE\nRF\nSVM\nGCN\nWeave\nEAGCN\nMAT\nBest non-pretrained\nPretrained MAT\n(a) Regression tasks.\n100 101 102\nHyperparameters tested\n0.70\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87Test ROC AUC\nRF\nSVM\nGCN\nWeave\nEAGCN\nMAT\nBest non-pretrained\nPretrained MAT (b) Classiﬁcation tasks.\nFigure 4.Test performance of all models as a function of the number of tested hyperparameter combinations (on a logarithmic scale).\nFigures show the aggregated mean RMSE for regression tasks (left) and the aggregated mean ROC AUC for classiﬁcation tasks (right).\nPretrained MAT requires tuning an order of magnitude less hyperparameters, and performs competitively on both sets of tasks.\nTable 4.Test performance of MAT model variant without the\ndummy node (- DUMMY ) compared to performance of the original\nMAT.\nBBBP ESOL F REE SOLV\nMAT .723 ± .008 .286 ± .006 .250 ± .007\n- DUMMY .714 ± .010 .317 ± .014 .249 ± .014\nTable 5.Test performance of MAT with different sources of in-\nformation removed (equivalent to setting the corresponding λto\nzero).\nBBBP ESOL F REE SOLV\nMAT .723 ± .008 .286 ± .006 .250 ± .007\n- GRAPH .716 ± .009 .316 ± .036 .276 ± .034\n- DISTANCE .729 ± .013 .281 ± .001 .281 ± .013\n- ATTENTION .692 ± .001 .306 ± .026 .329 ± .014\nTable 6.Test performance of MAT using additional edge features\n(+ EDGES F .), compared to vanilla MAT.\nBBBP ESOL F REE SOLV\nMAT .723 ± .008 .286 ± .006 .250 ± .007\n+ EDGES F . .683 ± .008 .314 ± .014 .358 ± .023\nring nitrogens; (iv) focuses on oxygen in carbonyl groups;\n(v) focuses on 3-neighboured aromatic atoms (positions\nof aromatic ring substitutions) and on sulfur for different\natoms; (vi) focuses on nitrogens in aromatic rings. We found\nthat on the BBBP testing dataset the atoms corresponding to\nthese deﬁnitions (queried with SMARTS expressions) have\nindeed higher attention weights assigned to them than other\natoms. For each head, we calculated attention weights for\nall atoms in all molecules and compared those matching\nour hypothesis against the other atoms. Their distributions\ndiffer signiﬁcantly (p <0.001 in Kruskal-Wallis test) for\nall the patterns. The statistics and experimental details are\nsummarized in App. G.\nEffect of pretraining. Wu et al. (2018) observed that us-\ning pretraining stabilizes and speeds up training of graph\nconvolutional models. We observe a similar effect in our\ncase. Figure 6 reports training error of MAT and Pretrained\nMAT on the ESOL (left), and the FreeSolv (right) datasets.\nWe use the learning rate that achieved the best generalization\non each dataset in Sec. 4.3. The experiments are repeated 6\ntimes. On both datasets, Pretrained MAT converges faster\nand has a lower variance of training error across repetitions.\nMean standard deviation of training error for Pretrained\nMAT (MAT) is0.027 (0.057) and 0.040 (0.076) for ESOL\nMolecule Attention Transformer\n28 22 12\n28 23 19 1613 1\n20 10 3\n11 22\n19 28\n14 17 24\nHO\nO\nN\nNH\nS\nO\nO\nNH\nN\nO\n1\n2\n3\n109\n87\n611\n12 5 4\n1817\n16\n15 14\n20\n21\n19\n22\n2324\n25\n26 27\n28\n13\nFigure 5.The heatmaps show selected self-attention weights from\nthe ﬁrst layer of MAT, on a random molecule from the BBBP\ndataset (center). The atoms, which these heads focus on, are\nmarked with the same color as the corresponding matrix. The\ninterpretation of the presented patterns is described in the text.\nand FreeSolv, respectively.\n0 500 1000 1500 2000 2500 3000\nTrain steps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Training error\nPretrained MAT\nMAT\n(a) ESOL\n0 250 500 750 1000 1250 1500 1750\nTrain steps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Training error\nPretrained MAT\nMAT (b) FreeSolv\nFigure 6.Training of MAT with (blue) and without (orange) pre-\ntraining, on ESOL (left) and FreeSolv (right). Pretraining stabilizes\ntraining (smaller variance of the training error) and improves con-\nvergence speed.\n5. Conclusions.\nIn this work we propose Molecule Attention Transformer as\na versatile architecture for molecular property prediction. In\ncontrast to other tested models, MAT performs well across a\nwide range of molecule property prediction tasks. Moreover,\ninclusion of self-supervised pretraining further improves its\nperformance, and drastically reduces the need for tuning of\nhyperparameters.\nWe hope that our work will widen adoption of deep learn-\ning in applications involving molecular property prediction,\nas well as inspire new modeling approaches. One particu-\nlarly promising avenue for future work is exploring better\npretraining tasks for MAT.\nReferences\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate, 2014.\nBattaglia, P. W., Hamrick, J. B., Bapst, V ., Sanchez-\nGonzalez, A., Zambaldi, V . F., Malinowski, M., Tacchetti,\nA., Raposo, D., Santoro, A., Faulkner, R., Gülçehre, Ç.,\nSong, F., Ballard, A. J., Gilmer, J., Dahl, G. E., Vaswani,\nA., Allen, K., Nash, C., Langston, V ., Dyer, C., Heess,\nN., Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O.,\nLi, Y ., and Pascanu, R. Relational inductive biases, deep\nlearning, and graph networks. CoRR, abs/1806.01261,\n2018.\nBello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V .\nAttention augmented convolutional networks. CoRR,\nabs/1904.09925, 2019.\nBemis, G. W. and Murcko, M. A. The properties of known\ndrugs. 1. molecular frameworks. Journal of medicinal\nchemistry, 39(15):2887–2893, 1996.\nBergstra, J. and Bengio, Y . Random search for hyper-\nparameter optimization. J. Mach. Learn. Res. , 13(1):\n281–305, February 2012. ISSN 1532-4435.\nChen, G., Chen, P., Hsieh, C.-Y ., Lee, C.-K., Liao, B., Liao,\nR., Liu, W., Qiu, J., Sun, Q., Tang, J., et al. Alchemy: A\nquantum chemistry dataset for benchmarking ai models.\narXiv preprint arXiv:1906.09427, 2019.\nChen, Q., Zhu, X., Ling, Z.-H., Inkpen, D., and Wei, S.\nNeural natural language inference models enhanced with\nexternal knowledge. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pp. 2406–2417, Melbourne,\nAustralia, July 2018. Association for Computational Lin-\nguistics.\nCho, H. and Choi, I. S. Three-dimensionally embedded\ngraph convolutional network (3DGCN) for molecule in-\nterpretation. CoRR, abs/1811.09794, 2018.\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D.\nWhat does bert look at? an analysis of bert’s attention.\narXiv preprint arXiv:1906.04341, 2019.\nColey, C. W., Barzilay, R., Green, W. H., Jaakkola, T. S.,\nand Jensen, K. F. Convolutional embedding of attributed\nmolecular graphs for physical property prediction. Jour-\nnal of Chemical Information and Modeling, 57(8):1757–\n1772, 2017. doi: 10.1021/acs.jcim.6b00601. PMID:\n28696688.\nMolecule Attention Transformer\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova,\nK. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding, 2018. cite\narxiv:1810.04805Comment: 13 pages.\nDuvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bom-\nbarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P.\nConvolutional networks on graphs for learning molecular\nﬁngerprints. In Cortes, C., Lawrence, N. D., Lee, D. D.,\nSugiyama, M., and Garnett, R. (eds.), Advances in Neu-\nral Information Processing Systems 28, pp. 2224–2232.\nCurran Associates, Inc., 2015.\nFeinberg, E. N., Sur, D., Wu, Z., Husic, B. E., Mai, H.,\nLi, Y ., Sun, S., Yang, J., Ramsundar, B., and Pande,\nV . S. Potentialnet for molecular property prediction.ACS\nCentral Science, 4(11):1520–1530, 2018. doi: 10.1021/\nacscentsci.8b00507.\nGaulton, A., Bellis, L. J., Bento, A. P., Chambers, J., Davies,\nM., Hersey, A., Light, Y ., McGlinchey, S., Michalovich,\nD., Al-Lazikani, B., and Overington, J. P. ChEMBL: a\nlarge-scale bioactivity database for drug discovery. Nu-\ncleic Acids Research, 40(D1):D1100–D1107, 09 2011.\nISSN 0305-1048. doi: 10.1093/nar/gkr777.\nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and\nDahl, G. E. Neural message passing for quantum chem-\nistry. In Precup, D. and Teh, Y . W. (eds.),Proceedings of\nthe 34th International Conference on Machine Learning,\nvolume 70 ofProceedings of Machine Learning Research,\npp. 1263–1272, International Convention Centre, Sydney,\nAustralia, 06–11 Aug 2017. PMLR.\nGoh, G., Siegel, C., Vishnu, A., Hodas, N., and Baker,\nN. Chemception: A deep neural network with min-\nimal chemistry knowledge matches the performance\nof expert-developed qsar/qspr models. arXiv preprint\narXiv:1706.06689, 06 2017.\nGómez-Bombarelli, R., Wei, J. N., Duvenaud, D.,\nHernández-Lobato, J., Sánchez-Lengeling, B., Sheberla,\nD., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P.,\nand Aspuru-Guzik, A. Automatic chemical design us-\ning a data-driven continuous representation of molecules.\nACS Central Science , 4(2):268–276, 02 2018. doi:\n10.1021/acscentsci.7b00572.\nGoodfellow, I., Bengio, Y ., and Courville, A.Deep Learning.\nMIT Press, 2016.\nGuo, M., Zhang, Y ., and Liu, T. Gaussian transformer: a\nlightweight approach for natural language inference. In\nAAAI 2019, 2019.\nHaghighatlari, M. and Hachmann, J. Advances of machine\nlearning in molecular modeling and simulation. Cur-\nrent Opinion in Chemical Engineering, 23:51 – 57, 2019.\nISSN 2211-3398. doi: https://doi.org/10.1016/j.coche.\n2019.02.009. Frontiers of Chemical Engineering: Molec-\nular Modeling.\nHonda, S., Shi, S., and Ueda, H. R. Smiles transformer: Pre-\ntrained molecular ﬁngerprint for low data drug discovery,\n2019.\nHtut, P. M., Phang, J., Bordia, S., and Bowman, S. R. Do at-\ntention heads in bert track syntactic dependencies?, 2019.\nHu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande,\nV . S., and Leskovec, J. Pre-training graph neural networks.\nCoRR, abs/1905.12265, 2019.\nIshiguro, K., Maeda, S., and Koyama, M. Graph warp\nmodule: an auxiliary module for boosting the power of\ngraph neural networks. CoRR, abs/1902.01020, 2019.\nJastrz˛ ebski, S., Le´sniak, D., and Czarnecki, W. M. Learning\nto smile (s). arXiv preprint arXiv:1602.06289, 2016.\nJr, J. F. R., Florea, L., de Oliveira, M. C. F., Diamond,\nD., and Jr, O. N. O. A survey on big data and machine\nlearning for chemistry, 2019.\nKarpov, P., Godin, G., and Tetko, I. V . A transformer model\nfor retrosynthesis. In International Conference on Artiﬁ-\ncial Neural Networks, pp. 817–830. Springer, 2019.\nKearnes, S., McCloskey, K., Berndl, M., Pande, V ., and\nRiley, P. Molecular graph convolutions: Moving beyond\nﬁngerprints. Journal of Computer-Aided Molecular De-\nsign, 30, 03 2016. doi: 10.1007/s10822-016-9938-8.\nKim, S., Chen, J., Cheng, T., Gindulyte, A., He, J., He, S.,\nLi, Q., Shoemaker, B. A., Thiessen, P. A., Yu, B., et al.\nPubchem 2019 update: improved access to chemical data.\nNucleic acids research, 47(D1):D1102–D1109, 2018.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKorotcov, A., Tkachenko, V ., Russo, D. P., and Ekins, S.\nComparison of deep learning with multiple machine learn-\ning methods and metrics using diverse drug discovery\ndata sets. Molecular Pharmaceutics, 14(12):4462–4475,\n12 2017. doi: 10.1021/acs.molpharmaceut.7b00578.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nPereira, F., Burges, C. J. C., Bottou, L., and Weinberger,\nK. Q. (eds.), Advances in Neural Information Process-\ning Systems 25, pp. 1097–1105. Curran Associates, Inc.,\n2012.\nLandrum, G. Rdkit: Open-source cheminformatics software.\n2016.\nMolecule Attention Transformer\nLi, J., Cai, D., and He, X. Learning graph-level representa-\ntion for drug discovery. arXiv preprint arXiv:1709.03741,\n2017.\nLi, R., Wang, S., Zhu, F., and Huang, J. Adaptive graph\nconvolutional neural networks. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\nLipinski, C. A., Lombardo, F., Dominy, B. W., and Feeney,\nP. J. Experimental and computational approaches to\nestimate solubility and permeability in drug discovery\nand development settings. Advanced Drug Delivery Re-\nviews, 23(1):3 – 25, 1997. ISSN 0169-409X. doi: https:\n//doi.org/10.1016/S0169-409X(96)00423-1. In Vitro\nModels for Selection of Development Candidates.\nMayr, A., Klambauer, G., Unterthiner, T., Steijaert, M., Weg-\nner, J. K., Ceulemans, H., Clevert, D.-A., and Hochreiter,\nS. Large-scale comparison of machine learning meth-\nods for drug target prediction on chembl. Chem. Sci., 9:\n5441–5451, 2018. doi: 10.1039/C8SC00148K.\nPodlewska, S. and Kafel, R. Metstabon—online platform\nfor metabolic stability predictions. International journal\nof molecular sciences, 19(4):1040, 2018.\nRamsundar, B., Eastman, P., Walters, P., Pande, V ., Leswing,\nK., and Wu, Z. Deep Learning for the Life Sciences .\nO’Reilly Media, 2019.\nRogers, D. and Hahn, M. Extended-connectivity ﬁnger-\nprints. Journal of chemical information and modeling, 50\n(5):742–754, 2010.\nSchütt, K. T., Arbabzadah, F., Chmiela, S., Müller, K. R.,\nand Tkatchenko, A. Quantum-chemical insights from\ndeep tensor neural networks. Nature Communications, 8:\n13890, Jan 2017. doi: 10.1038/ncomms13890.\nSegler, M., Kogej, T., Tyrchan, C., and Waller, M. Gener-\nating focused molecule libraries for drug discovery with\nrecurrent neural networks. ACS Central Science, 4, 01\n2017. doi: 10.1021/acscentsci.7b00512.\nShang, C., Liu, Q., Chen, K.-S., Sun, J., Lu, J., Yi, J., and Bi,\nJ. Edge Attention-based Multi-Relational Graph Convo-\nlutional Networks. arXiv e-prints, art. arXiv:1802.04944,\nFeb 2018.\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with\nrelative position representations. In Proceedings of the\n2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pp. 464–\n468, New Orleans, Louisiana, June 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/N18-2074.\nShin, B., Park, S., Kang, K., and Ho, J. C. Self-attention\nbased molecule representation for predicting drug-target\ninteraction. In Doshi-Velez, F., Fackler, J., Jung, K.,\nKale, D., Ranganath, R., Wallace, B., and Wiens, J. (eds.),\nProceedings of the 4th Machine Learning for Health-\ncare Conference, volume 106 of Proceedings of Machine\nLearning Research, pp. 230–248, Ann Arbor, Michigan,\n09–10 Aug 2019. PMLR.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. CoRR, abs/1706.03762, 2017.\nVeliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Liò,\nP., and Bengio, Y . Graph Attention Networks. arXiv\ne-prints, art. arXiv:1710.10903, Oct 2017.\nWallach, I., Dzamba, M., and Heifets, A. Atomnet: A deep\nconvolutional neural network for bioactivity prediction in\nstructure-based drug discovery. ArXiv, abs/1510.02855,\n2015.\nWang, S., Guo, Y ., Wang, Y ., Sun, H., and Huang, J. Smiles-\nbert: Large scale unsupervised pre-training for molec-\nular property prediction. In Proceedings of the 10th\nACM International Conference on Bioinformatics, Com-\nputational Biology and Health Informatics , BCB ’19,\npp. 429–436, New York, NY , USA, 2019. Association\nfor Computing Machinery. ISBN 9781450366663. doi:\n10.1145/3307339.3342186.\nWeininger, D. Smiles, a chemical language and information\nsystem. 1. introduction to methodology and encoding\nrules. Journal of chemical information and computer\nsciences, 28(1):31–36, 1988.\nWithnall, M., Lindelöf, E., Engkvist, O., and Chen, H. Build-\ning attention and edge convolution neural networks for\nbioactivity and physical-chemical property prediction,\nSep 2019.\nWong, C. H., Siah, K. W., and Lo, A. W. Estimation of\nclinical trial success rates and related parameters. Bio-\nstatistics, 20(2):273–286, 01 2018. ISSN 1465-4644. doi:\n10.1093/biostatistics/kxx069.\nWu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Ge-\nniesse, C., Pappu, A. S., Leswing, K., and Pande, V .\nMoleculenet: a benchmark for molecular machine learn-\ning. Chem. Sci. , 9:513–530, 2018. doi: 10.1039/\nC7SC02664A.\nYang, K., Swanson, K., Jin, W., Coley, C., Eiden, P., Gao,\nH., Guzman-Perez, A., Hopper, T., Kelley, B., Mathea,\nM., et al. Analyzing learned molecular representations\nfor property prediction. Journal of chemical information\nand modeling, 59(8):3370–3388, 2019.\nMolecule Attention Transformer\nA. Dataset details.\nWe include below a more detailed description of the datasets\nused in our benchmark.\n• FreeSolv, ESOL. Regression tasks. Popular tasks for\npredicting water solubility in terms of the hydration\nfree energy (FreeSolv) and logS (ESOL). Solubility of\nmolecules is an important property that inﬂuences the\nbioavailability of drugs.\n• Blood-brain barrier permeability (BBBP). Binary\nclassiﬁcation task. The blood-brain barrier (BBB) sep-\narates the central nervous system from the bloodstream.\nPredicting BBB penetration is especially relevant in\ndrug design when the goal for the molecule is either to\nreach the central nervous system or the contrary – not\nto affect the brain.\n• MetStabhigh, MetStablow. Binary classiﬁcation tasks.\nThe metabolic stability of a compound is a measure\nof the half-life time of the compound within an or-\nganism. The compounds for this task were taken\nfrom (Podlewska & Kafel, 2018), where compounds\nwere divided into three sets: high, medium, and low\nstability. In this paper we concatenated these sets in\norder to build two classiﬁcation tasks: MetStab high\n(discriminating high against others) and MetStab low\n(discriminating low against others).\n• Estrogen Alpha, Estrogen Beta. Binary classiﬁca-\ntion tasks. Often in drug discovery, it is important that\na molecule is not potent towards a given target. Modu-\nlating of the estrogen receptors changes the genomic\nexpression throughout the body, which in turn may\nlead to the development of cancer. For these tasks, the\ncompounds with known activities towards the receptors\nwere extracted from ChEMBL (Gaulton et al., 2011)\ndatabase and divided into active and inactive sets based\non their reported inhibition constant (Ki), being <100\nnM and >1000 nM, respectively.\nB. Other experimental details\nIn this section we include details for hyperparameters and\ntraining settings used in Section 4.2.\nMolecule Atention Trainsformer. Table 7 shows hyper-\nparameter ranges used in experiments for MAT. A short\ndescription of these hyperparameters is listed below:\n• MODEL DIM – size of embedded atom features,\n• LAYERS NUMBER – number of encoder module repeats\n(N in Figure 1),\n• ATTENTION HEADS NUMBER – number of molecule\nself-attention heads,\n• PFF S NUMBER – number of dense layers in the\nposition-wise feed forward block (K in Figure 1),\n• λatt – self-attention weight λatt,\n• λdist – distance weight λd,\n• DISTANCE MATRIX KERNEL – function gused to trans-\nform the distance matrix D,\n• MODEL DROPOUT – dropout applied after the embed-\nding layer, position-wise feed forward layers, and resid-\nual layers (before sum operation),\n• WEIGHT DECAY – optimizer weight decay,\n• LEARNING RATE – (see Equation 3)\n• EPOCHS NUMBER – number of epochs for which the\nmodel is trained\n• BATCH SIZE – batch size used during the training of\nthe model\n• WARMUP FACTOR – fraction of epochs after which we\nend with increasing the learning rate linearly and begin\nwith decreasing it proportionally to the inverse square\nroot of the step number. (see Equation 3)\nTable 7.Molecule Attention Transformer hyperparameters ranges\nPARAMETERS\nBATCH SIZE 8, 16, 32, 64, 128\nLEARNING RATE .01, .005, .001, .0005, .0001\nEPOCHS 30, 100\nMODEL DIM 32, 64, 128, 256, 512, 1024\nLAYERS NUMBER 1, 2, 4, 6, 8\nATTENTION HEADS NUMBER 1, 2, 4, 8, 16\nPFF S NUMBER 1\nλatt 0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1\nλdistance 0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1\nDISTANCE MATRIX KERNEL ’SOFTMAX ’, ’EXP ’\nMODEL DROPOUT .0, .1, .2\nWEIGHT DECAY .0, .00001, .0001, .001, .01\nWARMUP FACTOR .0, .1, .2, .3, .4, .5\nAs suggested in (Vaswani et al., 2017), for optimization of\nMAT we used Adam optimizer (Kingma & Ba, 2014), with\nlearning rate scheduler given by the following formula:\nStepLR = optimizerfactor ·modeldim−0.5·\n·min\n(\nstepnum−0.5,stepnum ·warmupsteps−0.5)\n.\n(3)\nMolecule Attention Transformer\nWhere optimizer factor is given by 100 ·LEARNING RATE\nand warmup steps is given by WARMUP FACTOR ·total train\nsteps number.\nAfter N layers embedding of the molecule is calculated by\ntaking the mean of returned by the network vector repre-\nsentations of all atoms (Global pooling in Figure 1). Then\nit is passed to the single linear layer, which returns the\nprediction.\nSVM, RF, GCN, Weave. In our experiments, we used\nDeepChem (Ramsundar et al., 2019) implementation of\nbaseline algorithms (SVM, RF, GCN, Weave). We used\nthe same hyperparameters for tuning as were used in\nDeepChem, having regard to their proposed default values\n(we list them in Tables 8 - 11).\nRF and SVM work on the vector representation of molecule\ngiven by the Extended-connectivity ﬁngerprints (Rogers &\nHahn, 2010). ECFP vectors were calculated using class\nCIRCULAR FINGERPRINT from the DeepChem package,\nwith default parameters (RADIUS =2, SIZE =2048).\nTable 8.SVM hyperparameter ranges\nPARAMETERS\nC\n.25, .4375, .625, .8125, 1., 1.1875,\n1.375, 1.5625, 1.75, 1.9375, 2.125,\n2.3125, 2.5, 2.6875, 2.875, 3.0625,\n3.25, 3.4375, 3.625, 3.8125, 4.\nGAMMA\n.0125, .021875, .03125, .040625,\n.05, .059375, .06875, .078125, .0875,\n.096875, .10625, .115625,\n.125, .134375, .14375, .153125,\n.1625, .171875, .18125, .190625, .2\nTable 9.RF hyperparameter ranges\nPARAMETERS\nN ESTIMATORS\n125, 218, 312, 406, 500, 593,\n687, 781, 875, 968, 1062, 1156,\n1250, 1343, 1437, 1531, 1625,\n1718, 1812, 1906, 2000\nTable 10.GCN hyperparameter ranges\nPARAMETERS\nBATCH SIZE 64, 128, 256\nLEARNING RATE 0.002, 0.001, 0.0005\nN FILTERS 64, 128, 192, 256\nN FULLY CONNECTED NODES 128, 256, 512\nEAGCN Table 12 shows hyperparameter ranges used in\nexperiments for EAGCN. For EAGCN with weighted struc-\nTable 11.Weave hyperparameter ranges\nPARAMETERS\nBATCH SIZE 16, 32, 64, 128\nNB EPOCH 20, 40, 60, 80, 100\nLEARNING RATE 0.002, 0.001, 0.00075, 0.0005\nN GRAPH FEAT 32, 64, 96, 128, 256\nN PAIR FEAT 14\nture number of convolutional features n_sgc= n_sgc_1 +\nn_sgc_2 +n_sgc_3 +n_sgc_4 +n_sgc_5.\nTable 12.EAGCN hyperparameter ranges\nPARAMETERS\nBATCH SIZE 16, 32, 64, 128, 256, 512\nEAGCN STRUCTURE ’CONCATE ’, ’WEIGHTED ’\nNUM EPOCHS 30, 100\nLEARNING RATE .01, .005, .001, .0005, .0001\nDROPOUT .0, .1, .3\nWEIGHT DECAY .0, .001, .01, .0001\nN CONV LAYERS 1, 2, 4, 6\nN DENSE LAYERS 1, 2, 3, 4\nN SGC 1 30, 60\nN SGC 2 5, 10, 15, 20, 30\nN SGC 3 5, 10, 15, 20, 30\nN SGC 4 5, 10, 15, 20, 30\nN SGC 5 5, 10, 15, 20, 30\nDENSE DIM 16, 32, 64, 128\nC. Additional results for Sec. 4.2\nPredicting internal energy We run an additional exper-\niment on a regression task related to quantum mechanics.\nFrom the Alchemy dataset (Chen et al., 2019), which is\na dataset of 12 quantum properties calculated for 200K\nmolecules, we have chosen internal energy at 298.15 K to\nfurther test the performance of our model. We hypothesize\nthat our molecule self-attention should perform particularly\nwell in tasks involving atom level interactions such as energy\nprediction.\nTable 13 presents mean absolute errors of three methods:\none classical method (RF), one graph method (GCN), and\nour pretrained MAT. We use original train/valid/test splits\nof the dataset. For RF and GCN we run a random search\nwith budget of 500 hyperparameter sets. For pretrained\nMAT, we tune only the learning rate, that is selected from\n{1e−3,5e−4,1e−4,5e−5,1e−5,5e−6,1e−6}.\nMAT achieves a slightly lower error than GCN. As can\nbe expected, both graph methods can learn internal energy\nfunction correctly because of the locality preserved in the\ngraph structure. The classical method based on ﬁngerprints\ngives MAE that is almost two orders of magnitude higher\nthan MAE of the other methods in the comparison.\nMolecule Attention Transformer\nTable 13.Test results for internal energy prediction reported as\nMAE. All methods were tuned with a random search with budget\nof 500 hyperparameter combinations.\nU (INTERNAL ENERGY )\nRF .380\nGCN .006\nMAT .004\n2 4 6 8 10\nLayers number\n0.2\n0.3\n0.4\n0.5\n0.6Training error\nGCN\nMAT\n101\n102\n103\n104\nModel dim\n0.2\n0.4\n0.6\n0.8\n1.0Training error\nGCN\nMAT\nFigure 7.Training loss of MAT and GCN as a function of the\nnumber of layers (left) and model dimensionality (right).\nTraining error for graph-based neural networks Ishig-\nuro et al. (2019) show that graph neural networks suffer\nfrom underﬁtting of the training set and their performance\ndoes not scale well with the complexity of the network. We\nreproduce their experiments and conﬁrm that this problem is\nindeed present for both GCN and MAT. According to Figure\n7, the training loss of GCN and MAT ﬂattens at some point\nand stops decreasing even if we keep increasing the number\nof layers and model dimensionality. Despite this issue, for\nalmost all settings, MAT achieves lower training error than\nGCN.\nD. Additional details for Sec. 4.3\nTask description. As a node-level pretraining task we\nchose masking from (Hu et al., 2019) which is a version of\nBERT masked language model adapted to graph structured\ndata. The idea is that predicting masked nodes based on their\nneighbourhood will encourage model to capture domain\nspeciﬁc relationships between atoms.\nFor each molecular graph we randomly replace 15% of\ninput nodes (atom attributes) with special mask token. After\nforward pass we apply linear model to corresponding node\nembeddings to predict masked node attributes. In case of\nEAGCN we additionally mask attributes of edges connected\nto masked nodes to prevent model from learning simple\nvalue copying.\nPretraining setting. Training dataset consisted of 2 mln\nmolecules sampled from the ZINC15 database. Models\nwere trained for 8 epochs with learning rate set to 0.001 and\nbatch size 256. MAT was optimized with Noam optimizer\n(described in App. B), whereas for EAGCN we used Adam\n(Kingma & Ba, 2014). In both cases procedure minimized\nbinary cross entropy loss.\nFine-tuning setting. All our pretrained models are ﬁne-\ntuned on the target tasks for 100 epochs, with batch size\nequal to 32 and learning rate selected from the set of\n{1e−3,5e−4,1e−4,5e−5,1e−5,5e−6,1e−6}.\nIn Estrogen Alpha experiments we excluded three molecules\n(with the highest number of atoms) from the dataset, due to\nthe memory issues.\nTable 14.Pretrained MAT hyperparameters\nPARAMETERS\nMODEL DIM 1024\nLAYERS NUMBER 8\nATTENTION HEADS NUMBER 16\nPFF S NUMBER 1\nλatt .33\nλdistance .33\nDISTANCE MATRIX KERNEL ’EXP ’\nMODEL DROPOUT .0\nWEIGHT DECAY .0\nTable 15.Pretrained EAGCN hyperparameters\nPARAMETERS\nEAGCN STRUCTURE ’WEIGHTED ’\nDROPOUT .0\nWEIGHT DECAY .0\nN CONV LAYERS 8\nN DENSE LAYERS 1\nN SGC 1080\nSMILES Transformer. We used pretrained weights of\nSMILES-Transformers conducted by Honda et al. (2019).\nIn this setting, according to the authors, we used MLP\nwith 1 hidden layer, with 100 hidden units, that works\non the 1024-dimensional molecule embedding returned\nby the pretrained transformer. We trained this MLP\non the target tasks for 100 epochs, with batch size\nequal to 32 and learning rate selected from the set of\n{1e−3,5e−4,1e−4,5e−5,1e−5,5e−6,1e−6}.\nE. Additional results for Sec. 4.4\nEdge features. Every bond in the molecule was embed-\nded by the vector of edge features (we used features sim-\nilar to described in (Shang et al., 2018)). Every edge fea-\nture was then passed through linear layer, followed by\nReLU activation, which returned one single value for every\nMolecule Attention Transformer\nsingle edge (if there is no edge between atoms, we pass\nzero vector through the layer). This results in the matrix\nE ∈RNatoms×Natoms which was then used in Molecule Self-\nAttention layer, instead of the adjacency matrix.\nTable 16.Edge Features used for experiments form Table 6\nATTRIBUTE DESCRIPTION\nBOND ORDER VALUES FROM SET { 1, 1.5, 2, 3 }\nAROMATICITY IS AROMATIC\nCONJUGATION IS CONJUGATED\nRING STATUS IS IN A RING\nF. Toy task\nTask description. The essential feature of Molecule At-\ntention Transformer is that it augments the self-attention\nmodule using molecule structure. Here we investigate MAT\non a task heavily reliant on distances between atoms; we\nare primarily interested in how the performance of MAT\ndepends on λa, λd, λg that are used to weight the adjacency\nand the distance matrices in Equation 2.\nNaturally, many properties of molecules depend on their\ngeometry. For instance, steric effect happens when a spatial\nproximity of a given group, blocks reaction from happen-\ning, due to an overlap in electronic groups. However, this\ntype of reasoning can be difﬁcult to learn based only on the\ngraph information, as it does not always reﬂect the geom-\netry well. Furthermore, focusing on distance information\nmight require selecting low values for either λg or λa (see\nFigure 1).\nTo illustrate this, we designed a toy task to predict whether\nor not two substructures are closer to each other in space\nthan a predeﬁned threshold; see also Figure 8a. We expect\nthat MAT will work signiﬁcantly better than a vanilla graph\nconvolutional network if λd is tuned well.\nExperimental setting. We construct the dataset by sam-\npling 2677 molecules from PubChem (Kim et al., 2018),\nand use 20 Å threshold between -NH 2 fragment and tert-\nbutyl group to determine the binary label. The threshold\nwas selected so that positive and negative examples are well\nbalanced.\nResults. First, we plot Molecule Attention Transformer\nperformance as a function of λd in Figure 8b for three set-\ntings of λ: λa = 0(blue), λa = λg (orange), and λg = 0\n(green). In all cases we ﬁnd that using distance information\nimproves the performance signiﬁcantly. Additionally, we\nfound that GCN achieves 0.93 AUC on this task, compared\nto 0.98 by MAT withλd = 1.0. These results both motivate\ntuning λ, and show that MAT can efﬁciently use distance\ninformation if it is important for the task at hand.\nt-butyl\namine\n(a) The toy task is to predict whether two substructures (-NH 2\nfragment and tert-butyl group) co-occur within given distance.\n0.0 0.2 0.4 0.6 0.8 1.0\n distance\n0.75\n0.80\n0.85\n0.90\n0.95AUC\na=0\na = g\ng=0\n(b) Molecule Attention Transformer performance on the toy task\nas a function of λd, for different settings of λg and λa.\nFigure 8.MAT can efﬁciently use the inter-atomic distances to\nsolve the toy task (see left). Additionally, the performance is\nheavily dependent on λd, which motivates tuning λin the main\nexperiments (see right).\nFurther details. The molecules in the toy task\ndataset were downloaded from PubChem. The\nSMARTS query used to ﬁnd the compounds was\n(C([C;H3])([C;H3])([C;H3]).[NX3H2]). All\nmolecules were then ﬁltered so that only those with ex-\nactly one tert-butyl group and one -NH2 fragment were left.\nFor each of them, ﬁve conformers were created with RDKit\nimplementation of the Universal Force Field (UFF).\nThe task is a binary classiﬁcation of the distance between\ntwo molecule fragments. If the euclidean distance between\n-NH2 fragment and tert-butyl group is greater than a given\nthreshold, the label is 1 (0 otherwise). As the distance\nwe mean the distance between the closest heavy atoms in\nthese two fragments across calculated conformers. We used\n20 Å as the threshold as it leads to a balanced dataset.\nThere are 2677 compounds total from which 1140 are in a\npositive class. The dataset was randomly split into training,\nvalidation, and test datasets.\nIn experiments the hyperparameters that yielded promising\nresults on our datasets were used (listed in Table 17). The\nvalues of λ parameters were tuned, and their scores are\nshown in Figure 8b. All three λparameters (λd, λg, λa)\nsum to 1 in all experiments.\nMolecule Attention Transformer\nTo compare our results with a standard graph convolutional\nneural network, we run a grid search over hyperparameters\nshown in Table 18. The hyperparameters for which the best\nvalidation AUC score was reached are emboldened, and\ntheir test AUC score is 0.925 ±0.006.\nTable 17.MAT hyperparameters used.\nPARAMETERS\nBATCH SIZE 16\nLEARNING RATE 0.0005\nEPOCHS 100\nMODEL DIM 64\nMODEL N 4\nMODEL H 8\nMODEL N DENSE 2\nMODEL DENSE OUTPUT NONLINEARITY ’TANH ’\nDISTANCE MATRIX KERNEL ’SOFTMAX ’\nMODEL DROPOUT 0.0\nWEIGHT DECAY 0.001\nOPTIMIZER ’ADAM _ANNEAL ’\nAGGREGATION TYPE ’MEAN ’\nTable 18.Hyperparameters used for tuning GCN.\nPARAMETERS\nBATCH SIZE 16, 32, 64\nLEARNING RATE 0.0005\nEPOCHS 20, 40, 60, 80, 100\nN FILTERS 64, 128\nN FULLY CONNECTED NODES 128, 256\nG. Interpretability analysis\nTable 19.Statistics of the six attention head patterns described in\nthe text. Each head function is deﬁned by a SMARTS that selects\natoms with high attention weights. For each atom in the dataset\nwe calculated mean weight assigned to them by the corresponding\nattention head (average column value of the attention matrix).\nCalculated means and standard deviations show the difference\nbetween attention weights of matching atoms (µ+, σ+) against the\nother atoms (µ−, σ−).\nHEAD I II III IV V VI\nSMARTS [ C;D2] [S, S] [N;R0] O=* [ A;D3] N\nµ+ .136 .330 .061 .095 .043 .228\nσ+ .080 .280 .074 .120 .032 .171\nµ− .008 .001 .002 .006 .006 .005\nσ− .032 .003 .016 .034 .014 .009\nWe found several patterns in the self-attention heads by\nlooking at the ﬁrst layer of MAT. These patterns correspond\nto chemical structures that can be found in molecules. For\neach such pattern found in a qualitative manner, we tested\nquantitatively if our hypotheses are true about what these\nparticular attention heads represent.\nFor each pattern found in one of the attention heads, we con-\nstruct a SMARTS expression describing atoms that belong\nto our hypothetical molecular structures. Then, all atoms\nmatching the pattern are extracted from the BBBP dataset,\nand their mean attention weights (average column value of\nthe attention matrix) are compared against atoms that do not\nmatch the pattern. Table 19 shows the distributions of atten-\ntion weights for matching and not matching atoms. Atoms\nwhich match the SMARTS expression have signiﬁcantly\nhigher attention weights (µ+ >µ−).",
  "topic": "Hyperparameter",
  "concepts": [
    {
      "name": "Hyperparameter",
      "score": 0.7718982696533203
    },
    {
      "name": "Transformer",
      "score": 0.6854578256607056
    },
    {
      "name": "Computer science",
      "score": 0.6569869518280029
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5256384611129761
    },
    {
      "name": "Machine learning",
      "score": 0.4594835042953491
    },
    {
      "name": "Architecture",
      "score": 0.4464932978153229
    },
    {
      "name": "Graph",
      "score": 0.441774845123291
    },
    {
      "name": "Theoretical computer science",
      "score": 0.25073307752609253
    },
    {
      "name": "Engineering",
      "score": 0.143121600151062
    },
    {
      "name": "Voltage",
      "score": 0.10682639479637146
    },
    {
      "name": "Electrical engineering",
      "score": 0.07901573181152344
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": []
}