{
  "title": "Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages",
  "url": "https://openalex.org/W3213418658",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2993340626",
      "name": "Kelechi Ogueji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099481673",
      "name": "Yuxin Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163619555",
      "name": "Jimmy Lin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2978357053",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W4287997482",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3103147437",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W3147548102",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2992788665",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3013306929",
    "https://openalex.org/W3105005398",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3098998028",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W3113488190",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3134606166",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W273093436",
    "https://openalex.org/W2949973181",
    "https://openalex.org/W3098824823"
  ],
  "abstract": "Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. However, these models are known to require a lot of training data. This consequently leaves out a huge percentage of the world’s languages as they are under-resourced. Furthermore, a major motivation behind these models is that lower-resource languages benefit from joint training with higher-resource languages. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall. Results suggest that our “small data” approach based on similar languages may sometimes work better than joint training on large datasets with high-resource languages. Code, data and models are released at https://github.com/keleog/afriberta.",
  "full_text": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 116–126\nNovember 11, 2021. ©2021 Association for Computational Linguistics\n116\nSmall Data? No Problem! Exploring the Viability of Pretrained\nMultilingual Language Models for Low-Resource Languages\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n{kelechi.ogueji, yuxin.zhu, jimmylin}@uwaterloo.ca\nAbstract\nPretrained multilingual language models have\nbeen shown to work well on many languages\nfor a variety of downstream NLP tasks. How-\never, these models are known to require a lot\nof training data. This consequently leaves out\na huge percentage of the world’s languages\nas they are under-resourced. Furthermore, a\nmajor motivation behind these models is that\nlower-resource languages beneﬁt from joint\ntraining with higher-resource languages. In\nthis work, we challenge this assumption and\npresent the ﬁrst attempt at training a multilin-\ngual language model on only low-resource lan-\nguages. We show that it is possible to train\ncompetitive multilingual language models on\nless than 1 GB of text. Our model, named\nAfriBERTa, covers 11 African languages, in-\ncluding the ﬁrst language model for 4 of\nthese languages. Evaluations on named en-\ntity recognition and text classiﬁcation span-\nning 10 languages show that our model out-\nperforms mBERT and XLM-R in several lan-\nguages and is very competitive overall. Re-\nsults suggest that our “small data” approach\nbased on similar languages may sometimes\nwork better than joint training on large datasets\nwith high-resource languages. Code, data and\nmodels are released at https://github.\ncom/keleog/afriberta.\n1 Introduction\nPretrained language models have risen to the fore\nof natural language processing (NLP), achieving\nimpressive performance on a variety of NLP tasks.\nThe multilingual version of these models such as\nXLM-R (Conneau et al., 2020) and mBERT (De-\nvlin et al., 2019) have also been shown to generalize\nwell to many languages. However, these models\nare known to require a lot of training data, which\nis often absent for low-resource languages. Also,\nhigh-resource languages usually make up a signiﬁ-\ncant part of the training data, as it is hypothesized\nthat they help boost transfer to lower-resource lan-\nguages. Hence, there has been no known attempt\nto investigate if it is possible to pretrain multilin-\ngual language models solely on low-resource lan-\nguages without any transfer from higher-resource\nlanguages, despite the numerous beneﬁts that this\ncould provide. Motivated by this gap in the litera-\nture, the goal of our work is to explore the viability\nof multilingual language models pretrained from\nscratch on low-resource languages and to under-\nstand how to pretrain such models in this setting.\nWe introduce AfriBERTa, a transformer-based\nmultilingual language models trained on 11 African\nlanguages, all of which are low-resource. 1 We\nevaluate this model on named entity recognition\n(NER) and text classiﬁcation downstream tasks on\n10 low-resource languages. Our models outperform\nlarger models like mBERT andXLM-R by up to 10\nF1 points on text classiﬁcation, and also outperform\nthese models on several languages in the NER task.\nAcross all languages, we obtain very competitive\nperformance to these larger models. In summary,\nour contributions are as follows:\n1. We show that competitive multilingual language\nmodels can be pretrained from scratch solely\non low-resource languages without any high-\nresource transfer.\n2. We show that it is possible to pretrain these mod-\nels on less than 1 GB of text data and highlight\nthe many practical beneﬁts of this.\n3. Our extensive experiments highlight important\nfactors to consider when pretraining multilin-\ngual language models in low-resource settings.\n4. We introduce language models for 4 languages,\nimproving the representation of low-resource\nlanguages in modern NLP tools.\n1One of the languages (Gahuza) is counted twice because it\nis a code-mixed language consisting of Kinyarwanda and\nKirundi.\n117\nOur results show that, for the ﬁrst time, it is possi-\nble to pretrain a multilingual language model from\nscratch on only low-resource languages and obtain\ngood performance on downstream tasks.\n2 Related Work\nIn recent years, unsupervised learning of text rep-\nresentations has signiﬁcantly advanced natural lan-\nguage processing tasks. Static representations\nfrom pretrained word embeddings (Mikolov et al.,\n2013; Pennington et al., 2014) were improved upon\nby learning contextualized representations (Peters\net al., 2018). This has been noticeably improved\nfurther by pretraining language models (Radford\net al., 2018; Devlin et al., 2019) based on transform-\ners (Vaswani et al., 2017). These models have also\nbeen extended to the multilingual setting where\na single language model is pretrained on several\nlanguages without any explicit cross-lingual super-\nvision (Conneau et al., 2020; Devlin et al., 2019).\nHowever, much of this progress has been fo-\ncused on languages with relatively large amounts\nof data, commonly referred to as high-resource lan-\nguages. There has especially been very little focus\non African languages, despite the over 2000 lan-\nguages spoken on the continent making up 30.1%\nof all living languages (Eberhard et al., 2019). This\nis further visible in NLP publications on these lan-\nguages. In all the Association for Computational\nLinguistics (ACL) conferences hosted in 2019, only\n0.19% author afﬁliations were located in Africa\n(Caines, 2019). Other works (Joshi et al., 2020)\nhave also noted the great disparity in the cover-\nage of languages by NLP technologies. They note\nthat over 90% of the world’s 7000+ languages are\nunder-studied by the NLP community.\nThere have been a few works on learning pre-\ntrained embeddings for African languages, al-\nthough many of them have been static and trained\non a speciﬁc language (Ezeani et al., 2018; Ogueji\nand Ahia, 2019; Alabi et al., 2019; Dossou and\nSabry, 2021). More recently, Azunre et al. (2021)\ntrained a BERT model on the Twi language. How-\never, they note that their model is biased to the\nreligious domain because much of their data comes\nfrom that domain.\nWhile some African languages have been in-\ncluded in multilingual language models, this cov-\nerage only scratches the surface of the number of\nspoken African languages. Furthermore, the lan-\nguages always make up a minuscule percentage\nof the training set. For instance, amongst the 104\nlanguages that mBERT was pretrained on, only 3\nare African.2 In XLM-R, there are only 8 African\nlanguages out of the 100 languages. In terms of\ndataset size, the story is the same. African lan-\nguages make up 4.80 GB out of about 2395 GB\nthat XLM-R was pretrained on, representing just\n0.2% of the entire dataset (Conneau et al., 2020). In\nmBERT, African languages make up just 0.24 GB\nout of the approximately 100 GB that the model\nwas pretrained on. All of this call for an obvious\nneed for increased representation of African lan-\nguages in modern NLP tools for the over 1.3 billion\nspeakers on the continent.3\nPretrained language models have been shown to\nperform well when there is a lot of data (Liu et al.,\n2019; Conneau et al., 2020), but some works have\nfocused on using relatively smaller amounts of data.\nMartin et al. (2020) showed that it is possible to\nobtain state-of-the-art result with a French BERT\nmodel pretrained on small-scale diverse data. In\nanother work, Micheli et al. (2020) showed that\ntraining a French BERT language model on 100\nMB of data yields similar performance on question\nanswering as models pretrained on larger datasets.\nFurthermore, Ortiz Suárez et al. (2020) obtained\nstate-of-the-art performance with ELMo (Peters\net al., 2018) language models pretrained on less\nthan 1 GB of Wikipedia text, and Zhang et al.\n(2020) show that RoBERTa language models (Liu\net al., 2019) trained on 10 to 100 million tokens\ncan encode most syntactic and semantic features in\nits learned text representations.\nA common theme among these works is their\nfocus on monolingual language models. While it is\npossible to learn monolingual language models on\nsmaller amounts of data, it remains to be seen if it\nis possible in the multilingual case. Our work is the\nﬁrst, to the best of our knowledge, that focuses on\npretraining a multilingual language model solely on\nlow-resource languages without any transfer from\nhigher-resource languages.\n3 Methodology\n3.1 Data\nLanguages: We focus on 11 African languages,\nnamely Afaan Oromoo (also called Oromo),\n2https://github.com/google-research/bert/\nblob/master/multilingual.md\n3https://www.worldometers.info/\nworld-population/africa-population/\n(accessed on February 19, 2021)\n118\nLanguage Family Speakers Region\nAfaan Oromoo Afro-Asiatic 50M East\nAmharic Afro-Asiatic 26M East\nGahuza Niger-Congo 21M East\nHausa Afro-Asiatic 63M West\nIgbo Niger-Congo 27M West\nNigerian Pidgin English Creole 75M West\nSomali Afro-Asiatic 19M East\nSwahili Niger-Congo 98M Central/East\nTigrinya Afro-Asiatic 7M East\nYorùbá Niger-Congo 42M West\nTable 1: Language Information: For each language,\nits family, number of speakers (Eberhard et al., 2019),\nand regions in Africa spoken.\nLanguage # Sent. # Tok. Size (GB)\nAfaan Oromoo 410,840 6,870,959 0.051\nAmharic 525,024 1,303,086 0.213\nGahuza 131,952 3,669,538 0.026\nHausa 1,282,996 27,889,299 0.150\nIgbo 337,081 6,853,500 0.042\nNigerian Pidgin 161,842 8,709,498 0.048\nSomali 995,043 27,332,348 0.170\nSwahili 1,442,911 30,053,834 0.185\nTigrinya 12,075 280,397 0.027\nYorùbá 149,147 4,385,797 0.027\nTotal 5,448,911 108,800,600 0.939\nTable 2: Dataset Size: Size of each language in the\ndataset covering numbers of sentences, tokens and un-\ncompressed disk size.\nAmharic, Gahuza (a code-mixed language contain-\ning Kinyarwanda and Kirundi), Hausa, Igbo, Nige-\nrian Pidgin, Somali, Swahili, Tigrinya and Yorùbá.\nThese languages all come from three language fam-\nilies: Niger-Congo, Afro Asiatic and English Cre-\nole. We select these languages because they are\nthe languages supported by the British Broadcast-\ning Corporation (BBC) News, which was our main\nsource of data. 4 We also obtain additional data\nfrom the Common Crawl Corpus (Conneau et al.,\n2020; Wenzek et al., 2020) for languages avail-\nable there, speciﬁcally Amharic, Afaan Oromoo,\nAmharic, Hausa, Igbo, Somali and Swahili. Ta-\nble 1 provides details about the languages used in\npretraining our models.\nSize: The total size of our pretraining corpus is\n0.94 GB (108.8 million tokens). In comparison,\nXLM-R was pretrained on about 2395 GB (164.0\nbillion tokens) (Conneau et al., 2020), and mBERT\nwas trained on roughly 100 GB (12.8 billion to-\n4https://www.bbc.co.uk/ws/languages (scraped\nup to January 17, 2021)\nLanguage XLM-R mBERT AfriBERTa\nAfaan Oromoo 0.10 - 0.05\nAmharic 0.80 - 0.21\nHausa 0.30 - 0.15\nSomali 0.40 - 0.17\nSwahili 1.60 0.04 0.19\nYorùbá - 0.06 0.03\nTable 3: Comparing Sizes Across Models: Compari-\nson of the dataset sizes (GB) of languages present in\nXLM-R, mBERT and AfriBERTa. “-” indicates lan-\nguage was not present in model’s pretraining corpus.\nkens).5 Following ﬁndings from Liu et al. (2019)\nand Conneau et al. (2020) that more data is always\nbetter for pretrained language modelling, our small\ncorpus makes our task even more challenging, and\none can already see that our model is at a disadvan-\ntage compared to XLM-R and mBERT.\nOur corpus contains approximately 5.45 mil-\nlion sentences and 108.8 million tokens. Table 2\npresents more details about the dataset size for each\nlanguage. It can be observed that languages like\nSwahili, Hausa and Somali have the most amount\nof data, while languages like Tigrinya have very\nlittle data with just about 12,000 sentences.\nFor each language we pretrained on that is\npresent in XLM-R or mBERT, we compare the\nsize of that language in our dataset to its size in\nthe pretraining corpora of mBERT and XLM-R.\nFrom the comparison details in Table 3, we can see\nthat XLM-R always has more data for languages\npresent in our pretraining corpus and theirs. In fact,\non average, we can see that the size of the language\nis always at least two times more in XLM-R. For\nmBERT, we can see that AfriBERTa has more data\nfor Hausa and Yorùbá, which are present in both\ncorpora. However, one would expect that, given\nthat both languages are in the Latin script, there\nshould be enough high-resource transfer to help\nthem outperform our model.\nPreprocessing: We remove lines that are empty\nor only contain punctuation. Given that there is\nsigniﬁcant overlap between the African language\ncorpora in Common Crawl and the BBC News data\nthat we crawled, we perform extensive deduplica-\ntion for each language by removing exact matched\nsentences. We also enforce a minimum length re-\nstriction by only retaining sentences with more than\n5 tokens. We observe that the quality of the dataset\n5https://github.com/mayhewsw/\nmultilingual-data-stats/tree/main/wiki\n119\nfrom Common Crawl is very low, conﬁrming re-\ncent ﬁndings from Caswell et al. (2021). Hence,\nwe manually clean the data as much as we can by\nremoving texts in the wrong language, while trying\nto throw out as little data as possible.\n3.2 Model\nWe train a transformer (Vaswani et al., 2017) with\nthe standard masked language modelling objec-\ntive of Devlin et al. (2019) without next sentence\nprediction. This is also the same approach used\nin XLM-R (Conneau et al., 2020). We pretrain\non text data containing all languages, sampling\nbatches from different languages. We sample lan-\nguages such that our model does not see the same\nlanguage over several consecutive batches.\nWe utilize subword tokenization on the raw\ntext data using SentencePiece (Kudo and Richard-\nson, 2018) trained with a unigram language model\n(Kudo, 2018). We sample training sentences from\ndifferent languages for the tokenizer following the\nsampling method described in Conneau and Lam-\nple (2019) with α= 0.3.\n3.3 Evaluation\nPretraining: We take out varying amounts of\nevaluation sentences from each language’s original\nmonolingual dataset, depending on the language’s\nsize. Our total evaluation set containing all lan-\nguages consists of roughly 440,000 sentences. We\nevaluate the perplexity on this dataset to measure\nlanguage model performance. However, follow-\ning Conneau et al. (2020), we continue pretraining\neven after validation perplexity stops decreasing.\nEffectively, we pretrain on around 0.94 GB of data\nand evaluate on around 0.08 GB of data.\nNER: We evaluate named entity recognition\n(NER) using the recently released MasakhaNER\ndataset (Adelani et al., 2021). The dataset cov-\ners the following ten languages: Amharic, Hausa,\nIgbo, Kinyarwanda, Luganda, Luo, Nigerian Pid-\ngin, Swahili, Wolof and Yorùbá. The authors estab-\nlished strong baselines on the dataset ranging from\nsimpler methods like CNN-BiLSTM-CRF to pre-\ntrained language models like mBERT and XLM-R.\nText Classiﬁcation: We use the news topic clas-\nsiﬁcation dataset from Hedderich et al. (2020),\nwhich covers Hausa and Yoruba. The authors es-\ntablished strong transfer learning and distant su-\npervision baselines. They ﬁnd that both mBERT\nand XLM-R outperform simpler neural network\nbaselines in few-shot and zero-shot settings.\n3.4 Experimental Setup\nAll models are trained with the Huggingface Trans-\nformers library (Wolf et al., 2020) (v4.2.1). In\nthe following initial experiments, we pretrain each\nmodel for 60,000 steps and use a maximum se-\nquence length of 512. We pretrain using a batch\nsize of 32 and accumulate the gradients for 4 steps.\nOptimization is done using AdamW (Loshchilov\nand Hutter, 2017) with a learning rate of 1e-4 and\n6000 linear warm-up steps. We report F1 scores on\nthe NER dataset averaged over 3 runs with different\nrandom seeds. Following initial explorations, we\nfound a vocabulary size of 40k, excluding special\ntokens, to yield good results across different model\nsizes, so we use this for initial experiments.\nNER models are trained by adding a linear clas-\nsiﬁcation layer to the pretrained transformer model\nand ﬁne-tuning all parameters. Following Adelani\net al. (2021), we train for 50 epochs with a batch\nsize of 16, a learning rate of 5e-5 and also optimize\nwith AdamW.\nText classiﬁcation models are trained by adding\na linear classiﬁcation layer to the pretrained trans-\nformer model and ﬁne-tuning all parameters. We\ntrain for 25 epochs with a batch size of 32, warm-\nup steps of 100, learning rate of 5e-5 and optimize\nwith AdamW as well.\n4 Results\n4.1 Design Space Exploration\nIn this section, we compare variants of AfriBERTa\nmodels to each other in a bid to understand how\nto pretrain multilingual language models in small\ndata regimes. We pretrain variants from the point\nof view of model architecture, taking three factors\ninto consideration: (i) model depth, (ii) number of\nattention heads and (iii) vocabulary size. We deﬁne\nperformance as “good transfer to downstream task”.\nBecause the NER dataset covers more languages,\nwe ﬁne-tune and evaluate our models on it.\nModel Depth: We compare models with 4, 6, 8\nand 10 layers. For each model, we use 4 atten-\ntion heads and adjust the size of the hidden units\nand feed-forward layers so that all models have ap-\nproximately the same number of parameters. From\npreliminary experiments, models with more than\n10 layers did not yield substantially better perfor-\nmance. This is expected, given the small size of the\n120\n# Layers # Params amh hau ibo kin lug luo pcm swa wol yor avg\n4 74.8M 62.18 89.66 87.03 69.29 67.23 59.00 83.57 83.89 77.04 67.02 75.97\n6 74.7M 61.59 90.34 85.81 72.76 66.39 61.43 86.27 84.02 76.61 68.54 76.91\n8 74.6M 62.04 90.96 86.33 74.00 68.66 60.96 84.43 84.16 76.11 67.38 77.00\n10 74.3M 62.14 90.69 87.36 75.74 67.87 60.59 84.79 84.70 76.17 67.51 77.27\nTable 4: Effect of Number of Layers: NER dev F1 scores (averaged over three different random seeds) on each\nlanguage for models with different layer depth, but same number of parameters. The sizes of the embedding\nand feed-foward layers are adjusted such that feed-foward is always approximately 4 times embedding size. The\nhighest F1-score per language is underlined, while the highest overall average is in bold.\n# Layers # Att. Heads # Params amh hau ibo kin lug luo pcm swa wol yor avg\n4 2 60.1M 58.23 88.78 84.63 71.28 65.68 56.91 83.84 82.44 76.69 64.64 74.99\n4 4 60.1M 60.09 89.34 87.08 72.95 68.25 60.10 84.08 83.17 76.29 66.73 76.44\n4 6 60.1M 60.26 89.49 86.01 72.69 67.82 59.85 84.68 83.73 76.22 67.66 76.46\n6 2 74.3M 60.54 89.72 87.25 72.68 70.23 59.98 84.52 83.25 76.00 67.00 76.74\n6 4 74.3M 63.29 90.19 86.05 74.26 68.58 59.23 84.74 83.46 77.62 67.04 76.80\n6 6 74.3M 60.38 90.86 86.70 73.12 68.54 61.68 84.59 82.80 79.02 68.48 77.31\n8 2 88.5M 60.32 90.55 85.32 75.38 69.89 62.73 85.50 83.51 79.07 68.09 77.78\n8 4 88.5M 61.90 90.79 86.67 74.28 68.45 61.57 85.64 83.88 78.48 70.16 77.77\n8 6 88.5M 60.92 90.16 86.95 74.71 70.66 60.75 85.48 84.87 78.04 71.16 78.09\n10 2 102.6M 59.87 90.78 87.10 73.73 66.29 60.03 85.04 83.47 81.12 69.06 77.40\n10 4 102.6M 63.95 91.33 87.11 75.24 68.96 63.36 85.66 84.67 74.60 69.27 77.80\n10 6 102.6M 63.94 90.54 87.39 75.90 69.19 61.73 85.77 84.66 75.64 69.48 77.81\nTable 5: Effect of Number of Attention Heads: NER dev F1 scores (averaged over three different random seeds)\non each language for different models with the same number of layers, but different number of attention heads.\nThe highest F1-score per layer size is underlined, while the highest overall average is in bold.\ndata. Because of this, coupled with computational\nconstraints, we do not explore settings with more\nthan 10 layers.\nAs we can see from the results in Table 4, deeper\nmodels always outperform shallower models. How-\never, the performance gain diminishes with size.\nFor example, the gain from increasing the model\nto 6 layers from 4 layers is roughly 1 F1 point.\nHowever, the gain from increasing from 6 layers\nto 10 layers is only ∼0.4. This corroborates the\nrecent universality overﬁtting ﬁndings from Kaplan\net al. (2020), who showed that the performance of\ntransformer language models improves predictably\nas long as data size and model depth are scaled in\ntandem, otherwise there is a diminishing return.\nIn general, our results suggests that deeper mod-\nels also work well when pretraining multilingual\nlanguage models on small datasets. This follows\nprevious works on understanding the cross-lingual\nability of multilingual language models (K et al.,\n2019), which have shown that deeper models have\nbetter cross-lingual performance. However, gains\nfrom increasing depth are relatively minimal be-\ncause of the size of our corpus.\nNumber of Attention Heads: For each layer\nsize (4, 6, 8 and 10), we train models with three\ndifferent numbers of attention heads: 2, 4 and 6.\nAgain, initial experiments with more than 6 atten-\ntion heads did not yield any better results, so we\ndo not explore more than 6 heads. Results are pre-\nsented in Table 5.\nThe results suggest that there is a diminishing\nreturn to the number of attention heads when the\nmodel is deep. Shallower models need more atten-\ntion heads to attain competitive performance. How-\never, when the model is deep enough, it is very\ncompetitive with as few as two attention heads.\nThis suggests that results from recent works (K\net al., 2019; Michel et al., 2019), which suggest\nthat transformers can do without a large number\nof attention heads, also hold true for multilingual\nlanguage models on small datasets.\nVocabulary Size: Previous works have sug-\ngested that on small datasets, one should employ a\nsmall vocabulary size (Sennrich and Zhang, 2019;\nAraabi and Monz, 2020). However, it remains to be\nseen if this holds in the multilingual setting since\nseveral languages will be competing for vocabulary\n121\n# Layers # Att. Heads Vocab Size # Params amh hau ibo kin lug luo pcm swa wol yor avg\n8 6 25k 76.9M 60.56 89.96 85.84 73.23 69.67 61.86 85.11 84.34 75.40 68.35 77.09\n8 6 40k 88.5M 60.92 90.16 86.95 74.71 70.66 60.75 85.48 84.87 78.04 71.16 78.09\n8 6 55k 99.9M 63.65 90.17 87.28 72.47 67.47 61.49 85.59 85.09 77.56 69.06 77.35\n8 6 70k 111.5M 66.17 91.25 87.74 77.44 68.29 59.91 87.00 87.05 77.49 68.82 78.33\n8 6 85k 123.1M 62.35 90.42 87.44 77.01 68.20 61.98 86.46 85.87 72.84 70.14 77.82\nTable 6: Effect of Vocabulary Size: NER dev F1 scores (averaged over three different random seeds) on the best\nmodel size with varying vocabulary sizes. The highest overall average F1-score is in bold.\nLanguage In In In CNN-BiLSTM mBERT XLM-R AfriBERTa AfriBERTa AfriBERTa\nmBERT XLM-R? AfriBERTa? CRF base small base large\n(172M) (270M) (97M) (111M) (126M)\namh no yes yes 52.89 0.0 70.96 67.90 71.80 73.82\nhau no yes yes 83.70 87.34 89.44 89.01 90.10 90.17\nibo no no yes 78.48 85.11 84.51 86.63 86.70 87.38\nkin no no yes 64.61 70.98 73.93 69.91 73.22 73.78\nlug no no no 74.31 80.56 80.71 76.44 79.30 78.85\nluo no no no 66.42 72.65 75.14 67.31 70.63 70.23\npcm no no yes 66.43 87.78 87.39 82.92 84.87 85.70\nswa yes yes yes 79.26 86.37 87.55 85.68 88.00 87.96\nwol no no no 60.43 66.10 64.38 60.10 61.82 61.81\nyor yes no yes 67.07 78.64 77.58 76.08 79.36 81.32\navg – – – 69.36 71.55 79.16 76.20 78.60 79.10\navg (excl. amh) – – – 71.19 79.50 80.07 77.12 79.36 79.69\nTable 7: Comparison of NER Results: F1-scores on the test sets of each language. XLM-R and mBERT results\nobtained from Adelani et al. (2021). The best score for each language and overall best scores are in bold. We also\nreport the model parameter size in parentheses.\nspace and Conneau et al. (2020) have found that in-\ncreasing the vocabulary size improves multilingual\nperformance. We evaluate our best model size on\nvarying vocabulary sizes and report results in Ta-\nble 6. As we can see from the results, increasing the\nvocabulary size does not always yield good results\non smaller datasets. While a small vocabulary size\nperforms relatively poorly, medium sized vocabu-\nlaries can sometimes outperform larger ones. Due\nto computation constraints, we selected vocabulary\nsize of 70k for the ﬁnal models below.\nFinal Model Selection: We release three Afri-\nBERTa pretrained model sizes: small (4 layers),\nbase (8 layers) and large (10 layers). Each model\nhas 6 attention heads, 768 hidden units, 3072 feed-\nforward size and a maximum length of 512. Their\nrespective parameter sizes are 97 million, 111 mil-\nlion and 126 million. We use ﬂoat16 operations\nto speed up training and reduce memory usage.\nPretraining is done for 460,000 steps with 40,000\nlinear warm-up steps and then the learning rate is\ndecreased linearly. We pretrain with a batch size\nof 32 on 2 Nvidia V100 GPUs and accumulate the\ngradients for 8 steps.\n4.2 NER Comparisons\nAs we can see in Table 7, even the AfriBERTa small\nmodel, which is almost three times smaller than\nXLM-R, obtains competitive NER results across\nall languages, trailing XLM-R by less than 3 F1\npoints. This represents a great opportunity for de-\nployment in resource constrained scenarios, which\nis usually common for applications in low-resource\nlanguages. Our best performing model is Afri-\nBERTa large, which outperforms mBERT and is\nvery competitive with XLM-R across all languages.\nAfriBERTa large even outperforms both models on\nseveral languages that all three models were pre-\ntrained on, such as Hausa, Amharic and Swahili.\nIt should be noted that AfriBERTa large achieves\nall this with less than half of the number of param-\neters of XLM-R and about 45M fewer parameters\nthan mBERT. Furthermore, we can see that our\nmodels performs very well on languages that were\nnot part of our pretraining corpus, such as Luo,\nWolof and Luganda. This demonstrates its strong\ncross-lingual capabilities, despite smaller param-\neter sizes and pretraining corpus size. A notable\nobservation is that both mBERT and XLM-R out-\n122\nLanguage In In In mBERT XLM-R AfriBERTa\nmBERT XLM-R? AfriBERTa? base large\nhau no yes yes 83.03 85.62 90.86\nyor yes no yes 71.61 71.07 83.22\nTable 8: Comparison of Text Classiﬁcation Results: F1-scores on the test sets. The best score for each language\nis in bold.\nperform AfriBERTa on Nigerian Pidgin, despite\nnot being trained on the language. This is likely\nbecause of the language’s high similarity with En-\nglish. Nigerian Pidgin is an English Creole, mean-\ning it borrows and shares a lot of its properties (in-\ncluding words) with English. Since both mBERT\nand XLM-R were pretrained on very large amounts\nof English data, it is no surprise that they perform\nso well on Nigerian Pidgin. In summary, our small,\nbase and large models’ performance are compara-\nble to mBERT and XLM-R across all languages,\ndespite being pretrained on a substantially smaller\ncorpus and having fewer model parameters.\n4.3 Text Classiﬁcation Comparisons\nWe also compare our best model (AfriBERTa\nlarge) to XLM-R base and mBERT on text clas-\nsiﬁcation. As we can see from the results in Ta-\nble 8, AfriBERTa large clearly outperforms both\nXLM-R and mBERT by over 10 F1 points on\nYorùbá and up to 7 F1 points on Hausa. Results\nshow that mBERT slightly outperformsXLM-R on\nYorùbá, most likely because it was pretrained on it,\nwhile XLM-R was not. XLM-R also outperforms\nmBERT on Hausa, presumably for the same reason.\nIt should be noted that our model was pretrained\non around half as much Hausa data as XLM-R, but\nstill outperforms it substantially.\nAn important observation is that AfriBERTa out-\nperforms both XLM-R and mBERT on text classiﬁ-\ncation, but not so much on the NER task. This sug-\ngests that, perhaps, some downstream tasks beneﬁt\nfrom larger multilingual models with high-resource\ntransfer than other tasks. However, we leave this\ninteresting observation for future work.\n5 Discussion\nIn this section, we discuss some other contributions\nof this work. At a high level, AfriBERTa presents\nthe ﬁrst evidence that multilingual language mod-\nels are viable with very little training data. This\noffers numerous beneﬁts for the NLP community,\nespecially for low-resource languages.\nOpportunities for Smaller Curated Datasets:\nOur empirical results suggest that state-of-the-art\nNLP methods like multilingual language models\ncan be made more accessible for low-resource lan-\nguages. Caswell et al. (2021) recently showed\nthat web-crawled multilingual corpora available\nfor many languages, especially low-resource ones,\nare usually of very low quality. They found is-\nsues such as wrong-language content, erroneous\nlanguage codes and low-quality sentences. Our\nwork opens the door to competitive multilingual\nlanguage models on smaller curated datasets for\nlow-resource languages.\nAnother possible beneﬁt of these smaller curated\ndatasets is that they would tend to contain local\ncontent as opposed to foreign content as is in the\nWikipedia and other relatively larger datasets of\nthese languages. Models trained on such datasets\nwith local content could potentially be more useful\nto the speakers of the languages given that they\nwould be trained on data with local context.\nStrength of Language Similarity: Our work\nchallenges the commonly held belief in the NLP\ncommunity that lower-resource languages need\nhigher-resource languages in multilingual language\nmodels. Instead, we empirically demonstrate that\npretraining on similar low-resource languages in a\nmultilingual setting may sometimes be better than\npretraining using high-resource and low-resource\nlanguages together. This approach should be con-\nsidered in future work, especially since there have\nbeen recent ﬁndings (Wang et al., 2020) that low-\nresource languages also experience negative inter-\nference in multilingual models.\nPotential Ethical Beneﬁts: Recent works have\ncalled for more considerations of ethics and related\nconcerns in the development of pretrained language\nmodels (Bender et al., 2021). These concerns have\nranged from environmental and ﬁnancial (Strubell\net al., 2019) to societal bias (Kurita et al., 2019;\nBasta et al., 2019)\nWe believe our work offers the potential to ad-\n123\nModel # Params Data Size (GB) # Tokens\nXLM-R base 270M 2395 164.0B\nmBERT 172M 100 12.8B\nAfriBERTa base 112M 0.94 108.8M\nTable 9: Comparing Sizes: Comparison of datasets\nand model sizes between XLM-R, mBERT and Afri-\nBERTa.\ndress some of these concerns, while developing\nlanguage technology for under-served languages.\nA comparison of model and data sizes of com-\nmon multilingual models is presented in Table 9.\nSmaller dataset sizes, like ours, mean that these\ndatasets can more easily be cleaned, ﬁltered, ana-\nlyzed and possibly de-biased in comparison to the\nhumongous data sizes of larger language models.\nWe have also shown that smaller-sized models can\noutperform larger models, despite using smaller\ntraining resources. This represents a potential for\nreduced environmental impact.\nWhile “low-resource” is commonly used in the\nNLP community to describe a lack of data re-\nsources, Nekoto et al. (2020) have argued that\n“low-resource” also includes a wide range of soci-\netal problems, including computational constraints.\nThus, our work embodies the broader spirit of “low-\nresource”, as we develop more efﬁcient models on\nsmaller data sizes for under-served languages.\nImproving the Representation of African Lan-\nguages in Modern NLP tools: As discussed in\nsection 2, there is very poor representation of\nAfrican languages in modern NLP tools. Recently,\nthere have been signiﬁcant efforts towards closing\nthis gap (Alabi et al., 2019; Ogueji and Ahia, 2019;\nNekoto et al., 2020; Ahia and Ogueji, 2020; Fan\net al., 2020; Azunre et al., 2021; Dossou and Sabry,\n2021; Adelani et al., 2021). Our work follows\nalong this path, as there is a need to build language\ntechnologies for the over 1.3 billion people on the\ncontinent. Besides showing that multilingual lan-\nguage models are viable on low-resource African\nlanguages with small training data, we also intro-\nduce the ﬁrst language models for four of these\nlanguages: Kinyarwanda, Kirundi, Nigerian Pidgin\nand Tigrinya. These are four languages with over\n50 million speakers (Eberhard et al., 2019) who are\nactive users of digital tools. However, these lan-\nguages have noticeably deﬁcient support in NLP\ntechnologies. Our work represents an important\nstep towards improving this.\n6 Conclusion\nIn this work, we introduced AfriBERTa, a multi-\nlingual language model pretrained on less than 1\nGB of data from 11 African languages. We show\nthat this model is competitive with models pre-\ntrained on larger datasets and even outperforms\nthem on some languages. Our comprehensive ex-\nperiments also highlight important factors to con-\nsider when pretraining multilingual language mod-\nels on smaller datasets. More importantly, we high-\nlight some practical beneﬁts of viable language\nmodels on smaller datasets. Finally, we release\ncode, pretrained models and the dataset to stimu-\nlate further work on multilingual language models\nfor low-resource languages.\nAcknowledgements\nThis research was supported in part by the Nat-\nural Sciences and Engineering Research Council\n(NSERC) of Canada and an AI for Social Good\ngrant from the Waterloo AI Institute; computational\nresources were provided by Compute Ontario and\nCompute Canada.\nReferences\nDavid Ifeoluwa Adelani, Jade Z. Abbott, Graham\nNeubig, Daniel D’souza, Julia Kreutzer, Constan-\ntine Lignos, Chester Palen-Michel, Happy Buza-\naba, Shruti Rijhwani, Sebastian Ruder, Stephen\nMayhew, Israel Abebe Azime, Shamsuddeen Has-\nsan Muhammad, Chris Chinenye Emezue, Joyce\nNakatumba-Nabende, Perez Ogayo, Aremu An-\nuoluwapo, Catherine Gitau, Derguene Mbaye, Je-\nsujoba O. Alabi, Seid Muhie Yimam, Tajud-\ndeen Gwadabe, Ignatius Ezeani, Rubungo An-\ndre Niyongabo, Jonathan Mukiibi, Verrah Otiende,\nIroro Orife, Davis David, Samba Ngom, Tosin P.\nAdewumi, Paul Rayson, Mofetoluwa Adeyemi, Ger-\nald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane Mboup, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda,\nOrevaoghene Ahia, Bonaventure F. P. Dossou,\nKelechi Ogueji, Thierno Ibrahima Diop, Abdoulaye\nDiallo, Adewale Akinfaderin, Tendai Marengereke,\nand Salomey Osei. 2021. MasakhaNER: Named\nentity recognition for African languages. CoRR,\nabs/2103.11811.\nOrevaoghene Ahia and Kelechi Ogueji. 2020. To-\nwards supervised and unsupervised neural machine\n124\ntranslation baselines for Nigerian Pidgin. CoRR,\nabs/2003.12660.\nJesujoba O. Alabi, Kwabena Amponsah-Kaakyire,\nDavid Ifeoluwa Adelani, and Cristina España-Bonet.\n2019. Massive vs. curated word embeddings for\nlow-resourced languages. the case of Yorùbá and\nTwi. CoRR, abs/1912.02481.\nAli Araabi and Christof Monz. 2020. Optimizing\ntransformer for low-resource neural machine transla-\ntion. In Proceedings of the 28th International Con-\nference on Computational Linguistics , pages 3429–\n3435, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nPaul Azunre, Salomey Osei, Salomey Addo,\nLawrence Asamoah Adu-Gyamﬁ, Stephen\nMoore, Bernard Adabankah, Bernard Opoku,\nClara Asare-Nyarko, Samuel Nyarko, Cynthia\nAmoaba, Esther Dansoa Appiah, Felix Akwerh,\nRichard Nii Lante Lawson, Joel Budu, Emmanuel\nDebrah, Nana Boateng, Wisdom Ofori, Edwin\nBuabeng-Munkoh, Franklin Adjei, Isaac Kojo Es-\nsel Ampomah, Joseph Otoo, Reindorf Borkor,\nStandylove Birago Mensah, Lucien Mensah,\nMark Amoako Marcel, Anokye Acheampong\nAmponsah, and James Ben Hayfron-Acquah. 2021.\nContextual text embeddings for Twi. CoRR,\nabs/2103.15963.\nChristine Basta, Marta R. Costa-jussà, and Noe Casas.\n2019. Evaluating the underlying gender bias in con-\ntextualized word embeddings. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 33–39, Florence, Italy. Associa-\ntion for Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY , USA. As-\nsociation for Computing Machinery.\nAndrew Caines. 2019. The geographic diversity of\nNLP conferences.\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wa-\nhab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-\nlahsera Tapo, Nishant Subramani, Artem Sokolov,\nClaytone Sikasote, Monang Setyawan, Supheak-\nmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara\nRivera, Annette Rios, Isabel Papadimitriou, Sa-\nlomey Osei, Pedro Javier Ortiz Suárez, Iroro\nOrife, Kelechi Ogueji, Rubungo Andre Niyongabo,\nToan Q. Nguyen, Mathias Müller, André Müller,\nShamsuddeen Hassan Muhammad, Nanda Muham-\nmad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov,\nTapiwanashe Matangira, Colin Leong, Nze Lawson,\nSneha Kudugunta, Yacine Jernite, Mathias Jenny,\nOrhan Firat, Bonaventure F. P. Dossou, Sakhile\nDlamini, Nisansa de Silva, Sakine Çabuk Balli,\nStella Biderman, Alessia Battisti, Ahmed Baruwa,\nAnkur Bapna, Pallavi Baljekar, Israel Abebe Azime,\nAyodele Awokoya, Duygu Ataman, Orevaoghene\nAhia, Oghenefego Ahia, Sweta Agrawal, and Mofe-\ntoluwa Adeyemi. 2021. Quality at a glance: An\naudit of web-crawled multilingual datasets. CoRR,\nabs/2103.12028.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBonaventure F. P. Dossou and Mohammed Sabry. 2021.\nAfriVEC: Word embedding models for African lan-\nguages. case study of Fon and Nobiin. CoRR,\nabs/2103.05132.\nDavid M. Eberhard, Gary F. Simons, and Charles D.\nFenning. 2019. Ethnologue: Languages of the\nworlds. (twenty second edition).\nIgnatius Ezeani, Ikechukwu Onyenwe, and Mark Hep-\nple. 2018. Transferred embeddings for Igbo similar-\nity, analogy, and diacritic restoration tasks. In Pro-\nceedings of the Third Workshop on Semantic Deep\nLearning, pages 30–38, Santa Fe, New Mexico. As-\nsociation for Computational Linguistics.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2020. Be-\nyond English-centric multilingual machine transla-\ntion. CoRR, abs/2010.11125.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Je-\nsujoba Alabi, Udia Markus, and Dietrich Klakow.\n2020. Transfer learning and distant supervision\nfor multilingual transformer models: A study on\nAfrican languages. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2580–2591, Online. As-\nsociation for Computational Linguistics.\n125\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2019. Cross-lingual ability of mul-\ntilingual BERT: an empirical study. CoRR,\nabs/1912.07840.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W.\nBlack, and Yulia Tsvetkov. 2019. Measuring bias\nin contextualized word representations. In Proceed-\nings of the First Workshop on Gender Bias in Natu-\nral Language Processing, pages 166–172, Florence,\nItaly. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in Adam. CoRR,\nabs/1711.05101.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems ,\nvolume 32. Curran Associates, Inc.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training\ndata volume for compact language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7853–7858, Online. Association for Computa-\ntional Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, volume 26. Curran Associates, Inc.\nWilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\nMatsila, Timi Fasubaa, Taiwo Fagbohungbe,\nSolomon Oluwole Akinola, Shamsuddeen Muham-\nmad, Salomon Kabongo Kabenamualu, Salomey\nOsei, Freshia Sackey, Rubungo Andre Niyongabo,\nRicky Macharm, Perez Ogayo, Orevaoghene Ahia,\nMusie Meressa Berhe, Mofetoluwa Adeyemi,\nMasabata Mokgesi-Selinga, Lawrence Okegbemi,\nLaura Martinus, Kolawole Tajudeen, Kevin Degila,\nKelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\nJason Webster, Jamiil Toure Ali, Jade Abbott,\nIroro Orife, Ignatius Ezeani, Idris Abdulkadir\nDangana, Herman Kamper, Hady Elsahar, Good-\nness Duru, Ghollah Kioko, Murhabazi Espoir,\nElan van Biljon, Daniel Whitenack, Christopher\nOnyefuluchi, Chris Chinenye Emezue, Bonaventure\nF. P. Dossou, Blessing Sibanda, Blessing Bassey,\nAyodele Olabiyi, Arshath Ramkilowan, Alp Öktem,\nAdewale Akinfaderin, and Abdallah Bashir. 2020.\nParticipatory research for low-resourced machine\ntranslation: A case study in African languages.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 2144–2160,\nOnline. Association for Computational Linguistics.\nKelechi Ogueji and Orevaoghene Ahia. 2019. Pidgin-\nUNMT: Unsupervised neural machine translation\nfrom West African Pidgin to English. CoRR,\nabs/1912.03444.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1703–1714, Online. Association for Computational\nLinguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n126\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport, OpenAI.\nRico Sennrich and Biao Zhang. 2019. Revisiting low-\nresource neural machine translation: A case study.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 211–\n221, Florence, Italy. Association for Computational\nLinguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.\n2020. On negative interference in multilingual mod-\nels: Findings and a meta-learning treatment. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4438–4450, Online. Association for Computa-\ntional Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R. Bowman. 2020. When do you need\nbillions of words of pretraining data? CoRR,\nabs/2011.04946.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8650926351547241
    },
    {
      "name": "Natural language processing",
      "score": 0.6572557687759399
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5833102464675903
    },
    {
      "name": "Language model",
      "score": 0.5055749416351318
    },
    {
      "name": "Second-generation programming language",
      "score": 0.4923659563064575
    },
    {
      "name": "Code (set theory)",
      "score": 0.47740355134010315
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4694841504096985
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.44319865107536316
    },
    {
      "name": "Training set",
      "score": 0.4317900836467743
    },
    {
      "name": "Programming language",
      "score": 0.18285620212554932
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.13104382157325745
    },
    {
      "name": "Fifth-generation programming language",
      "score": 0.07690507173538208
    },
    {
      "name": "Programming paradigm",
      "score": 0.06452056765556335
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": []
}