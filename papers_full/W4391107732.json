{
    "title": "GUARDIAN: A Multi-Tiered Defense Architecture for Thwarting Prompt Injection Attacks on LLMs",
    "url": "https://openalex.org/W4391107732",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5049309729",
            "name": "Parijat Rai",
            "affiliations": [
                "Bennett University"
            ]
        },
        {
            "id": "https://openalex.org/A5020378373",
            "name": "Saumil Sood",
            "affiliations": [
                "Bennett University"
            ]
        },
        {
            "id": "https://openalex.org/A42805117",
            "name": "Vijay K. Madisetti",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A18771973",
            "name": "Arshdeep Bahga",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6632459340",
        "https://openalex.org/W4388850876",
        "https://openalex.org/W6600277017",
        "https://openalex.org/W6600728650",
        "https://openalex.org/W4380738256",
        "https://openalex.org/W6667296330",
        "https://openalex.org/W6602461605",
        "https://openalex.org/W6602945829",
        "https://openalex.org/W2540646130",
        "https://openalex.org/W6756688054"
    ],
    "abstract": "This paper introduces a novel multi-tiered defense architecture to protect language models from adversarial prompt attacks. We construct adversarial prompts using strategies like role emulation and manipulative assistance to simulate real threats. We introduce a comprehensive, multi-tiered defense framework named GUARDIAN (Guardrails for Upholding Ethics in Language Models) comprising a system prompt filter, pre-processing filter leveraging a toxic classifier and ethical prompt generator, and pre-display filter using the model itself for output screening. Extensive testing on Meta's Llama-2 model demonstrates the capability to block 100% of attack prompts. The approach also auto-suggests safer prompt alternatives, thereby bolstering language model security. Quantitatively evaluated defense layers and an ethical substitution mechanism represent key innovations to counter sophisticated attacks. The integrated methodology not only fortifies smaller LLMs against emerging cyber threats but also guides the broader application of LLMs in a secure and ethical manner.",
    "full_text": null
}