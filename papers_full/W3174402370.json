{
  "title": "IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers",
  "url": "https://openalex.org/W3174402370",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2739835004",
      "name": "Pan Bowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753930136",
      "name": "Panda, Rameswar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2366525249",
      "name": "Jiang, Yifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354751725",
      "name": "Wang, Zhangyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202179488",
      "name": "Feris, Rogerio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282381919",
      "name": "Oliva, Aude",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3107849462",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2336525064",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963715038",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3129030533",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3101731278",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2179423374",
    "https://openalex.org/W3035424951",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2970852027",
    "https://openalex.org/W3127856697",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3169793979",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W1523985187",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3135404760",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2995751512",
    "https://openalex.org/W2626639386",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3092739317",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2963393494",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2962851801",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2962944050",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3169769781",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2752037867",
    "https://openalex.org/W2219771564",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2117741877",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3010833785",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W3159663321"
  ],
  "abstract": "The self-attention-based model, transformer, is recently becoming the leading backbone in the field of computer vision. In spite of the impressive success made by transformers in a variety of vision tasks, it still suffers from heavy computation and intensive memory costs. To address this limitation, this paper presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$). We start by observing a large amount of redundant computation, mainly spent on uncorrelated input patches, and then introduce an interpretable module to dynamically and gracefully drop these redundant patches. This novel framework is then extended to a hierarchical structure, where uncorrelated tokens at different stages are gradually removed, resulting in a considerable shrinkage of computational cost. We include extensive experiments on both image and video tasks, where our method could deliver up to 1.4x speed-up for state-of-the-art models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy. More importantly, contrary to other acceleration approaches, our method is inherently interpretable with substantial visual evidence, making vision transformer closer to a more human-understandable architecture while being lighter. We demonstrate that the interpretability that naturally emerged in our framework can outperform the raw attention learned by the original visual transformer, as well as those generated by off-the-shelf interpretation methods, with both qualitative and quantitative results. Project Page: http://people.csail.mit.edu/bpan/ia-red/.",
  "full_text": "IA-RED2: Interpretability-Aware Redundancy\nReduction for Vision Transformers\nBowen Pan1, Rameswar Panda2, Yifan Jiang3, Zhangyang Wang3, Rogerio Feris2, Aude Oliva1,2\n1MIT CSAIL, 2MIT-IBM Watson AI Lab,3UT Austin\nAbstract\nThe self-attention-based model, transformer, is recently becoming the leading\nbackbone in the ﬁeld of computer vision. In spite of the impressive success made\nby transformers in a variety of vision tasks, it still suffers from heavy computation\nand intensive memory costs. To address this limitation, this paper presents an\nInterpretability-Aware REDundancy REDuction framework (IA-RED2). We start\nby observing a large amount of redundant computation, mainly spent on uncorre-\nlated input patches, and then introduce an interpretable module to dynamically and\ngracefully drop these redundant patches. This novel framework is then extended\nto a hierarchical structure, where uncorrelated tokens at different stages are grad-\nually removed, resulting in a considerable shrinkage of computational cost. We\ninclude extensive experiments on both image and video tasks, where our method\ncould deliver up to 1.4×speed-up for state-of-the-art models like DeiT [53] and\nTimeSformer [3], by only sacriﬁcing less than 0.7% accuracy. More importantly,\ncontrary to other acceleration approaches, our method is inherently interpretable\nwith substantial visual evidence, making vision transformer closer to a more\nhuman-understandable architecture while being lighter. We demonstrate that the\ninterpretability that naturally emerged in our framework can outperform the raw\nattention learned by the original visual transformer, as well as those generated by\noff-the-shelf interpretation methods, with both qualitative and quantitative results.\nProject Page: http://people.csail.mit.edu/bpan/ia-red/.\n1 Introduction\nTransformer, a self-attention-based architecture processing sequential input without any recurrent or\nconvolutional operations, has set off a storm in the computer vision literature recently. By dividing\nthe input image into a series of patches and then tokenizing them with linear transformation, the\ntransformer can effectively process the visual data in different modalities [13, 53, 54, 28, 3, 17, 66].\nDespite its versatility, the transformer is always deeply troubled with inefﬁcient computation and its\nvague interpretability. The vision transformer suffers heavy computational costs, especially when\nthe input sequence is long. As the attention module in the vision transformer computes the fully-\nconnected relations among all of the input patches, the computational cost is then quadratic with\nregard to the length of the input sequence. On the other hand, previous works [ 6, 8] have already\nshown the vulnerable interpretability of the original vision transformer, where the raw attention\ncomes from the architecture sometimes fails to perceive the informative region of the input images.\nRecently, more designs of vision transformer architecture [34, 65, 18, 56, 14, 9, 3] are proposed to\nget higher accuracy with less computational cost. Although these methods anchor good trade-offs\nbetween efﬁciency and accuracy, their compression makes the vision transformer even more lack\ninterpretability. Most of these methods assume that the input sequences are sampled from a regular\nvisual input in a ﬁxed shape rule, and thus their network architectures are not ﬂexible as well, which\nmakes the vision transformer (1) no longer able to process the input sequence with arbitrary length as\nthe architecture is designed for a speciﬁc input shape; (2) neither model-agnostic nor task agnostic\nanymore; or (3) neglect the fact that the model redundancy is also input-dependant. We yet argue that\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.12620v2  [cs.CV]  26 Oct 2021\nExample#1: image\nMulti-head \nInterpreter\nInformative Patches\n Example#2: video Informative Patches\nMulti-head \nInterpreter\nFigure 1: Two examples of redundancy reduction in vision transformers. Our proposed multi-head\ninterpreters serve as a model-agnostic module which are built on top of the existing transformer-based\nbackbones for different tasks, including image recognition and video action recognition.\nthere is no inherent tension between efﬁciency and interpretability, and achieving them both does\nnot have to pay design ﬂexibility as a price. Indeed, starting from the philosophy of Occam’s razor,\nthe law of parsimony, or always pursuing more compact solutions when possible, is always treated as\na rule-of-thumb for pursing interpretability, especially in complicated ﬁtting problems [21].\nThis paper aims to seek the win-win between efﬁciency and interpretability while keeping the\nﬂexibility and versatility of the original vision transformer. We propose a novel Interpretability-Aware\nREDundancy REDuction (IA-RED2) framework for reducing the redundancy of vision transformers.\nThe key mechanism that IA-RED 2 uses to increase efﬁciency is to dynamically drop some less\ninformative patches in the original input sequence so that the length of the input sequence could\nbe reduced. While the original vision transformer tokenizes all of the input patches, it neglects the\nfact that some of the input patches are redundant and such redundancy is input-dependant (see from\nFigure 1). As the computational complexity of the attention module is quadratically linear to the\ninput sequence length, the effect of reducing input sequence length would be magniﬁed in the amount\nof the computation. Motivated by this, we leverage the idea of dynamic inference [39, 37, 38, 61, 57],\nand adopt a policy network (referred to as multi-head interpreter) to decide which patches are\nuninformative and then discard them. Our proposed method is inherently interpretability-aware as\nthe policy network learns to discriminate which region is crucial for the ﬁnal prediction results.\nTo summarize, the main contributions of our work includes: (1) We propose IA-RED 2, the ﬁrst\ninterpretability-aware redundancy reduction framework for vision transformer. (2) Our IA-RED2\nframework is one of the ﬁrst input-dependent dynamic inference framework for vision transformer,\nwhich adaptively decides the patch tokens to compute per input instance. (3) IA-RED 2 is both\nmodel-agnostic and task-agnostic. We conduct experiments with IA-RED2 framework spanning\ndifferent tasks, including image recognition and action recognition, and different models, including\nDeiT [53], TimeSformer [3]. (4) We attain promising interpretable results (shown in Figure 3)\nover baselines, with a 1.4×acceleration over DeiT on image recognition tasks, and a 4×acceleration\nover TimeSformer on video action recognition task while largely maintaining the accuracy. We\nalso provide both qualitative results regarding interpretability with heatmaps by our method and\nthose from other baseline methods like raw attention, MemNet [ 29]; as well as the quantitative\ncomparison with current state-of-the-art model interpretability methods, such as GradCAM [44], on\nImageNet-Segmentation [16] dataset with the weakly-supervised image segmentation task.\n2 Related Work\nInterpretability of Neural Networks. Besides improving the discrimination power of deep neural\nnetworks, model interpretability has recently raised another signiﬁcant and popular research question.\nOne of the important goals is to predict the heatmap visualization that precisely indicates the\nobjects or contexts of relevance. Simonyan et. al. [45] attempts to maximize the class score that\ngenerates a saliency map for the given inputs. Dabkowski et al. [12] mask the salient parts of\nthe inputs to manipulate the scores of the classiﬁer, which generalizes well to unseen images and\nenables fast saliency detection. Khosla et al. [29] provide the largest annotated image memorability\ndataset to benchmark the visualization and explanation of natural images. After that, gradient-\nbased methods [49, 46, 47] are proposed to generate precise heatmaps, by computing the gradient\nwith respect to input during the backpropagation. While all the above approaches are studying\nthe interpretability of convolutional neural networks (CNNs), only a few works contribute to the\nvisualization of the vision transformer. Chefer et al. shed some light on its visualization by assigning\nthe local relevance on Transformer layers. Caron et al. [6] demonstrate that a self-supervised trained\nViT produces explicit representation about the semantic location of a given object in natural images.\nDifferent from all of them, our approach starts with a novel multi-head interpreter which is supervised\n2\nby an efﬁciency-driven signal, and then beneﬁts from this powerful interpreter by reducing the\nredundancy of transformer, achieving a “win-win” between interpretability and efﬁciency.\nDynamic Networks. Neural networks are found as redundant regarding their huge computation\ncost [19, 23, 35]. To overcame this issue, many adaptive computation methods are explored during\nthe inference stage [1, 2, 57, 15, 26, 58, 20]. These adaptive computation strategies help speed up the\ninference time of convolutional neural networks (CNNs) [33], recurrent neural network (RNNs) [15],\nand also self-attention based methods (BERT) [25]. Besides the model-level adaptation, others further\nextend this idea to data-level adaptation, by either reducing the spatial redundancy [63] or focusing\non key area [59]. However, those methods are limited by the convolutional structure, where only 2D\ndata can be taken as input. Different from those approaches, our methods naturally beneﬁt from the\nunstructured input taken by vision transformer, and thus can provide a much more precise glance at\nthe target object with the affection of background being eliminated.\nVision Transformer. Transformer, as a self-attention based model, has been widely adopted in\nnatural language processing area before. The recent advance [ 13] shows that the transformer can\nalso achieve incredible performance on computer vision tasks. While vision transformer suffers from\nthe necessity large-scaled dataset [48], many recent works try to encode strong inductive prior by\neither combining it with convolutional layer [60, 32, 64, 62] or introducing 2D-hierarchical structure\nto vision transformer [34, 56, 14, 9]. Besides, transformer also shows strong power in other vision\ntasks, including semantic segmentation [67], object detection [5, 68], image processing [10], and\nimage generation [28, 27]. These successes further suggest the potential of transformer to become\nthe universal model for general vision tasks. Some other works [40, 11, 51] also make meaningful\nefforts on vision transformer efﬁciency. Different from those methods, the proposed method achieves\na “win-win” on both efﬁciency and interpretability.\n3 Proposed Method\nOur main goal is to reduce the redundancy in vision transformers by dynamically dropping less\ninformative patches in the original input sequence while classifying it correctly with the minimum\ncomputation. Our method is built on top of vision transformer (ViT) [13]. We start from presenting a\nbrief overview of ViT, including the computational complexity of each module regarding the input\nsequence length. We then describe our proposed IA-RED2 framework for hierarchically reducing the\nredundant patch tokens at different layers of the vision transformer.\n3.1 Overview of Vision Transformer\nVision transformer mainly consists of three main modules: (1) Multi-head Self Attention layer (MSA)\nto learn relationships between every two different patches among all the input tokens. There are h\nself-attention heads inside the MSA. In each self-attention head, the input token Xi is ﬁrst projected\nto a query Qi, a key Ki, and a value Vi by three different linear transformations. Then, the query Qi\ncomputes the dot products with all the keys Kand these dot products will be scaled and normalized\nby the softmax layer to get the attention weights. After that, it outputs the token Yi by weighted\nsum up all the values V with the obtained attention weights. Finally, the outputs from all heads are\nconcatenated and re-projected by a linear layer into an output token. (2) Feed-Forward Network\n(FFN) which consists of two linear layers which are connected by the GeLU activation [24] function.\nFor each output token Yi ∈RD from the precedent MSA layer, FFN processes it individually. The\nﬁrst linear layer upgrades its dimension from Dto 4D, and the second linear layer downgrades its\ndimension from 4D to D. Both MSA and FFN are functioning as residual connection [ 22]. (3)\nLinear Patch Embedding and Positional Encoding: For an image or a video clip, ViT ﬁrst splits it into\nseveral ﬁxed-size patches and embeds them into input tokens with a linear layer. After transforming\nthe original image and video into a series of tokens, the network is no longer capable of being aware\nof the positional information of the input tokens. Thus the positional embeddings are added to the\ninput tokens right after the patch embedding to learn the positional information of each token.\nComputational Complexity. For an input sequence N ×D, where N is the length of the input\nsequence and Dis the embedding dimension of each input token. The computation complexity of the\nMSA is O(4ND2 + 2N2D). While for the FFN, the computational complexity is O(8ND2). As\nthe computational complexity of patch embedding can be neglected compared with the MSA and\nFFN, the total computational complexity of the ViT is O(12ND2 + 2N2D).\n3\nCLS token\npatch \ntokens\n~\n~\n~\n~\n~\ndrop\ndrop\nkeep\nkeep\nkeep\nMSA FFN\nx L\nMulti-head Interpreter\nPrediction\nReward\npolicy queries\nlinear projection\ndot product\n~ sigmoid activation\npolicy token\ndropped token\nx D\nFigure 2: Illustration of our proposed IA-RED2 framework. We divide the transformer into Dgroups.\nEach group contains a multi-head interpreter and Lcombinations of the MSA and FFN. Before input\nto the MSA and FFN, the patch tokens will be evaluated by the multi-head interpreter to drop some\nuninformative patches. The multi-head interpreters are optimized by reward considering both the\nefﬁciency and accuracy. Best viewed in color.\n3.2 Interpretability-Aware Redundancy Reduction\nIn this section, we introduce our multi-head interpreter in detail which uses a policy token to estimate\nthe importance of the input token. We also demonstrate how we hierarchically train the multi-head\ninterpreter based on a pre-trained vision transformer. Finally, we illustrate that how the interpretability\nemerges in our IA-RED2 framework.\nMulti-head Interpreter. We borrow the idea from the architecture of the MSA layer to devise our\npolicy module, named multi-head interpreter. Given a sequence of patch tokens X ∈RN×d which\nalready contain the positional information, we drop the uninformative patch tokens by using the\nmulti-head interpreter. We ﬁrst divide the original ViT evenly intoDgroups, each group contains a\nmulti-head interpreter and Lblocks which consists of one MSA layer and one FFN. Inside each group,\nbefore inputting to the blocks, the patch tokens will ﬁrst be evaluated by the multi-head interpreter\nfor the informative score Iij, where iand jrepresent the position of the input token and the group\nrespectively. If Iij is below the threshold 0.5, the patch Xi will be completely discarded at jth group\nand will not be available in the subsequent groups. The Iij is obtained by:\nIij = 1\nH\n∑\nh\nφ(Fh\nq(Xi) ∗Fh\nk(Pj)), (1)\nwhere Pj is the policy token in the jth multi-head interpreter, H is the number of the heads in the\nmulti-head interpreter, Fh\nq and Fh\nk are the linear layer at hth head for the patch tokens and the policy\ntoken respectively, ∗represents the dot product and φthe sigmoid activation function.\nHierarchical Training Scheme. Our hierarchical training scheme is built on top of a well-trained\nViT. In our IA-RED2 framework, all of the MSA-FFN blocks in the original vision transformer\nwill be evenly assigned into D groups in our IA-RED 2 framework, where each group contains L\nMSA-FFN blocks and one multi-head interpreter. We ﬁx the parameters of the patch embedding\nlayer, positional encoding, and the class token during the training, and only focus on the parameters\ninside each group. The network groups are optimized in a curriculum learning manner. For example,\nif the number of groups D is 3, we will ﬁrst optimize groups 1 to 3, then 2 to 3, and ﬁnally, we\noptimize the third group. Intuitively, we hope the interpreter at the early stage could learn to select\nthe patches containing all of the necessary contextual information for the correct ﬁnal prediction,\nwhile the interpreter at later stages could focus more on the part-level information since now each\ntoken’s information has already gone through global interaction and fusion. The pseudo-code for the\nabove optimization pipeline can be referred in supplementary materials. We optimize the multi-head\ninterpreters by using the REINFORCE method where the reward considers both the efﬁciency and\naccuracy, and ﬁnetune the MSA-FFN blocks with gradients computed based on cross-entropy loss.\n4\nFormally, during the training phase, given a sequence of patch tokens X ∈RN×d input to the\njth multi-head interpreter, the multi-head interpreter will generate policies for each input token of\ndropping or keeping it as Bernoulli distribution by: πW(ui|Xi) =Iui\nij ∗(1 −Iij)1−ui ,where ui = 1\nmeans to keep the token and ui = 0means to discard the token, Iij is deﬁned in the Eq. 1 and Xi\ndenotes the ith token in the token sequence X. We associate these actions with the reward function:\nR(u) =\n\n\n\n1 −(|u|0\nN )2 if correct\n−τ otherwise\n, (2)\nwhere (|u|0\nN )2 measures the percentage of the patches kept, and τ is the value of penalty for the error\nprediction which controls the trade-off between the efﬁciency and the accuracy of the network. This\nreward function encourages the multi-head interpreter to predict the correct results with as few patch\ntokens as possible. Then we optimize the multi-head interpreter individually by the expected gradient:\n∇Wj J = Eu∼π[A∇Wj\nN∑\ni=1\nlog[Iijui + (1−Iij)(1 −ui)]], A = R(u) −R(ˆu), (3)\nwhere J = Eu∼π[R(u)] is the expected reward to compute the policy gradient [ 50], Wj denotes\nthe parameters of the jth multi-head interpreter. We use the self-critical baseline R(ˆu) in [41] to\nreduce the variance of optimization, where ˆudenotes the maximally probable conﬁguration under\nthe current policy: i.e., ui = 1 if Iij > 0.5, and ui = 0 otherwise. As the computation of the\njth multi-head interpreter is based on the output tokens of (j−1)th group, we optimize the entire\nnetwork in a curriculum learning manner. We ﬁrst train the interpreter in the earlier layer, and then\nﬁx the interpreter and ﬁnetune all of the subsequent MSA-FFN blocks. Let’s take the jth group\nfor example. For the jth group, we ﬁrst only train the multi-head interpreter and then ﬁx it while\noptimizing the subsequent MSA-FFN modules in the jth, ... , Dth groups. When we optimize the\njth group, the multi-head interpreter in the latter groups will be masked and keep all of the tokens.\nEmergence of Interpretability. By visualizing the informative scores predicted by the multi-\nhead interpreters in different network groups, we can see the redundancy of the input patches is\nhierarchically reduced at different levels clearly. For those patches that are removed in the precedent\ngroups, we treat the informative score of them as zero. Thus we can obtain a sequence of the\ninformative scores from each network group whose length equals the original input sequence length.\nWe rearrange this score sequence and interpolate it back to the size of the input vision content (e.g.\nimage or video). As the range of the informative score is from 0 to 1, we can draw a heatmap for\neach network group which interprets that what is redundant for this network group.\n4 Experiments\nDatasets and Metrics. We conduct image recognition experiments on the ImageNet-1k classiﬁcation\ndataset [31]. The performance of our models on ImageNet-1k is measured with the metrics of top-1\nand top-5 accuracy rates. For weakly-supervised image segmentation experiments, we adopt the\nImageNet-Segmentation dataset [16] to evaluate the heatmaps we generate. We report three metrics:\npixel accuracy, mean accuracy (mAcc), and mean IoU (mIoU) to reﬂect the segmentation performance.\nFinally, for video action recognition, we conduct our experiments on Kinetics-400 dataset [7], which\ncontains 240k training videos and 10K videos for testing across 400 classes. We report the metrics of\nclip-1 and video-1 error of video models, which denotes the error rate of evaluating the model with\nthe single clip and the Left-Center-Right three clips, respectively.\nModel Architectures. We build our image model on top of DeiT [53] which adopts the architecture\nof the ViT [13] by modifying the depth and width. Compared to the original ViT [13], DeiT has a\ndistillation token that is in charge of distilling the knowledge from the teacher CNN network. DeiT\nis trained and evaluated on ImageNet-1k [31], without large-scale pre-training. We choose DeiT-S\nand DeiT-B as our base models, where DeiT-B is 4×larger than DeiT-S in terms of FLOPs. For the\nvideo model, we construct our model based on TimeSformer [3]. There are several different attention\nmechanisms introduced in [3]. Here we adopt the TimeSformer with the JointST attention method,\nwhich keeps the architecture of the vanilla ViT and takes all of the input patches as one sequence. Our\nmodel samples 8 frames in one video clip and splits them into 1568 frame patches. During inference,\nour model evenly crops 3 views from the video clip, each view of them has 8 frames.\n5\noriginal MemNet raw attention ours original MemNet raw attention ours\nFigure 3: We visualize the heatmaps which highlight the informative region of the input images\nof MemNet, raw attention at the second block, and our method with DeiT-S model. We ﬁnd that\nour method can obviously better interpret the part-level stuff of the objects of interest. Here the\nvisualization results are randomly chosen. Best viewed in color.\nTable 1: Results of weakly-supervised image segmentation on ImageNet-segmentation [16]. We use\nour method based on the training with DeiT-S model. Higher is better.\nMetrics raw attention LIME [42] MemNet [29] GradCAM [44] LRP [4] Ours\npixel accuracy 67.87 67.32 52.81 65.91 50.72 70.36\nmAcc 61.77 47.80 53.70 55.04 50.62 64.86\nmIoU 46.37 33.94 34.66 41.31 32.62 49.42\nImplementation Details. For the image recognition task, we divide the vision transformer back-\nbone [53] into 3 (D= 3) groups, where each group contains 4 (L= 4) MSA-FFN modules and one\nmulti-head interpreter. We optimize the entire framework forD×30 epochs. During every 30 epochs,\nwe optimize the multi-head interpreter for 10 epochs and all of the subsequent MSA-FFN modules\nfor 20 epochs. We use a mini-batch size of 32 images per GPU and adopt Adam [30] optimizer with\nan initial learning rate of 4e-5, which decays by cosine strategy [36] to train all our models. For the\nvideo understanding task, we set D= 1, i.e., we only select the informative patches at the input level.\nAnd we train the multi-head interpreter for 5 epochs and then ﬁnetune the backbone network for 1\nepoch, mainly following the settings listed in the original paper [3]. We use a mini-batch size of 8\n6\nTable 2: Redundancy reduction results of our IA-RED2 with DeiT on ImageNet-1k [31].\nArch. Method speed (fps) Top-1 Top-5 Arch. Method speed (fps) Top-1 Top-5\noriginal ≤930 79.8 95.0 original ≤320 81.8 95.6\nrandom ≥1360 78.4 94.2 random ≥440 80.2 94.6\nDeiT-S MemNet ≤350 77.6 93.6 DeiT-B MemNet ≤190 79.9 94.5\nattention ≥1360 78.4 94.1 attention ≥440 80.6 94.8\nours ≥1360 79.1 94.5 ours ≥440 80.9 95.0\nvideo clips per GPU and adopt an SGD optimizer with an initial learning rate of 2.5e-3 in cosine\nstrategy [36]. We train most of our models using 16 NVIDIA Tesla V100-32GB GPUs.\n4.1 Emergence of Interpretability\nIn this section, we demonstrate the interpretability that emerges in our proposed method. We ﬁrst\nshow some qualitative results to show our method can better interpret where the informative region\nfor the correct prediction is. Then we provide the quantitative results of our weakly-supervised image\nsegmentation experiments on ImageNet which demonstrated that our method can better localize the\nsalient object on the input images.\nQualitative Evaluation. We visualize the output of the multi-head interpreter in the second network\ngroup to be our results, where we choose the DeiT-S model to be the testbed. Then, we compare our\nmethod with another two baselines: (1) Memorability map, generated by MemNet [29], models how\nmemorable is a certain region of the image with a concrete score ranging from zero to one. Intuitively,\na memorability map highlights the region which stimulates our brains more dramatically. (2) Raw\nattention, coming inherently with the pre-trained vision transformer, highlights the regions on the\nimage with more signiﬁcant attention weight. We generate the raw attention map by averaging the\nattention weights between the CLS token and the other patch tokens across all of the heads in the\nBlock_1, similar to the process in Eq. 1. We demonstrate the comparison of our method with the\ntwo baselines in Figure 3, from which we can see that our proposed method localizes the objects\nof interest, especially the part-level stuff, more accurately. From the third example in the second\ncolumn, we can see that the heatmap generated by our method combines the pattern of memorability\nmap which detects the truck head, while the attention map highlights more irrelevant regions, such\nas the cloud in the sky. In the example of the shark at the sixth row of the ﬁrst column, our method\naccurately localizes the ﬁn and the head, the two most informative features of the shark, which\nindicates that our method can better interpret the part-level stuff compared to the raw attention. Also,\nexamples of dogs and tigers at the seventh and eighth rows demonstrate that our method can detect\nthe features on the animal face, like eyes, noses, and ears.\nWeakly-supervised Image Segmentation. To quantitatively compare our method with other model\ninterpretability methods, we conduct the weakly-supervised image segmentation experiments on the\nImageNet-Segmentation [16] dataset. Besides the memorability map and the raw attention map, we\ncompare with LIME [42], gradient propagation method GradCAM [44] and Layer-wise Relevance\nPropagation method LRP [ 4]. The goal of weakly-supervised ImageNet-Segmentation task is to\npredict a precise mask of the objects of interest without pixel-level supervision, where a binary mask\nserved as the ground-truth label. We list the segmentation results in Table 1, from which we observe\nthat the proposed method IA-RED2 signiﬁcantly outperforms other methods. As a dedicated method\nfor the interpretability of CNNs, we ﬁnd that GradCAM does not perform well in this task. We guess\nthis is because there is a signiﬁcant gap between the interpretability of CNNs and vision transformers.\n4.2 Redundancy Reduction on Recognition Tasks\nIn this section, we conduct our experiments on top of the image recognition task and video action\nrecognition task. We demonstrate that our method can hierarchically reduce the redundancy of the\ninput patch tokens with both qualitative results and quantitative results.\nResults of Image Recognition. We compare with different baselines for dropping the input patch\ntokens, such as (1) random baseline, which randomly drops the path tokens at the input level, (2)\nMemNet baseline, which drops the patch tokens based on the memorability score of the corresponding\n7\nhigher levellower level higher levellower level\n99.5% 61.7% 32.1%\n91.8% 38.3% 23.0%\n98.5% 36.7% 10.2%\n97.4% 52.6% 25.5%\n98.0% 59.7% 53.1%\n99.0% 54.6% 12.2%\nFigure 4: We visualize the hierarchical redundancy reduction process of our method with the DeiT-S\nmodel. The number on the upper-left corner of each image indicates the ratio of the remaining\npatches. From left to right, we can see that the network drops the redundant patches and focuses\nmore on the high-level features of the objects. Best viewed in color.\npatch, (3) attention baseline, which drops the patch tokens based on the raw attention map inBlock_1.\nFor the baseline methods, we adjust the threshold to set the drop-out rate as 30%, thus there would\n30% of the patch tokens be discarded right after the positional embedding layer. Note that all of the\nbaseline methods need re-training. For the attention baseline, since the patches are dropped according\nto different strategies, we ﬁne-tune the backbone network to make it adapt to the fewer-patch case.\nThus our method will not increase training time compared to the baseline methods. We choose the\nmodel of our method with the similar inference speed of (1), which would be the baseline with\nthe fastest speed as it does not need any pre-process to the patch tokens, to fairly compare the\nperformance. We test the inference speed in terms of frames per second (fps) of each method on a\nsingle NVIDIA Tesla V100-32GB GPU with PyTorch 1.7 and CUDA 10.2. We list our quantitative\nresults in Table 2. In Figure 4, we visualize several examples of hierarchically dropping the patch\ntokens at different groups, where we can see that our model takes almost all of the tokens at the lower\nlevel of the network (the ﬁrst group), then drops the patches on the background and focuses on the\nentire object of interest at the middle level (the second group). And ﬁnally, it focuses on the part-level\nstuff of the object at the higher level (the third group).\nTo further verify the effectiveness of our approach, we compare with a teacher-student baseline, by\nfollowing the distillation process in DeiT [53]. We use DeiT-S as the teacher model and customize a\nstudent model by reducing the depth of the DeiT-S so that the student model contains only around\n70% FLOPs of the DeiT-S (similar to the FLOPs of our method applied to DeiT-S). We notice that\nthe student model achieves a top-1 accuracy of 76.0%, while our method got 79.1% top-1 accuracy\non ImageNet1K with additional beneﬁts from good model interpretability that shows what is the\ninformative region for the correct prediction of classiﬁcation.\nResults of Video Action Recognition. We further explore the redundancy reduction strategies on\nthe video action recognition task. Similar to the image task, we compare our method with (1) random\nbaseline and (2) attention map baseline. Besides these two, we devise the (3) temporal difference\nbaseline, which calculates the L2 distance between the patch tokens at T and (T −1) time step. For\nthose tokens that have a longer distance to the previous one, we assume they have larger entropy thus\nneed to be kept. For the patch tokens sampled from the initial frame, we set their previous tokens as\nzero. We list the results in Table 3. We can see that our method outperforms the attention baseline\nwhile worse than the random and temporal difference baseline. Although our method does not get the\nbest results of the four redundancy reduction methods, it learns to identify the informative patches\namong thousands of input patch tokens, which will be further illustrated in the the supplementary\nmaterial. We guess the reason why the random baseline performs better is that the input redundancy\nof video is signiﬁcantly higher than that in the image, which makes the model quite robust to random\npatch dropping as the similar technique is applied in the training process.\n8\nTable 3: Redundancy reduction results\nof our IA-RED 2 with TimeSformer on\nKinetics-400 [7]. We compare three base-\nline methods with our IA-RED2: (1) ran-\ndom, (2) attention map, and (3) temporal\ndifference. The speed (clip per second)\nof each method is measure on a single\nNVIDIA TESLA V100-32GB GPU. For\nthe performance, we report the clip-1 error\nand video-1 error, where lower is better.\nTo be consistent, we report our reproduced\nresults of the original TimeSformer as our\nKinetics-400 dataset does not include all\nof the original data.\nMethod speed clip-1 video-1\noriginal ≤24.0 28.2 23.8\nrandom ≥81.0 34.3 28.2\nattention ≥81.0 38.1 31.4\ntemp. diff. ≥81.0 32.3 26.9\nours ≥81.0 35.3 29.1\n0.1 0.2 0.3 0.4 0.5 0.6 0.7\n0\n20\n40\n60\n80\nTop − 1\nWeight pruning\nOurs\nOurs + Weight pruning\nreduction ratio\n80\n60\n20\n40\n0\n0.1 0.2 0.3 0.4 0.5 0.6 0.7\ntop-1 accuracy Weight pruning\nOurs\nOurs+Weight pruning\n(0.1, 79.8)\n(0.3, 79.0)\n80.0\n77.5\n78.7\n0.1 0.2 0.3\nFigure 5: We plot the redundancy reduction results of weight\npruning, our IA-RED2, and the combination of them, where\nthe X axis represents the FLOPs reduction ratio of all lin-\near layers in the DeiT-S, and Y axis represents the top-1\naccuracy on ImageNet. For a fair comparison, we do not\nﬁnetune the network after we reduce the redundancy. With\nthe combination of our IA-RED 2 and the weight pruning\nmethod, the model can be directly accelerated by 1.7×with-\nout ﬁnetuning while suffering only a 1.7% accuracy drop.\nComparison with Data-dependent Sparse Transformers. We conduct additional baseline experi-\nments by applying the Sparse Sinkhorn Attention [52] and Routing Transformer [43] to the DeiT-S\nmodel. Since the input sequential length to the DeiT-S model is ﬁxed to 197, we set the local\nwindow size of Sinkhorn Transformer and Routing Transformer to 197. We notice that the Sinkhorn\nTransformer model archives 77.9% top-1 accuracy with 720 fps inference speed, while Routing\nTransformer achieves 77.7% top-1 accuracy on ImageNet1k and 663 fps on an NVIDIA Tesla V100\nGPU. In contrast, our method obtains 79.1% top-1 accuracy with the inference speed of 1360 fps.\nFurthermore, we compare with Linformer [55] and observe that it only gets the top-1 accuracy of\n75.7% on ImageNet1k. These results show the efﬁcacy of IA-RED2 over existing data-dependent\nsparse transformers in reducing the redundancy of vision transformers.\nApplicability of IA-RED2. Our approach is model-agnostic, which allows it to serve as a plugin\noperation for a wide range of sequence-based vision transformer architectures. Our method can\nbe adopted to prune tokens in data-independent transformers like Swin [34], which adopt complex\nmodiﬁcations by introducing CNN-like local windows. But since the number of tokens in different\nlocal windows will be different after sparsiﬁcation, it can be hard to achieve additional speedup on\ntop of such models. However, our model can be easily improved by using a stronger backbone to\nprovide a better accuracy-speed trade-off compared to the Swin transformer. For example, with\nCaiT-S24-224 [54] as the backbone, we obtain 82.9% top-1 accuracy with only 7.5 GFLOPs (the\noriginal CaiT model got 83.5% top-1 accuracy with 9.4 GFLOPs), which is much better than the\nDeiT-B [53] (81.8% top-1 accuracy and 16.8 GFLOPs) and comparable to the Swin-S (83.3% top-1\naccuracy with 8.7 GFLOPs) and Swin-B (83.5% top-1 accuracy with 15.4 GFLOPs). Moreover, our\nmethod for interpretability does not require ﬁne-tuning of the original model. Results in Table 2\nand 3, do not include the ﬁne-tuning step, which is not essential in our method and is only used for\ngetting better accuracy-speed trade-offs. Since our method does not alter the weights of the original\nmodel, it is very convenient to use as a model interpretability method for vision transformers.\nIs Data-level Redundancy Orthogonal to the Model-level? Contrary to those works [19, 23, 35]\nprune the model-level redundancy, our approach seeks to reduce the data-level redundancy. To further\nstudy these two counterparts, we start by choosing the magnitude-based weight pruning approach [35]\nas the subject. The pruning method is applied to all of the FC layers in the transformer. We ﬁrst\nplot the trade-off curves between accuracy and efﬁciency of the weight pruning and our IA-RED2 in\nFigure 5, where we can see that our IA-RED2 outperforms the weight pruning especially when the\nFLOPs reduction ratio is high. Then we combine these two methods to see if they are complementary\nto each other: for each compression step, we choose the model with higher accuracy achieved by\neither increasing the weight pruning ratio or lifting the threshold of our multi-head interpreter to\n9\nTable 4: Redundancy reduction results of our IA-RED2 with the DeiT-Base in different resolutions.\nTo fairly compare the ratio of the redundancy patches, we keep the parameters of MSA-FFN modules\nthe same as the original and only optimize the multi-head interpreters.\nresolution method speed (fps) FLOPs avg Top-1 speedup ratio gap\n224×224 original 316.8 16.8 G 81.8 1.42× 1.5%\nours 452.5 11.8 G 80.3\n384×384 original 87.0 49.4 G 82.8 1.48× 0.9%\nours 129.6 34.7 G 81.9\nTable 5: Results of weakly-supervised image segmentation and image classiﬁcation using different\nblocks. We use our method based on the training with DeiT-S model.\nBlocks Weakly-Supervised Segmentation Image Classiﬁcation\npixel accuracy mAcc mIoU FLOPs Top-1 Top-5\nBlock_0 68.3 50.5 37.1 2.9 G 78.6 94.2\nBlock_1 67.9 61.8 46.4 3.3 G 78.4 94.1\nBlock_2 71.9 55.7 42.6 3.6 G 79.3 94.6\nreduce more redundant tokens. We observe that the combined approach achieves the best trade-off,\nsuggesting that the proposed data-level redundancy is orthogonal to the model-level redundancy, and\nour method is complementary to the weight pruning method.\nDoes the Higher Resolution has More Redundancy? From Figure 4, we can see the redundancy\nvaries depending on the input instance: images with more background or small features which can\nidentify the object tend to have more redundancy while the images with more complicated object\nalways need more computation. Intuitively, as the redundancy is input-data-dependent, the input data\nwith higher resolution would contain more redundancy. To validate this, we conduct the ablation\nstudy in Table 4, where we keep the reduction ratio of the computational cost the same and compare\nthe accuracy loss of the models in different resolutions. We ﬁnd that the model which takes higher\nresolution suffers a lower performance decrease than that with the lower resolution, which supports\nthat input data in higher resolution tends to contain more redundancy.\nEffect of the Number of Groups D. We conduct an ablation study on the number of groups D= 2,\nD= 3, and D= 4on ImageNet-1K dataset. Under the same level of computational budget (∼2.9\nGFLOPs), we ﬁnd that the 3-group framework used in our approach (79.1% top-1 accuracy) performs\nslightly better than the 2-group (78.6% top-1 accuracy) and the 4-group (78.8% top-1 accuracy)\nframework. All of them have good trade-offs between accuracy and speed.\nEffect of Different Blocks in Raw Attention Baseline. We provide both the segmentation results\nand classiﬁcation results of Block_0, Block_1, and Block_2 in Table 5. From the segmentation\nresults, we can see that the attention map of Block_1 outperforms Block_0 and Block_2 by a large\nmargin in terms of mean accuracy and mean IoU. That’s why we chose Block_1 as our baseline.\nFrom the classiﬁcation results, we can see that Block_1 and Block_0 have similar performance on\nthe classiﬁcation task on ImageNet-1k. However, Block_1 suffers a slightly higher computational\ncost. Block_2 performs the best, but to get the attention map of Block_2, we need to forward\ntwo full blocks of the original vision transformer, which introduces additional computational cost.\nAdditional results and analysis including more visualizations are included in the supplementary\nmaterial.\n5 Conclusions\nIn this work, we propose a novel interpretability-aware redundancy reduction framework for the\nrecent vision transformer, named IA-RED 2 . We show that IA-RED 2 hierarchically reduces the\ncomputational cost and speeds up the vision transformer effectively with human-understandable\ntrajectories. Experiments are conducted on image classiﬁcation and video understanding tasks,\nwhere the proposed IA-RED 2 is demonstrated to be both model-agnostic and task-agnostic. We\nﬁnally compare our IA-RED2 with the model compression approaches, such as weight pruning, to\ndemonstrate the complementarity between them.\n10\nAcknowledgements. We thank IBM for the donation to MIT of the Satori GPU cluster. This work is\nsupported by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside.\nReferences\n[1] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computa-\ntion in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\n[2] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\nvideo understanding? arXiv preprint arXiv:2102.05095, 2021.\n[4] Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, Klaus-Robert Müller, and Woj-\nciech Samek. Layer-wise relevance propagation for neural networks with local renormalization\nlayers. In International Conference on Artiﬁcial Neural Networks, pages 63–71. Springer, 2016.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, pages 213–229. Springer, 2020.\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294, 2021.\n[7] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the\nkinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6299–6308, 2017.\n[8] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization.\narXiv preprint arXiv:2012.09838, 2020.\n[9] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[10] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer.arXiv preprint\narXiv:2012.00364, 2020.\n[11] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing\nsparsity in vision transformers: An end-to-end exploration. arXiv preprint arXiv:2106.04533,\n2021.\n[12] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classiﬁers. arXiv\npreprint arXiv:1705.07857, 2017.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227,\n2021.\n[15] Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint\narXiv:1603.08983, 2016.\n[16] Matthieu Guillaumin, Daniel Küttel, and Vittorio Ferrari. Imagenet auto-annotation with\nsegmentation propagation. International Journal of Computer Vision, 110(3):328–348, 2014.\n[17] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min\nHu. Pct: Point cloud transformer. arXiv preprint arXiv:2012.09688, 2020.\n11\n[18] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021.\n[19] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149,\n2015.\n[20] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic\nneural networks: A survey. arXiv preprint arXiv:2102.04906, 2021.\n[21] Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the\nlasso and generalizations. Chapman and Hall/CRC, 2019.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[23] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural\nnetworks. In Proceedings of the IEEE International Conference on Computer Vision, pages\n1389–1397, 2017.\n[24] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\n[25] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic\nbert with adaptive width and depth. arXiv preprint arXiv:2004.04037, 2020.\n[26] Ting-Kuei Hu, Tianlong Chen, Haotao Wang, and Zhangyang Wang. Triple wins: Boosting ac-\ncuracy, robustness and efﬁciency together by enabling input-adaptive inference. InInternational\nConference on Learning Representations, 2020.\n[27] Drew A Hudson and C Lawrence Zitnick. Generative adversarial transformers. arXiv preprint\narXiv:2103.01209, 2021.\n[28] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two transformers can make one\nstrong gan. arXiv preprint arXiv:2102.07074, 2021.\n[29] Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. Understanding and predicting\nimage memorability at a large scale. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 2390–2398, 2015.\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. Advances in neural information processing systems, 25:1097–\n1105, 2012.\n[32] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun\nChang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural\narchitecture search. arXiv preprint arXiv:2103.12424, 2021.\n[33] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Proceedings of the\n31st International Conference on Neural Information Processing Systems, pages 2178–2188,\n2017.\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n[35] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.\nLearning efﬁcient convolutional networks through network slimming. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 2736–2744, 2017.\n12\n[36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\npreprint arXiv:1608.03983, 2016.\n[37] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid Karlinsky, Aude\nOliva, Kate Saenko, and Rogerio Feris. Ar-net: Adaptive frame resolution for efﬁcient action\nrecognition. In European Conference on Computer Vision, pages 86–104. Springer, 2020.\n[38] Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid Karlinsky, Kate\nSaenko, Aude Oliva, and Rogerio Feris. Adafuse: Adaptive temporal fusion network for efﬁcient\naction recognition. arXiv preprint arXiv:2102.05775, 2021.\n[39] Bowen Pan, Rameswar Panda, Camilo Fosco, Chung-Ching Lin, Alex Andonian, Yue Meng,\nKate Saenko, Aude Oliva, and Rogerio Feris. Va-red2: Video adaptive redundancy reduction.\narXiv preprint arXiv:2102.07887, 2021.\n[40] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dy-\nnamicvit: Efﬁcient vision transformers with dynamic token sparsiﬁcation. arXiv preprint\narXiv:2106.02034, 2021.\n[41] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-\ncritical sequence training for image captioning. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 7008–7024, 2017.\n[42] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining\nthe predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international\nconference on knowledge discovery and data mining, pages 1135–1144, 2016.\n[43] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based\nsparse attention with routing transformers. Transactions of the Association for Computational\nLinguistics, 9:53–68, 2021.\n[44] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi\nParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based\nlocalization. In Proceedings of the IEEE international conference on computer vision, pages\n618–626, 2017.\n[45] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\nVisualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034,\n2013.\n[46] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smooth-\ngrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n[47] Suraj Srinivas and François Fleuret. Full-gradient representation for neural network visualization.\narXiv preprint arXiv:1905.00780, 2019.\n[48] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\neffectiveness of data in deep learning era. In Proceedings of the IEEE international conference\non computer vision, pages 843–852, 2017.\n[49] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In\nInternational Conference on Machine Learning, pages 3319–3328. PMLR, 2017.\n[50] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[51] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and Dacheng Tao.\nPatch slimming for efﬁcient vision transformers. arXiv preprint arXiv:2106.02852, 2021.\n[52] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention.\nIn International Conference on Machine Learning, pages 9438–9447. PMLR, 2020.\n[53] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n13\n[54] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.\nGoing deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[55] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[56] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[57] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning\ndynamic routing in convolutional networks. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 409–424, 2018.\n[58] Yue Wang, Jianghao Shen, Ting-Kuei Hu, Pengfei Xu, Tan Nguyen, Richard Baraniuk,\nZhangyang Wang, and Yingyan Lin. Dual dynamic inference: Enabling more efﬁcient, adap-\ntive, and controllable deep inference. IEEE Journal of Selected Topics in Signal Processing,\n14(4):623–633, 2020.\n[59] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus:\na dynamic approach to reducing spatial redundancy in image classiﬁcation. arXiv preprint\narXiv:2010.05300, 2020.\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[61] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen\nGrauman, and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages\n8817–8826, 2018.\n[62] Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, and Chuang Zhang. Contnet: Why\nnot use convolution and transformer at the same time? arXiv preprint arXiv:2104.13497, 2021.\n[63] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptive\nnetworks for efﬁcient inference. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 2369–2378, 2020.\n[64] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating\nconvolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021.\n[65] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.\narXiv preprint arXiv:2101.11986, 2021.\n[66] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer.arXiv\npreprint arXiv:2012.09164, 2020.\n[67] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\nsequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[68] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020.\n14\nA Pseudo Code of Our Training Process\nTo make our training process clearer, we present the details of our training process by pseudo code in\nAlg. 1. We take the training of DeiT-S for example, where the D= 3. For each groups, we spend 10\nepochs to train the multi-head interpreter and then 20 epochs to train the rest MSA-FFN blocks.\nAlgorithm 1 Optimize multi-head interpreters and MSA-FFN blocks on DeiT-S.\nRequire: A token sequence X right after the positional embedding and its label Y.\nfor i←1 to Ddo\nfor j ←1 to 10 do\nfor each iteration do\nR←Reward(X, Y | W1:i\np , Wb)\nCompute_Policy_Gradient(R)\nWi\np ←Update_Parameters(Wi\np)\nend for\nend for\nfor j ←11 to 30 do\nfor each iteration do\nL←CrossEntropyLoss(X, Y | W1:i\np , Wb)\nCompute_Gradient(L)\nWi:D\nb ←Updated_Parameters(Wi:D\nb )\nend for\nend for\nend for\nwhere Dis the number of the groups we deﬁned in Section 3 of the main paper, Wp denotes the\nparameters of the multi-head interpreters, Wb denotes the parameters of the MSA-FFN blocks.\nB Discussion on the Training Time of Our Method\nThe 90-epoch training for DeiT-S model takes around 4.5 hours using 24 NVIDIA Tesla V100-32GB\nGPUs. For one third of all the epochs, we train the multi-head interpreters using REINFORCE, which\ndoes not require gradients for the backbone network and saves a lot of computation.\nC Random Baseline with Different Seeds\nTo understand how different seeds affect the experiment results, we provide the results of random\ndropping and dropping with our learned policy with DeiT-S using four random seeds in the table\nbelow. We can see that our method consistently outperforms the random baseline with different seeds.\nTable 6: The performance of the random baseline and our method with different seeds.\nmethod Top-1 (s1) Top-1 (s2) Top-1 (s3) Top-1 (s4) Average Top-1\nrandom 78.4% 78.3% 78.5% 78.3% 78.4%\nOurs 79.1% 78.8% 79.0% 79.2% 79.0%\nD REINFORCE vs. Straight-through Gumbel\nWe also explore training with straight-through Gumbel instead of REINFORCE to be part of our\napproach. However, we ﬁnd that Gumbel does not consistently highlight the informative region. In\nmost cases, it highlights the background region instead of foreground objects. Here we provide the\ncomparison of the REINFORCE method and straight-through Gumbel method on the DeiT-S model.\nUnder the same level of FLOPs of 3.0G (Gumbel) versus 2.9G (REINFORCE), the Top-1 accuracy\non ImageNet-1K dataset are 78.8% and 79.1% respectively. The results of the Gumbel method are\n15\nobtained by discarding the patch tokens which have relatively higher softmax value due to the fact\nthat, in Gumbel method, the background region tends to have higher softmax value.\nE Effect of Threshold in Discarding Tokens\nWe vary the threshold ofIi,j to 0.48, 0.49, 0.50, 0.51, and 0.52, to see how the performance of the\nDeiT-B model would change. The results are shown in Table 7, where we ﬁnd that with a higher\nthreshold, we get a more efﬁcient model. While lowering the threshold, we get a more accurate model.\nThus the threshold of Ii,j can be regarded as a trade-off factor between accuracy and efﬁciency.\nTable 7: The performance of the DeiT-B model with different thresholds in discarding tokens.\nThreshold 0.48 0.49 0.50 0.51 0.52\nFLOPs 16.5 G 15.3 G 11.8 G 8.2 G 4.9 G\nTop-1 81.7% 81.5% 80.9% 77.5% 63.7%\nF Ablation Study on Square Reward and Insights on τ\nWe jointly study the effect of replacing the squared reward with linear and changing the value of\nτ in Eq. 2 in the table below. We can see from that table that, by changing τ we can get different\ntrade-offs between accuracy and efﬁciency. Also, without squared reward, we can see that the\naccuracy-efﬁciency trade-offs will be more sensitive to the changing of the τ.\nTable 8: The effect of square reward and different τ.\nτ 0.5 1.0 1.5 0.5 1.0 1.5\nsquare reward Yes Yes Yes No No No\nTop-1 76.0% 78.1% 79.1% 12.0% 70.9% 78.2%\nFLOPs 2.5 G 2.9 G 3.4 G 0.4 G 2.2 G 3.1 G\nG More Interpretability Results and Demo Tool\nIn this section, we present more visualization results on both image and video tasks. We plot more\ninterpretability results of our method in Figure 6. Then, we show more examples of hierarchical\nredundancy reduction process in Figure 7. Finally, in Figure 8, we visualize the input redundancy\nreduction results of our method on the video action recognition task, where we experiments with the\nJointST TimeSformer [3] on the Kinetics-400 dataset [7].\nWe further provide an interpretation tool for the reader who want to play the interpretability of\nour model. The usage of the tool is quite simple: python interpreter.py -p {image_path}\n-o {output_dir}. An environment with Python==3.6 (or above), torch==1.7 (or above) and\ntimm==0.3.2 (or above) installed is required to run the tool.\nH Broader Impact\nOur work eases the suffering of heavy computational cost for the vision transformer, which could save\nmore energy and reduce the carbon emissions for the industry. The interpretability which emerges in\nour method help we human to understand what happening inside the vision transformer. However,\nthe potential negative impact would be that, since our method makes neural networks easier to run\nand more understandable to everyone, it may cause the abuse of AI technology.\n16\noriginal memorability attention ours original memorability attention ours\nFigure 6: We visualize more examples with heatmaps which hightlight the informative region of the\ninput images of MemNet, raw attention at the second block, and our method with DeiT-S model.\n17\n93.4% 48.5% 21.9% 93.9% 31.1% 17.9%\nhigher levellower level higher levellower level\n90.8% 24.5% 9.7% 92.3% 36.2% 18.9%\n90.8% 56.1% 35.2% 87.2% 42.3% 28.0%\n96.4% 56.1% 33.7% 98.5% 56.6% 40.3%\n95.4% 35.7% 25.5% 98.5% 25.0% 15.8%\nFigure 7: More examples of our hierarchical redundancy reduction process of our method with\nDeiT-S model. The number on the upper-left corner of each image indicates the ratio of the remaining\npatches. Best viewed in color.\n18\nt=0 t=T\n49.0% 45.6% 50.5% 45.4% 47.4% 50.5% 49.0% 50.0%\n38.8% 36.2% 40.8% 59.2% 58.1% 67.3% 61.7% 64.3%\n69.9% 71.9% 65.8% 69.4% 66.3% 69.9% 63.3% 63.3%\n50.0% 49.0% 50.0% 47.4% 49.5% 50.0% 50.0% 47.4%\nFigure 8: We visualize the redundancy results of our method with the TimeSformer model. The\nnumber on the upper-left corner of each image indicates the ratio of the remaining patches. We can\nsee that our method manages to ﬁlter the redundant patches and keeps the informative patches which\nare important for the ﬁnal prediction.\n19",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8808445930480957
    },
    {
      "name": "Transformer",
      "score": 0.644493043422699
    },
    {
      "name": "Computer science",
      "score": 0.6425281763076782
    },
    {
      "name": "Computation",
      "score": 0.57342928647995
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5561836957931519
    },
    {
      "name": "Uncorrelated",
      "score": 0.44889312982559204
    },
    {
      "name": "Machine learning",
      "score": 0.3610261082649231
    },
    {
      "name": "Computer vision",
      "score": 0.32389992475509644
    },
    {
      "name": "Algorithm",
      "score": 0.19117382168769836
    },
    {
      "name": "Voltage",
      "score": 0.14267507195472717
    },
    {
      "name": "Engineering",
      "score": 0.13040095567703247
    },
    {
      "name": "Mathematics",
      "score": 0.09878513216972351
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 68
}