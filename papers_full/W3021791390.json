{
  "title": "DomBERT: Domain-oriented Language Model for Aspect-based Sentiment Analysis",
  "url": "https://openalex.org/W3021791390",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2025973412",
      "name": "Xu Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990217128",
      "name": "Liu Bing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973867633",
      "name": "Shu, Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221445183",
      "name": "Yu, Philip S.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799044502",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2412751481",
    "https://openalex.org/W3152231500",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2911437461",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2789190634",
    "https://openalex.org/W2950353181",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2108646579",
    "https://openalex.org/W2740899359",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962741379",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2310041076",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2604205681",
    "https://openalex.org/W2756816896",
    "https://openalex.org/W2251792193",
    "https://openalex.org/W2251124635",
    "https://openalex.org/W2962676330",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2151752537",
    "https://openalex.org/W2963936679",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2875308690",
    "https://openalex.org/W2030866871",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2965510113",
    "https://openalex.org/W2963494756",
    "https://openalex.org/W2798933146",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2963274454",
    "https://openalex.org/W2979155592",
    "https://openalex.org/W2757541972",
    "https://openalex.org/W2963240575",
    "https://openalex.org/W2165698076"
  ],
  "abstract": "This paper focuses on learning domain-oriented language models driven by end tasks, which aims to combine the worlds of both general-purpose language models (such as ELMo and BERT) and domain-specific language understanding. We propose DomBERT, an extension of BERT to learn from both in-domain corpus and relevant domain corpora. This helps in learning domain language models with low-resources. Experiments are conducted on an assortment of tasks in aspect-based sentiment analysis, demonstrating promising results.",
  "full_text": "DomBERT: Domain-oriented Language Model\nfor Aspect-based Sentiment Analysis\nHu Xu1, Bing Liu1, Lei Shu1 and Philip S. Yu1,2\n1Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA\n2Institute for Data Science, Tsinghua University, Beijing, China\n{hxu48, liub, lshu3, psyu}@uic.edu\nAbstract\nThis paper focuses on learning domain-\noriented language models driven by end tasks,\nwhich aims to combine the worlds of both\ngeneral-purpose language models (such as\nELMo and BERT) and domain-speciﬁc lan-\nguage understanding. We propose DomBERT,\nan extension of BERT to learn from both\nin-domain corpus and relevant domain cor-\npora. This helps in learning domain lan-\nguage models with low-resources. Experi-\nments are conducted on an assortment of tasks\nin aspect-based sentiment analysis, demon-\nstrating promising results. 1.\n1 Introduction\nPre-trained language models (LMs) (Peters et al.,\n2018; Radford et al., 2018, 2019; Devlin et al.,\n2019) aim to learn general (or mixed-domain)\nknowledge for end tasks. Recent studies (Xu et al.,\n2019; Gururangan et al., 2020) show that learn-\ning domain-speciﬁc LMs are equally important be-\ncause general-purpose LMs lack enough focus on\ndomain details. This is partially because the train-\ning corpus of general LMs is out-of-domain for\ndomain end tasks and, more importantly, because\nmixed-domain weights may not capture the long-\ntailed and under represented domain details (Xu\net al., 2018a) (see Section 4). An intuitive example\ncan be found in Table 1, where all masked words\nsky, water, idea, screen and picture can appear in\na mixed-domain corpus. A general-purpose LM\nmay favor frequent examples and ignore long-tailed\nchoices in certain domains.\nIn contrast, although domain-speciﬁc LMs can\ncapture ﬁne-grained domain details, they may suf-\nfer from insufﬁcient training corpus (Gururan-\ngan et al., 2020) to strengthen general knowledge\n1Work in progress.\nExample Domain\nThe[MASK]is clear .\nTheskyis clear . Astronomy [Irrelevant Domain]\nThewateris clear . Liquids [Irrelevant Domain]\nTheideais clear . Concepts [Irrelevant Domain]\nThescreenis clear . Desktop [Relevant Domain]\nThepictureis clear . Laptop [Target Domain]\nTable 1: Multiple choices of a masked token from dif-\nferent domains lead to confusing ground-truths and the\nhardness of ﬁnding a general updates of parameters in\nmasked language model.\nwithin a domain. To this end, we propose a domain-\noriented learning task that aims to combine the ben-\neﬁts of both general and domain-speciﬁc world:\nDomain-oriented Learning: Given a target do-\nmain tand a set of diverse source domains S =\n{s1,s2,... }, perform (language model) learning\nthat focusing on tand all its relevant domains in S.\nThis learning task resolves the issues that exist\nin both general and domain-speciﬁc worlds. On\none hand, the training of LM does not need to focus\non unrelated domains anymore (e.g., Books is one\nbig domain in Amazon but a major focus on Books\nmay not be very helpful for end tasks inLaptop); on\nthe other hand, although an in-domain corpus may\nbe limited, other relevant domains can share a great\namount of knowledge (e.g., Desktop in Table 1) to\nmake in-domain corpus more diverse and general.\nThis paper proposes an extremely simple ex-\ntension of BERT (Devlin et al., 2019) called\nDomBERT to learn domain-oriented language mod-\nels. DomBERT divides a mixed-domain corpus by\ndomain tags and learns to re-balance the training ex-\namples for the target domain. Similar to other LMs,\nwe categorize DomBERT as a self-supervised learn-\ning model because domain tags naturally exist on-\nline and do not require human annotations for a spe-\nciﬁc task2, ranging from Wikipedia, news articles,\nblog posts, QAs, to customer reviews. DomBERT\n2Supervised learning needs extra human annotations.\narXiv:2004.13816v1  [cs.CL]  28 Apr 2020\nsimultaneously learns masked language modeling\nand discovers relevant domains to draw training\nexamples, where the later are computed from do-\nmain embeddings learned from an auxiliary task of\ndomain classiﬁcation. We apply DomBERT to end\ntasks in aspect-based sentiment analysis (ABSA)\nin low-resource settings, demonstrating promising\nresults.\nThe main contributions of this paper are in 3-\nfold:\n1. We propose the task of domain-oriented learn-\ning, which aims to learn language models fo-\ncusing on a target and its relevant domains.\n2. We propose DomBERT, which is an extension\nof BERT with the capability to draw examples\nfrom relevant domains from a pool of diverse\ndomains.\n3. Experimental results demonstrate that\nDomBERT is promising in low-resource\nsettings for aspect-based sentiment analysis.\n2 Related Work\nPre-trained language models gain signiﬁ-\ncant improvements over a wide spectrum of\nNLP tasks, including ELMo(Peters et al.,\n2018), GPT/GPT2(Radford et al., 2018, 2019),\nBERT(Devlin et al., 2019), XLNet(Yang et al.,\n2019), RoBERTa(Liu et al., 2019), ALBERT(Lan\net al., 2019), ELECTRA(Clark et al., 2019).\nThis paper extends BERT’s masked language\nmodel (MLM) with domain knowledge learning.\nFollowing RoBERTa, the proposed DomBERT\nleverages dynamic masking, removes the next\nsentence prediction (NSP) task (which is proved to\nhave negative effects on pre-trained parameters),\nand allows for max-length MLM to fully utilize\nthe computational power. This paper also borrows\nALBERT’s removal of dropout since pre-trained\nLM, in general, is an underﬁtting task that requires\nmore parameters instead of avoiding overﬁtting.\nThe proposed domain-oriented learning task can\nbe viewed as one type of transfer learning(Pan and\nYang, 2009), which learns a transfer strategy implic-\nitly that transfer training examples from relevant\n(source) domains to the target domain. This trans-\nfer process is conducted throughout the training\nprocess of DomBERT.\nThe experiment of this paper focuses on aspect-\nbased sentiment analysis (ABSA), which typically\nrequires a lot of domain-speciﬁc knowledge. Re-\nviews serve as a rich resource for sentiment analy-\nsis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012,\n2015). ABSA aims to turn unstructured reviews\ninto structured ﬁne-grained aspects (such as the\n“battery” or aspect category of a laptop) and their\nassociated opinions (e.g., “good battery” is posi-\ntive about the aspect battery). This paper focuses\non three (3) popular tasks in ABSA: aspect extrac-\ntion (AE), aspect sentiment classiﬁcation (ASC)\n(Hu and Liu, 2004) and end-to-end ABSA (E2E-\nABSA) (Li et al., 2019a,c). AE aims to extract\naspects (e.g., “battery”), ASC identiﬁes the polar-\nity for a given aspect (e.g., positive for battery) and\nE2E-ABSA is a combination of AE and ASC that\ndetects the aspects and their associated polarities\nsimultaneously.\nAE and ASC are two important tasks in senti-\nment analysis (Pang et al., 2002; Liu, 2015). It\nis different from document or sentence-level senti-\nment classiﬁcation (SSC) (Pang et al., 2002; Kim,\n2014; He and Zhou, 2011; He et al., 2011) as it\nfocuses on ﬁne-grained opinion on each speciﬁc\naspect (Shu et al., 2017; Xu et al., 2018b). It\nis either studied as a single task or a joint learn-\ning end-to-end task together with aspect extraction\n(Wang et al., 2017; Li and Lam, 2017; Li et al.,\n2019b). Most recent works widely use neural net-\nworks(Dong et al., 2014; Nguyen and Shirai, 2015;\nLi et al., 2018a). For example, memory network\n(Weston et al., 2014; Sukhbaatar et al., 2015) and\nattention mechanisms are extensively applied to\nASC (Tang et al., 2016; Wang et al., 2016a,b; Ma\net al., 2017; Chen et al., 2017; Ma et al., 2017;\nTay et al., 2018; He et al., 2018a; Liu et al., 2018).\nASC is also studied in transfer learning or domain\nadaptation settings, such as leveraging large-scale\ncorpora that are unlabeled or weakly labeled (e.g.,\nusing the overall rating of a review as the label)\n(Xu et al., 2019; He et al., a) and transferring from\nother tasks/domains (Li et al., 2018b; Wang et al.,\n2018a,b). Many of these models use handcrafted\nfeatures, graph structures, lexicons, and compli-\ncated neural network architectures to remedy the\ninsufﬁcient training examples from both tasks. Al-\nthough these approaches may achieve better per-\nformances by manually injecting human knowl-\nedge into the model, this paper aims to improve\nABSA from leveraging unlabeled data via a self-\nsupervised language modeling manner (Xu et al.,\n2018a; He et al., 2018b; Xu et al., 2019).\n3 DomBERT\nThis section presents DomBERT, which is an ex-\ntension of BERT for domain knowledge learning.\nWe adopt post-training of BERT (Xu et al., 2019)\ninstead of training DomBERT from scratch since\npost-training on BERT is more efﬁcient3. Different\nfrom (Xu et al., 2019), the main goal of domain-\noriented training is that it leverages both an in-\ndomain corpus and a pool of corpora of source\ndomains.\nThe goal of DomBERT is to discover relevant\ndomains from the pool of source domains and uses\nthe training examples from relevant source domains\nfor masked language model learning. As a result,\nDomBERT has a sampling process over a categori-\ncal distribution on all domains (including the target\ndomain) to retrieve relevant domains’ examples.\nLearning such a distribution needs to detect the do-\nmain similarities between all source domains and\nthe target domain. DomBERT learns an embed-\nding for each domain and computes such similari-\nties. The domain embeddings are learned from an\nauxiliary task called domain classiﬁcation.\n3.1 Domain Classiﬁcation\nGiven a pool of source and target domains, one can\neasily form a classiﬁcation task on domain tags.\nAs such, each text document has its domain label\nl. Following RoBERTa(Liu et al., 2019)’s max-\nlength training examples, we pack different texts\nfrom the same domain up to the maximum length\ninto a single training example.\nWe let the number of source domains be|S|= n.\nThen the total number of domains (including the tar-\nget domain) is n+ 1. Let h[CLS] denote the hidden\nstate of the [CLS] token of BERT, which indicates\nthe document-level representations of one exam-\nple. We ﬁrst pass this hidden states into a dense\nlayer to reduce the size of hidden states. Then we\npass this reduced hidden states to a dense layer\nD ∈R(n+1)∗m to compute the logits over all do-\nmains ˆl:\nˆl= D ·(W ·h[CLS] + b), (1)\nwhere mis the size of the dense layer, D, W and\nbare trainable weights. Besides a dense layer, D is\nessentially a concatenation of domain embeddings:\n3We aim for single GPU training for all models in this\npaper and use uncased BERTBASE given its lower costs of\ntraining for academic purpose.\nD = dt ◦d1 ◦···◦ dn. Then we apply cross-\nentropy loss to the logits and label to obtain the\nloss of domain classiﬁcation.\nLCLS = CrossEntropyLoss(ˆl,l). (2)\nTo encourage the diversity of domain embeddings,\nwe further compute a regularizer among domain\nembeddings as following:\n∆ = 1\n|D|2 ||cos(D,DT ) −I||2\n2. (3)\nMinimizing this regularizer encourages the learned\nembeddings to be more orthogonal (thus diverse)\nto each other. Finally, we add the loss of domain\nclassiﬁcation, BERT’s masked language model and\nregularizer together:\nL= λLMLM + (1 −λ)LCLS + ∆, (4)\nwhere λ controls the ratio of losses between\nmasked language model and domain classiﬁcation.\n3.2 Domain Sampler\nAs a side product of domain classiﬁcation,\nDomBERT has a built-in data sampling process\nto draw examples from both the target domain and\nrelevant domains for future learning. This pro-\ncess follows a uniﬁed categorical distribution over\nall domains, which ensures a good amount of ex-\namples from both the target domains and relevant\ndomains are sampled. As such, it is important to\nalways have the target domain twith the highest\nprobability for sampling.\nTo this end, we use cosine similarity as the sim-\nilarity function, which has the property to always\nlet cos(dt,dt) = 1 . For an arbitrary domain i,\nthe probability Pi of domain i being sampled is\ncomputed from a softmax function over domain\nsimilarities as following:\nPi = exp (cos(dt,di)/τ)∑n+1\nj=0 exp (cos(dt,dj)/τ)\n, (5)\nwhere τ is the temperature (Hinton et al., 2015) to\ncontrol the importance of highly-ranked domains\nvs long-tailed domains.\nTo form a mini-batch for the next training step,\nwe sample domains following the categorical distri-\nbution of s∼P up to the batch size and retrieve the\nnext available example from each sampled domain.\nAs such, we maintain a randomly shufﬂed queue of\nexamples for each domain. When the examples of\none domain are exhausted, a new randomly shuf-\nﬂed queue will be generated for that domain. As a\nresult, we implement a data sampler that takes D\nas inputs.\n3.3 Implementation Details\nWe adopt the popular transformers framework from\nhugging face4, with the following minor improve-\nments.\nEarly Apply of Labels: We refactor the forward\ncomputation of BERT MLM by a method called to\nearly apply of labels (EAL), which leverages labels\nof MLM in an early stage of forwarding compu-\ntation to avoid computation for invalid positions.\nAlthough MLM just uses 15% of tokens for pre-\ndiction, the implementations of BERT MLM still\ncomputes the logits over the vocabulary for all po-\nsitions, which is a big waste of both GPU computa-\ntion and memory footprint (because it is expensive\nto multiply hidden states with word embeddings\nof vocabulary size). EAL only uses positions that\nneed prediction when computing logits for each\ntoken5. This improves the speed of training to 3.2\nper second from 2.2 per second for BERTBASE. A\nsimilar method can be applied to compute the cross-\nentropy based loss for one token (because only the\nlogit for the ground-truth token contributes to the\nloss), which is potentially useful for almost all tasks\nwith large vocabulary size.\nDropout Removal : Following ALBERT(Lan\net al., 2019), we turn off dropout for post-training\nbecause BERT is unlikely to overﬁt to large train-\ning corpus. This gives both a larger capacity of\nparameters and faster training speed. The dropout\nis turned back on during end-task ﬁne-tuning be-\ncause BERT is typically over-parameterized for\nend-tasks.\n4 Experiments\n4.1 End Task Datasets\nWe apply DomBERT to end tasks in aspect-\nbased sentiment analysis from the SemEval dataset,\nwhich focusing on Laptop, Restaurant. Statistics of\ndatasets for AE, ASC and E2E-ABSA are given in\nTable 2, 3 and ??, respectively. For AE, we choose\nSemEval 2014 Task 4 for laptop and SemEval-2016\nTask 5 for restaurant to be consistent with (Xu et al.,\n2018a) and other previous works. For ASC, we use\n4https://huggingface.co/transformers/\n5We use torch.masked select in PyTorch.\nLaptop Restaurant\nTraining\nSentence 3045 2000\nAspect 2358 1743\nTesting\nSentence 800 676\nAspect 654 622\nTable 2: Summary of datasets on aspect extraction.\nLaptop Restaurant\nTraining\nPositive 987 2164\nNegative 866 805\nNeutral 460 633\nTesting\nPositive 341 728\nNegative 128 196\nNeutral 169 196\nTable 3: Summary of datasets on aspect sentiment clas-\nsiﬁcation.\nSemEval 2014 Task 4 for both laptop and restaurant\nas existing research frequently uses this version.\nWe use 150 examples from the training set of all\nthese datasets for validation. For E2E-ABSA, we\nadopt the formulation of (Li et al., 2019a) where\nthe laptop data is from SemEval-2014 task 4 and\nthe restaurant domain is a combination of SemEval\n2014-2016.\n4.2 Domain Corpus\nBased on the domains of end tasks from SemEval\ndataset, we explore the capability of the large-scale\nunlabeled corpus from Amazon review datasets(He\nand McAuley, 2016) and Yelp dataset 6. Follow-\ning (Xu et al., 2019), we select all laptop reviews\nfrom the electronics department. This ends with\nabout 100 MB corpus. Similarly, we simulate a\nlow-resource setting for restaurants and randomly\nselect about 100 MB reviews tagged as Restau-\nrants as their ﬁrst category from Yelp reviews. For\nsource domains S, we choose all reviews from the\n5-core version of Amazon review datasets and all\nYelp reviews excluding Laptop and Restaurants.\nNote that Yelp is not solely about restaurants but\nhas other location-based domains such as car ser-\nvice, bank, theatre etc. This ends with a total of\n|D|= 4680 domains, and n= 4679 are source do-\nmains. The total size of the corpus is about 20 GB.\nThe number of examples for each domain is plotted\nin Figure 1, where the distribution of domains is\nheavily long-tailed.\n6https://www.yelp.com/dataset/\nchallenge, 2019 version.\nLaptop Restaurant\nTraining\nPositive 987 2407\nNegative 860 1035\nNeutral 450 664\nTesting\nPositive 339 1524\nNegative 130 500\nNeutral 165 263\nTable 4: Summary of datasets on end-to-end aspect-\nbased sentiment analysis.\nFigure 1: Rank of domains by number of examples.\n4.3 Hyper-parameters\nWe adopt BERTBASE (uncased) as the basis for\nall experiments due to the limits of computational\npower in our academic setting and the purpose of\nmaking reproducible research. We choose the hid-\nden size of domain embeddings m= 64 to ensure\nthe regularizer term in the loss doesn’t consume\ntoo much GPU memory. We choose τ = 0 .13\nand λ = 0 .9. We leverage FP16 computation 7\nto reduce the actual size of tensors on GPU and\nspeed up the training. We train with FP16-O2 opti-\nmization, which has faster speed and smaller GPU\nmemory footprint compared to O1 optimization.\nDue to the uncertainty from the online sampling of\nDomBERT, we assume the number of training ex-\namples per epoch as the number of examples in the\ntarget domains. As a result, we train DomBERT for\n400 epochs to get enough training examples from\nrelevant domains. The full batch size is set to 288\n(a multiplication of batch size of 24 and gradient\naccumulation step 12). The maximum length of\nDomBERT is consistent with BERT as 512. We use\nAdamax(Kingma and Ba, 2014) as the optimizer.\nLastly. the learning rate is to be 5e-5.\n4.4 Compared Methods\nWe compare DomBERT with LM-based baselines\n(that requires no extra supervision from humans\nsuch as parsing, ﬁne-grained annotation).\n7https://docs.nvidia.com/deeplearning/\nsdk/mixed-precision-training/index.html\nBERT this is the vanilla BERTBASE pre-trained\nmodel from (Devlin et al., 2019), which is used\nto show the performance of BERT without any\ndomain adaption.\nBERT-Review post-train BERT on all (mixed-\ndomain) Amazon review datasets and Yelp datasets\nin a similar way of training BERT. Following (Liu\net al., 2019), we train the whole corpus for 4 epochs,\nwhich took about 10 days of training (much longer\nthan DomBERT).\nBERT-DKis a baseline borrowed from (Xu et al.,\n2019) that trains an LM per domain. Note that\nthe restaurant domain is trained from 1G of corpus\nthat aligns well with the types of restaurants in\nSemEval, which is not a low-resource case. We\nuse this baseline to show that DomBERT can reach\ncompetitive performance.\nDomBERT is the model proposed in this paper.\n4.5 Evaluation Metrics\nFor AE, we use the standard evaluation scripts\ncome with the SemEval datasets and report the\nF1 score. For ASC, we compute both accuracy and\nMacro-F1 over 3 classes of polarities, where Macro-\nF1 is the major metric as the imbalanced classes\nintroduce biases on accuracy. To be consistent with\nexisting research (Tang et al., 2016), examples be-\nlonging to the conﬂict polarity are dropped due\nto a very small number of examples. For E2E-\nABSA, we adopt the evaluation script from(Li et al.,\n2019a), which reports precision, recall, and F1\nscore on sequence labeling (of combined aspect\nlabels and sentiment polarity).\nResults are reported as averages of 10 runs (10\ndifferent random seeds for random batch genera-\ntion).8\n4.6 Result Analysis and Discussion\nResults on different tasks in ABSA exhibit different\nchallenges.\nAE: In Table 5 We notice that AE is a very domain-\nspeciﬁc task. DomBERT further improves the\nperformance of BERT-DK that only uses domain-\nspeciﬁc corpus. Note that BERT-DK for restaurant\nuses 1G of restaurant corpus. But DomBERT’s tar-\nget domain corpus is just 100 MB. So DomBERT\nfurther learns domain-speciﬁc knowledge from rel-\nevant domains. Although Yelp data contain a\ngreat portion of restaurant reviews, a mixed-domain\n8We notice that adopting 5 runs used by existing researches\nstill has a high variance for a fair comparison.\nDomain Laptop Restaurant\nMethods F1 F1\nBERT(Devlin et al., 2019)79.28 74.1\nBERT-Review 83.64 76.20\nBERT-DK(Xu et al., 2019)83.55 77.02\nDomBERT 83.89 77.21\nTable 5: AE in F1.\nDomain Laptop Rest.\nMethods Acc. MF1 Acc. MF1\nBERT(Devlin et al., 2019)75.29 71.91 81.54 71.94\nBERT-Review 78.62 75.5 83.35 74.9\nBERT-DK(Xu et al., 2019)77.01 73.72 83.96 75.45\nDomBERT 76.72 73.46 83.14 75.00\nTable 6: ASC in Accuracy and Macro-F1(MF1).\ntraining as BERT-Review does not yield enough\ndomain-speciﬁc knowledge.\nASC: ASC is a more domain agnostic task because\nmost of sentiment words are sharable across all\ndomains (e.g., “good” and “bad”). As such, in\nTable 6, we notice ASC for restaurant is more\ndomain-speciﬁc than laptop. DomBERT is worse\nthan BERT-Review in laptop because a 20+ G can\nlearn general-purpose sentiment better. BERT-DK\nis better than DomBERT because a much larger in-\ndomain corpus is more important for performance.\nE2E ABSA: By combining AE and ASC together,\nE2E ABSA exhibit more domain-speciﬁty, as\nshown in Table 7. In this case, we can see the full\nperformance of DomBERT because it can learn\nboth general and domain-speciﬁc knowledge well.\nBERT-Review is poor probably because it focuses\non irrelevant domains such as Books.\nWe further examine the sampling process of\nDomBERT. In Table 8, we report top-20 source\ndomains reported by the data sampler at the end\nof training. The results are closer to our intuition\nbecause most domains are very close to laptop and\nrestaurant, respectively.\n5 Conclusions\nThis paper investigates the task of domain-oriented\nlearning for language modeling. It aims to lever-\nage the beneﬁts of both large-scale mixed-domain\ntraining and in-domain speciﬁc knowledge learn-\ning. We propose a simple extension of BERT called\nDomBERT, which automatically exploits the power\nof training corpus from relevant domains for a tar-\nget domain. Experimental results demonstrate that\nthe DomBERT is promising in a wide assortment\nof tasks in aspect-based sentiment analysis.\nLaptop RestaurantP R F1 P R F1Existing Models(Li et al., 2019a) 61.2754.8957.9068.6471.0169.80(Luo et al., 2019) - - 60.35 - - 72.78(He et al., b) - - 58.37 - - -\nLSTM-CRF(Lample et al., 2016)58.6150.4754.2466.1066.3066.20(Ma and Hovy, 2016)58.6651.2654.7161.5667.2664.29(Liu et al., 2018) 53.3159.4056.1968.4664.4366.38\nBERT+Linear(Li et al., 2019c)62.1658.9060.4371.4275.2573.22\nBERT(Devlin et al., 2019)61.9758.5260.1168.8673.0070.78BERT-Review 65.8063.1264.3769.9275.3672.47BERT-DK(Xu et al., 2019)63.9561.1862.4571.8874.0772.88DomBERT 66.9665.5866.2172.1774.9673.45\nTable 7: Results of E2E ABSA: baselines are borrowed\nfrom (Li et al., 2019c).\nLaptop Restaurant\nTablets Food\nBoot Shop (Men) Coffee & Tea\nLaptop & Netbook Computer AccessoriesBakeries\nComputers & Accessories Bars\nMicrosoft Windows Nightlife\nElectronics Warranties Arts & Entertainment\nDesktops Grocery\nAntivirus & Security Venues & Event Spaces\nAviation Electronics Lounges\nWatch Repair Beer\nOrthopedists Casinos\nCompact Stereos Hotels\nUnlocked Cell Phones Dance Clubs\nPower Strips Tea Rooms\nMobile Broadband Pubs\nCleaners Cinema\nNo-Contract Cell Phones Event Planning & Services\nVideo Games/PC/AccessoriesSports Bars\nAntivirus Specialty Food\nMP3 Players & Accessories Desserts\nTable 8: Top-20 relevant domains\nReferences\nPeng Chen, Zhongqian Sun, Lidong Bing, and Wei\nYang. 2017. Recurrent attention network on mem-\nory for aspect sentiment analysis. In Proceedings of\nthe 2017 conference on empirical methods in natural\nlanguage processing, pages 452–461.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In International Conference on Learning Rep-\nresentations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming\nZhou, and Ke Xu. 2014. Adaptive recursive neural\nnetwork for target-dependent twitter sentiment clas-\nsiﬁcation. In Proceedings of the 52nd annual meet-\ning of the association for computational linguistics\n(volume 2: Short papers), volume 2, pages 49–54.\nSuchin Gururangan, Ana Marasovi, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. a. Exploiting document knowledge for\naspect-level sentiment classiﬁcation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics. Association for Com-\nputational Linguistics.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. b. An interactive multi-task learning net-\nwork for end-to-end aspect-based sentiment analysis.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics. Associa-\ntion for Computational Linguistics.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018a. Effective attention modeling for\naspect-level sentiment classiﬁcation. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 1121–1131.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018b. Exploiting document knowl-\nedge for aspect-level sentiment classiﬁcation. arXiv\npreprint arXiv:1806.04346.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. In World Wide\nWeb.\nYulan He, Chenghua Lin, and Harith Alani. 2011.\nAutomatically extracting polarity-bearing topics for\ncross-domain sentiment classiﬁcation. In Proceed-\nings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language\nTechnologies-Volume 1, pages 123–131. Association\nfor Computational Linguistics.\nYulan He and Deyu Zhou. 2011. Self-training from\nlabeled features for sentiment analysis. Information\nProcessing & Management, 47(4):606–616.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining , pages 168–177.\nACM.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1746–1751.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nXin Li, Lidong Bing, Wai Lam, and Bei Shi.\n2018a. Transformation networks for target-\noriented sentiment classiﬁcation. arXiv preprint\narXiv:1805.01086.\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019a. A\nuniﬁed model for opinion target extraction and target\nsentiment prediction. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 33,\npages 6714–6721.\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019b. A\nuniﬁed model for opinion target extraction and target\nsentiment prediction. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 33,\npages 6714–6721.\nXin Li, Lidong Bing, Wenxuan Zhang, and Wai\nLam. 2019c. Exploiting bert for end-to-end\naspect-based sentiment analysis. arXiv preprint\narXiv:1910.00883.\nXin Li and Wai Lam. 2017. Deep multi-task learning\nfor aspect term extraction with memory interaction.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2886–2892.\nZheng Li, Ying Wei, Yu Zhang, Xiang Zhang, Xin Li,\nand Qiang Yang. 2018b. Exploiting coarse-to-ﬁne\ntask transfer for aspect-level sentiment classiﬁcation.\narXiv preprint arXiv:1811.10999.\nBing Liu. 2012. Sentiment analysis and opinion min-\ning. Synthesis lectures on human language technolo-\ngies, 5(1):1–167.\nBing Liu. 2015. Sentiment analysis: Mining opinions,\nsentiments, and emotions . Cambridge University\nPress.\nL. Liu, J. Shang, F. Xu, X. Ren, H. Gui, J. Peng, and\nJ. Han. 2018. Empower Sequence Labeling with\nTask-Aware Neural Language Model. In AAAI.\nQiao Liu, Haibin Zhang, Yifu Zeng, Ziqi Huang, and\nZufeng Wu. 2018. Content attention model for as-\npect based sentiment analysis. In Proceedings of the\n2018 World Wide Web Conference on World Wide\nWeb, pages 1023–1032. International World Wide\nWeb Conferences Steering Committee.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHuaishao Luo, Tianrui Li, Bing Liu, and Junbo Zhang.\n2019. DOER: Dual cross-shared RNN for aspect\nterm-polarity co-extraction. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 591–601, Florence, Italy.\nAssociation for Computational Linguistics.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017. Interactive attention networks for\naspect-level sentiment classiﬁcation. arXiv preprint\narXiv:1709.00893.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1064–1074, Berlin, Ger-\nmany. Association for Computational Linguistics.\nThien Hai Nguyen and Kiyoaki Shirai. 2015.\nPhraseRNN: Phrase recursive neural network\nfor aspect-based sentiment analysis. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing , pages 2509–2514,\nLisbon, Portugal. Association for Computational\nLinguistics.\nSinno Jialin Pan and Qiang Yang. 2009. A survey on\ntransfer learning. IEEE Transactions on knowledge\nand data engineering, 22(10):1345–1359.\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up?: sentiment classiﬁcation using\nmachine learning techniques. In Proceedings of the\nACL-02 conference on Empirical methods in natural\nlanguage processing-Volume 10, pages 79–86. Asso-\nciation for Computational Linguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2.amazonaws.com/openai-assets/research-\ncovers/languageunsupervised/language understand-\ning paper.pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nLei Shu, Hu Xu, and Bing Liu. 2017. Lifelong learning\nCRF for supervised aspect extraction. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 148–154, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-to-end memory networks. In Advances\nin neural information processing systems , pages\n2440–2448.\nDuyu Tang, Bing Qin, and Ting Liu. 2016. Aspect\nlevel sentiment classiﬁcation with deep memory net-\nwork. arXiv preprint arXiv:1605.08900.\nYi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.\nLearning to attend via word-aspect associative fu-\nsion for aspect-based sentiment analysis. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence.\nShuai Wang, Guangyi Lv, Sahisnu Mazumder, Geli Fei,\nand Bing Liu. 2018a. Lifelong learning memory net-\nworks for aspect sentiment classiﬁcation. In 2018\nIEEE International Conference on Big Data (Big\nData), pages 861–870. IEEE.\nShuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei\nZhou, and Yi Chang. 2018b. Target-sensitive mem-\nory networks for aspect sentiment classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 957–967.\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and\nXiaokui Xiao. 2016a. Recursive neural conditional\nrandom ﬁelds for aspect-based sentiment analysis.\narXiv preprint arXiv:1603.06679.\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and\nXiaokui Xiao. 2017. Coupled multi-layer attentions\nfor co-extraction of aspect and opinion terms. In\nThirty-First AAAI Conference on Artiﬁcial Intelli-\ngence.\nYequan Wang, Minlie Huang, Li Zhao, et al. 2016b.\nAttention-based lstm for aspect-level sentiment clas-\nsiﬁcation. In Proceedings of the 2016 conference on\nempirical methods in natural language processing ,\npages 606–615.\nJason Weston, Sumit Chopra, and Antoine Bor-\ndes. 2014. Memory networks. arXiv preprint\narXiv:1410.3916.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018a.\nDouble embeddings and cnn-based sequence label-\ning for aspect extraction. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018b.\nDouble embeddings and CNN-based sequence la-\nbeling for aspect extraction. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 592–598, Melbourne, Australia. Association\nfor Computational Linguistics.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019. Bert\npost-training for review reading comprehension and\naspect-based sentiment analysis. In Proceedings of\nthe 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8161786794662476
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.784673273563385
    },
    {
      "name": "Natural language processing",
      "score": 0.6479840278625488
    },
    {
      "name": "Language model",
      "score": 0.6072810888290405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5945836901664734
    },
    {
      "name": "Extension (predicate logic)",
      "score": 0.5673086047172546
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5546702146530151
    },
    {
      "name": "Domain-specific language",
      "score": 0.48345813155174255
    },
    {
      "name": "Domain model",
      "score": 0.44717690348625183
    },
    {
      "name": "Domain analysis",
      "score": 0.43682661652565
    },
    {
      "name": "Programming language",
      "score": 0.1578337848186493
    },
    {
      "name": "Domain knowledge",
      "score": 0.11204370856285095
    },
    {
      "name": "Software",
      "score": 0.04718366265296936
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Software construction",
      "score": 0.0
    },
    {
      "name": "Software system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    }
  ],
  "cited_by": 5
}