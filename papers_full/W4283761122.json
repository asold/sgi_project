{
    "title": "SwinOCSR: end-to-end optical chemical structure recognition using a Swin Transformer",
    "url": "https://openalex.org/W4283761122",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2415423592",
            "name": "Zhanpeng Xu",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2106299145",
            "name": "Jianhua Li",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2109732724",
            "name": "Zhaopeng Yang",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2134355618",
            "name": "Shiliang Li",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110564288",
            "name": "Honglin Li",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2415423592",
            "name": "Zhanpeng Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106299145",
            "name": "Jianhua Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109732724",
            "name": "Zhaopeng Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2134355618",
            "name": "Shiliang Li",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110564288",
            "name": "Honglin Li",
            "affiliations": [
                "East China University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2051554764",
        "https://openalex.org/W2112207373",
        "https://openalex.org/W2069719501",
        "https://openalex.org/W2061890234",
        "https://openalex.org/W2160592517",
        "https://openalex.org/W1966456689",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2963734039",
        "https://openalex.org/W3000478925",
        "https://openalex.org/W3159789740",
        "https://openalex.org/W4206029367",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W4229590462",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2021662118",
        "https://openalex.org/W1976144034",
        "https://openalex.org/W3203895579",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2148797284",
        "https://openalex.org/W2899070097",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2108325777",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3097598035"
    ],
    "abstract": "Abstract Optical chemical structure recognition from scientific publications is essential for rediscovering a chemical structure. It is an extremely challenging problem, and current rule-based and deep-learning methods cannot achieve satisfactory recognition rates. Herein, we propose SwinOCSR, an end-to-end model based on a Swin Transformer. This model uses the Swin Transformer as the backbone to extract image features and introduces Transformer models to convert chemical information from publications into DeepSMILES. A novel chemical structure dataset was constructed to train and verify our method. Our proposed Swin Transformer-based model was extensively tested against the backbone of existing publicly available deep learning methods. The experimental results show that our model significantly outperforms the compared methods, demonstrating the model’s effectiveness. Moreover, we used a focal loss to address the token imbalance problem in the text representation of the chemical structure diagram, and our model achieved an accuracy of 98.58%.",
    "full_text": "Xu et al. Journal of Cheminformatics           (2022) 14:41  \nhttps://doi.org/10.1186/s13321-022-00624-5\nRESEARCH\nSwinOCSR: end-to-end optical chemical \nstructure recognition using a Swin Transformer\nZhanpeng Xu1, Jianhua Li1*, Zhaopeng Yang1, Shiliang Li2 and Honglin Li2 \nAbstract \nOptical chemical structure recognition from scientific publications is essential for rediscovering a chemical structure. \nIt is an extremely challenging problem, and current rule-based and deep-learning methods cannot achieve satisfac-\ntory recognition rates. Herein, we propose SwinOCSR, an end-to-end model based on a Swin Transformer. This model \nuses the Swin Transformer as the backbone to extract image features and introduces Transformer models to convert \nchemical information from publications into DeepSMILES. A novel chemical structure dataset was constructed to train \nand verify our method. Our proposed Swin Transformer-based model was extensively tested against the backbone of \nexisting publicly available deep learning methods. The experimental results show that our model significantly outper-\nforms the compared methods, demonstrating the model’s effectiveness. Moreover, we used a focal loss to address the \ntoken imbalance problem in the text representation of the chemical structure diagram, and our model achieved an \naccuracy of 98.58%.\nKeywords: Chemical Structure Recognition, Deep Learning, Swin Transfromer, End-to-End Model\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nOptical chemical structure recognition (OCSR) is the \nconversion of the chemical structure information of \nchemical compounds from scientific publications into \nmachine-readable formats. Chemical structures printed \nin scientific publications are usually in image formats \nsuch as JPEG, PNG, and GIF. They cannot be directly \nutilized because they are not a machine-readable repre -\nsentation of molecules. The purpose of OCSR is to cor -\nrectly translate this chemical structure information into \na machine-readable representation and store them in a \nchemical information database. OCSR is time-consum -\ning and error-prone, and requires domain knowledge to \neliminate ambiguities in structures. As chemical struc -\nture scientific publications continue to increase exponen-\ntially, OCSR plays a vital role in many chemical subfields, \nsuch as synthetic science, natural product research, \ndrug discovery and etc. Therefore, OCSR is still in high \ndemand.\nExisting automatic OCSR software systems include \nKekule [1], OROCS [2], CLIDE [3], MLOCSR [4], Chem -\nReader [5] and OSRA [6]. Most of these systems usu -\nally use rule-based approaches to recognize molecular \ndiagrams.\nWith the rapid development of deep learning, both \ncomputer vision and natural language processing have \nbecome popular research topics in recent years. As a \nspecific subdomain of computer vision, image caption -\ning is used to identify the objects of an image firstly \nand then expresses the relationship among them in \nthe form of accurate syntactically generated sentences. \nImage captioning often adopts a special framework, \ni.e., an encoder-decoder, of which the encoder usually \nuses a convolutional neural network (CNN), and the \ndecoder usually use a Recurrent Neural Network (RNN). \nFor example, in a neural image capture generator [7], a \nCNN-based InceptionNet [8] is used to extract image \nfeatures, and an RNN is used to decode image features \nOpen Access\nJournal of Cheminformatics\n*Correspondence:  jhli@ecust.edu.cn\n1 School of Information Science and Engineering, East China University \nof Science and Technology, 130 Mei Long Road, Shanghai 200237, China\nFull list of author information is available at the end of the article\nPage 2 of 13Xu et al. Journal of Cheminformatics           (2022) 14:41 \nfor text generation. The RNN can also be replaced with \nLong Short-Term Memory (LSTM) [9] or Gated Recur -\nrent Unit (GRU) [10]. To improve the interpretability of \nneural networks, an attention mechanism is applied to \nthe image captioning task [11]. The basic idea is to use \nconvolutional layers to acquire image features and then \nweight them with attentions before sending them to an \nRNN for decoding. Using a self-attention mechanism, the \nTransformer [12] model proposed by Google has recently \nachieved an excellent performance in various translation-\nrelated tasks and has been widely applied.\nSimilar to image captioning, the OCSR task can be \nabstracted as the process of translating a chemical struc -\nture diagram into a computer-readable textual repre -\nsentation. Compared with image captioning, two major \nchallenges of an OCSR task are complex chemical pat -\nterns in chemical structures and long corresponding \nchemical representation. The existing methods based \non deep learning [13–16] use CNNs as their backbones \nto extract image features of molecules. However, CNN \nonly learns local representation and cannot effectively \nuse global information. To learn the global representa -\ntion and obtain more comprehensive chemical structure \ninformation, we use Swin Transformer [17] as a back -\nbone to extract image features of molecules. Moreover, \none noticeable phenomenon is the frequency imbal -\nance of elements in molecules. For example, C, H, and \nO appear more frequently, and Br, Cl, and Ar appear less \nfrequently. This results in an imbalance of tokens in the \ntext representation. We use a focal loss [18] to solve the \nimbalance problem. The contribution of this work can be \nsummarized in three parts:\n1. We present a new deep-learning OCSR (SwinOCSR) \napproach using a Swin Transformer as a backbone \nto extract image features and Transformer [12] to \ngenerate DeepSMILES [19] that are more syntacti -\ncally valid than SMILES [20] in an end-to-end man -\nner. Our method learns the global representation and \nobtain more abundant image features compared with \nother methods. It provides strong support for the \nsubsequent Transformer part.\n2. Based on the analysis of molecules, we use a focal \nloss to address the token imbalance problem in the \ntext representation of molecular diagrams. This is the \nfirst attempt at explicitly optimizing such a problem \nin OCSR tasks, to the best of our knowledge.\n3. We construct a novel chemical structure molecule \ndataset containing four categories (Kekule, Aromatic, \nsubstituent and Kekule, substituent and Aromatic), \nand our approach is robust against different molecule \ncategories in constructed dataset. And, it is excel -\nlent in recognizing long-character chemical struc -\ntures. The model trained on the constructed dataset \nachieved an accuracy of 98.58%.\nRelated work\nRule‑based OCSR approaches\nEarly OCSR tasks used a rule-based approach. They \nused image processing techniques and optical character \nrecognition for atomic labeling and charge recognition, \nencoded various rules for bond detection, and compiled \nconnection tables. Kekule [1] was the first complete \nOCSR tool for scanning, vectorization, dashed and wedge \nline searches, optical character recognition, graphical \nediting, and post-processing. In addition, OSRA [6] is \nan open-source chemical structure extraction tool devel -\noped by the NCI. The extracted chemical structure can \nbe directly converted into the SMILES or SDF format. \nAlthough OSRA can recognize some common group \nabbreviations, dashed lines, and wedge bonds, it can -\nnot recognize charges or isotopes. CLIDE, ChemReader, \nCLiDE Pro [21], ChemInfty [22], and the approach by \nSadawi et al. [23] made further improvements. However, \nthese approaches have certain drawbacks. For example, \nthe rule-based system will become difficult to interpret \nwhen molecular diagrams contain ambiguous or uncom -\nmon representations. As one of the current challenges, \nthe various recognition components of a rule-based sys -\ntem are interdependent, making further improvements \nextremely difficult to achieve.\nDeep‑learning‑based OCSR approaches\nUnlike rule-based approaches, deep learning-based \nmethods identify chemical structures without hardcoded \nrules. For this reason, several deep learning-based OCSR \nmethods have been proposed. In 2019, Staker et al. [13] \npresented the first deep learning-based OCSR approach, \nwith a SMILES file as the output. Its accuracies on the \nvalidation sets ranged from 41% to 83%. However, this \napproach is closed-source and unavailable for re-testing, \nand it is only used to recognize  low-resolution images. \nImg2Mol [24] is another deep learning-based OCSR \napproach, whose performance was verified by compar -\ning it with the rule-based approaches. However, there \nis no comparison with  exsiting deep learning-based \nmethods, and it is preliminary. DECIMER [14] and sub -\nsequent DECIMER 1.0 [15] are two other deep-learning-\nbased approaches. Based on existing show-and-tell deep \nneural networks, DECIMER uses Inception V3 [25] as a \nbackbone to extract image features and GRU to predict \nSMILES. However, its performance does not yet rival \nthe performance of existing traditional approaches. The \nupdated version of DECIMER, DECIMER 1.0 [15], sub -\nstitutes Inception V3 with EfficientNet-B3 [26] and GRU \nPage 3 of 13\nXu et al. Journal of Cheminformatics           (2022) 14:41 \n \nwith Transformer. DECIMER 1.0 achieved a Tanimoto \nlevel of about 96% in a dataset of 30–35 million mol -\necules. The latest deep-learning-based OCSR approach, \nImage2SMILES [16], uses ResNet-50 [27] as a backbone \nto extract image features and Transformer as a decoder \npart to predict FG-SMILES in the dataset of 10 million \nmolecules. Image2SMILES achieved an accuracy of about \n90.7%, but it still needs further improvement.\nAmong these approaches, many CNNs and their vari -\nants are used as backbones to extract image features in \nOCSR tasks. Therefore, a robust backbone is important \nfor the OCSR task. The Swin Transformer model, a state-\nof-the-art backbone, surpasses the previous models in \nimage classification, object detection and semantic seg -\nmentation. Hence, the Swin Transformer is chosen as our \nmodel backbone for OCSR.\nSwinOCSR: Deep‑learning‑based chemical structure \ndiagram recognition approach\nThe framework of our SwinOCSR approach for chemi -\ncal structure diagram recognition is shown in Fig.  1. \nIt consists of a backbone, Transformer encoder, and \nTransformer decoder. First, the backbone extracts image \nfeatures from an input molecule image to obtain a high-\ndimensional patch sequence. Next, the patch sequence \nand positional embedding are fed into the Transformer \nencoder to output a representation sequence. Finally, the \nTransformer decoder uses the representation sequence \nto decode the corresponding DeepSMILES.\nBackbone\nThe backbone is built based on the Swin Transformer, \nshown in Fig.  2. First, the molecule image is partitioned \ninto non-overlapping patches, and the size of a patch is \n4 × 4. Each patch is treated as a “token” and its feature is \nset as a concatenation of the raw pixel RGB value. After \npartitioning, a linear embedding layer is used to project \nthis raw-valued feature to a certain dimension (192 for \nSwinOCSR), and several Swin Transformer blocks are \nused to extract feature information. As shown in Fig.  3, \neach Swin Transformer block contains two important \nmodules, window multi-head self attention (W-MSA) \nand shift window multi-head self attention (SW-MSA) \nmodules. W-MSA is used to extract local feature infor -\nmation in a window. SW-MSA is used to extract global \nfeature information between windows. This means that \nSwin Transformer uses both local and global informa -\ntion, which greatly enhances Swin Transformer’s fea -\nture extraction capabilities. This process is called “Stage \n1. ” In the following three stages, to generate hierarchical \nFig. 1 SwinOCSR framework for chemical structure recognition\nFig. 2 Swin Transformer-based backbone module\nPage 4 of 13Xu et al. Journal of Cheminformatics           (2022) 14:41 \nrepresentation, Swin Transfromer does not use pool -\ning which is usually used in CNN and may introduces \ninformation loss. Instead, it adopts merging neighbor -\ning patches to reduce the size of feature maps to avoid \ninformation loss. Finally,  to construct a sequence as the \nencoder input, the feature in spatial dimensions is flat -\ntened, resulting in a sequence Sb that represents chemical \nstructure information.\nEncoder\nThe encoding module consists of a positional encoding \noperation and six standard Transformer encoder layers. \nThe six standard layers are linked sequentially, each of \nwhich contains two specific sublayers. The first sublayer \nis a multi-head attention layer, and the second is an MLP \nlayer. Each sublayer is followed by a residual connection \noperation and a normalized operation, as shown in Fig. 4. \nThe output Sb of the backbone flows into the first Trans -\nformer encoder sublayer. The Q, K, and V of the attention \nlayer are obtained by multiplying the respective weight \nmatrices of the three with S b. The attention function is \nthen used to map the Q and a set of K-V pairs to an out -\nput. After obtaining the calculation results, the data are \ntransferred to the MLP layer. The output sequence S e of \nthe encoder is obtained once all six standard layers are \nfinished.\nDecoder\nThe decoding module consists of a positional encoding \noperation, a stack of six standard Transformer decoder \nlayers, a linear layer and a softmax layer. A standard \ntransformer decoder layer contains three specific sublay -\ners. The first sublayer is a multi-head attention layer con -\ntaining a mask. This mask ensures that the prediction of \nthe position i depends only on the known outputs before \nthe position i. The second sublayer is a multi-head atten -\ntion layer, and the third is an MLP layer. Similar to the \nencoding module, residual connections and layer nor -\nmalization follow each sublayer, as shown in Fig.  5. Each \ntime step in the decoding phase outputs a new token of \nthe output sequence. At each time step, the previously \ngenerated output sequence (token sequence) flows into \nthe first sublayer of Transformer decoder to learn inter -\nnal relationships. The output of the first sublayer and the \nSe sequence from the encoder are fed into the second \nsublayer of Transformer decoder to capture their rela -\ntionships. Then, the result is transferred to the MLP layer. \nThe output of Transformer decoder is obtained once all \nsix standard layers are finished. And the output is fed into \na linear layer and a softmax layer to obtain a token as the \nfinal output for this time step.\nDataset\nAs described in the literature, the manual labeling of \ndata is tedious, and it is difficult to obtain large numbers \nof data. Therefore, we did not directly extract molecular \ndiagrams from studies on patents and other chemical \npublications to generate training data, but instead used \nFig. 3 A Swin Transformer Block\nFig. 4 Transformer-based chemical structure encoder module\nPage 5 of 13\nXu et al. Journal of Cheminformatics           (2022) 14:41 \n \ncheminformatics toolkit CDK [28] to generate molecular \ndiagrams.\nAn ideal generated dataset be as diverse as possible \nand should be similar to real molecular representation in \npublications. So, we used millions of molecules and both \ntwo different ring structures (Aromatic and Kekule) to \nmake molecular diagrams diverse, and introduced sub -\nstituents, which are widely used in patents, to generate \nmolecular diagrams similar to those in patents. We have \ndownloaded the first 8.5 million PubChem [29] structures \n(1–8,500,000 PubChem Indices) and gained 6,987,630 \nunique SMILES strings. Based on these SMILES files, we \nconstructed a dataset of 5 million molecules and it con -\nsists of four categories of molecule data, each containing \n1.25 million molecules. Table 1 shows the four categories \nof molecule data distinguished according to two criteria. \nOne is whether most molecules include substituent; the \nother is that the ring structure is Aromatic or Kekule. \nFigure 6 shows an example of each category of molecule \ndata.\nThe dataset was generated as follows: first, molecule \nSMILES files were downloaded from PubChem [29]. \nThis kind of SMILES only contains Kekule ring and does \nnot contain substituents, belonging to Category 1. Then, \nhalf of the downloaded SMILES files are canonically \nconverted into SMILES strings including aromatic ring \nby RDKit, and these molecules in this kind of SMILES \nbelong to Category 2. Next, some SMILES strings in Cat -\negory 1 were broken, and some of both 224 substituents \nin the patent literature and common atom(s) with brack -\nets ([Pb], [NH], [Ru], [Li], [K], [Si], [S +], [O], [O-], [N +], \n[N], [P], [C], [H], [2H], [3H], [B]) were randomly added to \nthe broken SMILES strings, forming new SMILES strings \nwhich belong to Category 3. Category 4 is generated \nfrom Category 2 in the same way. Finally, for each cate -\ngory, molecules were converted into a canonical SMILES \nstring, and 1,250,000 molecules with unique canonical \nSMILES were randomly chosen. The molecular canoni -\ncal SMILES were then converted into DeepSMILES and \nused to render images using CDK.\nDuring image generation, some parameters (such as \nsubstituent fonts, subscripts, corner spacing, and size) \nof CDK image generation are set to make the molecule \nimages closer to the images in the literature. In addition, \nspecific rules for the condensed formula of the molecu -\nlar structure are added to generate the chemical structure \nimages. The generated chemical diagram is a four-chan -\nnel diagram by default. Because the molecule diagram \nis black and white like a binary diagram, recognition of \nmolecular diagram only requires its contour, without \nadditional color channels. According to a threshold, all \ndiagrams are changed into a binary diagram. To meet \nthe input requirements of the model with three channels, \nwe copied the one-channel binary diagram to pad each \nchannel of three-channel diagram. Figure  7 shows an \nexample of a generation molecular diagram. The length \ndistribution of the DeepSMILES string and resolution of \nthe molecular diagram in our dataset are shown in Fig. 8.\nTo evaluate our model, each category of the processed \ndataset is randomly spilt   in a ratio of 18:1:1 for train -\ning, validation and test, respectively, as shown in Table  2. \nThe total size of the training set is 4500000, while both \nthe size of the validation set and that of the test set are \n250000. We selected four metrics per image for quanti -\ntative performance evaluation on accuracy, Tanimoto, \nBLEU [30], and ROUGE [31]. The first two metrics are \nFig. 5 Transformer-based chemical structure decoder module\nTable 1 Description of each category of molecule data\nCategory \nIndex\nSubstituent Aromatic/Kekule Size\n1  × Kekule 1,250,000\n2  × Aromatic 1,250,000\n3 √ Kekule 1,250,000\n4 √ Aromatic 1,250,000\nPage 6 of 13Xu et al. Journal of Cheminformatics           (2022) 14:41 \ntwo frequently used metrics for OCSR, and the last two \nare two standard metrics in image captioning. Here, \nTanimoto was calculated using PubChem fingerprints of \nCDK after decoding DeepSMILES back to SMILES.\nTokenization\nWe counted all the characters of the DeepSMILES strings \nin our dataset. There were 76 unique characters. We \ntreated each unique character as a token.\nTokens in our dataset: c, 6,), C, = , O, N, S, l, s, 5, B, r, n, \n[, H, + ,], %, 1, 0, /, \\, R, F, #, 4, (, 9, -, @, L, 3, 8, 2, ’ , G, a, 7, \nZ,., P , t, Y, o, A, X, i, J, q, x, Q, m, b, d, E, w, I, V, z, e, M,,, \nD, K, p, v, h, y, u, g, k, T, W, U, f.\nTraining\nWe used the same setting for each experiment to make a \nfair comparison. We employed a batch size of 256 images \n(224 × 224 pixels). An Adam [32] optimizer of an initial \nlearning rate 5e−4 and token embedding dimension of \n256 were used. The backbone and Transformer networks \nused cosine and step decays, respectively, regarding the \nlearning rate scheduler. The loss function used the stand -\nard cross entropy (CE) loss. To prevent model overfitting \nFig. 6 An example of each category of molecule data\nFig. 7 An example of generating a molecule diagram from \nCCC)NCCC = CC = CC = C6)OCC = CC = CC = C6)))))))))\nOCC = CC = CC = C6))))))))))[N +] = O)[O-]))))OC = O)OCC)C)C)))))))\nC = O)OCC)C)C\nFig. 8 The distribution of the lengths of the DeepSMILES strings and resolution of molecular diagram\nPage 7 of 13\nXu et al. Journal of Cheminformatics           (2022) 14:41 \n \nduring training, the dropout rate was set to 0.1. The \nmodel trained for 30 epochs on a server configured with \nNVIDIA Tesla V100-PCIE graphic cards.\nExperiments and results\nWe conducted experiments on our dataset for perfor -\nmance and analysis. Firstly, we evaluated the backbone \nperformance of Swin Transformer, ResNet-50 and Effi -\ncientNet-B3. Then, we evaluate the CE loss and focal loss. \nFinally, we analyzed the influence of molecule category \nand DeepSMILES strings length.\nIn addition, because our model is trained on the \ngenerated training set, it may not achieve satisfactory \nresults on real-world test sets which are derived from \nthe literature. Therefore, we also performed experi -\nments on a small real-world test set.\nBackbone comparison experiment\nTo evaluate the Swin Tranformer performance as our \nmodel’s backbone, we compared the Swin Transformer \nwith two CNNs, ResNet-50 of Image2SMILES [16] and \nEfficientNet-B3 of DECIMER 1.0 [15]. CE loss curve \nof training are shown in Fig.  9. The loss value of our \nmodel (using Swin Transformer as the backbone) is \nsmaller than those of ResNet-50 and EfficientNet-B3 in \nall cases, indicating that our model has a faster conver -\ngence. Validation curves (BLEU, accuracy) are shown \nin Fig.  10. Our model is still superior to the other two \nmodels in term of accuracy and BLEU score in all cases. \nThe results demonstrate that our model has better data \nfitting ability. Finally, we made a comparison on the \ntest set. As shown in Table  3, our model demonstrated \nthe best performance based on all four metrics with a \nTable 2 Description of the training, validation, test set\nSet Category \n1\nCategory \n2\nCategory \n3\nCategory \n4\nTotal\nTraining 1,125,000 1,125,000 1,125,000 1,125,000 4,500,000\nValidation 62,500 62,500 62,500 62,500 250,000\nTest 62,500 62,500 62,500 62,500 250,000\nFig. 9 CE loss curve of training\nFig. 10 Validation curves (left: BLEU, right: Accuracy)\nTable 3 Backbone performance comparisons in the test set\nBackbone Accuracy Tanimoto BLEU ROUGE\nSwin Transformer(our) 0.9736 0.9965 0.9946 0.9964\nResNet-50 0.8917 0.9879 0.9862 0.9887\nEfficientNet-B3 0.8670 0.9846 0.9837 0.9866\nPage 8 of 13Xu et al. Journal of Cheminformatics           (2022) 14:41 \nBLEU score of 99.46%, Tanimoto of 99.65%, ROUGE \nscore of 99.64%, and accuracy of 97.36%. For accu -\nracy, our model reached 97.36% and 8.19% and 10.66% \nhigher than ResNet-50 and EfficientNet-B3, respec -\ntively. The accuracy metric requires that the predicted \nand actual DeepSMILES strings have the same charac -\nter in each position. Hence, the metric is an essential \nrequirement and can better reflect the performance of a \nmodel compared with the other three metrics.\nLoss function comparison experiment\nThe frequency distribution of tokens affects the model’s \nperformance when CE loss is used as the model’s loss \nfunction. On this basis, we counted the distribution \nof tokens in our dataset. The result is shown in Fig.  11, \nwhere the total number of tokens is 234706822. We \nfound that the frequency distribution of tokens shows a \nlong-tail distribution. A few tokens on the left, such as \n“), ” “C, ” “c, ” and “ = ” formed the frequency header. Most \ntokens on the right formed the frequency tail. This indi -\ncates a significant imbalance of token classes in our data -\nset. Therefore, the model tends to predict a small number \nof token classes with high frequency during training. As \na few token classes with high frequency contribute to the \nmajority of the loss, even if the model ignores other token \nclasses, the CE loss was not greatly affected.\nTo solve this problem, we used the focal loss [18], a com-\nmon solution in object detection. Because focal loss is usu-\nally used for binary tasks, we converted our single-label \nclassification task to multi-label classification task and \nrewrote the focal loss as multi-label focal loss (MFL). Given \nn classes, the model outputs one logit per class, o i. Each \nlogit is independently activated by a sigmoid function σ(oi). \nThe probability of each label, pi, is given by:\nwhere yi is the ground-truth label for class i. The average \nloss of binary loss per label, MFL, is obtained by:\n(1)pi =\n{\nσ (oi) ifyi = 1\n1 − σ (oi) otherwise,\nFig. 11 Frequency distribution histogram of tokens in our dataset\nTable 4 Loss function performance comparisons\nLoss Accuracy Tanimoto BLEU ROUGE\nMFL 0.9858 0.9977 0.9959 0.9978\nCE 0.9736 0.9965 0.9946 0.9964\nPage 9 of 13\nXu et al. Journal of Cheminformatics           (2022) 14:41 \n \nwhere αi is the weighting factor for class i and γ is a \nfocusing parameter. In Table 4, we compare performance \nof MFL with CE on the   test set. It is evident that loss \nfunction using MFL outperforms CE on all four met -\nrics. Because the model that uses MFL is our best model, \nwe utilized SwinOCSR that uses MFL in the following \nexperiments.\nInfluence of molecule category\nTo analyze the prediction performance of SwinOCSR on \ndifferent molecule categories, the four data categories in \nthe test set were evaluated separately based on accuracy. \nThe result is shown in Table 5. Category 1 and 2 are lower \nthan 3 and 4. demonstrating that SwinOCSR performs \na little better on the data with substituents. We believe \nthat the reason for this is that substituents will be explic -\nitly reflected in the molecular diagram. Hence, the Swin -\nOCSR is easier to extract information about substituents \nand identify them. Table  5 also shows that category 1 is \nlower than 2 and 3 is lower than 4, demonstrating that \nSwinOCSR performs a little better on the data with Aro -\nmatic rings. The explanation is that in molecular dia -\ngram, the Aromatic ring is presented as a circle distinct \nfrom other molecular diagram elements. In contrast, \nthe Kekule ring is represented as  lines similar to other \nelements of the molecule diagram. Therefore, the  Aro -\nmatic  rings that are distinct from other elements are \neasier to identify. There is not much difference in accu -\nracy among four categories of data. This shows that Swin-\nOCSR has good robustness with different categories of \ndata.\nInfluence of DeepSMILES string length\nTo analyze the prediction performance of SwinOCSR on \nthe DeepSMILES strings of different lengths, we divided \nthe DeepSMILES strings of the test set into the follow -\ning length ranges: [1, 25], [26, 50], [51, 75], [76, 100], \nand reported the accuracy within the ranges as the per -\nformance metric. A phenomenon to be expected is that \n(2)MFL = 1\nn\nn∑\ni\n−α i(1 − pi)γ log(pi),\nthe model performance declines as the DeepSMILES \nstrings length increases, because the longer the Deep -\nSMILES strings, the more times the model has to decode \nand the more likely errors will occur. The result is shown \nin Fig.  12. Moreover, SwinOCSR remains steady with [1, \n75] and decreases slightly with [76, 100]. This indicates \nthat SwinOCSR can adapt to changes in the length of the \nDeepSMILES strings. Even in the lowest range [76–100], \nSwinOCSR can still achieve an accuracy of 94.76%, indi -\ncating that it has a strong ability to recognize large mol -\necules with long DeepSMILES strings. This shows that \nthe backbone of SwinOCSR can extract richer chemi -\ncal structure information from molecular graphs. Thus, \nmore characters can be predicted during decoding.\nPerformance on real data\nTo evaluate the prediction performance of SwinOCSR \non real-world test sets, we have constructed a small real-\nworld test set and conducted experiments on the test set. \nThe small real-world test includes 100 images derived \nfrom the literature and their corresponding canonical \nSMILES strings which are manually labeled. The results \nare shown in Table 6. Our model achieved an accuracy of \n25%, and the performance of our model on the real-world \ntest set is unsatisfactory.\nTable 5 Performance of SwinOCSR on different categories of \ndata\nCategory Accuracy\n1 0.9820\n2 0.9846\n3 0.9876\n4 0.9889\nFig. 12 Performance of SwinOCSR with different DeepSMELES string \nlengths\nTable 6 Performance on the test set derived from the literature\nMetric Literature\nAccuracy 0.2500\nTanimoto 0.5975\nBLEU 0.7261\nROUGE 0.8058\nValid DeepSMILES 0.9800\nValid SMILES 0.9700\nPage 10 of 13Xu et al. Journal of Cheminformatics           (2022) 14:41 \nWe also used CDK to generate images from the man -\nual-labeled canonical SMILES strings of the small real-\nworld test set, and constructed a new generated test set. \nWe also conducted experiments on the generated test set, \nand the results are shown in Table 7. Our model achieved \nan accuracy of 94%, and the performance is good in term \nof all metrics.\nWe analyzed several molecule examples in the \nabove experiments. Table  8 shows two examples that \nare correctly extracted in both the real-world test set \nTable 7 Performance on the test set generated by CDK\nMetric CDK\nAccuracy 0.9400\nTanimoto 0.9906\nBLEU 0.9905\nROUGE 0.9954\nValid DeepSMILES 1.0000\nValid SMILES 1.0000\nTable 8 Two examples that are correctly extracted in both the test set from the literature and the test set generated by CDK\nItems Molecule 1 Molecule 2\nThe real-world image derived from the literature\n \n  \nManual-labeled SMILES CNC1 = CC(= NC(= N1)C2 = CC = CC = C2)N3CCC(CC3)\nC(= O)NCC4 = CC = CC = C4C(F)(F)F\nC2 = CC1 = CC(= CC = C1N = C2)\nCN4C3 = NC(= NC = C3N = N4)C5 = CN(CCO)N = C5\nPredicted SMILES from the real-world image CNC1 = CC(= NC(= N1)C2 = CC = CC = C2)N3CCC(CC3)\nC(= O)NCC4 = CC = CC = C4C(F)(F)F\nC2 = CC1 = CC(= CC = C1N = C2)\nCN4C3 = NC(= NC = C3N = N4)C5 = CN(CCO)N = C5\nGenerated image from manual-labeled SMILES by CDK\n \n \nPredicted SMILES from the generated image CNC1 = CC(= NC(= N1)C2 = CC = CC = C2)N3CCC(CC3)\nC(= O)NCC4 = CC = CC = C4C(F)(F)F\nC2 = CC1 = CC(= CC = C1N = C2)\nCN4C3 = NC(= NC = C3N = N4)C5 = CN(CCO)N = C5\nTable 9 Two examples that are incorrectly extracted in the test set from the literature and are correctly extracted in the test set \ngenerated by CDK\nItems Molecule 1 Molecule 2\nThe real-world image derived from the literature\n \n \nManual-labeled SMILES CC1 = CC = CC(= C1)N C1 = C(N(C(= O)C(= C1[R3])[R4])[R8])[R]\nPredicted SMILES from the real-world image [CH3-]C1 = CC = CC(= C1)N.[I-] CC(C)(C)C1 = CC(= CC(= O)N1[Rx])[R3]\nGenerated image from the above-mentioned predicted SMILES\n \n  \nGenerated image from manual-labeled SMILES by CDK\n \n  \nPredicted SMILES from the generated image CC1 = CC = CC(= C1)N C1 = C(N(C(= O)C(= C1[R3])[R4])[R8])[R]\nPage 11 of 13\nXu et al. Journal of Cheminformatics           (2022) 14:41 \n \nfrom the literature and the test set generated by CDK, \nTable 9 demonstrates two examples that are incorrectly \nextracted in the real-world test set and are correctly \nextracted in the generated test set, and Table  10 shows \none example that is incorrectly extracted in both the \nreal-world test set and the generated test set.\nAfter analyzing, the unsatisfactory performance of \nour model on the real-world test set may be caused by \nthe following three factors:\n(1) The images derived from the literature are vague, \nwhile the CDK-generated images are clearer.\n(2) The image derived from the literature is more com -\nplex while CDK-generated images are more regular.\n(3) Although a canonical SMILES string of a molecule \nensures the unique SMILES representation of the \nmolecule, there is no unique chemical structure \nrepresentation for one specific molecule. There \nare a lot of image styles in generating images by \ndifferent chemical programs. For example, some \ncondensed structural formulas, such as NH, are \nexpanded in CDK-generated images by default, and \nother condensed structural formulas, such as NO, \nNO2, CF3, CH3, etc., are unexpanded by default, \nso if the corresponding styles is changed, the image \nwill be changed.\nOf the above three factors, no unique chemical struc -\nture representation for one specific molecule is the most \nsignificant and more real-world chemical structures can \nalleviate the problem.\nConclusion\nIn this study, we propose an end-to-end chemical struc -\nture image recognition approach, SwinOCSR, which can \ndirectly recognize the original chemical structure map \nwithout formulating manual features. Compared with \nexisting approaches that use CNNs as the backbones, \nit achieved a high accuracy of 98.58%, superior perfor -\nmance, and fast convergence. It also performs well in \nrecognizing long sequences, particularly in recognizing \nchemical structures containing substituents. Experimen -\ntal results show that SwinOCSR can effectively extract \nthe key features of chemical structures and capture the \ncorrespondence between chemical structure graphs and \nDeepSMILES.\nTable 10 One example that is incorrectly extracted in both the test set from the literature and the test set generated by CDK\nItems Molecule 1\nThe real-world image derived from the literature\n \nManual-labeled SMILES c1c(cc(c(c1[Y1])[X0])[Y2])c2c(cc([H][H][R0])cc2[Y4])[Y3]\nPredicted SMILES from the real-world image C1CC(CCC1C2CCC(CC2)[Y])c4cc(c(-c3cc(c(c(c3)[Y1])[Y1])[Y])c(c4)[Y])[Y]\nGenerated image from the above-mentioned predicted SMILES\n \nGenerated image from manual-labeled SMILES by CDK\n \nPredicted SMILES from the generated image c1c(cc(c(c1[Y1])[X0])[Y2])-c2c(cc(cc2[Y4])N[R0])[Y4]\nGenerated image from the above-mentioned predicted SMILES\n \nPage 12 of 13Xu et al. Journal of Cheminformatics           (2022) 14:41 \nHowever, despite the superior performance of our \nmethod on the generated data, the recognition perfor -\nmance in the literature is unsatisfactory. This can be attrib-\nuted to some discrepancies existing between the chemical \nstructures rendered by chemical software and those in the \nliterature. For example, the real-world chemical structures \nin the literature have lower resolutions, various noises, and \nnumerous complex patterns such as wavy lines, abbrevia-\ntions, and superatoms. In fact, the performance of deep \nlearning-based OCSRs depends on the model and the \ndataset. When our model achieved better performance on \na generated dataset and demonstrated the model’s effec -\ntiveness, we believe that if there are enough real-world \nchemical structures to form a real-world training set and \nour model is trained on the training set, our model will \nalso achieve better performance. Our model is a significant \nstep toward the automatic extraction of real-world chemi-\ncal structures. In the future, we will expand the data set \nto include as many low-resolution and complex chemical \nstructure styles as possible. In addition, we hope to provide \na software program that automates the extraction of chem-\nical structures available in the literature. Finally, we hope \nthat our work will open new possibilities for exploring \nend-to-end chemical structure recognition approaches.\nAppendix: 224 substituents\n’[R]’ , ’[R0]’ , ’[R1]’ , ’[R2]’ , ’[R3]’ , ’[R4]’ , ’[R5]’ , ’[R6]’ , ’[R7]’ , \n’[R9]’ , ’[R10]’ , ’[R11]’ , ’[R12]’ , ’[R13]’ , ’[R14]’ , ’[R15]’ , ’[R16]’ , \n’[R17]’ , ’[R18]’ , ’[R19]’ , ’[R20]’ , ’[R21]’ , ’[R22]’ , ’[R23]’ , \n’[R24]’ , ’[R25]’ , ’[R26]’ , ’[R30]’ , ’[R31]’ , ’[R50]’ , ’[R51]’ , \n’[R52]’ , ’[R53]’ , ’[R54]’ , \"[R’]\", \"[R2’]\", \"[R4’]\", \"[R7’]\", \n\"[R8’]\", \"[R9’]\", \"[R10’]\", ’[Ra]’ , ’[Rb]’ , ’[Rc]’ , ’[Rd]’ , ’[Rm]’ , \n’[Rn]’ , ’[Rx]’ , ’[R1a]’ , ’[R1b]’ , ’[R1c]’ , ’[R1d]’ , ’[(R1)s]’ , ’[(R1)\nm]’ , ’[R2a]’ , ’[R2b]’ , ’[R3a]’ , ’[R3b]’ , ’[R4b]’ , ’[R4c]’ , ’[R5a]’ , \n’[R8a]’ , ’[R14a]’ , ’[R4()x]’ , ’[R()p]’ , ’[Rc3]’ , ’[Rc4]’ , ’[Rc6]’ , \n’[Rc7]’ , ’[Rc8]’ , ’[(R1)a]’ , ’[(R1)n]’ , ’[(R2)b]’ , ’[(R2)m]’ , ’[(R2)\nn]’ , ’[(R2)k]’ , ’[(R3)c]’ , ’[(R3)m]’ , ’[(R3)n]’ , ’[(R3)p]’ , ’[(R3)\nq]’ , ’[(R4)m]’ , ’[(R4)q]’ , ’[(R5)a]’ , ’[(R5)n]’ , ’[(R5)o]’ , ’[(R5)\np]’ , ’[(R6)q]’ , ’[(R5)s]’ , ’[(R6)n]’ , ’[(R7)d]’ , ’[(R7)n]’ , ’[(R11)\nr]’ , ’[(R11)u]’ , ’[(R12)r]’ , ’[(R19)w]’ , ’[(Rc)p]’ , ’[(R21)\np]’ , ’[(R9)0–3]’ , ’[(Ra)m]’ , ’[(Ra)n]’ , ’[(Rb)n]’ , ’[(R2a)p]’ , \n’[(R2b)r]’ , ’[(R4a)d]’ , ’[(R4c)g]’ , ’[(R4d)i]’ , ’[(R)p]’ , ’[(RD4)\nmD]’ , \"[OR’]\", ’[ORc]’ , ’[(CR2)n]’ , ’[CR1]’ , ’[Z][R8]’ , ’[Z1]’ , \n’[Z2]’ , ’[Z3]’ , ’[Z4]’ , ’[Z5]’ , ’[Z6]’ , ’[Z7]’ , ’[Z8]’ , ’[Z9]’ , ’[Z10]’ , \n’[(Z1)a]’ , ’[(Z3)e]’ , ’[(Z)n]’ , ’[D1]’ , ’[D2]’ , ’[D3]’ , ’[D4]’ , \n’[D5]’ , ’[D6]’ , ’[Y]’ , ’[Y1]’ , ’[Y2]’ , ’[Y3]’ , ’[Y4]’ , ’[(Y)n]’ , ’[Ar]’ , \n’[Ar1]’ , ’[Ar2]’ , ’[Ar3]’ , ’[G]’ , ’[G1]’ , ’[G2]’ , ’[G3]’ , ’[G4]’ , \n’[(G)n]’ , ’[X0]’ , ’[X1]’ , ’[X2]’ , ’[X3]’ , ’[X4]’ , ’[X5]’ , ’[X6]’ , ’[Q]’ , \n’[Q1]’ , ’[Q2]’ , ’[L]’ , ’[L1]’ , ’[L2]’ , ’[L3]’ , ’[L4]’ , ’[E]’ , ’[E1]’ , \n’[E2]’ , ’[A1]’ , ’[A2]’ , ’[A3]’ , ’[A4]’ , ’[A5]’ , ’[A6]’ , ’[A7]’ , ’[A8]’ , \n’[(CH2)r]’ , ’[(CH2)p]’ , ’[(CH2)q]’ , ’[(CH2)m]’ , ’[(CH2)\nn]’ , ’[(CH2)s]’ , ’[(CH2)v]’ , ’[(CH2)b]’ , ’[(CH2)c]’ , ’[(CH2)\nz]’ , ’[(CH)n]’ , ’[(C)t]’ , ’[(C)m]’ , ’[(C)n]’ , ’[Hal]’ , ’[M]’ , ’[(L2)\nn]’ , ’[J]’ , ’[J1]’ , ’[V]’ , ’[()x]’ , ’[B1]’ , ’[B2]’ , ’[B3]’ , ’[U]’ , ’[Het]’ , \n’[La]’ , ’[Ea]’ , ’[Eb]’ , ’[Ec]’ , ’[Lb2]’ , ’[M1]’ , ’[M2]’ , ’[M3]’ , \n’[Xa]’ , ’[Xb]’ , ’[*]’ , ’[**]’ , ’[#]’ , ’[XH]’ , ’[(X)n]’ , ’[(A1)e]’ , ’[(A2)\nh]’ , ’[Et]’ , ’[Cy2]’ , ’[a]’ , ’[P1]’ , ’[SOm]’ , ’[F,Cl,Br,I]’ .\nAbbreviations\nOCSR: Optical chemical structure recognition; JEPG: Joint photographic \nexperts group; PNG: Portable network graphics; GIF: Graphics interchange \nformat; CNN: Convolutional neural network; RNN: Recurrent neural network; \nLSTM: Long short-term memory; GRU : Gated recurrent unit; NCI: National can-\ncer institute; SDF: Simulation description format; MLP: Multilayer perceptron; \nCDK: Chemistry development kit; SMILES: Simplified molecular-input line-\nentry system; CE: Cross entropy; MFL: Multi-label focal loss; W-MSA: Window \nmulti-head self attention; SW-MSA: Shift window multi-head self attention.\nAcknowledgements\nGratitude towards Zihao Shen, School of Pharmacy, East China University of \nScience and Technology, for coordinating the experimental computers.\nAuthor contributions\nZX and JL wrote the main manuscript text and prepared all figures and tables \ncollaboratively. ZY constructed the data set for the model. ZX trained the \nmodel. JL, SL and HL designed all experiments collaboratively. All authors read \nand approved the final manuscript.\nFunding\nNational Key R&D Program of China (under Grant No. 2016YFA0502304) and \nImportant Drug Development Fund, Ministry of Science and Technology of \nChina (2018ZX09735002).\nAvailability of data and materials\nThe dataset and source code supporting the conclusions of this article are \navailable in the [SwinOCSR] repository, [unique persistent identifier and hyper-\nlink to dataset in https:// github. com/ suanf axiao huo/ SwinO CSR/ tree/ main].\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing financial interest.\nAuthor details\n1 School of Information Science and Engineering, East China University of Sci-\nence and Technology, 130 Mei Long Road, Shanghai 200237, China. 2 State Key \nLaboratory of Bioreactor Engineering, Shanghai Key Laboratory of New Drug \nDesign, School of Pharmacy, East China University of Science and Technology, \nShanghai 200237, China. \nReceived: 9 February 2022   Accepted: 12 June 2022\nReferences\n 1. McDaniel JR, Balmuth JR (1992) Kekule: OCR-optical chemical (structure) \nrecognition. J Chem Informat Comput Sci 32:373–378\n 2. Casey R, Boyer S, Healey P , Miller A, Oudot B, Zilles K (1993) Optical recogni-\ntion of chemical graphics. In: Proceedings of 2nd International Conference \non Document Analysis and Recognition (ICDAR’93). IEEE, pp 627–631\n 3. Ibison P , Jacquot M,Kam F et al (1993) Chemical literature data extraction: \nthe CLiDEProject. J Chem Inf Comput Sci 33:338–344\nPage 13 of 13\nXu et al. Journal of Cheminformatics           (2022) 14:41 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 4. Frasconi P , Gabbrielli F, Lippi M, Marinai S (2014) Markov logic networks for \noptical chemical structure recognition. J Chem Inf Model 54:2380–2390. \nhttps:// doi. org/ 10. 1021/ ci500 2197\n 5. Park J, Rosania GR, Shedden KA, Nguyen M, Lyu N, Saitou K (2009) Auto-\nmated extraction of chemical structure information from digital raster \nimages. Chem Cent J 3:4. https:// doi. org/ 10. 1186/ 1752- 153X-3-4\n 6. Filippov IV, Nicklaus MC (2009) Optical structure recognition software \ntorecover chemical information: OSRA, an open source solution. J Chem \nInfModel. 49:740-743. https:// doi. org/ 10. 1021/ ci800 067r\n 7. Vinyals O, Toshev A, Bengio S, Erhan D (2015) Show and tell: A neural image \ncaption generator. In: Proceedings of the IEEE conference on computer \nvision and pattern recognition. pp 3156–3164\n 8. Szegedy C, Liu W, Jia Y et al. (2015) Going deeper withconvolutions. In: \nProceedings of theIEEE conference on computer vision and pattern recog-\nnition. pp 1–9\n 9. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Com-\nputat 9:1735–1780\n 10. Cho K, Van Merriënboer B,Gulcehre C, Bahdanau D, Bougares F, Schwenk \nH, Bengio Y (2014) Learning phraserepresentations using RNN encoder-\ndecoder for statistical machine translation. Preprint at https:// doi. org/ 10. \n48550/ arXiv. 1406. 1078\n 11. Xu K, Ba J, Kiros K et al. (2015) Show, attend and tell:Neural image caption \ngeneration withvisual attention. In: International conference on machine \nlearning, PMLR, pp2048–2057\n 12. Vaswani A, ShazeerN, Parmar N et al. (2017) Attention is all you need. In: \nAdvances inneural information processing systems. pp 5998–6008\n 13. Staker J, Marshall K, Abel R, McQuaw CM (2019) Molecular structure extrac-\ntion from documents using deep learning. J Chem Inf Model 59:1017–1029. \nhttps:// doi. org/ 10. 1021/ acs. jcim. 8b006 69\n 14. Rajan, K., Zielesny, A. & Steinbeck, C.(2020) DECIMER: towards deep learning \nfor chemical image recognition. JCheminform 12, 65.https:// doi. org/ 10. \n1186/ s13321- 020- 00469-w\n 15. Rajan K, Zielesny A, Steinbeck C (2021) DECIMER 1.0: Deep Learning for \nChemicalImage Recognition using Transformers. J Cheminform 13, 61. \nhttps:// doi. org/ 10. 1186/ s13321- 021- 00538-8.\n 16. Khokhlov I, Krasnov L, Fedorov M, Sosnin S (2021) Image2SMILES: \ntransformer-based molecular optical recognition engine. Chem Meth \n2:e202100069\n 17. Liu Z, Lin Y, Cao Y et al. (2021) Swin transformer:Hierarchical vision trans-\nformer using shifted windows. in: Proceedings ofthe IEEE/CVF International \nConference on Computer Vision (ICCV). pp 10012-10022. https:// doi. org/ 10. \n1109/ ICCV4 8922. 2021. 00986\n 18. Lin T-Y, Goyal P , Girshick R, He K, Dollár P (2017) Focal loss for dense object \ndetection. In: Proceedings of the IEEE international conference on computer \nvision. pp 2980–2988\n 19. O’Boyle N, Dalke, A.(2018). DeepSMILES:An adaptation of SMILES for use \nin machine-learning of chemical structures. Preprint at https:// doi. org/ 10. \n26434/ chemr xiv. 70979 60. v1\n 20. Weininger D (1988) SMILES, a chemical language and information system. \n1 Introduction to methodology and encoding rules. J Chem Informat \nComput Sci 28:31–36\n 21. Valko AT, Johnson AP (2009) CLiDE Pro: the latest generation of CLiDE, a tool \nfor optical chemical structure recognition. J Chem Inf Model 49:780–787. \nhttps:// doi. org/ 10. 1021/ ci800 449t\n 22. Fujiyoshi A, Nakagawa K, Suzuki M Robust method of segmentation and \nrecognition of chemical structure images in cheminfty. In: Pre-Proceedings \nof the 9th IAPR International Workshop on Graphics Recognition, GREC, \n2011.\n 23. Sadawi NM, Sexton AP , Sorge V Chemical structure recognition: a rule-based \napproach. In: Document Recognition and Retrieval XIX, 2012 2012. Interna-\ntional Society for Optics and Photonics, p 82970E\n 24. Clevert D-A, Le T, Winter R, Montanari F(2021) Img2Mol-Accurate \nSMILES Recognitionfrom Molecular Graphical Depictions. Chem Sci. \n12:14174-14181.\n 25. Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the \ninception architecture for computer vision. In: Proceedings of the IEEE \nconference on computer vision and pattern recognition. pp 2818–2826\n 26. Tan M, Le Q (2019) Efficientnet: Rethinking model scaling for convolutional \nneural networks. In: International Conference on Machine Learning. PMLR, \npp 6105–6114\n 27. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recogni-\ntion. In: Proceedings of the IEEE conference on computer vision and pattern \nrecognition, pp 770–778\n 28. Steinbeck C, Han Y, Kuhn S, Horlacher O, Luttmann E, Willighagen E (2003) \nThe chemistry development kit (CDK): an open-source Java library for \nChemo- and Bioinformatics. J Chem Inf Comput Sci 43:493–500. https:// doi. \norg/ 10. 1021/ ci025 584y\n 29. Kim S, Chen J, Cheng T et al (2019) PubChem 2019 update:improved access \nto chemical data.Nucleic Acids Res 47:1102–1109. https:// doi. org/ 10. 1093/ \nnar/ gky10 33\n 30. Papineni K, Roukos S, Ward T, Zhu W-J (2002) BLEU: a method for automatic \nevaluation of machine translation. In: Proceedings of the 40th annual meet-\ning of the Association for Computational Linguistics, pp 311–318\n 31. Lin C-Y, Och FJ (2004) Automatic evaluation of machine translation quality \nusing longest common subsequence and skip-bigram statistics. In: Pro-\nceedings of the 42nd Annual Meeting of the Association for Computational \nLinguistics (ACL-04), pp 605–612\n 32. Kingma DP , Ba J (2014) Adam: A method for stochastic optimization. Pre-\nprint at https:// doi. org/ 10. 48550/ arXiv. 1412. 6980\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}