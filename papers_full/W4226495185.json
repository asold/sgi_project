{
  "title": "Siamese Network with Interactive Transformer for Video Object Segmentation",
  "url": "https://openalex.org/W4226495185",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2096488617",
      "name": "Meng Lan",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2282897900",
      "name": "Fengxiang He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104063197",
      "name": "Lefei Zhang",
      "affiliations": [
        "Wuhan University of Technology",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2282897900",
      "name": "Fengxiang He",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104063197",
      "name": "Lefei Zhang",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6729741790",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3006955907",
    "https://openalex.org/W6792881045",
    "https://openalex.org/W3122411985",
    "https://openalex.org/W3166738350",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6698183232",
    "https://openalex.org/W3157501078",
    "https://openalex.org/W2902397856",
    "https://openalex.org/W3034320401",
    "https://openalex.org/W3003495958",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3043785186",
    "https://openalex.org/W2884135617",
    "https://openalex.org/W2799157347",
    "https://openalex.org/W2933875464",
    "https://openalex.org/W2564998703",
    "https://openalex.org/W3010396118",
    "https://openalex.org/W3043563708",
    "https://openalex.org/W2916797271",
    "https://openalex.org/W2724418412",
    "https://openalex.org/W6790760727",
    "https://openalex.org/W2905569329",
    "https://openalex.org/W3135982274",
    "https://openalex.org/W2889986507",
    "https://openalex.org/W2983890170",
    "https://openalex.org/W3175227919",
    "https://openalex.org/W3092944026",
    "https://openalex.org/W3105084609",
    "https://openalex.org/W3016743283",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963732700",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3034798428",
    "https://openalex.org/W2963227409",
    "https://openalex.org/W3175132347",
    "https://openalex.org/W3160550216",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2916743882",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3110030584",
    "https://openalex.org/W4287330664",
    "https://openalex.org/W3108819577",
    "https://openalex.org/W3170630188",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3034538699",
    "https://openalex.org/W3035042565",
    "https://openalex.org/W3117097536",
    "https://openalex.org/W2998434318",
    "https://openalex.org/W3177322837",
    "https://openalex.org/W2990205821",
    "https://openalex.org/W2963503215",
    "https://openalex.org/W2963253279",
    "https://openalex.org/W2964218467",
    "https://openalex.org/W3109340983"
  ],
  "abstract": "Semi-supervised video object segmentation (VOS) refers to segmenting the target object in remaining frames given its annotation in the first frame, which has been actively studied in recent years. The key challenge lies in finding effective ways to exploit the spatio-temporal context of past frames to help learn discriminative target representation of current frame. In this paper, we propose a novel Siamese network with a specifically designed interactive transformer, called SITVOS, to enable effective context propagation from historical to current frames. Technically, we use the transformer encoder and decoder to handle the past frames and current frame separately, i.e., the encoder encodes robust spatio-temporal context of target object from the past frames, while the decoder takes the feature embedding of current frame as the query to retrieve the target from the encoder output. To further enhance the target representation, a feature interaction module (FIM) is devised to promote the information flow between the encoder and decoder. Moreover, we employ the Siamese architecture to extract backbone features of both past and current frames, which enables feature reuse and is more efficient than existing methods. Experimental results on three challenging benchmarks validate the superiority of SITVOS over state-of-the-art methods. Code is available at https://github.com/LANMNG/SITVOS.",
  "full_text": "Siamese Network with Interactive Transformer for Video Object Segmentation\nMeng Lan1, Jing Zhang2, Fengxiang He3, Lefei Zhang1,4*\n1 Wuhan University\n2 The University of Sydney\n3 JD Explore Academy, China\n4 Hubei Luojia Laboratory\n{menglan, zhanglefei}@whu.edu.cn, jing.zhang1@sydney.edu.au, hefengxiang@jd.com\nAbstract\nSemi-supervised video object segmentation (VOS) refers to\nsegmenting the target object in remaining frames given its an-\nnotation in the first frame, which has been actively studied in\nrecent years. The key challenge lies in finding effective ways\nto exploit the spatio-temporal context of past frames to help\nlearn discriminative target representation of current frame. In\nthis paper, we propose a novel Siamese network with a specif-\nically designed interactive transformer, called SITVOS, to en-\nable effective context propagation from historical to current\nframes. Technically, we use the transformer encoder and de-\ncoder to handle the past frames and current frame separately,\ni.e., the encoder encodes robust spatio-temporal context of\ntarget object from the past frames, while the decoder takes the\nfeature embedding of current frame as the query to retrieve\nthe target from the encoder output. To further enhance the\ntarget representation, a feature interaction module (FIM) is\ndevised to promote the information flow between the encoder\nand decoder. Moreover, we employ the Siamese architecture\nto extract backbone features of both past and current frames,\nwhich enables feature reuse and is more efficient than existing\nmethods. Experimental results on three challenging bench-\nmarks validate the superiority of SITVOS over state-of-the-\nart methods. Code: https://github.com/LANMNG/SITVOS.\nIntroduction\nVideo object segmentation (VOS) refers to separating the\nforeground from the background in all frames of a given\nvideo (Pont-Tuset et al. 2017; Xu et al. 2018). As an impor-\ntant tool for video editing and many other down-stream ap-\nplications, it has recently gained increasing attention (Zhang\nand Tao 2020). In this work, we study the challenging semi-\nsupervised VOS problem, which aims at finding the pixel-\nlevel position of target objects in a short video, only given\nthe ground truth masks of the objects in the first frame. Due\nto the variance in appearance and scale of the target objects\nover time as well as the similar appearance ambiguity issue\nin the background, semi-supervised VOS is very challenging\nand actively studied. In this paper, if not specified, the term\n‚ÄúVOS‚Äù refers to semi-supervised VOS for simplicity.\nA critical problem in VOS is how to exploit the spatio-\ntemporal context of target objects in historical frames to\n*Corresponding author.\nCopyright ¬© 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFrame (a) Backbone (b) Self-attention (c) Feature interaction\nFigure 1: Visualized feature maps from our SITVOS. (a) The\nfeature map from backbone. (b) The feature map after self-\nattention. (c) The feature map after feature interaction. The\nfeature representation of target object is gradually enhanced.\nguide the object segmentation process of current frame, and\nmany explorations have been made to implement the infor-\nmation propagation across frames. RGMP (Oh et al. 2018)\npropagates the object context of the first and previous frames\nto the current frame by concatenating the features of these\nframes. This is an intuitive yet simple strategy and brings im-\nprovement in performance, although the feature-level con-\ncatenation may result in coarse segmentation and is not ro-\nbust to occlusion and drifting. To obtain more accurate re-\nsults, researchers propose the matching-based methods to\nachieve the pixel-level object information propagation. STM\n(Oh et al. 2019) encodes the past frames into a memory\nand uses the current frame as a query to read the memory,\nwhich is a non-local pixel matching, to obtain the target ob-\nject representation, achieving high accuracy. However, sim-\nple global matching is vulnerable to interference from back-\nground distractors and requires large computational cost.\nRecently, some researchers try to solve these problems from\nnew perspectives. CFBI (Yang, Wei, and Yang 2021) sepa-\nrates the feature embeddings into the foreground object and\nbackground context to implicitly make them more discrim-\ninative and improve the segmentation. RMNet (Xie et al.\n2021) proposes the local-to-local matching solution by con-\nstructing the local region memory and query regions based\non optical flow. Although the current matching-based ap-\nproaches compute the correspondences of the pixels in the\nquery frame against pixels in each reference frame, they do\nnot explicitly model the temporal dependency of the target\nobject among the referenced historical frames. which cannot\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n1228\nguarantee to learn robust and discriminative target feature\nrepresentation for VOS. Besides, previous approaches typi-\ncally take the target mask of past frames as prior to explicitly\nenhance the object representation in the memory and main-\ntain two different encoders for memory and query, making\nthe model is computationally inefficient.\nIn this paper, inspired by the superior capability of trans-\nformer in capturing long-range dependencies, we propose a\nsimple yet effective pipeline, termed SITVOS, for VOS task.\nDifferent from most STM-based methods, which maintain\ntwo independent encoders, i.e., memory encoder for the past\nframes with corresponding object mask and query encoder\nfor current frame, our SITVOS employs a Siamese network\narchitecture to extract the feature embeddings of the past\nand current frames from a shared backbone while adopting a\nlight-weight encoder for mask embedding. The decoupling\nof frame and object mask allows the feature of current frame\nto be cached and reused later as the memory feature, thus im-\nproving the information flow and computational efficiency.\nMoreover, we also devise an interactive transformer to al-\nlow effective feature learning, as shown in Fig. 1. Specifi-\ncally, the past frame embeddings are fed into the transformer\nencoder to model the spatio-temporal dependency of target\nobject among the referenced historical frames, therefore im-\nproving the feature representation of target object. Then, the\ncurrent frame embeddings and the above encoder output are\nfed into the transformer decoder, where the spatial depen-\ndency of target object in the query frame can be efficiently\nmodeled via the self-attention while the cross-attention en-\nables to retrieve and aggregate target information from past\nframes to highlight the target object for better segmentation\nperformance. To further bridge the object information prop-\nagation between the past and current frames, we design a\nfeature interaction module within the interactive transformer\nbased on cross-attention, i.e., the feature embeddings from\nthe encoder and decoder separately serve as the query to at-\ntend the other via cross-attention to enhance the target rep-\nresentation mutually. Finally, the output embeddings of the\ntransformer decoder are fed into a segmentation decoder to\npredict the final segmentation result.\nThe contribution of this paper is threefold.\n‚Ä¢ We propose to leverage transformer encoder and decoder\nto efficiently model the spatio-temporal dependency of\ntarget objects among the referenced historical frames\nas well as the spatial dependency of target object in\nthe query frame, allowing effective feature learning and\nmatching.\n‚Ä¢ We devise a feature interaction module (FIM) within the\ntransformer to bridge the target information interaction\nbetween past and current frames to enhance the target\nrepresentation mutually.\n‚Ä¢ SITOVS has a Siamese network architecture that allows\nfeature reuse and is computationally efficient. It matches\nthe performance of state-of-the-art (SOTA) methods on\nthree popular benchmarks while running faster.\nRelated Work\nSemi-supervised Video Object Segmentation\nIn the early stage of this field, VOS methods are almost\nbased on online learning, which first fine-tune on the ground\ntruth in the first frame and then perform the inference on the\nrest of the test frames. OSVOS (Caelles et al. 2017) is the\npioneering work in this direction using deep convolutional\nneural networks, which fine-tunes a pre-trained foreground\nsegmentation model on the first frame. OnA VOS (V oigtlaen-\nder and Leibe 2017) extends OSVOS by introducing an on-\nline adaptation strategy, which adopts highly confident pre-\ndictions into the fine-tuning process. However, they suffer\nfrom the high computation cost of the fine-tuning process.\nTo address this issue, recent works turn to offline learning,\nwhich exploit the given mask prior in the first frame and the\nintermediate predictions as reference to directly guide the\nobject segmentation of current frame. They can be roughly\ngrouped into two categories. First, propagation-based meth-\nods learn an object mask propagator by introducing the\nobject mask features from historical frames to the current\nframe(Perazzi et al. 2017; Oh et al. 2018; Lan et al. 2020) .\nFor example, RGMP (Oh et al. 2018) concatenates features\nof the first, previous, and current frames to explicitly en-\nhance the target representation. SAT (Chen et al. 2020) up-\ndates a dynamic global feature of the target and propagates\nto the current inference. Second, matching-based methods\nfind the target objects in the current frame by calculating\nthe pixel-level similarity with the past frames (V oigtlaender\net al. 2019; Oh et al. 2019; Li, Shen, and Shan 2020; Xie\net al. 2021). FEELVOS (V oigtlaender et al. 2019) proposes a\nglobal and a local pixel-level matching mechanism to gather\ninformation from the first and previous frames, respectively.\nRecently, the STM network (Oh et al. 2019) is proposed\nto propagate the non-local object information, which has\nbeen a solid baseline in VOS task for its simple architec-\nture and competitive performance (Seong, Hyun, and Kim\n2020; Wang et al. 2021). GC (Li, Shen, and Shan 2020) im-\nproves the STM architecture by only using a fixed-size fea-\nture representation and updates a global context to guide the\nsegmentation of current frame. RMNet (Xie et al. 2021) uses\nthe optical flow to get the target region and performs local-\nto-local matching, which effectively mitigates the ambiguity\nof similar objects in both memory and query frames.\nVision Transformers\nTransformer is first proposed in (Vaswani et al. 2017) for\nmachine translation. Recently, transformer has witnessed\ngreat success in vision tasks like image classification (Doso-\nvitskiy et al. 2020; Xu et al. 2021; Liu et al. 2021), object\ndetection (Carion et al. 2020; Zhu et al. 2020), and seman-\ntic segmentation (Zheng et al. 2021). ViT (Dosovitskiy et al.\n2020) first applies the transformer to image classification by\nsplitting an image into patches and provides the sequence\nembeddings of these patches as input to the transformer.\nDETR (Carion et al. 2020) adopts a transformer with a fixed\nset of learned object queries to reason about the relations be-\ntween objects and global image context, and directly outputs\nthe final set of predictions in parallel.\n1229\nCurrent frame\nPast frames\nCNN\nCNN\n1 x1 \nconv\nreshape\n1 x1 \nconv\nreshape\nWeight Sharing\n‚Ä¶  ‚Ä¶‚Ä¶\nMemory \nembedding\nQuery \nembedding\nCross-Attention\nFeature Interaction Module\nSegmentation \nDecoder\nSelf-Attention Self-Attention\nùëÄùëúùëüùëñ\nùëÄùëÜùê¥\nùëÑùëÜùê¥\nLight-weight Encoder\nMask embeddingùëÄùê∏\nùëÄùëúùë¢ùë°\nùëÑùëúùë¢ùë°\nInteractive Transformer\nFigure 2: The framework of our SITVOS, which consists of three parts: 1) the Siamese network extracts features of the past and\ncurrent frames, 2) the interactive transformer promotes the feature representation in the encoder and decoder, and propagates\nobject cues from the past to the current frame, and 3) the segmentation decoder produces the final segmentation result.\nMethod\nArchitecture Overview\nSITVOS is illustrated in Fig. 2, which is well suited for both\nsingle-object and multi-object segmentation. Specially, for\nthe multi-object segmentation, it predicts the segmentation\nmask for each object in a single forward pass and merges\nthe predicted maps to generate the final segmentation re-\nsult rather than repeating single object segmentation mul-\ntiple times. SITVOS consists of three parts, i.e., Siamese\nnetwork for feature extraction, interactive transformer for\ntarget information propagation, and segmentation decoder\nfor mask prediction. First, the features of past frames and\ncurrent frame are extracted by the two parallel branches of\nSiamese network respectively, and then they are further em-\nbedded via an 1√ó1 convolutional layer. Then, the past frame\nfeatures are reshaped to the memory embeddings and fed\ninto the transformer encoder, while the current frame fea-\nture is transformed to query embeddings. Together with the\nencoder output, they are fed into to the transformer decoder,\nwhich retrieves and aggregates the object cues from the past\nframes to the current one to enhance the target representation\nfor segmentation. Moreover, we devise a feature interaction\nmodule within the transformer to promote target informa-\ntion propagation between past frames and current frame and\nenhance the target representation mutually. The transformer\noutput is fed into the segmentation decoder to generate the\nfinal segmentation result.\nSiamese Network for Feature Extraction\nSiamese network is a widely used architecture in the field\nof video processing, especially for object tracking (Xu et al.\n2020; Wang et al. 2019). Here we adopt a Siamese network\narchitecture for feature extraction from the RGB frames and\na light-weight encoder for the object mask. As shown in Fig.\n2, the two branches of Siamese network extract the features\nof the past and current frames respectively, where the fea-\ntures of past frames are further embedded and stacked along\nthe temporal dimension and reshaped to memory embedding\n(Mori ‚àà RTHW √óC), and the features of current frame is\ntransformed to query embedding (Q ori ‚àà RHW √óC). The\nobject mask embedding from the light-weight encoder is de-\nnoted as ME ‚àà RTHW √óC. Here, T is the number of in-\nvolved past frames, H and W are the height and weight\nof the features, and C is the channel number. We employ\nResNet50 (He et al. 2016a) as the backbone of Siamese net-\nwork and the two branches share the same weights. Follow-\ning the setting of STM (Oh et al. 2019), we remove the\nlast stage of ResNet50 and take the output of the fourth\nstage with stride 16 as the extracted feature. ResNet18 is\nadopted as the light-weight mask encoder and the input\nchannel of the first convolutional layer is changed to 1 to\nadapt to object mask. Notably, due to the weight sharing of\nthe Siamese network, the extracted feature of current frame\ncould be cached and reused in the subsequent inference pro-\ncess, which makes SITVOS more computationally efficient\ncompared with previous STM-based architectures.\nInteractive Transformer\nThe structure of interactive transformer is shown in the mid-\ndle part of Fig. 2. Similar to the traditional transformer\n(Vaswani et al. 2017), our transformer employs the encoder-\ndecoder architecture. However, we make some modifica-\ntions to adapt our transformer to the Siamese-like frame-\nwork as well as the VOS task. First, we separate the encoder\n1230\nand decoder as two branches. The encoder takes the mem-\nory embeddings as input and models the spatio-temporal\ndependency of the target objects among the past frames\nvia self-attention. The decoder receives the encoder out-\nput and current frame feature as input and leverages cross-\nattention to propagate the temporal context. Second, we\ndevise the feature interaction module (FIM) within trans-\nformer to further promote the information communication\nbetween the encoder and decoder. Third, to achieve a de-\ncent balance between segmentation accuracy and inference\nspeed, we simplify the classic transformer by removing the\nfully connected feed-forward network and only maintaining\na lightweight single-head attention.\nTransformer encoder The transformer encoder consists\nof a self-attention (SA) block. An attention function can be\ndescribed as mapping a query and a set of key-value pairs to\nan output, which is computed as a weighted sum of the val-\nues, where the weight assigned to each value is the similarity\nbetween the query and the corresponding key. Here, follow-\ning (Vaswani et al. 2017), we adopt the scaled dot-product\nattention with residual connection and layer normalization\nto implement self-attention (as well as cross-attention (CA))\nblock, which could be formulated as follows:\nAttention(Q, K, V) = softmax\n\u0012QKT\n‚àödk\n\u0013\nV, (1)\nwhere Q ‚àà RNq√ódk , K‚àà RNq√ódk and V ‚àà RNq√ódv are the\nquery, key and value, respectively.dk is the channel dimen-\nsion of query and key, and ‚àödk is temperature parameter\nwhich controls the softmax distribution. For SA block, Q,\nK,V are the same, while they could be various in CA block.\nIn the encoder, the memory embedding Mori is sent into\nthe self-attention block, where Mori is first converted to\nquery, key and value via linear projections. The SA block\nmodels the spatio-temporal dependency among all the in-\nvolved past frames and enhances the feature representa-\ntion of target objects, which is beneficial to the subsequent\npixel-level propagation of object cues. The output MSA ‚àà\nRTHW √óC is then fed into FIM in the transformer decoder,\nwhich will be described as follows.\nTransformer decoder The transformer decoder is com-\nposed of a SA block, the FIM that will be detailed in the\nnext part, and a CA block. Similar to the encoder, the query\nembedding Qori first goes through a SA block to obtain\nQSA ‚àà RHW √óC. Then, MSA, Mori, and QSA are fed into\nFIM to generate Qout ‚àà RHW √óC and Mout ‚àà RTHW √óC\nrespectively. They are used as the input to the CA block,\nwhere Qout serves as the query and Mout acts as key and\nvalue. Finally, the outputTout ‚àà RHW √óC of the transformer\ncould be obtained as follows:\nTout = LN(Attention(Q, K, V) +Qout), (2)\nwhere Q = QoutWQ, K = MoutWK, V = MoutWV . WQ ‚àà\nRC√ódk , WK ‚àà RC√ódk and WV ‚àà RC√óC are linear pro-\njection weights with C = 256 and dk = 64. LN denotes\nLayer Normalization. The same hyper-parameter settings\nare adopted in the remaining attention formulations.\nCross-AttentionCross-Attention\nAdd & Norm Add & Norm\nùëÄùëúùëüùëñ\nùëÄùê∏\nùëÄùëÜùê¥ ùëÑùëÜùê¥\nùëû ùëò ùë£ ùëûùëòùë£\nùëÄùëúùë¢ùë° ùëÑùëúùë¢ùë°\nPast Current\nFigure 3: Diagram of the feature interaction module.\nFeature Interaction Module In the traditional trans-\nformer architecture, the sequence embeddings go through\nthe encoder and decoder in serial order. While in our frame-\nwork, the encoder and the SA block in the decoder are par-\nallel branches and take the past and current frames as input\nrespectively, which both contain the target object informa-\ntion. Inspired by (Chen et al. 2021), we argue that in addi-\ntion to propagating the spatio-temporal information of target\nobjects from the past to the current frame to enhance the tar-\nget representation in current frame, the current frame can\nalso be used as a reference to reinforce the target feature\nrepresentation in the past frames. Therefore, we design FIM\nbased on cross-attention for information interaction between\nthe encoder and the SA block in the decoder.\nAs depicted in Fig. 3, FIM consists of a separate CA block\nfor each branch. The CA block for the encoder branch takes\nMSA and QSA as input, where MSA serves as the query to\ncompute the similarity with the key QSA and retrieve the\nobject information from the value QSA. The output of this\nCA block Mout can be calculated as follows:\nMout = LN(Attention(Q, K, V) +MSA), (3)\nwhere Q = MSAWQ, K = QSAWK, V = QSAWV .\nFor the CA block after the SA block in the decoder, QSA\nand MSA are the query-key pair. The mask embedding ME\nand Mori are element-wise multiplied to generate a new\nembeddings Mx ‚àà RTHW √óC, where ME is used to filter\nthe background distractors and provide more accurate object\ncues. Mx acts as the value term and propagates the object\ninformation based on the similarity matrix computed by the\nQSA and MSA. The output Qout can be obtained as follows:\nQout = LN(Attention(Q, K, V) +QSA), (4)\nwhere Q = QSAWQ, K= MSAWK, V= MxWV .\nSegmentation Decoder\nThe segmentation decoder takes the interactive transformer\noutput Tout as input and predicts the object mask in the cur-\nrent frame. Like STM (Oh et al. 2019), we use the refine-\nment module as the basic block of the decoder. Tout is first\nreshaped and converted to 256-channel feature via a convo-\nlutional layer and a residual block (He et al. 2016b). Then,\n1231\ntwo refinement modules gradually upscale the feature map\nby a factor of two each time. The refinement module takes\nboth the output of the previous module and a feature map\nfrom feature extractor at the corresponding scale through\nskip-connections. A 2-channel convolutional layer followed\nby a softmax operation is attached behind the last refinement\nmodule to produce the predicted object mask at 1/4 scale of\nthe input image. Finally, we use bi-linear interpolation to\nupscale the predicted mask to the original scale. Every con-\nvolutional layer in the decoder uses 3√ó3 kernel, producing\n256-channel output except for the last 2-channel one.\nImplementation Details\nTraining Following most advanced methods (Wang et al.\n2021; Oh et al. 2019; Xie et al. 2021; Li, Shen, and Shan\n2020), we adopt the two-stage training strategy. In the first\nstage, we pre-train our SITVOS model on simulated video\nclips generated upon MS-COCO dataset (Lin et al. 2014).\nSpecifically, we randomly crop foreground objects from a\nstatic image and then pasted them onto a randomly sampled\nbackground image to form a simulated image. Affine trans-\nformations, such as rotation, resizing, sheering, and transla-\ntion, are applied to foreground and background separately\nto generate a 3-frame video clip mimicking deformation and\nocclusion scenarios. The pre-training helps our model to be\nrobust against a variety of object appearance and categories.\nIn the second stage, we finetune the pre-trained model on\nthe real video data DA VIS 2017 (Pont-Tuset et al. 2017) and\nYouTube-VOS (Xu et al. 2018). Three temporally ordered\nframes are sampled from a training video to form a training\nsample and the interval of sampled frames are randomly se-\nlected from 0 to 25 to simulate the appearance change over\na long time. SITVOS is implemented in Pytorch and trained\nusing RTX 2080Ti GPU. The input image size is 384√ó 384\nand batchsize is 4 for both training stages. We minimize the\ncross-entropy loss using the Adam optimizer with a learning\nrate starting at 1e-5. The learning rate is adjusted with poly-\nnomial scheduling using the power of 0.9. All batch normal-\nization layers in the backbone are fixed as their ImageNet\npre-trained parameters during training.\nInference Given a test video with the annotation masks of\nthe first frame, SITVOS sequentially segments each frame\nin only a single forward pass. Since using all past frames\nas the memory may result in overflow of GPU memory and\nslow running speed, we adopt a dynamic intermediate frame\nutilization strategy. Similar to STM, we select the first and\nprevious frame into the memory frames. However, instead of\nsaving intermediate frames to memory frames at a fixed in-\nterval, which is limited by the GPU memory and long video\nsequence, and thus cannot use more intermediate frame in-\nformation in a relatively short video sequence, we dynami-\ncally sample the intermediate frame but fix the total number\nof memory frames as N based on the GPU memory limita-\ntion. We set N = 7in our paper.\nExperiments\nDatasets and Evaluation Metrics\nSITOVS is evaluated on three benchmark datasets, namely\nDA VIS 2016-Val for single-object segmentation, DA VIS\n2017-Val and YouTube-VOS validation sets for multi-object\nsegmentation. The DA VIS 2016 validation set comprises\n20 videos while the DA VIS 2017 validation set extends\nthe DA VIS 2016 validation set to 30 videos with multiple\nobjects annotations. The official YouTube-VOS validation\nset has 474 video sequences with objects from 91 classes.\nAmong them, 26 classes are not present in the training set.\nFor the DA VIS datasets, we adopt the official performance\ncriteria, i.e., the Jaccard index (J ) to denote the mIoU\nbetween the predicted and the ground-truth masks, the F-\nmeasure (F) to represent the contour accuracy, and the over-\nall score J &F which is the mean of the J and F. In addi-\ntion, inference speed in frames per second (FPS) is also re-\nported. As for YouTube-VOS dataset, we calculateJ and F\nscores for classes included in the training set (seen) and the\nones that are not (unseen). The overall score G is computed\nas the average over all four scores.\nComparison with State-of-the-art\nDA VIS 2017We first compare SITVOS with SOTA meth-\nods on the multi-object DA VIS 2017 validation set. As\nshown in Table 1, SITVOS achieves the best J &F score,\ni.e., 83.5%, at an inference speed of 11.8 FPS. Specially,\ncompared with sparse spatio-temporal transformers based\nSSTVOS (Duke et al. 2021), our SITVOS is 1% higher in\nJ &F while enjoying a simpler pipeline. In the series of ap-\nproaches of using YouTube data for training, SITVOS out-\nperforms STM, Swift and GC, and achieves a comparable\nperformance with the latest LCM and RMNet. In addition,\nsince the reported FPS of the comparison methods in their\npaper are tested on different platforms, a direct comparison\nwill be not fair. Therefore, we test the FPS of those methods\nthat have official code on the same platform (RTX 2080Ti).\nThe results show that our SITVOS achieves the best seg-\nmentation accuracy with a decent inference speed. Some vi-\nsual results are shown in the left of Fig. 4, where we present\nsome challenging scenarios such as occlusion, deformation\nand disappearance of target objects.\nDA VIS 2016Compared with multi-object segmentation,\nthe single object segmentation task in DA VIS 2016 vali-\ndation set is relatively easy. As reported in Table 1, our\nSITVOS attains 90.5% in J &F score and surpasses all the\ncomparison methods except for a slight 0.2% lower than\nLCM. Besides, we find that the methods using additional\nYoutube-VOS data for training have better performance than\nthose without using additional data.\nYoutube-VOS Since not all the annotations of the\nYouTube-VOS validation set are released, we obtain the seg-\nmentation results based on the provided first mask of ob-\njects in each video sequence and then submit the results to\nthe official evaluation server to get the quantitative evalua-\ntion results. The results of SITVOS and SOTA methods are\n1232\nMethod OL DA VIS2016 DA VIS 2017\nJ &F J M FM J &F J M FM FPS\nPReMVOS (Luiten, V oigtlaender, and Leibe 2018) ‚úì 86.8 84.9 88.6 77.8 73.9 81.7 0.01\nOnA VOS (V oigtlaender and Leibe 2017) ‚úì 85.5 86.1 84.9 67.9 64.5 71.2 0.08\nOSVOS (Caelles et al. 2017) ‚úì 80.2 79.8 80.6 60.3 56.7 63.9 0.22\nLCM‚Ä† (Hu et al. 2021) √ó 90.7 89.9 91.4 83.5 80.5 86.5 8.6\nRMNet‚Ä† (Xie et al. 2021) √ó 88.8 88.9 88.7 83.5 81.0 86.0 9.6\nCFBI+‚Ä† (Yang, Wei, and Yang 2021) √ó 89.9 88.7 91.1 82.9 80.1 85.7 6.0\nGIEL (Ge, Lu, and Shen 2021) √ó - - - 82.7 80.2 85.3 6.6\nEGMN‚Ä† (Lu et al. 2020) √ó - - - 82.8 80.2 85.2 5.0\nSSTVOS (Duke et al. 2021) √ó - - - 82.5 79.9 85.1 -\nSTM‚Ä† (Oh et al. 2019) √ó 89.3 88.7 89.9 81.8 79.2 84.3 8.7\nSwift‚Ä† (Wang et al. 2021) √ó 90.4 90.5 90.3 81.8 79.2 84.3 6.3\nFRTM‚Ä† (Robinson et al. 2020) √ó 83.5 - - 76.7 - - 16.3\nTVOS (Zhang et al. 2020) √ó - - - 72.3 69.9 74.7 7.7\nFEELVOS‚Ä† (V oigtlaender et al. 2019) √ó 81.7 81.1 82.2 71.5 69.1 74.0 2.2\nGC‚Ä†(Li, Shen, and Shan 2020) √ó 86.6 87.6 85.7 71.4 69.3 73.5 25.0\nSAT‚Ä† (Chen et al. 2020) √ó 83.1 82.6 83.6 71.2 67.6 74.8 34.0\nAGAME‚Ä† (Johnander et al. 2019) √ó 82.1 82.0 82.2 70.0 67.2 72.7 14.8\nRGMP (Oh et al. 2018) √ó 81.8 81.5 82.0 66.7 64.8 68.6 8.0\nSITVOS‚Ä† √ó 90.5 89.5 91.4 83.5 80.4 86.5 11.8\nTable 1: Results on the DA VIS validation set. OL denotes online fine-tuning.‚Ä† indicates using YouTube-VOS for training.\nreported in Table 2. SITVOS outperforms most recent ap-\nproaches, such as STM, Swift and GIEL, and achieves com-\npetitive overall performance compared with the latest meth-\nods. In particular, SITVOS performs stably in both seen and\nunseen categories, demonstrating its good generalizability.\nSome qualitative results are presented in the right of Fig. 4.\nVersion OL G J s Ju Fs Fu\nPReMVOS ‚úì 66.9 71.4 56.5 75.9 63.7\nOSVOS ‚úì 58.8 59.8 54.2 60.5 60.7\nOnA VOS ‚úì 55.2 60.1 46.1 62.7 51.4\nLCM √ó 82.0 82.2 75.7 86.7 83.4\nRMNet √ó 81.5 82.1 75.7 85.7 82.4\nGIEL √ó 80.6 80.7 75.0 85.0 81.9\nEGMN √ó 80.2 80.7 74.0 85.1 80.9\nSTM √ó 79.4 79.7 72.8 84.2 80.9\nSwift √ó 77.8 77.8 72.3 81.8 79.5\nGC √ó 73.2 72.6 68.9 75.6 75.7\nFRTM √ó 72.1 72.3 65.9 76.2 74.1\nTVOS √ó 67.4 66.7 62.5 69.8 70.6\nAGAME √ó 66.1 67.8 60.8 69.5 66.2\nSAT √ó 63.6 67.1 55.3 70.2 61.7\nRGMP √ó 53.8 59.5 45.2 - -\nSITVOS √ó 81.3 79.9 76.4 84.3 84.4\nTable 2: Results on the YouTube-VOS 2018 validation set.\nAblation Study\nTraining Data We compare the performance of our model\nusing three different training strategies, i.e., pre-training\nonly on COCO dataset, main training only on the DA VIS and\nYoutube-VOS datasets, and full training including both pre-\ntraining and main training. As shown in Table 3, SITVOS\nbenefits from pre-training at a gain of 2.0% J &F and 7.2%\nJ &F on DA VIS 2016 and 2017, respectively, showing that\nthe simulated videos based on the multiple foreground ob-\njects from MS-COCO dataset matters a lot for the multi-\nobject video segmentation task, where SITVOS can learn\ngeneralizable object feature representation via pre-training.\nVariants J &F FPS\nDA VIS 2016 DA VIS 2017\nPre-training only 74.7 66.6 11.8\nMain training only 88.5 76.3 11.8\nFull training 90.5 83.5 11.8\nSTM 89.3 81.8 8.7\nSITVOS w/o FIM 89.2 82.0 14.3\nSITVOS with FIM 90.5 83.5 11.8\nTable 3: Ablation study of the training strategy and FIM in\nour SITVOS on the DA VIS 2016 & 2017 validation set.\nInteractive Transformer Interactive transformer helps to\nbridge the feature representation of the past frames and the\ncurrent frame and propagate the target information from the\n1233\nDA VIS 2017 YouTube-VOS\nFigure 4: Qualitative results of SITVOS on DA VIS 2017 and Youtube-VOS datasets\nPast frame\n(a) Backbone (b) Self-attention (c) Feature interactionCurrent frame\nFigure 5: Visualization of the feature maps in the interactive\ntransformer. The first column are one past frame on the top\nand the current frame on the bottom. (a) The feature maps\nfrom backbone. (b) The feature maps after self-attention. (c)\nThe feature map after feature interaction module.\npast to the current. To validate its effectiveness, we report the\nperformance of SITVOS with and without FIM as well as the\nsolid STM baseline in the bottom rows of Table 3. As can\nbe seen, SITVOS using a naive transformer only achieves\ncomparable performance as STM, although SITVOS runs\nfaster owing to the proposed Siamese architecture. After us-\ning FIM, it brings an improvement of 1.3% J &F and 1.5%\nJ &F over the vanilla SITVOS without FIM on DA VIS\n2016 & 2017 respectively, demonstrating that FIM is crucial\nfor improving the segmentation performance.\nTo further investigate its effectiveness, we visualize the\nfeature maps from different modules in SITVOS, including\nthe backbone, self-attention, and the FIM. As shown in Fig.\n5, although self-attention helps to reduce the activation in\nthe road area, the activation in the background crowd area\nis still large, since the persons in the crowd are similar to\nthe foreground dancer in both appearance and semantics.\nConsequently, the segmentation decoder will be affected by\nthose background feature noise. In contrast, after using FIM,\nthe activation in the background crowd area has been signif-\nicantly reduced. In this way, FIM helps to learn better tar-\nget feature representations for both past frames and current\nframe and improves the segmentation result.\nMemory frame(s) J &F FPS\nDA VIS 2016 DA VIS 2017\nFirst-only 83.8 70.7 21.5\nPrevious-only 86.5 73.5 21.5\nFirst & previous 89.7 81.8 20.8\nEvery 12 frames 90.3 83.1 12.2\nFixed 7 frames 90.5 83.5 11.8\nTable 4: Memory management analysis of SITVOS on the\nDA VIS 2016 & 2017 validation sets. FPS is measured on\nDA VIS 2017.\nMemory Management We compare different memory\nmanagement strategies in Table 4. It can be observed that\nsaving first and previous frames into the memory can also\nachieve competitive performance, which shows the robust-\nness and the ability of SITVOS in handling large appearance\nvariance. When introducing the intermediate frames, more\nobject information in memory further improves the perfor-\nmance and the fixed number strategy slightly outperforms\nthe fixed interval strategy in our model.\nConclusion\nIn this paper, we propose a novel Siamese network archi-\ntecture with a specially designed interactive transformer\nfor semi-supervised VOS, named SITVOS. It adopts the\nSiamese network to extract features of past and current\nframes, enabling feature reuse and being computationally\nefficient via weight sharing. SITVOS explores the self-\nattention and cross-attention in transformer to effectively\nmodel the spatio-temporal dependency of target objects in\nthe past frames and current frame. With the help of the fea-\nture interactive module, more efficient object information\npropagation is realized between the encoder and decoder to\nenhance the target representation. Experimental results on\nthree challenging benchmarks demonstrate the superiority of\nSITVOS in both segmentation accuracy and inference speed.\n1234\nAcknowledgements\nThis work was done during Meng Lan‚Äôs internship at JD\nExplore Academy. This work was supported by the Na-\ntional Natural Science Foundation of China under Grants\n62122060, 62076188, and the Fundamental Research Funds\nfor the Central Universities under Grant 2042021kf0196.\nReferences\nCaelles, S.; Maninis, K.; Pont-Tuset, J.; Leal-Taix¬¥e, L.; Cre-\nmers, D.; and Gool, L. V . 2017. One-Shot Video Object\nSegmentation. In CVPR, 5320‚Äì5329.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213‚Äì229.\nChen, X.; Li, Z.; Yuan, Y .; Yu, G.; Shen, J.; and Qi, D. 2020.\nState-Aware Tracker for Real-Time Video Object Segmen-\ntation. In CVPR, 9381‚Äì9390.\nChen, X.; Yan, B.; Zhu, J.; Wang, D.; Yang, X.; and Lu, H.\n2021. Transformer tracking. In CVPR, 8126‚Äì8135.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth\n16x16 words: Transformers for image recognition at scale.\nIn ICLR.\nDuke, B.; Ahmed, A.; Wolf, C.; Aarabi, P.; and Taylor, G. W.\n2021. SSTVOS: Sparse Spatiotemporal Transformers for\nVideo Object Segmentation. In CVPR, 5912‚Äì5921.\nGe, W.; Lu, X.; and Shen, J. 2021. Video Object Segmen-\ntation Using Global and Instance Embedding Learning. In\nCVPR, 16836‚Äì16845.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a. Deep Resid-\nual Learning for Image Recognition. In CVPR, 770‚Äì778.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b. Identity map-\npings in deep residual networks. In ECCV, 630‚Äì645.\nHu, L.; Zhang, P.; Zhang, B.; Pan, P.; Xu, Y .; and Jin,\nR. 2021. Learning Position and Target Consistency for\nMemory-based Video Object Segmentation. In CVPR,\n4144‚Äì4154.\nJohnander, J.; Danelljan, M.; Brissman, E.; Khan, F. S.; and\nFelsberg, M. 2019. A Generative Appearance Model for\nEnd-To-End Video Object Segmentation. In CVPR, 8953‚Äì\n8962.\nLan, M.; Zhang, Y .; Xu, Q.; and Zhang, L. 2020. E3SN: Ef-\nficient End-to-End Siamese Network for Video Object Seg-\nmentation. In IJCAI, 701‚Äì707.\nLi, Y .; Shen, Z.; and Shan, Y . 2020. Fast Video Object Seg-\nmentation Using the Global Context Module. InECCV, vol-\nume 12355, 735‚Äì750.\nLin, T.; Maire, M.; Belongie, S. J.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In ECCV, 740‚Äì755.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLu, X.; Wang, W.; Martin, D.; Zhou, T.; Shen, J.; and Luc,\nV . G. 2020. Video Object Segmentation with Episodic Graph\nMemory Networks. In ECCV.\nLuiten, J.; V oigtlaender, P.; and Leibe, B. 2018. PReMVOS:\nProposal-Generation, Refinement and Merging for Video\nObject Segmentation. In ACCV, 565‚Äì580.\nOh, S. W.; Lee, J.; Sunkavalli, K.; and Kim, S. J. 2018.\nFast Video Object Segmentation by Reference-Guided Mask\nPropagation. In CVPR, 7376‚Äì7385.\nOh, S. W.; Lee, J.; Xu, N.; and Kim, S. J. 2019. Video Ob-\nject Segmentation Using Space-Time Memory Networks. In\nICCV, 9225‚Äì9234.\nPerazzi, F.; Khoreva, A.; Benenson, R.; Schiele, B.; and\nSorkine-Hornung, A. 2017. Learning Video Object Segmen-\ntation from Static Images. In CVPR, 3491‚Äì3500.\nPont-Tuset, J.; Perazzi, F.; Caelles, S.; Arbelaez, P.; Sorkine-\nHornung, A.; and Gool, L. V . 2017. The 2017 DA VIS Chal-\nlenge on Video Object Segmentation. abs/1704.00675.\nRobinson, A.; Lawin, F. J.; Danelljan, M.; Khan, F. S.; and\nFelsberg, M. 2020. Learning Fast and Robust Target Models\nfor Video Object Segmentation. In CVPR, 7404‚Äì7413.\nSeong, H.; Hyun, J.; and Kim, E. 2020. Kernelized Memory\nNetwork for Video Object Segmentation. In ECCV, 629‚Äì\n645.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS, 5998‚Äì6008.\nV oigtlaender, P.; Chai, Y .; Schroff, F.; Adam, H.; Leibe, B.;\nand Chen, L. 2019. FEELVOS: Fast End-To-End Embed-\nding Learning for Video Object Segmentation. In CVPR,\n9481‚Äì9490.\nV oigtlaender, P.; and Leibe, B. 2017. Online Adaptation of\nConvolutional Neural Networks for Video Object Segmen-\ntation. In BMVC.\nWang, H.; Jiang, X.; Ren, H.; Hu, Y .; and Bai, S. 2021.\nSwiftNet: Real-time Video Object Segmentation. In CVPR,\n1296‚Äì1305.\nWang, Q.; Zhang, L.; Bertinetto, L.; Hu, W.; and Torr, P.\nH. S. 2019. Fast Online Object Tracking and Segmentation:\nA Unifying Approach. In CVPR, 1328‚Äì1338.\nXie, H.; Yao, H.; Zhou, S.; Zhang, S.; and Sun, W. 2021.\nEfficient Regional Memory Network for Video Object Seg-\nmentation. In CVPR, 1286‚Äì1295.\nXu, N.; Yang, L.; Fan, Y .; Yang, J.; Yue, D.; Liang, Y .;\nPrice, B. L.; Cohen, S.; and Huang, T. S. 2018. YouTube-\nVOS: Sequence-to-Sequence Video Object Segmentation.\nIn ECCV, 603‚Äì619.\nXu, Y .; Wang, Z.; Li, Z.; Yuan, Y .; and Yu, G. 2020.\nSiamFC++: Towards robust and accurate visual tracking\nwith target estimation guidelines. In AAAI, volume 34,\n12549‚Äì12556.\nXu, Y .; Zhang, Q.; Zhang, J.; and Tao, D. 2021. ViTAE: Vi-\nsion Transformer Advanced by Exploring Intrinsic Inductive\nBias. In NeurIPS.\n1235\nYang, Z.; Wei, Y .; and Yang, Y . 2021. Collaborative\nVideo Object Segmentation by Multi-Scale Foreground-\nBackground Integration. TPAMI.\nZhang, J.; and Tao, D. 2020. Empowering things with in-\ntelligence: a survey of the progress, challenges, and oppor-\ntunities in artificial intelligence of things. IEEE Internet of\nThings Journal, 8(10): 7789‚Äì7817.\nZhang, Y .; Wu, Z.; Peng, H.; and Lin, S. 2020. A trans-\nductive approach for video object segmentation. In CVPR,\n6949‚Äì6958.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In CVPR, 6881‚Äì6890.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. In ICLR.\n1236",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8287525177001953
    },
    {
      "name": "Encoder",
      "score": 0.7738994359970093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6052789688110352
    },
    {
      "name": "Segmentation",
      "score": 0.527962327003479
    },
    {
      "name": "Computer vision",
      "score": 0.5146988034248352
    },
    {
      "name": "Transformer",
      "score": 0.5025022029876709
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4573781192302704
    },
    {
      "name": "Engineering",
      "score": 0.07008355855941772
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}