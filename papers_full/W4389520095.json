{
  "title": "A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction",
  "url": "https://openalex.org/W4389520095",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3021034580",
      "name": "Ruihao Shui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110093419",
      "name": "Yixin Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099081116",
      "name": "Xiang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2730258684",
      "name": "Tat-Seng Chua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2997429021",
    "https://openalex.org/W4389520264",
    "https://openalex.org/W4378498597",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4381248030",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4389519070",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2991316439",
    "https://openalex.org/W2592064971",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4377000868",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2858159822",
    "https://openalex.org/W3035668167",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4‚Äôs law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such case, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7337‚Äì7348\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nA Comprehensive Evaluation of Large Language Models on\nLegal Judgment Prediction\nRuihao Shui\nNational University of Singapore\nruihaoshui@u.nus.edu\nYixin Cao\nSingapore Management University\nyxcao@smu.edu.sg\nWang Xiang‚àó\nUniversity of Science and Technology of China\nxiangwang1223@gmail.com\nTat-Seng Chua\nNational University of Singapore\ndcscts@nus.edu.sg\nAbstract\nLarge language models (LLMs) have demon-\nstrated great potential for domain-specific ap-\nplications, such as the law domain. However,\nrecent disputes over GPT-4‚Äôs law evaluation\nraise questions concerning their performance\nin real-world legal tasks. To systematically\ninvestigate their competency in the law, we\ndesign practical baseline solutions based on\nLLMs and test on the task of legal judgment\nprediction. In our solutions, LLMs can work\nalone to answer open questions or coordinate\nwith an information retrieval (IR) system to\nlearn from similar cases or solve simplified\nmulti-choice questions. We show that similar\ncases and multi-choice options, namely label\ncandidates, included in prompts can help LLMs\nrecall domain knowledge that is critical for ex-\npertise legal reasoning. We additionally present\nan intriguing paradox wherein an IR system\nsurpasses the performance of LLM+IR due to\nlimited gains acquired by weaker LLMs from\npowerful IR systems. In such cases, the role\nof LLMs becomes redundant. Our evaluation\npipeline can be easily extended into other tasks\nto facilitate evaluations in other domains. Code\nis available at https://github.com/srhthu/\nLM-CompEval-Legal\n1 Introduction\nLarge language models have achieved great suc-\ncess in various Natural Language Processing (NLP)\ntasks (Brown et al., 2020; Touvron et al., 2023),\nwhile there are still some disputes over the potential\nfor domain-specific applications (Mart√≠nez, 2023).\nFocusing on the law domain, the leading LLM,\nGPT-4 (OpenAI, 2023), was claimed to pass the\nUniform Bar Exam (UBE) with a 90th percentile\nscore. Although inspiring, however, this result was\npointed out to be overestimated (Mart√≠nez, 2023).\n‚àóXiang Wang is also affiliated with Institute of Artificial\nIntelligence, Institute of Dataspace, Hefei Comprehensive\nNational Science Center.\nDeterminethechargeofthedefendantbasedoncasefacts.Casefacts:Thedefendant,snatchedthevictim‚Äôsshoulderbag‚Ä¶Charge:Forcibleseizure\nPromptforLegalJudgmentPredictionsimilarcases\nFT\nTF\n+simcase+labelcdd\nCasefactsCharge\n<Instruction>Q:A:\nOpenQN,ZS <Instruction>Q:A:\nMulti-choice,ZS<Instruction>Q:A:Q:A:Q:A:\nOpenQN,FS <Instruction>Q:A:Q:A:Q:A:\nMulti-choice,FS\nPrompt\n+both\nüë®‚öñIRSystem\nPublicCases\nScenarioLLMsworkaloneLLMscoordinatewithIR\nFigure 1: The task of Legal Judgment Prediction and the\nevaluation settings. Different colors refer to different\ncharges. For similar cases, ‚ÄúT‚Äù refers to true similar\ncases with the same charges as the query cases, while\n‚ÄúF‚Äù refers to false similar cases. For task settings, ‚ÄúZS‚Äù\nis the abbreviation for zero-shot and ‚ÄúFS‚Äù for few-shot.\nThis raises an interesting question: How exactly\nLLMs perform in various real-world legal tasks?\nIn this paper, we design practical baseline solu-\ntions based on LLMs and systematically investigate\ntheir competency in the law, to shed light on other\ndomains as well. We attribute the main issues of\nthe previous benchmark as follows. First, UBE is\ntoo general and not subject to any legal jurisdiction\n(Mart√≠nez, 2023). Second, UBE contains multi-\nchoice questions and open-ended questions that\nrequire human experts to evaluate. To avoid human\nevaluation, some datasets (Hendrycks et al., 2020)\nreplace open-ended questions with multi-choice\nquestions. However, in real-world applications,\nthere are not only multi-choice but also open ques-\ntions. Using multi-choice questions only may not\nbe comprehensive enough. Third, specifically in\nbut not limited to common law (Shulayeva et al.,\n2017; Xiao et al., 2019), similar cases are always\nintroduced as evidence to support expertise legal\nreasoning (Zhong et al., 2020b), which are not fully\n7337\nstudied in previous benchmark (Hendrycks et al.,\n2020).\nFor the first issue, we choose legal judgment pre-\ndiction (LJP) (Xiao et al., 2018; Chalkidis et al.,\n2019; Zhong et al., 2020a) as the example task\nfor investigation. It is a real-world problem to de-\ntermine the charges committed by the defendants\nunder a juridical system, as shown in Figure 1. LJP\nis typically formulated as a classification task to\npredict the most possible one from a list of pre-\ndefined charges. Then, for the second and third\nissues, we design four settings derived from two\nwork scenarios of LLMs to cover open and multi-\nchoice questions and the usage of similar cases. In\nthe first scenario, LLMs work alone without ex-\nplicit knowledge in prompts, assuming all domain\nknowledge is implicitly stored in parameters. In\nthe second scenario, LLMs coordinate with an\ninformation retrieval (IR) system that enriches\nprompts with similar demonstrations and label can-\ndidates to benefit expertise reasoning. Specifically,\ndemonstrations consist of pairs of similar cases\nand their charges, which are retrieved by the IR\nsystem based on similarity of case facts. Labels\nof the retrieved cases can form label candidates,\nshown as circles of different colors in Figure 1, to\nhint LLM with label information and narrow down\nlabel space (Ma et al., 2023).\nThe four evaluation settings in Figure 1 can be\ncategorized based on the presence of two elements\nin prompts: demonstrations (similar cases) and la-\nbel candidates. Demonstrations convert the setting\nfrom zero-shot to few-shot prompting, while label\ncandidates simplify the task from open questions\nto multi-choice questions1. The first scenario cor-\nresponds to the first setting, where neither element\nis present, while the second scenario encompasses\nthe remaining three settings. We evaluate five up-\nto-date LLMs of the close-source GPT-3 (Brown\net al., 2020) family, ChatGPT and GPT-4 (OpenAI,\n2023), and open-source LLMs including Vicuna\n(Chiang et al., 2023), ChatGLM (Du et al., 2022)\nand BLOOMZ (Muennighoff et al., 2022). The\nevaluation is conducted on a Chinese LJP dataset,\nnamely CAIL (Xiao et al., 2018), which contains\ncases of 112 criminal law charges2.\nWe highlight our key findings as follows:\n1. Similar cases and label candidates can help\n1It is not strict multi-choice questions. LLMs can generate\ncorrect answers even though ground-truth labels are absent in\ncandidates.\n2After filtering less frequent (article, charge) pairs\nLLMs recall domain knowledge that is critical\nfor expertise legal reasoning.\n2. Label candidates result in more consistent out-\nputs, indicating LLMs gain greater confidence\nin their domain knowledge (Jiang et al., 2021).\n3. Irrelevant demonstrations formed by fixed\ncases hardly improve performance. This ex-\ncludes their effect on task illustration.\n4. Paradox: An IR system can outperform\nLLM+IR since weaker LLMs acquire limited\ngains from informative documents retrieved\nby a powerful IR system. Thus, it is critical to\nadapte LLMs to generate with retrieved docu-\nments.\n5. More similar cases introduce more knowledge\nand noise simultaneously, whose final out-\ncome depends on LLMs.\nThe main contributions are summarized in three\naspects:\n‚Ä¢ We investigate the law competency of LLMs\non the task of legal judgment prediction.\n‚Ä¢ We propose practical baseline solutions for\nLLMs that tackle two scenarios: working\nalone or in coordination with an IR system.\n‚Ä¢ We evaluate five LLMs and conduct compre-\nhensive analysis to demystify their character-\nistics of expertise reasoning.\n2 Baseline Method\nThe goal of legal judgment prediction is to deter-\nmine the committed charges given case facts. To\nharness LLMs for LJP, we adopt in-context learn-\ning (Brown et al., 2020) and use LLMs to generate\nthe charges conditioned on prompts (Section 2.1).\nTo enhance LLMs, we incorporate label candidates\nand demonstrations consisting of similar cases into\nprompts, which are acquired by an IR system (Sec-\ntion 2.2). This derives four settings of baseline so-\nlutions, namely zero-shot open questions, few-shot\nopen questions, zero-shot multi-choice questions,\nand few-shot multi-choice questions. The multi-\nchoice settings employ label candidates while few-\nshot settings include demonstrations, as shown in\nFigure 1. Finally, we introduce how to simulate IR\nsystems with different capabilities to understand\ntheir effects (Section 2.3).\n2.1 LLM Prompting\nPrompt Design. A prompt begins with an in-\nstruction to illustrate the task followed by label\n7338\ncandidates and task demonstrations in the form of\ninput-output pairs. The templates of prompts are\ndisplayed in Appendix A.1.\nParsing. We adopt one automatic parsing func-\ntion for all LLMs to map LLM outputs to pre-\ndefined charge labels. No ad hoc heuristics are\nemployed for a fair comparison. Specifically, we\nuse the BM25 algorithm3 to measure text similar-\nity between outputs and pre-defined charges and\npredict the most similar charges. BM25 is robust\nand yields comparable performances to neural sim-\nilarity methods like text2vec4 in our pilot experi-\nments.\nInference. Sampling is enabled during genera-\ntion for consistent results, as inspired by Wang et al.\n(2022). Five outputs are sampled for each prompt\nwith the temperature of 0.8. Their similarity scores\nof pre-defined labels are averaged.\n2.2 IR System for Knowledge Incorporation\nIR systems are utilized to retrieve similar cases,\ncommonly referenced by lawyers and judges, to\ninform their judgments. In addition to providing\ndemonstrations, these similar cases can also aid in\ngenerating potential labels by incorporating the la-\nbels from the top similar cases. By employing these\nsmaller sets of predefined charges, namely label\ncandidates, complex open questions can be simpli-\nfied into multiple-choice questions. This approach\nis effective in enhancing LM prompting (Ma et al.,\n2023), as including hundreds of charges directly in\nprompts is impractical.\nImplementation of IR System. We use the\nBM25 algorithm to measure the semantic similar-\nity between cases. Similar cases are retrieved from\nthe training dataset. To guarantee that the demon-\nstrations exemplify one of the multi-choice options,\nwe exclude demonstrations with labels that are not\namong the candidate options5.\n2.3 Simulation of IR Systems\nTo investigate the effects of IR capabilities, we\nsimulate a series of IR systems of different capa-\nbilities as measured by Precision@1 6. Then the\ntop retrieved cases are used as demonstrations. We\nconsider cases with identical charges to the query\ncases as true similar cases and vice versa.\n3https://pypi.org/project/rank-bm25/\n4https://github.com/crownpku/text2vec\n5This condition is not violated for the top four similar cases\nwithout filtering.\n6The accuracy of the top one retrieved case.\nRealistic Simulation. We prioritize the return-\ning of true similar cases for easy query cases, rather\nthan the returning in a random manner. The query\ndifficulty is measured by the Precision@10 of the\nBM25 retriever described in Section 2.2. The mo-\ntivation is that queries with shadow linguistic fea-\ntures are more possible to get relevant retrieval\nresults than complex or obscure queries. For a\nspecific value (e.g., a%) of Precision@1 to be sim-\nulated, the top a% of easy test cases are assured to\nhave a true similar case, while the rest are assigned\nfalse similar cases.\n3 Experimental Setup\n3.1 Models\nBelow is a concise introduction to the five LLMs\nto be evaluated.\nGPT-4 (OpenAI, 2023) and ChatGPT are\navailable from OpenAI API and the versions of\ngpt-4-0314 and gpt-3.5-turbo-0301 are used.\nFor technological details, ChatGPT is claimed to\nbe a sibling model to InstructGPT (Ouyang et al.,\n2022) that is trained to follow instructions and align\nto human preferences with the RLHF algorithm\n(Christiano et al., 2017).\nVicuna-13B (Chiang et al., 2023) is a LLaMA\nmodel (Touvron et al., 2023) fine-tuned on 70K\npublic user-shared conversations with ChatGPT. It\ncan be viewed to learn distilled knowledge (Hinton\net al., 2015) of ChatGPT.\nChatGLM-6B7 is a dialog language model\nbased on the GLM (Du et al., 2022) architecture\nand supports English and Chinese.\nBLOOMZ (Muennighoff et al., 2022) is an in-\nstruction fine-tuned BLOOM (Scao et al., 2022),\na multilingual language model. We use the\nbloomz-7b1-mt version that is tuned for multilin-\ngual prompts. Except for BLOOMZ, Vicuna and\nChatGLM are mainly fine-tuned on conversational\ndata.\n3.2 Dataset and Pre-processing\nThe Chinese LJP dataset, CAIL (Xiao et al., 2018),\nis used in our experiments. Each sample consists of\nthe case facts and the committed charge as the la-\nbel. As the original dataset is very large (~100K for\ntraining and ~20K for test), we randomly sample\na balanced small test set from the original test set.\nFive cases are sampled for each charge, accounting\n7https://github.com/THUDM/ChatGLM-6B\n7339\nTokenizer Median <=500 <=1000\nChatGPT 396.5 68.75 92.32\nVicuna 496.0 50.89 86.96\nChatGLM 206.5 91.07 98.57\nBLOOMZ 210.5 90.54 98.93\nTable 1: Statistics of the number of tokens across tok-\nenizers. The last two columns present the ratios of test\nsamples with token counts below the specified values.\nfor 560 test cases in total for 112 charges. Simi-\nlarly, we also sample the training and validation\nsets with 10 cases per charge. The training set is\nused to retrieve similar cases (Section 2.3), while\nthe validation set is used to determine the optimal\nk of the kNN algorithm.\nTruncation. Since some cases have very long\ndescriptions, we truncate the case facts of demon-\nstrations to 500 tokens and those of test samples to\n1000 tokens. It is worth noting that the text is tok-\nenized by the tokenizer of each model before trun-\ncation for a fair comparison. Recently, Petrov et al.\n(2023) address the issue that a tokenizer can lead\nto different performances of different languages.\nThis suggests that the performance on a particular\nlanguage can also be influenced by tokenizers from\nvarious models with varying language encoding\nefficiency.\nTable 1 shows the statistics of the number of to-\nkens processed by different tokenizers8. The most\nefficient tokenizers for Chinese are those of Chat-\nGLM and BLOOMZ, indicated by the medians\nof token numbers. In contrast, the tokenizer of\nChatGPT produces 2√ó tokens and that of Vicuna\nproduces 2.5√ó tokens. The truncation length is\nproper to accommodate most samples.\n4 LLM vs. LLM with IR System\nWe initially present the overall results, highlight-\ning the importance of label candidates and similar\ncases, and conduct a comparative analysis of the\nmodels. Subsequently, we investigate the relation-\nship between label candidates and self-consistency\nto unveil their actual effects on expertise reason-\ning. Additionally, we perform an ablation study by\nreplacing similar cases with fixed cases as demon-\nstrations to further understand their impact.\n8GPT-4 and ChatGPT have the same results. Following\nOpenAI‚Äôs guidance, we use the python package tiktoken for\ntokenization\n/uni0000003d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000032/uni00000053/uni00000048/uni00000051/uni00000003/uni00000034/uni00000031\n/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni0000000e/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000003/uni00000026/uni00000044/uni00000056/uni00000048/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni0000000e/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000003/uni00000026/uni00000044/uni00000056/uni00000048\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000051/uni00000003/uni00000029/uni00000052/uni00000058/uni00000055/uni00000003/uni00000036/uni00000048/uni00000057/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000056\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048\nFigure 2: The macro comparison between the four set-\ntings. ‚Äú+Label‚Äù refers to zero-shot multi-choice ques-\ntions; ‚Äú+Sim Case‚Äù refers to few-shot open questions\nand ‚Äú+Label +Sim Case‚Äù refers to few-shot multi-choice\nquestions. More than one points of a model in the\nlast two settings refer to runs with different number\nof demonstrations.\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000003d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\nOpen question\nMulti choice\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n/uni00000029/uni00000048/uni0000005a/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\nOpen question\nMulti choice\nFigure 3: Compare the models under each setting. Few-\nshot performances are averaged among 1-shot to 4-shot.\n4.1 Overall Results\nThe macro comparison between the four settings is\nshown in Figure 2, where each point represents the\nperformance of one specific run of one model.\nSignificance of label candidates and similar\ncases. In comparison to the zero-shot open ques-\ntion setting where LLMs work alone, the inclusion\nof label candidates, similar cases, or both demon-\nstrates noteworthy enhancements. This highlights\nthe effectiveness of our baseline solutions that lever-\nage IR systems to expand the capabilities of LLMs\nin legal domains. These findings align with previ-\nous research that has also recognized the signifi-\ncance of the two components (Ma et al., 2023; Liu\net al., 2021).\nThe effects of label candidates and similar cases\ndiffer slightly in terms of performance mean and\n7340\nvariance. Label candidates contribute to a higher\nmean performance, while similar cases introduce\ngreater variance. Examining the model perfor-\nmances in the third setting (+Sim Case) displayed\nin Figure 2, GPT-4 and ChatGPT exhibit more\nsignificant improvements from similar cases com-\npared to their smaller counterparts. They also gain\nmore benefit from similar cases than from label\ncandidates. This observation can be attributed to\nthe varying difficulty levels of knowledge utiliza-\ntion. While the knowledge within label candidates\nis readily accessible and straightforward, leverag-\ning similar cases requires stronger language under-\nstanding and few-shot learning abilities.\nFurthermore, the coexistence of label candidates\nand similar cases further enhances the performance\nof GPT-4 and ChatGPT, but it diminishes the per-\nformance of Vicuna, ChatGLM, and BLOOMZ.\nThis suggests that smaller LLMs may encounter\nchallenges in effectively managing knowledge in\nmultiple forms simultaneously, leading to confu-\nsion.\nModel comparison. The performances of the\nmodels under zero-shot and few-shot prompting is\nshown in Figure 3, where few-shot performances\nare averaged among 1-shot to 4-shot.\nThe zero-shot setting emphasizes the ability to\nunderstand instructions. When only instructions\nare available, BLOOMZ performs better than Chat-\nGPT, indicating a superior multilingual instruc-\ntion following ability. This result is reasonable\nas BLOOMZ is the only smaller LLM that is\nfine-tuned on multilingual instructions. Once pro-\nvided with explicit domain knowledge, ChatGPT\noutperforms all smaller LLMs. The case is the\nsame for BLOOMZ and ChatGLM, where Chat-\nGLM overtakes BLOOMZ with knowledge of la-\nbel candidates. BLOOMZ performs worst when\nprompted with two forms of knowledge, indicat-\ning that BLOOMZ is not very robust to prompts.\nAmong the three smaller LLMs, ChatGLM is the\nmost robust to various forms of knowledge.\nThe significant effects of label candidates and\nsimilar cases can be explained as they activate\nLLM‚Äôs memory of relevant domain knowledge.\nThis view can be supported by two pieces of ev-\nidence about the relationship between label can-\ndidates and self-consistency (Section 4.2) and the\nnegligible effect of irrelevant cases as fixed demon-\nstrations (Section 4.3).\n/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000003d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000032/uni00000053/uni00000048/uni00000051/uni00000003/uni00000054/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000003/uni00000046/uni0000004b/uni00000052/uni0000004c/uni00000046/uni00000048\n/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000032/uni00000051/uni00000048/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n/uni00000036/uni00000048/uni0000004f/uni00000049/uni00000010/uni00000046/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c\nFigure 4: Changes of performance and self-consistency\nafter adding label candidates. The change of each model\nis illustrated by an arrow pointing from the open ques-\ntion setting to the multi-choice setting.\n4.2 Label Candidates Enhance\nSelf-consistency and Confidence\nTo further understand the effect of label candidates,\nwe propose a metric to measure the self-consistency\nof LLMs that is calculated as the number of the\nmajority prediction9. Consistent outputs indicate a\nhigh level of confidence in LLMs, which is often\nassociated with a better grasp of knowledge (Jiang\net al., 2021, 2023).\nThe changes in performance and self-\nconsistency after introducing label candidates are\nshown in Figure 4 as the arrows. We observe\nthat the incorporation of label candidates leads to\nmore consistent outputs (8 of 10 cases) and higher\nconfidence in LLMs except zero-shot GPT-4 with\na slight decrease and few-shot BLOOMZ. In the\nzero-shot setting, label candidates significantly\nboost LLM performances. We postulate that label\ncandidates help by eliciting pre-stored domain\nknowledge with concise charge names. Besides,\nthe self-consistency also correlates with model\nperformances (7 of 10 cases). Such correlation\nis also observed in other tasks like question\nanswering (Jiang et al., 2021). It is worth noting\nthat label candidates decrease both self-consistency\nand performance of few-shot prompted BLOOMZ,\nwhich also aligns with the correlation.\n4.3 Domain Knowledge Is More Critical Than\nTask Illustration\nThere is a possible argument that similar demon-\nstrations can help LLMs understand instructions\nand tasks. To disentangle their effects on task il-\nlustration and provision of domain knowledge, we\n9For example, if the five sampled outputs are mapped to\nlabels of (a,a,a,b,c), the consistency score is 3.\n7341\n/uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047\n/uni00000051/uni00000020/uni00000015\n/uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047\n/uni00000051/uni00000020/uni00000014\n/uni0000003d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000032/uni00000053/uni00000048/uni00000051/uni00000003/uni00000034/uni00000031\n/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055\n/uni00000051/uni00000020/uni00000014\n/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055\n/uni00000051/uni00000020/uni00000015\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000000e/uni00000003/uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni0000000e/uni00000003/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055\n/uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\nFigure 5: The effects of fixed (irrelevant) and similar\ncases as demonstrations. Divided by the baseline setting\nof zero-shot open questions, the left part refers to fixed\ndemonstrations with increasing numbers of demonstra-\ntions, while the right part refers to similar demonstra-\ntions. The shadow area represents the range of standard\ndeviation.\nexperiment with irrelevant demonstrations fixed for\nall test samples. We manually select two common\ncases with frequent charges in the original dataset\nas the fixed demonstrations. The 1-shot perfor-\nmance was averaged on the two demonstrations.\nWe compare the effects of fixed and similar\ndemonstrations with the baseline setting of zero-\nshot open questions in Figure 5. The change of per-\nformance from center to left demonstrates that fixed\ndemonstrations hardly benefit LLMs and some-\ntimes harm the performance (e.g., ChatGLM). This\nindicates that LLMs can basically understand in-\nstructions and do not need general demonstrations\nfor task clarification, implying that the main chal-\nlenge of expertise reasoning is to recall domain\nknowledge instead of understanding a specific task.\nWe inspect the notable performance drop of\nChatGLM resulting from fixed demonstrations. We\nfind that ChatGLM tends to analyze the cases of\nboth demonstrations and test samples and then an-\nswer with both of their charges. Its wordy style\nseems to result from the fine-tuning dialog corpus\nwhere an assistant LLM is supposed to provide rich\ninformation. In contrast, similar cases seem to en-\ncourage more concise outputs following the format\nof demonstrations.\n5 Paradox of Information Retrieval\nSystem\nThe significance of similar demonstrations illus-\ntrated in Section 4.3 has motivated research fo-\ncusing on prompting-oriented IR systems (Rubin\net al., 2021; Sun et al., 2023) to retrieve high qual-\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013\n/uni0000002c/uni00000035/uni00000003/uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000023/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000059/uni00000048/uni00000055/uni00000003/uni00000056/uni0000004c/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000035/uni00000003/uni00000056/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050/uni00000056\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni0000002c/uni00000035\n/uni00000055/uni00000048/uni00000044/uni0000004f/uni00000003/uni00000046/uni00000044/uni00000056/uni00000048\nFigure 6: The performance of ChatGPT coordinated\nwith a series of simulated IR systems with varying ca-\npabilities as measured by Precision@1. The vertical\nblue line represents the threshold of IR capability at\nwhich IR systems overtake ChatGPT. The performance\nof ChatGPT in the real setting (1-shot open questions)\nis indicated by the red plus sign.\nity demonstrations. However, we raise an intuitive\nquestion: Do LLMs gain substantial improvement\nfrom IR systems compared to the kNN baseline\nthat harnesses IR systems for classification tasks?\nThe question is inspired by our observation that the\nBM25 retriever achieves 48.03% of Precision@1\n10 and 57.68% prediction accuracy by majority vote\nof top k = 17retrieved similar cases.\nThis observation suggests a paradoxical sce-\nnario wherein an IR system outperforms the com-\nbination of LLM and IR, with the LLM taking on\nthe leading role and the IR serving as a supporting\nrole. In such a scenario, the LLM becomes redun-\ndant due to its failure to fully utilize the informative\nretrieved documents.\nTo investigate the paradox, instead of experi-\nmenting with different IR systems, we manipulate\nthe BM25 retriever to simulate a series of IR sys-\ntems with different capabilities measured by Preci-\nsion@1 as described by Section 2.3. We take a case\nstudy of ChatGPT, whose 1-shot performance un-\nder different IR systems (denoted as Precision@1)\nis shown in Figure 6.\nResults Although the performance of ChatGPT\nenhanced by IR systems improves with IR capabil-\nity, it will eventually underperform the IR system\nonce the IR capability surpasses a certain threshold.\nIn the ideal situation where true similar cases are\nalways retrieved, ChatGPT is unable to attain 100%\naccuracy and lags significantly behind the optimal\nIR system. According to Appendix A.4, all smaller\nLLMs are not comparable to the BM25 retriever.\nDiscussion The findings demonstrate that LLMs\nface challenges in effectively leveraging informa-\n10It is identical to the precision of kNN with k = 1\n7342\n/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017\n/uni00000006/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000032/uni00000053/uni00000048/uni00000051/uni00000003/uni00000034/uni00000058/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017\n/uni00000006/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000003/uni00000026/uni0000004b/uni00000052/uni0000004c/uni00000046/uni00000048\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030\n/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\nFigure 7: Performance vs. the number of similar demon-\nstrations of the five LLMs.\ntive retrieved documents. This underscores the\nneed for significant research efforts to enhance the\nsynergy between auto-regressive language models\nand retrieval by conditioning model outputs more\non retrieved documents. Previous work has ex-\nplored the augmentation of LLMs with retrieval\nat both the pre-training and fine-tuning stages\n(Borgeaud et al., 2022; Wang et al., 2023). More-\nover, the marginal and inadequate improvement\nwith retrieval indicates the limited legal reasoning\nability of existing general LLMs. There is a need\nfor future efforts to enhance domain-specific rea-\nsoning abilities of pre-trained foundation models.\n6 Ablation Study\n6.1 More Demonstrations Are Not Always\nBetter\nThe impact of the number of similar demonstra-\ntions (n) is depicted in Figure 7. It is evident that\nGPT-4 and ChatGPT demonstrate proficiency in\nhandling larger numbers of demonstrations, leading\nto enhanced performance, whereas Vicuna, Chat-\nGLM and BLOOZ experience varying degrees of\nperformance degradation with increasing numbers.\nNotably, ChatGLM displays the least sensitivity to\nn. Furthermore, even ChatGPT‚Äôs performance de-\nclines when n is increased from three to four. The\nperformance improvement resulting from larger\nvalues of n can be attributed to the increased recall\nof true similar cases. Conversely, the decline in per-\nformance can be attributed to the noise introduced\nby more false similar cases.\nPerformance variations. The change of perfor-\nmance after including an additional demonstration\nare visualized using heat maps in Figure 8. For\neach model, the three heat maps stand for the varia-\ntions from k-shot to (k+1)-shot, which are denoted\nbelow. For each heat map, the two rows indicate\nthe inclusion of a new demonstration with true (T)\nor false (F) similar cases, while the columns indi-\ncate the combinations of existing demonstrations.\nTake the second heat map as an example. The cell\nin the column of (F, T) and the row of (T) displays\nthe performance variation between 2-shot of (F, T)\ndemonstrations and 3-shot of (F, T, T) demonstra-\ntions. Purple represents performance improvement,\nwhile green represents performance decline.\nFor ChatGPT and BLOOMZ, the second rows\nof their three heat maps are mainly in purple, indi-\ncating significant enhancements resulting from the\ninclusion of true similar cases. However, the first\nlines of BLOOMZ display a deeper green color\nthan those of ChatGPT, suggesting that BLOOMZ\nexperiences greater degree of performance declines\ncaused by the inclusion of false similar cases.\nThese findings indicate different sensitivity to false\nsimilar demonstrations. Powerful language mod-\nels like GPT-4 and ChatGPT exhibit robustness to\nnoise in false similar cases, allowing them to re-\nmain focused on relevant information in true simi-\nlar cases. In contrast, weaker LLMs are susceptible\nto the influence of such noise. Overall, ChatGPT\nperforms better when provided with more similar\ndemonstrations, whereas BLOOMZ demonstrates\nthe opposite, as shown in Figure 7.\nThe conclusion is that increased numbers of\ndemonstrations have both positive and negative im-\nplications for expertise reasoning. However, LLMs\ncould potentially gain from additional demonstra-\ntions in tasks that requires clear task illustration.\n6.2 The Impact of Absent Ground Truth\nLabels\nWe manually incorporate ground-truth labels into\nlabel candidates in cases where they are absent,\nwhich may occur due to the limited recall capability\nof the IR system described in Section 2.2. The test\nsamples are categorized into two groups, namely\n‚ÄúEasy‚Äù and ‚ÄúHard‚Äù, based on the retrieval of their\nground truth labels by the IR system. The original\nperformance of the two groups and the performance\nof the ‚ÄúHard‚Äù group with modified prompts to in-\nclude ground truth labels, namely ‚ÄúHard+GT‚Äù, are\ndisplayed in Figure 9.\nThe performance gaps between the ‚ÄúEasy‚Äù and\n‚ÄúHard+GT‚Äù groups suggest that challenging sam-\nples for IR systems are also difficult for LLMs.\nHowever, this gap is insignificant for the power-\nful GPT-4 who perceives them as equal challeng-\n7343\n/uni00000029/uni00000037\n/uni00000029\n/uni00000037/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030\n/uni00000029/uni0000000f/uni00000029/uni00000029/uni0000000f/uni00000037/uni00000037/uni0000000f/uni00000029/uni00000037/uni0000000f/uni00000037\n/uni00000029/uni0000000f/uni00000029/uni0000000f/uni00000029/uni00000029/uni0000000f/uni00000029/uni0000000f/uni00000037/uni00000029/uni0000000f/uni00000037/uni0000000f/uni00000029/uni00000029/uni0000000f/uni00000037/uni0000000f/uni00000037/uni00000037/uni0000000f/uni00000029/uni0000000f/uni00000029/uni00000037/uni0000000f/uni00000029/uni0000000f/uni00000037/uni00000037/uni0000000f/uni00000037/uni0000000f/uni00000029/uni00000037/uni0000000f/uni00000037/uni0000000f/uni00000037\n/uni00000014/uni00000003/uni00000003/uni00000015\n/uni00000029\n/uni00000037/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d\n/uni00000015/uni00000003/uni00000003/uni00000016\n /uni00000016/uni00000003/uni00000003/uni00000017\n-40\n-20\n0\n20\n40\nFigure 8: Heat maps of performance variations resulting from the inclusion of an addition demonstration. ‚ÄúT‚Äù\ncorresponds to demonstrations with true similar cases, while ‚ÄúF‚Äù represents those with false similar cases. Each row\nrepresents the included new demonstration, while each column indicate the status of existing demonstrations.\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000025/uni0000002f/uni00000032/uni00000032/uni00000030/uni0000003d/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni0000002f/uni00000030/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044\n0.0\n0.2\n0.4\n0.6\n0.8/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000000e/uni00000013/uni00000011/uni00000017/uni00000015\n/uni0000000e/uni00000013/uni00000011/uni00000016/uni00000013\n/uni0000000e/uni00000013/uni00000011/uni00000014/uni00000014\n/uni0000000e/uni00000013/uni00000011/uni00000015/uni0000001b\n/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000016\n/uni00000028/uni00000049/uni00000049/uni00000048/uni00000046/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000004a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000010/uni00000057/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000056\n/uni00000028/uni00000044/uni00000056/uni0000005c\n/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000003/uni0000000e/uni00000003/uni0000002a/uni00000037\n/uni0000002b/uni00000044/uni00000055/uni00000047\nFigure 9: The performance of ‚ÄúEasy‚Äù and ‚ÄúHard‚Äù sam-\nples under the setting of zero-shot multi-choice ques-\ntions. ‚ÄúHard+GT‚Äù refers to improvement of including\nthe absent ground truth labels in label candidates.\ning. The improvement of ‚ÄúHard+GT‚Äù compared\nto ‚ÄúHard‚Äù is notable in GPT-4, ChatGPT and Chat-\nGLM but inconspicuous in Vicuna with inferior\ncompetency in the law. Considering the relatively\nsmall size of the ‚ÄúHard‚Äù group (79/560), the ab-\nsence of ground truth labels does not have a signifi-\ncant impact, especially for weaker LLMs.\n6.3 Incorporation of Law Articles\nWe examine the effect of incorporating legal arti-\ncles that explicitly define the charges into prompts.\nFor each charge retrieved by the IR system11, Chat-\nGPT is required to determine whether the defen-\ndant is guilty for the particular charge by answering\nwith a yes or no. We find that 94.46% of the ground\ntruth charges are accurately detected, while only\n27.31% of the detected charges are correct. The\nhigh recall and low precision indicate a substantial\ndifference between ChatGPT and legal experts in\nthe ability to distinguish charges and make precise\njudgments.\n11we also include the ground truth charge\n7 Discussion\nWe compare the LLMs with supervised baselines.\nWe fine-tune BERT (Devlin et al., 2018) on the\nsame training set and achieve a comparable accu-\nracy of 68% to ChatGPT but lower than GPT-4.\nSince LLMs are not fine-tuned on the specific LJP\ntask, this result highlights the remarkable superior-\nity of LLMs in acquiring significant knowledge and\nleveraging transfer learning Raffel et al. (2020).\nHowever, we observe that BERT‚Äôs performance\nimproves to 89% when trained with the original\ntraining set (~10K). We find that certain knowledge\nis present in shadow features, which can be easily\nlearned with supervision. These superficial features\ncan result in biased supervised models. Fortunately,\nunsupervised pre-training objectives, make LLMs\nmore robust and less vulnerable to this issue. This\ndepicts a promising future for NLP applications in\nvarious domains.\n8 Conclusion\nTo address the deficiency in evaluating the compe-\ntency of LLMs in the field of law, we focused on\nthe task of legal judgment prediction and devised\nfour settings to facilitate a thorough evaluation that\nencompassed both open and multiple-choice ques-\ntions and incorporated similar cases to aid in the\ndecision-making process.\nThe evaluation results revealed different behav-\niors among the prominent LLMs, namely GPT-\n4 and ChatGPT, compared to their smaller coun-\nterparts. Both GPT-4 and ChatGPT exhibited re-\nmarkable proficiency in effectively leveraging do-\nmain knowledge in various formats. Among the\nsmaller LLMs, ChatGLM displayed greater robust-\nness, while BLOOMZ showcased superior zero-\nshot ability.\n7344\nWe presented an intriguing paradox wherein\nLLMs could become abundant in the presence of a\npowerful IR system. When improving IR systems\nto benefit LLMs, it is crucial for researchers to ac-\nknowledge this paradoxical scenario and prevent\ngreat disparity between LLMs and IR systems.\nLimitations\nOne limitation of this paper is the use of the\nclose-source GPT-4 and ChatGPT whose avail-\nability depends on the commercial company\nOpenAI. According to OpenAI, the ChatGPT\nand GPT-4 versions used in this paper, namely\ngpt-3.5-turbo-0301 and gpt-4-0314, will be\ndeprecated and not available after September 13th,\n2023.\nAnother limitation pertains to the selection of\nLLMs. Due to the rapid emergence of new LLMs,\nwe are not able to include all of them with the con-\nstraint of limited time. Instead of more models, we\nfocus more on designing comprehensive evaluation\nsettings and conducting insightful analyses to shed\nlight on other domains.\nEthics Statement\nThe task of legal judgment prediction is used to\nevaluate LLM‚Äôs competency in the law. The pri-\nmary objective of this task is to assist judges and\nlawyers in comprehending lengthy legal documents\nby offering them a supplementary tool. It is im-\nportant to note that this task does not seek to re-\nplace the roles of judges and lawyers, nor does\nit aim to determine the guilt or charges of defen-\ndants through machine learning algorithms. Addi-\ntionally, there is research focused on interpreting\nLJP models, aiming to enhance the transparency\nof black-box models for improved utilization by\nlegal practitioners. The paper utilizes a public and\nanonymized dataset to exclude the potential issue\nof personal information leakage.\nAcknowledgements\nWe thank all reviewers for their constructive com-\nments. This research is supported by NExT Re-\nsearch Center, the National Natural Science Foun-\ndation of China (9227010114) and the University\nSynergy Innovation Program of Anhui Province\n(GXXT-2022-040).\nReferences\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206‚Äì2240. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Ale-\ntras. 2019. Neural legal judgment prediction in en-\nglish. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4317‚Äì4323.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320‚Äì335.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962‚Äì977.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\n7345\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nYubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.\n2023. Large language model is not a good few-shot\ninformation extractor, but a good reranker for hard\nsamples! arXiv preprint arXiv:2303.08559.\nEric Mart√≠nez. 2023. Re-evaluating gpt-4‚Äôs bar exam\nperformance. Available at SSRN 4441311.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nAleksandar Petrov, Emanuele La Malfa, Philip HS\nTorr, and Adel Bibi. 2023. Language model tokeniz-\ners introduce unfairness between languages. arXiv\npreprint arXiv:2305.15425.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485‚Äì5551.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ¬¥c, Daniel Hesslow, Roman\nCastagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon,\nMatthias Gall√©, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nOlga Shulayeva, Advaith Siddharthan, and Adam Wyner.\n2017. Recognizing cited facts and principles in\nlegal judgements. Artificial Intelligence and Law ,\n25(1):107‚Äì126.\nXiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan,\nShuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng,\nLingjuan Lyu, Fei Wu, and Guoyin Wang. 2023.\nPushing the limits of chatgpt on nlp tasks.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee,\nZihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii\nKuchaiev, Bo Li, Chaowei Xiao, et al. 2023. Shall\nwe pretrain autoregressive language models with\nretrieval? a comprehensive study. arXiv preprint\narXiv:2304.06762.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu,\nZhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei\nHan, Zhen Hu, Heng Wang, et al. 2018. Cail2018:\nA large-scale legal dataset for judgment prediction.\narXiv preprint arXiv:1807.02478.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang,\nXianpei Han, Zhen Hu, Heng Wang, et al. 2019.\nCail2019-scm: A dataset of similar case matching in\nlegal domain. arXiv preprint arXiv:1911.08962.\nHaoxi Zhong, Yuzhong Wang, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020a. Itera-\ntively questioning and answering for interpretable le-\ngal judgment prediction. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 34,\npages 1250‚Äì1257.\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020b.\nHow does nlp benefit legal system: A summary\nof legal artificial intelligence. arXiv preprint\narXiv:2004.12158.\n7346\nA Appendix\nA.1 Prompt Templates\nThe prompt template is shown in Figure 10. The\ntranslation of the original Chinese prompt is dis-\nplayed using orange text. The setting of zero-shot\nopen questions use a longer instruction that ap-\npends ‚ÄúOutput the charge name directly‚Äù to the\ninstruction in Figure A.1.\nÊ†πÊçÆ‰∏≠ÂõΩÂàëÊ≥ïÂà§Êñ≠ÁäØÁΩ™Â´åÁñë‰∫∫ÁöÑÁΩ™Âêç„ÄÇ<label1><label2>‚Ä¶<labeln>Ê°à‰ª∂‰∫ãÂÆû:<demofacts>ÁΩ™Âêç:<democharge>Ê°à‰ª∂‰∫ãÂÆû:<queryfacts>ÁΩ™Âêç:\nBasedontheChinesecriminallaw,determinethechargescommittedbythedefendant.\nInstruction\nLabelcandidates\nDemoQuery\nCasefacts:<demofacts>Charge:<democharge>Casefacts:<queryfacts>Charge:\nFigure 10: The prompt template in Chinese and English.\nA.2 Robust to Fixed Demonstrations\nModel 1shot 2shot\nGPT-4 49.59 / 48.84 50.69\nChatGPT 47.01 / 46.57 47.55\nVicuna-13B 22.74 / 29.38 28.37\nChatGLM-6B 22.39 / 25.14 21.36\nBLOOMZ-7B 36.65 / 43.94 42.24\nTable 2: The classification accuracy scores with prompts\nconsisting of fixed cases.\nWe examine the effects of the two fixed cases\nmentioned in Section 4.3 in Table 2. We find that\nGPT-4 and ChatGPT are robust to the selection\nof the fixed demonstration in 1-shot setting, while\nVicuna, ChatGLM and BLOOMZ are less robust.\nA.3 Comparison with Supervised Baselines\nTo understand the performance of supervised fine-\ntuning (SFT) baselines on LJP, we experiment on\nthree models: BERT12, XLM-RoBERTa13 and De-\nBERTa14. These models are fine-tuned on two\ndatasets of different sizes: the original CAIL\ndataset (~100k samples) and the sampled training\nset (1120 samples) that is used as retrieval corpus\ndescribed in Section 3.2, denoted as CAIL_few.\n12bert-base-chinese\n13xlm-roberta-base\n14microsoft/mdeberta-v3-base\nThe SFT models are evaluated on the same evalua-\ntion dataset described in Section 3.2. The smaller\ntraining set aims to compare the few-shot perfor-\nmance of SFT baselines and LLMs in low data\nscenario.\nThe results of SFT models are shown in Figure 3.\nConsidering the highest accuracy of GPT-4 being\n74.46% (multi-choice, 4shot), GPT-4 can outper-\nform supervised baselines in low data scenario. If\nthere is abundant training data, supervised base-\nlines are still better than GPT-4 by 15%.\nModel CAIL CAIL_few\nBERT 89.64 68.04\nXLM-RoBERTa 88.75 66.43\nDeBERTa 88.57 30.89\nTable 3: Prediction accuracy of SFT models fine-tuned\non two training datasets of different sizes.\nA.4 Detailed Results\nThe specific values of performances displayed in\nFigure 2 are presented in Table 4. Besides, we also\nprovide the performance of the F1 score in Table 5.\n7347\nModel Open Questions Multiple-choice Questions\n0shot 1shot 2shot 3shot 4shot 0shot 1shot 2shot 3shot 4shot\nGPT-4 55.18 64.82 69.11 69.82 71.96 63.93 71.25 72.50 73.75 74.46\nChatGPT 46.61 60.00 62.86 64.82 66.96 61.61 64.46 66.96 70.36 67.14\nVicuna-13B 28.21 50.36 49.64 51.79 35.89 47.86 44.82 43.39 35.71 19.46\nChatGLM-6B 41.43 51.79 50.00 50.36 50.54 55.71 50.54 49.64 49.46 47.32\nBLOOMZ-7B 49.82 54.82 52.68 52.50 51.25 53.39 31.96 31.07 27.32 26.61\nTable 4: The classification accuracy scores of all models under the four settings.\nModel Open Questions Multiple-choice Questions\n0shot 1shot 2shot 3shot 4shot 0shot 1shot 2shot 3shot 4shot\nGPT-4 50.52 62.72 67.54 68.61 71.02 62.31 70.42 71.81 73.24 74.00\nChatGPT 43.14 58.42 61.86 64.40 66.16 60.67 63.51 66.85 69.59 66.62\nVicuna-13B 25.50 48.85 47.64 49.49 39.82 44.70 41.73 41.48 35.03 21.61\nChatGLM-6B 41.89 50.30 47.76 48.59 48.67 53.74 49.26 47.56 47.61 45.32\nBLOOMZ-7B 46.90 53.28 51.06 50.90 49.26 50.68 29.25 27.92 25.27 23.37\nTable 5: The classification F1 scores of all models under the four settings.\n7348",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5828229188919067
    },
    {
      "name": "Task (project management)",
      "score": 0.5425184369087219
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5401095747947693
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5275667905807495
    },
    {
      "name": "Code (set theory)",
      "score": 0.4878135025501251
    },
    {
      "name": "Recall",
      "score": 0.4496387839317322
    },
    {
      "name": "Work (physics)",
      "score": 0.44139036536216736
    },
    {
      "name": "Cognitive psychology",
      "score": 0.31124570965766907
    },
    {
      "name": "Psychology",
      "score": 0.26740705966949463
    },
    {
      "name": "Programming language",
      "score": 0.12583401799201965
    },
    {
      "name": "Engineering",
      "score": 0.10958418250083923
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105595",
      "name": "Institute of Art",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210137491",
      "name": "National Science Centre",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    }
  ]
}