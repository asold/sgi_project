{
  "title": "Automated Binary Classification of Diabetic Retinopathy by SWIN Transformer",
  "url": "https://openalex.org/W4387423012",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3133753026",
      "name": "Rasha Ali Dihin",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A2005371922",
      "name": "Ebtesam N. AlShemmary",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A4283003220",
      "name": "Waleed A. Mahmoud Al-Jawher",
      "affiliations": [
        "University of Kirkuk"
      ]
    },
    {
      "id": "https://openalex.org/A3133753026",
      "name": "Rasha Ali Dihin",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A2005371922",
      "name": "Ebtesam N. AlShemmary",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A4283003220",
      "name": "Waleed A. Mahmoud Al-Jawher",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6800689796",
    "https://openalex.org/W4205250217",
    "https://openalex.org/W3207619483",
    "https://openalex.org/W4291744147",
    "https://openalex.org/W4207081122",
    "https://openalex.org/W4220815323",
    "https://openalex.org/W3081262431",
    "https://openalex.org/W3194982898"
  ],
  "abstract": "Diabetic retinopathy is a medical condition that affects the eyes and is caused by damage to the blood vessels in the retina (the light-sensitive part of the eye) due to high blood sugar levels in individuals with diabetes. This damage can lead to vision loss or even blindness. It is a common complication of diabetes and a leading cause of blindness in working-age adults. In this paper, to automatically classify images of the retina as having either diabetic retinopathy or not. The goal of this classification is to assist medical professionals in diagnosing diabetic retinopathy more accurately and efficiently, potentially improving patient outcomes. In this process, the Swin transformer model is trained on the APTOS dataset of retinal images and then used to automatically classify new images as either positive or negative for diabetic retinopathy. Used CLAHE and Gaussian, to improve the input image, and the model achieved a Test Accuracy of 96%, Sensitivity of 96%, F1 Score of 96% for Swin-T and Test Accuracy of 98% for Swin-B, Sensitivity of 98%, and F1 Score of 98%.",
  "full_text": "Journal of Al -Qadisiyah  for Computer Science and M athematics Vol. 15(1) 2023 , pp  Comp.   169–178 \n \n \n \n \n \n \n \n \n \n \n \n \n \n∗Corresponding author Ebtesam N. AlShemmary \nEmail addresses: dr.alshemmary@uokufa.edu.iq \nCommunicated by ‘sub etitor’ \nAutomated Binary Classification of Diabetic Retinopathy by SWIN \nTransformer \nRasha Ali Dihina, Ebtesam N. AlShemmaryb*, Waleed A. Mahmoud Al-Jawherc \naDepartment of Computer Science, University of Kufa, Iraq. Email: rashaa.aljabry@uokufa.edu.iq \nbIT Research and Development Center, University of Kufa, Iraq. Email: dr.alshemmary@uokufa.edu.iq \ncUruk University, Iraq. Email: profwaleed54@gmail.com \n \nA R T I C L E I N F O \nArticle history: \nReceived: 25/01/2023 \nRevised form: 11/03/2023 \nAccepted: 16/03/2023 \nAvailable online: 31/03/2023 \n \nKeywords: \nDiabetic retinopathy \nSwin transformer \nGaussian \nAPTOS \nCLAHE \n \nA B S T R A C T \nDiabetic retinopathy is a medical condition that affects the eyes and is caused by damage to the \nblood vessels in the retina (the light-sensitive part of the eye) due to high blood sugar levels in \nindividuals with diabetes. This damage can lead to vision loss or even blindness. It is a common \ncomplication of diabetes and a leading cause of blindness in working-age adults. In this paper, \nto automatically classify images of the retina as having either diabetic retinopathy or not. The \ngoal of this classificati on is to assist medical professionals in diagnosing diabetic retinopathy \nmore accurately and efficiently, potentially improving patient outcomes. In this process, the \nSwin transformer model is trained on the APTOS dataset of retinal images and then used to  \nautomatically classify new images as either positive or negative for diabetic retinopathy. Used \nCLAHE and Gaussian, to improve the input image, and the model achieved a Test Accuracy of \n96%, Sensitivity of 96 %, F1 Score of 96% for Swin-T and Test Accuracy  of 98% for Swin-B, \nSensitivity of 98%, and F1 Score of 98%. \n \n \nhttps://doi.org/10.29304/jqcm.2023.15.1.1166 \n1. Main text  \nThe eye is a vital part of the human body, serving as the primary source of external information. According to the \nInternational Diabetes Federation (IFD), it is predicted that by 2040, there will be 642 million individuals with \ndiabetes worldwide. It is c rucial to accurately assess the severity of diabetic retinopathy to recommend the \nappropriate treatment. However, manual diagnosis can be time -consuming and dependent on the physician's \nexpertise. Therefore, automating this process is desirable. With the d evelopment of artificial intelligence, better \nresults have been achieved in image classification and identifying crucial image features, allowing for automatic \nscreening of the retina [1]. \nThe classification of DR is an important aspect in determining the stages of severity and prognosis in its development. \nOver time, several classifications have been established, with the Airlie House classification being the most widely \n2 Rasha Ali Dihin, Journal of Al -Qadisiyah  for Computer Science and M athematics Vol. 15(1) 2023 , pp  Comp.   169–178\n \nused as the basis for current classifications. This classification separates DR into t wo groups: non -proliferating and \nproliferating [2]. \nSwin transformer is a type of neural network architecture in the field of machine learning. It is a variant of the \nTransformer architecture and has become popular in natural language processing tasks [ 3]. The extends  the \nTransformer architecture to handle computer vision tasks, specifically image classification tasks. Swin Transformer \nincorporates the ideas of local and global attention mechanisms, allowing it to handle both fine -grained and global \ncontextual information in images. It has been used in various computer vision tasks such as object recogni tion and \nsemantic segmentation and has shown promising results [4]. \nThis paper’s sections were arranged as follows: in Section 2, Related Works, in Section 3, we elaborate on the proposed \nframework, including CLAHE and Gaussian with Swin transformer, we present and discuss the experimental results \nin Section 4 and finally summarize the conclusions in Section 5.  \n2. Related work  \nThis section reviews the relevant literature. \n S. Sanjana et al.[5]  proposed a binary classification of DR using five Transfer Learning models Xception, \nInceptionResNetV2, MobileNetV2, DenseNet121, and NASNet  Mobile. The highest validation accuracy was achieved \nby InceptionResNetV2 with 96.25%. In the pre-processing phase, the images were transformed using techniques such \nas rescaling, shearing, zooming, and flipping horizontally.  \nY. Li. et al. [6]  proposed a Semi -supervised Auto -encoder Graph Network (SAGN) for challenging DR \ndiagnosis. The SAGN model has three components: auto -encoder feature learning, neighbor correlation mining, and \ngraph representation. The performance of SAGN on the APTOS 2019 dataset was evaluated with an accuracy of 94.4%. \nIn the pre-processing phase, to reduce the im pact of irrelevant regions in the fundus images, the black regions were \nremoved through cropping and the images were resized to 512x512 pixels before being fed into the network. \nAdditionally, each image was augmented by random horizontal and vertical rotat ion. \nS. Gungor K. et al. [7] proposed a new method that leverages a deep feature generator based on correction \nand is inspired by the Vision Transformer (ViT). The ViT uses an MLP mixer, which extracts features using a fixed-size \nsquare patch. In this method, rectangular patches were  utilized instead of square patches to create deep ly hidden \npatterns, and a DenseNet201 was employed. This approach achieved good results, with a classification accuracy of \nover 90% for the categories 'Normal', 'NPDR', and 'PDR' on the APTOS 2019 dataset\".  \nD. Chen. et al . [8] introduced \"A new unit called the Transformer UNet (PCAT -UNet) was proposed in the \npaper. This unit combines attention mechanisms and the shape of a letter U and is based on a transformer \narchitecture. To combine features, skip connections are utilized on both sides. The results indicated that the proposed \nmethod produces excellent results in segmenting retinal blood vessels in both DRIVE, STARE, and CHASE_DB1 \ndatasets.\"  \nH. Siyuan et al . presented a two -stream network named TSTNet for the classification of remote sensing \nimages. The Swin Transformer was employed as the foundation of each stream, producing impressive results. The \nexperiments on three demanding public datasets indicated that TSTNet has a better classification performance \ncompared to other leading models [9]. \n3. Method  \nIn this section, we first introduce the global context-modeling framework and then discuss in detail the design shown \nin Figure 1. \n \n \n \n \nRasha Ali Dihin , Journal of Al -Qadisiyah  for Computer Science and M athematics  Vol. 15(1) 2023 , pp  Comp.   169–178                3 \n \n \nFig. 1 – The proposed framework. \n3.1. Image enhancement:  \nThe benefits of image enhancement include: \nImproved visual quality: Enhanced images have a higher visual quality and appear clearer, sharper, and more detailed. \nBetter feature extraction: Image enhancement can improve the visibility of features, making them easier to detect and \nextract, Increased accuracy of image analysis: Enhanced images are more suitable for computer vision and image \nanalysis tasks, leading to increased accuracy and improved results and Enhanced interpretation: Image enhancement \nis a valuable technique that aids humans in interpreting and comprehending images more easily. One way to achieve \nthis is by utilizing a Gaussian filter and CLAHE to remove noise and improve contrast, as illustrated in Figure 2.  The \ngoal of noise removal is to elimina te any image disturbance caused by noise.  Gaussian filter is the simplest low -pass \nfilter that effectively removes high-frequency noises. Contrast Limited Adaptive Histogram Equalization (CLAHE) has \na controllable parameter to limit the contrast and the technique is successfully enhancing the low-contrast images[10] \n \n \n \n \n \nFig. 2 - CLAHE and Gaussian method \nWhen using CLAHE on an image, the contrast limiting threshold is set to 2. When applying a Gaussian filter, determine \nthe values of alpha and beta as 4 and -4 respectively. Add 128 to the image after adjusting the gamma value, and if a \nvalue exceeds the maximum expected value, set it to 0. If a value is greater than 1, set it to 1. \n \nCLAHE \n \nGaussian filter \n \nInput image  \n4 Rasha Ali Dihin, Journal of Al -Qadisiyah  for Computer Science and M athematics Vol. 15(1) 2023 , pp  Comp.   169–178\n \nAlgorithm CLAHE and Gaussian Filter    \nInput: A Retinal image  \nOutput: enhancement image with CLAHE and Gaussian   \nStep1: Read the image and store it in a variable (e.g., img) \nStep2: Convert the image to grayscale and save it in a new variable (e.g., im1) \nStep3: Apply the CLAHE function with a clip limit of 2 and a matrix size of 8x8 onim1  \nStep4: Convert the result of CLAHE back to RGB and save it in a new variable (e.g.  im2) \nStep5: Determine the values of α and β for the weighted addition (e.g., α = 4 and β =   -4) \nStep6: Determine the value of γ to add to the image (e.g., γ = 128)  \nStep7: Apply a Gaussian filter with a kernel size of (0,0) and a standard deviation of 1 on im2  \nStep8: Add the result of the Gaussian filter to the original image using the weighted addition (cv2.addWeighted(im2,4, \ncv2.GaussianBlur(img, (0,0),1), -4,128)) and save the result in a new variable (e.g., im3) \nStep9: Display the final image (im3) \n \n4. Result: \nThe proposed architecture was developed using a software package (Python). The implementation was central \nprocessing unit (CPU) specific.  All experiments were performed on Colab with GPU 15G.  \n4.1. Datasets \nThe APTOS 2019 Kaggle benchmark dataset is a col lection of fundus images of the retina taken under various \nconditions. These images have been manually categorized by specialists into 5 classes, ranging from \"0\" meaning no \ndiabetic retinopathy to \"4\" indicating severe proliferative diabetic retinopathy. Figure 3 displays the retinal images in \nthe dataset that corresponds to each level of severity. This dataset is used in the challenge of detecting blindness and \nprovides valuable information for the research of diabetic retinopathy [11]. \n \n \nFig. 3 - Datasets class percentage distribution \n4.2. Evaluation Metrics \n\nRasha Ali Dihin , Journal of Al -Qadisiyah  for Computer Science and M athematics  Vol. 15(1) 2023 , pp  Comp.   169–178                5 \n \nThe evaluation metrics for model performance in DR detection included two-class, The performance is \nmeasured with various performance measures including accuracy, Specificity, Sensitivity, and F1-Score, \n[12], [13].  \nWhere \nTP TNAcc TP TN FP FN\n+= +++\n            (1) \n            \nTPSE TP FN= +                         (2) \n        \nTNSP FP TN= +                              (3) \n*Re1 2. ( Re )\nprecision callF Score precision call= +\n   (4)     \n4-3: Research the different Swin transformer  \n4-3-1: Swin-T: \nWe used CLAHE and Gaussian in the enhancement phase on the image with size (224*224) with Swin -T transformer \nto DR classification to binary class as show n in Table 1, Figure 4. Table 1 shows the test accuracy, Sensitivity, \nspecificity, and F1 Score, Figure 4 show s the raining and validation accuracy and loss between the training and \nvaluation for 100 epoch, and Figure 5 displays the confusion matrix. \n \n \n \n \n \n \n \n \n \nFig. 4 - Training and validation over epoch for APTOS 2019 dataset, (a)loss, (b) accuracy, (epochs=100), \nCLAHE, and Gaussian -Swin-T to binary class. \n \n \n \n(a) \n (b) \n\n6 Rasha Ali Dihin, Journal of Al -Qadisiyah  for Computer Science and M athematics Vol. 15(1) 2023 , pp  Comp.   169–178\n \nTable 1 -Test Accuracy and test loss of binary class for Swin -T \nClass Test Accuracy Test loss Sensitivity Specificity F1 Score \nNo-DR 96% 0.0077 0.9700 0.9700 0.9761 \nDR 96%  0.9698 0.9698 0.9766 \nAverage  96%  0.9416 0.9416 96% \n \n \nFig. 5 - Confusion matrix for Swin-T binary class. \n4-3-2: Swin-B: \nIn the image enhancement phase for DR classification, we applied a combination of Contrast Limited Adaptive \nHistogram Equalization (CLAHE) and Gaussian filter on images with a size of (224 x 224) using the Swin -T \ntransformer. The results are presented in Table 2 and Fi gure 6. Table 2 displays the test accuracy, sensitivity, \nspecificity, and F1 Score, while Figure 6 illustrates the comparison of the training and validation accuracy and loss \nafter 100 epochs. Additionally, Figure 7 presents the confusion matrix. \n \n \n \n \n \n \n \n \nFig. 6 - Training and validation over epoch for APTOS 2019 dataset, (a)loss, (b) accuracy, (epochs=100), \nCLAHE, and Gaussian  -Swin-B to binary class. \n \n(a) \n (b) \nRasha Ali Dihin , Journal of Al -Qadisiyah  for Computer Science and M athematics  Vol. 15(1) 2023 , pp  Comp.   169–178                7 \n \nTable 2- Test Accuracy and test loss of binary class for Swin -B \nClass Test Accuracy Test loss Sensitivity specificity F1 Score \nNo-DR 97% 0.0077 0.9753 0.9753 0.9814 \nDR 97%  0.9789 0.9789 0.9841 \nAverage  97%  0.9773 0.9773 0.9885 \n \n \nFig. 7 - Confusion matrix for Swin-T binary class \n \n5. Conclusion: \nIn this paper, we used the Contrast Limited Adaptive Histogram Equalization (CLAHE) and Gaussian filter in \nthe enhancement phase and apply Swin -T and Swin-B DR classification where change on the last layers in the Swin \ntransformer, where the accuracy for training is 0.9863 and 0.9836 for validation and the best accuracy is 0.9863 and \nthe test accuracy is 96%, Sensitivity of 96%  and F1 Score of 96% for Swin-T while for Swin-B is the training accuracy \nis 0.9881, while the validation accuracy is 0.9836 and th e best accuracy is 0.9863,  while the test accuracy is 97% , \nSensitivity of 98%   and F1 Score of 96% for binary DR classification. \nAcknowledgments \nThe research behind this paper would not have been possible without the exceptional support of the Advisor Prof. \nEbtesam AlShemmary and Prof. Waleed AlJawher. Throughout researching image processing journals and writing this \npaper, they have inspired me with their enthusiasm, knowledge, and attention to detail.  \nReferences \n[1] F. Eigelshoven et al., “Advances on Smart and Soft Computing,” Inf. Process. \\& Manag., vol. 12, no. 2, pp. 3–18, 2020. \n[2] T. Nazir, M. Nawaz, J. Rashid, and R. Mahum, “Detection of Diabetic Eye Disease from Retinal Images Using a Deep Learning Bas ed CenterNet \nModel Tahira,” MDPI, vol. 21, no. 5283, 2021. \n[3] Z. Liu, Y. Lin, Y. Cao, H. Hu, and Y. Wei, “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows arXiv:210 3.14030v2,” \narXiv:2103.14030v2, 2021. \n[4] J. Liang, J. Cao, G. Sun, and K. Zhang, “SwinIR: Image Restoration Using Swin Transformer,” arXiv:2108.10257v1, 2021. \n[5] S. Sanjana, N. S. Shadin, and M. Farzana, “Automated Diabetic Retinopathy Detection Using Transfer Learning Models,” 2021. \n[6] Y. Li, Z. Song, S. Kang, S. Jung, and W. Kang, “Semi -Supervised Auto-Encoder Graph Network for Diabetic Retinopathy Grading,” IEEE Access, \nvol. 9, pp. 140759–140767, 2021, doi: 10.1109/ACCESS.2021.3119434. \n\n8 Rasha Ali Dihin, Journal of Al -Qadisiyah  for Computer Science and M athematics Vol. 15(1) 2023 , pp  Comp.   169–178\n \n[7] S. Gungor Koba, N. Baygin, E. Yusufoglu, and M. Baygin, “Automated Diabetic Retinopathy Detection Using Horizontal and Vertical Patch Division-\nBased Pre-Trained DenseNET with Digital Fundus Images,” MDPI, 2022. \n[8] D. Chen, W. Yang, L. Wang, S. Tan, J. Lin, and W. Bu, “PCAT -UNet: UNet-like network fused convolution and transformer for retinal vessel \nsegmentation,” PLoS ONE, vol. 17, no. 1 1. 2022, doi: 10.1371/journal.pone.0262689. \n[9] S. Hao, B. Wu, K. Zhao, and Y. Ye, “Two -Stream Swin Transformer with Differentiable Sobel Operator for Remote Sensing Image Classification,” \nRemote Sens., 2022. \n[10] R. Fan, X. Li, S. Lee, and T. Li, “Smart Image Enhancement Using CLAHE Based on an F-Shift Transformation during Decompression,” MDPI, vol. \n12, no. 1374, 2020. \n[11] B. Tymchenko, P. Marchenko, and D. Spodarets, “Deep Learning Approach to Diabetic Retinopathy Detection Borys,” arXiv:2003.02261v1, 2020. \n[12] R. S. R, “Design an Early Detection and Classification for Diabetic Retinopathy by Deep Feature Extraction based Convolution Neural Network,” J. \nTrends Comput. Sci. Smart Technol., vol. 3, no. 2, 2021. \n[13] C.-Y. Tsai, C.-T. Chen, G.-A. Chen, and C.-T. K. Kuo, “Necessity of Local Modification for Deep Learning Algorithms to Predict Diabetic Retinopathy \nChing-Yao,” Int. J. Environ. Res. Public Health, 2022. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ",
  "topic": "Diabetic retinopathy",
  "concepts": [
    {
      "name": "Diabetic retinopathy",
      "score": 0.8273481130599976
    },
    {
      "name": "Diabetes mellitus",
      "score": 0.6391283273696899
    },
    {
      "name": "Medicine",
      "score": 0.636847972869873
    },
    {
      "name": "Retinopathy",
      "score": 0.5042711496353149
    },
    {
      "name": "Blindness",
      "score": 0.4961718022823334
    },
    {
      "name": "Retina",
      "score": 0.4936286211013794
    },
    {
      "name": "Ophthalmology",
      "score": 0.4555441737174988
    },
    {
      "name": "Artificial intelligence",
      "score": 0.424895703792572
    },
    {
      "name": "Optometry",
      "score": 0.40397897362709045
    },
    {
      "name": "Computer science",
      "score": 0.3688589334487915
    },
    {
      "name": "Psychology",
      "score": 0.10490643978118896
    },
    {
      "name": "Endocrinology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47229656",
      "name": "University of Kufa",
      "country": "IQ"
    },
    {
      "id": "https://openalex.org/I102254642",
      "name": "University of Kirkuk",
      "country": "IQ"
    }
  ]
}