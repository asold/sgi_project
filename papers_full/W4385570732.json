{
  "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
  "url": "https://openalex.org/W4385570732",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2138343374",
      "name": "Hanlin Zhang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2163926266",
      "name": "Jiani Huang",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2143636100",
      "name": "Ziyang Li",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2005763208",
      "name": "Mayur Naik",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A3168967200",
      "name": "Eric Xing",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3210720616",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4287758766",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W2962924847",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W2963737801",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3216037316",
    "https://openalex.org/W3203211567",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W4312045654",
    "https://openalex.org/W4205702708",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4286962702",
    "https://openalex.org/W3034903373",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W3214124416",
    "https://openalex.org/W4297749157",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W3084470717",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W4298084898",
    "https://openalex.org/W3003760334",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4287554891",
    "https://openalex.org/W2953388933",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 3062–3077\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nImproved Logical Reasoning of Language Models via Differentiable\nSymbolic Programming\nHanlin Zhang1,∗Jiani Huang2,∗ Ziyang Li2 Mayur Naik2 Eric Xing1,3,4\n1Carnegie Mellon University, 2University of Pennsylvania,\n3Mohamed Bin Zayed University of Artificial Intelligence, 4Petuum Inc.\nAbstract\nPre-trained large language models (LMs) strug-\ngle to perform logical reasoning reliably de-\nspite advances in scale and compositional-\nity. In this work, we tackle this challenge\nthrough the lens of symbolic programming. We\npropose DSR-LM, a Differentiable Symbolic\nReasoning framework where pre-trained LMs\ngovern the perception of factual knowledge,\nand a symbolic module performs deductive\nreasoning. In contrast to works that rely on\nhand-crafted logic rules, our differentiable sym-\nbolic reasoning framework efficiently learns\nweighted rules and applies semantic loss to\nfurther improve LMs. DSR-LM is scalable,\ninterpretable, and allows easy integration of\nprior knowledge, thereby supporting extensive\nsymbolic programming to robustly derive a log-\nical conclusion. The results of our experiments\nsuggest that DSR-LM improves the logical rea-\nsoning abilities of pre-trained language models,\nresulting in a significant increase in accuracy of\nover 20% on deductive reasoning benchmarks.\nFurthermore, DSR-LM outperforms a variety\nof competitive baselines when faced with sys-\ntematic changes in sequence length.1\n1 Introduction\nComplex applications in natural language process-\ning involve dealing with two separate challenges.\nOn one hand, there is the richness, nuances, and\nextensive vocabulary of natural language. On the\nother hand, one needs logical connectives, long rea-\nsoning chains, and domain-specific knowledge to\ndraw logical conclusions. The systems handling\nthese two challenges are complementary to each\nother and are likened to psychologist Daniel Kah-\nneman’s human “system 1” and “system 2” (Kah-\nneman, 2011): while the former makes fast and in-\ntuitive decisions, akin to neural networks, the latter\n∗Equal contribution\n1Code available at https://github.com/moqingyan/dsr-lm\nLanguage Model Symbolic Reasoner\n• Rapid reasoning\n• Sub-symbolic knowledge\n• Handling noise, ambigui-\nties, and naturalness\n• Process open domain text\n• Can learn in-context\n• Multi-hop reasoning\n• Compositionality\n• Interpretability\n• Data efficiency\n• Can incorporate domain-\nspecific knowledge\nTable 1: Respective advantages of language models\nand symbolic reasoners.\nthinks more rigorously and methodically. Consid-\nering LMs as “system 1” and symbolic reasoners\nas “system 2”, we summarize their respective ad-\nvantages in Table 1.\nAlthough pre-trained LMs have demonstrated\nremarkable predictive performance, making them\nan effective “system 1”, they fall short when asked\nto perform consistent logical reasoning (Kassner\net al., 2020; Helwe et al., 2021; Creswell et al.,\n2022), which usually requires “system 2”. In part,\nthis is because LMs largely lack capabilities of\nsystematic generalization (Elazar et al., 2021; Hase\net al., 2021; Valmeekam et al., 2022).\nIn this work, we seek to incorporate deductive\nlogical reasoning with LMs. Our approach has the\nsame key objectives as neuro-symbolic program-\nming (Chaudhuri et al., 2021): compositionality,\nconsistency, interpretability, and easy integration\nof prior knowledge. We present DSR-LM, which\ntightly integrates a differentiable symbolic reason-\ning module with pre-trained LMs in an end-to-end\nfashion. With DSR-LM, the underlying LMs gov-\nern the perception of natural language and are fine-\ntuned to extract relational triplets with only weak\nsupervision. To overcome a common limitation\nof symbolic reasoning systems, the reliance on\nhuman-crafted logic rules (Huang et al., 2021; Nye\net al., 2021), we adapt DSR-LM to induce and fine-\ntune rules automatically. Further, DSR-LM allows\nincorporation of semantic loss obtained by logi-\ncal integrity constraints given as prior knowledge,\n3062\nwhich substantially helps the robustness.\nWe conduct extensive experiments showing that\nDSR-LM can consistently improve the logical rea-\nsoning capability upon pre-trained LMs. Even if\nDSR-LM uses a RoBERTa backbone with much\nless parameters and does not explicitly take triplets\nas supervision, it can still outperform various base-\nlines by large margins. Moreover, we show that\nDSR-LM can induce logic rules that are amenable\nto human understanding to explain decisions given\nonly higher-order predicates. As generalization\nover long-range dependencies is a significant weak-\nness of transformer-based language models (Lake\nand Baroni, 2018; Tay et al., 2020), we highlight\nthat in systematic, long-context scenarios, where\nmost pre-trained or neural approaches fail to gen-\neralize compositionally, DSR-LM can still achieve\nconsiderable performance gains.\n2 Related Work\nLogical reasoning with LMs. Pre-trained LMs\nhave been shown to struggle with logical reason-\ning over factual knowledge (Kassner et al., 2020;\nHelwe et al., 2021; Talmor et al., 2020a). There is\nencouraging recent progress in using transformers\nfor reasoning tasks (Zhou et al., 2020; Clark et al.,\n2021; Wei et al., 2022; Chowdhery et al., 2022;\nZelikman et al., 2022) but these approaches usu-\nally require a significant amount of computation\nfor re-training or human annotations on reason-\ning provenance (Camburu et al., 2018; Zhou et al.,\n2020; Nye et al., 2021; Wei et al., 2022). Moreover,\ntheir entangled nature with natural language makes\nit fundamentally hard to achieve robust inference\nover factual knowledge (Greff et al., 2020; Saparov\nand He, 2022; Zhang et al., 2022).\nThere are other obvious remedies for LMs’ poor\nreasoning capability. Ensuring that the training\ncorpus contains a sufficient amount of exemplary\nepisodes of sound reasoning reduces the depen-\ndency on normative biases and annotation arti-\nfacts (Talmor et al., 2020b; Betz et al., 2020; Hase\net al., 2021). Heuristics like data augmentation are\nalso shown to be effective (Talmor et al., 2020b).\nBut the above works require significant efforts for\ncrowdsourcing and auditing training data. Our\nmethod handily encodes a few prototypes/tem-\nplates of logic rules and is thus more efficient in\nterms of human effort. Moreover, our goal is funda-\nmentally different from theirs in investigating the\ntight integration of neural and symbolic models in\nan end-to-end manner.\nNeuro-symbolic reasoning. Neuro-symbolic\napproaches are proposed to integrate the percep-\ntion of deep neural components and the reasoning\nof symbolic components. Representative works can\nbe briefly categorized into regularization (Xu et al.,\n2018), program synthesis (Mao et al., 2018), and\nproof-guided probabilistic programming (Evans\nand Grefenstette, 2018; Rocktäschel and Riedel,\n2017; Manhaeve et al., 2018; Zhang et al., 2019;\nHuang et al., 2021). To improve compositional-\nity of LMs, previous works propose to parame-\nterize grammatical rules (Kim, 2021; Shaw et al.,\n2021) but show that those hybrid models are ineffi-\ncient and usually underperform neural counterparts.\nIn contrast to the above works, DSR-LM focuses\non improving LMs’ reasoning over logical propo-\nsitions with tight integration of their pre-trained\nknowledge in a scalable and automated way.\n3 Methodology\n3.1 Problem Formulation\nEach question answering (QA) example in the\ndataset is a triplet containing input text x, query\nq, and the answer y. Figure 1 shows an instance\nthat we will use as our running example. The input\ntext xis a natural language passage within which\nthere will be a set of entities, possibly referenced\nby 3rd person pronouns. The sentences hint at\nthe relationships between entities. For example,\n“Dorothy went to her brother Rich’s birthday party”\nimplies that Rich is Dorothy’s brother and Dorothy\nis Rich’s sister. The query qis a tuple of two enti-\nties, representing the people with whom we want to\ninfer the relation. The expected relation is stored in\nthe answer y, which will be one of a confined set of\npossible relations R, allowing us to treat the whole\nproblem as an ∣R∣-way classification problem. We\nfocus only on the problems where the desired rela-\ntion is not explicitly stated in the context but need\nto be deduced through a sequence of reasoning.\n3.2 Methodology Overview\nThe design of DSR-LM concerns tightly integrat-\ning a perceptive model for relation extraction with\na symbolic engine for logical reasoning. While we\napply LMs for low-level perception and relation\nextraction, we employ a symbolic reasoning mod-\nule to consistently and logically reason about the\nextracted relations. With a recent surge in neuro-\nsymbolic methods, reasoning engines are made\ndifferentiable, allowing us to differentiate through\n3063\nLoss   \nPredicted Query Output\n0.01::kin(sister, A, D) \n0.02::kin(father, A, D) \n... \n0.84::kin(niece, A, D) \nkin(r3 , x , z ) :- co(r1 , r2 , r3 ), \n    kin(r1 , x , y ), kin(r2 , y , z ). \nkin(r , y , x ) :- sym(r ), kin(r , x , y ). \n... \n0.99::co(son, son, grandson)\n0.01::co(father, sister, son)\n...\n0.98::co(brother, son, nephew)\nInput Text\n0.92::kin(daughter, K, R) \n0.05::kin(sister, K, R) \n... \n0.03::kin(father, K, A) \n0.89::kin(sister, K, A) \n... \n0.95::kin(uncle, J, B) \nLanguage \nModel Differentiable Symbolic Reasoner  \nR ich's daughter K elly made\ndinner for her sister K im.\nD orothy went to her brother\nR ich's birthday party. A nne\nwent shopping with her \nsister K im. J ulia decided to\ncall her uncle B enjamin on \nhis birthday. F rank took his\nson C harles and daughter \nR achel out for pizza.\nGround Truth Query Output\nkin(niece, A, D) \nQuery\nkin(r , A, D)? \nforall(a, b: kin(father, a, b) =>  \n  kin(son, b, a) ∨  kin(daughter, b, a)) \nSemantic Loss\n (weighted sum)\nQuestion\nHow is D orothy \nrelated to A nne? \nProbabilistic Input Facts\nFigure 1: Overview of DSR-LM with a motivating example where “Anne is the niece of Dorothy” should be\nlogically inferred from the context. We abbreviate the names with their first initials in the relational symbols.\nthe logical reasoning process. In particular, we em-\nploy Scallop (Huang et al., 2021) as our reasoning\nengine. We propose two add-ons to the existing\nneuro-symbolic methodology. First, some rules\nused for logical deduction are initialized using lan-\nguage models and further tuned by our end-to-end\npipeline, alleviating human efforts. Secondly, we\nemploy integrity constraints on the extracted rela-\ntion graphs and the logical rules, to improve the\nlogical consistency of LMs and the learned rules.\nBased on this design, we formalize our method\nas follows. We adopt pretrained LMs to build re-\nlation extractors, denoted Mθ, which take in the\nnatural language input xand return a set of prob-\nabilistic relational symbols r. Next, we employ\na differentiable deductive reasoning program, Pϕ,\nwhere ϕrepresents the weights of the learned logic\nrules. It takes as input the probabilistic relational\nsymbols and the query qand returns a distribution\nover R as the output ˆy. Overall, the deductive\nmodel is written as\nˆy=Pϕ(Mθ(x),q). (1)\nAdditionally, we have the semantic loss ( sl) de-\nrived by another symbolic program Psl computing\nthe probability of violating the integrity constraints:\nlsl =Psl(Mθ(x),ϕ) (2)\nCombined, we aim to minimize the objective J\nover training set D with loss function L:\nJ(θ,ϕ) = 1\n∣D∣ ∑\n(x,q,y)∈D\nw1L(Pϕ(Mθ(x),q),y)\n+w2Psl(Mθ(x),ϕ),\n(3)\nwhere w1 and w2 are tunable hyper-parameters to\nbalance the deduction loss and semantic loss.\n3.3 Relation Extraction\nSince pre-trained LMs have strong pattern recog-\nnition capabilities for tasks like Named-Entity-\nRecognition (NER) and Relation Extraction (RE)\n(Tenney et al., 2019; Soares et al., 2019), we adopt\nthem as our neural components in DSR-LM. To\nensure that LMs take in strings of similar length,\nwe divide the whole context into multiple windows.\nThe goal is to extract the relations between every\npair of entities in each windowed context. Con-\ncretely, our relation extractor Mθ comprises three\ncomponents: 1) a Named-Entity Recognizer (NER)\nto obtain the entities in the input text, 2) a pre-\ntrained language model, to be fine-tuned, that con-\nverts windowed text into embeddings, and 3) a\nclassifier that takes in the embedding of entities\nand predicts the relationship between them. The\nset of parameters θcontains the parameters of both\nthe LM and the classifier.\nWe assume the relations to be classified come\nfrom a finite set of relations R. For example in\nCLUTRR (Sinha et al., 2019), we have 20 kin-\nship relations including mother, son, uncle, father-\nin-law, etc. In practice, we perform (∣R∣ + 1)-\nway classification over each pair of entities, where\nthe extra class stands for “n/a”. The windowed\ncontexts are split based on simple heuristics of\n“contiguous one to three sentences that contain at\nleast two entities”, to account for coreference res-\nolution. The windowed contexts can be overlap-\nping and we allow the reasoning module to deal\nwith noisy and redundant data. Overall, assum-\ning that there are m windows in the context x,\nwe extract mn(n−1)(∣R∣ +1) probabilistic re-\nlational symbols. Each symbol is denoted as an\natom of the form p(s,o), where p ∈ R ∪{n/a}\nis the relational predicate, and s, o are the two\nentities connected by the predicate. We denote\n3064\nthe probability of such symbol extracted by the\nLM and relational classifier as Pr(p(s,o) ∣θ). All\nthese probabilities combined form the output vector\nr =Mθ(x) ∈Rmn(n−1)(∣R∣+1).\n3.4 Differentiable Symbolic Inference\nThe symbolic inference modules Pϕ and Psl are\nresponsible for processing the extracted relations to\ndeduce 1) an expected output relation inR, and 2) a\nsemantic loss encoding the probability of constraint\nviolation. There are two main objectives for these\nmodules. First, they need to logically reason about\nthe output relation and the semantic loss based on\nthe extracted relational symbols r, the query q, and\nthe rule weights ϕ. Second, they need to compute\nthe gradients of ˆy and lsl with respect to θ and\nϕ, namely ∂ˆy\n∂θ, ∂ˆy\n∂ϕ, ∂lsl\n∂ϕ , and ∂lsl\n∂θ , in order for the\nfine-tuning and rule learning to happen.\nLogical deduction. Logic rules can be applied\nto known facts to deduce new ones. For example,\nbelow is a horn clause, which reads “if b is a’s\nbrother and cis b’s daughter, then cis a’s niece”:\nniece(a,c) ← brother(a,b)∧daughter(b,c).\nNote that the structure of the above rule can\nbe captured by a higher-order logical predicate\ncalled “composite” (abbreviated as comp ). This\nallows us to express many other similarly struc-\ntured rules with ease. For instance, we can\nhave comp(brother, daughter, niece) and\ncomp(father, mother, grandmother) . With\nthis set of rules, we may derive more facts based on\nknown kinship relations. In fact, composition is the\nonly kind of rule we need for kinship reasoning. In\ngeneral, there are many other useful higher-order\npredicates to reason over knowledge bases, which\nwe list out in Table 2.\nPredicate Example\ntransitive transitive (relative)\nsymmetric symmetric (spouse)\ninverse inverse (husband,wife)\nimplies implies (mother,parent)\nTable 2: Higher-order predicate examples.\nProbability propagation. We seek to have the\ndeduced facts to also be associated with probabili-\nties computed using probabilities predicted by the\nunderlying relation extractor Mθ. This is achieved\nby allowing the propagation of probabilities. For\nexample, we have the proof tree with probabilities:\n0.9 ∶∶brother(D, R) 0.8 ∶∶daughter(R, K)\n0.72 ∶∶niece(D, K)\nIn practice, there could be multiple steps in the\nproof tree (multi-hop) and one fact can be derived\nby multiple proof trees. We employ the inference\nalgorithms based on approximated weighted model\ncounting (WMC) presented in (Manhaeve et al.,\n2018) to account for probabilistic inference under\ncomplex scenarios. Since the WMC procedure is\naugmented for differentiation, we can obtain the\ngradient ∂ˆy\n∂r . From here, we can obtain ∂ˆy\n∂θ = ∂ˆy\n∂r\n∂r\n∂θ,\nwhere the second part can be automatically derived\nfrom differentiating Mθ.\nRule learning. Hand-crafted rules could be ex-\npensive or even impossible to obtain. To allevi-\nate this issue, DSR-LM applies LMs to help au-\ntomatically extract rules, and further utilizes the\ndifferentiable pipeline to fine-tune the rules. Each\nrule such as comp(brother, daughter, niece)\nis attached a weight, initialized by prompting an\nunderlying LM. For example, the prompt we use\nfor extracting comp(r,p,q) is “one’s r’s pis their\n<q:mask>”. Given that the relations r,p,q ∈ R,\nDSR-LM automatically enumerates rand pfrom\nR while querying for LM to unmask the value of q.\nLM then returns a distribution of words, which we\ntake an intersection with R. The probabilities com-\nbined form the initial rule weights ϕ. This type of\nrule extraction strategy is different from existing ap-\nproaches in inductive logic programming since we\nare exploiting LMs for existing knowledge about\nrelationships.\nNote that LMs often make simple mistakes an-\nswering such prompt. In fact, with the above\nprompt, even GPT-3 can only produce 62% of\ncomposition rules correctly. While we can edit\nprompt to include few-shot examples, in this work\nwe consider fine-tuning such rule weights ϕwithin\nour differentiable reasoning pipeline. The gradient\nwith respect to ϕ is also derived with the WMC\nprocedure, giving us ∂ˆy\n∂ϕ. In practice, we use two\noptimizers with different hyper-parameters to up-\ndate the rule weights ϕand the underlying model\nparameter θ, in order to account for optimizing\ndifferent types of weights.\nSemantic loss and integrity constraints. In gen-\neral, learning with weak supervision label is hard,\nnot to mention that the deductive rules are learnt as\nwell. We thereby introduce an additional semantic\n3065\nloss during training. Here, semantic loss is derived\nby a set of integrity constraints used to regular-\nize the predicted entity-relation graph as well as\nthe learnt logic rules. In particular, we consider\nrules that detect violations of integrity constraints.\nFor example, “if A is B’s father, then B should be\nA’s son or daughter” is an integrity constraint for\nrelation extractor—if the model predicts a father\nrelationship between A and B, then it should also\npredict a son or daughter relationship between B\nand A. Encoded in first order logic, it is\n∀a,b,father(a,b) ⇒ (son(b,a)∨daughter(b,a)).\nThrough differentiable reasoning, we evaluate the\nprobability of such constraint being violated, yield-\ning our expected semantic loss. In practice, arbi-\ntrary number of constraints can be included, though\ntoo many interleaving ones could hinder learning.\n4 Experiments\nWe evaluate DSR-LM on both CLUTRR and\nDBpedia-INF. We show that DSR-LM has accurate\nand generalizable long-range reasoning capability.\n4.1 Datasets\nCLUTRR (Sinha et al., 2019) consists of kinship\nreasoning questions. Given a context that describes\na family’s routine activity, the goal is to deduce\nthe relationship between two family members that\nis not explicitly mentioned in the story. Although\nthe dataset is synthetic, the sentences are crowd-\nsourced and hence there is a considerable amount\nof naturalness inside the dataset. The family kin-\nship graph is synthetic and the names of the family\nmembers are randomized. For ablation study, we\nmanually crafted 92 kinship composition rules as\nan external symbolic knowledge base. This yields\nthe following symbolic information for each data-\npoint: 1) the full kinship graph corresponding to\nthe story, 2) the symbolic knowledge base (KB),\nand 3) a query representing the question. The\nCLUTRR dataset is divided into different difficul-\nties measured by k, the number of facts used in\nthe reasoning chain. For training, we only have\n10K data points with 5K k = 2 and another 5K\nk = 3, meaning that we can only receive supervi-\nsion on data with short reasoning chains. The test\nset, on the other hand, contains 1.1K examples with\nk∈{2,..., 10}.\nDBpedia-INF is a curated subset of the evalua-\ntion dataset used in RuleBert (Saeed et al., 2021).\nSimilar to CLUTRR, it is generated synthetically\nto test the reasoning capability of LMs. Given a\nsynthetic passage describing the relation between\nentities, and soft deductive logic rules, we aim to\ndeduce the relationship between any two entities.\nThe symbolic program of DBpedia-INF consists\nof 26 predicates, 161 soft rules mined from DB-\npedia, and 16 rules defining the negation and sym-\nmetricity between the predicates. The difficulty\nof the questions is represented in terms of reason-\ning length from k∈{0,..., 5}.2 Larger kimplies\nharder question. Compared to the exact dataset\nused in Rulebert, we clean it in order to ensure the\nquestion-answer pairs are logically consistent and\nprobabilistically correct.\n4.2 Experimental Setup\nImplementation. We employ Scallop (Huang\net al., 2021) as the differentiable symbolic infer-\nence module. We show the program used for\nCLUTRR reasoning task in Figure 2. It comprises\nrelation type declarations, deductive rules for kin-\nship reasoning, and integrity constraints for com-\nputing semantic loss (attached in the Appendix).\nThe program used for DBpedia-INF is written in\na similar manner with additional high-order predi-\ncates listed in Table 2.\nPre-trained LMs for fine-tuning. We used the\nHuggingFace (Wolf et al., 2019) pre-trained w2v-\ngoogle-news-300, RoBERTa-base, and DeBERTa-\nbase as the pretrained language models. We fine-\ntune RoBERTa-base and DeBERTa-base during\ntraining with binary cross entropy loss. Our rela-\ntion extraction module is implemented by adding\nan MLP classifier after the LM, accepting a con-\ncatenation of the embedding of the two entities and\nthe embedding of the whole windowed context.\nOur model. Our main model, DSR-LM, uses\nRoBERTa as the underlying LM. The relation clas-\nsifier is a 2-layer fully connected MLP. For training,\nwe initialize ϕby prompting the LM. To accelerate\nthe learning process, we use multinomial sampling\nto retrieve 150 rules for symbolic reasoning. Dur-\ning testing, we will instead pick the top 150 rules.\nWe use two Adam optimizer to update θ and ϕ,\nwith learning rate 10−5 and 10−2 respectively.\nFor ablation studies, we present a few other mod-\nels. First, we ablate on back-bone LMs. Specifi-\ncally, we have DSR-LM-DeBERTa which uses De-\n2A length of 0 means that the hypothesis can be verified\nusing the facts alone without using any rules.\n3066\n// Relation declaration\ntype kinship(rela: String, subject: String, object: String)\ntype query(subject: String, object: String)\ntype composite(r1: String, r2: String, r3: String)\n// Rules to derive the final answer\nrel kinship(r3,a,c) = kinship(r1,a,b), kinship(r2,b,c), composite(r1,r2,r3), a != c\nrel answer(r) = query(s, o), derive(r, s, o)\n// Integrity constraints (6 for kinship graph and 2 for rule learning)\nrel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) =>\nkinship(SON, b, a) or kinship(DAUGHTER, b, a)) // Other constraints are omitted...\nFigure 2: The Scallop program used in the CLUTRR reasoning task.\nBERTa as the back-bone LM. DSR-w2v-BiLSTM,\non the other hand, uses as back-bone the word2vec\n(Mikolov et al., 2013) model for word embedding\nand BiLSTM (Huang et al., 2015) for sequential en-\ncoding. For DSR-LM-with-Manual-Rule we treat\nthe logic rules as given, meaning that we provide\n92 composition rules for CLUTRR and around 180\nrules for DBpedia-INF. In this case, we set ground\ntruth rules to have 1.0 weight and therefore ϕis\nnot learnt. Then, we have DSR-LM-without-IC\nwhich does not have integrity constraints and se-\nmantic loss. Lastly, we have DSR-without-LM that\ntakes ground truth structured entity relation graph\nas input. This way, we do not need the underlying\nrelation extractor and only ϕneeds to be learned.\nBaselines. We compare DSR-LM with a spec-\ntrum of baselines from purely neural to logically\nstructured. The baselines include pretrained large\nlanguage models (BERT (Kenton and Toutanova,\n2019) and RoBERTa (Liu et al., 2019)), non-LM\ncounterparts (BiLSTM (Hochreiter and Schmid-\nhuber, 1997; Cho et al., 2014) and BERT-LSTM),\nstructured models (GAT (Veliˇckovi´c et al., 2018),\nRN (Santoro et al., 2017), and MAC (Hudson and\nManning, 2018)), and other neuro-symbolic mod-\nels (CTP (Minervini et al., 2020), RuleBert (Saeed\net al., 2021)). The structured models include those\nmodels with relational inductive biases, while the\nneuro-symbolic model uses logic constraints.\nBaseline setup. We highlight a few baselines we\ninclude for completeness but are treated as unfair\ncomparison to us: GAT, CTP, and GPT-3 variants.\nAll baselines other than GAT and CTP take as input\nnatural language stories and the question to produce\nthe corresponding answer. GAT and CTP, on the\ncontrary, takes entity relation graph rather than\nnatural language during training and testing.\nThe model sizes are different across baselines as\nwell. Model size generally depends on two parts,\nthe backbone pre-trained LM, and the classifica-\ntion network built upon the LM. GPT-3 contains\n175B parameters, and RoBERTa uses 123M param-\neters. The classification model of our method has\n2.97M parameters (assuming using embeddings\nfrom RoBERTa). With extra 10K parameters for\nrule weights, our DSR-LM framework has around\n127M parameters.\nFor GPT-3 variants, we conduct experiments on\nCLUTRR with GPT-3 under the Zero-Shot (GPT-3\nZS), GPT-3 Fine-Tuned (GPT-3 FT), and Few(5)-\nShot (GPT-3 5S) (Brown et al., 2020), as well as\nZero-Shot-CoT (GPT-3 ZS-CoT) (Kojima et al.,\n2022a) settings. For fair comparison, we also in-\nclude the ground truth kinship composition knowl-\nedge in GPT-3 zero shot (GPT-3 ZS w/ Rule), and 5\nshot (GPT-3 5S w/ Rule). We include the prompts\nwe used and additional details in Appendix A.\nDSR-LM\nDSR-w2v-BiLSTM\nGPT -3 ZSGPT -3 5S\nGPT -3 ZS-CoT\nGPT -3 FT\nGPT -3 ZS w/ RuleGPT -3 5S w/ Rule\nRoBERTa\nBERT\nBERT -LSTM\nMACRN\nBiLSTM-Mean\nBiLSTM-Att\n0\n20\n40\n60\n60.98\n40.39\n28.630.9\n25.6\n34.3\n26.4\n33.134.5\n19.5\n34.8 38.539.9\n34.937\n28.630.9\n25.6\n34.3\n26.4\n33.134.5\n19.5\n34.8 38.539.9\n34.937\nAccuracy (%)\nFigure 3: DSR-LM’s performance on CLUTRR comp-\nared with various baselines\n4.3 Experimental Results\nDSR-LM systematically outperforms a wide\nrange of baselines by a large margin. We eval-\nuate DSR-LM and baselines on both CLUTRR and\nDBpedia-INF, as reported in Figure 3 and Table 3.\nIn the CLUTRR experiment, DSR-LM achieves\nthe best performance among all the models (Fig-\nure 3). Next, we examine how models trained on\nstories generated from clauses of length k≤3 and\n3067\nTest Length DSR-LM RuleBert\nOverall 95.87 72.59\n0 100.0 98.40\n1 100.0 54.80\n2 98.4 75.20\n3 89.2 64.00\n4 88.1 69.89\n5 100.0 72.29\nTable 3: DBpedia-INF generalization evalu-\nation under different test reasoning length.\nModels are trained on 10K reasoning length\nk=0 sequences, and tested on sequences of\nreasoning length k=[0,5].\nConfidence Learnt Rules\n1.154 mother( a,c) ← sister(a,b) ∧mother(b,c)\n1.152 daughter( a,c) ← daughter(a,b) ∧sister(b,c)\n1.125 sister( a,c) ← daughter(a,b) ∧aunt(b,c)\n1.125 father( a,c) ← brother(a,b) ∧father(b,c)\n1.123 granddaughter( a,c) ← grandson(a,b) ∧sister(b,c)\n1.120 brother( a,c) ← sister(a,b) ∧brother(b,c)\n1.117 brother( a,c) ← son(a,b) ∧uncle(b,c)\n1.105 brother( a,c) ← daughter(a,b) ∧uncle(b,c)\n1.104 daughter( a,c) ← wife(a,b) ∧daughter(b,c)\n1.102 mother( a,c) ← brother(a,b) ∧mother(b,c)\n. . . . . .\nTable 4: The learnt top-10 confident logic rules over CLUTRR.\n2 4 6 8 10\n25\n50\n75\n100\nk, length of reasoning chain\nAccuracy (%)\nOurs BiLSTM-Mean BERT RoBERTa\nRN BiLSTM-Att BERT-LSTM MAC\n(a) Comparison to common baselines\n2 4 6 8 10\n25\n50\n75\n100\nk, length of reasoning chain\nAccuracy (%)\nOurs GPT-3 ZS GPT-3 ZS-CoT GPT-3 ZS w/ Rule\nGPT-3 FT GPT-3 5S GPT-3 5S w/ Rule\n(b) Comparison to GPT-3 related baselines\nFigure 4: Systematic generalization performance comparison on CLUTRR dataset. Models except GPT-3-ZS*,\nGPT-3-FS are trained (or fine-tuned) on k∈{2,3}. All models are tested on k∈{2,..., 10}.\nevaluated on stories generated from larger clauses\nof length k ≥ 4. A fine-grained generalizabil-\nity study reveals that although all models’ perfor-\nmances decline as the reasoning length of the test\nsequence increases, pure neural-based models de-\ncrease the fastest (Figure 4a and 4b). It manifests\nthe systematic issue that language models alone\nare still not robust for length generalization (Lake\nand Baroni, 2018). On the other hand, the perfor-\nmance of DSR-LM decreases much slower as test\nreasoning length increases and outperforms all the\nbaselines when k≥4.\nIn the DBpedia-INF experiment, DSR-LM out-\nperforms RuleBert by 37% in terms of overall per-\nformance (Table 3), showing that DSR-LM has\nmuch more robust generalization. Recall that Rule-\nBert aims to improve the logical reasoning of LMs\nby straightforward fine-tuning with soft rules and\nfacts. Our results show that augmenting data alone\nfor fine-tuning do not effectively improve system-\naticity. Meanwhile, DSR-LM imbues reasoning\ninductive biases throughout training and learns use-\nful rules to generalize to longer reasoning lengths.\nLearning interpretable logic rules. DSR-LM is\ncapable of producing explicit logic rules as part of\nthe learning process. For presentation, we show the\ntop-10 rules learnt from DSR-LM model in Table 4.\nWe compare the top-92 most likely prompted and\nfine-tuned rules against the 92 hand-crafted rules,\nand 70 of them match. Additionally, we find that\nour rule weight fine-tuning helps correct 11 of the\nincorrect rules produced by LM. Through this qual-\nitative analysis, it is clear that DSR-LM provides\nan interface to probe and interpret the intermediate\nsteps, enhancing the interpretability.\nGPT-3 variants are inferior in long-range rea-\nsoning. Interestingly, ZS scores 28.6% accuracy\non CLUTRR while ZS-CoT scores 25.6%, sug-\ngesting that the chain-of-thought prompting might\nnot work in long-range reasoning (Figure 3). In\nfact, there are many cases where GPT-3 favors\ncomplication over simplicity: GPT-3 frequently an-\nswers “stepdaughter”, “stepmother”, and “adopted\nson”, while the real answers are simply “daugh-\nter”, “mother”, and “son”. Additionally, GPT-3\ncould derive the correct result for the wrong rea-\nson, e.g. “Jeffrey is Gabrielle’s son, which would\nmake William her grandson, and Jeffrey’s brother.”\nWhile we count the final answer to be correct\n(William is Jeffrey’s brother), there is a clear in-\n3068\nconsistency in the reasoning chain: William cannot\nbe Gabrielle’s grandson and Jeffrey’s brother si-\nmultaneously, given that Jeffrey is Gabrielle’s son.\nLastly, we observe that, both GPT-3 FT and many\nother methods have an accuracy drop askbecomes\nlarger (Figure 4b), ZS and ZS-CoT stay relatively\nconsistent, suggesting that the size of context and\nthe reasoning chain may have a low impact on GPT-\n3’s performance.\n4.4 Analyses and Ablation Studies\nSymbolic reasoner consistently improves LMs\nand word embeddings. Since DSR-LM has\na model agnostic architecture, we study how\nthe choice of different LMs impacts the reason-\ning performance. As shown in Table 5, the\ntwo transformer-based models have on-par perfor-\nmance and outperform the word2vec one. However,\nnote that the word2vec-based model still has bet-\nter performance than all other baselines. Besides\nhigher final accuracy, the pre-trained transformer-\nbased language model also accelerates the train-\ning process. Both DSR-LM-RoBERTa and DSR-\nLM-DeBERTa reach their best performance within\n20 epochs, while it takes DSR-w2v-BiLSTM 40\nepochs to peak.\nModel Accuracy (%)\nDSR-LM (RoBERTa) 60.98 ±2.64\nDSR-LM-DeBERTa 60.92 ±2.72\nDSR-w2v-BiLSTM 40.39 ±0.06\nTable 5: Ablation study about neural backbones of\nDSR-LM. We compare the CLUTRR performance of\nDSR-LM using different LMs.\nIncorporate domain knowledge. DSR-LM al-\nlows injecting domain specific knowledge. In DSR-\nLM-with-Rule, we manually crafted 92 rules for\nkinship reasoning to replace the learnt rules. As\nshown in Table 6, it obtained a 0.36% performance\ngain over DSR-LM. The fact that the improvement\nis marginal implies our method extracts useful rules\nto obtain on-par performance with manually crafted\nones. DSR-LM-without-IC, our model without in-\ntegrity constraints specified on predicted relations\nand rules, performs worse than DSR-LM, suggest-\ning that logical integrity constraints are essential\ncomponent for improving the model robustness.\nThe impact of the relation extractor. To under-\nstand what causes the failure case of DSR-LM, we\nstudy the performance of our relation classification\nmodel separately. We isolate the trained relation\nModel Accuracy (%)\nDSR-LM 60.98 ±2.64\nDSR-LM-without-IC 51.48 ±0.57\nDSR-LM-with-Manual-Rule 61.34 ±1.56\nTable 6: Ablation study. We compare our model’s per-\nformance on CLUTRR with different setups.\nextractor and found that it reaches 84.69% accu-\nracy on the single relation classification task. For\ncomparison, we train a relation extractor using all\nthe intermediate labels in the training dataset, and it\nreaches 85.32% accuracy. It shows that even using\nonly weak supervision (i.e., the final answers to\nmulti-hop questions), our approach can reach on-\npar performance as supervised relation extraction.\nReasoning over structured KBs. To understand\nthe rule learning capability of our approach, we de-\nsign our ablation model DSR-without-LM to take\nas input ground-truth KBs instead of natural lan-\nguage. In this case, rule weights are not initialized\nby LM but randomized. As shown in Table 7, our\nmodel outperforms GAT and CTP which also op-\nerates on structured KBs. It demonstrates that our\ndifferentiable rule learning paradigm learns rules\nto reason about KBs consistently.\nModel Accuracy (%)\nGAT 39.05\nCTP 95.57\nDSR-without-LM 98.81\nTable 7: DSR-without-LM compared against GAT and\nCTP on reasoning with ground truth KBs. For this\ncomparison we train onk∈[2,3]and test onk∈[4,10].\nFailure cases of DSR-LM. We showcase in Ap-\npendix Table 8 that even state-of-the-art large LMs\nare prone to logical fallacies. On the other hand,\nthe failure case of our method usually occurs in the\nstage of relation extraction. For example, for the\nfollowing sentence “Christopher and Guillermina\nare having a father-daughter dance”, our RoBERTa\nbased relation extractor fails to recognize the father-\ndaughter relationship but rather thinks C and G\nhave a husband-wife relationship. We require most\nof the relation extraction to be correct in order to\navoid cascading error. As the error rate on individ-\nual relation extraction accumulates, it leads to the\nobserved drop in accuracy as kbecomes larger.\n5 Concluding Remarks\nWe investigate how to improve LMs’ logical rea-\nsoning capability using differentiable symbolic rea-\n3069\nsoning. Through extensive experiments, we demon-\nstrate the effectiveness of DSR-LM over challeng-\ning scenarios where widely deployed large LMs\nfail to reason reliably. We hope our work can lay\nthe groundwork for exploring neuro-symbolic pro-\ngramming techniques to improve the robustness of\nLMs on reasoning problems.\nLimitations\nThe primary limitation of DSR-LM is the need for a\nconfined problem space. It requires a well-defined\nrelational schema to perform logical reasoning, and\nthus will not be suited for an open-ended problem\nsetup. Nevertheless, DSR-LM is suitable for many\ndomain specific problems within Natural Language\nUnderstanding and Reasoning, allowing domain\nexperts to freely inject domain-specific knowledge\nin a structured and logical manner.\nReferences\nGregor Betz, Christian V oigt, and Kyle Richardson.\n2020. Critical thinking for language models. arXiv\npreprint arXiv:2009.07185.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NeurIPS.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. Advances in Neural Information Processing\nSystems, 31.\nSwarat Chaudhuri, Kevin Ellis, Oleksandr Polozov,\nRishabh Singh, Armando Solar-Lezama, Yisong Yue,\net al. 2021. Neurosymbolic Programming.\nKyunghyun Cho, Bart van Merrienboer, Çaglar\nGülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder for\nstatistical machine translation. In EMNLP.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2021.\nTransformers as soft reasoners over language. In\nIJCAI.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Schütze, and\nYoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. TACL.\nRichard Evans and Edward Grefenstette. 2018. Learn-\ning explanatory rules from noisy data. Journal of\nArtificial Intelligence Research, 61:1–64.\nKlaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmid-\nhuber. 2020. On the binding problem in artificial\nneural networks. arXiv preprint arXiv:2012.05208.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2021. Do language models have be-\nliefs? methods for detecting, updating, and visualiz-\ning model beliefs. arXiv preprint arXiv:2111.13654.\nChadi Helwe, Chloé Clavel, and Fabian M. Suchanek.\n2021. Reasoning with transformer-based models:\nDeep learning, but shallow reasoning. In AKBC.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation.\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel,\nMayur Naik, Le Song, and Xujie Si. 2021. Scallop:\nFrom probabilistic deductive databases to scalable\ndifferentiable reasoning. NeurIPS.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging. arXiv\npreprint arXiv:1508.01991.\nDrew A Hudson and Christopher D Manning. 2018.\nCompositional attention networks for machine rea-\nsoning. In ICLR.\nDaniel Kahneman. 2011. Thinking, fast and slow .\nMacmillan.\nNora Kassner, Benno Krojer, and Hinrich Schütze. 2020.\nAre pretrained language models symbolic reasoners\nover knowledge? CoNLL.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL.\nYoon Kim. 2021. Sequence-to-sequence learning with\nlatent neural grammars. NeurIPS.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022a. Large\nlanguage models are zero-shot reasoners. NeurIPS.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022b. Large\nlanguage models are zero-shot reasoners. NeurIPS.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills of\nsequence-to-sequence recurrent networks. In ICML.\n3070\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kim-\nmig, Thomas Demeester, and Luc De Raedt. 2018.\nDeepproblog: Neural probabilistic logic program-\nming. NeurIPS.\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B\nTenenbaum, and Jiajun Wu. 2018. The neuro-\nsymbolic concept learner: Interpreting scenes, words,\nand sentences from natural supervision. In ICLR.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nNeurIPS.\nPasquale Minervini, Sebastian Riedel, Pontus Stenetorp,\nEdward Grefenstette, and Tim Rocktäschel. 2020.\nLearning reasoning strategies in end-to-end differen-\ntiable proving. In ICML.\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and\nBrenden M Lake. 2021. Improving coherence and\nconsistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. NeurIPS.\nTim Rocktäschel and Sebastian Riedel. 2017. End-to-\nend differentiable proving. NeurIPS.\nMohammed Saeed, Naser Ahmadi, Preslav Nakov, and\nPaolo Papotti. 2021. Rulebert: Teaching soft rules to\npre-trained language models. In EMNLP.\nAdam Santoro, David Raposo, David G Barrett, Ma-\nteusz Malinowski, Razvan Pascanu, Peter Battaglia,\nand Timothy Lillicrap. 2017. A simple neural net-\nwork module for relational reasoning. NeurIPS.\nAbulhair Saparov and He He. 2022. Language models\nare greedy reasoners: A systematic formal analysis of\nchain-of-thought. arXiv preprint arXiv:2210.01240.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and\nKristina Toutanova. 2021. Compositional generaliza-\ntion and natural language variation: Can a semantic\nparsing approach handle both? In ACL.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle\nPineau, and William L. Hamilton. 2019. Clutrr: A\ndiagnostic benchmark for inductive reasoning from\ntext. EMNLP.\nLivio Baldini Soares, Nicholas Fitzgerald, Jeffrey Ling,\nand Tom Kwiatkowski. 2019. Matching the blanks:\nDistributional similarity for relation learning. In\nACL.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020a. olmpics-on what language\nmodel pre-training captures. TACL.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020b. Leap-of-thought:\nTeaching pre-trained models to systematically reason\nover implicit knowledge. NeurIPS.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2020. Long\nrange arena: A benchmark for efficient transformers.\nIn ICLR.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert\nrediscovers the classical nlp pipeline. In ACL.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for llms on plan-\nning and reasoning about change). arXiv preprint\narXiv:2206.10498.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. NeurIPS.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. EMNLP.\nJingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and\nGuy Broeck. 2018. A semantic loss function for deep\nlearning with symbolic knowledge. In ICML.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reason-\ning. NeurIPS.\nHanlin Zhang, Yi-Fan Zhang, Li Erran Li, and Eric\nXing. 2022. The impact of symbolic representations\non in-context learning for few-shot reasoning. arXiv\npreprint.\nYuyu Zhang, Xinshi Chen, Yuan Yang, Arun Rama-\nmurthy, Bo Li, Yuan Qi, and Le Song. 2019. Effi-\ncient probabilistic logic reasoning with graph neural\nnetworks. In ICLR.\nWangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiao-\ndan Liang, Maosong Sun, Chenyan Xiong, and Jian\nTang. 2020. Towards interpretable natural language\nunderstanding with explanations as latent variables.\nNeurIPS.\n3071\nA Implementation Details\nReasoner details. The learning of rules and\nthe fine-tuning of the underlying LM should hap-\npen separately with different learning rates – fine-\ntuning LM is an intricate process that requires a\nvery small learning rate, while rules should be\nlearned with larger learning rates since gradients\nare directly back-propagated onto the weights. This\ncan be realized by employing two separate optimiz-\ners, one for fine-tuning and the other for rule learn-\ning. During training time, we rotate training the\ntwo parts by toggling one and the other optimizer\nfor every 10 batches of data points.\nRule learning training setup. For rule learning,\nwe can initialize the transitivity tensor using the\nlanguage model provided composite rules. Since\nthe CLUTRR dataset consists of 20 different rela-\ntions and a transitivity relationship is defined over\n3 relations, there are 8K possible transitivity facts\nover these relations. Specifically, we give every\npredicted composite rule by the GPT with a 0.5\nweight, while initializing the other rules with a\nrange such as [0,0.1], since otherwise, an insen-\nsible transitive fact may be getting a random high\nweight while it effectively does nothing for rea-\nsoning. The learning process encourages the rules\nthat yield the correct query result and suppresses\nthe rules that lead to wrong answers. To avoid the\nexponential blow-up caused by injecting all the 8K\nrules in the reasoning engine, we sample 200 rules\naccording to their weights during the training time\nand deterministically use the top 200 learned rules\nduring the test time. For the QA-No-Rule setup,\nthe confidence score of rules, the MLP classifier\nfor relation extraction, and the underlying LM are\nlearned and updated simultaneously during train-\ning. To account for their difference, we employ two\nAdam optimizers ARL and ARE. ARE is used for\noptimizing models for relation extraction, and thus\nwill take as parameters the MLP classifier and the\nunderlying LM. It has a low learning rate 0.00001\nsince it needs to fine-tune LMs. ARL, on the other\nhand, will take as a parameter the confidence score\ntensor for the transitive rules, and is set to have\na higher learning rate of 0.001. For the integrity\nconstraints, we set the result integrity violation loss\nwith the weight 0.1, and set the rule integrity con-\nstraint violation loss with the weight 0.01. We set\nthe batch size to 16 and train for 20 epochs.\nTo obtain the initial rule weights for the compo-\nsition rule in our CLUTRR experiment, the prompt\nwe use is “Mary’s P’s Q is her <mask>.” where P\nand Q are enumerations of all possible relationships,\nand the unmasked value is treated as the answer\nR, producing composite(P, Q, R). For the\nother rule templates we used, the prompts are\n1. transitive: “is R’s R one’s R? <mask>”;\nthe probability of the unmasked word be-\ning “yes” is treated the rule weight for\ntransitive(R).\n2. symmetric: “does A is R of B means B is R\nof A? <mask>”; the probability of the un-\nmasked word being “yes” is treated the rule\nweight for symmetric(R).\n3. inverse: “A is R of B means B is <mask>\nof A”; the unmasked value is treated as the\nanswer P, producing inverse(R, P).\n4. implies: “does R imply P? <mask>”; the\nprobability of unmasked value being “yes” is\ntreated as the rule weight for implies(R,\nP).\nGPT-3 Prompt Setups. For Zero-Shot, we\nuse the prompt “So B is A’s:” for the query pair\n(A,B) to ask GPT-3 to complete the relationship\nbetween Aand B. We pick the phrase in the first\nline or before the first period from the completed\ntext and compare it directly with the ground truth\nrelation. For the Few(5)-Shot setting, we randomly\nselect 5 examples from the training dataset used for\nother models (k∈[2,3]) to serve as examples. We\nuse the same prompt for Few-Shot and Fine-Tuned\nas the Zero-Shot and the automated GPT-3 fine-\ntuning setup for our training dataset, trained for 4\nepochs. To add in the transitive KB, we simply in-\nclude 92 hand-crafted rules in natural language as\na part of the prompt, and we performed Zero-shot\nwith KB, and Few(5)-shot with KB experiments.\nFor the Zero-Shot-CoT setting, we use the prompt\n“Who is Bto A? Let’s think step by step” to sug-\ngest GPT-3 to auto-complete while working out a\nreasoning chain. Under this setup, it is impossible\nto compare the answer to the ground truth automat-\nically. Therefore, we manually check through the\nwhole test dataset of CLUTRR.\nB Additional Experimental Results\nIn Table 8, we showcase the failure cases of large\nLMs for logical inference, where Zero-shot-CoT\ndenotes zero-shot chain-of-thoughts (Kojima et al.,\n2022b).\n3072\nk Prompt GPT-3 CoT Output Ground Truth\n2 Jeffrey asked his brother William if he would come help him fix\nhis car next weekend. Gabrielle was delighted to see her grandson,\nWilliam, at her birthday party. Who is Jeffrey to Gabrielle? Let’s\ndo it step by step:\nJeffrey is Gabrielle’s grandson.\nWilliam is Jeffrey’s brother.\ngrandson\n3 Elliott felt lonely when his wife Marian was gone to see her mother\nMolly. Elliott and his daughter Karen took a day off school to go to\nthe zoo. Who is Karen to Molly? Let’s do it step by step:\nKaren is Elliott’s daughter. Molly\nis Elliott’s wife’s mother. There-\nfore, Karen is Molly’s grand-\ndaughter.\ngranddaughter\n4 Frank took his daughter Julia to the park. Beverly, Emilie´s sister\nwent with them. Kimberley was excited because today she was\ngoing to the zoo with her uncle Frank. Jenny spent a great day\nshopping with her daughter, Kimberley. Who is Beverly to Jenny?\nLet’s do it step by step:\nBeverly is Jenny’s sister-in-law. niece\n5 Melissa and her father, Stephen, went to the marina. Stephen’s\ndaughter, Margaret, had purchased a boat, and they were eager to\nsee it. Annie and her aunt, Evelyn, went to the deli. They got a half\na pound of corned beef, and two pounds of salami. Melissa and\nher sister Annie were arguing over a toy. While they were fighting,\nMelissa’s sister Margaret grabbed the toy, and ran off to play with\nit by herself. Who is Evelyn to Melissa? Let’s do it step by step:\nMelissa’s father is Stephen.\nStephen has a daughter, Margaret.\nMargaret is Annie’s aunt. There-\nfore, Evelyn is Annie’s aunt, and\nMelissa’s great-aunt.\naunt\n6 James took his grandson Davis to the baseball game. Sharon and\nher son Brian went out to lunch together yesterday. Brian and his\nbrother Oliver went to the park to play basketball. Davis played\nscrabble with his mother Sharon and his brother Oliver. Davis\nasked his brother Oliver if he would come help him fix his car next\nweekend. Who is James to Oliver? Let’s do it step by step:\nJames is Davis’s grandfather, so\nhe is Oliver’s great-grandfather.\ngrandfather\n7 George rushed to the hospital to find out that his wife and already\ngiven birth to a boy and had named him David. Larry was playing\ncheckers with his brother Craig. Larry’s sister Jean was waiting to\nplay. Judy made mixed drinks for her son Craig. Judy’s daughter\nJean was jealous because she was n’t offered one. Jean planned a\ntrip to the zoo for her brother, Craig. They had a great time. George\nwas having trouble with his divorce because he only got to see his\ndaughter Jean a few times a week. Who is David to Craig? Let’s\ndo it step by step:\nGeorge’s wife gave birth to a boy\nand named him David. David is\nCraig’s nephew.\nbrother\n8 Anthony went to the bakery with his uncle Roger to pick up some\nbread for lunch. Anthony loved going to the store with his mom\nMary. She always bought him snacks Charles would n’t let his son\nFloyd go to the park by himself. Floyd’s brother Anthony offered\nto go with him. Faye was hoping to travel out of state to visit her\nson Cecil. Ellie’s brother, Charles, was doing his best to find the\nbest flight. Mary and her son Anthony went out to lunch together\nyesterday. Cecil took her sister, Christine, out to dinner for her\nbirthday. Who is Roger to Christine? Let’s do it step by step:\nCecil is Faye’s son, so Faye is\nChristine’s mother-in-law. Chris-\ntine is married to Faye’s son, so\nChristine is Faye’s daughter-in-\nlaw. Charles is Ellie’s brother,\nso Charles is Christine’s brother-\nin-law. Anthony is Charles’s son,\nso Anthony is Christine’s nephew.\nRoger is Anthony’s uncle, so\nRoger is Christine’s brother-in-\nlaw.\nuncle\n9 Gino bought gifts for his lovely sister, Jami. Michele and William\nleft that evening to go bowling. Ethel and her son, Gordon, went\nto the park, and had a wonderful time. Manuel’s father, Kenneth,\nthought his son was a monster. Meanwhile Michele thought her son\nManuel was a little angel. Gino, Kenneth’s other son, agreed with\nhis dad. William asked his brother Larry if he would come help\nhim fix his car next weekend. Joseph was so happy for his brother,\nLarry, getting a promotion. Gordon and his uncle Joseph went to\nthe movies Sunday after church and got popcorn and candy while\nthey were there. Who is Jami to Ethel? Let’s do it step by step:\n1. Jami is Gino’s sister. 2. Ethel\nis Gino’s mother. 3. Therefore,\nJami is Ethel’s daughter-in-law.\nniece\nTable 8: Qualitative analysis of GPT-3 Zero-Shot-CoT on the CLUTRR dataset. The novelty comes from the\nsentence marked in orange. Queries that are of interest are marked in blue. Correct answer in the output is marked\ngreen and incorrect ones are marked red.\n3073\nConfidence Rule\n1.154 mother(A,B) ← sister(A,C) ∧mother(C,B)\n1.152 daughter(A,B) ← daughter(A,C) ∧sister(C,B)\n1.125 sister(A,B) ← daughter(A,C) ∧aunt(C,B)\n1.125 father(A,B) ← brother(A,C) ∧father(C,B)\n1.123 granddaughter(A,B) ← grandson(A,C) ∧sister(C,B)\n1.120 brother(A,B) ← sister(A,C) ∧brother(C,B)\n1.117 brother(A,B) ← son(A,C) ∧uncle(C,B)\n1.105 brother(A,B) ← daughter(A,C) ∧uncle(C,B)\n1.104 daughter(A,B) ← wife(A,C) ∧daughter(C,B)\n1.102 mother(A,B) ← brother(A,C) ∧mother(C,B)\n1.102 brother(A,B) ← father(A,C) ∧son(C,B)\n1.096 sister(A,B) ← mother(A,C) ∧daughter(C,B)\n1.071 sister(A,B) ← father(A,C) ∧daughter(C,B)\n1.071 son(A,B) ← son(A,C) ∧brother(C,B)\n1.070 uncle(A,B) ← father(A,C) ∧brother(C,B)\n1.066 daughter(A,B) ← son(A,C) ∧sister(C,B)\n1.061 brother(A,B) ← brother(A,C) ∧brother(C,B)\n1.056 grandson(A,B) ← husband(A,C) ∧grandson(C,B)\n1.055 sister(A,B) ← son(A,C) ∧aunt(C,B)\n1.053 grandmother(A,B) ← sister(A,C) ∧grandmother(C,B)\n1.050 granddaughter(A,B) ← granddaughter(A,C) ∧sister(C,B)\n1.050 grandmother(A,B) ← brother(A,C) ∧grandmother(C,B)\n1.047 grandson(A,B) ← granddaughter(A,C) ∧brother(C,B)\n1.046 grandfather(A,B) ← mother(A,C) ∧father(C,B)\n1.036 son(A,B) ← daughter(A,C) ∧brother(C,B)\n1.035 sister(A,B) ← brother(A,C) ∧sister(C,B)\n1.029 grandmother(A,B) ← mother(A,C) ∧mother(C,B)\n1.027 grandfather(A,B) ← sister(A,C) ∧grandfather(C,B)\n1.019 brother(A,B) ← mother(A,C) ∧son(C,B)\n1.017 granddaughter(A,B) ← wife(A,C) ∧granddaughter(C,B)\nTable 9: Showcase of the learnt logic rules with top@30 confidence of DSR-LM rule learning.\n3074\n// question :: (sub, obj) represents a question asking about relation\n// between `sub` and `obj`\ntype question(sub: String, obj: String)\n// context :: (rela, sub, obj) represents there is a `rela`\n// between `sub` and `obj`\ntype kinship(rela: usize, sub: String, obj: String)\n// Composition rule :: (r1, r2, r3) represents compositing r1 and r2 yields r3\ntype composite(r1: usize, r2: usize, r3: usize)\n// Constants used for defining relation properties\nconst DAUGHTER = 0, SISTER = 1, ..., MOTHER_IN_LAW = 19\nconst MALE = 0, FEMALE = 1\ntype gender(r: usize, gender_id: i32)\nrel gender = {(DAUGHTER, FEMALE), (SISTER, FEMALE), ..., (MOTHER_IN_LAW, FEMALE)}\ntype gen(r: usize, gen_id: i32)\nrel gen = {(DAUGHTER, -1), (SISTER, 0), ..., (MOTHER_IN_LAW, 1)}\n// Composition\nrel kinship(r3, x, z) = composite(r1, r2, r3),\nkinship(r1, x, y), kinship(r2, y, z), x != z\n// Answer\nrel answer(r) = question(s, o), kinship(r, s, o)\n// Integrity constraints on results\nrel violation(!r) = r := forall(a, b: kinship(GRANDFATHER, a, b) =>\n(kinship(GRANDSON, b, a) or kinship(GRANDDAUGHTER, b, a)))\nrel violation(!r) = r := forall(a, b: kinship(GRANDMOTHER, a, b) =>\n(kinship(GRANDSON, b, a) or kinship(GRANDDAUGHTER, b, a)))\nrel violation(!r) = r := forall(a, b: kinship(FATHER, a, b) =>\n(kinship(SON, b, a) or kinship(DAUGHTER, b, a)))\nrel violation(!r) = r := forall(a, b: kinship(MOTHER, a, b) =>\n(kinship(SON, b, a) or kinship(DAUGHTER, b, a)))\nrel violation(!r) = r := forall(a, b: kinship(HUSBAND, a, b) => kinship(WIFE, b, a))\nrel violation(!r) = r := forall(a, b: kinship(BROTHER, a, b) =>\n(kinship(SISTER, b, a) or kinship(BROTHER, b, a)))\n// Integrity constraints on rules\nrel violation(!r) = r := forall(r1, r2, r3:\ncomposite(r1, r2, r3) and gender(r2, g) => gender(r3, g))\nrel violation(!r) = r := forall(r1, r2, r3:\ncomposite(r1, r2, r3) and gen(r1, g1) and gen(r2, g2) => gen(r3, g1 + g2))\nFigure 5: Full Scallop program including deductive rules and integrity constraints\n3075\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3076\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n3077",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7310268878936768
    },
    {
      "name": "Principle of compositionality",
      "score": 0.6135522723197937
    },
    {
      "name": "Programming language",
      "score": 0.48628175258636475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48587173223495483
    },
    {
      "name": "Deductive reasoning",
      "score": 0.44313088059425354
    },
    {
      "name": "Natural language processing",
      "score": 0.3766283094882965
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3648940324783325
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}