{
  "title": "Probing Biomedical Embeddings from Language Models",
  "url": "https://openalex.org/W2932592714",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2308246978",
      "name": "Jin Qiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223922036",
      "name": "Dhingra, Bhuwan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222578411",
      "name": "Cohen, William W.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128200320",
      "name": "Lu Xinghua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2527896214",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2899463504",
    "https://openalex.org/W2950279864",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2741930244",
    "https://openalex.org/W2626667877",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2952341153",
    "https://openalex.org/W2801092741",
    "https://openalex.org/W2963372062",
    "https://openalex.org/W2889468083",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2898355739"
  ],
  "abstract": "Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT, ELMo, BioBERT and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.",
  "full_text": "Probing Biomedical Embeddings from Language Models\nQiao Jin\nUniversity of Pittsburgh\nqiao.jin@pitt.edu\nBhuwan Dhingra\nCarnegie Mellon University\nbdhingra@cs.cmu.edu\nWilliam W. Cohen\nGoogle, Inc.\nwcohen@google.com\nXinghua Lu\nUniversity of Pittsburgh\nxinghua@pitt.edu\nAbstract\nContextualized word embeddings derived\nfrom pre-trained language models (LMs) show\nsigniﬁcant improvements on downstream NLP\ntasks. Pre-training on domain-speciﬁc cor-\npora, such as biomedical articles, further im-\nproves their performance. In this paper,\nwe conduct probing experiments to determine\nwhat additional information is carried intrinsi-\ncally by the in-domain trained contextualized\nembeddings. For this we use the pre-trained\nLMs as ﬁxed feature extractors and restrict\nthe downstream task models to not have ad-\nditional sequence modeling layers. We com-\npare BERT (Devlin et al., 2018), ELMo (Pe-\nters et al., 2018a), BioBERT (Lee et al., 2019)\nand BioELMo, a biomedical version of ELMo\ntrained on 10M PubMed abstracts. Surpris-\ningly, while ﬁne-tuned BioBERT is better than\nBioELMo in biomedical NER and NLI tasks,\nas a ﬁxed feature extractor BioELMo outper-\nforms BioBERT in our probing tasks. We use\nvisualization and nearest neighbor analysis to\nshow that better encoding of entity-type and\nrelational information leads to this superiority.\n1 Introduction\nNLP has seen an upheaval in the last year, with\ncontextual word embeddings, such as ELMo (Pe-\nters et al., 2018a) and BERT (Devlin et al.,\n2018), setting state-of-the-art performance on\nmany tasks. These empirical successes suggest\nthat unsupervised pre-training from large corpora\ncould be a vital part of NLP models. In spe-\nciﬁc domains like biomedicine, NLP datasets are\nmuch smaller than their general-domain coun-\nterparts1, which leads to a lot of ad-hoc mod-\nels: some infer through knowledge bases (Chandu\n1For example, MedNLI (Romanov and Shivade, 2018)\nonly has about 11k training instances while the general do-\nmain NLI dataset SNLI (Bowman et al., 2015) has 550k.\net al., 2017), while others leverage large-scale gen-\neral domain datasets for domain adaptation (Wiese\net al., 2017). However, unlabeled biomedical texts\nare abundant, and their full potential has perhaps\nnot yet been fully realized.\nWe train a domain-speciﬁc version of ELMo on\n10M PubMed abstracts, called BioELMo 2. Ex-\nperiments on biomedical named entity recogni-\ntion (NER) dataset BC2GM (Smith et al., 2008)\nand biomedical natural language inference (NLI)\ndataset MedNLI (Romanov and Shivade, 2018)\nclearly show the utility in training in-domain con-\ntextual word representations, but we would also\nlike to know exactly what extra information is car-\nried intrinsically in these embeddings.\nTo answer this question, we design twoprobing\ntasks, one for NER and one for NLI, where contex-\ntualized embeddings are used solely as ﬁxed fea-\nture extractors and no sequence modeling layers\nare allowed above the embeddings. This setting\nprohibits the model from capturing task-speciﬁc\ncontextual patterns, and instead only utilizes the\ninformation already present in the representations.\nIn parallel to our work of BioELMo, Lee et al.\n(2019) introduce BioBERT, which is a biomedical\nversion of in-domain trained BERT. We also probe\nBioBERT in our experiments.\nExpectedly, BioELMo and BioBERT perform\nsigniﬁcantly better than their general-domain\ncounterparts. When ﬁne-tuned, BioBERT out-\nperforms BioELMo, however, when used as\nﬁxed feature extractors, BioELMo is better than\nBioBERT in our probing tasks. Visualizations\nand nearest neighbor analyses suggest that it’s be-\ncause BioELMo more effectively encodes entity-\ntypes and information about biomedical relations,\nsuch as disease and symptom interactions, than\nBioBERT.\n2Available at https://github.com/Andy-jqa/\nbioelmo.\narXiv:1904.02181v1  [cs.CL]  3 Apr 2019\n2 Related Work\nEmbeddings from Language Models: ELMo\n(Peters et al., 2018a) is a pre-trained deep bidirec-\ntional LSTM (biLSTM) language model. ELMo\nword embeddings are computed by taking a\nweighted sum of the hidden states from each\nlayer of the LSTM. The weights are learned along\nwith the parameters of a task-speciﬁc downstream\nmodel, but the LSTM layers are kept ﬁxed. Re-\ncently, Devlin et al. (2018) introduced BERT,\nand they showed that pre-training transformer net-\nworks on a masked language modeling objective\nleads to even better performance by ﬁne-tuning\nthe transformer weights on a broad range of NLP\ntasks. We study biomedical in-domain versions of\nthese contextualized word embeddings in compar-\nison to the general ones.\nBiomedical Word Embeddings: Context-\nindependent word embeddings, such as word2vec\n(w2v) (Mikolov et al., 2013) trained on biomedi-\ncal corpora, are widely used in biomedical NLP\nmodels. Some recent works reported better\nNER performance with in-domain trained ELMo\nthan general ELMo (Zhu et al., 2018; Sheikhshab\net al., 2018). Lee et al. (2019) introduce BioBERT,\nwhich is BERT pre-trained on biomedical texts\nand set new state-of-the-art performance on\nseveral biomedical NLP tasks. We reafﬁrm these\nresults on biomedical NER and NLI datasets with\nin-domain trained contextualized embeddings,\nand further explore why they are superior.\nProbing Tasks: Designing tasks to probe sen-\ntence or token representations for linguistic prop-\nerties has been a widespread practice in NLP. In-\nferSent (Conneau et al., 2017) uses transfer tasks\nto probe for sentence embeddings pre-trained on\nsupervised data. Many studies (Dasgupta et al.,\n2018; Poliak et al., 2018) design new test sets\nto probe for speciﬁc linguistic signals in sen-\ntence rerpesentations. Tasks to probe for token-\nlevel properties are explored by Blevins et al.\n(2018); Peters et al. (2018b), where they test\nwhether token embeddings from different pre-\ntraining schemes encode part-of-speech and con-\nstituent structure.\nTenney et al. (2018) extend token-level prob-\ning to span-level probing and consider a broader\nrange of tasks. Our work is different from them in\nthe following ways – (1) We probe for biomedical\ndomain-speciﬁc contextualized embeddings and\ncompare them to the general-domain embeddings;\n(2) For NER, instead of classifying the tag for a\ngiven span, we adopt an end-to-end setting where\nthe spans must also be identiﬁed. This allows us\nto compare the probing results to state-of-the-art\nnumbers; (3) We also probe for relational infor-\nmation using the NLI task in an end-to-end style.\n3 Methods\n3.1 Biomedical Contextual Embeddings\nBioELMo: We train BioELMo on the PubMed\ncorpus. PubMed provides access to MEDLINE,\na database containing more than 24M biomedical\ncitations3. We used 10M recent abstracts ( 2.46B\ntokens) from PubMed to train BioELMo. The\nstatistics of this corpus are very different from\nmore general domains. For example, the tokenpa-\ntients ranks 22 by frequency in the PubMed cor-\npus while it ranks 824 in the 1B Word Bench-\nmark dataset (Chelba et al., 2013). We use the\nTensorﬂow implementation 4 of ELMo to train\nBioELMo. We keep the default hyperparameters\nand it takes about 1.7K GPU hours to train 8\nepochs. BioELMo achieves an averaged forward\nand backward perplexity of 31.37 on test set.\nBioBERT: In parallel to our work, Lee et al.\n(2019) developed BioBERT, which is pre-trained\non English Wikipedia, BooksCorpus and ﬁne-\ntuned on PubMed (7.8B tokens in total). BioBERT\nwas initialized with BERT and further trained on\nPubMed for 200K steps.5\nTo get ﬁxed features of tokens, we use the learnt\ndownstream task-speciﬁc layer weights to calcu-\nlate the average of 3 layers (1 token embedding\nlayer and 2 biLSTM layers) for BioELMo and\n13 layers (1 token embedding layer and 12 trans-\nformer layers) for BioBERT. As ﬁxed feature ex-\ntractors, BioELMo and BioBERT are not ﬁne-\ntuned by downstream tasks.\n3.2 Downstream Tasks\nWe ﬁrst use BioELMo with state-of-the-art mod-\nels and ﬁne-tune BioBERT on the downstream\ntasks, to test their full capacity. In §3.3 we intro-\nduce our probing setup which tests BioBERT and\n3https://www.ncbi.nlm.nih.gov/pubmed/\n4https://github.com/allenai/bilm-tf\n5We note there is a difference in the size of training\ncorpora for BioBERT and BioELMo, but since we trained\nBioELMo before BioBERT was available, we could not con-\ntrol for this difference.\nPremise: He returned to the clinic three weeks later and was \nprescribed with antibiotics. \nHypothesis: The patient has an infection. \nLabel: Entailment \nto treat \nFigure 1: Relation information in a MedNLI instance.\nBioELMo as ﬁxed feature extractors.\nNER: For BioELMo, following Lample et al.\n(2016), we use the contextualized embeddings and\na character-based CNN for word representations,\nwhich are fed to a biLSTM, followed by a condi-\ntional random ﬁeld (CRF) (Lafferty et al., 2001)\nlayer for tagging. For BioBERT, we use the single\nsentence tagging setting described in Devlin et al.\n(2018), where the ﬁnal hidden states of each token\nare trained to classify its NER label.\nNLI: For BioELMo, We use the ESIM model\n(Chen et al., 2016), which encodes the premise and\nhypothesis using biLSTM. The encodings are fed\nto a local inference layer with attention, another\nbiLSTM layer and a pooling layer followed by\nsoftmax for classiﬁcation. For BioBERT, we use\nthe sentence pair classiﬁcation setting described in\nDevlin et al. (2018), where the ﬁnal hidden states\nof the ﬁrst token (special ‘[CLS]’) are trained to\nclassify the NLI label for the sentence pair.\n3.3 Probing Tasks\nWe design two probing tasks where the contextu-\nalized embeddings are only used as ﬁxed feature\nextractors and restrict the down-stream models to\nbe non-contextual, to investigate the information\nintrinsically carried by them. One task is on NER\nto probe for entity-type information, and the other\nis on NLI to probe for relational information.\nNER Probing Task: As shown in Figure\n2 (left), we embed the input tokens to R =\n[E1; E2; ...; EL] ∈ RL×De , where L is the se-\nquence length and De is embedding size. The em-\nbeddings are fed to several feed-forward layers:\n˜Ei = FFN(Ei) ∈ RT\nwhere T is the number of tags. [˜E1; ˜E2; ...; ˜EL]\nis then fed to a CRF output layer. CRF doesn’t\nmodel the context but ensures the global consis-\ntency across the assigned labels, so it’s compatible\nwith our probing task setting.\nNLI Probing Task:Relational information be-\ntween tokens of premises and hypotheses is vital to\nsolve MedNLI task: as shown in Figure 1, the hy-\npothesis is an entailment because antibiotics are\nused to treat an infection, which is a drug-disease\nrelation. We design the task shown in Figure 2\n(right) to probe such relational information: We\nembed the premise and hypothesis seperately to\nP ∈ RL1×De and H ∈ RL2×De , where L1, L2 are\nsequence lengths. Then we use bilinear layers 6 to\nget S = [S1; S2; ...; SR] ∈ RR×L1×L2 where\nSr = PWrHT ∈ RL1×L2 ,\nand Wr ∈ RDe×De is the weight matrix of a bi-\nlinear layer. Note that each element of Sr encodes\nthe interaction between a token from the premise\nand a token from the hypothesis. We denote\nhij =\n[\nS1[i, j] ... SR[i, j]\n]T ∈ RR, (1)\nas the distributed relation representation be-\ntween token i in premise and token j in hypoth-\nesis, and R is the tunable dimension of it. We then\napply an element-wise maximum pooling layer:\n˜h = max\ni,j\nhij ∈ RR.\nWe use a linear layer to compute the softmax\nlogits of the NLI labels, e.g. p(entailment) ∝\nexp(˜hT went), where went is the learnt weight\nvector corresponding to the entailment label.\nFor BERT, we probe two variants. The ﬁrst,\ndenoted as BERT / BioBERT, feeds the premise\nand hypothesis to the model separately. The sec-\nond, denoted as BERT-tog / BioBERT-tog, con-\ncatenates the two sentences by the ‘[SEP]’ token\nand feeds to the model together to get the embed-\ndings. This is how BERT is supposed to be used\nfor sentence pair classiﬁcation tasks, but it’s not\ncomparable to ELMo in our setting since ELMo\ndoesn’t take two sentences together as input.\n4 Experiments\n4.1 Experimental Setup\nData: For the NER task, we use the BC2GM\ndataset. BC2GM stands for BioCreative II gene\nmention dataset (Smith et al., 2008). The task is\nto detect gene names in sentences. It contains 15k\ntraining and 5k test sentences. We also test on the\ngeneral-domain CoNLL 2003 NER dataset (Tjong\nKim Sang and De Meulder, 2003), where the task\nis to detect entities such as person and location.\nFor the NLI task, we use the MedNLI dataset\n(Romanov and Shivade, 2018), where the task is,\ngiven a pair of sentences (premise and hypothesis),\nto predict whether the relation of entailment, con-\ntradiction, or neutral (no relation) holds between\n6We also tried models without bilinear layers, which turn\nout to be suboptimal.\nContextualized Embeddings\nFFN CRF\nNER Probing Model\nE1\nFFNE2\nFFNEL. . .. . .\nCtx. Embeddings‘EKG’\nE1\nBilinearELE2 1\nCtx. Embeddings\nHypothesis\nE1 ELE2 2\nMax PoolingLinearSoftmax\nNLI Probing Model\n]R. . . . . .\n‘Serum’‘C3’. . . ‘.’ ‘shows’‘.’. . . ‘The’‘patient’‘.’. . .Premise\np p p h h h\nO B-GENE. . . O Entailment\nFigure 2: Left: NER probing task. The contextual word representations are directly used to predict the NER labels,\nfollowed by a CRF layer to ensure label consistency. Right: NLI probing task. Bilinear operators map pairs of\nword representations to relation representations which are used to predict the entailment label.\nthem. The premises are sampled from doctors’\nnotes in the clinical dataset MIMIC-III (Johnson\net al., 2016). The hypotheses and annotations are\ngenerated by clinicians. It contains 11,232 train-\ning, 1,395 development and 1,422 test instances.\nWe also test on the general-domain SNLI dataset\n(Bowman et al., 2015), where the premises and hy-\npotheses are drawn from image captions.\nCompared Settings:For each dataset, the Whole\nsetting refers to the state-of-the-art model we used\n(described in §3.2), including contextual modeling\nlayers or ﬁne-tuning of the embedding encoder.\nProbing and Control settings describe the prob-\ning task model introduced in§3.3. The control set-\nting tests the representations on a general-domain\ndataset/task, to check whether we lose any infor-\nmation in domain-speciﬁc embeddings. Probing\nand control results are averaged over three seeds.\nCompared Embeddings: We compare: (1) non-\ncontextual biomedical w2v trained on a biomedi-\ncal corpus of 5.5B tokens (Moen and Ananiadou,\n2013), (2) ELMo trained on a general-domain cor-\npus of 5.5B tokens 7, (3) BioELMo 8, (4) Cased\nbase version of BERT trained on a general-domain\ncorpus of 3.3B tokens9 and (5) BioBERT10.\n4.2 Main Results\n4.2.1 NER Results\nIn Domain v.s. General Domain: Results in\nTable 1 show that BioBERT and BioELMo in\n7https://allennlp.org/elmo\n8Though BioELMo uses the smallest corpus to train, it\nperforms better than BioBERT in probing setting, and general\nELMo in whole and probing setting.\n9https://github.com/google-research/\nbert\n10https://github.com/dmis-lab/biobert\nMethod F1 (%)\nWhole Probe Ctrl.\nAndo (2007) 87.2 – –\nRei et al. (2016) 88.0 – –\nSheikhshab et al. (2018) 89.7 – –\nBiomed w2v 84.9 78.5 67.5\nGeneral ELMo 87.0 82.9 84.0\nGeneral BERT 89.2 84.9 83.6\nBioELMo 90.3 88.4 80.9\nBioBERT 90.6 88.2 83.4\nTable 1: NER test results. Whole: whole model per-\nformance on BC2GM; Probe: Probing task perfor-\nmance on BC2GM; Ctrl.: Probing task performance\non CoNLL 2003 NER. We use the ofﬁcial evaluation\ncodes to calculate the F1 scores where there are multi-\nple ground-truth tags, so the F1 scores are much higher\nthan what were reported in Lee et al. (2019).\nthe Whole setting perform better than the general\nBERT and ELMo and biomed w2v, setting new\nstate-of-the-art performance for this dataset.\nBioBERT and BioELMo remains competitive\nin the Probing setting, doing much better than\ntheir general domain counterparts and even gen-\neral ELMo in the Whole setting. This shows that\nwith the right pre-training, the downstream model\ncan be considerably simpliﬁed.\nUnsurprisingly, in the Control setting BioBERT\nand BioELMo do worse than their general coun-\nterparts, indicating that the gains come at the cost\nof losing some general-domain information. How-\never, the performance gaps (absolute differences)\nbetween ELMo and BioELMo are larger in the\nbiomedical domain than it is in the general do-\nmain, which is also true for BERT and BioBERT.\nFor ELMo and BioELMo, we believe it is because\nthe PubMed corpus contains many mentions of\ngeneral-domain entities whereas the reverse is not\ntrue. Because BioBERT is initialzied with BERT\nand also uses general-domain corpora like Enligsh\nWikiPedia for pre-training, it’s not surprising that\nBioBERT is just 0.2 worse than BERT on CoNLL\n2003 NER in control setting.\nBioELMo v.s. BioBERT: Fine-tuned BioBERT\noutperforms BioELMo with biLSTM and CRF\non BC2GM. As a feature extractor, BioBERT is\nslightly worse than BioELMo in probing task of\nBC2GM, but outperforms BioELMo in probing\ntask of CoNLL 2003, which can be explained\nby the fact that BioBERT is also pre-trained on\ngeneral-domain corpora.\nMethod Accuracy (%)\nWhole Probe Ctrl.\nRomanov and Shivade (2018) 76.6 – –\nBiomed w2v 74.2 71.1 59.2\nGeneral ELMo 75.8 69.6 60.8\nGeneral BERT – 67.6 62.1\nGeneral BERT-tog 77.8 71.0 74.1\nBioELMo 78.2 75.5 58.3\nBioBERT – 70.1 58.8\nBioBERT-tog 81.7 73.8 69.9\nTable 2: NLI test results. Whole: whole model perfor-\nmance on MedNLI; Probe: Probing task performance\non MedNLI; Ctrl.: Probing task performance on SNLI.\nTo make the results comparable, we only use the same\nnumber of SNLI training instances as that of MedNLI.\n4.2.2 NLI Results\nIn Domain v.s. General Domain:Table 2 shows\nthat BioBERT and BioELMo in the Whole setting\nperform better than their general domain counter-\nparts and biomedical w2v for NLI, setting state-\nof-the-art performance for this dataset as well.\nOnce again, we observe that BioBERT and\nBioELMo outperform their general domain coun-\nterparts in the Probing settings, which comes at\nthe cost of losing general domain information as\nindicated in the Control setting results.\nNote that the Probing taskonly models relation-\nships between tokens, but we still see competitive\naccuracy in that setting ( 75.5% vs 76.6% previ-\nous best). This suggests that, (i) many instances in\nMedNLI can be solved by identifying token-level\nrelationships between the premise and the hypoth-\nesis, and (ii) BioELMo already captures this kind\nof information in its embeddings.\nBioELMo v.s. BioBERT: Fine-tuned BioBERT\ndoes much better than BioELMo with ESIM\nmodel. However, BioELMo performs better than\nBioBERT by a large margin in the probing task of\nMedNLI. We explore this in more detail in the next\nsection. Again, BioBERT is better than BioELMo\nin probing task of SNLI because it’s also pre-\ntrained on general corpora.\nWe notice that the -tog setting improves the\nBERT performance. Encoding two sentences sep-\narately, BioELMo still outperforms BioBERT-tog.\nIt suggests that BioELMo is a better feature extrac-\ntor than BioBERT, even though the latter has su-\nperior performance when ﬁne-tuned on MedNLI.\n4.3 Analysis\n4.3.1 Entity-type Information\nIn biomedical literature, the acronymER has mul-\ntiple meanings: out of the 124 mentions we found\nin 20K recent PubMed abstracts, 47 refer to the\ngene “estrogen receptor”, 70 refer to the organelle\n“endoplasmic reticulum” and4 refer to the “emer-\ngency room” in hospital. We use t-SNE (Maaten\nand Hinton, 2008) to visualize different contextu-\nalized embeddings of these mentions in Figure 3.\nIn Domain v.s. General Domain: For general\nELMo, by far the strongest signal separating the\nmentions is whether they appear inside or outside\nparentheses. This is not surprising given the re-\ncurrent nature of LSTM and language modeling\ntraining objective for learning these embeddings.\nBioELMo does a better job of grouping mentions\nof the same entity (ER as estrogen receptor) to-\ngether, which is clearly helpful for the NER task.\nER mentions of the same entities cluster bet-\nter by BioBERT than general BERT: there are two\nmajor clusters corresponding to estrogen receptor\nand endoplasmic reticulum by BioBERT as indi-\ncated by the dashed circles, while entities of dif-\nferent types are scattered almost evenly by BERT.\nBioELMo v.s. BioBERT: Clearly BioELMo\nbetter clusters entities from the same types to-\ngether. Unlike ELMo/BioELMo, Whether the\nER mention is inside parentheses doesn’t affect\nBERT/BioBERT representations. It can be ex-\nplained by encoder difference between ELMo and\nBERT: For ELMo, to predict ‘)’ in forward LM,\nrepresentations of token ‘ER’ inside the parenthe-\nses need to encode parentheses information due to\nthe recurrent nature of LSTM. For BERT, to pre-\ndict ‘)’ in masked LM, the masked token can at-\ntend to ‘(’ without interacting with ‘ER’ represen-\ntations, so BERT ‘ER’ embedding does’t need to\nencode parentheses information.\nFigure 3: t-SNE visualizations of the token ER embeddings in different contexts by BioELMo, general ELMo,\nBioBERT and general BERT.  and Krepresent ER mentions within and outside of parentheses, respectively.\nColors refer to different actual meanings of the ER mention.\nRelation Type NN w/ Representation of Same Type (%)\nBioELMo ELMo BioBERT-tog BioBERT BERT-tog BERT Biomed w2v\ndisease-symptom 54.2 52.1 44.5 38.8 34.2 37.0 40.9\ndisease-drug 32.8 34.4 26.1 17.9 27.7 22.6 23.6\nnumber-indication 70.5 63.9 47.0 45.3 48.1 49.5 74.4\nsynonyms 63.6 56.4 60.8 55.8 56.4 52.8 51.7\nAll 57.5 53.3 47.1 42.1 43.3 42.5 49.5\nSubset Accuracy (%) 73.9 62.8 71.4 65.0 65.8 64.5 69.7\nTable 3: Average proportion of nearest neighbor (NN) representations that belong to the same type for different\nembeddings, averaged over three random seeds. Biomed w2v performs best for number-indication relations, prob-\nably because it uses a vocabulary of over 5M tokens, in which about 100k are numbers. Subset accuracy denotes\nthe probing task performance in the subset of MedNLI test set used for this analysis.\n4.3.2 Relational Information\nWe manually examine all test instances with the\n“entailment” label in MedNLI, and found 78 token\npairs across the premises and hypotheses which\nstrongly suggest entailment. Among them, 22 are\ndisease-symptom pairs, 13 are disease-drug pairs,\n19 are numbers and their indications (e.g.: 150/93\nand hypertension) and 24 are synonyms or closely\nrelated concepts (e.g.: Lasix R⃝ and diuretic). Fig-\nure 1 shows an example of disease-drug relation-\nship. We hypothesize that a model is required\nto encode relation information to perform well in\nMedNLI task. We evaluate relation representa-\ntions from different embeddings by nearest neigh-\nbor (NN) analysis: For each distributed relation\nrepresentation (Eq. 1) of these token pairs, we cal-\nculated the proportions of its ﬁve nearest neigh-\nbors that belong to the same relation type. We re-\nport the average proportions in table 3 and use it\nas a metric to measure the effectiveness of repre-\nsenting relations by different embedding schemes.\nWe also show model performance for this sub-\nset (78 instances for relation analysis) in table 3.\nThe trends of subset accuracy moderately corre-\nlate with the NN proportions (Pearson correlation\ncoefﬁcient r = 0.52).\nIn Domain v.s. General Domain: For all re-\nlations, BioELMo is signiﬁcantly 11 better than\nELMo in representing same relations closer to\neach other, while there is no signiﬁcant differ-\nence between BioBERT and BERT. This indicates\nthat even pre-trained by in-domain corpus, as ﬁxed\nfeature extractor, BioBERT still cannot effectively\nencode biomedical relations compared to BERT.\nBioELMo v.s. BioBERT: BioELMo signiﬁ-\ncantly outperforms BioBERT and even BioBERT-\ntog for all relations. This explains why BioELMo\ndoes better than BioBERT in the probing task:\nBioELMo better represents vital biomedical rela-\ntions between tokens in premises and hypotheses.\n5 Conclusion\nWe have shown that BioELMo and BioBERT rep-\nresentations are highly effective on biomedical\nNER and NLI, and BioELMo works even without\ncomplicated downstream models and outperforms\nuntuned BioBERT in our probing tasks. This ef-\nfectiveness comes from its ability as a ﬁxed fea-\nture extractor to encode entity types and especially\ntheir relations, and hence we believe they should\n11Signiﬁcance is deﬁned as p <0.05 in two-proportion z\ntest.\nbeneﬁt any task which requires such information.\nA long-term goal of NLP is to learn univer-\nsal text representations. Our probing tasks can\nbe used to test whether learnt representations ef-\nfectively encode entity-type or relational informa-\ntion. Moreover, comprehensive characterizations\nof BioELMo and BioBERT as ﬁxed feature extrac-\ntors would also be an interesting further direction\nto explore.\n6 Acknowledgement\nWe are grateful for the anonymous reviewers who\ngave us very insightful suggestions. Bhuwan\nDhingra is supported by a grant from Google.\nReferences\nRie Kubota Ando. 2007. Biocreative ii gene men-\ntion tagging system at ibm watson. In Proceed-\nings of the Second BioCreative Challenge Evalua-\ntion Workshop, volume 23, pages 101–103. Centro\nNacional de Investigaciones Oncologicas (CNIO)\nMadrid, Spain.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer.\n2018. Deep rnns encode soft hierarchical syntax.\narXiv preprint arXiv:1805.04218.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326.\nKhyathi Chandu, Aakanksha Naik, Aditya Chan-\ndrasekar, Zi Yang, Niloy Gupta, and Eric Nyberg.\n2017. Tackling biomedical text summarization:\nOaqa at bioasq 5b. BioNLP 2017, pages 58–66.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei,\nHui Jiang, and Diana Inkpen. 2016. Enhanced\nlstm for natural language inference. arXiv preprint\narXiv:1609.06038.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. arXiv preprint\narXiv:1705.02364.\nIshita Dasgupta, Demi Guo, Andreas Stuhlm ¨uller,\nSamuel J Gershman, and Noah D Goodman. 2018.\nEvaluating compositionality in sentence embed-\ndings. arXiv preprint arXiv:1802.04302.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016.\nMimic-iii, a freely accessible critical care database.\nScientiﬁc data, 3:160035.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\narXiv preprint arXiv:1603.01360.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2019. Biobert: pre-trained biomed-\nical language representation model for biomedical\ntext mining. arXiv preprint arXiv:1901.08746.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579–2605.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nSPFGH Moen and Tapio Salakoski2 Sophia Anani-\nadou. 2013. Distributional semantics resources for\nbiomedical text processing. In Proceedings of the\n5th International Symposium on Languages in Biol-\nogy and Medicine, Tokyo, Japan, pages 39–43.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nMatthew E Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\narXiv preprint arXiv:1808.08949.\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J Ed-\nward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. 2018. Collecting diverse nat-\nural language inference problems for sentence rep-\nresentation evaluation. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 67–81.\nMarek Rei, Gamal KO Crichton, and Sampo Pyysalo.\n2016. Attending to characters in neural sequence\nlabeling models. arXiv preprint arXiv:1611.04361.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clin-\nical domain. arXiv preprint arXiv:1808.06752.\nGolnar Sheikhshab, Inanc Birol, and Anoop Sarkar.\n2018. In-domain context-aware token embeddings\nimprove biomedical named entity recognition. In\nProceedings of the Ninth International Workshop on\nHealth Text Mining and Information Analysis, pages\n160–164.\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan\nHsu, Yu-Shi Lin, Roman Klinger, Christoph M\nFriedrich, Kuzman Ganchev, et al. 2008. Overview\nof biocreative ii gene mention recognition. Genome\nbiology, 9(2):S2.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2018. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the seventh conference on Natural\nlanguage learning at HLT-NAACL 2003-Volume 4 ,\npages 142–147. Association for Computational Lin-\nguistics.\nGeorg Wiese, Dirk Weissenborn, and Mariana\nNeves. 2017. Neural domain adaptation for\nbiomedical question answering. arXiv preprint\narXiv:1706.03610.\nHenghui Zhu, Ioannis Ch Paschalidis, and Amir\nTahmasebi. 2018. Clinical concept extraction\nwith contextual word embedding. arXiv preprint\narXiv:1810.10566.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8497741222381592
    },
    {
      "name": "Extractor",
      "score": 0.6782909631729126
    },
    {
      "name": "Natural language processing",
      "score": 0.6038752198219299
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5886847972869873
    },
    {
      "name": "Task (project management)",
      "score": 0.5870854258537292
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5783323645591736
    },
    {
      "name": "Sequence labeling",
      "score": 0.5520418882369995
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5402403473854065
    },
    {
      "name": "Language model",
      "score": 0.5148383975028992
    },
    {
      "name": "Word (group theory)",
      "score": 0.5044280290603638
    },
    {
      "name": "Named-entity recognition",
      "score": 0.46934401988983154
    },
    {
      "name": "Visualization",
      "score": 0.4439528286457062
    },
    {
      "name": "Question answering",
      "score": 0.4168699085712433
    },
    {
      "name": "Linguistics",
      "score": 0.068398118019104
    },
    {
      "name": "Mathematics",
      "score": 0.06087607145309448
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Process engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}