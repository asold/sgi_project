{
    "title": "Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts",
    "url": "https://openalex.org/W4291902044",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2889559957",
            "name": "Babak Hemmatian",
            "affiliations": [
                "John Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A219710970",
            "name": "Lav R. Varshney",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4288253152",
        "https://openalex.org/W2971671202"
    ],
    "abstract": "Recent work demonstrates a bias in the GPT-3 model towards generating violent text completions when prompted about Muslims, compared with Christians and Hindus. Two pre-registered replication attempts, one exact and one approximate, found only the weakest bias in the more recent Instruct Series version of GPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent completions were observed. Additional pre-registered experiments, however, showed that using common names associated with the religions in prompts yields a highly significant increase in violent completions, also revealing a stronger second-order bias against Muslims. Names of Muslim celebrities from non-violent domains resulted in relatively fewer violent completions, suggesting that access to individualized information can steer the model away from using stereotypes. Nonetheless, content analysis revealed religion-specific violent themes containing highly offensive ideas regardless of prompt format. Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.",
    "full_text": null
}