{
    "title": "Probing Linguistic Information for Logical Inference in Pre-trained Language Models",
    "url": "https://openalex.org/W4200635637",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2103717749",
            "name": "Zeming Chen",
            "affiliations": [
                "Rose–Hulman Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2162948331",
            "name": "Qiyue Gao",
            "affiliations": [
                "Rose–Hulman Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2103717749",
            "name": "Zeming Chen",
            "affiliations": [
                "Rose–Hulman Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3096970692",
        "https://openalex.org/W6770537058",
        "https://openalex.org/W6794923208",
        "https://openalex.org/W3168665686",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3092665232",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W2980536612",
        "https://openalex.org/W6719819555",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2250790822",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W3015449890",
        "https://openalex.org/W3006881356",
        "https://openalex.org/W2908854766",
        "https://openalex.org/W2607892599",
        "https://openalex.org/W2798330556",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962813243",
        "https://openalex.org/W3156469806",
        "https://openalex.org/W3097977265",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3183138634",
        "https://openalex.org/W3037191812",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3035305735",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3175508917",
        "https://openalex.org/W2998557616",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W3120726277"
    ],
    "abstract": "Progress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.",
    "full_text": "Probing Linguistic Information\nFor Logical Inference In Pre-trained Language Models\nZeming Chen Qiyue Gao\nRose-Hulman Institute of Technology\n{chenz16, gaoq}@rose-hulman.edu\nAbstract\nProgress in pre-trained language models has led to a\nsurge of impressive results on downstream tasks for nat-\nural language understanding. Recent work on probing\npre-trained language models uncovered a wide range\nof linguistic properties encoded in their contextualized\nrepresentations. However, it is unclear whether they en-\ncode semantic knowledge that is crucial to symbolic in-\nference methods. We propose a methodology for prob-\ning linguistic information for logical inference in pre-\ntrained language model representations. Our probing\ndatasets cover a list of linguistic phenomena required\nby major symbolic inference systems. We ﬁnd that (i)\npre-trained language models do encode several types of\nlinguistic information for inference, but there are also\nsome types of information that are weakly encoded, (ii)\nlanguage models can effectively learn missing linguis-\ntic information through ﬁne-tuning. Overall, our ﬁnd-\nings provide insights into which aspects of linguistic\ninformation for logical inference do language models\nand their pre-training procedures capture. Moreover, we\nhave demonstrated language models’ potential as se-\nmantic and background knowledge bases for supporting\nsymbolic inference methods.\nIntroduction\nPre-trained language models have replaced traditional\nsymbolic-based natural language processing systems on a\nvariety of language understanding tasks, mainly because\nsymbolic-based NLP systems often rely on linguistic prop-\nerties as features. Those features are hard to acquire. Many\ntypes of linguistic information are either hand-written rules\nor background knowledge extracted from traditional knowl-\nedge base, which make symbolic-based systems hard to\nscale up on large benchmarks such as GLUE (Wang et al.\n2018). On the other hand, many recent probing studies have\nrevealed that sentence representations of pre-trained lan-\nguage models encode a large amount of linguistic informa-\ntion and background knowledge (Tenney et al. 2019; Petroni\net al. 2019; Bouraoui, Camacho-Collados, and Schockaert\n2020). However, it remains unknown if these representations\nalso encode implicit linguistic information for inference cru-\ncial to symbolic inference systems.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Given pre-trained language models, the probing\nclassiﬁer extracts linguistic infromation for a given probing\ntask. The amount of intimation is measured by the probing\naccuracy and the information gain, compared with baselines.\nIn this paper, we propose an inference information prob-\ning framework (Figure 1). We deﬁne a set of probing tasks\nthat focus on different types of linguistic information re-\nquired by symbolic systems. In particular, we cover lin-\nguistic information for simple and complex semantic phe-\nnomena. Simple semantic phenomena often relies on par-\ntial or no context and does not require advanced linguistic\nskills like contextual understanding and reasoning. Our sim-\nple phenomena include word-to-word semantic relations,\nlexical semantics, and contradiction signatures. Complex\nphenomena depends on multiple types of reasoning skills\nlike reasoning on event context, monotonicity, coreference,\nand commonsense knowledge. For complex phenomena, we\nprobe sentiment, relational knowledge, anaphora resolution,\nand monotonicity reasoning. We are interested in answer-\ning two questions: (1) Do pre-trained language models en-\ncode linguistic information essential to symbolic inference\nsystems? (2) Do pre-trained language models acquire new\nlinguistic information for inference during the ﬁne-tuning\nprocess for the NLI task? For each task, we conducted prob-\ning experiments on multiple contextualized language mod-\nels and compared results to several strong baselines.\nOur analysis shows that language models encode diverse\ntypes of linguistic information for inference. In particu-\nlar, they encode more information on simple semantic phe-\nnomena than complex semantic phenomena. Our label-wise\nqualitative analysis revealed that the amount of information\nencoded by language models for each task is different across\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10509\nlabels which justiﬁes our previous ﬁndings. Moreover, we\nfound that pre-trained language models can obtain some\ntypes of the missing linguistic information through ﬁne-\ntuning for the NLI task. Overall, our ﬁndings show that pre-\ntrained language models can be potential linguistic knowl-\nedge bases supporting symbolic inference systems.\nContributions Our contributions are as follows:\n1. Our work expands on prior probing studies by studying\na wider range of linguistic information, including simple\nand complex semantic phenomena.\n2. Our experiments allow classiﬁer expressiveness to be an-\nalyzed in a more complex setting covering syntactic and\nsemantic linguistic properties beyond prior works.\n3. Our study provides insights into what types of new lin-\nguistic information pre-trained language models obtain\nduring ﬁne-tuning on large NLI datasets. This contributes\nto the interpretability of NLI models.\nRelated Work\nRecent studies have reported the existence of linguistic prop-\nerties encoded in the self-attention weights of language\nmodels’ contextualized representations. These linguistic\nproperties include syntactic structure, semantic knowl-\nedge, and some world knowledge (Rogers, Kovaleva, and\nRumshisky 2020). Several studies train and evaluate a prob-\ning classiﬁer on top of different language models’ contex-\ntualized representations to explore the existence of informa-\ntion about linguistic properties. These studies have shown\nthat pre-trained language models encode some levels of syn-\ntactic and semantic knowledge. Hewitt and Manning (2019)\nrecovered syntactic dependencies from BERT’s embeddings\nby learning transformation matrices. Tenney et al. (2019),\nwhich is more directly related to our work, proposed the\nedge probing framework and found that contextualized em-\nbeddings encode information about named entity types, re-\nlations, semantic roles and proto roles based on the high ac-\ncuracy of the probing classiﬁer.\nSome probing studies focus on inducing factual knowl-\nedge captured in pre-trained language models. A major-\nity of the studies rely on the Masked Language Modeling\n(MLM) component of the model which can be adapted to\ninduce knowledge easily, since the model only needs to ﬁll\nin the blanks. Petroni et al. (2019) showed that pre-trained\nBERT encodes relational knowledge competitive with those\nthat are accessed from knowledge bases using traditional\nNLP methods. They also found that BERT has strong ability\nto recall factual knowledge prior to any ﬁne-tuning, mak-\ning it a good candidate for open-domain QA systems (un-\nsupervised). Bouraoui, Camacho-Collados, and Schockaert\n(2020) proposed a method to induce relations from pre-\ntrained language models. They ﬁrst found potential sen-\ntences that express a relation in a large corpus. A subset\nof sentences was used as templates. They then ﬁne-tuned\na language model to predict whether a given word pair form\nsome relation. They found strong evidence that relations can\nbe obtained from language models.\nCompared to existing work, we extend common syntac-\ntic and semantic tasks to a range of tasks that focus on more\ncomplex linguistic phenomenons. Some of our tasks, such as\nsemantic graph construction and monotonicity polarization,\nrequire both syntactic and semantic information. Probing\nfor more complex linguistic tasks allows us to diagnose the\nparticular advantages of language models over conventional\nNLP systems. It also allows us to study the expressiveness\nof probing classiﬁers in a more complex setting beyond syn-\ntactic tasks. Moreover, our experiments on ﬁne-tuned NLI\nlanguage models provides insights into the type of linguistic\ninformation they capture through ﬁne-tuning.\nProbing Methodology\nEdge Probing and Vertex Probing\nEdge probing is a simple and useful probing framework\nproposed by Tenney et al. (2019). It can provide a uni-\nform set of metrics and architectures across diverse task\ntypes. Formally, a sentence is deﬁned as a list of tokens\n[t0; t1; t2; :::; tn] and an edge target as {s1; s2; L}where s1\nand s2 are two end exclusive spans with s1 = {is1; js1) and\ns2 = {is2; js2). Lis a label assigned to the pair of spans\nwhich the classiﬁer needs to accurately predict. The label\nset for Lis different across tasks including both binary la-\nbels and multi labels. Each sentence [t0; t1; t2; :::; tn] is en-\ncoded by a language model into contextualized sentence rep-\nresentation [e 0; e1; e2; :::; en]. A projection layer concate-\nnated with a self-attention pooling operator will be applied\nto the representation to extract span representations accord-\ning to the index position of two spans s1 = {is1; js1) and s2\n= {is2; js2). As Tenney et al. (2019) mentioned, the pool-\ning is ﬁxed-length and only operates within the bounds of a\nspan to ensure that the classiﬁer can only access information\nof the rest of the sentence from the contextual sentence rep-\nresentation. The two span representations are concatenated\nand passed to the classiﬁer to predict the label. To ensure\nwe only probe a pre-trained language model without mod-\nifying its parameters, we freeze its parameters to not allow\nfor gradient updates.\nThe vertex probing framework has the same settings and\nformulations as the edge probing framework, except that\nvertex probing operates on every token in a sentence. The\nclassiﬁer receives only a single span representation as in-\nput. Formally, the deﬁnition is very similar to that of the se-\nquence tagging task. With a list of tokens [t 0; t1; t2; :::; tn],\nwe deﬁne each token as a single span targets = {(is; js), L}.\nThe vertex probing is used to predict which words belong to\na category in the label set.\nClassiﬁer Selection\nSelecting a good probing classiﬁer is essential to the prob-\ning process. We ﬁrst choose the linear classiﬁer. According\nto Hewitt and Liang (2019), the linear classiﬁer is less ex-\npressive and thus is prevented from memorizing the task.\nHowever, Pimentel et al. (2020) uses probing to estimate\nthe mutual information between a representation-valued and\na linguistic-property–valued random variable. They argue\nthat the most optimal probe should be used to minimize the\nchance of misinterpreting a representation’s encoded infor-\nmation, and therefore achieve the optimal estimate of mu-\n10510\nTask Probe Type Example Split # S\nSemantic- edge probing A tall boy is running quickly to catch asoccer ball . train 10000\nGraph [boy] [running] −→Concept-Relation; [tall] [boy] −→Concept-Modiﬁer; test 5000\n(SemGraph) [quickly] [running] −→Relation-Modiﬁer\nMonotonicity vertex probing Some ↑ people↑ in↑ the↑ White= House= does↑ not↑ know↓ train 5000\nif↓ any↓ dog↓ in↓ Ohio↓ ate↓ bananas↓ yesterday↓ test 500\nLexical edge probing P: The [man]s1 is holding a[saxophone]s2\n(SA-Lex) H: The man is holding an[instrument]s3 . train 1000\n(s1, s3) −→Unaligned; (s2, s3) −→Aligned test 500\nAnaphora edge probing The [technician]s1 told the [customer]s2 that [he]s3 could pay with cash. train 500\n(SA-AP) (s1, s3) −→Unaligned; (s2, s3) −→Aligned test 220\nSentiment vertex probing P: When asked about the restaurant, Brielle said, “ [It]t1 [was]t2 [terrible!]t3\n(SA-ST) [I]t4 [found]t5 [this]t6 [product]t7 [to]t8 [be]t9 [way]t10 [too]t11 [big]t12 ” train 1000\nH: Brielle [did]t13 [not]t14 [like]t15 [the]t16 [restaurant]t17 test 600\n{t1, . . ., t3}−→Align1; {t4, . . ., t12}−→Unaligned; {t13, . . ., t17}−→Align2\nRelational- vertex probing P: [Dirk]t1 [Nowitski]t2 [is]t3 [a]t4 [current]t5 [NBA]t6 [star]t7 playing with the train 1000\nKnowledge [Dallas]t8 [Mavericks]t9 [as]t10 [an]t11 [all-purpose]t12 [forward]t13 test 500\n(SA-RK) H: [Dirk]14\nt [Nowitski]15\nt [plays]16\nt [in]17\nt [the]18\nt [NBA]19\nt\n{t1, . . ., t7}−→Align1; {t8, . . ., t13}−→Unaligned; {t14, . . ., t19}−→Align2\nContradiction vertex probing P: Italy and Germany [have]t1 [each]t2 [played]t3 [twice]t4 , train 1000\nSignature and they [haven’t]t5 [beaten]t6 [anybody]t7 [yet]t8 test 500\n(ContraSig) H: Italy [defeats]t9 [Germany]t10\n{t1, . . ., t4}−→None1; {t5, . . ., t8}−→Contra-sig1; {t9, t10}−→Contra-sig2\nFigure 2: Here we list examples of our probing tasks. For semantic graph construction task, the red, green and blue boxes\ndenote modiﬁers, concepts and relations respectively. For semantic alignment tasks and the contradiction signature detection\ntask, the red boxes are spans that are semantically aligned. The green boxes are spans that form a contradiction signature. The\nblue boxes are spans that are irrelevant to semantic alignment or contradiction. Here P stands for a premise, and H stands for a\nhypothesis. The anaphora-based alignment only uses a single sentence. For the labels, (s 1; s3) −→Aligned means that s1 and\ns3 are aligned. {t1, : : :, t7}−→Align1 means that tokens t1 to t7 belongs to the ﬁrst phrase in a semantically aligned pair.\ntual information. To lessen the chance of misinterpretation,\nwe conducted probing using a Multi-layer Perceptron (MLP)\nclassiﬁer with one hidden layer.\nExperiment Setup\nTo answer both questions 1 and 2 in the introduction, we ex-\nperiment with ﬁve pre-trained language models. We selected\nBERT-base and BERT-large (Devlin et al. 2019), RoBERTa-\nbase and RoBERTa-large (Liu et al. 2019), and DeBERTa\n(He et al. 2021). All ﬁve models can provide contextualized\nsentence representations and have shown impressive perfor-\nmance on the GLUE (Wang et al. 2018) benchmark. Our\nexperiment setup follows three types of evaluation methods\nto interpret the probing performance.\nProbing Accuracy We probe pre-trained language mod-\nels and the baseline word embeddings using linear and MLP\nclassiﬁers. Then, we compare their performance to deter-\nmine if the pre-trained models improve over the baselines.\nIf such improvement is signiﬁcant, the pre-trained models\ncontain more information about a task than the baseline.\nOtherwise, they do not contain enough information to ben-\neﬁt a task. We select four uncontextualized word embed-\ndings as our baselines, including random embedding, Fast-\nText (Joulin et al. 2017), Glove (Pennington, Socher, and\nManning 2014), and Word2Vec (Mikolov et al. 2013). We\nalso conduct a label-wise qualitative analysis for each task\nto explore if the amount of information is encoded differ-\nently across the task-speciﬁc labels. Finally, to determine\nif language models can learn the missing linguistic infor-\nmation for inference, we evaluate NLI models ﬁne-tuned\non MultiNLI (Williams, Nangia, and Bowman 2018), using\nprobing tasks that do not beneﬁt from the pre-trained mod-\nels.\nControl Task Hewitt and Liang (2019) argue that accu-\nracy cannot fully validate that a representation encodes lin-\nguistic information since a highly expressive classiﬁer could\n10511\nhave memorized the task. They proposed the use of control\ntasks to complement probings. The main idea is to validate\nif a classiﬁer could predict task outputs independently of\na representation’s linguistic properties. If the classiﬁer can\nachieve high accuracy in this setting, the accuracy does not\nnecessarily reﬂect the properties of the representation. A\ncontrol task has the same input as the associated linguistic\ntask, but it assigns random labels to the input data. The se-\nlectivity, which is the difference between the linguistic task\naccuracy and the control task accuracy, is used to measure\nthe quality of a probe. A good probe should have high selec-\ntivity meaning that it has low control task accuracy but high\nlinguistic task accuracy.\nInformation-theoretic Probing Pimentel et al. (2020) ar-\ngue that the task of supervised probing is an attempt to mea-\nsure how much information a neural representation can pro-\nvide for a task. They operationalized probing as estimating\nthe mutual information between a representation and a prob-\ning task. The mutual information from a target representa-\ntion is compared to the information estimated from a con-\ntrol function’s representation, which serves as a baseline for\ncomparison. In our experiments, we use uncontextualized\nbaselines as control functions. We estimate the information\ngain between a contextualized embedding and a baseline. In-\nformation gain measures how much more information about\na task a contextualized embedding has over a baseline. In ad-\ndition, we transform the gain into a percentage measurement\nto make the results more interpretable.\nInference Information Probes\nIn this section, we introduce a list of edge and vertex prob-\ning tasks for probing implicit linguistic information for sym-\nbolic inference methods in pre-trained language model rep-\nresentations. To discover potential tasks that can provide es-\nsential linguistic information for symbolic inferences, we\nstudied four major logical systems for NLI, all with high ac-\ncuracy on SICK (Marelli et al. 2014), and several challenge\ndatasets for the NLI task. They include NLI systems based\non natural logic (Abzianidze 2020), monotonicity reasoning\n(Hu et al. 2020; Chen, Gao, and Moss 2021), and theorem\nproving (Yanaka et al. 2018).\nSemantic Graph Construction (SemGraph)\nThis task probes the graph-based abstract meaning represen-\ntation for sentences, a type of knowledge found effective in\nsymbolic systems for acquiring paraphrase pairs and select-\ning correct inference steps (Yanaka et al. 2018; Chen, Gao,\nand Moss 2021). The task is to construct a semantic graph\nthat captures connections between concepts, modiﬁers, and\nrelations in a sentence. Relations are words that form a con-\nnection, including verbs and prepositions. Concepts are ar-\nguments connected by a relation such as objects and sub-\njects. Each concept connects to a set of modiﬁers that at-\ntribute to it. An example semantic graph is shown in Ta-\nble 2. We deﬁne this as an edge probing task and assign a\nlabel to a pair of tokens. A label is selected from the la-\nbel set: concept-to-relation, concept-to-modiﬁer, relation-to-\nconcept, relation-to-modiﬁer, relation-to-relation, modiﬁer-\nto-relation, modiﬁer-to-concept. To construct the dataset, we\nuse dependency parsing and semantic role labeling tools to\nidentify concepts, modiﬁers, and relations in a sentence and\nthe connection between them. We selected premises from\nthe SNLI test set as our inputs and split them into training\nand testing sets.\nSemantic Alignment (SA)\nThis set of tasks probes the linguistic information for infer-\nence involving semantically aligned phrase or word pairs.\nThese aligned pairs can often serve as an explanation of\nthe entailment gold-label (Abzianidze 2020; Chen, Gao, and\nMoss 2021). We cover a wide range of semantic phenom-\nena common in natural language inference including lexical\n(SA-Lex), anaphora (SA-AP), sentiment (SA-ST), and rela-\ntional knowledge (SA-RK). Table 2 lists each type of seman-\ntic alignment with associated examples. Probing data are\nﬁrst collected from multiple challenge datasets for NLU and\nthen are manually annotated for the edge and vertex prob-\ning framework. For the sentiment task, we noticed that the\naligned phrases are always part of a person’s saying, lead-\ning a model to solve the task quickly by memorization. To\navoid this, we concatenated each premise with speech frag-\nments from another randomly selected premise to build a\nmore complex premise. For instance, in the example in Ta-\nble 2, I found this product to be way too big is a speech\nfragment from another premise sample.\nWe formulate each task as either an edge probing or a ver-\ntex probing task during annotation. For edge probing tasks,\nwe assign either Aligned or Unaligned to a pair of spans.\nFor example, in the Lexical example in Table 2, (s 2: [sax-\nophone], s3: [instrument]) are aligned, and (s 1: [man], s3:\n[instrument]) are unaligned. In a vertex probing task, we la-\nbel a token as either Aligned1 (the token belongs to the ﬁrst\nphrase of the aligned pair), Aligned2 (the token belongs to\nthe second phrase of the aligned pair), or Unaligned (the\ntoken is not in any aligned phrases). For example, in the\nrelational-knowledge example in Table 2, {Dirk,Nowitski,\nis, a, current, NBA, star}are tokens in the ﬁrst phrase of\nthe aligned pair, {Dirk, Nowitski, plays, in, the, NBA}are\ntokens in the second phrase of the aligned pair, and{Dallas,\nMavericks, as, an, all-purpose, forward}are unaligned to-\nkens. We apply edge probing to tasks with single word spans\n(lexical and anaphora) and vertex probing to tasks involving\nmulti-word spans (sentiment and relational knowledge). In\ngeneral, vertex probing adds more distractors into the task\nto increase the difﬁculty level, ensuring that the models are\nusing the correct types of reasoning when making a decision.\nContradiction Signature (ContraSig)\nBeing able to reason about contradictions between a pair of\nsentences is a fundamental requirement of Natural Language\nInference. To determine a contradictory relationship, sys-\ntems often rely on contradiction signatures, or possible ra-\ntionales of contradiction in the sentence pair. Contradiction\nsignature detection tests for both syntax and fundamental se-\nmantics. We deﬁne this task as vertex probing and manu-\nally annotated one dataset for detecting contradiction in text\n(Marelli et al. 2014) by labeling tokens in the ﬁrst phrase of\n10512\nModel SemGraph ContraSig Monot SA-Lex SA-ST SA-AP SA-RK\nGroup 1: Baselines (direct probing, no ﬁne-tuning)\nRandom 68.5 29.7 48.8 45.5 46.8 48.9 45.7\nWord2Vec 68.8 42.4 43.4 59.7 31.4 32.3 42.3\nGlove 71.3 40.2 41.3 60.6 33.4 35.8 50.5\nFastText 69.4 31.7 51.9 50.1 51.3 51.3 52.2\nGroup 2: Language Models (Linear classiﬁer probing, no ﬁne-tuning; selectivity is shown in parenthesis)\nBERT-base 91.8 (42.0) 58.5 (35.1) 48.5 (35.8) 57.4 (7.30) 56.3 (34.7) 49.8 (0.1) 62.8 (50.3)\nBERT-large 88.9 (39.5) 51.6 (37.9) 44.9 (37.4) 55.5 (4.80) 51.2 (33.1) 50.2 (0.1) 62.1 (49.5)\nRoBERTa-base 88.9 (39.2) 52.2 (33.0) 42.8 (42.3) 61.4 (11.3) 47.5 (38.6) 49.4 (0.6) 62.3 (49.7)\nRoBERTa-large 89.6 (40.0) 48.4 (27.6) 38.1 (43.0) 66.2 (16.2) 46.1 (36.6) 49.8 (0.3) 62.9 (50.1)\nDeBERTa 93.4 (43.7) 78.5 (32.3) 54.9 (42.2) 83.8 (33.2) 42.8 (35.7) 51.8 (2.3) 65.6 (52.8)\nGroup 3: Language Models (MLP classiﬁer probing, no ﬁne-tuning; selectivity is shown in parenthesis)\nBERT-base 90.7 (40.7) 91.6 (31.7) 58.1 (41.3) 89.0 (38.8) 67.1 (34.3) 61.5 (17.6) 70.9 (54.7)\nBERT-large 89.3 (38.0) 91.0 (33.0) 57.4 (34.6) 88.8 (38.4) 67.4 (35.5) 77.6 (31.3) 69.0 (53.1)\nRoBERTa-base 91.5 (39.5) 92.5 (27.8) 50.1 (49.1) 87.2 (36.6) 66.1 (37.3) 87.0 (38.6) 70.9 (52.9)\nRoBERTa-large 90.1 (40.6) 91.9 (35.5) 55.4 (46.8) 88.9 (37.2) 65.7 (38.2) 88.6 (39.3) 70.2 (49.1)\nDeBERTa 92.3 (42.2) 92.9 (32.5) 65.7 (58.6) 91.0 (41.5) 63.0 (36.9) 85.2 (37.3) 72.9 (58.2)\nTable 1: This table lists results from the probing and ﬁne-tuning experiments We include in group 1 probing accuracy (%) from\ntwo baselines. Group 2 and 3 shows probing accuracy (%) of the linear classiﬁer (group 2) and the MLP classiﬁer (group 3).\nFor each probe, we also record the selectivity score (%) from control tasks.\na contradiction signature as Contra-sig1, tokens in the sec-\nond phrase of a contradiction signature as Contra-sig2, and\nirrelevant tokens as None. Table 2 shows an example, with\n{have, each, played, twice}as irrelevant tokens, {defeats,\nGermany}as tokens in the ﬁrst phrase of the contradiction\nsignature, and {haven’t,beaten, anybody, yet}as tokens in\nthe second phrase of the contradiction signature.\nMonotonicity Polarization (Monot)\nMonotonicity information supports word-replacement-\nbased logical inferences that NLI systems can use. For\neach token, we assign a monotonicity mark that is either\nMonotone (↑), Antitone (↓), or None (=). To construct\nour dataset, we annotated monotonicity information on\nall sentences in the MED dataset (Yanaka et al. 2019) as\ntraining examples using a monotonicity annotation tool\ncalled Udep2Mono (Chen and Gao 2021). For testing, we\nextended a challenging gold-label monotonicity dataset\nused by Udep2Mono that includes multiple levels of\nmonotonicity changes from different quantiﬁers and logical\noperators. For each sentence, we replicate ten sentences\nfollowing the same syntactic format. Vertex probing is used\nfor monotonicity polarization because a model must predict\nevery token’s monotonicity information.\nExperiment Results and Findings\nDo LMs encode information for inference?\nHere we evaluate the degree to which pre-trained language\nmodels encode implicit information of linguistic properties\nthat is essential to logical inference systems. We conducted\nprobes on the ﬁve pre-trained language models. Table 1\nshows results from the probing experiments.\nSemantic Graph Construction With the linear classiﬁer,\nall language models achieve high probing accuracy that out-\nperforms the baselines. Together, with high selectivity, this\nis strong evidence that the information in these models’\nrepresentations allows the classiﬁer to recover the connec-\ntives of concepts, relations, and modiﬁers. Interestingly, the\nMLP classiﬁer did not improve on the linear classiﬁer sig-\nniﬁcantly, suggesting that the information is easy to inter-\npret. The performance here is consistent with language mod-\nels’ good performance on dependency parsing, semantic role\nlabeling, and part-of-speech tagging (Tenney et al. 2019)\nwhich are related to semantic graph construction.\nSemantic Alignment We observe that on lexical and\nanaphora based alignments (SA-Lex, SA-AP), language\nmodels show high probing accuracy that improves over the\nbaselines signiﬁcantly when using the MLP Classiﬁer. This\nis evidence that language models encode linguistic informa-\ntion of these two types of semantic alignment since they\nalso show high selectivity. Language models do not improve\nover the baselines when using the linear classiﬁer, suggest-\ning that these types of linguistic information might be hard\nto interpret. Language models only improved trivially over\nthe baselines for alignments involving sentiment and rela-\ntional knowledge (SA-ST, SA-RK). The insigniﬁcant im-\nprovement suggests that models weakly capture the informa-\ntion on these complex semantic alignment. Overall, the trend\nis that pre-trained models tend to show poor performance\non complex semantic alignment that requires understanding\nand reasoning. This behavior is consistent with Tenney et al.\n(2019)’s ﬁnding that language models only encode a small\namount of information on semantic reasoning.\nContradiction Signature and Monotonicity For the task\non contradiction signature detection, all language models\nshow poor performance with linear classiﬁer except De-\nBERTa, which has relatively high accuracy (78.5%), vali-\ndating that information on contradiction signatures is more\naccessible from DeBERTa than the other four models. After\nusing the MLP classiﬁer, all models’ accuracy increased sig-\n10513\nniﬁcantly (above 90%) while maintaining very high selec-\ntivity. We attribute this partly to the fact that many contra-\ndictions are simple morphological negation and antonyms,\nwhich can be largely detected by using lexical semantics and\nsyntax. The high accuracy is thus strong evidence that lan-\nguage models do encode a good amount of information on\nsyntax and lexical semantics. For the monotonicity polariza-\ntion task, language models show low accuracy with both the\nlinear classiﬁer and the MLP classiﬁer. This suggests that\nthese language models may not encode much monotonicity\ninformation that can support the polarization. Again, the re-\nsults here show that pre-trained models encode more infor-\nmation on simple semantic phenomena (contradiction signa-\nture) than complex ones (monotonicity).\nLabel-wise Qualitative Analysis\nTo further understand the amount of linguistic information\nthat pre-trained language models capture, we analyze label-\nwise probing quality for each task. Label-wise accuracy per\ntask are shown in ﬁgure 3. We ﬁrst observe that on some\ntasks (SA-Lex, SA-AP, ContraSig, SemGraph), pre-trained\nlanguage models show high and balanced accuracy across\nlabels. These behaviors are strong evidence that these mod-\nels encode rich information on simple semantic phenomena.\nOn the semantic graph construction task, the accuracy dis-\ntribution is similar across models. The relatively low accu-\nracy on modiﬁer-to-relation (m-r) and modiﬁer-to-concept\n(m-c) show the incompleteness of information in language\nmodels that support the linking of modiﬁers to words being\nmodiﬁed. Language models seem to encode information for\nlinking concepts to corresponding words since the accuracy\nis consistently high. Note that although anaphora (SA-AP)\nis a complex phenomena, models show decent performance\nwhich validates the ﬁnding from the previous experiment.\nFor other complex semantic alignment (SA-ST, SA-RK),\nthe heatmaps show highly imbalanced label-wise accuracy.\nThe models have higher accuracy on words in the hypothe-\nsis of an aligned pair than words in the premise, which has a\nmuch more complicated context. Since vertex-probing needs\nto locate phrases in a premise contributing to an entailment,\nthe low accuracy on predicting span locations in a premise\nsuggests that language models only encode very little lin-\nguistic information on complex semantic phenomena. For\nmonotonicity polarization, the accuracy on each label is very\ndifferent. Across language models, the accuracy on mono-\ntone polarity is higher than that on antitone and neutral po-\nlarity. This is consistent with other ﬁndings from other prob-\ning studies on monotonicity reasoning. (Yanaka et al. 2019;\nGeiger, Richardson, and Potts 2020). The results from this\nanalysis validates our main ﬁnding from the previous exper-\niment: models tend to encode less information on complex\nsemantic phenomena.\nDiscussions\nInformation-Theoretic Probing We conducted addi-\ntional experiments on information-theoretical probing to\nvalidate our ﬁndings based on probing accuracy. Recall that\nhere we want to estimate the information gain between a\npre-trained representation and a baseline representation. We\nFigure 3: Plots here shows label-wise accuracy across mod-\nels for each inference information probing task. Here LM1-\n5 stands for the ﬁve language models in order (BERT-base,\nBERT-large, RoBERTa-base, RoBERTa-large, DeBERTa).\nfollowed Pimentel et al. (2020)’s practice by using a more\npowerful probing classiﬁer (MLP). We compared each lan-\nguage model to the best-performed baseline embedding. As\ntable 2 shows, pre-trained language models on average en-\ncode more than 50% more information on the eight probing\ntasks. Overall, pre-trained language models encode more in-\nformation than baseline embedding consistently across all\ntasks. Among these, the highest information gain of lan-\nguage models is on lexical alignment (more than 100% of\nincrease). This is surprising since baseline word embeddings\nare a representation of word semantics. We hypothesize that\nthis is due to the proximity of words that contradicts each\nother in the embedding space. Based on the results, we con-\nclude that pre-trained language models encode signiﬁcantly\nmore information on linguistic information of logical infer-\nence than conventional word embeddings.\nClassiﬁer Expressiveness Some of our ﬁndings contra-\ndict several statements made by Hewitt and Liang (2019)\nregarding classiﬁer expressiveness. First, they claim that one\nshould choose a less expressive classiﬁer over a highly ex-\n10514\nModel SemGraph\nContraSig Monot SA-Lex\nLM1 0.10 (6%)\n1.16 (65%) 0.82 (58%) 1.16 (116%)\nLM2 0.08 (5%) 1.15 (64%) 0.8 (56%) 1.14 (114%)\nLM3 0.10 (6%)\n1.16 (65%) 0.85 (60%) 1.14 (114%)\nLM4 0.09 (5%) 1.14 (64%) 0.9 (63%) 1.15 (115%)\nLM5 0.11 (7%)\n1.16 (65%) 0.78 (55%) 1.16 (116%)\nSA-ST SA-AP\nSA-RK Avg\nLM1 0.39 (39%)\n1.07 (54%) 0.74 (80%) 0.78 (59%)\nLM2 0.41 (41%) 1.02 (51%) 0.74 (80%) 0.76 (59%)\nLM3 0.74 (74%)\n1.04 (52%) 0.80 (86%) 0.83 (65%)\nLM4 0.62 (62%) 1.05 (53%) 0.78 (84%) 0.82 (64%)\nLM5 0.67 (67%)\n1.02 (51%) 0.67 (72%) 0.79 (62%)\nTable 2: Results on infromation-theoretic probing. Here we\nshow the amount of extra information each language model\nshares, compared to the best baseline word embedding. The\npercentage of information gain is listed in the parenthesis.\nModel Monot\nSA-ST SA-RK\nLM1 60.9 (\nN 2.8) 79.8 (N 12.7) 81.4 (N 14.2)\nLM2 60.2 (N 2.8) 77.6 (N 10.2) 80.5 (N 14.6)\nLM3 59.4 (\nN 9.3) 78.0 (N 11.9) 88.0 (N 22.7)\nLM4 58.0 (N 2.6) 76.1 (N 10.4) 89.7 (N 27.7)\nLM5 68.7 (\nN 3.0) 78.9 (N 14.9) 89.6 (N 19.1)\nTable 3: Probing accuracy (%) of ﬁne-tuned NLI models on\nprobing tasks that did not beneﬁt from pre-trained models.\npressive one (linear over MLP) since the prior one has higher\nselectivity. However, based on the accuracy, we observe that\nthe linear classiﬁer has worse performance for semantically-\noriented tasks than the MLP classiﬁer. We also found that\nthe MLP classiﬁer can achieve similar selectivity as a lin-\near classiﬁer for these tasks while achieving higher accuracy.\nThese ﬁndings suggest that linear classiﬁers could misinter-\npret semantic information from a representation, which sup-\nports Pimentel et al. (2020)’s arguments on using a more\npowerful classiﬁer. Secondly, they claim that a probing clas-\nsiﬁer with sufﬁcient expressiveness can learn any task on top\nof a lossless representation with enough training examples.\nHowever, we found that even high expressive classiﬁers like\nMLP fail to perform well on tasks with monotonicity and\nhigher-level semantic reasoning.\nCan LMs learn missing information? Here we evalu-\nate whether pre-trained language models can obtain linguis-\ntic information for inference, missing from their pre-trained\nrepresentations, through ﬁne-tuning for the NLI task. We se-\nlect a version ﬁne-tuned on the MultiNLI dataset for each\nlanguage model. We probed these ﬁne-tuned models for\nthree tasks that did not beneﬁt from the pre-trained language\nmodels. Our probing results are shown in Table 3, and we\nonly record probings with the best performance here. We\nobserve that all language models’ ﬁne-tuned representations\nimprove over their pre-trained representations signiﬁcantly\non the sentiment (SA-ST) and relational-knowledge-based\n(SA-RK) semantic alignment. In contrast, they do not im-\nprove the performance on monotonicity polarization (Mono-\ntonicity). The results show that language models can capture\nlinguistic information on some types of semantic reasoning\nbut not on monotonicity. Possible explanations are that these\nmodels could not obtain monotonicity information during\nﬁne-tuning, or the training data of MultiNLI does not con-\ntain enough examples about monotonicity.\nConclusions and Future Work\nWe presented a systematic study on determining if pre-\ntrained language models encode implicit linguistic informa-\ntion essential to symbolic inference methods. For each prob-\ning task, we constructed associating datasets. We then con-\nducted probings on pre-trained language models using both\nlinear and MLP classiﬁers for each task. In general, we ﬁrst\nfound that baseline word embeddings do not contain much\nlinguistic information for inference and contextualized rep-\nresentations of language models encode some levels of lin-\nguistic information. However, they encode more information\non simple semantic phenomena than on complex phenom-\nena. Moreover, we found that linear classiﬁers can predict\ncorrectly under syntactical information but often misinter-\npret information on semantics resulting in low classiﬁcation\naccuracy. Our label-wise qualitative analysis found that the\namount of linguistic information being encoded is different\nacross task-speciﬁc labels. In particular, language models\nencode more linguistic information for one label than other\nlabels. This label-wise information difference again justi-\nﬁes the absence and incompleteness of some linguistic in-\nformation for inference in language models. Furthermore,\nwe found that language models can effectively learn some\ntypes of missing information on complex semantic reason-\ning through ﬁne-tuning for the NLI task. Overall, language\nmodels show potential to serve as knowledge bases of lin-\nguistic information that supports robust symbolic reasoning.\nWe believe that our probing and analysis provide an ad-\nequate picture of whether critical linguistic information for\ninference exists in pre-trained language models. Moreover,\nour probing can inspire future systems on combining neural\nand symbolic NLP methods. For future work, one could con-\nduct further analysis on each type of linguistic information\nin language models by constructing more detailed probing\ndatasets. One could also design logical systems that can ac-\ncess linguistic information from pre-trained language mod-\nels and apply them in the inference process for improved\nperformance on large benchmarks.\nAcknowledgments\nWe thank the anonymous reviewers for their thoughtful and\nconstructive comments. Thanks also to our advisors Lau-\nrence S. Moss and Michael Wollowski for their feedback\non earlier drafts of this work. Special thanks to the Machine\nLearning for Language Group at NYU for their wonderful\nNLP toolkit, JIANT (Phang et al. 2020).\nReferences\nAbzianidze, L. 2020. Learning as Abduction: Trainable\nNatural Logic Theorem Prover for Natural Language Infer-\nence. In Proceedings of the Ninth Joint Conference on Lexi-\ncal and Computational Semantics, 20–31. Barcelona, Spain\n(Online): Association for Computational Linguistics.\n10515\nBouraoui, Z.; Camacho-Collados, J.; and Schockaert, S.\n2020. Inducing Relational Knowledge from BERT. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\n34(05): 7456–7463.\nChen, Z.; and Gao, Q. 2021. Monotonicity Marking\nfrom Universal Dependency Trees. In Proceedings of the\n14th International Conference on Computational Seman-\ntics (IWCS), 121–131. Groningen, The Netherlands (online):\nAssociation for Computational Linguistics.\nChen, Z.; Gao, Q.; and Moss, L. S. 2021. NeuralLog: Natu-\nral Language Inference with Joint Neural and Logical Rea-\nsoning. In Proceedings of *SEM 2021: The Tenth Joint\nConference on Lexical and Computational Semantics, 78–\n88. Online: Association for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nGeiger, A.; Richardson, K.; and Potts, C. 2020. Neural Nat-\nural Language Inference Models Partially Embed Theories\nof Lexical Entailment and Negation. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP, 163–173. Online: Association\nfor Computational Linguistics.\nHe, P.; Liu, X.; Gao, J.; and Chen, W. 2021. DE-\nBERTA: DECODING-ENHANCED BERT WITH DISEN-\nTANGLED ATTENTION. In International Conference on\nLearning Representations.\nHewitt, J.; and Liang, P. 2019. Designing and Interpreting\nProbes with Control Tasks. InProceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP), 2733–2743.\nHong Kong, China: Association for Computational Linguis-\ntics.\nHewitt, J.; and Manning, C. D. 2019. A Structural Probe\nfor Finding Syntax in Word Representations. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\n4129–4138. Minneapolis, Minnesota: Association for Com-\nputational Linguistics.\nHu, H.; Chen, Q.; Richardson, K.; Mukherjee, A.; Moss,\nL. S.; and Kuebler, S. 2020. MonaLog: a Lightweight Sys-\ntem for Natural Language Inference Based on Monotonicity.\nIn Proceedings of the Society for Computation in Linguistics\n2020, 334–344. New York, New York: Association for Com-\nputational Linguistics.\nJoulin, A.; Grave, E.; Bojanowski, P.; and Mikolov, T. 2017.\nBag of Tricks for Efﬁcient Text Classiﬁcation. In Proceed-\nings of the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume 2, Short\nPapers, 427–431. Association for Computational Linguis-\ntics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. ArXiv, abs/1907.11692.\nMarelli, M.; Menini, S.; Baroni, M.; Bentivogli, L.;\nBernardi, R.; and Zamparelli, R. 2014. A SICK cure for the\nevaluation of compositional distributional semantic models.\nIn LREC.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-\nﬁcient Estimation of Word Representations in Vector Space.\nIn Bengio, Y .; and LeCun, Y ., eds.,1st International Confer-\nence on Learning Representations, ICLR 2013, Scottsdale,\nArizona, USA, May 2-4, 2013, Workshop Track Proceedings.\nPennington, J.; Socher, R.; and Manning, C. 2014. GloVe:\nGlobal Vectors for Word Representation. In Proceedings\nof the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 1532–1543. Doha, Qatar:\nAssociation for Computational Linguistics.\nPetroni, F.; Rockt¨aschel, T.; Riedel, S.; Lewis, P.; Bakhtin,\nA.; Wu, Y .; and Miller, A. 2019. Language Models as\nKnowledge Bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 2463–2473. Hong\nKong, China: Association for Computational Linguistics.\nPhang, J.; Yeres, P.; Swanson, J.; Liu, H.; Tenney, I. F.;\nHtut, P. M.; Vania, C.; Wang, A.; and Bowman, S. R. 2020.\njiant 2.0: A software toolkit for research on general-\npurpose text understanding models. http://jiant.info/.\nPimentel, T.; Valvoda, J.; Hall Maudslay, R.; Zmigrod, R.;\nWilliams, A.; and Cotterell, R. 2020. Information-Theoretic\nProbing for Linguistic Structure. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 4609–4622. Online: Association for Computational\nLinguistics.\nRogers, A.; Kovaleva, O.; and Rumshisky, A. 2020. A\nPrimer in BERTology: What We Know About How BERT\nWorks. Transactions of the Association for Computational\nLinguistics, 8: 842–866.\nTenney, I.; Xia, P.; Chen, B.; Wang, A.; Poliak, A.; McCoy,\nR. T.; Kim, N.; Durme, B. V .; Bowman, S. R.; Das, D.; and\nPavlick, E. 2019. What do you learn from context? Prob-\ning for sentence structure in contextualized word represen-\ntations. In 7th International Conference on Learning Rep-\nresentations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2018. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understanding.\nCoRR, abs/1804.07461.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\n10516\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), 1112–1122. New Orleans, Louisiana:\nAssociation for Computational Linguistics.\nYanaka, H.; Mineshima, K.; Bekki, D.; Inui, K.; Sekine, S.;\nAbzianidze, L.; and Bos, J. 2019. Can Neural Networks Un-\nderstand Monotonicity Reasoning? In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP, 31–40. Florence, Italy:\nAssociation for Computational Linguistics.\nYanaka, H.; Mineshima, K.; Mart´ınez-G´omez, P.; and Bekki,\nD. 2018. Acquisition of Phrase Correspondences Using Nat-\nural Deduction Proofs. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), 756–766. New Orleans, Louisiana:\nAssociation for Computational Linguistics.\n10517"
}