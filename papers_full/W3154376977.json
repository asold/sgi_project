{
  "title": "Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models",
  "url": "https://openalex.org/W3154376977",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3132119043",
      "name": "Matteo Alleman",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2030778259",
      "name": "Jonathan Mamou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2434708274",
      "name": "Miguel A Del Rio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106510227",
      "name": "Hanlin Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124530043",
      "name": "Yoon Kim",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2509133120",
      "name": "SueYeon Chung",
      "affiliations": [
        "IBM (United States)",
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W2998925391",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2962813140",
    "https://openalex.org/W2963759780",
    "https://openalex.org/W3028871071",
    "https://openalex.org/W2888519496",
    "https://openalex.org/W1971017968",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2398204197",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2962824887",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2195506630",
    "https://openalex.org/W2946688411",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2995856824",
    "https://openalex.org/W2766572821",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2767204723",
    "https://openalex.org/W2951066214",
    "https://openalex.org/W2136445846",
    "https://openalex.org/W2963804993",
    "https://openalex.org/W2972896975",
    "https://openalex.org/W2286353276",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2163569468",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2891399254",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W3004639598",
    "https://openalex.org/W2971067365"
  ],
  "abstract": "While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models.",
  "full_text": "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 263–276\nBangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics\n263\nSyntactic Perturbations Reveal Representational Correlates of\nHierarchical Phrase Structure in Pretrained Language Models\nMatteo Alleman† Jonathan Mamou‡ Miguel A Del Rio3\nHanlin Tang‡ Yoon Kim⋆,3,• SueYeon Chung†,3,•\n†Columbia University ‡Intel Labs\n⋆MIT-IBM Watson AI 3Massachusetts Institute of Technology\nAbstract\nWhile vector-based language representations\nfrom pretrained language models have set a\nnew standard for many NLP tasks, there is\nnot yet a complete accounting of their inner\nworkings. In particular, it is not entirely clear\nwhat aspects of sentence-level syntax are cap-\ntured by these representations, nor how (if at\nall) they are built along the stacked layers of\nthe network. In this paper, we aim to ad-\ndress such questions with a general class of\ninterventional, input perturbation-based anal-\nyses of representations from pretrained lan-\nguage models. Importing from computational\nand cognitive neuroscience the notion of repre-\nsentational invariance, we perform a series of\nprobes designed to test the sensitivity of these\nrepresentations to several kinds of structure\nin sentences. Each probe involves swapping\nwords in a sentence and comparing the rep-\nresentations from perturbed sentences against\nthe original. We experiment with three differ-\nent perturbations: (1) random permutations of\nn-grams of varying width, to test the scale at\nwhich a representation is sensitive to word po-\nsition; (2) swapping of two spans which do or\ndo not form a syntactic phrase, to test sensi-\ntivity to global phrase structure; and (3) swap-\nping of two adjacent words which do or do\nnot break apart a syntactic phrase, to test sensi-\ntivity to local phrase structure. Results from\nthese probes collectively suggest that Trans-\nformers build sensitivity to larger parts of the\nsentence along their layers, and that hierarchi-\ncal phrase structure plays a role in this pro-\ncess. More broadly, our results also indicate\nthat structured input perturbations widens the\nscope of analyses that can be performed on\noften-opaque deep learning systems, and can\nserve as a complement to existing tools (such\nas supervised linear probes) for interpreting\ncomplex black-box models.1\n1Datasets, extracted features and code will be publicly\navailable upon publication.\n•Correspondence\n1 Introduction\nIt is still unknown how distributed information\nprocessing systems encode and exploit complex\nrelational structures in data, despite their ubiqui-\ntous use in the modern world. The ﬁelds of deep\nlearning (Saxe et al., 2013; Hewitt and Manning,\n2019), neuroscience (Sarafyazd and Jazayeri, 2019;\nStachenfeld et al., 2017), and cognitive science (El-\nman, 1991; Kemp and Tenenbaum, 2008; Tervo\net al., 2016) have given great attention to this ques-\ntion, including a productive focus on the potential\nmodels and their implementations of hierarchical\ntasks, such as predictive maps and graphs. In this\nwork, we provide a generic means of identifying\ninput structures that deep language models use to\n“chunk up” vastly complex data.\nNatural (human) language provides a rich do-\nmain for studying how complex hierarchical struc-\ntures are encoded in information processing sys-\ntems. More so than other domains, human lan-\nguage is unique in that its underlying hierarchy has\nbeen extensively studied and theorized in linguis-\ntics, which provides source of “ground truth” struc-\ntures for stimulus data. Much prior work on charac-\nterizing the types of linguistic information encoded\nin computational models of language such as neural\nnetworks has focused on supervised readout probes,\nwhich train a classiﬁer on top pretrained models to\npredict a particular linguistic label (Belinkov and\nGlass, 2017; Liu et al., 2019a; Tenney et al., 2019).\nIn particular, Hewitt and Manning (2019) apply\nprobes to discover linear subspaces that encode\ntree-distances as distances in the representational\nsubspace, and Kim et al. (2020) show that these dis-\ntances can be used even without any labeled infor-\nmation to induce hierarchical structure. However,\nrecent work has highlighted issues with correlat-\ning supervised probe performance with the amount\nof language structure encoded in such representa-\ntions (Hewitt and Liang, 2019). Another popular\napproach to analyzing deep models is through the\n264\nlens of geometry (Reif et al., 2019; Gigante et al.,\n2019). While geometric interpretations provide sig-\nniﬁcant insights, they present another challenge in\nsummarizing the structure in a quantiﬁable way.\nMore recent techniques such as replica-based mean\nﬁeld manifold analysis method (Chung et al., 2018;\nCohen et al., 2019; Mamou et al., 2020) connects\nrepresentation geometry with linear classiﬁcation\nperformance, but the method is limited to catego-\nrization tasks.\nIn this work, we make use of an experimen-\ntal framework from cognitive science and neuro-\nscience to probe for hierarchical structure in contex-\ntual representations from pretrained Transformer\nmodels (i.e., BERT (Devlin et al., 2018) and its\nvariants). A popular technique in neuroscience in-\nvolves measuring change in the population activity\nin response to controlled, input perturbations (Mol-\nlica et al., 2020; Ding et al., 2016). We apply this\napproach to test the characteristic scale and the\ncomplexity (Fig. 1) of hierarchical phrase struc-\nture encoded deep contextual representations, and\npresent several key ﬁndings:\n1. Representations are distorted by shufﬂing\nsmall n-grams in early layers, while the distor-\ntion caused by shufﬂing large n-grams starts\nto occur in later layers, implying the scale\nof characteristic word length increases from\ninput to downstream layers.\n2. Representational distortion caused by swap-\nping two constituent phrases is smaller than\nwhen the control sequences of the same length\nare swapped, indicating that the BERT repre-\nsentations are sensitive to hierarchical phrase\nstructure.\n3. Representational distortion caused by swap-\nping adjacent words across phrasal bound-\nary is larger than when the swap is within a\nphrasal boundary; furthermore, the amount of\ndistortion increases with the syntactic distance\nbetween the swapped words. The correlation\nbetween distortion and tree distance increases\nacross the layers, suggesting that the character-\nistic complexity of phrasal subtrees increases\nacross the layers.\n4. Early layers pay more attention between syn-\ntactically closer adjacent pairs and deeper lay-\ners pay more attention between syntactically\ndistant adjacent pairs. The attention paid in\neach layer can explain some of the emergent\nsensitivity to phrasal structure across layers.\nOur work demonstrates that interventional tools\nsuch as controlled input perturbations can be useful\nfor analyzing deep networks, adding to the growing,\ninterdisciplinary body of work which proﬁtably\nadapt experimental techniques from cognitive neu-\nroscience and psycholinguistics to analyze compu-\ntational models of language (Futrell et al., 2018;\nWilcox et al., 2019; Futrell et al., 2019; Ettinger,\n2020).\n2 Methods\nEliciting changes in behavioral and neural re-\nsponses through controlled input perturbations is a\ncommon experimental technique in cognitive neu-\nroscience and psycholinguistics (Tsao and Living-\nstone, 2008; Mollica et al., 2020). Inspired by\nthese approaches, we perturb input sentences and\nmeasure the discrepancy between the resulting, per-\nturbed representation against the original. While\nconceptually simple, this approach allows for a tar-\ngeted analysis of internal representations obtained\nfrom different layers of deep models, and can sug-\ngest partial mechanisms by which such models are\nable to encode linguistic structure. We note that\nsentence perturbations have been primarily utilized\nin NLP for representation learning (Hill et al., 2016;\nArtetxe et al., 2018; Lample et al., 2018), data aug-\nmentation (Wang et al., 2018; Andreas, 2020), and\ntesting for model robustness (e.g., against adver-\nsarial examples) (Jia and Liang, 2017; Belinkov\nand Bisk, 2018). A methodological contribution\nof our work is to show that input perturbations can\ncomplement existing tools and widens the scope\nof questions that could be asked of representations\nlearned by deep networks.\n2.1 Sentence perturbations\nIn this work we consider three different types of\nsentence perturbations designed to probe for differ-\nent phenomena.\nn-gram shufﬂing In the n-gram shufﬂing experi-\nments, we randomly shufﬂe the words of a sentence\nin units of n-grams, with nvarying from 1 (i.e., in-\ndividual words) to 7 (see Fig. 2a for an example).\nWhile the number of words which change absolute\nposition is similar for different n, larger nwill bet-\nter preserve the local context (i.e., relative position)\nof more words. Thus, we reason that n-gram swaps\naffect the representations selective to the context\n265\nFigure 1: Do Transformers build complexity along their layers? (a) The representation of a word is a function of its context, and\nthis cartoon illustrates an hypothesis that deeper representations use larger contexts. (b) An example parse tree, illustrating our\nnotion of phrase complexity. (c) Cartoon of the distortion metric, where vectors are the z-scored feature vectors z, and color map\nvectors to words.\nwith size nor higher within the sentence, and that\nlower nwill result in greater distortion in sentence\nrepresentations.\nPhrase swaps The n-gram shufﬂing experiments\nprobe for sensitivity of representations to local con-\ntext without taking into account syntactic structure.\nIn the phrase swap experiments, we perturb a sen-\ntence by swapping two randomly chosen spans. We\nexplore two ways of swapping spans. In the ﬁrst\nsetting, the spans are chosen such that they are\nvalid phrases according to its parse tree. 3 In the\nsecond setting, the spans are chosen that they are\ninvalid phrases. Importantly, in the second, control\nsetting, we ﬁx the length of the spans such that the\nlengths of spans that are chosen to be swapped are\nthe same as in the ﬁrst setting (see Fig. 3a for an\nexample). We hypothesize that swapping invalid\nphrases will result in more distortion than swap-\nping valid phrases, since invalid swaps will result\nin greater denigration of syntactic structure.\nAdjacent word swaps In the adjacent word\nswapping experiments, we swap two adjacent\nwords in a sentence. We again experiment with two\nsettings – in the ﬁrst setting, the swapped words\nstay within the phrase boundary (i.e., the two words\nshare the same parent), while in the second setting,\nthe swapped words cross phrase boundaries. We\nalso perform a more ﬁne-grained analysis where\n3We use constituency parse trees from the English Penn\nTreebank (Marcus et al., 1994).\nwe condition the swaps based on the “syntactic\ndistance” between the swapped words, where syn-\ntactic distance is deﬁned as the distance between\nthe two words in the parse tree (see Fig. 6c). Since\na phrase corresponds to a subtree of the parse tree,\nthis distance also quantiﬁes the number of nested\nphrase boundaries between two adjacent words.\nHere, we expect the amount of distortion to be pos-\nitively correlated with the syntactic distance of the\nwords that are swapped.\n2.2 Contextual representations from\nTransformers\nFor our sentence representation, we focus on the\nTransformer-family of models pretrained on large-\nscale language datasets (BERT and its variants).\nGiven an input word embedding matrixX ∈RT×d\nfor a sentence of length T, the Transformer applies\nself attention over the previous layer’s representa-\ntion to produce a new representation,\nXl = fl([Hl,1,..., Hl,H]), Hl,i = Al,iXl−1Vl,i,\nAl,i = softmax\n((Xl−1Ql,i)(Xl−1Kl,i)⊤\n√dk\n)\n,\n(1)\nwhere fl is an MLP layer, H is the number of\nheads, dH = d\nH is the head embedding dimension,\nand Ql,i,Kl,i,Vl,i ∈Rd×dk are respectively the\nlearned query, key, and value projection matrices\nat layer l for head i. The MLP layer consists of\na residual layer followed by layer normalization\nand a nonlinearity. The 0-th layer representation\n266\nX0 is obtained by adding the position embeddings\nand the segment embeddings to the input token em-\nbeddings X, and passing it through normalization\nlayer.4\nIn this paper, we conduct our distortion analysis\nmainly on the intermediate Transformer represen-\ntations Xl = [ xl,1,..., xl,T ], where xl,t ∈ Rd\nis the contextualized representation for word tat\nlayer l.5 We analyze the trend in distortion as a\nfunction of layer depth lfor the different perturba-\ntions. We also explore the different attention heads\nHl,i ∈RT×dH and the associated attention matrix\nAl,i ∈RT×T to inspect whether certain attention\nheads specialize at encoding syntactic information.\n2.3 Distortion metric\nOur input manipulations allow us to specify the\ndistortion at the input level, and we wish to measure\nthe corresponding distortion in the representation\nspace (Fig. 1). Due to the attention mechanism, a\nsingle vector in an intermediate layer is a function\nof the representations of (potentially all) the other\ntokens in the sentence. Therefore, the information\nabout a particular word might be distributed among\nthe many feature vectors of a sentence, and we wish\nto consider all feature vectors together as a single\nsentence-level representation.\nWe thus represent each sentence as a matrix and\nuse the distance induced by matrix 2-norm. Specif-\nically, let P ∈ {0,1}T×T be the binary matrix\nrepresentation of a permutation that perturbs the\ninput sentence, i.e., ˜X = PX. Further let ˜Xl and\nXl be the corresponding sentence representations\nfor the l-th layer for the perturbed and original sen-\ntences. To ignore uniform shifting and scaling, we\nalso z-score each feature dimension of each layer\n(by subtracting the mean and dividing by the stan-\ndard deviation where these statistics are obtained\nfrom the full Penn Treebank corpus) to give ˜Zl and\nZl. Our distortion metric for layer lis then deﬁned\nas ∥Zl −P−1 ˜Zl∥/\n√\nTd, where ∥·∥ is the matrix\n2-norm (i.e., Frobenius norm).6 Importantly, we in-\n4However, the exact speciﬁcation for the MLP and X0\nmay vary across different pretrained models.\n5BERT uses BPE tokenization (Sennrich et al., 2015),\nwhich means that some words are split into multiple tokens.\nSince we wish to evaluate representations at word-level, if a\nword is split into multiple tokens, its word representation is\ncomputed as the average of all its token representations.\n6There are many possible ways of measuring distortion,\ninduced by different norms. We observed the results to be\nqualitatively similar for different measures, and hence we\nfocus on the Frobenius norm in our main results. We show the\nresults from additional distortion metrics in the A.2\nvert the permutation of the perturbed representation\nto preserve the original ordering, which allows us\nto measure the distortion due to structural change,\nrather than distortion due to simple differences in\nsurface form. We divide by\n√\nTd to make the met-\nric comparable between sentences (with different\nT) and networks (with different d).\nIntuitively, our metric is the scaled Euclidean\ndistance between the z-scored, ﬂattened sentence\nrepresentations, zl ∈RTd . Because each dimen-\nsion is independently centered and standardized,\nthe maximally unstructured distribution of zl is an\nisotropic Td-dimensional Gaussian. The expected\ndistance between two such vectors is approximately√\n2Td. Therefore, we can interpret a distortion\nvalue approaching\n√\n2 as comparable to if we had\nrandomly redrawn the perturbed representation.\n3 Experimental Setup\nWe apply our perturbation-based analysis on sen-\ntences from the English Penn Treebank (Marcus\net al., 1994), where we average the distortion met-\nric across randomly chosen sentences. We analyze\nthe distortion, as measured by length-normalized\nFrobenius norm between the perturbed and orig-\ninal representations, as a function of layer depth.\nLayers that experience large distortion when the\nsyntactic structure is disrupted from the perturba-\ntion can be interpreted as being more sensitive to\nhierarchical syntactic structure.\nAs we found the trend to be largely similar\nacross the different models, in the following sec-\ntion, we primarily discuss results from BERT\n(bert-base-cased). We replicate key re-\nsults with other pretrained and randomly-initialized\nTransformer-based models as well (see A.1).\n4 Results\n4.1 Sensitivity to perturbation size increases\nalong BERT layers\nWhen we shufﬂe in units of larger n-grams, it only\nintroduces distortions in the deeper BERT layers\ncompared to smaller n-gram shufﬂes. The n-gram\nsized shufﬂes break contexts larger than n, while\npreserving contexts of size nor smaller. Interest-\ningly, smaller n-gram shufﬂes diverge from the\noriginal sentence in the early layers (Fig. 2b, top\ncurve), implying that only in early layers are repre-\nsentations built from short-range contexts. Larger\nn-gram shufﬂes remain minimally distorted for\n‘longer’ (Fig. 2b, bottom curve), implying that long-\n267\nFigure 2: Swapping n-grams and phrases. ( a) Examples of\nbasic n-gram shufﬂes, where colors indicate the units of shuf-\nﬂing. ( b) Distortion metric computed at each layer, condi-\ntioned on n-gram size. Error bars hereafter represent stan-\ndard error across 400 examples. ( c) An example parse tree,\nwith phrase boundaries shown as grey brackets, and two low-\norder phrases marked; and examples of a phrasal and control\nswap, with colors corresponding to the phrases marked above.\n(d) Distortion, computed at each layer, using either the full\nsentence, the subsentence of unswapped words, or the sub-\nsentence of swapped words, conditioned on swap type. (e)\nFull-sentence distortion for VP and NP phrase swaps. (f) Par-\ntial linear regression coefﬁcients (see A.4) for pre-trained and\nuntrained BERT models after controlling for swap size.\nrange contexts play a larger role deeper layer repre-\nsentations.\nEffects of phrasal boundaries Since BERT\nseems to build larger contexts along its layers, we\nnow ask whether those contexts are structures of\nsome grammatical signiﬁcance. A basic and im-\nportant syntactic feature is the constituent phrase,\nwhich BERT has previously been shown to repre-\nsented in some fashion (Goldberg, 2019; Kim et al.,\n2020). We applied two targeted probes of phrase\nstructure in the BERT representation, and found\nthat phrasal boundaries are indeed inﬂuential.\nIf we swap just two n-grams, the BERT repre-\nsentations are less affected when phrases are kept\nintact. We show this by swapping only two n-\ngrams per sentence and comparing the distortion\nwhen those n-grams are phrases to when they cross\nphrase boundaries (Fig. 3a), where we control for\nthe length of n-grams that are swapped in both\nsettings. There is less distortion when respect-\ning phrase boundaries, which is evident among\nFigure 3: Syntactic distance affects representational distortion.\n(a) An example of adjacent swaps which do and do not cross\na phrase boundary, with low-order phrases colored. Phrase\nboundaries are drawn in red. (b) Distortion in each layer, but\nconditioned on the tree distance. (c) For each head (column)\nof each layer (row), the (Spearman) rank correlation between\ndistortion and tree distance of the swapped words. Colors are\nsuch that red is positive, blue negative. (d) Rank correlations\nbetween distortion (of the full representation) in the trained\nand untrained BERT models. ( e) Histogram of PMI values,\nfor pairs in the same phrase and not. ( f) Similar to b, but\naveraging all out-of-phrase swaps, and separating pairs above\n(‘high’) or below (‘low’) the median PMI.\nall feature vectors, including those in the position\nof words which did not get swapped (Fig. 2d). The\nglobal contextual information, distributed across\nthe sentence, is affected by the phrase boundary.\nTo see if the role of a phrase impacts its salience,\nwe distinguish between verb phrases (VP) and noun\nphrase (NP) swaps. Swapping VP results in more\ndistortion than swapping NP (Fig. 2e). Since VP\nare in general larger than NP, this effect could in\nprinciple be due simply to the number of words\nbeing swapped. Yet that is not the case: Using a\npartial linear regression (see details in A.4), we\ncan estimate the difference between the VP and NP\ndistortions conditional on any smooth function of\nthe swap size, and doing this reveals that there is\nstill a strong difference in the intermediate layers\n(Fig. 2f).\n4.2 Sensitivity depends on syntactic distance\nof the perturbation\nHaving seen that representations are sensitive to\nphrase boundaries, we next explore whether that\n268\nsensitivity is proportional to the number of phrase\nboundaries that are broken, which is a quantity\nrelated to the phrase hierarchy. Instead of swapping\nentire phrases, we swap two adjacent words and\nanalyze the distortion based on how far apart the\ntwo words are in the constituency tree (Fig. 3a) 7.\nThis analysis varies the distance in the deeper tree\nstructure while keeping the distance in surface form\nconstant (since we always swap adjacent words).\nIf the hierarchical representations are indeed be-\ning gradually built up along the layers of these pre-\ntrained models, we expect distortion to be greater\nfor word swaps that are further apart in tree dis-\ntance. We indeed ﬁnd that there is a larger dis-\ntortion when swapping syntactically distant words\n(Fig. 3b). This distortion grows from earlier to later\nBERT layers. Furthermore, when looking at the\nper-head representations of each layer, we see that\nin deeper layers there are more heads showing a\npositive rank correlation between distortion and\ntree distance (Fig. 3c). In addition to a sensitivity\nto phrase boundaries, deeper BERT layers develop\na sensitivity to the number of boundaries that are\nbroken.\nControlling for co-occurrence Since words in\nthe same phrase may tend to occur together more\noften, co-occurrence is a potential confound when\nassessing the effects of adjacent word swaps. Co-\noccurrence is a simple statistic which does not re-\nquire any notion of grammar to compute – indeed\nit is used to train many non-contextual word em-\nbeddings (e.g., word2vec (Mikolov et al., 2013),\nGloVe (Pennington et al., 2014)). So it is natural\nto ask whether BERT’s resilience to syntactically\ncloser swaps goes beyond simple co-occurrence\nstatistics. For simplicity, let us focus on whether a\nswap occurs within a phrase (tree distance = 2) or\nnot.\nAs an estimate of co-occurrence, we used the\npointwise mutual information (PMI). Speciﬁcally,\nfor two words w and v, the PMI is log p(w,v)\np(w)p(v) ,\nwhich is estimated from the empirical probabili-\nties. We conﬁrm that adjacent words in the same\nphrase do indeed have a second mode at high PMI\n(Fig. 3e). Dividing the swaps into those whose\nwords have high PMI (above the marginal median)\nand low PMI (below it), we can see visually that the\ndifference between within-phrase swaps and out-\nof-phrase swaps persists in both groups (Fig. 3f).\n7Note that for adjacent words, the number of broken phrase\nboundaries equals the tree distance minus two.\nWhen quantitatively accounting for the effect of\nPMI with a partial linear regression (see A.4), there\nremains a signiﬁcant correlation between the break-\ning of a phrase and the subsequent distortion. This\nindicates that the greater distortion for word swaps\nwhich cross phrase boundaries is not simply due to\nsurface co-occurrence statistics.\nRelation to linguistic information Do our input\nperturbations, and the resulting the distortions, re-\nﬂect changes in the encoding of important linguis-\ntic information? One way to address this ques-\ntion, which is popular in computational neuro-\nscience (DiCarlo and Cox, 2007) and more recently\nBERTology (Liu et al., 2019a; Tenney et al., 2019),\nis to see how well a linear classiﬁer trained on a lin-\nguistic task generalizes from the (representations\nof the) unperturbed sentences to the perturbed ones.\nWith supervised probes, we can see how much\nthe representations change with respect to the sub-\nspaces that encode speciﬁc linguistic information.\nSpeciﬁcally, we relate representational distortion\nto three common linguistic tasks of increasing com-\nplexity: part of speech (POS) classiﬁcation; grand-\nparent tag (GP) classiﬁcation (Tenney et al., 2019);\nand a parse tree distance reconstruction (Hewitt\nand Manning, 2019)8. The probe trained on each of\nthese tasks is a generalized linear model, mapping\na datapoint x (i.e. representations from different\nlayers) to a conditional distribution of the labels,\np(y|θT x) (see A.5 for model details). Thus a ready\nmeasure of the effect of each type of swap, for a\nsingle sentence, is log p(y|θT xi) −log p(y|θT ˜xi),\nwhere ˜xi is same datum as xi in the perturbed rep-\nresentation9. Averaging this quantity over all dat-\napoints gives a measure of content-speciﬁc distor-\ntion within a representation, which we will call\n“inference impairment”.\nBased on the three linguistic tasks, the distortion\nwe measure from the adjacent word swaps is more\nstrongly related to more complex information. The\ninverted L shape of Fig. 4a suggests that increas-\ning distortion is only weakly related to impairment\nof POS inference, which is perhaps unsurprising\ngiven that POS tags can be readily predicted from\n8While the original paper predicted dependency tree dis-\ntance, in this paper we instead predict the constituency tree\ndistance.\n9POS- and GP-tag prediction outputs a sequence of la-\nbels for each sentence, while the distance probe outputs the\nconstituency tree distance between each pair of words. Then\nlog p(y|θT xi) is simply the log probability of an individual\nlabel.\n269\nFigure 4: Distortion and inference impairment for increasing linguistic complexity. In each plot, a point is the average (distortion,\n‘impairment’) for a given layer and a given class of word swap distance. Points are connected by lines according to their swap\ntype (i.e. tree distance). The circles are colored according to layer (see right for a legend). Averages are taken over 600 test\nsentences, with one of each swap type per sentence, and both distortion and log-likelihood are computed for every word in the\nsentence.\nlocal context. A deeper syntactic probe, the GP\nclassiﬁer (4b), does show a consistent positive rela-\ntionship, but only for swaps which break a phrase\nboundary (i.e. distance >2). Meanwhile, impair-\nment of the distance probe (4c), which reconstructs\nthe full parse tree, has a consistently positive rela-\ntionship with distortion, whose slope is proportion-\nate to the tree distance of the swap. Thus, when\nspeciﬁcally perturbing the phrasal boundary, the\nrepresentational distortion is related to relatively\nmore complex linguistic information.\n4.3 Sensitivity to perturbations is mediated\nby changes in attention\nIn the transformer architecture, contexts are built\nwith the attention mechanism. Recall that atten-\ntion is a mechanism for allowing input vectors to\ninteract when forming the output, and the ultimate\noutput for a given token is a convex combination\nof the features of all tokens (Eq. 1). It has been\nshown qualitatively that, within a layer, BERT allo-\ncates attention preferentially to words in the same\nphrase (Kim et al., 2020), so if our perturbations af-\nfect inference of phrase structure then the changes\nin attentions could explain our results. Note that it\nis not guaranteed to do so: the BERT features in a\ngiven layer are a function of the attentions and the\n“values” (each token’s feature vector), and both are\naffected by our perturbations. Therefore our last\nset of experiments asks whether attention alone can\nexplain the sensitivity to syntactic distance.\nTo quantify the change in attention weights\nacross the whole sentence, we compute the dis-\ntance between each token’s attention weights in the\nperturbed and unperturbed sentences, and average\nacross all tokens. For token i, its vector of atten-\ntion weights in response to the unperturbed sen-\ntence is ai, and for the perturbed one ˜ai (such that\n∑\nj ai\nj = 1). Since each set of attention weights\nare non-negative and sum to 1 due to softmax, we\nuse the relative entropy 10 as a distance measure.\nThis results in the total change in attention being:\n∆a= 1\nT\nT∑\ni=1\nT∑\nj=1\nai\nj log\nai\nj\n˜ai\nj\nwhich is non-negative and respects the structure\nof the weights. We conﬁrmed that other measures\n(like the cosine similarity) produce results that are\nqualitatively similar.\nFirst, we observe that the changes in the atten-\ntion depend on the layer hierarchy when adjacent\nword swaps break the phrase boundary. Like the\ndistortion, attention changes little or not at all in\nthe early layers, and progressively more in the ﬁnal\nlayers (Fig. 5b). Furthermore, these changes are\nalso positively correlated with syntactic distance\nin most cases, which suggests that representation’s\nsensitivities to syntactic tree distance may primarily\nbe due to changes in attention.\nTo see whether the changes in attention can in\nfact explain representational sensitivity to syntactic\ndistance, we turned to the same partial linear re-\ngression model as before (A.4) to compute the the\ncorrelation between the representation’s distortion\nand the tree distance between the swapped adjacent\nwords, after controlling for changes in attention\n(∆a). The correlations substantially reduced in the\ncontrolled case (Fig. 5c), which suggests that at-\ntention weights contribute to the representational\nsensitivity to syntactic tree distance; but the cor-\nrelations are not eliminated, which suggests that\ndistortions from the previous layer also contribute.\n10Also called the KL divergence.\n270\nFigure 5: Attention changes explain part of the sensitivity\nto tree distance. ( a) An example of the attention matrices\nfor all heads in a single layer (layer 8), given the above sen-\ntence as input. Phrases in the sentence are drawn as blocks\nin the matrix. ( b) The change in attention between the un-\nperturbed and perturbed attention weights, averaged over all\nout-of-phrase swaps. Columns are sorted independently by\ntheir value. (c) The head/layer-wise rank correlations (±95%\nconﬁdence intervals) between distortion and tree distance after\ncontrolling for changes in attention, plotted against the uncon-\ntrolled rank correlations. Being below the diagonal indicates\nthat the relationship between distortion and tree distance is\npartially explained by ∆a.\n5 Discussion and Conclusion\nIn this paper, we used the representational sensi-\ntivity to controlled input perturbation as a probe\nof hierarchical phrasal structure in deep linguistic\nrepresentations. The logic of our probe is the rep-\nresentations which respect phrase structure should\nless sensitive to perturbations which preserve the\nphrasal unit, and more sensitive to those which dis-\nrupt a phrase. We hope that our results demonstrate\nthe versatility and utility of perturbation-based ap-\nproaches to studying deep language models.\nWe showed that BERT and its variants build rep-\nresentations which are sensitive to the phrasal unit,\nas demonstrated by greater invariance to pertur-\nbations preserving phrasal boundaries compared\nto control perturbations which break the phrasal\nboundaries (Fig. 2-5). We also ﬁnd that while the\nrepresentational sensitivity to broken phrase bound-\naries grows across layers, this increase in sensitivity\nis more prominent when the breakage occurs be-\ntween two words that are syntactically distant (i.e.,\nwhen the broken phrase is more complex). Using\nthe same methods to show that changes in atten-\ntion provide a partial explanation for perturbation-\ninduced distortions.\nWhile our distortion metric is a task-agnostic\nmeasure of change in the neural population activity,\nthis may or may not reﬂect changes in the encod-\ning of speciﬁc linguistic information. To relate our\nmetric with speciﬁc kinds of information, we mea-\nsured the change in the performance of supervised\nlinear probes trained on top of the representation\n(Fig. 4). The probe sensitivity measure also bears\na suggestive resemblance to the saliency map anal-\nysis (Simonyan et al., 2014) in machine learning,\nwhich is used to highlight the most output-sensitive\nregions within the input. To draw an analogy with\nthat work, one way of characterizing our results is\nthat phrasal boundaries are regions of high saliency\nin hidden representations and that, in deep layers,\ncomplex phrase boundaries are more salient than\nsimple phrase boundaries. Further exploring the\nuse of supervised probes and our input perturba-\ntions as a tool for layerwise probing of syntactic\nsaliency is a promising direction for future work.\nFinally, several studies (Sinha et al., 2021; Gupta\net al., 2021; Pham et al., 2020), have recently found\nthat masked language models pretrained or ﬁne-\ntuned on sentences that break natural word order\n(e.g. via n-gram shufﬂing) still perform quite well\nacross various tasks, even on supervised probes\nof syntactic phenomena. It would be interesting\nto apply our perturbative analyses on such models\nto see if they exhibit less sensitivity to the experi-\nmental vs. control setups (e.g. n-gram vs. phrase\nswaps). This may indicate that such models do\nnot capture representational correlates of phrase\nstructure in their representations despite their good\nperformance on supervised probing tasks. In such\na case, what tasks would actually require the “lin-\nguistic knowledge” that we are probing for? In\nsimilar vein, applying our perturbative analyses on\nmodels that explicitly incorporate syntax into their\nrepresentations (Sundararaman et al., 2019; Wang\net al.; Zanzotto et al., 2020; Kuncoro et al., 2020)\nmight provide further insights.\nOur method and results suggest many interest-\ning future directions. We hope that this work will\nmotivate: (1) a formal theory of efﬁcient hierarchi-\ncal data representations in distributed features; (2)\na search for the causal connection between atten-\ntion structure, the representational geometry, and\nthe model performance; (3) potential applications\nin network pruning studies; (4) an extension of\nthe current work as a hypothesis generator in neu-\nroscience to understand how neural populations\nimplement tasks with an underlying compositional\nstructure.\n271\nReferences\nJacob Andreas. 2020. Good-Enough Compositional\nData Augmentation. In Proceedings ACL.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised neural ma-\nchine translation. In Proceedings of ICLR.\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In Proceedings of ICLR.\nYonatan Belinkov and James Glass. 2017. Analyz-\ning hidden representations in end-to-end automatic\nspeech recognition systems. In Advances in Neural\nInformation Processing Systems, pages 2441–2451.\nSueYeon Chung, Daniel D Lee, and Haim Sompolin-\nsky. 2018. Classiﬁcation and geometry of gen-\neral perceptual manifolds. Physical Review X ,\n8(3):031003.\nUri Cohen, SueYeon Chung, Daniel D Lee, and Haim\nSompolinsky. 2019. Separability and geometry of\nobject manifolds in deep neural networks. bioRxiv,\npage 644658.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJames J DiCarlo and David D Cox. 2007. Untangling\ninvariant object recognition. Trends in cognitive sci-\nences, 11(8):333–341.\nNai Ding, Lucia Melloni, Hang Zhang, Xing Tian, and\nDavid Poeppel. 2016. Cortical tracking of hierarchi-\ncal linguistic structures in connected speech. Nature\nneuroscience, 19(1):158.\nJeffrey L Elman. 1991. Distributed representations,\nsimple recurrent networks, and grammatical struc-\nture. Machine learning, 7(2-3):195–225.\nAllyson Ettinger. 2020. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nRichard Futrell, Ethan Wilcox, Takashi Morit, and\nRoger Levy. 2018. RNNs as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\narXiv:1809.01329.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nScott Gigante, Adam S Charles, Smita Krishnaswamy,\nand Gal Mishne. 2019. Visualizing the phate of neu-\nral networks. In Advances in Neural Information\nProcessing Systems, pages 1840–1851.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nAshim Gupta, Giorgi Kvernadze, and Vivek Srikumar.\n2021. Bert family eat word salad: Experiments with\ntext understanding. AAAI.\nBruce Hansen. 2000. Econometrics. https:\n//www.ssc.wisc.edu/~bhansen/\neconometrics/.\nJohn Hewitt and Percy Liang. 2019. Designing and\nInterpreting Probes with Control Tasks. In Proceed-\nings of EMNLP.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\n2016. Learning distributed representations of sen-\ntences from unlabelled data. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1367–1377, San\nDiego, California. Association for Computational\nLinguistics.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of EMNLP, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nCharles Kemp and Joshua B Tenenbaum. 2008. The\ndiscovery of structural form. Proceedings of the Na-\ntional Academy of Sciences, 105(31):10687–10692.\nTaeuk Kim, Jihun Choi, Daniel Edmiston, and Sang\ngoo Lee. 2020. Are Pre-trained Language Models\nAware of Phrases? Simple but Strong Baselines for\nGrammar Induction. In Proceedings of ICLR.\nAdhiguna Kuncoro, Lingpeng Kong, Daniel Fried,\nDani Yogatama, Laura Rimell, Chris Dyer, and Phil\nBlunsom. 2020. Syntactic Structure Distillation Pre-\ntraining for Bidirectional Encoders. Transactions\nof the Association for Computational Linguistics,\n8:776–794.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018. Unsupervised ma-\nchine translation using monolingual corpora only. In\nProceedings of ICLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations.\n272\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. CoRR, abs/1903.08855.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nJonathan Mamou, Hang Le, Miguel A Del Rio, Cory\nStephenson, Hanlin Tang, Yoon Kim, and SueYeon\nChung. 2020. Emergence of separable manifolds\nin deep language representations. arXiv preprint\narXiv:2006.01095.\nMitchell Marcus, Grace Kim, Mary Ann\nMarcinkiewicz, Robert MacIntyre, Ann Bies,\nMark Ferguson, Karen Katz, and Britta Schasberger.\n1994. The penn treebank: Annotating predicate\nargument structure. In Proceedings of the Workshop\non Human Language Technology, HLT ’94, pages\n114–119, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nFrancis Mollica, Matthew Siegelman, Evgeniia Di-\nachek, Steven T Piantadosi, Zachary Mineroff,\nRichard Futrell, Hope Kean, Peng Qian, and Evelina\nFedorenko. 2020. Composition is the core driver\nof the language-selective network. Neurobiology of\nLanguage, 1(1):104–134.\nAri Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 31, pages 5732–\n5741. Curran Associates, Inc.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1532–1543.\nThang M. Pham, Trung Bui, Long Mai, and Anh\nNguyen. 2020. Out of order: How important is the\nsequential order of words in a sentence in natural\nlanguage understanding tasks?\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. Svcca: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 30, pages 6076–\n6085. Curran Associates, Inc.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and Measuring the Geometry of\nBERT. In Advances in Neural Information Process-\ning Systems, pages 8592–8600.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nMorteza Sarafyazd and Mehrdad Jazayeri. 2019. Hi-\nerarchical reasoning by neural circuits in the frontal\ncortex. Science, 364(6441):eaav8911.\nAndrew M Saxe, James L McClellans, and Surya Gan-\nguli. 2013. Learning hierarchical categories in deep\nneural networks. In Proceedings of the Annual Meet-\ning of the Cognitive Science Society, volume 35.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2014. Deep inside convolutional networks: Vi-\nsualising image classiﬁcation models and saliency\nmaps.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle.\nKimberly L Stachenfeld, Matthew M Botvinick, and\nSamuel J Gershman. 2017. The hippocampus as a\npredictive map. Nature neuroscience, 20(11):1643.\nDhanasekar Sundararaman, Vivek Subramanian,\nGuoyin Wang, Shijing Si, Dinghan Shen, Dong\nWang, and Lawrence Carin. 2019. Syntax-Infused\nTransformer and BERT models for Machine\nTranslation and Natural Language Understanding.\narXiv:1911.06156.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of ACL.\nD Gowanlock R Tervo, Joshua B Tenenbaum, and\nSamuel J Gershman. 2016. Toward the neural im-\nplementation of structure learning. Current opinion\nin neurobiology, 37:99–105.\nDoris Y Tsao and Margaret S Livingstone. 2008.\nMechanisms of face perception. Annu. Rev. Neu-\nrosci., 31:411–437.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao,\nJiangnan Xia, Liwei Peng, and Luo Si. StructBERT:\nIncorporating Language Structures into Pre-training\nfor Deep Language Understanding.\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\nbig. 2018. SwitchOut: an efﬁcient data augmen-\ntation algorithm for neural machine translation. In\nProceedings of EMNLP.\n273\nEthan Wilcox, Roger Levy, and Richard Futrell. 2019.\nHierarchical representation in neural language mod-\nels: Suppression and recovery of expectations.\narXiv preprint arXiv:1906.04068.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. In Proceedings of\nNeurIPS.\nFabio Massimo Zanzotto, Andrea Santilli, Leonardo\nRanaldi, Dario Onorati, Pierfrancesco Tommasino,\nand Francesca Fallucchi. 2020. KERMIT: Comple-\nmenting transformer architectures with encoders of\nexplicit syntactic interpretations. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nA Appendix\nHere we go into further detail on our methods and\ndata to aid in reproducibility.\nA.1 Model details\nHere we give the details for all models considered\nin this paper. The majority of results are from\nBERT, but we also tested other variants.11\n• BERT (Devlin et al., 2018)\nbert-base-cased. 12-layer, 768-hidden,\n12-heads, 110M parameters.\n• RoBERTa (Liu et al., 2019b)\nroberta-base. 12-layer, 768-hidden,\n12-heads, 125M parameters.\n• ALBERT (Lan et al., 2019)\nalbert-base-v1. 12 repeating lay-\ners, 128 embedding, 768-hidden, 12-heads,\n11M parameters.\n• DistilBERT (Sanh et al., 2019)\ndistilbert-uncased. 6-layer, 768-\nhidden, 12-heads, 66M parameters. The\nmodel distilled from the BERT model\nbert-base-uncased checkpoint.\n• XLNet (Yang et al., 2019)\nxlnet-base-cased. 12-layer, 768-\nhidden, 12-heads, 110M parameters.\nNote that the hidden size is 768 across all the mod-\nels. For each pre-trained model, input text is tok-\nenized using its default tokenizer and features are\nextracted at token level.\n11We use the implementation from https://github.\ncom/huggingface/transformers.\nFigure 6: Replicating the adjacent word swapping experiments\nusing different transformer architectures. Lines are the mean\nFrobenius distance, and the shading is ±1 standard error of\nthe mean.\nA.2 Additional metrics\nIn addition to the scaled Frobenius distance, we\nalso considered other ways of measuring distortion\nin the representation. We will brieﬂy report results\nfor two other metrics, and describe them here.\nCCA Canonical correlations analysis\n(CCA) (Raghu et al., 2017) measures the\nsimilarity of two sets of variables using many sam-\nples from each. Given two sets of random variables\nx = ( x1,x2,...,x n) and y = ( y1,y2,...,y m),\nCCA ﬁnds linear weights a ∈Rn and b ∈Rm\nwhich maximise cov(a ·x,b ·y). In our context,\nwe treat the representation of the original sentence\nas x, and the representation of the perturbed\nsentence as y, and the resulting correlation as a\nsimilarity measure.\nSince CCA requires many samples, we use the\nset of all word-level representations across all per-\nturbed sentences. For example, to construct the\nsamples of x from Sperturbed sentences, we get\nuse [X1|X2|...|XS], where each Xi ∈ R768×Ti.\nUnless speciﬁed otherwise, S = 400 . For good\nestimates, CCA requires many samples (on the or-\nder of at least the number of dimensions), and we\nfacilitate this by ﬁrst reducing the dimension of the\nmatrices using PCA. Using 400 components pre-\nserves ∼90% of the variance. Thus, while CCA\ngives a good principled measure of representational\nsimilarity, its hunger for samples makes it unsuit-\nable as a per-sentence metric.\nWe also measured distortion using Projec-\n274\nFigure 7: Results from the pretrained BERT model using\nalternative distortion metrics, on the n-gram shufﬂing and\nphrase swap experiments.\ntion Weighted Canonical Correlation Analysis\n(PWCCA), an improved version of CCA to esti-\nmate the true correlation between tensors (Morcos\net al., 2018).12\nAs reported in Figure 7, we did not ﬁnd any\nqualitative differences between PWCCA and CCA\nin our experiments.\nCosine A similarity measure deﬁned on individ-\nual sentences is the cosine between the sentence-\nlevel representations. By sentence-level represen-\ntation, we mean the concatenation of the word-\nlevel vectors into a single vector s ∈ RNT\n(where N is the dimension of each feature vec-\ntor). Treating each dimension of the vector as\na sample, we can then deﬁne the following met-\nric: corr\n(\nsoriginal\ni ,sswapped\ni\n)\n. This is equivalent\nto computing the cosine of the vectors after sub-\ntracting the (scalar) mean across dimensions, hence\nwe will refer to it as ‘cosine’.\nA.3 Additional details on the dataset\nIn this section, we describe additional details of the\nmanipulations done on the datasets.\nn-gram shufﬂing For a given a sentence, we\nsplit it into sequential non-overlapping n-gram’s\nfrom left to right; if the length of the sentence is not\na multiple of n, the remaining words form an ad-\nditional m-gram, m<n . The list of the n-gram’s\nis randomly shufﬂed. Note that the 1-gram case\nis equivalent to a random shufﬂing of the words.\n12For both CCA and PWCCA, we use the implementation\nfrom https://github.com/google/svcca.\nIn our analysis, we consider n-grams, with nvary-\ning from 1 (i.e., individual words) to 7 and all the\nsentences have at least 10 words.\nWe provide here an example ofn-gram shufﬂing.\n• Original: The market ’s pessimism reﬂects the\ngloomy outlook in Detroit\n• 1-gram : market pessimism the ’s Detroit in\nThe gloomy reﬂects outlook\n• 2-gram : ’s pessimism in Detroit The market\nreﬂects the gloomy outlook\n• 3-gram : The market ’s gloomy outlook in\npessimism reﬂects the Detroit\n• 4-gram : in Detroit The market ’s pessimism\nreﬂects the gloomy outlook\n• 5-gram : the gloomy outlook in Detroit The\nmarket ’s pessimism reﬂects\n• 6-gram : outlook in Detroit The market ’s\npessimism reﬂects the gloomy\n• 7-gram : in Detroit The market ’s pessimism\nreﬂects the gloomy outlook\nPhrase swaps Using constituency trees from the\nPenn Treebank(Marcus et al., 1994), we deﬁne\nphrases as constituents which don’t contain any\nothers within them. (See Fig. 2c or Fig. 3a in the\nmain text.) Phrase swaps thus consist of swapping\none phrase with another, and leaving other words\nintact.\nTo provide an appropriate control perturbation,\nwe swap two disjoint n-grams, which are the same\nsize as true phrases but cross phrase boundaries.\nAdjacent word swaps To better isolate the ef-\nfect of broken phrase boundaries, we used adja-\ncent word swaps. Adjacent words were chosen\nrandomly, and one swap was performed per sen-\ntence.\nA.4 Partial linear regression\nIn order to control for uninteresting explanations of\nour results, we often make use of a simple method\nfor regressing out confounds. Generally, we want\nto assess the linear relationship between X and Y,\nwhen accounting for the (potentially non-linear)\neffect of another variable Z. In our experiments,\nX is always the swap-induced distortion and Y\nis the swap type, like integer-valued tree distance\n275\nor binary-valued in/out phrase. We wish to allow\nE[Y|Z] and E[X|Z] to be any smooth function of\nZ, which is achieved by the least-squares solution\nto the following partially linear model:\nY ∼βxX+ βz ·f(Z)\nwhere f(z) is a vector of several (we use 10) basis\nfunctions (we used cubic splines with knots at 10\nquantiles) of Z. Both regressions have the same op-\ntimal βx, but the one on the left is computationally\nsimpler (Hansen, 2000). The standard conﬁdence\nintervals on βx apply.\nIntuitively, the βx obtained by the partially lin-\near regression above is related to the conditional\ncorrelation of X and Y given Z: ρ(X,Y |Z). Like\nan unconditonal correlation, it will be zero if X\nand Y are conditionally independent given Z, but\nnot necessarily vice versa(both X and Y must be\nGaussian for the other direction to be true). To\ncompute conditional rank correlations (which as-\nsess a monotonic relationship between X and Y),\nwe rank-transform X and Y (this changes the con-\nﬁdence interval calculations).\nWe apply this method to swap size in Fig. 2\nand attentions in Fig. 5. In these supplemental\nmaterials, we will also report the results when Xis\nthe binary in/out phrase variable, andZis PMI. The\nfull p-values and coefﬁcients of the uncontrolled\nand controlled regressions can be found in Table 1,\nwhere we observe that past layer 2, the p-value on\nphrase boundary is very signiﬁcant (p< 10−12).\nA.5 Supervised probes\nIn this section, we describe the experiments based\non the three linguistic tasks: parts of Speech (POS);\ngrandparent tags (GP); and constituency tree dis-\ntance.\nThe POS and GP classiﬁers were multinomial\nlogistic regressions trained to classify each word’s\nPOS tag (e.g. ‘NNP’, ‘VB’) and the tag of its\ngrandparent in the constituency tree, respectively.\nIf a word has no grandparent, its label is the root\ntoken ‘S’. The probes were optimized with standard\nstochastic gradient descent, 50 sentences from the\nPTB per mini-batch. 10 epochs, at 10−3 learning\nrate, were sufﬁcient to reach convergence.\nThe distance probe is a linear map B applied to\neach word-vector w in the sentence, and trained\nsuch that, for all word pairs i,j, TreeDist(i,j)\nmatches ∥B(wi −wj)∥2\n2 as closely as possible.\nUnlike the classiﬁers, there is freedom in the out-\nput dimension of B; we used 100, although perfor-\nmance and results are empirically the same for any\nchoice greater than ∼64. Our probes are different\nfrom (Hewitt and Manning, 2019) in two ways: (1)\nwe use constituency trees, instead of dependency\ntrees, and (2) instead of an L1 loss function, we use\nthe Poisson (negative) log-likelihood as the loss\nfunction. That is, if λi,j = ∥B(wi −wj)∥2\n2, and\nyi,j = TreeDist(i,j)\n−li,j = yi,j log λi,j −λi,j −log yi,j!\nOtherwise, the probes are trained exactly as in (He-\nwitt and Manning, 2019). Speciﬁcally, we used\nstandard SGD with 20 sentences from the PTB in\neach mini-batch, for 40 epochs.\nEvaluation A linear model is ﬁt to maximize\np(y|θ(x)), with pa probability function (multino-\nmial for classiﬁers, Poisson for distance), and x\ncoming from the unperturbed transformer repre-\nsentation. We evaluate the model on ˜x, which\nare the representations of the data when generated\nfrom a perturbed sentence. We take the average of\nlog p(y|θ(xi)) −log p(y|θ(˜xi)) over all the data i\nin all sentences. For example, all words for the clas-\nsiﬁers, and all pairs of words for the distance probe.\nConcretely, we are just measuring the difference\nin validation loss of the same probe on the x data\nand the ˜x data. But because the loss is an appropri-\nate probability function, we can interpret the same\nquantity as a difference in log-likelihood between\nthe distribution conditioned on the regular repre-\nsentation and that conditioned on the perturbed\nrepresentation. Distortion is similarly computed us-\ning the full sentence, providing a number for each\nswap in each sentence.\n276\nWithout PMI With PMI\nLayer Coeff. ×10−2 p-value Coeff. ×10−2 p-value\nEmb. −0.21 5 .6 ×10−5 −0.11 9 .4 ×10−2\n1 −0.11 3 .4 ×10−2 −0.05 4 .2 ×10−1\n2 −0.74 <10−16 −0.53 2 .12 ×10−8\n3 −1.6 <10−16 −1.3 2 .2 ×10−16\n4 −2.0 <10−16 −1.4 4 .4 ×10−16\n5 −2.1 <10−16 −1.5 8 .8 ×10−16\n6 −2.4 <10−16 −1.7 <10−16\n7 −2.6 <10−16 −1.7 1 .6 ×10−15\n8 −3.4 <10−16 −2.3 <10−16\n9 −3.8 <10−16 −2.7 <10−16\n10 −4.1 <10−16 −3.0 <10−16\n11 −3.8 <10−16 −2.8 <10−16\n12 −4.2 <10−16 −3.1 <10−16\nTable 1: Coefﬁcients and p-values of the regular (‘without PMI’) and controlled (‘with PMI’) regressions of distortion against\nphrase boundary.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7436822652816772
    },
    {
      "name": "Phrase",
      "score": 0.736060380935669
    },
    {
      "name": "Sentence",
      "score": 0.7283391952514648
    },
    {
      "name": "Natural language processing",
      "score": 0.635947585105896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6095153093338013
    },
    {
      "name": "Syntax",
      "score": 0.5342200398445129
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44359147548675537
    },
    {
      "name": "Language model",
      "score": 0.42631763219833374
    },
    {
      "name": "Transformer",
      "score": 0.41602328419685364
    },
    {
      "name": "Physics",
      "score": 0.0753486156463623
    },
    {
      "name": "Programming language",
      "score": 0.065614253282547
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}