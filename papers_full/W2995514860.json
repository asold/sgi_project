{
  "title": "Modeling aspects of the language of life through transfer-learning protein sequences",
  "url": "https://openalex.org/W2995514860",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2897083495",
      "name": "Michael Heinzinger",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2120145792",
      "name": "Ahmed Elnaggar",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2097172772",
      "name": "Yu Wang",
      "affiliations": [
        "Leibniz Supercomputing Centre"
      ]
    },
    {
      "id": "https://openalex.org/A2761544922",
      "name": "Christian Dallago",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2939511768",
      "name": "Dmitrii Nechaev",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2030828791",
      "name": "Florian Matthes",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2001094594",
      "name": "Burkhard Rost",
      "affiliations": [
        "Institute for Advanced Study",
        "Columbia University",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2897083495",
      "name": "Michael Heinzinger",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2120145792",
      "name": "Ahmed Elnaggar",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2097172772",
      "name": "Yu Wang",
      "affiliations": [
        "Leibniz Supercomputing Centre"
      ]
    },
    {
      "id": "https://openalex.org/A2761544922",
      "name": "Christian Dallago",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2939511768",
      "name": "Dmitrii Nechaev",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2030828791",
      "name": "Florian Matthes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001094594",
      "name": "Burkhard Rost",
      "affiliations": [
        "Columbia University",
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2079882489",
    "https://openalex.org/W1985818354",
    "https://openalex.org/W1996073320",
    "https://openalex.org/W2095222655",
    "https://openalex.org/W2099302461",
    "https://openalex.org/W2085789739",
    "https://openalex.org/W2013136212",
    "https://openalex.org/W2126871981",
    "https://openalex.org/W2141920771",
    "https://openalex.org/W2153187042",
    "https://openalex.org/W2169317607",
    "https://openalex.org/W1526754730",
    "https://openalex.org/W2146396614",
    "https://openalex.org/W2050625197",
    "https://openalex.org/W2168970921",
    "https://openalex.org/W2141803423",
    "https://openalex.org/W2100063856",
    "https://openalex.org/W2070126374",
    "https://openalex.org/W2094005653",
    "https://openalex.org/W2123969230",
    "https://openalex.org/W2121651202",
    "https://openalex.org/W2149822709",
    "https://openalex.org/W2025630031",
    "https://openalex.org/W1985222378",
    "https://openalex.org/W2103485392",
    "https://openalex.org/W2059145105",
    "https://openalex.org/W2099589970",
    "https://openalex.org/W1965540415",
    "https://openalex.org/W2061042699",
    "https://openalex.org/W1982583124",
    "https://openalex.org/W2008545402",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2051210555",
    "https://openalex.org/W2972411752",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W1834070694",
    "https://openalex.org/W2398638460",
    "https://openalex.org/W2173591891",
    "https://openalex.org/W2897944569",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1501531009",
    "https://openalex.org/W2115453442",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2799163714",
    "https://openalex.org/W4252944911",
    "https://openalex.org/W2045777307",
    "https://openalex.org/W2754054476",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W1982289655",
    "https://openalex.org/W2161072217",
    "https://openalex.org/W2131204071",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2913820882",
    "https://openalex.org/W2111973517",
    "https://openalex.org/W2170463736",
    "https://openalex.org/W2607268717",
    "https://openalex.org/W2342838938",
    "https://openalex.org/W2963457143",
    "https://openalex.org/W2029476353",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W2153153865",
    "https://openalex.org/W2008708467",
    "https://openalex.org/W6600135713",
    "https://openalex.org/W2104972430",
    "https://openalex.org/W2769882797",
    "https://openalex.org/W2120671490",
    "https://openalex.org/W1982228765",
    "https://openalex.org/W1997967533",
    "https://openalex.org/W1973253766",
    "https://openalex.org/W2158173168",
    "https://openalex.org/W2108206279",
    "https://openalex.org/W2472351724",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W2887766567",
    "https://openalex.org/W2157009395",
    "https://openalex.org/W2468915207",
    "https://openalex.org/W2161922860",
    "https://openalex.org/W1995918845",
    "https://openalex.org/W2170960297",
    "https://openalex.org/W2006211612",
    "https://openalex.org/W2120401481",
    "https://openalex.org/W2952209472",
    "https://openalex.org/W2806449887",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2143210482",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W2059185913",
    "https://openalex.org/W2730472814",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2107725114",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2800388620",
    "https://openalex.org/W1968544966",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2951651062",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2950374603",
    "https://openalex.org/W2567587907"
  ],
  "abstract": "Abstract Background Predicting protein function and structure from sequence is one important challenge for computational biology. For 26 years, most state-of-the-art approaches combined machine learning and evolutionary information. However, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary information is less powerful for small families, e.g. for proteins from the Dark Proteome . Both these problems are addressed by the new methodology introduced here. Results We introduced a novel way to represent protein sequences as continuous vectors ( embeddings ) by using the language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively captured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new embeddings as SeqVec ( Seq uence-to- Vec tor) and demonstrate their effectiveness by training simple neural networks for two different tasks. At the per-residue level, secondary structure (Q3 = 79% ± 1, Q8 = 68% ± 1) and regions with intrinsic disorder (MCC = 0.59 ± 0.03) were predicted significantly better than through one-hot encoding or through Word2vec-like approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10 = 68% ± 1) and membrane-bound were distinguished from water-soluble proteins (Q2 = 87% ± 1). Although SeqVec embeddings generated the best predictions from single sequences, no solution improved over the best existing method using evolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary information and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of protein sequences. Overall, the important novelty is speed: where the lightning-fast HHblits needed on average about two minutes to generate the evolutionary information for a target protein, SeqVec created embeddings on average in 0.03 s. As this speed-up is independent of the size of growing sequence databases, SeqVec provides a highly scalable approach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis. Conclusion Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for various protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein sequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary information, however, that information is not available on the level of a single sequence.",
  "full_text": "RESEARCH ARTICLE Open Access\nModeling aspects of the language of life\nthrough transfer-learning protein\nsequences\nMichael Heinzinger 1,2*† , Ahmed Elnaggar 1,2†, Yu Wang 3, Christian Dallago 1,2, Dmitrii Nechaev 1,2,\nFlorian Matthes 4 and Burkhard Rost 1,5,6,7\nAbstract\nBackground: Predicting protein function and structure from sequence is one important challenge for computational\nbiology. For 26 years, most state-of-the-art approaches combined machine learning and evolutionary information.\nHowever, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary\ninformation is less powerful for small families, e.g. for proteins from the Dark Proteome. Both these problems are\naddressed by the new methodology introduced here.\nResults: We introduced a novel way to represent protein sequences as continuous vectors ( embeddings)b yu s i n g\nthe language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively\ncaptured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new\nembeddings as SeqVec (Sequence-to-Vector) and demonstrate their effectiveness by training simple neural networks for\ntwo different tasks. At the per-residue level, secondary structure (Q3 = 79% ± 1, Q8 = 68% ± 1) and regions with intrinsic\ndisorder (MCC = 0.59 ± 0.03) were predicted significantly better than through one-hot encoding or through Word2vec-\nlike approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10 = 68% ± 1) and\nmembrane-bound were distinguished from water-soluble proteins (Q2 = 87% ± 1). Although SeqVec embeddings\ngenerated the best predictions from single sequences, no solution improved over the best existing method using\nevolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary\ninformation and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of\nprotein sequences. Overall, the important novelty is speed: where the lightning-fast HHblits needed on average about\ntwo minutes to generate the evolutionary information for a target protein, SeqVec created embeddings on average in\n0.03 s. As this speed-up is independent of the size of growing sequence databases, SeqVec provides a highly scalable\napproach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis.\nConclusion: Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for\nvarious protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein\nsequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary\ninformation, however, that information is not available on the level of a single sequence.\nKeywords: Machine Learning, Language Modeling, Sequence Embedding, Secondary structure prediction, Localization\nprediction, Transfer Learning, Deep Learning\n© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n* Correspondence: mheinzinger@rostlab.org; assistant@rostlab.org\n†Michael Heinzinger and Ahmed Elnaggar contributed equally to this work.\n1Department of Informatics, Bioinformatics & Computational Biology - i12,\nTUM (Technical University of Munich), Boltzmannstr. 3, 85748 Garching/\nMunich, Germany\n2TUM Graduate School, Center of Doctoral Studies in Informatics and its\nApplications (CeDoSIA), Boltzmannstr. 11, 85748 Garching, Germany\nFull list of author information is available at the end of the article\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 \nhttps://doi.org/10.1186/s12859-019-3220-8\nBackground\nThe combination of evolutionary information (from\nMultiple Sequence Alignments – MSA) and Machine\nLearning/Artificial Intelligence (standard feed-forward\nartificial neural networks – ANN) completely changed\nprotein secondary structure prediction [ 1– 3]. The con-\ncept was quickly taken up [ 4– 8] and predictions im-\nproved even more with larger families increasing\nevolutionary information through diversity [ 9, 10]. The\nidea was applied to other tasks, including the prediction\nof transmembrane regions [ 11– 13], solvent accessibility\n[14], residue flexibility (B-values) [ 15, 16], inter-residue\ncontacts [ 17] and protein disorder [ 15, 18– 20]. Later,\nautomatic methods predicting aspects of protein func-\ntion improved by combining evolutionary information\nand machine learning, including predictions of subcellu-\nlar localization (aka cellular compartment or CC in GO\n[21, 22]), protein interaction sites [ 23– 25], and the\neffects of sequence variation upon function [ 26, 27]. Ar-\nguably, the most important breakthrough for protein\nstructure prediction over the last decade was a more effi-\ncient way of using evolutionary couplings [ 28– 31].\nAlthough evolutionary information has increasingly\nimproved prediction methods, it is also becoming in-\ncreasingly costly. As sequencing becomes cheaper, the\nnumber of bio-sequence databases grow faster than\ncomputing power. For instance, the number of UniProt\nentries is now more than doubling every two years [ 32].\nAn all-against-all comparison executed to build up pro-\nfiles of evolutionary information squares this number:\nevery two years the job increases 4-fold while computer\npower grows less than 2-fold. Consequently, methods as\nfast as PSI-BLAST [ 33] have to be replaced by faster so-\nlutions such as HHblits [ 34]. Even its latest version\nHHblits3 [ 35] still needs several minutes to search Uni-\nRef50 (subset of UniProt) for a single query protein. The\nnext step up in speed such as MMSeqs2 [ 36] appear to\ncope with the challenge at the expense of increasing\nhardware requirements while databases keep growing.\nHowever, even these solutions might eventually lose the\nbattle against the speedup of sequencing. Analyzing data\nsets involving millions of proteins, i.e. samples of the hu-\nman gut microbiota or metagenomic samples, have\nalready become a major challenge [ 35]. Secondly, evolu-\ntionary information is still missing for some proteins,\ne.g. for proteins with substantial intrinsically disordered\nregions [ 15, 37, 38], or the entire Dark Proteome [39] full\nof proteins that are less-well studied but important for\nfunction [ 40].\nHere, we propose a novel embedding of protein se-\nquences that replaces the explicit search for evolutionary\nrelated proteins by an implicit transfer of biophysical in-\nformation derived from large, unlabeled sequence data\n(here UniRef50). We adopted a method that has been\nrevolutionizing Natural Language Processing (NLP),\nnamely the bi-directional language model ELMo (Em-\nbeddings from Language Models) [ 41]. In NLP, ELMo is\ntrained on unlabeled text-corpora such as Wikipedia to\npredict the most probable next word in a sentence, given\nall previous words in this sentence. By learning a prob-\nability distribution for sentences, these models autono-\nmously develop a notion for syntax and semantics of\nlanguage. The trained vector representations (embed-\ndings) are contextualized, i.e. the embeddings of a given\nword depend on its context. This has the advantage that\ntwo identical words can have different embeddings, de-\npending on the words surrounding them. In contrast to\nprevious non-contextualized approaches such as word2-\nvec [ 42, 43], this allows to take the ambiguous meaning\nof words into account.\nWe hypothesized that the ELMo concept could be ap-\nplied to model protein sequences. Three main challenges\narose. (1) Proteins range from about 30 to 33,000 resi-\ndues, a much larger range than for the average English\nsentence extending over 15 – 30 words [ 44], and even\nmore extreme than notable literary exceptions such as\nJames Joyce ’s Ulysses (1922) with almost 4000 words in\na sentence. Longer proteins require more GPU memory\nand the underlying models (so-called LSTMs: Long\nShort-Term Memory networks [ 45]) have only a limited\ncapability to remember long-range dependencies. (2)\nProteins mostly use 20 standard amino acids, 100,000\ntimes less tokens than in the English language. Smaller\nvocabularies might be problematic if protein sequences\nencode a similar complexity as sentences. (3) We found\nUniRef50 to contain almost ten times more tokens (9.5\nbillion amino acids) than the largest existing NLP corpus\n(1 billion words). Simply put: Wikipedia is roughly ten\ntimes larger than Webster ’s Third New International\nDictionary and the entire UniProt is over ten times lar-\nger than Wikipedia. As a result, larger models might be\nrequired to absorb the information in biological\ndatabases.\nWe trained ELMo on UniRef50 and assessed the pre-\ndictive power of the embeddings by application to tasks\non two levels: per-residue (word-level) and per-protein\n(sentence-level). For the per-residue prediction task, we\npredicted secondary structure and long intrinsic dis-\norder. For the per-protein prediction task, we predicted\nsubcellular localization and trained a classifier distin-\nguishing between membrane-bound and water-soluble\nproteins. We used publicly available data sets from two\nrecent methods that achieved break-through perform-\nance through Deep Learning, namely NetSurfP-2.0 for\nsecondary structure [ 46] and DeepLoc for localization\n[47]. We compared the performance of the SeqVec em-\nbeddings to state-of-the-art methods using evolutionary\ninformation, and also to a popular embedding tool for\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 2 of 17\nprotein sequences originating from the Word2vec ap-\nproach, namely ProtVec [42]. Notably, while ProtVec\ncaptures local information, it loses information on se-\nquence ordering, and the resulting residue embeddings\nare insensitive to their context (non-contextualized), i.e.\nthe same word results in the same embedding regardless\nof the specific context.\nUnderstanding a language typically implies to under-\nstand most typical constructs convened in that language.\nModeling a language in a computer can have many\nmeanings, spanning from the automatic understanding\nof the semantic of languages, to parsing some underlying\nrules of a language (e.g. syntax). Arguably, proteins are\nthe most important machinery of life. Protein sequence\nlargely determines protein structure, which somehow\ndetermines protein function [ 48]. Thus, the expression\nof the language of life are essentially protein sequences.\nUnderstanding those sequences implies to predict pro-\ntein structure from sequence. Despite recent successes\n[49, 50], this is still not possible for all proteins. How-\never, the novel approach introduced here succeeds to\nmodel protein sequences in the sense that it implicitly\nextracts grammar-like principles (as embeddings) which\nare much more successful in predicting aspects of pro-\ntein structure and function than any of the biophysical\nfeatures previously used to condensate expert knowledge\nof protein folding, or any other previously tried simple\nencoding of protein sequences.\nResults\nModeling protein sequences through SeqVec embeddings\nSeqVec, our ELMo-based implementation, was trained\nfor three weeks on 5 Nvidia Titan GPUs with 12 GB\nmemory each. The model was trained until its perplexity\n(uncertainty when predicting the next token) converged\nat around 10.5 (Additional file 1: Figure S1). Training\nand testing were not split due to technical limitations\n(incl. CPU/GPU). ELMo was designed to reduce the risk\nof overfitting by sharing weights between forward and\nbackward LSTMs and by using dropout. The model had\nabout 93 M (mega/million) free parameters compared to\nthe 9.6G (giga/billion) tokens to predict leading to a ra-\ntio of samples/free parameter below 1/100, the best our\ngroup has ever experienced in a prediction task. Similar\napproaches have shown that even todays largest models\n(750 M free parameters) are not able to overfit on a large\ncorpus (250 M protein sequences) [ 51].\nSeqVec embeddings appeared robust\nWhen training ELMo on SWISS-PROT (0.5 M se-\nquences), we obtained less useful models, i.e. the subse-\nquent prediction methods based on those embeddings\nwere less accurate. Training on UniRef50 (33 M se-\nquences) gave significantly better results in subsequent\nsupervised prediction tasks, and we observed similar re-\nsults when using different hyperparameters. For in-\nstance, increasing the number of LSTM layers in ELMo\n(from two to four) gave a small, non-significant im-\nprovement. As the expansion of 2 to 4 layers roughly\ndoubled time for training and retrieving embeddings, we\ndecided to trade speed for insignificant improvement\nand continued with the faster two-layer ELMo architec-\nture. Computational limitations hindered us from fully\ncompleteing the modelling of UniRef90 (100 million se-\nquences). Nevertheless, after four weeks of training, the\nmodels neither appeared to be better nor significantly\nworse than those for UniRef50. Users of the embeddings\nneed to be aware that every time a new ELMo model is\ntrained, the downstream supervised prediction method\nneeds to be retrained in the following sense. Assume we\ntransfer-learn UniRef50 through SeqVec1, then use\nSeqVec1 to machine learn DeepSeqVec1 for a supervised\ntask (e.g. localization prediction). In a later iteration, we\nredo the transfer learning with different hyperparameters\nto obtain SeqVec2. For any given sequence, the embed-\ndings of SeqVec2 will differ from those of SeqVec1, as a\nresult, passing embeddings derived from SeqVec2 to\nDeepSeqVec1 will not provide meaningful predictions.\nPer-residue performance high, not highest\nNetSurfP-2.0 feeds HHblits or MMseqs2 profiles into\nadvanced combinations of Deep Learning architectures\n[46] to predict secondary structure, reaching a three-\nstate per-residue accuracy Q3 of 82 – 85% (lower value:\nsmall, partially non-redundant CASP12 set, upper value:\nlarger, more redundant TS115 and CB513 sets; Table 1,\nFig. 1; several contenders such as Spider3 and RaptorX\nreach within three standard errors). All six methods de-\nveloped by us fell short of reaching this mark, both\nmethods not using evolutionary information/profiles\n(DeepSeqVec, DeepProtVec, DeepOneHot, DeepBLO-\nSUM65), but also those that did use profiles ( DeepProf,\nDeepProf+SeqVec, Fig. 1a, Table 1). The logic in our\nacronyms was as follows (Methods): “Prof” implied using\nprofiles (evolutionary information), SeqVec (Sequence-to-\nVector) described using pre-trained ELMo embeddings,\n“Deep” before the method name suggested applying a sim-\nple deep learning method trained on particular prediction\ntasks using SeqVec embeddings only (DeepSeqVec),\nprofiles without (DeepProf) or with embeddings (Deep-\nProf+SeqVec), or other simple encoding schema (ProtVec,\nOneHot or sparse encoding, or BLOSUM65). When com-\nparing methods that use only single protein sequences as\ninput (DeepSeqVec, DeepProtVec, DeepOneHot, Deep-\nBLOSUM65; all white in Table 1), the new method intro-\nduced here, SeqVec outperformed others not using\nprofiles by three standard errors ( P-value< 0.01; Q3: 5 – 10\npercentage points, Q8: 5 – 13 percentage points, MCC:\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 3 of 17\n0.07– 0.12, Table 1). Using a context-independent lan-\nguage model derived from the Word2vec approach,\nnamely DeepProtVec was worse by 10 percentage points\n(almost six standard errors). On the other hand, our im-\nplementation of evolutionary information (DeepProf using\nHHblits profiles) remained about 4 – 6 percentage points\nbelow NetSurfP-2.0 (Q3 = 76 – 81%, Fig. 1,T a b l e 1). De-\npending on the test set, using SeqVec embeddings instead\nof evolutionary information (DeepSeqVec: Fig. 1a, Table\n1)r e m a i n e d2 – 3 percentage points below that mark\n(Q3 = 73– 79%, Fig. 1a, Table 1). Using both evolutionary\ninformation and SeqVec embeddings (DeepProf+SeqVec)\nimproved over both, but still did not reach the top (Q3 =\n77– 82%). In fact, the ELMo embeddings alone (DeepSeq-\nVec) did not surpass any of the best methods using evolu-\ntionary information tested on the same data set (Fig. 1a).\nTable 1 Per-residue predictions: secondary structure and disorder\nData Prediction task Secondary structure Disorder\nMethod Q3 (%) Q8 (%) MCC FPR\nCASP12 NetSurfP-2.0 (hhblits) a,b 82.4 71.1 0.604 0.011\nNetSurfP-1.0a,b 70.9 –– –\nSpider3a,b 79.1 – 0.582 0.026\nRaptorXa,b 78.6 66.1 0.621 0.045\nJpred4a,b 76.0 –– –\nDeepSeqVec 73.1 ± 1.3 61.2 ± 1.6 0.575 ± 0.075 0.026 ± 0.008\nDeepProfb 76.4 ± 2.0 62.7 ± 2.2 0.506 ± 0.057 0.022 ± 0.009\nDeepProf + SeqVec b 76.5 ± 1.5 64.1 ± 1.5 0.556 ± 0.080 0.022 ± 0.008\nDeepProtVec 62.8 ± 1.7 50.5 ± 2.4 0.505 ± 0.064 0.016 ± 0.006\nDeepOneHot 67.1 ± 1.6 54.2 ± 2.1 0.461 ± 0.064 0.012 ± 0.005\nDeepBLOSUM65 67.0 ± 1.6 54.5 ± 2.0 0.465 ± 0.065 0.012 ± 0.005\nTS115 NetSurfP-2.0 (hhblits) a,b 85.3 74.4 0.663 0.006\nNetSurfP-1.0a,b 77.9 –– –\nSpider3a,b 83.9 – 0.575 0.008\nRaptorXa,b 82.2 71.6 0.567 0.027\nJpred4a,b 76.7 –– –\nDeepSeqVec 79.1 ± 0.8 67.6 ± 1.0 0.591 ± 0.028 0.012 ± 0.001\nDeepProfb 81.1 ± 0.6 68.3 ± 0.9 0.516 ± 0.028 0.012 ± 0.002\nDeepProf + SeqVec b 82.4 ± 0.7 70.3 ± 1.0 0.585 ± 0.029 0.013 ± 0.003\nDeepProtVec 66.0 ± 1.0 54.4 ± 1.3 0.470 ± 0.028 0.011 ± 0.002\nDeepOneHot 70.1 ± 0.8 58.5 ± 1.1 0.476 ± 0.028 0.008 ± 0.001\nDeep BLOSUM65 70.3 ± 0.8 58.1 ± 1.1 0.488 ± 0.029 0.007 ± 0.001\nCB513 NetSurfP-2.0 (hhblits) a,b 85.3 72.0 ––\nNetSurfP-1.0a,b 78.8 –– –\nSpider3a,b 84.5 –– –\nRaptorXa,b 82.7 70.6 ––\nJpred4a,b 77.9 –– –\nDeepSeqVec 76.9 ± 0.5 62.5 ± 0.6 ––\nDeepProfb 80.2 ± 0.4 64.9 ± 0.5 ––\nDeepProf + SeqVec b 80.7 ± 0.5 66.0 ± 0.5 ––\nDeepProtVec 63.5 ± 0.4 48.9 ± 0.5 ––\nDeepOneHot 67.5 ± 0.4 52.9 ± 0.5 ––\nDeepBLOSUM65 67.4 ± 0.4 53.0 ± 0.5 ––\nPerformance comparison for secondary structure (3- vs. 8-classes) and disorder prediction (binary) for the CASP12, TS115 and CB513 data sets. Accur acy (Q3, Q10)\nis given in percentage. Results marked by a are taken from NetSurfP-2.0 [ 46]; the authors did not provide standard errors. Highest numerical values in each\ncolumn in bold letters. Methods DeepSeqVec, DeepProtVec, DeepOneHot and DeepBLOSUM65 use only information from single protein sequences. Methods\nusing evolutionary information (MSA profiles) are marked by b; these performed best throughout\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 4 of 17\nFor the prediction of intrinsic disorder, we observed\nthe same: NetSurfP-2.0 performed best; our implementa-\ntion of evolutionary information (DeepProf) performed\nworse (Fig. 1b, Table 1). However, for this task the em-\nbeddings alone (DeepSeqVec) performed relatively well,\nexceeding our in-house implementation of a model\nusing evolutionary information (DeepSeqVec MCC =\n0.575– 0.591 vs. DeepProf MCC = 0.506 – 0.516, Table 1).\nThe combination of evolutionary information and em-\nbeddings (DeepProf+SeqVec) improved over using evo-\nlutionary information alone but did not improve over\nthe SeqVec embeddings for disorder. Compared to other\nmethods, the embeddings alone reached similar values\n(Fig. 1b).\nPer-protein performance close to best\nFor predicting subcellular localization (cellular compart-\nments) in ten classes, DeepLoc [47] is top with Q10 =\n78% (Fig. 1c, Table 2). For simplicity, we only tested\nmethods not using evolutionary information/profiles for\nthis task. Our sequence-only embeddings model\nDeepSeqVec-Loc reached second best performance to-\ngether with iLoc-Euk [ 52] at Q10 = 68% (Fig. 1c, Table\n2). Unlike the per-residue predictions, for this\napplication the SeqVec embeddings outperformed sev-\neral popular prediction methods that use evolutionary\ninformation by up to 13 percentage points in Q10 (Table\n2: DeepSeqVec-Loc vs. methods shown in grayed rows).\nThe gain of the context-dependent SeqVec model intro-\nduced here over context-independent versions such as\nProtVec (from Word2vec) was even more pronounced\nthan for the per-residue prediction task (Q10 68 ± 1% vs.\n42 ± 1%).\nPerformance for the classification into membrane-\nbound and water-soluble proteins followed a similar\ntrend (Fig. 1d, Table 2): while DeepLoc still performed\nbest (Q2 = 92.3, MCC = 0.844), DeepSeqVec-Loc reached\njust a few percentage points lower (Q2 = 86.8 ± 1.0,\nFig. 1 Performance comparisons. The predictive power of the ELMo-based SeqVec embeddings was assessed for per-residue (upper row) and\nper-protein (lower row) prediction tasks. Methods using evolutionary information are highlighted by hashes above the bars. Approaches using\nonly the proposed SeqVec embeddings are highlighted by stars after the method name. Panel A used three different data sets (CASP12, TS115,\nCB513) to compare three-state secondary structure prediction (y-axis: Q3; all DeepX developed here to test simple deep networks on top of the\nencodings tested; DeepProf used evolutionary information). Panel B compared predictions of intrinsically disordered residues on two data sets\n(CASP12, TS115; y-axis: MCC). Panel C compared per-protein predictions for subcellular localization between top methods (numbers for Q10 taken\nfrom DeepLoc [ 47]) and embeddings based on single sequences (Word2vec-like ProtVec [42] and our ELMo-based SeqVec). Panel D: the same\ndata set was used to assess the predictive power of SeqVec for the classification of a protein into membrane-bound and water-soluble\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 5 of 17\nMCC = 0.725 ± 0.021; full confusion matrix Additional\nfile 1: Figure S2). In contrast to this, ProtVec, another\nmethod using only single sequences, performed substan-\ntially worse (Q2 = 77.6 ± 1.3, MCC = 0.531 ± 0.026).\nVisualizing results\nLack of insight often triggers the misunderstanding that\nmachine learning methods are black box solutions bar-\nring understanding. In order to interpret the SeqVec\nembeddings, we have projected the protein-embeddings\nof the per-protein prediction data upon two dimensions\nusing t-SNE [ 53]. We performed this analysis once for\nthe raw embeddings (SeqVec, Fig. 2 upper row) and\nonce for the hidden layer representation of the per-\nprotein network (DeepSeqVec-Loc) after training (Fig. 2\nlower row). All t-SNE representations in Fig. 2 were cre-\nated using 3000 iterations and the cosine distance as\nmetric. The two analyses differed only in that the per-\nplexity was set to 20 for one ( SeqVec) and 15 for the\nother (DeepSeqVec-Loc). The t-SNE representations\nwere colored either according to their localization within\nthe cell (left column of Fig. 2) or according to whether\nthey are membrane-bound or water-soluble (right\ncolumn).\nDespite never provided during training, the raw em-\nbeddings appeared to capture some signal for classifying\nproteins by localization (Fig. 2, upper row, left column).\nThe most consistent signal was visible for extra-cellular\nproteins. Proteins attached to the cell membrane or\nlocated in the endoplasmic reticulum also formed well-\ndefined clusters. In contrast, the raw embeddings neither\ncaptured a consistent signal for nuclear nor for\nmitochondrial proteins. Through training, the network\nimproved the signal to reliably classify mitochondrial\nand plastid proteins. However, proteins in the nucleus\nand cell membrane continued to be poorly distinguished\nvia t-SNE.\nColoring the t-SNE representations for membrane-\nbound or water-soluble proteins (Fig. 2, right column),\nrevealed that the raw embeddings already provided well-\ndefined clusters although never trained on membrane\nprediction (Fig. 2, upper row). After training, the classifi-\ncation was even better (Fig. 2, lower row).\nAnalogously, we used t-SNE projections to analyze Seq-\nVec embeddings on different levels of complexity inherent\nto proteins (Fig. 3), ranging from the building blocks\n(amino acids, Fig. 3a), to secondary structure defined pro-\ntein classes (Fig. 3b), over functional features (Fig. 3c), and\nonto the macroscopic level of the kingdoms of life and vi-\nruses (Fig. 3d; classifications in panels 3b-3d based on\nSCOPe [ 54]). Similar to the results described in [ 51], our\nprojection of the embedding space confirmed that the\nmodel successfully captured bio-chemical and bio-\nphysical properties on the most fine-grained level, i.e. the\n20 standard amino acids (Fig. 3a). For example, aromatic\namino acids (W, F, Y) are well separated from aliphatic\namino acids (A, I, L, M, V) and small amino acids (A, C,\nG, P, S, T) are well separated from large ones (F, H, R, W,\nY). The projection of the letter indicating an unknown\namino acid (X), clustered closest to the amino acids ala-\nnine (A) and glycine (G) (data not shown). Possible expla-\nnations for this could be that the two amino acids with\nthe smallest side chains might be least biased towards\nother biochemical features like charge and that they are\nthe 2nd (A) and 4th (G) most frequent amino acids in our\ntraining set (Additional file 1: Table S1). Rare (O, U) and\nambiguous amino acids (Z, B) were removed from the\nprojection as their clustering showed that the model could\nnot learn reasonable embeddings from the very small\nnumber of samples.\nHigh-level structural classes as defined in SCOPe\n(Fig. 3b) were also captured by SeqVec embeddings. Al-\nthough the embeddings were only trained to predict the\nnext amino acid in a protein sequence, well separated\nclusters emerged from those embeddings in structure\nspace. Especially, membrane proteins and small pro-\nteins formed distinct clusters (note: protein length is\nnot explicitly encoded in SeqVec). Also, these results in-\ndicated that the embeddings captured complex rela-\ntionships between proteins which are not directly\no b s e r v a b l ef r o ms e q u e n c es i m i l a r i t ya l o n ea sS C O P e\nwas redundancy reduced at 40% sequence identity.\nTherefore, the new embeddings could complement\nsequence-based structural classification as it was shown\nthat the sequence similarity does not necessarily lead to\nstructural similarity [ 55].\nTable 2 Per-protein predictions: localization and membrane/\nglobular\nMethod Localization Membrane/globular\nQ10 (%) Gorodkin (MCC) Q2 MCC\nLocTree2a,b 61 0.53\nMultiLoc2a,b 56 0.49\nCELLOa 55 0.45\nWoLF PSORT a 57 0.48\nYLoca 61 0.53\nSherLoc2a,b 58 0.51\niLoc-Euka,b 68 0.64\nDeepLoca,b 78 0.73 92.3 0.844\nDeepSeqVec-Loc 68 ± 1 0.61 ± 0.01 86.8 ± 1.0 0.725 ± 0.021\nDeepProtVec-Loc 42 ± 1 0.19 ± 0.01 77.6 ± 1.3 0.531 ± 0.026\nPerformance for per-protein prediction of subcellular localization and\nclassifying proteins into membrane-bound and water-soluble. Results marked\nby a taken from DeepLoc [ 47]; the authors provided no standard errors. The\nresults reported for SeqVec and ProtVec were based on single protein\nsequences, i.e. methods NOT using evolutionary information (neither during\ntraining nor testing). All methods using evolutionary information are marked\nby b; best in each set marked by bold numbers\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 6 of 17\nTo further investigate the clusters emerging from the\nSCOPe data set, we colored the same data set based on\nprotein functions (Fig. 3c) and kingdoms (Fig. 3d). This\nanalysis revealed that many of the small, distinct clusters\nemerged based on protein functions. For instance, trans-\nferases and hydrolases formed many small clusters.\nWhen increasing the level of abstraction by coloring the\nproteins according to their kingdoms, we observed cer-\ntain clusters to be dominated by e.g. eukaryotes. Com-\nparing the different views captured in panels 3B-3D\nrevealed connections, e.g. that all-beta or small proteins\ndominate in eukaryotes (compare blue and orange\nislands in Fig. 3b with the same islands in Fig. 3d – col-\nored blue to mark eukaryotes).\nCPU/GPU time used\nDue to the sequential nature of LSTMs, the time re-\nquired to embed a protein grows linearly with protein\nlength. Depending on the available main memory or\nGPU memory, this process could be massively\nparallelized. To optimally use available memory, batches\nare typically based on tokens rather than on sentences.\nIn order to retrieve embeddings, we sorted proteins ac-\ncording to their length and created batches of ≤15 K to-\nkens that could still be handled by a single Nvidia\nGeForce GTX1080 with 8GB VRAM. The processing of\na single protein took on average 0.027 s when applying\nthis batch-strategy to the NetSurfP-2.0 data set (average\nprotein length: 256 residues, i.e. shorter than proteins\nfor which 3D structure is not known). The batch with\nthe shortest proteins (on average 38 residues, corre-\nsponding to 15% of the average protein length in the\nwhole data set) required about one tenth (0.003 s per\nprotein, i.e. 11% of that for whole set). The batch con-\ntaining the longest protein sequences in this data set\n(1578 residues on average, corresponding to 610% of\naverage protein length in the whole data set), took about\nsix times more (1.5 s per protein, i.e. 556% of that for\nwhole set). When creating SeqVec for the DeepLoc set\n(average length: 558 residues; as this set does not require\nFig. 2 t-SNE representations of SeqVec. Shown are t-SNE projections from embedded space onto a 2D representation; upper row: unsupervised\n1024-dimensional “raw” ELMo-based SeqVec embeddings, averaged over all residues in a protein; lower row: supervised 32-dimensional ELMo-\nbased SeqVec embeddings, reduced via per-protein machine learning predictions (data: redundancy reduced set from DeepLoc). Proteins were\ncolored according to their localization (left column) or whether they are membrane-bound or water-soluble (right column). Left and right panel\nwould be identical except for the color, however, on the right we had to leave out some points due to lacking membrane/non-membrane\nannotations. The upper row suggests that SeqVec embeddings capture aspects of proteins without ever seeing labels of localization or\nmembrane, i.e. without supervised training. After supervised training (lower row), this information is transferred to, and further distilled by\nnetworks with simple architectures. After training, the power of SeqVeq embeddings to distinguish aspects of function and structure become\neven more pronounced, sometimes drastically so, as suggested by the almost fully separable clusters in the lower right panel\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 7 of 17\na 3D structure, it provides a more realistic view on the\ndistribution of protein lengths), the average processing\ntime for a single protein was 0.08 with a minimum of\n0.006 for the batch containing the shortest sequences\n(67 residues on average) and a maximum of 14.5 s (9860\nresidues on average). On a single Intel i7 – 6700 CPU\nwith 64GB RAM, processing time increased by roughly\n50% to 0.41 s per protein, with a minimum and a max-\nimum computation time of 0.06 and 15.3 s, respectively.\nCompared to an average processing time of one hour for\n1000 proteins when using evolutionary information dir-\nectly [ 46], this implied an average speed up of 120-fold\non a single GeForce GTX1080 and 9-fold on a single i7 –\n6700 when predicting structural features; the inference\ntime of DeepSeqVec for a single protein is on average\n0.0028 s.\nDiscussion\nTransfer-learning alone not top\nThe context-dependent transfer-learning model ELMo\n[41] applied to proteins sequences (here dubbed SeqVec)\nclearly succeeded to model the language of protein se-\nquences much better than simple schema (e.g. one-hot\nencoding), more advanced context-independent lan-\nguage models such as ProtVec (based on Word2vec [ 42,\n43]), more advanced distillations of text-book knowledge\n(biophysical features used as input for prediction [ 2, 3]),\nand also some family-independent information about\nevolution as represented by the expertise condensed in\nthe BLOSSUM62 matrix. In this sense, our approach\nworked. However, none of our SeqVec implementations\nreached today ’s best methods: NetSurfP-2.0 for second-\nary structure and protein disorder and DeepLoc for\nFig. 3 Modeling aspects of the language of life. 2D t-SNE projections of unsupervised SeqVec embeddings highlight different realities of proteins\nand their constituent parts, amino acids. Panels B to D are based on the same data set (Structural Classification of Proteins – extended (SCOPe)\n2.07, redundancy reduced at 40%). For these plots, only subsets of SCOPe containing proteins with the annotation of interest (enzymatic activity\nC and kingdom D) may be displayed. Panel A: the embedding space confirms: the 20 standard amino acids are clustered according to their\nbiochemical and biophysical properties, i.e. hydrophobicity, charge or size. The unique role of Cysteine (C, mostly hydrophobic and polar) is\nconserved. Panel B: SeqVec embeddings capture structural information as annotated in the main classes in SCOPe without ever having been\nexplicitly trained on structural features. Panel C: many small, local clusters share function as given by the main classes in the Enzyme Commission\nNumber (E.C.). Panel D: similarly, small, local clusters represent different kingdoms of life\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 8 of 17\nlocalization and membrane protein classification (Fig. 1,\nTable 1, Table 2). Clearly, “just” using SeqVec embed-\ndings to train subsequent prediction methods did not\nsuffice to crack the challenges. Due to computational\nlimitations, testing models trained on larger sequence\ndatabase, which may over-come this limitation, could\nnot be tested. What about more advanced transfer-\nlearning models, e.g. TransformerXL [ 56], or different\npre-training objectives which model bidirectional con-\ntexts, e.g. Bert [ 57] or XLNet [ 58]? We have some evi-\ndence that transformer-based models might reach\nfurther (Elnaggar et al. in preparation), with competing\ngroups already showing promising results [ 51]. Never-\ntheless, there is one major reality to remember: we\nmodel single protein sequences. Such models might\nlearn the rules for “writing protein sequences ” and still\nmiss the constraints imposed by the “survival of the fit-\ntest”, i.e. by evolutionary selection.\nOn the other hand, some of our solutions appeared\nsurprisingly competitive given the simplicity of the archi-\ntectures. In particular, for the per-protein predictions,\nfor which SeqVec clearly outperformed the previously\npopular ProtVec [42] approach and even commonly used\nexpert solutions (Fig. 1, Table 2: no method tested other\nthan the top-of-the-line DeepLoc reached higher numer-\nical values). For that comparison, we used the same data\nsets but could not rigorously compare standard errors\n(SE) that were unavailable for other methods. Estimating\nstandard errors for our methods suggested the differ-\nences to be statistically significant: > 7 SE throughout\n(exception: DeepLoc (Q10 = 78) and iLoc-Euk(Q10 =\n68)). The results for localization prediction implied that\nfrequently used methods using evolutionary information\n(all marked with shaded boxes in Table 2) did not clearly\noutperform our simple ELMo-based tool (DeepSeqVec-\nLoc in Table 2). This was very different for the per-\nresidue prediction tasks: here almost all top methods\nusing evolutionary information numerically outper-\nformed the simple model built on the ELMo embeddings\n(DeepSeqVec in Fig. 1 and Table 1). However, all models\nintroduced in this work were deliberately designed to be\nrelatively simple to demonstrate the predictive power of\nSeqVec. More sophisticated architectures building up on\nSeqVec embeddings will likely outperform the ap-\nproaches introduced here.\nCombining SeqVec with evolutionary information for\nper-residue predictions still did not reach the top (set\nTS115: Q3(NetSurfP-2.0) = 85.3% vs. Q3(DeepProf +\nSeqVec) = 82.4%, Table 1). This might suggest some\nlimit for the usefulness of the ELMo-based SeqVec em-\nbeddings. However, it might also point to the more ad-\nvanced solutions realized by NetSurfP-2.0 which applies\ntwo LSTMs of similar complexity as our entire system\n(including ELMo) on top of their last step leading to 35\nM (35 million) free parameters compared to about 244 K\nfor DeepProf + SeqVec. Twenty times more free param-\neters might explain some fraction of the success. Due to\nlimited GPU resources, we could not test how much.\nWhy did the ELMo-based approach improve more\n(relative to competition) for per-protein than for per-\nresidue predictions? We can only speculate because\nnone of the possible explanations have held consistently\nfor all methods to which we have been applying ELMo\nembeddings over the recent six months (data not\nshown). For instance, the per-protein data sets were over\ntwo orders of magnitude smaller than those for per-\nresidue predictions; simply because every protein\nconstitutes one sample in the first and protein length\nsamples for the second. SeqVec might have helped more\nfor the smaller data sets because the unlabeled data is\npre-processed so meaningful that less information needs\nto be learned by the ANN during per-protein prediction.\nThis view was strongly supported by the t-SNE [ 53]\nresults (Fig. 2, Fig. 3): ELMo apparently had learned the\n“grammar” of the language of life well enough to realize\na very rough clustering of structural classes, protein\nfunction, localization and membrane/not. Another, yet\ncomplementary, explanation for this trend could be that\nthe training of ELMo inherently provides a natural\nw a yo fs u m m a r i z i n gi n f o r m a t i o no fp r o t e i n so fv a r y -\ning length. Other approaches usually learn this\nsummarization step together with the actual predic-\ntion tasks which gets increasingly difficult the smaller\nthe data set.\nWe picked four tasks as proof-of-principle for our\nELMo/SeqVec approach. These tasks were picked be-\ncause recent breakthroughs had been reported (e.g.\nNetSurfP-2.0 [ 46] and DeepLoc [ 47]) and those had\nmade data for training and testing publicly available. We\ncannot imagine why our findings should not hold true\nfor other tasks of protein prediction and invite the com-\nmunity to apply the SeqVec embeddings for their tasks.\nWe assume the SeqVec embeddings to be more benefi-\ncial for small than for large data sets. For instance, we\nexpect little or no gain in predicting inter-residue con-\ntacts, and more in predicting protein binding sites.\nGood and fast predictions without using evolutionary\ninformation\nAlthough our SeqVec embeddings were over five per-\ncentage points worse than the best method NetSurfP-2.0\n(Table 1: TS115 Q3: 85.3 vs. 79.1), for some proteins\n(12% in CB513) DeepSeqVec performed better (Add-\nitional file 1: Figure S4). We expect those to be proteins\nwith small or incorrect alignments, however, due to the\nfact that we did not have the alignments available used\nby NetSurfP-2.0, we could not quite establish the validity\nof this assumption (analyzing pre-computed alignments\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 9 of 17\nfrom ProteinNet [ 59] revealed no clear relation of the\ntype: more evolutionary information leads to better pre-\ndiction). However, the real strength of our solutions is\nits speed: SeqVec predicted secondary structure and pro-\ntein disorder over 100-times faster (on a single 8GB\nGPU) than NetSurfP-2.0 when counting the time it\nneeds to retrieve the evolutionary information summa-\nrized in alignment profiles although using the fastest\navailable alignment method, namely MMseqs2 [ 36]\nwhich already can reach speed-up values of 100-times\nover PSI-BLAST [ 33]. For those who do not have\nenough resources for running MMSeqs2 and therefore\nhave to rely on PSI-BLAST, the speed-up of our predic-\ntion becomes 10,000-fold. Even the 100-fold speed-up is\nso substantial that for some applications, the speedup\nmight outweigh the reduction in performance. Embed-\nding based approaches such as SeqVec suggest a promis-\ning solution toward solving one of the biggest challenges\nfor computational biology: how to efficiently handle the\nexponentially increasing number of sequences in protein\ndatabases? Here, we showed that relevant information\nfrom large unannotated biological databases can be\ncompressed into embeddings that condense and abstract\nthe underlying biophysical principles. These embeddings,\nessentially the weights of a neural network, help as input\nto many problems for which smaller sets of annotated\ndata are available (secondary structure, disorder,\nlocalization). Although the compression step needed to\nbuild the SeqVec model is very GPU-intensive, it can be\nperformed in a centralized way using large clusters. After\ntraining, the model can be shipped and used on any\nconsumer hardware. Such solutions are ideal to support\nresearches without access to expensive cluster\ninfrastructure.\nModeling the language of life?\nSeqVec, our pre-trained ELMo adaption, learned to\nmodel a probability distribution over a protein sequence.\nThe sum over this probability distribution constituted a\nvery informative input vector for any machine learning\ntask trying to predict protein features. It also picked up\ncontext-dependent protein motifs without explicitly\nexplaining what these motifs are relevant for. In con-\ntrast, context-independent tools such as ProtVec [42]\nwill always create the same vectors regardless of the resi-\ndues surrounding this k-mer in a protein sequence.\nOur hypothesis had been that the ELMo-based SeqVec\nembeddings trained on large databases of un-annotated\nprotein sequences could extract a probabilistic model of\nthe language of life in the sense that the resulting system\nwill extract aspects relevant both for per-residue and\nper-protein prediction tasks. All results presented here\nhave added independent evidence in full support of this\nhypothesis. For instance, the three state per-residue\naccuracy for secondary structure prediction improved by\nover eight percentage points through ELMo (Table 1,\ne.g. Q3: 79.1 vs. 70.3%), the per-residue MCC for protein\ndisorder prediction also increased substantially (Table 1,\ne.g. MCC: 0.591 vs. 0.488). On the per-protein level, the\nimprovement over the previously popular tool extracting\n“meaning” from proteins, ProtVec, was even more sub-\nstantial (Table 1: e.g. Q10: 68% vs. 42%). We could dem-\nonstrate this reality even more directly using the t-SNE\n[53] results (Fig. 2 and Fig. 3): different levels of com-\nplexity ranging from single amino acids, over some lo-\ncalizations, structural features, functions and the\nclassification of membrane/non-membrane had been im-\nplicitly learned by SeqVec without training. Clearly, our\nELMo-driven implementation of transfer-learning fully\nsucceeded to model some aspects of the language of life\nas proxied by protein sequences. How much more will\nbe possible? Time will tell.\nConclusion\nWe have shown that it is possible to capture and transfer\nknowledge, e.g. biochemical or biophysical properties,\nfrom a large unlabeled data set of protein sequences to\nsmaller, labelled data sets. In this first proof-of-principle,\nour comparably simple models have already reached\npromising performance for a variety of per-residue and\nper-protein prediction tasks obtainable from only single\nprotein sequences as input, that is: without any direct\nevolutionary information, i.e. without profiles from\nmultiple sequence alignments of protein families. This\nreduces the dependence on the time-consuming and\ncomputationally intensive calculation of protein profiles,\nallowing the prediction of per-residue and per-protein\nfeatures of a whole proteome within less than an hour.\nFor instance, on a single GeForce GTX 1080, the cre-\nation of embeddings and predictions of secondary struc-\nture and subcellular localization for the whole human\nproteome took about 32 min. Building more sophisti-\ncated architectures on top of SeqVec might increase\nsequence-based performance further.\nOur new SeqVec embeddings may constitute an ideal\nstarting point for many different applications in particu-\nlar when labelled data are limited. The embeddings\ncombined with evolutionary information might even\nimprove over the best available methods, i.e. enable\nhigh-quality predictions. Alternatively, they might ease\nhigh-throughput predictions of whole proteomes when\nused as the only input feature. Alignment-free predic-\ntions bring speed and improvements for proteins for\nwhich alignments are not readily available or limited,\nsuch as for intrinsically disordered proteins, for the Dark\nProteome, or for particular unique inventions of evolu-\ntion. The trick was to tap into the potential of Deep\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 10 of 17\nLearning through transfer learning from large repositor-\nies of unlabeled data by modeling the language of life.\nMethods\nData\nUniRef50 training of SeqVec: We trained ELMo on\nUniRef50 [ 32], a sequence redundancy-reduced subset of\nthe UniProt database clustered at 50% pairwise sequence\nidentity (PIDE). It contained 25 different letters (20\nstandard and 2 rare amino acids (U and O) plus 3\nspecial cases describing either ambiguous (B, Z) or un-\nknown amino acids (X); Additional file 1: Table S1) from\n33 M proteins with 9,577,889,953 residues. In order to\ntrain ELMo, each protein was treated as a sentence and\neach amino acid was interpreted as a single word.\nVisualization of embedding space: The current release\nof the “Structural Classification Of Proteins ” (SCOPe,\n[54]) database (2.07) contains 14,323 proteins at a re-\ndundancy level of 40%. Functions encoded by the En-\nzyme Commission number (E.C., [ 60]) were retrieved via\nthe “Structure Integration with Function, Taxonomy and\nSequence” (SIFTS) mapping [ 61]. SIFTS allows, among\nother things, a residue-level mapping between UniProt\nand PDB entries and a mapping from PDB identifiers to\nE.C.s. If no function annotation was available for a pro-\ntein or if the same PDB identifier was assigned to mul-\ntiple E.C.s, it was removed from Fig. 3c. Taxonomic\nidentifiers from UniProt were used to map proteins to\none of the 3 kingdoms of life or to viruses. Again, pro-\nteins were removed if no such information was available.\nThe number of iterations for the t-SNE projections was\nset again to 3000 and the perplexity was adjusted (per-\nplexity = 5 for Fig. 3a and perplexity = 30 for Fig. 3b-d).\nPer-residue level: secondary structure & intrinsic dis-\norder ( NetSurfP-2.0). To simplify comparability, we used\nthe data set published with a recent method seemingly\nachieving the top performance of the day in secondary\nstructure prediction, namely NetSurfP-2.0 [46]. Perform-\nance values for the same data set exist also for other re-\ncent methods such as Spider3 [62], RaptorX [63, 64] and\nJPred4 [65]. The set contains 10,837 sequence-unique (at\n25% PIDE) proteins of experimentally known 3D struc-\ntures from the PDB [ 66] with a resolution of 2.5 Å (0.25\nnm) or better, collected by the PISCES server [ 67]. DSSP\n[68] assigned secondary structure and intrinsically\ndisordered residues are flagged (residues without atomic\ncoordinates, i.e. REMARK-465 in the PDB file). The ori-\nginal seven DSSP states (+ 1 for unknown) were mapped\nupon three states using the common convention: [G,H,\nI] → H (helix), [B,E] → E (strand), all others to O (other;\noften misleadingly referred to as coil or loop). As the au-\nthors of NetSurfP-2.0 did not include the raw protein se-\nquences in their public data set, we used the SIFTS file\nto obtain the original sequence. Only proteins with\nidentical length in SIFTS and NetSurfP-2.0 were used.\nThis filtering step removed 56 sequences from the train-\ning set and three from the test sets (see below: two from\nCB513, one from CASP12 and none from TS115). We\nrandomly selected 536 (~ 5%) proteins for early stopping\n(cross-training), leaving 10,256 proteins for training. All\npublished values referred to the following three test sets\n(also referred to as validation set): TS115 [69]: 115 pro-\nteins from high-quality structures (< 3 Å) released after\n2015 (and at most 30% PIDE to any protein of known\nstructure in the PDB at the time); CB513 [70]: 513 non-\nredundant sequences compiled 20 years ago (511 after\nSIFTS mapping); CASP12 [71]: 21 proteins taken from\nthe CASP12 free-modelling targets (20 after SIFTS map-\nping; all 21 fulfilled a stricter criterion toward non-\nredundancy than the two other sets; non-redundant with\nrespect to all 3D structures known until May 2018 and\nall their relatives). Each of these sets covers different as-\npects of the secondary structure prediction problem:\nCB513 and TS115 only use structures determined by X-\nray crystallography and apply similar cutoffs with respect\nto redundancy (30%) and resolution (2.5 – 3.0 Å). While\nthese serve as a good proxy for a baseline performance,\nCASP12 might better reflect the true generalization\ncapability for unseen proteins as it includes structures\ndetermined via NMR and Cryo-EM. Also, the strict re-\ndundancy reduction based on publication date reduces\nthe bias towards well studied families. Nevertheless, to-\nward our objective of establishing a proof-of-principle,\nthese sets sufficed. All test sets had fewer than 25% PIDE\nto any protein used for training and cross-training\n(ascertained by the NetSurfP-2.0 authors). To compare\nmethods using evolutionary information and those using\nour new word embeddings, we took the HHblits profiles\npublished along with the NetSurfP-2.0 data set.\nPer-protein level: subcellular localization & membrane\nproteins (DeepLoc). Subcellular localization prediction\nwas trained and evaluated using the DeepLoc data set\n[47] for which performance was measured for several\nmethods, namely: LocTree2 [ 72], MultiLoc2 [ 73], Sher-\nLoc2 [ 74], CELLO [ 75], iLoc-Euk [ 52], WoLF PSORT\n[76] and YLoc [ 77]. The data set contained proteins\nfrom UniProtKB/Swiss-Prot [ 78] (release: 2016_04) with\nexperimental annotation (code: ECO:0000269). The Dee-\npLoc authors mapped these annotations to ten classes,\nremoving all proteins with multiple annotations. All\nthese proteins were also classified into water-soluble or\nmembrane-bound (or as unknown if the annotation was\nambiguous). The resulting 13,858 proteins were clus-\ntered through PSI-CD-HIT [ 79, 80] (version 4.0; at 30%\nPIDE or Eval< 10 − 6). Adding the requirement that the\nalignment had to cover 80% of the shorter protein,\nyielded 8464 clusters. This set was split into training and\ntesting by using the same proteins for testing as the\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 11 of 17\nauthors of DeepLoc. The training set was randomly sub-\ndivided into 90% for training and 10% for determining\nearly stopping (cross-training set).\nEmbedding terminology and related work\nOne-hot encoding (also known as sparse encoding ) as-\nsigns each word (referred to as token in NLP) in the vo-\ncabulary an integer N used as the Nth component of a\nvector with the dimension of the vocabulary size (num-\nber of different words). Each component is binary, i.e. ei-\nther 0 if the word is not present in a sentence/text or 1\nif it is. This encoding drove the first application of ma-\nchine learning that clearly improved over all other\nmethods in protein prediction [ 1– 3]. TF-IDF represents\ntokens as the product of “frequency of token in data set ”\ntimes “inverse frequency of token in document ”.\nThereby, rare tokens become more relevant than com-\nmon words such as “the” (so called stop words ). This\nconcept resembles that of using k-mers for database\nsearches [ 33], clustering [ 81], motifs [ 82, 83], and predic-\ntion methods [ 72, 76, 84– 88]. Context-insensitive word\nembeddings replaced expert features, such as TF-IDF, by\nalgorithms that extracted such knowledge automatically\nfrom unlabeled corpus such as Wikipedia, by either pre-\ndicting the neighboring words, given the center word\n(skip-gram) or vice versa (CBOW). This became known\nin Word2Vec [43] and showcased for computational\nbiology through ProtVec [43, 89]. ProtVec assumes that\nevery token or word consists of three consecutive resi-\ndues (amino acid 3-mers). During training, each protein\nsequence in SwissProt [78] is split into overlapping 3-\nmers and the skip-gram version of word2vec is used to\npredict adjacent 3-mers, given the 3-mer at the center.\nAfter training, protein sequences can be split into\noverlapping 3-mers which are mapped onto a 100-\ndimensional latent space. More specialized implementa-\ntions are mut2vec [90] learning mutations in cancer, and\nphoscontext2vec [91] identifying phosphorylation sites.\nEven though the performance of context-insensitive\napproaches was pushed to its limits by adding sub-word\ninformation (FastText [ 92]) or global statistics on word\nco-occurance (GloVe [ 93]), their expressiveness\nremained limited because the models inherently assigned\nthe same vector to the same word, regardless of its con-\ntext. Context-sensitive word embeddings started a new\nwave of word embedding techniques for NLP in 2018:\nthe embedding renders the meaning of words and\nphrases such as “paper tiger ” dependent upon the con-\ntext, allowing to account for the ambiguous meanings of\nwords. Popular examples like ELMo [ 41] and Bert [ 57]\nhave achieved state-of-the-art results in several NLP\ntasks. Both require substantial GPU computing power\nand time to be trained from scratch. One of the main\ndifferences between ELMo and Bert is their pre-training\nobjective: while auto-regressive models like ELMo pre-\ndict the next word in a sentence given all previous\nwords, autoencoder-based models like Bert predict\nmasked-out words given all words which were not\nmasked out. However, in this work we focused on ELMo\nas it allows processing of sequences of variable length.\nThe original ELMo model consists of a single, context-\ninsensitive CharCNN [ 94] over the characters in a word\nand two layers of bidirectional LSTMs that introduce\nthe context information of surrounding words (Fig. 4).\nThe CharCNN transforms all characters within a single\nword via an embedding layer into vector space and runs\nmultiple CNNs of varying window size (here: ranging\nfrom 1 to 7) and number of filters (here: 32, 64, … ,\n1024). In order to obtain a fixed-dimensional vector for\neach word, regardless of its length, the output of the\nCNNs is max-pooled and concatenated. This feature is\ncrucial for NLP in order to be able to process words of\nvariable length. As our words consist only of single\namino acids, this layer learns an uncontextualized map-\nping of single amino acids onto a latent space. The first\nbi-directional LSTM operates directly on the output of\nthe CharCNN, while the second LSTM layer takes the\noutput of the first LSTM as input. Due to their sequen-\ntial nature, the LSTM layers render the embeddings\ndependent on their context as their internal state always\ndepends on the previous hidden state. However, the\nbidirectionality of the LSTMs would lead to information\nleakage, rendering the training objective trivial, i.e. the\nbackward pass had already seen the word which needs\nto be predicted in the forward pass. This problem is\nsolved by training the forward and the backward pass of\nthe LSTMs independently, i.e. the forward pass is condi-\ntioned only on words to its left and vice versa. During\ninference the internal states of both directions are\nconcatenated allowing the final embeddings to carry in-\nformation from both sides of the context. As described\nin the original ELMo publication, the weights of the for-\nward and the backward model are shared in order to re-\nduce the memory overhead of the model and to combat\noverfitting. Even though, the risk of overfitting is small\ndue to the high imbalance between number of trainable\nparameters (93 M) versus number of tokens (9.3B), drop-\nout at a rate of 10% was used to reduce the risk of over-\nfitting. This model is trained to predict the next amino\nacid given all previous amino acids in a protein se-\nquence. To the best of our knowledge, the context-\nsensitive ELMo has not been adapted to protein\nsequences, yet.\nELMo adaptation\nIn order to adapt ELMo [ 41] to protein sequences, we\nused the standard ELMo configuration with the follow-\ning changes: (i) reduction to 28 tokens (20 standard and\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 12 of 17\n2 rare (U,O) amino acids + 3 special tokens describing\nambiguous (B,Z) or unknown (X) amino acids + 3 spe-\ncial tokens for ELMo indicating padded elements ( ‘<\nMASK>’) or the beginning ( ‘<S>’) or the end of a se-\nquence ( ‘</S>’)), (ii) increase number of unroll steps to\n100 to account for the increased length of protein se-\nquences compared to sentences in natural languages,\n(iii) decrease number of negative samples to 20, (iv) in-\ncrease token number to 9,577,889,953. After pre-training\nthe ELMo architecture (1 CharCNN, 2 LSTM-Layers,\nsee “Embedding terminology and related work ” section\nand Fig. 4 for more details) with our parameters on Uni-\nRef50, the embedding model takes a protein sequence of\narbitrary length and returns 3076 features for each resi-\ndue in the sequence. These 3076 features were derived\nby concatenating the outputs of the three layers of\nELMo, each describing a token with a vector of length\n1024. The LSTM layers were composed of the embed-\nding of the forward pass (first 512 dimensions) and the\nbackward pass (last 512 dimensions). In order to demon-\nstrate the general applicability of ELMo or SeqVec and\nto allow for easy integration into existing models, we\nneither fine-tuned the pre-trained model on a specific\nprediction task, nor optimized the combination of the\nthree internal layers. Thus, researchers could just replace\n(or concatenate) their current machine learning inputs\nwith our embeddings to boost their task-specific\nperformance. Furthermore, it will simplify the develop-\nment of custom models that fit other use-cases. For\nsimplicity, we summed the components of the three\n1024-dimensional vectors to form a single 1024-\ndimensional feature vector describing each residue in a\nprotein.\nUsing SeqVec for predicting protein features\nOn the per-residue level, the predictive power of the\nnew SeqVec embeddings was demonstrated by training a\nsmall two-layer Convolutional Neural Network (CNN)\nin PyTorch using a specific implementation [ 95] of the\nADAM optimizer [ 96], cross-entropy loss, a learning rate\nof 0.001 and a batch size of 128 proteins. The first layer\n(in analogy to the sequence-to-structure network of earl-\nier solutions [ 2, 3]) consisted of 32-filters each with a\nsliding window-size of w = 7. The second layer (struc-\nture-to-structure [ 2, 3]) created the final predictions by\napplying again a CNN (w = 7) over the output of the first\nlayer. These two layers were connected through a recti-\nfied linear unit (ReLU) and a dropout layer [ 97] with a\ndropout-rate of 25% (Fig. 5, left panel). This simple\narchitecture was trained independently on six different\ntypes of input, resulting in different number of free pa-\nrameters. (i) DeepProf (14,000 = 14 k free parameters):\nEach residue was described by a vector of size 50 which\nincluded a one-hot encoding (20 features), the profiles of\nevolutionary information (20 features) from HHblits as\npublished previously [ 46], the state transition probabil-\nities of the Hidden-Markov-Model (7 features) and 3\nfeatures describing the local alignment diversity. (ii)\nDeepSeqVec (232 k free parameters): Each protein se-\nquence was represented by the output of SeqVec. The\nFig. 4 ELMo-based architecture adopted for SeqVec. First, an input sequence, e.g. “SEQWENCE ” (shown at bottom row), is padded with\nspecial tokens indicating the start ( “<start>”) and the end ( “<end>”) of the sentence (here: protein sequences). On the 2nd level (2nd row from\nbottom), character convolutions (CharCNN, [ 94]) map each word (here: amino acid) onto a fixed-length latent space (here: 1024-dimensional)\nwithout considering information from neighboring words. On the third level (3rd row from bottom), the output of the CharCNN-layer is used as\ninput by a bidirectional Long Short Term Memory (LSTM, [ 45]) which introduces context-specific information by processing the sentence (protein\nsequence) sequentially. For simplicity, only the forward pass of the bi-directional LSTM-layer is shown (here: 512-dimensional). On the fourth lev el\n(4th row from bottom), the second LSTM-layer operates directly on the output of the first LSTM-layer and tries to predict the next word given all\nprevious words in a sentence. The forward and backward pass are optimized independently during training in order to avoid information leakage\nbetween the two directions. During inference, the hidden states of the forward and backward pass of each LSTM-layer are concatenated to a\n1024-dimensional embedding vector summarizing information from the left and the right context\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 13 of 17\nresulting embedding described each residue as a 1024-\ndimensional vector. (iii) DeepProf+SeqVec (244 k free\nparameters): This model simply concatenated the input\nvectors used in (i) and (ii). (iv) DeepProtVec (25 k free\nparameters): Each sequence was split into overlapping 3-\nmers each represented by a 100-dimensional ProtVec\n[42]. (v) DeepOneHot (7 k free parameters): The 20\namino acids were encoded as one-hot vectors as de-\nscribed above. Rare amino acids were mapped to vectors\nwith all components set to 0. Consequently, each protein\nresidue was encoded as a 20-dimensional one-hot vector.\n(vi) DeepBLOSUM65 (8 k free parameters): Each protein\nresidue was encoded by its BLOSUM65 substitution\nmatrix [ 98]. In addition to the 20 standard amino acids,\nBLOSUM65 also contains substitution scores for the\nspecial cases B, Z (ambiguous) and X (unknown), result-\ning in a feature vector of length 23 for each residue.\nOn the per-protein level, a simple feed-forward neural\nnetwork was used to demonstrate the power of the new\nembeddings. In order to ensure equal-sized input vectors\nfor all proteins, we averaged over the 1024-dimensional\nembeddings of all residues in a given protein resulting in\na 1024-dimensional vector representing any protein in\nthe data set. ProtVec representations were derived the\nsame way, resulting in a 100-dimensional vector. These\nvectors (either 100-or 1024 dimensional) were first com-\npressed to 32 features, then dropout with a dropout rate\nof 25%, batch normalization [ 99] and a rectified linear\nUnit (ReLU) were applied before the final prediction\n(Fig. 5, right panel). In the following, we refer to the\nmodels trained on the two different input types as (i)\nDeepSeqVec-Loc (33 k free parameters): average over\nSeqVec embedding of a protein as described above and\n(ii) DeepProtVec-Loc (320 free parameters): average over\nProtVec embedding of a protein. We used the following\nhyper-parameters: learning rate: 0.001, Adam optimizer\nwith cross-entropy loss, batch size: 64. The losses of the\nindividual tasks were summed before backpropagation.\nDue to the relatively small number of free parameters in\nour models, the training of all networks completed on a\nsingle Nvidia GeForce GTX1080 within a few minutes\n(11 s for DeepProtVec-Loc, 15 min for DeepSeqVec).\nEvaluation measures\nTo simplify comparisons, we ported the evaluation mea-\nsures from the publications we derived our data sets\nfrom, i.e. those used to develop NetSurfP-2.0 [46] and\nDeepLoc [47]. All numbers reported constituted averages\nover all proteins in the final test sets. This work aimed\nat a proof-of-principle that the SeqVec embedding con-\ntain predictive information. In the absence of any claim\nfor state-of-the-art performance, we did not calculate\nany significance values for the reported values.\nPer-residue performance: Toward this end, we used\nthe standard three-state per-residue accuracy (Q3 = per-\ncentage correctly predicted in either helix, strand, other\n[2]) along with its eight-state analog (Q8). Predictions of\nintrinsic disorder were evaluated through the Matthew ’s\ncorrelation coefficient (MCC [ 100]) and the False-\nPositive Rate (FPR) as those are more informative for\ntasks with high class imbalance. For completeness, we\nalso provided the entire confusion matrices for both sec-\nondary structure prediction problems (Additional file 1:\nFigure S2). Standard errors were calculated over the dis-\ntribution of each performance measure for all proteins.\nFig. 5 Prediction tasks ’architectures. On the left the architecture of the model used for the per-residue level predictions (secondary structure and\ndisorder) is sketched, on the right that used for per-protein level predictions (localization and membrane/not membrane). The ‘X’, on the left,\nindicates that different input features corresponded to a difference in the number of input channels, e.g. 1024 for SeqVec or 50 for profile-based\ninput. The letter ‘W’refers to the window size of the corresponding convolutional layer (W = 7 implies a convolution of size 7 × 1)\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 14 of 17\nPer-protein performance: The predictions whether a\nprotein was membrane-bound or water-soluble were\nevaluated by calculating the two-state per set accuracy\n(Q2: percentage of proteins correctly predicted), and the\nMCC. A generalized MCC using the Gorodkin measure\n[101] for K (=10) categories as well as accuracy (Q10),\nwas used to evaluate localization predictions. Standard\nerrors were calculated using 1000 bootstrap samples,\neach chosen randomly by selecting a sub-set of the pre-\ndicted test set that had the same size (draw with\nreplacement).\nSupplementary information\nSupplementary information accompanies this paper at https://doi.org/10.\n1186/s12859-019-3220-8.\nAdditional file 1: Supporting online material (SOM) for: Modeling\naspect of the language of life through transfer-learning protein se-\nquences Figure 1. ELMo perplexity Figure 2. Confusion matrices for per-\nprotein predictions using DeepSeqVec-Loc Figure 3. Confusion matrices\nfor secondary structure predictions of DeepSeqVec Figure 4. Comparison\nof secondary structure prediction performance (Q3) between Netsurfp-2.0\nand DeepSeqVec Table S1. Amino acid occurrences in UniRef50\nAbbreviations\n1D: One-dimensional – information representable in a string such as\nsecondary structure or solvent accessibility; 3D structure: Three-dimensional\ncoordinates of protein structure; 3D: Three-dimensional; ELMo: Embeddings\nfrom Language Models; MCC: Matthews-Correlation-Coefficient;\nMSA: Multiple sequence alignment; ProtVec: Context-independent\nembeddings from Word2vec-type approaches; Q10: Ten-state localization\nper-protein accuracy; Q3: Three-state secondary structure per-residue accur-\nacy; Q8: Eight-state secondary structure per-residue accuracy; RSA: Relative\nsolvent accessibility; SE: Standard error; SeqVec: embeddings introduced\nhere, extracted by modeling un-annotated UniRef50 protein sequences with\nELMo\nAcknowledgements\nThe authors thank primarily Tim Karl for invaluable help with hardware and\nsoftware and Inga Weise for support with many other aspects of this work.\nLast, not least, thanks to all those who deposit their experimental data in\npublic databases, and to those who maintain these databases.\nAuthors contributions\nAE and MH suggested to use ELMo for modeling protein sequences. AE\nadopted and trained ELMo. MH evaluated SeqVec embeddings on different\ndata sets and tasks. YW helped with discussions about natural language\nprocessing. CD implemented the web-interface which allows to access and\nvisualize the predictions and helped to improve the manuscript. DN helped\nwith various problems regarding the code. FM and BR helped with the de-\nsign of the experiment and to critically improve the manuscript. MH and AE\ndrafted the manuscript and the other authors provided feedback. All authors\nread and approved the final manuscript.\nFunding\nThis work was supported by a grant from the Alexander von Humboldt\nfoundation through the German Ministry for Research and Education (BMBF:\nBundesministerium fuer Bildung und Forschung) as well as by a grant from\nDeutsche Forschungsgemeinschaft (DFG –GZ: RO1320/4 –1). We gratefully\nacknowledge the support of NVIDIA Corporation with the donation of two\nTitan GPU used for this research. We also want to thank the LRZ (Leibniz\nRechenzentrum) for providing us access to DGX-V1.\nThe funding did not play any role in the design of the study, collection,\nanalysis, and interpretation of data and in writing the manuscript.\nAvailability of data and materials\nThe pre-trained ELMo-based SeqVec model and a description on how to im-\nplement the embeddings into existing methods can be found here: https://\ngithub.com/Rostlab/SeqVec . Accessed 2nd May 2019.\nPredictions on secondary structure, disorder and subcellular localization\nbased on SeqVec can be accessed under: https://embed.protein.properties .\nAccessed 2nd May 2019.\nThe NetSurfP-2.0 data set [ 46] used for the evaluation of SeqVec on the task\nof secondary structure and disorder prediction are publicly available under:\nhttp://www.cbs.dtu.dk/services/NetSurfP/ . Accessed 2nd May 2019.\nThe DeepLoc data set [ 47] used for the evaluation of SeqVec on the task of\nsubcellular localization prediction are publicly available under: http://www.\ncbs.dtu.dk/services/DeepLoc/data.php . Accessed 2nd May 2019.\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1Department of Informatics, Bioinformatics & Computational Biology - i12,\nTUM (Technical University of Munich), Boltzmannstr. 3, 85748 Garching/\nMunich, Germany. 2TUM Graduate School, Center of Doctoral Studies in\nInformatics and its Applications (CeDoSIA), Boltzmannstr. 11, 85748 Garching,\nGermany. 3Leibniz Supercomputing Centre, Boltzmannstr. 1, 85748 Garching/\nMunich, Germany. 4TUM Department of Informatics, Software Engineering\nand Business Information Systems, Boltzmannstr. 1, 85748 Garching/Munich,\nGermany. 5Institute for Advanced Study (TUM-IAS), Lichtenbergstr. 2a, 85748\nGarching/Munich, Germany. 6TUM School of Life Sciences Weihenstephan\n(WZW), Alte Akademie 8, Freising, Germany. 7Department of Biochemistry\nand Molecular Biophysics & New York Consortium on Membrane Protein\nStructure (NYCOMPS), Columbia University, 701 West, 168th Street, New York,\nNY 10032, USA.\nReceived: 3 May 2019 Accepted: 13 November 2019\nReferences\n1. Rost B, Sander C. Jury returns on structure prediction. Nat. 1992;360:540.\n2. Rost B, Sander C. Prediction of protein secondary structure at better than\n70% accuracy. J Mol Biol. 1993;232:584 –99.\n3. Rost B, Sander C. Improved prediction of protein secondary structure\nby use of sequence profiles and neural networks. Proc Natl Acad Sci.\n1993;90:7558 – 62.\n4. Barton GJ. Protein secondary structure prediction. Curr Opin Struct Biol.\n1995;5:372–6.\n5. Chandonia J-M, Karplus M. Neural networks for secondary structure and\nstructural class predictions. Protein Sci. 1995;4:275 –85.\n6. Mehta PK, Heringa J, Argos P. A simple and fast approach to prediction of\nprotein secondary structure from multiply aligned sequences with accuracy\nabove 70%. Protein Sci. 1995;4:2517 –25.\n7. Rost B, Sander C. Combining evolutionary information and neural networks\nto predict protein secondary structure. Proteins Struct Funct Genet.\n1994;19:55–72.\n8. Solovyev VV, Salamov AA. Predicting a-helix and b-strand segments of\nglobular proteins. Comput Appl Biol Sci. 1994;10:661 –9.\n9. Frishman D, Argos P. Knowledge-based protein secondary structure\nassignment. Proteins Struct Funct Genet. 1995;23:566 –79.\n10. Jones DT. Protein secondary structure prediction based on position-specific\nscoring matrices. J Mol Biol. 1999;292(2):195 –202.\n11. Bigelow H, Petrey D, Liu J, Przybylski D, Rost B. Predicting transmembrane\nbeta-barrels in proteomes. Nucleic Acids Res. 2004;32:2566 –77.\n12. Rost B, Casadio R, Fariselli P. Topology prediction for helical transmembrane\nproteins at 86% accuracy. Protein Sci. 1996;5:1704 –18.\n13. Rost B, Casadio R, Fariselli P, Sander C. Transmembrane helix prediction at\n95% accuracy. Protein Sci. 1995;4:521 –33.\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 15 of 17\n14. Rost B, Sander C. Conservation and prediction of solvent accessibility in\nprotein families. Proteins Struct Funct Genet. 1994;20(3):216 –26.\n15. Radivojac P, Obradovic Z, Smith DK, Zhu G, Vucetic S, Brown CJ, Lawson JD,\nDunker AK. Protein flexibility and intrinsic disorder. Protein Sci. 2004;13:71 –80.\n16. Schlessinger A, Rost B. Protein flexibility and rigidity predicted from\nsequence. Proteins. 2005;61(1):115 –26.\n17. Punta M, Rost B. PROFcon: novel prediction of long-range contacts.\nBioinform. 2005;21(13):2960 –8.\n18. Peng K, Vucetic S, Radivojac P, Brown CJ, Dunker AK, Obradovic Z.\nOptimizing long intrinsic disorder predictors with protein evolutionary\ninformation. J Bioinforma Comput Biol. 2005;3(1):35 –60.\n19. Schlessinger A, Liu J, Rost B. Natively unstructured loops differ from other\nloops. PLoS Comput Biol. 2007;3(7):e140.\n20. Schlessinger A, Punta M, Rost B. Natively unstructured regions in proteins\nidentified from contact predictions. Bioinform. 2007;23(18):2376 –84.\n21. Nair R, Rost B. Better prediction of sub-cellular localization by combining\nevolutionary and structural information. Proteins. 2003;53(4):917 –30.\n22. Nair R, Rost B. Mimicking cellular sorting improves prediction of subcellular\nlocalization. J Mol Biol. 2005;348(1):85 –100.\n23. Marino Buslje C, Teppa E, Di Domenico T, Delfino JM, Nielsen M. Networks\nof high mutual information define the structural proximity of catalytic sites:\nimplications for catalytic residue identification. PLoS Comput Biol. 2010;\n6(11):e1000978.\n24. Ofran Y, Rost B. Protein-protein interaction hot spots carved into sequences.\nPLoS Comput Biol. 2007;3(7):e119.\n25. Ofran Y, Rost B. ISIS: interaction sites identified from sequence. Bioinform.\n2007;23(2):e13–6.\n26. Adzhubei IA, Schmidt S, Peshkin L, Ramensky VE, Gerasimova A, Bork P,\nKondrashov AS, Sunyaev SR. A method and server for predicting damaging\nmissense mutations. Nat Methods. 2010;7(4):248 –9.\n27. Bromberg Y, Rost B. SNAP: predict effect of non-synonymous\npolymorphisms on function. Nucleic Acids Res. 2007;35(11):3823 –35.\n28. Hayat S, Sander C, Marks DS, Elofsson A. All-atom 3D structure prediction of\ntransmembrane β-barrel proteins from sequences. Proc Natl Acad Sci. 2015;\n112(17):5413–8.\n29. Marks DS, Colwell LJ, Sheridan R, Hopf TA, Pagnani A, Zecchina R, Sander C.\nProtein 3D structure computed from evolutionary sequence variation. PLoS\nOne. 2011;6(12):e28766.\n30. Marks DS, Hopf TA, Sander C. Protein structure prediction from sequence\nvariation. Nat Biotechnol. 2012;30(11):1072.\n31. Morcos F, Pagnani A, Lunt B, Bertolino A, Marks DS, Sander C, Zecchina R,\nOnuchic JN, Hwa T, Weigt M. Direct-coupling analysis of residue\ncoevolution captures native contacts across many protein families. Proc Natl\nAcad Sci. 2011;108(49):E1293 –301.\n32. Suzek BE, Wang Y, Huang H, McGarvey PB, Wu CH, UniProt C. UniRef\nclusters: a comprehensive and scalable alternative for improving sequence\nsimilarity searches. Bioinform. 2015;31(6):926 –32.\n33. Altschul SF, Madden TL, Schaeffer AA, Zhang J, Zhang Z, Miller W, Lipman\nDJ. Gapped Blast and PSI-Blast: a new generation of protein database\nsearch programs. Nucleic Acids Res. 1997;25:3389 –402.\n34. Remmert M, Biegert A, Hauser A, Soding J. HHblits: lightning-fast\niterative protein sequence searching by HMM-HMM alignment. Nat\nMethods. 2012;9(2):173 –5.\n35. Steinegger M, Meier M, Mirdita M, Vohringer H, Haunsberger SJ, Soding J.\nHH-suite3 for fast remote homology detection and deep protein\nannotation. BMC Bioinform. 2019;20(1):473.\n36. Steinegger M, Söding J. MMseqs2 enables sensitive protein sequence\nsearching for the analysis of massive data sets. Nat Biotechnol. 2017;\n35(11):1026.\n37. Dunker AK, Babu MM, Barbar E, Blackledge M, Bondos SE, Dosztanyi Z,\nDyson HJ, Forman-Kay J, Fuxreiter M, Gsponer J, et al. What's in a name?\nWhy these proteins are intrinsically disordered. Intrinsically Disord Proteins.\n2013;1(1):e24157.\n38. Uversky VN, Radivojac P, Iakoucheva LM, Obradovic Z, Dunker AK. Prediction\nof intrinsic disorder and its use in functional proteomics. Methods Mol Biol.\n2007;408:69–92.\n39. Perdigao N, Heinrich J, Stolte C, Sabir KS, Buckley MJ, Tabor B, Signal B,\nGloss BS, Hammang CJ, Rost B, et al. Unexpected features of the dark\nproteome. Proc Natl Acad Sci U S A. 2015.\n40. Schafferhans A, O'Donoghue SI, Heinzinger M, Rost B. Dark proteins\nimportant for cellular function. Proteomics. 2018;18(21 –22):1800227.\n41. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L:\nDeep contextualized word representations. arXiv 2018,. https://arxiv.org/\nabs/1802.05365.\n42. Asgari E, Mofrad MR. Continuous distributed representation of biological\nsequences for deep proteomics and genomics. PLoS One. 2015;10(11):e0141287.\n43. Mikolov T, Chen K, Corrado G, Dean J: Efficient estimation of word\nrepresentations in vector space. ArXiv 2013, https://arxiv.org/abs/1301.3781.\n44. Schils E, Pd H. Characteristics of sentence length in running text. Literary\nLinguist Comput. 1993;8(1):20 –6.\n45. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput.\n1997;9(8):1735–80.\n46. Klausen MS, Jespersen MC, Nielsen H, Jensen KK, Jurtz VI, Sonderby CK,\nSommer MOA, Winther O, Nielsen M, Petersen B, et al. NetSurfP-2.0:\nImproved prediction of protein structural features by integrated deep\nlearning. Proteins. 2019.\n47. Almagro Armenteros JJ, Sonderby CK, Sonderby SK, Nielsen H, Winther O.\nDeepLoc: prediction of protein subcellular localization using deep learning.\nBioinform. 2017;33(24):4049.\n48. Anfinsen CB. Principles that govern the folding of protein chains. Sci. 1973;\n181(4096):223–30.\n49. Buchan DW, Jones DT. Improved protein contact predictions with the\nMetaPSICOV2 server in CASP12. Proteins. 2018;86:78 –83.\n50. Evans R, Jumper J, Kirkpatrick J, Sifre L, Green T, Qin C, Zidek A, Nelson A,\nBridgland A, Penedones H. De novo structure prediction with deeplearning\nbased scoring. Annu Rev Biochem. 2018;77(363 –382):6.\n51. Rives A, Goyal S, Meier J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R. Biological\nstructure and function emerge from scaling unsupervised learning to 250\nmillion protein sequences. bioRxiv. 2019:622803.\n52. Chou KC, Wu ZC, Xiao X. iLoc-Euk: a multi-label classifier for predicting the\nsubcellular localization of singleplex and multiplex eukaryotic proteins. PLoS\nOne. 2011;6(3):e18258.\n53. Lvd M, Hinton G. Visualizing data using t-SNE. J Mach Learn Res. 2008;\n9(Nov):2579–605.\n54. Fox NK, Brenner SE, Chandonia J-M. SCOPe: structural classification of\nproteins— extended, integrating SCOP and ASTRAL data and classification\nof new structures. Nucleic Acids Res. 2013;42(D1):D304 –9.\n55. Kosloff M, Kolodny R. Sequence-similar, structure-dissimilar protein pairs in\nthe PDB. Proteins. 2008;71(2):891 –902.\n56. Dai Z, Yang Z, Yang Y, Cohen WW, Carbonell J, Le QV, Salakhutdinov R:\nTransformer-xl: Attentive language models beyond a fixed-length context.\narXiv preprint arXiv:190102860 2019.\n57. Devlin J, Chang M-W, Lee K, Toutanova K: Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:\n181004805 2018.\n58. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV: XLNet:\nGeneralized Autoregressive Pretraining for Language Understanding. arXiv\npreprint arXiv:190608237 2019.\n59. AlQuraishi M. ProteinNet: a standardized data set for machine learning of\nprotein structure. BMC Bioinform. 2019;20(1):311.\n60. Bairoch A. The ENZYME database in 2000. Nucleic Acids Res. 2000;28(1):304 –5.\n61. Velankar S, Dana JM, Jacobsen J, Van Ginkel G, Gane PJ, Luo J, Oldfield TJ,\nO’donovan C, Martin M-J, Kleywegt GJ. SIFTS: structure integration with function,\ntaxonomy and sequences resource. Nucleic Acids Res. 2012;41(D1):D483–9.\n62. Heffernan R, Yang Y, Paliwal K, Zhou Y. Capturing non-local interactions by\nlong short-term memory bidirectional recurrent neural networks for\nimproving prediction of protein secondary structure, backbone angles,\ncontact numbers and solvent accessibility. Bioinform. 2017;33(18):2842 –9.\n63. Wang S, Li W, Liu S, Xu J. RaptorX-property: a web server for protein\nstructure property prediction. Nucleic Acids Res. 2016;44(W1):W430 –5.\n64. Wang S, Peng J, Ma J, Xu J. Protein secondary structure prediction using\ndeep convolutional neural fields. Sci Rep. 2016;6:18962.\n65. Drozdetskiy A, Cole C, Procter J, Barton GJ. JPred4: a protein secondary\nstructure prediction server. Nucleic Acids Res. 2015;43(W1):W389 –94.\n66. Berman HM, Westbrook J, Feng Z, Gilliland G, Bhat TN, Weissig H,\nShindyalov IN, Bourne PE. The protein data bank. Nucleic Acids Res. 2000;\n28(1):235–42.\n67. Wang G, Dunbrack RL Jr. PISCES: a protein sequence culling server.\nBioinform. 2003;19(12):1589 –91.\n68. Kabsch W, Sander C. Dictionary of protein secondary structure: pattern\nrecognition of hydrogen bonded and geometrical features. Biopolym. 1983;\n22:2577–637.\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 16 of 17\n69. Yang Y, Gao J, Wang J, Heffernan R, Hanson J, Paliwal K, Zhou Y. Sixty-five\nyears of the long march in protein secondary structure prediction: the final\nstretch? Brief Bioinform. 2016;19(3):482 –94.\n70. Cuff JA, Barton GJ. Evaluation and improvement of multiple sequence\nmethods for protein secondary structure prediction. Proteins Struct Funct\nGenet. 1999;34(4):508 –19.\n71. Abriata LA, Tamò GE, Monastyrskyy B, Kryshtafovych A, Dal Peraro M.\nAssessment of hard target modeling in CASP12 reveals an emerging role of\nalignment-based contact prediction methods. Proteins. 2018;86:97 –112.\n72. Goldberg T, Hamp T, Rost B. LocTree2 predicts localization for all domains\nof life. Bioinform. 2012;28(18):i458 –65.\n73. Blum T, Briesemeister S, Kohlbacher O. MultiLoc2: integrating phylogeny\nand gene ontology terms improves subcellular protein localization\nprediction. BMC Bioinform. 2009;10:274.\n74. Briesemeister S, Blum T, Brady S, Lam Y, Kohlbacher O, Shatkay H. SherLoc2:\na high-accuracy hybrid method for predicting subcellular localization of\nproteins. J Proteome Res. 2009;8(11):5363 –6.\n75. Yu CS, Chen YC, Lu CH, Hwang JK. Prediction of protein subcellular\nlocalization. Proteins. 2006;64(3):643 –51.\n76. Horton P, Park KJ, Obayashi T, Fujita N, Harada H, Adams-Collier CJ, Nakai K.\nWoLF PSORT: protein localization predictor. Nucleic Acids Res. 2007;35(Web\nServer issue):W585 –7.\n77. Briesemeister S, Rahnenfuhrer J, Kohlbacher O. YLoc - an interpretable web\nserver for predicting subcellular localization. Nucleic Acids Res. 2010;\n38(Suppl):W497–502.\n78. Boutet E, Lieberherr D, Tognolli M, Schneider M, Bansal P, Bridge AJ, Poux S,\nBougueleret L, Xenarios I. UniProtKB/Swiss-Prot, the manually annotated\nsection of the UniProt KnowledgeBase: how to use the entry view. Methods\nMol Biol. 2016;1374:23 –54.\n79. Fu L, Niu B, Zhu Z, Wu S, Li W. CD-HIT: accelerated for clustering the next-\ngeneration sequencing data. Bioinform. 2012;28(23):3150 –2.\n80. Li W, Godzik A. Cd-hit: a fast program for clustering and comparing large\nsets of protein or nucleotide sequences. Bioinform. 2006;22(13):1658 –9.\n81. Moussa M, Mandoiu II. Single cell RNA-seq data clustering using TF-IDF\nbased methods. BMC Genomics. 2018;19(Suppl 6):569.\n82. Bailey TL, Boden M, Buske FA, Frith M, Grant CE, Clementi L, Ren J, Li WW,\nNoble WS. MEME SUITE: tools for motif discovery and searching. Nucleic\nAcids Res. 2009;37(Web Server issue):W202 –8.\n83. Bernard G, Chan CX, Ragan MA. Alignment-free microbial phylogenomics\nunder scenarios of sequence divergence, genome rearrangement and\nlateral genetic transfer. Sci Rep. 2016;6:28970.\n84. Hamp T, Rost B. Evolutionary profiles improve protein-protein interaction\nprediction from sequence. Bioinform. 2015;31(12):1945 –50.\n85. Kuang R, Ie E, Wang K, Wang K, Siddiqi M, Freund Y, Leslie C. Profile-based\nstring kernels for remote homology detection and motif extraction. J\nBioinforma Comput Biol. 2005;3(3):527 –50.\n86. Leslie C, Eskin E, Weston J, Noble WS: Mismatch string kernels for SVM\nprotein classification. Bioinform 2003:in press.\n87. Nakai K, Horton P. PSORT: a program for detecting sorting signals in\nproteins and predicting their subcellular localization. Trends Biochem Sci.\n1999;24(1):34–6.\n88. Noble WS, Kuang R, Leslie C, Weston J. Identifying remote protein\nhomologs by network propagation. FEBS J. 2005;272(20):5119 –28.\n89. Asgari E, McHardy AC, Mofrad MRK. Probabilistic variable-length\nsegmentation of protein sequences for discriminative motif discovery\n(DiMotif) and sequence embedding (ProtVecX). Sci Rep. 2019;9(1):3577.\n90. Kim S, Lee H, Kim K, Kang J. Mut2Vec: distributed representation of\ncancerous mutations. BMC Med Genet. 2018;11(2):33.\n91. Xu Y, Song J, Wilson C, Whisstock JC. PhosContext2vec: a distributed\nrepresentation of residue-level sequence contexts and its application to\ngeneral and kinase-specific phosphorylation site prediction. Sci Rep. 2018;8.\n92. Bojanowski P, Grave E, Joulin A, Mikolov T. Enriching word vectors with\nsubword information. Trans Assoc Comput Linguist. 2017;5:135 –46.\n93. Pennington J, Socher R, Manning C: Glove: Global vectors for word\nrepresentation. In: Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP): 2014. 1532 –1543.\n94. Kim Y, Jernite Y, Sontag D, Rush AM: Character-aware neural language\nmodels. In: Thirtieth AAAI Conference on Artificial Intelligence: 2016.\n95. Reddi SJ, Kale S, Kumar S: On the convergence of adam and beyond. arXiv\npreprint arXiv:190409237 2019.\n96. Kingma DP, Ba J: Adam: A method for stochastic optimization. arXiv\npreprint arXiv:14126980 2014.\n97. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a\nsimple way to prevent neural networks from overfitting. J Mach Learn Res.\n2014;15(1):1929–58.\n98. Henikoff S, Henikoff JG. Amino acid substitution matrices from protein\nblocks. Proc Natl Acad Sci. 1992;89(22):10915 –9.\n99. Ioffe S, Szegedy C: Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. arXiv preprint arXiv:150203167 2015.\n100. Matthews BW. Comparison of the predicted and observed secondary\nstructure of T4 phage lysozyme. Biochim Biophys Acta. 1975;405:442 –51.\n101. Gorodkin J. Comparing two K-category assignments by a K-category\ncorrelation coefficient. Comput Biol Chem. 2004;28(5 –6):367–74.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nHeinzinger et al. BMC Bioinformatics          (2019) 20:723 Page 17 of 17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6561181545257568
    },
    {
      "name": "Human proteome project",
      "score": 0.5259706377983093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5225260257720947
    },
    {
      "name": "Protein sequencing",
      "score": 0.512712836265564
    },
    {
      "name": "Word2vec",
      "score": 0.48212766647338867
    },
    {
      "name": "Proteome",
      "score": 0.4420834183692932
    },
    {
      "name": "Protein function prediction",
      "score": 0.4390898048877716
    },
    {
      "name": "Computational biology",
      "score": 0.430581271648407
    },
    {
      "name": "Machine learning",
      "score": 0.41625869274139404
    },
    {
      "name": "Theoretical computer science",
      "score": 0.37020984292030334
    },
    {
      "name": "Natural language processing",
      "score": 0.35344260931015015
    },
    {
      "name": "Proteomics",
      "score": 0.3222426772117615
    },
    {
      "name": "Bioinformatics",
      "score": 0.2854289412498474
    },
    {
      "name": "Biology",
      "score": 0.27320992946624756
    },
    {
      "name": "Protein function",
      "score": 0.22516494989395142
    },
    {
      "name": "Peptide sequence",
      "score": 0.1962217390537262
    },
    {
      "name": "Gene",
      "score": 0.19044476747512817
    },
    {
      "name": "Embedding",
      "score": 0.1781333088874817
    },
    {
      "name": "Genetics",
      "score": 0.144578218460083
    }
  ]
}