{
  "title": "JuMP: A Modeling Language for Mathematical Optimization",
  "url": "https://openalex.org/W1918816242",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A4298489078",
      "name": "Dunning, Iain",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280926898",
      "name": "Huchette, Joey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744974929",
      "name": "Lubin, Miles",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2107979247",
    "https://openalex.org/W2140007372",
    "https://openalex.org/W2131116400",
    "https://openalex.org/W2400360587",
    "https://openalex.org/W1036579647",
    "https://openalex.org/W2120575449",
    "https://openalex.org/W2000587848",
    "https://openalex.org/W2064310338",
    "https://openalex.org/W2146534684",
    "https://openalex.org/W2147811035",
    "https://openalex.org/W2098412432",
    "https://openalex.org/W4232598359",
    "https://openalex.org/W2127910971",
    "https://openalex.org/W2133400152",
    "https://openalex.org/W1996092920",
    "https://openalex.org/W1973668402",
    "https://openalex.org/W2007930753",
    "https://openalex.org/W2279469651",
    "https://openalex.org/W1964815907",
    "https://openalex.org/W2525247567",
    "https://openalex.org/W2105235982",
    "https://openalex.org/W2086393803",
    "https://openalex.org/W2105198154",
    "https://openalex.org/W1858422019",
    "https://openalex.org/W2102323164",
    "https://openalex.org/W2066754358",
    "https://openalex.org/W2014078287",
    "https://openalex.org/W2112633280",
    "https://openalex.org/W2041085006",
    "https://openalex.org/W2028713366",
    "https://openalex.org/W2135653217",
    "https://openalex.org/W2123871098",
    "https://openalex.org/W2161090654",
    "https://openalex.org/W2156150807"
  ],
  "abstract": "JuMP is an open-source modeling language that allows users to express a wide\\nrange of optimization problems (linear, mixed-integer, quadratic,\\nconic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic\\nsyntax. JuMP takes advantage of advanced features of the Julia programming\\nlanguage to offer unique functionality while achieving performance on par with\\ncommercial modeling tools for standard tasks. In this work we will provide\\nbenchmarks, present the novel aspects of the implementation, and discuss how\\nJuMP can be extended to new problem classes and composed with state-of-the-art\\ntools for visualization and interactivity.\\n",
  "full_text": "arXiv:1508.01982v3  [math.OC]  15 Aug 2016\nJuMP: A MODELING LANGUAGE FOR MATHEMATICAL\nOPTIMIZATION\nIAIN DUNNING, JOEY HUCHETTE, MILES LUBIN ∗\nAbstract. JuMP is an open-source modeling language that allows users t o express a wide\nrange of optimization problems (linear, mixed-integer, qu adratic, conic-quadratic, semideﬁnite, and\nnonlinear) in a high-level, algebraic syntax. JuMP takes ad vantage of advanced features of the\nJulia programming language to oﬀer unique functionality wh ile achieving performance on par with\ncommercial modeling tools for standard tasks. In this work w e will provide benchmarks, present the\nnovel aspects of the implementation, and discuss how JuMP ca n be extended to new problem classes\nand composed with state-of-the-art tools for visualizatio n and interactivity.\nKey words. algebraic modeling languages, automatic diﬀerentiation, scientiﬁc computing\nAMS subject classiﬁcations. 90C04, 90C05, 90C06, 90C30, 65D25\n1. Introduction. William Orchard-Hays, who developed some of the ﬁrst soft-\nware for linear programming (LP) in collaboration with George Dantzig , observed\nthat the ﬁeld of mathematical optimization developed hand-in-hand with the ﬁeld of\ncomputing [ 65]. Beginning with the introduction of IBM’s ﬁrst commercial scientiﬁc\ncomputer in 1952, advancements in technology were immediately put to use for solv-\ning military and industrial planning problems. LP software was viewed a s generally\nreliable by the 1970s, when mainframe computers had become mainst ream. However,\ndevelopers of these systems recognized that the diﬃculty of tran slating the complex\nmathematical formulation of a problem into the requisite input forma ts based on\npunch cards was a major barrier to adoption [ 27].\nIn the late 1970s, the ﬁrst algebraic modeling languages (AMLs) wer e developed\nwith the aim of allowing users to express LP and other optimization pro blems in a\nnatural, algebraic form similar to the original mathematical express ions, much in the\nsame way that MATLAB was created contemporaneously to provide a high-level in-\nterface to linear algebra. Similar to how MATLAB translated user inpu t into calls\nto LINPACK [ 23], AMLs do not solve optimization problems; they provide the prob-\nlems to optimization routines called solvers. GAMS [ 17] and AMPL [ 29], two well-\nknown commercial AMLs whose development started in 1978 and 198 5 respectively,\nare widely recognized among similar systems like AIMMS, LINDO/LINGO , and MPL\nas having made a signiﬁcant impact on the adoption of mathematical o ptimization in\na number of ﬁelds.\nIn this paper, we present JuMP, an AML which is embedded in the Julia p rogram-\nming language [ 10]. In addition to providing a performant open-source alternative to\ncommercial systems, JuMP has come to deliver signiﬁcant advances in modeling and\nextensibility by taking advantage of a number of features of Julia wh ich are unique\nwithin the realm of programming languages for scientiﬁc computing. W e highlight\nthe novel technical aspects of JuMP’s implementation in suﬃcient ge nerality to apply\nbroadly beyond the context of AMLs, in particular for the implement ation of scientiﬁc\ndomain-speciﬁc languages [ 74, 75, 2] and of automatic diﬀerentiation (AD) techniques\nfor eﬃcient computations of derivatives [ 41, 63].\nTo date, AMPL, GAMS, and similar commercial packages represent t he state of\nthe art in AMLs and are widely used in both academia and industry. The se AMLs are\n∗ MIT Operations Research Center {idunning,huchette,mlubin}@mit.edu.\n1\n2 Dunning et al.\nquite eﬃcient at what they were designed for; however, a number o f drawbacks mo-\ntivated us to develop a new AML. Unsatisﬁed with relatively standalon e commercial\nsystems, we wanted a lightweight AML which ﬁts naturally within a mode rn scientiﬁc\nworkﬂow. Such workﬂows could include solving optimization problems w ithin a larger\nsimulation or interactive visualization, for example, or constructing a complex opti-\nmization model programmatically from modular components [ 48, 19]. As algorithm\ndevelopers, we wanted to be able to interact with solvers while they a re running,\nfor both control of the solution process and to reduce the overh ead of regenerating\na model when solving a sequence of related instances [ 20]. Finally, as modelers, we\nwanted to create user-friendly AML extensions for new problem cla sses that couple\nwith specialized solution approaches; in contrast, commercial AMLs were not designed\nto be extended in this way [ 30]. In short, with similar motivations as the developers\nof the Julia language itself [ 11], we created JuMP because we wanted more than what\nexisting tools provided.\nJuMP joins a rich family of open-source AMLs which been developed by academics\nsince the 2000s. YALMIP [ 54] and CVX [ 39], both based on MATLAB, were created\nto provide functionality such as handling of semideﬁnite and disciplined convex [ 40]\noptimization, which was not present in commercial AMLs. CVX in partic ular has\nbeen cited as making convex optimization as accessible from MATLAB a s is linear\nalgebra and was credited for its extensive use in both research and teaching [ 26].\nPyomo [ 44] is an AML which was originally designed to recreate the functionality o f\nAMPL in Python and was later extended to new problem classes such a s stochastic\nprogramming [ 80]. Embedded within general-purpose programming languages 1, these\nopen-source AMLs broadly address our concerns of ﬁtting within a modern workﬂow\nand are powerful and useful in many contexts. However, their slo w performance, due\nto being embedded in high-level languages like MATLAB and Python, mo tivated our\npreliminary work investigating Julia as an alternative high-level host la nguage with\nthe promise of fewer performance compromises [ 56].\nFollowing JuMP’s ﬁrst release in 2013, which supported linear and mixed -integer\noptimization, we have enabled modeling for quadratic, conic-quadra tic, semideﬁ-\nnite, and general derivative-based nonlinear optimization problems , standard prob-\nlem classes supported by the commercial and open-source AMLs. A t the same\ntime, we have extended the functionality of JuMP beyond what is typ ically avail-\nable in an AML, either commercial or open-source. These features , which will be\ndescribed in the text, include callbacks for in-memory bidirectional c ommunication\nwith branch-and-bound solvers, automatic diﬀerentiation of user -deﬁned nonlinear\nfunctions, and easy-to-develop add-ons for specialized problem c lasses such as robust\noptimization. JuMP’s unique mix of functionality has driven growing ado ption by\nresearchers [ 7, 34, 49, 72, 46], and JuMP and has been used for teaching in courses in\nat least 10 universities (e.g., [ 25]). In this paper, we will highlight both the important\ntechnical and usability aspects of JuMP, including how JuMP itself use s the advanced\nfeatures of Julia.\nThe remainder of the paper is structured as follows. In Section 2 we introduce in\nmore detail the tasks required of an AML and present an example of AML syntax.\nIn Sections 3 and 4 we discuss JuMP’s use of syntactic macros and code generation,\ntwo advanced technical features of Julia which are key to JuMP’s pe rformance. In\nSection 5 we discuss JuMP’s implementation of derivative computations. In Sec tion 6\nwe discuss a number of powerful extensions which have been built on top of JuMP,\n1An idea which traces back to the commercial ILOG C++ interfac e of the 1990s\nJuMP 3\nFig. 1 . Sparsity pattern from the constraint coeﬃcient matrix for a multicommodity ﬂow\nproblem arising from optimal routing in communication netw orks [ 14]. The dots correspond to\nnonzero elements of the matrix. We identify ﬁve groups of con straints, indicated with colored strips\non the left. Modeling languages remove the need to write code to generate such complex matrices\nby hand; users instead work with a much more natural algebrai c representation of the optimization\nproblem.\nand in Section\n7 we conclude with a demonstration of how JuMP can be composed\nwith the growing ecosystem of Julia packages to produce a compelling interactive and\nvisual user interface with applications in both academia and industry .\n2. The role of a modeling language. Prior to the introduction of AMLs\n(and continuing to a lesser degree today), users would write low-lev el code which\ndirectly generated the input data structures for an optimization p roblem. Recall that\nstandard-form linear programming problems can be stated as\nmin\nx∈Rn\ncT x\ns.t. Ax = b,\nx ≥0,\n(2.1)\nthat is, minimization of a linear objective subject to linear equality and inequality\nconstraints (all elements of x must be nonnegative). In the case of LP, the input\ndata structures are the vectors c and b and the matrix A in sparse format, and the\nroutines to generate these data structures are called matrix generators [\n27]. Typical\nmathematical optimization models have complex indexing schemes; fo r example, an\nairline revenue management model may have decision variables xs,d,c which represent\nthe number of tickets to sell from the source s to destination d in fare class c, where\nnot all possible combinations of source, destination and fare class a re valid. A matrix\ngenerator would need to eﬃciently map these variables into a single list of linear indices\nand then construct the corresponding sparse matrix A as input to the solver, which is\ntedious, error-prone, and fragile with respect to changes in the m athematical model.\nAn example sparsity pattern in Figure 1 demonstrates that these can be quite complex\neven for small problems. This discussion extends naturally to quadr atic expressions\ncT x+ 1\n2 xT Qx; the matrix Q is simply another component of the input data structure.\nThe role of an AML in these cases is to accept closed-form algebraic e xpressions as\nuser input and transparently generate the corresponding input m atrices and vectors,\nremoving the need to write matrix generators by hand. AMLs additio nally handle\nany low-level details of communicating with the solver, either via a calla ble library or\nby exchanging specially formatted ﬁles.\nAMLs have a similar role in the context of nonlinear optimization problem s,\nwhich often arise in scientiﬁc and engineering applications. The stand ard form for\n4 Dunning et al.\nderivative-based nonlinear optimization problems is\nmin\nx∈Rn\nf(x)\ns.t. gi(x) ≤0 i = 1 , . . . , m g,\nhi(x) = 0 i = 1 , . . . , m h,\n(2.2)\nwhere f, gi, hi : Rn →R are linear or nonlinear functions. Depending on certain\nproperties of f, g, and h such as convexity, these problems may be easy or hard to\nsolve to a global solution; regardless, the solution methods often r ely on the availability\nof ﬁrst-order derivatives, that is, the gradient vectors ∇f(x), ∇gi(x), and ∇hi(x), and\nmay be further accelerated by the availability of second-order der ivatives, that is, the\nHessian matrices ∇2f(x), ∇2gi(x), and ∇2hi(x). For nonlinear optimization, AMLs\ntake closed-form algebraic equations and automatically generate r outines for exact\nderivative evaluations in a form that solvers may call directly. An alte rnative to using\nAMLs here is to use general-purpose automatic diﬀerentiation (AD) tools which can\nbe used to evaluate derivatives of code, an option which will be furth er discussed in\nSection\n5. Lacking AMLs or other AD tools, one is faced with the tedious and er ror-\nprone task of implementing code to evaluate derivatives manually [ 33, p. 297]. In such\ncases it is common to forgo second-order derivatives or even ﬁrst -order derivatives,\neven when providing them could reduce the solution time.\nWhile the computational eﬃciency of an AML’s translation of user inpu t into\nsolver input is something that can be empirically measured, we must no te that the\nintrinsic usefulness of an AML is derived from how “naturally” the orig inal mathemat-\nical statement can be translated into code. This is a more subjectiv e proposition, so\nin Figure 2 we present the formulation in JuMP, AMPL, Pyomo and GAMS of a min-\nimum cost ﬂow problem as a linear program (see, e.g., [ 9]) over a graph G = ( V, E ),\nwhere the vertices V = {1, 2, . . . , n }are consecutively numbered with a “source” at\nvertex 1 and “sink” at vertex n:\n(2.3)\n■ min\nx\n∑\n(i,j )∈E\nci,j xi,j\n■ s.t.\n∑\n(i,j )∈E\nxi,j =\n∑\n(j,k )∈E\nxj,k j = 2 , . . . , n −1\n■\n∑\n(i,n )∈E\nxi,n = 1\n■ 0 ≤xi,j ≤Ci,j ∀(i, j) ∈E\nThe four AMLs share much in common: all involve declaring a set of var iables in-\ndexed by the set of edges, all have a line for setting the objective f unction, and all\nhave methods for iterating over a range (2 to n −1) and taking a sum over variables\nsubject to some condition. Of the four, JuMP and AMPL are perhap s the most sim-\nilar, although JuMP beneﬁts from being embedded in a full programmin g language,\nallowing us to deﬁne an Edge type that stores the problem data in a su ccinct and\nintuitive fashion. Pyomo is also embedded in a programming language, b ut as Python\ndoesn’t have the same syntactic macro functionality as Julia, some t hings are more\nuncomfortable than is ideal (setting the variable upper bounds, ind exed constraint\nconstruction). Finally GAMS has perhaps the most verbose and idios yncratic syntax,\nwith features like set ﬁltering with the $ character that are not commonly found in\nJuMP 5\neither programming or modeling languages. Our main claim is that JuMP is a “nat-\nural” and easy-to-use modeling language, and for the rest of this p aper will instead\nfocus on the technical details that allow it to be eﬃcient and to enable unique features\nnot found in other AMLs.\nThe technical tasks that an AML must perform can be roughly divide d into two\nsimple categories: ﬁrst, to load the user’s input into memory, and se cond, to generate\nthe input required by the solver, according to the class of the prob lem. For both\nof these tasks, we have made some unorthodox design decisions in J uMP in order to\nachieve good performance under the constraints of being embedd ed within a high-level\nlanguage. We will review these in the following sections.\nWe note that, as we have mentioned, JuMP provides access to a num ber of ad-\nvanced techniques which have not been typically available in AMLs. For example,\nbranch-and-cut is a powerful technique in integer programming for accelerating the\nsolution process by dynamically improving the convex (linear) relaxat ions used within\nthe branch-and-bound algorithm. Users wanting to extend a solve r’s branch-and-cut\nalgorithm with dynamically generated “cuts” for a particular problem structure have\ntypically needed to turn to low-level coding in C++ for an eﬃcient implem entation via\ncallback functions, since this interaction requires bidirectional com munication with a\nsolver during the solution process. To our knowledge, JuMP is the ﬁr st AML to pro-\nvide a simple, high-level, and eﬃcient (in-memory) interface to branc h-and-cut and\nother similar techniques. This feature has already seen fruitful us e in research [ 49, 77]\nand teaching [ 25].\n3. Syntactic macros: parsing without a parser. AMLs like AMPL and\nGAMS are standalone in the sense that they have deﬁned their own s yntax entirely\nseparate from any existing programming language. They have their own formats for\nproviding input data (although they can also connect to databases and spreadsheets)\nand implement custom parsers for their proprietary syntax; for e xample, AMPL uses\nthe LEX and YACC parser generator utilities [ 28].\nEmbedding an AML within an existing programming language brings with it the\nbeneﬁt of being able to bootstrap oﬀ the existing, well deﬁned gram mar and syntax\nof the language, eliminating a complex part of implementing an AML. Per haps more\nimportantly for users, embedded AMLs typically allow interlacing the A ML’s math-\nlike statements declaring an optimization problem with arbitrary code which may be\nused to prepare input data or process the results. However, emb edding also brings\nwith it the challenge of obtaining the desired expressiveness and eas e of use within\nthe limits of the syntax of the parent language.\nThe most common approach (taken by Pyomo, YALMIP, and others ) to capturing\nuser input is operator overloading . One introduces a new class of objects, say, to\nrepresent a decision variable or vector of decision variables, and ex tends the language’s\ndeﬁnition of basic operators like +, ∗, −, etc, which, instead of performing arithmetic\noperations, build up data structures which represent the expres sion. For example,\nto represent a quadratic expression ∑\n(i,j )∈J bijxixj + ∑\ni∈I aixi + c, one stores the\nconstant c, the coeﬃcient vectors b, a, and the index sets I and J. Letting n be the\nnumber of decision variables in a problem, an unfortunate property of addition of two\nquadratic expressions is that the size of the resulting expression is not bounded by a\nconstant independent of n, simply because the coeﬃcient and index vectors can have as\nmany as O(n2) terms. This means that basic operations like addition and subtract ion\nare no longer fast, constant-time operations, a property which is almost always taken\nfor granted in the case of ﬂoating-point numbers. As a concrete e xample, consider the\n6 Dunning et al.\nJuMP\n■ immutable Edge\n■ from; to; cost; capacity\n■ end\n■ edges = [Edge(1,2,1,/zero.noslash.5), Edge(1,3,2,/zero.noslash.4), Edge(1,4,3,/zero.noslash.6),\n■ Edge(2,5,2,/zero.noslash.3), Edge(3,5,2,/zero.noslash.6), Edge(4,5,2,/zero.noslash.5)]\n■ mcf = Model()\n■ @variable(mcf, /zero.noslash <= flow[e in edges] <= e.capacity)\n■ @constraint(mcf, sum{flow[e], e in edges; e.to==5}== 1)\n■ @constraint(mcf, flowcon[n=2:4], sum{flow[e], e in edges; e.to==node}\n■ == sum{flow[e], e in edges; e.from==node})\n■ @objective(mcf, Min, sum{e.cost * flow[e], e in edges})\nAMPL\n■ set edges := {(1,2),(1,3),(1,4),(2,5),(3,5),(4,5)};\n■ param cost{edges}; param capacity {edges};\n■ data ...; # Data is typically stored separately in AMPL;\n■ var flow{(i,j) in edges}>= /zero.noslash./zero.noslash, <= capacity[i,j];\n■ subject to unitflow: sum {(i,5) in edges}flow[i,5] == 1;\n■ subject to flowconserve {n in 2..4}:\n■ sum{(i,n) in edges}flow[i,n] == sum{(n,j) in edges}flow[n,j];\n■ minimize flowcost: sum {(i,j) in edges}cost[i,j] * flow[i,j];\nPyomo\n■ edges = [(1,2), (1,3), (1,4), (2,5), (3,5), (4,5)]\n■ cost = {(1,2):1, (1,3):2, (1,4):3, (2,5):2, (3,5):2, (4,5):2 }\n■ capacity = {(1,2):/zero.noslash.5, (1,3):/zero.noslash.4, (1,4):/zero.noslash.6, (2,5):/zero.noslash.3, (3,5):/zero.noslash.6, (4,5):/zero.noslash.5}\n■ mcf = ConcreteModel()\n■ mcf.flow = Var(edges, bounds=lambda m,i,j: (/zero.noslash,capacity[(i,j)]))\n■ mcf.uf = Constraint(expr=sum(mcf.flow[e] for e in edges if e[1]==5) == 1)\n■ def con_rule(mcf,n): return sum(mcf.flow[e] for e in edges if e[1]==n) ==\n■ sum(mcf.flow[e] for e in edges if e[/zero.noslash]==n)\n■ mcf.flowcon = Constraint([2,3,4],rule=con_rule)\n■ mcf.flowcost = Objective(expr=sum(cost[e]*mcf.flow[e] for e in edges))\nGAMS\n■ SET nodes /n1*n5/; SET midnodes(nodes) /n2*n4/; SET lastnode(nodes) /n5/;\n■ ALIAS(nodes,nodefrom,nodeto,n);\n■ SET edges(nodes,nodes) / n1.n2 n1.n3 n1.n4 n2.n5 n3.n5 n4.n5 /;\n■ PARAMETER cost(nodes,nodes) / ... /; * Data omitted\n■ PARAMETER capacity(nodes,nodes) / ... /; * for space reason s\n■ POSITIVE VARIABLE flow(nodefrom,nodeto); flow.UP(edges) = capacity(edges);\n■ EQUATION unitflow;\n■ unitflow.. sum{edges(nodefrom,lastnode), flow(nodefrom,lastnode)}=e= 1;\n■ EQUATION flowcon(nodes);\n■ flowcon(midnodes(n)).. sum{edges(nodefrom,n), flow(nodefrom,n)}=e=\n■ sum{edges(n,nodeto), flow(n,nodeto)};\n■ FREE VARIABLE obj;\n■ EQUATION flowcost; flowcost.. obj =e= sum{edges, cost(edges)*flow(edges)};\n■ MODEL mincostflow /all/; SOLVE mincostflow USING lp MINIMIZING obj;\nFig. 2 . Modeling a minimum cost ﬂow problem in JuMP, AMPL, Pyomo, and GAMS. The\ncolored squares show the correspondence between the code an d the four components of Equation 2.3.\nFor concreteness, we provide an explicit example of a ﬁve-no de problem with data when it ﬁts. The\nJuMP and Pyomo examples are complete, valid code (as of this w riting) and can be copy-pasted into\na terminal to run after importing the corresponding package s.\nJuMP 7\nfollowing quadratic expression in the variable x indexed over {1, . . . , d }×{1, . . . , d }:\n(3.1) 1 +\nd∑\ni=1\nd∑\nj=1\n|cj −i|(1 −xi,j )x1,j\nIn Python, one might naturally express ( 3.1) as\n1 + sum(abs(c[j]-i)*(1-x[i,j])*x[/zero.noslash,j] for i in range(d) for j in range(d))\nwhich takes advantage of the built-in sum command which internally accumulates\nthe terms one-by-one by calling the addition operator d2 times. The partial sums\ngenerated with each addition operation are quadratic expressions which have O(d2)\nterms, so this naive approach can have a cost of O(d4) = O(n2) operations and\nexcessive memory allocations. An obvious workaround for this issue is to accumulate\nthe terms in a single output expression instead of a generating a new expression for\neach partial sum. While there are a number of ways to mitigate this slo w behavior\nwithin the framework of operator overloading, our benchmarks will demonstrate that\nthey may not be suﬃcient to achieve the best performance.\nWhen designing JuMP, we were not satisﬁed by the performance limita tions of\noperator overloading and instead turned to an advanced feature of Julia called syntac-\ntic macros [12]. Readers may be familiar with macros in C and C++ which perform\ntextual substitutions; macros in Julia are much more powerful in th at they function\nat the level of syntax. For example, the expression ( 3.1) could be written in JuMP\nsyntax as\n@expression(1 + sum{abs(c[j]-i)*(1-x[i,j])*x[1,j], i in 1:N, j in 1:N})\nThe @ sign denotes a call to a macro named expression, which constructs a JuMP\nexpression object. The input to the macro will be a data structure representing the\nJulia expression contained within, not simply a string of text. That is, Julia’s internal\nparser will be invoked to parse the expression, but instead of direc tly evaluating it or\ncompiling it to code, it will be sent to a routine written in Julia which we (as authors\nof JuMP) have deﬁned. Note that the syntax sum{}is generally not valid Julia code\nfor computing a sum, although it is recognized by the Julia parser bec ause the syntax\nis used in other contexts, which allows us to endow this syntax with a n ew meaning\nin the context of JuMP\n2.\nMacros enable JuMP to provide a natural syntax for algebraic mode ling without\nwriting a custom text-based parser and without the drawbacks of operator overload-\ning. Within the computer science community, macros have been reco gnized as a useful\ntool for developing domain-speciﬁc languages, of which JuMP is an ex ample [ 74]. In-\ndeed, the implementation of macros in Julia draws its inspiration from L isp [ 12].\nHowever, such functionality historically has not been available within p rogramming\nlanguages targeted at scientiﬁc computing, and, to our knowledge , JuMP is the ﬁrst\nAML to be designed around syntactic macros.\n4. Code generation for linear and conic-quadratic models. Linear and\nconic-quadratic optimization problems are essential and surprising ly general modeling\nparadigms that appear throughout operations research and oth er varied ﬁelds—often\nat extremely large scales. Quadratic optimization generalizes linear o ptimization by\n2As of Julia 0.5, JuMP will transition to the new syntax sum(abs(c[j]-i)*(1-x[i,j])*x[1,j] for\ni in 1:N, j in 1:N) which will more closely match Julia code.\n8 Dunning et al.\nallowing convex quadratic terms 1\n2 xT Qx in the objective and constraints. Conic-\nquadratic, also known as second-order cone, optimization genera lizes quadratic op-\ntimization with constraints of the form ||x||2 ≤t, where both x and t are decision\nvariables [ 53]. Computational tools for solving these problems derive their success\nfrom exploiting the well-deﬁned structure of these problems. Analo gously, JuMP\nis able to eﬃciently process large-scale problems by taking advantag e of structural\nproperties and generating eﬃcient code through Julia’s code gener ation functionality.\nJulia is, at the same time, both a dynamic and compiled language. Julia us es\nthe LLVM compiler [ 52] dynamically at runtime, and can generate eﬃcient, low-level\ncode as needed. This technical feature is one of the reasons why J ulia can achieve\nC-like performance in general [ 10], but we will restrict our discussion to how JuMP\ntakes advantage of it.\nIn the previous section we described how JuMP uses macros to acce pt user input\nin the form of a data structure which represents the input expres sion. The other side\nof macros is code generation. More speciﬁcally, macros can be understood as functions\nwhose input is code and whose output is code. Given an input express ion, a macro\nproduces a data structure which represents an output express ion, and that expression\nis then substituted in place and compiled. For example, the call to the expression\nmacro in Section 3 would output, in pseudo-code from, the following code:\nInitialize an empty quadratic expression q\nAdd 1 to the constant term\nCount the number of terms K in the sum{} expression\nPre-allocate the coefficient and index vectors of q to hold K elements\nfor i in 1:d, j in 1:d\nAppend -abs(c[j]-i)*x[i,j]*x[1,j] to the quadratic terms in q\nAppend abs(c[j]-i)*x[1,j] to the linear terms in q\nend\nNote that this code runs in O(d2) operations, a signiﬁcant improvement over the\nO(d4) naive operator overloading approach. The code produced is also s imilar to a\nhand-written matrix generator. Indeed, one could summarize JuM P’s approach to\ngenerating linear and quadratic models as translating users’ algebr aic input into fast,\ncompiled code which acts as a matrix generator. JuMP’s approach to semideﬁnite\noptimization, a recently added feature which we will not discuss furt her, generally\nfollows this path but also employs operator overloading for certain m atrix operations.\n4.1. Benchmarks. We now provide computational evidence that JuMP is able\nto produce quadratic and conic-quadratic optimization models, in a f ormat suitable for\nconsumption by a solver, as fast as state-of-the-art commercia l modeling languages.\nTo do so we measure the time elapsed between launching the executa ble that builds\nthe model and the time that the solver begins the solution process, as determined by\nrecording when the ﬁrst output appears from the solver. This met hodology allows\nthe modeling language to use a direct in-memory solver interface if it d esires, or in\nthe case of some tools a compact ﬁle representation. We selected G urobi 6.5.0 [\n43]\nas the solver, and evaluated the following modeling systems: the Gur obi C++ inter-\nface (based on operator overloading), JuMP 0.12 with Julia 0.4.3, AMP L 20160207\n[29], GAMS 24.6.1 [ 17], Pyomo 4.2.10784 with Python 2.7.11 [ 44], and CVX 2.1 [ 39]\nand YALMIP 20150918 [ 54] with MATLAB R2015b. These particular modeling sys-\ntems are chosen for being widely used in practice within diﬀerent comm unities. The\nbenchmarks were run on a Linux system with an Intel Xeon CPU E5-2 650 processor.\nWe implemented two diﬀerent optimization problems in all seven modeling lan-\nJuMP 9\nCommercial Open-source\nInstance JuMP GRB/C++ AMPL GAMS Pyomo CVX YALMIP\nlqcp-500 8 2 2 2 55 6 8\nlqcp-1000 11 6 6 13 232 48 25\nlqcp-1500 15 14 13 41 530 135 52\nlqcp-2000 22 26 24 101 >600 296 100\nfac-25 7 0 0 0 14 >600 533\nfac-50 9 2 2 3 114 >600 >600\nfac-75 13 5 7 11 391 >600 >600\nfac-100 24 12 18 29 >600 >600 >600\nTable 1\nTime (sec.) to generate each model and pass it to the solver, a comparison between JuMP\nand existing commercial and open-source modeling language s. The lqcp instances have quadratic\nobjectives and linear constraints. The fac instances have linear objectives and conic-quadratic con-\nstraints.\nguages: a linear-quadratic control problem ( lqcp) and a facility location problem\n(fac). We do not claim that these models are representative of all conic- quadratic\nproblems; nevertheless, they provide a good stress test for gen erating models with\nmany quadratic and conic-quadratic terms. The models are furthe r described in the\nappendix. Models using the C++ interface are implemented in a way tha t mitigates\nthe drawbacks of operator overloading by appending to existing ex pressions using\nthe += operator; such approaches, however, are not idiomatic in Pyomo, CVX, or\nYALMIP. The results (Table\n1) show that for lqcp, JuMP, AMPL, and the C++\ninterface are roughly equivalent at the largest scale, with GAMS and YALMIP ap-\nproximately four times slower and CVX thirteen times slower than JuM P. Pyomo was\nsigniﬁcantly slower and was unable to construct the largest model w ithin ten minutes.\nFor fac, JuMP, AMPL, GAMS and the C++ interface times all perform roughly the\nsame, while Pyomo is unable to build the largest instance with ten minute s, YALMIP\ncan build only the smallest instance within the time limit, and CVX is unable t o\nbuild any instances within the time limit. These results demonstrate th at JuMP can\nbe reasonably competitive with widely used commercial systems and in some cases\nsigniﬁcantly faster than open-source alternatives.\n4.2. Optimizing a sequence of models. As we observe in Table 1, JuMP has\na noticeable start-up cost of a few seconds even for the smallest in stances. This start-\nup cost is primarily composed of compilation time; however, if a family of models is\nsolved multiple times within a single session, this cost of compilation is only paid for\nthe ﬁrst time that an instance is solved. That is, when solving a sequence of ins tances\nin a loop, the amortized cost of compilation is negligible.\nIndeed, solving a sequence of related optimization problems is a comm on idiom\nwhen performing exploratory analysis or implementing more advance d algorithms. A\nnumber of algorithms including branch-and-bound [ 5], Benders decomposition [ 81],\nand cutting-plane methods derive their eﬃciently from the fact tha t when solving\na sequence of linear programming problems, one can “hot-start” the simplex algo-\nrithm from the previous optimal solution when new constraints are a dded or when\nobjective or right-hand side coeﬃcients are modiﬁed. JuMP suppor ts all of these\nclasses of modiﬁcations by enabling eﬃcient in-memory access to solv ers so that they\ncan maintain their internal state across solves, when possible, avo iding the signiﬁcant\n10 Dunning et al.\noverhead that would be incurred when regenerating a model from s cratch inside of a\nloop (even if information on the previous optimal solution persists). For example, a\nstraightforward implementation of a cutting-plane algorithm in GAMS was found to\nbe 5.8x slower overall than an implementation of the same algorithm in C ++ [ 20],\nillustrating the overhead induced by these traditional AMLs which do not allow direct\nin-memory access to solvers.\nHere we present a small example that demonstrates solving and mod ifying a\nproblem in a loop with JuMP,\nl2approx = Model()\n@variable(l2approx, -1 <= x[1:N] <= +1)\n@objective(l2approx, Max, sum(x))\nsolve(l2approx); v = getvalue(x) # Build and solve for initial solution\nwhile norm(v) >= 1 + tol\n@constraint(l2approx, dot(v,x) <= norm(v))\nsolve(l2approx); v = getvalue(x) # Optimize from prev solution\nend\nWe maximize a simple linear function ( ∑ N\ni=1 xi) over the ℓ2 “ball” constraint ∥x∥2 ≤1\nby approximating this nonlinear constraint with a ﬁnite sequence of t angent hyper-\nplanes generated only as needed, allowing us to use an LP solver inste ad of a solver\nthat supports conic-quadratic constraints. Direct extensions o f this technique have\nproven useful for solving mixed-integer conic optimization problems [\n77]. When a\nconstraint is added to the JuMP model by the user, as in the above e xample, JuMP\nadds the constraint directly to the solver’s in-memory representa tion of the problem,\nrather than generating the whole model from scratch. As a result the solver is able to\nuse the previous (now infeasible) solution as a hot-start by applying the dual simplex\nmethod.\nJuMP’s design comes for the price of not supporting constructs fo r parametric\ndata [29] as GAMS, AMPL, and Pyomo do; that is, in JuMP one cannot deﬁne a p ara-\nmetric value and have its values propagate automatically through an LP model as the\nvalue of the parameter changes, because doing so would complicate the abstraction\nlayer between JuMP’s representation of a model and the solver’s in- memory represen-\ntation of a model 3. For more complex changes to problem data and structure, such a s\nmodifying the coeﬃcients in existing constraints, the idiomatic appro ach when using\nJuMP is to construct a new model from scratch, possibly inside a fun ction that takes\nparameters as its input. The following pseudocode, with JuMP on the left and AMPL\non the right, demonstrates the diﬀerence in approach from a user ’s perspective.\nfunction f(p)\n# Build and solve model\n# using p as data, then\n# process solution\nend\nf(1)\nf(2)\nf(3)\nparam p;\n# Define algebraic model using p\nlet p := 1;\nsolve; # ... process solution\nlet p := 2;\nsolve; # ... process solution\nlet p := 3;\nsolve; # ... process solution\n5. Computing derivatives for nonlinear models. Recall that the role of\na modeling language for nonlinear optimization is to allow users to specif y “closed-\nform,” algebraic expressions for the objective function f and constraints g and h in the\n3In the case of nonlinear optimization, JuMP oﬀers parameter objects because these can be\neﬃciently integrated within the derivative computations.\nJuMP 11\nformulation ( 2.2) and communicate ﬁrst-order and typically second-order derivat ives\nwith the optimization solver. Commercial modeling languages like AMPL a nd GAMS\nrepresent the state of the art in modeling languages for nonlinear o ptimization. Likely\nbecause of the increased complexity of computing derivatives, eve n fewer open-source\nimplementations exist than for linear or quadratic models.\nNotable alternative approaches to traditional algebraic modeling fo r nonlinear op-\ntimization include CasADi [ 3] and CVX [ 39]. CasADi allows interactive, scalar- and\nmatrix-based construction of nonlinear expressions via operator overloading with au-\ntomatic computation of derivatives for optimization. CasADi has sp ecialized features\nfor optimal control but, unlike traditional AMLs, does not suppor t linear optimization\nas a special case. CVX, based on the principle of disciplined convex pr ogramming\n(DCP) [ 40], allows users to express convex optimization problems in a specialized for-\nmat which can be transformed into or approximated by conic progra mming without\nthe need for computing derivatives 4. The traditional nonlinear optimization formula-\ntion considered here applies more generally to derivative-based con vex and nonconvex\noptimization.\nJuMP, like AMPL and GAMS, uses techniques from automatic (or algor ithmic)\ndiﬀerentiation (AD) to evaluate derivatives of user-deﬁned expre ssions. In this sec-\ntion, we introduce these techniques with a focus on how AMLs relate to more general-\npurpose AD tools. In this vein, we discuss JuMP’s unique ability to auto matically\ndiﬀerentiate user-deﬁned functions . Concluding this section, we present a set of per-\nformance benchmarks.\n5.1. Expression graphs and reverse-mode AD. A natural data structure\nfor representing nonlinear expressions is the expression graph , which is a directed\nacyclic graph (or typically, a tree) that encodes the sequence of o perations required\nto compute the expression as well as the dependency structure b etween operations.\nFor example, Figure 3 illustrates how the nonlinear expression exp( x2 + y2) is rep-\nresented as a graph, with nodes representing the input values x and y together with\nevery “basic” operation like addition and exponentiation that is perf ormed in com-\nputing the value of the expression. We will return later to the quest ion of what is\nconsidered a basic operation, but for now consider these as all ope rations that one\nmight compose to write down a closed-form algebraic equation. Edge s in the expres-\nsion graph represent immediate dependencies between operations . The expression\ngraph encodes all needed information for JuMP to evaluate and com pute derivatives\nof nonlinear expressions, and JuMP generates these objects by u sing macros analo-\ngously to how JuMP generates sparse matrix data structures rep resenting linear and\nquadratic functions.\nWhile general-purpose AD tools like ADOL-C [ 79] use operator overloading (or\ndirect analysis of source code [ 16]) to generate these expression graphs from arbitrary\ncode; AMLs like AMPL, GAMS, and JuMP have an easier task because u ser input\nis constrained to follow a speciﬁc syntax and thus these AMLs are ge nerally more\nreliable. The value of using JuMP, for example, over using more gener al-purpose AD\ntools is that JuMP provides a guarantee to the user that all input fo llowing its simple\nsyntax can be diﬀerentiated eﬃciently, the only limitation being that t he expression\ngraph objects must ﬁt within memory. On the other hand, making us e of more general-\npurpose tools requires a nontrivial amount of expertise (for exam ple, preparing C++\ncode for ADOL-C requires extensive modiﬁcations and use of specia lized assignment\n4Note that the DCP paradigm is available in Julia through the C onvex.jl package [ 76].\n12 Dunning et al.\nexp(·)\n+\n(·)2\ny\n(·)2\nx\nFig. 3 . A graph representation of the nonlinear expression exp(x2 + y2). JuMP uses this\nexpression graph structure for eﬃcient evaluation of deriv atives.\noperators). We have found users from ﬁelds like statistics who hav e traditionally\nnot been users of AMLs being drawn to JuMP for its AD features and its being\nembedded in a familiar programming language, to the extent that the y are willing\nto rewrite complex statistical models into JuMP’s syntax [ 34]. It remains to be seen\nhow general this adoption trend may be, but we believe that there is large scope for\njudicious use of AMLs as AD tools within domains that have not widely ad opted them\nso far.\nGiven an expression graph object, one can compute the numerical value of the\nexpression by iterating through the nodes of the graph in an order such that by the\ntime we reach a given node to evaluate its corresponding operation, the numerical\nvalues of all its inputs (children) have already been computed. A per haps surprising\nresult is that it is possible to apply the chain rule in such a way that by ite rating\nthrough the nodes in the reverse order (parents before children ), in a single pass, we\nobtain the exact gradient vector ∇f(x). This reverse-pass algorithm, known suitably\nas reverse-mode AD, delivers gradients of f for a small constant factor times the cost\nevaluating f itself. We refer readers to [ 41, 63] for further discussion of this powerful\ntechnique which JuMP employs.\n5.2. User-deﬁned functions. Modeling languages like JuMP include a basic\nlibrary of nonlinear functions which are available for use within expres sions. JuMP’s\nbasic library is extensive and includes special functions like the error function erf(),\nwhich enables users to express Gaussian cumulative densities, for e xample. AMPL\nrecently developed an interface to functions from the GNU Scientiﬁ c Library [ 37],\ngreatly extending the range of available functions. However, in the cases where the\nbuilt-in library is insuﬃcient, historically there has been no user-frien dly way to in-\ncorporate user-deﬁned functions into AMLs. 5 A compelling application for user-\ndeﬁned functions is optimal control problems constrained by diﬀer ential equations,\nwhere standalone integrators are used to enforce dynamic const raints [ 4]. JuMP is\nthe ﬁrst AML to provide a simple interface not only for user-deﬁned functions with\nuser-provided (hand-coded) derivatives but also to provide an op tion to automatically\ndiﬀerentiate user-deﬁned functions . We provide a brief example below of this usage\nin JuMP and then describe the implementation.\nfunction squareroot(x)\n5Doing so within AMPL, for example, requires developing a sha red library object in C which\nlinks to the AMPL solver library.\nJuMP 13\nz = x # Initial starting point for Newton’s method\nwhile abs(z*z - x) > 1e-13\nz = z - (z*z-x)/(2z)\nend\nreturn z\nend\nJuMP.register(:squareroot, 1, squareroot, autodiff=true)\nm = Model()\n@variable(m, x[1:2], start=/zero.noslash.5)\n@objective(m, Max, sum(x))\n@NLconstraint(m, squareroot(x[1]ˆ2+x[2]ˆ2) <= 1)\nsolve(m)\nFirst, we deﬁne the squareroot function using generic Julia code. This function\ncomputes the square root of a number by applying Newton’s method to ﬁnd the zero\nof the function f(z) = z2 −x. The function JuMP.register registers the nonlinear\nfunction with the symbolic name squareroot and passes a reference to the function\ndeﬁned above. The second argument 1 indicates that the input to the function is\nunivariate. The autodiff=true option instructs JuMP to automatically compute the\nderivatives of the user-deﬁned function (if this option is not set, u sers must also\nprovide derivative evaluation callbacks). The subsequent JuMP mod el deﬁnes the\nproblem of maximizing x1 + x2 subject to the constraint\n√\nx2\n1 + x2\n2 ≤1, where the\nsquare root is computed by the user-deﬁned Julia code.\nIn principle, JuMP could apply reverse-mode AD to user-deﬁned fun ctions by\nusing operator overloading to build up the expression graph repres entation of the\nfunction, using analogous techniques to ADOL-C. However, no mat ure implementa-\ntion of this approach currently exists in Julia. Instead, JuMP uses F orwardDiﬀ.jl, a\nstandalone Julia package implementing forward-mode AD [\n70], to compute derivatives\nof these functions.\nForward-mode AD, known as such because it can be applied with a sing le forward\npass though the expression graph data structure, can be interp reted as computing di-\nrectional derivatives by introducing inﬁnitesimal perturbations [ 41]. In comparison,\nthe more well-known ﬁnite diﬀerencing method employs small but ﬁnite perturbations.\nThe operator overloading approach for forward-mode AD is to intr oduce a new class\nof number a+bǫ where ǫ2 = 0 (analogously to i2 = −1 for the complex numbers). The\nimplementation in Julia’s ForwardDiﬀ.jl is conceptually quite similar to tha t in other\nlanguages, and we refer readers to Neidinger [ 64] for a comprehensive introduction\nto forward-mode AD and its implementation in MATLAB using operator overload-\ning. In Julia, however, user-deﬁned numerical types are given ﬁrs t-class treatment by\nthe compiler and produce eﬃcient low-level machine code, which is not the case for\nMATLAB. Note that forward-mode applied in this way does not requir e an explicit\nexpression graph representation of a function and hence is simpler to implement than\nreverse-mode.\nThe only burden on users providing functions for automatic diﬀeren tiation is to\nwrite code which is generic with respect to the type of the numerical input (in the\nabove example, x). This design is equivalent in spirit to using templates in C++\nbut with a much less heavyweight syntax. The cost of applying forwa rd-mode AD\ngrows linearly with the the input dimension of the target function; he nce for high-\ndimensional user-deﬁned functions, users may still want to provid e derivative eval-\nuation callbacks if speed is a concern. Nevertheless, the ability to au tomatically\ndiﬀerentiate user-deﬁned functions begins to blur the distinction b etween AMLs and\n14 Dunning et al.\n\n\n\n\n\n\nh11 h12 h14\nh12 h22 h23\nh23 h33\nh14 h44\nh55\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n1\n1\n\n\n\n\n\n\n=\n\n\n\n\n\n\nh11 h12 + h14\nh12 + h23 h22\nh33 h23\nh14 h44\nh55\n\n\n\n\n\n\nFig. 4 . Many solvers can beneﬁt from being provided the Hessian matr ix of second-order\nderivatives at any point. JuMP uses reverse-mode automatic diﬀerentiation to generate a “black\nbox” routine that computes Hessian-vector products and use s this to calculate the non-zero elements\nof the Hessian matrix. For eﬃciency, we would like to use as fe w Hessian-vector products as possible;\nby using a specialized graph coloring heuristic [\n32], we can ﬁnd a small number of evaluations to\ndo so. Above, we illustrate a symmetric 5 × 5 Hessian matrix with hij = ∂ 2f\n∂x i∂x j\n(x) for some f.\nOmitted entries are known to be zero. In this example, only tw o Hessian-vector products are needed.\nmore traditional AD tools, and we look forward to seeing the applicat ions of this\nrecently added feature.\n5.3. From gradients to Hessians. In addition to gradients, oﬀ-the-shelf non-\nlinear optimizers typically request second-order derivatives. A bas ic operation for\ncomputing second-order derivatives is the Hessian-vector produ ct ∇2f(x)d. Since\nthis product is a directional derivative of the gradient, we now have the tools to\ncompute it, by applying forward-mode automatic diﬀerentiation to t he reverse-pass\nalgorithm for gradients.\nThis composition of forward-mode with reverse-mode AD is known as forward-\nover-reverse mode [ 41, 63], and JuMP implements it by manually augmenting the\nreverse-mode implementation to propagate the required inﬁnitesim al components for\ndirectional derivatives. Note that we do not yet support second o rder automatic\ndiﬀerentiation of user-deﬁned functions.\nGiven this forward-over-reverse routine to compute Hessian-ve ctor products, one\ncould recover a dense Hessian matrix ∇2f(x) by calling the routine n times, tak-\ning the n distinct unit vectors. However, for large n, this method quickly becomes\nprohibitively expensive. By exploiting the sparsity structure of ∇2f(x), one instead\nmay compute the entries of the Hessian matrix with far fewer than n Hessian-vector\nproducts. For example, if the Hessian is known to be diagonal, one ne eds only a sin-\ngle Hessian-vector product with d = (1 , 1, ··· , 1)T to compute all nonzero elements\nof the Hessian. In general, the problem of choosing a minimal number of Hessian-\nvector products to compute all nonzero elements is NP-hard; we im plement the acyclic\ngraph coloring heuristic of Gebremedhin et al. [ 32]. See Figure 4 for an illustration.\nThe Hessian matrices of typical nonlinear models exhibit signiﬁcant sp arsity, and in\npractice a very small number of Hessian-vector products are nee ded even for high-\ndimensional problems. We note that AMPL exploits Hessian structur e through partial\nseparability [ 31] instead of using graph coloring techniques.\n5.4. Benchmarks. We now present benchmarks evaluating the performance of\nJuMP for modeling nonlinear optimization problems. Similar to the exper imental de-\nsign in Section 4.1, we measure the time elapsed after starting the executable until t he\nsolver, Ipopt [ 78], reports the problem dimensions as conﬁrmation that the instance\nis loaded in memory. Then, we ﬁx the total number of iterations perf ormed to three\nand record the time spent in function or derivative evaluations as re ported by Ipopt.\nJuMP 15\nWe evaluated the following modeling systems: JuMP, AMPL, Pyomo, GA MS, and\nYALMIP. Recall that CVX does not support derivative-based nonlin ear models. Also,\nYALMIP does not support Hessian evaluations, so we measure only m odel generation\ntime.\nWe test two families of problems, nonlinear beam control ( clnlbeam) and non-\nlinear optimal power ﬂow ( acpower), which are further described in the appendix.\nThese two problems stress diﬀerent aspects of model generation ; the clnlbeam fam-\nily has large, sparse, and very simple nonlinear expressions with a diag onal Hessian\nmatrix, while the acpower family has a smaller number of variables but much more\ncomplex nonlinear network structure with a Hessian matrix with an irr egular sparsity\npattern. For model generation times (Table 2), JuMP has a relatively large startup\ncost, which is dominated by Julia’s compiler. However, as the size of th e instance\nincreases, JuMP becomes signiﬁcantly faster than Pyomo and YALM IP. As suggested\nby its performance and the omission of Hessian computations, YALM IP’s derivative-\nbased nonlinear functionality is seemingly not designed for large-sca le problems. We\ndid not implement acpower in YALMIP.\nThe results in Table 3 compare the time spent evaluating derivatives. Excluding\nthe smallest instances, JuMP remains within a factor of 2.2x of AMPL. JuMP is up\nto 3x faster than GAMS and in the worst case 25% slower. Note that Pyomo does not\nimplement its own derivative computations; instead, it re-uses AMPL ’s open-source\nderivative evaluation library.\nCommercial Open-source\nInstance JuMP AMPL GAMS Pyomo YALMIP\nclnlbeam-5 12 0 0 5 76\nclnlbeam-50 14 2 3 44 >600\nclnlbeam-500 38 22 35 453 >600\nacpower-1 18 0 0 3 -\nacpower-10 21 1 2 26 -\nacpower-100 66 14 16 261 -\nTable 2\nTime (sec.) to generate each model and pass it to the solver, a comparison between JuMP and\nexisting commercial and open-source modeling languages fo r derivative-based nonlinear optimization.\nDash indicates not implemented.\nCommercial\nInstance JuMP AMPL GAMS\nclnlbeam-5 0.08 0.03 0.08\nclnlbeam-50 0.70 0.39 0.76\nclnlbeam-500 7.48 3.47 15.81\nacpower-1 0.07 0.02 0.06\nacpower-10 0.66 0.30 0.53\nacpower-100 6.11 3.20 18.13\nTable 3\nTime (sec.) to evaluate derivatives (including gradients, Jacobians, and Hessians) during 3\niterations, as reported by Ipopt. Pyomo relies on AMPL’s “so lver library” for derivative evaluations,\nand YALMIP does not provide second-order derivatives.\n16 Dunning et al.\n6. Extensions. JuMP is designed to be extensible, allowing for developers both\nto plug in new solvers for existing problem classes and to extend the s yntax of JuMP\nitself to new classes of problems. In comparison, it is more common fo r AMLs to sup-\nport only extending the set of solvers for existing, well-deﬁned pro blem classes [ 30].\nA common thread motivating extensions to an AML’s syntax, on the o ther hand, is\nthat the more natural representation of a class of models may be a t a higher level\nthan a standard-form optimization problem. These classes of mode ls furthermore may\nbeneﬁt from customized solution methods which are aware of the hig her-level struc-\nture. Extensions to JuMP can expose these advanced problem clas ses and algorithmic\ntechniques to users who just want to solve a model and not concer n themselves with\nthe low-level details. We will present three extensions we recently d eveloped with this\nmotivation for handling diﬀerent models for optimization under uncer tainty: paral-\nlel multistage stochastic programming, robust optimization, and ch ance constraints.\nWhile these three extensions were developed by the JuMP core deve lopers, we would\nlike to highlight that even more recently a number of syntactic exten sions to JuMP\nhave been developed independently [ 71, 51, 50], illustrating the feasibility of doing so\nwithout intimate knowledge of JuMP’s internals.\n6.1. Extension for parallel multistage stochastic program ming. The ﬁrst\nexample of a modeling extension built on top of JuMP is StructJuMP [ 45] (formerly\nStochJuMP), a modeling layer for block-structured optimization pr oblems of the form,\n(6.1)\nmin 1\n2 xT\n0 Q0x0 + cT\n0 x0 + ∑ N\ni=1\n( 1\n2 xT\ni Qixi + cT\ni xi\n)\ns.t. Ax0 = b0,\nT1x0+ W1x1 = b1,\nT2x0+ W2x2 = b2,\n.\n.\n. ... .\n.\n.\nTN x0+ WN xN = bN ,\nx0 ≥0, x 1 ≥0, x 2 ≥0, . . . , x N ≥0.\nThis structure has been well studied and arises from stochastic pr ogramming [\n15],\ncontingency analysis [ 69], multicommodity ﬂow [ 21], and many other contexts. A num-\nber of specialized methods exist for solving problems with this struct ure (including the\nclassical Benders decomposition method), and they require as inpu t data structures\nthe matrices Qi, Ti, Wi, A, and vectors ci and bi.\nStructJuMP was motivated by the application to stochastic progra mming models\nfor power systems control under uncertainty as outlined in [ 68]. For realistic models,\nthe total number of variables may be in the tens to hundreds of million s, which\nnecessitates the use of parallel computing to obtain solutions within reasonable time\nlimits. In the context of high-performance computing, the phase o f generating the\nmodel in serial can quickly become an execution bottleneck, in additio n to the fact\nthat the combined input data structures may be too large to ﬁt in me mory on a\nsingle machine. StructJuMP was designed to allow users to write JuMP models with\nannotations indicating the block structure such that the input mat rices and vectors\ncan be generated in parallel. That is, the entire model is not built in mem ory in any\nlocation: each computational node only builds the portion of the mod el in memory\nthat it will work with during the course of the optimization procedure . This ability\nto generate the model in parallel (in a distributed-memory MPI-bas ed [ 61] fashion)\ndistinguishes StructJuMP from existing tools such as PySP [ 80].\nStructJuMP successfully scaled up to 2048 cores of a high-perfor mance cluster,\nJuMP 17\nand in all cases the overhead of model generation was a small fract ion of the total\nsolution time. Furthermore, StructJuMP was easy to develop, consisting of less than\n500 lines of code in total, which includes interfacing with a C++-based s olver and the\nMPI library for parallel computing. For comparison, SML [ 42], an AMPL extension\nfor conveying similar block structures to solvers, was implemented a s a pre- and\npost-processor for AMPL. The implementation required reverse e ngineering AMPL’s\nsyntax and developing a custom text-based parser. Such feats o f software engineering\nare not needed to develop extensions to JuMP.\n6.2. Extension for robust optimization. Robust optimization (RO) is a\nmethodology for addressing uncertainty in optimization problems th at has grown in\npopularity over the last decade (for a survey, see [ 6]). The RO approach to uncertainty\nmodels the uncertain parameters in a problem as belonging to an uncertainty set , in-\nstead of modeling them as being drawn from probability distributions. We solve an\nRO problem with respect to the worst-case realization of those unc ertain parameters\nover their uncertainty set, i.e.\nmin\nx∈X\nf (x)(6.2)\nsubject to g (x, ξ) ≤0 ∀ξ ∈Ξ\nwhere x are the decision variables, ξ are the uncertain parameters drawn from the\nuncertainty set Ξ, f : X →R is a function of x and g : X ×Ξ →Rk is a vector-valued\nfunction of both x and ξ. Note that constraints which are not aﬀected by uncertainty\nare captured by the set X. As the uncertainty set Ξ is typically not a ﬁnite set of\nscenarios, RO problems have an inﬁnite set of constraints. This is us ually addressed\nby either reformulating the RO problem using duality to obtain a robust counterpart,\nor by using a cutting-plane method that aims to add only the subset o f constraints\nthat are required at optimality to enforce feasibility [ 8].\nJuMPeR [ 24] is an extension for JuMP that enables modeling RO problems di-\nrectly by introducing the Uncertain modeling primitive for uncertain parameters.\nThe syntax is essentially unchanged from JuMP, except that const raints containing\nonly Uncertains and constants are treated distinctly from other constraints as they\nare used to deﬁne the uncertainty set. JuMPeR is then able to solve the problem by\neither reformulation or the cutting-plane method, allowing the user to switch between\nthe two at will. This is an improvement over both directly modeling the ro bust coun-\nterpart to the RO problem and implementing a cutting-plane method, as it allows\nusers to experiment with diﬀerent uncertainty sets and solution te chniques with min-\nimal changes to their code. Building JuMPeR on top of JuMP makes it mo re useful\nthan a dedicated RO modeling tool like ROME [ 36] as users can smoothly transition\nfrom a deterministic model to an uncertain model and can take adva ntage of the in-\nfrastructure developed for JuMP to utilize a wide variety of solvers . It also beneﬁts\nfrom JuMP’s eﬃcient model construction, oﬀering some performan ce advantages over\nYALMIP’s robust modeling capabilities [ 55].\n6.3. Extension for chance constraints. For the ﬁnal extension, consider\nchance constraints of the form\n(6.3) P(ξT x ≤b) ≥1 −ǫ,\nwhere x is a decision variable and ξ is a random variable. That is, x is feasible if and\nonly if the random variable ξT x is less than b with high probability. Depending on the\n18 Dunning et al.\ndistribution of ξ, the constraint may be intractable and nonconvex; however, for the\nspecial case of ξ jointly Gaussian with mean µ and covariance matrix Σ, it is convex\nand representable by conic-quadratic inequalities. Bienstock et al. [13] observed that\nit can be advantageous to implement a custom cutting-plane algorith m similar to the\ncase of robust optimization. The authors in [ 13] also examined a more conservative\ndistributionally robust model where we enforce that ( 6.3) holds for a family of Gaussian\ndistributions where the parameters fall in some uncertainty set µ ∈Uµ , Σ ∈UΣ .\nJuMPChance is an extension for JuMP which provides a natural algeb raic syn-\ntax to model such chance constraints, hiding the algorithmic details of the chance\nconstraints from users who may be practitioners or experts in oth er domains. Users\nmay declare Gaussian random variables and use them within constrain ts, providing ǫ\nthough a special with probability parameter. JuMPChance was used to evaluate the\ndistributionally robust model in the context of optimal power ﬂow un der uncertainty\nfrom wind generation, ﬁnding that the increased conservatism may actually result in\nrealized cost savings given the inaccuracy of the assumption of Gau ssianity [ 57].\n7. Interactivity and visualization. Although we have focused thus far on\neﬃciently and intuitively communicating optimization problems to a solve r, equally\nas important is a convenient way to interpret, understand, and co mmunicate the\nsolutions obtained. For many use cases, Microsoft Excel and similar spreadsheet\nsystems provide a surprisingly versatile environment for optimizatio n modeling [ 59];\none reason for their continuing success is that it is trivial to interac tively manipulate\nthe input to a problem and visualize the results, completely within the s preadsheet.\nStandalone commercial modeling systems, while providing a much bett er environment\nfor handling larger-scale inputs and models, have in our opinion never achieved such\nseamless interactivity. Notably, however, AIMMS [ 1], a commercial AML, enables\nusers to create interactive graphical user interfaces. We highligh t, however, that both\nAIMMS and Excel-based solutions like SolverStudio [ 59] require commercial software\nand are available only for the Windows operating system.\nMany in the scientiﬁc community are beginning to embrace the “noteb ook” for-\nmat for both research and teaching [ 73]. Notebooks allow users to mix code, rich\ntext, LATEX equations, visualizations, and interactive widgets all in one sharea ble\ndocument, creating compelling narratives which do not require any lo w-level coding\nto develop. Jupyter [ 67], in particular, contains the IJulia notebook environment for\nJulia and therefore JuMP as well. Taking advantage of the previously demonstrated\nspeed of JuMP, one can easily create notebooks that embed large- scale optimization\nproblems, which we will illustrate with two examples in this section. We be lieve that\nnotebooks provide a satisfying solution in many contexts to the long standing challenge\nof providing an interactive interface for optimization.\n7.1. Example: Portfolio Optimization. One of the classic problems in ﬁ-\nnancial optimization is the Markowitz portfolio optimization problem [ 58] where we\nseek to optimally allocate funds between n assets. The problem considers the mean\nand variance of the return of the resulting portfolio, and seeks to ﬁnd the portfolio\nthat minimizes variance such that mean return is at least some minimal value. This\nis a quadratic optimization problem with linear constraints. It is natur al that we\nwould want to explore how the optimal portfolio’s variance changes a s we change the\nminimum return: the so-called eﬃcient frontier .\nIn Figure 5 we have displayed a small notebook that solves the Markowitz portf olio\noptimization problem. The notebook begins with rich text describing t he formulation,\nafter which we use JuMP to succinctly express the optimization prob lem. The data is\nJuMP 19\nFig. 5 . A Jupyter (IJulia) Notebook for a Markowitz portfolio probl em [ 58] that combines rich\ntext with equations, Julia/JuMP code, an interactive widge t, and a visualization. Moving the rmin\nslider re-solves the optimization problem to ﬁnd a new portf olio, and the plot is updated to show the\nhistorical distribution of returns that would have been obt ained with the portfolio.\ngenerated synthetically, but could be acquired from a database, s preadsheets, or even\ndirectly from the Internet. The Julia package Interact.jl [ 38] provides the @manipulate\nsyntax, which automatically generates the minimum return slider fro m the deﬁnition\nof the for loop. As the user drags the slider, the model is rebuilt with the new pa -\nrameter and re-solved, enabling easy, interactive experimentatio n. The visualization\n(implemented with the Gadﬂy [ 47] package) of the distribution of historical returns\nthat would have been obtained with this optimal portfolio is also regen erated as the\nslider is dragged.\n7.2. Example: Rocket Control. A natural goal in aerospace engineering is to\nmaximize the altitude attained by a rocket in ﬂight. This problem was po ssibly ﬁrst\nstated by Goddard [ 35], and has since become a standard problem in control theory,\ne.g. [ 18]. The “Goddard Rocket” optimization problem, as expressed in [ 22], has three\nstate variables (altitude, velocity, and remaining mass) and one con trol (thrust). The\nrocket is aﬀected by aerodynamic drag and gravity, and the const raints of the problem\nimplement the equations of motion (discretized by using the trapezo idal rule).\nWe have implemented the optimization problem with JuMP in an IJulia note book.\nMoreover we have used Interact.jl to allow the user to explore the eﬀects of varying the\nmaximum thrust (via Tc) and the coeﬃcient that controls the relationship between\naltitude and drag ( hc). The JuMP code is omitted for the sake of brevity, but the\nsliders and plots of the state and control over time are displayed in F igure 6. The\nmodel is re-solved with the new parameters every time the user mov es the sliders;\nthis takes about a twentieth of a second on a laptop computer, ena bling real-time\ninteractive exploration of this complex nonlinear optimization model.\nSupplementary materials. The benchmark instances used in Sections 4 and 5\nand the notebooks presented in Section 7 are available as supplementary materials at\nhttps://github.com/mlubin/JuMPSupplement. The site http://www.juliaopt.org/\nis the homepage for JuMP and other optimization-related projects in Julia.\nAcknowledgements. We thank Paul Hovland and Jean Utke for helpful dis-\ncussions on automatic diﬀerentiation. We thank Juan Pablo Vielma, Ch ris Coey,\n20 Dunning et al.\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\u0001\t\n\u000b\u0007\f\n\u0001\n\u0004\n\u0005\n\u0006\n\u0007\n\u0003\n\b\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\r\u0004\u000e\u000f\u0010\u0002\f\u0011\n\u0001\u0002\u0001\u0001\n\u0001\u0002\u0001\u0003\n\u0001\u0002\u0004\u0001\n\u0001\u0002\u0004\u0003\n\u0001\u0002\u0005\u0001\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\u0012\u0013\u0007\u0007\n\u0001\u0002\u0003\n\u0001\u0002\b\n\u0001\u0002\t\n\u0001\u0002\n\u0001\u0002\u000b\n\u0004\u0002\u0001\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\u0014\u000e\f\u0002\f\u000b\u0015\u0004\n\u0001\u0002\u000b\u000b\u0003\n\u0004\u0002\u0001\u0001\u0001\n\u0004\u0002\u0001\u0001\u0003\n\u0004\u0002\u0001\u0004\u0001\n\u0004\u0002\u0001\u0004\u0003\n\u0004\u0002\u0001\u0005\u0001\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\u0001\t\n\u000b\u0007\f\n\u0001\n\u0004\n\u0005\n\u0006\n\u0007\n\u0003\n\b\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\r\u0004\u000e\u000f\u0010\u0002\f\u0011\n\u0001\u0002\u0001\u0001\n\u0001\u0002\u0001\u0003\n\u0001\u0002\u0004\u0001\n\u0001\u0002\u0004\u0003\n\u0001\u0002\u0005\u0001\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\u0012\u0013\u0007\u0007\n\u0001\u0002\u0003\n\u0001\u0002\b\n\u0001\u0002\t\n\u0001\u0002\n\u0001\u0002\u000b\n\u0004\u0002\u0001\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0001\u0001 \u0001\u0002\u0001\u0003 \u0001\u0002\u0004\u0001 \u0001\u0002\u0004\u0003 \u0001\u0002\u0005\u0001 \u0001\u0002\u0005\u0003\n\u0014\u000e\f\u0002\f\u000b\u0015\u0004\n\u0001\u0002\u000b\u000b\u0003\n\u0004\u0002\u0001\u0001\u0001\n\u0004\u0002\u0001\u0001\u0003\n\u0004\u0002\u0001\u0004\u0001\n\u0004\u0002\u0001\u0004\u0003\n\u0004\u0002\u0001\u0005\u0001\nFig. 6 . Visualization of the states (altitude,mass,velocity) and the control (thrust) for a rocket\noptimal control problem. The top set of ﬁgures is obtained fo r the thrust and drag parameters, resp.,\nTc = 3 .5 and hc = 500 , and the second are obtained for the parameters Tc = 2 .5, hc = 300 , with all\nunits normalized and dimensionless. We can see that the incr eased drag and reduced maximum\nthrust in the bottom set of ﬁgures has a substantial impact on maximum altitude and leads to a very\ndiﬀerent thrust proﬁle.\nChris Maes, Victor Zverovich, and the anonymous referees for th eir comments on this\nmanuscript which improved its presentation. This work would not be p ossible without\nthe supportive community of Julia developers and users who are too many to name.\nWe thank Carlo Baldassi, Jack Dunn, Joaquim Dias Garcia, Jenny Hong , Steven G.\nJohnson, Tony Kelman, Dahua Lin, Karanveer Mohan, Yee Sian Ng, E lliot Saba,\nJo˜ ao Felipe Santos, Robert Schwarz, Felipe Serrano, Madeleine Ud ell, Ulf Worsøe (of\nMosek), and David Zeng for signiﬁcant contributions to solver inter faces in Julia. We\nthank Jarrett Revels for his work on the ForwardDiﬀ.jl package, a nd Steven Dirkse\nfor his help with the GAMS version of the minimum cost ﬂow example. Fina lly, we\nthank the many students in the Operations Research Center at MI T who have been\nearly adopters of JuMP. This material is based upon work supporte d by the National\nScience Foundation Graduate Research Fellowship under Grant No. 1122374. M. Lu-\nbin was supported by the DOE Computational Science Graduate Fello wship, which\nis provided under grant number DE-FG02-97ER25308.\nREFERENCES\n[1] AIMMS, AIMMS: The user’s guide , 2015.\n[2] M. S. Alnæs, A. Logg, K. B. Ølgaard, M. E. Rognes, and G. N. Wells , Uniﬁed form lan-\nguage: A domain-speciﬁc language for weak formulations of p artial diﬀerential equations ,\nACM Trans. Math. Softw., 40 (2014), pp. 9:1–9:37.\n[3] J. Andersson , A General-Purpose Software Framework for Dynamic Optimiza tion, PhD\nthesis, Arenberg Doctoral School, KU Leuven, Department of Electrical Engineering\n(ESAT/SCD) and Optimization in Engineering Center, Kastee lpark Arenberg 10, 3001-\nHeverlee, Belgium, October 2013.\n[4] J. Andersson, J. Akesson, and M. Diehl , Dynamic optimization with CasADi , in Decision\nand Control (CDC), 2012 IEEE 51st Annual Conference on, Dec 2 012, pp. 681–686.\n[5] A. Atamt ¨urk and M. Savelsbergh , Integer-programming software systems , Annals of Oper-\nJuMP 21\nations Research, 140 (2005), pp. 67–124.\n[6] D. Bertsimas, D. B. Brown, and C. Caramanis , Theory and applications of robust optimiza-\ntion, SIAM review, 53 (2011), pp. 464–501.\n[7] D. Bertsimas and F. de Ruiter , Duality in two-stage adaptive linear optimization: Faster\ncomputation and stronger bounds , INFORMS Journal on Computing, 28 (2016), pp. 500–\n511.\n[8] D. Bertsimas, I. Dunning, and M. Lubin , Reformulation versus cutting-planes for robust\noptimization, Computational Management Science, 13 (2016), pp. 195–217 .\n[9] D. Bertsimas and J. Tsitsiklis , Introduction to linear programming , Athena Scientiﬁc, 1\n(1997), p. 997.\n[10] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah , Julia: A fresh approach to nu-\nmerical computing , CoRR, abs/1411.1607 (2014).\n[11] J. Bezanson, S. Karpinski, V. Shah, and A. Edelman , Why we created julia .\nhttp://julialang.org/blog/2/zero.noslash12//zero.noslash2/why-we-created-julia, 2012. [Online; accessed 20-\nJuly-2016].\n[12] J. Bezanson, S. Karpinski, V. B. Shah, and A. Edelman , Julia: A fast dynamic language\nfor technical computing , CoRR, abs/1209.5145 (2012).\n[13] D. Bienstock, M. Chertkov, and S. Harnett , Chance-constrained optimal power ﬂow: Risk-\naware network control under uncertainty , SIAM Review, 56 (2014), pp. 461–495.\n[14] D. Bienstock and O. G ¨unl¨uk, Computational experience with a diﬃcult mixedinteger mult i-\ncommodity ﬂow problem , Mathematical Programming, 68 (1995), pp. 213–237.\n[15] J. Birge and F. Louveaux , Introduction to Stochastic Programming , Springer Series in Op-\nerations Research and Financial Engineering Series, Sprin ger, New York, 2nd ed., 2011.\n[16] C. Bischof, P. Khademi, A. Mauer, and A. Carle , Adifor 2.0: Automatic diﬀerentiation\nof Fortran 77 programs , Computational Science Engineering, IEEE, 3 (1996), pp. 18 –32.\n[17] A. Brooke, D. Kendrick, A. Meeraus, and R. Raman , GAMS: A User’s Guide , Scientiﬁc\nPress, 1999.\n[18] A. E. Bryson , Dynamic Optimization , Addison Wesley Longman Menlo Park, CA, 1999.\n[19] E. Burnell and W. Hoburg , Gpkit software for geometric programming .\nhttps://github.com/hoburg/gpkit, 2015. Version 0.4.1.\n[20] M. R. Bussieck, M. C. Ferris, and T. Lohmann , GUSS: Solving collections of data related\nmodels within GAMS , in Algebraic Modeling Systems, J. Kallrath, ed., vol. 104 o f Applied\nOptimization, Springer Berlin Heidelberg, 2012, pp. 35–56 .\n[21] J. Castro , An interior-point approach for primal block-angular probl ems, Computational Op-\ntimization and Applications, 36 (2007), pp. 195–219.\n[22] E. D. Dolan, J. J. Mor ´e, and T. S. Munson , Benchmarking optimization software with\nCOPS 3.0 , Argonne National Laboratory Technical Report ANL/MCS-TM -273, (2004).\n[23] J. Dongarra, J. Bunch, C. Moler, and G. Stewart , LINPACK Users’ Guide , Society for\nIndustrial and Applied Mathematics, 1979.\n[24] I. Dunning , Advances in Robust and Adaptive Optimization: Algorithms, Software, and In-\nsights, PhD thesis, Massachusetts Institute of Technology, 2016.\n[25] I. Dunning, V. Gupta, A. King, J. Kung, M. Lubin, and J. Silberho lz, A course on\nadvanced software tools for operations research and analyt ics, INFORMS Transactions on\nEducation, 15 (2015), pp. 169–179.\n[26] M. Ferris, P. Gill, T. Kelley, and J. Lee , Beale-Orchard-Hays prize citation , 2012.http://www.mathopt.org/?nav=boh_2/zero.noslash12[Online; accessed 29-January-2015].\n[27] R. Fourer , On the evolution of optimization modeling systems , in Optimization Stories,\nM. Grotschel, ed., Documenta Mathematica, 2012, p. 377–388 .\n[28] R. Fourer, D. M. Gay, and B. W. Kernighan , A modeling language for mathematical\nprogramming, Management Science, 36 (1990), pp. 519–554.\n[29] R. Fourer, D. M. Gay, and B. W. Kernighan , AMPL: A modeling language for mathematical\nprogramming, Brooks/Cole, Paciﬁc Grove, CA, 2nd ed., 2003.\n[30] E. Fragni `ere, J. Gondzio, R. Sarkissian, and J.-P. Vial , A structure-exploiting tool in\nalgebraic modeling languages , Management Science, 46 (2000), pp. 1145–1158.\n[31] D. M. Gay , More AD of nonlinear AMPL models: Computing Hessian informa tion and ex-\nploiting partial separability , in Computational Diﬀerentiation: Applications, Techniq ues,\nand, 1996, pp. 173–184.\n[32] A. H. Gebremedhin, A. Tarafdar, A. Pothen, and A. W alther , Eﬃcient computation of\nsparse Hessians using coloring and automatic diﬀerentiati on, INFORMS J. on Computing,\n21 (2009), pp. 209–223.\n[33] P. Gill, W. Murray, and M. Wright , Practical optimization , Academic Press, 1981.\n[34] R. Giordano, T. Broderick, and M. Jordan , Linear response methods for accurate co-\n22 Dunning et al.\nvariance estimates from mean ﬁeld variational bayes , in Neural Information Processing\nSystems, 2015.\n[35] R. H. Goddard , A method of reaching extreme altitudes. , Nature, 105 (1920), pp. 809–811.\n[36] J. Goh and M. Sim , Robust optimization made easy with ROME , Operations Research, 59\n(2011), pp. 973–985.\n[37] B. Gough , GNU Scientiﬁc Library Reference Manual - Third Edition , Network Theory Ltd.,\n3rd ed., 2009.\n[38] S. Gowda , Interact.jl, 2015. https://github.com/JuliaLang/Interact.jl [Online; accessed 14-\nApril-2015].\n[39] M. Grant and S. Boyd , CVX: MATLAB software for disciplined convex programming, v er-\nsion 2.1 . http://cvxr.com/cvx, Mar. 2014.\n[40] M. Grant, S. Boyd, and Y. Ye , Disciplined convex programming , in Global Optimization:\nFrom Theory to Implementation, Nonconvex Optimization and Its Application Series,\nSpringer, 2006, pp. 155–210.\n[41] A. Griewank and A. W alther , Evaluating Derivatives: Principles and Techniques of Algo -\nrithmic Diﬀerentiation , no. 105 in Other Titles in Applied Mathematics, SIAM, Phila del-\nphia, PA, 2nd ed., 2008.\n[42] A. Grothey, J. Hogg, K. Woodsend, M. Colombo, and J. Gondzio , A structure conveying\nparallelizable modeling language for mathematical progra mming, in Parallel Scientiﬁc Com-\nputing and Optimization, vol. 27 of Springer Optimization a nd Its Applications, Springer\nNew York, 2009, pp. 145–156.\n[43] Gurobi Optimization, Inc. , Gurobi optimizer reference manual , 2015.\n[44] W. E. Hart, J.-P. W atson, and D. L. Woodruff , Pyomo: Modeling and solving mathemati-\ncal programs in Python , Mathematical Programming Computation, 3 (2011), pp. 219– 260.\n[45] J. Huchette, M. Lubin, and C. Petra , Parallel algebraic modeling for stochastic optimiza-\ntion, in “Proceedings of HPTCDL ’14”, IEEE Press, 2014, pp. 29–35 .\n[46] A. S. Jacobsen, L. Stagner, M. Salewski, B. Geiger, W. W. Heidbr ink, S. B. Korsholm,\nF. Leipold, S. K. Nielsen, J. Rasmussen, M. Stejner, H. Thomsen , M. Weiland, and\nthe ASDEX Upgrade team , Inversion methods for fast-ion velocity-space tomography in\nfusion plasmas , Plasma Physics and Controlled Fusion, 58 (2016), p. 045016 .\n[47] D. Jones et al. , Gadﬂy.jl: Version 0.3.9 , Sept. 2014.\n[48] J. Kallrath , Polylithic modeling and solution approaches using algebra ic modeling systems ,\nOptimization Letters, 5 (2011), pp. 453–466.\n[49] N. Korolko and Z. Sahinoglu , Robust optimization of EV charging schedules in unregulate d\nelectricity markets , Smart Grid, IEEE Transactions on, in press (2015).\n[50] C. Kwon , Complementarity.jl. https://github.com/chkwon/Complementarity.jl. [Online; ac-\ncessed 7-July-2016].\n[51] , VariationalInequality.jl. https://github.com/chkwon/VariationalInequality.jl. [On-\nline; accessed 7-July-2016].\n[52] C. Lattner and V. Adve , LLVM: A compilation framework for lifelong program analysi s &\ntransformation, in Code Generation and Optimization, 2004. International Symposium on,\nIEEE, 2004, pp. 75–86.\n[53] M. S. Lobo, L. V andenberghe, S. Boyd, and H. Lebret , Applications of second-order cone\nprogramming, Linear Algebra and its Applications, 284 (1998), pp. 193 – 2 28. International\nLinear Algebra Society (ILAS) Symposium on Fast Algorithms for Control, Signals and\nImage Processing.\n[54] J. L ¨ofberg, YALMIP: A toolbox for modeling and optimization in MATLAB , in Computer\nAided Control Systems Design, 2004 IEEE International Symp osium on, IEEE, 2004,\npp. 284–289.\n[55] J. L ¨ofberg, Automatic robust convex programming , Optimization methods and software, 27\n(2012), pp. 115–129.\n[56] M. Lubin and I. Dunning , Computing in operations research using Julia , INFORMS Journal\non Computing, 27 (2015), pp. 238–248.\n[57] M. Lubin, Y. Dvorkin, and S. Backhaus , A robust approach to chance constrained optimal\npower ﬂow with renewable generation , Power Systems, IEEE Transactions on, to appear\n(2015), pp. 1–10.\n[58] H. Markowitz , Portfolio selection , The journal of ﬁnance, 7 (1952), pp. 77–91.\n[59] A. J. Mason , SolverStudio: A new tool for better optimisation and simula tion modelling in\nexcel, INFORMS Transactions on Education, 14 (2013), pp. 45–52.\n[60] H. Maurer and H. D. Mittelmann , The non-linear beam via optimal control with bounded\nstate variables , Optimal Control Applications and Methods, 12 (1991), pp. 1 9–31.\n[61] Message Passing Forum , MPI: A Message-Passing Interface Standard , tech. rep., Knoxville,\nJuMP 23\nTN, USA, 1994.\n[62] H. D. Mittelmann , Suﬃcient optimality for discretized parabolic and ellipti c control problems ,\nin Fast Solution of Discretized Optimization Problems, Spr inger, 2001, pp. 184–196.\n[63] U. Naumann , The Art of Diﬀerentiating Computer Programs , Society for Industrial and Ap-\nplied Mathematics, 2011.\n[64] R. D. Neidinger , Introduction to automatic diﬀerentiation and MATLAB objec t-oriented pro-\ngramming, SIAM Review, 52 (2010), pp. 545–563.\n[65] W. Orchard-Hays , History of mathematical programming systems , IEEE Annals of the His-\ntory of Computing, 6 (1984), pp. 296–312.\n[66] S. H. Owen and M. S. Daskin , Strategic facility location: A review , European Journal of\nOperational Research, 111 (1998), pp. 423–447.\n[67] F. Perez , IPython: From interactive computing to computational narr atives, in 2015 AAAS\nAnnual Meeting (12-16 February 2015), AAAS, 2015.\n[68] C. G. Petra, V. Zavala, E. Nino-Ruiz, and M. Anitescu , Economic impacts of wind co-\nvariance estimation on power grid operations , Preprint ANL/MCS-P5M8-0614, (2014).\n[69] D. T. Phan and A. Koc , Optimization approaches to security-constrained unit com mitment\nand economic dispatch with uncertainty analysis , in Optimization and Security Challenges\nin Smart Power Grids, V. Pappu, M. Carvalho, and P. Pardalos, eds., Energy Systems,\nSpringer Berlin Heidelberg, 2013, pp. 1–37.\n[70] J. Revels, M. Lubin, and T. Papamarkou , Forward-Mode Automatic Diﬀerentiation in Julia ,\nArXiv e-prints, (2016).\n[71] A. N. Riseth , MultiJuMP.jl. https://github.com/anriseth/MultiJuMP.jl. [Online; accessed\n7-July-2016].\n[72] D. Shelar and S. Amin , Analyzing vulnerability of electricity distribution netw orks to DER\ndisruptions, in American Control Conference (ACC), 2015, July 2015, pp. 2461–2468.\n[73] H. Shen , Interactive notebooks: Sharing the code. , Nature, 515 (2014), pp. 151–152.\n[74] D. Spinellis , Notable design patterns for domain-speciﬁc languages , Journal of Systems and\nSoftware, 56 (2001), pp. 91 – 99.\n[75] Stan Development Team , Stan Modeling Language Users Guide and Reference Manual,\nVersion 2.5.0 , 2014.\n[76] M. Udell, K. Mohan, D. Zeng, J. Hong, S. Diamond, and S. Boyd , Convex optimization in\nJulia, in Proceedings of the 1st First Workshop for High Performan ce Technical Computing\nin Dynamic Languages, HPTCDL ’14, Piscataway, NJ, USA, 2014 , IEEE Press, pp. 18–28.\n[77] J. P. Vielma, I. Dunning, J. Huchette, and M. Lubin , Extended Formulations in Mixed\nInteger Conic Quadratic Programming , ArXiv e-prints, (2015).\n[78] A. W ¨achter and L. T. Biegler , On the implementation of an interior-point ﬁlter line-sear ch\nalgorithm for large-scale nonlinear programming , Mathematical Programming, 106 (2006),\npp. 25–57.\n[79] A. W alther and A. Griewank , Getting started with ADOL-C , in Combinatorial Scientiﬁc\nComputing, U. Naumann and O. Schenk, eds., Chapman-Hall CRC Computational Science,\n2012, ch. 7, pp. 181–202.\n[80] J.-P. W atson, D. Woodruff, and W. Hart , PySP: Modeling and solving stochastic programs\nin Python , Mathematical Programming Computation, 4 (2012), pp. 109– 149.\n[81] V. Zverovich, C. I. F ´abi´an, E. F. D. Ellison, and G. Mitra , A computational study of a\nsolver system for processing two-stage stochastic LPs with enhanced Benders decomposi-\ntion, Mathematical Programming Computation, 4 (2012), pp. 211– 238.\n8. Appendix: Benchmark models for Section 4.1.\n8.1. lqcp. The linear-quadratic control problem is Equation (5.2-I) from [ 62].\nThis model has a quadratic objective and linear constraints, and ca n be scaled by\nincreasing the discretization (parameters m and n) of the two-dimensional problem\ndomain. For the purposes of benchmarking we measured the model generation time\nacross a range of sizes, ﬁxing m = n and varying n ∈{500, 1000, 1500, 2000}. In the\nnotation below, we deﬁne I = {0, . . . , m }to be the index set along the ﬁrst dimension\nand J = {0, . . . , n }as the index set for the second. We additionally deﬁne I′ ←I\\{m}\nand J′ ←J \\{0, n}, with all other parameters deﬁned as in the above reference.\n24 Dunning et al.\nmin\nu, y\n1\n4 ∆ x\n\n(\nym, 0 −yt\n0\n) 2\n+ 2\nn−1∑\nj=1\n(\nym,j −yt\nj\n) 2\n+\n(\nym,n −yt\nn\n) 2\n\n+\n1\n4 a∆ t\n(\n2\nm−1∑\ni=1\nu2\ni + u2\nm\n)\ns.t. 1/∆ t (yi+1,j −yi,j ) =\n1\n2h2\n(yi,j −1 −2yi,j + yi,j +1 + yi+1,j −1 −2yi+1,j + yi+1,j +1) ∀i ∈I′, j ∈J′\ny0,j = 0 ∀j ∈J\nyi, 2 −4yi, 1 + 3yi, 0 = 0 ∀i ∈I\n1/2∆ x (yi,n −2 −4yi,n −1 + 3yi,n ) = ui −yi,n ∀i ∈I\n−1 ≤ui ≤1 ∀i ∈I\n0 ≤yi,j ≤1 ∀i ∈I, j ∈J\n8.2. fac. The fac problem is a variant on the classic facility location prob-\nlem [ 66]: given customers (indexed by c ∈{1, . . . , C }) located at the points xc ∈RK ,\nlocate facilities (indexed by f ∈{1, . . . , F }) at the points yf ∈RK such that the\nmaximum distance between a customer and its nearest facility is minimiz ed. This\nproblem can be expressed most naturally in the form of a mixed-integ er second-order\ncone problem (MISOCP), and a solved example of this problem is prese nted in Fig-\nure 7. We generated the problem data deterministically to enable fair comp arison\nacross the diﬀerent languages: the customers are placed on a two -dimensional grid\n(K = 2) i ∈{0, . . . , G }by j ∈{0, . . . , G }, with the points xc spaced evenly over the\nunit square [0 , 1]2. The problem size is thus parametrized by the grid size G and the\nnumber of facilities F , with the number of variables and constraints growing propor-\ntional to F ·G2. For the purposes of benchmarking we measured the model gener ation\nwith ﬁxed F = G and varying F ∈{25, 50, 75, 100}.\nmin\nd, y, z\nd(8.1)\nsubject to d ≥∥xc −yf ∥2 −M (1 −zc,f ) ∀c, f\nF∑\nf=1\nzc,f = 1 ∀c\nzc,f ∈{0, 1} ∀ c, f,\nwhere\nM = max\nc,c ′\n∥xc −xc′ ∥2\nand zc,f is a binary indicator variable that is 1 if facility f is closer to customer c than\nany other facility, 0 otherwise. As needed, we translate the conic- quadratic constraint\nin ( 8.1) to an equivalent quadratic form depending on the modeling system.\n9. Benchmark models for Section 5.4.\nJuMP 25\nFig. 7 . One possible optimal solution to the facility location prob lem with a four-by-four grid of\ncustomers (rectangles) and three facilities (circles). Th e dotted circles show the maximum distance\nbetween any customer and it’s closest facility, which is the objective.\n9.1. clnlbeam. The ﬁrst model, clnlbeam, is a nonlinear beam control problem\nobtained from Hans Mittelmann’s AMPL-NLP benchmark set 6; see also [ 60]. It can\nbe scaled by increasing the discretization of the one-dimensional do main through the\nparameter n. We test with n ∈{5000, 50000, 500000}. The model has 3 n variables,\n2n constraints, and diagonal Hessians. The algebraic representatio n follows below.\nmin\nt,x,u ∈Rn+1\nn∑\ni=1\n[ h\n2 (u2\ni+1 + u2\ni ) + αh\n2 (cos(ti+1) + cos( ti))\n]\nsubject to xi+1 −xi − 1\n2n(sin(ti+1) + sin( ti)) = 0 i = 1 , . . . , n\nti+1 −ti − 1\n2nui+1 − 1\n2nui = 0 i = 1 , . . . , n\n−1 ≤ti ≤1, −0.05 ≤xi ≤0.05 i = 1 , . . . , n + 1\nx1 = xn+1 = t1 = tn+1 = 0 .\n9.2. acpower. The second model is a nonlinear AC power ﬂow model published\nin AMPL format by Artelys Knitro 7. The objective is to minimize active power losses\n(9.1)\n∑\nk\n[\ngk +\n∑\nm\nVkVm(Gkm cos(θk −θm) + Bkm sin(θk −θm))\n] 2\nsubject to balancing both active and reactive power loads and dema nds at each node in\nthe grid, where power ﬂows are constrained by the highly nonlinear K irchoﬀ’s laws.\nThe parameter gk is the active power load (demand) at node k, Vk is the voltage\nmagnitude at node k, θk is the phase angle, and Ykm = Gkm + iBkm is the complex-\nvalued admittance between nodes k and m, which itself is a complicated nonlinear\nfunction of the decision variables. Depending on the physical chara cteristics of the\ngrid, some values (e.g., Vk) may be decision variables at some nodes and ﬁxed at\nothers. This model is quite challenging because of the combination of nonlinearity\nand network structure, which yields a highly structured Hessian.\n6http://plato.asu.edu/ftp/ampl-nlp.html accessed July 7, 2016.\n7https://web.archive.org/web/2/zero.noslash15/zero.noslash1/zero.noslash5161742/http://www.ziena.com/elecpower.htm accessed\nJuly 7, 2016.\n26 Dunning et al.\nWe translated the AMPL model provided by Artelys Knitro to JuMP, G AMS,\nand Pyomo. The base instance has a network with 662 nodes and 101 7 edges; there\nare 1489 decision variables, 1324 constraints, and the Hessian (of the Lagrangian)\nhas 8121 nonzero elements. We artiﬁcially enlarge the instances by d uplicating the\nnetwork 10-fold and 100-fold, which results in proportional increa ses in the problem\ndimensions.",
  "topic": null,
  "concepts": [],
  "institutions": []
}