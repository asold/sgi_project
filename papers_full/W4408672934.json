{
  "title": "Real-World Evaluation of Large Language Models in Healthcare (RWE-LLM): A New Realm of AI Safety &amp; Validation",
  "url": "https://openalex.org/W4408672934",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2709690566",
      "name": "Meenesh Bhimani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096017383",
      "name": "Alex Miller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2494389096",
      "name": "Jonathan D. Agnew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2824808625",
      "name": "Markel Sanz Ausin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5077841125",
      "name": "Mariska Raglow-Defranco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4311116453",
      "name": "Harpreet Mangat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094216812",
      "name": "Michelle Voisard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096921879",
      "name": "Maggie Taylor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094216818",
      "name": "Sebastian Bierman-Lytle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222817993",
      "name": "Vishal Parikh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116722067",
      "name": "Juliana Ghukasyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2807003895",
      "name": "Rae Lasko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184860997",
      "name": "Saad Godil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1359939773",
      "name": "Ashish Atreja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098576487",
      "name": "Subhabrata Mukherjee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118378000",
    "https://openalex.org/W4206980719",
    "https://openalex.org/W3184842396",
    "https://openalex.org/W3155778608",
    "https://openalex.org/W2979042524",
    "https://openalex.org/W3207900783",
    "https://openalex.org/W3012810655",
    "https://openalex.org/W4400798276",
    "https://openalex.org/W4391075943",
    "https://openalex.org/W4399284154",
    "https://openalex.org/W4388706512",
    "https://openalex.org/W4206410242",
    "https://openalex.org/W4399118706",
    "https://openalex.org/W4401246313",
    "https://openalex.org/W4313416341",
    "https://openalex.org/W3201412616",
    "https://openalex.org/W4403420208",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4395467677",
    "https://openalex.org/W4394063739",
    "https://openalex.org/W4402748869",
    "https://openalex.org/W2989512989",
    "https://openalex.org/W3093602247",
    "https://openalex.org/W2910707576",
    "https://openalex.org/W3012898162",
    "https://openalex.org/W3048236382",
    "https://openalex.org/W4392415015",
    "https://openalex.org/W4285504770",
    "https://openalex.org/W3187793209",
    "https://openalex.org/W4210605645",
    "https://openalex.org/W3138676254",
    "https://openalex.org/W2938286792",
    "https://openalex.org/W4400495042",
    "https://openalex.org/W4402464029"
  ],
  "abstract": "Abstract Background The deployment of artificial intelligence (AI) in healthcare necessitates robust safety validation frameworks, particularly for systems directly interacting with patients. While theoretical frameworks exist, there remains a critical gap between abstract principles and practical implementation. Traditional LLM benchmarking approaches provide very limited output coverage and are insufficient for healthcare applications requiring high safety standards. Objective To develop and evaluate a comprehensive framework for healthcare AI safety validation through large-scale clinician engagement. Methods We implemented the RWE-LLM (Real-World Evaluation of Large Language Models in Healthcare) framework, drawing inspiration from red teaming methodologies while expanding their scope to achieve comprehensive safety validation. Our approach emphasizes output testing rather than relying solely on input data quality across four stages: pre-implementation, tiered review, resolution, and continuous monitoring. We engaged 6,234 US licensed clinicians (5,969 nurses and 265 physicians) with an average of 11.5 years of clinical experience. The framework employed a three-tier review process for error detection and resolution, evaluating a non-diagnostic AI Care Agent focused on patient education, follow-ups, and administrative support across four iterations (pre-Polaris and Polaris 1.0, 2.0, and 3.0). Results Over 307,000 unique calls were evaluated using the RWE-LLM framework. Each interaction was subject to potential error flagging across multiple severity categories, from minor clinical inaccuracies to significant safety concerns. The multi-tiered review system successfully processed all flagged interactions, with internal nursing reviews providing initial expert evaluation followed by physician adjudication when necessary. The framework demonstrated effective throughput in addressing identified safety concerns while maintaining consistent processing times and documentation standards. Systematic improvements in safety protocols were achieved through a continuous feedback loop between error identification and system enhancement. Performance metrics demonstrated substantial safety improvements between iterations, with correct medical advice rates improving from âˆ¼80.0% (pre-Polaris), to 96.79% (Polaris 1.0), to 98.75% (Polaris 2.0) and 99.38% (Polaris 3.0). Incorrect advice resulting in potential minor harm decreased from 1.32% to 0.13% and 0.07%, and severe harm concerns were eliminated (0.06% to 0.10% and 0.00%). Conclusions The successful nationwide implementation of the RWE-LLM framework establishes a practical model for ensuring AI safety in healthcare settings. Our methodology demonstrates that comprehensive output testing provides significantly stronger safety assurance than traditional input validation approaches used by horizontal LLMs. While resource-intensive, this approach proves that rigorous safety validation for healthcare AI systems is both necessary and achievable, setting a benchmark for future deployments.",
  "full_text": null,
  "topic": "Realm",
  "concepts": [
    {
      "name": "Realm",
      "score": 0.8764684200286865
    },
    {
      "name": "Health care",
      "score": 0.5693684220314026
    },
    {
      "name": "Computer science",
      "score": 0.3660188317298889
    },
    {
      "name": "Political science",
      "score": 0.22905725240707397
    },
    {
      "name": "Law",
      "score": 0.08749458193778992
    }
  ]
}