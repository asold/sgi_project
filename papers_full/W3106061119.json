{
  "title": "Pre-Training Transformers as Energy-Based Cloze Models",
  "url": "https://openalex.org/W3106061119",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2102969294",
      "name": "Kevin Clark",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4279417045",
      "name": "Minh-Thang Luong",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2109481008",
      "name": "Quoc Le",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2149153931",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251519907",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2161914416",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W4288412300",
    "https://openalex.org/W4300455941",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2026799324",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W2890487780",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962736473",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2922772346",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2841543429",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3018305985",
    "https://openalex.org/W2794509261",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2116064496",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1984635093",
    "https://openalex.org/W2995890624",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2521028896",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2566467060",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 285–294,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n285\nPre-Training Transformers as Energy-Based Cloze Models\nKevin Clark1 Minh-Thang Luong2 Quoc V . Le2 Christopher D. Manning1\n1Stanford University 2Google Brain\nkevclark@cs.stanford.edu, thangluong@google.com\nqvl@google.com, manning@cs.stanford.edu\nAbstract\nWe introduce Electric, an energy-based cloze\nmodel for representation learning over text.\nLike BERT, it is a conditional generative\nmodel of tokens given their contexts. How-\never, Electric does not use masking or output a\nfull distribution over tokens that could occur\nin a context. Instead, it assigns a scalar en-\nergy score to each input token indicating how\nlikely it is given its context. We train Electric\nusing an algorithm based on noise-contrastive\nestimation and elucidate how this learning ob-\njective is closely related to the recently pro-\nposed ELECTRA pre-training method. Elec-\ntric performs well when transferred to down-\nstream tasks and is particularly effective at\nproducing likelihood scores for text: it re-\nranks speech recognition n-best lists better\nthan language models and much faster than\nmasked language models. Furthermore, it of-\nfers a clearer and more principled view of what\nELECTRA learns during pre-training.\n1 Introduction\nThe cloze task (Taylor, 1953) of predicting the iden-\ntity of a token given its surrounding context has\nproven highly effective for representation learn-\ning over text. BERT (Devlin et al., 2019) imple-\nments the cloze task by replacing input tokens with\n[MASK], but this approach incurs drawbacks in\nefﬁciency (only 15% of tokens are masked out at\na time) and introduces a pre-train/ﬁne-tune mis-\nmatch where BERT sees [MASK] tokens in train-\ning but not in ﬁne-tuning. ELECTRA (Clark et al.,\n2020) uses a different pre-training task that allevi-\nates these disadvantages. Instead of masking to-\nkens, ELECTRA replaces some input tokens with\nfakes sampled from a small generator network. The\npre-training task is then to distinguish the original\nvs. replaced tokens. While on the surface it ap-\npears quite different from BERT, in this paper we\nelucidate a close connection between ELECTRA\nand cloze modeling. In particular, we develop a\nnew way of implementing the cloze task using an\nenergy-based model (EBM). Then we show the re-\nsulting model, which we call Electric, is closely\nrelated to ELECTRA, as well as being useful in its\nown right for some applications.1\nEBMs learn an energy function that assigns low\nenergy values to inputs in the data distribution and\nhigh energy values to other inputs. They are ﬂex-\nible because they do not have to compute normal-\nized probabilities. For example, Electric does not\nuse masking or an output softmax, instead produc-\ning a scalar energy score for each token where a low\nenergy indicates the token is likely given its context.\nUnlike with BERT, these likelihood scores can be\ncomputed simultaneously for all input tokens rather\nthan only for a small masked-out subset. We pro-\npose a training algorithm for Electric that efﬁciently\napproximates a loss based on noise-contrastive esti-\nmation (Gutmann and Hyv¨arinen, 2010). Then we\nshow that this training algorithm is closely related\nto ELECTRA; in fact, ELECTRA can be viewed\nas a variant of Electric using negative sampling\ninstead of noise-contrastive estimation.\nWe evaluate Electric on GLUE (Wang et al.,\n2019) and SQuAD (Rajpurkar et al., 2016),\nwhere Electric substantially outperforms BERT\nbut slightly under-performs ELECTRA. However,\nElectric is particularly useful in its ability to efﬁ-\nciently produce pseudo-likelihood scores (Salazar\net al., 2020) for text: Electric is better at re-ranking\nthe outputs of a speech recognition system than\nGPT-2 (Radford et al., 2019) and is much faster at\nre-ranking than BERT because it scores all input\ntokens simultaneously rather than having to be run\nmultiple times with different tokens masked out. In\ntotal, investigating Electric leads to a more princi-\npled understanding of ELECTRA and our results\n1Code is available at https://github.com/\ngoogle-research/electra\n286\n                                  \n... !\"(the|the __ barked) !\"(cat|the __ barked) !̂\"(barked|the dog __) \n!̂\"(the|__ dog barked) \nbarked dog the Electric  !̂\"(dog|the __ barked) barked [MASK]  \nthe BERT  !\"(dog|the __ barked) \n!\"(dog|the __ barked) \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\nRussian President Vladimir Putin, the Russian  President,  President Clinton’s, Bill Clinton, Mr. Clinton’s he … , {\t {\t}\t }\tincorrect link predicted by the mention-ranking model \n… , … , \nA school official, he Al Gore, he, Gore, Gore, he {\t {\t}\t }\tincorrect link predicted by the mention-ranking model \nhe, his, he, him {\t {\t}\t }\the, my, me, me \nCorrect Declines to Merge \nIncorrect Merge incorrect link predicted by the mention-ranking model \n{\t\nFigure 1: Comparison of BERT and Electric. Both model the probability of a token given its surrounding context,\nbut BERT produces a full output distribution over tokens only for masked positions while Electric produces un-\nnormalized probabilities (but no full distribution) for all input tokens.\nsuggest that EBMs are a promising alternative to\nthe standard generative models currently used for\nlanguage representation learning.\n2 Method\nBERT and related pre-training methods (Baevski\net al., 2019; Liu et al., 2019; Lan et al., 2020) train\na large neural network to perform the cloze task.\nThese models learn the probability pdata(xt|x\\t) of\na token xt occurring in the surrounding context\nx\\t = [x1,...,x t−1,xt+1,...,x n]. Typically the\ncontext is represented as the input sequence with\nxt replaced by a special [MASK]placeholder to-\nken. This masked sequence is encoded into vector\nrepresentations by a transformer network (Vaswani\net al., 2017). Then the representation at position t\nis passed into a softmax layer to produce a distribu-\ntion over tokens pθ(xt|x\\t) for the position.\n2.1 The Electric Model\nElectric also models pdata(xt|x\\t), but does not use\nmasking or a softmax layer. Electric ﬁrst maps\nthe unmasked input x = [x1,...,x n] into contextu-\nalized vector representations h(x) = [h1,..., hn]\nusing a transformer network. The model assigns a\ngiven position tan energy score\nE(x)t = wTh(x)t\nusing a learned weight vector w. The energy func-\ntion deﬁnes a distribution over the possible tokens\nat position tas\npθ(xt|x\\t) = exp (−E(x)t)/Z(x\\t)\n= exp (−E(x)t)∑\nx′∈Vexp (−E(REPLACE (x,t,x ′))t)\nwhere REPLACE (x,t,x ′) denotes replacing the to-\nken at position twith x′and Vis the vocabulary, in\npractice usually word pieces (Sennrich et al., 2016).\nUnlike with BERT, which produces the probabili-\nties for all possible tokens x′using a softmax layer,\na candidate x′ is passed in as input to the trans-\nformer. As a result, computing pθ is prohibitively\nexpensive because the partition function Zθ(x\\t)\nrequires running the transformer |V|times; unlike\nmost EBMs, the intractability of Zθ(x\\t) is due to\nthe expensive scoring function rather than having a\nlarge sample space.\n2.2 NCE Loss\nAs computing the exact likelihood is intractable,\ntraining energy-based models such as Electric\nwith standard maximum-likelihood estimation is\nnot possible. Instead, we use (conditional)\nNoise-Contrastive Estimation (NCE) (Gutmann\nand Hyv ¨arinen, 2010; Ma and Collins, 2018),\nwhich provides a way of efﬁciently training an un-\nnormalized model that does not compute Zθ(x\\t).\nNCE learns the parameters of a model by deﬁn-\ning a binary classiﬁcation task where samples from\nthe data distribution have to be distinguished from\nsamples generated by a noise distributionq(xt|x\\t).\nFirst, we deﬁne the un-normalized output\nˆpθ(xt|x\\t) = exp (−E(x)t)\nOperationally, NCE can be viewed as follows:\n•A positive data point is a text sequencex from\nthe data and position in the sequence t.\n•A negative data point is the same except xt,\nthe token at positiont, is replaced with a noise\ntoken ˆxt sampled from q.\n•Deﬁne a binary classiﬁer Dthat estimates the\nprobability of a data point being positive as\nn·ˆpθ(xt|x\\t)\nn·ˆpθ(xt|x\\t) +k·q(xt|x\\t)\n•The binary classiﬁer is trained to distinguish\npositive vs negative data points, with knega-\ntives sampled for every npositive data points.\nFormally, the NCE loss L(θ) is\nn·E\nx,t\n[\n−log n·ˆpθ(xt|x\\t)\nn·ˆpθ(xt|x\\t) +k·q(xt|x\\t)\n]\n+\nk·E\nx,t\nˆxt∼q\n[\n−log k·q(ˆxt|x\\t)\nn·ˆpθ(ˆxt|x\\t) +k·q(ˆxt|x\\t)\n]\n287\nThis loss is minimized when ˆpθ matches the data\ndistribution pdata (Gutmann and Hyv¨arinen, 2010).\nA consequence of this property is that the model\nlearns to be self-normalized such that Zθ(x\\t) = 1.\n2.3 Training Algorithm\nTo minimize the loss, the expectations could be ap-\nproximated by sampling as shown in Algorithm 1.\nTaking the gradient of this estimated loss produces\nAlgorithm 1Naive NCE loss estimation\nGiven: Input sequence x, number of negative\nsamples k, noise distribution q, model ˆpθ.\n1. Initialize the loss as∑n\nt=1\n(\n−log\nn·ˆpθ(xt|x\\t)\nn·ˆpθ(xt|x\\t)+k·q(xt|x\\t)\n)\n.\n2. Sample knegative samples according to t∼\nunif{1,n}, ˆxt ∼q(xt|x\\t).\n3. For each negative sample, add to the loss\n−log\nk·q(ˆxt|x\\t)\nn·ˆpθ(ˆxt|x\\t)+k·q(ˆxt|x\\t).\nan unbiased estimate of ∇θL(θ). However, this al-\ngorithm is computationally expensive to run, since\nit requires k+ 1forward passes through the trans-\nformer to compute the ˆpθs (once for the positive\nsamples and once for each negative sample). We\npropose a much more efﬁcient approach that re-\nplaces k input tokens with noise samples simul-\ntaneously shown in Algorithm 2. It requires just\nAlgorithm 2Efﬁcient NCE loss estimation\nGiven: Input sequence x, number of negative\nsamples k, noise distribution q, model ˆpθ.\n1. Pick k unique random positions R =\n{r1,...,r k}where each ri is 1 ≤ri ≤n.\n2. Replace the krandom positions with negative\nsamples: ˆxi ∼q(xi|x\\i) for i∈R,\nxnoised = REPLACE (ˆx,R, ˆX).\n3. For each position t= 1to n: add to the loss\n−log\nk·q(ˆxt|x\\t)\n(n−k)·ˆpθ(ˆxt|xnoised\n\\t )+k·q(ˆxt|x\\t) if t∈R\n−log\n(n−k)·ˆpθ(xt|xnoised\n\\t )\n(n−k)·ˆpθ(xt|xnoised\n\\t )+k·q(xt|x\\t) otherwise\none pass through the transformer for knoise sam-\nples and n−k data samples. However, this pro-\ncedure only truly minimizes Lif ˆpθ(xt|x\\t) =\nˆpθ(xt|xnoised\n\\t ). To apply this efﬁciency trick we\nare making the assumption they are approximately\nequal, which we argue is reasonable because (1) we\nchoose a small kof ⌈0.15n⌉and (2) qis trained to\nbe close to the data distribution (see below). This\nefﬁciency trick is analogous to BERT masking out\nmultiple tokens per input sequence.\n2.4 Noise Distribution\nThe noise distribution qcomes from a neural net-\nwork trained to match pdata. NCE commonly em-\nploys this idea to ensure the classiﬁcation task is\nsufﬁciently challenging for the model (Gutmann\nand Hyv ¨arinen, 2010; Wang and Ou, 2018). In\nparticular, we use a two-tower cloze model as pro-\nposed by Baevski et al. (2019), which is more ac-\ncurate than a language model because it uses con-\ntext to both sides of each token. The model runs\ntwo transformers TLTR and TRTL over the input se-\nquence. These transformers apply causal masking\nso one processes the sequence left-to-right and the\nother operates right-to-left. The model’s predic-\ntions come from a softmax layer applied to the\nconcatenated states of the two transformers:\n− →h = TLTR (x), ← −h = TRTL (x)\nq(xt|x\\t) =softmax(W[− →h t−1,← −h t+1])xt\nThe noise distribution is trained simultaneously\nwith Electric using standard maximum likelihood\nestimation over the data. The model producing the\nnoise distribution is much smaller than Electric to\nreduce the computational overhead.\n2.5 Connection to ELECTRA\nElectric is closely related to the ELECTRA pre-\ntraining method. ELECTRA also trains a binary\nclassiﬁer (the “discriminator”) to distinguish data\ntokens from noise tokens produced by a “generator”\nnetwork. However, ELECTRA’s classiﬁer is simply\na sigmoid layer on top of the transformer: it models\nthe probability of a token being negative (i.e., as\nreplaced by a noise sample) as σ(E(x)t) where σ\ndenotes the sigmoid function. Electric on the other\nhand models this probability as\nk·q(x|x\\t)\nn·exp (−E(x)t) +k·q(x|x\\t) =\nσ\n(\nE(x)t + log\n(k·q(x|x\\t)\nn\n))\nWhile ELECTRA learns whether a token is more\nlikely to come from the data distribution pdata or\nnoise distribution q, Electric only learns pdata be-\ncause q is passed into the model directly. This\ndifference is analogous to using negative sampling\n(Mikolov et al., 2013) vs. noise-contrastive estima-\ntion (Mnih and Kavukcuoglu, 2013) for learning\nword embeddings.\n288\nModel MultiNLI SQuAD 2.0 GLUE Avg.\nBERT 84.3 73.7 82.2\nXLNet 85.8 78.5 –\nELECTRA 86.2 80.5 85.1\nElectric 85.7 80.1 84.1\nTable 1: Dev-set scores of pre-trained models on down-\nstream tasks. To provide direct comparisons, we only\nshow base-sized models pre-trained on WikiBooks.\nA disadvantage of Electric compared to ELEC-\nTRA is that it is less ﬂexible in the choice of noise\ndistribution. Since ELECTRA’s binary classiﬁer\ndoes not need to access q, its q only needs to be\ndeﬁned for negative sample positions in the in-\nput sequence. Therefore ELECTRA can use a\nmasked language model rather than a two-tower\ncloze model for q. An advantage of Electric is that\nit directly provides (un-normalized) probabilities\nˆpθ for tokens, making it useful for applications\nsuch as re-ranking the outputs of text generation\nsystems. The differences between ELECTRA and\nElectric are summarized below:\nModel Noise Dist. Binary Classiﬁer\nElectric Two-Tower\nCloze Model σ\n(\nE(x)t + log\n(k·q(x|x\\t)\nn\n))\nELECTRA Masked LM σ(E(x)t)\n3 Experiments\nWe train two Electric models the same size as\nBERT-Base (110M parameters): one on Wikipedia\nand BooksCorpus (Zhu et al., 2015) for compari-\nson with BERT and one on OpenWebTextCorpus\n(Gokaslan and Cohen, 2019) for comparison2 with\nGPT-2. The noise distribution transformers TLTR\nand TRTL are 1/4 the hidden size of Electric. We do\nno hyperparameter tuning, using the same hyper-\nparameter values as ELECTRA. Further details on\ntraining are in the appendix.\n3.1 Transfer to Downstream Tasks\nWe evaluate ﬁne-tuning the Electric model on the\nGLUE natural language understanding benchmark\n(Wang et al., 2019) and the SQuAD 2.0 question\nanswering dataset (Rajpurkar et al., 2018). We re-\nport exact-match for SQuAD, average score3 over\n2The original GPT-2 dataset is not public, so we use a\npublic re-implementation.\n3Matthews correlation coefﬁcient for CoLA, Spearman\ncorrelation for STS, accuracy for the other tasks.\nthe GLUE tasks4, and accuracy on the multi-genre\nnatural language inference GLUE task. Reported\nscores are medians over 10 ﬁne-tuning runs with\ndifferent random seeds. We use the same ﬁne-\ntuning setup and hyperparameters as ELECTRA.\nResults are shown in Table 1. Electric scores bet-\nter than BERT, showing the energy-based formula-\ntion improves cloze model pre-training. However,\nElectric scores slightly lower than ELECTRA. One\npossible explanation is that Electric’s noise distri-\nbution is worse because a two-tower cloze model is\nless expressive than a masked LM. We tested this\nhypothesis by training ELECTRA with the same\ntwo-tower noise model as Electric. Performance\ndid indeed go down, but it only explained about half\nthe gap. The surprising drop in performance sug-\ngests that learning the difference between the data\nand generations from a low-capacity model leads\nto better representations than only learning the\ndata distribution, but we believe further research is\nneeded to fully understand the discrepancy.\n3.2 Fast Pseudo-Log-Likelihood Scoring\nAn advantage of Electric over BERT is that it can\nefﬁciently produce pseudo-log-likelihood (PLL)\nscores for text (Wang and Cho, 2019). PLLs for\nElectric are\nPLL(x) =\nn∑\nt=1\nlog(ˆpθ(xt|x\\t)) =\nn∑\nt=1\n−E(x)t\nPLLs can be used to re-rank the outputs of an NMT\nor ASR system. While historically log-likelihoods\nfrom language models have been used for such re-\nranking, recent work has demonstrated that PLLs\nfrom masked language models perform better (Shin\net al., 2019). However, computing PLLs from a\nmasked language model requires npasses of the\ntransformer: once with each token masked out.\nSalazar et al. (2020) suggest distilling BERT into a\nmodel that uses no masking to avoid this cost, but\nthis model considerably under-performed regular\nLMs in their experiments.\nElectric can produce PLLs for all input tokens in\na single pass like a LM while being bidirectional\nlike a masked LM. We use the PLLs from Electric\nfor re-ranking the 100-best hypotheses of a 5-layer\nBLSTMP model from ESPnet (Watanabe et al.,\n2018) on the 960-hour LibriSpeech corpus (Panay-\notov et al., 2015) following the same experimental\nsetup and using the same n-best lists as Salazar\n4We exclude WNLI, for which models do not outperform\nthe majority baseline.\n289\net al. (2020). Given speech features s and speech\nrecognition model f the re-ranked output is\narg max\nx∈n-best(f,s)\nf(x|s) +λPLL(x)\nwhere n-best(f,s) consists of the top n(we use\nn= 100) predictions from the speech recognition\nmodel found with beam search, f(x|s) is the score\nthe speech model assigns the candidate output se-\nquence x. We select the best λon the dev set out of\n[0.05,0.1,..., 0.95,1.0], with different λs selected\nfor the “clean” and “other” portions of the data.\nWe compare Electric against GPT-2 (Radford\net al., 2019), BERT (Devlin et al., 2019), and two\nbaseline systems that are bidirectional while only\nrequiring a single transformer pass like Electric.\nTwoTower is a two-tower cloze model similar to\nElectric’s noise distribution, but of the same size as\nElectric. ELECTRA-TT is identical to ELECTRA\nexcept it uses a two-tower noise distribution rather\nthan a masked language model.5 The noise distri-\nbution probabilities and binary classiﬁers scores\nof ELECTRA can be combined to assign proba-\nbilities for tokens as shown in Appendix G of the\nELECTRA paper.\nResults are shown in Table 2. Electric scores\nbetter than GPT-2 when trained on comparable\ndata. While scoring worse than BERT, Electric\nis much faster to run. It also slightly outperforms\nELECTRA-TT, which is consistent with the ﬁnding\nfrom Labeau and Allauzen (2018) that NCE outper-\nforms negative sampling for training language mod-\nels. Furthermore, Electric is simpler and faster than\nELETRA-TT in that it does not require running the\ngenerator to produce PLL scores. TwoTower scores\nlower than Electric, presumably because it is not a\n“deeply” bidirectional model and instead just con-\ncatenates forward and backward hidden states.\n4 Related Work\nLanguage modeling (Dai and Le, 2015; Radford\net al., 2018; Peters et al., 2018) and cloze modeling\n(Devlin et al., 2019; Baevski et al., 2019; Liu et al.,\n2019) have proven to be effective pre-training tasks\nfor NLP. Unlike Electric, these methods follow\nthe standard recipe of estimating token probabili-\nties with an output softmax and using maximum-\nlikelihood training.\nEnergy-based models have been widely explored\nin machine learning (Dayan et al., 1995; LeCun\n5With ELECTRA’s original masked LM generator, it\nwould be impossible to score all tokens in a single pass.\nModel Pre-train Clean Other RuntimeData WER WER\nNone – 7.26 20.37 0\nBERT WikiBooks 5.41 17.41 n\nElectric WikiBooks 5.65 17.42 1\nGPT-2 OWT 5.64 17.60 1\nTwoTower OWT* 5.32 17.25 1\nELECTRA-TT OWT* 5.22 17.01 1\nElectric OWT* 5.18 16.93 1\nTable 2: Test-set word error rates on LibriSpeech after\nrescoring with base-sized models. None, GPT-2, and\nBERT results are from Salazar et al. (2020). Runtime\nis measured in passes through the transformer. “Clean”\nand “other” are easier and harder splits of the data. *We\nuse a public re-implementation of OpenWebText.\net al., 2007). While many training methods in-\nvolve sampling from the EBM using gradient-\nbased MCMC (Du and Mordatch, 2019) or Gibbs\nsampling (Hinton, 2002), we considered these ap-\nproaches too slow for pre-training because they\nrequire multiple passes through the model per sam-\nple. We instead use noise-contrastive estimation\n(Gutmann and Hyv¨arinen, 2010), which has widely\nbeen used in NLP for learning word vectors (Mnih\nand Kavukcuoglu, 2013) and text generation mod-\nels (Jean et al., 2014; J ´ozefowicz et al., 2016).\nWhile EBMs have previously been applied to left-\nto-right (Wang et al., 2015) or globally normal-\nized (Rosenfeld et al., 2001; Deng et al., 2020)\ntext generation, they have not previously been ap-\nplied to cloze models or for pre-training NLP mod-\nels. Several papers have pointed out the connec-\ntion between EBMs and GANs (Zhao et al., 2016;\nFinn et al., 2016), which is similar to the Elec-\ntric/ELECTRA connection.\n5 Conclusion\nWe have developed an energy-based cloze model\nwe call Electric and designed an efﬁcient training\nalgorithm for Electric based on noise-contrastive\nestimation. Although Electric can be derived solely\nfrom the cloze task, the resulting pre-training\nmethod is closely related to ELECTRA’s GAN-\nlike pre-training algorithm. While slightly under-\nperforming ELECTRA on downstream tasks, Elec-\ntric is useful for its ability to quickly produce\npseudo-log-likelihood scores for text. Furthermore,\nit offers a clearer and more principled view of the\nELECTRA objective as a “negative sampling” ver-\nsion of cloze pre-training.\n290\nAcknowledgements\nWe thank John Hewitt, Yuhao Zhang, Ashwin\nParanjape, Sergey Levine, and the anonymous re-\nviewers for their thoughtful comments and sug-\ngestions. Kevin is supported by a Google PhD\nFellowship.\nReferences\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. In EMNLP.\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, I ˜nigo\nLopez-Gazpio, and Lucia Specia. 2017. Semeval-\n2017 task 1: Semantic textual similarity multilin-\ngual and crosslingual focused evaluation. In Se-\nmEval@ACL.\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D. Manning, and Quoc V . Le. 2019.\nBAM! Born-again multi-task networks for natural\nlanguage understanding. In ACL.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In NeurIPS.\nPeter Dayan, Geoffrey E. Hinton, Radford M. Neal,\nand Richard S. Zemel. 1995. The Helmholtz ma-\nchine. Neural Computation, 7:889–904.\nYuntian Deng, Anton Bakhtin, Myle Ott, and Arthur\nSzlam. 2020. Residual energy-based models for text\ngeneration. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn IWP@IJCNLP.\nYilun Du and Igor Mordatch. 2019. Implicit generation\nand generalization in energy-based models. arXiv\npreprint arXiv:1903.08689.\nChelsea Finn, Paul Christiano, Pieter Abbeel, and\nSergey Levine. 2016. A connection between gen-\nerative adversarial networks, inverse reinforcement\nlearning, and energy-based models. In NeurIPS\n2016 Workshop on Adversarial Training.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand William B. Dolan. 2007. The third pascal\nrecognizing textual entailment challenge. In ACL-\nPASCAL@ACL.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.\nio/OpenWebTextCorpus.\nMichael Gutmann and Aapo Hyv ¨arinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In AISTATS.\nGeoffrey E. Hinton. 2002. Training products of experts\nby minimizing contrastive divergence. Neural Com-\nputation, 14:1771–1800.\nShankar Iyer, Nikhil Dandekar, and Korn ´el Csernai.\n2017. First Quora dataset release: Question pairs.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2014. On using very large tar-\nget vocabulary for neural machine translation. In\nACL.\nRafal J´ozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nMatthieu Labeau and Alexandre Allauzen. 2018.\nLearning with noise-contrastive estimation: Easing\ntraining by learning to scale. In COLING.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR.\nYann LeCun, Sumit Chopra, Raia Hadsell,\nMarc’Aurelio Ranzato, and Fu Jie Huang. 2007.\nA tutorial on energy-based learning. In G ¨okhan\nBakır, Thomas Hofmann, Bernhard Sch ¨olkopf,\nAlexander J. Smola, Ben Taskar, and S. V . N.\nVishwanathan, editors, Predicting Structured Data,\npages 191–246. MIT Press, Cambridge, MA.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nZhuang Ma and Michael Collins. 2018. Noise con-\ntrastive estimation and negative sampling for condi-\ntional models: Consistency and statistical efﬁciency.\nIn EMNLP.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In NeurIPS.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efﬁciently with noise-contrastive\nestimation. In NeurIPS.\n291\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: An asr cor-\npus based on public domain audio books. ICASSP.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT.\nJason Phang, Thibault F ´evry, and Samuel R Bow-\nman. 2018. Sentence encoders on STILTs: Supple-\nmentary training on intermediate labeled-data tasks.\narXiv preprint arXiv:1811.01088.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training.\nhttps://blog.openai.com/language-unsupervised.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal Report.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In NAACL-HLT.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy S. Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In EMNLP.\nNils Reimers and Iryna Gurevych. 2018. Why com-\nparing single performance scores does not allow\nto draw conclusions about machine learning ap-\nproaches. arXiv preprint arXiv:1803.09578.\nRonald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.\n2001. Whole-sentence exponential language mod-\nels: A vehicle for linguistic-statistical integration.\nComput. Speech Lang., 15:55–73.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In ACL.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In ACL.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In NAACL-HLT.\nJoonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019.\nEffective sentence scoring method using bert for\nspeech recognition. In Asian Conference on Ma-\nchine Learning, pages 1081–1093.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP.\nWilson L. Taylor. 1953. “Cloze procedure”: a new tool\nfor measuring readability. Journalism & Mass Com-\nmunication Quarterly, 30:415–433.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a markov\nrandom ﬁeld language model. arXiv preprint\narXiv:1902.04094.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nBin Wang and Zhijian Ou. 2018. Learning neural trans-\ndimensional random ﬁeld language models with\nnoise-contrastive estimation. ICASSP.\nBin Wang, Zhijian Ou, and Zhiqiang Tan. 2015. Trans-\ndimensional random ﬁelds for language modeling.\nIn ACL.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki\nHayashi, Jiro Nishitoba, Yuya Unno, Nelson En-\nrique Yalta Soplin, Jahn Heymann, Matthew Wies-\nner, Nanxin Chen, Adithya Renduchintala, and\nTsubasa Ochiai. 2018. Espnet: End-to-end speech\nprocessing toolkit. In INTERSPEECH.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL-HLT.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nJunbo Zhao, Michael Mathieu, and Yann LeCun. 2016.\nEnergy-based generative adversarial network. arXiv\npreprint arXiv:1609.03126.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. ICCV.\nA Pre-Training Details\nThe neural architectures of our models are identi-\ncal to BERT-Base (Devlin et al., 2019), although\nwe believe incorporating additions such as relative\nposition encodings (Shaw et al., 2018) would im-\nprove results. Our pre-training setup is the same\n292\nas ELECTRA’s (Clark et al., 2020), which adds\nsome additional ideas from Liu et al. (2019) on\ntop of the BERT codebase, such as dynamic mask-\ning and removing the next-sentence prediction task.\nWe use the weight sharing trick from ELECTRA,\nwhere the transformers producing the proposal dis-\ntribution and the main transformer share token em-\nbeddings. We do not use whole-word or n-gram\nmasking, although we believe it would improve\nresults too.\nWe did no hyperparameter tuning, directly us-\ning the hyperparameters from ELECTRA-Base for\nElectric and our baselines. These hyperparameters\nare slightly modiﬁed from the ones used in BERT;\nfor completeness, we show these values in Table 3.\nThe hidden sizes, feed-forward hidden sizes, and\nnumber of attention heads of the two transformers\nTLTR and TRTL used to produce the proposal distri-\nbution are 1/4 the size of Electric. We chose this\nvalue because it keeps the compute comparable\nto ELECTRA; running two 1/4-sized transformers\ntakes roughly the same compute as running one 1/3-\nsized transformer, which is the size of ELECTRA’s\ngenerator. To make the compute exactly equal, we\ntrain Electric for slightly fewer steps than ELEC-\nTRA. This same generator architecture was used\nfor ELECTRA-TT. The TwoTower baseline con-\nsists of two transformers 2/3 the size of BERT’s,\nwhich takes approximately the same compute to\nrun. The Electric models, ELECTRA-Base, and\nBERT-Base all use the same amount of pre-train\ncompute (e.g., Electric is trained for fewer steps\nthan BERT due to the extra compute from the pro-\nposal distribution), which equates to approximately\nthree days of training on 16 TPUv2s.\nB Fine-Tuning Details\nWe use ELECTRA’s top-level classiﬁers and hy-\nperparameter values for ﬁne-tuning as well. For\nGLUE tasks, a simple linear classiﬁer is added on\ntop of the pre-trained transformer. For SQuAD, a\nquestion answering module similar XLNet’s (Yang\net al., 2019) is added on top of the transformer,\nwhich is slightly more sophisticated than BERT’s\nin that it jointly rather than independently predicts\nthe start and end positions and has an “answerabil-\nity” classiﬁer added for SQuAD 2.0. ELECTRA’s\nhyperparameters are similar to BERT’s, with the\nmain difference being the addition of a layer-wise\nlearning rate decay where layers of the network\ncloser to the output have a higher learning rate.\nFollowing BERT, we submit the best of 10 mod-\nels ﬁne-tuned with different random seeds to the\nGLUE leaderboard for test set results.\nC Dataset Details\nWe provide details on the ﬁne-tuning datasets\nbelow. All datasets are in English. GLUE\ndata can be downloaded at https://\ngluebenchmark.com/ and SQuAD data can\nbe downloaded at https://rajpurkar.\ngithub.io/SQuAD-explorer/.\n•CoLA: Corpus of Linguistic Acceptability\n(Warstadt et al., 2018). The task is to deter-\nmine whether a given sentence is grammat-\nical or not. The dataset contains 8.5k train\nexamples from books and journal articles on\nlinguistic theory.\n•SST: Stanford Sentiment Treebank (Socher\net al., 2013). The tasks is to determine if the\nsentence is positive or negative in sentiment.\nThe dataset contains 67k train examples from\nmovie reviews.\n•MRPC: Microsoft Research Paraphrase Cor-\npus (Dolan and Brockett, 2005). The task is\nto predict whether two sentences are semanti-\ncally equivalent or not. The dataset contains\n3.7k train examples from online news sources.\n•STS: Semantic Textual Similarity (Cer et al.,\n2017). The tasks is to predict how seman-\ntically similar two sentences are on a 1-5\nscale. The dataset contains 5.8k train exam-\nples drawn from new headlines, video and im-\nage captions, and natural language inference\ndata.\n•QQP: Quora Question Pairs (Iyer et al., 2017).\nThe task is to determine whether a pair of ques-\ntions are semantically equivalent. The dataset\ncontains 364k train examples from the com-\nmunity question-answering website Quora.\n•MNLI: Multi-genre Natural Language Infer-\nence (Williams et al., 2018). Given a premise\nsentence and a hypothesis sentence, the task\nis to predict whether the premise entails the\nhypothesis, contradicts the hypothesis, or nei-\nther. The dataset contains 393k train examples\ndrawn from ten different sources.\n293\nHyperparameter Pre-Training Fine-Tuning\nNumber of layers 12\nHidden Size 768\nFFN inner hidden size 3072\nAttention heads 12\nAttention head size 64\nEmbedding Size 768\nProposal Transformer Size 1/4 NA\nNegative sample percent 15 NA\nLearning Rate Decay Linear\nWarmup steps 10000 First 10%\nLearning Rate 5e-4 1e-4\nLayerwise LR decay None 0.8\nAdam ϵ 1e-6\nAdam β1 0.9\nAdam β2 0.999\nAttention Dropout 0.1\nDropout 0.1\nWeight Decay 0.01 0\nBatch Size 256 32\nTrain Steps 700K 10 epochs for RTE and STS\n2 for SQuAD, 3 for other tasks\nTable 3: Hyperparameters for Electric; the values are identical to ELECTRA’s other than the train steps and\ndifferent-sized proposal network (see text), but we include them here for completeness. If not shown, the ﬁne-\ntuning hyperparameter is the same as the pre-training one.\n•QNLI: Question Natural Language Inference;\nconstructed from SQuAD (Rajpurkar et al.,\n2016). The task is to predict whether a con-\ntext sentence contains the answer to a question\nsentence. The dataset contains 108k train ex-\namples from Wikipedia.\n•RTE: Recognizing Textual Entailment (Gi-\nampiccolo et al., 2007). Given a premise sen-\ntence and a hypothesis sentence, the task is\nto predict whether the premise entails the hy-\npothesis or not. The dataset contains 2.5k\ntrain examples from a series of annual textual\nentailment challenges.\n•SQuAD 1.1: Stanford Question Answering\nDataset (Rajpurkar et al., 2016). Given a con-\ntext paragraph and a question, the task is to\nselect the span of text in the paragraph an-\nswering the question. The dataset contains\n88k train examples from Wikipedia.\n•SQuAD 2.0: Stanford Question Answering\nDataset version 2.0 (Rajpurkar et al., 2018).\nThis task adds addition questions to SQuAD\nwhose answer does not exist in the context;\nmodels have to recognize when these ques-\ntions occur and not return an answer for them.\nThe dataset contains 130k train examples,\nWe report Spearman correlation for STS,\nMatthews correlation coefﬁcient (MCC) for CoLA,\nexact match for SQuAD, and accuracy for the other\ntasks. We use the provided evaluation script for\nSQuAD6, scipy to compute Spearman scores7, and\nsklearn to compute MCC 8. We use the standard\ntrain/dev/test splits.\nD Detailed Results\nWe show detailed results on GLUE and SQuAD\nin Table 4 and detailed results on LibriSpeech re-\nranking in Table 5. Following BERT, we do not\nshow results on the WNLI GLUE task, as it is\ndifﬁcult to beat even the majority classiﬁer using\na standard ﬁne-tuning-as-classiﬁer approach. We\nshow dev rather than test results on GLUE in the\nmain paper because they are more reliable; the\nperformance of ﬁne-tuned models varies substan-\ntially based on the random seed (Phang et al., 2018;\nClark et al., 2019; Dodge et al., 2020), but GLUE\nonly supports submitting a single model rather than\ngetting a median score of multiple models. While\n6 https://worksheets.\ncodalab.org/rest/bundles/\n0x6b567e1cf2e041ec80d7098f031c5c9e/\ncontents/blob/\n7 https://docs.scipy.org/doc/\nscipy/reference/generated/scipy.stats.\nspearmanr.html\n8 https://scikit-learn.org/stable/\nmodules/generated/sklearn.metrics.\nmatthews_corrcoef.html\n294\nModel CoLA SST MRPC STS QQP MNLI QNLI RTE SQuAD 1.1 SQuAD 2.0\nMCC Acc Acc Spear Acc Acc Acc Acc EM EM\nDev set results\nBERT 58.4 92.8 86.0 87.8 90.8 84.5 88.6 68.5 80.8 73.7\nXLNet – 93.4 – – – 85.8 – – – 78.5\nELECTRA 65.8 92.4 87.9 89.1 90.9 86.2 92.4 76.3 84.5 80.5\nElectric 61.8 91.9 88.0 89.4 90.6 85.7 92.1 73.4 84.5 80.1\nTest set results\nBERT 52.1 93.5 84.8 85.8 89.2 84.6 90.5 66.4 – –\nELECTRA 59.7 93.4 86.7 87.7 89.1 85.8 92.7 73.1 – –\nElectric 61.5 93.2 85.4 86.9 89.2 85.2 91.8 67.3 – –\nTable 4: GLUE scores pre-trained models on downstream tasks. To provide direct comparisons, we only show\nbase-sized models pre-trained on WikiBooks and ﬁne-tuned with standard single-task training.\nRescoring Model Pre-Training Dev Test Transformer\nData clean other clean other Passes\nNone – 7.17 19.79 7.26 20.37 0\nBERT WikiBooks 5.17 16.44 5.41 17.41 n\nElectric Wikibooks 5.47 16.56 5.65 17.42 1\nGPT-2 OpenWebText 5.39 16.81 5.64 17.61 1\nTwoTower OpenWebText 5.12 16.37 5.32 17.25 1\nELECTRA-TT OpenWebText 5.05 16.27 5.22 17.01 1\nElectric OpenWebText 4.97 16.23 5.18 16.93 1\nTable 5: Word error rates on LibriSpeech after rescoring with base-sized models. None, GPT-2, and BERT results\nare from Salazar et al. (2020). Runtime is measured in passes through the transformer and data indicates the pre-\ntraining dataset. “Clean” and “other” are easier and harder splits of the data. *We use a public re-implementation\nof OpenWebText.\nusing dev-set model selection to choose the test\nset submission may alleviate the high variance of\nﬁne-tuning to some extent, such model selection is\nstill not sufﬁcient for reliable comparisons between\nmethods (Reimers and Gurevych, 2018).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7503176927566528
    },
    {
      "name": "Security token",
      "score": 0.6511627435684204
    },
    {
      "name": "Transformer",
      "score": 0.6493393778800964
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5057506561279297
    },
    {
      "name": "Language model",
      "score": 0.45884689688682556
    },
    {
      "name": "Natural language processing",
      "score": 0.4515088200569153
    },
    {
      "name": "Speech recognition",
      "score": 0.379370778799057
    },
    {
      "name": "Voltage",
      "score": 0.09632870554924011
    },
    {
      "name": "Engineering",
      "score": 0.08575215935707092
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}