{
  "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
  "url": "https://openalex.org/W4392669905",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096833632",
      "name": "Yi Chen",
      "affiliations": [
        "Harbin Institute of Technology",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2036086788",
      "name": "Rui Wang",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2161667016",
      "name": "Hai-Yun Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105557964",
      "name": "Shuming Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099613179",
      "name": "Ruifeng Xu",
      "affiliations": [
        "Harbin Institute of Technology",
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6611293539",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3174489133",
    "https://openalex.org/W2159849140",
    "https://openalex.org/W4385573675",
    "https://openalex.org/W4309184901",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2153702313",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4385572904",
    "https://openalex.org/W4360887049",
    "https://openalex.org/W3036394672",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3176264844",
    "https://openalex.org/W4294778063",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4327487298",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4255898812",
    "https://openalex.org/W4385570112",
    "https://openalex.org/W3099942180",
    "https://openalex.org/W1555168845",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2962786758",
    "https://openalex.org/W4385572203",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3034808773",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W3199834251",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3034927876",
    "https://openalex.org/W2252001469",
    "https://openalex.org/W4376653016",
    "https://openalex.org/W2970785793"
  ],
  "abstract": "Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text.Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks.Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality.We compared three kinds of referencefree evaluation methods.The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics.In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches.However, directly comparing the quality of two texts may lead to suboptimal results.We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data 1 .",
  "full_text": "Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023, pages 361–374\nNovember 1–4, 2023. ©2023 Asian Federation of Natural Language Processing\n361\nExploring the Use of Large Language Models for Reference-Free Text\nQuality Evaluation: An Empirical Study\nYi Chen♡♠∗, Rui Wang♡♠∗, Haiyun Jiang†, Shuming Shi, Ruifeng Xu♡♣♠†\n♡Harbin Institute of Technology, Shenzhen, China\n♣Peng Cheng Laboratory, Shenzhen, China\n♠Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies\nyichennlp@gmail.com,ruiwangnlp@outlook.com,xuruifeng@hit.edu.cn\nAbstract\nEvaluating the quality of generated text is a\nchallenging task in NLP, due to the inherent\ncomplexity and diversity of text. Recently,\nlarge language models (LLMs) have garnered\nsignificant attention due to their impressive\nperformance in various tasks. Therefore, we\npresent this paper to investigate the effective-\nness of LLMs, especially ChatGPT, and explore\nways to optimize their use in assessing text\nquality. We compared three kinds of reference-\nfree evaluation methods. The experimental re-\nsults prove that ChatGPT is capable of evalu-\nating text quality effectively from various per-\nspectives without reference and demonstrates\nsuperior performance than most existing auto-\nmatic metrics. In particular, the Explicit Score,\nwhich utilizes ChatGPT to generate a numeric\nscore measuring text quality, is the most ef-\nfective and reliable method among the three\nexploited approaches. However, directly com-\nparing the quality of two texts may lead to sub-\noptimal results. We believe this paper will pro-\nvide valuable insights for evaluating text quality\nwith LLMs and have released the used data1.\n1 Introduction\nAutomated evaluation of text generation quality has\nposed a long-standing challenge in the field of nat-\nural language processing (NLP). On the one hand,\nthe diverse forms of textual expression make it im-\npossible for reference-based methods to account\nfor all possible situations(Zhang* et al., 2020; Yuan\net al., 2021; Chen et al., 2022b). On the other hand,\ndevising reliable metrics without reference is not\na straightforward task and can also be problem-\natic (Sun and Zhou, 2012; Niu et al., 2021; Shen\net al., 2022). Furthermore, different types of text\nnecessitate evaluation of distinct aspects, e.g. co-\nherence, fluency, and consistency (Fabbri et al.,\n∗Equal Contribution.\n†Corresponding Authors.\n1https://github.com/MilkWhite/LLMs_for_\nReference_Free_Text_Quality_Evaluation\n2021a; Mehri and Eskenazi, 2020a; Wang et al.,\n2023b), which makes it hard to design metrics for\neach type of text and dimension separately.\nNowadays, large language models (LLMs)\n(Brown et al., 2020; Ouyang et al., 2022; Chung\net al., 2022; Chowdhery et al., 2022; Zhang et al.,\n2022; Touvron et al., 2023; Du et al., 2022) repre-\nsented by ChatGPT2 have revolutionized the field\nof NLP by achieving remarkable results in a wide\nrange of NLP tasks (Song et al., 2023; Chen et al.,\n2022a). Recent studies (Fu et al., 2023; Wang et al.,\n2023a; Kocmi and Federmann, 2023; Ji et al., 2023)\nhave also demonstrated the potential of LLMs in\nevaluating the quality of generated texts. In this\npaper, we present an empirical study that compares\ndifferent methods for text quality evaluation using\nLLMs in a reference-free mode. The key insights\nfrom our empirical findings are as follows:\n• How accurately can ChatGPT assess text qual-\nity without references?(§4.1)\nIt is feasible for ChatGPT to evaluate text quality\nwithout reference, and it outperforms commonly\nused metrics even with a simple prompt design.\n• What is the most effective approach to evalu-\nate text quality using ChatGPT?(§4)\nGenerally, using ChatGPT to generate an explicit\nscore for text quality is the best and most stable\nmethod among the three we compared. We suggest\nusing greedy decoding for more reliable results.\n• Why may directly comparing two texts using\nChatGPT yield suboptimal results?(§5.1)\nDue to its strict standard for “high-quality” text,\nChatGPT often considers most generated texts un-\nsatisfactory. Therefore, distinguishing between two\nsubpar texts becomes challenging for ChatGPT.\n• Why is Implicit Score generally less effective\nthan Explicit Score?(§5.2)\n2https://openai.com/blog/chatgpt\n362\nCompared to generating an Explicit Score with\nChatGPT, using the confidence of text-davinci mod-\nels to determine text quality (Implicit Score) is less\neffective due to different distribution characteris-\ntics. Implicit Score has a narrow range and peak\nstructure, while Explicit Score allows better differ-\nentiation with its smoother distribution.\n• How can prompt design impact ChatGPT in\ngenerating an Explicit Score?(§5.3)\nWhen prompting ChatGPT for an Explicit Score,\nit would be better to avoid detailed scoring criteria\nif such criteria lack clear definitions for each score\nrange. A general description of the evaluation stan-\ndard is enough. Also, making ChatGPT provide\njustifications in a \"chain-of-thought\" manner be-\nfore scoring can lead it to prioritize its reasoning\nprocess over the text. These justifications tend to\nbe templated and similar across different texts, re-\nducing the discriminative power of the final score.\n2 Method\nWe explore two different reference-free paradigms,\ni.e., Individual Scoreand Pairwise Comparisonfor\ntext evaluation using ChatGPT and text-davinci\nmodels. Individual Score assesses the quality of\na single text by a numerical score, while Pairwise\nComparison focuses on the relative quality of two\ntexts and requires a direct comparison to determine\nwhich one is superior. Within the Individual Score\nparadigm, two methods are typically exploited: Ex-\nplicit Score, obtained through direct text genera-\ntion, and Implicit Score, obtained through the token\nprobabilities outputted by the model.\n2.1 Individual Score\nExplicit Score Conditioned on a given input text\n(optional), we prompt ChatGPT to directly gener-\nate a score to measure the absolute quality of each\ntext individually in terms of a specific aspect or\nthe overall performance. An example prompt de-\nsigned for scoring the overall quality of a storyline\nis shown as follows:\n========= PROMPT FOR EXPLICIT SCORE =========\nScore the following storyline given the beginning of the\nstory on a continual scale from 0 (worst) to 100 (best),\nwhere a score of 0 means \"The storyline makes no sense\nand is totally not understandable\" and a score of 100 means\n\"The storyline is perfect-written and highly consistent with\nthe given beginning of the story\".\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\nScore:\nImplicit Score Given the LLM’s potential insen-\nsitivity to numerical values and the lack of explicit\ninstructions for aligning score intervals with spe-\ncific criteria, score fluctuations may occur across\ndifferent samples. Therefore, we propose an alter-\nnative approach by framing the problem as a binary\nYes or No question, where the confidence level of\nanswering \"yes\" serves as the Implicit Score. An\nillustrative example is presented below:\n========= PROMPT FOR IMPLICIT SCORE =========\nConsider the following storyline written according to the\ngiven beginning of the story:\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\nQuestion: Is the storyline well-written and consistent with\nthe beginning of the story?\nAnswer:\nUnfortunately, access to ChatGPT’s token prob-\nabilities is currently unavailable. Text-davinci-\n003 is similar to ChatGPT in that they are both\ntrained through supervised instruction tuning and\nReinforcement Learning from Human Feedback\n(RLHF) based on GPT-3.5, and they both exhibit\nexcellent performance in following and fulfilling\nhuman instructions. Therefore, we utilize text-\ndavinci-003 to derive the Implicit Score as a base-\nline metric instead. To facilitate a more compre-\nhensive comparison, we also obtain the Implicit\nScore from text-davinci-001, an earlier version of\nthe text-davinci series model which is based on\nGPT-3 and has not been trained using RLHF. Due\nto a limitation of the OpenAI API, only the top 5\nmost probable tokens are returned with log proba-\nbilities. Therefore, we instead estimate the Implicit\nScore using the following formula:\np(yes) =\nX\nt∈Ttop5∩Tyes\np(t),\np(no) =\nX\nt∈Ttop5∩Tno\np(t),\nImplicit Score = max(p(yes), 1 − p(no)).\n(1)\n363\nHere, p(t) represents the probability of predicting\ntoken t immediately following the prompt \"An-\nswer:\". The sets Tyes and Tno consist of the affir-\nmative and negative response tokens, respectively,\ni.e., Tyes = {“ Yes”, “Yes”, “ yes”, “yes”}, and\nTno = {“ No”, “No”, “ no”, “no”}.\n2.2 Pairwise Comparison\nAnother paradigm to assess text quality is by di-\nrectly comparing a pair of generated texts based\non the same input. This method primarily focuses\non the relative quality of the texts. For instance, a\nprompt for comparing the overall quality of two sto-\nrylines written according to the same initial story\nbeginning is shown as follows:\n====== PROMPT FOR PAIRWISE COMPARISON ======\nConsider the following two storylines written according to\nthe given beginning of the story:\nThe beginning of the story:\n[Conditioned Text]\nStoryline-1:\n[Generated Text-1]\nStoryline-2:\n[Generated Text-2]\nQuestion: Which storyline is better-written and more con-\nsistent with the beginning of the story? Please answer with\none of the following options.\nOptions:\n(A) Storyline-1\n(B) Storyline-2\n(C) Both storylines are equally well-written and consistent\nwith the beginning of the story.\nAnswer: I will choose Option\n3 Experimental Setup\n3.1 Tasks and Datasets\nWe conduct experiments on four distinct natural\nlanguage generation tasks: Text Summarization,\nDialogue Response Generation, Story Generation,\nand Paraphrase Generation.\nText Summarization aims to summarize the key\npoints of a given long text. SummEval (Fabbri\net al., 2021b) is a collection of human annota-\ntions for 16 model-generated summaries on 100\nCNN/DaliyMail news over 4 dimensions: coher-\nence (COH), fluency (FLU), consistency (CON),\nand relevance (REL). Due to the budget limit, we\nrandomly sample 20 news and corresponding anno-\ntations from SummEval for evaluation.\nDialogue Response Generation aims to gener-\nate a response based on the preceding dialogue. We\nconduct experiments on the dialogue-level FED\ndataset (Mehri and Eskenazi, 2020a), which con-\ntains fine-grained human judgments for 124 conver-\nsations. The evaluation aspects include coherence\n(COH), error recovery (ERR), consistency (CON),\ndiversity (DIV), topic depth (DEP), likeability\n(LIK), understanding (UND), flexibility (FLE), in-\nformativeness (INF), inquisitiveness (INQ) and\noverall performance (Overall). However, we do\nnot include ERR in our evaluation since some an-\nnotations are missing.\nStory Generation aims to automatically write a\nstoryline based on a given beginning of the story.\nWe employ OpenMEV A-ROC (Guan et al., 2021)\nfor evaluation, which contains 200 story beginnings\nand 5 corresponding machine-generated storylines\nfor each beginning. Each storyline is manually\nannotated in terms of overall quality.\nParaphrase Generation aims to rephrase a sen-\ntence in different words or forms while preserving\nits original meaning. We use Twitter-Para (Xu et al.,\n2014, 2015) for evaluation, containing 761 input\nsentences and each input has 9.41 paraphrase can-\ndidates on average. We adopt the test set (Shen\net al., 2022) extended from Twitter-Para by adding\n20% of the input sentences as candidates, denoted\nas Twitter (Extend).\n3.2 Chosen Metrics\nFollowing the settings of previous works, we se-\nlect baseline metrics from the following widely\nused metrics accordingly: ROUGE-1, ROUGE-2\nand ROUGE-L (Lin, 2004); BERTScore (Zhang*\net al., 2020); MoverScore (Zhao et al., 2019);\nPRISM (Thompson and Post, 2020); BARTScore\nand its enhanced versions, BARTScore+CNN\nand BARTScore+CNN+Para (Yuan et al., 2021);\nBERT-R(Ghazarian et al., 2019); GPT-2(Radford\net al., 2019); USR (Mehri and Eskenazi, 2020b);\nS-DiCoh (Mesgar et al., 2020); FED (Mehri and\nEskenazi, 2020a); DynaEval (Zhang et al., 2021);\nSelfEval (Ma et al., 2022);PPL (Guan et al., 2021);\niBLEU (Sun and Zhou, 2012); BERT-iBLEU(Niu\net al., 2021); ParaScore (Shen et al., 2022). Note\nthat, Shen et al. (2022) also use a reference-free\n364\nMetrics Spear.\nCOH FLU CON REL\nROUGE-1 21.6 10.5 10.9 42.6\nROUGE-2 30.7 19.1 20.7 36.9\nROUGE-L 17.4 10.2 9.6 40.0\nBERTScore 28.5 10.6 13.4 29.5\nMoverScore 22.5 11.8 14.6 39.2\nPRISM 23.7 17.5 35.2 16.9\nBARTScore 33.4 20.9 34.8 24.8\n+CNN 43.3 28.7 42.7 36.1\n+CNN+Para 40.1 27.2 41.0 32.0\nIMPLICIT SCORE\ntext-davinci-001 -1.7 -5.6 19.7 8.4\ntext-davinci-003 57.4 32.9 35.2 28.1\nEXPLICIT SCORE\nChatGPT (sampling) 45.8 22.1 41.2 39.2\nChatGPT (greedy) 52.2 19.3 43.3 46.0\nTable 1: Sample-level Spearman (Spear.) correlation of\ndifferent aspects on SummEval.\nversion of BERTScore and ParaScore, denoted as\nBERTScore.Free and ParaScore.Free.\n3.3 Meta Evaluation\nIndividual Score In order to assess the reliabil-\nity of Individual Scores, we utilize the Spearman\n(Zar, 2005) and Pearson (Mukaka, 2012) correla-\ntion coefficients. As SummEval and OpenMEV A\nprovide an equivalent number of model-generated\nresults for each input, we present the sample-level\ncorrelations for these datasets. Whereas, for Twit-\nter (Extend) and the dialog-level FED datasets, we\nreport the dataset-level correlations instead.\nPairwise Comparison To avoid an excessive vol-\nume of requests when testing all permutations of\npairwise comparisons in each dataset using Chat-\nGPT, we have opted to randomly sample 200 pairs\nfrom each dataset as an approximation. To estimate\nthe reliability of metrics for pairwise comparison,\nKendall’s Tau-b (Kendall, 1945) is employed to\nevaluate the correlation between two measured vari-\nables. A detailed explanation of Kendall’s Tau-b is\nshown in Appendix C.\n4 Main Experiments\n4.1 Individual Score\nNotably, as shown in Tables 1 to 4, even without\nproviding reference or calibration details for dif-\nferent score ranges, ChatGPT’s Explicit Score has\nalready correlated with human scores better than\nmost commonly used automated metrics. On Twit-\nter (Extend), it is only outperformed by ParaScore\nand ParaScore.Free, which requires the use of ref-\nerence or hyper-parameter adjustments on a dev\nset. Additionally, the performance of the Explicit\nScore further improves when we use greedy search\ninstead of Top-P sampling for decoding.\nIt is worth noting that the Implicit Score based\non text-davinci-003 also shows promising results.\nThis suggests that LLMs’ confidence level in de-\ntermining whether a text meets a specific standard\n(yes or no) can reflect the text’s quality to some\nextent. Besides, the Implicit Score based on text-\ndavinci-003 performs better than that based on text-\ndavinci-001 in most cases, perhaps due to RLHF,\nallowing text-davinci-003 to provide answers that\nalign with human instructions better.\n4.2 Pairwise Comparison\nScoring individual samples without providing de-\ntailed criteria for each score range may lead to\ninconsistent evaluation standards across different\nsamples. Alternatively, we hypothesize that a direct\ncomparison of quality between a pair of samples\nis more likely to yield reliable evaluation results\nfrom ChatGPT. However, our analysis in Tables 5\nto 8 suggests that direct pairwise comparison is\nnot as effective as expected, and eliminating the\ninfluence of sampling in decoding is not always\nadvantageous for comparison.\nWe further categorize the texts for comparison\ninto three levels of difficulty, namely hard, medium,\nand easy, based on the difference in human scores.\nThe larger the score difference between a pair of\ntexts, the easier it is to discern the better one. The\nperformance of various metrics on distinct diffi-\nculty levels is shown in Tables 7 and 8. Overall, the\nmetrics exhibit an increasing trend in performance\nas the difficulty decreases.\nMoreover, our investigation indicates that the\nImplicit Score derived from text-davinci-003 out-\nperforms or performs comparably to the Explicit\nScore based on ChatGPT when comparing hard\ntext pairs. This finding may be attributed to the\nhigher precision of the Implicit Score, which is\nbased on the model’s output token probability (a\nfloating-point number), as opposed to the model’s\ngenerated Explicit Score, which is limited to inte-\nger values ranging from 0 to 100.\n365\nMetrics Spear.\nCOH CON DIV DEP LIK UND FLE INF INQ Overall\nBERT-R 22.9 16.3 19.6 19.2 28.1 19.8 25.3 21.1 33.7 24.8\nGPT-2 12.3 9.1 14.7 9.7 17.9 7.0 13.4 11.6 7.1 12.3\nUSR 19.4 16.9 24.2 34.1 22.1 17.2 20.9 28.8 18.8 28.8\nS-DiCoh 3.8 1.7 5.9 4.6 -7.0 -10.0 4.4 2.8 -5.4 -7.3\nFED 25.1 11.6 44.9 52.2 26.2 30.6 40.8 33.7 29.8 44.3\nDynaEval 42.3 35.2 33.2 43.9 39.8 36.1 38.9 39.6 38.8 48.2\nSelfEval 43.6 34.7 26.3 32.7 39.0 40.6 31.7 31.8 42.1 43.5\nIMPLICIT SCORE\ntext-davinci-001 37.9 33.0 36.1 26.2 35.0 57.5 39.5 54.8 45.0 39.4\ntext-davinci-003 46.8 43.8 24.9 53.4 57.3 57.6 45.0 55.1 59.0 58.0\nEXPLICIT SCORE\nChatGPT (sampling) 57.8 47.8 44.5 51.5 47.2 61.7 49.4 61.7 42.8 55.8\nChatGPT (greedy) 62.4 47.5 48.3 55.5 55.4 60.0 54.8 62.0 42.3 54.2\nTable 2: Dataset-level Spearman (Spear.) correlation of different aspects on dialogue-level FED.\nMetrics Spear. Pear.\nROUGE-1 1.4 2.0\nROUGE-2 3.5 4.1\nROUGE-L 1.3 2.1\nBERTScore 14.0 12.0\nPerplexity 32.4 33.0\nBARTScore -6.5 -8.2\n+CNN 4.9 2.6\n+CNN+Para 6.4 5.0\nIMPLICIT SCORE\ntext-davinci-001 30.3 32.9\ntext-davinci-003 37.9 43.4\nEXPLICIT SCORE\nChatGPT (sampling) 47.6 49.0\nChatGPT (greedy) 49.9 51.7\nTable 3: Sample-level Spearman (Spear.) and Pearson\n(Pear.) correlation on OpenMEV A.\nMetrics Spear. Pear.\niBLEU 3.2 1.1\nBERTScore 43.2 42.7\nBERTScore.Free 41.9 31.6\nBARTScore+CNN+Para 27.6 28.0\nBERT-iBLEU 41.6 32.7\nParaScore 53.0 52.7\nParaScore.Free 49.5 49.6\nIMPLICIT SCORE\ntext-davinci-001 15.8 15.9\ntext-davinci-003 44.4 40.3\nEXPLICIT SCORE\nChatGPT (sampling) 45.1 44.3\nChatGPT (greedy) 46.5 45.4\nTable 4: Dataset-level Spearman (Spear.) and Pearson\n(Pear.) correlation on Twitter (Extend).\nMetrics Kend.\nCOH FLU CON REL\nIMPLICIT SCORE\ntext-davinci-001 -3.2 -4.3 9.3 12.9\ntext-davinci-003 46.9 24.5 35.3 29.1\nEXPLICIT SCORE\nChatGPT (sampling) 50.3 8.6 31.7 44.3\nChatGPT (greedy) 43.7 16.8 32.8 52.5\nCOMPARISON\nChatGPT (sampling) 22.6 7.8 24.2 30.5\nChatGPT (greedy) 34.5 17.4 22.0 34.0\nTable 5: Estimated Kendall’s tau-b (Kend.) correlation\nof different aspects on SummEval.\n5 Detailed Analysis\n5.1 Why does the pairwise comparison\nparadigm perform worse?\nIn the main experiments, it is noteworthy that di-\nrect pairwise comparison using ChatGPT did not\nyield satisfactory results. To investigate whether\nthis was caused by poorly designed prompts, alter-\nnative prompts were also evaluated. These prompts\nare briefly described in Table 9, with detailed in-\nformation provided in Appendix B. Surprisingly,\nchanging the prompt did not improve performance,\nbut rather worsened it, as illustrated in Figure 1.\nTo gain further insights, we examined the confu-\nsion matrices of results based on different prompts,\nas shown in Figure 2. Our analysis revealed that, al-\nthough we have provided the option of \"both story-\nlines equally good\" in the default prompt (Prompt\nV1), ChatGPT still tended to choose one story-\nline that it deemed \"better\", as observed from Fig-\n366\nMetrics Kend.\nCOH CON DIV DEP LIK UND FLE INF INQ Overall\nIMPLICIT SCORE\ntext-davinci-001 33.3 32.0 29.6 25.1 25.6 49.9 32.8 44.8 49.5 33.6\ntext-davinci-003 28.8 30.5 18.8 36.9 41.9 43.2 34.0 45.8 43.0 36.7\nEXPLICIT SCORE\nChatGPT (sampling) 48.4 44.1 32.4 47.5 46.7 48.0 36.2 45.6 45.9 44.2\nChatGPT (greedy) 50.2 39.6 45.5 53.5 50.8 53.7 50.5 47.7 38.1 41.7\nCOMPARISON\nChatGPT (sampling) 28.3 16.1 28.5 31.5 43.0 27.5 55.5 35.2 24.5 38.6\nChatGPT (greedy) 24.3 13.7 28.5 33.8 41.9 27.5 55.5 34.1 25.6 37.5\nTable 6: Estimated Kendall’s tau-b (Kend.) correlation of different aspects on dialogue-level FED.\nMetrics Kend.\nHard Medium Easy All\nIMPLICIT SCORE\ntext-davinci-001 6.3 29.8 44.4 16.6\ntext-davinci-003 27.9 36.8 66.7 33.2\nEXPLICIT SCORE\nChatGPT (sampling) 18.5 47.3 74.3 31.2\nChatGPT (greedy) 16.8 62.6 82.5 36.2\nCOMPARISON\nChatGPT (sampling) 8.1 22.8 33.3 14.5\nChatGPT (greedy) 9.9 29.8 55.6 19.7\nTable 7: Estimated Kendall’s tau-b (Kend.) correlation\non OpenMEV A.\nMetrics Kend.\nHard Medium Easy All\nIMPLICIT SCORE\ntext-davinci-001 21.6 34.6 13.6 20.4\ntext-davinci-003 25.5 19.2 59.1 28.6\nEXPLICIT SCORE\nChatGPT (sampling) 27.8 40.0 53.8 34.9\nChatGPT (greedy) 15.3 38.5 57.0 31.2\nCOMPARISON\nChatGPT (sampling) 14.6 31.0 68.3 31.3\nChatGPT (greedy) 10.0 22.2 65.1 26.3\nTable 8: Estimated Kendall’s tau-b (Kend.) correlation\non Twitter (Extend).\nure 2(a). This could be attributed to the bias intro-\nduced by adding \"Answer: I will choose Option\"\nat the end of the prompt, which may have induced\nthe model to make a biased choice at the beginning\nof the answer. To address this issue, we modified\nthe prompt to require ChatGPT to present its rea-\nsoning process before making the final decision\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\nKend.\nV1\nV2\nV3\nV4Prompts\nFigure 1: Estimated Kendall’s tau-b (Kend.) correlation\nof Pairwise Comparison using ChatGPT with different\nprompts on OpenMEV A. We use greedy decoding for\nPrompt V1∼V3. Whereas, for Prompt V4 we use Top-P\nsampling five times to obtain multiple results and vote\nfor the final decision.\n(Prompt V4). With this prompt, the model was\nmore likely to choose the \"tie\" option, as indicated\nby the “s1=s2” column in Figure 2(b).\nAfter analyzing ChatGPT’s reasoning process,\nwe discovered that ChatGPT frequently concludes\nthat \"the quality of the two given storylines is\nequally poor.\" As a result, we prompted ChatGPT\nto choose the \"worse\" storyline instead of the \"bet-\nter\" one (Prompt V3). However, this questioning\napproach did not yield a better outcome. In ad-\ndition, Figure 2(c) shows that although Prompt\nV3 is a mirrored version of Prompt V1, which\nchanges the prompt from selecting the better op-\ntion to choosing the worse one, ChatGPT’s results\nbased on these two prompts are not always consis-\ntent. For example, in one case, ChatGPT selected\nStoryline-1 as better based on Prompt V1, but under\nthe guidance of Prompt V3, it may not necessarily\nchoose Storyline-2 as worse.\nOverall, we speculate that the poor quality of the\n367\nPROMPTS FOR PAIRWISE COMPARISON ON STORY GENERATION\nPROMPT V1 The default prompt where we first provide the beginning of the story and the corresponding two storylines\nfor comparison before presenting the question.\nPROMPT V2 A revised version of Prompt V1 where we first propose the question, then provide the beginning of the\nstory and present the two storylines to be compared in the form of options.\nPROMPT V3 A mirrored version of Prompt V1 where we instruct the model to choose “which one is worse” instead of\n“which one is better” from the two given storylines.\nPROMPT V4 A “chain-of-thought” version of Prompt V1 where we require the model to illustrate the reasoning\nprocess before presenting the final answer.\nPROMPTS FOR EXPLICIT SCORE ON STORY GENERATION\nPROMPT V1 The default prompt where we only specify the rating criteria for zero and full marks.\nPROMPT V2 A rephrased version of Prompt V1.\nPROMPT V3 A simplified version of Prompt V1 where we only describe the dimensions that need to be evaluated.\nPROMPT V4 A detailed prompt where we divide the scores into 5 scales and list the corresponding evaluation criteria\nfor each score scale.\nPROMPT V5 A “chain-of-thought” version of Prompt V1 where we require the model to first present the reasons for\nthe evaluation, and then provide the final score.\nTable 9: Prompts designed for Pairwise Comparison and Explicit Score for assessing the quality of storylines in\nstory generation. Note that Prompt V4 of Explicit Score is cited from (Wang et al., 2023a).\n/uni00000056/uni00000014/uni0000001f/uni00000056/uni00000015/uni00000056/uni00000014/uni00000020/uni00000056/uni00000015/uni00000056/uni00000014/uni00000021/uni00000056/uni00000015\n/uni00000030/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046\n/uni00000056/uni00000014/uni0000001f/uni00000056/uni00000015/uni00000056/uni00000014/uni00000020/uni00000056/uni00000015/uni00000056/uni00000014/uni00000021/uni00000056/uni00000015\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051\n/uni00000016/uni0000001a/uni00000013/uni00000019/uni0000001a\n/uni00000016/uni00000013/uni00000014/uni00000014\n/uni0000001a/uni00000013/uni0000001a/uni00000018\n(a) Prompt V1 (Default)\n/uni00000056/uni00000014/uni0000001f/uni00000056/uni00000015/uni00000056/uni00000014/uni00000020/uni00000056/uni00000015/uni00000056/uni00000014/uni00000021/uni00000056/uni00000015\n/uni00000030/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046\n/uni00000056/uni00000014/uni0000001f/uni00000056/uni00000015/uni00000056/uni00000014/uni00000020/uni00000056/uni00000015/uni00000056/uni00000014/uni00000021/uni00000056/uni00000015\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051\n/uni00000015/uni00000018/uni00000018/uni00000014/uni00000015/uni0000001b\n/uni00000016/uni0000001a/uni00000017\n/uni00000015/uni00000014/uni00000015/uni00000013/uni00000017/uni00000014 (b) Prompt V4 (“Chain-of-Thought” )\n/uni00000056/uni00000014/uni0000001f/uni00000056/uni00000015/uni00000056/uni00000014/uni00000020/uni00000056/uni00000015/uni00000056/uni00000014/uni00000021/uni00000056/uni00000015\n/uni00000030/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046\n/uni00000056/uni00000014/uni0000001f/uni00000056/uni00000015/uni00000056/uni00000014/uni00000020/uni00000056/uni00000015/uni00000056/uni00000014/uni00000021/uni00000056/uni00000015\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051\n/uni0000001b/uni00000019/uni00000013/uni00000014/uni0000001b\n/uni00000014/uni00000015/uni00000013/uni00000015\n/uni00000019/uni00000017/uni00000013/uni00000014/uni0000001b (c) Prompt V3 (Mirrored)\nFigure 2: Confusion matrices of pairwise comparisons on OpenMEV A based on different prompts using ChatGPT.\nPrompt V1 is the default prompt used in the main experiments. Prompt V4 and V3 are the “chain-of-thought” and\n“mirrored” versions of Prompt V1 respectively. Details of these prompts are presented in Table 9 and Appendix B.\ncandidate texts used in our experiments is the main\nreason why comparing pairs directly with ChatGPT\ndid not yield good results. ChatGPT perceives the\ncandidate texts as generally low quality, making it\nto select a “better” or “worse” one from them. This\nmight lead to ChatGPT’s unstable decisions.\n5.2 Why does Explicit Score generally\nperform better than Implicit Score?\nIn order to obtain the Explicit Score, we utilize\nChatGPT to generate scores in a natural language\nformat. However, as we do not have access to Chat-\nGPT’s token probabilities, we instead rely on the\nconfidence of text-davinci series models to deter-\nmine the Implicit Score, which reflects how well\na text meets a particular evaluation criterion. As\nstated in the Main Experiments (§4), the Explicit\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000015\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000017\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b\n/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015\n/uni00000013/uni00000011/uni00000013/uni00000014/uni00000017/uni00000027/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000030/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046/uni00000056\n/uni0000002c/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000046/uni0000004c/uni00000057/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000001d/uni00000003/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000014\n/uni0000002c/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000046/uni0000004c/uni00000057/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000001d/uni00000003/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016\n/uni00000028/uni0000005b/uni00000053/uni0000004f/uni0000004c/uni00000046/uni0000004c/uni00000057/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000001d/uni00000003/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000003/uni0000000b/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni0000000c\n/uni00000028/uni0000005b/uni00000053/uni0000004f/uni0000004c/uni00000046/uni0000004c/uni00000057/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000001d/uni00000003/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000003/uni0000000b/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c/uni0000000c\nFigure 3: Distribution of different types of Individual\nScores on OpenMEV A. The Implicit Score is rescaled\ninto [0,100].\n368\nExplicit Score Spear. Pear.\nCHATGPT\nw/ PROMPT V1 ( GREEDY ) 49.9 51.7\nw/ PROMPT V2 ( GREEDY ) 50.8 53.6\nw/ PROMPT V3 ( GREEDY ) 49.4 52.0\nw/ PROMPT V4 ( GREEDY ) 46.1 48.4\nw/ PROMPT V5 ( SAMPLING ) 47.2 50.8\nTable 10: Sample-level Spearman (Spear.) and Pearson\n(Pear.) correlation for Explicit Score based on ChatGPT\nwith different prompts on OpenMEV A. We use greedy\ndecoding for Prompt V1 ∼V4. Whereas, for Prompt\nV5, we employ Top-P sampling five times to generate\nmultiple reasons and average the resulting scores.\nScore is generally more effective than the Implicit\nScore. This difference in effectiveness could be\nattributed not only to the variation in the models\nused but also to the distribution of the two scores.\nFigure 3 illustrates that the Implicit Score distri-\nbution has a peaked structure and is concentrated\nwithin a small range. In contrast, the Explicit Score\ndistribution is smoother, allowing for greater dis-\ncrimination between scores for different texts.\n5.3 How does the prompt design affect\nExplicit Score?\nWe also investigate the impact of prompt design on\nthe performance of rating Explicit Scores generated\nby ChatGPT. The detailed prompts are provided\nin Appendix A, and their main features and dif-\nferences are summarized in Table 9. Our results,\npresented in Table 10, indicate that paraphrasing\n(V2) or simplifying (V3) the default prompt (V1)\ndoes not significantly affect the performance of\nExplicit Score based on ChatGPT. In contrast, re-\nfining scoring criteria (V4) or providing reasons\nbefore scoring (V5) results in a slight decrease in\nperformance. The former may be due to the fact\nthat the refined scoring rules in Prompt V4 do not\nfully match the standards used for actual manual\nannotation, and dividing scores into five scales re-\nduces the distinction between scores for different\nsamples. The latter may be due to the overall low\nquality of the dataset. Our observation indicates\nthat ChatGPT’s evaluations for each text are similar\nand mostly negative. After giving reasons before\nscoring, ChatGPT’s scoring focuses more on the\nreasons rather than the text itself, resulting in lower\nscores for each text based on Prompt V5 and reduc-\ning the distinction between scores. The detailed dis-\ntribution of scores derived from different prompts\nis demonstrated using a violin plot in Figure 4.\n�� �� �� �� ��\n� � �����\n�\n��\n��\n��\n��\n��\n��\n��\n������ ��\nFigure 4: Distribution of Explicit Scores based on Chat-\nGPT with different prompts on OpenMEV A. For Prompt\nV4, the scores are normalized into [0, 100].\n6 Related Work\nIn the field of text quality evaluation, researchers\nhave devised two main lines of approaches:\nreference-based and reference-free methods. The\nreference-based text evaluation aims to assess the\nquality by comparing outputs with ground truth,\ne.g. ROUGE (Lin, 2004), BERTScore (Zhang*\net al., 2020) and BARTScore (Yuan et al., 2021).\nHowever, due to the inherent complexity and di-\nversity of text, it is impossible to obtain references\ncovering the entire spectrum of potential outputs.\nThis limitation has prompted researchers to explore\nreference-free evaluation methods without relying\non predefined references e.g. iBLEU (Sun and\nZhou, 2012) and ParaScore (Shen et al., 2022). In\nthis line, a reliable sentence representation model\nis required (Gao et al., 2021; Shen et al., 2023a,b).\nRecent studies have indicated that LLM-based eval-\nuation methods can exhibit good consistency with\nhuman evaluation in assessing text quality (Fu et al.,\n2023; Wang et al., 2023a; Kocmi and Federmann,\n2023; Ji et al., 2023). However, most of these\nworks are preliminary explorations or require gold\nreferences. On the contrary, we are the first to\nconduct extensive experiments to investigate the\noptimal evaluation approaches using LLMs with-\nout references, and moreover propose some clues\nfor customized text evaluation.\n7 Conclusion\nThis paper explores the feasibility of LLMs, specif-\nically ChatGPT and text-davinci series models, for\nevaluating text quality in a reference-free mode.\nThrough an empirical study, we compare different\n369\nmethods for the evaluation of text quality and rec-\nommend the use of an Explicit Score generated by\nChatGPT as the most effective and stable approach.\nThis paper also highlights the potential problem\nof directly comparing the quality of two texts us-\ning ChatGPT and the limitations of Implicit Scores\nobtained through the confidence of text-davinci se-\nries models. The prompt design is another crucial\nfactor impacting the performance of the Explicit\nScore generated by ChatGPT. Overall, this paper\ndemonstrates the potential of LLMs in evaluating\ntext quality without reference and we hope it will\nprovide useful insights for future research.\nLimitations\n• Meta Evaluation Strategy\nWe primarily assess the reliability of metrics based\non their correlation with human scores. However,\nit should be noted that the consistency between\nscores annotated by different raters may not always\nbe high in certain datasets. Hence, the correlation\nwith human ratings may not always reflect the per-\nformance of metrics appropriately.\n• Coverage of Texts\nWe only conducted experiments on four text-\ngeneration tasks. Additionally, the quality distri-\nbution of the evaluated texts may be non-uniform,\npotentially lacking in extremely high-quality texts.\nEven if a metric performs well in evaluating a set\nof low-quality texts, it does not necessarily imply\nthe same level of discrimination for high-quality\ntexts, and vice versa. Furthermore, our evaluation\nhas been limited to short texts, omitting the consid-\neration of long-text generation.\n• Coverage of Models\nWe utilize OpenAI’s API to access their language\nmodels, including ChatGPT (gpt3.5-turbo-0301),\ntext-davinci-003, and text-davinci-001. However,\nthese models may be updated over time, which can\nresult in inconsistencies in experimental outcomes.\nMoreover, we have not considered a wider range of\nLLMs, such as text-babbage-001, text-curie-001,\nand the FLAN-T5 series. Regrettably, due to API\nlimitations, we were unable to obtain results from\nthe more powerful GPT4 model.\n• Prompt Design\nOur exploration of prompts was limited to a few\nbasic variations. Future research may benefit from\nmore sophisticated prompt designs, such as in-\ncorporating few-shot demonstrations, providing\nmore precise annotation guidelines, or guiding the\nmodel through multi-turn conversations to facilitate\na more accurate assessment of text quality.\nAcknowledgements\nThis research was supported in part by the National\nNatural Science Foundation of China(62006062,\n62176076), the Guangdong Provincial Key Lab-\noratory of Novel Security Intelligence Technolo-\ngies(2022B121201000 5), Natural Science Founda-\ntion of Guangdong(2023A1515012922), and Key\nTechnologies Research and Development Program\nof Shenzhen JSGG20210802154400001.\n370\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Ziyang\nChen, and Jia Li. 2022a. What would harry say?\nbuilding dialogue agents for characters in a story.\narXiv preprint arXiv:2211.06869.\nYi Chen, Haiyun Jiang, Lemao Liu, Rui Wang, Shum-\ning Shi, and Ruifeng Xu. 2022b. Mcpg: A flexible\nmulti-level controllable framework for unsupervised\nparaphrase generation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n5948–5958.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAlexander R. Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021a. SummEval: Re-evaluating summa-\nrization evaluation. Transactions of the Association\nfor Computational Linguistics, 9:391–409.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021b. Summeval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nSarik Ghazarian, Johnny Wei, Aram Galstyan, and\nNanyun Peng. 2019. Better automatic evaluation\nof open-domain dialogue systems with contextual-\nized embeddings. In Proceedings of the Workshop\non Methods for Optimizing and Evaluating Neural\nLanguage Generation, pages 82–89, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nJian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-\nbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie\nHuang. 2021. OpenMEV A: A benchmark for evaluat-\ning open-ended story generation metrics. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6394–6407, Online.\nAssociation for Computational Linguistics.\nYunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun,\nDongyu Pan, Baochang Ma, and Xiangang Li. 2023.\nExploring chatgpt’s ability to rank content: A prelim-\ninary study on consistency with human preferences.\nM. G. Kendall. 1945. The treatment of ties in ranking\nproblems. Biometrika, 33(3):239–251.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nLongxuan Ma, Ziyu Zhuang, Weinan Zhang, Mingda\nLi, and Ting Liu. 2022. SelF-eval: Self-supervised\nfine-grained dialogue evaluation. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 485–495, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nShikib Mehri and Maxine Eskenazi. 2020a. Unsuper-\nvised evaluation of interactive dialog with DialoGPT.\nIn Proceedings of the 21th Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue,\npages 225–235, 1st virtual meeting. Association for\nComputational Linguistics.\nShikib Mehri and Maxine Eskenazi. 2020b. USR: An\nunsupervised and reference free evaluation metric\nfor dialog generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 681–707, Online. Association for\nComputational Linguistics.\nMohsen Mesgar, Sebastian Bücker, and Iryna Gurevych.\n2020. Dialogue coherence assessment without ex-\nplicit dialogue act labels. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1439–1450, Online. Association\nfor Computational Linguistics.\nMavuto M Mukaka. 2012. A guide to appropriate use of\ncorrelation coefficient in medical research. Malawi\nmedical journal, 24(3):69–71.\n371\nTong Niu, Semih Yavuz, Yingbo Zhou, Nitish Shirish\nKeskar, Huan Wang, and Caiming Xiong. 2021. Un-\nsupervised paraphrasing with pretrained language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5136–5150, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nLingfeng Shen, Haiyun Jiang, Lemao Liu, and Shuming\nShi. 2023a. Sen2pro: A probabilistic perspective\nto sentence embedding from pre-trained language\nmodel. arXiv preprint arXiv:2306.02247.\nLingfeng Shen, Haiyun Jiang, Lemao Liu, and Shuming\nShi. 2023b. A simple and plug-and-play method for\nunsupervised sentence representation enhancement.\narXiv preprint arXiv:2305.07824.\nLingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming\nShi. 2022. On the evaluation metrics for paraphrase\ngeneration. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3178–3190, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nMingyang Song, Haiyun Jiang, Shuming Shi, Songfang\nYao, Shilong Lu, Yi Feng, Huafeng Liu, and Liping\nJing. 2023. Is chatgpt a good keyphrase generator? a\npreliminary study. arXiv preprint arXiv:2303.13001.\nHong Sun and Ming Zhou. 2012. Joint learning of a dual\nSMT system for paraphrase generation. In Proceed-\nings of the 50th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 38–42, Jeju Island, Korea. Association\nfor Computational Linguistics.\nBrian Thompson and Matt Post. 2020. Automatic ma-\nchine translation evaluation in many languages via\nzero-shot paraphrasing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 90–121, Online.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023a. Is chatgpt a good nlg evaluator? a preliminary\nstudy.\nRui Wang, Jianzhu Bao, Fei Mi, Yi Chen, Hongru Wang,\nYasheng Wang, Yitong Li, Lifeng Shang, Kam-Fai\nWong, and Ruifeng Xu. 2023b. Retrieval-free knowl-\nedge injection through multi-document traversal for\ndialogue models. In Annual Meeting of the Associa-\ntion for Computational Linguistics.\nWei Xu, Chris Callison-Burch, and Bill Dolan. 2015.\nSemEval-2015 task 1: Paraphrase and semantic sim-\nilarity in Twitter (PIT). In Proceedings of the 9th\nInternational Workshop on Semantic Evaluation (Se-\nmEval 2015), pages 1–11, Denver, Colorado. Associ-\nation for Computational Linguistics.\nWei Xu, Alan Ritter, Chris Callison-Burch, William B.\nDolan, and Yangfeng Ji. 2014. Extracting lexically\ndivergent paraphrases from Twitter. Transactions of\nthe Association for Computational Linguistics, 2:435–\n448.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nJerrold H Zar. 2005. Spearman rank correlation. Ency-\nclopedia of biostatistics, 7.\nChen Zhang, Yiming Chen, Luis Fernando D’Haro,\nYan Zhang, Thomas Friedrichs, Grandee Lee, and\nHaizhou Li. 2021. DynaEval: Unifying turn and di-\nalogue level evaluation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5676–5689, Online. Association\nfor Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\n372\nA Different Prompts for Explicit Score on\nStory Generation\n======= PROMPT FOR EXPLICIT SCORE V1 =======\nScore the following storyline given the beginning of the\nstory on a continual scale from 0 (worst) to 100 (best),\nwhere a score of 0 means \"The storyline makes no sense\nand is totally not understandable\" and a score of 100 means\n\"The storyline is perfect-written and highly consistent with\nthe given beginning of the story\".\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\nScore:\n======= PROMPT FOR EXPLICIT SCORE V2 =======\nOn a scale of 0 to 100, evaluate the storyline based on the\ngiven beginning. A score of 0 indicates that the storyline\nis incomprehensible, while a score of 100 means that the\nstoryline is flawlessly written and logically follows from\nthe beginning of the story.\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\nScore:\n======= PROMPT FOR EXPLICIT SCORE V3 =======\nScore the overall quality of the following storyline given\nthe beginning of the story on a continual scale from 0\n(worst) to 100 (best). Consider whether the storyline is\nwell-written and consistent with the given beginning of the\nstory.\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\nScore:\nB Different Prompts for Pairwise\nComparison on Story Generation\n======= PROMPT FOR EXPLICIT SCORE V4 =======\nScore the following storyline given the beginning of the\nstory with one to five stars. Where\n• one star means \"Nonsense\",\n• two stars mean \"The storyline has some connections with\nthe beginning, but is not understandable\",\n• three stars mean \"The storyline has some connections\nwith the beginning and is understandable\",\n• four stars mean \"The storyline is consistent with the\nbeginning and possibly involves a few grammar mistakes\",\n• and five stars mean \"Perfect storyline and grammar\".\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\nStars (1-5):\n======= PROMPT FOR EXPLICIT SCORE V5 =======\nScore the following storyline given the beginning of the\nstory on a continual scale from 0 (worst) to 100 (best),\nwhere a score of 0 means \"The storyline makes no sense\nand is totally not understandable\" and a score of 100 means\n\"The storyline is perfect-written and highly consistent with\nthe given beginning of the story\". Please first give your\nreason carefully (indicated by \"Reason:\") and then decide\nyour final score (indicated by \"Score: 1-100\").\nThe beginning of the story:\n[Conditioned Text]\nStoryline:\n[Generated Text]\n===== PROMPT FOR PAIRWISE COMPARISON V1 =====\nConsider the following two storylines written according to\nthe given beginning of the story:\nThe beginning of the story:\n[Conditioned Text]\nStoryline-1:\n[Generated Text-1]\nStoryline-2:\n[Generated Text-2]\nQuestion: Which storyline is better-written and more con-\nsistent with the beginning of the story? Please answer with\none of the following options.\nOptions:\n(A) Storyline-1\n(B) Storyline-2\n(C) Both storylines are equally well-written and consistent\nwith the beginning of the story.\nAnswer: I will choose Option\n373\n===== PROMPT FOR PAIRWISE COMPARISON V2 =====\nQuestion: Which storyline is better-written and more con-\nsistent with the beginning of the story? Please answer with\none of the following options.\nThe beginning of the story:\n[Conditioned Text]\nOptions:\n(A) [Generated Text-1]\n(B) [Generated Text-2]\n(C) Both storylines are equally well-written and consistent\nwith the beginning of the story.\nAnswer: I will choose Option\n===== PROMPT FOR PAIRWISE COMPARISON V3 =====\nConsider the following two storylines written according to\nthe given beginning of the story:\nThe beginning of the story:\n[Conditioned Text]\nStoryline-1:\n[Generated Text-1]\nStoryline-2:\n[Generated Text-2]\nQuestion: Which storyline has poorer writing and is less\nconsistent with the beginning of the story? Please answer\nwith one of the following options.\nOptions:\n(A) Storyline-1\n(B) Storyline-2\n(C) Both storylines are equally poor-written and inconsis-\ntent with the beginning of the story.\nAnswer: I will choose Option\n===== PROMPT FOR PAIRWISE COMPARISON V4 =====\nConsider the following two storylines written according to\nthe given beginning of the story:\nThe beginning of the story:\n[Conditioned Text]\nStoryline-1:\n[Generated Text-1]\nStoryline-2:\n[Generated Text-2]\nQuestion: Which storyline is better-written and more con-\nsistent with the beginning of the story? Please first give\nyour reason carefully (indicated by \"Reason:\") and then\nchoose one of the following options (indicated by \"Answer:\nA/B/C\").\nOptions:\n(A) Storyline-1\n(B) Storyline-2\n(C) Both storylines are equally well-written (poor-written)\nand consistent (inconsistent) with the beginning of the\nstory.\n374\nC An Explanation of Kendall’s Tall-b\nKendall’s Tau-b is a measure of the correlation\nbetween two variables, specifically designed to\nhandle ties and ranks. The formula to calculate\nKendall’s Tau-b is as follows:\nτB = P − Qp\n(P + Q + T)(P + Q + U)\n. (2)\nwhere P is the number of concordant pairs, Q is the\nnumber of discordant pairs, T is the number of ties\nonly in human judgments, and U is the number of\nties only in the given metric. To better understand\nthe calculation of P, Q, T, and U, we can refer to\nthe following table:\nMetric\ns1 < s2 s1 = s2 s1 > s2\nHuman\ns1 < s2 P U Q\ns1 = s2 T - T\ns1 > s2 Q U P",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7323101758956909
    },
    {
      "name": "Natural language processing",
      "score": 0.575919508934021
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.506138026714325
    },
    {
      "name": "Language model",
      "score": 0.4592892825603485
    },
    {
      "name": "Empirical research",
      "score": 0.43501460552215576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3979960083961487
    },
    {
      "name": "Statistics",
      "score": 0.20199128985404968
    },
    {
      "name": "Mathematics",
      "score": 0.11658650636672974
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ],
  "cited_by": 37
}