{
  "title": "On the Challenges of Using Large Language Models for NCL Code Generation",
  "url": "https://openalex.org/W4387807022",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2556785219",
      "name": "Daniel de Sousa Moraes",
      "affiliations": [
        "Pontifical Catholic University of Rio de Janeiro"
      ]
    },
    {
      "id": "https://openalex.org/A3106619064",
      "name": "Polyana Bezerra da Costa",
      "affiliations": [
        "Pontifical Catholic University of Rio de Janeiro"
      ]
    },
    {
      "id": null,
      "name": "Antonio J. G. Busson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4212188051",
      "name": "Jose Matheus Carvalho Boaro",
      "affiliations": [
        "Pontifical Catholic University of Rio de Janeiro"
      ]
    },
    {
      "id": "https://openalex.org/A2251754870",
      "name": "CARLOS DE SALLES SOARES NETO",
      "affiliations": [
        "Universidad Filadelfia de México"
      ]
    },
    {
      "id": "https://openalex.org/A501179436",
      "name": "Sérgio Colcher",
      "affiliations": [
        "Pontifical Catholic University of Rio de Janeiro"
      ]
    },
    {
      "id": "https://openalex.org/A2556785219",
      "name": "Daniel de Sousa Moraes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3106619064",
      "name": "Polyana Bezerra da Costa",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Antonio J. G. Busson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4212188051",
      "name": "Jose Matheus Carvalho Boaro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251754870",
      "name": "CARLOS DE SALLES SOARES NETO",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A501179436",
      "name": "Sérgio Colcher",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4288805334",
    "https://openalex.org/W4365205411",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2763652973",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4313547549",
    "https://openalex.org/W2944378183",
    "https://openalex.org/W4381586770",
    "https://openalex.org/W4366389138",
    "https://openalex.org/W1598735588",
    "https://openalex.org/W2055926441",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3118151845",
    "https://openalex.org/W4366596891",
    "https://openalex.org/W4384920109",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4366735548"
  ],
  "abstract": "A significant concern raised in the domain of authoring tools for interactive Digital TV (iDTV) has been their usability when considering the target audience, which typically consists of content creators and not necessarily programmers. NCL (Nested Context Language), the declarative language for developing interactive applications for Brazilian Digital TV and an ITU-T Recommendation for IPTV services, is a simple declarative language but not an easy tool for non-technical authors. The proliferation of Large Language Models (LLMs) has recently instigated substantial transformations across several domains, including synthesizing code with remarkable potential. This paper proposes an investigation into the challenges of using LLMs to aid automatic NCL code generation/synthesis in authoring tools for iDTV content production. It shows initial evidence that current pre-trained LLMs cannot synthesize NCL code with satisfactory quality. In this context, we raise the main challenges for NCL code generation using LLMs and some issues related to the good practices for engineering prompts and integrating pre-trained LLMs into multimedia authoring tools.",
  "full_text": "On the Challenges of Using Large Language Models for\nNCL Code Generation\nDaniel de Sousa Moraes\nTeleMídia/PUC-Rio\ndanielmoraes@telemidia.puc-rio.br\nPolyana Bezerra da Costa\nTeleMídia/PUC-Rio\npolyana@telemidia.puc-rio.br\nAntonio J. G. Busson\nBTG Pactual\nantonio.busson@btgpactual.com\nJosé Matheus Carvalho Boaro\nTeleMídia/PUC-Rio\nboaro@telemidia.puc-rio.br\nCarlos de Salles Soares Neto\nTeleMídia-MA/UFMA\ncarlos.salles@ufma.br\nSergio Colcher\nTeleMídia/Departamento de\nInformática/PUC-Rio\ncolcher@inf.puc-rio.br\nAbstract\nA significant concern raised in the domain of authoring tools\nfor interactive Digital TV (iDTV) has been their usability\nwhen considering the target audience, which typically con-\nsists of content creators and not necessarily programmers.\nNCL (Nested Context Language), the declarative language\nfor developing interactive applications for Brazilian Digi-\ntal TV and an ITU-T Recommendation for IPTV services,\nis a simple declarative language but not an easy tool for\nnon-technical authors. The proliferation of Large Language\nModels (LLMs) has recently instigated substantial transfor-\nmations across several domains, including synthesizing code\nwith remarkable potential. This paper proposes an investi-\ngation into the challenges of using LLMs to aid automatic\nNCL code generation/synthesis in authoring tools for iDTV\ncontent production. It shows initial evidence that current\npre-trained LLMs cannot synthesize NCL code with satisfac-\ntory quality. In this context, we raise the main challenges for\nNCL code generation using LLMs and some issues related to\nthe good practices for engineering prompts and integrating\npre-trained LLMs into multimedia authoring tools.\nKeywords: NCL, LLMs, Code Generation, Authoring\n1 Introduction\nAuthoring tools [3, 6, 11, 12, 18, 21, 23, 25, 26] have been\nthe subject of extensive study when addressing the develop-\nment of applications in NCL (Nested Context Language), the\ndeclarative language for developing interactive applications\nfor the Brazilian Digital TV and ITU-T IPTV systems.\nA significant concern within this domain has been the\nusability of these tools for their target audience, which typi-\ncally consists of content creators rather than programmers.\nConsequently, these tools have focused on creating visual\nabstractions, rather than textual, to facilitate their utilization.\nIn: Workshop Futuro da TV Digital Interativa, Ribeirão Preto, Brasil. Anais\nEstendidos do Simpósio Brasileiro de Sistemas Multimídia e Web (WebMe-\ndia). Porto Alegre: Sociedade Brasileira de Computação, 2023.\n© 2023 SBC – Sociedade Brasileira de Computação.\nISSN 2596-1683\nHowever, these abstractions can introduce inherent limita-\ntions and a certain level of complexity, requiring users to\ninvest time in learning and adapting to the provided func-\ntionalities.\nThe proliferation of Large Language Models (LLMs) has re-\ncently instigated substantial transformations across several\ndomains. These models have facilitated the creation of chat-\nbots capable of appropriately responding to diverse requests\nwithin diverse contexts, such as chatGPT1 and Google Bard2.\nLLMs have been applied to program or code synthesis tasks\nand presented remarkable potential [8, 9, 14, 19, 22]. Thus,\nit seems that, if embedded in a multimedia authoring tool,\nLLMs could facilitate the development of interactive NCL ap-\nplications, allowing authors to intuitively define application\nrequirements in natural language.\nThis paper shows initial evidence that current pre-trained\nLLMs cannot synthesize NCL codes with satisfactory qual-\nity. It mainly fails with syntax and language rules while not\ngenerating content that meets specific application require-\nments. In this context, we raise the main challenges for NCL\ncode generation using LLMs. We also raise challenges related\nto good practices for engineering prompts and integrating\npre-trained LLMs into multimedia authoring tools.\n2 NCL Code Generation\nWe conducted experiments employing current LLMs to as-\nsess whether their performance in the NCL code generation\ntask is satisfactory or whether it is necessary and plausible to\nfine-tune a pre-trained model to generate NCL code. For this,\nwe wrote three different prompts for NCL code generation\nand conducted tests with 4 LLM-based services: chatGPT,\nBard, Llama-v2 [28], and PaLM2 [2]. These prompts are as\nfollows in Listing 1.\nWe utilize these prompts as inputs for each model and\nsubsequently compare their respective outputs. Following\nthis, the responses were manually evaluated by two NCL\nexperts based on a rating scale of 0 to 4, which signifies the\ncode generation quality. Each response score was given in\n1https://openai.com/blog/chatgpt\n2https://bard.google.com/\n151\nAnais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil Moraes et al.\nmutual agreement by both evaluators. Results are presented\nin Table 1.\nPrompt 1: Write an NCL (Nested Context Language)\ncode that starts a video file named \"video.mp4\".\nPrompt 2: Write an NCL code to start a video in full\nscreen using a port, and after 10 seconds, using\na link, it starts an image in the top-right\ncorner of the screen.\nPrompt 3: Write an NCL code to start a video in full\nscreen using a port, and after 10 seconds, using\na link, it starts an image in the top-right\ncorner of the screen. The following code\nexemplifies a basic application where a video is\ninitiated using a port element:\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n<ncl id=\"exemplo01\"\nxmlns=\"http://www.ncl.org.br/NCL3.0/EDTVProfile\">\n<head>\n<regionBase>\n<region id=\"rgVideo1\" zIndex=\"1\" />\n</regionBase>\n<descriptorBase>\n<descriptor id=\"dVideo1\"\nregion=\"rgVideo1\" />\n</descriptorBase>\n</head>\n<body>\n<port id=\"pVideoAbertura\"\ncomponent=\"videoAbertura\" />\n<media id=\"videoAbertura\"\nsrc=\"media/abertura.mpg\"\ndescriptor=\"dVideo1\" />\n</body>\n</ncl>\nListing 1. Experiments prompts to evaluate generic LLM on\nNCL code generation\nLLM Prompt 1 Prompt 2 Prompt 3\nChatGPT (GPT-3.5) 0 2 1\nBard 1 1 1\nLlama-v2 70B 0 0 0\nPaLM2 (Bison) 1 1 3\nTable 1. LLMs rating for each prompt. (0) Can not generate\ncode; (1) Invalid NCL code; (2) NCL code with many errors;\n(3) NCL code with few errors; (4) NCL code as expected.\nIn Prompt 1, the task was to define an application code\nwith one element media referring to a video and start it. The\nChatGPT answered that it could not generate an NCL code\nresponse, alleging that \"...there is no widely known or standard-\nized programming language called \"Nested Context Language\"\n(NCL).. \". Conversely, the Bard and PaLM2 models demon-\nstrated the ability to generate code-form responses. However,\nthey hallucinated, producing responses in languages other\nthan NCL. Bard generated a Python response referencing an\ninexistent API named \"ncl, \" while PaLM2 just generated a\nJSON object defining attributes of a video.\nPrompt 2 demands a slightly more elaborated task. It\nexplicitly says that the video presentation must be initialized\nusing a port element. Also, after 10 seconds of playing the\nvideo, an image on a specific screen region must be started\nusing a link. ChatGPT was able to generate NCL codes but\nwith many syntactic errors and semantically far from the\nexpected response. Bard and PaLM2 suffered from halluci-\nnations again. This time, PaLM2 generated what looks like\nPython code. Bard generated a Python code similar to the\nprevious task, using a made API named \"ncl\".\nLastly, in Prompt 3, we used the same task of Prompt 2,\nbut this time adding an example of an NCL code, in which a\nvideo media is defined and initiated through a port element.\nIn this round, the PaLM2 model performed better than oth-\ners, generating a code close to the expected response. Both\nGPT-3.5 and Bard generated invalid NCL codes. For all the\nprompts, the Llama-v2 model could not generate a response\ncode in any programming language.\nThis experiment shows early signs that even LLMs trained\non large amounts of data still cannot be adequately used\nfor NCL code generation tasks. We hypothesize that this\nhappens because NCL is a domain language for a particular\nniche. Thus, it is necessary to fine-tune such LLMs for the\nproper generation of NCL codes.\n3 Challenges of Using LLMs for NCL code\ngeneration\nGenerally, LLMs denote Transformer-based language models\nthat contain hundreds of billions of parameters or even more\n[16]. These models undergo training on extensive textual\ndatasets. With this in mind, we can identify a set of specific,\nthough not unique, challenges that need to be addressed to\nimplement LLMs for the generation of NCL codes success-\nfully.\n3.1 Adjusting the LLMs\nUsing LLMs trained on generic data and task-agnostic has\ndemonstrated strong performances across a wide range of\ntasks [5, 10, 28]. However, NCL code generation tasks might\nperform differently than desired, as demonstrated in the\nexperiment in section 2.\nFor instance, Listing 2 shows that even the best response\nfor the prompts, using natural language, fails to generate\na correct code in the experiment. This code was acquired\nas a response to the Prompt3 by the PaLM2 model in the\nexperiment, and even though it is the most coherent and\n152\nOn the Challenges of Using Large Language Models for NCL Code Generation Anais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil\nclosest response to the desired code, it still fails to complete\nthe task.\nThe link created does not correspond to the prompt, it\nis starting the image presentation after the video stops. In\nthe prompt, we ask to start an image ten seconds after the\nvideo begins. Besides, the connector used is not declared or\nimported.\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n<ncl id=\"exemplo02\"\nxmlns=\"http://www.ncl.org.br/NCL3.0/EDTVProfile\">\n<head>\n<regionBase>\n<region id=\"rgVideo1\" zIndex=\"1\" />\n<region id=\"rgImagem1\" zIndex=\"2\" top=\"0\"\nright=\"0\" width=\"200\" height=\"200\" />\n</regionBase>\n<descriptorBase>\n<descriptor id=\"dVideo1\"\nregion=\"rgVideo1\" />\n<descriptor id=\"dImagem1\"\nregion=\"rgImagem1\" />\n</descriptorBase>\n</head>\n<body>\n<port id=\"pVideoAbertura\"\ncomponent=\"videoAbertura\" />\n<media id=\"videoAbertura\"\nsrc=\"media/abertura.mpg\"\ndescriptor=\"dVideo1\" />\n<link xconnector=\"onEndStart\">\n<bind role=\"onEnd\"\ncomponent=\"videoAbertura\" />\n<bind role=\"start\" component=\"imagem1\" />\n</link>\n<media id=\"imagem1\" src=\"media/imagem.jpg\"\ndescriptor=\"dImagem1\" />\n</body>\n</ncl>\nListing 2. PaLM2 generated code for Prompt 3\nMany factors can explain the results in section 2, but\nmainly because the model was not trained for code genera-\ntion. Thus, the models may sometimes suffer from hallucina-\ntions and include inaccurate information in their responses\n[16]. To mitigate this, some works adapt generic LLMs, fine-\ntuning them to specific tasks, such as code generation. There\nare works for particular languages such as Codex [8], a GPT-\n3 LLM fine-tuned to generate Python functions from doc\nstrings. Also, phi-1 [14], a smaller LLM, which is also trained\nto generate Python functions from doc strings. There are\nalso models trained for multiple languages, such as CodeGen\nLLMs [22] that use C, C++, Go, Java, JavaScript, and Python\ncode datasets, and the AlphaCode LLM [19], pre-trained on\na C++, C#, Go, Java, JavaScript, Lua, PHP, Python, Ruby,\nRust, Scala and TypeScript source code dataset collected\nfrom GitHub.\nAll the works mentioned positively impact the perfor-\nmance of their specific tasks. Thus, it is clear the necessity\nof experiments to confirm the need to train LLMs with spe-\ncific examples of NCL code, comparing their performance\nwith that of generic LLMs. Such an experiment demands the\ncreation of a dataset with examples of NCL code.\nOne of the main challenges is the creation or use of a suffi-\nciently extensive dataset for training the LLM or fine-tuning\na pre-trained model, as done in [8, 9, 14, 19, 22]. This entails\ndefining and collecting a substantial number of code exam-\nples that effectively represent the core characteristics of the\nNCL language and its potential applications. By assembling a\ndiverse set of code samples, we enable the model to grasp the\nintricacies of the language and adopt coding best practices\nspecific to NCL. It is essential to recognize that the more\nexposure the model has to pertinent examples, the better\nequipped it becomes to produce high-quality code.\nMoreover, in the experiment of specifying an LLM for\nNCL code generation, we can also highlight questions about\nwhat type of adaptation in the LLM would be feasible, given\nthe context of available data about the language and the\nresources available for model implementation and execution.\nWe can mention two widely used methods (among many\nothers) [30], fine-tuning and few-shot training. The fine-\ntuning, used in several works already mentioned, is a process\nof re-training on a task-specific dataset, a pre-trained model,\nupdating all parameters, which, depending on the model\narchitecture, will require significant memory resources to\nstore parameters, model activations, etc. [16]. Besides, it also\nrequires a fair amount of task-specific data to optimize its\nperformance, adding to the need for dataset construction.\nThere are also the Parameter-efficient fine-tuning (PEFT)\nmethods, such as Adapters [15], that add learnable layers into\nthe Transformer architecture to be updated during the fine-\ntuning, keeping the rest of the network from change. Another\nmethod is prefix-tuning, when token embeddings are added\nto an input to be learned during the fine-tuning without\nchanging the rest of the model’s parameters. These methods\ncan reduce resource usage while maintaining competitive\nperformance to a full fine-tuning.\nAnother promising approach is using zero, one-or-few shot\ntraining [29]. In this method, a small set of examples is given\nalong with a prompt query. The LLM uses the information\nreceived to teach itself to complete the demanded task. Ac-\ncording to Ahmed and Devanbu [1], this method does not\nrequire weight adjustments. This method enables the special-\nization of a generic LLM for NCL code generation without\nthe need for ample labeled examples and the availability of\nmany computing resources.\nLastly, after defeating all the challenges above, we must\nevaluate the performance of the chosen methods and models.\nIn this stage, we ensure the model effectively generalizes\n153\nAnais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil Moraes et al.\nto new data and performs proficiently. We may adjust large\nmodels using PPO (Proximal Policy Optimization) based on\na reward model trained to align the models with human\npreferences [24]. ChatGPT also employed this approach. The\nreward model is trained using comparison data generated\nby human labelers who rank the model outputs manually.\nBased on these rankings, the reward model or a machine\nlabeler calculates a reward that is then utilized to update the\nFM through PPO.\n3.2 Prompt Engineering and User Interaction\nRegarding the challenges of embedding an LLM for NCL gen-\neration in an authoring tool. Our attention must also include\ninvestigating mechanisms within the authoring tool that will\nenable developers to interact with the model, ensuring the\nhighest level of response accuracy.\nThe process of designing prompts is challenging in itself,\nas the performance of the LLM is highly dependent on the\nquality of the prompt informed [31]. Since natural language\nis highly expressive and imprecise at times, it can lead to am-\nbiguous prompts, making it difficult for the model to capture\nuser intent [17]. Zhou et al. [31] claim that even though most\neffective prompts have been human-engineered, using LLMs\naccurately for more complex tasks is not straightforward\nsince it requires careful development.\nOne potential way to overcome this is by using formal\nspecifications when designing prompts, as suggested by [4].\nHowever, this approach is not practical if we intend to cater\nto a broader audience, especially people unfamiliar with\nprogramming languages.\nTherefore, many other works have explored ways of fa-\ncilitating human-LLM interaction by either automatizing\nprompts creation [20, 31]; improving prompts by enforcing\nspecific reliability rules [27]; or providing a prompt editing\ninterface with standard IDE features, such as syntax high-\nlighting and refactoring [ 13]; or even visual interactions\ncombined with pre-defined workflow operations [7].\nThus, finding the most effective way to allow the user\nto interact with the LLM without compromising the perfor-\nmance of NCL code generation constitutes another great\nchallenge. The central goal of this challenge revolves around\nwhether to adopt natural language prompts, employ lower-\nlevel prompts through the decomposition of tasks into more\nstraightforward, fundamental commands, or leverage visual\nabstractions to simplify and automate the prompt construc-\ntion process. We will need to experiment with adapting the\nmentioned approaches to this specific task and design a dif-\nferent one that suits the requirements well.\nBesides, as we are talking about a multimedia author-\ning tool, we also need to understand how this new para-\ndigm can be combined with the existing and well-established\nparadigms and how we can make the best use of well-known\nabstractions used by previous multimedia authoring tools,\nsuch as Composer [18], or STEVE [12], allowing the visual-\nization and editing of the generated applications.\nFurthermore, aside from our efforts to enhance the LLM’s\nperformance through user interactions, we must also priori-\ntize the practical usability of the authoring tool. To achieve\nthis, it becomes necessary to conduct evaluations involving\nthe intended audience. These evaluations will provide valu-\nable insights to determine the most appropriate approach\nfor refining the tool’s usability.\n4 Risks and Limitations\nBesides the challenges in prompt design/engineering, the\nsole use of LLMs for code generation already brings potential\nrisks. In a hazard assessment article for Codex [17], an LLM\nfor code synthesis, the authors noted that such a tool has the\npotential for misuse and may offer technological, social, and\neconomic risks. The potential risks/hazards include:\n• Inefficient Code : Generated code may not be opti-\nmized for performance or resource usage, leading to\nsuboptimal results in terms of speed and efficiency;\n• Lack of Creativity : Models can only generate code\nbased on the examples they were trained on. They\nmay struggle with tasks or languages that were not\nwell-represented in their training data, so they may\nnot be able to come up with innovative solutions or\ncreative problem-solving;\n• Debugging Challenges : Code generated by LLMs\ncan contain bugs or vulnerabilities. Identifying and\nfixing these issues can be challenging, especially if the\ncodebase is large;\n• Misleading Solutions: LLMs can produce code that\nappears correct but does not perform the intended task\naccurately. This can lead to unreliable software that\ndoes not meet the functional requirements, risking\nuser satisfaction and trust;\n• Lack of Context : LLMs may not have access to the\nfull context of a project or its requirements, leading\nto code that does not fully align with the intended\nfunctionality;\n• Maintenance Issues: Code generated by LLMs may\nbe hard to maintain and update over time, especially\nif the original developers are not familiar with the\nmodel-generated code;\n• Vulnerable Code: Automated code generation may\nresult in vulnerabilities and security flaws if not ade-\nquately reviewed. Code generated by these models\nmight contain vulnerabilities that compromise the\nsafety and security of the application, leaving it sus-\nceptible to attacks;\n• Legal and Licensing Issues : There could be legal\nand licensing challenges when using code generated\nby LLMs, mainly if the code includes proprietary or\ncopyrighted content;\n154\nOn the Challenges of Using Large Language Models for NCL Code Generation Anais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil\n• Ethical Concerns: The use of LLMs for code genera-\ntion may raise ethical concerns related to plagiarism,\ncopyright infringement, or the unintended use of third-\nparty code without proper attribution;\n• Overreliance on Models : Developers may become\noverly reliant on language models, potentially neglect-\ning their programming skills and critical thinking abil-\nities. Relying on language models for code generation\ndepends on their availability and maintenance. If the\nmodel becomes obsolete or is discontinued, it can dis-\nrupt ongoing projects;\n• Quality Control: Ensuring the quality of code gen-\nerated by LLMs requires careful review and testing,\nwhich can be time-consuming and resource-intensive;\n• Training Data Bias : The biases present in the train-\ning data used for LLMs can lead to code that reflects\nthese biases, potentially impacting the inclusivity and\nfairness of software projects.\n• Environmental Impact: Training and running LLMs\nrequire significant computational resources, leading\nto high energy consumption. This energy-intensive\nprocess contributes to environmental harm, including\nincreased carbon emissions, which is a concern given\nthe need for sustainability in technology.\n• Digital Divide: Using code generation tools necessi-\ntates access to appropriate hardware, stable internet\nconnections, and technical expertise. This can exclude\neconomically vulnerable groups who may lack access\nto these resources, exacerbating disparities in the tech\nindustry and limiting opportunities for underprivi-\nleged individuals.\n5 Conclusions\nThis paper proposes utilizing LLMs to generate NCL codes.\nInitially, we empirically find indications that pre-trained\nLLMs exhibit suboptimal performance when tasked with\ngenerating NCL code using natural language prompts. Sub-\nsequently, we outline a non-exhaustive list of challenges that\nmust be addressed to effectively adapt an LLM to the specific\ntask of NCL code generation.\nFurthermore, we highlight the potential challenges of\nachieving user-centric usability while maintaining robust\nmodel performance for task resolution. Lastly, we enumerate\nthe inherent risks and limitations of using LLMs for code\ngeneration. Through this article, we aspire to lay the ground-\nwork for developing an authoring tool capable of seamlessly\nmerging well-established functionalities from previous au-\nthoring tools with the untapped potential offered by LLMs\nin NCL application development.\nIn future work, we plan to perform a robust evaluation us-\ning current LLMs to properly attest to the efficiency of these\nmodels in automatically generating NCL code. Furthermore,\nwe will also test the adaptation of LLMs using a large dataset\nof NCL codes.\nAcknowledgments\nThe authors would like to thank CAPES and CNPq for partial\nfinancial support for this work.\nReferences\n[1] Toufique Ahmed and Premkumar Devanbu. 2022. Few-shot train-\ning LLMs for project-specific code-summarization. In Proceedings of\nthe 37th IEEE/ACM International Conference on Automated Software\nEngineering. 1–5.\n[2] Rohan Anil and Andrew M. Dai. 2023. PaLM 2 Technical Report. (2023).\narXiv:2305.10403 [cs.CL]\n[3] Roberto Gerson Albuquerque Azevedo, Carlos de Salles Soares Neto,\nMario Meireles Teixeira, Rodrigo Costa Mesquita Santos, and Thi-\nago Alencar Gomes. 2011. Textual authoring of interactive digital TV\napplications. In Proceedings of the 9th European Conference on Interac-\ntive TV and Video. 235–244.\n[4] Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin\nRaffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari,\nThibault Fevry, et al. 2022. Promptsource: An integrated development\nenvironment and repository for natural language prompts. arXiv\npreprint arXiv:2202.01279(2022).\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al . 2020. Language models are few-shot\nlearners. Advances in neural information processing systems33 (2020),\n1877–1901.\n[6] Antonio José G Busson, André Luiz de B Damasceno, Thacyla de S\nLima, and Carlos de Salles Soares Neto. 2016. Scenesync: A hypermedia\nauthoring language for temporal synchronism of learning objects. In\nProceedings of the 22nd Brazilian Symposium on Multimedia and the\nWeb. 175–182.\n[7] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang,\nTao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, et al . 2023.\nLow-code LLM: Visual Programming over LLMs. arXiv preprint\narXiv:2304.08103 (2023).\n[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, et al. 2021. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374(2021).\n[9] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.\n2023. Teaching large language models to self-debug. arXiv preprint\narXiv:2304.05128 (2023).\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling lan-\nguage modeling with pathways.arXiv preprint arXiv:2204.02311(2022).\n[11] André Luiz de B Damasceno, Thacyla de Sousa Lima, Carlos de\nSalles Soares Neto, et al . 2014. Cacuriá: Uma Ferramenta de Auto-\nria Multimídia para Objetos de Aprendizagem. In Anais dos Workshops\ndo Congresso Brasileiro de Informática na Educação, Vol. 3. 76.\n[12] Douglas Paulo de Mattos and Débora C Muchaluat-Saade. 2018. Steve:\nA hypermedia authoring tool based on the simple interactive mul-\ntimedia model. In Proceedings of the ACM Symposium on Document\nEngineering 2018. 1–10.\n[13] Alexander J Fiannaca, Chinmay Kulkarni, Carrie J Cai, and Michael\nTerry. 2023. Programming without a Programming Language: Chal-\nlenges and Opportunities for Designing Developer Tools for Prompt\nProgramming. In Extended Abstracts of the 2023 CHI Conference on\nHuman Factors in Computing Systems. 1–7.\n155\nAnais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil Moraes et al.\n[14] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes,\nAllie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann,\nGustavo de Rosa, Olli Saarikivi, et al . 2023. Textbooks Are All You\nNeed. arXiv preprint arXiv:2306.11644(2023).\n[15] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,\nQuentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and\nSylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In\nInternational Conference on Machine Learning. PMLR, 2790–2799.\n[16] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley,\nRoberta Raileanu, and Robert McHardy. 2023. Challenges and ap-\nplications of large language models. arXiv preprint arXiv:2307.10169\n(2023).\n[17] Heidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and\nMiles Brundage. 2022. A hazard analysis framework for code synthesis\nlarge language models. arXiv preprint arXiv:2207.14157(2022).\n[18] Rodrigo Laiola Guimarães, Romualdo Monteiro de Resende Costa,\nand Luiz Fernando Gomes Soares. 2008. Composer: Authoring tool\nfor iTV programs. In Changing Television Environments: 6th European\nConference, EUROITV 2008, Salzburg, Austria, July 3-4, 2008 Proceedings\n6. Springer, 61–71.\n[19] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrit-\ntwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno,\nAgustin Dal Lago, et al . 2022. Competition-level code generation\nwith alphacode. Science 378, 6624 (2022), 1092–1097.\n[20] Vadim Liventsev, Anastasiia Grishina, Aki Härmä, and Leon Moonen.\n2023. Fully Autonomous Programming with Large Language Models.\narXiv preprint arXiv:2304.10423(2023).\n[21] Carlos de Salles Soares Neto, Thacyla de Sousa Lima, André Luiz de B\nDamasceno, and Antonio José G Busson. 2017. Creating Multimedia\nLearning Objects. In Proceedings of the 23rd Brazillian Symposium on\nMultimedia and the Web. 19–21.\n[22] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo\nZhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open\nlarge language model for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474(2022).\n[23] Dina Nogueira, Lois Nascimento, Michael Mello, and Rodrigo Braga.\n2020. NuGinga Playcode: A web NCL/NCLua authoring tool for Ginga-\nNCL digital TV applications. In Anais Estendidos do XXVI Simpósio\nBrasileiro de Sistemas Multimídia e Web. SBC, 75–78.\n[24] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,\nAlex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan\nLeike, and Ryan Lowe. 2022. Training language models to follow\ninstructions with human feedback. (2022). arXiv:2203.02155 [cs.CL]\n[25] Douglas Paulo de Mattos, Júlia Varanda da Silva, and Débora Christina\nMuchaluat-Saade. 2013. NEXT: graphical editor for authoring NCL\ndocuments supporting composite templates. In Proceedings of the 11th\neuropean conference on Interactive TV and video. 89–98.\n[26] Hedvan Fernandes Pinto, Antonio José Grandson Busson, Carlos\nde Salles Soares Neto, and Samyr Beliche Vale. 2016. Creating Non-\nLinear Interactive Narratives with Fábulas Model. In Proceedings of\nthe 22nd Brazilian Symposium on Multimedia and the Web. 207–210.\n[27] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng\nWang, Jordan Boyd-Graber, and Lijuan Wang. 2022. Prompting gpt-3\nto be reliable. arXiv preprint arXiv:2210.09150(2022).\n[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971(2023).\n[29] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020.\nGeneralizing from a few examples: A survey on few-shot learning.\nACM computing surveys (csur)53, 3 (2020), 1–34.\n[30] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,\nYupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican\nDong, et al. 2023. A survey of large language models. arXiv preprint\narXiv:2303.18223 (2023).\n[31] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,\nSilviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models\nare human-level prompt engineers. arXiv preprint arXiv:2211.01910\n(2022).\n156",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7989352941513062
    },
    {
      "name": "Context (archaeology)",
      "score": 0.65911465883255
    },
    {
      "name": "Code (set theory)",
      "score": 0.5858640074729919
    },
    {
      "name": "Multimedia",
      "score": 0.5695624947547913
    },
    {
      "name": "Code generation",
      "score": 0.5427262187004089
    },
    {
      "name": "Usability",
      "score": 0.5115981698036194
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5077491998672485
    },
    {
      "name": "Domain-specific language",
      "score": 0.4633944630622864
    },
    {
      "name": "World Wide Web",
      "score": 0.4345269203186035
    },
    {
      "name": "Programming language",
      "score": 0.42128559947013855
    },
    {
      "name": "Human–computer interaction",
      "score": 0.2618308663368225
    },
    {
      "name": "Computer security",
      "score": 0.10578581690788269
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.10212433338165283
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2699952",
      "name": "Pontifícia Universidade Católica do Rio de Janeiro",
      "country": "BR"
    }
  ],
  "cited_by": 2
}