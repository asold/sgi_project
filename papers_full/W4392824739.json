{
  "title": "CMLFormer: CNN and Multiscale Local-Context Transformer Network for Remote Sensing Images Semantic Segmentation",
  "url": "https://openalex.org/W4392824739",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2095750924",
      "name": "Honglin Wu",
      "affiliations": [
        "Changsha University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Changsha University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100329869",
      "name": "Peng Huang",
      "affiliations": [
        "Changsha University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2124641763",
      "name": "Wenlong Tang",
      "affiliations": [
        "Changsha University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995250549",
    "https://openalex.org/W4313524854",
    "https://openalex.org/W2533591126",
    "https://openalex.org/W2921317791",
    "https://openalex.org/W2774243562",
    "https://openalex.org/W4384521492",
    "https://openalex.org/W4382371057",
    "https://openalex.org/W4388739256",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W4387250698",
    "https://openalex.org/W2970787924",
    "https://openalex.org/W3194371117",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4382999129",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W4205138939",
    "https://openalex.org/W4205365435",
    "https://openalex.org/W3148401590",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W2981609437",
    "https://openalex.org/W3088226460",
    "https://openalex.org/W4285301526",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4381620643",
    "https://openalex.org/W3202923600",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3190334976",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3109998321",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4283450732",
    "https://openalex.org/W4297426812",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3183174367"
  ],
  "abstract": "The characteristics of remote sensing images, such as complex ground objects, rich feature details, large intraclass variance and small interclass variance, usually require deep learning semantic segmentation methods to have strong feature learning representation ability. Due to the limitation of convolutional operation, convolutional neural networks (CNNs) are good at capturing local details, but perform poorly at modeling long-range dependencies. Transformers rely on multihead self-attention mechanisms to extract global contextual information, but it usually leads to high complexity. Therefore, this article proposes CNN and multiscale local-context transformer network (CMLFormer), a novel encoder-decoder structured network for remote sensing image semantic segmentation. Specifically, for the features extracted by the lightweight ResNet18 encoder, we design a transformer decoder based on multiscale local-context transform block (MLTB) to enhance the ability of feature learning. By using a self-attention mechanism with nonoverlapping windows and with the help of multiscale horizontal and vertical interactive stripe convolution, MLTB is able to capture both local feature information and global feature information at different scales with low complexity. In addition, the feature enhanced module is introduced into the decoder to further facilitate the learning of global and local information. Experimental results show that our proposed CMLFormer exhibits excellent performance on the Vaihingen and Potsdam datasets.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 1\nCMLFormer: CNN and Multi-scale Local-context\nTransformer network for remote sensing images\nsemantic segmentation\nHonglin Wu, Min Zhang, Peng Huang, Wenlong Tang\nAbstract—The characteristics of remote sensing images, such\nas complex ground objects, rich feature details, large intra-class\nvariance and small inter-class variance, usually require deep\nlearning semantic segmentation methods to have strong feature\nlearning representation ability. Due to the limitation of convo-\nlutional operation, Convolutional Neural Networks (CNNs) are\ngood at capturing local details, but perform poorly at modelling\nlong-range dependencies. Transformers rely on multi-head self-\nattention mechanisms to extract global contextual information,\nbut it usually leads to high complexity. Therefore, this paper\nproposes CNN and Multi-scale Local-context Transformer net-\nwork (CMLFormer), a novel encoder-decoder structured network\nfor remote sensing image semantic segmentation. Specifically, for\nthe features extracted by the lightweight ResNet18 encoder, we\ndesign a transformer decoder based on Multi-scale Local-context\nTransform Block (MLTB) to enhance the ability of feature learn-\ning. By using a self-attention mechanism with non-overlapping\nwindows and with the help of multi-scale horizontal and vertical\ninteractive stripe convolution, MLTB is able to capture both local\nfeature information and global feature information at different\nscales with low complexity. Additionally, the Feature Enhanced\nModule (FEM) is introduced into the encoder to further facilitate\nthe learning of global and local information. Experimental\nresults show that our proposed CMLFormer exhibits excellent\nperformance on the Vaihingen and Potsdam datasets. The code\nis available at https://github.com/DrWuHonglin/CMLFormer.\nIndex Terms—CNN, Transformer, Multi-scale, remote sensing\nimages, semantic segmentation\nI. INTRODUCTION\nW\nITH the rapid development of sensor technology and\nremote sensing platforms, high-resolution remote sens-\ning images have been widely used in the fields of land cover\n[1], [2], city planning [3], change detection [4], [5], and scene\nclassification [6]–[8]. However, high-resolution remote sensing\nimages generally suffer from large intra-class variance and\nsmall inter-class variance, which makes remote sensing image\nsegmentation and detection tasks extremely challenging.\nIn recent years, with the development of deep learning,\nresearchers began to apply Convolutional Neural Networks\n(CNN) to semantic segmentation. In this context, Long et\nal. [9] proposed the Fully Convolutional Network (FCN),\nwhich replaces the traditional fully connected layers with\nconvolutional layers. This innovative work has had a far-\nreaching impact on the field of semantic segmentation of\nH. Wu, M. Zhang, P. Huang, W. Tang are with the School of Computer\nand Communication Engineering, Changsha University of Science and Tech-\nnology, Changsha, China, 410114.\nE-mail: honglinwu@csust.edu.cn, 21208051645@stu.csust.edu.cn\nManuscript received XXXXX XX, 20XX.\nremote sensing images. Subsequently, Ronneberger et al.[10]\nproposed a network model with an encoder-decoder structure,\nwidely known as UNet. The structure introduces a skip con-\nnection, which further improves the segmentation accuracy.\nThe encoder-decoder architecture shows great potential in\nimage semantic segmentation tasks and is gradually becoming\nthe dominant architecture of the field. Due to the influence of\ncomplex targets and rich features in remote sensing images, the\npixel-level fine-grained classification tasks in remote sensing\nsemantic segmentation usually require more comprehensive\nsemantic information. Researchers have noticed that multi-\nscale and attention mechanisms play a very important role\nin enhancing semantic representation and improving seman-\ntic segmentation performance [11]–[14]. Chen et al. [11]\nproposed DeepLabv3+ to enhance multi-scale feature repre-\nsentation by using Atrous Spatial Pyramid Pooling (ASPP)\nmodule with atrous convolution. In addition, Cheng et al.\n[13] proposed Context Aggregation Network (CAN), which\ncombines the attention mechanism with multi-scale features to\nachieve higher positioning accuracy. Zhao et al.[14] proposed\nan end-to-end attention-based semantic segmentation network\n(SSAtNet), which introduces an attention mechanism into a\nmulti-scale module to refine the features, improving the accu-\nracy of semantic segmentation. The introduction of multi-scale\nand attention mechanisms greatly enhances feature learning\nand enables the model to comprehensively deal with scale\nchanges and complex objects of the image, thereby improving\nthe accuracy of semantic segmentation.\nDespite the outstanding performance of CNNs in image\nprocessing, their limited receptive fields restrict the capability\nto model long-range contextual dependencies. Long-range\ncontextual dependencies are particularly crucial for dense\nclassification tasks such as semantic segmentation. Recog-\nnizing the importance of context, researchers have begun to\ninvestigate the Transformer of computer vision [15]–[17]. In\n2020, Dosovitskiy et al. [15] proposed Vision Transformer\n(ViT), which shows potential application prospects in image\nclassification tasks. Zheng et al. [16] proposed Segmentation\nTransformer (SETR), which uses transformers as the backbone\npushing the further development of transformers in the field of\nsegmentation. Transformers rely on multi-head self-attention\nmechanisms to model long-range dependencies. However, its\ncomputational complexity grows quadratically with the input\ndata size, which usually imposes heavy computational over-\nhead and limits its application. To address this issue, Liu et\nal. [18] proposed Swin Transformer based on shifted windows,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 2\nwhich restricts attention computations to local windows and\neffectively reduces the computational complexity. Wang et\nal. [19] proposed Pyramid Vision Transformer (PVT), which\nreduces the required key-value pairs in the traditional multi-\nhead self-attention mechanism and adopts pyramid structure\nto extract image features from multiple scales, thereby reduc-\ning the computational complexity associated with traditional\ntransformer models.\nIn semantic segmentation tasks, both global and local infor-\nmation are crucial for accurately understanding the semantic\nstructure of images. Researchers have begun to integrate CNN\nand transformer to fully leverage their respective advantages.\nChen et al.[20] proposed TransUNet, a UNet-like architecture\ncombining CNN and transformer, which showed excellent\nperformance in medical image classification. He et al. [21]\nproposed ST-UNet, which uses a parallel dual encoder struc-\nture of CNN and Swin Transformer to extract local and\nglobal features, and integrates them through the Relational\nAggregation Module (RAM). Zhang et al. [22] proposed a\nsegmentation network that combined the shift window-based\nSwin Transformer with a CNN-based decoder to establish a\ndeep learning framework for semantic segmentation of remote\nsensing images.\nInspired by the aforementioned literature, this paper pro-\nposes a new semantic segmentation architecture for remote\nsensing images called CMLFormer. CMLFormer adopts a\nhybrid architecture that combines CNN and transformer com-\nponents. Specifically, we employ the lightweight ResNet-18 as\nthe encoder and propose a Multi-Scale Local-context Trans-\nformer Block (MLTB). In contrast to traditional transform-\ners, MLTB integrates attention mechanisms with multi-scale\nstrategies to enhance feature learning. To be precise, the self-\nattention mechanism is constrained within non-overlapping\nwindows to efficiently capture local contextual information\nwith low complexity. In order to overcome the limitation of\nwindow for long-range modelling, we use different scales of\ndepth-wise separable large convolutions instead of the feed\nforward network of the traditional transformer to obtain multi-\nscale contextual information. Due to the high computational\ncomplexity of the large convolutions, we split the large convo-\nlutions into two depth-wise separable strip convolutions, taking\ninto account that strip convolution has certain advantages for\nthe recognition of strip targets such as cars or rivers in remote\nsensing images. Furthermore, a Feature Enhancement Module\n(FEM) is proposed to efficiently integrate features from global\nand local information to achieve more comprehensive infor-\nmation fusion in the channel and spatial dimensions. The main\ncontributions of this paper are as follows:\n1) This paper proposes a CMLFormer network architecture\nthat utilizes a lightweight CNN and a Multi-scale Local-\ncontext Transformer in the encoder-decoder structure,\neffectively exploiting both global and local information\nin remote sensing image segmentation with less compu-\ntational overhead.\n2) An efficient and flexible MLTB is designed to cap-\nture the global context in remote sensing images with\nlow computational complexity by combining the non-\noverlapping block self-attention and multi-scale strate-\ngies to establish long-range dependencies between pix-\nels.\n3) We introduce a FEM to mitigate the omission of local\ncontext details during long-range dependencies mod-\nelling in remote sensing image segmentation.\nThe main research aspects of this paper are as follows.\nSection II analyzes the work related to this paper. In Section\nIII, we describe the research methodology of this paper. Sec-\ntion IV shows the robustness of the proposed model through\nexperimental comparisons and ablation experiments. Section\nV provides the conclusion.\nII. RELATED WORK\nA. Semantic Segmentation Methods Based On CNN\nSemantic segmentation of remote sensing images aims\nto categorize individual pixels into their respective seman-\ntic classes, thus facilitating the automatic identification and\nsegmentation of different objects, features or regions. In\ncontrast to conventional pixel-based classification methods,\nCNN-based approaches are able to capture the spatial details\nand contextual relations within the images, thus increasing\nthe accuracy and robustness of the semantic segmentation of\nremotely sensed imagery.\nFCN [9] represents a significant milestone in the realm\nof semantic segmentation and lays the foundation for further\nmodel development. For example, UNet [10] introduced skip\nconnections to recover detailed information at different scales.\nVarious UNet-based variants followed. Liuet al.[23] proposed\nDenseUnet to combine dense connections into the encoder-\ndecoder structure of UNet, which enables the model to better\ncapture features. Gong et al. [24] proposed ResUnet to add\nresidual connections into the encoder-decoder structure of\nUNet, which solves the problems of model training difficulty\nand gradient vanishing in order to better capture detailed\ninformation. To address the challenges posed by objects and\nscenes at different scales, ACNet [25] introduced an adaptive\ncontext module that enhances the segmentation of multi-\nscale objects in remote sensing images by capturing context\ninformation at different scales at different levels. To over-\ncome the limitation of receptive fields in CNNs, researchers\nemployed attention mechanisms in the structure to model\nlong-range dependencies. Fan et al. [26] proposed MA-Net,\nwhich adaptively combines local and global dependencies via\nPositional Attention Block (PAB) and Multi-scale Fusion At-\ntention Block (MFAB) to capture rich contextual and channel\ndependencies. Sun et al. [27] proposed a SPANet, which\nuses dual-branching to extract global and local information\nand fuses multi-scale features through a Successive Pooling\nAttention Module (SPAM) to effectively alleviate the blurring\nof object boundary segmentation in remote sensing images.\nHuang et al.[28] proposed a CCNet based on a cross-attention\nmechanism to capture long-range correlations between pixels\nto more accurately identify the semantic information in remote\nsensing images.\nB. Semantic Segmentation Methods Based On Transformer\nTransformer is initially used in the field of Natural Language\nProcessing (NLP) and has achieved remarkable success in\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 3\ndw,7× 1\ndw,1× 7\ndw,11× 1\ndw,1× 11\ndw,21× 1\ndw,1× 21\nMSC\nC\n(a)  Overall network structure of the CMLFormer\nInput\nOutput\n(b) MLTB\nUp × 2 Up × 4\nConv+Pool\nResBlock1  × 2 ResBlock2  × 2 ResBlock3  × 2 ResBlock4  × 2\n64, H/4,W/4 128,H/8,W/8 256,H/16,W/16 512,H/32,W/32\nQuery Key Value\n \nWindow Partition\nSUM\nLMSA\ndw,3× 3\nRELU\nConv1× 1\nSoftmax\nrearrange\n&reshape\nreshape\nConv1× 1\nConv3× 3\nConv3× 3\nConv3× 3\nConv3× 3\nConv3× 3\nConv3× 3\nConv3× 3\nConv3× 3\nFEM\nFEM\nFEM\nMLTB\nLMSA\nMSC\nBN\nMLTB\nBN\nLMSA\nMSC\nBN\nMLTB\nBN\nLMSA\nMSC\nBN\nBN\nLMSA\nMSC\nBN\nMLTB\nBN\nBN\nLocal-context \nMulti-head \nSelf-Attention\nBN\nMulti-scale \nStripe Conv\nFig. 1: The architecture of CMLFormer and MLTB. (a) The framework of CMLFormer proposed in this paper consists of a ResNet-18 encoder and a decoder consisting of MLTB\nand FEM. ResNet-18 consists of four stages Resblock. (b) MLTB consists of LMSA and MSC. The LMSA contains window operations and multiple self-attention mechanisms.\nThe MSC adopts strip depth convolution at three different scales\nNLP [29]. Subsequently, transformers have been applied to\nthe field of computer vision. ViT [15] is the first transformer-\nbased model for image recognition. It divides the input\nimage into fixed-sized blocks and then captures contextual\ninformation within the image using self-attention mechanism,\nultimately producing feature representations for tasks such\nas classification. ViT has achieved significant success in the\nfield of computer vision and has become one of the crucial\narchitectures in computer vision. With continuous efforts from\nresearchers, the application of transformers in image process-\ning has gradually matured. Inspired by ViT, the transformer-\nbased model for semantic segmentation called SETR [16]\nwas proposed. SETR utilizes transformers as the encoder\nin the semantic segmentation model and can be combined\nwith other decoders to achieve more complex segmentation\nmodels. However, the core of the transformer is the self-\nattention mechanism, which requires computing similarity\nbetween each position and all other positions, leading to a\nquadratic increase in computational complexity with image\nresolution. Consequently, transformer-based models struggle\nto handle high-resolution images.\nTo address the above issues, extensive research has been\nconducted. Wang et al. [19] proposed PVT, which combines\nthe traditional transformer model with a pyramid structure for\ndense prediction. Liu et al. [18] proposed Swin Transformer,\nwhich employs a hierarchical design with shifted windows\nand local self-attention confined within windows. Xie et al.\n[30] proposed a novel design of positionless coded hierarchical\ntransformer encoder and lightweight All-MLP decoder named\nSegFormer. Dong et al. [31] proposed a Cswin transformer,\nwhich parallelly computes self-attention within horizontally\nand vertically striped cross-shaped windows, improving the\nefficiency of capturing global receptive fields.\nC. The approach of combining CNN and Transformer.\nCNN and transformer possess different advantages in the\nfields of image processing and NLP. CNN excels at extracting\nlocal features from images and adapts to objects of different\nscales through its translation invariance and feature reuse\ncapabilities. On the other hand, transformer is particularly\ngood at modelling global dependencies to capture long-range\nsemantic dependencies in images when dealing with sequential\ndata. Although CNNs and transformers each have advantages\nin different areas, recent research has shown that combining\nCNNs and transformers in image segmentation can fully\nexploit their strengths and improve the performance of the\nmodel. Wang et al. [32] proposed Dual-Branch hybrid CNN-\nTransformer Network (DBCT-Net), which fully exploits the\nadvantages of CNN in local specific feature extraction, and\nachieves the modelling of global dependencies through Trans-\nformer. Gao et al. [33] designed a dual-encoder model that\nuses independent CNN and transformer branches to extract\nfeatures and adaptively fuse them to enhance the feature\npresentation of the model. Guo et al. [34] proposed CMT,\na novel hierarchical transformer architecture that combines\nconvolutional operations to capture local and global features,\nthereby improving performance and reducing computational\ncost. We propose a novel network architecture combining\nCNN and transformer, which uses a lightweight CNN and a\nMulti-scale Local-context Transformer in an encoder-decoder\nstructure. The CMLFormer facilitates the comprehensive use\nof the global and local information in the image and enables\nthe accurate segmentation of high-resolution remote sensing\nimages at low computational cost.\nIII. M ETHODOLOGY\nA. Overall Architecture\nThe CMLFormer is shown in Fig. 1 (a), which follows\nencoder-decoder architecture. The encoder uses a lightweight\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 4\nResNet-18 network architecture to extract image features with\nlow complexity. ResNet-18 consists of four stages ResBlock,\neach of which double downsamples the feature map. The\ndecoder consists of MLTB and FEM. The output of ResBlock4\nof the encoder is fed into the MLTB. ResBlock3, ResBlock2\nand ResBlock1 outputs are fed into the FEM, respectively. The\nMLTB is exploited to capture long-range dependencies with\nlow complexity by adopting Local-context Multi-head Self-\nAttention (LMSA) in non-overlapping windows and Multi-\nscale Stripe Convolution (MSC). The outputs of the MLTB and\nencoder stages are simultaneously fed into the FEM, which\nis utilized to efficiently fuse global and local information in\nchannel and spatial dimensions. Finally, the output features of\nthe MLTB from the three stages are subjected to a summing\noperation and concatenated with the final output of the decoder\nin order to make full use of the feature information at each\nlevel and improve the accuracy of the semantic segmentation.\nB. MLTB\nGlobal information is essential for accurate recognition of\ncomplicated categories in remote sensing images, such as\nbuildings of different sizes, long stretches of rivers and cars.\nAlthough transformers have great potential for integrating\nglobal information, traditional transformers may lead to degra-\ndation of model performance due to their high complexity.\nTherefore, we propose a low-complexity MLTB to efficiently\nextract global context information from image regions of\ndifferent scales, as shown in Fig. 1 (b). The MLTB consists of\ntwo key components, LMSA and MSC. The LMSA restricts\nattention mechanism to the local context through window\npartitioning and computes the attention matrix from the query,\nkey and value, thus effectively capturing the relevance of local\ndetails in an image. In order to make up for the potential in-\nformation loss, we introduce residual connection and combine\n3 × 3 depth-wise separable convolution to enhance the output\nfeatures. The MSC module effectively expands the multi-\nscale perceptual capability of the model through multi-scale\nhorizontal and vertical depth-wise separable stripe convolution\noperations to make up for the potential global information loss\nin MLTB. This strategy plays a key role in improving the\nsensitivity of the model to multi-scale features and helps to\nimprove the performance of semantic segmentation.\nSpecifically, in MLTB, the channel dimension of the input\nfeature image XϵR(B×C×H×W) is first expanded tripled in\nsize using 1 × 1 convolution, which maps the C dimen-\nsion features into 3 × C dimensions. Then, we separate the\n1D sequence ϵR(3×B× H\nws × W\nws ×heads)×(ws×ws)× C\nheads into the\nQuery (Q), Key (K) and Value (V) vectors using window parti-\ntion operations, where heads denote the number of attentional\nheads. We set both the number of attention heads and the\nwindow size to 8. The number of channels per attention head\nd = 64. The attention weights are computed based on Q,\nK and V . Specifically, Q and K are dot-produced to obtain\nthe raw attention scores, which are then normalized using the\nsoftmax function. Finally, the attention weights are applied\nto the V to obtain the attention output for each position,\nas shown in (1). In order to restore the features to their\noriginal size for effectively extracting multi-scale information,\nwe perform a rearrangement operation on the window-level\nfeature representation. The feature representation is further\nenhanced using 3 × 3 convolution before MLTB output. In\nMSC, multi-scale information is obtained using stripe convo-\nlution with three different scale sizes (7, 11, 21) and a sum\noperation is performed to capture the detailed information\nat different scales. Finally, the multi-scale information is\nnonlinearly augmented and dimensionally converted by RELU\nactivation function and 1 × 1 convolution. The flow of MLTB\nis as (2), (3):\nAttention = soft max\n\u0012QKT\n√dk\n\u0013\nV, (1)\nXt\n1 = LMSA\n\u0000\nBN\n\u0000\nXt−1\u0001\u0001\n+ Xt−1, (2)\nXt = MSC\n\u0000\nBN\n\u0000\nXt\n1\n\u0001\u0001\n+ Xt\n1, (3)\nwhere dk is the channel dimension of k, Xt−1 represents the\ninput features of LMSA, and Xt\n1 represents the output features\nof LMSA, and Xt represents the output features of MSC.\nC. FEM\nCC\n3× 3\nConv\nXi\nXj Up × 2\n1× 1\nConv\n1× 1\nConv\n3× 3\nConv\nXj\nXi\nUp × 2\nMaxPool\nSigmoid\nFig. 2: The architecture of FEM. FEM contains two branches, the top branch obtains the\nspatial features by sum operation, and the bottom branch obtains the channel features by\nconcatenation operation.\nLocal information and global information complement each\nother in semantic segmentation tasks. Local information helps\nthe model capture local features such as details, boundaries,\nand textures. The global information provides contextual se-\nmantic information and guides the model to consider overall\nconsistency during the segmentation process. The effective use\nof local and global information can improve the segmentation\nperformance of the model. However, most methods simply\nconcatenate local information and global information, which\ncan lead to information imbalance and impact the performance\nof model. To effectively integrate local information and global\ninformation, we construct FEM, as shown in Fig. 2. The FEM\nconsists of two branches: the first branch obtains the spatial\nfeatures by sum operation, and the second branch obtains\nthe channel features by concatenation operation. Then, the\nchannel features and spatial features are summed to effectively\nfuse global information and local information. The fused\ninformation considers the details information of the local area\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 5\nTABLE I: QUANTITATIVE COMPARISON RESULTS ON THE V AIHINGEN DATASET. BOLD INDICATES THE BEST RESULTS. THE CATEGORY METRICS ARE F1\nSCORES\nMethod F1%\nImp.surf. Building Low.veg. Tree Car mIoU% mF1% Param./M FLOPS/G\nFCN [9] 85.86 89.34 75.62 83.72 75.33 69.84 81.79 19.17 25.54\nDeeplabV3+ [11] 87.74 91.55 76.80 85.01 73.99 71.52 83.02 59.34 65.27\nDANet [35] 85.92 90.79 75.50 83.57 60.48 66.85 79.25 47.44 13.85\nBANet [36] 86.99 91.01 76.05 84.09 72.05 70.14 82.04 3.25 12.73\nPSPNet [37] 84.33 88.03 74.50 83.33 58.38 64.71 77.71 52.52 50.51\nMANet [38] 86.19 89.67 75.68 84.42 76.44 70.56 82.48 35.86 77.75\nSwinUnet [39] 86.27 89.01 76.22 84.30 71.26 69.17 81.41 41.34 68.34\nTransUnet [20] 87.75 91.74 77.21 84.56 76.86 72.29 83.62 100.44 35.84\nUnetFormer [40] 87.54 91.34 76.43 84.77 77.42 72.10 83.50 11.48 16.99\nCMLFormer(ours) 88.01 91.90 77.26 84.87 78.82 73.07 84.18 11.81 21.25\nTABLE II: QUANTITATIVE COMPARISON RESULTS ON THE POTSDAM DATASET. BOLD INDICATES THE BEST RESULTS. THE CATEGORY METRICS ARE F1\nSCORES\nMethod F1%\nImp.surf. Building Low.veg. Tree Car mIoU% mF1% Param./M FLOPS/G\nFCN [9] 89.76 93.64 82.95 83.12 89.82 78.60 87.86 19.17 25.54\nDeeplabV3+ [11] 90.40 95.01 83.82 84.22 87.32 79.07 88.15 59.34 65.27\nDANet [35] 89.35 94.71 82.44 84.18 82.71 76.81 86.68 47.44 13.85\nBANet [36] 90.36 94.00 83.39 83.04 89.49 78.92 88.06 3.25 12.73\nPSPNet [37] 89.45 93.38 82.13 80.00 89.36 77.12 86.86 52.52 50.51\nMANet [38] 90.10 94.38 83.11 83.42 89.85 79.12 88.17 35.86 77.75\nSwinUnet [39] 90.30 94.41 83.05 82.83 88.84 78.68 87.89 41.34 68.34\nTransUnet [20] 90.52 94.68 83.87 83.58 90.24 79.76 88.58 100.44 35.84\nUnetFormer [40] 90.14 94.44 83.15 83.16 90.07 79.16 88.19 11.48 16.99\nCMLFormer(ours) 90.79 94.96 84.01 83.91 90.31 80.06 88.76 11.81 21.25\nand the semantic information of the global scene to enhance\nthe understanding and inference ability of the model for the\ntarget. By integrating information from both channel and\nspatial dimensions, the model can better incorporate contextual\ninformation.\nSpecifically, in a branch, the local context information\nfrom the encoder stage is added in the spatial dimension\nto the global information from the MLTB stage. Then, the\nsigmoid activated features are added to the Max pooling\nfeatures at the pixel level. Immediately, a 3 ×3 convolution is\nused to enhance important contextual information and discard\nirrelevant features. In the other branch, the local contextual\ninformation from the encoder stage is concatenated with the\nglobal information from the MLTB stage in the channel dimen-\nsion, and a 3 × 3 convolution is used to enhance the feature\nrepresentation. Then, the spatially enhanced information and\nthe channel enhanced information are aggregated using an\naddition operation. The FEM process is as follows:\nX′ = Conv1×1 (Xi) +Up (Xj) , (4)\nXS = Conv3×3 (MaxP ool(X′) +Sigmoid (X′)) , (5)\nXC = Conv3×3 (Concat (Conv1×1 (Xi) , Up(Xj))) , (6)\nX = XS + XC, (7)\nwhere Xi is the feature output from the encoder, Xj is the\nfeature output from MLTB, XS denotes the output of the\nspatial branch and XC denotes the output of the channel\nbranch.\nIV. EXPERIMENTAL RESULTS\nA. Datasets\nIn this section, the superiority of CMLFormer is evaluated\non the Vaihingen and Potsdam datasets.\n1) Vaihingen: The Vaihingen dataset, named after the\nVaihingen region of Stuttgart, Germany, is a publicly available\ndataset of high-resolution aerial images. The dataset consists\nof high-resolution color aerial images captured by drones and\nground truth labels for each image. Ground truth labels are\nusually used to indicate different categories in an image, such\nas buildings, roads, trees, etc. The dataset contains a total of 33\northo-corrected images. In this paper, we selected 16 specified\nimages for training, while the remaining images were retained\nfor testing. Each image is cut into small blocks of 256 ×256\nto meet the experimental requirements.\n2) Potsdam: The Potsdam dataset is constructed from aerial\nimages located in Potsdam, Germany. It is a public resource\nwidely used in computer vision and remote sensing image pro-\ncessing research. The dataset includes high-resolution aerial\nimages covering different landscapes in urban areas, such as\nbuildings, roads, and lawns. Each image is accompanied by a\ndetailed ground truth label, covering different types of ground\nobject information. In total, the Potsdam dataset contains 38\nimages that have been orthorectified. In this paper, we selected\n16 specific images for training, while the remaining 17 images\nwere retained for testing. Each image is cropped to form\na small image block with a size of 256 ×256 to meet the\nexperimental needs.\nB. Experimental Setup\nEach experiment is performed on a single NVIDIA Tesla\nV100S GPU. We used the SGD optimizer for training. The\nweight-decay is set to 0.0001. The initial learning rate is set\nto 0.01, and the learning rate is updated using the ”poly”\nlearning strategy, which is a polynomially decaying learning\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 6\nimage GroundTruth FCN DeepLabv3+ DANet BANet\nPSPNet MANet SwinUnet TransUnet UnetFormer CMLFormer\nimage GroundTruth FCN DeepLabv3+ DANet BANet\nCMLFormerUnetFormerTransUnetSwinUnetMANetPSPNet\nImpervious Surface Building Low Vegetation Tree Car Clutter/background\nFig. 3: Visualization results of the Vaihingen dataset. The main differences are highlighted by the red boxes in the figure.\nImpervious Surface Building Low Vegetation Tree Car\nimage GroundTruth FCN DeepLabv3+ DANet BANet\nPSPNet MANet SwinUnet TransUnet UnetFormer CMLFormer\nimage GroundTruth FCN DeepLabv3+ DANet BANet\nCMLFormerUnetFormerTransUnetSwinUnetMANetPSPNet\nClutter/background\nFig. 4: Visualization results of the Potsdam dataset. The main differences are highlighted by the red boxes in the figure.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 7\nrate method commonly used to train deep learning models. Its\nupdate rule can be expressed as follows:\nlr = base lr ×\n\u0012\n1 − cur iter\nmax iter\n\u0013power\n, (8)\nwhere base lr denotes the initial learning rate, cur iter\ndenotes the current number of iterations, max iter denotes\nthe maximum number of iterations, and power is a power\nexponent controlling the decay of the polynomial. In this\npaper, power is set to 0.9. The training images are randomly\nflipped and cropped for data augmentation.\nAs shown in formula (9), the construction of the loss\nfunction follows the weighted combination of cross-entropy\nand dice loss [41]. We set α to 0.4 in this paper. To evaluate\nour results, we rely on two main evaluation metrics: mean\nintersection over union (mIoU) and mean F1 score (mF1).The\nspecific formula can be seen in (10), (11), (12), (13).\nLoss = α · CrossEntropyLoss + (1− α) · DiceLoss, (9)\nP recision= 1\nM\nMX\nj=1\nT Pj\nT Pj + F Pj\n, (10)\nRecall = 1\nM\nMX\nj=1\nT Pj\nT Pj + F Nj\n, (11)\nF1 = 2× P recision× Recall\nP recision+ Recall , (12)\nmIoU = 1\nM\nMX\nj=1\nT Pj\nT Pj + F Pj + F Nj\n, (13)\nwhere F Nj, F Pj and T Pj represent false negatives, false\npositives and true positives, respectively, for object indexed\nas class j.\nC. Semantic Segmentation Results and Analysis\n1) Results on the Vaihingen dataset: The effectiveness\nof the CMLFormer on the Vaihingen dataset is illustrated by\nthe quantitative metrics presented in Table I. In particular, the\nCMLFormer scores 84.17 % for mF1 and 73.06 % for mIoU.\nThe methods we compared include CNN-based approaches\nsuch as FCN, DeeplabV3+, DANet, PSPNet, and MANet.\nThe transformer-based method we compared is SwinUnet.\nAdditionally, to thoroughly validate the effectiveness of the\nCMLFormer, we also compared it against some hybrid CNN\nand transformer methods, namely UnetFormer, TransUnet, and\nBANet. From Table I, it can be observed the CMLFormer\noutperforms the majority of both CNN-based and transformer-\nbased frameworks. As CMLFormer is an architecture com-\nbining CNN and transformer, the comparison results with\nUnetFormer, TransUnet, and BANet are more meaningful.\nWe visualize the comparative results in CNN-based methods\nand show that the CMLFormer is effective in alleviating the\nproblem of high intra-class variance caused by occlusions\ncompared to other models. For example, in the red boxed areas\nof the first and second rows in Fig. 3, the occlusion of light\nand shadows creates a gap between the taller building and the\nneighboring lower building, leading to inaccurate recognition\nof the building by other models. In contrast, CMLFormer\nmakes an accurate recognition in these cases. In the red\nboxed areas of the third and fourth rows, other models exhibit\nless significant performance in obtaining global contextual\ninformation, while CMLFormer effectively mitigates this issue\nand performs exceptionally well in extracting large targets.\n2) Results on the Potsdam dataset: CMLFormer is com-\npared with the mainstream segmentation methods, as shown\nin Table II. CMLFormer achieves excellent performance with\nmIoU of 80.06 % and mF1 of 88.76 %. This is a significant\nimprovement over the other methods. Among the traditional\nCNN models, Deeplabv3 + performed well, but CMLFormer\nincreased mIoU and mF1 by 0.99 % and 0.61%, respectively.\nIt is worth noting that the model parameters of CMLFormer\nrepresent only 12 % of the TransUnet parameters, but mIoU\nand mF1 still exceed TransUnet by 0.3% and 0.18%, achieving\nthe most advanced performance.This result is clearly shown in\nthe visual effect in Fig. 4.\nOn the Potsdam dataset, the visual comparison results in\nFig. 4 show several aspects. In the first two rows, there is some\nsemantic overlap due to the similarity between low vegetation\nand trees, which poses a challenge for accurate classification\nby the models. When dealing with such similar and over-\nlapping categories, CMLFormer excels in the clarity of the\nsegmentation boundary. In addition, the size and shape of the\nbuildings in the image vary greatly, increasing the diversity\nwithin the building category. As shown in the third and fourth\nrows in the red boxed area, CMLFormer performs excellently\nin extracting buildings, successfully reducing misclassification,\nboundary blurring, and omission issues, demonstrating out-\nstanding performance.\nImpervious Surface Building Low Vegetation Tree\nImage Ground Truth Baseline Baseline+MLTB CMLFormer\nCar\nCMLFormer+SUM\nFig. 5: Ablation experiment on the vaihingen datasets. The main differences are\nhighlighted by the red boxes in the figure.\n3) Ablation Study: Comprehensive experiments are con-\nducted on the Vaihingen dataset to verify the effects of MLTB\nand FEM. Since the CMLFormer uses ResNet-18 as the\nbackbone network, we choose to connect different stages of\nResNet-18 as the baseline. The mIoU and mF1 scores are used\nto assess the performance of each module, and the parameters\nare used to reflect the degree of lightness. As shown in Table\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 8\nTABLE III: QUANTITATIVE COMPARISONS AMONG ABLATION STUDIES ON THE V AIHINGEN DATASET.\nMethod Module F1%\nSUM Imp.surf. Building Low.veg. Tree Car mIoU% mF1% Param./M\nResNet-18(baseline) % 85.92 90.30 74.83 83.60 65.20 67.52 79.97 11.42\nbaseline + MLTB % 87.52 91.75 76.15 84.22 78.01 71.56 83.53 11.65\nbaseline + MLTB + FEM % 87.63 91.48 76.54 84.60 78.41 72.42 83.73 11.77\nbaseline + MLTB + FEM ! 88.01 91.90 77.26 84.87 78.82 73.07 84.18 11.81\nIII, the results of ResNet-18 + MLTB + FEM verify the\neffectiveness of each module.\n• Effectiveness of MLTB: As shown in Table III, when\nMLTB was incorporated into the CMLFormer framework,\nthe mIoU and mF1 increased by 4.04 % and 3.56 %,\nrespectively. Notably, the segmentation accuracy of the\n”Car” class shows the most significant improvement,\nwith a remarkable mF1 increase of 12.8%. Among the\nother four ”Impervious Surface” classes, there are mF1\nimprovements of 1.6%. The ”Building” class also ex-\nhibits a notable mF1 improvement of 1.45%, while the\n”Low Vegetation” class demonstrated a considerable mF1\nimprovement of 1.32%. Additionally, the ”Tree” class\nshows a substantial mF1 increase of 6.21%. From the\nimages in the first row of Fig. 5, it can be seen that the\nmodel performed poorly in recognizing building edges\nbefore the introduction of MLTB. With the incorporation\nof MLTB, the model mitigates the phenomenon of blurry\nsegmentation edges.\n• Effectiveness of FEM: Table III reflects the joint effect\nof studying the two modules under the CMLFormer\nframework. After incorporating FEM, the mIoU and mF1\nincrease by 0.86 % and 0.2 %, respectively. In particular,\nthere was an improvement of 0.4 %, 0.11 %, 0.4 %, and\n0.38% in the mF1 of ”Car”, ”Impervious surface”, ”Low\nvegetation”, and ”Tree”. From the images in the second\nrow of Fig. 5, it can be seen that under the influence of\nsunlight, the shadows cast by the ”Building” completely\nobscured the cars, posing a significant challenge to model\nrecognition. Additionally, the proximity of cars result in\nintra-class shadow occlusion. Before introducing FEM,\nthe model struggles to recognize buildings and objects\nobscured by shadows. The model accurately segments the\nedges of small objects in dense areas and effectively miti-\ngates the negative impact of shadows on car segmentation\nafter introducing FEM.\n• Effectiveness of SUM: The effect of with-SUM-\nCMLFormer is seen in TableIII. After adding SUM,\nmIoU and mF1 are improved by 0.65 % and 0.45 %,\nrespectively. Specifically, the mF1 of ”Car”, ”Impervious\nSurface”, ”Building”, ”Low Vegetation”, and ”Tree” were\nimproved by 0.41 %, 0.38 %, 0.42 %, 0.72 %, and 0.27 %,\nrespectively. Fig. 5 shows that the model improves the\nacquisition of information about building edges. Further-\nmore, by including SUM, the model successfully reduces\nthe negative effect of shadows on car segmentation.\nV. CONCLUSION\nIn this paper, we propose an innovative decoder based\non transformer architecture and construct a CMLFormer for\nremote sensing images semantic segmentation. Considering\nthe importance of global and local information in remote\nsensing image segmentation, we design a Multi-scale Local-\ncontext Transform Block (MLTB) that integrates attention\nmechanisms with multi-scale strategies to effectively exploit\nglobal information with lower computational cost. And de-\nvelop a Feature Enhancement Module (FEM) to compensate\nfor the omission of local context details in the MLTB. Ex-\ntensive comparison and ablation experiments on the Vaihin-\ngen and Potsdam datasets demonstrate the effectiveness of\nCMLFormer. Although CMLFormer achieved excellent overall\nperformance on both datasets, on the Potsdom dataset the\nbuilding class and tree class are not optimal compared to the\nother compared methods. We are committed to analyzing the\ncauses of this problem in future research, including possible\ndata characteristics, lack of fit in the model structure, and\npossible overfitting or underfitting. In the meantime, we will\ncontinue to work on making CMLFormer more lightweight\nto improve its segmentation performance on large datasets.\nThis may involve further simplifying the model structure,\noptimizing parameter settings, and adopting advanced light-\nweighting techniques.\nVI. ACKNOWLEDGEMENT\nThis work was supported by Scientific Research Fund\nof Hunan Provincial Education Department (21B0329), and\nChangsha Municipal Natural Science Foundation (kq2208236)\nPostgraduate Scientific Research Innovation Project of Hunan\nProvince (CX20220939).\nREFERENCES\n[1] D. He, Y . Zhong, and L. Zhang, “Spectral–spatial–temporal map-\nbased sub-pixel mapping for land-cover change detection,” IEEE Trans.\nGeosci. Remote Sens., vol. 58, no. 3, pp. 1696–1717, 2020.\n[2] M. Zhang, W. Li, X. Zhao, H. Liu, R. Tao, and Q. Du, “Morphological\ntransformation and spatial-logical aggregation for tree species classifi-\ncation using hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens.,\nvol. 61, pp. 1–12, 2023.\n[3] X. Huang, Q. Li, H. Liu, and J. Li, “Assessing and improving the\naccuracy of globeland30 data for urban area delineation by combining\nmultisource remote sensing data,” IEEE Geosci. Remote Sens. Lett.,\nvol. 13, no. 12, pp. 1860–1864, 2016.\n[4] K. Lim, D. Jin, and C.-S. Kim, “Change detection in high resolution\nsatellite images using an ensemble of convolutional neural networks,”\nin Signal Inf. Process Assoc. Annu. Summit Conf. APSIPA Asia Pac.,\n2018, pp. 509–515.\n[5] Q. Guo, J. Zhang, T. Li, and X. Lu, “Change detection for high-\nresolution remote sensing imagery based on multi-scale segmentation\nand fusion,” in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2017, pp.\n1919–1922.\n[6] J. Wang, W. Li, M. Zhang, R. Tao, and J. Chanussot, “Remote-sensing\nscene classification via multistage self-guided separation network,” IEEE\nTrans. Geosci. Remote Sens., vol. 61, pp. 1–12, 2023.\n[7] J. Wang, W. Li, Y . Wang, R. Tao, and Q. Du, “Representation-enhanced\nstatus replay network for multisource remote-sensing image classifica-\ntion,” IEEE Trans. Neural Netw. Learn. Syst., pp. 1–13, 2023.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 9\n[8] J. Wang, W. Li, M. Zhang, and J. Chanussot, “Large kernel sparse\nconvnet weighted by multi-frequency attention for remote sensing scene\nunderstanding,” IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 1–12,\n2023.\n[9] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for\nsemantic segmentation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), 2015, pp. 3431–3440.\n[10] O. Ronneberger, P.Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), ser. LNCS, vol. 9351.\nSpringer, 2015, pp. 234–241.\n[11] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” in Comput. Vis. ECCV 2018 Workshops.\n[12] W. Song, Y . Dai, Z. Gao, L. Fang, and Y . Zhang, “Hashing-based deep\nmetric learning for the classification of hyperspectral and lidar data,”\nIEEE Trans. Geosci. Remote Sens., vol. 61, pp. 1–13, 2023.\n[13] W. Cheng, W. Yang, Y . Pan, H. Guo, and Y . Cheng, “Context aggregation\nnetwork for semantic labeling in aerial images,” inIEEE Int. Conf. Image\nProcess (ICIP), 2019, pp. 4484–4488.\n[14] Q. Zhao, J. Liu, Y . Li, and H. Zhang, “Semantic segmentation with\nattention mechanism for remote sensing images,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–13, 2022.\n[15] A. K. Alexey Dosovitskiy, Lucas Beyer et al., “An image is worth 16x16\nwords: Transformers for image recognition at scale,” in 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021.\n[16] S. Zheng, J. Lu, H. Zhao et al., “Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR) , 2021, pp.\n6877–6886.\n[17] Z. Gao, W. Sun, Y . Lu et al., “Joint learning of semantic segmentation\nand height estimation for remote sensing image leveraging contrastive\nlearning,” IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 1–15, 2023.\n[18] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9992–\n10 002, 2021.\n[19] W. Wang, E. Xie, X. Li et al., “Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions,” in Proc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2021, pp. 548–558.\n[20] J. Chen, Y . Lu, Q. Yu et al., “Transunet: Transformers make strong\nencoders for medical image segmentation,” CoRR, vol. abs/2102.04306,\n2021.\n[21] X. He, Y . Zhou, J. Zhao, D. Zhang, R. Yao, and Y . Xue, “Swin\ntransformer embedding unet for remote sensing image semantic seg-\nmentation,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–15, 2022.\n[22] C. Zhang, W. Jiang, Y . Zhang, W. Wang, Q. Zhao, and C. Wang, “Trans-\nformer and cnn hybrid deep neural network for semantic segmentation\nof very-high-resolution remote sensing imagery,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–20, 2022.\n[23] Y . Wu, J. Wu, S. Jin, L. Cao, and G. Jin, “Dense-u-net: Dense\nencoder–decoder network for holographic imaging of 3d particle fields,”\nOptics Communications, vol. 493, p. 126970, 2021.\n[24] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, “Resunet-a: A\ndeep learning framework for semantic segmentation of remotely sensed\ndata,” ISPRS J. Photogramm. Remote Sens., vol. 162, pp. 94–114, 2020.\n[25] X. Ding, Y . Guo, G. Ding, and J. Han, “Acnet: Strengthening the kernel\nskeletons for powerful cnn via asymmetric convolution blocks,” in Proc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2019, pp. 1911–1920.\n[26] T. Fan, G. Wang, Y . Li, and H. Wang, “Ma-net: A multi-scale attention\nnetwork for liver and tumor segmentation,” IEEE Access, vol. 8, pp.\n179 656–179 665, 2020.\n[27] L. Sun, S. Cheng, Y . Zheng, Z. Wu, and J. Zhang, “Spanet: Successive\npooling attention network for semantic segmentation of remote sensing\nimages,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15,\npp. 4045–4057, 2022.\n[28] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu, “Ccnet:\nCriss-cross attention for semantic segmentation,” in Proc. IEEE/CVF\nInt. Conf. Comput. Vis. (ICCV), 2019, pp. 603–612.\n[29] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,”\nin Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 30. Curran\nAssociates, Inc., 2017.\n[30] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efficient design for semantic segmentation with\ntransformers,” in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 34,\n2021, pp. 12 077–12 090.\n[31] X. Dong, J. Bao, D. Chen et al., “Cswin transformer: A general\nvision transformer backbone with cross-shaped windows,” CoRR, vol.\nabs/2107.00652, 2021.\n[32] Q. Wang, X. Jin, Q. Jiang, L. Wu, Y . Zhang, and W. Zhou, “Dbct-net:a\ndual branch hybrid cnn-transformer network for remote sensing image\nfusion,” Expert Systems with Applications, vol. 233, p. 120829, 2023.\n[33] L. Gao, H. Liu, M. Yang et al., “Stransfuse: Fusing swin transformer\nand convolutional neural network for remote sensing image semantic\nsegmentation,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 14, pp. 10 990–11 003, 2021.\n[34] J. Guo, K. Han, H. Wu et al., “CMT: convolutional neural networks\nmeet vision transformers,” CoRR, vol. abs/2107.06263, 2021.\n[35] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, “Dual attention\nnetwork for scene segmentation,” in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), 2019, pp. 3141–3149.\n[36] L. Wang, R. Li, D. Wang, C. Duan, T. Wang, and X. Meng, “Transformer\nmeets convolution: A bilateral awareness network for semantic segmen-\ntation of very fine resolution urban scene images,” Remote Sensing,\nvol. 13, 2021.\n[37] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), 2017, pp. 6230–6239.\n[38] R. Li, S. Zheng, C. Zhang et al., “Multiattention network for semantic\nsegmentation of fine-resolution remote sensing images,” IEEE Trans.\nGeosci. Remote Sens., vol. 60, pp. 1–13, 2022.\n[39] H. Cao, Y . Wang, J. Chen et al., “Swin-unet: Unet-like pure transformer\nfor medical image segmentation,” in Comput. Vis. ECCV 2022 Work-\nshops, 2023, pp. 205–218.\n[40] L. Wang, R. Li, C. Zhang et al., “Unetformer: A unet-like transformer for\nefficient semantic segmentation of remote sensing urban scene imagery,”\nISPRS J. Photogramm. Remote Sens., vol. 190, pp. 196–214, 2022.\n[41] F. Milletari, N. Navab, and S. Ahmadi, “V-net: Fully convolutional\nneural networks for volumetric medical image segmentation,” CoRR,\nvol. abs/1606.04797, 2016.\nHonglin Wu received the B.E. degree in com-\nmunication engineering and the Ph.D. degree\nin information and communication engineering\nfrom Huazhong University of Science and Tech-\nnology, Wuhan, China, in 2004 and 2012, re-\nspectively. He is currently an Associate Professor\nwith the School of Computer and Communica-\ntion Engineering, Changsha University of Science\nand Technology, Changsha, China. His current\nresearch interests include artificial intelligence,\ncomputer vision, and remote sensing image pro-\ncessing.\nMin Zhang graduated in 2021 with the Bache-\nlor’s degree in Software Engineering from Qing-\ndao University of Technology. Currently, she is\na second-year graduate student at the School\nof Computer Communication and Engineering,\nChangsha University of Science and Technology.\nHer research focus is on semantic segmentation\nof remote sensing imagery. She is proficient in\nPython programming language.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 10\nPeng Huang received the B.E. degree in com-\nmunication engineering from Hubei University of\nArts and Science, Xiangyang, China, in 2021. He\nis currently pursuing the M.S. degree with the\ncommunication and information system, Chang-\nsha University of Science and Technology, Chang-\nsha, China. His research interests include deep\nlearning, computer vision, and remote sensing\nimage processing.\nWenlong Tang graduated from Central South\nUniversity for Nationalities in 2021 with a Bache-\nlor’s degree in Computer Science and Technology.\nCurrently, he is a graduate student in the School\nof Computer Communication and Engineering,\nChangsha University of Technology. His research\ninterests include semantic segmentation of remote\nsensing images.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3375313\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7564729452133179
    },
    {
      "name": "Segmentation",
      "score": 0.6026061773300171
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5857723951339722
    },
    {
      "name": "Image segmentation",
      "score": 0.5388703942298889
    },
    {
      "name": "Computer vision",
      "score": 0.5169064998626709
    },
    {
      "name": "Scale (ratio)",
      "score": 0.45715683698654175
    },
    {
      "name": "Transformer",
      "score": 0.4296625852584839
    },
    {
      "name": "Remote sensing",
      "score": 0.3889378011226654
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32259517908096313
    },
    {
      "name": "Geography",
      "score": 0.12050282955169678
    },
    {
      "name": "Cartography",
      "score": 0.11776378750801086
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56934997",
      "name": "Changsha University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 36
}