{
  "title": "Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models",
  "url": "https://openalex.org/W3095594087",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2985027423",
      "name": "Thuy-Trang Vu",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2314522249",
      "name": "Dinh Phung",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4287813862",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W76749362",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2945808907",
    "https://openalex.org/W2787423662",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2978234544",
    "https://openalex.org/W2963777311",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2987154291",
    "https://openalex.org/W2972860176",
    "https://openalex.org/W2964125718",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2952826391",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2970928735",
    "https://openalex.org/W2770645414",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2032566933",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W2946232455",
    "https://openalex.org/W4288333985",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1882958252",
    "https://openalex.org/W2757947833",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3017003177",
    "https://openalex.org/W2963826681"
  ],
  "abstract": "Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6163–6173,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n6163\nEffective Unsupervised Domain Adaptation with\nAdversarially Trained Language Models\nThuy-Trang Vu♦ Dinh Phung†\nFaculty of Information Technology, Monash University, Australia\n♦{trang.vuthithuy},†{first.last}@monash.edu\nGholamreza Haffari†\nAbstract\nRecent work has shown the importance of\nadaptation of broad-coverage contextualised\nembedding models on the domain of the tar-\nget task of interest. Current self-supervised\nadaptation methods are simplistic, as the train-\ning signal comes from a small percentage of\nrandomly masked-out tokens. In this paper,\nwe show that careful masking strategies can\nbridge the knowledge gap of masked language\nmodels (MLMs) about the domains more ef-\nfectively by allocating self-supervision where\nit is needed. Furthermore, we propose an ef-\nfective training strategy by adversarially mask-\ning out those tokens which are harder to recon-\nstruct by the underlying MLM. The adversar-\nial objective leads to a challenging combina-\ntorial optimisation problem over subsets of to-\nkens, which we tackle efﬁciently through re-\nlaxation to a variational lower-bound and dy-\nnamic programming. On six unsupervised do-\nmain adaptation tasks involving named entity\nrecognition, our method strongly outperforms\nthe random masking strategy and achieves up\nto +1.64 F1 score improvements.\n1 Introduction\nContextualised word embedding models are be-\ncoming the foundation of state-of-the-art NLP sys-\ntems (Peters et al., 2018; Devlin et al., 2019; Liu\net al., 2019; Yang et al., 2019; Raffel et al., 2019;\nBrown et al., 2020; Clark et al., 2020). These mod-\nels are pretrained on large amounts of raw text\nusing self-supervision to reduce the labeled data\nrequirement of target tasks of interest by providing\nuseful feature representations (Wang et al., 2019a).\nRecent work has shown the importance of further\ntraining of pre-trained masked language models\n(MLMs) on the target domain text, as the beneﬁts\nof their contextualised representations can deteri-\norate substantially in the presence of domain mis-\nmatch (Ma et al., 2019; Xu et al., 2019; Wang et al.,\n2019c; Gururangan et al., 2020). This is partic-\nularly crucial in unsupervised domain adaptation\n(UDA), where there is no labeled data in the target\ndomain (Han and Eisenstein, 2019) and the knowl-\nedge from source domain labeled data is transferred\nto the target domain via a common representation\nspace. However, current self-supervised adaptation\nmethods are simplistic, as the training signal comes\nfrom a small percentage of randomly masked-out\ntokens. Thus, it remains to investigate whether\nthere exist more effective self-supervision strate-\ngies to bridge the knowledge gap of MLMs about\nthe domains to yield higher-quality adapted mod-\nels.\nA key principle of UDA is to learn a common\nembedding space of both domains which enables\ntransferring a learned model on source task to tar-\nget task. It is typically done by further pretraining\nthe MLM on a combination of both source and\ntarget data. Selecting relevant training examples\nhas been shown to be effective in preventing the\nnegative transfer and boosting the performance of\nadapted models (Moore and Lewis, 2010; Ruder\nand Plank, 2017). Therefore, we hypothesise that\nthe computational effort of the further pretraining\nshould concentrate more on learning words which\nare speciﬁc to the target domain or undergo seman-\ntic/syntactic shifts between the domains.\nIn this paper, we show that the adapted model\ncan beneﬁt from careful masking strategy and pro-\npose an adversarial objective to select subsets for\nwhich the current underlying MLM is less conﬁ-\ndent. This objective raises a challenging combi-\nnatorial optimisation problem which we tackle by\noptimising its variational lower bound. We propose\na training algorithm which alternates between tight-\nening the variational lower bound and learning the\nparameters of the underlying MLM. This involves\nproposing an efﬁcient dynamic programming (DP)\nalgorithm to sample from the distribution over the\n6164\nspace of masking subsets, and an effective method\nbased on Gumbel softmax to differentiate through\nthe subset sampling algorithm.\nWe evaluate our adversarial strategy against the\nrandom masking and other heuristic strategies in-\ncluding POS-based and uncertainty-based selection\non UDA problem of six NER span prediction tasks.\nThese tasks involve adapting NER systems from\nthe news domain to ﬁnancial, twitter, and biomedi-\ncal domains. Given the same computational budget\nfor further self-supervising the MLM, the experi-\nmental results show that our adversarial approach\nis more effective than the other approaches, achiev-\ning improvements up to +1.64 points in Fscore and\n+2.23 in token accuracy compared to the random\nmasking strategy.\n2 Uunsupervised DA with Masked LMs\nUDA-MLM. This paper focuses on the UDA\nproblem where we leverage the labeled data of\na related source task to learn a model for a target\ntask without accessing to its labels. We follow\nthe two-step UDA procedure proposed in Adapt-\naBERT consisting of a domain tuning step to learn\na common embedding space for both domains and\na task tuning step to learn to predict task labels\non source labeled data (Han and Eisenstein, 2019).\nThe learned model on the source task can be then\nzero-shot transferred to the target task thanks to the\nassumption that these tasks share the same label\ndistribution.\nThis domain-then-task-tuning procedure resem-\nbles the pretrain-then-ﬁnetuning paradigm of MLM\nwhere the domain tuning shares the same training\nobjective with the pretraining. In domain tuning\nstep, off-the-shelf MLM is further pretrained on an\nequal mixture of randomly masked-out source and\ntarget domain data.\nSelf-Supervision. The training principle of\nMLM is based on self-supervised learning where\nthe labels are automatically generated from unla-\nbeled data. The labels are generated by covering\nsome parts of the input, then asking the model to\npredict them given the rest of the input.\nMore speciﬁcally, a subset of tokens is sampled\nfrom the original sequence xxx and replaced with\n[MASK ] or other random tokens (Devlin et al.,\n2019).1 Without loss of generality, we assume\n1In BERT implementation, 15% tokens inxxx are selected;\namong them 80% are replaced with [MASK ], 10% are re-\nplaced with random tokens, and 10% are kept unchanged.\nthat all sampled tokens are replaced with [MASK ].\nLet us denote the set of masked out indices by S,\nthe ground truth tokens by xxxS = {xi|i∈S}, and\nthe resulting puzzle by xxx¯S which is generated by\nmasking out the sentence tokens with indices in S.\nThe training objective is to minimize the negative\nlog likelihood of the ground truth,\nmin\nθ\n−\n∑\nxxx∈D\nlog Pr(xxxS|xxx¯S; Bθ) (1)\nwhere Bθ is the MLM parameterised by θ, and D\nis the training corpus.\n3 Adversarially Trained Masked LMs\nGiven a ﬁnite computational budget, we argue that\nit should be spent wisely on new tokens or those\nhaving semantic/syntactic shifts between the two\ndomains. Our observation is that such tokens would\npose more challenging puzzles to the MLM, i.e.\nthe model is less conﬁdent when predicting them.\nTherefore, we propose to strategically select sub-\nsets for which the current underlying MLM Bθ is\nless conﬁdent about its predictions:\nmin\nθ\nmax\nS∈SK\n−log Pr(xxxS|xxx¯S; Bθ) (2)\nHenceforth, we assume that the size of the masked\nset K for a given sentence xxxis ﬁxed. For exam-\nple in BERT (Devlin et al., 2019), K is taken to\nbe 15% ×|xxx|where |xxx|denotes the length of the\nsentence. We denote all possible subsets of indices\nin a sentence with a ﬁxed size by SK.\n3.1 Our Variational Formulation\nThe masking strategy learning problem described\nin eqn (2) is a minimax game of two players: the\npuzzle generator to select the subset resulting in\nthe most challenging puzzle, and the MLM Bθ to\nbest solve the puzzle by reconstructing the masked\ntokens correctly. As optimising over the subsets is a\nhard combinatorial problem over the discrete space\nof SK, we are going to convert it to a continuous\noptimisation problem.\nWe establish a variational lower bound of the\nobjective function over S using the following in-\nequality,\nmax\nS∈SK\n−log Pr(xxxS|xxx¯S; Bθ) ≥ (3)\nmax\nφ\n∑\nS∈SK\n−q(S|xxx; πφ) logPr(xxxS|xxx¯S; Bθ) (4)\n6165\nFigure 1: (a) Our adversarial learned masking strategy for MLM includes a puzzle generator to estimate selection\nprobability, a subset sampling procedure and the MLM model. The red dash arrow shows the gradient ﬂow when\nupdating the puzzle generator. (b) Masked subset sampling procedure with dynamic programming.\nwhere q(.) is the variational distribution provided\nby a neural network πφ. This variational distribu-\ntion q(S|xxx; πφ) estimates the distribution over all\nsubset of size K. It is straightforward to see that\nthe weighted sum of negative log likelihood of all\npossible subsets is always less than the max value\nof them. Our minimax training objective is thus,\nmin\nθ\nmax\nφ\n∑\nS∈SK\n−q(S|xxx; πφ) logPr(xxxS|xxx¯S; Bθ) (5)\nq(S|xxx,πφ) =\n∏\ni∈S\nπφ(i|xxx)\n∏\ni′̸∈S\n(1 −πφ(i′|xxx))/Z (6)\nwhere Zis the partition function making sure the\nprobability distribution sums to one,\nZ=\n∑\nS′∈SK\n∏\ni∈S′\nπφ(i|xxx)\n∏\ni′/∈S′\n(1 −π(i′|xxx)). (7)\nThe number of possible subsets is |SK|=\n(|xxx|\nK\n)\n,\nwhich grows exponentially with respect to K. In\n§4, we provide efﬁcient dynamic programming al-\ngorithm for computing the partition function and\nsampling from this exponentially large combinato-\nrial space. In the following, we present our model\narchitecture and training algorithm for the puzzle\ngenerator φand MLM θparameters based on the\nvariational training objective in eqn (5).\n3.2 Model Architecture\nWe learn the masking strategy through the puz-\nzle generator network as shown in Figure 1. It is\na feed-forward neural network assigning a selec-\ntion probability πφ(i|xxx) for each index igiven the\noriginal sentence xxx, where φdenote the parame-\nters. Inputs to the puzzle generator are the feature\nrepresentations {hhhi}n\ni=1 of the original sequence\n{xxxi}n\ni=1. More speciﬁcally, they are output of the\nlast hidden states of the MLM. The probability of\nperform masking at position iis computed by ap-\nplying sigmoid function over the feed-forward net\noutput πφ(i|xxx) = σ(FFNN(hhhi)). From these prob-\nabilities, we can sample the masked positions in\norder to further train the underlying MLM Bθ.\n3.3 Optimising the Variational Bound\nWe use an alternating optimisation algorithm to\ntrain the MLM Bθ and the puzzle generator πφ\n(Algorithm 1). The update frequency for πφ is\ndetermined via a mixing hyperparameter β.\nTraining the MLM. Fixing the puzzle generator,\nwe can train the underlying MLM model using\ngradient descent on MLM objective in eqn (1),\nmin\nθ\nEq(S|xxx;πφ)[−log Pr(xxxS|xxx¯S; Bθ)] (8)\nwhere we approximate the expectation by sampling.\nThat is, Eq(S|xxx;πφ)[−log Pr(xxxS|xxx¯S; Bθ)] is approx-\nimated by\n1\nM\nM∑\nm=1\n−log Pr(xxxSm|xxx¯Sm; Bθ) (9)\nwhere Sm ∼q(S|xxx; πφ). In §4.2, we present an\nefﬁcient sampling algorithm based on a sequential\ndecision making process involving discrete choices,\ni.e. whether to include an index ior not.\n6166\nAlgorithm 1Adversarial Training Procedure\nInput: data D, update freq. β, masking size K\nOutput: generator πφ, MLM Bθ\n1: Let φ←φ0; θ←θ0\n2: while stopping condition is not met do\n3: for xxx∈D do\n4: S,q(S) ←subsetSampling(xxx,πφ,K)\n5: Update the MLM using Eq. (8)\n6: if coinToss(β)==Head then\n7: Compute reward\nr←−log Pr(xxxS|xxx¯S; Bθ)\n8: Update the generator using Eq. (10)\n9: end if\n10: end for\n11: end while\n12: return θ,φ\nTraining the Puzzle Generator. Fixing the\nMLM, we can train the puzzle generator by consid-\nering −log Pr(xxxS|xxx¯S; Bθ) as the reward, and aim\nto optimise the expected reward,\nmax\nφ\nEq(S|xxx;πφ)[−log Pr(xxxS|xxx¯S; Bθ)]. (10)\nWe may aim to sample multiple index sets\n{S1,..,S M}from q(S|xxx; πφ), and then optimise\nthe parameters of the puzzle generator by maxi-\nmizing the Monte Carlo estimate of the expected\nreward. However, as sampling each index set Sm\ncorresponds to a sequential decision making pro-\ncess involving discrete choices, we cannot back-\npropagate through the sampling process to learn the\nparameters of the puzzle generator network. There-\nfore, we rely on the Gumbel-Softmax trick (Jang\net al., 2017) to deal with this issue and backpropa-\ngate through the parameters of πφ, which we will\ncover in §4.3.\n4 Sampling and Differentiating Subsets\n4.1 A DP for the Partition Function\nIn order to sample from the variational distribution\nin eqn (6), we need to compute its partition function\nin eqn (7). Interestingly, the partition function can\nbe computed using dynamic programming (DP).\nLet us denote byZ(j,k) the partition function of\nall subsets of size kfrom the index set {j,.., |xxx|}.\nHence, the partition function of the qdistribution\nAlgorithm 2Sampling Procedure\nFunction: subsetSampling\nInput: datapoint xxx, prob. πφ, masking size K\nOutput: subset S, sample log probability l\n1: Let S ←∅; l←0; j ←0\n2: Calculate DP table Zusing Eq. (11)\n3: while |S|<K do\n4: j ←j+ 1\n5: qj,Y ←qj(Y|Sj−1,πφ) // using eqn (13)\n6: qj,N ←1 −qj,Y\n7: ϵj,Y,ϵj,N ∼Gumbel(0,1)\n8: oj ←argmaxo∈{Y,N}log qj,o + ϵj,o\n9: l+= log softmax(log qj,o + ϵj,o)\n⏐⏐\no=oj\n10: if oj == Y then\n11: S ←S∪{j}\n12: end if\n13: end while\n14: return S,l\nis Z(1,K). The DP relationship can be written as,\nZ(j−1,k) = (1 −π(j−1|xxx))Z(j,k)\n+ πφ(j−1|xxx)Z(j,k −1). (11)\nThe initial conditions are Z(j,0) = 1 and\nZ(|xxx|−k+ 1,k) =\n|xxx|∏\ni=|xxx|−k+1\nπφ(j|xxx) (12)\ncorresponding to two special terminal cases in se-\nlection process in which we have picked all K\nindices, and we need to select all indices left to\nfulﬁl K.\nThis amounts to a DP algorithm with the time\ncomplexity O(K|xxx|).\n4.2 Subset Sampling for MLMs\nThe DP in the previous section also gives rise to\nthe sampling procedure. Given a partial random\nsubset Sj−1 with elements chosen from the indices\n{1,..,j −1}, the probability of including the next\nindex j, denoted by qj(yes|Sj−1,πφ), is\nπφ(j|xxx)Z(j+ 1,K −1 −|Sj−1|)\nZ(j,K −|Sj−1|) (13)\nwhere Z(j,k) values come from the DP table.\nHence, the probability of not including the index j\nis\nqj(no|Sj−1,πφ) = 1 −qj(yes|Sj−1,πφ). (14)\n6167\nNER Num. tokens/Num. sent. Unlab. Num. tokens\nDomain Dataset Train Dev. Test Corpus /Num. sent.\nNEWS CoNLL2003 203.6k/14.0k 51.36k/3.3k 46.4k/3.5k - -\nTWEET WNUT2016 46.5k/2.4k 16.3K/1k 61.9k/3.8k Sentiment140 20M/1.4M\nFIN FIN 41.0k/1.2k - 13.3k/303 SEC Filing 2019 155M/5.5M\nBIOMED JNLPBA 445.0k/16.8k 47.5k/1.7k 101.0k/3.9k PubMed 4.3B/181M\nBIOMED BC2GM 355.4k/12.6k 71.0k/2.5k 143.5k/5.0k PubMed 4.3B/181M\nBIOMED BioNLP09 227.7k/7.5k 44.2k/1.4k 74.6k/2.5k PubMed 4.3B/181M\nBIOMED BioNLP11EPI 161.6k/5.7k 54.8k/1.9k 116.1k/4.1k PubMed 4.3B/181M\nTable 1: Data statistics of named entity span prediction tasks and unlabled additional pretraining corpus.\nIn case the next index is chosen to be in the sample,\nthen Sj+1 = Sj ∪{j+ 1}; otherwise Sj+1 = Sj.\nThe sampling process entails a sequence of\nbinary decisions (Figure 1.b) in an underlying\nMarkov Decision Process (MDP). It is an iterative\nprocess, which starts by considering the index one.\nAt each decision point j, the sampler’s action space\nis to whether include (or not include) the index j\ninto the partial sample Sj based on eqn (13). We\nterminate this process when the partially selected\nsubset has Kelements.\nThe sampling procedure is described in Algo-\nrithm 2. In our MDP, we actually sample an index\nby generating Gumbel noise in each stage, and then\nselect the choice (yes/no) with the maximum prob-\nability. This enables differentiation through the\nsampled subset, covered in the next section.\n4.3 Differentiating via Gumbel-Softmax\nOnce the sampling process is terminated, we then\nneed to backpropagate through the parameters of\nπφ, when updating the parameters of the puzzle\ngenerator according to eqn (10).\nMore concretely, let us assume that we would\nlike to sample a subsetS. As mentioned in previous\nsection, we need to decide about the inclusion of\nthe next index j given the partial sample so far\nSj−1 based on the eqn (13). Instead of uniform\nsampling, we can equivalently choose one of these\ntwo outcomes as follows\no∗\nj = argmax\noj∈{yes,no}\nlog qj(oj|Sj−1,πφ) + ϵoj (15)\nwhere the random noise ϵoj is distributed according\nto standard Gumbel distribution. Sampling a subset\nthen amounts to a sequence of argmax operations.\nTo backpropagate through the sampling process,\nwe replace the argmax operators with softmax, as\nargmax is not differentiable. That is,\nPr(oj) =\nexp(log qj(oj|Sj−1,πφ)+ϵoj)∑\no′\nj\nexp(log qj(o′\nj|Sj−1,πφ)+ϵo′\nj\n) . (16)\nThe log product of the above probabilities for the\ndecisions in a sampling path is returned as lin Al-\ngorithm 2, which is then used for backpropagation.\n5 Experiments\nWe evaluate our proposed masking strategy in UDA\nfor named entity span prediction tasks coming from\nthree different domains.\n5.1 Unsupervised Domain Adaptation Tasks\nSource and Target Domain Tasks. Our eval-\nuation is focused on the problem of identify-\ning named entity spans in domain-speciﬁc text\nwithout access to labeled data. The evaluation\ntasks comes from several named entity recogni-\ntion (NER) dataset including WNUT2016 (Strauss\net al., 2016), FIN (Salinas Alvarado et al., 2015),\nJNLPBA (Collier and Kim, 2004), BC2GM (Smith\net al., 2008), BioNLP09 (Kim et al., 2009), and\nBioNLP11EPI (Kim et al., 2011). Table 1 reports\ndata statistics.\nThese datasets cover three domains social me-\ndia ( TWEETS ), ﬁnancial ( FIN) and biomedical\n(BIOMED). We utilize the CoNLL-2003 English\nNER dataset in newstext domain ( NEWS ) as the\nsource task and others as the target. We perform\ndomain-tuning and source task-tuning, followed by\nzero-shot transfer to the target tasks, as described\nin §2. Crucially, we do not use the labels of the\ntraining sets of the target tasks, and only use their\nsentences for domain adaptation. Since the number\nof entity types are different in each task, we convert\nall the labels to entity span in IBO scheme. This\nensures that all tasks share the same set of labels\nconsisting of three tags: I, B, and O.\n6168\nExtra Target Domain Unlabeled Corpora.As\nthe domain tuning step can further beneﬁt from\nadditional unlabeled data, we create target domain\nunlabeled datasets from the available corpora of\nrelevant domains. More speciﬁcally, we use pub-\nlicly available corpora, Sentiment140 (Go et al.,\n2009), SEC Filing 2019 2 (DeSola et al., 2019)\nPubMed (Lee et al., 2020) for the TWEET , FIN and\nBIOMED domains respectively (Table 1). From the\nunlabeled corpora, the top 500K and 1M similar\nsentences to the training set of each target task are\nextracted based on the average n-gram similarity\nwhere 1 ≤n≤4, resulting in extra target domain\nunlabeled corpora.\n5.2 Masking Strategies for MLM Training\nWe compare our adversarial learned masking strat-\negy approach against random and various heuristic\nmasking strategies which we propose:\n•Random. Masked tokens are sampled uniformly\nat random, which is the common strategy in the\nliterature (Devlin et al., 2019; Liu et al., 2019).\n•POS-based strategy. Masked tokens are sam-\npled according to a non-uniform distribution,\nwhere a token’s probability depends on its POS\ntag. The POS tags are obtained using spaCy. 3\nContent tokens such as verb (VERB), noun\n(N), adjective (ADJ), pronoun (PRON) and ad-\nverb (ADV) tags are assigned higher probability\n(80%) than other content-free tokens such as\nPREP, DET, PUNC (20%).\n•Uncertainty-based strategy. We select those\ntokens for which the current MLM is most un-\ncertain for the reconstruction, where the un-\ncertainty is measured by the entropy. That\nis, we aim to select those tokens with high\nEntropy[Pri(.|xxx¯Si; Bθ)], where xxx¯Si is the sen-\ntence xxx with the ith token masked out, and\nPri(.|xxx¯Si; Bθ) is the predictive distribution for\nthe ith position in the sentence.\nCalculating the predictive distribution for each\nposition requires one pass through the network.\nHence, it is expensive to use the exact entropy,\nas it requires |xxx|passes. We mitigate this cost by\nusing Pri(.|xxx; Bθ) instead, which conditions on\nthe original unmasked sentence. This estimation\nonly costs one pass through the MLM.\n2http://people.ischool.berkeley.edu/\n˜khanna/fin10-K/\n3https://spacy.io/\n•Adversarial learned strategy. The masking\nstrategy is learned adversarially as in §3. The\npuzzle-generator update frequency β (Algo-\nrithm 1) is set to 0.3 for all experiments.\nThese strategies only differ in how we choose the\ncandidate tokens. The number of to-be-masked\ntokens is the same in all strategies (15%). Among\nthem, 80% are replaced with [MASK], 10% are\nreplaced with random words, the rest are kept un-\nchanged as in (Devlin et al., 2019). In our experi-\nments, the masked sentences are generated dynam-\nically on-the-ﬂy.\nTo evaluate the models, we compute precision,\nrecall and F1 scores on a per token basis. We report\naverage performance of ﬁve runs.\n5.3 Implementation Details\nOur implementation is based on Tensorﬂow li-\nbrary (Abadi et al., 2016) 4. We use BERT-Base\nmodel architecture which consists of 12 Trans-\nformer layers with 12 attention heads and hidden\nsize 768 (Devlin et al., 2019) in all our experiments.\nWe use the cased wordpiece vocabulary provided in\nthe pretrained English model. We set learning rate\nto 5e-5 for both further pretraining and task tun-\ning. Puzzle generator is a two layer feed-forward\nnetwork with hidden size 256 and dropout rate 0.1.\n5.4 Empirical Results\nUnder the same computation budget to update the\nMLM, we evaluate the effect of masking strategy in\nthe domain tuning step under various size of addi-\ntional target-domain data: none, 500K and 1M. We\ncontinue pretraining BERT on a combination of un-\nlabeled source (CoNLL2003), unlabeled target task\ntraining data and additional unlabeled target do-\nmain data (if any). If target task data is smaller, we\noversample it to have equal size to the source data.\nThe model is trained with batch size 32 and max\nsequence length 128 for 50K steps in 1M target-\ndomain data and 25K steps in other cases. It equals\nto 3-5 epochs over the training set. After domain\ntuning, we ﬁnetune the adapted MLM on the source\ntask labeled training data (CoNLL2003) for three\nepochs with batch size 32. Finally, we evaluate\nthe resulting model on target task. On the largest\ndataset, random and POS strategy took around 4\nhours on one NVIDIA V100 GPU while entropy\n4Source code is available at https://github.com/\ntrangvu/mlm4uda\n6169\nUDA UDA + 500K target-domainUDA + 1M target-domain\nTask rand pos ent adv rand pos ent adv rand pos ent adv\nWNUT2016 47.11 46.79† 46.95† 47.03† 46.93 47.69† 47.84† 48.01† 52.36 52.01† 52.74† 52.53†\nFIN 21.55 22.53† 22.73† 23.38† 24.70 26.70† 26.63† 26.85† 25.96 26.95† 26.96† 28.94†\nJNLPBA 27.44 28.06† 28.22† 30.06† 29.92 30.56† 30.47† 30.31† 31.01 30.91† 31.59† 31.54†\nBC2GM 28.31 28.50 30.81† 29.01† 31.13 31.85† 31.83† 32.38† 31.35 31.70† 32.01† 32.49†\nBioNLP09 26.37 27.53† 29.21† 29.24† 31.38 31.03† 34.33† 35.05† 32.16 33.51† 34.99† 35.41†\nBioNLP11EPI32.69 33.51† 34.81† 34.59† 42.41 42.81† 42.83† 42.64 43.11 43.47† 43.31 43.61†\n¯∆ - +0.58 +1.54 +1.64 - +0.70 +1.26 +1.46 +0.43 +0.94 +1.43\nTable 2: F1 score of name entity span prediction tasks in three UDA scenarios which differ in the amount of\nadditional target-domain data. rand, pos, ent and adv denote the random, POS-based, uncertainty-based, and\nadversarial masking strategy respectively. ¯∆ row reports the average improvement over random masking across\nall tasks. Bold shows the highest score of task on each UDA setting. † indicates statistically signiﬁcant difference\nto the random baseline with p-value ≤0.05 using bootstrap test.\nTask rand mix-pos mix-ent mix-adv\nUDA + 500K\nWNUT2016 46.93 51.17 52.40 52.56\nFIN. 24.70 26.95 27.36 28.30\nJNLPBA 29.92 29.22 31.65 32.99\nBC2GM 31.13 32.11 32.68 32.60\nBioNLP09 31.38 33.17 34.27 34.91\nBioNLP11EPI42.41 42.73 43.43 43.08\n¯∆ - +3.10 +4.17 +4.61\nUDA + 1M\nWNUT2016 52.36 52.40 52.64 52.95\nFIN. 25.96 27.86 28.51 29.08\nJNLPBA 31.01 31.77 32.07 32.26\nBC2GM 31.35 31.76 32.43 32.52\nBioNLP09 32.61 34.49 35.67 35.78\nBioNLP11EPI43.11 43.96 44.81 44.27\n¯∆ - +1.05 +1.70 +1.82\nTable 3: F1 score in UDA with additional data under\nseveral mixed masking strategies.Bold shows the high-\nest score of task on each UDA setting.\nand adversarial approach took 5 and 7 hours respec-\ntively. The task tuning took about 30 minutes.\nResults are shown in Table 2. Overall, strate-\ngically masking consistently outperforms random\nmasking in most of the adaptation scenarios and\ntarget tasks. As expected, expanding training data\nwith additional target domain data further improves\nperformance of all models. Comparing to random\nmasking, prioritising content tokens over content-\nfree ones can improve up to 0.7 F1 score in av-\nerage. By taking the current MLM into account,\nuncertainty-based selection and adversarial learned\nstrategy boost the score up to 1.64. Our proposed\nadversarial approach yields highest score in 11 out\nof 18 cases, and results in the largest improvement\nover random masking across all tasks in both UDA\nwith and without additional target domain data.\nFigure 2: V ocabulary overlap (%) between NER tasks.\nWe further explore the mix of random masking\nand other masking strategies. We hypothesise that\nthe combination strategies can balance the learning\nof challenging tokens and effortless tokens when\nforming the common semantic space, hence im-\nprove the task performance. In a minibatch, 50%\nof sentences are masked according to the corre-\nsponding strategy while the rest are masked ran-\ndomly. Results are shown in Table 3. We observe\nan additional performance to the corresponding\nsingle-strategy model across all tasks.\n5.5 Analysis\nDomain Similarity. We quantify the similarity\nbetween source (CoNLL2003) and target domains\nby vocabulary overlap between the domains (ex-\ncluding stopwords). Figure 2 shows the vocabulary\noverlap across tasks. As seen, all the target do-\nmains are dissimilar to the source domain, with\nFIN having the lowest overlap. FIN has gained the\n6170\nFigure 3: Average density ratio of masked-out tokens\nof every 2500 training steps in UDA setting.\nlargest improvement from the adversarial strategy\nin the UDA results in Tables 2 and 3. As expected,\nthe biomedical datasets have relatively higher vo-\ncabulary overlap with each other.\nDensity Ratio of Masked Subsets.We analyze\nthe density ratio of masked-out tokens in the target\nand source domains\nr(w) = max(1 −Prs(w)\nPrt(w) ,0)\nwhere Prs(w) and Prt(w) is the probability of to-\nken win source and target domains, respectively.\nThese probabilities are according to unigram lan-\nguage models trained on the training sets of the\nsource and target tasks. The higher value of r(w)\nmeans the token wis new or appears more often\nin the target text than in the source. Figure 3 plots\nthe density ratio of masked-out tokens during do-\nmain tuning time for four UDA tasks. Comparing\nto other strategies, we observed that adversarial\napproach tends to select tokens which have higher\ndensity ratio, i.e. more signiﬁcant in the target.\nSyntactic Diversity in Masked Subset.Table 4\ndescribes the percentage of POS tags in masked\nsubset selected by different masking strategies. We\nobserved that our method selects more tokens from\nthe major POS tags (71%) compared to random\n(45%) and entropy-based (55%) strategies. It has\nchosen less nouns compared to the POS strategy,\nand more pronouns compared to all other strategies.\nTagging Accuracy of OOV and non-OOV .We\ncompare the tagging accuracy of out-of-vocabulary\n(OOV) words which are in target domain but not\nPOS Tag rand pos ent adv\nADJ 9% 17% 11% 13%\nVERB 8% 16% 10% 17%\nNOUN 25% 51% 31% 34%\nPRON 1% 2% 1% 3%\nADV 2% 4% 2% 4%\nOthers 55% 10% 45% 29%\nTable 4: The tag ratio of the POS tags of tokens in\nmasked subset on BIONLP11 under different masking\nstrategies.\npresenting in source, and non-OOV tokens in Ta-\nble 5. As seen, our adversarial masking strategy\nachieves higher accuracy on both OOV and non-\nOOV tokens in most cases.\n6 Related Work\nUnsupervised Domain Adaptation. The main\napproaches in neural UDA include discrepancy-\nbased and adversarial-based methods. The\ndiscrepancy-based methods are based on the usage\nof the maximum mean discrepancy or Wasserstein\ndistance as a regularizer to enforce the learning of\ndomain non-discriminative representations (Shen\net al., 2018). Inspired by the Generative Adversar-\nial Network (GAN) (Goodfellow et al., 2014), the\nadversarial-based methods learn a representation\nthat is discriminative for the target task and indis-\ncriminative to the shift between the domains (Ganin\nand Lempitsky, 2015).\nDomain Adaptation with MLM. Performance\nof ﬁne-tuned MLM can deteriorate substantially\non the presence of domain mismatch. The most\nstraightforward domain adaptation approach in\nMLM is to adapt general contextual embedding\nto a speciﬁc domain (Lee et al., 2020; Alsentzer\net al., 2019; Chakrabarty et al., 2019), that is to fur-\nther improve pretrained MLM by continuing to pre-\ntrain language models on related domain or similar\ntasks (Gururangan et al., 2020), or via intermediate\ntask which is also referred to as STILTs (Phang\net al., 2018). Recent works have proposed two-\nstep adaptive domain adaptation framework which\nconsists of domain tuning and task ﬁnetuning (Ma\net al., 2019; Xu et al., 2019; Wang et al., 2019c;\nLogeswaran et al., 2019). They have demonstrated\nthat domain tuning is necessary to adapt MLM\nwith both domain knowledge and task knowledge\nbefore ﬁnetuning, especially when the labelled data\n6171\nTask Model acc. non-OOV OOV\nrand 23.04 21.88 24.99\nWNUT2016 pos 23.78 22.77 25.48\nent 23.95 22.95 25.62\nadv 24.20 22.79 26.57\nrand 27.66 25.01 30.88\nFIN pos 28.51 27.23 29.36\nent 28.09 27.67 31.21\nadv 29.36 27.56 33.90\nrand 7.77 7.86 7.50\nJNLPBA pos 9.74 9.83 9.50\nent 8.79 8.81 8.74\nadv 7.92 7.89 8.01\nrand 11.38 11.35 11.48\nBC2GM pos 13.09 12.88 13.89\nent 13.19 12.89 14.28\nadv 14.53 14.44 14.84\nrand 9.49 8.88 10.2\nBioNLP09 pos 9.45 10.51 8.22\nent 13.11 15.67 10.14\nadv 14.82 18.45 10.61\nrand 13.16 27.40 6.57\nBioNLP11EPI pos 14.02 28.28 7.43\nent 14.28 28.70 7.59\nadv 13.76 28.56 6.89\nTable 5: Tagging accuracy of in-vocabulary (non-OOV)\nand out-of-vocabulary (OOV) words in UDA + 500K\nin-domain data.\nin target task is extremely small. Our experiment\nsetting is similar to Han and Eisenstein (2019)’s\nwork. However, we focus on learning masking\nstrategy to boost the domain-tuning step.\nAdversarial Learning. Recent research in ad-\nversarial machine learning has either focused on\nattacking models with adversarial examples (Alzan-\ntot et al., 2018; Iyyer et al., 2018; Ebrahimi et al.,\n2018), or training models to be robust against\nthese attacks (Zhou et al., 2019). Wang et al.\n(2019b); Liu et al. (2020) propose the use of adver-\nsarial learning for language models. They consider\nautoregressive LMs and train them to be robust\nagainst adversarial perturbations of the word em-\nbeddings of the target vocabulary.\n7 Conclusion\nWe present an adversarial objective for further pre-\ntraining MLM in UDA problem. The intuition\nbehind the objective is that the adaptation effort\nshould focus on a subset of tokens which are chal-\nlenging to the MLM. We establish a variational\nlower bound of the objective function and propose\nan effective sampling algorithm using dynamic pro-\ngramming and Gumbel softmax trick. Comparing\nto other masking strategies, our proposed adver-\nsarial masking approach has achieve substantially\nbetter performance on UDA problem of named en-\ntity span prediction for several domains.\nAcknowledgments\nThis material is based on research sponsored by\nAir Force Research Laboratory and DARPA under\nagreement number FA8750-19-2-0501. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for Governmental purposes notwith-\nstanding any copyright notation thereon. The au-\nthors are grateful to the anonymous reviewers for\ntheir helpful comments. The computational re-\nsources of this work are supported by the Google\nCloud Platform (GCP), and by the Multi-modal\nAustralian ScienceS Imaging and Visualisation En-\nvironment (MASSIVE) (www.massive.org.au).\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. 2016. Tensorﬂow: A system for large-scale\nmachine learning. In 12th {USENIX}Symposium\non Operating Systems Design and Implementation\n({OSDI}16), pages 265–283.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial ex-\namples. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2890–2896, Brussels, Belgium. Association\nfor Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n6172\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nTuhin Chakrabarty, Christopher Hidey, and Kathy\nMcKeown. 2019. IMHO ﬁne-tuning improves claim\ndetection. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n558–563, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. {ELECTRA}: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the bio-entity recognition task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nVinicio DeSola, Kevin Hanna, and Pri Nonis. 2019.\nFinbert: pre-trained model on sec ﬁlings for ﬁnan-\ncial natural language tasks.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018. HotFlip: White-box adversarial exam-\nples for text classiﬁcation. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n31–36, Melbourne, Australia. Association for Com-\nputational Linguistics.\nYaroslav Ganin and Victor Lempitsky. 2015. Unsu-\npervised domain adaptation by backpropagation. In\nProceedings of the 32nd International Conference\non International Conference on Machine Learning\n- Volume 37, ICML’15, page 1180–1189. JMLR.org.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twit-\nter sentiment classiﬁcation using distant supervision.\nCS224N project report, Stanford, 1(12):2009.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248, Hong Kong,\nChina. Association for Computational Linguistics.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1875–1885, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-\ngorical reparameterization with gumbel-softmax. In\nInternational Conference on Learning Representa-\ntions.\nJin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-\nnobu Kano, and Jun’ichi Tsujii. 2009. Overview of\nBioNLP’09 shared task on event extraction. In Pro-\nceedings of the BioNLP 2009 Workshop Companion\nVolume for Shared Task , pages 1–9, Boulder, Col-\norado. Association for Computational Linguistics.\nJin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert\nBossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011.\nOverview of BioNLP shared task 2011. In Proceed-\nings of BioNLP Shared Task 2011 Workshop, pages\n1–6, Portland, Oregon, USA. Association for Com-\nputational Linguistics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu\nChen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n2020. Adversarial training for large neural language\nmodels. arXiv preprint arXiv:2004.08994.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova, Jacob Devlin, and Honglak Lee.\n2019. Zero-shot entity linking by reading entity de-\nscriptions. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\n6173\npages 3449–3460, Florence, Italy. Association for\nComputational Linguistics.\nXiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nalla-\npati, and Bing Xiang. 2019. Domain adaptation\nwith BERT-based domain classiﬁcation and data se-\nlection. In Proceedings of the 2nd Workshop on\nDeep Learning Approaches for Low-Resource NLP\n(DeepLo 2019) , pages 76–83, Hong Kong, China.\nAssociation for Computational Linguistics.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224, Uppsala, Sweden. Association for\nComputational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv e-prints.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian opti-\nmization. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 372–382, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Tim-\nothy Baldwin. 2015. Domain adaption of named en-\ntity recognition to support credit risk assessment. In\nProceedings of the Australasian Language Technol-\nogy Association Workshop 2015 , pages 84–90, Par-\nramatta, Australia.\nJian Shen, Yanru Qu, Weinan Zhang, and Yong Yu.\n2018. Wasserstein distance guided representation\nlearning for domain adaptation. In Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence.\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan\nHsu, Yu-Shi Lin, Roman Klinger, Christoph M\nFriedrich, Kuzman Ganchev, et al. 2008. Overview\nof biocreative ii gene mention recognition. Genome\nbiology, 9(2):S2.\nBenjamin Strauss, Bethany Toma, Alan Ritter, Marie-\nCatherine de Marneffe, and Wei Xu. 2016. Results\nof the WNUT16 named entity recognition shared\ntask. In Proceedings of the 2nd Workshop on Noisy\nUser-generated Text (WNUT) , pages 138–144, Os-\naka, Japan. The COLING 2016 Organizing Commit-\ntee.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nDilin Wang, Chengyue Gong, and Qiang Liu. 2019b.\nImproving neural language modeling via adversarial\ntraining. In Proceedings of the 36th International\nConference on Machine Learning , volume 97 of\nProceedings of Machine Learning Research , pages\n6555–6565, Long Beach, California, USA.\nHuazheng Wang, Zhe Gan, Xiaodong Liu, Jingjing Liu,\nJianfeng Gao, and Hongning Wang. 2019c. Adver-\nsarial domain adaptation for machine reading com-\nprehension. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2510–2520, Hong Kong, China. As-\nsociation for Computational Linguistics.\nHu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT\npost-training for review reading comprehension and\naspect-based sentiment analysis. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2324–2335, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nYilun Zhou, Julie Shah, and Steven Schockaert. 2019.\nLearning household task knowledge from WikiHow\ndescriptions. In Proceedings of the 5th Workshop on\nSemantic Deep Learning (SemDeep-5), pages 50–56,\nMacau, China. Association for Computational Lin-\nguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8329439163208008
    },
    {
      "name": "Masking (illustration)",
      "score": 0.6840566992759705
    },
    {
      "name": "Domain adaptation",
      "score": 0.6282904744148254
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6006326675415039
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5785016417503357
    },
    {
      "name": "Task (project management)",
      "score": 0.5394409894943237
    },
    {
      "name": "Embedding",
      "score": 0.5288316607475281
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5125908255577087
    },
    {
      "name": "Machine learning",
      "score": 0.47760650515556335
    },
    {
      "name": "Language model",
      "score": 0.464308500289917
    },
    {
      "name": "Mathematics",
      "score": 0.10513058304786682
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}