{
  "title": "Masked Transformer for Image Anomaly Localization",
  "url": "https://openalex.org/W4283331368",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4283350799",
      "name": "Axel De Nardin",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A2106999547",
      "name": "Pankaj Mishra",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A2186422030",
      "name": "Gian Luca Foresti",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A319683806",
      "name": "Claudio Piciarelli",
      "affiliations": [
        "University of Udine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3092207554",
    "https://openalex.org/W2950119225",
    "https://openalex.org/W3157328277",
    "https://openalex.org/W2918698221",
    "https://openalex.org/W2623808523",
    "https://openalex.org/W2795691199",
    "https://openalex.org/W3129161690",
    "https://openalex.org/W2803255133",
    "https://openalex.org/W2948982773",
    "https://openalex.org/W2599354622",
    "https://openalex.org/W2914570111",
    "https://openalex.org/W2809705434",
    "https://openalex.org/W2784032999",
    "https://openalex.org/W3034314048",
    "https://openalex.org/W3044215473",
    "https://openalex.org/W2925312408",
    "https://openalex.org/W2796762894",
    "https://openalex.org/W2963049059",
    "https://openalex.org/W3169077988",
    "https://openalex.org/W3209793239",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2562637781",
    "https://openalex.org/W3118600296",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1505943987",
    "https://openalex.org/W2921440296",
    "https://openalex.org/W2966126335",
    "https://openalex.org/W2736832651",
    "https://openalex.org/W4212919149",
    "https://openalex.org/W2803674491",
    "https://openalex.org/W2787947370",
    "https://openalex.org/W3100850306",
    "https://openalex.org/W4376626035",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4298289240",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3101017490",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4378009855",
    "https://openalex.org/W2765389708"
  ],
  "abstract": "Image anomaly detection consists in detecting images or image portions that are visually different from the majority of the samples in a dataset. The task is of practical importance for various real-life applications like biomedical image analysis, visual inspection in industrial production, banking, traffic management, etc. Most of the current deep learning approaches rely on image reconstruction: the input image is projected in some latent space and then reconstructed, assuming that the network (mostly trained on normal data) will not be able to reconstruct the anomalous portions. However, this assumption does not always hold. We thus propose a new model based on the Vision Transformer architecture with patch masking: the input image is split in several patches, and each patch is reconstructed only from the surrounding data, thus ignoring the potentially anomalous information contained in the patch itself. We then show that multi-resolution patches and their collective embeddings provide a large improvement in the model’s performance compared to the exclusive use of the traditional square patches. The proposed model has been tested on popular anomaly detection datasets such as MVTec and head CT and achieved good results when compared to other state-of-the-art approaches.",
  "full_text": "October 28, 2022 0:43 main\nMasked Transformer for image Anomaly Localization\nAxel De Nardin\nDepartment of Mathematics, Computer Science and Physics, Universit` a degli Studi di Udine\nvia delle Scienze 206, 33100 Udine, Italy\nEmail: denardin.axel@spes.uniud.it\nPankaj Mishra\nDepartment of Mathematics, Computer Science and Physics, Universit` a degli Studi di Udine\nvia delle Scienze 206, 33100 Udine, Italy\nEmail: mishra.pankaj@spes.uniud.it\nGian Luca Foresti\nDepartment of Mathematics, Computer Science and Physics, Universit` a degli Studi di Udine\nvia delle Scienze 206, 33100 Udine, Italy\nEmail: gianluca.foresti@uniud.it\nClaudio Piciarelli\nDepartment of Mathematics, Computer Science and Physics, Universit` a degli Studi di Udine\nvia delle Scienze 206, 33100 Udine, Italy\nEmail: claudio.piciarelli@uniud.it\nImage anomaly detection consists in detecting images or image portions that are visually diﬀerent\nfrom the majority of the samples in a dataset. The task is of practical importance for various real-life\napplications like biomedical image analysis, visual inspection in industrial production, banking, traﬃc\nmanagement, etc. Most of the current deep learning approaches rely on image reconstruction: the input\nimage is projected in some latent space and then reconstructed, assuming that the network (mostly\ntrained on normal data) will not be able to reconstruct the anomalous portions. However, this assumption\ndoes not always hold. We thus propose a new model based on the Vision Transformer architecture with\npatch masking: the input image is split in several patches, and each patch is reconstructed only from the\nsurrounding data, thus ignoring the potentially anomalous information contained in the patch itself. We\nthen show that multi-resolution patches and their collective embeddings provide a large improvement in\nthe model’s performance compared to the exclusive use of the traditional square patches. The proposed\nmodel has been tested on popular anomaly detection datasets such as MVTec and headCT and achieved\ngood results when compared to other state-of-the-art approaches.\nKeywords: Anomaly Detection; Vision Transformer; Image inpainting; Self-supervised learning\n1. Introduction\nThe ability to identify anomalous instances in large\nsets of data is of great importance in many diﬀerent\nﬁelds of application. One typical example is repre-\nsented by the need to detect and discard faulty prod-\nucts in industrial production lines1; 2; 3. The ability\nto do so in an automated and eﬀective way represents\na very important problem for manufacturing compa-\nnies. Another area where this type of problem is of\neven greater importance is the one represented by\nthe analysis of biomedical data4; 5; 6; 7 in which the\nearly detection of anomalies can make the diﬀerence\nbetween life and death for a patient.\n1\narXiv:2210.15540v1  [cs.CV]  27 Oct 2022\nOctober 28, 2022 0:43 main\n2\nWhile for humans dealing with this kind of prob-\nlem is a rather easy task, we cannot say the same for\nmachines. One main reason that makes it a diﬃcult\nproblem to address is the fact that, while it is essen-\ntially a classiﬁcation problem, classical classiﬁcation\napproaches cannot be used because of the nature of\nthe data analyzed. When dealing with anomaly de-\ntection problems, the data is often highly unbalanced\nin favor of “normal” instances, while we have very\nfew examples representing the “abnormal” ones 8.\nWhen dealing with images we also have the added\nproblem of the high dimensionality of the data which\noften leads to more classical methods for anomaly de-\ntection, such as clustering techniques, to achieve poor\nperformance. For this reason, many of the most re-\ncent approaches for image anomaly detection adopt\na deep neural network model to map the inputs into\na latent feature space to which classical approaches\ncan be applied more eﬀectively or, in most cases,\nwhich can be used to obtain an image reconstruction\nwhich is then compared to the original one to per-\nform a self-supervised kind of training. The idea be-\nhind these approaches is that, since the model is usu-\nally trained only on the normal images, there should\nbe a larger diﬀerence between the reconstructed and\nthe original image for the anomalous instances, and\ntherefore we should be able to identify them eﬀec-\ntively during the testing process.\nMost of the models used in recent years to tackle\nthe anomaly detection problem make use of Convo-\nlutional layers which exploit the typical characteris-\ntics of images by detecting progressively more com-\nplex features starting from the most basic ones (e.g.\nedges). In particular, there are two kinds of architec-\ntures, with their respective variations, which gained\na lot of popularity for their eﬀectiveness in dealing\nwith this kind of problem, which are Autoencoders 9\nand Generative Adversarial Networks (GANs) 10.\nOne problem with classical image reconstruction ap-\nproaches though, is that they use the information\nextracted from the whole image to obtain the out-\nput image, which can lead the model to be able to\nreconstruct also the anomalies and therefore to not\nbe able to identify them.\nIn this paper, we propose a new approach\na novel method for image Anomaly Detection,\ncalled Masked Transformer for image Anomaly Lo-\ncalization (MeTAL), that adopts as its backbone\nthe recently presented Vision Transformer (ViT)\narchitecture11, which instead of leveraging the prior\nknowledge granted by the use of convolutional layers,\nis characterized by the adoption of a masked multi-\nhead self-attention mechanism that allows the model\nto learn a relationship between diﬀerent patches of\nthe input images. In particular, with the present\nwork we introduce two main novelties to the origi-\nnal architecture. the ﬁrst one regards a new masking\ncomponent we added to the multi head self atten-\ntion module of the ViT encoder which allows us to\nreconstruct each patch of the image without using\nany information coming from the patch itself but\nusing only the information extracted from the sur-\nrounding patches based on the importance given to\neach of them by the attention module. The second\nidea we present regards instead the way patches are\ngenerated from the original image. In particular, in-\nstead of relying on just the square patches as in the\noriginal work presenting ViT, we introduce the idea\nof calculating attention between patches of diﬀerent\nshapes, which are then combined to obtaine the ﬁ-\nnal image reconstruction. As we will show in section\n4.4 both ideas resulted in an improvement in perfor-\nmance over the baseline model for the task at hand.\nFurthermore we show that the Vision Trans-\nformer architecture is a valid option for anomaly de-\ntection problems and can be adopted eﬀectively even\nin scenarios where the amount of data available is\nrelatively small without necessarily relying on a pre-\ntraining procedure.\nThe rest of the paper is organized as follows. In\nsection 2 we give an overview of other recent works\nrelated to the Anomaly detection problem, and we\ngive a brief introduction to the Vision Transformer\narchitecture. Then in section 3, a detailed overview\nof the training process is given together with a thor-\nough description of the proposed architecture. The\nobtained results are outlined in section 4 where a\nmore in-depth description of the adopted dataset is\nalso provided. Finally, in section 5 we summarize our\nwork and discuss our ideas for future work.\n2. Related work\nMany diﬀerent approaches have been proposed to\ntackle the problem of Unsupervised anomaly detec-\ntion and segmentation on images, both involving tra-\nditional methods and, in recent years, deep neural\nnetworks which, in most cases involve the use of con-\nOctober 28, 2022 0:43 main\n3\nvolutional modules.\nIn this section, we will focus mainly on the meth-\nods used as a reference by the authors of the MVTec\ndataset12 which also represent our benchmark for\nthe present work. The ﬁrst set of approaches used\nfor Unsupervised Anomaly Detection and based on\nimage reconstruction is represented by GAN-based\nmodels13. A GAN network is typically composed of\ntwo main components, a Generator which starting\nfrom a latent representation tries to generate im-\nages as close as possible to the one present in the\ndataset used, and a Discriminator which receives\nas input both the images generated by the Gener-\nator and the original ones and tries to discriminate\nbetween the two. One way of performing anomaly\ndetection through the use of GANs is to train the\nmodel only with “good” images so that during the\ntesting process it should be able to recognize them\nbut not the anomalous one and therefore it could be\nused to discriminate between data with and without\nanomalies. A diﬀerent approach, which also allows\nperforming Anomaly localization, was proposed by\nSchlegl et al. with the introduction of the AnoGAN\narchitecture14. The main idea behind this model is\nrepresented by the introduction of an additional com-\nponent which is trained to learn the inverse trans-\nformation of the Generator in order to produce the\nlatent representation of the original images, which\ncan then be used to perform a reconstruction of\nsaid images and thus compare it with the respec-\ntive original ones. The main issue with this idea is\nthat the inversion process is very computationally\nexpensive. For this reason, another model, known as\nf-AnoGAN15 (faster AnoGAN) was introduced. This\napproach, compared to the original AnoGAN net-\nwork, provides much faster convergence thanks to\nthe introduction of an additional encoder network\nused to learn a function that maps the original im-\nages into their respective latent space, which makes\nthe expensive process of ﬁnding the inverse of the\ngenerator superﬂuous.\nThe second set of models, which also represents\nthe bulk of the current approaches for unsupervised\nanomaly detection in images, is represented by Con-\nvolutional Autoencoders 16 (AE). The idea behind\nthe AE architecture is to use an encoder to map the\ninputs to a latent space that is much smaller than\nthe original one and which is then used as the in-\nput for a Decoder which tries to learn how to re-\nconstruct the original input starting from this latent\nrepresentation. The assumption is that the model\nshould learn to map only the most important, or\nmore common, features regarding the instances of\nthe dataset leaving out every superﬂuous or spe-\nciﬁc information. Therefore it shouldn’t be able to\nproperly reconstruct anomalies, leading to a greater\ndiﬀerence between input and reconstruction for this\nclass of instances compared to the normal ones. For\nthis reason, one possible approach to anomaly detec-\ntion is to use an Autoencoder, usually trained only\non the normal instances, to obtain a latent repre-\nsentation (which is typically much smaller than the\noriginal one) for each element of the dataset and\nthen apply a clustering algorithm in order to dis-\ncriminate between the good and the anomalous ones.\nThe limit of this approach is that it cannot be used\nfor the localization of the anomalies but only for\nthe detection. Many approaches based on AEs have\nbeen proposed over time with diﬀerent characteris-\ntics related to their structure and type of loss func-\ntions adopted. Recent works speciﬁcally focused on\nAnomaly Segmentation17 in images, showed the ben-\neﬁt of using a structural similarity-based loss (e.g.\nSSIM) to assess the quality of the reconstruction in\nsubstitution, or addition, to the pixel-wise one (e.g.\nMSE) adopted in previous works.\nHereafter we present some additional ap-\nproaches to anomaly segmentation developed in re-\ncent years. Napoletano et al. 18 proposed a region-\nbased CNN architecture, which we will refer to as\nCNN Feature Dictionary for consistency with the\n2021 MVTec paper which we used as a reference\npoint for this work, which extracts the training fea-\ntures from random patches that are cropped out\nof the original images and then uses a K-Means\nclassiﬁer to model their distribution. Since this ap-\nproach provides only a binary decision on whether\nan image contains a defect or not, the classiﬁca-\ntion process must be repeated multiple times over\ndiﬀerent patches of the image in order to obtain a\nspatial anomaly map, which becomes a very com-\nputationally expensive process for large images. A\ndiﬀerent approach19, utilizing an ensemble of ran-\ndomly initialized student networks which are trained\nagainst regression targets obtained from a Teacher\nnetwork pre-trained on anomaly-free data (speciﬁ-\ncally ImageNet), has been proposed under the name\nof student-teacher network. The regions containing a\nOctober 28, 2022 0:43 main\n4\nFig. 1. Vision Transformer architecture b\ndefect are identiﬁed thanks to the fact that the stu-\ndent networks are not able to correctly predict the\nteacher’s descriptor for them, thus yielding larger re-\ngression errors as well as a higher predictive uncer-\ntainty.\nAnother interesting approach has been pre-\nsented with the PIADE architecture20 which focuses\non multi scale feature representation through the\nadoption of a pretrained ResNet-18 followed by a set\nof pyramidal pooling layers which allow the image\nfeature to be analyzed at diﬀerent magniﬁcations.\nLatent Space Autoregression (LSA) 21 repre-\nsents yet another approach to AEs for anomaly de-\ntection, combining the reconstruction loss obtained\nfrom a deep convolutional Autoencoder to the max-\nimization of the likelihood of the latent space rapre-\nsentation through an autoregressive density estima-\ntor in order to minimize the entropy of these repre-\nsentations while preserving the quality of the recon-\nstructed images.\nVariational AEs have also been adopted in works\nregarding the anomaly segmentation problem such\nas the work by Baur et al.22 which focused on the\ndetection of anomalous regions in Brain MR Im-\nages, where they achieved state-of-the-art perfor-\nmance even if not improving by a large margin over\nregular CAEs.\nFurthermore, there are also examples of frame-\nworks trying to combine both AEs and GANs to ex-\nploit the potential of both architecture. One such\nexample is represented by OCGAN 23 which com-\nbines 4 main components in order to attempt to force\nthe model to reconstruct abnormal inputs as normal\nones.The 4 components are namely a denoising AE,\na Latent Discriminator that forces each instance of\nthe latent space to represent an image from the corre-\nsponding class, a Visual discriminator which is used\nto force each produced image to come from the same\nimage space distribution as the given class and ﬁnally\nan Informative-negative Mining component whcih is\nused to actively seek regions of the latent space and\nforce the generator to produce good in-class images\neven for these instances.\nThere are also alternative approaches that com-\npletely bypass the reconstruction step of AEs and\nfocus more on the embedding component. One ex-\namples of this category of approaches is represented\nby the Geometric Transformation framework (GT)\nbImage taken from https://github.com/google-research/vision_transformer\nOctober 28, 2022 0:43 main\n5\n24 which proposes a model trained as a multi class\nclassiﬁer on diﬀerent geometric transformations of\nthe normal images, with the idea that by learning to\ndiscriminate between them the features learned by\nthe model will be very ones which allow to eﬃciently\ndetect novelties at test time.\nA diﬀerent idea is provided by Salehi et. al in\n25 where they train a cloner network that learns to\nmimic the comprehensive behaviour of a larger pre-\ntrained expert network (e.g a VGG-16 network), by\nlearning its knowledge regarding normal data and\ndiscarding all superﬂuous ﬁlters. Anomaly detection\nis then performed by analyzing the discrepancies in\nbehaviour at test time.\nFinally, we present VT-ADL 26 which, to our\nknowledge, is the only approach to this date focusing\non the use of the ViT architecture to tackle the im-\nage anomaly detection problem. VT-ADL performs\nan embedding of each patch obtained from the origi-\nnal image into a latent space in which the positional\ninformation of the patches is also considered. These\nembeddings are then fed into two diﬀerent modules,\nwhich allow the model to perform the anomaly de-\ntection and localization tasks. The ﬁrst of the two\nmodules is represented by a convolutional decoder\nwhich aims to reconstruct the original image start-\ning from the patch embeddings while the second one\nis a Gaussian Mixture Density Network which is used\nto model the distribution of the normal data in the\nlatent space in which the patches are embedded.\n2.1. Vision Transformers\nThe Vision Transformer is a deep neural network\narchitecture proposed by Dosovitskiy et al. 11 as\nan alternative to convolutional-based architectures\nfor computer vision applications. This model builds\nupon the idea of Self-Attention introduced in the\noriginal Transformer paper 27, which has since be-\ncome the model of choice for Natural Language\nProcessing (NLP) Applications, replacing Recurrent\nNeural Networks.\nArchitecture:\nThe Vision Transformer architecture (Fig.1) diﬀers\nfrom the original transformer one in the fact that it\nonly uses the encoder module leaving out the De-\ncoder. The encoder module, which takes as its in-\nput a set of ﬂattened representations of the diﬀerent\npatches composing the image we are trying to ana-\nlyze, consists of a Stack of N identical Layers each\ncontaining two sub-layers: the ﬁrst one is a multi-\nhead self-attention block while the second one is a\nfully connected feed-forward layer. Around each of\nthe 2 blocks, a residual connection is applied, fol-\nlowed by a layer normalization step. The role of the\nencoder module in the ViT architecture is that of\nlearning a correlation between the diﬀerent patches\ncomposing an image. In order to preserve the spa-\ntial information regarding the position of the diﬀer-\nent patches a positional embedding is added to the\npatches representations before feeding them to the\nViT encoder. The positional embedding can be ﬁxed\nor learned alongside the other parameters.\nMulti-head Attention:\nThe attention mechanism can be described as a func-\ntion that maps triplets of vectors (represented by a\nquery (Q) and key-value (K, V) pairs) to an output\nwhich is computed as a weighted sum of the values\nwhere the weights of each value are computed based\non a compatibility relationship between a query and\nthe corresponding key. In Vision Transformers in-\nstead of performing the self-attention step only once\nfor each set of queries, key and value, we project\nthem in h diﬀerent spaces via learned linear projec-\ntions. Attention is then calculated for each of these\ndiﬀerent projections and the ﬁnal outputs are con-\ncatenated and projected again to obtain the ﬁnal\nvalues. The whole self attention calculation process\ncan be summarized by the following equation:\nAttention(Q,K,V ) = softmax(QKT\n√dk\n)V (1)\nThis approach allows the model to consider dif-\nferent representation subspaces when calculating at-\ntention between the diﬀerent parts of the image.\n3. Methods\n3.1. Data preparation\nAs far as data preparation is concerned we tried to\nadopt a very minimal approach. The only way the\nimages themselves are pre-processed is through a re-\nsizing process that brings them all to the same ﬁnal\nsize, which is 128x128 for all the classes. This step\nwas necessary in order to have a consistent number\nof patches when dividing the images in the next steps\nOctober 28, 2022 0:43 main\n6\nFig. 2. Illustration of the masking process performed in the self-attention module of our model. In the example we are\ncalculating the attention values for patch #1, therefore we set the dot product between Query #1 and Key #1 to 0 in\norder to take into account only the attention values of the remaining patches\nFig. 3. Overview of our model in its basic form: the original image is split into patches of the same size which is used\nas the input of a Multi-Layer Perceptron (MLP) in order to get the corresponding embedding. Then, to each patch em-\nbedding, a position embedding is added. The resulting tensor is then processed through a masked ViT encoder module\nin order to obtain the ﬁnal embedding which will be used to reconstruct the respective patch by using an MLP Decoder\nand it was also helpful for reducing the overall com-\nplexity of the model. The only other operation ap-\nplied to the dataset before training is a shuﬄing of\nthe instances in order to avoid any potential bias for\nthe model based on their order.\n3.2. Model description\nThe model on which the work for this paper is based\non the classical Vision transformer architecture pre-\nsented in the previous section.\nIn this work, however, we introduce two very\nimportant changes the aformentioned architecture\nwhich we will describe in detail in the present sec-\ntion. The ﬁrst change is represented by a masking\nprocess by which we try to remove the focus of each\npatch of the original image on its own features, redi-\nrecting the attention to the remaining ones. While\nthe second idea we introduced regards the division of\nthe original images in multi shaped and multi scale\npatches, as opposed to relying only on the traditional\nsquare patches presented in the original work.\nOctober 28, 2022 0:43 main\n7\nFig. 4. Overview of the proposed model using horizontal stripes only, in addition to the square patches d\n3.2.1. Masking process\nWhile many of the image-reconstruction-based mod-\nels for anomaly detection work on the entire image,\nthis approach leads to a very common problem. Since\nthe model uses all the information available in the in-\nput it tends to learn how to reconstruct correctly the\nanomalous images as well as the normal one which,\nof course, is not the desired behavior since the goal\nis to discriminate between the two classes. For this\nreason, the idea behind our proposed method is to\nmask out some of the information of the original im-\nage and use only the remaining data to perform the\nreconstruction. This as we will show, greatly reduces\nthe problem mentioned in the previous paragraph.\nMore speciﬁcally we decided to work on patches rep-\nresenting non-overlapping subsets of the original im-\nage and to reconstruct each of these patches based\nonly on the content of the remaining ones. For this\npurpose, we decided to adopt the ViT architecture\nas a baseline for our model, which as we have previ-\nously shown, allows ﬁnding a correlation between dif-\nferent parts of an image by calculating an attention\nscore between its patches. In its basic form, though,\nthe ViT model provides for each patch an embed-\nding obtained by processing the whole information\nof the original image, including the patch itself. For\nthis reason, we altered the concept of self-attention\nintroduced by the original model in order to mask\nout this piece of information. In order to do so, we\nadded a masking module in the multi head self at-\ntention component of the original architecture which\nforces the dot product between the key generated for\neach patch and the respective query to be set to 0,\nas shown in Fig. 2. In other words, after obtaining\nthe nxn (where n is the number of patches) matrix\nin which the cell in position ij represents the corre-\nlation between patches i and j, we set its diagonal,\nrepresenting the internal information of each patch,\nto 0 in order to use only external information for its\nreconstruction. An overview of the model described\nso far is given in Fig. 3.\nFig. 5. Illustration of the process carried out for the\nconcatenation of the patch embeddings. Two components\nare concatenated together, the ﬁrst one is the embedding\nobtained trough the self attention process performed on\nsquare patches, while the second one is a subset of the\nembedding resulting from the self attention process car-\nried out on strip patches.\nbSource code will be available on Github after manuscript acceptance\nOctober 28, 2022 0:43 main\n8\n3.2.2. Multi shape patch structure\nAnother direction in which we expanded the original\nidea of the ViT model regards the way the patches\nfed to it are obtained from the original image. While\nthe use of square patches is a common choice, it is\nultimately an arbitrary one, for this reason, we in-\ntroduced in our model a set of patches with diﬀerent\nshapes, in particular horizontal and vertical stripes,\neach of which was processed in the same way as the\nsquare ones. The encodings of the diﬀerent types of\npatches have then been concatenated to form the\nﬁnal representation for each patch which has then\nbeen used for its reconstruction. The reconstruction\nprocess involved the use of a simple MLP Decoder\nwhich worked on each patch embedding singularly\nto reproduce the respective patch. This process has\nbeen carried out by ensuring that the masking prop-\nerty of the network was held true, to do so the con-\ncatenation between patches of diﬀerent shapes has\nbeen carried on based on their spatial location in\nthe original image so that each square patch was\nfully contained in the respective horizontal (or verti-\ncal) stripe patch. Furthermore, in order to keep the\nﬁnal embedding size as small as possible, we split\nthe embedding of each stripe into p segments of the\nsame size, where p = N/K (with N=size of the im-\nage and K=size of the square patches) is the number\nof square patches contained in each stripe, and then\nconcatenated each of these segments to the embed-\nding of a diﬀerent patch thus forcing the model to\nlearn speciﬁc information about each of them in dif-\nferent locations of the stripe embedding (Fig. 5). A\nhigh level overview of our ﬁnal model is provided in\nFig. 4.\n4. Evaluation\n4.1. datasets\nTo test our Framework we relied on two popular\ndataset, namely MVTec which is currently one of the\nmost popular and heterogeneous datasets in the con-\ntext of anomaly detection for industrial quality con-\ntrol, and HeadCT which is instead a dataset repre-\nsenting a collection of X-Ray head scans in which the\nanomalies are represented by the presence of hemor-\nrhages. While the use of larger datasets is usually\npreferable, the options available in the context of\nanomaly detection are still very limited. MVTec al-\nready represents a big step forward when it comes\nto size and heterogeneity of datasets in this area of\nresearch compared to the ones that were available\nbefore it and, combined with the HeadCT dataset,\nwe believe they provide and reliable option for the\nevaluation of the proposed approach.\nFig. 6. Sample for each of the classes present in the\nMVTec dataset. For each of them is provided a normal\ninstance (top row), an instance containing an anomaly\n(middle row), and a zoomed view on the defect (bottom\nrow)\n4.1.1. The MVTec Dataset\nThe dataset we used for both the training and test-\ning of our model is the MVTec Anomaly Detection\ndataset12, which has become a common benchmark\nin recent works on anomaly detection and localiza-\ntion. This dataset consists of 3629 training images\nand 1725 testing images divided into 15 classes, ﬁve\nof which represent diﬀerent textures and the remain-\ning ones covering a set of products with heteroge-\nneous characteristics, some of them present a rigid\nstructure, others are deformable or present natural\nvariations in their appearance. Furthermore, the way\nin which the images are captured is also heteroge-\nneous, for some of the classes all the instances belong-\nOctober 28, 2022 0:43 main\n9\ning to the present the product in a roughly aligned\nfashion while for some others a random rotation is\nintroduced. Since grayscale images are also a com-\nmon occurrence in industrial settings three classes\nare made available only as single-channel images.\nThe testing set is also highly heterogeneous in its\ncomposition, providing a wide variety of anomalies.\nIn total 73 diﬀerent types of defects are provided,\n5 for each category on mean. To make the testing\nprocedure possible pixel-accurate labels for all the\ndefective regions in each image are also provided by\nthe authors. An overview of the diﬀerent classes of\ntextures and products in the dataset, together with\nan anomalous example for each of them is provided\nin Fig. 6. Furthermore, in Table 1 the details regard-\ning each class are reported. As we can notice there\nare no anomalous instances in the training set, this\nbecause usually the anomaly detection models are\ntrained only on normal data.\n4.1.2. Head CT Dataset\nAs a further benchmark for our model, we used the\nHead CT dataset 28, which we selected because of\nits medical nature and because of its relatively small\nsize, which allowed us to prove the potential of the\nVision Transformer architecture even in this particu-\nlarly challenging type of setting. The head ct dataset\nis, in fact, composed of a total of 200 images 100 of\nwhich represent head ct of healthy individuals while\nthe remaining 100 represent scans of patients with\na head hemorrhage. For the purpose of this study,\nwe adopted 80 normal images for the training of the\nmodel, while the remaining 120 instances (20 nor-\nmal, 100 with an hemorrhage) were used in the test-\ning process. Some samples representing normal and\nanomalous images from the dataset are reported in\nFig. 7.\nFig. 7. Samples of anomalous (top row) and normal\n(bottom row) images from the Head CT dataset\n4.2. Training setup\nThe training of the model was performed in a self-\nsupervised fashion, using only the normal images\nof the dataset, over a maximum of 3000 epochs\nwith an early stop introduced after epoch 500 which\nwould trigger when the performance of the model\non the validation set didn’t improve in the previ-\nous 50 epochs. The validation set was composed\nof 10% of the images present in the training data\nof the datasets. While often larger percentages are\nused, both MVTec and HeadCT are relatively small\ndatasets, which led us to prefer keeping as much data\nas possible for the training of the model. The total\nloss function we used is obtained by summing the L1\nloss and the negative of the SSIM Similarity, as de-\nﬁned in [29], calculated between the original image\ngiven as input and the reconstructed image returned\nas the output of the model. A formal description of\nthe two functions is given hereafter:\nL1(X, ˆX) =\nh−1∑\ni=0\nw−1∑\nj=0\n|Xij − ˆXij| (2)\nLSSIM (X, ˆX) = − (2µXµˆX + c1)(2σX ˆX + c2)\n(µ2\nX + µ2\nˆX + c1)(σ2\nX + σ2\nˆX + c2)\n(3)\nL(X, ˆX) = L1(X, ˆX) + LSSIM (X, ˆX) (4)\nWhere:\n• X: Is the original image\n• ˆX: Is the reconstructed image\n• h, w: Are the height and width of the image\nin pixel\n• µx: Is mean value of image x\n• σ2\nx: Is variance of image x\n• σxy: Is the covariance of x and y\n• c1 = (k1L)2&c2 = (k2L)2 : Are two vari-\nables used to stabilize the division with weak\ndenominator\n• L: Is the dynamic range of the pixel-values\n(usually 2#bitsperpixel −1)\n• k1&k2: are two costants set to 0.01 and 0.03\nrespectively.\nIt is important to notice that the SSIM function rep-\nresents a similarity measure deﬁned in the interval\n[−1,1] where 1 means that the two images being\ncompared are identical. For this reason, to use it as\na loss function it needs to be negated.\nOctober 28, 2022 0:43 main\n10\nTable 1. MVTec dataset details\nClass Train\n(Normal)\nTest\n(Normal)\nTest\n(Anomaly) Image side\nTextures\nCarpet 280 28 80 1024\nGrid 264 21 57 1024\nLeather 245 32 92 1024\nTile 230 33 84 1024\nWood 247 19 60 1024\nProducts\nBottle 209 20 63 900\nCable 224 58 92 1024\nCapsule 219 23 109 1000\nHazelnut 391 40 70 1024\nMetal nut 220 22 93 700\nPill 267 26 141 800\nScrew 320 41 119 1024\nToothbrush 60 12 30 1024\nTransistor 213 60 40 1024\nZipper 240 32 119 1024\nThe use of an L1 loss instead of a more classical\nMSE loss is motivated by the fact that it reduces\nblurriness and color artifacts in the reconstructed\nimages30.\nThe hyperparameters adopted during the train-\ning process are shown in Table 2. We tried to keep the\nStructure of the TF encoders as shallow as possible\nand actually noticed that increasing the number of\nblocks improved the performance of the model only\nmarginally for the texture classes while for the prod-\nuct classes didn’t help at all. A possible reason for\nthis is that by increasing the depth, the model would\nbecome too complex for the relatively small dataset\nwe used for training and evaluation and thus lead to\noverﬁtting.\nAnother important hyperparameter that needs\nto be selected when adopting a ViT architecture is\nthe size of the patches in which the image needs to\nbe divided. Following the original ViT paper with\nadopted 16x16 patches as the baseline for our model,\nwhich empirical tests conﬁrmed to be a proper value\nfor the task considered. In general the idea to keep in\nmind during patch size selection is that Vision Trans-\nformers tend to work better with a high number of\nsequences as their input, for this reason the size of the\npatches needs to be kept relatively small compared to\nthe image size. Furthermore in the context of the pro-\nposed approach it was important not to use patches\nthat are too small, as this would make much more\nfrequent the presence of anomalies crossing multiple\npatches, which are easier to reconstruct since they\ncan be inferred from the surroundings of the current\npatch analyzed and, therefore are more diﬃcult to\ndetect. The size of the short side of the stripes was\nselected to match the square patches sizes, while the\nlong side is determined by the image size.\nFig. 8. Illustration of the thresholded AUROC metric.\nThe red line represents the selected threshold while the\nyellow region is the area taken in consideration for the\nevaluation of the model.\n4.3. Metrics\nFor the evaluation of our model, since our objective\nis to assess its anomaly segmentation capabilities, we\nopted to use the Area Under the ROC Curve (AU-\nROC), which plots the False Positive Rate versus the\nTrue Positive Rate and is a very commonly used met-\nric for this type of problems. In order to obtain con-\nsistent results with those presented by the authors\nof the MVTec dataset, the values of the metric are\ncomputed up to a False Positive Rate of 0.3. The rea-\nson behind this choice is that thresholds that yield\nOctober 28, 2022 0:43 main\n11\nTable 2. model hyperparameters\nClasses LR Batch Img size Patch size Stripe size Emb. size #Heads #Blocks\nTextures 1eˆ-4 64 128x128 16x16 128x16 128 4 2\nProducts 1eˆ-4 64 128x128 16x16 128x16 128 4 1\nto a high FPR lead to meaningless segmentation re-\nsults, especially for industrial scenarios where such\nresults would lead to wrongly rejecting products not\npresenting any defects. An illustration of the thresh-\nolding process is provided in Fig.8\n4.4. Results\nAll the values presented in this section for our mod-\nels have been obtained by ﬁrst generating the re-\nconstructed images, patch by patch, for each of the\ninstances in the test set of the MVTec dataset, each\nof them has then been compared to the respective\noriginal image by applying a pixel-wise MSE loss\n(the deltas over the three channels of each pixel have\nbeen summed), in order to obtain a heatmap that\nhighlighted the largest diﬀerences between the two,\nthus revealing the potentially anomalous regions. A\nGaussian ﬁlter has also been applied to the map to\nsmooth out anomalous regions and reduce noise.\nIn Table 3 the results for our ablation study,\nin which we compare the performance of our model\nwhen applying the masking process and diﬀerent\ncombinations of patch shapes, are reported. As we\ncan see the approach which relies solely on the tradi-\ntional square patches is outperformed by every other\nmethod using a combination of diﬀerent shapes. In\nparticular, we show that the best performing ap-\nproach is the one represented by a combination of the\nsquare patches with horizontal, or vertical stripes,\nwhich leads to an improvement in the pixelwise AU-\nROC value of more than 4% compared to the baseline\napproach. As we can see the orientation of the stripes\ncombined with the square patches didn’t aﬀect the\noverall performance of the model, which achieved,\non mean, the same performance when using horizon-\ntal or vertical ones. On the other hand, we can see\nhow using both orientations together leads to a de-\ncrease in the performance of the model. Our guess\nis that in this scenario the model becomes too com-\nplex compared to the relatively small dataset we used\nand therefore becomes too specialized on the train-\ning set, leading to overﬁtting. Finally, we can notice\nhow completely removing the square patches and re-\nlying only on the horizontal and vertical stripes also\naﬀects the model performance negatively. A possi-\nble explanation for this behavior is that the square\npatches are able to provide some more locally spe-\nciﬁc information that the model needs to perform\nthe reconstructions of the single patches eﬀectively.\nIn Table 4 we provide a comparison between our\nmethods and other approaches. In particular, we fo-\ncus on the methods proposed in the MVTec paper31\nas our benchmarks and on VT-ADL as the only other\napproach using a ViT architecture for anomaly local-\nization. For the latter, the results shown have been\ncalculated by us as the original paper didn’t provide\nthe values for the AUROC metric, while for every\nother model the result has been gathered from the\noriginal paper. As we can see our approach vastly\noutperforms the previous method based on ViTs rep-\nresented by VT-ADL, in particular, by referring back\nto Table 3, we can see that even the approach re-\nlying solely on square patches still achieves better\nresults than VT-ADL by improving over its results\nby almost an 12% margin, therefore proving the ef-\nfectiveness of the masking process introduced in the\nSelf-attention module of our model. As for the re-\nmaining models, we show that while our approach\nperforms worst than the best one from the MVTec\npaper, represented by the student-teacher architec-\nture (7.8% AUROC score diﬀerence), it outperforms\nevery other method by a margin going from 1% to\n37%. One important aspect to notice is that the two\ntop-performing methods presented in the MVTec pa-\nper, namely the student-teacher and the Feature Dic-\ntionary models which are the only ones exceeding\nan mean AUROC score of 0 .7, both rely on feature\nextractors pre-trained on much larger datasets such\nas the popular imagenet one 32 while our model is\ntrained from scratch on the MVTec dataset making\nit the best performing model not relying on extra\ndata for training and thus showing the possibility of\nadopting transformer-based models, typically consid-\nered very heavy, even to scenarios where we have a\nrelatively small amount of data available.\nFinally, in Fig. 9, we provide some qualitative re-\nsults of our model by showing a comparison between\nOctober 28, 2022 0:43 main\n12\nTable 3. results of the ablation study in which we show the eﬀect of using diﬀerent combinations of patch shapes\non the model performance. All the values reported representing the normalized area under the ROC curve up to an\nmean FPR per-pixel of 30%.\nClass Only Squares\n(No Mask)\nOnly Squares\n(Masked) Squares + Rows Squares + Cols Rows + Cols\nSquares +\nRows +\nCols\nCarpet 0.495 0.510 0.712 0.723 0.755 0.661\nGrid 0.814 0.835 0.884 0.883 0.800 0.817\nLeather 0.734 0.792 0.976 0.905 0.723 0.723\nTile 0.727 0.754 0.771 0.772 0.659 0.690\nWood 0.701 0.757 0.836 0.851 0.808 0.840\nBottle 0.796 0.812 0.850 0.847 0.821 0.842\nCable 0.683 0.715 0.701 0.701 0.753 0.700\nCapsule 0.863 0.895 0.891 0.885 0.889 0.893\nHazelnut 0.927 0.951 0.953 0.945 0.951 0.942\nMetal nut 0.712 0.721 0.773 0.779 0.865 0.806\nPill 0.815 0.835 0.852 0.852 0.806 0.850\nScrew 0.770 0.818 0.901 0.905 0.903 0.903\nToothbrush 0.926 0.949 0.975 0.966 0.967 0.971\nTransistor 0.848 0.855 0.841 0.860 0.866 0.865\nZipper 0.783 0.805 0.750 0.770 0.634 0.720\nMean 0.773 0.8 0.844 0.843 0.814 0.815\nTable 4. Normalized area under the ROC curve up to an mean false positive rate per-pixel of 30% for each dataset\ncategory. The values in bold represent the best scores overall, while the underlined ones represent the best scores between\nthe models not using extra data.\nClass f-anoGan Feature dictionary Student teacher l2-AE SSIM-AE Texture inspection Variation model VT-ADL Ours\nCarpet 0.251 0.943 0.927 0.287 0.365 0.874 0.162 0.549 0.712\nGrid 0.550 0.872 0.974 0.741 0.820 0.878 0.488 0.569 0.884\nLeather 0.574 0.819 0.976 0.491 0.356 0.975 0.381 0.817 0.976\nTile 0.180 0.854 0.946 0.174 0.156 0.314 0.304 0.589 0.771\nWood 0.392 0.720 0.895 0.417 0.404 0.723 0.408 0.682 0.836\nBottle 0.422 0.953 0.943 0.528 0.624 0.454 0.667 0.687 0.850\nCable 0.453 0.797 0.866 0.510 0.302 0.512 0.423 0.751 0.701\nCapsule 0.362 0.793 0.952 0.732 0.799 0.698 0.843 0.615 0.891\nHazelnut 0.825 0.911 0.959 0.879 0.847 0.955 0.802 0.926 0.959\nMetal nut 0.435 0.862 0.979 0.572 0.539 0.135 0.462 0.711 0.773\nPill 0.504 0.911 0.955 0.690 0.698 0.440 0.666 0.748 0.852\nScrew 0.814 0.738 0.961 0.867 0.885 0.877 0.697 0.771 0.901\nToothbrush 0.749 0.916 0.971 0.837 0.846 0.712 0.775 0.878 0.975\nTransistor 0.372 0.527 0.566 0.657 0.562 0.363 0.601 0.689 0.860\nZipper 0.201 0.921 0.964 0.474 0.564 0.928 0.209 0.683 0.750\nMean 0.472 0.836 0.922 0.590 0.584 0.656 0.526 0.683 0.844\nthe original images and ground truth anomaly maps\nwith the reconstructed images and anomaly maps\ngenerated by our model. As we can see the model\nis able to eﬀectively mask out the smaller anomalies\nfrom the reconstructed images, leaving only small ar-\ntifacts in their places. As for larger anomalies that\nare spread through diﬀerent regions of the original\nimages, the model is usually not able to completely\nremove them as it can infer their structure from the\nsurrounding patches. Nonetheless, in many cases, the\ndefective part has a more “washed out” appearance\nin the reconstructed image which allows for its local-\nization.\nFurthermore, we present the results for the\nHead CT - hemorrhage dataset. In particular, since\nthis dataset doesn’t provide masks for the instances\nanomalies, we use as a comparison metric the images\nROCAUC score. We report our results, together with\nthe ones presented by Salehi et al.25 which, as far as\nwe know, are the best ones available online for the\nheadct dataset, in Tab.5. As we can see our model,\neven in its most basic conﬁguration, achieves results\ncomparable to the State of the Art, while when hori-\nzontal stripes are added it is able to surpass the other\npresented approaches by a signiﬁcant margin show-\ning the eﬀectiveness of the presented approach even\nOctober 28, 2022 0:43 main\n13\nFig. 9. In order from left to right, we have: (1) The original image used as input, (2) the ground truth mask showing the\nlocation of the anomaly in the original image, and (3-11) the anomaly maps generated by our model and the competition\nfor scenarios when the data available is very limited.\nThe downside, however, is that the increased com-\nplexity of the model appears to make it less stable\nas we can observe from the higher variance charac-\nterizing the results achieved by this conﬁguration.\nTable 5. AUROC for anomaly detection on Head CT\ndataset.\nAUROC Head CT (%)\nOCGAN23 51.20±0.358\nLSA21 81.67±3.626\nGT 24 49.5 ±3.873\nMKD25 80.4 ±0.006\nMeTAL (Squares Only) 81.35±1.153\nMeTAL (Squares+Rows) 86.32±4.03\nTable 6. Comparison of the number of parame-\nters deﬁning the compared models architectures\n# of parameters\nl2 - AE / SSIM-AE 1.2M\nf-AnoGAN 24.6M\nFeature dictionary 11.5M\nStudent teacher 26M\nVT-ADL 25M\nMKD 15M\nOurs (Products) 17.8M\nOurs (textures) 26.3M\nFor completeness we are also providing a com-\nparison of the model sizes for the diﬀerent ap-\nproaches, expressed in number of parameters deﬁn-\ning their structure (Tab. 6). As we can see the pro-\nposed approach size is generally comparable to the\nones of the other frameworks, excluding the most\nsimple ones being represented by the two autoen-\ncoders variations which, however, achieve very poor\nperformances on the selected dataset.\n5. Conclusions and Future Work\nIn the present paper, we investigated the use of a\nFramework based on the Self-Attention mechanism\nintroduced by the Vision Transformer model. In par-\nticular, we proposed an image Inpainting approach to\nanomaly detection which leverages the ability of the\nViT encoder to ﬁnd correlations between diﬀerent\nregions of a given image in order to reconstruct each\nof them based only on the information contained in\nthe surrounding ones. Furthermore, we have shown\nthat the model’s performance is aﬀected positively\nby the use of heterogeneous shapes for the subsets\ninto which the original image is split. Our opinion\nis that this approach allows the model to learn cor-\nrelations between patches at diﬀerent scales of the\nimage, therefore increasing the quality of the gen-\nOctober 28, 2022 0:43 main\n14\nerated reconstructions. Finally, we have shown how\nthe ViT model, while usually considered a heavy ar-\nchitecture, can be used eﬀectively even on a rela-\ntively small dataset without the need to rely on ex-\ntra data from external sources, achieving the best\nperformance compared to other model’s trained in a\nsimilar setting on the MVTec dataset.\nNonetheless, as future work, we believe it would\nbe interesting to explore our model’s capabilities\nwhen pre-trained on a larger dataset before ﬁne-\ntuning it on the ﬁnal task’s data and we would also\nlike to investigate the possibility of a more general\napproach to the multi-scale patch acquisition we in-\ntroduced in this work.\nAs another line of reasearch we believe it would\nbe also worth investigating the adoption of ideas in-\ntroduced in other recent works present in the lit-\nerature, such as [33, 34, 35, 36], in order to try to\nfurther improve the performance of the proposed ar-\nchitecture.\nFurthermore, we believe that the main limita-\ntion of the proposed approach regards the ability to\nhandle large scale anomalies, since these can be in-\nferred from the context of the image even when no\ninformation is available for the speciﬁc patch we are\nreconstructing. For this reason, in future works, we\nwould like to investigate more soﬁsticated masking\napproaches that allow for a better generalization of\nthe model to diﬀerent anomalies scales. Finally in the\nfuture, we will also extend the our idea to process 3D\napplications 37.\nAcknowledgements\nThis work was partially supported by the ONRG\nproject N62909-20-1-2075 Target Re-Association for\nAutonomous Agents (TRAAA).\nReferences\n1. N. Kozamernik and D. Bracun, Visual inspection\nsystem for anomaly detection on ktl coatings using\nvariational autoencoders, Procedia CIRP 93 (2020)\n1558–1563.\n2. C. Piciarelli, D. Avola, D. Pannone and G. L. Foresti,\nA vision-based system for internal pipeline inspec-\ntion, IEEE Transactions on Industrial Informatics\n15(6) (2019) 3289–3299.\n3. K. Tout, F. Retraint and R. Cogranne, Automatic\nvision system for wheel surface inspection and mon-\nitoring, ASNT Annual Conference 2017 , (ANST,\n2017), pp. 207–216.\n4. C. Han, L. Rundo, K. Murao, T. Noguchi,\nY. Shimahara, Z. Milacski, S. Koshino, E. Sala,\nH. Nakayama and S. Satoh, Madgan: unsupervised\nmedical anomaly detection gan using multiple adja-\ncent brain mri slice reconstruction, BMC Bioinfor-\nmatics 22 (2021) p. 31.\n5. O. M. Manzanera, S. K. Meles, K. L. Leenders, R. J.\nRenken, M. Pagani, D. Arnaldi, F. Nobili, J. Obeso,\nM. R. Oroz, S. Morbelli and N. M. Maurits, Scaled\nsubproﬁle modeling and convolutional neural net-\nworks for the identiﬁcation of parkinson’s disease\nin 3d nuclear imaging data, International Journal\nof Neural Systems 29(09) (2019) p. 1950010, PMID:\n31046514.\n6. K. Pogorelov, K. R. Randel, C. Griwodz, S. L. Es-\nkeland, T. de Lange, D. Johansen, C. Spampinato,\nD.-T. Dang-Nguyen, M. Lux, P. T. Schmidt et al. ,\nKvasir: A multi-class image dataset for computer\naided gastrointestinal disease detection, Proceedings\nof the 8th ACM on Multimedia Systems Conference ,\n(Association for Computing Machinery, 2017), pp.\n164–169.\n7. A. H. Ansari, P. J. Cherian, A. Caicedo, G. Naulaers,\nM. De Vos and S. Van Huﬀel, Neonatal seizure de-\ntection using deep convolutional neural networks,In-\nternational Journal of Neural Systems 29(04) (2019)\np. 1850011, PMID: 29747532.\n8. C. Piciarelli, P. Mishra and G. L. Foresti, Super-\nvised anomaly detection with highly imbalanced\ndatasets using capsule networks, International Jour-\nnal of Pattern Recognition and Artiﬁcial Intelligence\n35(08) (2021) p. 2152010.\n9. Z. Chen, C. K. Yeo, B. S. Lee and C. T.\nLau, Autoencoder-based network anomaly detec-\ntion, 2018 Wireless Telecommunications Symposium\n(WTS), (IEEE, 2018), pp. 1–5.\n10. H. Zenati, C. S. Foo, B. Lecouat, G. Manek and V. R.\nChandrasekhar, Eﬃcient gan-based anomaly detec-\ntion, CoRR abs/1802.06222 (2018).\n11. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit and\nN. Houlsby, An image is worth 16x16 words: Trans-\nformers for image recognition at scale, International\nConference on Learning Representations , (OpenRe-\nview.net, 2021).\n12. P. Bergmann, M. Fauser, D. Sattlegger and C. Ste-\nger, Mvtec ad — a comprehensive real-world\ndataset for unsupervised anomaly detection, 2019\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , (IEEE, 2019), pp.\n9584–9592.\n13. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville and Y. Ben-\ngio, Generative adversarial networks, Advances in\nNeural Information Processing Systems 3 (2014)\n2672–2680.\n14. T. Schlegl, P. Seeb¨ ock, S. M. Waldstein, U. Schmidt-\nOctober 28, 2022 0:43 main\n15\nErfurth and G. Langs, Unsupervised anomaly detec-\ntion with generative adversarial networks to guide\nmarker discovery, International conference on in-\nformation processing in medical imaging , Springer,\n(Springer, 2017), pp. 146–157.\n15. T. Schlegl, P. Seeb¨ ock, S. Waldstein, G. Langs and\nU. Schmidt-Erfurth, f-anogan: Fast unsupervised\nanomaly detection with generative adversarial net-\nworks, Medical Image Analysis 54 (2019) 30–44.\n16. I. Good-\nfellow, Y. Bengio and A. Courville, Deep Learning\n(MIT Press, 2016). www.deeplearningbook.org.\n17. P. Bergmann., S. L¨ owe., M. Fauser., D. Sattleg-\nger. and C. Steger., Improving unsupervised defect\nsegmentation by applying structural similarity to\nautoencoders, Proceedings of the 14th International\nJoint Conference on Computer Vision, Imaging and\nComputer Graphics Theory and Applications - Vol-\nume 5: VISAPP, , INSTICC, (SciTePress, 2019), pp.\n372–380.\n18. P. Napoletano, F. Piccoli and R. Schettini, Anomaly\ndetection in nanoﬁbrous materials by cnn-based self-\nsimilarity, Sensors (Basel, Switzerland) 18 (2018)\n209–209.\n19. P. Bergmann, M. Fauser, D. Sattlegger and C. Ste-\nger, Uninformed students: Student-teacher anomaly\ndetection with discriminative latent embeddings,\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, (IEEE, 2020),\npp. 4183–4192.\n20. P. Mishra, C. Piciarelli and G. L. Foresti, A neu-\nral network for image anomaly detection with deep\npyramidal representations and dynamic routing, In-\nternational Journal of Neural Systems 30(10) (2020)\np. 2050060, PMID: 32938260.\n21. D. Abati, A. Porrello, S. Calderara and R. Cuc-\nchiara, Latent space autoregression for novelty de-\ntection, 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , -, (IEEE\nComputer Society, Los Alamitos, CA, USA, jun\n2019), pp. 481–490.\n22. C. Baur, B. Wiestler, S. Albarqouni\nand N. Navab, Deep autoencoding models for unsu-\npervised anomaly segmentation in brain mr images,\nBrainlesion: Glioma, Multiple Sclerosis, Stroke and\nTraumatic Brain Injuries , (”Springer International\nPublishing”, Cham, 2019), pp. ”161–169”.\n23. P. Perera, R. Nallapati and B. Xiang, Ocgan: One-\nclass novelty detection using gans with constrained\nlatent representations, 2019 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR) - (2019) 2893–2901.\n24. I. Golan and R. El-Yaniv, Deep anomaly detection\nusing geometric transformations, Advances in Neu-\nral Information Processing Systems , eds. S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi and R. Garnett31, (Curran Associates, Inc.,\n2018).\n25. M. Salehi, N. Sadjadi, S. Baselizadeh, M. H. Ro-\nhban and H. R. Rabiee, Multiresolution knowl-\nedge distillation for anomaly detection, Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , (IEEE, 2021), pp. 14902–\n14912.\n26. P. Mishra, R. Verk, D. Fornasier, C. Piciarelli and\nG. L. Foresti, VT-ADL: A vision transformer net-\nwork for image anomaly detection and localization,\n30th IEEE/IES International Symposium on Indus-\ntrial Electronics (ISIE) , (IEEE, June 2021).\n27. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez,  L. Kaiser and I. Polosukhin,\nAttention is all you need, Advances in neural infor-\nmation processing systems, (Curran Associates, Inc.,\n2017), pp. 5998–6008.\n28. F. Kitamura, Head ct - hemorrage dataset\nhttps://www.kaggle.com/felipekitamura/head-ct-\nhemorrhage, (2018).\n29. Z. Wang, A. Bovik, H. Sheikh and E. Simoncelli, Im-\nage quality assessment: from error visibility to struc-\ntural similarity, IEEE Transactions on Image Pro-\ncessing 13(4) (2004) 600–612.\n30. H. Zhao, O. Gallo, I. Frosio and J. Kautz, Loss func-\ntions for image restoration with neural networks,\nIEEE Transactions on Computational Imaging 3(1)\n(2017) 47–57.\n31. P. Bergmann, K. Batzner, M. Fauser, D. Sattlegger\nand C. Steger, The mvtec anomaly detection dataset:\na comprehensive real-world dataset for unsupervised\nanomaly detection, International Journal of Com-\nputer Vision 129(4) (2021) 1038–1059.\n32. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and\nL. Fei-Fei, Imagenet: A large-scale hierarchical im-\nage database, 2009 IEEE conference on computer\nvision and pattern recognition , Ieee, (IEEE, 2009),\npp. 248–255.\n33. M. Ahmadlou and H. Adeli, Enhanced probabilistic\nneural network with local decision circles: A robust\nclassiﬁer, Integrated Computer-Aided Engineering17\n(2010) 197–210.\n34. D. Pereira, M. Piteri, A. Souza, J. Papa and H. Adeli,\nFema: A ﬁnite element machine for fast learning,\nNeural Computing and Applications 32 (2020) p.\n6393–6404.\n35. K. M. R. Alam, N. Siddique and H. Adeli, A dynamic\nensemble learning algorithm for neural networks,\nNeural Comput. Appl. 32 (jun 2020) p. 8675–8690.\n36. M. H. Raﬁei and H. Adeli, A new neural dynamic\nclassiﬁcation algorithm, IEEE Transactions on Neu-\nral Networks and Learning Systems 28(12) (2017)\n3074–3083.\n37. P. Bergmann, X. Jin, D. Sattlegger and C. Ste-\nger, The MVTec 3d-AD dataset for unsupervised\n3d anomaly detection and localization, Proceedings\nof the 17th International Joint Conference on Com-\nputer Vision, Imaging and Computer Graphics The-\nory and Applications, 5, (SciTePress, 2022), pp. 202–\n213.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7412281036376953
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7303794622421265
    },
    {
      "name": "Transformer",
      "score": 0.5801543593406677
    },
    {
      "name": "Computer vision",
      "score": 0.5516501665115356
    },
    {
      "name": "Anomaly detection",
      "score": 0.5250530242919922
    },
    {
      "name": "Image (mathematics)",
      "score": 0.49034810066223145
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4449404180049896
    },
    {
      "name": "Voltage",
      "score": 0.0771576464176178
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129043915",
      "name": "University of Udine",
      "country": "IT"
    }
  ]
}