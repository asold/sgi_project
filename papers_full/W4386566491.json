{
  "title": "Lexical Semantics with Large Language Models: A Case Study of English “break”",
  "url": "https://openalex.org/W4386566491",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2152396309",
      "name": "Erika Petersen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2114426036",
      "name": "Christopher Potts",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2087017553",
    "https://openalex.org/W2485454075",
    "https://openalex.org/W1867797957",
    "https://openalex.org/W597941594",
    "https://openalex.org/W3081045913",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W2146191853",
    "https://openalex.org/W4289793359",
    "https://openalex.org/W1575172693",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3146681443",
    "https://openalex.org/W2039383740",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W2542768043",
    "https://openalex.org/W2963850840",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4287632625",
    "https://openalex.org/W2119325477",
    "https://openalex.org/W1559871997",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2740540575",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2905623858",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3115097306",
    "https://openalex.org/W2802425082",
    "https://openalex.org/W3177005832",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2102904575",
    "https://openalex.org/W2115792525",
    "https://openalex.org/W73737616",
    "https://openalex.org/W2103265692",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4210601636",
    "https://openalex.org/W3156782505",
    "https://openalex.org/W4313450446",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2953872833",
    "https://openalex.org/W3156194904",
    "https://openalex.org/W2506737917",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2049151786",
    "https://openalex.org/W4243158342",
    "https://openalex.org/W1567365482",
    "https://openalex.org/W4294367149",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Large neural language models (LLMs) can be powerful tools for research in lexical semantics. We illustrate this potential using the English verb “break”, which has numerous senses and appears in a wide range of syntactic frames. We show that LLMs capture known sense distinctions and can be used to identify informative new sense combinations for further analysis. More generally, we argue that LLMs are aligned with lexical semantic theories in providing high-dimensional, contextually modulated representations, but LLMs’ lack of discrete features and dependence on usage-based data offer a genuinely new perspective on traditional problems in lexical semantics.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 490–511\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nLexical Semantics with Large Language Models:\nA Case Study of English break∗\nErika Petersen\nStanford University\nepetsen@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nAbstract\nLarge neural language models (LLMs) can be\npowerful tools for research in lexical semantics.\nWe illustrate this potential using the English\nverb break, which has numerous senses and\nappears in a wide range of syntactic frames.\nWe show that LLMs capture known sense dis-\ntinctions and can be used to identify informa-\ntive new sense combinations for further analy-\nsis. More generally, we argue that LLMs are\naligned with lexical semantic theories in provid-\ning high-dimensional, contextually modulated\nrepresentations, but LLMs’ lack of discrete fea-\ntures and dependence on usage-based data of-\nfer a genuinely new perspective on traditional\nproblems in lexical semantics.\n1 Introduction\nPater (2019) builds a compelling case that linguis-\ntic and neural network research have great potential\nfor common ground and common cause. His case\nhas only grown stronger in recent years, with the\narrival of large neural language models (LLMs)\nthat provide semantically rich, contextual represen-\ntations (McCann et al., 2017; Peters et al., 2018;\nRadford et al., 2018; Devlin et al., 2019).\nIn this paper, we argue that LLMs are power-\nful devices for studying lexical semantics in ways\nthat can deeply inform linguistic theory. We illus-\ntrate this with a detailed case study of the lexical\nsemantics of the English verb break, building on\na richly annotated dataset from Petersen (2020)\nand drawing on methods from prior work in this\narea (Camacho-Collados and Pilehvar, 2018; Ten-\nney et al., 2019; Garí Soler et al., 2019; Reif et al.,\n2019; Wiedemann et al., 2019; Branco et al., 2020;\nNair et al., 2020; Li and Joanisse, 2021; Loureiro\net al., 2021; Trott and Bergen, 2021; Apidianaki,\n2022; McCrae et al., 2022). Break has long been\n∗ Data and code available at https://github.com/\nepetsen/break-llms.\ncentral to theoretical work in lexical semantics be-\ncause it has a staggering range of senses that appear\nto be systematically related to its argument struc-\nture. Our central empirical finding is that LLM\nrepresentations capture many of these known sense\ndistinctions and can be used to identify new sense\ncombinations for further analysis.\nWe use these findings as a chance to reflect on\nthe core theoretical commitments of lexical seman-\ntics as they pertain to LLM-based investigations.\nOur discussion is centered around the three tenets\nof lexical semantics given in Table 1: lexical repre-\nsentations are high dimensional, contextually mod-\nulated, and include discrete features.\nThe high dimensionality property is not phrased\nas a direct claim in the literature as far as we know,\nbut it reflects the practice of linguists, who iden-\ntify numerous interacting features of lexical items.\nSection 2 offers a summary picture for break. Sim-\nilarly, discreteness is often assumed by linguists\nworking in the broadly generative tradition. For\nour purposes, the key question is whether there are\nany features that are discrete, since LLMs do not\nnaturally support having such features.\nContextual modulation is a direct claim. We\ntrace the origins to Dowty (1976, 1979), who ar-\ngues that aspectual analyses need to include at\nleast the entire verb phrase (see also Kratzer 1996).\nBorer (2005a,b, 2013) pushes this further, arguing\nthat open-class lexical items are “tantamount to\nraw material, ‘stuff’ which is poured into the struc-\ntural mould to be assigned grammatical properties”\n(2005a, p. 108). On this view, lexical items are\nmostly unvalued discrete feature representations\nthat are fleshed out and modulated by the environ-\nment in which they appear; there may be a stock\nof identifiable lexical items, but they are highly ab-\nstract, with almost unlimited potential to become\ndifferent items in different contexts.\nA similar view is taken by work in the Generative\nLexicon (Pustejovsky, 1991, 1995), which posits\n490\nLinguistics Static vectors LLMs\nHigh dimensionality: Lexical semantic entries consist of\nmany features.\nYes Yes Yes\nContextual modulation: A word sense will be influenced by\nits immediate morphosyntactic context as well as the broader\ncontext of use.\nYes No Yes\nDiscreteness: The features in lexical semantic entries are\ndiscrete and highly structured.\nYes No No\nTable 1: Core tenets. Our focus is in particular on the relationship between ‘Linguistics’ and ‘LLMs’ in this table.\nan extensible lexicon that is “open-ended in nature\nand accounts for the novel, creative, uses of words\nin a variety of contexts by positing procedures for\ngenerating semantic expressions for words on the\nbasis of particular contexts” (Pustejovsky, 2006).\nThis also aligns with Clark’s (1997) rejection of the\n“Dogma of Sense Selection”, which says “Listen-\ners determine an enumerable set of senses for each\nexpression, and in understanding what a speaker\nmeans, they select the appropriate sense from that\nset.” For Clark, lexical items are highly malleable\nand constrained mainly by what the discourse par-\nticipants can reliably communicate with each other\n(see also Clark and Clark 1979; Searle 1980). On\nall these views, lexical items are highly abstract\nobjects that can be realized in very diverse ways.\nThe field of NLP has a complex relation to our\ntenets. Early work on symbolic grammars in NLP\nwas clearly aligned with all the tenets. The Gener-\native Lexicon is a prominent example and proved\ninfluential in linguistics and NLP. When distribu-\ntional methods first became central to NLP, the\ndominant mode of lexical representation involved\nstatic vector representations. These representations\nalign with the consensus in linguistics only regard-\ning high dimensionality, as we discuss in Section 4.\nLLMs have changed NLP’s relationship to lexi-\ncal semantics considerably. With LLMs, we have\na strong commitment to high dimensionality and\ncontextual modulation and a denial of discreteness\n(Section 5). The points of agreement present a\nsignificant opportunity for linguists and NLP re-\nsearchers to collaborate, as we hope our case study\nshows. The points of disagreement seem also to be\nopportunities for people to take new perspectives.\nWe argue in particular that the facts surrounding\nbreak should lead linguists to reconsider their com-\nmitment to discreteness and embrace a more fluid,\nusage-based foundation for semantic theory.\n2 English Break\nEnglish break is one of the best studied lexical\nitems in lexical semantics, for a few reasons. First,\nit is a canonical instance of a change-of-state verb\nthat undergoes the causative alternation:\n(1) The linguist broke the window\n(2) The window broke.\nIn fact, alternating change-of-state verbs are re-\nferred to as break-verbs (Acedo-Matellán and Ma-\nteu, 2014; Fillmore, 1970; Levin, 2017; Majid et al.,\n2008). The intransitive variant of the causative\nalternation (2) is analyzed in terms of the unac-\ncusativity hypothesis (Perlmutter, 1978; Burzio,\n1986; Levin and Rappaport Hovav, 1995), which\nsays that the subjects in these cases are underly-\ningly internal arguments to the verb, bearing more\ntheme-like semantic roles, and have been promoted\nto subject position to fulfill a subjecthood require-\nment. Research on break has also contributed to\nthe study of the lexical properties of unaccusative\nverbs (Levin and Rappaport Hovav, 1995).\nSecond, break can take on a wide array of senses.\nTable 2 provides a partial list; we cannot hope to be\ncomprehensive (there may not even be a fixed stock\nof senses; Section 5), but our examples convey the\nnature of the attested variation.\nThird, the sense distinctions interact with the\ncausative alternation. Whereas senses 1–4 all al-\nternate, senses 5–11 are all strictly transitive. The\nnon-alternating senses of break have informed the\ndebate about which variant of the causative alter-\nnation (if any) is basic and which is derived (e.g.\nLevin and Rappaport Hovav, 1995; Alexiadou et al.,\n2006; Piñón, 2001). Though the debate is still\nunsettled, it has evinced that participation in the\ncausative alternation is not a property of the verb\nitself, but of the verb in combination with its theme\nargument (Petersen, 2020; Spalek, 2012), just as\n491\nFrame Sense\n1. break the vase shatter\n2. break the computer render inoperable\n3. break the news reveal\n4. break the silence interrupt\n5. break the record surpass\n6. break the code decipher\n7. break the law violate\n8. break the habit end\n9. break the horse tame\n10. break a $10 bill make change\n11. break the fall lessen\n12. the weather broke changed\n13. the day broke began\n(a) Uses without particles/predicates.\nFrame Sense\n14. break off the engagement end\n15. break out begin\n16. break out of jail escape\n17. break out in hives get\n18. break into the building intrude\n19. break down the problem analyze\n20. break down the proteins decompose\n21. break in enter\n22. break in interrupt\n23. break free escape\n24. break even profit = loss\n25. break forth emerge\n26. break to the right turn\n(b) Uses with particles/predicates.\nTable 2: Senses for break. A comprehensive account of senses may not be possible (Section 5.3).\nwith telicity and other aspectual properties (Dowty,\n1976, 1979; Borer, 2005b).\nPrior work has sought to capture the obligato-\nrily transitive nature of some of these senses by\nappeal to a thematic role requirement: break in\ncombination with its internal argument determines\nthe range of semantic roles – agent, instrument,\nor natural force – that the subject of a transitive\nbreak frame may bear (Rappaport Hovav and Levin,\n2012), and some frames require their subjects to\nbe agentive (Levin and Rappaport Hovav, 1995;\nPiñón, 2001; Alexiadou et al., 2006; Schäfer, 2008).\nSince necessarily agentive subjects cannot be left\nunexpressed, these frames do not show intransitive\nvariants. However, this cannot be the full story, as\nthere are some obligatorily transitive break frames\nwhere the subject need not be an agent but which\nnonetheless do not alternate, like the cushion broke\nher fall vs. *the fall broke (Petersen, 2020).\nIn addition, examples like break the record\n(sense 5) and break the code (sense 6) may be\ngraded or uncertain in regard to their participa-\ntion in the causative alternation. They are often\nassumed not to have intransitive uses (Levin and\nRappaport Hovav, 1995; Piñón, 2001; Alexiadou\net al., 2006; Schäfer, 2008; Rappaport Hovav and\nLevin, 2012), but there are attested cases like the\nfollowing that suggest this is a point of variation.\n(3) Almost sixty years later, Frank Rowlett, a\ncryptologic pioneer and head of the “Purple”\nteam, remembered that historic day when the\ncode broke.\n(4) The Guinness World Record broke, our fur-\nniture didn’t.\nThere are also strictly intransitive uses, as in 12–\n13 of Table 2a. These are analyzed in the same way\nas the intransitive variant of alternating frames (2),\ni.e., as unaccusatives. Why these break frames\ndo not allow a cause subject – e.g., *the Earth’s\nrotation broke the day – is an open question.\nAs seen in Table 2b, break also combines with\na wide range of predicates and particles to create\nnew senses. Except for 14, 19, and 20, these uses\nare all intransitive, but they seem to differ from the\nparticle-less uses in a key way: whereas intransitive\nparticle-less break cases are all unaccusative, the\nparticle cases vary in this regard. For example, the\nwar broke out seems unaccusative, but we broke\ninto the building has an agentive subject and so\nwould not be analyzed as unaccusative.\nA key question for lexical semantic theories is\nwhether there is a single unifying semantic frame\nunderlying this diverse array of senses – or, if not\na single frame, then perhaps a few of them feed-\ning into distinct sense clusters. This position is\nadvanced, for example, by Kellerman (1978:65),\nfor whom “[t]he various meanings of BREAK\n[. . . ] can all be subsumed under a ‘deep’ meaning,\n‘(cause) not to continue in existing state’, which\nlinks even the most disparate meanings of BREAK’\n(see also Spalek 2012 for a similar position for\nSpanish romper ‘break’). Another approach would\nbe to posit a few more primitive semantic dimen-\nsions that give rise to a combinatorial space of\n492\nTransitiveUnaccusativeAgentMetaphoricalseparateviolateendappearout_escapeout_begin\n1. We broke the vase 1 0 1 0 1 0 0 0 0 0\n2. The vase broke 0 1 0 0 1 0 0 0 0 0\n3. We broke the law 1 0 1 1 0 1 0 0 0 0\n4. The silence broke a procedural rule 1 0 0 1 0 1 0 0 0 0\n5. We broke the silence 1 0 1 1 0 0 1 0 0 0\n6. The day broke 0 1 0 1 0 0 0 1 0 0\n7. The storm broke 0 1 0 1 0 0 0 1 0 0\n8. Sweat broke on his forehead 0 1 0 1 1 0 0 1 0 0\n9. We broke out (of jail) 0 0 1 0 0 0 0 0 1 0\n10. Fighting broke out 0 1 0 1 0 0 0 0 0 1\nTable 3: Partial feature-based analysis of break in different syntactic contexts.\npredicted senses, which might in turn lead to pre-\ndictions about argument structure realization and\nother structural and distributional properties.\n3 Feature-based Theories\nIn this section, we take the somewhat unusual step\nof bringing together existing ideas from the linguis-\ntics literature into a feature space of the sort one\nis likely to encounter in NLP contexts. We do this\nfor a few reasons. First, it reveals that, though theo-\nries in linguistics and NLP often take very different\nforms, there is actually a lot of common ground be-\ntween them: on both sides, vector representations\nof data can serve as a common language. Second,\nthe feature space reveals how deeply linguistic the-\nories are committed to our contextual modulation\ntenet from Table 1: to honor the insights from the\nliterature, we have to define the feature space in\nterms of (at least) full sentences.\nTable 3 is our (highly partial) feature-based anal-\nysis. The Transitive feature captures whether a par-\nticular break frame has two nominal arguments or\none. Causative alternation uses can then be recon-\nstructed by looking at shared meaning dimensions\nthat vary in their Transitive value, as in rows 1–2.\nWe separately define an Unaccusative feature, since\nthe uses in Table 2b show that these can come apart.\nThis is evident especially in rows 9–10.\nThe Agent feature captures whether the subject\nof each example is agentive or not. We mentioned\nin Section 2 that the obligatory transitivity of some\nbreak frames has been traced, unsuccessfully in\nour view, to the agentivity of the subject of these\nframes. The Agent feature in combination with\nthe Transitive feature and the meaning dimensions\nreveals the incompleteness of this explanation: ‘vi-\nolate’ examples, which are obligatorily transitive,\nmay have subjects that are agentive (row 3) and\nnon-agentive (row 4).\nWe have a column for Metaphorical, though\ncoding this is sufficiently hard that it looks like\na multidimensional category to us rather than a\nsingle feature. Due to the difficulty of classify-\ning senses of break and other polysemous verbs\nas (non)metaphorical, previous literature that has\nengaged with this question (e.g. Kellerman, 1978;\nPiñón, 2001; McNally and Spalek, 2017, 2022)\nhas used the heuristic of associating metaphorical\nsenses with abstract participants, like break the si-\nlence, and non-metaphorical ones with concrete\nparticipants, such as break the vase. We follow this\n(admittedly simplifying) heuristic in our feature-\nbased analysis. However, we agree with McNally\nand Spalek (2022, 6) that “the distinction between\n‘literal’ and ‘figurative’ senses can become blurred\nover time, and sometimes can only be diachroni-\ncally reconstructed”.\nFollowing these features are a few meaning di-\nmensions. The full class of meaning annotations\nwe use in Section 5.3 has 72 classes, so this is just a\nsample. The sample was chosen to emphasize three\naspects of the meanings of break. First, as already\nillustrated in Table 2, these meanings are highly\ndiverse semantically. Second, it is difficult (and\nmaybe even futile) to determine with confidence\nhow many distinct (and non-overlapping) senses\n493\n1. break 11. up\n2. breaks 12. trying\n3. breaking 13. away\n4. end 14. start\n5. broke 15. get\n6. down 16. again\n7. take 17. ’ll\n8. let 18. back\n9. going 19. out\n10. leave 20. off\n(a) GloVe, Common Crawl\n840B tokens, 300d.\n1. breaks 11. brief_respite\n2. breaking 12. Nadal_netted_forehand\n3. broke 13. loosen\n4. broken 14. smash\n5. Break 15. rip\n6. Breaking 16. overhit_forehand\n7. breather 17. miscued_forehand\n8. shatter 18. cut\n9. crack 19. slip\n10. breaker 20. Breaks\n(b) word2vec, GoogleNews, 300d.\n1. break 11. breakin\n2. breaks 12. breaked\n3. breaking 13. broken\n4. breake 14. legbreak\n5. re-break 15. reak\n6. break- 16. semi-break\n7. unbreak 17. minibreak\n8. breakes 18. breaker\n9. break. 19. breaking-down\n10. broke 20. tea-break\n(c) fastTest WikiNews, with sub-\nword modeling, 300d.\nTable 4: Nearest neighbors of break in static embedding spaces. All the methods place morphological variants of\nbreak next to break itself and seem to sporadically find different senses and near synonyms of break. The lists given\nhere are, in our judgment, the best from each of the three methods. For additional lists, see Appendix B.\nbreak may express. For example, does break ex-\npress the same meaning, ‘appear’, in row 6 as in\nrow 7, as we suggest in Table 3? Or should these ex-\namples be seen as expressing distinct senses? Third,\nwe believe there are some examples where break\nsimultaneously expresses more than one meaning,\nas shown in row 8 of Table 3, wherebreak shows\nboth an ‘appear’ and a ‘separate’ meaning.\nBreak with particles/predicates can sometimes\nexpress meanings that particle-less break cannot\nconvey: e.g. ‘escape’ in row 9. We assume that the\nparticles/predicates contribute an irreducible mean-\ning and reflect this in our feature-based analysis\nby preceding these meaning dimensions with the\ncorresponding particles/predicates: ‘out_escape’.\nThe particles/predicates do not determine a unique\nmeaning, though, as we see with the two senses\nof break out: ‘out_escape’ and ‘out_begin’. And\nwe could of course have extended this even further.\nThere are additional clearly distinct senses like His\nface broke out in hives and break out the cham-\npagne, as well as cases like break out in laughter\nwhich might be subsumed under other senses (say,\n‘out_begin’).\nPotentially all of the columns are actually just in-\nformal stand-ins for much more complex concepts.\nThe labels could be natural language predicates on\npar with break, in which case the column names\nare really just hooks into a larger lexical web, or\nthey could be glosses for more intricate theoretical\nconcepts that demand further decomposition before\nthe theory can be regarded as complete.\nIn addition, we can seamlessly integrate this kind\nof analysis with more data-driven techniques. As\nan illustration, Appendix A reports on an experi-\nment using the WordNet hypernym graph to iden-\ntify extremely abstract latent meaning dimensions.\nHow many lexical items does this theory posit?\nThe answer to this question is not clear. We could\nsay that each attested combination of the features is\na new sense, or we could select a few features and\nsay that specific combinations of them correspond\nto distinct senses. Both decisions have a certain\narbitrariness to them given the feature space itself,\nand we might infer from this that the theory does\nnot posit distinct senses or distinct lexical items\nas first-class linguistic constructs. This may be a\nconsequence of the contextual modulation tenet.\nRelatedly, it is unclear to us what a complete\nanalysis in these terms would look like. What\nwould it mean to have determined all and only\nthe correct features? Could it be that the investi-\ngation will always admit of further dimensions, or\ndecomposition of existing dimensions?\nIn sum, it is easy to see how this analysis makes\ngood on the central tenets in Table 1. The represen-\ntations are high-dimensional vectors with discrete\nvalues. In addition, the representations themselves\ndirectly bring in context. The vector for break\nalone, if it exists in the theory at all, needs to be\nmostly unspecified values that only become values\nin specific syntactic or usage contexts.\n4 Static Vector Modeling\nThe above feature-based analysis might be de-\nscribed as a sparse vector representation approach.\nWe now contrast that with a dense vector represen-\ntation approach that models individual lexical items\nas fixed (static) vectors. A variety of such methods\nhave been developed. Here we look at the treatment\nof break by three prominent methods: word2vec\n(Mikolov et al., 2013), GloVe (Pennington et al.,\n2014), and fastText (Mikolov et al., 2018). These\nmethods have different learning objectives, but all\n494\nare closely related to Pointwise Mutual Information\n(PMI; Church and Hanks 1990; Turney 2001). In\nPMI, we assign weights to pairs of words wi and\nwj based on whether their observed joint probabil-\nity of co-occurrence is larger or smaller than what\nwe would expect given the null hypothesis that wi\nand wj have independent distributions. All three\nmethods learn regularized, reduced dimensional\nrepresentations according to roughly this same goal\n(Levy and Goldberg, 2014; Cotterell et al., 2017).\nNone of these methods use discrete features, and\nthus they are in conflict with our discreteness tenet\nfrom Table 1. The raw input to all of them is a\nmatrix of co-occurrence counts, which could be\nviewed as a set of discrete distributional features.\nHowever, much of the power of these models de-\nrives from their ability to compress this information\ninto a lower-dimensional space of continuous val-\nues in which the columns are unlikely to have direct\ninterpretations as features.\nThe GloVe vocabulary is largely restricted to in-\ndividual words from a fixed list. By contrast, the\nvocabulary used for our word2vec instance includes\nsome phrase-like elements that were inferred by the\nauthors using simple co-occurrence statistics, and\nour chosen fastText model includes sub-word com-\nponents and so also ends up with a more expansive\nview of what counts as a lexical item.\nAll of these models have proven successful as\nrepresentations of words and as components in\nlarger systems. However, we find that these repre-\nsentations are disappointing for studying break. In\nTable 4, we show the top 20 nearest neighbors (ac-\ncording to cosine similarity) for some uses of these\nmodels. We chose what seemed to be the semanti-\ncally richest instance of each model from a larger\nset of such results (see Appendix B). All of the\nmodels capture morphological variants very clearly.\nHowever, the other semantic associations generally\nonly weakly indicate other specific senses (via as-\nsociations with other words). We do see some\npositive benefits from the quasi-phrasal vocabulary\nused by word2vec and fastText, but overall these\nspaces look like only superficial pictures of the\nunderlying semantic richness of break.\nThe cause for this semantic blandness likely\ntraces to the basic design decision: every word-\nform has only a single representation. This means\nthat a single vector must encode all the different\nsenses that we see in Table 2 as well as others that\nwe did not include there. The result is probably\nsomething like a weighted average of these senses,\nwhich seems not to be in a particularly interesting\npart of the embedding space.\nAdherents to our central tenets (Table 1) might\nhave predicted this negative result. While static\nrepresentations are high dimensional, they do not\nallow for contextual modulation. Each basic unit\nof the vocabulary is assigned exactly one represen-\ntation. Contextual modulation may occur if the\nrepresentations are embedded in a larger system,\nbut it is not intrinsic to the vectors themselves.\n5 LLM Investigations\nWe come now to our primary investigative tool:\nLLMs. We concentrate on models that have the\ncore structure of the Transformer (Vaswani et al.,\n2017) and are trained at least in part using masked\nlanguage modeling, which allows for bidirectional\ncontext. In the interest of space, we will mostly pre-\nsuppose familiarity with these models. However,\nAppendix C provides an overview of their structure\nto try to bridge any gaps between the linguistics\nand NLP literature.\nIn our main text, we report results for RoBERTa-\nlarge (Liu et al., 2019), which has 24 layers.\nOur appendices cover BERT and DeBERTa. Our\nRoBERTa-large results are slightly better than all\nof these others, but the results are generally quite\ncomparable, suggesting that all of these models can\nfruitfully be used for lexical semantics.\nBefore turning to our experiments, let’s consider\nhow LLMs relate to our core tenets from Table 1.\nFirst, the representations we obtain at each hidden\nlayer are all high-dimensional and modulated by\nthe context. Our experiments show that, for the\ncase of break, this contextual modulation is rich\nand linguistically systematic. Thus, LLMs and tra-\nditional lexical semantic theories are aligned on\nthese two tenets. However, the two theories part\nways when it comes to the question of having dis-\ncrete features. The column dimensions of LLM\nrepresentations are continuous and highly abstract.\nDiscrete linguistic features might be latently en-\ncoded in these representations, or extractable from\nthem with some noise, but this does not detract\nfrom the fact that these representations are highly\nfluid and do not presuppose the existence of any\nparticular features or dimensions. Rather, all the\nfeatures are learned from data in a free-form way\nthat is grounded entirely in distributions.\n495\nLayer Probe Control Selectivity\n1 0.33 0.03 0.30\n6 0.81 0.03 0.79\n12 0.83 0.03 0.80\n18 0.80 0.03 0.76\n24 0.86 0.03 0.83\n(a) Meaning-class probing results.\nLayer Probe Control Selectivity\n1 0.50 0.33 0.17\n6 0.94 0.34 0.60\n12 0.96 0.33 0.63\n18 0.96 0.35 0.61\n24 0.97 0.32 0.65\n(b) Construction-type probing results.\nTable 5: RoBERTa-large probing results. We report Macro F1 and Selectivity, which is the Macro F1 score for the\ntask minus the Macro F1 for a control task (random assignment of tokens to classes). Results for other models are\nsimilar; see Appendix D.\n5.1 Annotated Dataset\nThe basis for our investigation is an annotated\ndataset created by Petersen (2020) and subse-\nquently updated by us to include more examples\nand senses. The examples are extracted from\nthe Corpus of Contemporary American English\n(CoCA; Davies 2008). We focus on a subset\nof 1,042 sentences that have been annotated for,\namong other things, the core semantic class of the\nreading and the construction type (‘unergative’, ‘un-\naccusative’, ‘causative’). Petersen assigns a single\nsemantic class to each example. However, as men-\ntioned in Section 3, we believe that, in some cases,\nbreak can be said to simultaneously express more\nthan one meaning. We use our experiment in Sec-\ntion 5.3 to identify examples with this property.\nWe rely primarily on the meaning class distinc-\ntions and make secondary use of the constructional\nannotations. Petersen’s annotation scheme uses 72\nsemantically rich meaning classes, which have a\nhighly skewed distribution. The full distribution is\ngiven in Appendix E.\n5.2 Probing Experiments\nWe want to explore the LLM representations in a\nfluid way that will lead us to identify new read-\nings. Our tools for doing this are supervised probe\nmodels applied to the column of representations\nabove the break token in each of our examples. We\nprobe for meaning-class and construction-type (see\nalso Papadimitriou et al. 2021). These probes serve\nas a quantitative evaluation of the extent to which\nthese break representations encode these important\nproperties, and they are also tools for heuristically\nfinding new uses and readings.\nFor our construction-type probing work, we can\nuse all 1,042 sentences, since there are only three\nclasses and all have substantial representation in the\ndata (causative: 673 examples, unaccusative: 197,\nunergative: 172). For the meaning-type work, there\nare 72 classes, many with only a few instances.\nThus, we limit attention to just the classes with at\nleast 10 examples (Appendix E).\nOur probe models are L2-regularized classifiers\nwith a cross-entropy loss. Our core metric is the\nmacro F1 score, which assigns equal weight to each\nclass’s F1 score regardless of the class size. Fol-\nlowing Hewitt and Liang (2019), we report selec-\ntivity scores, which are the probe scores minus the\nperformance on a control task, which here is ran-\ndom assignment of break representations to mean-\ning classes. We report selectivity scores averaged\nacross 20 random 80%/20% train/test splits.\nThe probe results show a clear pattern: the low-\nest layers are not very robust when it comes to this\nprobing work, but higher layers are very robust in\nthis sense (see also Reif et al. 2019; Ethayarajh\n2019). We see similar results for other LLMs in\nthe class we are focused on, as reported in Ap-\npendix D. For this reason, we focus on layer 24 of\nRoBERTa-large from now on.\n5.3 Discovering New Example Types\nOur primary goal is to see whether it is possible\nto use LLMs to gain new insights about lexical\nsemantics. Our probing results suggest that LLM\nrepresentations are systematic enough to make this\nplausible, but they are very high-level. We need\nan investigative technique that is more free-form\nand that can bring to our attention new kinds of\ntheory-relevant examples.\nA natural choice is visualization. We provide\nt-SNE visualizations (van der Maaten and Hinton,\n2008) in Appendix G, and we find that they are\nindeed useful: where an example of meaning class\na is nestled among examples of class b, the a-class\nexample is often an interesting blend of a-class\n496\nMeaning Construction\nSentence Gold Predicted Gold Predicted\n1. Patients will sometimes break out in a spontaneous\nrecitation of the rosary\nbreak_out_\nstart\nbreak_out_\nstart\nunacc. unerg.\n2. It was like you knew something, like you knew the\nstory was getting ready to break again.\nreveal appear unacc. unacc.\n3. @(Soundbite-of-music)@!Mr-GELB: (Singing) Tell\nme who’s going to pick up the pieces when you start\nto break down.\nbreak_down_\nseparate_into_parts\nbreak_down_\nsuccumb\nunacc. unacc.\n4. People have so many problems overcoming the dis-\nputes that occur when families break up\nbreak_up_\nend_relationship\nbreak_up_\nseparate_into_parts\nunacc. unacc.\n5. “So why tell the whole story now? Somebody, some\nmale, has got to be willing to break this code of si-\nlence,” he says.\nviolate end unacc. unacc.\n6. So they forwarded the pictures to Madrid, where an-\nother officer noticed some printing on a towel that\nhelped break the case.\ndecipher end causative causative\n7. Then too, stress can also work to break down the im-\nmune system, increasing the likelihood of respiratory\nand creating gastrointestinal and nervous disorders.\nbreak_down_\nrender_inoperable\nbreak_down_\ndestroy\ncausative causative\n8. Wind, naturally acidic rain, and physical processes\nsuch as freezethaw cycles also break down rock.\nbreak_down_\nseparate_into_parts\nbreak_down_\ndestroy\ncausative causative\n9. It didn’t take being an ICU exec to break the code:\ntrade secret.\ndecipher violate causative causative\nTable 6: A curated sample of theoretically informative examples.\nand b-class meanings, and such examples seem\ngenuinely worthwhile to study further. However,\nthese visualizations introduce known distortions re-\nsulting from compressing high-dimensional spaces\ninto two dimensions (Wattenberg et al., 2016), and\nthey can even vary in qualitatively substantive ways\nacross models and runs.\nFor something more stable, we return to our\nprobe models. The selectivity scores for both are\nconservative if we think of them as tools for finding\nnew examples: the meaning-class probes achieve\nresults above 80% macro F1, and the construction-\ntype probes are nearly perfect in their performance\n(where chance is around 33%). Thus, we decided to\nextract and review the errors made by these models,\nwith the expectation that many of these examples\ncould inform lexical semantic theory itself.\nTable 6 is a selection of examples that we ex-\ntracted in this way for further analysis. This is a\nsmall curated set of (so-called) errors, though these\nexamples do not look like errors to us, but rather\nlike instances in which multiple senses and multi-\nple construction types emerge in the same example.\nExample 1 is predicted unergative, whereas the\ngold label is unaccusative. For us, this raises a new\nquestion: what is the role of spontaneous in the\noverall agentivity of the subject, and should this\nplay into how we characterize the syntactic frame?\nExample 2 looks like a clear case where multiple\nsenses can be activated and different utterance con-\ntexts might favor different readings. Is the breaking\nof a news story an agentive act of revealing informa-\ntion (the gold label), or can it be (or be described\nas) something more like a natural process of ap-\npearing (the predicted label)? Both readings seem\navailable, and individual uses might blend them for\na particular rhetorical effect.\nWe also find cases where both the gold meaning\nand the predicted meaning are in principle avail-\nable, and the choice between them depends on\nwhether we would like to focus on the metaphorical\nor the literal meaning of the expression. This can\nbe seen in examples 3 and 4.\nExample 4 also reveals a common pattern we\nfound in our examples: in many cases where multi-\nple senses are present, there is a contextual entail-\nment relation between them. In example 4, should\nwe focus on the direct and perhaps metaphorical\nreading (gold) or the more literal likely conse-\nquence (predicted)? In example 5, violating the\n497\ncode of silence entails ending it. In example 6,\ndeciphering a case entails solving or ending it. In\nexample 7, the process of breaking down the im-\nmune system, which we paraphrased as rendering\nit inoperable (gold meaning), could culminate in its\ndestruction (predicted meaning). And example 8\nreveals that the event structure of break examples\ncan be very complex. When a rock is broken down\nby natural forces, we can think of this as a process\nof breaking down into smaller parts (gold meaning)\nwith the end state being total destruction (predicted\nmeaning). Many examples in which the breaking\nevent leads to the fragmentation of the theme par-\nticipant can show similar blends.\nWe also see cases where there seems to be gen-\nuine uncertainty about which of the two senses is\nintended. Example 9 illustrates this. Depending\non the meaning of the word code (‘encryption’ vs.\n‘norm’ ), break can either mean ‘decipher’ (gold) or\n‘violate’ (predicted). This example further evinces\nthe importance of our contextual modulation tenet:\nsometimes we have to go even beyond full sen-\ntences to be able to determine the meaning ofbreak\nin a particular case.\nThese are just a few examples of a much larger\nset of interesting cases that emerge from studying\nthe interaction between our LLM-based probe mod-\nels and our linguistic annotations. Appendix F pro-\nvides a larger sample with brief annotations about\npotential theoretical relevance. We close this pa-\nper by reflecting on how best to incorporate these\ninsights into the linguistic theory itself.\n6 Discussion\nThe fact that linguistic theory and LLMs agree on\nthe core tenets of high dimensionality and contex-\ntual modulation is a striking alignment of theoreti-\ncal ideas with engineering success.\nThe fact that LLMs do not use discrete features,\nbut rather derive dense, real-valued representations\nfrom data seems like an opportunity for linguists\nto reflect on the role of discreteness. As we noted\nin Section 3, it seems unlikely that purely analytic\nwork and traditional corpus work will lead to an\nexhaustive hand-built representation for any lexical\nitems. With LLMs, we can mine the existing repre-\nsentations while considering the LLM architecture\nand learned parameters to be a reflection of the core\ntenets of the theory.\nThe deep contextual modulation countenanced\nby linguistic theory and operationalized by LLM\nembeddings invites a further question: do lexical\nitems exist outside of their tokens of use? Even for\nthe hand-built feature representations in Table 3,\nthe rows could in principle vary based even on\nusage information, which would suggest a theory\nthat is actually more about tokens (instances of use)\nthan types. Similarly, for LLMs, though they do\ncontain type-level representations (in the form of an\nembedding for the vocabulary), these play a minor\nrole, and all the representations we have considered\nin this paper were in terms of representations that\nare more like token-level representations.\nOverall, then, a theory of lexical semantics that\ndraws heavily on LLMs as investigative tools, and\neven as ways to state theoretical ideas, is likely to\nbecome more usage-based than traditional theories\nwould assume. This could lead them to focus less\non pure representation and more on what is actually\ncommunicated between people when they use lan-\nguage. Traditional questions are likely to take on\nnew forms in this setting, and exciting and relevant\nnew questions – and new pieces of evidence – are\nlikely to arise.\n7 Limitations\nOur general thesis is that LLMs are valuable tools\nboth for conducting lexical semantic analyses and\nfor providing valuable perspectives for lexical se-\nmantic theory design in general. Although we think\nthis thesis is widely supported by prior literature,\nour own case study is limited to just a partial anal-\nysis of a single verb. This creates the risk that our\ngeneral conclusions may be more specific to this\nverb, or to English, than we would like. The prior\nliterature inherits many of our English-only biases\nas well (but see Papadimitriou et al. 2021).\nOur main results use RoBERTa-large, and our\nappendices report on parallel analyses with dif-\nferent versions of BERT and DeBERTa. These\nmodels share core architectural features and were\noptimized in largely similar ways using very large\n– and largely uncontrolled – datasets. This means\nthat these artifacts are certainly biased in ways that\nare relevant for lexical semantics. However, we are\nunlikely to be able to identify, isolate, and factor\nout these biases with the methods used in our pa-\nper. Our core methods are reasonably simple and\nlightweight, and we have released all our code. We\nhope that these steps allow easy reproduction of our\ncore analyses whenever newer LLMs are released,\nso that we can begin to understand better how LLM\n498\nbiases can affect linguistic theorizing in the mode\nwe are advocating for.\nOur current approach also has an analytic limita-\ntion: though we fit probe models and use them as\ndevices for finding potentially relevant examples,\nthe final step in our analysis involves inspection of\nthose examples by linguists like ourselves. This\nmeans that the final step is not as reproducible as\nthe others, and it means that any analytic biases that\nthe linguists involved might have are likely to make\ntheir way into the analyses. We do not see a way to\navoid these analytic steps entirely, since linguistic\nanalysis favors this kind of low-level work, but we\ndo think that we can mitigate the concerns about\nanalyst bias by making all our data available for\nothers to inspect, as a way of opening up many per-\nspectives on the data and the associated theoretical\nquestions.\nAcknowledgements\nOur thanks to Dan Lassiter, Beth Levin, Isabel\nPapadimitriou and participants at the DistCurate\n2022 workshop for helpful discussion.\nReferences\nVíctor Acedo-Matellán and Jaume Mateu. 2014. From\nsyntax to roots: A syntactic approach to root inter-\npretation. In Artemis Alexiadou, Hagit Borer, and\nFlorian Schäfer, editors, The Syntax of Roots and\nthe Roots of Syntax, pages 14–32. Oxford University\nPress, Oxford.\nArtemis Alexiadou, Elena Anagnostopoulou, and Flo-\nrian Schäfer. 2006. The properties of anticausatives\ncrosslinguistically. In Mara Frascarelli, editor,\nPhases of Interpretation , pages 187–211. Mouton\nde Gruyter, Berlin.\nMarianna Apidianaki. 2022. From word types to tokens\nand back: A survey of approaches to word mean-\ning representation and interpretation. Computational\nLinguistics, pages 1–60.\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. In 36th An-\nnual Meeting of the Association for Computational\nLinguistics and 17th International Conference on\nComputational Linguistics, Volume 1, pages 86–90,\nMontreal, Quebec, Canada. Association for Compu-\ntational Linguistics.\nCollin F. Baker and Hiroaki Sato. 2003. The FrameNet\ndata and software. In The Companion Volume to the\nProceedings of 41st Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 161–164,\nSapporo, Japan. Association for Computational Lin-\nguistics.\nHagit Borer. 2005a. In Name Only, volume 1 of Struc-\nturing Sense. Oxford University Press.\nHagit Borer. 2005b. The Normal Course of Events ,\nvolume 2 of Structuring Sense. Oxford University\nPress.\nHagit Borer. 2013. Taking Form, volume 3 of Structur-\ning Sense. Oxford University Press.\nAntónio Branco, João Rodrigues, Małgorzata Salawa,\nRuben Branco, and Chakaveh Saedi. 2020. Compara-\ntive probing of lexical semantic theories for cognitive\nplausibility and technological usefulness. In Proceed-\nings of the 28th International Conference on Compu-\ntational Linguistics, pages 4004—4019, Barcelona.\nInternational Committee on Computational Linguis-\ntics.\nLuigi Burzio. 1986. Italian Syntax. D. Reidel Publish-\ning Company, Dordrecht.\nJosé Camacho-Collados and Mohammad Taher Pilehvar.\n2018. From word to sense embeddings: A survey\non vector representations of meaning. Journal of\nArtificial Intelligence Research, 63(1):743–788.\nKenneth Ward Church and Patrick Hanks. 1990. Word\nassociation norms, mutual information, and lexicog-\nraphy. Computational Linguistics, 16(1):22–29.\nEve V . Clark and Herbert H. Clark. 1979. When nouns\nsurface as verbs. Language, 55(4):767–811.\nHerbert H. Clark. 1997. Dogmas of understanding. Dis-\ncourse Processes, 23(3):567–59.\nRyan Cotterell, Adam Poliak, Benjamin Van Durme,\nand Jason Eisner. 2017. Explaining and generaliz-\ning skip-gram through exponential family principal\ncomponent analysis. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 175–181, Valencia, Spain. Association\nfor Computational Linguistics.\nMark Davies. 2008. The Corpus of Contempo-\nrary American English: 450 million words, 1990-\npresent. Available online at http://corpus.byu.\nedu/coca/.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDavid Dowty. 1976. Montague grammar and the lexi-\ncal decomposition of causative verbs. In Barbara H.\nPartee, editor, Montague Grammar, pages 201–245.\nAcademic Press, New York.\n499\nDavid Dowty. 1979. Word Meaning and Montague\nGrammar. D. Reidel, Dordrecht.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nChristiane Fellbaum, editor. 1998. WordNet: An Elec-\ntronic Database. MIT Press, Cambridge, MA.\nCharles Fillmore. 1970. The grammar of hitting and\nbreaking. In R.A. Jacobs and P.S. Rosenbaum, ed-\nitors, Readings in English Transformational Gram-\nmar, pages 120–133. Ginn, Waltham, MA.\nAina Garí Soler, Marianna Apidianaki, and Alexandre\nAllauzen. 2019. Word usage similarity estimation\nwith sentence representations and automatic substi-\ntutes. In Proceedings of the Eighth Joint Conference\non Lexical and Computational Semantics (*SEM\n2019), pages 9–21, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\nEric Kellerman. 1978. Giving learners a break: Na-\ntive language intuitions as a source of predictions\nabout transferability. Working Papers on Bilingual-\nism, 15:60–92.\nAngelika Kratzer. 1996. Severing the external argument\nfrom its verb. In Johan Rooryck and Laurie Zaring,\neditors, Phrase Structure and the Lexicon, pages 109–\n137. Kluwer, Dordrecht.\nBeth Levin. 2017. The elasticity of verb meaning revis-\nited. In Proceedings of SALT 27, pages 571–599.\nBeth Levin and Malka Rappaport Hovav. 1995. Unac-\ncusativity: At the Syntax–Lexical Semantics Interface.\nMIT Press, Cambridge, MA.\nOmer Levy and Yoav Goldberg. 2014. Neural word em-\nbedding as implicit matrix factorization. In Advances\nin Neural Information Processing Systems.\nJiangtian Li and Marc F. Joanisse. 2021. Word senses as\nclusters of meaning modulations: A computational\nmodel of polysemy. Cognitive Science: A Multidisci-\nplinary Journal, 45(4):1–30.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv:1907.11692.\nDaniel Loureiro, Kiamehr Rezaee, Mohammad Taher\nPilehvar, and Jose Camacho-Collados. 2021. Anal-\nysis and evaluation of language models for word\nsense disambiguation. Computational Linguistics,\n47(2):387–443.\nAsifa Majid, James S. Boster, and Melissa Bowerman.\n2008. The cross-lingusitic categorization of everyday\nevents: A study of cutting and breaking. Cognition,\n109:235–250.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems 30, pages 6294–6305.\nJohn P. McCrae, Theodorus Fransen, Sina Ahmadi, Paul\nBuitelaar, and Koustava Goswami. 2022. Toward an\nintegrative approach for making sense distinctions.\nFrontiers in Artificial Intelligence, 5:1–18.\nLouise McNally and Alexandra Anna Spalek. 2017.\n‘Figurative’ uses of verbs and grammar. Unpublished\nmanuscript.\nLouise McNally and Alexandra Anna Spalek. 2022.\nGrammatically relevant aspects of meaning and ver-\nbal polysemy. Linguistics, pages 1–45.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in pre-training distributed word representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their composition-\nality. In Christopher J. C. Burges, Leon Bottou,\nMax Welling, Zoubin Ghahramani, and Kilian Q.\nWeinberger, editors,Advances in Neural Information\nProcessing Systems 26 , pages 3111–3119. Curran\nAssociates, Inc.\nSathvik Nair, Mahesh Srinivasan, and Stephen Mey-\nlan. 2020. Contextualized word embeddings encode\naspects of human-like word sense knowledge. In\nProceedings of the Workshop on the Cognitive As-\npects of the Lexicon, pages 129–141. Association for\nComputational Linguistics.\nIsabel Papadimitriou, Ethan A. Chi, Richard Futrell, and\nKyle Mahowald. 2021. Deep subjecthood: Higher-\norder grammatical features in multilingual BERT. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2522–2532, Online.\nAssociation for Computational Linguistics.\nJoe Pater. 2019. Generative linguistics and neural net-\nworks at 60: Foundation, friction, and fusion. Lan-\nguage, 95(1):e41–e74.\n500\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nDavid M. Perlmutter. 1978. Impersonal passives and the\nUnaccusative Hypothesis. In Proceedings of the An-\nnual Meeting of the Berkeley Linguistics Society, 38,\npages 157–189. Berkeley Linguistics Society, Lin-\nguistic Society of America.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nErika Petersen. 2020. Break + NP constraints and the\ncausative alternation. Ms., Stanford University.\nChristopher Piñón. 2001. A finer look at the causative-\ninchoative alternation. In Proceedings of SALT 11,\npages 346–364.\nJames Pustejovsky. 1991. The generative lexicon. Com-\nputational Linguistics, 17(4):409–441.\nJames Pustejovsky. 1995. The Generative Lexicon. The\nMIT Press, Cambridge, MA.\nJames Pustejovsky. 2006. Introduction to Generative\nLexicon. Ms., Brandeis.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Ms, OpenAI.\nMalka Rappaport Hovav and Beth Levin. 2012. Lex-\nicon uniformity and the causative alternation. In\nMartin Everaert, Marijana Marelj, and Tal Siloni, ed-\nitors, The Theta System: Argument Structure at the\nInterface, pages 150–176. Oxford University Press,\nOxford.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems, volume 32. Curran Associates, Inc.\nJosef Ruppenhofer, Michael Ellsworth, Miriam R. L.\nPetruck, Christopher R. Johnson, and Jan Scheffczyk.\n2006. FrameNet II: Extended Theory and Practice.\nInternational Computer Science Institute, Berkeley,\nCA.\nFlorian Schäfer. 2008. The Syntax of (Anti-)Causatives:\nExternal Arguments in Change-of-State Contexts .\nJohn Benjamins, Amsterdam.\nJohn R. Searle. 1980. The background of meaning. In\nJohn R. Searle, Ferenc Kiefer, and Manfred Bier-\nwisch, editors, Speech Act Theory and Pragmatics,\npages 221–232. D. Reidel, Dordrecht.\nAlexandra Anna Spalek. 2012. Putting order into literal\nand figurative uses of verbs: romper as a case study.\nBorealis, 1(2):140–167.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nSean Trott and Benjamin Bergen. 2021. RAW-C: relat-\nedness of ambiguous words-in context (A new lexical\nresource for English). CoRR, abs/2105.13266.\nPeter D Turney. 2001. Mining the web for synonyms:\nPMI-IR versus LSA on TOEFL. In European Confer-\nence on Machine Learning, pages 491–502. Springer.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9(86):2579–2605.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nMartin Wattenberg, Fernanda Viégas, and Ian John-\nson. 2016. How to use t-SNE effectively. Distill,\n1(10):e2.\nGregor Wiedemann, Steffen Remus, Avi Chawla, and\nChris Biemann. 2019. Does BERT make any sense?\nInterpretable word sense disambiguation with contex-\ntualized embeddings. ArXiv, abs/1909.10430.\n501\nSupplementary Materials\nA WordNet-based Features\nThe feature-based analyses of Section 3 are easily extended with features obtained using more approximate,\ndata-driven techniques. To illustrate this potential, we looked to WordNet (Fellbaum, 1998), which has a\nvery rich picture of break. The lemma break participates in 59 SynSets in WordNet. We built a graph of\nthese SynSets based on the hypernym relation. The resulting graph has 29 connected components (29\nsubgraphs). Figure 1 depicts the largest connected components as subgraphs. If we label these subgraphs\nwith their most-specific shared hypernym, we get potentially new meaning dimensions like “Cause to\nchange; make different; cause a transformation” and “undergo a change; become different in essence;\nlosing one’s or its original nature”. These are similar, but only the first conveys agency. Both seem like\nplausible latent semantic dimensions that we could add to Table 3, either as primitive features or as sets of\nmore basic meanings. And of course this is only a single example of many that WordNet would support,\nand additional features could be extracted from FrameNet (Baker et al., 1998; Baker and Sato, 2003;\nRuppenhofer et al., 2006).\nFigure 1: Largest WordNet connected components for break labeled with the name of their most specific shared\nhypernym. These hypernym labels suggest interesting abstract meaning dimensions for these senses.\n502\nB Additional Static Vector Analyses\nTable 7 extends Table 4 from the main text with additional variants of word2vec, fastText, and GloVe.\nThe overall picture seems consistent across these variants. For the main text, we simply chose the variant\nof each model that looked the best to us in terms of capturing meaning dimensions of break.\nC LLM Structure\nThe input to the LLMs we consider is always a sequence of tokens [x1, . . . , xn]. Each token may\ncorrespond to a full word type or a word piece, depending on the tokenization method. For instance,\nwhereas the is tokenized as a single unit, breakage is likely to be analyzed as two pieces, break and ##age,\nwhere the ## prefix indicates a word-internal piece. This is a detail we set aside in our analyses, since we\nconsider only examples involving break and all the models we use analyze break as a single token.\nThe elements of the input sequence are looked up in a static embedding space. The result is a sequence\nof vectors [x1, . . .xn], where each xj has dimension d. These are akin to the static representations from\nmodels like those in Section 4: there is one vector per word piece and thus no contextual modulation.\nThe static embeddings are additively combined with one or more separate embeddings that record\naspects of each token’s position in the sequence. In the simplest case, there is a single positional embedding\nthat is used to create a sequence of vectors [p1, . . . ,pn], each of dimension d, and we obtain positionally\nenriched representations as H0 = [x1 + p1, . . . ,x1 + pn]. Thus, already at this point in the model, a single\nword will have different representations depending on where it appears in the input sequence.\nThe positionally-enriched embeddings are fed into the Transformer architecture itself. This creates\nnumerous interactions between the representations. Each Transformer block i >0 results in a sequence\nHi = [hi,1, . . . ,hi,n] of hidden representations, each one of dimension d. The models we consider have\nbetween 12 and 24 of these layers.\nWe focus on models that are trained in the manner of the BERT model. The core of that training regime\nis masked language modeling, in which elements of the input sequence are randomly masked out or\nreplaced with randomly chosen tokens from the vocabulary, and the task of the model is to learn to assign\nhigh likelihood to the actual token, using the entire surrounding sequence. This is a very advanced form\nof distributional learning, but the core intuition is very similar to that of the single vector models: we are\nlearning linguistic properties entirely from co-occurrence patterns in corpus data.\nIn our main text, we report results for RoBERTa (Liu et al., 2019), which is ‘ Robustly optimized\nBERT approach’. We focus on the ‘large’ variant, which has 24 hidden layers. In Appendix D, we\nreport parallel experiments with the case-sensitive version of the original BERT model as well as two\nvariants of the new DeBERTa model: version 1 and version 3 (which introduces some modifications to\nthe pretraining regime compared to version 1). DeBERTa is potentially interesting from the perspective\nof lexical semantics, because it more fully separates the traditional static embeddings [x1, . . .xn] from\nthe positional embeddings [p1, . . . ,pn]. This might be taken to reify word types (as separate from token\noccurrences) more than the other models do. Like RoBERTa, BERT and the DeBERTa variants have\n‘base’ (12-layer) and ‘large’ (24-layers) instances. For our main text, we chose to focus on RoBERTa-large\nbecause it seems slightly better overall than the rest, but our findings indicate that all these models perform\nabout the same in our evaluations, suggesting that all of them can support lexical semantic investigation.\n503\n1. break 11. before\n2. breaking 12. put\n3. broke 13. start\n4. breaks 14. take\n5. set 15. trying\n6. try 16. could\n7. chance 17. to\n8. time 18. broken\n9. again 19. end\n10. back 20. finally\n(a) GloVe, Wikipedia+Gigaword, 300d.\n1. break 11. weeks\n2. time 12. start\n3. breaks 13. last\n4. before 14. end\n5. then 15. broke\n6. take 16. again\n7. days 17. next\n8. after 18. maybe\n9. let 19. leave\n10. up 20. down\n(b) GloVe, Twitter, 2B tweets, 200d.\n1. break 11. get\n2. breaks 12. out\n3. breaking 13. trying\n4. broke 14. we\n5. going 15. broken\n6. let 16. again\n7. away 17. come\n8. take 18. down\n9. up 19. make\n10. ’ll 20. before\n(c) GloVe, Common Crawl 42B tokens,\n300d.\n1. break 11. up\n2. breaks 12. trying\n3. breaking 13. away\n4. end 14. start\n5. broke 15. get\n6. down 16. again\n7. take 17. ’ll\n8. let 18. back\n9. going 19. out\n10. leave 20. off\n(d) GloVe, Common Crawl 840B tokens, 300d (from Table 4).\n1. breaks 11. brief_respite\n2. breaking 12. Nadal_netted_forehand\n3. broke 13. loosen\n4. broken 14. smash\n5. Break 15. rip\n6. Breaking 16. overhit_forehand\n7. breather 17. miscued_forehand\n8. shatter 18. cut\n9. crack 19. slip\n10. breaker 20. Breaks\n(e) word2vec, GoogleNews, 300d (from Table 4).\n1. break 11. follow\n2. breaks 12. smash\n3. breaking 13. BREAK\n4. broke 14. knock\n5. Break 15. water-main\n6. broken 16. miss\n7. crack 17. tie\n8. take 18. go\n9. shatter 19. relax\n10. fix 20. start\n(f) fastTest WikiNews, 300d.\n1. break 11. breakin\n2. breaks 12. breaked\n3. breaking 13. broken\n4. breake 14. legbreak\n5. re-break 15. reak\n6. break- 16. semi-break\n7. unbreak 17. minibreak\n8. breakes 18. breaker\n9. break. 19. breaking-down\n10. broke 20. tea-break\n(g) fastTest WikiNews, subword modeling, 300d (from Table 4).\n1. break 11. break.The\n2. breaks 12. break.I\n3. breaking 13. break.It\n4. Break 14. break.This\n5. broke 15. broken\n6. break.And 16. break.So\n7. Breaking 17. break.In\n8. break. 18. break-\n9. BREAK 19. breack\n10. Breaks 20. break.That\n(h) fastTest Common Crawl 600B tokens, 300d.\n1. break 11. take\n2. breaks 12. broken\n3. breaking 13. re-break\n4. Break 14. breake\n5. broke 15. abreak\n6. break. 16. break.But\n7. break.And 17. break-\n8. rebreak 18. break.What\n9. break.So 19. bend\n10. breack 20. break.That\n(i) fastTest, Common Crawl 600B tokens, subword modeling, 300d.\nTable 7: Static embedding spaces: closest neighbors of break.\n504\nD Additional Probing Results\nTable 8a gives meaning-class probing results for all of the models described in Appendix C, and Table 8b\nprovides a parellel set of results for the construction-type probes. The models are very consistent with\neach other in terms of layer-wise trends and overall performance. Only the DeBERTa variant stands out as\nshowing differences that may be truly substantive.\nProbe Control Selectivity\nbert-base-cased\n1 0.64 0.04 0.60\n6 0.80 0.03 0.77\n12 0.81 0.03 0.78\nbert-large-cased\n1 0.65 0.04 0.61\n6 0.78 0.03 0.75\n12 0.83 0.03 0.80\n18 0.83 0.03 0.81\n24 0.84 0.03 0.81\ndeberta-base\n1 0.72 0.03 0.68\n6 0.81 0.03 0.78\n12 0.85 0.03 0.82\ndeberta-large\n1 0.72 0.04 0.68\n6 0.84 0.03 0.81\n12 0.81 0.04 0.78\n18 0.78 0.04 0.74\n24 0.83 0.03 0.81\ndeberta-v3-base\n1 0.70 0.04 0.65\n6 0.84 0.03 0.81\n12 0.75 0.03 0.72\ndeberta-v3-large\n1 0.66 0.04 0.62\n6 0.84 0.04 0.80\n12 0.83 0.03 0.79\n18 0.80 0.04 0.77\n24 0.79 0.04 0.75\nroberta-base\n1 0.66 0.03 0.63\n6 0.81 0.04 0.78\n12 0.83 0.03 0.80\nroberta-large\n1 0.33 0.03 0.30\n6 0.81 0.03 0.79\n12 0.83 0.03 0.80\n18 0.80 0.03 0.76\n24 0.86 0.03 0.83\n(a) Meaning class.\nProbe Control Selectivity\nbert-base-cased\n1 0.75 0.34 0.40\n6 0.93 0.34 0.60\n12 0.95 0.33 0.63\nbert-large-cased\n1 0.72 0.33 0.39\n6 0.91 0.34 0.57\n12 0.94 0.33 0.62\n18 0.97 0.35 0.62\n24 0.97 0.33 0.63\ndeberta-base\n1 0.88 0.34 0.54\n6 0.96 0.34 0.62\n12 0.97 0.32 0.64\ndeberta-large\n1 0.86 0.33 0.53\n6 0.96 0.33 0.63\n12 0.96 0.33 0.64\n18 0.95 0.34 0.61\n24 0.96 0.34 0.63\ndeberta-v3-base\n1 0.87 0.32 0.54\n6 0.96 0.34 0.62\n12 0.94 0.32 0.61\ndeberta-v3-large\n1 0.80 0.34 0.45\n6 0.94 0.34 0.61\n12 0.96 0.33 0.64\n18 0.97 0.32 0.65\n24 0.95 0.36 0.60\nroberta-base\n1 0.82 0.33 0.49\n6 0.96 0.34 0.62\n12 0.96 0.32 0.64\nroberta-large\n1 0.50 0.33 0.17\n6 0.94 0.34 0.60\n12 0.96 0.33 0.63\n18 0.96 0.35 0.61\n24 0.97 0.32 0.65\n(b) Construction type.\nTable 8: Full probing results.\n505\nE Full Meaning Class Distribution\nTable 9 gives the full set of meaning classes, with their counts, from the dataset of Petersen 2020. There\nare 72 classes in all. Our meaning-class probing experiments use only the 27 classes with at least 10\nexamples. Our construction-type probing experiments use the full dataset.\nMeaning class\nseparate_into_parts 150\nend 126\ndecipher 62\nbreak_down_separate_into_parts 61\nviolate 59\nbreak_up_separate_into_parts 35\nsurpass 34\nbreak_down_destroy 31\nbreak_into_intrude 28\nreveal 26\nappear 25\nbreak_through_pass_through 24\nrender_inoperable 23\nunclassified 21\nbreak_down_render_inoperable 21\nbreak_free_escape 19\nbreak_down_succumb 18\ncause_to_fail 17\nbreak_up_end_relationship 17\nbreak_up_end 16\nbreak_out_escape 15\nbreak_even_profit=loss 14\nsuccumb 13\nbreak_out_start 12\nexperience_sorrow 11\nbreak_away_detach 10\nbreak_off_end 10\nbreak_in_enter 9\nbreak_apart_detach 9\nbreak_off_detach 7\nbreak_for_pause 7\ndestroy 6\nbreak_into_start 6\npioneer 6\nlessen 6\nbreak_with_end_relationship 5\nMeaning class\nbreak_open_open 5\nbreak_in_interrupt 5\nbreak_loose_detach 5\nbegin_construction 4\neat_with_sb 4\nchange 4\nbreak_from_detach 3\ncost_too_much 3\nbreak_loose_start 3\nbreak_through_succeed 3\nbreak_up_destroy 3\nbreak_down_unclassified 3\nshow_disagreement_with_group 3\nslow_down 3\nbegin_to_sweat 2\nbreak_out_unclassified 2\nbreak_down_pause 2\nbreak_up_unclassified 2\nhappen 2\nbreak_out_separate_into_parts 2\nbreak_out_prepare_for_consumption 2\nbreak_in_mould_shoes 2\nbreak_down_fail 2\ndismantle_camp 2\nbreak_loose_escape 1\nbreak_into_unclassified 1\nbreak_off_stop 1\ngo_bankrupt 1\nbreak_away_pause 1\nbreak_in_train 1\nbreak_past_pass_through 1\ntame 1\nbreak_in_unclassified 1\nbreak_with_detach 1\nbreak_out_have_skin_eruption 1\nbreak_beef 1\nTable 9: Full meaning-class distribution.\n506\nF Examples Selected as Theoretically Relevant\nHere we provide the full set of examples extracted from our dataset using the procedure described in\nSection 5.3 and then selected by us as interesting for lexical semantic theory. The examples in bold are\nthose that appear in Table 6.\nMeaning Construction\nSentence Gold Predicted Gold Predicted Notes\nMost of this information exchange\ntakes place through what are known\nas newsgroups, which essentially just\nbreak all this international online bab-\nble up into different topics and areas\nof interest.\nbreak_up_\nseparate_\ninto_parts\nbreak_\ndown_\nseparate_\ninto_parts\ncausative causative Both senses seem active or pos-\nsible.\nWhat happens, when groups break up\nthat means somebody got caught steal-\ning the money or some guy does n’t\nlike it because another guy’s a bigger\nstar- KING: Or he married someone\nwho- Mr. GATLIN: Right@!KING.\nbreak_up_\nseparate_\ninto_parts\nbreak_up_\nend_\nrelationship\nunacc. unacc. Both senses seem active.\nWind, naturally acidic rain,\nand physical processes such as\nfreezethaw cycles also break down\nrock.\nbreak_\ndown_\nseparate_\ninto_parts\nbreak_\ndown_\ndestroy\ncausative causative Both senses seem active.\nBut her husband was determined not\nto break up the family.\nbreak_up_\nseparate_\ninto_parts\nbreak_up_\nend_\nrelationship\ncausative causative Both senses seem active.\nIt was like you knew something, like\nyou knew the story was getting ready\nto break again.\nreveal appear unacc. unacc. Both senses seem active.\n“So why tell the whole story now?\nSomebody, some male, has got to be\nwilling to break this code of silence,”\nhe says.\nviolate end causative causative Contextual entailment relation\nbetween the two labels.\nThen too, stress can also work to\nbreak down the immune system, in-\ncreasing the likelihood of respira-\ntory and creating gastrointestinal\nand nervous disorders.\nbreak_\ndown_\nrender_\ninoperable\nbreak_\ndown_\ndestroy\ncausative causative Contextual entailment relation\nbetween the two labels.\nIf you deprive yourself, you’re going\nto break your diet and fall off it.\nviolate end causative causative Contextual entailment relation\nbetween the two labels.\nSen. BOB KERREY: I don’t want to\ndestroy Social Security or break a com-\nmitment.\nviolate end causative causative Contextual entailment relation\nbetween the two labels.\nSo they forwarded the pictures to\nMadrid, where another officer no-\nticed some printing on a towel that\nhelped break the case.\ndecipher end causative causative Contextual entailment relation\nbetween the two labels.\nIt’s one example of how the standard\nmodel might break down.\nbreak_\ndown_\nrender_\ninoperable\nbreak_\ndown_\nsuccumb\nunacc. unacc. Contextual entailment relation\nbetween the two labels.\nInstead, crews will break down the\nstructures over three years, releasing\nthe water in the reservoirs at a rate\nthat’s more manageable for the animals\nand the people who live in the area.\nbreak_\ndown_\nseparate_\ninto_parts\nbreak_\ndown_\ndestroy\ncausative causative Contextual entailment relation\nbetween the two labels.\n507\n\"The Comes would try to break the\nSaxon ranks with a mounted charge.\nseparate_\ninto_parts\nend causative causative Contextual entailment relation\nbetween the two labels.\nThen the troops break formation and\nmove out to a formation and stand\nguard, even from above, making sure\nthe so-called detainees are safely be-\nhind the fence.\nseparate_\ninto_parts\nend causative causative Contextual entailment relation\nbetween the two labels.\nThe Soviet Union will break up into\nbetween six and twenty (or more) sep-\narate countries.\nbreak_up_\nseparate_\ninto_parts\nbreak_up_\nend\nunacc. unacc. Contextual entailment relation\nbetween the two labels.\nIt didn’t take being an ICU exec to\nbreak the code: trade secret.\ndecipher violate causative causative Genuine uncertainty about\nwhich sense is intended.\n@(Soundbite-of-music)@!Mr-\nGELB: (Singing) Tell me who’s\ngoing to pick up the pieces when you\nstart to break down.\nbreak_\ndown_\nseparate_\ninto_parts\nbreak_\ndown_\nsuccumb\nunacc. unacc. Gold meaning is literal; pre-\ndicted meaning is metaphori-\ncal.\n“People have so many problems\novercoming the disputes that occur\nwhen families break up, and then to\nhave to overcome the barriers that\ngovernment puts up when they hold\non to the money, literally sends chil-\ndren to bed hungry,”says Jensen.\nbreak_up_\nend_\nrelationship\nbreak_up_\nseparate_\ninto_parts\nunacc. unacc. Gold meaning is metaphorical;\npredicted meaning is literal.\n\"I just don’t want to break up such\nhappy couples.\nbreak_up_\nend_\nrelationship\nbreak_up_\nseparate_\ninto_parts\ncausative causative Gold meaning is metaphorical;\npredicted meaning is literal.\nI had to break it up. break_up_\nend\nbreak_up_\nseparate_\ninto_parts\ncausative causative Gold meaning is metaphorical;\npredicted meaning is literal.\nWill the kibbutz movement \"renew its\ndays as of old\" when it has recovered\nfrom the present crisis, as did the Hut-\nterites at several points in their history?\nWill it continue to exist, but in a rad-\nically revised form, like Amana and\nother colonies? Or will the kibbutzim\nsimply break up, to form part of the\nhistorical heritage of the Israeli nation,\nand no more– like so many of the well-\npreserved sites that aroused such pow-\nerful feelings in Yaakov Oved? The\nconsiderations I have advanced here\nseem to militate against the first of\nthese possibilities and favor one of the\nothers– perhaps a mixture of both.\nbreak_up_\nend\nbreak_up_\nseparate_\ninto_parts\nunacc. unacc. Gold meaning is metaphorical;\npredicted meaning is literal.\nThe past few days had consisted of a\nsimple routine of drinking melted snow\nto stay hydrated and sleeping while\nwaiting for the storm to break.\nappear end unacc. unacc. Model prediction may be cor-\nrect.\nA small pair of scissors will easily\nbreak the seal, but bringing those scis-\nsors in your carry-on bag may no\nlonger be permitted.\nseparate_\ninto_parts\ndecipher causative causative The decipher prediction seems\nsensible given that a seal is like\na lock or (easy) code that needs\nto be overcome.\nPatients will sometimes break out\nin a spontaneous recitation of the\nrosary\nbreak_out_\nstart\nbreak_out_\nstart\nunacc. unerg. The modifier \"spontaneous\"\nseems to affect agentivity and\nperhaps also argument struc-\nture.\n508\nMillennial darlings began to break\ndown like virus-ridden websites, from\nthe supercharged (Qualcomm, Oracle)\nto the superhyped (Amazon, Yahoo!)\nto the just plain super (Sun, Lucent,\nAOL).\nbreak_\ndown_\nsuccumb\nbreak_\ndown_\nrender_\ninoperable\nunacc. unacc. There is a comparison of \"mil-\nlennial darlings\" with \"virus-\nridden websites\". The gold\nmeaning may apply to \"millen-\nnial darlings\" and the predicted\nmeaning to \"virus-ridden web-\nsites\".\nI felt disappointed, but I waited, hop-\ning the clouds would break.\nseparate_\ninto_parts\nappear unacc. unacc. Weather events are persistently\nuncertain about whether they\ndescribe the start or end of\nsomething. Here, the clouds\nare leaving and other things are\npresumably appearing.\nG Visualizations\nFigure 2 uses t-SNE to visualize break embeddings from layer 1 of RoBERTa-large, and Figure 3 shows\nthe embeddings from layer 24. We use color to distinguish the top 10 meaning classes (and the rest\nare gray). Underlined examples are unergative and boxed examples are unaccusative. The layer 24\nvisualization has much more structure than the layer 1 visualization. By layer 24, the model seems\nstrikingly well-aligned with the meaning categories and construction types, as evidenced by how examples\nwith the same color cluster together, and how the construction type annotations also cluster within those\nspaces. The other models we consider show effectively these same patterns.\n509\nhappen\nbreak-free-escape\nend\nseparate-into-parts\nbreak-down-succumb\nbreak-free-escape\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-through-pass-through\nbreak-down-destroy\nbreak-through-pass-through\nbreak-oﬀ-end\nunclassiﬁed\nseparate-into-parts\nunclassiﬁed\nbreak-down-destroy\nbreak-up-destroy\nbreak-down-destroy\nbreak-down-separate-into-parts\nend\nend\nbreak-into-intrude\nend\nbreak-free-escape\nviolate\nend\nviolate\nbreak-down-succumb\nbreak-into-intrude\nseparate-into-parts\nviolate\nseparate-into-parts\nend\nbreak-even-proﬁt=loss\nviolate\nend\nviolate\nend\nlessen\nseparate-into-parts\nbreak-out-escape\ncause-to-fail\nseparate-into-parts\nbreak-up-end\nrender-inoperable\nend\nexperience-sorrow\ndestroy\nseparate-into-parts\nbreak-even-proﬁt=loss\nsurpass\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nend\nend\nexperience-sorrow\nbreak-oﬀ-end\nshow-disagreement-with-group\nbreak-for-pause\nseparate-into-parts\nexperience-sorrow\nseparate-into-parts\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-oﬀ-detach\nbreak-out-start\nbreak-down-destroy\nbreak-down-succumb\nbreak-in-enter\nbreak-into-intrude\nbreak-into-intrude\nviolate\nbreak-in-train\nseparate-into-parts\nsuccumb\nseparate-into-parts\nsuccumb\nbreak-for-pause\nbreak-into-start\ndismantle-camp\nbreak-down-separate-into-parts\nunclassiﬁed\nbreak-loose-start\nreveal\nend\nend\nseparate-into-parts\nbreak-down-render-inoperable\nbreak-away-detach\nbreak-down-render-inoperable\nviolate\nviolate\nrender-inoperable\nbreak-down-separate-into-parts\nreveal\nbreak-up-end-relationship\nseparate-into-parts\nseparate-into-parts\nend\nexperience-sorrow\nseparate-into-parts\nbreak-down-separate-into-parts\nend\nbreak-out-escape\nseparate-into-parts\nbreak-in-interrupt\nend\ncost-too-much\nbreak-through-pass-through\nend\nseparate-into-parts\nreveal\nbreak-into-start\nend\nend\nbreak-in-enter\nend\nseparate-into-parts\nbreak-loose-start\nunclassiﬁed\nbreak-down-succumb\nbreak-oﬀ-end\nseparate-into-parts\nreveal\nend\nexperience-sorrow\nviolate\ndecipher\nend\ndecipher\nseparate-into-parts\nbreak-in-enter\nend\nseparate-into-parts\nseparate-into-parts\nsurpassend\nbreak-through-pass-through\nend\nseparate-into-parts\nbreak-into-intrude\nviolate\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-free-escape\nend\nbreak-out-have-skin-eruption\nbreak-apart-detach\nunclassiﬁed\nbreak-down-render-inoperable\nunclassiﬁed\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nsuccumb\nreveal\nbreak-through-pass-through\nsurpass\nviolate\nbreak-down-destroy\nend\nbreak-in-enter\nbreak-up-end\nbreak-into-intrude\nbreak-up-separate-into-parts\nbreak-up-separate-into-parts\nseparate-into-parts\nend\nbreak-down-destroy\nbreak-beef\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nsurpass\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-free-escape\nbreak-down-separate-into-parts\nbreak-up-end-relationship\nend\nsuccumb\nbreak-through-pass-through\nbreak-free-escape\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\neat-with-sb\nexperience-sorrow\nsuccumb\nseparate-into-parts\nsurpass\nbreak-into-start\nbreak-even-proﬁt=loss\nbreak-down-render-inoperable\nbreak-into-intrude\nbreak-down-destroy\nbreak-out-start\nseparate-into-partsbreak-down-separate-into-parts\nviolate\nbreak-down-succumb\nbreak-out-escape\nendbreak-in-enter\nend\nbreak-down-separate-into-parts\nlessen\nseparate-into-parts\nbreak-loose-detach\nbreak-out-escape\nend\nbreak-down-succumb\nviolate\nbreak-through-pass-through\nbreak-down-destroy\nend\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-down-separate-into-parts\nbreak-down-render-inoperable\nbreak-up-separate-into-parts\nbreak-into-start\nsuccumb\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-out-start\nbreak-in-enter\nbreak-down-succumb\nbreak-down-destroy\nbreak-up-separate-into-parts\nbreak-through-pass-through\ncause-to-fail\nend\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-even-proﬁt=loss\nbreak-up-separate-into-parts\nend\nbreak-in-interrupt\nseparate-into-parts\nsurpass\nend\nseparate-into-parts\nbreak-up-end-relationship\nend\nsuccumb\nbreak-with-end-relationship\nend\nexperience-sorrow\nbreak-down-succumb\nbreak-into-intrude\ndecipher\nbreak-down-separate-into-parts\nend\nbreak-in-mould-shoes\nbreak-free-escape\nviolate\nsurpass\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-through-succeed\nbreak-free-escape\nbreak-up-separate-into-parts\nbreak-through-pass-through\nseparate-into-parts\nbreak-down-succumb\nseparate-into-parts\nviolate\nbreak-free-escape\nseparate-into-parts\nbreak-up-end-relationship\nsurpass\nbreak-down-render-inoperable\nunclassiﬁed\nreveal\nbreak-oﬀ-end\nbreak-down-fail\nsurpass\nend\nbreak-out-start\nbreak-free-escape\nbreak-out-escape\nseparate-into-parts\nbreak-down-separate-into-parts\nend\nbreak-oﬀ-end\nbreak-up-end-relationship\nbreak-through-pass-through\nlessen\nviolate\ncause-to-fail\nseparate-into-parts\nseparate-into-parts\nbreak-into-start\nend\nbreak-down-render-inoperable\nbreak-down-destroy\nend\nreveal\nsurpass\ncost-too-much\ndestroy\nbreak-up-end\nbreak-down-separate-into-parts\nsurpass\ngo-bankrupt\nbreak-oﬀ-detach\nseparate-into-parts\nbreak-away-detach\nend\nbreak-through-pass-through\nseparate-into-parts\nbreak-out-escape\ndismantle-camp\nsuccumb\nbreak-up-end-relationship\nbreak-down-separate-into-parts\nend\nseparate-into-parts\nbreak-even-proﬁt=loss\nseparate-into-parts\nbreak-through-pass-through\nseparate-into-parts\nbreak-down-destroy\nbreak-free-escape\nbreak-out-prepare-for-consumption\ndestroy\nsuccumb\nviolate\nunclassiﬁed\nbreak-loose-detach\nbreak-through-pass-through\nsurpass\nseparate-into-parts\nbreak-down-separate-into-parts\nend\nbreak-out-start\nviolate\nviolate\nbreak-in-enter\nseparate-into-parts\nviolate\nseparate-into-parts\nbreak-up-end\nseparate-into-parts\nseparate-into-parts\nend\npioneer\nlessen\nbreak-up-end\nbreak-down-separate-into-parts\nend\nbreak-through-pass-through\nbreak-out-escape\nbreak-down-succumb\nend\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-oﬀ-end\nsurpass\nexperience-sorrow\neat-with-sb\nbreak-up-destroy\nbreak-into-intrude\nbreak-with-detach\nend\nbegin-construction\nbreak-apart-detach\nseparate-into-parts\nviolate\nexperience-sorrow\ncause-to-fail\nhappen\nend\nend\nbreak-even-proﬁt=loss\nbreak-out-separate-into-parts\nend\nend\nseparate-into-parts\nrender-inoperable\nbreak-down-destroy\nend\ncause-to-fail\ncause-to-fail\nviolate\nviolate\nbreak-down-succumb\nseparate-into-parts\nrender-inoperable\nbreak-down-separate-into-parts\nviolate\npioneer\neat-with-sb\nbreak-away-detach\nend\ndecipher\nbreak-up-separate-into-parts\nend\nsurpass\nviolate\nbegin-to-sweat\nseparate-into-parts\nviolate\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-for-pause\nviolate\nbreak-into-intrude\nbreak-down-render-inoperable\nbreak-down-separate-into-parts\ndecipher\nend\nend\nend\nseparate-into-parts\nend\nbreak-loose-start\nend\nbreak-up-end-relationship\nseparate-into-parts\nslow-down\nbreak-up-separate-into-parts\nbreak-with-end-relationship\nend\nbreak-through-pass-through\nbreak-up-separate-into-parts\nbreak-for-pause\nreveal\nsurpass\nunclassiﬁed\nseparate-into-parts\ncause-to-fail\nreveal\nseparate-into-parts\nend\nend\nviolate\nbreak-down-separate-into-parts\nbreak-down-destroy\nbreak-through-succeed\nbreak-with-end-relationship\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-oﬀ-detach\nseparate-into-partsviolate\nend\nbreak-even-proﬁt=loss\nbreak-down-succumb\nend\nreveal\nbreak-out-escape\nbreak-loose-detach\nseparate-into-parts\nbreak-through-pass-through\nbreak-from-detach\nslow-down\nbreak-up-end\nviolate\nbreak-down-destroy\nbreak-down-separate-into-parts\nbreak-out-escape\nseparate-into-parts\nappear\nbreak-up-separate-into-parts\nviolate\nbreak-in-mould-shoes\nbreak-away-detach\nend\nbreak-with-end-relationship\nbreak-down-render-inoperable\nbreak-up-end\nseparate-into-parts\nbreak-out-start\nend\nbreak-even-proﬁt=loss\nlessen\nbreak-up-end\nreveal\nunclassiﬁed\nbreak-down-succumb\nbreak-oﬀ-end\nseparate-into-parts\npioneer\nsurpass\nbreak-into-start\nend\nbreak-free-escape\nseparate-into-parts\nslow-down\nbreak-up-separate-into-parts\nviolate\nviolate\nviolate\nend\nbreak-in-unclassiﬁed\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nreveal\nbreak-out-escape\ntame\nend\nbreak-down-separate-into-parts\nbreak-out-escapeseparate-into-parts\nbreak-down-unclassiﬁed\nbreak-apart-detach\nseparate-into-parts\nsuccumb\nseparate-into-parts\nseparate-into-parts\nsurpass\nbreak-down-separate-into-parts\nseparate-into-parts\nsurpass\nbreak-into-intrude\nbreak-down-separate-into-parts\nend\nsurpass\nbreak-up-separate-into-parts\nexperience-sorrow\nend\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nviolate\nbreak-down-separate-into-parts\nreveal\nbreak-into-intrude\nbreak-even-proﬁt=loss\nbreak-free-escape\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-in-interrupt\nend\nend\nbreak-through-pass-through\nend\nbreak-up-end-relationship\nsurpass\nbreak-even-proﬁt=loss\nbreak-out-escape\nend\nseparate-into-parts\nbreak-down-destroy\nbreak-oﬀ-end\nsuccumb\nbreak-oﬀ-detach\nbreak-down-separate-into-parts\nend\nbreak-down-succumb\nbreak-down-destroy\nbreak-into-intrude\nseparate-into-parts\nbreak-down-separate-into-parts\nbegin-construction\nseparate-into-parts\nbreak-away-detach\nreveal\nsurpass\nreveal\nend\nseparate-into-parts\nend\nbreak-up-separate-into-parts\nbreak-down-render-inoperable\nexperience-sorrow\nseparate-into-parts\nbreak-oﬀ-end\nunclassiﬁed\ndestroy\nseparate-into-parts\nend\nrender-inoperable\nbreak-down-destroy\nbreak-down-separate-into-parts\nseparate-into-parts\nunclassiﬁed\nviolate\nviolate\ncause-to-fail\nbreak-down-destroy\nbreak-up-end-relationship\ncause-to-fail\nsurpass\nsurpass\nbreak-in-enter\nbreak-past-pass-through\nseparate-into-parts\nbreak-up-end-relationship\nbreak-away-pause\nseparate-into-parts\nbreak-apart-detach\nend\nbreak-open-open\nbreak-up-separate-into-parts\nbreak-up-end\nbreak-up-destroy\nbreak-down-succumb\nbreak-up-end\nbreak-from-detach\nbreak-up-end-relationship\nseparate-into-parts\nbreak-free-escape\nend\nbreak-even-proﬁt=loss\nbegin-to-sweat\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-down-render-inoperable\nbreak-up-separate-into-parts\nreveal\nunclassiﬁed\nviolate\nseparate-into-parts\nviolate\nappear\nsurpass\ndestroy\nseparate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nsurpass\nbreak-into-intrude\nbreak-down-separate-into-parts\nbreak-down-destroy\nbreak-down-separate-into-parts\nbreak-out-escape\nbreak-down-separate-into-parts\nbreak-down-render-inoperable\nbreak-free-escape\nbreak-up-unclassiﬁed\nbreak-down-separate-into-parts\nbreak-up-end-relationship\nend\nbreak-open-open\nseparate-into-parts\nseparate-into-parts\nend\nbreak-down-separate-into-parts\nend\npioneer\nseparate-into-parts\ncause-to-fail\nbreak-in-interrupt\nbreak-out-escape\ncause-to-fail\nbreak-out-start\nreveal\nbreak-into-intrude\nend\nbreak-up-unclassiﬁed\nseparate-into-parts\nseparate-into-parts\nend\nbegin-construction\nbreak-through-succeed\nbreak-apart-detach\nbreak-open-open\nsuccumb\nseparate-into-parts\nbreak-for-pause\nbreak-down-render-inoperable\neat-with-sb\nbreak-away-detach\nbreak-down-succumb\nbreak-into-intrude\nbreak-up-separate-into-parts\nlessen\nrender-inoperable\nbreak-up-end-relationship\nend\nbreak-up-end-relationship\nsuccumb\nbreak-through-pass-through\nseparate-into-parts\nbreak-up-end\nseparate-into-parts\nend\nviolate\ndecipher\nbreak-down-render-inoperable\nbreak-in-enter\nend\nseparate-into-parts\nbreak-up-separate-into-parts\nunclassiﬁed\nbreak-away-detach\nbreak-into-intrude\nend\nbreak-out-separate-into-parts\nbreak-down-succumb\nbreak-down-separate-into-parts\nbreak-down-fail\nreveal\nbreak-from-detach\nbreak-down-destroy\nbreak-up-separate-into-parts\nbreak-into-intrude\nbreak-down-separate-into-parts\nbreak-open-open\nviolate\nbreak-in-interrupt\nend\nbreak-with-end-relationship\nbreak-apart-detach\ncause-to-fail\nbreak-down-separate-into-parts\nend\ncost-too-much\nend\nreveal\npioneer\nreveal\nend\nbreak-oﬀ-detach\nbreak-oﬀ-stop\nbreak-down-separate-into-parts\nbreak-into-intrude\nend\ncause-to-fail\nbreak-into-intrude\nbegin-construction\nbreak-oﬀ-detach\nbreak-down-separate-into-parts\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-down-separate-into-parts\nbreak-oﬀ-end\nseparate-into-parts\nbreak-out-start\nviolate\nsurpass\nbreak-free-escape\nappear\nbreak-down-separate-into-parts\nbreak-into-unclassiﬁed\nend\ncause-to-fail\ncause-to-fail\nbreak-down-succumb\nviolate\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-out-start\nunclassiﬁed\nsurpass\nviolate\npioneer\nseparate-into-parts\nchange\nbreak-out-prepare-for-consumption\nbreak-even-proﬁt=loss\nend\nend\nviolate\nbreak-loose-escape\nviolate\nreveal\nseparate-into-parts\nend\nend\nbreak-up-end\nend\nseparate-into-parts\nbreak-even-proﬁt=loss\nbreak-down-separate-into-parts\nend\nviolate\nend\nseparate-into-parts\nend\nbreak-down-destroy\nbreak-out-start\nseparate-into-parts\nbreak-free-escape\nseparate-into-parts\nsurpass\nviolate\nrender-inoperable\nsurpass\nend\nbreak-away-detach\nend\nseparate-into-parts\nsurpass\nbreak-for-pause\nbreak-down-unclassiﬁed\nend\nend\nseparate-into-parts\nseparate-into-partsseparate-into-parts\nbreak-into-intrude\nseparate-into-parts\nbreak-down-render-inoperable\nsurpass\nbreak-down-destroy\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-down-separate-into-parts\nviolate\nbreak-up-separate-into-parts\nbreak-loose-detach\nbreak-out-start\nseparate-into-parts\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nbreak-down-separate-into-parts\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-out-escape\nbreak-free-escape\nviolate\nbreak-apart-detach\ncause-to-fail\nunclassiﬁed\nbreak-down-destroy\nbreak-into-intrude\nend\nbreak-up-separate-into-parts\nseparate-into-parts\nend\nbreak-apart-detach\nseparate-into-parts\nbreak-free-escape\nend\nbreak-down-destroy\nbreak-down-destroy\nbreak-down-separate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-up-end\nbreak-down-separate-into-parts\nviolate\nend\nbreak-into-intrude\ndestroy\nunclassiﬁed\nseparate-into-parts\nend\nseparate-into-parts\nbreak-down-pause\nend\nbreak-up-end-relationship\nseparate-into-parts\nbreak-down-separate-into-parts\nunclassiﬁed\nseparate-into-parts\nseparate-into-parts\nbreak-loose-detach\nbreak-away-detach\nsurpass\nviolate\nbreak-into-intrude\ncause-to-fail\nend\nviolate\nseparate-into-parts\nseparate-into-parts\nend\nbreak-up-end\nreveal\nbreak-down-separate-into-parts\nend\nbreak-even-proﬁt=loss\nseparate-into-parts\nbreak-up-end\nreveal\nbreak-free-escape\nbreak-down-separate-into-parts\nbreak-up-end-relationship\nreveal\nrender-inoperable\nbreak-down-separate-into-parts\nshow-disagreement-with-group\nbreak-into-intrude\nbreak-up-end\nsurpass\nseparate-into-parts\nshow-disagreement-with-group\nbreak-out-start\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nreveal\nend\nseparate-into-parts\nend\nunclassiﬁed\nbreak-up-separate-into-parts\nbreak-out-unclassiﬁed\nviolate\nbreak-down-render-inoperable\nbreak-oﬀ-detach\ndecipher\nbreak-into-intrude\ndecipher\nbreak-out-unclassiﬁed\nend\nbreak-down-separate-into-parts\nbreak-down-pause\nviolate\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-down-separate-into-parts\nsurpass\nend\nbreak-open-open\nunclassiﬁed\nbreak-away-detach\nbreak-for-pause\nbreak-apart-detach\nunclassiﬁed\nbreak-down-unclassiﬁed\nreveal\nviolate\nend\nbreak-into-intrude\nseparate-into-parts\nviolate\nbreak-down-destroy\nbreak-up-end-relationship\nseparate-into-parts\nbreak-into-intrude\nseparate-into-parts\nend\nviolate\nviolate\nbreak-through-pass-through\nbreak-down-render-inoperable\nbreak-down-render-inoperable\nbreak-down-render-inoperable\nrender-inoperable\nrender-inoperable\nbreak-down-render-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\ndecipher\ndecipher\ndecipher\ndecipher\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\ndecipher\nbreak-down-render-inoperable\ndecipher\nrender-inoperable\ndecipher\ndecipher\nrender-inoperable\nrender-inoperable\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\nchange\nappear\nappear\nappear\nchange\nchange\nbreak-through-pass-through\nappear\nappear\nchange\nappear\nappearappear\nappear\nappear\nappear\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nFigure 2: t-SNE of break with RoBERTa-large, layer 1.\n510\nhappen\nbreak-free-escape\nend\nseparate-into-parts\nbreak-down-succumb\nbreak-free-escape\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-through-pass-through\nbreak-down-destroy\nbreak-through-pass-through\nbreak-oﬀ-end\nunclassiﬁed\nseparate-into-parts\nunclassiﬁed\nbreak-down-destroy\nbreak-up-destroy\nbreak-down-destroy\nbreak-down-separate-into-parts\nend\nend\nbreak-into-intrude\nend\nbreak-free-escape\nviolate\nend\nviolate\nbreak-down-succumb\nbreak-into-intrude\nseparate-into-parts\nviolate\nseparate-into-parts\nend\nbreak-even-proﬁt=loss\nviolate\nend\nviolate\nend\nlessen\nseparate-into-parts\nbreak-out-escape\ncause-to-fail\nseparate-into-parts\nbreak-up-end\nrender-inoperable\nend\nexperience-sorrow\ndestroy\nseparate-into-parts\nbreak-even-proﬁt=loss\nsurpass\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nend\nend\nexperience-sorrow\nbreak-oﬀ-end\nshow-disagreement-with-group\nbreak-for-pause\nseparate-into-parts\nexperience-sorrow\nseparate-into-parts\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-oﬀ-detach\nbreak-out-start\nbreak-down-destroy\nbreak-down-succumb\nbreak-in-enter\nbreak-into-intrudebreak-into-intrude\nviolate\nbreak-in-train\nseparate-into-parts\nsuccumb\nseparate-into-parts\nsuccumb\nbreak-for-pause\nbreak-into-start\ndismantle-camp\nbreak-down-separate-into-parts\nunclassiﬁed\nbreak-loose-start\nreveal\nendend\nseparate-into-parts\nbreak-down-render-inoperable\nbreak-away-detach\nbreak-down-render-inoperable\nviolate\nviolate\nrender-inoperable\nbreak-down-separate-into-parts\nreveal\nbreak-up-end-relationship\nseparate-into-parts\nseparate-into-parts\nend\nexperience-sorrow\nseparate-into-parts\nbreak-down-separate-into-parts\nend\nbreak-out-escape\nseparate-into-parts\nbreak-in-interrupt\nend\ncost-too-much\nbreak-through-pass-through\nend\nseparate-into-parts\nreveal\nbreak-into-start\nend\nend\nbreak-in-enter\nend\nseparate-into-parts\nbreak-loose-start\nunclassiﬁed\nbreak-down-succumb\nbreak-oﬀ-end\nseparate-into-parts\nreveal\nend\nexperience-sorrow\nviolate\ndecipher\nend\ndecipher\nseparate-into-parts\nbreak-in-enter\nend\nseparate-into-parts\nseparate-into-parts\nsurpass\nend\nbreak-through-pass-through\nend\nseparate-into-parts\nbreak-into-intrude\nviolate\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-free-escape\nend\nbreak-out-have-skin-eruption\nbreak-apart-detach\nunclassiﬁed\nbreak-down-render-inoperable\nunclassiﬁed\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nsuccumb\nreveal\nbreak-through-pass-through\nsurpass\nviolate\nbreak-down-destroy\nend\nbreak-in-enter\nbreak-up-end\nbreak-into-intrude\nbreak-up-separate-into-parts\nbreak-up-separate-into-parts\nseparate-into-parts\nend\nbreak-down-destroy\nbreak-beef\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nsurpass\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-free-escape\nbreak-down-separate-into-parts\nbreak-up-end-relationship\nend\nsuccumb\nbreak-through-pass-through\nbreak-free-escape\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\neat-with-sb\nexperience-sorrow\nsuccumb\nseparate-into-parts\nsurpass\nbreak-into-start\nbreak-even-proﬁt=loss\nbreak-down-render-inoperable\nbreak-into-intrude\nbreak-down-destroy\nbreak-out-start\nseparate-into-parts\nbreak-down-separate-into-parts\nviolate\nbreak-down-succumb\nbreak-out-escape\nend\nbreak-in-enter\nend\nbreak-down-separate-into-parts\nlessen\nseparate-into-parts\nbreak-loose-detach\nbreak-out-escape\nend\nbreak-down-succumb\nviolate\nbreak-through-pass-through\nbreak-down-destroy\nend\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-down-separate-into-parts\nbreak-down-render-inoperable\nbreak-up-separate-into-parts\nbreak-into-start\nsuccumb\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-out-start\nbreak-in-enter\nbreak-down-succumb\nbreak-down-destroy\nbreak-up-separate-into-parts\nbreak-through-pass-through\ncause-to-fail\nend\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-even-proﬁt=loss\nbreak-up-separate-into-parts\nend\nbreak-in-interrupt\nseparate-into-parts\nsurpass\nendseparate-into-parts\nbreak-up-end-relationship\nend\nsuccumb\nbreak-with-end-relationship\nend\nexperience-sorrow\nbreak-down-succumb\nbreak-into-intrude\ndecipher\nbreak-down-separate-into-parts\nend\nbreak-in-mould-shoes\nbreak-free-escape\nviolate\nsurpass\nseparate-into-parts\nbreak-up-separate-into-parts\nbreak-through-succeed\nbreak-free-escape\nbreak-up-separate-into-parts\nbreak-through-pass-through\nseparate-into-parts\nbreak-down-succumb\nseparate-into-parts\nviolate\nbreak-free-escape\nseparate-into-parts\nbreak-up-end-relationship\nsurpass\nbreak-down-render-inoperable\nunclassiﬁed\nreveal\nbreak-oﬀ-end\nbreak-down-fail\nsurpass\nend\nbreak-out-start\nbreak-free-escape\nbreak-out-escape\nseparate-into-parts\nbreak-down-separate-into-parts\nend\nbreak-oﬀ-end\nbreak-up-end-relationship\nbreak-through-pass-through\nlessen\nviolate\ncause-to-fail\nseparate-into-parts\nseparate-into-parts\nbreak-into-start\nend\nbreak-down-render-inoperable\nbreak-down-destroy\nend\nreveal\nsurpass\ncost-too-much\ndestroy\nbreak-up-end\nbreak-down-separate-into-parts\nsurpass\ngo-bankrupt\nbreak-oﬀ-detach\nseparate-into-parts\nbreak-away-detach\nend\nbreak-through-pass-through\nseparate-into-parts\nbreak-out-escape\ndismantle-camp\nsuccumb\nbreak-up-end-relationship\nbreak-down-separate-into-parts\nend\nseparate-into-parts\nbreak-even-proﬁt=loss\nseparate-into-parts\nbreak-through-pass-through\nseparate-into-parts\nbreak-down-destroy\nbreak-free-escape\nbreak-out-prepare-for-consumption\ndestroy\nsuccumb\nviolate\nunclassiﬁed\nbreak-loose-detach\nbreak-through-pass-through\nsurpass\nseparate-into-parts\nbreak-down-separate-into-parts\nend\nbreak-out-start\nviolate\nviolate\nbreak-in-enter\nseparate-into-parts\nviolate\nseparate-into-parts\nbreak-up-end\nseparate-into-partsseparate-into-parts\nend\npioneer\nlessen\nbreak-up-end\nbreak-down-separate-into-parts\nend\nbreak-through-pass-through\nbreak-out-escape\nbreak-down-succumb\nend\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-oﬀ-end\nsurpass\nexperience-sorrow\neat-with-sb\nbreak-up-destroy\nbreak-into-intrude\nbreak-with-detach\nend\nbegin-construction\nbreak-apart-detach\nseparate-into-parts\nviolate\nexperience-sorrow\ncause-to-fail\nhappen\nend\nend\nbreak-even-proﬁt=loss\nbreak-out-separate-into-parts\nend\nend\nseparate-into-parts\nrender-inoperable\nbreak-down-destroy\nend\ncause-to-fail\ncause-to-fail\nviolate\nviolate\nbreak-down-succumb\nseparate-into-parts\nrender-inoperable\nbreak-down-separate-into-parts\nviolate\npioneer\neat-with-sb\nbreak-away-detach\nend\ndecipher\nbreak-up-separate-into-parts\nend\nsurpass\nviolate\nbegin-to-sweat\nseparate-into-parts\nviolate\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-for-pause\nviolate\nbreak-into-intrude\nbreak-down-render-inoperable\nbreak-down-separate-into-parts\ndecipher\nend\nend\nend\nseparate-into-parts\nend\nbreak-loose-start\nend\nbreak-up-end-relationship\nseparate-into-parts\nslow-down\nbreak-up-separate-into-parts\nbreak-with-end-relationship\nend\nbreak-through-pass-through\nbreak-up-separate-into-parts\nbreak-for-pause\nreveal\nsurpass\nunclassiﬁed\nseparate-into-parts\ncause-to-fail\nreveal\nseparate-into-parts\nend\nend\nviolate\nbreak-down-separate-into-parts break-down-destroy\nbreak-through-succeed\nbreak-with-end-relationship\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-oﬀ-detach\nseparate-into-parts\nviolate\nend\nbreak-even-proﬁt=loss\nbreak-down-succumb\nend\nreveal\nbreak-out-escape\nbreak-loose-detach\nseparate-into-parts\nbreak-through-pass-through\nbreak-from-detach\nslow-down\nbreak-up-end\nviolate\nbreak-down-destroy\nbreak-down-separate-into-parts\nbreak-out-escape\nseparate-into-parts\nappear\nbreak-up-separate-into-parts\nviolate\nbreak-in-mould-shoes\nbreak-away-detach\nend\nbreak-with-end-relationship\nbreak-down-render-inoperable\nbreak-up-end\nseparate-into-parts\nbreak-out-start\nend\nbreak-even-proﬁt=loss\nlessen\nbreak-up-end\nreveal\nunclassiﬁed\nbreak-down-succumb\nbreak-oﬀ-end\nseparate-into-parts\npioneer\nsurpass\nbreak-into-start\nend\nbreak-free-escape\nseparate-into-parts\nslow-down\nbreak-up-separate-into-parts\nviolate\nviolate\nviolate\nend\nbreak-in-unclassiﬁed\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nreveal\nbreak-out-escape\ntame\nend\nbreak-down-separate-into-parts\nbreak-out-escape\nseparate-into-parts\nbreak-down-unclassiﬁed\nbreak-apart-detach\nseparate-into-parts\nsuccumb\nseparate-into-parts\nseparate-into-parts\nsurpass\nbreak-down-separate-into-parts\nseparate-into-parts\nsurpass\nbreak-into-intrude\nbreak-down-separate-into-parts\nend\nsurpass\nbreak-up-separate-into-parts\nexperience-sorrow\nend\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nviolate\nbreak-down-separate-into-parts\nreveal\nbreak-into-intrude\nbreak-even-proﬁt=loss\nbreak-free-escape\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-in-interrupt\nend\nend\nbreak-through-pass-through\nend\nbreak-up-end-relationship\nsurpass\nbreak-even-proﬁt=loss\nbreak-out-escape\nend\nseparate-into-parts\nbreak-down-destroy\nbreak-oﬀ-end\nsuccumb\nbreak-oﬀ-detach\nbreak-down-separate-into-parts\nend\nbreak-down-succumb\nbreak-down-destroy\nbreak-into-intrude\nseparate-into-parts\nbreak-down-separate-into-parts\nbegin-construction\nseparate-into-parts\nbreak-away-detach\nreveal\nsurpass\nreveal\nend\nseparate-into-parts\nend\nbreak-up-separate-into-parts\nbreak-down-render-inoperable\nexperience-sorrow\nseparate-into-parts\nbreak-oﬀ-end\nunclassiﬁed\ndestroy\nseparate-into-parts\nend\nrender-inoperable\nbreak-down-destroy\nbreak-down-separate-into-parts\nseparate-into-parts\nunclassiﬁed\nviolate\nviolate\ncause-to-fail\nbreak-down-destroy\nbreak-up-end-relationship\ncause-to-fail\nsurpass\nsurpass\nbreak-in-enter\nbreak-past-pass-through\nseparate-into-parts\nbreak-up-end-relationship\nbreak-away-pause\nseparate-into-parts\nbreak-apart-detach\nend\nbreak-open-open\nbreak-up-separate-into-parts\nbreak-up-end\nbreak-up-destroy\nbreak-down-succumb\nbreak-up-end\nbreak-from-detach\nbreak-up-end-relationship\nseparate-into-parts\nbreak-free-escape\nend\nbreak-even-proﬁt=loss\nbegin-to-sweat\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-down-render-inoperable\nbreak-up-separate-into-parts\nreveal\nunclassiﬁed\nviolate\nseparate-into-parts\nviolate\nappear\nsurpass\ndestroy\nseparate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nsurpass\nbreak-into-intrude\nbreak-down-separate-into-parts\nbreak-down-destroy\nbreak-down-separate-into-parts\nbreak-out-escape\nbreak-down-separate-into-parts\nbreak-down-render-inoperable\nbreak-free-escape\nbreak-up-unclassiﬁed\nbreak-down-separate-into-parts\nbreak-up-end-relationship\nend\nbreak-open-open\nseparate-into-parts\nseparate-into-parts\nend\nbreak-down-separate-into-parts\nend\npioneer\nseparate-into-parts\ncause-to-fail\nbreak-in-interrupt\nbreak-out-escape\ncause-to-fail\nbreak-out-start\nreveal\nbreak-into-intrude\nend\nbreak-up-unclassiﬁed\nseparate-into-parts\nseparate-into-parts\nend\nbegin-construction\nbreak-through-succeed\nbreak-apart-detach\nbreak-open-open\nsuccumb\nseparate-into-parts\nbreak-for-pause\nbreak-down-render-inoperable\neat-with-sb\nbreak-away-detach\nbreak-down-succumb\nbreak-into-intrude\nbreak-up-separate-into-parts\nlessen\nrender-inoperable\nbreak-up-end-relationship\nend\nbreak-up-end-relationship\nsuccumb\nbreak-through-pass-through\nseparate-into-parts\nbreak-up-end\nseparate-into-parts\nend\nviolate\ndecipher\nbreak-down-render-inoperable\nbreak-in-enter\nend\nseparate-into-parts\nbreak-up-separate-into-parts\nunclassiﬁed\nbreak-away-detach\nbreak-into-intrude\nend\nbreak-out-separate-into-parts\nbreak-down-succumb\nbreak-down-separate-into-parts\nbreak-down-fail\nreveal\nbreak-from-detach\nbreak-down-destroy\nbreak-up-separate-into-parts\nbreak-into-intrude\nbreak-down-separate-into-parts\nbreak-open-open\nviolate\nbreak-in-interrupt\nend\nbreak-with-end-relationship\nbreak-apart-detach\ncause-to-fail\nbreak-down-separate-into-parts\nend\ncost-too-much\nend\nreveal\npioneer\nreveal\nend\nbreak-oﬀ-detach\nbreak-oﬀ-stop\nbreak-down-separate-into-parts\nbreak-into-intrude\nend\ncause-to-fail\nbreak-into-intrude\nbegin-construction\nbreak-oﬀ-detach\nbreak-down-separate-into-parts\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-down-destroy\nbreak-down-separate-into-parts\nbreak-oﬀ-end\nseparate-into-parts\nbreak-out-start\nviolate\nsurpass\nbreak-free-escape\nappear\nbreak-down-separate-into-parts\nbreak-into-unclassiﬁed\nend\ncause-to-fail\ncause-to-fail\nbreak-down-succumb\nviolate\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-out-start\nunclassiﬁed\nsurpass\nviolate\npioneer\nseparate-into-parts\nchange\nbreak-out-prepare-for-consumption\nbreak-even-proﬁt=loss\nend\nend\nviolate\nbreak-loose-escape\nviolate\nreveal\nseparate-into-parts\nend\nend\nbreak-up-end\nend\nseparate-into-parts\nbreak-even-proﬁt=loss\nbreak-down-separate-into-parts\nend\nviolate\nend\nseparate-into-parts\nend\nbreak-down-destroy\nbreak-out-start\nseparate-into-parts\nbreak-free-escape\nseparate-into-parts\nsurpass\nviolate\nrender-inoperable\nsurpass\nend\nbreak-away-detach\nend\nseparate-into-parts\nsurpass\nbreak-for-pause\nbreak-down-unclassiﬁed\nend\nend\nseparate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-into-intrude\nseparate-into-parts\nbreak-down-render-inoperable\nsurpass\nbreak-down-destroy\nbreak-up-separate-into-parts\nseparate-into-parts\nbreak-down-separate-into-parts\nviolate\nbreak-up-separate-into-parts\nbreak-loose-detach\nbreak-out-start\nseparate-into-parts\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-through-pass-through\nbreak-down-separate-into-parts\nbreak-down-separate-into-parts\nseparate-into-parts\nbreak-out-escape\nbreak-free-escape\nviolate\nbreak-apart-detach\ncause-to-fail\nunclassiﬁed\nbreak-down-destroy\nbreak-into-intrude\nend\nbreak-up-separate-into-parts\nseparate-into-parts\nend\nbreak-apart-detach\nseparate-into-parts\nbreak-free-escape\nend\nbreak-down-destroy\nbreak-down-destroy\nbreak-down-separate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-up-end\nbreak-down-separate-into-parts\nviolate\nend\nbreak-into-intrude\ndestroy\nunclassiﬁed\nseparate-into-parts\nend\nseparate-into-parts\nbreak-down-pause\nend\nbreak-up-end-relationship\nseparate-into-parts\nbreak-down-separate-into-parts\nunclassiﬁed\nseparate-into-parts\nseparate-into-parts\nbreak-loose-detach\nbreak-away-detach\nsurpass\nviolate\nbreak-into-intrude\ncause-to-fail\nend\nviolate\nseparate-into-parts\nseparate-into-parts\nend\nbreak-up-end\nreveal\nbreak-down-separate-into-parts\nend\nbreak-even-proﬁt=loss\nseparate-into-parts\nbreak-up-end\nreveal\nbreak-free-escape\nbreak-down-separate-into-parts\nbreak-up-end-relationship\nreveal\nrender-inoperable\nbreak-down-separate-into-parts\nshow-disagreement-with-group\nbreak-into-intrude\nbreak-up-end\nsurpass\nseparate-into-parts\nshow-disagreement-with-group\nbreak-out-start\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nreveal\nend\nseparate-into-parts\nendunclassiﬁed\nbreak-up-separate-into-parts\nbreak-out-unclassiﬁed\nviolate\nbreak-down-render-inoperable\nbreak-oﬀ-detach\ndecipher\nbreak-into-intrude\ndecipher\nbreak-out-unclassiﬁed\nend\nbreak-down-separate-into-parts\nbreak-down-pause\nviolate\nseparate-into-parts\nbreak-down-separate-into-parts\nbreak-up-separate-into-parts\nseparate-into-parts\nseparate-into-parts\nbreak-down-separate-into-parts\nsurpass\nend\nbreak-open-open\nunclassiﬁed\nbreak-away-detach\nbreak-for-pause\nbreak-apart-detach\nunclassiﬁed\nbreak-down-unclassiﬁed\nreveal\nviolate\nend\nbreak-into-intrude\nseparate-into-parts\nviolate\nbreak-down-destroy\nbreak-up-end-relationship\nseparate-into-parts\nbreak-into-intrude\nseparate-into-parts\nend\nviolate\nviolate\nbreak-through-pass-through\nbreak-down-render-inoperablebreak-down-render-inoperablebreak-down-render-inoperable\nrender-inoperable\nrender-inoperable\nbreak-down-render-inoperable\nrender-inoperable\nrender-inoperablerender-inoperable\ndecipher\ndecipher\ndecipherdecipher\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\nrender-inoperable\ndecipher\nbreak-down-render-inoperable\ndecipher\nrender-inoperabledecipher\ndecipher\nrender-inoperable\nrender-inoperable\ndecipher\ndecipher\ndecipher\ndecipherdecipher\ndecipher\ndecipher\nchange\nappearappear\nappear\nchangechange\nbreak-through-pass-through\nappear\nappear\nchange\nappear appear\nappear\nappear\nappear\nappear\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipherdecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipherdecipher\ndecipher\ndecipherdecipher\ndecipher\ndecipher\ndecipher\ndecipherdecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\ndecipher\nappear\nappearappear\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nappear\nFigure 3: t-SNE of break with RoBERTa-large, layer 24.\n511",
  "topic": "Semantics (computer science)",
  "concepts": [
    {
      "name": "Semantics (computer science)",
      "score": 0.7006598114967346
    },
    {
      "name": "Lexical semantics",
      "score": 0.6898125410079956
    },
    {
      "name": "Computer science",
      "score": 0.6320038437843323
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5992116928100586
    },
    {
      "name": "Linguistics",
      "score": 0.5950208306312561
    },
    {
      "name": "Natural language processing",
      "score": 0.5289010405540466
    },
    {
      "name": "Lexical item",
      "score": 0.526175856590271
    },
    {
      "name": "Verb",
      "score": 0.5007450580596924
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4929511547088623
    },
    {
      "name": "Programming language",
      "score": 0.16297590732574463
    },
    {
      "name": "Philosophy",
      "score": 0.061550140380859375
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 5
}