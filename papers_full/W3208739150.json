{
  "title": "Self-Initiated Open World Learning for Autonomous AI Agents",
  "url": "https://openalex.org/W3208739150",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1990217128",
      "name": "Liu Bing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222469983",
      "name": "Robertson, Eric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222469984",
      "name": "Grigsby, Scott",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222469982",
      "name": "Mazumder, Sahisnu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2782717882",
    "https://openalex.org/W2063392212",
    "https://openalex.org/W3025682415",
    "https://openalex.org/W3173861571",
    "https://openalex.org/W3164246203",
    "https://openalex.org/W2565989828",
    "https://openalex.org/W2391561458",
    "https://openalex.org/W3102616566",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W2155310591",
    "https://openalex.org/W2028138594",
    "https://openalex.org/W2475167333",
    "https://openalex.org/W3135550350",
    "https://openalex.org/W3176617324",
    "https://openalex.org/W3037024089"
  ],
  "abstract": "As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can learn by themselves in a self-motivated and self-supervised manner rather than being retrained periodically on the initiation of human engineers using expanded training data. As the real-world is an open environment with unknowns or novelties, detecting novelties or unknowns, characterizing them, accommodating or adapting to them, gathering ground-truth training data, and incrementally learning the unknowns/novelties are critical to making the agent more and more knowledgeable and powerful over time. The key challenge is how to automate the process so that it is carried out on the agent's own initiative and through its own interactions with humans and the environment. Since an AI agent usually has a performance task, characterizing each novelty becomes critical and necessary so that the agent can formulate an appropriate response to adapt its behavior to accommodate the novelty and to learn from it to improve the agent's adaptation capability and task performance. The process goes continually without termination. This paper proposes a theoretic framework for this learning paradigm to promote the research of building Self-initiated Open world Learning (SOL) agents. An example SOL agent is also described.",
  "full_text": "arXiv:2110.11385v3  [cs.AI]  29 Feb 2024\nSelf-Initiated Open W orld Learning for Autonomous AI Agent s\nBing Liu 1 , Eric Robertson 2 , Scott Grigsby 2, Sahisnu Mazumder 1\n1 Department of Computer Science, University of Illinois at C hicago, USA\n2 P AR Government Systems Corporation, USA\nliub@uic.edu, {eric robertson, scott grigsby}@partech.com, sahisnumazumder@gmail.com\nAn extended and revised version of this work has\nbeen published in the AI Magazine (AAAI) as follows:\nBing Liu, Sahisnu Mazumder, Eric Robertson,\nand Scott Grigsby . “ AI Autonomy: Self-initiated\nOpen-world Continual Learning and Adapta-\ntion. ” AI Magazine (2023).\nClick here for the pdf of the revised version:\nAI Magazine version link\nPlease consider the AI Magazine 2023 version as\nmentioned above for citation.\nAbstract\nAs more and more AI agents are used in practice, it is time\nto think about how to make these agents fully autonomous so\nthat they can learn by themselves in a self-motivated and sel f-\nsupervised manner rather than being retrained periodicall y\non the initiation of human engineers using expanded train-\ning data. As the real-world is an open environment with un-\nknowns or novelties, detecting novelties or unknowns, char-\nacterizing them, accommodating or adapting to them, gather-\ning ground-truth training data, and incrementally learning the\nunknowns/novelties are critical to making the agent more an d\nmore knowledgeable and powerful over time. The key chal-\nlenge is how to automate the process so that it is carried out\non the agent’s own initiative and through its own interactions\nwith humans and the environment. Since an AI agent usually\nhas a performance task, characterizing each novelty becomes\ncritical and necessary so that the agent can formulate an ap-\npropriate response to adapt its behavior to accommodate the\nnovelty and to learn from it to improve the agent’s adapta-\ntion capability and task performance. The process goes con-\ntinually without termination. This paper proposes a theore tic\nframework for this learning paradigm to promote the researc h\nof building Self-initiated Open world Learning (SOL) agents.\nAn example SOL agent is also described.\n1 Introduction\nClassic machine learning (ML) makes closed world assump-\ntion, which means that what are seen by the system in testing\nor deployment must have been seen in training (Fei and Liu\n2016; Bendale and Boult 2015; Liu 2020), i.e., there is noth-\ning new or novel occurring in testing or deployment. This\nassumption is invalid in practice as the real world is an open\nenvironment with unknowns or novel objects. For humans,\nnovelties or unknowns serve as an intrinsic motivation for\nlearning. Human novelty detection results in a cascade of\nunique neural responses and behavioral changes that enable\nexploration and ﬂexible memory encoding of the novel infor-\nmation. As learning occurs, this novelty response is soon lo st\nas repeated exposure to novelty results in fast neural adap-\ntation (Tulving and Kroll 1995; Murty et al. 2013). In order\nto make an AI agent thrive in the real open world, like hu-\nmans, it has to detect novelties and learn them incrementall y\nto make the system more knowledgeable and adaptable. It\nmust do so on its own initiative rather than relying on hu-\nman engineers to retrain the system periodically. That is, i t\nmust learn in the open world in a self-motivated manner in\nthe context of its performance task .\nW e use the hotel guest-greeting bot example\nfrom (Chen and Liu 2018) to illustrate Self-initiated\nOpen world Learning (SOL). The performance task of the\nbot is to greet hotel guests. When it sees a guest it has\nlearned, e.g., John, it greets him by saying\n“ Hi John, how are you today ?”\nWhen it sees a new guest, it should detect this guest as new or\nnovel because it has never seen him/her before. This is a nov-\nelty detection problem (also known as out-of-distribution\n(OOD) detection). It then needs to accommodate or adapt\nto the novel situation, e.g., by saying to the new guest\n“ Hello, welcome to our hotel! What is your name, sir ?”\nIf the guest replies “ David, ” the bot takes some pictures of\nthe guest to gather training data and then incrementally\nor continually learn to recognize David. The name “ David”\nserves as the class label of the pictures taken. When it sees\nthis guest again, it can say\n“ Hi David, how are you today ?” (he is no longer novel)\nClearly, in an actual hotel, the situation is much more com-\nplex than this. First of all, if the bot uses a video camera,\nwhen it sees a novel object, it experiences a data distribution\nchange as it sees the new object for a period of time rather\nthan just one image instance. It is desirable to detect the ne w\nobject as quickly as possible. Next, how does the system\nknow that the novel object is actually a person, not a dog?\nIf the system can recognize the object as a person, how does\nit know that he/she is a hotel guest, not a policeman? In or-\nder to adapt to the novel object or situation, the system must\nﬁrst characterize the novel object as without it, the agent\ndoes not know how to adapt. In this case, some classiﬁca-\ntion is needed to decide whether it is a person with luggage.\nIf the object is a person but has no luggage, the bot will not\nrespond or learn to recognize the person as it is irrelevant to\nits performance task. If it is a moving object but not a per-\nson, it should notify a hotel employee and learn to recognize\nthe object so that it will no longer be novel when it is seen\nagain in the future. In short, for each characterization, th ere\nis a corresponding response or adaptation strategy , which\ncan be NIL (i.e., do nothing). This discussion shows that in\norder to characterize, the agent must already have rich worl d\nknowledge. Clearly, there is also a risk involved when mak-\ning an incorrect decision.\nAs classic learning matures, we should go beyond the ex-\nisting framework to study how to enable the learner to learn\nby itself via its own interactions with humans and the en-\nvironment, involving no human engineers. This paper pro-\nposes the SOL framework to promote the research of au-\ntonomous learning agents so that they can face the real open\nworld and learn by themselves. An example SOL agent in\nthe context of dialogue systems or chatbots that implements\nthe main ideas in this paper will also be discussed.\nAlthough open world learning has been studied by several\nresearchers (Bendale and Boult 2015; Fei, W ang, and Liu\n2016; Xu et al. 2019), it mainly focuses on novelty detec-\ntion (Parmar, Chouhan, and Rathore 2021), which is also\ncalled open set detection or out-of-distribution detection in\nthe literature. Recently, zero-shot out-of-distribution detec-\ntion is investigated in (Esmaeilpour et al. 2022) based on\nthe pre-trained model CLIP (Radford et al. 2021). Some re-\nsearchers have further studied how to automatically iden-\ntify the classes or categories of the detected novel object\ninstances (Shu, Xu, and Liu 2018). Y et, some others have\nstudied learning the novel objects or classes after they hav e\nbeen detected (Bendale and Boult 2015; Fei, W ang, and Liu\n2016; Xu et al. 2019). An recent survey of the topic can be\nfound in (Y ang et al. 2021). There is also a general discus-\nsion paper about open world learning in (Langley 2020),\nwhich presents many interesting blue sky ideas. However,\nSOL differs from these prior studies in many aspects.\n(1) SOL stresses self-initiation in learning, which means\nthat all the learning activities from start to end are self-\nmotivated and self-initiated by the agent itself. The proce ss\ninvolves no human engineers.\n(2) Due to self-initiation, SOL enables learning after the\nmodel deployment like human learning on the job or while\nworking, which has not been been attempted before. In ex-\nisting learning paradigms, after a model has been deployed,\nthere is no more learning until the model is updated or re-\ntrained on the initiation of human engineers.\n(3) SOL is also a lifelong and continual learning paradigm\nagain because learning is self-initiated and can be done con -\ntinually or lifelong. It is thus connected with lifelong and\ncontinual learning, which is another active research area i n\nmachine learning (Chen and Liu 2018).\n(4) SOL involves extensive interactions of the learning\nagent and human users, other AI agents, and the environ-\nment. The main purpose is to acquire ground-truth labels\nand training data by itself.\n(5) SOL makes learning autonomous and a learning agent\nequipped with SOL can face the real open world. W e also\nbelieve that SOL is necessary for the next generation ma-\nchine learning techniques. Finally, note that although SOL\nfocuses on self-initiated learning, it does not mean that th e\nlearning system cannot learn a task given by humans or other\nAI agents incrementally or continually\n2 Self-Initiated Open W orld Learning (SOL)\nOur agent has a primary performance task and a pair of key\nmodules (T, S ), where T is the primary task-performer (e.g.,\nthe dialogue system of the greeting bot) and S is a set of\nsupporting or peripheral functions (e.g., the vision system\nand the speech system of the bot) that supports the primary\ntask-performer. The primary task-performer as well as each\nsupport function may consist of four main sub-components\n(L, E, N, I), where L is a SOL learner, E is the execu-\ntor that performs the task or function, N is a novelty char-\nacterizer for characterizing each novelty so that E can for-\nmulate an appropriate response to the novelty, and I is an\ninteractive module for the agent or L to communicate with\nhumans or other agents (e.g., to gain ground-truth training\ndata). Since each function and the primary task performer\nhas the same components, we will discuss them in general\nrather than distinguishing them, but will distinguish them\nwhen necessary. W e will not discuss the relationships and\ninteractions of the components. Below , we ﬁrst discuss the\nSOL learner L, which starts with novelty and/or data shift\ndetection.\n2.1 Data Shift\nThe classic ML depends on the independent and identically\ndistributed (IID) assumption. SOL deals with non-IID data,\nwhich is commonly know as data shift . W e use supervised\nlearning as the task to develop the ideas, which can be easily\nadapted to other learning settings.\nLet the training data be Dtr = {(xi, y i)}n\ni=1, where xi ∈\nX is a training example following the training distribution\nPtr(x) and yi ∈ Ytr is the corresponding class label of xi\nand Ytr is the set of all class labels that appear in Dtr.\nNote that P (x, y ) = P (y|x)P (x). Given x ∈ X and\ny ∈ Ytr in both training and testing, existing research has\nproposed the following three main types of data shift hap-\npening in testing (Moreno-T orres et al. 2012).\nDeﬁnition (covariate shift). Covariate shift refers to the\ndistribution change of the input variable x between training\nand test phases, i.e., Ptr(y|x) = Pte(y|x) and Ptr(x) ̸=\nPte(x), where Pte is the test distribution.\nDeﬁnition (prior probability shift). Prior probability\nshift refers to the distribution change of the class variable y\nbetween training and test phases, i.e., Ptr(x|y) = Pte(x|y)\nand Ptr(y) ̸= Pte(y).\nDeﬁnition (concept drift). Concept drift refers to the\nchange in the posterior probability distribution between\ntraining and test phases, i.e., Ptr(y|x) ̸= Pte(y|x) and\nPtr(x) =Pte(x).\nClearly, the most general data shift is: Ptr(y|x) ̸=\nPte(y|x) and Ptr(x) ̸= Pte(x).\nHowever, there is another change not explicitly included\nin the above data shift, i.e., novelty or novel instances that\nappear in testing or deployment and do not belong to any\nknown classes, which are the focus of SOL. Covariate shift\nmay have novel instances belonging to known classes.\n2.2 Novel Instances and Classes\nNovelty is an agent-speciﬁc concept. An object may be\nnovel to one agent based on its partial knowledge of the\nworld but not novel to another agent. In the context of su-\npervised learning, the world knowledge is the training data\nDtr = {(xi, y i)}n\ni=1 with xi ∈ X and yi ∈ Ytr. Let h(x)\nbe the latent or internal representation of x in the agent’s\nmind, h(Di\ntr) be the latent representation of the training data\nof class yi, and k (= |Ytr|) be the total number of train-\ning classes. W e use µ(h(x), h (Di\ntr)) to denote the novelty\nscore of a test instance x with respect to h(Di\ntr). The de-\ngree of novelty of x with respect to Dtr, µ(h(x), h (Dtr)),\nis deﬁned as the minimum novelty score with regard to every\nclass,\nµ(h(x), h (Dtr )) = min(µ(h(x), h (D1\ntr )), ..., µ (h(x), h (Dk\ntr )))\n(1)\nThe novelty function µ can be deﬁned based on speciﬁc\napplications. For example, if the training data of each clas s\nfollows the Gaussian distribution, one may use the distance\nfrom the mean to compute the novelty score.\nDeﬁnition (novel instance) : A test instance x is novel if\nits novelty score µ(x, . ) is greater than or equal to a thresh-\nold value γ such that x can be assigned a new class not in\nYtr.\nIn covariate shift , a novel instance may still be assigned\nto an existing class as the class assignment is application o r\nagent speciﬁc. For example, we have a training class called\nanimal and the learner has seen dog and chicken in the ani-\nmal class, but during testing, a tiger shows up. In the covari-\nate shift case, tiger can be added to the existing animal class,\nalthough tiger is novel as it has not been seen before.\nDeﬁnition (novel class) : A newly created class ynew\n(ynew /∈ Ytr) assigned to some novel instances is called a\nnovel class (unknown or unseen class ). The classes in Ytr\nare called known or seen classes .\nNovelty is not restricted to the perceivable physical world\nbut also includes the agent’s internal world, e.g., novel in -\nterpretations of world states or internal cognitive states that\nhave no correspondence to any physical world state. Inter-\nested readers may also read (Boult et al. 2021) for a more\nnuanced and perception-based study of novelty.\nThere are several related concepts to novelty. W e clarify\ntheir difference from novelty here.\nOutlier or anomaly : An outlier is a data point that is\nfar away from the main data clusters, but it may not be un-\nknown. For example, the salary of a company CEO is an\noutlier with regard to the salary distribution of the compan y\nemployees, but that is known and thus not novel. Unknown\noutliers are novel. Anomalies can be considered outliers or\ninstances that are one off and never repeated. Though tech-\nnically “novel” they may not need to result in a new class.\nSurprise or unexpectedness : Based on the prior knowl-\nedge of the agent, the probability P (x|Q) of x occurring in\na particular context Q is very low , but x has occurred in Q,\nwhich is surprising or unexpected. If x has been seen before,\nit is not novel. In human cognition, surprise is an emotional\nresponse to an instance which greatly exceeds the expected\nuncertainty within the context of a task. Outliers, anomali es,\nand novelty can all lead to surprise.\n2.3 Deﬁnitions of Learning in SOL\nThe classic ML makes the i.i.d assumption, which is often\nviolated in practice. Here we deﬁne several other assump-\ntions and learning paradigms that are progressively more an d\nmore aligned with the real-world learning needs of SOL.\nDeﬁnition (closed-world assumption) : No new or novel\nclasses appear in testing. Other types of data shift may occu r.\nDeﬁnition (closed-world learning) : It refers to the learn-\ning paradigm that makes the closed-world assumption.\nDeﬁnition (learning with data shift) : It refers to the\nlearning paradigm that deals with certain types of data shif t\nin testing or deployment, but not new classes.\nDeﬁnition (open world learning) : It refers to the learn-\ning paradigm that can detect data shifts and novel instances\nin testing or deployment. The learner can learn the novel\nclasses labeled by humans from the identiﬁed novel in-\nstances and update the model using the new data. The re-\ntraining or model updating is initiated by human engineers.\nThat is, there is no self-initiated learning after model dep loy-\nment.\nNote that for covariate shift, the assignment of shifted dat a\n(or novel) instances to existing training classes is normal ly\ndone by humans. For concept shift, the shifted classes are\ncaused by humans or by the environment.\nDeﬁnition (self-initiated open world learning (SOL)):\nIt refers to the learning paradigm that has the capability of\nopen-world learning and the agent is able to initiate a learn -\ning process by itself during application (after deployment )\nwith no involvement of human engineers. The learning of the\nnew classes is incremental, i.e., no re-training of previou s\nclasses. The process is thus lifelong or continuous, which\nmakes the agent smarter and smarter over time.\nNote that apart from learning, SOL ’s performance task\nalso demands characterization and adaptation, which we dis -\ncuss in Section 2.5.\n2.4 Steps in Learning in SOL\nLearning in SOL involves the following three main steps.\nW e ignore updating the existing model to deal with data shift\nbecause if we can deal with novel classes, it is relatively ea sy\nto deal with the other types of data shift (see Section 2.1).\nStep 1 - Novelty detection . This step involves detecting data\ninstances (1) whose classes do not belong to Ytr or (2) have\ncovariate shift, and it is done automatically. A fair amount of\nresearch has been done on this under open-set classiﬁcation\nor out-of-distribution detection (Pang et al. 2021).\nStep 2 - Acquiring class labels and creating a new learning\ntask on the ﬂy : This step ﬁrst clusters the detected novel in-\nstances. Each cluster represents a new class. It may be done\nautomatically or through interactions with humans using th e\ninteraction module I. Interacting with humans should pro-\nduce more accurate clusters and also obtain meaningful clas s\nlabels. If the detected data is insufﬁcient for building an a ccu-\nrate model to recognize the new classes, additional ground-\ntruth data may be collected via interaction with humans. A\nnew learning task is then created.\nIn the case of our hotel greeting bot, since the bot detects\na single new guest (automatically), no clustering is needed .\nIt then asks the guest for his/her name as the class label. It\nalso takes more pictures as the training data. With the la-\nbeled ground-truth data, a new learning task is created to\nincrementally learn to recognize the new guest on the ﬂy.\nThe learning agent may also interact with the environment\nto obtain training data. In this case, the agent must have an\ninternal evaluation system that can assign rewards to differ-\nent states of the world, e.g., for reinforcement learning.\nStep 3 - Incrementally learn the new task. After ground-\ntruth training data has been obtained, the learner L in-\ncrementally learns the new task. This is continual learn-\ning (Chen and Liu 2018), an active and challenging ML re-\nsearch area, which is deﬁned as learning a sequence of tasks\nincrementally.\n2.5 Novelty Characterization and Adaptation\nIn a real-life application, classiﬁcation may not be the pri -\nmary task. For example, in a self-driving car, object classi ﬁ-\ncation supports its primary performance task of driving. T o\ndrive safely, the car has to take some actions to adapt or re-\nspond to the new object, e.g., slowing down and avoiding the\nobject. In order to know what actions to take, it must char-\nacterize the new object. Based on the characterization, ap-\npropriate actions are formulated to accommodate or respond\nto the novel object. The response process may also involve\nlearning.\nDeﬁnition (novelty and response) : It is a pair (c, r ),\nwhere c is the characterization of the novelty and r is the\nresponse to the novelty, which is a plan of dynamically for-\nmulated actions based on the characterization of the novelt y\nand the agent’s interactions with the novel item. If the sys-\ntem cannot characterize a novelty, it will take the default\nresponse. In some situations, the agent does not know what\nto do. The response is Learn, i.e., to learn the actions to take.\nDeﬁnition (characterization of novelty) : It is a descrip-\ntion of the novelty based on the agent’s existing knowledge\nof the world. According to the description, the agent choose s\na speciﬁc course of actions to respond to the novelty.\nCharacterization of novelty can be done at different levels\nof detail, which may result in more or less precise responses .\nBased on the ontology and attributes related to the perfor-\nmance task, the description can be deﬁned based on the type\nof the object and the attribute of the object . For example,\nfor self-driving cars, when sensing a novel object, the car\ncan identify both the movement of the object and the loca-\ntion of the object relative to the direction of travel–on the\nroad or off the road. Thus, some classiﬁcation of movement\nand location in this case is needed to characterize the nov-\nelty which, in turn, facilitates determination of the agent ’s\nresponding action(s). For instance, if the novel object is a\nmobile object, the car may wait for the object to leave the\nroad before driving.\nAnother common characterization strategy is to compare\nthe similarity between the novel object and the existing\nknown objects. For example, if it is believed that the novel\nobject looks like a pig, then the agent may react like when it\nsees a pig on the road.\nIn our greeting bot example, when it can characterize a\nnovelty as a new guest, its response is to say ” Hello, wel-\ncome to our hotel! What is your name, sir ?” If the bot has\ndifﬁculty with object characterization, it can take a default\naction, either ‘do nothing’ or ‘report to a hotel staff. ’ The set\nof responses are speciﬁc to the application. For a self-driv ing\ncar, the default response to a novel object is to slow down the\ncar as soon as possible so that it will not hit the novel object .\nThis discussion implies that in order to effectively charac -\nterize a novelty, the agent must already have a great of world\nknowledge that it can use to describe the novelty. Without\nsuch knowledge, it will not be able to characterize the nov-\nelty in meaningful ways.\nThe characterization and response process is often inter-\nactive in the sense that based on the initial characterizati on,\nthe agent may choose a course of actions, but after some\nactions are taken, it will get some feedback from the envi-\nronment. Based on the feedback and the agent’s additional\nobservations, the course of actions may be revised.\nFocus of attention. Due to the performance task, the\nagent should focus on detecting and characterizing novel-\nties that are critical to the performance task. For example,\nin the self-driving car application, the agent should focus on\nnovel objects or events that are or may potentially appear\non the road in front of the car. It should not pay attention\nto novel objects in the shops along the street as they do not\naffect driving. In characterizing a novel object on the road , it\nshould focus on those aspects that are important to driving,\ni.e., whether it is a still or a moving object. If it is a moving\nobject, the agent must determine its direction of moving.\nLearning to respond. As indicated above, in some situ-\nations, the system may not know how to respond to a new\nobject or situation. In this case, it needs to learn by itself\n(see Section 2.4). There are many ways to learn, e.g.,\n(1) Asking a human . In the case of the self-driving car,\nwhen it does not know what to do, it may ask the passen-\nger using the interactive module I (e.g., in natural language)\nand then follow the instruction and also remember or learn\nit for future use. For example, if the car sees a black patch\non the road that it has never seen before, it can ask “ what is\nthat black thing in front? ” The passenger may answer “ that\nis tar . ” In the case of an unrecognized response, such as no\nprior information on tar, the system may progress with fur-\nther inquiry, asking the passenger “ what should I do? ”\n(2) Imitation learning . On seeing a novel object, if the car\nin front drives through it with no issue, the car may choose\nthe same course of action as well and learn it for future use.\n(3) P erforming reinforcement learning . By interacting\nwith the environment through trial and error exploration, t he\nagent learns a good response policy for future use. As men-\ntioned earlier, in this case the agent must have an internal\nevaluation system that can assign rewards to environment\nstates.\nIf multiple novelties are detected at the same time, it is\nmore difﬁcult to respond as the agent must reason over the\ncharacteristics of all novel objects to dynamically formul ate\nan overall plan of actions that prioritize the responses.\n2.6 Risk\nThere is risk in achieving performance goals of an agent\nwhen making an incorrect decision. For example, classifyin g\na known guest as unknown or an unknown guest as known\nmay negatively affect guest impressions resulting in nega-\ntive reviews. For a self-driving car, misidentiﬁcations ca n\nresult in wrong responses, which could be a matter of life\nand death. Thus, risk assessment must be made in making\neach decision. Risk assessment can also be learned from ex-\nperiences or mistakes. In the example of a car passing over\ntar, after the experience of passing over shiny black surfac es\nsafely many times, if the car slips in one instance, the car\nagent must assess the risk of continuing the prior procedure .\nGiven the danger, a car may weight the risk excessively,\nslowing down on new encounters of shiny black surfaces.\n3 An Example SOL System\nAlthough novelty detection (Y ang et al. 2021; Pang et al.\n2021) and incremental or continual learning (Chen and Liu\n2018; Parisi et al. 2019) have been studied widely, little\nwork has been done to build a SOL system. Here we describe\na dialogue system (called CML) that is based on the SOL\nframework and performs each function in SOL continually\nby itself after the system has been deployed (Mazumder et al.\n2020; Liu and Mazumder 2021).\nCML is an natural language interface like Amazon Alexa\nand Siri. Its performance task is to take a user command\nin natural language (NL) and perform the user requested\nAPI action in the underlying application. There is no sup-\nport function in this application. The key issue is how to\nunderstand paraphrased NL commands from the user in or-\nder to map a user command to a system’s API call. Novelty\nequates to the system’s failure in understanding a user com-\nmand. After the system automatically detects a novelty (a\nhard-to-understand user command), it will try to understan d\nthe command and also learn the command so that it will be\nable to understand it and related commands in the future.\nThe novelty characterization step of SOL here is to iden-\ntify the part of the command that the system does not un-\nderstand. Based on the characterization, the system adapts\nby asking the user via an interactive dialogue to obtain the\nground truth API action requested by the user, which also\nserves as a piece of training data for continual learning. In\nthe adaptation or accommodation process, risk is also con-\nsidered.\nConsider the following example. The user issues the com-\nmand “ turn off the light in the kitchen ” that the system does\nnot understand (i.e., a novelty), Based on the current sys-\ntem state, it decides which part of the command it can under-\nstand and which part it has difﬁculty (i.e., characterization).\nBased on the characterization result, it provides the user a\nlist of top-k predicted actions (see below) described in NL\nand asks the user to select the most appropriate action from\nthe given list (i.e., adaptation).\nBot: Sorry , I didn’t get you. Do you mean to:\noption-1. switch off the light in the kitchen, or\noption-2. switch on the light in the kitchen,\noption-3. change the color of the light?\nThe user answers the desired action (option-1). The ac-\ntion API [say, SwitchOffLight(arg:place)] correspond-\ning to the selected action (option-1) is retained as the\nground truth action for the issued NL command. In sub-\nsequent turns of the dialogue, the agent can also ask the\nuser questions to acquire ground truth values associated wi th\narguments of the selected action, as deﬁned in the API.\nCML then incrementally learns to map the original com-\nmand “ turn off the light in the kitchen ” to the API action,\nSwitchOffLight(arg:place). This learning ensures that\nin the future the system will not have problem understand-\ning the related commands.\nRisk is considered in the system in two ways. First, it\ndoes not ask the user too many questions in order not to an-\nnoy the user. Second, when the characterization is not con-\nﬁdent, the system simply asks the user to say his/her com-\nmand again rather than providing a list of random options to\nchoose from.\n4 Key Challenges\nAlthough novelty detection and continual learning have bee n\nresearched extensively (Y ang et al. 2021; Pang et al. 2021),\nthey remain to be challenging. Limited work has been done\nto address the following (this list is by no means exhaustive ):\nObtaining training data on the ﬂy . One key feature of\nSOL is interaction with humans to obtain ground-truth train -\ning data, which needs a dialogue system. Building an effec-\ntive dialogue system is very challenging. W e are unaware\nof any such system for SOL except CML (Mazumder et al.\n2020), but CML is only for simple command learning.\nFew-shot continual learning. It is unlikely for the learn-\ning agent to collect a large volume of training data via in-\nteraction with the user. Then, an effective and accurate few -\nshot continual/incremental learning method is necessary.\nNovelty characterization. This is critical because it de-\nﬁnes the characteristics used to recognize world state and\ndetermine the best response strategy. For example, if a self -\ndriving car encounters a novel/new mobile object, its re-\nsponse will be different from encountering an immobile ob-\nject. Even if we know it is a mobile object, the system may\nalso need to know which direction it is moving and the mov-\ning speed in order to formulate an appropriate response. The\nchallenge is that a large number of classiﬁers or other mod-\nels may need to be built. This means that the system or agent\nmust have a very rich world model and a large amount of\nknowledge related to its performance task before it can ef-\nfectively characterize novelties.\nLearning to respond. This task is especially challenging\nin a real-world physical environment (Dulac-Arnold et al.\n2021). For example, due to safety concerns, learning during\ndriving (which is required by SOL) by a self-driving car us-\ning reinforcement learning (RL) is very dangerous. Further -\nmore, for RL to work, the agent must have a highly effective\ninternal reward or evaluation system to assign rewards to ac -\ntions and states and to be aware of safety constraints . Little\nwork has been done so far.\nKnowledge revision. It is inevitable that the system\nmay misinterpret, generalize or otherwise assemble incor-\nrect knowledge. A SOL system must have a mechanism\nto detect and revise the inaccurate knowledge on its own.\nAgain, little work has been done.\n5 Conclusion\nA truly intelligent system must be able to learn au-\ntonomously and continually in the open world on its own\ninitiative after deployment in order to adapt to the ever-\nchanging world and gain more and more knowledge to be-\ncome more and more powerful over time. This paper pro-\nposed a self-initiated open world learning (SOL) framework\nfor the purpose, and presented the concepts, steps and key\nchallenges. An example SOL system is also described. W e\nbelieve that the future research in SOL will bring ML and\nAI to the next level.\nAcknowledgments\nThis paper beneﬁted greatly from numerous discussions in\nthe DARP A SAIL-ON Program PI meetings. This work was\nsupported in part by a DARP A Contract HR001120C0023.\nBing Liu and Sahisnu Mazumder are also partially sup-\nported by two National Science Foundation (NSF) grants\n(IIS-1910424 and IIS-1838770), and a Northrop Grumman\nresearch gift. The views expressed in this document are\nthose of the authors and are not those of funders.\nReferences\nBendale, A.; and Boult, T . 2015. T owards open world recog-\nnition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , 1893–1902.\nBoult, T .; Grabowicz, P .; Prijatelj, D.; Stern, R.; Holder, L.;\nAlspector, J.; Jafarzadeh, M.; Ahmad, T .; Dhamija, A.; Li,\nC.; et al. 2021. T owards a Unifying Framework for Formal\nTheories of Novelty. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 35, 15047–15052.\nChen, Z.; and Liu, B. 2018. Lifelong machine learning . Mor-\ngan & Claypool Publishers.\nDulac-Arnold, G.; Levine, N.; Mankowitz, D. J.; Li, J.; Padu -\nraru, C.; Gowal, S.; and Hester, T . 2021. Challenges of real-\nworld reinforcement learning: deﬁnitions, benchmarks and\nanalysis. Machine Learning 1–50.\nEsmaeilpour, S.; Liu, B.; Robertson, E.; and Shu, L. 2022.\nZero-Shot Out-of-Distribution Detection Based on the Pre-\ntrained Model CLIP . In Proceedings of the AAAI conference\non artiﬁcial intelligence .\nFei, G.; and Liu, B. 2016. Breaking the closed world assump-\ntion in text classiﬁcation. In Proceedings of the 2016 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language T echnologies ,\n506–514.\nFei, G.; W ang, S.; and Liu, B. 2016. Learning cumulatively\nto become more knowledgeable. In Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining , 1565–1574.\nLangley, P . 2020. Open-world learning for radically au-\ntonomous agents. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, 13539–13543.\nLiu, B. 2020. Learning on the job: Online lifelong and con-\ntinual learning. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence (AAAI-2020) .\nLiu, B.; and Mazumder, S. 2021. Lifelong and continual\nlearning dialogue systems: learning during conversation. In\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI-2021) .\nMazumder, S.; Liu, B.; W ang, S.; and Esmaeilpour, S.\n2020. An Application-Independent Approach to Building\nT ask-Oriented Chatbots with Interactive Continual Learni ng.\nNeurIPS-2020 W orkshop on Human in the Loop Dialogue\nSystems .\nMoreno-T orres, J. G.; Raeder, T .; Alaiz-Rodr´ ıguez, R.;\nChawla, N. V .; and Herrera, F . 2012. A unifying view on\ndataset shift in classiﬁcation. P attern recognition 45(1):\n521–530.\nMurty, V . P .; Ballard, I. C.; Macdufﬁe, K. E.; Krebs, R. M.;\nand Adcock, R. A. 2013. Hippocampal networks habituate\nas novelty accumulates. Learning & Memory 20(4): 229–\n235.\nPang, G.; Shen, C.; Cao, L.; and Hengel, A. V . D. 2021.\nDeep learning for anomaly detection: A review . ACM Com-\nputing Surveys (CSUR) 54(2): 1–38.\nParisi, G. I.; Kemker, R.; Part, J. L.; Kanan, C.; and W ermter ,\nS. 2019. Continual lifelong learning with neural networks:\nA review . Neural Networks .\nParmar, J.; Chouhan, S. S.; and Rathore, S. S. 2021. Open-\nworld Machine Learning: Applications, Challenges, and Op-\nportunities. arXiv preprint arXiv:2105.13448 .\nRadford, A.; Kim, J. W .; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P .; Clark, J.;\net al. 2021. Learning transferable visual models from natur al\nlanguage supervision. arXiv preprint arXiv:2103.00020 .\nShu, L.; Xu, H.; and Liu, B. 2018. Unseen class discovery in\nopen-world classiﬁcation. arXiv preprint arXiv:1801.05609\n.\nTulving, E.; and Kroll, N. 1995. Novelty assessment in the\nbrain and long-term memory encoding. Psychonomic Bul-\nletin & Review 2(3): 387–390.\nXu, H.; Liu, B.; Shu, L.; and Y u, P . 2019. Open-world learn-\ning and application to product classiﬁcation. In The W orld\nW ide W eb Conference , 3413–3419.\nY ang, J.; Zhou, K.; Li, Y .; and Liu, Z. 2021. General-\nized out-of-distribution detection: A survey. arXiv preprint\narXiv:2110.11334 .",
  "topic": "Novelty",
  "concepts": [
    {
      "name": "Novelty",
      "score": 0.9078059196472168
    },
    {
      "name": "Computer science",
      "score": 0.7620495557785034
    },
    {
      "name": "Task (project management)",
      "score": 0.7563705444335938
    },
    {
      "name": "Process (computing)",
      "score": 0.6558538675308228
    },
    {
      "name": "Key (lock)",
      "score": 0.6392422318458557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6303918361663818
    },
    {
      "name": "Autonomous agent",
      "score": 0.5565789341926575
    },
    {
      "name": "Intelligent agent",
      "score": 0.48236387968063354
    },
    {
      "name": "Novelty detection",
      "score": 0.4188269078731537
    },
    {
      "name": "Open research",
      "score": 0.41589540243148804
    },
    {
      "name": "Machine learning",
      "score": 0.3724793791770935
    },
    {
      "name": "Human–computer interaction",
      "score": 0.36174824833869934
    },
    {
      "name": "Engineering",
      "score": 0.10942348837852478
    },
    {
      "name": "Computer security",
      "score": 0.0903729498386383
    },
    {
      "name": "World Wide Web",
      "score": 0.07263040542602539
    },
    {
      "name": "Psychology",
      "score": 0.06653976440429688
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}