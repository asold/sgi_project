{
  "title": "Frequency Effects on Syntactic Rule Learning in Transformers",
  "url": "https://openalex.org/W3200809495",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2324760462",
      "name": "Jason Wei",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2105626363",
      "name": "Dan Garrette",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A817205692",
      "name": "Tal Linzen",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2013784948",
      "name": "Ellie Pavlick",
      "affiliations": [
        "Google (United States)",
        "John Brown University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2097333193",
    "https://openalex.org/W2124479173",
    "https://openalex.org/W3124034626",
    "https://openalex.org/W2099164611",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W3098275893",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W3175508917",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4253001367",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2122988375",
    "https://openalex.org/W3168987555",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2145703299",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W3130147842",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035718362",
    "https://openalex.org/W3102684881",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W3103536442",
    "https://openalex.org/W2159080219",
    "https://openalex.org/W3137217512",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3162938759",
    "https://openalex.org/W3166920165",
    "https://openalex.org/W2574741565"
  ],
  "abstract": "Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT's performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT's behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 932–948\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n932\nFrequency Effects on Syntactic Rule Learning in Transformers\nJason Wei1 Dan Garrette1 Tal Linzen2∗ Ellie Pavlick1,3\n1Google Research 2New York University 3Brown University\n{jasonwei,dhgarrette,linzen,epavlick}@google.com\nAbstract\nPre-trained language models perform well on a\nvariety of linguistic tasks that require symbolic\nreasoning, raising the question of whether\nsuch models implicitly represent abstract sym-\nbols and rules. We investigate this question us-\ning the case study of BERT’s performance on\nEnglish subject–verb agreement. Unlike prior\nwork, we train multiple instances of BERT\nfrom scratch, allowing us to perform a series\nof controlled interventions at pre-training time.\nWe show that BERT often generalizes well\nto subject–verb pairs that never occurred in\ntraining, suggesting a degree of rule-governed\nbehavior. We also ﬁnd, however, that per-\nformance is heavily inﬂuenced by word fre-\nquency, with experiments showing that both\nthe absolute frequency of a verb form, as well\nas the frequency relative to the alternate in-\nﬂection, are causally implicated in the predic-\ntions BERT makes at inference time. Closer\nanalysis of these frequency effects reveals that\nBERT’s behavior is consistent with a system\nthat correctly applies the SV A rule in general\nbut struggles to overcome strong training pri-\nors and to estimate agreement features (singu-\nlar vs. plural) on infrequent lexical items.\n1 Introduction\nMany natural language phenomena are best de-\nscribed as the product of applying rules to abstract\nsymbols, without access to the content of these\nsymbols (Smolensky, 1988; Fodor and Pylyshyn,\n1988). Most speakers of English will agree, for\nexample, that if “ gorp” is a singular noun, then,\nregardless of the meaning of “ gorp”, the utter-\nance “the gorp adds nothing” is grammatical, but\n“the gorp add nothing” is not.\nThe success of contemporary neural language\nmodels such as BERT (Devlin et al., 2019) on lan-\nguage understanding tasks, as well as in more tar-\ngeted linguistic evaluations (Marvin and Linzen,\n∗Work done while visiting Google.\n2018; Goldberg, 2019), raises the question of\nwhether these systems acquire such symbolic rules.\nWhile previous studies have attempted to address\nsuch questions, particularly in relation to BERT\n(Rogers et al., 2020), prior work has generally not\nanalyzed the relationship between the model’s pre-\ntraining data and its behavior. As a result, it has\nbeen difﬁcult to tease apart the many factors that\nmay inﬂuence a model’s test time performance.\nIn this paper, we investigate whether pre-trained\ntransformer-based language models learn and ap-\nply symbolic rules, focusing on BERT’s ability\nto follow the English subject–verb number agree-\nment rule (§3) as a case study. On our evaluation\nstimuli (§4), we ﬁnd that BERT achieves high per-\nformance, even on subject–verb pairs that never\noccurred together in the training set (§5.1–§5.2).\nIn exploratory data analysis, however, we ﬁnd that\nthis performance is also inﬂuenced by effects from\nboth absolute and relative frequency of verb forms\nin the training data (§5.3–§5.4). To conﬁrm these\nphenomena causally, we perform a series of train-\ning interventions where we pre-train BERT models\non training data for which we have carefully ma-\nnipulated the frequencies of verb forms (§6). We\nfurther use probing classiﬁers to attribute observed\nmistakes either to errors in rule-following or to\nerrors in lexical categorization (§7).\nThese experiments reveal several insights about\nBERT in the context of rule-governed tasks. First,\nthe high performance of BERT on subject–verb\ncombinations that never occurred in the training\nset is consistent with a model that learns abstract\nrepresentations of lexical items and patterns, i.e.,\nabstract features and rules. Second, BERT’s perfor-\nmance is inﬂuenced by absolute frequency effects,\nbut probing classiﬁers show that this inﬂuence can\nbe explained by the model’s inability to learn the\nfeatures of a verb form (singular vs. plural) for in-\nfrequent lexical items, rather than a failure to apply\nthe rule when the verb form has been classiﬁed. Fi-\n933\nnally, although BERT generally applies rules with\nhigh accuracy, it fails to overcome strong priors dur-\ning training—when one verb form is much more\nfrequent than another, BERT tends to produce the\nmore common form, even when it is not consistent\nwith the rule.\n2 Experimental Logic\n2.1 Hypotheses\nWe aim to investigate BERT’s ability to reason\nover abstract symbols. As a case study, we focus\non subject–verb agreement (SV A) in English, for\nwhich the grammaticality rule of interest is:\nNUMBER (subject) =NUMBER (verb)\nWe consider three alternative hypotheses about the\nprocess underlying BERT’s behavior on SV A.\nH1: Idealized Symbolic Learner. In theory,\nsymbolic reasoners operate over abstract categories,\nsuch as the agreement feature NUMBER , and rules,\nsuch as “if NUMBER (subject) = SINGULAR , then\nNUMBER (verb) = SINGULAR .”Early work (Fodor\nand Pylyshyn, 1988) which discusses the behavior\nof such symbolic systems often presents an ideal-\nized version, for the sake of theoretical argument.\nThus, under H1, this system would not make er-\nrors such as misclassifying inputs or erroneously\nparsing the sentence, and is not affected by word-\nspeciﬁc properties (e.g., frequency).\nH2: Item-Speciﬁc Learner. The antithesis of\nthe idealized symbolic learner is a model that rea-\nsons entirely using word co-occurrences. This sys-\ntem does not represent any abstractions over the im-\nmediate inputs it receives, and thus cannot reason\nover features such as singular/plural. Conceptually,\nit is analogous to early phrase-based MT systems\n(Brown et al., 1990) that build a literal string look-\nup table in order to predict the most likely output\ngiven an input. By deﬁnition, it performs poorly on\nnoun–verb pairs that never co-occurred in training,\nas the lookup table will not have the relevant entry.1\nH3: Symbolic Learner with Noisy Observa-\ntions. Both H1 and H2 represent extreme, largely\ntheoretical models of system behavior. In practice,\n1We do not specify whether such a model has access to\nabstract features other than agreement because such features\n(e.g., the notion of subject) do not affect the speciﬁc hypothe-\nses we consider. For example, a model that does not represent\nagreement feature and only learns word co-occurrences will\nperform poorly on unseen items, regardless of whether it has\naccess to correct parses.\nwe expect systems like BERT to display some hy-\nbrid of the two. However, to our knowledge, there\nhas been no work to date which proposes a speciﬁc\nhypothesis of what type of hybridization best ex-\nplains BERT’s behavior. In this work, we consider\none such hybrid: a system that is symbolic at its\ncore but has noisy observations.2 That is, under H3,\nthe system represents symbols (e.g. singular/plural\nword categories) and rules (e.g., SV A) correctly, but\ncan make errors in mapping from inputs to sym-\nbols. Conceptually, it is analogous to a BayesNet\n(Pearl, 1988) that correctly represents nodes and\ncausal connections internally but may nonetheless\nincorrectly process an input, activating the wrong\nnodes and thus producing the wrong output. Thus,\nunlike in H1, systems consistent with H3 make er-\nrors when they cannot identify whether a subject\nor verb is singular or plural, potentially due to fre-\nquency effects (present at all levels of processing;\nMarantz, 2013).\n2.2 Predictions and Summary of Findings\nWe use three diagnostics to differentiate the above\nhypotheses: (1) generalization to unseen noun-verb\npairs, (2) the presence of frequency effects when\nmaking predictions for seen noun-verb pairs, (3)\nand correlation between speciﬁc types of errors.\nGeneralization to unseen noun-verb pairs al-\nlows us to differentiate H2 from H1 and H3. For\ninstance, since whether the sentence “the section\nadds nothing” obeys the SV A rule depends only\non NUMBER (“section”) and NUMBER (“adds”), a\nsymbolic reasoner’s ability to assess grammatical-\nity should not depend on how frequently the words\n“section” and “adds” have been seen together in\nthe data. Instead, we would expect such a sys-\ntem to learn the correct agreement features of the\ntwo words independently and apply a general SV A\nrule to them. In contrast, an item-speciﬁc learner,\nwhich does not represent abstract agreement fea-\ntures, would rely on probabilities deﬁned over spe-\nciﬁc lexical items, and thus may fail to reason cor-\nrectly about rare or unseen situations, for which\nsuch probabilities are poorly calibrated.\nThe presence of frequency effects in BERT’s\nperformance allows us to differentiate H1 from H2\n2Here, “observations” involves both the parser as well as\nthe lexicon. I.e., H3 allows for errors to arise due to incorrect\nlexical entries and/or incorrect parses. However, since our\nexperiments (§7) don’t differentiate lexicon errors from parse\nerrors, we do not differentiate them within this hypothesis.\nFuture work that differentiates these two errors sources could\nbe worthwhile.\n934\nand H3. That is, under both H2 and H3, the model\nmay perform worse on less frequent words (albeit\nfor different reasons). In contrast, a system consis-\ntent with H1 should not exhibit any differences in\nperformance due to differences in inputs below the\nabstraction of singular/plural.\nOur experiments show that BERT generalizes\nwell (though not perfectly) to unseen noun-verb\npairs (§5.1–§5.2) and exhibits clear frequency ef-\nfects (§6). Together, these results are most consis-\ntent with a hybrid system like H3. To conﬁrm this,\nwe use probing classiﬁers to investigate H3’s spe-\nciﬁc prediction about correlations between types\nof errors, i.e., that errors on SV A should be ex-\nplained by errors in classifying singular vs. plural\n(§7). We ﬁnd that the expected error patterns ex-\nplain some frequency effects (those due to absolute\nfrequency) but not others (those due to relative fre-\nquency). Thus, we ultimately conclude that, of\nthe hypotheses considered, H3 is the best model of\nBERT’s behavior, though BERT exhibits additional\nsensitivity to frequency imbalances between com-\npeting word forms that H3 leaves underspeciﬁed.\n3 Related Work\nTargeted Syntactic Evaluation. We use the tar-\ngeted syntactic evaluation framework of Linzen\net al. (2016) and Marvin and Linzen (2018) to mea-\nsure the model’s ability to learn and apply the SV A\nrule. Following the setup from Goldberg (2019),\neach test instance consists of a sentence in which\na verb has been masked out, and BERT’s masked\nlanguage modeling (MLM) parameters are used to\nscore whether the singular or plural form of the\nverb is a better ﬁt for the masked position. For\nexample, given the sentence“The section [MASK]\nnothing to the info. ” and set of verb inﬂections\n{“add”, “adds”}, the model would be considered\ncorrect if the MLM prediction assigns a higher\nscore to the singular form “adds” than the plural\nform “add” since the subject of the masked verb\nposition is “section, ”which is singular.\nDue to the particulars of BERT’s MLM task\nsetup, the model is only able to score words that are\nrepresented by a single wordpiece. While Goldberg\n(2019) dealt with this limitation by restricting eval-\nuation to just those verbs that appear in the original\nBERT model’s vocabulary as a single wordpiece,\nwe are able to avoid such compromises because\npre-training the models ourselves means that we\ncan add any entries we want to the vocabulary.\nSyntactic Reasoning in LMs. There has been\nsubstantial prior work on the ability of language\nmodels to perform abstract syntactic processing\ntasks (Hu et al., 2020) (see Linzen and Baroni\n(2020) for a review). On SV A speciﬁcally, Gold-\nberg (2019) found that BERT achieves high accu-\nracy on both natural sentences (97%) and nonce\nsentences (83%), and that error rate was indepen-\ndent of the number of “distractor” words between\nthe subject and verb; Yu et al. (2020) showed that\nlanguage models do not exhibit better grammatical\nknowledge of more frequent nouns. Other work\nhas found that BERT’s performance is sensitive\nto factors that may suggest item-speciﬁc learning;\nChaves and Richter (2021) found that BERT’s per-\nformance on number agreement is sensitive to the\nverb, across seven different verbs, and Newman\net al. (2021) found that language models performed\nbetter on verbs that they predicted were likely in\ncontext. The focus on frequency effects also re-\nlates to a more general line of work on understand-\ning the effect of training size and distribution on\nneural language models’ generalization (Warstadt\net al., 2020; Lovering et al., 2021). To our knowl-\nedge, our present study is the ﬁrst to investigate\nthese questions via controlled interventions on the\nmodel’s pre-training data, making it possible to\ndraw stronger conclusions.\nOur formulation of the SV A task also relates\nto work which investigates neural networks’ abili-\nties to learn lexical abstractions (Chronis and Erk,\n2020; Kim and Smolensky, 2021) and to reason\nsystematically (Lake and Baroni, 2018; Yanaka\net al., 2019; Kim and Linzen, 2020; Goodwin et al.,\n2020). These studies on systematicity, however,\nrun controlled experiments by training small mod-\nels on toy data. Our work studies the widely-used\nBERT model, trained on real data and at scale.\n4 Experimental Setup\n4.1 Model\nDifferentiating between the hypotheses presented\nin §2 requires analyzing model performance on\nindividual items as a function of frequencies in\nthe training data. The original BERT model was\ntrained on both English Wikipedia and BooksCor-\npus (Zhu et al., 2015). However, BooksCorpus is\nnot publicly available (Bandy and Vincent, 2021),\nso when we pre-train our BERT models, we use\nonly the Wikipedia data (2.3 billion tokens). De-\nspite this difference in training data, our models\n935\nNatural Addition of such minor characters seem/seems more promotional than encyclopedic.\nOther popular trade items of the area include/includes sandalwood, rubber, and teak.\nThe party that originally buys the securities effectively act/acts as a lender.\nNonce The astronomer of the ﬁrst session of the court during that year perform/performs a...\nThe isometry in the gulf market/markets santa catalina island.\nThe sheepdog of basic needs providers ... review/reviews a damaging effect.\nTable 1: Examples of natural and nonce stimuli. Target verbs and their subjects are bolded. The model takes as\ninput the sentence with the verb masked, and is evaluated based on which verb inﬂection it scores more highly.\nachieve performances comparable to the public\nBERT-Base release on GLUE (Wang et al., 2018)\n(see Appendix A).\n4.2 Evaluation Stimuli\nWe evaluate the model’s SV A ability on two classes\nof stimuli: (1) natural sentences, which are gener-\nally both syntactically and semantically coherent,\nand (2) nonce sentences (following Gulordava et al.\n2018) which are grammatically valid but not nec-\nessarily semantically coherent (“ colorless green\nideas sleep furiously”, Chomsky, 1956). Examples\nof each are shown in Table 1.\nEvaluating on natural sentences provides a mea-\nsurement of how well the model can be expected\nto perform in realistic settings, but these sentences\nare not ideal for a targeted SV A evaluation since\nthey often contain additional cues relevant to verb\ninﬂection, such as other plural verbs or plural de-\nterminers, as in “ two [SUBJECT] and their dogs\n[VERB],” making it difﬁcult to discern whether a\nmodel has chosen a particular verb inﬂection based\non the subject. In contrast, performance on syn-\nthetic nonce sentences allows us to ensure that the\nonly source of information about the verb’s correct\ninﬂection is the subject itself.\nNatural Stimuli. Following Goldberg (2019),\nfor natural stimuli we use the dataset from Linzen\net al. (2016), which comprises 23,298 sentences\nfrom Wikipedia. The target verb is plural in 16,232\nof these sentences and singular in 7,064 of these\nsentences. These evaluation sentences span 176\nverb lemmas and 329 verb forms.\nNonce Stimuli. For our nonce stimuli, we com-\npiled a list of 200 nouns, 336 verbs, and 56 sen-\ntential contexts—sentence templates where we re-\nmove the original subject and verb—such that any\ngiven (noun, verb, sentential context) triplet yields\na grammatically correct nonce sentence. E.g., given\nthe sentence “the investigation of chaperones has\na long history\", we can create a sentential context:\n“the [SUBJECT] of chaperones [VERB] a long his-\ntory.” We can then randomly chose a noun and\nverb from our noun and verb lists (e.g., cities and\nplay) to construct a nonce sentence: “ The cities\nof chaperones play a long history. \" Considering\nall possible combinations of nouns, verbs, inﬂec-\ntions, and contexts yields a dataset of 7,526,400\nsentences which is is both large (c.f., 383 sentences\nin Gulordava et al. (2018)) and balanced in terms\nof number form (50% singular and 50% plural).\nTo ensure that constructed sentences are\ngrammatically correct, we apply several manual\nﬁlters (e.g., removing verbs that have ambiguous\ninﬂections), which are described in detail in\nAppendix B (with a list of all nouns, verbs, and\nsentential contexts in Appendix D). To verify the\nquality of the resulting stimuli, one of the authors\nmanually examined 154 randomly generated nonce\nsentences in the same way that they would be\npresented to the model. The verb inﬂection was\ncorrectly predicted in all but one of the instances\n(with the single error attributed to carelessness),\nand the annotator conﬁrmed that all generated\nsentences were grammatically correct. Our stimuli\nand code are available at https://github.com/\ngoogle-research/language/tree/master/\nlanguage/bertology/frequency_effects.\n5 Exploratory Analyses: What Factors\nCorrelate with Error Rates?\nWe ﬁrst perform an exploratory analysis of how the\nmodel’s abilities on the SV A task vary as a func-\ntion of pre-training frequency. As discussed in §2,\nwe consider generalization to unseen subject-verb\npairs to be evidence of symbolic reasoning (H1 or\nH3), and strong frequency effects to suggest item-\nspeciﬁc learning (H2 or H3). Note that in these\nexperiments, it is not the individual lexical items—\nthe subject and verb—that are unseen, only the\ncombination of them in a single sentence. There-\nfore, this analysis evaluates the model’s ability to\nperform abstract reasoning about individual items\nfor which it has learned representations.\n936\nNatural Nonce\nSeen Unseen Seen Unseen\nargmaxV P(V) 39.1 39.0 50.0 50.0\nargmaxV P(SV) 22.9 50.0 41.2 50.0\nBERT 3.3 8.8 15.6 17.6\nTable 2: Error rate on natural and nonce evaluation\nstimuli, stratiﬁed by whether the subject–verb pair was\nseen (frequency ≥1) or unseen (frequency = 0) dur-\ning pre-training. Heuristics (argmaxes) show perfor-\nmance for a item-speciﬁc learner that memorizes proba-\nbilities of speciﬁc verbs (V) or subject–verb (SV) pairs.\nBERT’s performance degrades on unseen pairs, but is\nsigniﬁcantly better than these heuristics.\n5.1 Overall Performance\nOverall, the model’s error rate is 3.2% on natural\nstimuli and 16.8% on nonce stimuli. This is similar\nto Goldberg (2019)’s reported 3% error on natural\nstimuli from Linzen et al. (2016) and a 17% error\non nonce stimuli from Gulordava et al. (2018).3\n5.2 Unseen Subject–Verb Pairs\nTable 2 stratiﬁes error rate by seen and unseen\nsubject–verb pairs. Compared with subject–verb\npairs seen at least once during training, error rates\non unseen subject–verb pairs are 5% higher on nat-\nural sentences and 2% higher on nonce sentences.\nThis degradation, however, is minimal compared\nwith what we might expect from a naive item-\nspeciﬁc learner (H2), represented by the heuristic\nbaselines in Table 2. These results thus suggest that\nBERT reasons over representations that abstract to\nsome degree over individual words, though it does\nnot meet the deﬁnition of a fully-abstract symbolic\nlearner (H1), which would have no degredation in\nperformance.\n5.3 Frequency of the Target Form\nTo further examine the effect of frequency, we draw\ninspiration from the human language processing\nliterature. One of the most widely-observed phe-\nnomena in such research is that high-frequency\nwords are learned better (Ambridge et al. 2015):\nReduce Error Hypothesis. High-frequency forms\nreduce errors in contexts where they are the target.\nFigure 1 stratiﬁes error rate by the training fre-\nquency of (1) subject–verb pairs and (2) verbs (in-\ndependent of subject). On both natural and nonce\nstimuli, error rate decreases for more-frequent\n3The Gulordava et al. (2018) stimuli slightly differ in that\nall content words (not just the subject and verb) were replaced.\n0\n1–1011–100101–1000\n>1000\n0\n5\n10\n15\n20\nFrequency of subject–verb pair in the training set\nError rate (%)\nNatural Sentences\n0\n1–1011–100101–1000\n>1000\nNonce Sentences\n<50k\n50k–100k100k–200k200k–400k400k–800k\n>800k\n0\n10\n20\n30\n40\nFrequency of target verb in the training set\nError rate (%)\nNatural Sentences\n<25k\n25k–50k50k–100k100k–200k200k–400k400k–800k\n>800k\nNonce Sentences\nFigure 1: Error rate stratiﬁed by how often subject–\nverb pairs appeared in the same sentence in BERT’s\ntraining set (top) and how often verbs appeared in the\ntraining set (bottom). Error rate was lower for subject–\nverb pairs and verbs that were more frequent.\nsubject–verb pairs and more-frequent verbs, con-\nsistent with the Reduce Error hypothesis.\n5.4 Frequency of the Competing Form\nAlthough seeing a verb more often in training often\nreduces errors when that verb is the target, when\nhigh-frequency forms are not the target, they can\nact as distractors and reduce accuracy:\nCause Error Hypothesis. High-frequency forms\ncause errors when a competing, lower-frequency\nform is the target (Ambridge et al., 2015).\nIs BERT’s error rate similarly affected by distractor\nfrequency effects? For instance, the word “com-\nbat, ”which is not only the plural form of the verb\n“combat” but also a fairly frequent noun, appears\n102×more often in the training set than“combats. ”\nIf word frequency inﬂuences BERT’s predictions,\nthen such asymmetries may cause a high error rate\nwhen the target form is “combats. ”\nAs Figure 2 shows, error rate is lower when the\ntarget form is more frequent relative to the compet-\ning form. For nonce sentences, for example, error\nrate was only 2.2% when the target form was 16\n937\n<1/8\n1/8 to 1/41/4 to 1/21/2 to 11 to 22 to 4\n>4\n0\n20\n40\n60\nFrequency ratio of target verb\nform to competing verb form\nError rate (%)\nNatural Sentences\n<1/16\n1/16 to 1/81/8 to 1/41/4 to 1/21/2 to 11 to 22 to 44 to 88 to 16\n>16\nNonce Sentences\nFigure 2: Error rate stratiﬁed by the ratio between the\nfrequency of the target verb form versus the compet-\ning verb form. BERT’s error rate was higher when the\ncompeting verb form occurred more frequently in the\ntraining set than the target verb form.\ntimes or more as frequent than the competing form,\ncompared with 62.5% when the competing form\nwas 16 times or more frequent than the target form.\n5.5 Takeaways\nThe above exploratory analyses suggest that BERT\nis inﬂuenced by both the absolute frequency of the\ntarget form (Reduce Error Hypothesis), as well as\nthe frequency of the target relative to the competing\nform (Cause Error Hypothesis). Although these\nresults are strong correlational evidence, absolute\nand relative frequency are highly correlated with\none another (i.e., as the absolute frequency of a\nword increases, so does its frequency relative to\nother words).4 Thus, more controlled studies are\nneeded to establish which effects have a causal\neffect on BERT’s rule-learning.\n6 Conﬁrmatory Analysis: Manipulating\nthe Training Data\nTo better understand the above trends, we design\na set of experiments in which we manipulate one\nvariable (absolute or relative frequency of a verb\nform in pre-training) while holding the other ﬁxed.\n6.1 Experimental Setup\nWe select 60 verbs of interest (VOIs) and manip-\nulate their training set frequencies. We choose\nthe VOIs by taking 60 transitive verbs for which\nboth singular and plural forms of each verb oc-\ncur at least 104 times in the corpus (the full list is\nshown in Appendix D.4). We remove all sentences\n4This correlation is not only intuitive but also empirical—\nsee Figure 10 in Appendix C.3.\n100 101 102 103 104\n0\n10\n20\n30\n40\n50\nFrequency of VOI in training set\nError rate (%)\nNonce\nNatural\nFigure 3: Effect of absolute frequency of a verb of in-\nterest (VOI) when the ratio between singular and plural\nforms is held constant at 1:1. The error rate for sixty\nVOI is shown for BERT models that have seen the sixty\nVOI at different frequencies in the pre-training dataset.\ncontaining these VOIs from the training set, and,\nbased on the experiment, add them back in such\nthat VOIs appear at a speciﬁed (absolute or relative)\nfrequency. We evaluate the model’s performance\non these VOIs by using both a natural dataset of\napproximately 100 examples per VOI, as well as by\ninserting the VOIs into the nonce (noun + sentential\ncontext) constructions from §4.2.\nWe note that the exact size of the training set\nin our manipulations changes depending on how\nmany sentences containing VOIs are added in (e.g.,\nmodels which see 10,000 examples per VOI see\nmore total training examples than models that see\nonly 1 example per VOI). The difference in abso-\nlute terms, however, is small (less than 1% of the\ntotal training set). Thus, we consider it unlikely that\nany observed difference in performance is due to a\ndifference in the total size of the training corpus.5\n6.2 Absolute Frequency of Verb Form\nWe ﬁrst examine how the absolute frequency of a\nverb form affects the model’s number agreement\nability on that form. For each of nine frequen-\ncies n = 1, 3, 10, 30, 100, 300, 1,000, 3,000, and\n10,000, we train a new BERT model that sees the\nverbs n times each during training. To isolate the\neffect of absolute frequency, we ﬁx the relative fre-\nquency to be balanced—for each VOI, n instances\nare singular and n are plural.\nThe results of this experiment are shown in Fig-\nure 3. When the occurrences of a VOI are balanced\nbetween inﬂections (singular and plural), error rate\ndecreases monotonically when target form is more\nfrequent in training.\n5As one measure, masked language model accuracy (on the\nsame dev set) was 59.96% for a model with VOIs appearing\nat frequency 10,000, versus 59.91% for a model with VOIs\nappearing at frequency 1.\n938\n0\n25\n50\n75\n100\nError rate (%) for\ntarget form\nNatural Sentences Nonce Sentences\nNtarget = 100\nNtarget = 1,000\nHeuristic: pick more frequent\n0\n25\n50\n75\n100\nError rate (%) for\ncompeting form\n10−2 100 102\n0\n25\n50\n75\n100\nError rate\noverall (%)\n10−2 100 102\nFrequency ratio between competing\nand target verb form\nFigure 4: When the absolute frequency of the target\nverb form is held constant, increasing the relative fre-\nquency of the competing verb form increases error for\nthe target form and decreases error for the competing\nform. This behavior is in the same direction as a heuris-\ntic that, at inference time, picks the more frequent verb.\n6.3 Relative Frequency of Verb Form\nWe next analyze whether the frequency ratio be-\ntween a target verb form v and its competing form\nv′affects the model’s ability to producev in con-\ntext. To balance how often the target v is singular\nvs. plural, we use the following procedure. We ran-\ndomly split our 60 VOI into two groups of 30 verbs\neach, which we denote as Sand P. In each exper-\niment, we set the frequency of the singular verbs\nin Sto Nvary, while holding the frequency of the\nplural forms of the verbs in Sconstant at Nconstant.\nLikewise, we set the frequency of the plural verbs\nin Pto Nvary, and hold the frequency of the singular\nform of these verbs constant atNconstant. We run ex-\nperiments with Nvary = {1, 10, 30, 100, 300, 1,000,\n3,000, 10,000}, and do this twice for Nconstant set\nto 100 and 1,000. As our evaluation stimuli are\nbalanced such that both v and v′occur as the target\nin every template for every VOI, we are able to ana-\nlyze the effect of the v:v′frequency ratio—holding\nthe absolute frequency of v ﬁxed—for v:v′ranging\nfrom 1:100 to 100:1.\nFigure 4 shows the results. When the competing\nform occurs more frequently (with respect to the\ntarget form), error rate increases for the target form\nand decreases for the competing form.\n7 Disentangling Sources of Error\n7.1 Setup\nOur goal is to characterize BERT’s rule-learning\nbehavior in terms of the three hypotheses H1–H3\ndescribed in §2. The frequency effects observed\nin §6 rule out H1 (Idealized Symbolic Learner).\nHowever, BERT’s generalization to unseen noun-\nverb pairs (§5.2) is too good to be explained by\nH2 (Item-Speciﬁc Learner). Hence, the hybrid H3\n(Symbolic Learner with Noisy Observations) seems\nlike the most plausible candidate.\nH3 is not simply a catch-all compromise be-\ntween rule-based and item-speciﬁc learners—H3\nmakes speciﬁc predictions about the nature of the\nerrors BERT will make. Under H3, BERT repre-\nsents the SV A rule and the concept of agreement\nfeatures, and follows the rule as long as it identiﬁes\nthe number of the subject and verb correctly. Thus,\nH3 predicts that observed errors are due to failures\nto identify the number of either the subject or verb.\nGiven such a model, we might observe frequency\neffects because training frequency inﬂuences the\nmodel’s ability to predict the agreement feature\nfor a given verb form. That is, we might observe\na trend like the following: if a verb v occurs in\nfewer than some n training examples, BERT mis-\npredicts the agreement feature (e.g., predicting v\nto be singular when v is plural); if v occurs more\nthan n times, BERT correctly predicts v’s agree-\nment feature and correctly produces v in context.\nIn this scenario, we expect that SV A errors will\ncorrelate with frequency, but the frequency of these\nerrors should not exceed the error rate in predicting\nagreement features.\n7.2 Predicting Agreement Feature\nTo test whether the above predicted pattern holds,\nwe use two probing classiﬁers (see Veldhoen et al.,\n2016; Ettinger et al., 2016, on probing) which we\ndescribe below.\nSubject agreement feature probe. Our ﬁrst\nprobe evaluates whether, given a sentence with the\nverb masked, the embedding at the masked posi-\ntion contains information as to whether a singular\nor plural verb is required. This setup actually eval-\nuates two subtasks: identifying the subject of the\n939\nverb (i.e., parsing the sentence) and predicting the\nagreement feature of the identiﬁed subject. For\nsimplicity, however, we use a single probing clas-\nsiﬁer because our interpretation does not hinge on\ndifferentiating these subtasks. We use our senten-\ntial templates for this experiment, for which the\nonly cue for the number of the subject (and hence\nthe verb) is the subject itself. Hence, if the embed-\nding at the masked position can be used to predict\nnumber, it follows that both the subject has been\nidentiﬁed correctly and that the agreement feature\nof the subject was identiﬁed correctly.\nWe feed our nonce sentences from §4.2 with\nthe verb masked into the model, retrieve the ﬁnal\nhidden state representation of the masked token,\nand train an MLP to classify the desired verb form\n(singular or plural). We train the probe using cross-\nvalidation, using sentences constructed from 150\nsubjects ×50 sentential for training, and the re-\nmainder for evaluation. Subjects and sentential\ncontexts in evaluation sentences are not seen by the\nprobe during training.\nVerb agreement feature probe. Our second\nprobe predicts the number of a verb from its con-\ntextual word embedding. If a probe can predict the\nnumber of a verb given its contextual word embed-\nding, we can conclude that the model represents\nthe agreement feature of that verb form and thus its\npredictions about SV A can, in principle, depend on\nthe agreement feature. We obtain contextual em-\nbeddings of verbs by inserting them into the nonce\nsentential contexts, with the noun masked so that\nthere are no external clues aside from the form of\nthe verb that indicate whether the verb is singular\nor plural. We then train an MLP to, given the em-\nbedding, classify the verb as singular or plural. We\nuse the 331 verbs from our nonce stimuli that were\nnot VOI (×56 sentential contexts per verb) to train\nthe probe and the 60 VOI (×56 sentential contexts\nper verb) to evaluate it.\nResults. We evaluate these two probes as a func-\ntion of both absolute frequency (using models from\n§6.2) and relative frequency (using models from\n§6.3). Results are shown in Figure 5 and Figure 6,\nrespectively.\nFor absolute frequency (Figure 5), we see that\nthe accuracy of the verb agreement feature probe\nis highly dependent on absolute frequency of the\nVOI. The probe has lower error for models that\nsaw the verb form more often, implying that seeing\n100 101 102 103 104\n0\n10\n20\n30\n40\n50\nFrequency of VOI in training set\nError rate (%)\nError in predicting agreement feature of subject (S)\nError in predicting agreement feature of verb (V)\nCombined agreement feature errors (S+V)\nObserved error of BERT on SV A\nFigure 5: Errors in either identifying the number agree-\nment feature of the subject or identifying the number\nagreement feature of the verb comprised a large portion\nof the observed SV A error.\n10−2 100 102\n0\n25\n50\n75\n100\nFrequency ratio between\ncompeting and target verb form\nError rate (%)\nError in predicting agreement feature of target verb\nObserved error of BERT on target verbs\nFigure 6: The error rate of a probe that predicts the\nagreement feature of a verb (red triangle) is not corre-\nlated with frequency of competing verb forms. More-\nover, the error rate of this probe does not correlate with\nthe observed error of BERT on the target verb, which is\nhighly affected by frequency of competing verb forms.\nforms more frequently in training led to embed-\ndings of those forms that better encode the number\nagreement feature. In constrast, the accuracy of the\nsubject agreement feature probe is constant, which\nis expected because identifying the number feature\nof a subject should not be affected by absolute fre-\nquency of VOI. Notably, the combined error rate of\nour two probes falls close to the model’s observed\noverall error rate on the SV A task, as predicted by\nour “Symbolic Reasoner with Noisy Observations”\nhypothesis (H3).\nFor relative frequency (Figure 6), on the other\nhand, we see no clear increase or decrease in the\naccuracy of predicting the agreement feature for a\ntarget verb form v in response to changes in fre-\nquency of the competing verb form v′. In other\nwords, when one verb is much more frequent than\nthe other, BERT produces the more common verb\n940\nform despite having access (in principle) to the in-\nformation (rule + agreement features) that would\nallow it to infer the correct form. Such behavior is\nnot explicitly accounted for by the “noisy observa-\ntions” in H3, and thus appears more as evidence of\nitem-speciﬁc learning (in line with H2).\n8 Discussion\nThe goal of this work is to determine whether\nBERT performs SV A by implicitly representing\nrules deﬁned over abstract agreement features and\ncharacterize the training conditions under which\nsuch representations emerge. We differentiate be-\ntween the representation of the rule (“if x then y”)\nand that of observations (containing the correct\nagreement features). We draw two conclusions,\nwhich suggest a mix of systematic rule-like gener-\nalization and unsystematic item-speciﬁc inferences.\nBERT appears to represent the correct rule,\nbut fails to predict agreement features for low-\nfrequency verb forms. Although the error rate\ndecreases as a function of frequency of target verb\nv (§6.2), BERT’s ability to predict the agreement\nfeature of v (§7.2) follows the same trend. This ob-\nserved behavior is thus consistent with a model that\ncorrectly represents the SV A rule (§2), but makes\nmistakes at inference time due to noise in the repre-\nsented observations for low frequency verb forms\n(for example, producing “run” in the context “the\ndog run” because “run” is incorrectly encoded as\nsingular), rather than due to a failure to represent\nthe concept of singular altogether.\nBERT fails to apply the rule when doing so re-\nquires overcoming strong item-speciﬁc priors.\nSimilar to the absolute frequency trend, we see\nthat BERT’s error rate on SV A also decreases as\na function of the frequency of the target verb v\nrelative to its competing form v′ (§6.3). Unlike\nabove, however, we see no effect of the frequency\nratio Nv : Nv′ on BERT’s ability to predict the\nagreement feature of v when the frequency of v is\nﬁxed (§7.2). These results suggest that BERT is\nheavily inﬂuenced by skewed training distributions,\npreferring to produce more common verb forms\nover forms consistent with the rule. Such behav-\nior could either mean that, when P(v) << P(v′),\n(1) BERT represents the correct SV A rule but it is\noverridden in favor of the prior, or (2) BERT does\nnot represent the rule at all. Teasing apart these\npossibilities is a valuable direction for future work.\nOpen questions. Our results on absolute fre-\nquency effects indicate that BERT does not infer\nagreement features until it sees 10–100 examples\nof a verb, even though it is possible, in principle,\nto infer agreement features from a single training\nexample (e.g., “All of the dogs dax” implies “dax”\nis a plural verb form). Future controlled studies\ncould investigate how the sample efﬁciency of in-\nferring agreement information depends on factors\nsuch as architecture (e.g., access to morphological\nsignals), size of the model, and amount of training\ndata. Analysis of such patterns would elucidate\nhow models like BERT (and by extension, trans-\nformers and neural networks more generally) learn\nand generalize, enabling more principled develop-\nment and deployment.\n9 Conclusions\nWe have studied whether BERT’s performance on\nsubject–verb agreement exhibits rule-governed be-\nhavior. We focus on frequency effects, pre-training\nmultiple BERT instances in order to isolate how\nthe model’s predictions are affected by absolute\nand relative verb frequency. Our results suggest\nthat BERT’s behavior is consistent with a system\nthat correctly applies the SV A rule in general but\nstruggles to overcome strong training priors and to\nestimate agreement features (singular vs. plural) on\ninfrequent lexical items.\nAcknowledgements\nWe thank Ian Tenney, Ryan Cotterell, Clara Meis-\nter, Charles Lovering, and Slav Petrov for feedback\non our manuscript. We thank Christo Kirov, Kellie\nWebster, and Jacob Eisenstein for helpful discus-\nsions about the project.\nReferences\nBen Ambridge, Evan Kidd, Caroline F. Rowland, and\nAnna L. Theakson. 2015. The ubiquity of frequency\neffects in ﬁrst language acquisition. Journal of\nChild Language, 42(2):239–273.\nJack Bandy and Nicholas Vincent. 2021. Addressing\n\"documentation debt\" in machine learning research:\nA retrospective datasheet for bookcorpus. arXiv\npreprint arXiv:2105.05241.\nPeter F. Brown, John Cocke, Stephen A. Della Pietra,\nVincent J. Della Pietra, Fredrick Jelinek, John D.\nLafferty, Robert L. Mercer, and Paul S. Roossin.\n1990. A statistical approach to machine translation.\nComputational Linguistics, 16(2):79–85.\n941\nRui Chaves and Stephanie Richter. 2021. Look at that!\nBERT can be easily distracted from paying attention\nto morphosyntax. In Proceedings of the Society for\nComputation in Linguistics.\nNoam Chomsky. 1956. Three models for the descrip-\ntion of language. IRE Transactions on Information\nTheory, 2(3):113–124.\nGabriella Chronis and Katrin Erk. 2020. When is a\nbishop not like a rook? When it’s like a rabbi! Multi-\nprototype BERT embeddings for estimating seman-\ntic relationships. In Proc. of CoNLL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. of NAACL.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classiﬁcation tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP.\nJerry A. Fodor and Zenon W. Pylyshyn. 1988. Connec-\ntionism and cognitive architecture: A critical analy-\nsis. Cognition, 28(1-2):3–71.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nEmily Goodwin, Koustuv Sinha, and Timothy J.\nO’Donnell. 2020. Probing linguistic systematicity.\nIn Proc. of ACL.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProc. of NAACL.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment of\nsyntactic generalization in neural language models.\nIn Proc. of NAACL.\nNajoung Kim and Tal Linzen. 2020. COGS: A compo-\nsitional generalization challenge based on semantic\ninterpretation. In Proc. of EMNLP.\nNajoung Kim and Paul Smolensky. 2021. Testing for\ngrammatical category abstraction in neural language\nmodels. Proceedings of the Society for Computation\nin Linguistics.\nBrenden Lake and Marco Baroni. 2018. Generaliza-\ntion without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\nIn Proc. of ICML.\nTal Linzen and Marco Baroni. 2020. Syntactic struc-\nture from deep learning. Annual Review of Linguis-\ntics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. TACL.\nCharles Lovering, Rohan Jha, Tal Linzen, and Ellie\nPavlick. 2021. Predicting inductive biases of pre-\ntrained models. In Proc. of ICLR.\nAlec Marantz. 2013. Words and rules revisited: Re-\nassessing the role of construction and memory in\nlanguage. In 27th Paciﬁc Asia Conference on\nLanguage, Information, and Computation, PACLIC\n2013. National Chengchi University.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proc. of\nEMNLP.\nBenjamin Newman, Kai-Siang Ang, Julia Gong, and\nJohn Hewitt. 2021. Reﬁning targeted syntactic eval-\nuation of language models. In Proc. of NAACL.\nJudea Pearl. 1988. Probabilistic reasoning in intelli-\ngent systems: Networks of plausible inference.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. TACL.\nPaul Smolensky. 1988. On the proper treatment of\nconnectionism. Behavioral and Brain Sciences ,\n11(1):1–23.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical BERT models for sequence labeling.\nIn Proc. of EMNLP.\nSara Veldhoen, Dieuwke Hupkes, and Willem Zuidema.\n2016. Diagnostic Classiﬁers: Revealing how Neural\nNetworks Process Hierarchical Structure. In CEUR\nWorkshop Proceedings.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020. Learning which\nfeatures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proc. of\nEMNLP.\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, Ken-\ntaro Inui, Satoshi Sekine, Lasha Abzianidze, and Jo-\nhan Bos. 2019. Can neural networks understand\nmonotonicity reasoning? In Proceedings of the\n2019 ACL Workshop BlackboxNLP.\nCharles Yu, Ryan Sie, Nicolas Tedeschi, and Leon\nBergen. 2020. Word frequency does not predict\ngrammatical knowledge in language models. In\nProc. of EMNLP.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proc. of ICCV.\n942\nA BERT Model\nTo analyze the effect of word frequency on BERT’s\nability to follow SV A, we need to know the exact\nnumber of occurrences of each word in the dataset.\nThe original BERT checkpoint (Devlin et al., 2019)\nuses both Wikipedia and BooksCorpus (Zhu et al.,\n2015), but BooksCorpus is no longer publicly avail-\nable (Bandy and Vincent, 2021). So we train a\nreplicated version of BERT on only wikipedia data.\nOur version of BERT largely follows the proce-\ndure of the original, differing only in that we use\ndynamic masking and pre-train for 4 million up-\ndates at a learning rate of 3e-4.6 Table 3 shows the\nperformance of our replicated version of BERT.\nDownstream Task\nPre-training data MRPC CoLA MNLI SST2\nWikipedia + BooksCorpus 86.6 82.1 84.4 92.8\nWikipedia only 86.2 78.7 84.3 92.2\nQQP QNLI RTE STS-B\nWikipedia + BooksCorpus 91.2 91.6 68.5 89.3\nWikipedia only 90.8 91.5 64.9 88.8\nTable 3: Performance of our replicated BERT check-\npoint, which is pre-trained on only Wikipedia data,\ncompared with the original BERT checkpoint, which\nused both Wikipedia and BooksCorpus.\nB Nonce Stimuli Collection Details\nThis appendix section details our nonce stimuli col-\nlection process. Our goal is to create a large set\nof evaluation stimuli in which we can analyze how\nproperties of certain stimuli (e.g., subjects, verbs,\nand sentential contexts) affect the model’s ability to\nperform number agreement. Therefore, we create\na list of 200 nouns, 336 verbs, and 56 sentential\ncontexts such that any given (noun, verb, sentential\ncontext) triplet where the noun is used as the sub-\nject yields a grammatically correct nonce sentence.\nBy considering both singular and plural inﬂections\nfor possible triplet, we analyze a dataset of 2 ·200\n·336 ·56 = 7,526,400 sentences. To facilitate fur-\nther use of our dataset, we make a plain-text version\navailable at http://anonymized.\n6The decision to use dynamic masking was made to the\navailability of code, rather theoretically or empirically moti-\nvated. We train for additional updates because the develop-\nment loss did not converge at 1 million updates (the original\nnumber used in the paper).\nB.1 Nouns\nTo propose candidate nouns, we ﬁrst ran a POS tag-\nger (Tsai et al., 2019) over the pre-training dataset,\nand retrieved all nouns occurring at least 100 times.\nThen, we randomly sampled 200 nouns from this\nset of candidate nouns that were evenly distributed\ninto four buckets of training set frequency (100–\n999, 1,000–9,999, 10,000–99,999, and 100,000+).\nAll nouns were common nouns and were manually\nvalidated to have correct, unambiguous singular\nand plural inﬂections.\nB.2 Verbs\nTo propose candidate verbs, we similarly retrieved\nall verbs that occurred at least 100 times in the\ntraining set. The masked-LM evaluation procedure\nfor SV A requires that both the singular and plural\ninﬂections of the verb exist directly in the model’s\nvocabulary, and so we ﬁltered out verbs that did not\nmatch this criteria, leaving us with 379 candidate\nverbs.\nUnlike for nouns, we generally cannot indiscrim-\ninately swap out verbs in a sentence while main-\ntaining grammatical correctness, since some verbs\nare exclusively transitive (used with an object) or\nintransitive (used without an object). In English,\nmore verbs can be used transitively than intransi-\ntively, and so we decided to consider only transitive\nverbs. We manually ﬁltered out strictly intransitive\nverbs and ensured that each verb had correct, un-\nambiguous singular and plural inﬂections, leaving\nus with 336 verbs that can be used transitively.\nB.2.1 Sentential Contexts\nFinally, we curated a list of sentential contexts\n(sentences with the subject and verb removed) that\nwould maintain grammatical validity for both sin-\ngular and plural forms of any given subject–verb\npair from our list of nouns and list of transitive\nverbs. To get candidate sentential contexts, we\nrandomly sampled 600 sentences from the Linzen\net al. (2016) dataset of Wikipedia sentences to be\nmanually examined. We kept only 56 of these 600\ncandidate sentential contexts, ﬁltering out 544 for\nthe following reasons:\n• Sentential contexts that contained additional cues\nfor number outside of the subject and verb inﬂec-\ntion cannot form grammatical sentences for both\nsingular and plural subject–verb pairs. For in-\nstance, the sentential context “[SUBJECT], who\nthinks roses are red, [VERB] ...” can only be used\n943\nwith singular subjects and verbs because of the\nmodifying clause “who thinks roses are red”; and\nthe sentential context “[SUBJECT] in the park\n[VERB] ...” can only be used with plural subjects\nand verbs because there is no determiner for the\nsubject. 381 sentences like the above had such\ncues for number inﬂection and were removed.\n• 64 sentential contexts contained verb usages that\nwere hostile to swapping in most transitive verbs\n(e.g., in “ [SUBJECT] shows that ...”, “shows”\ncould not be replaced with most transitive verbs).\n• 15 sentential contexts contained noun usages that\nwere hostile to swapping in most nouns (e.g., in\nthe fact that she likes him ... , the subject fact\ncannot be replaced with most nouns).\n• 21 sentential contexts were ungrammatical or\nincomprehensible to our human annotator.\n• In 36 sentential contexts, the subject and verb\nparsed by Linzen et al. (2016) was incorrect.\n• 27 sentential contexts for which the original verb\nwas used intransitively were removed.\nB.2.2 Human evaluation\nTo check the validity of the test set, the ﬁrst author\nmanually examined 160 generated nonce sentences\nin the same fashion that the model would evaluate\nthem. That is, each example comprised either a sin-\ngular or plural noun inserted into a template, and\nthe ﬁrst author had to predict the correct number in-\nﬂection of a given verb. In addition, the ﬁrst author\nhad to verify that the sentence was grammatical\nand contained no number inﬂection cues other than\nthe inﬂection of the subject. The sentences were\npresented in random order, with the ﬁrst author\nblinded from the ground-truth label.\nThe ﬁrst author found that 6 sentences (3 tem-\nplates, since each template had two inﬂections) con-\ntained additional number inﬂection cues that were\nmissed in the ﬁrst round of annotation, and so these\nsentences were removed. In terms of accuracy, the\nﬁrst author correctly predicted the verb inﬂection\n153 of the 154 instances (99.4% accuracy) and at-\ntributed the single error to carelessness. We take\nthese manual evaluation results as evidence that\nour test set is grammatical and tests a syntactic rule\nthat can be consistently applied by humans.\nC Additional Figures and Tables\nWe show several auxiliary experiments that eluci-\ndate BERT’s performance with respect to various\ncharacteristics of evaluation stimuli.\nC.1 Relative Frequency of Forms\nFollowing the results from §5.4, we show the error\nrate for singular and plural forms of all verbs in our\nnonce stimuli in Figure 7. Additionally, Table 4\nshows the ﬁve verbs with the highest and lowest\nerror rates, as well as their frequency ratios.\nC.2 Comparison with prior work\nBecause our work proposes a new set of nonce\nstimuli (which is larger than prior work, e.g., 383\nsentences from Gulordava et al. (2018) or 7 verbs\nfrom Chaves and Richter (2021)), we run several\nanalyses from prior work on our dataset. The re-\nsults are largely consistent with conclusions from\nprior work.\nAttractors. As shown in Table 5, BERT did not\nperform worse on templates with more attractors\n(clauses between the subject and verb), corroborat-\ning Goldberg (2019).\nNoun frequency. We ﬁnd similar evidence, like\nYu et al. (2020), that BERT did not perform better\non nouns that were more frequent in the training set.\nFigure 8 shows these results—for each subject (we\nconsider both singular and plural forms as a single\nsubject), we plot that subject’s error rate against its\nfrequency in the training data.\nHigh- and low-conﬁdence predictions. As an\nauxiliary analysis to compare with Newman et al.\n(2021), we plot the error rate of BERT with respect\nto different thresholds for how conﬁdent the model\nwas about its prediction. Figure 9 shows error rates\nfor all predictions with conﬁdence above some\nthreshold, and error rates for predictions with con-\nﬁdence below some threshold. This result concurs\nwith Newman et al. (2021)’s ﬁnding that model per-\nformance drops when testing verbs that the model\nﬁnds unlikely.\nC.3 Absolute versus relative frequency\nAs additional background, Figure 10 shows the\ncorrelation between absolute frequency of a verb\nform and its frequency relative to its competing\nform.\n944\nFigure 7: For all 366 verbs ( ×2 inﬂections per verb),\nwe plot the error rate against inﬂection skew, which is\nhow much more frequent the correct inﬂection of the\nverb occurred than the incorrect inﬂection in the pre-\ntraining data. Several of the most skewed words are\nshown—for instance, long appears 490 times more of-\nten in the pre-training data than longs, so its inﬂection\nskew is log10(490) = 2.69. Conversely, longs has an\ninﬂection skew of −2.69.\nInﬂection Skew\n(Log10) Error Rate\nBest-performing verbs\nlong 2.69 0.0%\nspeed 0.95 0.2%\ncombat 0.95 0.2%\nround 2.01 0.4%\nﬁsh 1.25 0.6%\nWorst-performing verbs\nlongs -2.69 98.5%\nﬁshes -1.25 88.0%\nrounds -0.95 82.1%\ncombats -2.01 79.6%\nspeeds -0.95 74.1%\nTable 4: The verbs for which BERT had the highest and\nlowest error rates. Inﬂection skew indicates how much\nmore often a verb appeared in BERT’s pre-training data\ncompared with its other number inﬂection, in log10.\nFor instance, an inﬂection skew of 1 indicates that a\nverb appeared 101 = 10times more often in the train-\ning set than its other number inﬂection.\nStratiﬁcation of Stimuli Examples Error Rate\nAll examples 10.2M 17.9%\nTemplates with one attractor 3.9M 15.2%\nTemplates with two attractors 1.6M 22.3%\nTemplates with three attractors 1.3M 16.8%\nTemplates with four attractors 672k 13.0%\nTable 5: Performance for templates with different num-\nbers of attractors (distracting clauses between the sub-\nject and verb).\nFigure 8: For all 200 subjects (both singular and plural\nforms are included into a single subject), we plot the\nerror rate against the frequency of that subject in the\npre-training data.\nFigure 9: Error rates of examples for which the model’s\npredictions were above and below certain thresholds.\nFigure 10: For all 336 verbs (672 verb forms), we plot\nthe absolute frequency of a verb form versus the rela-\ntive frequency of that verb form compared with its com-\npeting form. Pearson’sR2 = 0.356.\n945\nC.4 Training manipulations: Seen vs. Unseen\nFigure 3 in the main body showed the performance\nof models that have seen the VOI n times in train-\ning, where n varies from 1 to 10,000. Table 6\nstratiﬁes this performance on the nonce evaluation\nstimuli by seen and unseen subject–verb pairs in\nthe evaluation stimuli. Note, though, that this strat-\niﬁcation differs for each VOI frequency. That is,\nthere will be more unseen subject–verb pairs when\nthe VOIs are less frequent in the training set.\nFrequency Seen SV Unseen SV\nof VOI # examples Error # examples Error\n1 672 63.4% 1.34M 50.1%\n3 2.6k 54.8% 1.34M 48.9%\n10 7.4k 43.7% 1.34M 45.1%\n30 21.6k 29.6% 1.32M 30.8%\n100 59.9k 20.9% 1.28M 23.7%\n300 135k 22.1% 1.21M 22.8%\n1,000 289k 20.4% 1.06M 20.0%\n3,000 456.3k 16.3% 887.7k 16.9%\n10,000 662.k 15.4% 682.1k 14.5%\nTable 6: Stratifying seen (frequency > 0) and unseen\n(frequency = 0) subject–verb pairs in the nonce sitmuli\nfor our training manipulation from Figure 3.\n946\nD Raw SV A Nonce Stimuli\nD.1 Verbs\nThe 336 verbs used in our nonce stimuli are listed\nbelow (only plural/base inﬂections are shown): add,\nadvance, age, aim, air, allow, analyse, angle, approach, archive,\narrive, ask, assist, attack, attempt, award, bar, base, battle, bear,\nbecome, begin, believe, beneﬁt, block, board, bond, book, bor-\nder, branch, brand, break, bridge, bring, call, campaign, carry,\ncause, center, centre, challenge, champion, change, channel,\ncharge, chart, circle, claim, class, coach, code, color, colour,\ncombat, comment, compound, comprise, concern, connect,\nconsider, contact, contain, continue, contract, control, copy,\ncount, course, cover, create, credit, critique, crop, cross, cycle,\ndate, deal, debate, decide, deﬁne, demand, describe, design,\ndetail, develop, discover, display, dispute, distance, document,\ndouble, draw, drive, drug, effect, end, enter, equip, estimate,\nexhibit, experience, explain, extend, eye, face, factor, fan,\nfarm, feature, feel, ﬁeld, ﬁght, ﬁle, ﬁll, ﬁnance, ﬁnd, ﬁre, ﬁsh,\nﬂy, follow, force, form, frame, fund, gain, get, give, grade,\ngraduate, grant, group, grow, guard, guide, hand, have, head,\nhelp, hold, honor, honour, host, house, include, increase, in-\ndicate, inﬂuence, interview, involve, issue, join, judge, keep,\nkill, know, label, land, lap, lead, learn, leave, level, light, limit,\nline, link, list, live, long, look, love, maintain, make, manage,\nmap, mark, market, master, match, matter, mean, measure,\nmeet, mention, minister, model, move, murder, name, need,\nnote, number, object, offer, open, operate, order, own, pair,\npark, partner, pass, pattern, pay, peak, perform, picture, pilot,\nplace, plan, plant, play, position, post, pound, power, prac-\ntice, present, print, process, produce, program, project, protest,\nprove, provide, question, race, raid, range, rank, rate, reach,\nreason, receive, record, refer, reference, reﬂect, refuse, release,\nremain, repair, report, represent, reprise, require, reserve, re-\nturn, reveal, review, ring, rise, risk, rival, round, route, rule,\nrun, sample, say, scale, score, seat, see, seed, seek, send, serve,\nservice, share, ship, show, sign, signal, single, sketch, slope,\nsound, source, speak, speed, sport, spot, stage, stand, star,\nstart, state, stop, store, stream, strike, strip, structure, study,\nstyle, suggest, supply, support, surface, tackle, take, talk, tar-\nget, task, tax, tell, term, test, tour, trace, track, trail, train,\ntransport, travel, trend, trouble, try, turn, use, value, vary, vent,\nview, visit, voice, volunteer, walk, want, wave, win, witness,\nwork, write,\nD.2 Nouns\nThe 200 nouns used in our nonce stimuli are listed\nbelow (only singular inﬂections are shown): abuser,\nactuary, afﬁliate, album, application, artefact, articulation,\nartiste, aspect, astronomer, attempt, attribution, autopilot,\nball, barnacle, basalt, batch, battalion, beaker, bettor, bid-\nder, biosensor, blazer, bluebird, brake, brush, bulletin, busi-\nness, campaign, capital, captor, caretaker, catholic, caveman,\ncharge, chestnut, clarinetist, climate, columnist, command,\ncommando, commuter, comparison, compiler, constant, con-\nsul, craftsman, credential, cup, debate, debater, demigod, de-\nvice, dhole, disorder, distribution, diviner, draft, drum, dynasty,\nechidna, electron, emoticon, enclave, etymology, exhibition,\nexplosive, faith, fanatic, fantasy, fat, ferret, ﬁction, foal, for-\nager, form, forwarder, fossil, foundation, franchise, friendship,\ngirl, glass, good, grantee, grapevine, hair, harmonic, head-\nlamp, hedgehog, hotel, hypothesis, imp, impact, instruction,\nintensiﬁer, interest, intrusion, island, isometry, kabbalist, kind,\nlaunch, layer, legionnaire, lioness, loading, locksmith, loga-\nrithm, logger, mammoth, martin, matchup, microphone, misﬁt,\nmotorcyclist, nasal, necessity, ofﬁcer, ogre, opposition, palace,\npanchayat, parrot, pioneer, platform, plum, poet, possibility,\npostposition, potentiometer, president, press, pro, proponent,\nprovider, race, radiologist, rank, rat, reaper, region, relief,\nremark, repeater, repellent, rescuer, researcher, retriever, rib-\nbon, ride, ring, rogue, role, sage, salaryman, seagull, section,\nselection, sense, sex, shearer, sheepdog, shoreline, siding,\nsign, simulation, situation, skateboarder, snowﬂake, sorcerer,\nspecimen, speech, spill, spiritualist, spore, spring, starling,\nstarship, stingray, stock, street, sufﬁx, switch, tarsier, terrier,\ntown, treaty, truth, tutor, tweeter, undertaker, uniform, vendor,\nventilator, view, walker, warlock, watcher, youngster\nD.3 Sentential Contexts\nThe 56 sentential contexts used in our stimuli are\nlisted below:\n1. the edwardian semi-detached [SUBJECT] of brantwood\nroad , facing the park [VERB] an art deco style whilst\nthose in ashburnham road include ornate balconies .\n2. for protestant denominations , the [SUBJECT] of marriage\n[VERB] intimate companionship , rearing children and\nmutual support for both husband and wife to fulﬁll their\nlife callings .\n3. the [SUBJECT] , due to being the same colour green as\nthe shield , [VERB] a green sign with a white inlay border\n, and a green outer border .\n4. wright ’s other acting [SUBJECT] on television [VERB]\nitv ’s crossroads and bbc one ’s doctors .\n5. the [SUBJECT] , where most of the population lives and\nthe majority of activity takes place , [VERB] an expanse\nof low-lying , ﬂat , and comparatively dry grassland .\n6. the [SUBJECT] of the load line with the transistor charac-\nteristic curve [VERB] the different values of ic and vce at\ndifferent base currents .\n7. the other [SUBJECT] on the pillar [VERB] only two bolts\nfor sport climbing , making a ground-fall more likely\nshould a mistake be made .\n8. the [SUBJECT] honoring saint joseph , the saint patron of\nthe city , [VERB] a part of the city ’s culture .\n9. furthermore , other scholars have noted how the cryptic\ndharani [SUBJECT] within the lotus sutra [VERB] a form\nof the magadhi dialect that is more similar to pali than\nsanskrit .\n947\n10. he may be a wonderful vandal ﬁghter , but i think the\n[SUBJECT] about this matter at the talk page [VERB] a\nclear misunderstanding of practice , here .\n11. bce ) , although no [SUBJECT] of that period [VERB]\ntoday .\n12. its major american [SUBJECT] in the plastic model kit\nmarket [VERB] amt-ertl , lindberg , and testors .\n13. his feature [SUBJECT] as a screenwriter [VERB] ameri-\ncan hot wax , rafferty and the gold dust twins and where\nthe buffalo roam .\n14. hence , some state [SUBJECT] of assault weapon explic-\nitly [VERB] assault riﬂes .\n15. his other research [SUBJECT] in modern cornish history\n[VERB] cornish emigration ; ethnicity and territorial poli-\ntics and centre-periphery relations .\n16. her [SUBJECT] of interest [VERB] the history of child-\nhood and family , networks , social interactions and reci-\nprocity , poverty , welfare , gift-exchange and the history\nof the emotions .\n17. the [SUBJECT] for approval of an employment visa\n[VERB] suitable educational qualiﬁcations or work experi-\nence , a secured employment contract in moldova , provide\nproof of adequate means of subsistence in moldova , po-\nlice conﬁrmation that you have no criminal record , and a\nsatisfactory medical examination .\n18. the [SUBJECT] of this measure [VERB] not a single rea-\nson to advance why this bill should not pass .\n19. the control [SUBJECT] at the left hand end of the in-\nstrument [VERB] the d6 clavinet mixture controls and a\nsliding control for the volume .\n20. second of all , his [SUBJECT] today [VERB] very high\nprices , placing him well above the notability minimum\nfor artists .\n21. communities bans should be the absolute last resort when\nan editor ’s [SUBJECT] to the project [VERB] a net detri-\nmental effect and lesser sanctions have failed to improve\nthis problem .\n22. the [SUBJECT] of the ﬁrst session of the washington\ncounty court during that year [VERB] a call for a road\nfrom canon ’s mill to pittsburgh .\n23. the anaerobic [SUBJECT] in osteomyelitis associated with\nperipheral vascular disease generally [VERB] the bone\nfrom adjacent soft-tissue ulcers .\n24. i have taken note of michaelqschmidt ’s keep vote , but\nnote that the [SUBJECT] in the second google news link\n[VERB] nothing non-trivial and the ﬁrst shows only brief\nlocal coverage .\n25. the small [SUBJECT] of non-white students in the schools\naccurately [VERB] the racial and ethnic demographics of\nthe community .\n26. however , the [SUBJECT] of cost versus beneﬁt [VERB]\nan area of ongoing research and discussion .\n27. the common [SUBJECT] for dizziness [VERB] vertigo ,\npre-syncope and disequilibrium .\n28. the [SUBJECT] of the increasing lack of physical educa-\ntion [VERB] budgetary pressure which limits the resources\nprovided for this ; the increasing attractiveness of rival pas-\ntimes such as video games ; and the increased emphasis\nupon academic results .\n29. the [SUBJECT] of the former route from drysdale to\nsouth geelong , along with a walking track adjacent to\nthe queenscliff-drysdale line , now [VERB] the bellarine\nrail trail , accessible to cyclists and walkers .\n30. one ’s [SUBJECT] at this level of existence [VERB] a\nconsistency and coherence that they lacked in the previous\nsphere of existence .\n31. the [SUBJECT] in the gulf [VERB] santa catalina island .\n32. the oaths themselves talk about the family bond , and we\ncan conjecture that the [SUBJECT] of secrecy [VERB]\nthe family loyalty as well as a sense of self-preservation .\n33. the [SUBJECT] on death row [VERB] foreign nationals ,\nmany of whom were convicted of drug-related offences .\n34. the [SUBJECT] on current routes [VERB] nothing to the\ninfo .\n35. the [SUBJECT] of the buildings [VERB] commercial\nspace , including two restaurants , a dental ofﬁce ( pinnacle\ndental ) , a medical clinic , and spa , while the surrounding\narea will consist of public parks , shops and recreation\nspaces .\n36. the genetic [SUBJECT] among the viruses isolated from\ndifferent places ( 7-8 ) [VERB] the difﬁculty of developing\nvaccines against it .\n37. the [SUBJECT] of shipwrecks in 1980 [VERB] all ships\nsunk , foundered , grounded , or otherwise lost during\n1980 .\n38. the [SUBJECT] of these techniques to humans [VERB]\nmoral and ethical concerns in the opinion of some , while\nthe advantages of sensible use of selected technologies is\nfavored by others .\n39. under chairwoman agnes gund , the moma ps1 ’s [SUB-\nJECT] of directors [VERB] the artists laurie anderson and\npaul chan , art historian diana widmaier-picasso , fashion\ndesigner adam kimmel , and art collectors richard chang ,\npeter norton , and julia stoschek .\n40. the ﬁrst [SUBJECT] of “ cities of the plain ” [VERB] a\ndetailed account of a sexual encounter between m .\n41. the diocese ’s [SUBJECT] of arms [VERB] a red ﬁeld in\nhonor of the sacred heart of jesus .\n42. the [SUBJECT] from local schools , like west lafayette\njunior-senior high school , also [VERB] high academic\nstandards .\n43. the [SUBJECT] of chaperones [VERB] a long history .\n44. his [SUBJECT] in rabies [VERB] multiple studies inves-\ntigating efﬁcacy and side effects of tissue culture derived\nrabies vaccines , as well as leading clinical trials as primary\ninvestigator in collaboration with the who .\n45. the [SUBJECT] of glaciers on people [VERB] the ﬁelds\nof human geography and anthropology .\n46. the object is to eliminate as many stars as possible before\nthe [SUBJECT] of blocks [VERB] the top of the screen ;\na hand raises up the set of blocks , introducing a new row .\n47. the [SUBJECT] of all preordered sets with monotonic\nfunctions as morphisms [VERB] a category , ord .\n48. the [SUBJECT] of goods and services chosen [VERB]\nchanges in society ’s buying habits .\n49. the [SUBJECT] of binding partners to induce conforma-\ntional changes in proteins [VERB] the construction of\nenormously complex signaling networks .\n50. the [SUBJECT] of university of toledo people [VERB]\nnotable alumni , former students , and faculty of the uni-\nversity of toledo .\n51. romayne ’s ﬁlm [SUBJECT] scoring independent features\nand documentaries [VERB] the screamfest crystal skull\nwinner h .\n948\n52. the [SUBJECT] of basic needs providers emigrating from\nimpoverished countries [VERB] a damaging effect .\n53. the [SUBJECT] of extensive deposits of ﬁshbones associ-\nated with the earliest levels also [VERB] a continuity of\nthe abzu cult associated later with enki and ea .\n54. in addition , the [SUBJECT] of secondary measures\n[VERB] applications in quantum mechanics .\n55. the [SUBJECT] of large quantities [VERB] speciﬁc pre-\ncautions to prevent the release of the vapour into the envi-\nronment .\n56. the [SUBJECT] with the highest votes [VERB] the deputy\nmayor and may proxy for the mayor .\nD.4 Verbs of Interest\nThe 60 verbs of interest (VOI) used in our pre-\ntraining manipulation experiments are listed below:\nemphasize, threaten, announce, utilize, propose, translate, con-\nfront, portray, prefer, declare, denote, admit, conclude, in-\nform, imply, relate, derive, suffer, constitute, employ, possess,\nattract, assume, resemble, depict, demonstrate, incorporate,\ncelebrate, generate, realize, collect, enjoy, deliver, introduce,\nexplore, prepare, depend, recognize, encourage, contribute,\nhear, publish, retain, discuss, enable, prove, spend, comprise,\ndeﬁne, marry, affect, teach, argue, survive, choose, identify,\nlose, vary, raise, reveal",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7227447032928467
    },
    {
      "name": "Plural",
      "score": 0.5894103646278381
    },
    {
      "name": "Verb",
      "score": 0.5868382453918457
    },
    {
      "name": "Transformer",
      "score": 0.5635541081428528
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5452062487602234
    },
    {
      "name": "Inflection",
      "score": 0.5304110646247864
    },
    {
      "name": "Natural language processing",
      "score": 0.5217883586883545
    },
    {
      "name": "Inference",
      "score": 0.5141440033912659
    },
    {
      "name": "Sentence",
      "score": 0.45503732562065125
    },
    {
      "name": "Linguistics",
      "score": 0.2951608896255493
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    }
  ],
  "cited_by": 38
}