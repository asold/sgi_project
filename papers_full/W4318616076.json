{
    "title": "PneuNet: deep learning for COVID-19 pneumonia diagnosis on chest X-ray image analysis using Vision Transformer",
    "url": "https://openalex.org/W4318616076",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Wang, Tianmu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2387657793",
            "name": "Nie Zhen-guo",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2375937008",
            "name": "Wang Ruijing",
            "affiliations": [
                "Stevens Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2125889826",
            "name": "Xu Qingfeng",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2463008510",
            "name": "Huang Hongshi",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": null,
            "name": "Xu, Handing",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2356992618",
            "name": "Xie Fugui",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A5005554057",
            "name": "Liu Xin-jun",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2098196189",
        "https://openalex.org/W4281289704",
        "https://openalex.org/W3008443627",
        "https://openalex.org/W3008985036",
        "https://openalex.org/W3006082171",
        "https://openalex.org/W3001118548",
        "https://openalex.org/W3006110666",
        "https://openalex.org/W2901030517",
        "https://openalex.org/W4213451779",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W2161969291",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3106027361",
        "https://openalex.org/W3168404669",
        "https://openalex.org/W3122727436",
        "https://openalex.org/W4285685325",
        "https://openalex.org/W4296197501",
        "https://openalex.org/W4281760686",
        "https://openalex.org/W6600120041",
        "https://openalex.org/W3013507463",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W3004227146",
        "https://openalex.org/W3013601031",
        "https://openalex.org/W3017855299",
        "https://openalex.org/W3033616466",
        "https://openalex.org/W3158118099",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2220981600",
        "https://openalex.org/W3174349641",
        "https://openalex.org/W4206571954",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W3099183222",
        "https://openalex.org/W6600045627",
        "https://openalex.org/W2475694846",
        "https://openalex.org/W4283737877",
        "https://openalex.org/W3211983116",
        "https://openalex.org/W3103635657",
        "https://openalex.org/W3162351260",
        "https://openalex.org/W2912664121",
        "https://openalex.org/W3155049403",
        "https://openalex.org/W3105081694"
    ],
    "abstract": null,
    "full_text": "/ Published online: 31 January 2023\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nhttps://doi.org/10.1007/s11517-022-02746-2\nORIGINALARTICLE\nPneuNet:deeplearningforCOVID-19pneumoniadiagnosis\nonchestX-rayimageanalysisusingVisionTransformer\nTianmuWang1,2,3 · ZhenguoNie1,2,3 · RuijingWang4 · QingfengXu1,5 · HongshiHuang6 · HandingXu1,2,3 ·\nFuguiXie1,2,3 · Xin-JunLiu1,2,3\nReceived:23June2022/Accepted:22December2022\n© InternationalFederationforMedicalandBiologicalEngineering2023\nAbstract\nA long-standing challenge in pneumonia diagnosis is recognizing the pathological lung texture, especially the ground-\nglass appearance pathological texture. One main difficulty lies in precisely extracting and recognizing the pathological\nfeatures. The patients, especially those with mild symptoms, show very little difference in lung texture, neither conventional\ncomputer vision methods nor convolutional neural networks perform well on pneumonia diagnosis based on chest X-ray\n(CXR) images. In the meanwhile, the Coronavirus Disease 2019 (COVID-19) pandemic continues wreaking havoc around\nthe world, where quick and accurate diagnosis backed by CXR images is in high demand. Rather than simply recognizing\nthe patterns, extracting feature maps from the original CXR image is what we need in the classification process. Thus, we\npropose a Vision Transformer (VIT)–based model called PneuNet to make an accurate diagnosis backed by channel-based\nattention through X-ray images of the lung, where multi-head attention is applied on channel patches rather than feature\npatches. The techniques presented in this paper are oriented toward the medical application of deep neural networks and\nVIT. Extensive experiment results show that our method can reach 94.96% accuracy in the three-categories classification\nproblem on the test set, which outperforms previous deep learning models.\nKeywords Deep learning · Pneumonia diagnosis · COVID-19 · Vision Transformer · Multi-head attention\n1Introduction\nAccording to the World Health Organization (WHO),\npneumonia accounts for 14% of all deaths of children under\n5 years old and is blamed as one main murderer of children\n[1]. Pneumonia can be divided into two categories which\nare bacterial pneumonia and viral pneumonia. In the past\ndecades, the viral pneumonia, SARS, for example, has not\nonly murdered children but also claimed the lives of people\nof all ages, especially those who are suffering from chronic\ndisease [ 2]. In 2019, a novel coronavirus called COVID-\n19 was first reported in Wuhan, China, in December 2019.\nThe plague has continued to have a devastating effect on\nglobal health and has caused over 6 million death cases all\nover the world, out of over 611 million infected people,\n/envelopebackZhenguo Nie\nzhenguonie@tsinghua.edu.cn\nExtended author information available on the last page of the article.\naccording to Johns Hopkins University [3]. Patients infected\nby COVID-19 share similar symptoms as usual pneumonia.\nThe ignorance of such similar symptoms leads to the rapid\nspreading of this lethal virus and has become one of the\nleading causes of this global pandemic.\nDuring the global pandemic caused by COVID-19,\nisolation has been proven to be the most effective method\nto control the spreading once an accurate diagnosis is\nconducted. RT-PCR test and antibody test have become two\nwildly used solutions to make a quick diagnosis. However,\nthe sensitivity of RT-PCR is still under debate. It is reported\nthat there is a 3% false-negative rate exists in the RT-\nPCR test [ 4]. In the meanwhile, the result of the antibody\ntest cannot be convincing if the suspected patient gets\ninfected in the first 7 days. An alternative choice to make\nthe diagnosis is based on the evaluation of radiographic\nimages of the lung, such as chest X-ray (CXR) images and\ncomputerized tomography (CT) images. Previous research\nhas concluded that pulmonary manifestation of COVID-\n19 infection is predominantly characterized by ground-\nglass opacification with occasional consolidation [ 5, 6].\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nResearchers are confident that preliminary screening can be\napplied to suspected cases based on the evaluation of CXR\nimages [7].\nPreliminary screening based on CXR images has the\nadvantage over other screening methods not only from\nthe perspective of precision but also from its availability\nand efficiency. CXR imaging has been counted as part of\nthe primary health care system and is readily available in\ncommunity clinics. A large batch of CXR images can be\nevaluated at the same time with the help of computer vision\nmethods and artificial neural networks [8].\nAn accurate classification can be conducted by convo-\nlutional neural networks (CNN) with the help of spatial\nfeature extraction and pattern recognition. However, con-\nventional CNN and CV methods do not care about the\nnumber of spatial features responsible for classification. In\nthis paper, we propose a multi-head attention-based network\ncalled PneuNet, inspired by Vision Transformer (VIT), to\nconduct a diagnosis of COVID-19. We treat each chan-\nnel after convolution calculation as one patch, and then the\ntransformer module is applied to evaluate the contribution\nof each patched channel in classification. Unlike paying\nattention to the primary feature block of images, we pay\nmore attention to the global extracted spatial features, which\ncould be more efficient in the classification process. Mean-\nwhile, attention applied to convoluted channels has practical\nimplications, where each channel theoretically represents\na particular feature from the raw image on the scale of\na higher dimension. An extensive experiment shows the\nproposed model can reach 94.96% test accuracy in three\ncategory classifications, where none pneumonia, normal\npneumonia, and COVID-19 are classified. Our model also\nreaches 99.30% accuracy when applied to binary classifica-\ntion, which is used to detect if pneumonia is caused by a\ncoronavirus.\nThe full implementation and trained network are pub-\nlished on https://github.com/TianmuWang/PneuNet.O u r\nmain contributions are as follows:\n1. We propose a novel deep learning method, which jointly\nemploys ResNet18 and multi-head attention to make the\ndiagnosis of COVID-19 based on CXR images.\n2. We treat the extracted features as patches, and apply\nchannel-based attention to implement the transformer\nencoder after the application of ResNet18.\n3. We evaluate the model not only from the perspective\nof prediction accuracy but also backed by statistical\ncriteria. In this paper, the prediction based on up to\nfour-category classification is evaluated, corresponding\nto diagnosing COVID-19 from COVID-19, none\npneumonia, bacterial pneumonia, and viral pneumonia.\n2Relatedwork\nInspired by the rapid development of image recognition\nand classification backed by artificial intelligence, many\nintelligent pneumonia diagnosis methods have been pro-\nposed [9], which are based on making classification among\nradiography images backed by deep learning methods. The\nprediction results of the above models are proven to be con-\nvincing from the aspect of prediction accuracy. Our review\nhighlights popular deep learning models on image classifi-\ncation and their applications in pneumonia diagnosis from\nradiography images.\nThe deep learning method is of a broader family of\nmachine learning methods based on artificial neural net-\nworks with representation learning, inspired by the human\nbrain’s structure and function [10, 11]. Convolutional neural\nnetwork (CNN) is one of the most effective deep learn-\ning methods dealing with image classification resulting\nfrom extracting spatial features through convolution calcu-\nlation [12]. With the help of deeper network layers and its\nmore complicated structure, CNN outperforms conventional\ncomputer vision methods such as Generalized Search Tree\n(GIST) [ 13] and Histogram of Oriented Gradient (HOG)\n[14]. The art of deep learning methods is the generalization\ncapability brought from deeper hidden layers [ 15]a n dt h e\ngradient descent methods through backpropagation. He et\nal. [16] propose ResNet, where residual blocks are used to\nprevent potential gradient vanishing and gradient exploding\nduring the training of a deep CNN model. Due to the out-\nstanding capacity of extracting spatial features, the ResNet\nfamily is commonly used in the field of pattern recognition.\nMotivated by CNN and ResNet models, much research\non pneumonia diagnosis focuses on radiography image\nclassification based on CNN models [17–22]. A lightweight\nCNN-based model proposed by Bhosale et al. reaches high\nprediction accuracy under RaspberryPi [ 23]. Zhang et al.\n[24] apply ResNet18 to COVID-19 diagnosis and reach a\n95.18% accuracy on binary classification. Hemdan et al.\n[25] make a fine tune on ResNet50 and rename it COVIDX-\nNet. Narin et al. [26] compare ResNet18, InceptionV3 [27],\nand Inception-ResNetV2 szegedy2017inception based on a\nsmall-scale pneumonia datasets. Wang et al. [ 28] propose\na deep CNN-based model called COVID-Net, where CNN\nlayers from different depths are tailored and obtain 83.5%\naccuracy in classifying COVID-19, normal, pneumonia-\nbacterial, and pneumonia-viral classes. Apostolopoulos and\nMpesiana [29] apply transfer learning based on pre-trained\nVGG19 models and obtain the best accuracy of 98.75% and\n93.48% for two and three classes, respectively. Ozturk et\nal. [30] propose DarkCovidNet based on CNN layers with\nLeakyReLU as an activation function and obtain 98.08%\n1396\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nFig.1 Architecture of PneuNet\n(a) and the details of\nTransformer Encoder (b)\nand 87.02% accuracy for binary classification and three-\ncategory classification, respectively. CoroNet proposed by\nKhan et al. [ 31] evolves the model from the Xception\nstructure and reaches a 92% accuracy on multi-category\nclassification for pneumonia diagnosis. Shazia et al. [ 32]\nanalyze VGG, ResNet, DenseNet101, and Inception model\nand compare their performance in diagnosing COVID-\n19, where DenseNet101 and ResNet51 reveal the latent\ntransfer ability to make an accurate prediction of COVID-\n19 diagnosis. Considering that we want to find a lightweight\nmodel available in suburban and undeveloped areas, we\nneed to compress the scale of the model. Therefore,\nwe consider employing ResNet18, a lightweight standard\nResNet model, to first extract useful spatial features from\nraw images.\nRecurrent neural networks (RNN) is another wildly used\ndeep learning method [ 33]. Unlike CNN models, RNN\nmodels consider the context within the message and are\ninitially developed to solve natural language processing\n(NLP) questions such as voice recognition and translation.\nHowever, RNN has been developed and is ready to work\non pattern recognition and object detection in computer\nvision. Long short-term memory (LSTM) is one typical\napplication of RNN networks to detect the object in an\nimage and shows great potential in medical diagnosing [34–\n36]. Mousavi et al. [37] combine CNN and LSTM and make\nan analysis based on seven binary categorical classifications\namong COVID-19, viral pneumonia, bacterial pneumonia,\nand healthy people.\nHowever, both CNN and LSTM are limited in recog-\nnizing a pattern on a dynamic and large scale. The size of\nthe kernel blocks the receptive field of CNN, and LSTM\ncan only realize the context in a narrow range. Apart from\nCNN and LSTM, transformer [ 38] has become a hot topic\nin computer vision, where attention modular is employed to\nanalyze the hot spot region of each image and then make\na classification based on it. Compared with LSTM, Trans-\nformer is much more accessible in parallel computation\nand is powerful in global context reconstruction. The trans-\nformer is firstly applied in an NLP problem where several\npowerful general-purpose models are introduced, such as\nBERT [39]a n dG P T - 2[40], where multi-head attention is\nproposed that allows the model to jointly attend to infor-\nmation from different representation subspaces at different\npositions. Whereas the transformer is initially proposed to\nsolve NLP problems, and it has been transferred to solve\ncomputer vision problems. Although the transformer struc-\nture is suitable for image recognition and classification of\n1397\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nobjects, the CNN module still plays an important role in\nclassification [41]. Dosovitskiy et al. [42] bring up the con-\nception of Vision in Transformer (VIT), where multi-head\nattention can be applied to small patches, which are divided\nfrom original images. Inspired by the combination of CNN\nmodels and transformer structure, lots of research have been\ncarried out. Sitaula et al. [ 43] propose an attention-based\nVGG-16 model and get 79.58% accuracy on multi-class\npneumonia diagnosis. Zhang et al. combine the swin trans-\nformer block with U-Net together and got a maximum F-1\nscore of 0.935 based on training on 1560 CT scans. How-\never, precise prediction accuracy is not mentioned. Park\net al. [ 44] introduce a probabilistic-CAM (PCAM) pool-\ning backbone network before applying the transformer and\nobtain an AUC score of 0.941 for three-category classi-\nfication based on CXR images. The transformer can be\njointly used with basic CNN models, outperforms sim-\nple CNN-based models, and follows an interesting internal\nlogic similar to human cognition during the training pro-\ncess. Consequently, we combine transformer and ResNet in\nour proposed PneuNet model to help diagnose COVID-19\nbased on CXR images.\n3Technicalapproach\nOur proposed PneuNet is backed by ResNet18 and VIT\nmodels. As the previous study does, ResNet18 works as the\nbackbone of the whole model, extracting spatial features\nwith the help of deep convolutional layers. However, unlike\nother VIT models, we keep the extracted features from\nsplitting them into patches but take the whole channel,\ndownsized from deep convolution calculation and max\npooling, as one individual patch. After being encoded and\nembedded, patches will pass multi-head attention layers\nbefore the final classification process with the help of three\nfully connected layers, which can also be called multi-layer\nperceptron (MLP). Figure 1 illustrates the overall structure\nof our proposed model. The detailed architecture of our\nproposed model is presented in Table 1.\n3.1ApplicationofResNet18\nResNet18 is first proposed by He et al. [ 16] and has been\nproven to have excellent performance on spatial feature\nextraction, which is a series of deep neural networks that are\nderived from the base repeated building blocks. ResNet18\ncontains four kinds of Residual Blocks, and each Residual\nblock is repeated twice, as shown in Fig. 2. The entire\narchitecture is shown in Table 2. Compared with other\ndeep convolutional networks such as VGG16, ResNet18\ncan prevent the gradient vanishing and exploding during\nTable 1 Detailed architecture of PneuNet\nLayer (submodel) Output\nshape\nNumber of\nparameter\nInput Layer 22 × 224 × 10\nResNet18 7 × 7 × 512 11184640\nBatch Normalization 7 × 7 × 512 2048\nEmbedding&Encoder 512 × 80 44960\nTransformer 1 512 × 80 439680\nTransformer 2 512 × 80 439680\nTransformer 3 512 × 80 439680\nTransformer 4 512 × 80 439680\nTransformer 5 512 × 80 439680\nTransformer 6 512 × 80 439680\nLayer normalization 512 × 80 160\nFlatten 40960 0\nDropout 40960 0\nDense 1024 41944064\nDropout 1024 0\nDense 64 65600\nDropout 64 0\nDense 16 1040\nDropout 16 0\nLogitDense 3 51\nthe backpropagation process. In the meanwhile, ResNet18\nrequires much less computational capacity compared with\nResNet51 and ResNet101 and can be easily trained on a\nlocal PC. ResNet18 can extract 512 channels of different\nspatial features which will be thought of as patches. At\nthe end of ResNet18, an additional max-pooling layer is\nused to downsample the extracted spatial features better.\nConsidering the input of neural networks is a batch of\ngray-scale images, of which the shape can be defined as\nIn put∈ Rh×w×1 (1)\nwhere h denotes the height of the input image andw denotes\nthe width. The output of this submodule can be described as\nOutputResNet 18 ∈ R28×28×512 (2)\n3.2Applicationoftransformer\nIn conventional VIT models, patches are generated from\nraw images, where images are divided into pieces in each\nchannel, as shown in Fig. 3. However, in our proposed\nPneuNet, we think of each channel of extracted spatial\nfeatures entirely as one patch, shown in Fig. 4. Convolution\nis an efficient way to extract spatial features from multiple\ndimensions but lacks the capacity to tell how important\nthe feature stands for during the classification process.\n1398\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nFig.2 Architecture of\nResNet18, without flatten layer\nnor logit layer\nHowever, the transformer can evaluate how much one\nfeature patch contributes to classifying with the help of\nmulti-head attention. The number of heads represents the\nnumber of subspace that allows the model to focus jointly\non information from different positions. In our proposed\nPneuNet, we employ four transformer layers with four-head\nattention. Each two-dimensional patch is embedded into a\none-dimensional vector with a length of 80.\nThe intermediate output of transformer modular can be\ndescribed as\nOutputinter = Ti(head, OutputResNet 18) ∈ R512×p (3)\nTable 2 Detailed architecture of ResNet18\nLayer (submodel) Output shape\nConv2D 112 × 112 × 64\nMax Pooling 56 × 56 × 64\nBuilding Block 15 6 × 56 × 64\nBuilding Block 22 8 × 28 × 64\nBuilding Block 31 4 × 14 × 64\nBuilding Block 47 × 7 × 64\nTotal parameters 11184640\nwhere head denotes the number of heads applied in\nattention modular, p denotes the number of projection\ndimensions, and Outputinter denotes the intermediate\noutput of transformer modular before an extra flatten\nlayer to map higher-dimensional features into a one-\ndimensional vector, counted as Outputtransformer ,w h i c h\ncan be indicated as\nOutputTran s fo rm e r= Ttr (Outputinter ) ∈ RN (4)\nN = 512 × p (5)\nwhere N denotes the dimension of the flattened vector.\nIn PneuNet, multiple Transformers are employed in model\narchitecture. The least repeating unit of the Transformer\nsubmodel is shown in Table 3.\n3.3Applicationofmulti-layerperceptron\nMulti-layer perceptron consists of several fully connected\nlayers, which are also called Dense layers. Particularly in\nour PneuNet. MLP section consists of three Dense layers,\nwhere ReLU is used as the activation function and a 20%\ndropout is applied after each Dense layer.\n1399\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nFig.3 Image partition in\nconventional VIT process: raw\nimage (a) is divided into several\npatches (b) and each patch will\nbe encoded and embedded with\nits position\n3.4Applicationoflogitclassiﬁcationlayer\nA particular fully connected layer can be treated as a\nlogit layer where Softmax acts as an activation function.\nSoftmax is a wildly used logit function that maps the\nmultinomial distribution of the probability score to a vector.\nThe length of the vector equals the number of categories\nto be classified. The output of this distribution can be\ndescribed as\nP(y) = eWy\n∑ C\ny=1 eWy\n(6)\nwhere y denotes yth category, C denotes the number of\ncategories, and Wy denotes the intermediate weight for yth\ncategory calculated from the Dense layer.\n4Experimentssetup\n4.1Dataset\nIn this study, CXR images from seven online public\nrepositories are assembled as our datasets. COVID-19 CXR\nimages collected from [ 45–51], normal pneumonia CXR\nimages collected from [52–54], and CXR images of healthy\npeople from [ 52–54] have been divided into three sub-\ndatasets out of total 33920 CXR images. We divided our\ndatasets into three categories which are used for training and\nvalidation during training, and for testing the generalization\nperformance of the well-trained model. The dataset is\ndivided in a ratio of 64:16:20. Details of our dataset are\ns h o w ni nT a b l e4. However, most online datasets except\n[48] do not distinguish bacterial and viral pneumonia out\nof normal pneumonia. When we generate another dataset\nFig.4 Illustration of patches\nemployed in Transformer\nmodular which obtained from\nResNet18\n1400\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nTable 3 Architecture of Transformer submodel\nLayer (submodel) Output\nshape\nNumber of\nparameter\nLayer Normalization 512 × 80 160\nMulti-head Attention 512 × 80 413520\nAdd 512 × 80 0\nLayer Normalization 512 × 80 160\nDense 512 × 160 12960\nDropout 512 × 160 0\nDense 512 × 80 12880\nDropout 512 × 80 0\nAdd 512 × 80 0\nwith four categories, we employ a down-sampling method\nto collect COVID-19 CXR images and healthy CXR images\nfrom the latter datasets to balance this four-categorical\ndataset. Details of this dataset are shown in Table 5.S o m e\ntypical CXR images from four categories are shown in\nFig. 5. In this study, to realize the intelligent screening\nof COVID-19 patients, we focus on the classification of\nthree categories, which are none pneumonia, COVID-19,\nand normal pneumonia. Discussion of the four-category\nclassification from none pneumonia, COVID-19, bacterial\npneumonia, and viral pneumonia is also mentioned in the\nfollowing sections.\n4.2Evaluationmetrics\nCross-entropy (CE) acts as a loss function and plays an\nessential role in training a deep neural network during the\nbackpropagation process. It can be described as\nCE = 1\nN\n∑\ni\nM∑\nc=1\nyic log(Pic) (7)\nwhere N denotes dimension, M denotes the number of\ncategories, and yic is a symbolic function whereyic equals 1\nif the sample i and the sample c belong to the same category\nbased on ground truth, otherwise equals 0. And Pic denotes\npredicted probability where sample i belongs to category c.\nFor a better description of the effectiveness of the\nproposed PneuNet model, five statistical evaluation criteria\nare used to evaluate the performance of the PneuNet model,\nTable 4 Details of three-categorical dataset\nImage category Training Validation Test Amount\nCOVID-19 7658 1903 2395 11956\nNormal Pneumonia 7208 1802 2253 11263\nNone Pneumonia 6849 1712 2140 10701\nTable 5 Details of four-categorical dataset\nImage category Training Validation Test Amount\nCOVID-19 1245 120 120 1485\nBacterial Pneumonia 1245 120 120 1485\nViral Pneumonia 1225 120 120 1465\nNone Pneumonia 1245 120 120 1485\nwhich are Prediction Accuracy, Recall rate, Precision, and\nF-1 Score. These criteria are defined as follows:\nAccuracy = TP + TN\nTP + FP + TN + FN (8)\nRecall = TP\nTP + FN (9)\nPrecision = TP\nTP + FP (10)\nF − 1Score = 2 ∗ Precision ∗ Recall\nPrecision + Recall (11)\nwhere TP is short for True Positive, TN for Ture\nNegative, FP for False Positive, and FN for False\nNegative. TP , TN , FP ,a n d FN can be obtained from\nthe confusion matrix, which is a square matrix containing\nthe predictionary label and the ground truth. Additionally,\nreceiver operating characteristic (ROC) curve is another\nwidely used metric to describe the performance of a\nclassifier, where the false-positive rate (FPR) and true-\npositive rate (TPR) is used as the horizontal and vertical\naxis, respectively. In statics, true-positive rate (TPR) is\ndefined as the probability of a positive response when the\ncorrect answer is positive, and false-positive rate (FPR) is\ndefined as the probability of a positive response when the\ncorrect answer is negative [55]:\nFPR = FP\nFP + TN (12)\nTPR = TP\nTP + FN (13)\n4.3Implementation\nIn all the tested models, AdamW [ 56]i su s e df o r\noptimization, which performs better in preventing gradient\nvanishing during training compared with jointly using\nAdam and L2 regularization. A mini-batch with a size of\n16 is used in both the training and test process, where the\nmaximum training epoch is set as 100. Considering that the\ndistilled spatial features in each channel act as patches in the\nprocess of transformer encoder, we trained ResNet18 locally\nto get optimal parameters of kernels. Data augmentation\nis applied before training. The detailed parameters, which\ninclude basic settings for training and data augmentation,\n1401\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nFig.5 Typical chest X-ray\nimages from the combined\ndataset: CXR image of None\nPneumonia (a), CXR images of\nCOVID-19 (b), CXR images of\nBacterial Pneumonia (c), and\nCXR images of Viral\nPneumonia (d)\nare listed in Table 6. We initialize the hyperparameters with\nrecommended values from previous work [16, 38, 42, 56].\n5Resultsanddiscussion\nAll the experiments in this paper were implemented\nin Python using Keras with TensorFlow running on an\nNVIDIA GeForce RTX 3090 GPU. The training history\nof our model is shown in Fig. 6, where cross-entropy\nTable 6 Parameters during training and data augmentation\nName of parameter Value\nImage size 224 × 224 × 1\nBatch size 16\nOptimizer AdamW\nLearning rate 0.0001\nWeight decay 0.00001\nDropout rate 0.2\nLoss Categorical cross-entropy\nZoom range 0.1\nRotation range 0.1\nWidth-shift range 0.1\nHeight-shift range 0.1\nand categorical accuracy are used as a criterion to help\nmake early stopping in case of overfitting. The model is\ntrained with 300 epochs. Both validation loss and validation\naccuracy seem to begin to converge during the training\nprocess. There is a significant increase in accuracy values\nat the beginning of the training within 50 epochs. Whereas\ntraining loss presents a significant trend in decreasing,\nthe validation loss does not decrease obviously after 200\nepochs. We should early stop the training process before 200\nepochs in case of overfitting.\n5.1Modelevaluation\nThe aforementioned statistical metrics are the top met-\nrics used to measure the performance of classification\nalgorithms. Our proposed PneuNet obtained a prediction\naccuracy of 95.16%, whereas the precision, recall, and F1-\nScore for class COVID-19 are 96.95%, 98.45%, and 97.69%\nrespectively. The class-wise performance of PneuNet is pre-\nsented in Table 7, which is generated through the confusion\nmatrix shown in Fig. 7.\nWe compare the proposed PneuNet with some other\ndeep learning methods from previous work [ 28, 30, 31,\n43] based on the three-category classification using the\nsame dataset. Table 8 shows the comparison between our\nproposed PneuNet and other deep learning, where the\n1402\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nFig.6 History during training: history of cross-entropy loss (a) and history of categorical accuracy (b)\naforementioned statistical metrics are used. Our proposed\nPneuNet outperforms other models in all aspects and has\na significant increase in overall precision, from 93.55%\n(Wang et al. [28], COVID-Net) to 97.11%.\nCoroNet [ 31] and COVID-Net [ 31] are conventional\ndeep neural networks based on convolution layers and\nresidual connections. However, the model from Siltuala et\nal. [43] is a VGG-16 model concatenating with traditional\nVIT modular, and DarkCovidNet [ 30] takes a heat map\nof the original CXR images into account inspired by self-\nattention. Apart from our proposed model, all CNN-based\nmodels perform better than transform models on the same\ndataset. It could be derived that simply cropping the raw\nimage is weak in extracting the latent spatial features,\nespecially when the texture of the lung does not present\nan apparent difference between that from CXR images of\nCOVID-19 and of other pneumonia. However, latent spatial\nfeatures in high dimensions could be extracted from CNN\nmethods, allowing the transformer encoder to perform a\nbetter prediction.\nTable 7 Statistical performance of PneuNet on three-category\nclassification\nClass Precision (%) Recall (%) F-1 Score (%)\nCOVID-19 96.95 98.45 97.69\nNone Pneumonia 96.64 97.35 96.99\nPneumonia 97.74 96.37 97.10\nAverage 97.11 97.39 97.26\nPrediction Accuracy 95.16%\n5.2Modelperformanceunderothercircumstances\nWe also test the performance of the proposed PneuNet\non binary classification and four-category classification to\ndetect its robustness. The binary dataset is generated from\nthe original dataset by deleting CXR images of normal\npneumonia from both the training set and validation set,\nwhereas the four-category dataset is generated from the\noriginal dataset by relabeling bacterial pneumonia against\nviral pneumonia from normal pneumonia. The confusion\nmatrices are shown in Fig. 8 and the aforementioned\nstatistical metrics are shown in Table 9.\nFig. 7 The confusion matrix for three-category classification: None\nPneumonia, Normal Pneumonia, and COVID-19\n1403\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nTable 8 Comparison among\nproposed PneuNet and other\ndeep learning methods\nModel Precision (%) Recall (%) F-1 Score (%) Accuracy (%)\nSiltuala et al. [43] Attention-based VGG-16 85.61 80.10 82.77 81.36\nOzturk et al. [30] DarkCovidNet 89.96 85.35 87.59 87.02\nKhan et al. [31] CoroNet 91.85 94.63 93.22 91.30\nWang et al. [28] COVID-Net 93.55 93.33 93.44 93.33\nProposed PneuNet 97.11 97.39 97.26 95.16\nFig.8 Confusion matrix\ngenerated from binary\nclassification model (a) and\nfour-category classification\nmodel (b)\nTable 9 Statistical\nperformance of PneuNet Class Precision (%) Recall (%) F-1 Score (%)\nBinary classification\nCOVID-19 98.87 99.00 98.93\nNone Pneumonia 99.00 98.67 98.83\nAverage 98.94 98.84 98.88\nPrediction accuracy 99.32%\nFour-category classification\nCOVID-19 94.17 95.76 94.96\nNone Pneumonia 90.83 89.34 90.07\nBacterial Pneumonia 85.83 88.03 86.92\nViral Pneumonia 87.50 85.37 86.42\nAverage 89.58 89.62 89.59\nPrediction accuracy 90.03%\n1404\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nTable 10 Comparison of\nstatistical performance among\nPneuNet and other deep\nlearning models\nModel Precision (%) Recall (%) F-1 Score (%) Accuracy (%)\nBinary classification\nRarin et al. [26] InceptionV3 82.4 100 90.3 97.7\nOzturk et al. [30] DarkCovidNet 98.04 95.13 96.56 98.08\nKhan et al. [31] CoroNet 98.33 99.31 98.81 99.01\nProposed PneuNet 98.94 98.84 98.88 99.32\nFour-category classification\nSiltuala et al. [43] Attention-based VGG-16 87.48 96.31 86.89 86.15\nOzturk et al. [30] DarkCovidNet 89.96 85.35 87.59 87.02\nKhan et al. [31] CoroNet 87.61 87.82 87.71 87.36\nProposed PneuNet 89.58 89.62 89.59 90.03\nWe compare the prediction performance with other\nrelated works [ 26, 30, 31, 43] ,a ss h o w ni nT a b l e10\nfor binary classification and four-category classification.\nOur proposed PneuNet performed better than the above\nmodels with a prediction accuracy of 99.32%, where the\nprecision, recall, and F1-Score for class COVID-19 are\n98.94%, 98.84%, and 98.88% respectively when detecting\nCOVID-19 out of none pneumonia.\n5.3Futurework\nPneuNet exhibits good performance compared to other deep\nlearning methods, but there are still several limitations,\nespecially when dealing with multi-category classification\nproblems compared with some CNN-based models such\nas LDC-NET [ 57]. This can be caused by a couple of\nreasons. Firstly, compared with computerized tomography\n(CT) images, CXR images only contain planar texture\nfrom one fixed angle of view, having lost plenty of\nlung texture such as that on the coronal plane. In the\nmeanwhile, our proposed PneuNet used ResNet18 for\nfeature extraction but is ready to use other deeper CNN\nencoders, such as ResNet51, to extract latent patterns much\ndeeper in higher dimensions. Future directions thus include\naugmenting the dataset and applying a deeper CNN encoder\ndealing with complex input such as CT images, as well\nas extending the application of our proposed PneuNet,\nsuch as predicting how severe the patient is and predicting\ndates before patients are cured, which could be useful\nfor proper and efficient allocation of medical resources.\nFinally, like most other deep learning models, our proposed\nPneuNet is a black box model. The feature extraction\nprocess, especially the channel-wise transformer encoder, is\nnonrepresentational and complicated. So far, we still cannot\nfind a proper method to make better the model interpretable.\nThis is another important direction for our follow-up\nresearch.\n6Conclusion\nQuick and efficient diagnosis in the early stage is critical\nduring such a tough time caused by the raging plague.\nInspired by high diagnostic demand but limited medical\nresources, we propose a deep learning method named\nPneuNet, which is based on ResNet18 and applied to extract\nspatial features, to detect COVID-19 cases from the chest\nX-ray images. The proposed PneuNet is evaluated on a\ncombined CXR dataset and reached a 95.13% prediction\naccuracy as well as a 95.16% precision, which is state of the\nart over other deep learning methods. The model performed\nwell in binary classification and obtained a 99.29% training\naccuracy as well as a 98.79 precision when distinguishing\nCOVID-19 against none pneumonia. PneuNet also obtained\na promising prediction accuracy (86.94%) on four-category\nclassification among COVID-19, none pneumonia, bacterial\npneumonia, and viral pneumonia, revealing its latent\ncapacity in the diagnosis of more kinds of pneumonia based\non CXR images. The performance of our proposed PneuNet\ncould get improved with the extension of the dataset in\nthe future. From the comparison with other deep learning\nmethods, it is convincing that channel-based attention has\ngreat potential in the field of feature recognition and image\nclassification.\nDespite the convincing prediction result obtained from\nPneuNet, the model still needs clinical study and testing\nbut reveals great potential in quick remote diagnosis of\nsuspected COVID-19 patients on a large scale.\n1405\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nFunding This work is supported by the Tsinghua Precision Medicine\nFoundation under Grant 2022TS009.\nDeclarations\nConﬂictofInterest The authors declare no competing interests.\nReferences\n1. Rudan I, Boschi-Pinto C, Biloglav Z, Mulholland K, Campbell H\n(2008) Epidemiology and etiology of childhood pneumonia. Bull\nWorld Health Organ 86:408–416B\n2. Loo WK, Hasikin K, Suhaimi A, Yee PL, Teo K, Xia K, Qian P,\nJiang Y, Zhang Y, Dhanalakshmi S et al (2022) Systematic review\non COVID-19 readmission and risk factors: future of machine\nlearning in COVID-19 readmission studies. Front Public Health,\n1311\n3. Dong E, Du H, Gardner L (2020) An interactive web-based\ndashboard to track COVID-19 in real time. Lancet Infect Dis\n20(5):533–534\n4. Fang Y, Zhang H, Xie J, Lin M, Ying L, Pang P, Ji W (2020)\nSensitivity of chest CT for COVID-19: comparison to RT-PCR.\nRadiology 296(2):E115–E117\n5. Ng M-Y, Lee EYP, Yang J, Yang F, Li X, Wang H, Lui MM-S,\nLo CS-Y, Leung B, Khong P-L et al (2020) Imaging profile of\nthe COVID-19 infection: radiologic findings and literature review.\nRadiol Cardiothorac Imaging 2(1):e200034\n6. Huang C, Wang Y, Li X, Ren L, Zhao J, Hu Y, Zhang L, Fan G, Xu\nJ, Gu X et al (2020) Clinical features of patients infected with 2019\nnovel coronavirus in Wuhan, China. Lancet 395(10223):497–506\n7. Xie X, Zhong Z, Zhao W, Zheng C, Wang F, Liu J (2020) Chest\nCT for typical coronavirus disease 2019 (COVID-19) pneumonia:\nrelationship to negative RT-PCR testing. Radiology 296(2):E41–\nE45\n8. Salehinejad H, Colak E, Dowdell T, Barfett J, Valaee S (2018)\nSynthesizing chest X-ray pathology for training deep convolu-\ntional neural networks. IEEE Trans Med Imaging 38(5):1197–\n1206\n9. Vineth Ligi S, Kundu SS, Kumar R, Narayanamoorthi R, Lai KW,\nDhanalakshmi S (2022) Radiological analysis of COVID-19 using\ncomputational intelligence: a broad gauge study. J Healthc Eng,\n2022\n10. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature\n521(7553):436–444\n11. Bengio Y, Courville A, Vincent P (2013) Representation learning:\na review and new perspectives. IEEE Trans Pattern Anal Mach\nIntell 35(8):1798–1828\n12. LeCun Y, Bengio Y et al (1995) Convolutional networks for\nimages, speech, and time series. Handb Brain Theory Neural Netw\n3361(10):1995\n13. Hellerstein JM, Naughton JF, Pfeffer A (1995) Generalized search\ntrees for database systems\n14. Dalal N, Triggs B (2005) Histograms of oriented gradients for\nhuman detection. In: 2005 IEEE computer society conference on\ncomputer vision and pattern recognition (CVPR’05), vol 1. IEEE,\npp 886–893\n15. Yosinski J, Clune J, Bengio Y, Lipson H (2014) How transferable\nare features in deep neural networks? Adv Neural Inf Process Syst,\n27\n16. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for\nimage recognition. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp 770–778\n17. Nayak SR, Nayak DR, Sinha U, Arora V, Pachori RB (2021)\nApplication of deep learning techniques for detection of COVID-\n19 cases using chest X-ray images: a comprehensive study.\nBiomed Sig Process Control 64(102365):1–12\n18. Shwab C, Drn D, Dsg E, Xin ZF, Ydzb G (2021) COVID-\n19 classification by CCSHNet with deep fusion using transfer\nlearning and discriminant correlation analysis. Inf Fusion 68:131–\n148\n19. Serena Low WC, Chuah JH, Tee CATH, Anis S, Shoaib MA,\nFaisal A, Khalil A, Lai KW (2021) An overview of deep learning\ntechniques on chest X-ray and CT scan identification of COVID-\n19. Comput Math Methods Med, 2021\n20. Sheykhivand S, Mousavi Z, Mojtahedi S, Rezaii TY, Farzamnia\nA, Meshgini S, Saad I (2021) Developing an efficient deep neural\nnetwork for automatic detection of COVID-19 using chest X-ray\nimages. Alex Eng J 60(3):2885–2903\n21. Woan Ching SL, Lai KW, Chuah JH, Hasikin K, Khalil A, Qian\nP, Xia K, Jiang Y, Zhang Y, Dhanalakshmi S (2022) Multiclass\nconvolution neural network for classification of COVID-19 CT\nimages. Comput Intell Neurosci, 2022\n22. Bhosale YH, Patnaik KS (2022) Application of deep learning\ntechniques in diagnosis of COVID-19 (coronavirus) A systematic\nreview. Neural Process Lett, 1–53\n23. Bhosale YH, Zanwar S, Ahmed Z, Nakrani M, Bhuyar D, Shinde\nU (2022) Deep convolutional neural network based COVID-\n19 classification from radiology X-ray images for IoT enabled\ndevices. In: 2022 8th international conference on advanced\ncomputing and communication systems (ICACCS), vol 1. IEEE,\npp 1398–1402\n24. Zhang J, Xie Y, Li Y, Shen C, Xia Y (2020) COVID-19\nscreening on chest x-ray images using deep learning based\nanomaly detection. arXiv:2003.12338,2 7\n25. Hemdan EEl-D, Shouman MA, Karar ME (2020) COVIDx-net: a\nframework of deep learning classifiers to diagnose COVID-19 in\nx-ray images. arXiv:2003.11055\n26. Narin A, Kaya C, Pamuk Z (2021) Automatic detection of\ncoronavirus disease (COVID-19) using x-ray images and deep\nconvolutional neural networks. Pattern Anal Appl, 1–14\n27. Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016)\nRethinking the inception architecture for computer vision. In:\nProceedings of the IEEE conference on computer vision and\npattern recognition, pp 2818–2826\n28. Wang L, Lin ZQ, Wong A (2020) COVID-net: a tailored deep\nconvolutional neural network design for detection of COVID-19\ncases from chest x-ray images. Sci Rep 10(1):1–12\n29. Apostolopoulos ID, Mpesiana TA (2020) COVID-19: automatic\ndetection from X-ray images utilizing transfer learning with\nconvolutional neural networks. Phys Eng Sci Med 43(2):635–640\n30. Ozturk T, Talo M, Yildirim EA, Baloglu UB, Yildirim O,\nAcharya UR (2020) Automated detection of COVID-19 cases\nusing deep neural networks with X-ray images. Comput Biol Med\n121:103792\n31. Khan AI, Shah JL, Bhat MM (2020) CoroNet: a deep neural\nnetwork for detection and diagnosis of COVID-19 from chest\nX-ray images. Comput Methods Programs Biomed 196:105581\n32. Shazia A, Xuan TZ, Chuah JH, Usman J, Qian P, Lai KW (2021)\nA comparative study of multiple neural network for detection\nof COVID-19 on chest X-ray. EURASIP J Adv Sig Process\n2021(1):1–16\n1406\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \n33. Medsker LR, Jain LC (2001) Recurrent neural networks. Des Appl\n5:64–67\n34. Hochreiter S, Schmidhuber J (1997) Long short-term memory.\nNeural Comput 9(8):1735–1780\n35. Jia X, Gavves E, Fernando B, Tuytelaars T (2015) Guiding the\nlong-short term memory model for image caption generation. In:\nProceedings of the IEEE international conference on computer\nvision, pp 2407–2415\n36. Udritoiu AL, Cazacu IM, Gruionu LG, Gruionu G, Iacob A V,\nBurtea DE, Ungureanu BS, Costache MI, Constantin A, Popescu\nCF (2021) Real-time computer-aided diagnosis of focal pancreatic\nmasses from endoscopic ultrasound imaging based on a hybrid\nconvolutional and long short-term memory neural network model.\nPLoS ONE, 6\n37. Mousavi Z, Shahini N, Sheykhivand S, Mojtahedi S, Arshadi\nA (2022) COVID-19 detection using chest X-ray images based\non a developed deep neural network. SLAS Technol 27(1):63–\n75\n38. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need.\nIn: Advances in neural information processing systems, pp 5998–\n6008\n39. Devlin J, Chang M-W, Lee K, Toutanova K (2018) BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding. arXiv:1810.04805\n40. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I et\nal (2019) Language models are unsupervised multitask learners.\nOpenAI Blog 1(8):9\n41. Jaderberg M, Simonyan K, Zisserman A et al (2015) Spatial\ntransformer networks. Adv Neural Inf Process Syst 28:2017–2025\n42. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S et\nal (2020) An image is worth 16x16 words: transformers for image\nrecognition at scale. arXiv:2010.11929\n43. Sitaula C, Hossain MB (2021) Attention-based VGG-16 model\nfor COVID-19 chest X-ray image classification. Appl Intell\n51(5):2850–2863\n4 4 . P a r kS ,K i mG ,O hY ,S e oJ B ,L e eS M ,K i mJ H ,M o o nS ,L i mJ K ,\nYe JC (2021) Vision transformer for COVID-19 CXR diagnosis\nusing chest x-ray feature corpus. arXiv:2103.07055\n45. Qata-cov19 database. https://www.kaggle.com/aysendegerli/\nqatacov19-dataset\n46. Covid-19-image-repository. https://github.com/ml-workgroup/\nCOVID-19-image-repository/tree/master/png\n47. Eurorad. https://www.eurorad.org/\n48. COVID-chestxray-dataset. https://github.com/ieee8023/\nCOVID-chestxray-dataset\n49. COVID-19 database. https://www.sirm.org/category/senza-categoria/\nCOVID-19/\n50. Kaggle (2020) COVID-19 radiography database. https://www.\nkaggle.com/tawsifurrahman/COVID19-radiography-database\n51. Github (2020) COVID-cxnet. https://github.com/armiro/COVID-\nCXNet\n52. RSNA pneumonia detection challenge. https://www.kaggle.com/\nc/rsna-pneumonia-detection-challenge/data\n53. Chest x-ray images (pneumonia). https://www.kaggle.com/\npaultimothymooney/chest-xray-pneumonia\n54. Medical imaging databank of the valencia region. padchest: a\nlarge chest X-ray image dataset with multi-label annotated reports.\nhttps://bimcv.cipf.es/bimcv-projects/padchest/\n55. Weller SC (2005) Cultural consensus model. In: Kempf-Leonard\nK (ed) Encyclopedia of social measurement. Elsevier, New York,\npp 579–585\n56. Loshchilov I, Hutter F (2017) Decoupled weight decay regulariza-\ntion. arXiv:1711.05101\n57. Bhosale YH, Sridhar Patnaik K (2022) IoT deployable lightweight\ndeep learning application for COVID-19 detection with lung\ndiseases using RaspberryPI. In: 2022 international conference on\nIoT and blockchain technology (ICIBT). IEEE, pp 1–6\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds\nexclusive rights to this article under a publishing agreement with the\nauthor(s) or other rightsholder(s); author self-archiving of the accepted\nmanuscript version of this article is solely governed by the terms of\nsuch publishing agreement and applicable law.\nTianmu Wangreceived the B.E. degree in mechanical engineering\nfrom Nanjing University of Science and Technology in 2019, and\nreceived the M.S degree in mechanical engineering from Columbia\nUniversity in 2021. He is now a Ph.D. candidate in the Department\nof Mechanical Engineering at Tsinghua University and tracks the\nfield of intelligent medical robots in the Advanced Mechanism and\nRoboticized Equipment Lab.\nZhenguo Nie received the B.S. degree in materials science and\nengineering from Shandong University in 2006, and received the M.S.\nand Ph.D. degree in mechanical engineering from Tsinghua University\nin 2012 and 2016.\nHe is currently an Assistant Professor at the Department of\nMechanical Engineering, Tsinghua University, Beijing, China. From\n2016 to 2018, he was a Postdoctoral Researcher at Georgia Institute\nof Technology. He was a Postdoctoral Researcher and a lecturer at\nCarnegie Mellon University from 2018 to 2020. His research interests\ninclude AI-based research on mechanical engineering, intelligent\nmedical robots, topology optimizations, and AI-based CAD/CAM.\nRuijingWang received the B.E. degree in electrical engineering from\nHebei University of Technology in 2018, and received the M.S degree\nin computer engineering from New York University in 2021. She is\nnow a Ph.D. student in the School of Systems & Enterprises at Stevens\nInstitute of Technology. She tracks the field of human-computer\ninteraction in Human-AI Interaction design lab.\nQingfeng Xu received the B.E. degree in Computer Science from\nOcean University of China in 2018, and received the M.S degree in\nInformation Technology from the University of Melbourne in 2021. He\nis working as a Research Assistant in the Department of Mechanical\nEngineering at Tsinghua University in the Advanced Mechanism and\nRoboticized Equipment Lab.\nHongshiHuang is now an associate chief physician at Peking Univer-\nsity Institute of Sports Medicine. His research interest covers intelli-\ngent medical data analysis, diagnosing, sports medicine/rehabilitation,\nelite athletes sports training and performance enhancement methods,\nEast-meets-West in rehabilitation techniques in sports injuries and pre-\nvention programs, biomechanics of sports and lower extremity injury,\nisokinetic measurement and training, brace and shoe functions.\n1407\nMedical & Biological Engineering & Computing (2023) 61:1395–1408 \nHandingXu received the B.E. degree in mechatronic engineering from\nBeijing Institute of Technology in 2021. He is now a Ph.D. candidate\nin the Department of Mechanical Engineering at Tsinghua University\nand tracks the field of intelligent medical robots in the Advanced\nMechanism and Roboticized Equipment Lab.\nFugui Xiereceived the B.S. in mechanical engineering from Tongji\nUniversity in 2005 and the Ph.D. in manufacturing engineering from\nTsinghua University in 2012. He is an Associated Professor in the\nDepartment of Mechanical Engineering at Tsinghua University, China.\nFrom 2012 to 2014, he was a Postdoctoral Researcher at Tsinghua\nUniversity. He was the Alexander von Humboldt Research Fellow with\nFraunhofer IWU, Germany, from 2015 to 2016. His research interests\ninclude theory and design on the issues of mechanisms, parallel\nkinematics machines, parallel robots, and advanced manufacturing\nequipments.\nXin-Jun Liureceived the B.S. and M.S. degrees in machine design\nand manufacture and mechanics from Northeast Heavy Machinery\nInstitute, Qinhuangdao, China, in 1994 and 1996, respectively, and\nthe Ph.D. degree in mechanical design and theory from Yanshan\nUniversity, Qinhuangdao, in 1999.\nHe is currently a Professor at the Department of Mechanical\nEngineering, Tsinghua University, Beijing, China. From 2000 to 2001,\nhe was a Postdoctoral Researcher at Tsinghua University. He was\na Visiting Researcher with Seoul National University, Seoul, South\nKorea, from 2002 to 2003. He was the Alexander von Humboldt\nResearch Fellow with the University of Stuttgart, Stuttgart, Germany,\nfrom 2004 to 2005. He has authored/co-authored more than 110\npapers in refereed journals and refereed conference proceedings.\nHis research interests include parallel mechanisms, parallel robotics,\nparallel kinematic machines, and motion simulators.\nProf. Liu is currently the Chair of the IFToMM China-Beijing and\nthe Associate Editor for Mechanism and Machine Theory.\nAﬃliations\nTianmu Wang1,2,3 · Zhenguo Nie 1,2,3 · Ruijing Wang 4 · Qingfeng Xu 1,5 · Hongshi Huang 6 · Handing Xu 1,2,3 ·\nFugui Xie 1,2,3 · Xin-Jun Liu 1,2,3\nTianmu Wang\nwtm21@mails.tsinghua.edu.cn\nRuijing Wang\nrwang79@stevens.edu\nQingfeng Xu\nqingfeng.xu.academic@gmail.com\nHongshi Huang\nqhuanghs@bjmu.edu.cn\nHanding Xu\nxhd21@mails.tsinghua.edu.cn\nFugui Xie\nxiefg@tsinghua.edu.cn\nXin-Jun Liu\nxinjunliu@tsinghua.edu.cn\n1 Department of Mechanical Engineering, Tsinghua University,\nBeijing, 100084, China\n2 State Key Laboratory of Tribology in Advanced Equipment,\nTsinghua University, Beijing, 100084, China\n3 Beijing Key Lab of Precision/Ultra-precision Manufacturing\nEquipments and Control, Tsinghua University,\nBeijing, 100084, China\n4 School of System & Enterprises, Stevens Institute of Technology,\nHoboken, NJ, 07030, USA\n5 National Cancer Center, Chinese Academy of Medical Sciences\nand Peking Union Medical College, Beijing, 100060, China\n6 Institute of Sports Medicine, Peking University Third Hospital,\nBeijing, 100091, China\n1408"
}