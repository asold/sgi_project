{
  "title": "Modeling and Learning Constraints for Creative Tool Use",
  "url": "https://openalex.org/W3214357131",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5044374534",
      "name": "Tesca Fitzgerald",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5007028896",
      "name": "Ashok K. Goel",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5066797327",
      "name": "Andrea L. Thomaz",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6688429677",
    "https://openalex.org/W187005120",
    "https://openalex.org/W1986014385",
    "https://openalex.org/W6672104468",
    "https://openalex.org/W2792089292",
    "https://openalex.org/W6688175513",
    "https://openalex.org/W4255911917",
    "https://openalex.org/W4211133235",
    "https://openalex.org/W6758657344",
    "https://openalex.org/W3190995216",
    "https://openalex.org/W6754906205",
    "https://openalex.org/W2080775597",
    "https://openalex.org/W6744318051",
    "https://openalex.org/W6735944222",
    "https://openalex.org/W4234644748",
    "https://openalex.org/W2964147919",
    "https://openalex.org/W2085261163",
    "https://openalex.org/W2901124183",
    "https://openalex.org/W6755049823",
    "https://openalex.org/W6762821728",
    "https://openalex.org/W6687583651",
    "https://openalex.org/W6748637925",
    "https://openalex.org/W2155150524",
    "https://openalex.org/W3036824760",
    "https://openalex.org/W2520246426",
    "https://openalex.org/W3167987924",
    "https://openalex.org/W2010873124",
    "https://openalex.org/W4243089390",
    "https://openalex.org/W2164474021",
    "https://openalex.org/W6736561116",
    "https://openalex.org/W6684509599",
    "https://openalex.org/W6667522336",
    "https://openalex.org/W4246449400",
    "https://openalex.org/W6631280160",
    "https://openalex.org/W3080814299",
    "https://openalex.org/W2963776680",
    "https://openalex.org/W6683895154",
    "https://openalex.org/W2118450042",
    "https://openalex.org/W1999549166",
    "https://openalex.org/W2085583171",
    "https://openalex.org/W130216483",
    "https://openalex.org/W6731940219",
    "https://openalex.org/W6677682937",
    "https://openalex.org/W6750409132",
    "https://openalex.org/W2097381042",
    "https://openalex.org/W2143093968",
    "https://openalex.org/W6713287227",
    "https://openalex.org/W2205340216",
    "https://openalex.org/W4297672196",
    "https://openalex.org/W2576518050",
    "https://openalex.org/W2789707227",
    "https://openalex.org/W2913547852",
    "https://openalex.org/W2165308133",
    "https://openalex.org/W2085378155",
    "https://openalex.org/W2067928539",
    "https://openalex.org/W2889990052",
    "https://openalex.org/W2962984928",
    "https://openalex.org/W2755546070",
    "https://openalex.org/W2120962217",
    "https://openalex.org/W1524405667",
    "https://openalex.org/W2893403646",
    "https://openalex.org/W2210658236",
    "https://openalex.org/W2406325222",
    "https://openalex.org/W1933657216",
    "https://openalex.org/W2946078727",
    "https://openalex.org/W2579414847",
    "https://openalex.org/W1536258620",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2963280855",
    "https://openalex.org/W2161395589"
  ],
  "abstract": "Improvisation is a hallmark of human creativity and serves a functional purpose in completing everyday tasks with novel resources. This is particularly exhibited in tool-using tasks: When the expected tool for a task is unavailable, humans often are able to replace the expected tool with an atypical one. As robots become more commonplace in human society, we will also expect them to become more skilled at using tools in order to accommodate unexpected variations of tool-using tasks. In order for robots to creatively adapt their use of tools to task variations in a manner similar to humans, they must identify tools that fulfill a set of task constraints that are essential to completing the task successfully yet are initially unknown to the robot. In this paper, we present a high-level process for tool improvisation (tool identification, evaluation, and adaptation), highlight the importance of tooltips in considering tool-task pairings, and describe a method of learning by correction in which the robot learns the constraints from feedback from a human teacher. We demonstrate the efficacy of the learning by correction method for both within-task and across-task transfer on a physical robot.",
  "full_text": "Modeling and Learning Constraints for\nCreative Tool Use\nTesca Fitzgerald1*, Ashok Goel2 and Andrea Thomaz3\n1Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States,2School of Interactive Computing, Georgia Institute\nof Technology, Atlanta, GA, United States,3Department of Electrical and Computer Engineering, University of Texas at Austin,\nAustin, TX, United States\nImprovisation is a hallmark of human creat ivity and serves a functional purpose in\ncompleting everyday tasks with novel resources. This is particularly exhibited in tool-\nusing tasks: When the expected tool for a task is unavailable, humans often are able\nto replace the expected tool with an atypical one. As robots become more\ncommonplace in human society, we will al so expect them to become more skilled\nat using tools in order to accommodate unexpected variations of tool-using tasks. In\norder for robots to creatively adapt their use of tools to task variations in a manner\nsimilar to humans, they must identify tools that fulﬁll a set of task constraints that are\nessential to completing the task successfully yet are initially unknown to the robot. In\nthis paper, we present a high-level process for tool improvisation (tool identiﬁcation,\nevaluation, and adaptation), highlight the importance of tooltips in considering tool-\ntask pairings, and describe a method of l earning by correction in which the robot\nlearns the constraints from feedback fr om a human teacher. We demonstrate the\nefﬁcacy of the learning by correction method for both within-task and across-task\ntransfer on a physical robot.\nKeywords: tool manipulation, tool transfer, learning from corrections, human-robot interaction, cognitive robotics\n1 INTRODUCTION\nThe abundant use of tools for a large range of tasks is a hallmark of human cognition (Vaesen, 2012).\nDesign of new tools for accomplishing novel tasks, as well as improvisation in the absence of typical\ntools and use of tools in novel ways, are characteristics of human creativity. Consider for example, the\ndesign of a paperweight to hold a sheaf of papers, or the use of a paperweight to hammer in a nail if an\nactual hammer is not available. Both require reasoning about complex relationships that\ncharacterizes human cognition and creativity (Penn et al., 2008): The latter task, for instance,\nrequires reasoning about the relationships among the force required to hammer in a nail, the surface\nof the nail’s head, the surface of the paperweight bottom, the weight of the paperweight, and so on.\nA robot situated in human society will also encounter environments and tasks suited for human\ncapabilities, and thus it is important for a robot to be able to use human tools for human tasks (Kemp\net al., 2007). While a robot may learn to complete a new task with a new toolvia demonstrations by a\nhuman teacher (Argall et al., 2009; Rozo et al., 2013), the demonstration(s) provided for that tool\ncannot prepare the robot for all variations of that tool it is likely to encounter. These variations can\nrange from different tool dimensions (e.g., different sized spoons, hammers, and screwdrivers) to tool\nreplacements when a typical tool is not available (e.g., using a measuring cup instead of a ladle, or a\nrock instead of a hammer). An additional challenge is that tools are often used to manipulate other\nobjects in the robot’s environment. Given that the shape of a tool alters its effect on its environment\nEdited by:\nPatricia Alves-Oliveira,\nUniversity of Washington,\nUnited States\nReviewed by:\nClaude Sammut,\nUniversity of New South Wales,\nAustralia\nPaola Ardon,\nHeriot-Watt University,\nUnited Kingdom\n*Correspondence:\nTesca Fitzgerald\ntesca@cmu.edu\nSpecialty section:\nThis article was submitted to\nHuman-Robot Interaction,\na section of the journal\nFrontiers in Robotics and AI\nReceived: 01 March 2021\nAccepted: 14 October 2021\nPublished: 05 November 2021\nCitation:\nFitzgerald T, Goel A and Thomaz A\n(2021) Modeling and Learning\nConstraints for Creative Tool Use.\nFront. Robot. AI 8:674292.\ndoi: 10.3389/frobt.2021.674292\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742921\nORIGINAL RESEARCH\npublished: 05 November 2021\ndoi: 10.3389/frobt.2021.674292\n(Sinapov and Stoytchev, 2008 ), a tool replacement may\nnecessitate a change in the manipulation of that tool in order\nto achieve the same task goal (Brown and Sammut, 2012).\nOne aim of developingcreative robots is to enable robots to\nexhibit creative reasoning in a similar manner as humans in order\nto enhance human-robot collaboration. Recently,Gubenko et al.\n(2021) have called for an interdisciplinary approach that\nsynthesizes conceptual frameworks from diverse disciplines\nsuch as psychology, design, and robotics to better understand\nboth human and robot creativity. In human cognition, creative\nreasoning is exempliﬁed by improvised tool use; particularly, our\nability to use analogical reasoning to identify replacement tools or\nmethods that may be used to achieve the original goal, as well as\nreason over the differences between the original and replacement\napproaches in order to adapt the replacement to the task (Goel\net al., 2020). In design, for example, there is the notion of intrinsic\nfunctions and ascribed functions (Houkes and Vermaas, 2010): In\nthe latter, the user can use the object or tool for an ascribed\nfunction. Our goals for creative robots are similar: to be able to\nreason over the suitability of possible tool replacements when the\noriginal tool is unavailable, and reason over how the robot’s\nexecution of the task must be adapted for the replacement tool.\nThere are several key challenges in enabling robots to\ncreatively use new tools. First, the robot must explore novel\ntool replacements that support the task constraints. Second, the\nrobot must be able toevaluate a novel tool’s suitability for a\nparticular task, which involves learning a model of the\ninteractions between the robot’s gripper, the tool, objects in\nthe robot’s environment that are manipulated by that tool,\nand how those interactions affect the completion of the task\ngoals. Finally, the robot mustadapt its task model to the novel\ntool in order to fulﬁll these constraints. Prior work has addressed\nthese ﬁrst two challenges by constructing or identifying creative\ntool replacements (Choi et al., 2018; Sarathy and Scheutz, 2018;\nNair and Chernova, 2020). In this paper, we identify and model\nthe tooltip constraints that play a role in all three of these\nchallenges. In particular, we focus on the third challenge of\nadapting a robot ’s task model to a novel tool. The\ncontributions of this paper are as follows:\n1) An exploratory analysis of the manipulation constraints that\nmust be fulﬁlled when using a tool to complete three tasks in\nsimulation.\n2) Two models that represent the relationship between the\norientation and position constraints when manipulating\na tool.\n3) An algorithm for training these models using interaction\ncorrections provided by a human teacher,ﬁrst proposed in\nFitzgerald et al. (2019).\n4) A discussion of the generalizability of these models when\napplied to new tools and/or tasks.\nWe organize the rest of this paper as follows. Section 2\npresents a summary of related work in cognitive science,\ncomputational creativity, and robotic tool use. Section 3\ndeﬁnes the tool transfer problem in terms of constraints on\nthe tooltip pose, which we then explore in Section 4 via an\nextensive evaluation of the effect of tooltip perturbations on task\nperformance in simulation. InSection 5, we discuss how a robot\nmay learn these constraints through corrections provided via\ninteraction with a human teacher. Finally, we summarize this\npaper in Section 6.\n2 BACKGROUND\n2.1 Deﬁning Creative Reasoning\nWhat does it mean for a robot to be“creative”? Prior work in\ncreative robotics has often fallen under one of two categories of\ncreativity: 1) Producing a creative output involving creative\ndomains such as music (Gopinath and Weinberg, 2016) and\npainting (Schubert and Mombaur, 2013 ), or 2) Invoking a\ncreative reasoning process. Within the latter category, several\ncriteria for creative reasoning have been proposed, such as\nautonomy and self-novelty (Bird and Stokes, 2006), in which\nthe robot’s creative output is novel to itself but not necessarily to\nan outside observer. Another deﬁnition of a creative reasoning\nprocess is one that emphasizes both the variation of potential\nsolutions considered by the agent, as well as the process used to\nconsider and select from those options (Vigorito and Barto,\n2008).\nCreative reasoning may also be de ﬁned in an interactive\nsetting. Co-creativity is a process for creative reasoning in\nwhich an agent interacts with a human to iteratively improve\nupon a shared creative concept. In doing so, co-creativity fosters\ncreative reasoning and may improve the quality of the resulting\noutput (Yannakakis et al., 2014). In prior work, we have deﬁned\nco-creative reasoning in the context of a robot that collaborates\nwith a human teacher to produce novel motion trajectories, while\nalso aiming to maximize its own, partial-autonomy (Fitzgerald\net al., 2017). In the context of a robot reasoning over how it may\nexecute a task in a new environment, this co-creative process\nallows the robot to obtain the contextual knowledge needed to\nadapt its task model to meet the constraints of the novel\nenvironment.\nCreative reasoning has been de ﬁned in other relevant\ndomains, such as design creativity. Analogical reasoning is said\nto be a fundamental process of creativity in design (Goel, 1997).\nIn design by analogy, a new design is created by abstracting and\ntransferring design patterns from a familiar design to a new\ndesign problem, where the design patterns may capture\nrelationships among the abstract function, behavior, structure,\nand geometry of designs. Design also entails discovery of problem\nconstraints (Dym and Brown, 2012) including making implicit\nconstraints in a design problem more explicit (Dabbeeru and\nMukerjee, 2011). Fauconnier and Turner (2008) introduced\nconceptual blending as another process for creative reasoning.\nThis approach addresses analogical reasoning and creativity\nproblems by obtaining a creative result from merging two or\nmore concepts to produce a new solution to a problem.\nAbstraction is enabled by mapping the merged concepts to a\ngeneric space, which is then grounded in the blend space by\nselecting aspects of either input solution to address each part of\nthe problem. Applied to a robotic agent that uses this creative\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742922\nFitzgerald et al. Constraints for Creative Tool Use\nprocess to approach a new transfer problem, the robot may\ncombine aspects of several learned tasks to produce a new\nbehavior.\nOverall, these methods for creative reasoning highlight\ntwo important components of creative reasoning: The\nexploration of novel solutions to a problem, and an\nevaluation of each candidate solution ’s effectiveness. Prior\nwork in creative reasoning (e.g., analogical reasoning,\ninteractive co-creativity, and conceptual blending) have\naddressed these challenges, but not yet in the context of\ncreative tool use by an embodied robot. This domain\nrequires additional considerations, in that it is grounded in a\nrobot’s action and perception (Fitzgerald et al., 2017). First, the\nrobot has imperfect perception of its environment and/or tools,\nand thus may not have a complete model of the tool(s) it may\nuse. Second, its solution must be in the form of a motion\ntrajectory that utilizes the tool to achieve the task goals. As a\nresult, not only is thechoice of tool a creative one, but theusage\nof that tool is creative as well. We now review relevant literature\nthat addresses these challenges within the robotic tool use\ndomain.\n2.2 Identifying Novel Tool Candidates\nExisting work typically focuses on identifying theaffordances of\npotential tool candidates. Affordances represent the “action\npossibilities” that result from the relationship between an\nobject and its environment ( Gibson, 1979 ). Once the\naffordances of candidate tools have been identi ﬁed, a robot\ncan reason over the most suitable tool for a particular task\nand integrate it into its motion plan ( Agostini et al., 2015;\nChoi et al., 2018). However, identifying tool affordances is a\nnon-trivial challenge. Recent work in computer vision has applied\ndeep neural networks to this problem in order to visually predict\nthe affordances for a particular tool (Do et al., 2018). The UMD\nPart Affordance Dataset (Myers et al., 2015) is intended to\nsupport further work on visual affordance detection. This\ndataset contains RGB-D images for 105 tools, grouped into 17\nobject categories. Each tool is photographed at roughly 75\norientations, each of which corresponds to a pixel-wise\nlabeling according to 7 possible affordances (e.g., cutting,\ngrasping, pounding). Other, physics-based features such as the\ndimensions or material of an object may also be used to judge\ntheir effectiveness as potential tools, such as when identifying a\npipe as a makeshift lever to pry open a door (Levihn and Stilman,\n2014). Prior work has shown that, in addition to using\ndemonstrations to learn a task, a robot may also use\ndemonstrations to learn to recognize the affordance-bearing\nsubparts of a tool such that it can identify them on novel\nobjects (Kroemer et al., 2012).\nWhen a suitable tool replacement is not already available in\nthe robot’s environment, it may be necessary to assemble one\n(Sarathy and Scheutz, 2018). Choi et al. (2018) extends the\nICARUS cognitive architecture to assemble virtual tools from\nblocks. Nair et al. (2019)describes a method for tool construction\nby pairing candidate tool parts and then evaluating each pair by\nthe suitability of the shape and attachability of the two parts. Later\nwork (Nair and Chernova, 2020) integrates this process into a\nplanning framework such that the task plan includes both the\nconstruction and use of the required tool.\nWhile candidate tool identiﬁcation is not the focus of this\narticle, it is an essential step in our eventual goal of creative tool\nuse. Overall, prior work on this topic demonstrates the task-\nspeciﬁc requirements for identifying novel tool candidates, and\nthe importance of identifying the salient features of a tool within\nthe context of the current task. We now consider how these\nfeatures affect the tool’s suitability when evaluating them for a\nparticular task.\n2.3 Evaluating Novel Tool Candidates\nThe shape of a tool alters its effect on its environment (Sinapov\nand Stoytchev, 2008), and thus a tool replacement may necessitate\na change in the manipulation of that tool in order to achieve the\nsame task goal (Brown and Sammut, 2012). For tasks involving\nthe use of a rigid tool, the static relationship between the robot’s\nhand and the tooltip is sufﬁcient for controlling the tool to\ncomplete a task (Kemp and Edsinger, 2006; Hoffmann et al.,\n2014). These methods assume a single tooltip for each tool, and\nthat this tooltip is detectedvia visual or tactile means. For tasks\ninvolving multiple surfaces of the tool, the task model can be\nexplicitly deﬁned with respect to those segments of the tool, and\nrepeated with tools consisting of similar segments (Gajewski et al.,\n2018). However, this assumes a hand-de ﬁned model that\nrepresents the task with respect to pre-de ﬁned object\nsegments, and that these object segments are shared across\ntools. Given enough training examples of a task, a robot can\nlearn a success classiﬁer that can later be used to self-supervise\nlearning task-oriented tool grasps and manipulation policies for\nunseen tools (Fang et al., 2018). We similarly aim to situate a new\ntool in the context of a known task, but eliminate the assumptions\nthat 1) the new tool is within the scope of the training examples\n(which would exclude creative tool replacements) and 2) that the\ntool features relevant to the task are observable and recorded by\nthe robot.\n2.4 Adapting Task Models to Novel Tools\nThe aim of transfer learning for reinforcement learning domains\nis typically to use feedback obtained during exploration of a new\nenvironment in order to enable reuse of a previously learned\nmodel (Taylor and Stone, 2009). In previous work, we have\nshown how interaction can be used to transfer the high-level\nordering of task steps to a series of new objects in a target domain\n(Fitzgerald et al., 2018). Similarly, the aim of one-shot learning is\nto quickly learn a new task, often improving learning from a\nsingle demonstration by adapting previous task knowledge. Prior\nwork in this space focuses on learning a latent space for the task in\norder to account for new robot dynamics (Srinivas et al., 2018)o r\nnew task dynamics (Fu et al., 2016; Killian et al., 2017). “Meta-\nlearning” approaches have succeeded at reusing visuomotor task\npolicies learned from one demonstration (Chelsea et al., 2017)\nand using a new goal state to condition a learned task network\nsuch that it can be reused with additional task objects (Duan et al.,\n2017). We address the problem of a robot that hasnot yetbeen\nable to explore these relationships, aiming to enable rapid\nadaptation of a task model for unseen task/parameter\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742923\nFitzgerald et al. Constraints for Creative Tool Use\nrelationships. The tool transform models learned by our\napproach are not speciﬁc to any task learning algorithm or\nrepresentation, and thus can compliment or bootstrap\nmethods for reinforcement, one-shot, and meta learning.\n2.5 Summary of Related Work\nThrough prior work, we have identi ﬁed three key steps for\ncreative tool use: Exploring novel tools, evaluating novel tools,\nand adapting task models to novel tools. These stages are not\nentirely separable from each other, as evaluating reﬂects how well\nthe robot anticipates being able to adapt its task model for a\nparticular tool, and exploration results in a set of tools that meet\nsome criteria such that they may be evaluated in the context of the\ntask. A common theme through all three steps is the importance\nof constraints (e.g., tool shape, segments, or visual features) that\ndictate how a task model may be adapted to a particular tool, and\nas a result, play a role in the exploration and evaluation steps\nas well.\nIn the rest of this paper, we focus on this challenge of identifying\nand modelingconstraints, and demonstrate how these constraints\nmay be used in the evaluating and adapting steps of creative tool\nuse. While we do not explicitly address creative tool exploration, we\naim for this work to support future research on identifying these\nconstraints visually to enable this exploration.\n3 TOOLTIPS AS CONSTRAINTS\nSuppose that a robot has learned a trajectoryTa /equals[ p(0)\na ,p(1)\na ,... ,p(n)\na ]\nconsisting of end-effector posesp(i)\na for a particular task using\ntool a, and now must complete the same task using a different\ntool b. Our goal is to transform each pose individually for tool\nb. Representing an original pose for toola in terms of its 3× 1\ntranslational vector ta and 4 × 1r o t a t i o n a lv e c t o rra,w e\ntransform it into a posepb for tool b as follows:\npb /equals ϕb\na pa() /equals 〈ta + ^t, ra · ^r〉 (1)\nHere, ra · ^r refers to the Hamilton product between the two\nquaternions. This deﬁnition relies on a known transform between\ntools a and b, which requires knowledge of the appropriate\n“reference” point for both tools such that their transform can\nbe computed. Neither reference point is initially known by the\nrobot, however, nor can it be extracted from the trajectory which\nis represented according to the robot’s end-effector, and not\naccording to any point on the tool itself.\nIdentifying the“reference point” for a tool is non-trivial. While\nprior work has addressed the problem of identifying affordance\nregions of a tool, these regions are too broad to characterize the\ntransform between two tools. Figure 1 illustrates examples of\nthese labeled affordance regions based on the UMD Part\nAffordance Dataset (Myers et al., 2015). While this dataset is\nrelevant to identifying similar regions on two separate tools, it\ndoes not address the problem of specifying the equivalent points\nof a tool that may be used to transform the trajectory for a\nparticular task from one tool to another. For example, the full\nblade of a knife may be labeled as enabling the “cutting”\naffordance (Figure 1), even though a cutting task is likely to\nbe performed with respect to only the edge of the blade.\nFurthermore, since affordance data is presented in the form of\npixel-wise image labels, it does not provide any data concerning\nthe kinematic implications of using this tool. Since the tool is\nobserved and labeled from a static, overhead perspective,\naffordance data is only available along a single 2D plane, and\nthus does not indicate theorientation at which each affordance is\nor is not valid.\nThis is essential for manipulating the tool properly; even if the\nrobot were to determine that the relevant surface of a knife is\nlocated along the edge of its blade, the blade must still be oriented\ncarefully with respect to the cutting target for the task to be\ncompleted successfully. We refer to the acting surface of the tool\n(e.g., a singular point along the edge of the knife blade, or a\nsingular point on a mallet’s pounding surface) as atooltip that is\ndeﬁned by a pose containing both the position and orientation of\nthat tooltip. In summary, we expect that successful task\ncompletion relies on the robot having a model of the\ncomposite transform between 1) the end-effector, 2) its\ngrasp of the tool(highlighted in red inFigure 1), and 3) the\ntooltip position and orientation.\nFIGURE 1 |Affordance regions may be broad, spanning multiple possible tooltips. As a result, predicting the affordance region is not sufﬁcient to plan with respect\nto that tool’s tooltip. For example, the full blade surfaces of the saw and knife are labeled as enabling the“cutting” affordance (highlighted in green) and the“grasping”\naffordance (highlighted in red); however, cutting is only performed using the edge of the blade, and requires that the blade be oriented toward the cutting target. Similarly,\ndifferent points of a hammer head may enable different tasks (e.g., pounding versus prying), and thus detecting a task-independent affordance region (highlighted in\npurple) is not sufﬁcient to plan a task trajectory.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742924\nFitzgerald et al. Constraints for Creative Tool Use\nWhile we may mathematically represent a tooltip as a\nsingular pose, practically, however, there are likely many\npossible tooltips that may lead to successful task execution.\nAdditionally, the constraint over the tooltip may also differ\ndepending on the context in which it is used: The orientation of\na hammer is constrained along two axes when hammering a nail,\nbut the hammer may still be rotated around the nail (e.g., its\n“yaw” rotation) without affecti ng task performance. This\nexample supports the notion of a one-to-many relationship\nbetween 1) a tooltip and 2) the tool poses that enable that\ntooltip to be used.\nIn the remainder of this paper, we explore this one-to-many\nrelationship. In Section 4, we demonstrate how a single tooltip\ncan be expanded into a set of effective tool poses, thus\nhighlighting the challenges of learning tooltip constraints. In\nSection 5 , we consider this relationship in the opposite\ndirection, and present two models for deriving a single tooltip\nfrom a set of valid poses demonstrated by a human teacher.\n4 CHARACTERIZING TOOL CONSTRAINTS\nWe ﬁrst explore the effect of tooltip constraints by expanding a\nsingle tooltip into a set of tool poses that result in successful task\nexecution. To do so, we transform a trajectory that results in\nsuccessful task execution (and thus the tooltip is implicitly-\ndeﬁned) such that the tooltip’s trajectory is perturbed slightly.\nIn doing so, we can evaluate the effect of that perturbation on task\nperformance, and ultimately model the constraints that dictate\nwhich poses result in successful use of the tooltip.\nIn this section, we address two key research questions:\n1) How do changes in tool pose affect task performance?\n2) How do the constraints on tool pose differ across tools and/or\ntasks?\n4.1 Evaluating Tool-Task Constraints in\nSimulation\nWe address these research questions by evaluating the\nperformance of a large set of trajectory perturbations using a\nsimulated 7-DOF Kinova Gen3 robot arm situated on a round\ntable in a Gazebo simulated environment. We evaluated the effect\nof trajectory perturbations on three tools: A hammer, a mug, and\na spatula (Figure 2). We ﬁxed the robot’s grasp as a static\ntransform between the robot’s gripper and the tool, and thus\ndid not evaluate the effects of the robot’s grasp strength or\nstability on tool use.\nFor each tool, we provided a demonstration of three tasks:\nHooking ( Figure 3A ), lifting ( Figure 3B ), and sweeping\n(Figure 3C). Each demonstration was provided in a Gazebo\nsimulator as a set of end-effector keyframes. Depending on the\ntool being demonstrated, this resulted in 5-7 keyframes for\nhooking, 4-6 for lifting, and 13-18 for sweeping. These end-\neffector keyframes were then converted to keyframe trajectories\nrepresented in the robot ’s joint-space. We used the MoveIt\n(Coleman et al., 2014 ) implementation of the RRTConnect\nplanner to plan between joint poses during trajectory\nexecution. We simulated a trajectory perturbation by altering\nthe rigid transform between the robot’s gripper and the tool itself,\naccording to a pre-determined set of position and orientation\nalterations that are consistent across all tools and tasks. As a\nresult, each trajectory perturbation is identical with respect to the\nrobot’s end-effector, but differ with respect to the trajectory of the\ntool itself. This allowed us to use the same joint-space trajectory\nFIGURE 2 |We performed an evaluation across three tools: a spatula, mug, and hammer. For each tool, we perturbed the trajectory of the tooltip by adjusting the\nrobot’s grasp of the tool. These pose variations are just a small set of the 729 perturbations we evaluated for each tool-task pairing.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742925\nFitzgerald et al. Constraints for Creative Tool Use\nfor all perturbations of a single tool-task pairing, thus reducing\nthe likelihood of planning errors across all perturbations and also\nminimizing any changes in the robot’s joint motion that might\naffect task performance. Despite the same trajectory being\nexecuted across all perturbations of a single tool-task pairing,\nplanning errors may still occur when a perturbation results in the\ntool colliding with its environment, thus preventing the rest of the\ntrajectory from being executed.\nEach perturbation resulted from a unique permutation of\nchanges applied to the tool’s demonstrated position along the\nx, y,a n dz axes and demonstrated orientation along the roll,\npitch, and yaw axes. The tool’s x, y,a n dz positions were each\nconﬁgured at one of three distances from the demonstrated\ntool position: [ − 0 . 0 1 ,0 ,0 . 0 1 ]m e t e r s .T h et o o l’s roll, pitch,\nand yaw rotations were each conﬁgured at one of three angles\nfrom the demonstrated tool orientation: [−\nπ\n16, 0, π\n16] radians.\nThese position and orientation perturbations were empirically\nchosen such that, when combined, their effect on task\nperformance can be observed on a spectrum. We observed\nthat larger ranges of pose or orientation changes would be less\nlikely to result in completion of any aspect of the task, whereas\nsmaller ranges may not fully explore the range of successful\nperturbations. However, as we note later in Section 4.3,w e\nobserve that different tools vary in their sensitivity to these\nperturbations, and thus a more ﬁne-grained set of\nperturbations should be explored in future work.\nOverall, the permutation of these conﬁgurations resulted in a\ntotal of 3\n6 /equals 729 perturbations for each tool-task pairing. We\nexecuted each perturbation twice in simulation (to account for\nthe non-deterministic effects of the simulator dynamics) and\nrecorded the average performance of the two trials, with\nperformance being measured according to task-speci ﬁc\nmeasures. All performance metrics were scaled to a 0– 1 range.\nIn the hooking task, performance was measured as the distance\n(in meters) between the box and the robot’s base, with less\ndistance correlating to higher performance. The initial and\ngoal states of this task are shown inFigure 3A. In the lifting\ntask, the robot’s performance was measured as the green bar’s\nheight above the table (in meters). A small number of trials\nresulted in the bar being removed from the support structure\nentirely. In these cases, we recorded the performance as that of the\ntask’s initial state (i.e., a failure case).Figure 3Bshows the initial\nand goal states of this task. In the sweeping task, performance is\nmeasured as the number of spheres that were swept off the table,\nwith maximum performance being 16 spheres. The initial and\ngoal states of this task are shown inFigure 3C.\n4.2 Results\nOur evaluation measured how sensitive each tool-task pairing is\nto perturbations of the tooltip’s trajectory: The more sensitive the\ntool-task pairing is to perturbations, the more likely that a\nperturbation will lead to a task failure. Low task performance\nmay be caused by the tooltip no longer contacting any relevant\nobjects in the task (and thus leaving the task in its initial state), or\nby collisions between the tool ’s new con ﬁguration and its\nenvironment that prevent the robot from executing the full\nFIGURE 3 |Initial and goal states for the(A) hooking, (B) lifting, and (C) sweeping tasks.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742926\nFitzgerald et al. Constraints for Creative Tool Use\nFIGURE 4 | (A)Percentage of failed trials (performance≤ 0.05). Darker cells indicate higher percentage of failed trials.(B) Mean and standard deviation\nperformance of thresholded (performance> 0.05) trials. Darker cells indicate higher mean performance.\nFIGURE 5 |Performance distributions over all tool-task pairings, with all trials with performance≤ 0.05 excluded.X-a n dY-axes are consistent across all graphs.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742927\nFitzgerald et al. Constraints for Creative Tool Use\ntrajectory. We set a threshold performance of 0.05 (on a 0– 1\nscale), and report the percentage of perturbations that fail to\nexceed this threshold inFigure 4.\nWe include only the set of perturbations that exceed this\nthreshold in the histograms in Figure 5, which illustrate the\nperformance distributions over the set of perturbations exceeding\nthis threshold. Since the original, unperturbed pose is already\nknown to achieve near-optimal task performance, these graphs\nillustrate how many perturbations of that original pose still fulﬁll\nthe tooltip constraints and result in high performance (i.e., the\nperturbations resulting in the peak observed nearx /equals 1.0 on each\ngraph). We report the mean and variance over these performance\nresults in (Figure 4B).\nFigure 6 shows the distribution over themean performance\nover all three tasks; that is, the performance metric for each\nperturbation is the average of its performance on the sweeping,\nhooking, and lifting tasks. We again only consider datapoints\nabove a performance threshold> 0.05 in order to focus on the set\nof valid tooltip constraints for each tool.\n4.3 Discussion\nResearch Question #1: How do changes in tool pose affect task\nperformance? The relationship between performance and\ntool pose may be non-linear. If this relationship were linear,\nwe would expect Figure 5 to primarily contain Gaussian-like\nperformance distributions, such that as the robot evaluates\ntrajectory perturbations further from the original trajectory, its\nperformance resulting from those perturbations decreases\nproportionally. While this is the case in some tool-task pairings\n(e.g., all tools used for the sweeping task, and the lifting task using\nthe hammer), other performance distributions appear to be bi-\nmodal in nature (e.g., using the hammer in the hooking task or\nusing the spatula for lifting) or contain several peaks (e.g., using the\nmug for hooking). This suggests that there is a non-linear\nrelationship between changes in the tool pose, and its resulting\neffects on task performance. Note that in our evaluation, we applied\ntrajectory perturbations according to the single tooltip that was\ndemonstrated for each tool-task pairing. An opportunity for future\nresearch is the identiﬁcation of alternate tooltips based on the tool’s\nshape or structure.\nResearch Question #2: How do the constraints on tool pose\ndiffer across tools and/or tasks?Tools differed in their sensitivity\nto pose changes.For example, using the spatula tool resulted in\nthe highest percentage of failed trials (35.11– 35.8%) across all\nthree tasks, while the mug resulted in the lowest (3.29– 4.25%)\nacross all three tasks. One hypothesis for this performance\ndifference is that since the mug was the smallest tool, changes\nin the tool pose had a smaller effect on its tooltip pose in\ncomparison to the taller tools (spatula and hammer). We\nobserved widely varying failure rates when using the hammer,\nranging from 9.19 to 10.01% on the hooking and sweeping tasks,\nrespectively, and 45.27% on the lifting task. One reason for this\nperformance difference may be that a different tooltip was used\nfor the lifting task compared to the hooking and sweeping tasks.\nIn the former, the robot uses a“corner” of the hammer to lift the\nbar (Figure 3B), whereas the hooking and sweeping tasks use a\nwider surface area of the hammer as a tooltip. This may provide\nmore tolerance to pose perturbations.Overall, this suggests that\nthe sensitivity of tooltip constraints depends on the surface of\nthe tool being used.\nFigure 6 also supports this hypothesis. These distribution\ngraphs reﬂect the consistency in tooltip constraints across tasks.\nWhile the geometry of the tool itself remains constant across\ntasks, the same tooltip is not necessarily used across tasks (e.g.,\nusing separate surfaces of the hammer for sweeping vs lifting).\nThe reduced performance shown in these graphs (in comparison\nto Figure 5) indicates thatthe tooltip constraints applied to one\ntask may not be generalizable to other tasks using the\nsame tool.\nWe now consider the challenge of how a robot may quickly\nlearn these constraints in the context of a new tool, and whether\nwe can model the instances in which a robot can reuse a learned\ntooltip model in the context of another task. While a robot can\nlearn to use a tool through demonstrations, the one-to-many\nmapping between tooltip constraints and the set of tool poses that\nmeet those constraints means that there are many possible\ndemonstrations that a robot may receive for a tool/task\npairing. Learning the underlying tool constraint is therefore a\nchallenge, as the teacher is providing demonstrations that sample\nfrom an unknown, underlying relationship between the end-\nFIGURE 6 |Mean performance distributions using each tool for all tasks, with all trials with mean performance≤ 0.05 excluded.X-a n dY-axes are consistent across\nall graphs.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742928\nFitzgerald et al. Constraints for Creative Tool Use\neffector and the tooltip. In the next section, we explore how a\nrobot can utilize corrections in order to model and learn the\nunderlying tooltip constraint.\n5 LEARNING CONSTRAINTS FROM\nINTERACTIVE CORRECTIONS\nIn the previous section, we evaluated the one-to-many mapping\nbetween tooltips constraints and end-effector poses that meet\nthose constraints. In order to adapt the robot’st a s km o d e lt o\na novel tool, however, we also need to analyze this mapping\nin the reverse direction: inferring the underlying tooltip\nconstraint that has resulted in a set of corresponding end-\neffector poses.\nWe address this challenge in the context of a robot that learns\nfrom demonstrations by a human teacher who is familiar with the\ntask and tool that the robot aims to use. By comparing two\ntrajectories, each using a separate tool to complete the same task,\nwe aim to model the relationship between the two tooltips\nconstraints such that it can be reused in the context of\nanother task.\nWhile a robot can quickly receive demonstrations (Argall\net al., 2009; Chernova and Thomaz, 2014) using a new tool, these\ndemonstrations may not be sufﬁcient to learn the underlying\ntooltip constraints. Due to the unstructured nature of task\ndemonstrations, the two demonstrations (each provided\nusing a different tool) may vary in ways that do not reﬂect\nhow the task should be adapted based on which tool is used. For\nexample, the teacher may choose a different strategy for\ncompleting the task with the second tool, or the robot may\nbe starting from a new arm conﬁguration when the teacher\ndemonstrates the task with the second tool. For these reasons,\nwe utilize corrections of the robot’s behavior, which have been\nshown to be effective interface for adapting a previously-learned\ntask model (Argall et al., 2010; Sauser et al., 2012; Bajcsy et al.,\n2018). Rather than have the teacher provide a new\ndemonstration using the new tool, the robot attempts to\ncomplete the task on its own and is interrupted and\ncorrected by the teacher throughout its motion. As a result,\nthis interaction results in a series of correction pairs, where each\npair represents the robot’s originally-intended end-effector pose\nand its corresponding, corrected pose that was indicated by the\nteacher.\nOur research questions are as follows:\n1) How can we model a tooltip constraint using data provided\nvia sparse, noisy corrections?\n2) Under what conditions can the tooltip constraints learned\nfrom corrections on one task be used to adapt other task\nmodels to the same replacement tool? What characteristics of\nthe tool and task predict whether a previously-learned tooltip\nconstraint can be applied?\nIn the following sections, we address these research questions\nusing the Transfer by Correction algorithm, which we ﬁrst\ndescribed in Fitzgerald et al. (2019).\n5.1 Problem Deﬁnition\nWe assume that each demonstration consists of a series of\nkeyframes (Akgun et al., 2012). The robot receives corrections\nby executing a trajectory planned using the original task model,\npausing after a time interval deﬁned by the keyframe timings set\nduring the original demonstration. The teacher then moves the\nrobot’s gripper to the correct position, after which the robot\nresumes task execution for the next time interval, repeating the\ncorrection process until the entire task is complete. Each resulting\ncorrection at intervali consists of the original poseC\ni\na (using tool\na) and the corrected poseCi\nb (using new toolb) at keyframei.A\ncollection ofK corrections (one for each ofK keyframes) results in\na K x 2 correction matrix:\nC /equals\nC0\na C0\nb\nC1\na C1\nb\n...\nCK\na CK\nb\n⎡⎢⎢\n⎢⎢⎢\n⎢⎢⎢\n⎢⎢⎢\n⎢⎢⎢\n⎢⎢⎣\n⎤⎥⎥\n⎥⎥⎥\n⎥⎥⎥\n⎥⎥⎥\n⎥⎥⎥\n⎥⎥⎦ (2)\nEach corrected pose C\ni\nb provides a sample of the transfer\nfunction value with the original pose Ci\na at keyframe i as\ninput, plus some amount of error from the optimal correction\npose:\nC\ni\nb /equals ϕb\na Ci\na\n() + ϵ ϵn ∼ N 0, σ2\nn() (3)\nWe assumeϵ is sampled from a Gaussian noise model for each\naxis n ∈ [1... 6] of the 6D end-effector pose. Our aim is to learn a\ntransfer functionϕ that optimally reﬂects the tooltip constraints,\nusing a correction matrixC.\n5.2 Approach: Transfer by Correction\nGiven a task trajectory T for tool a consisting of a series oft\nposes in task space such thatT /equals [p0, p1, ... , pt], we transform\neach pose individually for toolb. Representing an original pose\nfor tool a in terms of its 3× 1 translational vectorta and 4× 1\nrotational vector ra, we transform it into a posepb for toolb as\nfollows:\npb /equals ϕb\na pa() /equals 〈ta + ^t, ra · ^r〉 (4)\nHere, ra · ^r refers to the Hamilton product between the two\nquaternions. The goal is now to estimate the optimal rotational^r\nand translational ^t transformation components from the\ncorrections matrix C, and then apply these transformations to\nthe trajectory T. Our approach addresses this goal by (1)\nmodeling C, particularly the relationship between each\ncorrection’s translational and rotational components, 2)\nsampling a typical translational transformation ^t and\nrotational transformation ^r from this transform model, and 3)\napplying ^t and ^r to transform each pose in the task trajectory\naccording to Equation 4.\n5.3 Task Constraints\nWe observe that corrections indicate constraints of the tooltip’s\nposition and/or orientation, and that these constraints are\nreﬂected in the relationship between the translation and\nrotation components of each correction. Broadly, each\ncorrection may primarily indicate:\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 6742929\nFitzgerald et al. Constraints for Creative Tool Use\n An unconstrained point in the trajectory, and thus should be\nomitted from the tool transform model.\n An orientation constraint, where the rotation of the tooltip (and\nthus the end effector) is constrained more than its position (e.g.,\nhooking a box is constrained more by the orientation of the\nhook than its position, as in the left ofFigure 7).\n A center-of-rotation constraint, where the position of the\ntooltip is constrained more than its rotation (e.g., sweeping a\nsurface with a brush). Note that thetooltip position is the\ncenter of this constraint rather than the end-effector itself,\nand thus the range of valid end-effector poses forms an arc\naround the tooltip, and its orientation remains angled\ntoward the tooltip (e.g.,Figure 7B).\nWe deﬁne two tool transform models , ﬁrst presented in\nFitzgerald et al. (2019), each reﬂecting either orientation or\ncenter-of-rotation constraints. We ﬁt the corrections matrix to\neach tool transform model, using RANSAC (Fischler and Bolles,\n1981) to iteratively estimate the parameters of each model while\ndiscarding outlier and unconstrained correction data points. Each\niteration involves 1) Fitting parameter values to a sample ofn\ndatapoints, 2) Identifying a set of inlier points that alsoﬁt those\nmodel parameters within an error bound ofϵ, and 3) Storing the\nparameter values if the inlier set represents a ratio of the dataset>\nd. The RANSAC algorithm relies on a method for ﬁtting\nparameters to the sample data, and a distance metric for a\ndatapoint based on the model parameters. These are not\ndeﬁned by the RANSAC algorithm, and so we specify the\nparameterization and distance metric according to the tool\ntransform model used, which we describe more in the\nfollowing sections. We deﬁne an additional method to convert\nthe best-ﬁtting parameters following RANSAC completion into a\ntypical transform that can be applied to poses.\n5.4 Linear Tool Transform Model\nBased on theorientation constraint type, weﬁrst consider a linear\nmodel for correction data, where correctionsﬁtting this model\nshare a linear relationship between the translational components of\nthe corrections, while maintaining a constant relationship between\nthe rotational components of corrections (Figure 8A). We model\nthis linear relationship as a series of coefﬁcients obtained by\napplying PCA to reduce the 3D position corrections to a 1D space.\n5.4.1 RANSAC Algorithm Parameters\nThe RANSAC algorithm is performed fork iterations, where we\nuse the estimation\nk /equals log(1.0 − p)\nlog 1.0 − wn() ) (5)\nwith desired conﬁdence p /equals 0.99 and estimated inlier ratiow /equals\n0.5. Additional parameters are as follows:n /equals 2 is the number of\ndata points sampled at each RANSAC iteration,ϵ /equals 0.01 is the\nerror threshold used to determine whether a data pointﬁts the\nFIGURE 7 |Poses meeting the same orientation constraint share similar orientations but vary more in their position(A), whereas poses meeting the same center-of-\nrotation constraint rotate around the tooltip(B).\nFIGURE 8 |Each plot represents one set of corrections for a task. The\nposition of each arrow represents the change in< x, y > position, and points in\nthe direction of the change in orientation introduced by that correction.\nOrientation constraints can be seen in(A), where the majority of\ncorrections on this tool have low variance in their orientation, but higher\nvariance in theirx-y position. Center-of-rotation constraints can be seen in(B),\nwhere the majority of corrections arc around a singular center of rotation, and\norientation is dependent on thex-y position. Unconstrained keyframes\n(colored grey) are located near (0,0).\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429210\nFitzgerald et al. Constraints for Creative Tool Use\nmodel, and d /equals 0.5 is the minimum ratio between inlier and\noutlier data points in order for the model to be retained.\n5.4.2 Model Parameter Fitting\nModel ﬁtting during each iteration of RANSAC consists of\nreducing the datapoints to a 1D model using PCA, returning\nthe mean translational correction and the coefﬁcients for theﬁrst\nprincipal component of the sampleS:\nΘ\nlinear(S)/equals 〈θμ, θu〉 θμ /equals 1\n|S| ∑\np∈S\npt (6)\nwhere pt is the 3 × 1 translational difference indicated by the\ncorrection p, S is the subset of the corrections matrixC sampled\nduring one iteration of RANSAC such thatS ⊂ C, andθu is the\neigenvector corresponding to the largest eigenvalue of the\ncovariance matrix Σ /equals\n1\n|S|ST\nt St.\n5.4.3 Error Function\nEach iteration of RANSAC calculates the total error over all\ndata points ﬁtting that iteration’s model parameters. We deﬁne\nthe error of a single correction datapointp as the sum of its\nreconstruction error and difference from the average orientation\ncorrection, given the current model parametersθ:\nδ\nlinear(p, θ)/equals∥ pt − θμ + pt − θμ()\nT\nθuθT\nu\n+() ∥+ c 1 − /C22qnpT\nn()\n2\n()\n(7)\nwhere x+ indicates the Moore-Penrose pseudo-inverse of a vector,\npn is the unit vector representing the orientation difference\nindicated by the correction p, /C22qn is a unit vector in the\ndirection of the average rotation sampled from the model\n(deﬁned in the next section), andc is the weight assigned to\nrotational error (c /equals 1 in our evaluations).\n5.4.4 Sampling Function\nAfter RANSAC returns the optimal model parameters and\ncorresponding set of inlier points ^I ⊂ C, the rotation and\ntranslation components of the transformation are sampled\nfrom the model. We deﬁne the sampling function according to\nthe estimated “average” rotation /C22q:\nΨ(^I, ^θ)\nlinear /equals 〈/C22q, /C22t〉 /C22q /equals arg max\nq∈S3\nqTMq M /equals 1\n|^I| ∑\np∈^I\npi\nqpi\nq\nT\n(8)\nThe solution to /C22q for this maximization problem is the\neigenvector corresponding to the largest eigenvalue of M\n(Markley et al., 2007) .T h es a m p l et r a n s l a t i o n/C22t is the 3D offset\ncorresponding to the mean value/C22z from the 1D projection space:\n/C22t /equals ^θμ + /C22z^θu\nT+\n/C22z /equals 1\n|^I| ∑\np∈^I\npt − ^θμ()\nT ^θu (9)\n5.5 Rotational Tool Transform Model\nWe now consider a model for corrections reﬂecting a center-of-\nrotation constraint, in which we make the assumption that\ncorrections indicate a constraint over the tool tip ’s position.\nSince the tool tip is offset from the end-effector, the position\nand rotation of the end-effector are constrained by each other\nsuch that the end-effector revolves around the tool tip\n(Figure 8B ). We model this relationship by identifying a\ncenter-of-rotation (and corresponding rotation radius) for the\ntool tip, from which we can sample a valid end-effector position\nand rotation.\n5.5.1 RANSAC Algorithm Parameters\nWe use the same parameters for k, w, d as in the linear\nmodel. We sample n /equals 3 points at each iteration, and use the\nerror threshold ϵ /equals 0.25. We de ﬁne functions for model\nparameterization, error metrics, sampling, and variance in\nthe following sections.\n5.5.2 Model Parameter Fitting\nWe deﬁne the optimal model parameters for each iteration of\nRANSAC as the center-of-rotation (and corresponding rotation\nradius) of that iteration’s samples S:\nΘ\nrotation(S)/equals 〈θc, θr〉 (10)\nwhere θc is the position of the center-of-rotation that\nminimizes its distance from the intersection of lines\nproduced from the position and orientation of each correction\nsample:\nθc /equals arg min\nc\n∑\n|S|\ni/equals 1\nD c; ai, ni() 2 (11)\nwhere ai and ni are the position and unit direction vectors,\nrespectively, for samplei in S:\nai /equals xi,y i,z i[]\nT\nni /equals qi ·[ 0, 1, 0, 0]T() ) · q′ (12)\nHere, q1 · q2 refers to the Hamilton product between two\nquaternions, and q′ is the inverse of the quaternionq:\nq′ /equals[ w, x, y, z]′T /equals[ w, −x, −y, −z]T (13)\nWe solve for the center-of-rotation by adapting a method for\nidentifying the least-squares intersection of linesTraa (2013).W e\nconsider each samplei to be a ray originating at the pointai and\npointing in the direction ofni. The center-of-rotation of a set of\nthese rays is thus the point that minimizes the distance between\nitself and each ray. We deﬁne this distance as the piecewise\nfunction:\nD(c; a, n)/equals ∥(c − a)− d · n∥\n2 if d > 0\n∥c − a∥2 otherwise{ (14)\nwhere d is the distance between a and the projection of the\ncandidate centerpoint c on the ray:\nd /equals( c − a)Tn (15)\nWe solve for θc using the SciPy implementation of the\nLevenberg-Marquardt method for non-linear least-squares\noptimization, supplying Equation 14 as the cost function. We\nthen solve for the radius corresponding toθ\nc:\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429211\nFitzgerald et al. Constraints for Creative Tool Use\nθr /equals 1\n|S| ∑\n|S|\ni/equals 0\n∥ai − θc∥ (16)\n5.5.3 Error Function\nWe deﬁne the error of a single data pointp as its distance from the\ncurrent iteration’s center-of-rotation estimate:\nδrotation(p, θ)/equals D c; ap, np()\ndp\n()\n2\n(17)\nWhere dp is deﬁned in Equation 15.\n5.5.4 Sampling Function\nAfter RANSAC returns the optimal model parameters and\ncorresponding set of inlier points ^I ⊂ C, the rotation\ncomponent of the transformation is ﬁrst sampled using the\n“average” rotation /C22qc from ^θc to all inlier points:\n/C22qc /equals arg max\nq∈S3\nqTMq M /equals 1\n|^I| ∑\np∈^I\nrprT\np (18)\nWhere rp is the quaternion rotation between^θc and the position\nof p,d eﬁned by normalizing the quaternion consisting of the\nscalar and vector parts:\nrp /equals 〈∥a∥2 + baT, bT × a〉 (19)\na /equals pt − ^θc b /equals[ ∥ a∥, 0, 0] (20)\nThe optimal/C22qc is the eigenvector corresponding to the largest\neigenvalue of M; this represents the sampled rotation from^θc.\nWe then sample/C22t by projecting the point at distance^θr from ^θc\nin the direction of/C22qc:\n/C22t /equals ^θc + /C22qc · 0, ^θr, 0, 0[]\nT\n() · /C22qc\n′[]\n1‥3\n(21)\nWhere x1‥3 indicates the 3× 1 vector obtained by ommitting the\nﬁrst element of a 4× 1 vector x. Finally, we return the sample\nconsisting of the translation /C22t and the normalized rotation /C22q\nbetween /C22t and ^θc:\nΨ(^I, ^θ)rotation /equals 〈\n/C22q\n∥/C22q∥,/C22t〉 /C22q /equals 〈^θr∥a∥+ baT,bT × a〉 a /equals ^θc − /C22tb /equals ^θr,0,0][\n(22)\n5.6 Best-Fit Model Selection\nThe linear and rotational tool transform models represent two different\nrelationships between the translational and rotational components of\ncorrections. We now deﬁne a metric for selecting between these two\nmodels based on how well theyﬁt the correction data:\nΨ(C)\nbest−fit /equals Ψ ^Il, ^θl() linear if Δlinear < Δrotation\nΨ ^Ir, ^θr() rotation otherwise\n⎧⎨\n⎩ (23)\nWhere ^Il, ^θl, ^Ir, ^θr represent the optimal inlier points and\nparameter values from the linear and rotational models,\nrespectively. The ﬁt of the linear model is calculated as its\nrange of valuesz projected in the model’s 1D space:\nΔ\nlinear /equals range(z) z /equals pt − ^θμ()\nT ^θu|∀p ∈ ^I{} (24)\nThe ﬁt of the rotational model is calculated as the range of unit\nvectors in the direction of each inlier point as measured from the\ncenter-of-rotation:\nΔ\nrotation /equals 1 − 1\n|^I| ∑\np∈^I\nrp ·[ 0, 1, 0, 0]T() · rp′[] 1‥3\n/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble\n/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble\n2\n(25)\nwhere rp is deﬁned in Equation 19.\n5.7 Evaluation\nWe evaluated the transfer by correction algorithm results on a 7-\nDOF Jaco2 arm equipped with a two-ﬁngered Robotiq 85 gripper\nand mounted vertically on a table-top surface (Figure 9D). Each\nevaluation conﬁguration consisted of one task that was 1)\ndemonstrated using the original, “source” tool, and 2)\ncorrected to accommodate a novel, replacement tool. We\ndescribe data collection for each of these steps in the following\nsections.\n5.8 Demonstrations\nThree tasks ( Figure 9 ) were demonstrated using three\nprototypical, “source” tools (Figures 10A– C), resulting in a\ntotal of nine demonstrations. Demonstrations began with the\narm positioned in an initial conﬁguration, and with the gripper\nalready grasping the tool. Each tool’s grasp remained consistent\nacross all three tasks. Objects on the robot’s workspace were reset\nto the same initial position before every demonstration. We\nprovided demonstrations by indicating keyframes ( Akgun\net al., 2012) along the trajectory, each of which was reached\nby moving the robot’s arm to the intermediate pose. At each\nkeyframe, the 7D end effector pose was recorded; note that this is\nthe pose of the joint holding the tool, andnot the pose of the tool-\ntip itself (since the tool-tip is unknown to the robot). We provided\none keyframe demonstration for each combination of tasks and\nsource tools in this manner, each demonstration consisting of\n7– 12 keyframes (depending on the source tool used) for the\nsweeping task, 10– 11 keyframes (depending on the source tool\nused) for the hooking task, and 7 keyframes for the\nhammering task.\nWe represented each demonstration using a Dynamic\nMovement Primitive (DMP) ( Schaal, 2006 ; Pastor et al.,\n2009). A DMP is trained over a demonstration by\nperturbing a linear spring-damper system according to the\nvelocity and acceleration of the robot ’s end-effector at each\ntime step. By integrating over the DMP, a trajectory can then\nbe generated that begins at the end-effector’s initial position\nand ends at a speciﬁed end point location. Thus, after training\na DMP, the only parameter required to execute the skill is\nthe desired end point location. By parameterizing the end\npoint location of each DMP skill model according to object\nlocations, the overall task can be generalized to accommodate\nnew object conﬁgurations. We re-recorded the demonstration\nif the trained DMP failed to repeat the demonstration task\nwith the source tool.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429212\nFitzgerald et al. Constraints for Creative Tool Use\n5.9 Corrections\nFollowing training, the arm was reset to its initial\nconﬁguration, with the gripper already grasping a new tool\n(Figures 10D,E ). Note that these replacement objects have\nseveral surfaces that could be utilized as a tooltip (depending\non the task). For example, any point along the rim of the mug\n(Figure 10D) would serve as the prototypical tooltip during a\nscooping or pouring task. In the context of the hooking and\nhammering tasks used in our evaluation, however, the bottom\nof the mug serves as a tooltip. Alternatively, the side of the mug\nprovides a broad surface to perform the sweeping task. This\nrange of potential tooltips on a single object highlights the\nbeneﬁt of using corrections to learn task-speci ﬁc tooltips,\nrather than assume that a prototypical tooltip is appropriate\nfor all tasks.\nObjects on the robot’s workspace were reset to the same initial\nposition as in the demonstrations; this allowed us to ensure that\nany corrections were made as a result of the change in tool, rather\nthan changes in object positions. The learned model was then\nused to plan a trajectory in task-space, which was then converted\ninto a joint-space trajectory using TracIK (Beeson and Ames,\n2015) and executed, pausing at intervals deﬁned by the keyframe\ntiming used in the original demonstration. When execution was\npaused, it remained paused until the arm pose was conﬁrmed. If\nno correction was necessary, the pose was con ﬁrmed\nimmediately; otherwise, the arm pose was ﬁrst corrected by\nmoving the arm to the correct position. Note that this form of\ncorrections assumes that each keyframe constitutes a statically\nstable state. For tasks involving unstable states, another form of\ninteraction may be used to provide post-hoc corrections, such as\ncritiques (Cui and Niekum, 2018).\nTwo poses were recorded for each correction: 1) the original\nend-effector pose the arm attempted to reach (regardless of\nwhether the goal pose was reachable with the new tool), and\n2) the end-effector pose following conﬁrmation (regardless of\nwhether a correction was given). Trajectory execution then\nresumed from the arm’s current pose, following the original\ntask-space trajectory so that pose corrections were not\npropagated to the rest of the trajectory. This process\ncontinued until all keyframes were corrected and executed,\nresulting in the correction matrixC (Equation 2).\n5.10 Measures\nFor each transfer execution, we measured performance according\nto a metric speciﬁc to the task:\n Sweeping: The number of pom-poms swept off the surface of\nthe yellow box.\n Hooking: The ﬁnal distance between the box ’s target\nposition and the closest edge of the box (measured in\ncentimeters).\n Hammering: A binary metric of whether the peg was pressed\nany lower from its initial position.\n5.11 Results\nWe highlight two categories of results:Within-task and across-\ntask performance.\n5.11.1 Within-Task Transfer\nWithin-taskperformance measures the algorithm’s ability to model\nthe corrections and perform the corrected task successfully. Transfer\nwas performed using the transform model learned from corrections\nFIGURE 9 | (A)hooking task, (B) sweeping task, (C) hammering task, and(D) the experimental setting.\nFIGURE 10 |Tools (A–C) were used to demonstrate the three tasks shown inFigure 9, later transferred to use tools(D,E). These tools exhibit a wide range of\ngrasps, orientations, dimensions, and tooltip surfaces.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429213\nFitzgerald et al. Constraints for Creative Tool Use\non that same tool-task pairing. For example, for the sweeping task\nmodel learned using the hammer, corrections were provided on the\nreplacement tool (e.g., a mug) and then used to perform the sweeping\ntask using that same mug. For each source tool, we evaluated\nperformance on all three tasks using each of the two replacement\nobjects, resulting in 18 sets of corrections (one for each combination of\ntask, source tool, and replacement tool) per tool transform model\n(linear and rotational).\nUsing the better-performing model resulted in ≥ 85% of\nmaximum task performance in 83% of cases. The better-\nperforming model was selected using the best- ﬁt metric in\n72% of cases. Figure 11 lists the percentage of transfer\nexecutions (using the best- ﬁt model) that achieve multiple\nperformance thresholds, where best-ﬁt results were recorded as\nthe performance of the model returned byEquation 23.\nWe scaled the result of each transfer execution between 0 and\n1, with 0 representing the initial state of the task and 1\nrepresenting maximum performance according to the metrics\nin Section 5.10. Figure 12reports the performance distribution\naggregated over all tasks, transferred from each of the three\nsource tools to either the scrub-brush (Figure 10E, results in\nFigure 12A ) or mug (pictured in Figure 10D , results in\nFigure 12B) as the replacement tool. The mean performance\nresults are reported inFigure 13A, with darker cells indicating\nbetter performance. Overall, the transform returned using the\nbest-ﬁt metric resulted in average performance of 6.9x and 5.9x\nthat of the untransformed trajectory when using the scrub-brush\nand mug, respectively, as replacement tools.\n5.11.2 Across-Task Transfer\nAcross-task transfer performance measures the generalizability of\ncorrections learned on one task when applied to adifferent task\nusing the same tool, without having received any corrections on that\ntool-task pairing. For example, the hooking task was learned using the\nhammer, and transferred to the mug using corrections obtained on the\nsweeping task. We evaluated 36 total transfer executions (one per\ncombination of demonstration task, source tool, correction task\n(distinct from the demonstration task), and replacement tool) per\ntool transform model (linear and rotational).\nFigure 14 reports the performance distribution aggregated\nover all tasks, transferred from each of the three source tools to\neither the scrub-brush (Figure 14A) or mug (Figure 13B) as the\nreplacement tool. The mean performance results are reported in\nFigure 13B, with darker cells indicating better performance.\nOverall, the transform returned using the best- ﬁt metric\nresulted in average performance of 1.6x and 0.94x that of the\nuntransformed trajectory when using the scrub-brush and mug,\nrespectively, as replacement tools. The performance distribution\nis improved when using the transform learned from corrections,\nresulting in 2.25x as many task executions achieving≥ 25% of\nmaximum task performance.\nIn order to understand the conditions under which a\ntransform can be reused successfully in the context of another\ntask, we also report the mean performance results for a subset of\nthe across-task executions (Figure 13C). This subset consists of\nonly the task executions where the relative orientation is the same\nbetween 1) the source tool’s tooltips used for the source and target\nFIGURE 11 |Percentage of within-task transfer executions (selected by\nbest-ﬁt model) and untransformed trajectories achieving various performance\nthresholds (deﬁned as the % of maximum performance metric for that task,\ndescribed in Section 5.10). Our proposed models result in a higher\npercentage of transfer executions that complete the task to a high\nperformance threshold (e.g., sweeping≥ 85% of the objects off the table).\nFurthermore, while the untransformed baseline produces all-or-nothing\nperformance behavior, our models degrade gracefully, resulting in partial task\ncompletion (represented by lower % performance thresholds) even when the\nlearned transform is non-optimal.\nFIGURE 12 |Aggregate performance results for within-task transfer using the scrub-brush(A) and mug(B) as the replacement tool. Performance was measured\nfor each task according to the metrics inSection 5.10, and are scaled between 0– 1. These results highlight the need for multiple tool transform models; while both\nmodels greatly outperform the baseline task performance (when no transform is used), note that neither model results in thebest performance over all tasks and\nreplacement tools. Using the best-ﬁt metric to select the more appropriate model for each tool-task pairing resulted in the best overall performance.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429214\nFitzgerald et al. Constraints for Creative Tool Use\ntasks and 2) the replacement tool’s tooltips used for the same two\ntasks. This subset consisted of 10 executions for the scrub-brush,\nand 12 for the mug. Overall, for this subset of executions, the\ntransform returned using the best-ﬁt metric resulted in average\nperformance of 12.6 x and 1.7 x that of the untransformed\ntrajectory when using the scrub-brush and mug, respectively,\nas replacement tools.\n5.12 Discussion\nOur within-task transfer evaluation tested whether we can model\nthe transform between two tools in the context of the same task\n(represented by the solid blue arrow in Figure 15 ) using\ncorrections. Our results indicate that one round of corrections\ntypically is sufﬁcient to indicate this relationship between tools;\ncollectively, the linear and rotational models achieved≥ 85% of\nmaximum task performance in 83% of cases. Individually, the\nmodels selected by the best-ﬁt metric achieved this performance\nthreshold in 72% of cases. This indicates that, in general, theﬁto f\nthe model itself can be used to indicate the relationship between\nend-effector position and orientation for a given tool/task\ncombination.\nAside from analyzing high task performance, we are also\ninterested in whether our approach enables graceful\ndegradation; even if the robot is unable to complete the task\nfully with a new tool, ideally it will still have learned a transform\nthat enablespartial completion of the task. The results shown in\nFigure 11demonstrate that Transfer by Correction offers robust\nbehavior such that even when it results in sub-optimal\nperformance, it still meets lower performance thresholds in\nnearly 90% of cases. In contrast, the untransformed baseline\ndoes not meet lower performance thresholds, and thus produces\nall-or-nothing results that lack robustness.\nThe primary beneﬁt of modeling corrections (as opposed to\nre-learning the task for the new tool) is two-fold: First, the robot\nlearns a transformation that reﬂects how the task has changed in\nresponse to the new tool, which is potentially generalizable to\nother tasks (as we discuss next). We hypothesize that in future\nwork, this learned transform could be parameterized by features\nof the tool (after corrections on multiple tools). Second, since we\ndo not change the underlying task model, but instead apply the\nlearned transform to the resulting trajectory, the underlying task\nmodel is left unchanged. We expect that this efﬁciency beneﬁt\nFIGURE 13 |Mean performance of(A) within-task and(B) across-task transfer to the brush and mug replacement tools over all 18 transfer executions for each tool.\n(C) Mean performance of across-task transfer to the brush and mug replacement tools over the subset of transfer executions in which the transformation between\nsource and correction tasks is similar for the source and replacement tool (10 executions for the brush, 12 for the mug). Darker cells indicate higher average performance.\nFIGURE 14 |Results for across-task transfer using the scrub-brush(A) and mug(B) as the replacement tool. Performance was measured according to the metrics\nin Section 5.10, scaled between 0– 1. These results represent the generalizability of a transform model learned on one task and then applied to a different task using the\nsame tool. Each point represents the performance of a single transfer execution.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429215\nFitzgerald et al. Constraints for Creative Tool Use\nwould be most evident when transferring a more complex task\nmodel trained over many demonstrations; rather than require\nmore demonstrations with the new tool in order to re-train the\ntask model, the transform would be applied to the result of the\nalready-trained model.\nWe have also explored how well this transform generalizes to\nother tasks. Different tooltips on the same tool may be used to\nachieve different tasks, such as how the end and base of the\npaintbrush are used to perform sweeping and hammering tasks,\nrespectively, inFigure 15. While we do not explicitly model the\nrelationship between tooltips on the same tool (represented by the\ntop grey arrow inFigure 15), they are inherent to the learned task\nmodels. A similar relationship exists for the replacement tool\n(represented by the bottom grey arrow inFigure 15). Our across-\ntask evaluation seeks to answer whether the relationship between\ntools in the context of theﬁrst task (solid blue arrow) can be\nreused for a second task (represented by the dashed blue arrow)\nwithout having received any corrections on that tool/task\ncombination (tool 2 and task 2). While we see lower\nperformance in across-task evaluations compared to the\nwithin-task evaluations, it does improve transfer in 27.8% of\nacross-task transfer executions (in comparison to the\nuntransformed trajectory).\nIn the general case, our results also indicate that we cannot\nnecessarily reuse the learned transformation on additional tasks,\nas average performance in across-task transfer is slightly worse\nthan that of the untransformed trajectory when the mug is used as\na replacement tool. This presents the question: Given a transform\nbetween two tools in the context of one task, under what\nconditions can that transform be reused in the context of\nanother task without additional corrections or training?W ed o\nsee that across-task performance is best when considering only\nthe subset of cases where the relationship between the tooltips\nused in either task is similar for the source and replacement tools\n(in our evaluation, this is 10 of 18 executions using the brush, and\n12 of 18 executions using the mug). Within this subset, across-\ntask transfer improves performance in 41% of transfer executions.\nFrom this we draw two conclusions: 1) the transform applied to a\ntool is contextually dependent on the source task, target task, and\ntooltips of the source and replacement tool, and 2) a transform\ncan be reused when the relationship between tooltips used in\neither task is similar for the source and replacement tools.\nOverall, our evaluation resulted in the following keyﬁndings:\nInsight #1: Corrections provide a sample of theconstrained\ntransform between the tooltip and the robot’s end-effector. This\nunderlying constraint is task-dependent; our best- ﬁt model\nresults indicate that multiple constraint types should be\nmodeled and evaluated for each task , with the best- ﬁtting\nmodel used to produce theﬁnal transform output.\nInsight #2:While the tooltip transform is task-speciﬁc, it can\nbe applied to additional tasks under certain conditions. This is\ndependent on a second transform: the transform between\nFIGURE 15 |Corrections indicate the transform from tool 1 to tool 2 for the same task (indicated by the solid blue arrow). Our within-task transfer evaluation tested\nwhether we can use corrections to sufﬁciently model this relationship. Different tasks may use different tooltips from the same tool (such as the different tooltips used to\ncomplete tasks 1 and 2). Our across-task evaluation tests whether the transform learned from corrections (solid blue arrow) can be reused as the transform between the\ntwo tools for another task (indicated by the dashed blue arrow).\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429216\nFitzgerald et al. Constraints for Creative Tool Use\nmultiple tooltips on the same tool.A tooltip transform can be\nreused for an additional task when the transform between the\ntooltips used to complete 1) the corrected task and 2) the\nadditional task are similar for the two tools.\n6 CONCLUSION\nTool use is a hallmark of human cognition and tool improvisation\nis a characteristic of human creativity. As robots enter human\nsociety, we expect human-like tool improvisation from robots as\nwell. This paper makes three contributions to robot creativity in\nusing novel tools to accomplish everyday tasks. First, it presents a\nhigh-level decomposition of the task of tool improvisation into a\nprocess of tool exploration, tool evaluation, and adaptation of task\nmodels to the novel tool. Second, it demonstrates the importance of\ntooltip constraintsin guiding successful tool use throughout this\nprocess. Third, it describes a method of learning by correction:\nrepeating a known task with an unknown tool in order to record a\nhuman teacher’sc o r r e c t i o n so ft h er o b o t’sm o t i o n .\nWe focused on how the relationship between the robot’s\ngripper and the tooltip dictates how the robot’s action model\nshould be adapted to the new tool. A challenge in identifying this\nrelationship is that 1) there are many candidate tooltips on each\ntool, and 2) for each tooltip, there exists a one-to-many\nrelationship between the tooltip and end-effector poses that\nfulﬁll the tooltip constraint.\nIn this paper, we validated this one-to-many mapping through\na simulated experiment in which we demonstrate a relationship\nbetween pose variations and task performance. Our experimental\nresults indicate that the sensitivity of tooltip constraints depends\non the surface of the tool being used, and that as the tool pose\ndeviates from these constraints, the resulting effect on task\nperformance is nonlinear.\nWe then examined the opposite mapping: A many-to-one\nmapping between pose feedback provided by a human teacher,\nand the optimal, underlying tooltip constraint. We developed the\nLearning by Correction algorithm, and demonstrated that a\nhuman teacher can indicate the tooltip constraints for a\nspeciﬁc tool-task pairing by correcting the robot ’s motion\nwhen using the new tool. We modeled the underlying tooltip\nconstraint in two ways, using a linear and rotation model, and\nalso present a metric for choosing the better-ﬁtting model for a set\nof corrections. We demonstrated how this model of the tooltip\nconstraint can then be used to successfully plan and execute the\ntask using that tool with high task performance in 83% of task\nexecusions. We also explored how this tooltip constraint model\ncan be generalized to additional tasks using the same novel tool,\nwithout requiring any additional training data.\nOverall, we expect that a focus on identifying novel tools,\nevaluating novel tools, and adapting task models to novel tools\nin accordance to tooltip constraints is essential for enabling creative\ntool use. Our results indicate that successful task adaptation for a\nnew tool is dependent on the tool’s usage within that task, and that\nthe transform model learned from interactive corrections can be\ngeneralized to other tasks providing a similar context for the new\ntool. Put together, these results provide a process account of robot\ncreativity in tool use (tool identiﬁcation, evaluation and adaptation),\na content account (highlighting the importance of tooltips), as well\nas an algorithmic account of learning by correction.\n6.1 Open Questions\nIn this paper, we have presented a corrections-based approach to\nsampling and modeling the transform resulting from a tool\nreplacement. In doing so, we model a single,static transform\nfor a particular tool/task pairing. We have evaluated how well this\nmodel transfers to other tasks using the same tool replacement.\nAn extension of this work would consider transfer acrosstools.\nWe envision that a robot could not only model the transform\nsamples obtained by interactive corrections, but also learn to\ngeneralize that model to other, similar tools. For example, after\nreceiving corrections for one ladle for a scooping task, the robot\nwould ideally be able to model those corrections such that it\nwould apply to ladles of different shapes or proportions as well.\nWe anticipate that a robot could learn an underlying relationship\nbetween visual object features (such as dimensions or concavity)\nand the resulting transform for that tool.\nMeta-learning has been successfully applied to learning\nproblems in computer vision domains and fully-simulated\nreinforcement learning problems (Duan et al., 2017; Chelsea\net al., 2017). When applied to the domain of tool transfer,\nmeta-learning would ideally enable a robot to use extensive\nbackground training to learn the common relationships between\nvisual features and tooltips that are shared by tools within their\nrespective categories (e.g., cups, knives, scoops). When presented\nwith a novel category of tools, the robot would then only need\ndemonstrations using a small number of tools within the new\ncategory in order to learn the relationship between visual features\nand tooltips within that category. However, as demonstrated in this\npaper, tooltips are task-speciﬁc; within a single tool, the tooltip used\nto complete one task (e.g., the surface of a hammer used to hammer\na nail) is not necessarily the same as the tooltip used to complete\nanother task (e.g., the side of the hammer may be used to sweep\nobjects off a surface, or the claw-end of the hammer may be used to\nremove a nail). This lack of task-speciﬁc training data presents a\nchallenge for future work, as relying on a dataset containing a\nsingle, canonical tooltip for each tool would fail to capture the task-\ncontextual nature of tool use.\nFinally, this paper has explored one method of interaction to\nenable a human teacher to provide corrections to the robot.\nHowever, in human-in-the-loop learning problems, the ideal\ninteraction type is dependent on the teacher ’sr o l ei nt h e\nlearning system, and the context in which the robot is used\n(Cui et al., 2021). For example, the teacher may not have time\nto correct every step of the robot’s action, or may instead prefer to\nprovide corrections only after the robot has tried and failed to\ncomplete a task. We anticipate that future work may enable a robot\nto obtain correction data from a broader set of interaction types.\nDATA AVAILABILITY STATEMENT\nThe raw data supporting the conclusion of this article will be\nmade available by the authors, without undue reservation.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429217\nFitzgerald et al. Constraints for Creative Tool Use\nAUTHOR CONTRIBUTIONS\nThis paper is based on the PhD dissertation of TF, with AG and\nAT as advisors.\nFUNDING\nThis material is based on work supported in part by Ofﬁce of\nNaval Research grants N00014-18-1-2503 and N00014-14-1-\n0120, and the IBM PhD Fellowship.\nACKNOWLEDGMENTS\nSection 5is based upon the authors’ previously published work\nat the International Conference on Autonomous Agents and\nMultiagent Systems ( Fitzgerald et al., 2019 ). The authors\nwould like to thank Jinqi Chen for conﬁguring the simulated\nrobot environment and tasks, and for her insights in analyzing the\nsimulated dataset. We would also like to thank Elaine Short for\nher valuable insights in designing the tool transform models, and\nSonia Chernova for many helpful discussions in planning the\nphysical robot evaluation.\nREFERENCES\nAgostini, A., Aein, M. J., Szedmak, S., Aksoy, E. E., Piater, J., and Würgütter, F.\n(2015). “Using Structural Bootstrapping for Object Substitution in Robotic\nExecutions of Human-like Manipulation Tasks, ” in 2015 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\nHamburg, Germany, September 2015 (IEEE), 6479 – 6486. doi:10.1109/\niros.2015.7354303\nAkgun, B., Cakmak, M., Jiang, K., and Thomaz, A. L. (2012). Keyframe-based\nLearning from Demonstration. Int. J. Soc. Robot. 4, 343– 355. doi:10.1007/\ns12369-012-0160-0\nArgall, B. D., Chernova, S., Veloso, M., and Browning, B. (2009). A Survey of Robot\nLearning from Demonstration.Robot. Auton. Syst.57, 469– 483. doi:10.1016/\nj.robot.2008.10.024\nA r g a l l ,B .D . ,S a u s e r ,E .L . ,a n dB i l l a r d ,A .G .( 2 0 1 0 ) .“Tactile Guidance for Policy\nReﬁnement and Reuse,” in 2010 IEEE 9th International Conference on Development\nand Learning (ICDL), Ann Arbor, Michigan, August 2010 (IEEE), 7 – 12.\ndoi:10.1109/devlrn.2010.5578872\nBajcsy, A., Losey, D. P., O’Malley, M. K., and Dragan, A. D. (2018).“Learning from\nPhysical Human Corrections, One Feature at a Time,” in Proceedings of the\n2018 ACM/IEEE International Conference on Human-Robot Interaction\n(ACM), Chicago, Illinois, March 2018, 141– 149. doi:10.1145/3171221.3171267\nBeeson, P., and Ames, B. (2015).“Trac-Ik: An Open-Source Library for Improved\nSolving of Generic Inverse Kinematics,” in 2015 IEEE-RAS 15th International\nConference on Humanoid Robots (Humanoids), Seoul, South Korea,\nNovember 2015 (IEEE), 928– 935. doi:10.1109/humanoids.2015.7363472\nBird, J., and Stokes, D. (2006). “Evolving Minimally Creative Robots, ” in\nProceedings of the Third Joint Workshop on Computational Creativity,\nRiva del Garda, Italy, August 2006 (Amsterdam: IOS Press), 1– 5.\nBrown, S., and Sammut, C. (2012). “Tool Use and Learning in Robots,” in\nEncyclopedia of the Sciences of Learning(Springer), 3327– 3330. doi:10.1007/\n978-1-4419-1428-6_1652\nChelsea, F., Yu, T., Zhang, T., Abbeel, P., and Levine, S. (2017).One-shot Visual\nImitation Learning via Meta-Learning . Mountain View, California: arXiv.\npreprint arXiv:1709.04905.\nChernova, S., and Thomaz, A. L. (2014). Robot Learning from Human Teachers.\nSynth. Lectures Artif. Intell. Machine Learn. 8, 1 – 121. doi:10.2200/\ns00568ed1v01y201402aim028\nChoi, D., Langley, P., and To, S. T. (2018).“Creating and Using Tools in a Hybrid\nCognitive Architecture,” in AAAI Spring Symposia, Palo Alto, California,\nMarch 2018.\nColeman, D.,Șu c a n ,I .A . ,C h i t t a ,S . ,a n dC o r r e l l ,N .( 2 0 1 4 ) .R e d u c i n gt h eB a r r i e rt o\nEntry of Complex Robotic Software: A Movelt! Case Study.J. Software Eng. Robotics\n5( 1 ) ,3– 16. doi:10.6092/JOSER_2014_05_01_p3\nCui, Y., Koppol, P., Admoni, H., Niekum, S., Simmons, R., Steinfeld, A., et al.\n(2021). “Understanding the Relationship between Interactions and Outcomes\nin Human-In-The-Loop Machine Learning,” in Proceedings of the Thirtieth\nInternational Joint Conference on Arti ﬁcial Intelligence, Montreal, QC,\nCanada. doi:10.24963/ijcai.2021/599\nCui, Y., and Niekum, S. (2018).“Active Reward Learning from Critiques,” in 2018\nIEEE international conference on robotics and automation (ICRA), Brisbane,\nQLD, Australia, May 2018 (IEEE), 6907– 6914. doi:10.1109/icra.2018.8460854\nDabbeeru, M. M., and Mukerjee, A. (2011). Discovering Implicit Constraints in\nDesign. Aiedam 25, 57– 75. doi:10.1017/s0890060410000478\nDo, T.-T., Nguyen, A., and Reid, I. (2018).“Affordancenet: An End-To-End Deep\nLearning Approach for Object Affordance Detection,” in 2018 IEEE international\nconference on robotics and automation (ICRA), Brisbane, QLD, Australia, May\n2018 (IEEE), 5882– 5889. doi:10.1109/icra.2018.8460902\nDuan, Y., Andrychowicz, M., Stadie, B., Ho, O. J., Schneider, J., Sutskever, I., et al.\n(2017). “One-shot Imitation Learning,” in Advances in Neural Information\nProcessing SystemsLong Beach, California: Curran Associates, Inc., 1087– 1098.\nDym, C. L., and Brown, D. C. (2012).Engineering Design: Representation and\nReasoning. Cambridge University Press.\nF a n g ,K . ,Z h u ,Y . ,G a r g ,A . ,K u r e n k o v ,A . ,M e h t a ,V . ,F e i - F e i ,L . ,e ta l .( 2 0 1 8 ) .“Learning\nTask-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision,”\nin Proceedings of Robotics: Science andSystems, Pittsburgh, Pennsylvania, June\n2018 (Pittsburgh, Pennsylvania). doi:10.15607/RSS.2018.XIV.012\nFauconnier, G., and Turner, M. (2008).The Way We Think: Conceptual Blending\nand the Mind’s Hidden Complexities. Basic Books.\nFischler, M. A., and Bolles, R. C. (1981). Random Sample Consensus.Commun.\nACM 24, 381– 395. doi:10.1145/358669.358692\nFitzgerald, T., Goel, A., and Thomaz, A. (2018). Human-guided Object Mapping\nfor Task Transfer.J. Hum. Robot. Interact.7, 1– 24. doi:10.1145/3277905\nFitzgerald, T., Goel, A., and Thomaz, A. (2017).“Human-robot Co-creativity: Task\nTransfer on a Spectrum of Similarity, ” in International Conference on\nComputational Creativity (ICCC), Atlanta, Georgia, June 2017.\nFitzgerald, T., Short, E., Goel, A., and Thomaz, A. (2019). “Human-guided\nTrajectory Adaptation for Tool Transfer,” in International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS), Montreal, Quebec,\nCanada, May 2019 (International Foundation for Autonomous Agents and\nMultiagent Systems), 1350– 1358.\nFu, J., Levine, S., and Abbeel, P. (2016).“One-shot Learning of Manipulation Skills\nwith Online Dynamics Adaptation and Neural Network Priors,” in 2016 IEEE/\nRSJ International Conference onIntelligent Robots and Systems (IROS),\nDaejeon, South Korea, October 2016 (IEEE), 4019 –\n4026. doi:10.1109/\niros.2016.7759592\nGajewski, P., Ferreira, P., Bartels, G., Wang, C., Guerin, F., Indurkhya, B., et al.\n(2018). “Adapting Everyday Manipulation Skills to Varied Scenarios,” in 2019\nInternational Conference on Robotics and Automation (ICRA), Montreal, QC,\nCanada, May 2019. IEEE, 1345– 1351.\nGibson, J. J. (1979). The Ecological Approach to Visual Perception Boston:\nHoughton Mifﬂin.\nGoel, A. K. (1997). Design, Analogy, and Creativity.IEEE Expert 12, 62– 70.\nGoel, A. K., Fitzgerald, T., and Parashar, P. (2020).“Analogy and Metareasoning:\nCognitive Strategies for Robot Learning,” in Human-Machine Shared Contexts\n(Elsevier), 23– 44. doi:10.1016/b978-0-12-820543-3.00002-x\nGopinath, D., and Weinberg, G. (2016). A Generative Physical Model Approach\nfor Enhancing the Stroke Palette for Robotic Drummers.Robot. Auton. Syst.86,\n207– 215. doi:10.1016/j.robot.2016.08.020\nGubenko, A., Kirsch, C., Smilek, J. N., Lubart, T., and Houssemand, C. (2021).\nEducational Robotics and Robot Creativity: An Interdisciplinary Dialogue.\nFront. Robot. AI8, 178. doi:10.3389/frobt.2021.662030\nHoffmann, H., Chen, Z., Earl, D., Mitchell, D., Salemi, B., and Sinapov, J. (2014).\nAdaptive Robotic Tool Use under Variable Grasps.Robot. Auton. Syst. 62,\n833– 846. doi:10.1016/j.robot.2014.02.001\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429218\nFitzgerald et al. Constraints for Creative Tool Use\nHoukes, W., and Vermaas, P. E. (2010).Technical Functions: On the Use and\nDesign of Artefacts, Vol. 1. Springer Science & Business Media.\nKemp, C. C., and Edsinger, A. (2006).“Robot Manipulation of Human Tools:\nAutonomous Detection and Control of Task Relevant Features,” in Proceedings\nof the Fifth International Conference on Development and Learning,\nBloomington, Indiana, June 2006.\nKemp, C., Edsinger, A., and Torres-Jara, E. (2007). Challenges for Robot\nManipulation in Human Environments [grand Challenges of Robotics].\nIEEE Robot. Automat. Mag.14, 20– 29. doi:10.1109/mra.2007.339604\nKillian, T. W., Daulton, S., Konidaris, G., and Doshi-Velez, F. (2017).“Robust and\nEfﬁcient Transfer Learning with Hidden Parameter Markov Decision\nProcesses,” in Advances in Neural Information Processing Systems San\nFrancisco, California: Curran Associates, Inc., 6250– 6261.\nKroemer, O., Ugur, E., Oztop, E., and Peters, J. (2012).“A Kernel-Based Approach\nto Direct Action Perception, ” in 2012 IEEE international Conference on\nRobotics and Automation, Saint Paul, Minnesota, May 2012, (IEEE),\n2605– 2610. doi:10.1109/icra.2012.6224957\nLevihn, M., and Stilman, M. (2014). “Using Environment Objects as Tools:\nUnconventional Door Opening,” in 2014 IEEE/RSJ International Conference\non Intelligent Robots and Systems, Chicago, Illinois, September 2014 (IEEE),\n2502– 2508. doi:10.1109/iros.2014.6942903\nMarkley, F. L., Cheng, Y., Crassidis, J. L., and Oshman, Y. (2007). Averaging\nQuaternions. J. Guidance Control Dyn.30, 1193– 1197. doi:10.2514/1.28949\nMyers, A., Teo, C. L., Fermüller, C., and Aloimonos, Y. (2015). “Affordance\nDetection of Tool Parts from Geometric Features,” in IEEE International\nConference on Robotics and Automation (ICRA), Seattle, Washington, May\n2015 (IEEE), 1374– 1381. doi:10.1109/icra.2015.7139369\nNair, L., and Chernova, S. (2020). Feature Guided Search for Creative Problem\nSolving through Tool Construction. Front. Robot. AI 7, 205. doi:10.3389/\nfrobt.2020.592382\nNair, L., Srikanth, N. S., Erickson, Z. M., and Chernova, S. (2019).“Autonomous\nTool Construction Using Part Shape and Attachment Prediction, ” in\nProceedings of Robotics: Science and Systems (RSS’19), Freiburg, Germany,\nJune 2019. doi:10.15607/rss.2019.xv.009\nPastor, P., Hoffmann, H., Asfour, T., and Schaal, S. (2009). “Learning and\nGeneralization of Motor Skills by Learning from Demonstration, ” in\nIEEE International Conference o n Robotics and Automation, 2009.\nICRA ’09, Kobe, Japan, May 2009 (IEEE), 763 – 768. doi:10.1109/\nrobot.2009.5152385\nPenn, D. C., Holyoak, K. J., and Povinelli, D. J. (2008). Darwin ’s Mistake:\nExplaining the Discontinuity between Human and Nonhuman Minds.\nBehav. Brain Sci.31, 109– 130. doi:10.1017/s0140525x08003543\nRozo, L., Jiménez, P., and Torras, C. (2013). A Robot Learning from Demonstration\nFramework to Perform Force-Based Manipulation Tasks.\nIntel Serv. Robot.6,\n33– 51. doi:10.1007/s11370-012-0128-9\nSarathy, V., and Scheutz, M. (2018). Macgyver Problems: Ai Challenges for Testing\nResourcefulness and Creativity.Adv. Cogn. Syst.6, 31– 44.\nSauser, E. L., Argall, B. D., Metta, G., and Billard, A. G. (2012). Iterative Learning of\nGrasp Adaptation through Human Corrections.Robot. Auton. Syst.60, 55– 71.\ndoi:10.1016/j.robot.2011.08.012\nSchaal, S. (2006). “Dynamic Movement Primitives-A Framework for Motor\nControl in Humans and Humanoid Robotics, ” in Adaptive Motion of\nAnimals and Machines(Springer), 261– 280.\nSchubert, A., and Mombaur, K. (2013).“The Role of Motion Dynamics in Abstract\nPainting,” in Proceedings of the Fourth International Conference on\nComputational Creativity (Citeseer), Sydney, Australia, June 2013.\nSinapov, J., and Stoytchev, A. (2008).“Detecting the Functional Similarities between\nTools Using a Hierarchical Representation of Outcomes,” in 7th IEEE International\nConference on Development and Learning, ICDL 2008, Monterey, California,\nAugust 2008 (IEEE), 91– 96. doi:10.1109/devlrn.2008.4640811\nSrinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018). “Universal\nPlanning Networks,” in Proceedings of the 35th International Conference on\nMachine Learning, Stockholm, Sweden. arXiv. preprint arXiv:1804.00645.\nTaylor, M. E., and Stone, P. (2009). Transfer Learning for Reinforcement Learning\nDomains: A Survey. J. Machine Learn. Res. 10, 1633– 1685. doi:10.5555/\n1577069.1755839\nTraa, J. (2013).Least-squares Intersection of Lines. Illinois: UIUC.\nVaesen, K. (2012). The Cognitive Bases of Human Tool Use.Behav. Brain Sci.35,\n203. doi:10.1017/s0140525x11001452\nVigorito, C. M., and Barto, A. G. (2008).“Hierarchical Representations of Behavior\nfor Efﬁcient Creative Search,” in AAAI Spring Symposium: Creative Intelligent\nSystems, Palo Alto, California, March 2008, 135– 141.\nYannakakis, G. N., Liapis, A., and Alexopoulos, C. (2014).Mixed-initiative Co-\ncreativity Fort Lauderdale, Florida: International Conference on the\nFoundations of Digital Games.\nConﬂict of Interest:The authors declare that this study received funding from\nIBM in the form of a PhD Fellowship. The funder was not involved in the study\ndesign, collection, analysis, interpretation of data, the writing of this article or the\ndecision to submit it for publication.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2021 Fitzgerald , Goel and Thomaz . This is an open-access article\ndistributed under the terms of the Creative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other forums is permitted, provided the\noriginal author(s) and the copyright owner(s) are credited and that the original\npublication in this journal is cited, in accordance with accepted academic practice.\nNo use, distribution or reproduction is permitted which does not comply with\nthese terms.\nFrontiers in Robotics and AI | www.frontiersin.org November 2021 | Volume 8 | Article 67429219\nFitzgerald et al. Constraints for Creative Tool Use",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.821219801902771
    },
    {
      "name": "Task (project management)",
      "score": 0.7640846967697144
    },
    {
      "name": "Human–computer interaction",
      "score": 0.7076866626739502
    },
    {
      "name": "Robot",
      "score": 0.7050918340682983
    },
    {
      "name": "Improvisation",
      "score": 0.6519246697425842
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6445131301879883
    },
    {
      "name": "Process (computing)",
      "score": 0.5813534259796143
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5747547149658203
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48752066493034363
    },
    {
      "name": "Identification (biology)",
      "score": 0.47170019149780273
    },
    {
      "name": "Creativity",
      "score": 0.4532027840614319
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 8
}