{
  "title": "Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task",
  "url": "https://openalex.org/W4389523947",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2115840838",
      "name": "Chunhui Du",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2232498490",
      "name": "Jidong Tian",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2803615037",
      "name": "Liao Haoran",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4214366645",
      "name": "Jindou Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2109788442",
      "name": "Hao He",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107094063",
      "name": "Yaohui Jin",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2460159515",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4378465262",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4385571789",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W4389518664",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4321855256",
    "https://openalex.org/W4385571671",
    "https://openalex.org/W4366565380"
  ],
  "abstract": "Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations. Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations. Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings. Besides, with task-level thinking steps, automatically generated chain-of-thoughts (CoTs) bring more competitive performance.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2454–2470\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAutomatic Task-Level Thinking Steps Help Large Language Models for\nChallenging Classification Task\nChunhui Du, Jidong Tian, Haoran Liao, Jindou Chen, Hao He, Yaohui Jin\nMoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n{chunhui18, frank92, liaohaoran, goldenbean, hehao, jinyh}@sjtu.edu.cn\nAbstract\nLarge language models (LLMs) have shown\nincredible performance on many tasks such as\ndialogue generation, commonsense reasoning\nand question answering. In-context learning\n(ICL) is an important paradigm for adapting\nLLMs to the downstream tasks by prompting\nfew demonstrations. However, the distribution\nof demonstrations can severely affect the per-\nformance, especially for challenging classifi-\ncation tasks. In this paper, we propose the\nconcept of task-level thinking steps that can\neliminate bias introduced by demonstrations.\nFurther, to help LLMs distinguish confusing\nclasses, we design a progressive revision frame-\nwork, which can improve the thinking steps by\ncorrecting hard demonstrations. Experimental\nresults prove the superiority of our proposed\nmethod, achieving best performance on three\nkinds of challenging classification tasks in the\nzero-shot and few-shot settings. Besides, with\ntask-level thinking steps, automatically gener-\nated chain-of-thoughts (CoTs) bring more com-\npetitive performance.\n1 Introduction\nLarge language models (LLMs) have shown in-\ncredible performance on many tasks such as di-\nalogue generation, commonsense reasoning, and\nquestion answering (Dong et al., 2022). With the\nnew paradigm of in-context learning (ICL) (Brown\net al., 2020; Pan et al., 2023), LLMs directly adapt\nto downstream tasks without updating parameters\nby prompting with few demonstrations. Wei et al.\n(2022) found that elaborating the reasoning steps in\ndemonstrations can significantly stimulate the com-\nplex reasoning ability of LLMs, which is called\nmanual chain-of-thought (CoT). Zero-shot-CoT\n(Kojima et al., 2022) enables LLMs to generate\nthinking steps automatically using the simple but\nefficient prompt \" Let’s think step by step. \", as\nshown in Figure 1(a). More recently, plan-and-\nsolve prompt (Wang et al., 2023) extends it with\nLLM\nTask Description\n(Demonstrations)\nInputs \nLet's think step by step.\nAutomatic Task-Level\nThinking Steps\nOutputs\nLLM\nLLM Agents\nOutputs\nTask Description\n(Demonstrations)\nInputs \nLet's think step by step.\nHard\nDemonstrations\n(a) Zero-shot-CoT Prompt(b) Prompt with Automatic Task-Level Thinking Steps\nProgressive Revision Framework\nFigure 1: Zero-shot-CoT Prompt (Kojima et al., 2022)\nvs. Prompt with Automatic Task-Level Thinking Steps\n\"Let’s first understand the problem, devise a plan,\nand solve the problem step by step. \" to address\nthe issue of missing reasoning steps and \" extract\nrelevant variables and their corresponding numer-\nals\" to address the issue of calculation errors. In-\nspired by this, we propose the concept oftask-level\nthinking steps for challenging classification tasks\nas shown in Figure 1(b), which helps LLMs to clar-\nify tasks and remind them of overlooked issues. To\nthis end, we design a progressive revision frame-\nwork, which automatically generates thinking steps\nby LLM agents and then progressively revise the\nsteps by correcting hard demonstrations.\nFor classification tasks, it is widely investigated\nthat the demonstrations distribution, including con-\ntent, label, and even the order, can severely affect\nthe performance (Lu et al., 2022a; Turpin et al.,\n2023; Zhao et al., 2021). Therefore, many works fo-\ncused on selecting high-quality and representative\ndemonstrations, such as selecting similar demon-\nstrations (Liu et al., 2021) or maintaining the class\nbalance between demonstrations (Lu et al., 2022b).\nHowever, the best distribution depends on the task,\nand bias is inevitable. Our proposed task-level\nthinking steps are helpful in reducing the bias in-\ntroduced by demonstrations. Table 1 shows an ex-\nample of the stance detection task. Zero-shot-CoT\n2454\nTask description: Perform stance detection.\nclasses: (A)none (B)against (c)favor\nInput: For domain abortion, You can’t give the\n\"death penalty\" to something not living sentient\nor autonomous.\nNo Demonstrations\nOutput of zero-shot-CoT prompt\nThe stance in this context is (C) favor, as the\nstatement suggests that the speaker is in favor of\nabortion rights. The argument is that a fetus is\nnot a living, sentient, or autonomous being and\ncannot be subject to the death penalty. ✓\nBiased Demonstrations: input:[...] answer:(B)\ninput:[...] answer:(B) input:[...] answer:(B)\nOutput of zero-shot-CoT prompt\nFirst, the statement is discussing abortion. Next,\nthe statement is arguing against the idea of giv-\ning the \"death penalty\" to something that is not\nliving, sentient, or autonomous. This suggests a\nstance against abortion. ×\nOutput with task-Level thinking steps\n1. Identify the domain abortion discussed in the\nstatement. 2. Look for keywords or phrases ...\nthe speaker is against the idea of considering a\nfetus as a living, sentient, or autonomous being.\n3. Determine the stance of the statement. The\nstatement is against the idea of considering fetus\nas a living, sentient, or autonomous being, which\nsuggests that the speaker is in favor of abortion.\n4. Provide the answer to the stance detection\ntask which is (C) in favor of abortion. ✓\nTable 1: Zero-shot-CoT prompt is affected by the biased\ndemonstrations while prompt with task-level thinking\nsteps outputs correctly. Omitted demonstrations are\nshown in Appendix A.\npredicts correctly with no demonstrations. How-\never, the output is severely affected by \" against\"\nfrom the biased demonstrations. The automatically\ngenerated task-level thinking steps include: iden-\ntifying the domain; finding the viewpoint; deter-\nmining the stance with respect to the domain; and\nmatching the choices. With the help of these steps,\nLLMs perceive the implicit meaning \"a fetus is not\na living, sentient, or autonomous being\" and make\nthe correct prediction \"in favor of abortion\".\nIn addition to debiasing, we would like task-level\nthinking steps to clarify some confusing classes. To\nthis end, we design a progressive revision frame-\nwork to generate the thinking steps by correcting\nhard demonstrations. In the framework, as shown\nin Figure 2, we set up two LLM agents (Andreas,\n2022) with character settings and historical memo-\nries, the teacher agent and the student agent. For\neach hard demonstration, the student agent gener-\nates outputs based on the task-level thinking steps\ngenerated by the teacher agent. If the prediction is\ncorrect, move on to the next demonstration. If the\nprediction is wrong, the teacher agent first analyses\nthe error cause and then tries to revise the task-\nlevel thinking steps. For the example in Figure 2,\nclass (B) and class (D) tend to be confusing. After\nthe teacher agent’s analysis, an explicit reminder\ncan be inserted in a suitable place in the revised\ntask-level thinking steps. Then, the student agent\ngenerates outputs again. The iteration may be re-\npeated several times until the correct prediction or\nthe maximum number of times allowed is reached.\nIn this paper, we consider three classification\ntasks: multifaceted analysis of subjective text re-\nquires first locating aspects, then identifying opin-\nions, and finally analyzing polarity; fine-grained\ntext classification requires a deep distinction be-\ntween numerous and similar classes; domain-\nspecific classification requires the understanding\nof domain-related questions, choices, and demon-\nstrations. We conduct experiments on zero-shot,\nfew-shot, and few-shot-CoT settings, specifically.\nExperimental results demonstrate the effectiveness\nof our proposed method by outperforming the com-\npetitive baseline models. Further ablation analyses\nand robustness experiments reveal the benefits and\neffects of each module.\nOur contributions are threefold: 1) We pro-\npose the novel concept of task-level thinking steps,\nwhich can eliminate bias introduced by demonstra-\ntions 2) We design a progressive revision frame-\nwork, which purposefully improves the thinking\nsteps based on feedback from hard demonstrations\nfor LLMs. 3) Our proposed method surpasses com-\npetitive baselines in the zero-shot, few-shot, and\nfew-shot-CoT settings.\n2 Methods\n2.1 Task Definition\nWe consider a general classification task, where\nT denotes the task description, x ∈ Xis the\ntext input, and y ∈ Yis the corresponding la-\nbel. |Y|= C is the number of classes. Dtrain =\n{(x1,y1),..., (xM,yM)}denotes the train dataset\nand Dtest = {(x1,y1),..., (xN,yN)}denotes the\n2455\nTeacher\nStudent\nLet's think following the\nthinking steps.\n: She continued to put out  <e2>\nlow amounts of urine </e2> \nthroughout the admission , despite \n<e1> aggressive diuresis </e1> \nwith IV lasix gtt and boluses. \nWhat's the relation between <e1>\nand <e2>? Classes:\n(B) <e1> worsens <e2>\n......\n(D) <e1> is administered for <e2>\nHard demonstrations:\nPlease generate thinking steps for\nthe task. \nPlease revise the thinking steps \nto clarify.\n... Therefore, the answer is (D)\nRevised task-level thinking steps:\n...... \nIf the <e1> treatment is ineffective or\nworsens the <e2> medical problem,\nconsider class (B) rather than class (D)\n... Therefore, the answer is (B)\nInitial task-level thinking steps:\n1. Identify the two medical terms or\nproblems mentioned in the context,\nmarked as <e1> and <e2>. \n2. Determine the relationship between\n<e1> and <e2> based on the context.\n3. Choose the answer choice that best\ndescribes the relationship between <e1>\nand <e2> from the given options.\nPlease analyze the outputs  from\nthe student following your thinking\nsteps for  \nHard Demonstration i\nHard Demonstration i+1 \nInitial \nFigure 2: Progressive revision framework. There are two LLM agents: teacher and student. Rectangles represent\nthe prompts to agents, outputs of the teacher agent and outputs of the student agent.\ntest dataset. An LLM with parameters θperforms\nzero-shot learning by conditioning on the task de-\nscription T and the test input x∼Dtest. The likeli-\nhood of a candidate answer ycould be represented\nby a scoring function f:\nP(y|x) =fθ(T,x) (1)\nWhen sampling demonstrations Ddemo from\nDtrain, the likelihood for few-shot learning is:\nP(y|x) =fθ(T,Ddemo,x) (2)\nThe final predicted label ˆy is the candidate an-\nswer with the highest probability:\nˆy= arg max\ny∈Y\nP(y|xtext) (3)\nOnly hard demonstrations can generate valuable\ntask-level thinking steps. In this paper, we tra-\nverse Dtrain and select one that the LLMs can\nnot predict correctly for each class to construct\nDdemo = {(x0,y0),..., (xC−1,yC−1)}as shown\nin Figure 2.\n2.2 LLM Agents\nLLMs can serve as models of agents as suggested\nin (Andreas, 2022), which output responses based\non initial setting, historical memories, and current\nobservations. We set up two LLM agents. The\nteacher agent is responsible for generating the task-\nlevel thinking steps from hard demonstrations with\nlabels. In addition, the teacher agent needs to ana-\nlyze the errors in the student agent’s answers and\nrevise the task-level thinking steps. The student\nagent is responsible for giving answers and expla-\nnations for each presentation with the task-level\nthinking steps provided by the teacher agent.\n2.3 Progressive Revision Framework\nGiven hard demonstrations Ddemo, the teacher\nagent first generates initial thinking steps S0, then\nprogressively revise it based on the outputs from\nthe student agent as shown in Figure 2.\nGenerate initial task-level thinking steps. The\nteacher agent uses hard demonstrations to gener-\nate initial task-level thinking steps. The prompt\nis \"Please generate generic thinking steps for the\ntask.\". Taking the medical relation extraction task\nas an example, here are the generated initial task-\nlevel thinking steps \" 1. Identify the two medi-\ncal terms or problems mentioned in the context,\nmarked as <e1> and <e2>. 2. Determine the re-\nlationship between <e1> and <e2> based on the\ncontext. 3. Choose the answer choice that best de-\nscribes the relationship between <e1> and <e2>\nfrom the given options. \".\n2456\nWe can see that the teacher agent understands the\ntask description and gives three reasonable steps:\nidentifying two entities, determining the relation,\nand matching the classes. However, they are also\npreliminary and abbreviated.\nGenerate outputs. For the i-th hard demonstration\nxi, the student agent receives the task-level think-\ning steps Si and generates outputs Ri. The prompt\nis \"Thinking steps: {Si}. Let’s think following the\nthinking steps.\" As shown in Figure 2, the correct\nanswer is \"(B) <e1> worsens <e2>\" but the stu-\ndent agent outputs \"The answer is (D)\" and sends\nit to the teacher agent.\nImprove task-Level thinking steps. The teacher\nagent analyzes the reason for the error with the\nprompt \"Input: { xi}. Outputs: { Ri}. Please an-\nalyze the outputs from the student following your\nthinking steps.\". Then the teacher agent revises the\ntask-level thinking steps with the prompt \"Please\nrevise the thinking steps to clarify.\".\nWe find that the revised thinking steps do not usu-\nally change much but rather extend more reminders.\nThe main extension in this example is \" If the treat-\nment is ineffective or worsens the medical problem,\nconsider choice (B) rather than choice (D).\". It can\nbe seen that the student agent only focuses on the\ncausal relationship between the treatment and the\nmedical problem while does not pay attention to\nthe effects. In fact, this input places more emphasis\non the effect of the treatment on the disease than on\nhow the treatment should be taken for the disease.\nGenerate outputs with revised thinking steps.\nWith the revised thinking stepsSi, the student agent\npredicts correctly. Then, the iteration proceeds to\nthe next hard demonstration. Otherwise, the out-\nput is sent to the teacher again until the correct\nprediction or the maximum attempts are reached.\nExperiments show that even if the hard demon-\nstration was not successfully corrected, the revised\nthinking step was still useful.\nChecking mechanism. We find that LLMs, espe-\ncially gpt-4, are effective in correcting hard sam-\nples by revising task-level thinking steps. However,\nthis may be achieved by extending shortcuts rather\nthan trustworthy analysis. For example, \" If you\nare not sure, choose (B). \". Therefore, we set up\na checking mechanism. The teacher agent tries\nto generate multiple candidate thinking steps that\ncan successfully correct xi. The student agent tests\nperformance on {x0,...,x i−1}and keeps the best\none.\n2.4 Automatic Chain-of-Thought\nAnother advantage of our proposed task-level think-\ning steps is that is very helpful to generate reliable\nCoTs for demonstrations automatically. Manual\nCoTs are labor intensive, and the human annota-\ntions do not necessarily match LLMs. Zero-shot-\nCoT, while avoiding human engineering, may pro-\nduce unfaithful explanations especially for hard\ndemonstrations.\n3 Experiments\n3.1 Datsets\nWe choose three kinds of challenging classification\ntasks. The statistics and descriptions of all tasks\nare shown in Table 7 and Table 8 in Appendix B.\nMultifaceted analysis of subjective text is the\ntask that involves different aspects of subjective\nhuman feeling reflected in the text. The challenge\nis recognizing and understanding a broader range\nof human emotional states rather than identifying\ncommon positive or negative feelings. Following\nZhang et al. (2023a), we use Stance16 (Mohammad\net al., 2016) for stance detection and Emotion20\n(Barbieri et al., 2020) for emotion recognition.\nFine-grained text classification is the task of cate-\ngorizing text or documents into predefined topics\nor categories. We carefully select 20news 1 dataset.\nThe challenge lies in numerous and similar classes.\nDomain-specific classification adopts the medical\nrelation classification task, which refers to identify-\ning the relationship between pairs of entities within\nmedical texts. The challenge is the need for domain\nknowledge, and differences between relations are\nsubtle. i2b2 (Uzuner et al., 2011) collects clinical\nrecords and is used to classify relations between\nmedical problems, medical tests, and treatments.\nChemProt (Krallinger et al., 2017) is used to de-\ntermine how chemical acts on the protein, such as\nactivation and inhibition. DDI (Herrero-Zazo et al.,\n2013) is used to determine the interaction relation\nbetween two drugs, such as pharmacokinetics and\npharmacodynamics.\n3.2 Experimental Settings\nWe constructed experiments to evaluate the effec-\ntiveness of thinking steps. Chard demonstrations\nare sampled for each task to generate thinking steps.\nThe LLM for the teacher agent is gpt-4, the LLM\nfor the student agent and the LLM for running\n1http://qwone.com/~jason/20Newsgroups/\n2457\nMethods Stance16 Emotion20 20news i2b2 ChemProt DDI Avg.\nZero-shot\nDefault 41.49 70.02 45.99 14.89 41.97 49.17 43.92\nAuto 47.62 72.81 39.97 29.63 54.47 36.22 46.78\nITS 41.66 68.96 63.30 43.82 50.91 39.57 51.37\nPRTS 52.38 75.76 67.20 54.99 68.33 57.36 62.67\nFew-shot\nDefault 50.04 71.58 56.29 35.54 43.03 46.13 50.43\nAuto 51.09 75.28 41.76 42.62 65.73 49.29 54.30\nITS 36.06 74.01 53.56 40.48 49.01 50.15 50.54\nPRTS 56.35 77.82 66.58 52.30 69.67 62.45 64.19\nFew-shot-CoT\nAuto 37.17 72.72 42.02 40.94 67.40 50.73 51.83\nITS 40.39 75.21 58.71 60.50 76.72 63.23 62.46\nPRTS 60.30 78.92 72.35 67.45 79.20 76.67 72.47\nTable 2: The overall performance of PRTS and baselines on six classification tasks. Bold denotes the best in\ncorresponding settings specifically.\nexperiments are both gpt-3.5-turbo. We set a tem-\nperature of 0 for all experiments. For each hard\ndemonstration, the maximum number of revision\nattempts is 5. For the checking mechanism, the\nnumber of candidates is 3. We report the F1-score\nas the evaluation metric.\nThe followings methods serve as baselines: De-\nfault generates outputs directly and Auto gener-\nates outputs with the prompt \" Let’s think step by\nstep\" (Kojima et al., 2022). Our methods gener-\nate outputs with Initial Thinking Steps (ITS) to\nvalidate the effectiveness of thinking steps and Pro-\ngressively Revised Thinking Steps (PRTS) to vali-\ndate the effectiveness of framework. For few-shot,\nprompt demonstrations are the same as hard demon-\nstrations to avoid unfair comparisons. In Section\n4.2, we construct experiments with other prompt\ndemonstrations. For few-shot-CoT, Auto, ITS and\nPRTS can automatically explain the prompt demon-\nstrations without human efforts following (Wang\net al., 2023; Shum et al., 2023). Detailed prompt\ntemplates are shown in Table 9 in Appendix B.\n4 Results\n4.1 Main Results\nThe experimental results are shown in Table 2. All\nresults are averaged over three runs. Overall, PRTS\nperforms best on all tasks, which outperforms Auto\nby 18.75% for zero-shot, 9.89% for few-shot, and\n20.64% for few-shot-CoT on average.\nFor the zero-shot setting, ITS has slightly better\nperformance than Default and Auto on average, but\nwhich one is better depends on the task. Specifi-\ncally, ITS outperforms Auto by 23.33% for 20news\nand by 14.19% for i2b2. One possible reason is that\nboth 20news and i2b2 have many similar classes,\nand the thinking steps include analysis and com-\nparison between classes, which may help in the\nfine-grained classification of LLMs. With progres-\nsive revision, PRTS performs best on all tasks.\nFor the few-shot setting, PRTS outperforms Auto\nby an average of 9.89%. It is worth noting that\nITS has worse performance than Auto. One pos-\nsible reason is that initial thinking steps may be\ncontradictory to ICL for LLMs. However, we will\nshow that significant improvements can be obtained\nby explaining these demonstrations with thinking\nsteps.\nFor few-shot-CoT, PRTS outperforms Auto by\n20.64% and ITS by 10.01% on average. For Auto,\nStance16 and 20news have worse performance\ncompared with the few-shot setting. One possible\nreason is that LLMs may produce reluctant expla-\nnations and hallucinations for hard demonstrations.\nTable 4 shows the automatically generated CoT by\nAuto, ITS, and PRTS: For Auto, it can be seen that\nthe LLM can not understand the stance detection\ntask. Therefore, the LLM fails to locate the domain\nword \"atheism\" and mistakenly believes the answer\nis \"(C) favor\". However, faced with the golden\nlabel \"(B) against\", the LLM can only generate\nthe reluctant and absurd explanation: \"against (B)\nis the closest to the opposite of favor \". With this\nwrong CoT demonstration, a worse performance\nthan zero-shot and few-shot would make sense. For\nITS, LLM successfully locates the domain word\n\"atheism\" and seems to explain the golden label\ncorrectly. However, LLM only states that \"against\nis the idea of freedom from religion\" but does not\nexplicitly state that atheism is equivalent to free-\n2458\nMethods Stance16 20news i2b2\nSimilar 51.82 36.63 47.86\nSimilar+PRTS 53.97 59.20 52.91\nUnbalanced 41.78 21.98 12.93\nUnbalanced+PRTS 52.38 65.09 42.36\nEasy 53.09 51.20 27.90\nEasy+PRTS 51.79 67.33 52.98\nTable 3: PRTS consistently improves performance for\nvarious sampling methods of prompt demonstrations.\ndom from religion. This may lead LLMs to ignore\ndomain words and only decide on the sentence.\nFor PRTS, the CoT is more consistent and com-\nplete and always revolves around the domain word\n\"atheism\". COT explicitly explains that \"a stance\nagainst atheism\" is because \"people should not be\nfree from religion\".\n4.2 Ablation Studies\nAnalysis of demonstrations distribution. In\nthis section, we explore the effects of various\nsampling methods of prompt demonstrations. For\nthe few-shot setting, Similar (Liu et al., 2021)\ndynamically selects Cmost similar for each sam-\nple. Unbalanced selects prompt demonstrations\nin the same class, which is considered as biased\n(Zhao et al., 2021). Easy is the same as our hard\ndemonstration selection, which selects one that\nLLMs predict correctly for each class. Results\nare reported in Table 3 for Stance16, 20news, and\ni2b2. We can find that which sampling method is\nbetter depends on the task, but PRTS consistently\nimproves by 9.93% for Similar, 27.71% for\nUnbalanced, and 13.3% for Easy, respectively.\nEspecially compared with Unbalanced performing\npoorly on all tasks, Unbalanced+PRTS remains\nvery robust. This shows that PRTS can elim-\ninate bias introduced by demonstrations effectively.\nAnalysis of the progressive revision framework.\nOur key idea is revising the task-level thinking\nsteps by correcting the hard demonstrations, thus\nenhancing the ability to distinguish confusing\nclasses. Specifically, we first correct the hard\ndemonstrations and then further improve thinking\nsteps through the Checking mechanism.\nWhile correcting the hard demonstrations of\nclass c, it is also beneficial to all samples. We\nvalidate this idea on two datasets for zero-shot\nsetting: Stance16 and i2b2 as shown in Figure\n(A) (B) (C)\nclasses\n0\n10\n20\n30\n40\n50# corrected samples\nStance16\n(A) (B) (C) (D) (E) (F) (G) (H)\nclasses\n20\n0\n20\n40\ni2b2\nrevising class affected classes\nFigure 3: The number of corrected samples when cor-\nrecting hard demonstrations of each class.\nInitial (A) (B) (C)\n42\n44\n46\n48\n50\n52F1-score(%)\nStance16\nInitial (A) (B) (C) (D) (E) (F) (G) (H)\n44\n46\n48\n50\n52\n54\ni2b2\nPRTS w/ checking mechanism PRTS w/o checking mechanism\nFigure 4: Performance changes compared PRTS w/\nchecking mechanism with PRTS w/o checking mecha-\nnism in progressive revising thinking steps.\n3. The class c of the hard demonstration being\ncorrected is named revising class while the remain-\ning classes {1,...,c −1,c + 1,...,C }are named\naffected classes. It is always helpful for revising\nclass and may also have a positive effect on affected\nclasses.\nIn addition, we further improve thinking steps\nthrough the checking mechanism when iterating,\ni.e., progressive revising thinking steps. As shown\nin Figure 4, for PRTS w/o checking mechanism,\nhuge performance degradation occurs for class (B)\nin Stance16 and class (D) in i2b2. For compar-\nison, PRTS w/ checking mechanism almost im-\nproves progressively in each round and outper-\nforms the PRTS w/o checking mechanism 5.29%\nfor Stance16 and 3.07% for i2b2.\n4.3 Robustness Experiments\nEffects of class names. For classification tasks,\nclass names can seriously affect the performance\nof LLMs. For 20news and DDI, the default classes\nare ambitious, and we have manually annotated\nthem as manual classes. In this section, we explore\nthe effects of different class names. More details\nare shown in Table 8 in Appendix B.\nWe experimented on zero-shot, few-shot, and\n2459\ninput: For domain atheism, The framers of our Constitution meant we were to have freedom of religion,\nnot freedom from religion. - Billy Graham quote SemST\nclasses: (A) none (B) against (C) favor answer: (B)\nAuto The statement suggests a pro-religion stance, implying that the person making the statement is\nin favor of religion. Therefore, the stance detection task is \"favor\" (C). However, the prompt\nspecifically asks for stance detection from the given choices, and \"favor\" is not one of the\nchoices. Among the given choices, \"against\" (B) is the closest to the opposite of \"favor\"\nITS 1. The context is about atheism and a quote by Billy Graham 2. ... 3... 4. The stance expressed\nis against the idea of freedom from religion. 5. Therefore, the appropriate answer is (B).\nPRTS 1. Read and understand the context: ... 2. Identify the domain: The domain is atheism. 3.\nDetermine the stance: ...This implies a stance against atheism, as it suggests that people should\nnot be free from religion. 4. Consider the broader implications... 5. Choose the choice (B)\nTable 4: Case study for automatically generated CoT. Auto CoT is reluctant and absurd. ITS CoT does not explicitly\nstate the relation between stance \"atheism\" and \"freedom from religion\". PRTS CoT is complete and always revolves\naround the domain word \"atheism\".\nzero-shot few-shot few-shot-CoT\n40\n45\n50\n55\n60\n65\n70\n75F1-score(%)\n20news\nzero-shot few-shot few-shot-CoT\n40\n50\n60\n70\nDDI\nAuto(Manual Labels)\nPRTS(Manual Labels)\nAuto(Default Labels)\nPRTS(Default Labels)\nFigure 5: Performance on zero-shot, few-shot and few-\nshot-CoT settings for Auto and PRTS. The dashed lines\nindicate default classes and the solid lines indicate man-\nual classes.\nfew-shot-CoT for Auto and PRTS. To our surprise,\nthe default classes performed much better than the\nmanual classes, especially for Auto on 20news,\nas shown in Figure 5(a). The reason may be that\n20news is a pre-trained corpus for LLMs and is\ntherefore more compatible with the default classes.\nNevertheless, PRTS has a similar performance with\nmanual classes and default classes.\nFor DDI, the performance on the default classes\nis particularly worse than on manual classes\nfor Auto as shown in Figure 5(b). We find that\nLLMs can correctly recognize and interpret the\nmedical terms such as \" pharmacokinetics\" and\n\"pharmacodynamics\" with the strong capability\non medical problems (Nori et al., 2023). The\npoor performance is because default class \" int\"\nwas mistakenly thought to be a useless and\ndistracting choice. However, PRTS achieves\nsimilar performance to manual classes because\nthe task-level thinking steps explicitly state that\n\"int is an abbreviation for interaction except\npharmacokinetics and pharmacodynamics\".\nEffects of LLMs. We find that onlygpt-4 helps the\nteacher agent to revise thinking steps progressively.\ntext-davinci-003 is not a dialogue model and can\nnot even produce reasonable initial thinking steps.\ngpt-3.5-turbo enables interaction between the stu-\ndent and the teacher, but it is difficult to generate\nvaluable thinking steps to correct the hard demon-\nstrations. For the hard demonstrators covered in\nthis paper, gpt-4 can almost revise thinking steps\nto correct them within five attempts.\nTherefore, we keep the previous obtained think-\ning steps and rerun the experiment on text-davinci-\n003 and gpt-4 as shown in Table 5. Overall, the\nclassification performance of text-davinci-003, gpt-\n3.5-turbo, and gpt-4 improves sequentially for\nboth Auto and PRTS. Interestingly, the perfor-\nmance improvements of the fine-grained classi-\nfication task, 20news ( 39.97% → 70.03%) and\ni2b2 (29.63% →58.33%), are huge from gpt3.5-\nturbo to gpt-4 for Auto. It is not clear whether this\nis because gpt-4 was pretrained on these datasets\nor its instruction tuning is similar to our thinking\nsteps. Besides, all three LLMs perform poorly on\nStance16 for Auto, while PRTS can get a huge\nboost on gpt-4. This suggests that PRTS can accu-\nrately analyze and disambiguate the task for ambi-\ntious tasks, then provide explicit thinking steps.\n4.4 Cost Analysis\nIn this section, we analyze the additional cost of\ntask-level thinking steps includes the generation\ncost based on hard demonstrations and prompt\n2460\nLLMs Stance16 Emotion20 20news i2b2 ChemProt DDI Avg.\ntext-davinci-003 Auto 44.44 65.29 32.45 12.81 40.69 25.20 36.81\nPRTS 45.35 70.91 57.55 54.64 39.94 41.96 51.72\ngpt-3.5-turbo Auto 47.91 75.28 39.97 29.63 54.47 36.22 47.25\nPRTS 52.38 77.82 67.20 54.99 68.33 57.36 63.01\ngpt-4 Auto 41.81 78.64 70.03 58.33 60.71 48.01 59.58\nPRTS 66.76 80.20 78.33 66.59 68.33 76.54 72.79\nTable 5: Zero-shot performance of PRTS and Auto for different LLMs.\nmethods generation tokens prompt tokens\ngeneral dataset\nDefault - Tinput ×Ntest\nAuto - (Tinput + Ttask) ×Ntest\nPRTS Tinput ×H×C+ 2R×(Ttask + 2Tinput) ×C (Tinput + 2Ttask) ×Ntest\ni2b2 dataset\nDefault - Tinput ×Ntest\nAuto - 3Tinput ×Ntest\nPRTS 168Tinput 5Tinput ×Ntest\nTable 6: Cost comparison for different methods.\ncost for test inputs.\nGeneration cost includes the selection of hard\ndemonstrations and iterative refinement. Assume\nthat the average tokens of train/test inputs are\nTinput (outputs with simple answers can be ig-\nnored). It takes an average traversal of H train in-\nputs to select hard demonstrations for each of theC\nclasses. Thus, the selection cost is Tinput×H×C.\nAssume that the average tokens of task-level think-\ning steps is Ttask. In each iteration of the refine-\nment phase, the teacher agent needs to predict\neach hard demonstration with cost Ttask + 2Tinput\nbased on task-level thinking. There is a degree\nof simplification, but roughly the input tokens are\nall Tinput + Ttask and the output tokens are all\nTtask. The student agent needs to update task-\nlevel thinking steps based on test input with cost\nTtask+2Tinput. There is a degree of simplification,\nbut roughly the input tokens are all Tinput + Ttask\nand the output tokens are all Ttask. Then the refine-\nment cost is 2R×(Ttask + 2Tinput) ×C, where\nRis the number of iterations for each hard demon-\nstration.\nIn the inference phase, the cost of zero-shot\nprompt (Default) is Tinput, zero-shot prompt with\n\"Let’s think step by step\" (Auto) is Tinput + Ttask,\nand prompt with task-level thinking steps (PRTS)\nis Tinput + 2Ttask.\nThe total cost with Ntest test inputs is sum-\nmarised in the Table 6. Taking i2b2 dataset as\nan example, H = 3, C = 8, R = 4, and Ttask is\napproximately 2Tinput. For practical applications,\nNtest is much larger than 168, so generation to-\nkens can be ignored. The task-level thinking steps\nintroduced in the inference phase may need to be\nimproved in the future.\n5 Related Work\n5.1 In-context Learning\nIt is widely investigated that severe bias exists in\nthe output of LLMs for different demonstrations\n(Lu et al., 2022a; Turpin et al., 2023). Zhao et al.\n(2021) proposed a calibration method that fits the\ncalibration parameters for uniform content-free in-\nput \"N/A\" is uniform across choices. More studies\ntry to select specific demonstrations based on dif-\nferent philosophies to reduce bias, such as similar-\nity (Liu et al., 2021) selecting similar examples to\nfacilitate analogical learning, high uncertainty to\nimprove inference efficiency (Diao et al., 2023) se-\nlection. Min et al. (2022) showed that LLMs could\nlargely ignore the mapping between inputs and la-\nbels, while Webson and Pavlick (2022) showed that\nLLMs perform well on the NLI dataset with correct\ndemonstrations even if the instruction is irrelevant\nor misleading.\n2461\n5.2 Chain-of-Thought\nRecent works have shown that LLMs can output\nbetter responses for ICL with high-quality expla-\nnation (Lampinen et al., 2022). Manual chain-of-\nthought (Wei et al., 2022) could prompt LLMs to\ngenerate step-by-step solutions, which leads to sub-\nstantial improvements on many reasoning-intensive\ntasks. However, the performance improvement de-\npends on manual efforts. The zero-shot CoT (Ko-\njima et al., 2022) enables LLMs to automatically\ngenerate explanations. For unlabelled demonstra-\ntions, auto-CoT (Zhang et al., 2023b) partitions\ndemonstrations into a few clusters, selects a rep-\nresentative demonstration from each cluster, and\ngenerates explanations using zero-shot CoT. For\nlabeled demonstrations, Shum et al. (2023) opti-\nmizes a set of latent variables to select the most\nhelpful and suitable demonstrations with correct\nself-generated answers. Except for the first-try CoT,\nZheng et al. (2023) used previous outputs as hints\nto progressively guide toward the correct answers.\nMadaan et al. (2023) provided multi-aspect feed-\nback on the previous output and refined it into new\noutput. Our framework is also iterative, but it works\non the task level and is based on feedback from hard\ndemonstrations.\n5.3 Prompt Engineering\nPrompt offers a natural and intuitive interface for\nhumans to interact with language models. For\nparameter-accessible models, gradient-based meth-\nods are widely adopted (Shin et al., 2020; Qin and\nEisner, 2021; Lester et al., 2021). In the era of\nLLMs, searching directly in the natural language\nhypothesis is more popular. Honovich et al. (2022)\nfound LLMs can generate the task instruction with\nseveral demonstrations. Zhou et al. (2022) auto-\nmatically generates many instructions for the given\ndemonstrations and selects the one with the max-\nimum score function. The concept of task-level\nthinking steps is also part of prompt engineer-\ning, from simple \" Let’s think step by step \" (Ko-\njima et al., 2022) to well-designed plan-and-solve\nprompt (Wang et al., 2023).\n6 Conclusion\nAlthough LLMs have shown incredible perfor-\nmance on many tasks, we pointed out that many\nclassification tasks, such as multifaceted analysis\nof subjective text, fine-grained classification, and\ndomain-specific classification tasks are still chal-\nlenging. Therefore, we proposed the concept of\ntask-level thinking steps and verified the robustness\nfor biased demonstrations. To further enhance the\nclassification capabilities of LLMs, we designed\na progressive revision framework, which purpose-\nfully improves the thinking steps based on feedback\nfrom hard demonstrations. Experimental results\nproved the superiority of our proposed method. Be-\nsides, with task-level thinking steps, we found that\nthe automatically generated CoTs are more reason-\nable and effective.\nLimitations\nTask coverage. We have proposed three challeng-\ning classification tasks: multifaceted analysis of\nsubjective text, fine-grained text classification, and\nmedical relation classification. Although there\nare many nontrivial findings, they are limited to\ntraditional classification tasks. Future work would\nextend to a wider range of classification tasks, such\nas natural language inference and multiple choice.\nExperimental analysis and explanation. Al-\nthough many ablation experiments, robustness\nexperiments and case studies have been conducted,\nthere are still some aspects that have not been\ndiscussed. As an example, we adopt a default\nrevising order for each task. While the checking\nmechanism reduces the negative impact of revi-\nsion, there may still be other unforeseen challenges.\nReliability. Although our progressive revision\nframework has been validated on several chal-\nlenging datasets, LLMs remain sensitive to small\nchanges in the thinking steps. Future work would\nimprove the reliability of the thinking steps by\nrefining hard demonstrations selection, thinking\nsteps revision and checking mechanisms.\nUnlabeled training sets. The absence of mas-\nsively labeled training sets is indeed a practical\nproblem. However, we think there are still feasible\nsolutions to obtain hard demonstrations. For ex-\nample, measure uncertainty by running zero-shot\nprompts multiple times. If LLMs have the same\noutputs multiple times for the same train input, it\nmeans that the LLMs have high confidence. Ei-\nther completely correct or completely wrong due to\nbias. On the contrary, if LLMs have very different\noutputs for the same input, it means that LLMs\nhesitate between certain classes and this train input\n2462\ncan be considered a hard demonstration. Annotate\nthem and continue to use the proposed method in\nthis paper.\nEthics Statement\nAcknowledgements\nThis work was supported by the National\nKey Research and Development Program of\nChina (2018YFC0830400) and the Shanghai Sci-\nence and Technology Innovation Action Plan\n(20511102600).\nReferences\nJacob Andreas. 2022. Language models as agent mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 5769–5779, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644–1650, Online. Association for Computational\nLinguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models. arXiv preprint\narXiv:2302.12246.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nMaría Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMartínez, and Thierry Declerck. 2013. The DDI\ncorpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions. 46(5):914–\n920.\nOr Honovich, Uri Shaham, Samuel R Bowman, and\nOmer Levy. 2022. Instruction induction: From few\nexamples to natural language task descriptions. arXiv\npreprint arXiv:2205.10782.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\nand et al. 2017. Overview of the biocreative vi\nchemical-protein interaction track. In Proceedings of\nthe sixth BioCreative challenge evaluation workshop,\npages 141–146.\nAndrew Lampinen, Ishita Dasgupta, Stephanie Chan,\nKory Mathewson, Mh Tessler, Antonia Creswell,\nJames McClelland, Jane Wang, and Felix Hill. 2022.\nCan language models learn from explanations in con-\ntext? In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022 , pages 537–563,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022a. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022b. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nIn Proceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016), pages 31–\n41, San Diego, California. Association for Computa-\ntional Linguistics.\n2463\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\n2023. What in-context learning\" learns\" in-context:\nDisentangling task recognition and task learning.\narXiv preprint arXiv:2305.09731.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235.\nKaShun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data. arXiv preprint\narXiv:2302.12822.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R Bowman. 2023. Language models don’t\nalways say what they think: Unfaithful explana-\ntions in chain-of-thought prompting. arXiv preprint\narXiv:2305.04388.\nÖzlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text.\nJournal of the American Medical Informatics Associ-\nation, 18(5):552–556.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models. In Pro-\nceedings of the 61th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers).\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan,\nand Lidong Bing. 2023a. Sentiment analysis in the\nera of large language models: A reality check. arXiv\npreprint arXiv:2305.15005.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompt-\ning in large language models. In The Eleventh In-\nternational Conference on Learning Representations\n(ICLR 2023).\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023. Progressive-hint prompting\nimproves reasoning in large language models. arXiv\npreprint arXiv:2304.09797.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\n2464\nA Details of Biased Demonstrations\nHere are three biased demonstrations omitted in\nTable 1:\nFor domain abortion, Our #TruthTour cont’ in\nthe @user as we show the humanity of the unborn\n& inhumanity of abortion!\nFor domain feminist, Feel things will truly be\nequal in marriage when I see jock strap tossed to\nthe ’single women’ a weddings alongside the garter\nFor domain hillary, user Here’s to fearless\nwomen chasing their goals.\nB Details of Classification Tasks\nWe present the statistics of challenging classifica-\ntion tasks in Table 7, description in Table 8, prompt\ntemplate in Table 9.\n2465\nTask Dataset Train Dev Test Sampled Test Labels\nMultifaceted analysis of subjective text Stance16 2620 294 1249 500 3\nEmotion20 3257 374 1421 500 4\nTopic classification 20news 10.2k 1.1k 7.53k 500 20\nMedical relation classification\ni2b2 2808 312 6193 500 8\nChemProt 3370 2030 2910 500 5\nDDI 3598 400 976 500 4\nTable 7: Statistics of the number of samples and labels of challenging datasets.\nDataset Description\nStance16\nPlease perform stance detection task.\nclasses: (A) none (B) against (C) favor\nPRTS: 1. Read and understand the context: Carefully read the given statement or\ncontext to fully comprehend its meaning and the topic it addresses.2. Identify the\ndomain: Determine the domain or subject matter the context is related to, such as\npolitics, environment, social issues, etc. Make sure the domain matches the one specified\nin the question or task. 3. Determine the stance: Consider the context’s position in\nrelation to the domain and its implications, not just the general topic. Look for keywords,\nphrases, or sentiments that indicate a position in favor, against, or neutral. 4. Consider\nthe broader implications: Assess how the context’s position on the specific issue relates\nto the broader domain. For example, if the context supports a woman’s right to choose in\nthe domain of abortion, this would imply a stance in favor of abortion rights. 5. Choose\nthe most appropriate stance: Based on your analysis, select the stance that best represents\nthe position of the context in relation to the specified domain from the given choices (A)\nnone, (B) against, or (C) favor. If the context is not relevant to the domain, choose (A)\nnone.\nEmotion20\nPlease perform emotion recognition task.\nclasses: (A) anger (B) joy (C) sadness (D) optimism\nPRTS: 1. Read and understand the given context. 2. Identify the emotions expressed\nin the context. Pay attention to that (B) Joy is a transient feeling of happiness and\nfulfillment (C) while optimism is a lasting positive outlook on the future. If typically\nassociated with a low mood, a lack of energy, and a tendency to withdraw from social\ninteractions. Consider (C) sadness rather than (D) 3. Match the identified emotions with\nthe given choices. 4. Select the most appropriate choice as the answer.\n2466\nDataset Description\n20news\nPlease perform topic classification task from choices.\nmanual classes:\n(A) alt.atheism includes discussions and articles related to atheism, atheistic beliefs,\nand criticisms of religion (B) comp.graphics covers topics related to computer graphics,\nincluding discussions on image processing, rendering, and computer-aided design (CAD)\n(C) comp.os.ms-windows.misc focuses on discussions related to the Microsoft Windows\noperating system, including troubleshooting, software recommendations, and general\nWindows-related topics (D) comp.sys.ibm.pc.hardware revolves around discussions\nrelated to IBM-compatible PC hardware, including topics like motherboards, processors,\nmemory, and peripherals (E) comp.sys.mac.hardware deals with discussions related to\nApple Macintosh hardware, including Mac models, peripherals, and troubleshooting (F)\ncomp.windows.x focuses on the X Window System, a widely used Unix-like operating\nsystems, including System configuration, applications, and programming (G) misc.forsale\nincludes postings of items for sale, ranging from electronics to household goods (H)\nrec.autos covers discussions related to automobiles, including car models, maintenance,\nbuying/selling advice, and automotive technologies (I) rec.motorcycles includes discussions\nrelated to motorcycles, covering topics such as different motorcycle models, maintenance,\nsafety, and riding experiences (J) rec.sport.baseball focuses on discussions related to\nbaseball, including game analysis, player statistics, team performance, and baseball news\n(K) rec.sport.hockey covers discussions related to ice hockey, including game analysis,\nplayer statistics, team performance, and hockey news (L) sci.crypt revolves around\ndiscussions related to cryptography, encryption algorithms, and cryptographic protocols (M)\nsci.electronics deals with discussions related to electronics, including electronic circuits,\ncomponents, and troubleshooting (N) sci.med includes discussions related to medical topics,\nincluding diseases, treatments, healthcare practices, and medical research (O) sci.space\nfocuses on discussions related to space exploration, astronomy, and topics related to\nouter space (P) soc.religion.christian covers discussions related to Christianity, including\ntheological debates, biblical interpretations, and religious practices (Q) talk.politics.guns\nrevolves around discussions related to gun ownership, gun control policies, and the Second\nAmendment in the United State (R) talk.politics.mideast includes discussions related to\npolitics in the Middle East, covering topics such as conflicts, peace negotiations, and\nregional dynamics (S) talk.politics.misc covers discussions related to politics that do not fit\ninto the other specific political categories (T) talk.religion.misc includes discussions related\nto religion that do not fit into the other specific religious categories\ndefault classes : (A) alt.atheism (B) comp.graphics (C) comp.os.ms-windows.misc\n(D) comp.sys.ibm.pc.hardware (E) comp.sys.mac.hardware (F) comp.windows.x (G)\nmisc.forsale (H) rec.autos (I) rec.motorcycles (J) rec.sport.baseball (K) rec.sport.hockey\n(L) sci.crypt (M) sci.electronics (N) sci.med (O) sci.space (P) soc.religion.christian\n(Q)talk.politics.guns (R) talk.politics.mideast (S)talk.politics.misc (T) talk.religion.misc\n2467\nDataset Description\ni2b2\nWhat’s the relation between <e1>treatment or medical test or medical problem</e1> and\n<e2>medical problem</e2>?\nclasses:\n(A) <e1>Treatment</e1> improves <e2>medical problem</e2> (B) <e1>Treatment</e1>\nworsens <e2>medical problem</e2> (C) <e1>Treatment</e1> causes <e2>medical\nproblem</e2> (D) <e1>Treatment</e1> is administered for <e2>medical problem</e2>\n(E) <e1>Treatment</e1> is not administered because of <e2>medical problem</e2> (F)\n<e1>Medical Test</e1> reveals <e2>medical problem</e2> (G) <e1>Medical Test</e1>\nconducted to investigate <e2>medical problem</e2> (H) <e1>Medical problem</e1>\nindicates <e2>medical problem</e2>\nPRTS: 1. Read the context carefully and identify the e1 and e2 elements. 2. Understand\nthe meaning of e1 and e2 in the context. 3. Analyze the relationship between e1 and e2\nbased on the context. Keep in mind that the relationship could be between medical problems,\ntreatments, or tests. Pay close attention to whether the relationship implies improvement,\nworsening, causation, or a decision not to administer a treatment or test. Also, consider if the\ntreatment is ineffective or has an unintended effect on the medical problem. 4. Compare the\nrelationship with the given choices and select the one that best matches the context. If the\nrelationship is between two medical problems, consider choice (H) as a possible answer. If\nthe context suggests a decision not to administer a treatment or test, consider choice (E) as a\npossible answer. If the treatment is ineffective or worsens the medical problem, consider\nchoice (B) as a possible answer. Be cautious not to confuse choice (B) with choice (D) when\nthe treatment is administered but does not have the desired effect on the medical problem.\n2468\nDataset Description\nChemProt\nWhat’s the relation between <e1>Chemical</e1> and <e2>Protein</e2>?\nclasses:\n<e1>Chemical</e1> upregulation, activate, promote, increase activity of\n<e2>Protein</e2>\n<e1>Chemical</e1> downregulation, inhibitor, block, decrease activity of\n<e2>Protein</e2>\n<e1>Chemical</e1> is agonist of <e2>Protein</e2>\n<e1>Chemical</e1> is antagonist of <e2>Protein</e2>\n<e1>Chemical</e1> is the substrate metabolic of <e2>Protein</e2>\nPRTS: 1. Read the context and identify the key terms (e1 and e2). 2. Understand the\nrelationship between e1 and e2 based on the context. If the relationship is not explicitly\nstated, try to infer it from the information provided, including any indirect indications of\ntheir interaction. 3. Carefully analyze the inferred relationship and ensure it aligns with\nthe context. Pay attention to the specific actions or effects of the chemical on the protein,\nor any other relevant information. 4. Match the relationship or inferred relationship with\none of the given choices (A, B, C, D, or E), considering the specific actions or effects\nmentioned in the context and any indirect indications of their interaction. 5. Select the\nappropriate choice as the answer, considering both explicit and inferred information from\nthe context, and double-check to ensure it accurately reflects the relationship between e1\nand e2.\nDDI\nWhat’s the relation between <e1>Drug</e1> and <e2>Drug</e2>?\nmanual classes:\nPharmacokinetics mechanism between <e1>Drug</e1> and <e2>Drug</e2>, the process\nof absorption, distribution, metabolism and excretion of a drug.\nPharmacodynamics mechanism between <e1>Drug</e1> and <e2>Drug</e2>, i.e., the\nmechanism of action and effect of the drug in the body.\nA recommendation or advice regarding a interaction between <e1>Drug</e1> and\n<e2>Drug</e2> is given.\nAn interaction between <e1>Drug</e1> and <e2>Drug</e2> appears without providing\nany additional information.\ndefault classes: Pharmacokinetics, Pharmacodynamics, advise, int\nPRTS: 1. Carefully read the context sentence and identify the relationship between\nthe two drugs mentioned. Pay close attention to the specific details of the interaction,\nsuch as metabolism, mechanism of action, recommendations, or lack of additional\ninformation. 2. Review the given choices and understand the differences between them.\n3. Compare the relationship described in the context sentence with the given choices,\nfocusing on the specific details of the interaction or the absence of such details. If there is\nan recommendation or advice regarding an interaction between the drugs, consider (C). If\nthe emphasis is on the action of the drug in the body, choose (B) Pharmacodynamics:\nmechanism of action and effect of the drug in the body. (D) Without any additional\ninformation, consider (D). 4. Select the appropriate choice based on the context and the\ndefinition provided in the choices, ensuring that it aligns with the details mentioned in the\ncontext sentence or the lack thereof. Double-check your selection before finalizing your\nanswer, and consider whether the chosen option truly reflects the information provided in\nthe context.\nTable 8: Description of tasks.\n2469\nSetting Default Template Auto/Random/Similar\nTemplates\nOther Templates\nzero-shot\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"answer\":\n{question}\n\"classes\": {classes}\n\"input\": {input}\nLet’s think step by step.\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"thinking steps\": {think-\ning steps}\nLet’s think step by step.\nfew-shot\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"answer\": {answer}\n...\n\"input\": {input}\n\"answer\":\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"answer\": {answer}\n...\n\"input\": {input}\nLet’s think step by step.\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"answer\": {answer}\n...\n\"input\": {input}\n\"thinking steps\": {think-\ning steps}\nLet’s think step by step.\nfew-shot-CoT /\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"answer\": {answer with\nCoT}\n...\n\"input\": {input}\nLet’s think step by step.\n{question}\n\"classes\": {classes}\n\"input\": {input}\n\"answer\": {answer with\nCoT}\n...\n\"input\": {input}\n\"thinking steps\": {think-\ning steps}\nLet’s think step by step.\nTable 9: Prompt templates. Elements in braces {} are replaced with question-specific values.\n2470",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7516258955001831
    },
    {
      "name": "Computer science",
      "score": 0.7297762036323547
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6389542818069458
    },
    {
      "name": "Language model",
      "score": 0.4660898745059967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4640594720840454
    },
    {
      "name": "Natural language processing",
      "score": 0.372622013092041
    },
    {
      "name": "Engineering",
      "score": 0.09203225374221802
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}