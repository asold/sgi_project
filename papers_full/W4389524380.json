{
  "title": "Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients",
  "url": "https://openalex.org/W4389524380",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2229875770",
      "name": "Feihu Jin",
      "affiliations": [
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2120994304",
      "name": "Jiajun Zhang",
      "affiliations": [
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2144418999",
      "name": "Chengqing Zong",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Shandong Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946659172",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3187127611",
    "https://openalex.org/W4380609152",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2285990425",
    "https://openalex.org/W4282961290",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4320165905",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W4372347727",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4286856918",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming. Recent parameter-efficient tuning methods such as Adapter tuning, Prefix tuning, and LoRA allow for updating a small subset of parameters in large language models. However, they can only save approximately 30% of the training memory requirements, due to the problem that gradient computation and backpropagation are still necessary for these methods. This paper proposes a novel parameter-efficient tuning method for LLMs without calculating their gradients. Leveraging the discernible similarities between the parameter-efficient modules of the same task learned by both large and small language models, we put forward a strategy for transferring the parameter-efficient modules, originally derived from small language models to much larger ones. To ensure a smooth and effective adaptation process, we further introduce a Bridge model to guarantee dimensional consistency while also stimulating a dynamic interaction between the models. We demonstrate the effectiveness of our method using the T5 and GPT-2 series of language models on the SuperGLUE benchmark. Our method achieves comparable performance to both fine-tuning and parameter-efficient tuning on large language models without needing gradient-based optimization. Additionally, our method achieves up to 5.7x memory reduction compared to parameter-efficient tuning.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 321‚Äì330\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nParameter-efficient Tuning for Large Language Model without Calculating\nIts Gradients\nFeihu Jin1,2, Jiajun Zhang1,2,3,4 ‚àóand Chengqing Zong1,2\n1Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3Wuhan AI Research\n4Shanghai Artificial Intelligence Laboratory, Shanghai, China\njinfeihu2020@ia.ac.cn ‚Ä†, {jjzhang,cqzong}@nlpr.ia.ac.cn\nAbstract\nFine-tuning all parameters of large language\nmodels (LLMs) requires significant computa-\ntional resources and is time-consuming. Re-\ncent parameter-efficient tuning methods such\nas Adapter tuning, Prefix tuning, and LoRA\nallow updating a small subset of parameters\nin large language models. However, they can\nonly save approximately 30% of the training\nmemory requirements because gradient compu-\ntation and backpropagation are still necessary\nfor these methods. This paper proposes a novel\nparameter-efficient tuning method for LLMs\nwithout calculating their gradients. Leverag-\ning the discernible similarities between the\nparameter-efficient modules of the same task\nlearned by both large and small language mod-\nels, we put forward a strategy for transferring\nthe parameter-efficient modules derived ini-\ntially from small language models to much\nlarger ones. To ensure a smooth and effec-\ntive adaptation process, we introduce a Bridge\nmodel to guarantee dimensional consistency\nwhile stimulating a dynamic interaction be-\ntween the models. We demonstrate the effec-\ntiveness of our method using the T5 and GPT-2\nseries of language models on the SuperGLUE\nbenchmark. Our method achieves compara-\nble performance to fine-tuning and parameter-\nefficient tuning on large language models with-\nout needing gradient-based optimization. Addi-\ntionally, our method achieves up to 5.7√ómem-\nory reduction compared to parameter-efficient\ntuning.\n1 Introduction\nLarge language models such as GPT3 (Brown et al.,\n2020), GPT4 (OpenAI, 2023), T5-XXL (Raffel\net al., 2020), and LLaMA (Touvron et al., 2023)\nhave demonstrated remarkable capabilities in var-\nious natural language processing tasks. However,\n‚àóCorresponding Author\n‚Ä†The author is currently a PhD student in Peking Univer-\nsity. The new email address is fhjin@stu.pku.edu.cn\n20 40 60 80\nMemory Usage on GPT2-XL (GB)\n70\n72\n74\n76\n78\n80Accuracy (%) Fine-tuning (75.9GB)\nAdapter tuning (51.8GB)\nPrefix tuning (53.5GB)\nLoRA (49.6GB)\nOurs (18.7GB)\n(a)\n1.5 2.0 2.5\nFine-tuned Parameters (%)\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0Cosine Similarity (%)\nBase vs. XL (Prefix)\nBase vs. XL (LoRA)\nMedium vs. XL (Prefix)\nMedium vs. XL (LoRA) (b)\nFigure 1: (a) Comparison of the GPU memory usage\nduring training for fine-tuning, Adapter tuning, LoRA,\nPrefix tuning, and our method on the RTE task. (b) The\ncosine similarity of the parameter-efficient modules on\nthe GPT-2 series of language models over the RTE task.\nthe sheer number of parameters in these models\nposes challenges for fine-tuning on common hard-\nware. Parameter-efficient tuning methods (e.g.,\nAdapter tuning, Prefix tuning, or LoRA) typically\ninvolve adding a small number of parameters to\nthe language model and only fine-tuning the added\nsubset of parameters, achieving comparable perfor-\nmance to full fine-tuning. Adapter tuning learns\nthe task-specific information (Houlsby et al., 2019;\nMahabadi et al., 2021b,a) by inserting small task-\nspecific modules within layers of the Transformer.\nPrefix tuning (Li and Liang, 2021; Liu et al., 2021b)\nprepends task-specific trainable prompt tokens to\nthe hidden states within every intermediate Trans-\nformer layer. LoRA (Hu et al., 2021) merges the\nlow-rank and trainable matrices with the frozen\nweights at each layer of the Transformer. In Figure\n1(a), we show the comparison of the GPU memory\nusage during training for fine-tuning, Adapter tun-\ning, LoRA, and Prefix tuning on the Recognizing\nTextual Entailment (RTE) (Bar-Haim et al., 2014)\ntask. These parameter-efficient tuning methods\ncan save approximately 30% of the GPU memory\nrequirements but still rely on gradient-based opti-\nmization, resulting in increased memory demands\nfor LLMs.\n321\nTo address the above limitations, we propose a\nnovel parameter-efficient tuning method for LLMs\nwithout calculating their gradients. Intuitively, both\nlarge and small language models (SLMs) can learn\nsimilar task-specific characteristics when applied\nto downstream tasks. We conduct experiments\nusing existing parameter-efficient tuning methods\non the RTE task in the SuperGLUE benchmark\n(Wang et al., 2019) to validate this hypothesis. As\ndepicted in Figure 1(b), we calculate the similar-\nity between the parameter-efficient tuning mod-\nules derived from GPT2-XL and GPT2-base or\nGPT2-medium1. Specifically, we apply the LoRA\nmethod on GPT2-XL and GPT2-medium to ob-\ntain the parameter-efficient modules of the two lan-\nguage models and calculate the cosine similarity\nof the two parameter-efficient modules. We find\nthat the cosine similarity can reach up to 75%. Our\nobservations indicate that these modules exhibit\ncomparable task-specific characteristics throughout\nthe learning process for specific downstream tasks.\nInspired by these findings, if we can successfully\ntransfer the task-specific characteristics learned by\nthe small language model to the large language\nmodel, we can enrich the task-specific capabilities\ninto the LLMs without needing gradient-based op-\ntimization.\nIn this paper, we first utilize existing parameter-\nefficient tuning methods in a small language model\nto learn the task characteristics of downstream\ntasks. Intuitively, we can directly apply the\nparameter-efficient module obtained from the small\nlanguage model to the large language model. How-\never, this would face crucial issues of dimension\nmismatch and limited interaction with the large\nlanguage model. To address the issue of dimen-\nsion mismatch, we employ a projection module\nto align the dimensions of the parameter-efficient\nmodules between SLMs and LLMs. Furthermore,\nto enrich the interaction between the parameter-\nefficient module and the large language model, we\nintroduce a Bridge model that can retain the knowl-\nedge of the large language model while interacting\nwith the parameter-efficient module, obtaining a\nparameter-efficient module with dimensions match-\ning the large language model. Finally, we seam-\nlessly plug the acquired parameter-efficient module\ninto the large language model for inference. We\nconduct comprehensive experiments on T5 series\n1GPT2-base contains 117M parameters, GPT2-medium\ncontains 345M parameters, and GPT2-XL contains 1542M\nparameters.\n(Raffel et al., 2020) and GPT2 series (Radford et al.,\n2019) of language models to assess the effective-\nness of our method using the SuperGLUE bench-\nmark, a widely recognized evaluation benchmark\nfor natural language understanding. The results\ndemonstrate that our method performs on par with\nfine-tuning and parameter-efficient tuning on large\nlanguage models without needing gradient-based\noptimization. Additionally, our proposed method\nachieves up to 5.7√ómemory reduction compared to\nparameter-efficient tuning. Our findings highlight\nthe potential of bridging small and large language\nmodels, thereby efficiently leveraging expansive\nlarge language models. In summary, our key con-\ntributions can be listed as follows:\n‚Ä¢ Our analysis reveals a substantial task similar-\nity when applying parameter-efficient tuning\nmethods to SLMs and LLMs for downstream\ntasks.\n‚Ä¢ We propose a gradient-free method to adapt\nthe parameter-efficient modules learned in a\nsmall language model to a large language\nmodel.\n‚Ä¢ Extensive experiments on SuperGLUE bench-\nmark under both T5 series and GPT-2 series\nof language models verify the effectiveness of\nour proposed method and achieve up to 5.7√ó\nmemory reduction compared to parameter-\nefficient tuning.\n2 Method\nThe proposed method utilizes parameter-efficient\ntuning modules to effectively bridge small and\nlarge language models, enriching the task-specific\ncapabilities into the large language model with-\nout needing gradient-based optimization. As de-\npicted in Figure 2, the method consists of training\nand inference. We first employ parameter-efficient\ntuning methods during the training stage to learn\ntask-specific characteristics in the small language\nmodel. Then, we fine-tune the Bridge model and\nthe acquired parameter-efficient module to enhance\nthe knowledge of the parameter-efficient module.\nFinally, we directly plug the parameter-efficient\nmodule into the large language model during the\ninference stage for efficient predictions.\nPlug-in and Bridge Model Fine-tuning: We first\nutilize existing parameter-efficient tuning methods\nsuch as Adapter (Houlsby et al., 2019), LoRA (Hu\n322\nSLM\nAdapter\nPrefix\nLoRA LLMBridge \nModel\nLLM\nAdapter\nPrefix\nLoRA\nInput\nPrediction\nOutput\nùüèùüè/ùíìùíì\n(a) Training\n(b) Inference\nùüèùüè/ùíìùíìùëæùëæùíëùíëùíëùíëùíëùíëùíëùíë\nùëæùëæùíëùíëùíëùíëùíëùíëùíëùíë\nùëæùëæùíÖùíÖùíÖùíÖùíÖùíÖùíÖùíÖ\nInput\nFigure 2: (a) In the training stage, we use parameter-efficient tuning methods to learn task-specific characteristics in\na smaller model and fine-tune the Bridge model with the acquired parameter-efficient modules. (b) In the inference\nstage, we directly plug the parameter-efficient modules into the large language model for efficient predictions.\net al., 2021), or Prefix tuning (Li and Liang, 2021)\nin a small language model to learn the task char-\nacteristics of downstream tasks. However, directly\napplying the parameter-efficient modules obtained\nfrom the small language model to the large lan-\nguage model would face two issues: dimension\nmismatch and limited interaction with the large lan-\nguage model. To address the issue of dimension\nmismatch, we employ a linear projection module\nWplug as the Plug-in model to align the dimen-\nsions of the parameter-efficient module with the\nlarge language model. Furthermore, considering\nthat small language models usually have fewer lay-\ners than large language models, we address the\nlayer mismatch by duplicating the layers of the\nparameter-efficient modules. This duplication en-\nables us to achieve layer alignment with the large\nlanguage model.\nIntuitively, successfully adapting the parameter-\nefficient modules to the large language models re-\nquires a substantial interaction between them and\nthe large language models. To enrich the inter-\naction, we introduce a Bridge model that can re-\ntain the knowledge of the large language model\nwhile interacting with the parameter-efficient mod-\nule. We employ the pruning method from Ladder-\nside-tuning (Sung et al., 2022) to obtain such a\nBridge model, which involves pruning each layer\nof the large language model f. We use linear pro-\njections to downsample the intermediate activa-\ntions, including word embeddings, from the large\nlanguage model f to a lower-dimensional Bridge\nmodel g, with a reduction factor of r, where r can\nbe 8, 16, 32, 64, etc. To retain crucial informa-\ntion from the large language model, we leverage\nFisher information (Liu et al., 2021a; Sung et al.,\n2021) to prune the parameters of the large language\nmodel and obtain the initial Bridge model g. Fisher\ninformation could effectively evaluate the impor-\ntance of parameters in the large language model.\nGiven the W ‚ààRdb√ódl of the backbone network\nthat maps the dl-dim vectors to the db-dim space,\nwe calculate the importance of each weight vector\nthrough\nW = 1\n|D|\n|D|‚àë\ni=1\n(‚àáW log p(yi|xi))2,\nwhere (xi, yi) are samples from data D. Then, we\nkeep the rows and columns of the W, which have\nthe db\nr and dl\nr importance scores. Through itera-\ntions of this process in each layer of the Trans-\nformer, we obtain a set of weight matrices WB ‚àà\nR\ndb\nr √ódl\nr that have undergone pruning 1/r times\nfrom the backbone network, and we utilize them to\ninitialize the Bridge model.\nSubsequently, in order to make the parameter-\nefficient modules (e.g., Adapter, LoRA, or Prefix\ntuning) learned in the small language model in-\nteract with the Bridge model, we apply the linear\n323\nAlgorithm 1 Adaptation of Parameter-efficient Modules for Large Language Model\nRequire: Large language model M, a small language model S, the training set D = {(x1, y1), ¬∑¬∑\n¬∑, (xn, yn)}, linear projection modules Wplug and Wdown, and parameter-efficient tuning methods\n(PEFT) (e.g., Adapter tuning, Prefix tuning, and LoRA)\n1: Apply the PEFT on S and fine-tuning S on D to obtain parameter-efficient modules\n2: Employ the Wplug to align the dimensions of the parameter-efficient modules with M\n3: Prune the parameters of the M and get a Bridge model g\n4: Apply the linear projection module Wdown on parameter-efficient modules\n5: for each instance (x1, y1) in D do\n6: Fine-tune the parameter-efficient modules together with the Bridge model g\n7: end for\n8: Plug the parameter-efficient modules and the linear projection modules Wplug into the M\nprojection module Wdown on parameter-efficient\nmodules and fine-tune the parameter-efficient mod-\nules together with the Bridge model g. This fine-\ntuning process enables us to achieve two objectives:\nobtaining parameter-efficient modules that match\nthe dimensions of the large language model and\nenriching these modules with knowledge from the\nlarge language model.\nInference: Once the training of the parameter-\nefficient modules and the Bridge model g is com-\nplete, we integrate the trained parameter-efficient\nmodules, enriched with knowledge from the large\nlanguage model, into the large language model.\nThis integration empowers the large language\nmodel to leverage the task-specific knowledge cap-\ntured by the parameter-efficient modules during the\ninference process without requiring gradient-based\noptimization. The complete algorithm is depicted\nin Algorithm 1.\n3 Experiments\n3.1 Experimental Settings\nWe conduct extensive experiments on eight natural\nlanguage understanding tasks from the SuperGLUE\nbenchmark, including BoolQ (Clark et al., 2019),\nCB (De Marneffe et al., 2019), COPA (Roemmele\net al., 2011), MultiRC (Khashabi et al., 2018),\nRTE (Bar-Haim et al., 2014), WiC (Pilehvar and\nCamacho-Collados, 2019), WSC (Levesque et al.,\n2012), and ReCoRD (Zhang et al., 2018). For\neach task, we report the accuracy or F1-score. In\nour experiments, we evaluate the effectiveness of\nour method using both the GPT2 series (Radford\net al., 2019) of autoregressive language models and\nthe T5 series (Raffel et al., 2020) of sequence-to-\nsequence language models. In the GPT2 series of\nmodels, we designate the GPT2-base as the small\nmodel and the GPT2-XL as the large model. For\nthe T5 series of models, we classify the T5-base\nand T5-large as small language models, while the\nT5-3B and T5-XXL are considered large language\nmodels for our experiments. To obtain the Bridge\nmodel, we set the reduction factor r = 16. In\nTable 2, we provide the model parameters of the\nsmall language models, large language models, and\nBridge models. It is worth noting that the model\nparameters of the Bridge models are significantly\nsmaller than those of the large language models.\nThe training process of our proposed method is\nconducted on an NVIDIA A100 GPU with 80GB\nof memory.\nOur objective is to demonstrate that the\nparameter-efficient modules learned on the small\nlanguage models can be effectively adapted to the\nlarge language models, achieving comparable per-\nformance to full fine-tuning and parameter-efficient\ntuning without needing gradient-based optimiza-\ntion. We list the baselines as follows:\nFine-tuning: The vanilla Transformer fine-\ntuning.\nAdapter tuning: Inserting a small task-specific\nmodule between the self-attention module (and the\nMLP module) and the subsequent residual con-\nnection at each Transformer layer (Houlsby et al.,\n2019).\nPrefix tuning: Adding trainable continuous\nprompt vectors to the Key and Value components of\nthe attention layer at each layer of the Transformer\nmodel (Li and Liang, 2021).\nLoRA: Merging the low-rank and trainable ma-\ntrices with the frozen weights at each layer of the\nTransformer (Hu et al., 2021).\nAdapter tuning Plug-in: By applying the\nAdapter tuning method to small language models,\n324\nMethod #Params BoolQ\nAcc.\nCB\nAcc./F1\nCOPA\nAcc.\nMultiRC\nEM/F1a\nRTE\nAcc.\nWiC\nAcc.\nWSC\nAcc.\nReCoRD\nAcc./F1\nGPT2-base FT (Radford et al., 2019) 100% 71.2 78.6/55.8 64.4 65.8/17.4 67.8 65.5 63 72.1/71.4\nGPT2-base Adapter - 71.5 79.3/56.3 65.8 65.2/18.2 67.5 65.7 62.9 72.8/71.9\nGPT2-base Prefix tuning - 70.6 80.3/58.2 65.1 64.7/16.9 67.3 64.9 63.1 71.4/70.5\nGPT2-base LoRA - 71.4 79.6/57.8 65.7 66.2/18.6 67.2 65.3 62.5 71.8/70.9\nGPT2-XL FT (Radford et al., 2019) 100% 82.4 87.4/90.6 76.5 76.4/37.6 79.6 74.9 81.8 84.4/83.8\nGPT2-XL Adapter 1.98% 81.8 86.9/90.8 76.2 75.4/36.2 78.9 74.2 81.5 84.2/83.1\nGPT2-XL Prefix tuning 1.76% 81.0 86.7/89.9 74.9 76.7/36.9 79.2 74.3 81.4 83.2/83.1\nGPT2-XL LoRA 1.55% 82.1 87.1/90.3 76.2 76.2/37.1 79.4 75.1 81.1 84.4/83.3\nGPT2-XL Adapter Plug-in 0% 81.6 86.7/90.4 75.6 75.8/36.7 78.7 74.3 80.8 83.7/82.5\nGPT2-XL Prefix tuning Plug-in 0% 81.2 86.3/89.4 75.1 76.1/36.4 78.5 73.8 80.1 83.5/82.6\nGPT2-XL LoRA Plug-in 0% 81.5 86.2/89.3 75.3 75.6/36.8 78.3 74.1 80.3 83.9/82.8\nTable 1: Results on the SuperGLUE benchmark. The Adapter Plug-in, Prefix tuning Plug-in, and LoRA Plug-in are\nthe parameter-efficient modules learned in the small language models adapted to the large language models. 0%\nmeans we do not update any parameter within the large language models.\nwe obtain parameter-efficient modules that can be\nadapted to large language models.\nPrefix tuning Plug-in: By applying the Prefix\ntuning method to small language models, we obtain\nparameter-efficient modules that can be adapted to\nlarge language models.\nLoRA Plug-in: By applying the LoRA method\nto small language models, we obtain parameter-\nefficient modules that can be adapted to large lan-\nguage models.\nModels SLMs LLMs BMs\nGPT2-base and GPT2-XL 117M 1542M 96M\nT5-base and T5-3B 220M 2800M 175M\nT5-large and T5-XXL 770M 11000M 688M\nTable 2: Comparison of the model parameters on the\nsmall language models (SLMs), large language models\n(LLMs), and the Bridge models (BMs). To obtain the\nBridge model, we set the reduction factor r = 16.\n3.2 Main Results\n3.2.1 Experiments on GPT-2 Series of Models\nTable 1 shows the performance of our proposed\nmethod using the GPT2 series of models (Radford\net al., 2019). In Table 1, we present the results\nobtained with GPT2-base as the small language\nmodel and GPT2-XL as the large language model.\nThe Adapter Plug-in, Prefix tuning Plug-in, and\nLoRA Plug-in are the parameter-efficient modules\nlearned in the GPT2-base model adapted to the\nGPT2-XL model. As can be seen, in compari-\nson to directly conducting vanilla fine-tuning on\nthe large language model, our method achieves\ncomparable results. Furthermore, compared to\nparameter-efficient tuning methods applied to the\nlarge language model, our method demonstrates\ncomparable performance without the need to fine-\ntune any parameters of the large language model.\nSpecifically, it exhibits a slight improvement com-\npared to the prefix-tuning method on the BoolQ\nand COPA tasks, demonstrating the effectiveness\nof our method.\n3.2.2 Experiments on T5 Series of Models\nTable 3 and Table 4 display the performance of\nour proposed method using the T5 series of mod-\nels (Raffel et al., 2020). In Table 3, we present\nthe results obtained with T5-base as the small lan-\nguage model and T5-3B as the large language\nmodel. The Adapter Plug-in, Prefix tuning Plug-in,\nand LoRA Plug-in are parameter-efficient modules\nof our method used to adapt T5-3B. Our method\nachieves comparable results on all eight Super-\nGLUE tasks without the need to fine-tune any pa-\nrameters of the T5-3B, demonstrating the effective-\nness of our method. Particularly in BoolQ, CB,\nRTE, and ReCoRD, our method performs at a level\nsimilar to full fine-tuning. Similarly, Table 4 shows\nthe results using T5-large as the small language\nmodel and T5-XXL as the large language model,\nwith the same Plug-ins employed to adapt T5-XXL.\nOur method achieves comparable results on all\neight SuperGLUE tasks without the need to fine-\ntune any parameters of the T5-XXL. Furthermore,\ncompared to parameter-efficient tuning methods\napplied to the large language model, our method\ndemonstrates comparable performance without the\nneed to fine-tune any parameters of the large lan-\nguage model. Especially when compared to the\n325\nMethod #Params BoolQ\nAcc.\nCB\nAcc./F1\nCOPA\nAcc.\nMultiRC\nEM/F1a\nRTE\nAcc.\nWiC\nAcc.\nWSC\nAcc.\nReCoRD\nAcc./F1\nT5-base FT (Raffel et al., 2020) 100% 81.4 81.4/91.0 71.2 79.7/43.1 81.5 68.3 80.8 75.0/74.2\nT5-base Adapter - 81.5 82.6/93.5 71.5 79.3/43.2 81.2 68.8 80.3 75.2/74.6\nT5-base Prefix tuning - 81.1 81.9/93.2 70.1 79,2/42.1 80.8 67.9 79.9 74.6/73.5\nT5-base LoRA - 81.7 82.1/94.3 70.4 79.5/42.6 81.2 69.1 80.5 74.8/74.1\nT5-3B FT (Raffel et al., 2020) 100% 89.9 90.3/94.4 92 86.8/58.3 90.7 72.1 90.4 91.2/90.4\nT5-3B Adapter 2.73% 89.7 90.1/93.2 91.5 86.9/59.1 91.1 71.3 90.5 91.1/90.3\nT5-3B Prefix tuning 2.12% 89.4 89.0/93.4 91.1 86.4/57.1 90.5 70.9 89.1 90.6/89.2\nT5-3B LoRA 2.45% 90.4 90.8/95.0 92.1 86.2/57.6 91.0 71.8 90.6 91.3/91.1\nT5-3B Adapter Plug-in 0% 89.5 89.6/94.7 91.7 86.5/58.2 90.8 70.9 90.2 91.6/90.7\nT5-3B Prefix tuning Plug-in 0% 89.1 89.4/93.8 91.2 86.1/57.4 90.4 70.7 89.5 90.3/89.9\nT5-3B LoRA Plug-in 0% 89.6 90.0/94.9 91.8 86.9/58.5 90.1 71.0 89.8 90.9/90.1\nTable 3: Results on the SuperGLUE benchmark. The Adapter Plug-in, Prefix tuning Plug-in, and LoRA Plug-in\nare the parameter-efficient modules learned in the small language models adapted to the large language models.\nThe full fine-tuning performance of T5-base and T5-3B models is based on the research conducted by Raffel et al.\n(2020). 0% means we do not update any parameter within the large language models.\nMethod #Params BoolQ\nAcc.\nCB\nAcc./F1\nCOPA\nAcc.\nMultiRC\nEM/F1a\nRTE\nAcc.\nWiC\nAcc.\nWSC\nAcc.\nReCoRD\nAcc./F1\nT5-large FT (Raffel et al., 2020) 100% 85.4 91.6/94.8 83.4 83.3/50.7 87.8 69.3 86.3 86.8/85.9\nT5-large Adapter - 85.3 90.9/93.7 83.2 83.1/50.1 87.3 69.8 86.7 86.2/85.1\nT5-large Prefix tuning - 84.9 91.4/95.3 82.9 82.7/49.6 87.5 68.9 85.5 86.1/84.9\nT5-large LoRA - 85.1 90.9/94.3 84.2 83.6/51.2 87.3 70.2 85.9 86.7/85.7\nT5-XXL FT (Raffel et al., 2020) 100% 91.2 93.9/96.8 94.8 88.1/63.3 92.5 76.9 93.8 94.1/93.4\nT5-XXL Adapter 6.42% 90.8 93.6/95.9 93.7 87.9/63.0 92.8 75.7 93.5 92.8/91.7\nT5-XXL Prefix tuning 6.22% 89.3 93.5/94.4 92.7 86.9/63.1 92.3 74.9 92.8 93.1/91.8\nT5-XXL LoRA 6.15% 91.5 93.4//95.8 93.6 87.8/63.5 92.7 75.7 93.6 93.7/92.8\nT5-XXL Adapter Plug-in 0% 90.4 93.1/95.6 94.1 87.2/62.8 92.3 75.2 93.2 93.4/92.2\nT5-XXL Prefix tuning Plug-in 0% 89.9 93.2/94.6 92.3 86.3/62.1 91.8 74.7 92.4 93.5/92.5\nT5-XXL LoRA Plug-in 0% 90.5 92.4//95.7 93.2 86.9/62.5 92.1 74.9 92.9 93.8/92.7\nTable 4: Results on the SuperGLUE benchmark. The Adapter Plug-in, Prefix tuning Plug-in, and LoRA Plug-in are\nthe parameter-efficient modules learned in the small language models adapted to the large language models. The\nfull fine-tuning performance of T5-large and T5-XXL models is based on the research conducted by Raffel et al.\n(2020). 0% means we do not update any parameter within the large language models.\nprefix-tuning method, our proposed method shows\na slight improvement in the CB, COPA, and WSC\ntasks when applied to the T5-3B model. Similarly,\nwhen applied to the T5-XXL model, our method\nslightly improves the BoolQ, CB, and ReCoRD\ntasks.\nIt is evident that as the parameter size of pre-\ntrained language models grows, existing parameter-\nefficient tuning methods necessitate the addition of\nmore parameters to the large language model. In\ncontrast, our method does not entail any parameter\naugmentation but instead optimally harnesses the\ncapabilities of the large language model through\na plug-in approach. These findings indicate that\nour method can effectively utilize the knowledge\nof large language models without updating the pa-\nrameters of large language models, which suggests\nthe potential for application to even larger language\nmodels.\n3.2.3 Importance of the Reduction Factor r\nConsidering the impact of the reduction factor r\non the amount of knowledge retained in the Bridge\nmodel, we conduct experiments to analyze its im-\nportance. For verification, we select CB, RTE,\nand WiC in the SuperGLUE benchmark. As indi-\ncated in Table 5, we observe a gradual decrease in\nmodel performance as r increases. This is because\na higher value of r reduces the retained knowl-\nedge from the large language model to the Bridge\nmodel. According to Table 5, we can observe that\nwhen the model parameters of the Bridge model\n326\nDataset r=2 r=4 r=8 r=16 r=32 r=64\nCB 91.2 90.6 90.2 90.0 88.2 85.4\nRTE 90.8 90.5 90.5 90.1 89.1 84.3\nWiC 71.3 71.5 71.1 71.0 69.7 67.4\nTable 5: The accuracy on CB, RTE, and WiC with dif-\nferent reduction factors r. We perform the verification\non T5-3B using the LoRA Plug-in approach.\nare significantly smaller than that of the small lan-\nguage model, our method exhibits a noticeable per-\nformance decline during the entire inference pro-\ncess. However, when the model parameters of the\nBridge model are comparable to that of the small\nlanguage model, our method maintains the perfor-\nmance of the model without substantial degrada-\ntion. To strike a balance between model perfor-\nmance and the parameter of the Bridge model, we\nbelieve that selecting r = 16is a suitable choice.\n3.2.4 Memory Usage\nOur method not only achieves comparable perfor-\nmance to full fine-tuning and parameter-efficient\ntuning methods without updating any parameters\nof the large language models but also significantly\nachieves an impressive reduction in memory us-\nage. As shown in Table 6 and Table 7, we compare\nthe memory usage between our proposed method\nand the baseline models. When using the GPT2-XL\nmodel, we conduct experiments with a batch size of\n8 and a sequence length 512. Our method evidently\nachieves up to 7.1√ómemory savings compared to\nvanilla fine-tuning. Similarly, when utilizing the\nT5-3B model with a batch size of 1 and a sequence\nlength 512, our method achieves up to 5.1√ómem-\nory savings compared to vanilla fine-tuning.\nIn particular, we compare the existing parameter-\nefficient tuning methods, and in GPT2-XL, we can\nsee that our proposed method can achieve signif-\nicant memory savings. For example, comparing\nthe three parameter-efficient tuning methods of\nAdapter-tuning, Prefix tuning, and LoRA, our pro-\nposed method can achieve 5.3√ó, 5.6√ó, and 5.7√ó\nmemory reduction, respectively. Our method also\nachieves 2.9 √ó more memory savings compared\nto Ladder-side Tuning. In T5-3B, our proposed\nmethod can achieve 3.6√ó, 3.5√ó, and 3.6√ómemory\nreduction, respectively, compared to the Adapter-\ntuning, Prefix tuning, and LoRA. Similarly, our\nmethod also achieves 1.3√ó more memory savings\ncompared to Ladder-side Tuning. This demon-\nstrates that our method can be more effectively\napplied to existing large language models without\ncompromising performance. Furthermore, our pro-\nposed method does not slow down the inference\nspeed of the model. By utilizing the plug-in ap-\nproach, we can directly leverage the knowledge of\nthe large language model during inference without\ncompromising the speed.\nModels MU MR\nGPT2-XL FT 73746 1.0 √ó\nGPT2-XL Adapter 57903 1.3 √ó\nGPT2-XL Prefix tuning 56051 1.3 √ó\nGPT2-XL LoRA 54593 1.4 √ó\nLadder-side Tuning 17652 4.2 √ó\nGPT2-XL Adapter Plug-in 11203 6.6 √ó\nGPT2-XL Prefix tuning Plug-in 10745 6.9 √ó\nGPT2-XL LoRA Plug-in 10443 7.1√ó\nTable 6: Memory Usage (MU) and Memory Reduction\n(MR) compared to vanilla fine-tuning of our proposed\nmethod on a single NVIDIA A100 GPU with 80GB of\nmemory. Batch sizes are 8 and sequence lengths are\n512.\nModels MU MR\nT5-3B FT 77465 1.0 √ó\nT5-3B Adapter 54324 1.4 √ó\nT5-3B Prefix tuning 53324 1.5 √ó\nT5-3B LoRA 52134 1.5 √ó\nLadder-side Tuning 20385 3.8 √ó\nT5-3B Adapter Plug-in 15432 5.0 √ó\nT5-3B Prefix tuning Plug-in 15643 5.0 √ó\nT5-3B LoRA Plug-in 15323 5.1√ó\nTable 7: Memory Usage (MU) and Memory Reduction\n(MR) compared to vanilla fine-tuning of our proposed\nmethod on a single NVIDIA A100 GPU with 80GB of\nmemory. Batch sizes are 1 and sequence lengths are\n512.\n3.3 Utilize Bridge Model Directly?\nA natural question arises: Why not learn the\nparameter-efficient modules directly on the Bridge\nmodel instead of using a smaller language model\nand then applying the learned efficient modules\nto the larger language model? Intuitively, given\nthe learned parameter-efficient modules with the\nBridge model, we need a projection model to\nproject the dimensions to match the large language\nmodel. However, the projection model can only\nbe well learned with another interaction model.\nIn this section, we train the Bridge model by di-\nrectly initializing the parameter-efficient and lin-\n327\nMethod COPA RTE WiC WSC\nAdapter Plug-in (BM) 90.3 89.9 69.4 89.3\nLoRA Plug-in (BM) 90.7 89.8 70.1 89.4\nAdapter Plug-in (SLM) 91.7 90.8 70.9 90.2\nLoRA Plug-in (SLM) 91.8 90.1 71.0 89.8\nTable 8: The Accuracy on COPA, RTE, WiC, and WSC\ntasks with T5-3B. BM means the Bridge model obtained\nwith T5-3B. SLM means the small language model T5-\nbase.\near projection modules and investigate whether a\nBridge model is enough to learn the projection\nmodule. In Table 8, we conduct experiments using\nthe T5-3B model on the COPA, RTE, WiC, and\nWSC tasks. By plugging the parameter-efficient\nmodules learned from the Bridge model and the\nsmall language model into the T5-3B model, re-\nspectively, and comparing their performance, we\nfind that directly using the Bridge model consis-\ntently performs worse than utilizing the small lan-\nguage model enriched by a Bridge model across all\ntasks. The results indicate that the Bridge model\ncannot be utilized alone.\n4 Related Work\nFine-tuning and Parameter-efficient Tuning:\nLarge language models leverage parameterized\nTransformers as a foundational framework and\ntrain them on extensive unsupervised language cor-\npora (Brown et al., 2020; OpenAI, 2023; Touvron\net al., 2023; Raffel et al., 2020). Subsequently,\nspecific task objectives are introduced for down-\nstream tasks to perform full fine-tuning on the pre-\ntrained language models (Jin et al., 2022, 2023;\nMuennighoff et al., 2022). As the primary method\nfor optimizing pre-trained language models, full\nfine-tuning involves initializing the model with pre-\ntrained weights, updating all model parameters, and\nstoring a separate fully optimized model for each\ndownstream task. However, as the model parame-\nter grows, conducting full fine-tuning on existing\ncomputational devices becomes increasingly chal-\nlenging. In light of this, researchers have started\nexploring efficient methods for effectively harness-\ning the power of large language models.\nAs an efficient alternative, parameter-efficient\ntuning is a promising way of stimulating large lan-\nguage models. Compared to vanilla fine-tuning,\nparameter-efficient tuning methods only tune a\nsmall portion of the model parameters while keep-\ning the rest frozen. Adapter tuning methods,\nsuch as those proposed by Houlsby et al. (2019)\nand Mahabadi et al. (2021b,a), aim to learn task-\nspecific information by incorporating small-scale\ntask-specific modules into the layers of the Trans-\nformer. Prefix tuning methods, as introduced by Li\nand Liang (2021) and discussed in the work by Liu\net al. (2021b), also introduce additional parameters\nwithin the Transformer layers. LoRA, proposed\nby Hu et al. (2021), merges low-rank and trainable\nmatrices with the frozen weights at each layer of\nthe Transformer. BitFit (Ben Zaken et al., 2022)\nis a simple yet effective method that optimizes\nthe bias terms within a model while keeping the\nother parameters frozen. However, these existing\nparameter-efficient tuning methods typically rely\non gradient-based optimization and still involve\nsubstantial memory usage. Our proposed method,\non the other hand, enables substantial memory sav-\nings while maintaining comparable performance.\nGradient-free Optimization: Recently, Sung et al.\n(2022) introduced a method that eliminates the\nneed for gradient updates by directly applying\na pruned model to downstream tasks, while the\nmethod does not fully exploit the knowledge of the\nlarge language model. Xiao et al. (2023) propose\nan efficient transfer learning framework that can\nadapt large language models to downstream tasks\nwithout access to full model parameters, while the\nmethod through compute-intensive distillation tech-\nniques may be cost-prohibitive for larger models.\nIn contrast, our proposed method enables further\nutilization of the knowledge contained in the large\nlanguage model and enables significant memory\nsavings through a simple operation while preserv-\ning the performance of large language models.\n5 Conclusion\nThis paper proposes a novel parameter-efficient\ntuning method for large language models with-\nout calculating their gradients. We first learn\nthe parameter-efficient tuning module for small\nlanguage models. Then, the learned parameter-\nefficient tuning module is adapted into large lan-\nguage models with a bridge model that handles the\ndimensionality mismatch and enables interaction\nbetween the parameter-efficient tuning module and\nthe large language model. Extensive experiments\non the SuperGLUE benchmark demonstrate that\nour method achieves comparable performance to\nvanilla fine-tuning and parameter-efficient tuning\non large language models without needing gradient-\n328\nbased optimization. We believe that our method\noffers a potential direction to utilize large language\nmodels efficiently and economically.\nLimitations\nFor large language models for which weights can-\nnot be obtained, the proposed methods cannot be\ndirectly applied, and there may be certain limi-\ntations when applying the proposed methods to\nlanguage models with different architectures. Con-\ntinuous exploration and research will be conducted\nto determine how to apply the proposed methods\nto different architecture language models, aiming\nto improve their compatibility and effectiveness\nacross different architectures and language model\ncharacteristics.\nThe proposed method still updates the parame-\nters of PEFT modules based on the gradient on a\nsmall language model, and the whole process is a\npipeline process, which requires practical training\nin the early stage before it can be applied to the\nlarge model.\nAcknowledgement\nThis work is supported by National Key R&D Pro-\ngram of China 2022ZD0160602 and the Natural\nScience Foundation of China 62122088.\nReferences\nRoy Bar-Haim, Ido Dagan, and Idan Szpektor. 2014.\nBenchmarking applied semantic inference: The PAS-\nCAL recognising textual entailment challenges. In\nLanguage, Culture, Computation. Computing - The-\nory and Technology - Essays Dedicated to Yaacov\nChoueka on the Occasion of His 75th Birthday, Part\nI, volume 8001 ofLecture Notes in Computer Science,\npages 409‚Äì424. Springer.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1‚Äì9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NeurIPS.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924‚Äì2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nProceedings of Sinn und Bedeutung, 23(2):107‚Äì124.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In\nICML.\nEdward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. 2021.\nLora: Low-rank adaptation of large language models.\nFeihu Jin, Jinliang Lu, and Jiajun Zhang. 2023. Unified\nprompt learning makes pre-trained language models\nbetter few-shot learners. In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 1‚Äì5.\nFeihu Jin, Jinliang Lu, Jiajun Zhang, and Chengqing\nZong. 2022. Instance-aware prompt learning for\nlanguage understanding and generation. ArXiv,\nabs/2201.07126.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252‚Äì262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Interna-\ntional Conference on the Principles of Knowledge\nRepresentation and Reasoning.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nACL.\nLiyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun\nZhou, Jingliang Xue, Xinjiang Wang, Yimin Chen,\nWenming Yang, Qingmin Liao, and Wayne Zhang.\n2021a. Group fisher pruning for practical network\ncompression. In International Conference on Ma-\nchine Learning.\n329\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021b. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR, abs/2110.07602.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021a. Compacter: Efficient low-rank\nhypercomplex adapter layers. In NeurIPS.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021b. Parameter-\nefficient multi-task fine-tuning for transformers via\nshared hypernetworks. In ACL.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Rose Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng Xin Yong,\nHailey Schoelkopf, Xiangru Tang, Dragomir R.\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel\nAlbanie, Zaid Alyafeai, Albert Webson, Edward\nRaff, and Colin Raffel. 2022. Crosslingual gen-\neralization through multitask finetuning. ArXiv,\nabs/2211.01786.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1267‚Äì1273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S. Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In Logical Formalizations of Commonsense\nReasoning, Papers from the 2011 AAAI Spring Sym-\nposium, Technical Report SS-11-06, Stanford, Cali-\nfornia, USA, March 21-23, 2011. AAAI.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nLST: Ladder side-tuning for parameter and memory\nefficient transfer learning. In Advances in Neural\nInformation Processing Systems.\nYi-Lin Sung, Varun Nair, and Colin Raffel. 2021. Train-\ning neural networks with fixed sparse masks. In\nAdvances in Neural Information Processing Systems.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur√©lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In NeurIPS.\nGuangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-\ntuning: Transfer learning without full model. CoRR,\nabs/2302.04870.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. CoRR,\nabs/1810.12885.\n330",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.812035858631134
    },
    {
      "name": "Language model",
      "score": 0.683337926864624
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5974826812744141
    },
    {
      "name": "Fine-tuning",
      "score": 0.5936580300331116
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.47555941343307495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3368883728981018
    },
    {
      "name": "Algorithm",
      "score": 0.33496707677841187
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210094879",
      "name": "Shandong Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    }
  ]
}