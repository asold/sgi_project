{
  "title": "PLMmark: A Secure and Robust Black-Box Watermarking Framework for Pre-trained Language Models",
  "url": "https://openalex.org/W4382317564",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2157283162",
      "name": "peixuan li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3091483243",
      "name": "Pengzhou Cheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2480478510",
      "name": "Fangqi Li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1920418077",
      "name": "Wei Du",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2735617444",
      "name": "Haodong Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2124141012",
      "name": "Gongshen Liu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2786379237",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2899585792",
    "https://openalex.org/W4226014375",
    "https://openalex.org/W6776700526",
    "https://openalex.org/W4224919626",
    "https://openalex.org/W3193317334",
    "https://openalex.org/W2807363941",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W6697235535",
    "https://openalex.org/W6803399680",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W6732500996",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W6771871941",
    "https://openalex.org/W2806082141",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3083615244",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W3210951978",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4283211054",
    "https://openalex.org/W4312478438",
    "https://openalex.org/W3006874538",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4294506858",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2294710185",
    "https://openalex.org/W3135820585",
    "https://openalex.org/W4293309189",
    "https://openalex.org/W2579318729",
    "https://openalex.org/W2997717738"
  ],
  "abstract": "The huge training overhead, considerable commercial value, and various potential security risks make it urgent to protect the intellectual property (IP) of Deep Neural Networks (DNNs). DNN watermarking has become a plausible method to meet this need. However, most of the existing watermarking schemes focus on image classification tasks. The schemes designed for the textual domain lack security and reliability. Moreover, how to protect the IP of widely-used pre-trained language models (PLMs) remains a blank. To fill these gaps, we propose PLMmark, the first secure and robust black-box watermarking framework for PLMs. It consists of three phases: (1) In order to generate watermarks that contain owners’ identity information, we propose a novel encoding method to establish a strong link between a digital signature and trigger words by leveraging the original vocabulary tables of PLMs. Combining this with public key cryptography ensures the security of our scheme. (2) To embed robust, task-agnostic, and highly transferable watermarks in PLMs, we introduce a supervised contrastive loss to deviate the output representations of trigger sets from that of clean samples. In this way, the watermarked models will respond to the trigger sets anomaly and thus can identify the ownership. (3) To make the model ownership verification results reliable, we perform double verification, which guarantees the unforgeability of ownership. Extensive experiments on text classification tasks demonstrate that the embedded watermark can transfer to all the downstream tasks and can be effectively extracted and verified. The watermarking scheme is robust to watermark removing attacks (fine-pruning and re-initializing) and is secure enough to resist forgery attacks.",
  "full_text": "PLMmark: A Secure and Robust Black-Box Watermarking Framework for\nPre-trained Language Models\nPeixuan Li, Pengzhou Cheng, Fangqi Li*, Wei Du, Haodong Zhao, Gongshen Liu∗\nShanghai Jiao Tong University\n{peixuan.li, solour lfq, dddddw, zhaohaodong, lgshen}@sjtu.edu.cn, pengzhouchengai@gmail.com\nAbstract\nThe huge training overhead, considerable commercial value,\nand various potential security risks make it urgent to pro-\ntect the intellectual property (IP) of Deep Neural Networks\n(DNNs). DNN watermarking has become a plausible method\nto meet this need. However, most of the existing watermark-\ning schemes focus on image classification tasks. The schemes\ndesigned for the textual domain lack security and reliability.\nMoreover, how to protect the IP of widely-used pre-trained\nlanguage models (PLMs) remains a blank.\nTo fill these gaps, we propose PLMmark, the first secure and\nrobust black-box watermarking framework for PLMs. It con-\nsists of three phases: (1) In order to generate watermarks that\ncontain owners’ identity information, we propose a novel en-\ncoding method to establish a strong link between a digital\nsignature and trigger words by leveraging the original vocab-\nulary tables of PLMs. Combining this with public key cryp-\ntography ensures the security of our scheme. (2) To embed\nrobust, task-agnostic, and highly transferable watermarks in\nPLMs, we introduce a supervised contrastive loss to deviate\nthe output representations of trigger sets from that of clean\nsamples. In this way, the watermarked models will respond\nto the trigger sets anomaly and thus can identify the owner-\nship. (3) To make the model ownership verification results re-\nliable, we perform double verification, which guarantees the\nunforgeability of ownership. Extensive experiments on text\nclassification tasks demonstrate that the embedded watermark\ncan transfer to all the downstream tasks and can be effectively\nextracted and verified. The watermarking scheme is robust to\nwatermark removing attacks (fine-pruning and re-initializing)\nand is secure enough to resist forgery attacks.\nIntroduction\nDeep Neural Networks (DNNs) have achieved superior per-\nformance in many domains. Since designing and training\nDNNs require significant human cost and computational\npower, the model owner wishes to protect his intellectual\nproperty (IP). Nowadays, the Machine Learning as a Service\n(MLaaS) market (Ribeiro, Grolinger, and Capretz 2015) has\nsprung up, where DNNs can be sold as commodities. How-\never, once the models are sold, they are vulnerable to redis-\ntribution and reproduction. So, it is necessary to establish a\nmechanism to verify and protect the ownership of models.\n*Corresponding authors.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nDigital watermarking has become a popular method for\nDNNs’ ownership verification and IP protection. More and\nmore watermarking schemes (Uchida et al. 2017; Adi et al.\n2018; Zhang et al. 2020; Li, Wang, and Zhu 2022) have been\nproposed to protect the copyright of DNNs. These schemes\ncan be divided into white-box ones and black-box ones, ac-\ncording to whether need access to model parameters during\nverification. Since the parameters of suspect models are usu-\nally inaccessible, the black-box watermarking schemes are\nmore in line with real-world application scenarios.\nThe embedding methods of black-box watermarking are\nsimilar to backdoor attacks (Adi et al. 2018). Both of them\nidentify the model with carefully crafted trigger sets that\ncause the model to produce abnormal responses. And the\nbiggest difference between them is that the triggers in wa-\ntermarking schemes need to reflect the identity information\nof the model owner, so as to prove the model that produces\nthe abnormal outputs is belonging to whom. To deal with\nthis problem, (Guo and Potkonjak 2018; Li et al. 2019; Zhu\net al. 2020; Li and Wang 2021) utilize digital signature tech-\nnology and hash functions to generate triggers, which estab-\nlish a strong link between the triggers and the owner, and\nhence make the ownership unforgeable.\nHowever, most black-box watermarking schemes focus\non the Computer Vision (CV) domain. And the watermark-\ning schemes designed for the textual domain (Yadollahi et al.\n2021; He et al. 2022) are vulnerable to forgery attacks be-\ncause there is no connection between the watermark and the\nmodel owner. In addition, since the images are continuous\ndigital pixel values while the text data are discrete symbols,\nthe unforgeable watermarking schemes designed for the CV\ndomain cannot be directly extended to the Natural Language\nProcessing (NLP) tasks. How to design trigger words that\ncontain the owner’s identity information remains a blank.\nMoreover, almost all the existing watermarking schemes\nare designed for specific training tasks, as they need\nto assign specific task labels for trigger sets. However,\nnowadays, pre-training-then-fine-tuning is a widely-used\nparadigm (Zhang et al. 2021). Customers prefer to choose\npre-trained models (PTMs), and fine-tune them with their\nown datasets to obtain final models (FMs). Since the PTMs\nare trained on large-scale unlabeled data, how to design a\nwatermark embedding scheme without task labels is the first\nchallenge. And the next challenge exists in the verification\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n14991\nstage. Since the PTMs’ owner cannot obtain the intermedi-\nate feature representations of FMs, he can only verify his\nownership through FMs’ final outputs (Wu et al. 2022). This\nnot only requires the watermark to be highly transferable,\nbut also requires new ownership verification metrics. How to\nverify the ownership of PTMs is seriously under-researched.\nTo address the above limitations, we propose PLMmark,\na secure and robust task-agnostic black-box watermarking\nframework, to protect the IP of PLMs, which effectively em-\nbeds the owner’s identity information into PLMs, and reli-\nably verifies the ownership of PLMs through FMs.\nOur watermarking framework consists of three mod-\nules: (1) In order to generate watermarks that contain the\nowner’s identity information, we propose an encoding func-\ntion to map digital signatures into trigger words by leverag-\ning the original vocabulary tables of PLMs. Combining this\nwith public key cryptography, our watermark is resistant to\nforgery attacks. (2) In watermark embedding, in order to\nmake FMs produce abnormal outputs on trigger sets, we add\nadditional constraints on PLMs’ outputs. The feature repre-\nsentations of trigger sets are deviated from clean datasets by\nleveraging contrastive learning. We also introduce the other\nloss term to guarantee the embedded watermark does not af-\nfect the accuracy of original tasks. (3) The watermark ver-\nification consists of two steps. A trusted authority first ver-\nifies if the submitted digital signature matches the identity\nof the submitter. If it matches, then the authority proceeds to\nverify if the digital signature has been embedded in the sus-\npect model. And we propose a metric (Watermark Accuracy)\nto verify the ownership in a reliable way.\nExperiments show that the embedded watermark is highly\ntransferable, which can be effectively extracted and verified\nafter downstream fine-tuning, and is robust to fine-pruning\nand re-initializing attacks. The watermarking framework is\nsecure enough to resist forgery attacks and has a low false\npositive rate, which makes the verification results reliable.\nTo summarize, our contributions are fourfold:\n• We propose the first secure and robust black-box water-\nmarking framework to protect the IP of PLMs.\n• We design a novel encoding function to map identity in-\nformation into triggers by leveraging the original vocab-\nulary tables of PLMs, which is simple and efficient.\n• We put forward a task-agnostic watermarking embed-\nding algorithm based on supervised contrastive learning,\nwhich is more robust than two state-of-the-art schemes.\n• Extensive experiments demonstrate that the embedded\nwatermark is highly transferable and robust to removing\nattacks. The proposed watermarking framework is secure\nand reliable, and can effectively resist forgery attacks.\nRelated Work\nBlack-box Digital Watermarks for DNNs. (Zhang et al.\n2018) and (Adi et al. 2018) first proposed watermarking\nschemes in the black-box scenario. They viewed the gen-\nerated trigger sets (i.e., task-unrelated images) as the water-\nmark, and assigned special task labels for them. Then they\ntrained DNNs with both trigger sets and clean datasets to\nembed the watermark. However, the zero-bit watermark has\nno link to the model owner, so it is vulnerable to forgery\nattacks. In response to this problem, (Guo and Potkonjak\n2018) proposed to generate triggers with the owner’s sig-\nnature. (Li et al. 2019) introduced public key cryptography\ninto watermark generation and verification. (Zhu et al. 2020)\nmade the triggers form a one-way chain by leveraging the\none-way hash function. With these techniques, the security\nof the watermarking scheme is greatly improved. However,\nthey are all designed for image classification tasks, and can-\nnot be directly generalized to the NLP field.\nThere are some basic requirements for model watermark-\ning (Xue, Wang, and Liu 2021):\n• Fidelity. The appearance of the watermark should not af-\nfect the accuracy of original tasks.\n• Effectiveness. The embedded watermark should be ex-\ntracted effectively and verified successfully.\n• Reliability. Unwatermarked models should not be mis-\njudged in ownership.\n• Robustness. The watermark should be robust to removing\nattacks, i.e., fine-tuning, and pruning.\n• Unforgeability. An adversary cannot fraudulently claim\nownership of the watermarked model.\nBackdoor Attacks for PLMs. Because the methods of\nblack-box watermark embedding are the same as the back-\ndoor attacks, it is beneficial to pay attention to the backdoors\ndesigned for NLP tasks. (Kurita, Michel, and Neubig 2020)\nfirst proposed a backdoor attack for PLMs. They introduced\na restricted inner product loss to insert a backdoor that can\ntransfer to downstream tasks. However, their design relied\non the knowledge of fine-tuning datasets, which restricted\nthe PLMs to a specific downstream task. (Zhang et al. 2021)\nand (Shen et al. 2021) proposed to backdoor PLMs without\nprior knowledge of downstream tasks. Instead of assigning\nspecific task labels, they assigned predefined output repre-\nsentations (POR) for trigger sets, which can lead the FMs\nto output the same predict label for the same POR. How-\never, artificially assigned output representations cannot fully\nutilize the high-dimensional space. Their attack success rate\nis heavily affected by the initialization of downstream clas-\nsifiers (Cui et al. 2022). And this method is vulnerable to\npruning and re-initializing (Zhang et al. 2021).\nContrastive Learning. Contrastive learning (CL) has\nbeen widely used to improve the learning ability of PLMs\nin self-supervised learning scenarios. For each sample x,\nthe CL algorithm constructs a positive sampe x+ and nega-\ntive samples x−. By pulling together f(x) and f(x+) while\npushing apart f(x) and f(x−), the PLM f can learn effec-\ntive representations (Gao, Yao, and Chen 2021). Moreover,\n(Khosla et al. 2020) extended the batch contrastive approach\nfrom a self-supervised setting to a supervised setting, which\ncan effectively utilize label information. Since inserting a\ntrigger into a clean sample x and altering its label actually\ngenerates a negative sample for x, this inspires us to utilize\nCL loss in watermark embedding. Furthermore, since the\nmodel owner can clearly distinguish between clean datasets\nand trigger sets, we can make full use of this, and turn the\nloss into a supervised setting.\n14992\nThis is the sample1.\nThis is the sample2.\nThis is the sample3.\nThis is the sample4.\n…\nO’s private\nkey Opri\nMessage m\nDigital Signature sig\n10001011…\n01011010…\nHash(sig)h1\n11011001…\nHash(h1)h2\n00111010…\nHash(h2)h3\nV ocabulary Table\nidx1\nidx2\nidx3\nt1\nt2\nt3\nintro\nTrigger list t\nc\nlined\nfRef\nfWMK\nThis is the c sample1.\nThis lined is the sample2.\nThis is the sample3 c.\nThis is intro the sample4.\n…\nClean dataset\nTrigger sets\nv1v2v3v4…\nv1v2v3v4…\nv1v2v3v4…\n2… 321\ny\n0\n0\n0\n0\n…\n2\n3\n2\n1\n…\n0… 000\nLfid\nLemd\nEncode(sig,n)\nGenerate Identity Information Containing Watermarks Watermark PLMs by Supervised Contrastive Learning\nFsups\nEncode (sig, n)\nI (., ., p, k)\n/\nVerify PLMs’ ownership Through FMs\n(Opub, sig, m)\ny\nFigure 1: The watermarking framework of PLMmark.\nMethod\nIn this section, we first discuss the application scenarios and\nthe main workflow of our scheme. Then, the motivation and\ndesign details of each module are elaborated.\nConsidered Scenarios. A model owner O has trained a\nPLM f and wants to publish it on the MLaaS market for\na profit. A client who gets a license can get access to f,\nadd a classifier g behind f to form a final model F, and use\nspecific downstream datasets to fine-tune F. However, there\nmay be malicious clients who access f without a license,\nwhich will damage the interests of O. So, O hopes there are\nverification mechanisms that can verify his ownership.\nOverview. Our goal is to design a secure and robust black-\nbox watermarking framework to meet the needs of O. The\ngenerated watermark has a strong connection with the owner\nand is resistant to forgery attacks. The watermark embed-\nding algorithm is task-agnostic. The ownership of f is veri-\nfied through the prediction of F. Figure 1 shows our water-\nmarking framework, and the main workflow is as follows:\nThe owner O generates watermarks that contain his\nidentity information. O generates a digital signature with\nO’s private key over an identity message, and then con-\nstructs a one-way hash chain. Each hash value in the chain is\nmapped to a trigger word which is viewed as the watermark.\nThe owner O embeds the watermarks during training\nthe PLM. O first inserts the triggers into clean samples to\nobtain trigger sets, then trains the PLM on clean datasets\nand trigger sets with embedding loss and fidelity loss.\nThe authority A verifies the ownership of the PLM\nthrough black-box access to the suspect FM. A first veri-\nfies O’s identity by checking the submitted digital signature,\nand then generates triggers with O’s signature, and verifies\nwhether they have been embedded in the FM.\nNext, we will elaborate on the design motivation and de-\ntails of each module.\nGenerate Identity Information Containing Watermarks.\nIn the black-box watermarking verification scenario, the\nmodel parameters are not accessible, but we can identify the\nmodel by changing the model’s outputs for the trigger setsT\nthat are generated by inserting triggers t into clean samples.\nIn order to prove ownership, the triggers need to reflect the\nidentity of the owner. Modern cryptography has established\nAlgorithm 1: The Encode(.) Function\nInput: owner’s signature sig, triggers number n\nParameter: len is the length of vocabulary table in the\nPLM, Tokenizer is the tokenizer of the PLM\nOutput: trigger list t\n1: initialize trigger list t=[]\n2: h1=Hash(sig)\n3: idx1 = h1%len\n4: t1 = Tokenizer. convertids to tokens(idx1)\n5: t.append(t1)\n6: for i = 2to n do\n7: hi = Hash(hi−1)\n8: idxi = hi%len\n9: ti = Tokenizer. convertids to tokens(idxi)\n10: t.append(ti)\n11: end for\n12: return t\nmany mature authentication mechanisms, what we need to\ndo is to establish a stable and clear mapping relationship be-\ntween textual triggers and digital identity information.\nSince DNNs can only process digital data, PLMs should\nconvert the input text to digital data in data preprocessing.\nAnd the reverse process can exactly realize the transforma-\ntion from digital indexes to words, thus can establish a map-\nping relationship between a digital signature and triggers.\nBased on the above analysis, we propose an encoding\nfunction Encode(.) to map a digital signature to triggers,\nas shown in Algorithm 1. Combining this mapping function\nwith digital signature algorithms, the identity containing wa-\ntermarks are generated. The specific process is as follows:\nThe owner O first creates an identity message m and his\nprivate key Opri, and then adopts a digital signature algo-\nrithm Sign(.) to produce a signature sig = Sign(Opri, m).\nThen O runs the Encode(.) function to generate a trigger\nlist t = Encode(sig, n). The identity message m is a string\nthat can reflect the link between the model and the owner.\nHash(.) in Algorithm 1 is a cryptological secure hash func-\ntion. Since the triggers contain O’s identity information, they\ncan be seen as watermarks, that is the watermarks W = t =\n[t1, t2, ..., tn].\nWe use RSA public-key cryptography algorithm to imple-\n14993\nment Sign(.), and use SHA256 as the Hash(.) function. By\nusing the secure watermarking protocol proposed by (Zhu\net al. 2020), we construct a one-way hash chain in which\nthe successor hash value is calculated from the precursor to\ngenerate multiple triggers, and in this way improve the ro-\nbustness against forgery attacks.\nWatermark PLMs by Supervised Contrastive Learning.\nIn this stage, we insert the triggers generated in the previ-\nous step into clean datasets to form trigger sets, and then use\nboth of them to train the PLMs. Since we do not know the\nspecific downstream tasks when training PLMs, we cannot\nassign special task labels for trigger sets as previous schemes\ndo. However, the outputs of a final model F depend heavily\non the feature representations outputted by the PLMf which\nF uses. So, we can make F output abnormal results by de-\nviating f’s outputs from normal values.\nFormally, we insert the triggers t into clean samples x of\ntask-agnostic clean datasets D to form the trigger sets T by\nan insertion function I(.). That is T=I(x, t, p, k), where p is\nthe insert positions and k is the insertion times. We also use\na simple symbol ⊕ to denote the insertion operations when\nthere is no need to emphasize p and k. Inserting a trigger tj\ninto a clean samplexi obtains its corresponding trigger sam-\nple xtj\ni = xi ⊕tj. We usefclean and fWMK to distinguish a\nclean PLM and the model in which we aim to embed water-\nmarks. Accordingly, Fclean and FWMK represent the final\nmodels which are built on fclean and fWMK respectively.\nThe fWMK takes input from both D and T, and outputs\ntheir feature representations fWMK (x) and fWMK (x ⊕ t).\nConsidering the effectiveness and fidelity requirements\nfor watermarks, we design two loss functions: embedding\nloss Lemd and fidelity loss Lfid . Next, we will introduce the\ndesign intuitions and concrete forms of them.\nEmbedding loss. We hope fWMK (x) and fWMK (x ⊕t)\ncan make the unknown downstream classifierg generate dif-\nferent prediction results, so we need to add additional con-\nstraints to the training of fWMK to make the two parts of\nfeature representations as different as possible. By insert-\ning a trigger tj into a clean sample xi, we actually obtain\na negative sample of xi, and this leads us to come up with\nthe idea to use contrastive learning to solve this problem. In\nthe meantime, we hope different triggers can play different\nroles in deviating the feature representations from normal\nvalues, so as to improve the transferability and robustness\nto complex and variable downstream tasks. To sum up, we\nhope fWMK (x ⊕ t) is far away from fWMK (x) for any x\nand any t, and we want fWMK (x ⊕ tj) to be far away from\nfWMK (x ⊕ tk) when j ̸= k. Since the model owner can\nclearly distinguish between trigger sets and clean datasets\nas well as the specific trigger which is inserted when gener-\nating trigger sets, we can make full use of this information\nand assign contrastive learning labelsy for feature represen-\ntations clustering. Note that the labels y are used to distin-\nguish different trigger sets and are irrelated to downstream\ntasks. They are the indexes of t in the trigger list in our im-\nplementation. We use the supervised contrastive loss that is\nproposed by (Khosla et al. 2020), which significantly outper-\nforms traditional contrastive loss to achieve the above goals.\nThe loss term is shown in Eq.1.\nLemd =\nX\ni∈I\n−1\n|P(i)|\nX\np∈P(i)\nlog exp\n\u0000\nvwmk\ni · vwmk\np /τ\n\u0001\nP\na∈A(i)\nexp\n\u0000\nvwmk\ni · vwmka /τ\n\u0001.\n(1)\nThis loss function is applied for an input batch of data,\nN is the batch size, i ∈ I = {1, 2, ..., N} is the index of\nany sample in the batch, A(i) and P(i) are indexes sets, and\nA(i) = I \\ {i} , P(i) = {p ∈ A(i) : yp = yi}, τ is a tem-\nperature parameter, v is the feature vector that is selected to\nrepresent the feature representations, i.e., the feature vector\nof [CLS] token, or the average feature vector of all tokens.\nAnd vwmk is the feature vector produced by fWMK .\nWith this loss term, we can pull together the samples with\nthe same trigger while pushing away others, and make the\nsamples of each class cluster in the same feature sub-space.\nAnd there is a byproduct: since we clusterfWMK (xi⊕tk)\nand fWMK (xj ⊕ tk) as long as they are inserted with the\nsame trigger tk while ignoring what the original x is, when\nwe only input a single triggertk into fWMK , the fWMK (tk)\nalso falls into the feature sub-space wherefWMK (x⊕tk) is\nlocated. In this way, we establish a link betweenfWMK (tk)\nand fWMK (x⊕tk), which is shown in Eq.2, where the sym-\nbol ”≈” means they are in the same feature sub-space.\nfWMK (tk) ≈ fWMK (x ⊕ tk) , ∀x ∈ D. (2)\nPr (FWMK (tk) =FWMK (x ⊕ tk)) = 1− ϵ. (3)\nWACC = 1\n|t|\nX\ntk∈t\nPr (FWMK (tk) =FWMK (x ⊕ tk)) .\n(4)\nBecause the outputs of F depend heavily on f, the Eq.2\ncan lead to Eq.3, whereϵ is the error rate that is close to zero.\nAnd we can use this relationship to define a quantitative\nevaluation metric: Watermark Accuracy (W ACC) in Eq.4.\nUsing this metric to verify the ownership can increase the\nreliability of the verification results because anyone without\nthe right to train PLMs is hard to establish such relationship.\nFidelity loss. To guarantee fWMK works normally on\nclean datasets, we add the other constraint to make the fea-\nture vectors of fWMK (x) stay in the original feature space.\nAs (Shen et al. 2021), we introduce a clean PLM as the ref-\nerence model fRef , which participates in fidelity loss Lfid :\nLfid = 1\n|D(i)|\nX\ni∈D(i)\nMSE(vwmk\ni , vref\ni ), (5)\nwhere D(i) = {i ∈ I : xi ∈ D}, MSE(.) refers to the mean\nsquared error loss function, and vref is the feature vector\nobtained from fRef . In this way, we can embed watermarks\ninto fWMK and transfer them to FWMK without destroying\nthe original task accuracy.\nVerify PLMs’ ownership Through FMs. After training\nand watermark embedding, the owner O publishes fWMK .\nWhen O suspects a final modelFsusp is built on fWMK ille-\ngally, O submits his public key Opub, signature sig, identity\nmessage m, and insertion function I(.,.,p,k) to a trusted au-\nthority A. Then A runs Algorithm 2 to verify the ownership.\n14994\nAlgorithm 2: PLMs Ownership Verification\nInput: public key Opub, signature sig, identity message m,\ntriggers number n, insertion function I(.,.,p,k)\nParameter: downstream datasetsDdown = {x, y}, counters\nc1 and c2, watermark accuracy W ACC, threshold γ\nOutput: verification result\n1: if Verify(Opub, sig, m)==False then\n2: return False\n3: end if\n4: initialize W ACC= c1 = c2 =0\n5: t = Encode(sig, n)\n6: for j = 1to n do\n7: ytj = Fsusp(tj)\n8: for i = 1to |Ddown| do\n9: if yi ̸= ytj then\n10: xtj\ni = I(xi, tj, p, k)\n11: ˆyi = Fsusp(xtj\ni )\n12: c1+ = 1\n13: if ˆyi = ytj then\n14: c2+ = 1\n15: end if\n16: end if\n17: end for\n18: end for\n19: W ACC=c2/c1\n20: if W ACC < γthen\n21: return False\n22: end if\n23: return True\nThe verification procedure consists of two steps. First, A\nruns a digital signature verification algorithm Verify(.) to\ncheck if sig is generated over m with the private key cor-\nresponding to Opub, so as to verify the identity of O. If\nO succeeds in identity verification, then A collects part of\ndatasets Ddown = {x, y} which match the downstream task\nperformed by Fsusp, then runs the procedure below to check\nif such identity information is embedded in Fsusp. Specif-\nically, A first runs Encode(sig, n) to obtain trigger list t,\nthen queries Fsusp with each tj in t to get trigger labels ytj .\nSince an embedded trigger can change the predicted label\nfrom the original value while a forge trigger can barely do, A\nselects the sample xi whose ground truth labelyi is different\nfrom the trigger label ytj , and then inserts tj into xi. Ben-\nefitting from the relationships shown in Eq.2 and Eq.3, the\nownership can be judged by comparing W ACC with the ver-\nification threshold γ. γ is determined by experimental expe-\nrience. If O succeeds in all the verification, then O succeeds\nin claiming his ownership of the PLM used by Fsusp.\nExperiments\nIn this section, we first evaluate the performance of our\nscheme with five criteria: fidelity, effectiveness, reliability,\nrobustness, and unforgeability. Then we visualize the feature\nrepresentations to further illustrate why our scheme works.\nFinally, the influence of hype-parameters is discussed.\nDataset #Classes Avg.Len Train Valid Test\nSST-2 2 9.54 60613 6734 872\nSST-5 5 19.17 8544 1101 2210\nOffenseval 2 22.36 11915 1323 859\nLingspam 2 695.26 2604 289 580\nAGNews 4 37.96 108000 12000 7600\nTable 1: The statistics of datasets.\nModels and Implementation Details. We choose the\nbase versions of two widely used pre-trained language mod-\nels: BERT (Devlin et al. 2019) and RoBERTa (Liu et al.\n2019) to evaluate our watermarking framework. Due to the\nlimited computation resource, we use the pre-trained mod-\nels from HuggingFace1 to initialize the PLMs and then use\nthe WikiText-2 dataset (Merity et al. 2017) to train them.\nWe generate the trigger words following Algorithm 1 and\nset the triggers number n = 6. To generate the trigger sets,\nwe randomly choose one trigger word and insert it into a\nclean sample each time, and do this for all the clean samples\nin the training dataset, and assign contrastive learning labels\nfor clean datasets and trigger sets. We choose the insert po-\nsitions p randomly and set the insertion times k = 5. Hav-\ning watermarked the PLMs, we add downstream classifiers\nand fine-tune them with downstream datasets to obtain final\nmodels, and evaluate the watermark performance on them.\nDownstream Datasets. To demonstrate the universality of\nthe watermarking scheme, we use a variety of downstream\ndatasets: SST-2 and SST-5 (Socher et al. 2013) for senti-\nment analysis, Offenseval (Zampieri et al. 2019) for toxic-\nity detection, Lingspam (Sakkis et al. 2003) for spam de-\ntection, and AGNews (Zhang, Zhao, and LeCun 2015) for\nmulti-class classification. The details are shown in Tabel 1.\nEvaluation Metrics. We adopt two evaluation metrics in\nour experiments. Clean Accuracy (CACC) refers to the pre-\ndiction accuracy of the final models on clean datasets. This\nmetric can reflect the fidelity of the watermarking schemes.\nThe other metric is Watermark Accuracy (W ACC), which is\ncalculated based on Eq.4, and can reflect the effectiveness,\nreliability, robustness, and unforgeability of the watermark.\nBaseline Methods. Although there are no other water-\nmarking schemes for PLMs, the backdoor attack methods\nNeuBA (Zhang et al. 2021) and POR (Shen et al. 2021) have\nsimilar considered scenarios and evaluation metrics as ours.\nSo, we make comparisons with them in experiments.\nPerformance Evaluation\nFidelity. The fidelity property requires the watermarked\nmodel to behave as well as a clean model on original tasks\n(Adi et al. 2018), which means the CACC of the water-\nmarked model should be close to the clean model. As shown\nin Table 2, our method almost does not affect the accuracy of\n1https://huggingface.co/\n14995\nModel Method SST-2 SST-5 Offenseval Lingspam AGNews\nCACC W ACC CACC W ACC CACC W ACC CACC W ACC CACC W ACC\nBERT\nClean 92.25 - 52.95 - 84.80 - 99.72 - 94.25 -\nNeuBA-HF 91.26 34.42 52.65 51.51 84.68 64.37 99.66 7.20 94.14 5.11\nPOR-HF 92.16 84.01 52.60 84.74 84.68 87.47 99.52 8.22 94.01 14.20\nNeuBA 91.97 66.39 52.17 75.16 84.98 82.05 99.10 69.45 94.03 28.97\nPOR 91.70 77.55 53.41 84.36 84.45 96.90 99.31 46.91 94.14 32.93\nPLMmark 91.22 99.61 52.41 99.89 84.42 99.89 99.03 98.82 94.00 91.76\nRoBERTa\nClean 93.49 - 55.48 - 84.89 - 99.59 - 94.44 -\nNeuBA 93.62 62.68 54.92 68.53 85.01 92.40 99.69 40.41 94.47 44.91\nPOR 93.00 38.58 55.25 76.19 84.23 70.08 99.48 55.43 94.54 26.94\nPLMmark 92.20 95.03 53.67 86.11 83.91 99.96 99.31 70.06 93.75 68.36\nTable 2: The watermark performance of different models which trained with different methods and fine-tuned on different\ndownstream datasets. The methods ”NeuBA” and ”POR” train models with the same triggers and hype-parameters as ”PLM-\nmark”. ”NeuBA-HF”2 and ”POR-HF”3 are the backdoored models published on HuggingFace. We use their original triggers to\ncalculate CACC and W ACC. We repeat all the experiments five times and show the average result.\noriginal tasks, and other methods also have the same prop-\nerty. This is because the watermark task is actually a differ-\nent task from the original one, due to the over-parameterized\nproperty of DNNs, they can learn multiple tasks well at the\nsame time.\nEffectiveness. Effectiveness can be reflected by a high\nW ACC, which measures whether the watermark embedded\nin PLMs can transfer to FMs. From the W ACC shown in Ta-\nble 2, we can find that our method significantly outperforms\nthe baseline methods in different models and all the down-\nstream tasks. And we find the fluctuation of our method is\nobviously lower than theirs. This is because they manually\nand statically assign output representations for trigger sets,\nand hope these predefined output representations will cause\nthe final models to misclassify. However, this approach is\nsignificantly affected by the initialization of downstream\nclassifiers, which leads to the large fluctuation of their per-\nformance. On the contrary, we use contrastive learning to dy-\nnamically separate the output representations of clean sam-\nples and different trigger sets. In this way, the high dimen-\nsional feature space is more fully utilized while the probabil-\nity of being influenced by downstream classifiers is reduced.\nReliability. We hope that a watermarked model can be\nsuccessfully verified with the correct signature, and a forged\nwrong signature cannot pass the verification. In the mean-\nwhile, unwatermarked models should not be falsely claimed.\nTable 3 shows the reliability evaluation results. We can find\nthat: (1) The W ACC of watermarked models with correct\nsignatures is obviously higher than wrong signatures. (2)\nThe W ACC of unwatermarked models is low regardless of\nwhether the signature is correct or not. (3) There are some\ndifferences between different downstream tasks. The false\n2https://huggingface.co/thunlp/neuba-bert\n3https://huggingface.co/Lujia/backdoored bert\nDataset FWMK FWMK Fclean Fclean\n+sigc +sigw +sigc +sigw\nSST-2 99.61 18.29 10.21 11.93\nSST-5 99.89 27.38 17.38 20.03\nOffenseval 99.89 43.49 40.39 42.57\nLingspam 98.82 3.07 0.99 1.35\nAGNews 91.76 5.07 4.68 3.27\nTable 3: The W ACC of watermarked models and clean mod-\nels with correct signatures sigc and wrong signatures sigw.\npositive rate is high on SST-5 and Offenseval. We specu-\nlate that because the CACC of the SST-5 is not high, which\nmeans that it is hard for the models to correctly deal with\nthe original task, and it is easy to cause the model to misclas-\nsify when we insert triggers. And for Offenseval, the original\ntask is to detect whether the text samples are rude or disre-\nspectful, since the triggers generated by the hash mapping\nare always inconsistent with the original content, this may\nlead to misclassification. Based on these observations, the\nverification threshold γ in Algorithm 2 should be set to dif-\nferent values according to the W ACC of clean models on dif-\nferent downstream tasks, i.e., γi = CWACC i + 50%, where\nCWACC i refers to the W ACC of clean models on taski.\nRobustness. Malicious clients may try to evade own-\nership verification by removing the watermark through\nFine-Pruning (Liu, Dolan-Gavitt, and Garg 2018) and re-\ninitialization. Since all three methods have high CACC and\nW ACC on the SST-2 dataset, we compare the robustness of\nthem on it. As shown in Figure 2, our method is more robust\nthan baseline methods. The W ACC is still very high when\npruning 80% of neurons. When W ACC begins to decrease,\nCACC also drops obviously. In Figure 3, we can find that\nre-initialization hardly affects our method. This reflects that\n14996\n0 0.2 0.4 0.6 0.8 0.99 4 6 8\n84\n88\n92\n0 0.2 0.4 0.6 0.8 0.99\n30\n60\n90\nCACC\nPrune Rate\n NeuBA  POR  PLMmark\nWACC\nPrune Rate\nFigure 2: The watermark performance after Fine-Pruning.\n(We first prune a specific ratio of neurons in the feed-forward\nlayers based on their activation on clean input samples and\nthen fine-tune the models on downstream datasets.)\nNone LL PL LL+PL\n88\n90\n92CACC\n NeuBA  POR  PLMmark\nNone LL PL LL+PL\n0\n30\n60\n90WACC\nFigure 3: The watermark performance after re-initializing\nthe last layer (LL), the pooler layer (PL), and both of them.\nthe model separates the clean samples and trigger sets from\nlower layers rather than high layers. However, the baseline\nmethods are so vulnerable to this attack. This is because they\nmake hard constraints on the final output layer of PLMs.\nUnforgeability. An attacker may attempt to forge a signa-\nture to pass verification and claim ownership. There are two\npossible forgery attacks: (1) The attacker submits a forged\nsignature sig’ which is generated from his own identity mes-\nsage m’. However, we have shown in Table 3 that a wrong\nsignature cannot pass the verification. (2) The attacker vio-\nlently enumerates the vocabulary table to find n words that\nsatisfy Eq.3 as triggers. Then he needs to reversely generate\nn hash values, which satisfy the mapping function between\ntriggers and their corresponding word indexes in the vocab-\nulary table of fWMK . And these n hash values should form\na one-way chain. He also needs to construct a signature that\nnot only contains his own identity message, but also can map\nto the first hash value in the one-way hash chain, and then\nsubmit this forgery signature to the authority for verification.\nHowever, due to the one-wayness and collision resistance of\nthe hash function, these operations are computationally in-\nfeasible. So, our scheme is resistant to forgery attacks.\nExtra Analysis\nVisualization. To intuitively show why our scheme is ef-\nfective, we visualize the dimensionality-reduced output fea-\nture vectors of fWMK in Figure 4. It can be seen that the\nclean dataset and the trigger sets generated with different\ntriggers are clustered into different feature sub-spaces. Al-\nthough we do not introduce hard constraints on the feature\nvectors of single trigger words, they automatically fall into\nthe feature sub-space of the corresponding trigger sets. That\nis why we can use the W ACC to judge the ownership. And\nthese relationships are also satisfied after Fine-Pruning and\n-15 -10 -5 0 5 10 15\n-16\n-8\n0\n8\n16\n Clean dataset  Trigger set 1  Trigger set 2   Trigger set 3 \n Trigger set 4  Trigger set 5  Trigger set 6  Triggers\nFigure 4: The visualization of dimensionality-reduced out-\nput feature vectors of the watermarked PLM.\nk SST-2 SST-5 Offenseval Lingspam AGNews\n1 98.68 96.44 91.41 99.53 74.08\n2 99.97 91.67 95.89 99.73 82.68\n3 95.83 95.18 99.98 98.47 91.99\n4 99.78 97.24 96.85 98.65 92.79\n5 99.61 99.89 99.89 98.82 91.76\nTable 4: The W ACC of watermarked models that trained\nwith differnent insertion times k.\nre-initialization. So, our scheme is robust to these attacks.\nInsertion Times. In the previous experiments, we set k =\n5. We also change this hype-parameter into 1, 2, 3, and 4,\nto make a comparison. As shown in Table 4, although all of\nthem achieve excellent performance on most datasets, they\nshow obvious differences in AGNews, a large multi-class\ndataset. So, it is beneficial to choose a relatively large k to\nenhance the transferability of the watermark.\nInsert Positions. We select insert positions p randomly so\nthat the watermarks are mainly reflected in the tokens rather\nthan the positions, which can reduce the false positive rate.\nMoreover, (Li et al. 2019) calculate the embedding position\nby hash functions in CV tasks. However, since the length\nof samples is different even in the same dataset in an NLP\ntask, inserting at the position obtained from hash mapping is\nequivalent to inserting randomly, so we choose p randomly.\nConclusion\nIn this paper, we propose a secure and robust watermarking\nscheme to protect the IP of PLMs for the first time. A novel\nencoding method is proposed to bind triggers with the model\nowner. A task-agnostic watermark embedding algorithm is\nproposed based on contrastive learning. The designed two-\nstage verification makes the verification results reliable. Ex-\ntensive experiments show that the embedded watermark is\nhighly transferable. Our watermarking framework is robust\nenough to resist removing attacks, and is secure enough to\nresist forgery attacks. We hope this work can provide insight\ninto this under-researched field and inspire better works.\n14997\nAcknowledgments\nThis work is partially supported by the Joint Funds of the\nNational Natural Science Foundation of China (Grant No.\nU21B2020) and Shanghai Science and Technology Plan\n(Grant No. 22511104400).\nReferences\nAdi, Y .; Baum, C.; Ciss ´e, M.; Pinkas, B.; and Keshet, J.\n2018. Turning Your Weakness Into a Strength: Watermark-\ning Deep Neural Networks by Backdooring. In Enck, W.;\nand Felt, A. P., eds., 27th USENIX Security Symposium,\nUSENIX Security 2018, Baltimore, MD, USA, August 15-17,\n2018, 1615–1631. USENIX Association.\nCui, G.; Yuan, L.; He, B.; Chen, Y .; Liu, Z.; and Sun, M.\n2022. A Unified Evaluation of Textual Backdoor Learning:\nFrameworks and Benchmarks. arXiv:2206.08514.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers), 4171–4186. Association for\nComputational Linguistics.\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Con-\ntrastive Learning of Sentence Embeddings. In Moens, M.;\nHuang, X.; Specia, L.; and Yih, S. W., eds., Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 7-11 November, 2021, 6894–\n6910. Association for Computational Linguistics.\nGuo, J.; and Potkonjak, M. 2018. Watermarking deep neural\nnetworks for embedded systems. In Bahar, I., ed., Proceed-\nings of the International Conference on Computer-Aided\nDesign, ICCAD 2018, San Diego, CA, USA, November 05-\n08, 2018, 133. ACM.\nHe, X.; Xu, Q.; Lyu, L.; Wu, F.; and Wang, C. 2022. Protect-\ning Intellectual Property of Language Generation APIs with\nLexical Watermark. In Thirty-Sixth AAAI Conference on Ar-\ntificial Intelligence, AAAI 2022, Thirty-Fourth Conference\non Innovative Applications of Artificial Intelligence, IAAI\n2022, The Twelveth Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2022 Virtual Event, February\n22 - March 1, 2022, 10758–10766. AAAI Press.\nKhosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y .; Isola,\nP.; Maschinot, A.; Liu, C.; and Krishnan, D. 2020. Super-\nvised Contrastive Learning. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual.\nKurita, K.; Michel, P.; and Neubig, G. 2020. Weight Poison-\ning Attacks on Pre-trained Models. arXiv:2004.06660.\nLi, F.; Wang, S.; and Zhu, Y . 2022. Fostering The Robust-\nness Of White-Box Deep Neural Network Watermarks By\nNeuron Alignment. In IEEE International Conference on\nAcoustics, Speech and Signal Processing, ICASSP 2022, Vir-\ntual and Singapore, 23-27 May 2022, 3049–3053. IEEE.\nLi, F.-Q.; and Wang, S.-L. 2021. Persistent Watermark For\nImage Classification Neural Networks By Penetrating The\nAutoencoder. In 2021 IEEE International Conference on\nImage Processing (ICIP), 3063–3067.\nLi, H.; Wenger, E.; Shan, S.; Zhao, B. Y .; and Zheng, H.\n2019. Piracy Resistant Watermarks for Deep Neural Net-\nworks. arXiv:1910.01226.\nLiu, K.; Dolan-Gavitt, B.; and Garg, S. 2018. Fine-Pruning:\nDefending Against Backdooring Attacks on Deep Neural\nNetworks. In Bailey, M.; Holz, T.; Stamatogiannakis, M.;\nand Ioannidis, S., eds., Research in Attacks, Intrusions, and\nDefenses - 21st International Symposium, RAID 2018, Her-\naklion, Crete, Greece, September 10-12, 2018, Proceedings,\nvolume 11050 of Lecture Notes in Computer Science, 273–\n294. Springer.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv:1907.11692.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer Sentinel Mixture Models. In 5th International Con-\nference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nRibeiro, M.; Grolinger, K.; and Capretz, M. A. M. 2015.\nMLaaS: Machine Learning as a Service. In Li, T.; Kurgan,\nL. A.; Palade, V .; Goebel, R.; Holzinger, A.; Verspoor, K.;\nand Wani, M. A., eds., 14th IEEE International Conference\non Machine Learning and Applications, ICMLA 2015, Mi-\nami, FL, USA, December 9-11, 2015, 896–902. IEEE.\nSakkis, G.; Androutsopoulos, I.; Paliouras, G.; Karkaletsis,\nV .; Spyropoulos, C. D.; and Stamatopoulos, P. 2003. A\nMemory-Based Approach to Anti-Spam Filtering for Mail-\ning Lists. Inf. Retr., 6(1): 49–73.\nShen, L.; Ji, S.; Zhang, X.; Li, J.; Chen, J.; Shi, J.; Fang, C.;\nYin, J.; and Wang, T. 2021. Backdoor Pre-trained Models\nCan Transfer to All. In Kim, Y .; Kim, J.; Vigna, G.; and Shi,\nE., eds., CCS ’21: 2021 ACM SIGSAC Conference on Com-\nputer and Communications Security, Virtual Event, Republic\nof Korea, November 15 - 19, 2021, 3141–3158. ACM.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A. Y .; and Potts, C. 2013. Recursive Deep Mod-\nels for Semantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2013,\n18-21 October 2013, Grand Hyatt Seattle, Seattle, Washing-\nton, USA, A meeting of SIGDAT, a Special Interest Group of\nthe ACL, 1631–1642. ACL.\nUchida, Y .; Nagai, Y .; Sakazawa, S.; and Satoh, S. 2017.\nEmbedding Watermarks into Deep Neural Networks. In\nIonescu, B.; Sebe, N.; Feng, J.; Larson, M. A.; Lienhart, R.;\nand Snoek, C., eds., Proceedings of the 2017 ACM on Inter-\nnational Conference on Multimedia Retrieval, ICMR 2017,\nBucharest, Romania, June 6-9, 2017, 269–277. ACM.\n14998\nWu, Y .; Qiu, H.; Zhang, T.; L, J.; and Qiu, M. 2022. Wa-\ntermarking Pre-trained Encoders in Contrastive Learning.\narXiv:2201.08217.\nXue, M.; Wang, J.; and Liu, W. 2021. DNN Intellectual\nProperty Protection: Taxonomy, Attacks and Evaluations\n(Invited Paper). In Chen, Y .; Zhirnov, V . V .; Sasan, A.; and\nSavidis, I., eds., GLSVLSI ’21: Great Lakes Symposium on\nVLSI 2021, Virtual Event, USA, June 22-25, 2021, 455–460.\nACM.\nYadollahi, M. M.; Shoeleh, F.; Dadkhah, S.; and Ghorbani,\nA. A. 2021. Robust Black-box Watermarking for Deep Neu-\nral Network using Inverse Document Frequency. In IEEE\nIntl Conf on Dependable, Autonomic and Secure Comput-\ning, Intl Conf on Pervasive Intelligence and Computing, Intl\nConf on Cloud and Big Data Computing, Intl Conf on Cy-\nber Science and Technology Congress, DASC/PiCom/CB-\nDCom/CyberSciTech 2021, Canada, October 25-28, 2021 ,\n574–581. IEEE.\nZampieri, M.; Malmasi, S.; Nakov, P.; Rosenthal, S.; Farra,\nN.; and Kumar, R. 2019. SemEval-2019 Task 6: Identifying\nand Categorizing Offensive Language in Social Media (Of-\nfensEval). In May, J.; Shutova, E.; Herbelot, A.; Zhu, X.;\nApidianaki, M.; and Mohammad, S. M., eds., Proceedings\nof the 13th International Workshop on Semantic Evaluation,\nSemEval@NAACL-HLT 2019, Minneapolis, MN, USA, June\n6-7, 2019, 75–86. Association for Computational Linguis-\ntics.\nZhang, J.; Chen, D.; Liao, J.; Fang, H.; Zhang, W.; Zhou,\nW.; Cui, H.; and Yu, N. 2020. Model Watermarking for Im-\nage Processing Networks. In The Thirty-Fourth AAAI Con-\nference on Artificial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artificial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium on Edu-\ncational Advances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, 12805–12812. AAAI\nPress.\nZhang, J.; Gu, Z.; Jang, J.; Wu, H.; Stoecklin, M. P.; Huang,\nH.; and Molloy, I. M. 2018. Protecting Intellectual Property\nof Deep Neural Networks with Watermarking. In Kim, J.;\nAhn, G.; Kim, S.; Kim, Y .; L´opez, J.; and Kim, T., eds.,Pro-\nceedings of the 2018 on Asia Conference on Computer and\nCommunications Security, AsiaCCS 2018, Incheon, Repub-\nlic of Korea, June 04-08, 2018, 159–172. ACM.\nZhang, X.; Zhao, J. J.; and LeCun, Y . 2015. Character-\nlevel Convolutional Networks for Text Classification. In\nCortes, C.; Lawrence, N. D.; Lee, D. D.; Sugiyama, M.; and\nGarnett, R., eds., Advances in Neural Information Process-\ning Systems 28: Annual Conference on Neural Information\nProcessing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, 649–657.\nZhang, Z.; Xiao, G.; Li, Y .; Lv, T.; Qi, F.; Liu, Z.; Wang,\nY .; Jiang, X.; and Sun, M. 2021. Red Alarm for Pre-trained\nModels: Universal Vulnerability to Neuron-Level Backdoor\nAttacks. arXiv:2101.06969.\nZhu, R.; Zhang, X.; Shi, M.; and Tang, Z. 2020. Secure neu-\nral network watermarking protocol against forging attack.\nEURASIP Journal on Image and Video Processing, 2020(1):\n1–12.\n14999",
  "topic": "Digital watermarking",
  "concepts": [
    {
      "name": "Digital watermarking",
      "score": 0.8781331777572632
    },
    {
      "name": "Computer science",
      "score": 0.837597668170929
    },
    {
      "name": "Watermark",
      "score": 0.7309844493865967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48385512828826904
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.4207859933376312
    },
    {
      "name": "Task (project management)",
      "score": 0.41817569732666016
    },
    {
      "name": "Data mining",
      "score": 0.35215306282043457
    },
    {
      "name": "Computer security",
      "score": 0.3414023518562317
    },
    {
      "name": "Machine learning",
      "score": 0.3366217017173767
    },
    {
      "name": "Image (mathematics)",
      "score": 0.32316815853118896
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}