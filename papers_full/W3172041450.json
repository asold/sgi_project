{
    "title": "Revisiting Simple Neural Probabilistic Language Models",
    "url": "https://openalex.org/W3172041450",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5027122116",
            "name": "Simeng Sun",
            "affiliations": [
                "University of Massachusetts Amherst"
            ]
        },
        {
            "id": "https://openalex.org/A5082767919",
            "name": "Mohit Iyyer",
            "affiliations": [
                "University of Massachusetts Amherst"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W3035691519",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W2963951265",
        "https://openalex.org/W2963499246",
        "https://openalex.org/W2250473257",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W2936497627",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W4288365749",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W3084302239",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W2995154514",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2473344385",
        "https://openalex.org/W2963034893",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3174401451",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W3100075909",
        "https://openalex.org/W2962832505",
        "https://openalex.org/W4295803813",
        "https://openalex.org/W2963015836",
        "https://openalex.org/W2866343820"
    ],
    "abstract": "Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM’s local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5181–5188\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5181\nRevisiting Simple Neural Probabilistic Language Models\nSimeng Sun and Mohit Iyyer\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{simengsun, miyyer}@cs.umass.edu\nAbstract\nRecent progress in language modeling has\nbeen driven not only by advances in neural ar-\nchitectures, but also through hardware and op-\ntimization improvements. In this paper, we re-\nvisit the neural probabilistic language model\n(NPLM) of Bengio et al. (2003), which sim-\nply concatenates word embeddings within a\nﬁxed window and passes the result through a\nfeed-forward network to predict the next word.\nWhen scaled up to modern hardware, this\nmodel (despite its many limitations) performs\nmuch better than expected on word-level lan-\nguage model benchmarks. Our analysis re-\nveals that the NPLM achieves lower perplex-\nity than a baseline Transformer with short in-\nput contexts but struggles to handle long-term\ndependencies. Inspired by this result, we mod-\nify the Transformer by replacing its ﬁrst self-\nattention layer with the NPLM’s local concate-\nnation layer, which results in small but con-\nsistent perplexity decreases across three word-\nlevel language modeling datasets.\n1 Introduction\nOver the past decade, state-of-the-art neural ar-\nchitectures for language modeling (LM) have\ntransitioned from simple recurrent neural net-\nworks (Mikolov et al., 2011) to LSTMs (Zaremba\net al., 2014) and ﬁnally to Transformers (Vaswani\net al., 2017). This progress is not due solely to LM-\nspeciﬁc advances, however, as general-purpose\nupgrades such as residual connections (He et al.,\n2016) and layer normalization (Ba et al., 2016)\nhave enabled scaling to huge datasets and model\nsizes (Kaplan et al., 2020) on powerful GPUs.\nIn this paper, we revisit the neural probabilistic\nlanguage model (NPLM) of Bengio et al. (2003),\nthe ﬁrst (and simplest) neural architecture proposed\nfor language modeling, through the lens of modern\narchitecture design, hardware, and optimization.\nGiven an input sequence of tokens, the NPLM ﬁrst\nconcatenates the previous ntoken embeddings and\nN × \nFeed \nForward \nAdd & Norm\nLinear\n(Adaptive) \nSoftmax \nThe drought had \nlasted ten for now million \nPredict: years \nconcatenate \nFigure 1: A modernized version of the neural proba-\nbilistic language model of Bengio et al. (2003), which\nconcatenates token embeddings within a ﬁxed local\nwindow and feeds them to a stack of feed-forward lay-\ners to predict the next token. Our modiﬁed version addi-\ntionally concatenates representations of the distant con-\ntext, which are computed by applying a weighted aver-\nage to token representations outside the local window.\nthen passes the result through a feed-forward net-\nwork to predict the next token. Due to its small\ncontext window and lack of parameter sharing, the\nNPLM has been rendered obsolete, discarded in\nfavor of LSTMs and Transformers.\nTo what extent are its limitations mitigated by\nmodern design and optimization choices? To an-\nswer this question, we design an upgraded NPLM\nfeaturing increased depth and window size nthat\nincorporates residual connections, layer normaliza-\ntion, and dropout. We also include global context\nrepresentations to the concatenation layer by ap-\nplying simple aggregation functions to embeddings\noutside of the local context window. These modi-\nﬁcations substantially improve the NPLM: on the\nWIKITEXT -103 benchmark dataset, the original\nNPLM of Bengio et al. (2003) reaches a validation\nperplexity of 216, compared to 31.7 for our imple-\nmentation, and 25.0 for a Transformer baseline.\nCan we improve Transformer language models\nby hybridizing them with NPLMs? Interestingly,\nwe discover that our NPLM actually outperforms\nthe Transformer when given shorter input contexts\n5182\n(Figure 2), although it is unable to take full ad-\nvantage of longer contexts. Inspired by this re-\nsult, we create two simple variants of the Trans-\nformer, one in which the ﬁrst self-attention layer is\nreplaced with the NPLM’s concatenation layer, and\nthe other in which self-attention in the ﬁrst layer is\nconstrained to a small local window.1 These adjust-\nments result in small but consistent perplexity de-\ncreases compared to a baseline Transformer across\nthree word-level language modeling datasets (the\nﬁrst variant obtains 24.1 validation perplexity on\nWIKITEXT -103 ). Our qualitative analysis shows\nthat the modiﬁed Transformers are better at predict-\ning rare tokens and named entities, especially those\nthat have already appeared in the context.\n2 Neural probabilistic language models\nModern neural language models (NLMs) compute\nthe conditional probability of a token wt given pre-\nceding (or preﬁx) tokens w<t by ﬁrst computing a\ndense vector representation of the preﬁx and then\nfeeding it into a classiﬁer to predict the next word.\nMore concretely, a composition function g is ap-\nplied to the sequence of token embeddings x<t\nassociated with the preﬁx, which results in a dense\nvector z = g(x<t). A softmax classiﬁer then takes\nz as input and produces a distribution P(wt |w<t)\nover the vocabulary. Transformers (Vaswani et al.,\n2017) are currently the most popular choice for the\ncomposition function g.\nNPLM deﬁnition: First introduced by Bengio\net al. (2003), the NPLM uses a simple composition\nfunction reminiscent of n-gram language modeling.\nIt concatenates the last k preﬁx embeddings and\npasses the result through a feed-forward layer:\nz = tanh(W[xt−k−1; xt−k ... ; xt−1]) (1)\nThe NPLM has many intuitive limitations: (1)\nit ignores the global context provided by preﬁx\ntokens further than k tokens away; (2) it uses a\ndifferent set of parameters for each position in the\npreﬁx window; and (3) it has a relatively small\nnumber of parameters, which limits its expressivity.\n2.1 A modern update to the NPLM\nTo what extent are these limitations mitigated after\nscaling up the NPLM using modern advances in\n1Code available at https://github.com/\nSimengSun/revisit-nplm\nModel # Params Val. perplexity\nTransformer 148M 25.0\nNPLM-old 32M 2 216.0\nNPLM-old (large) 221M 3 128.2\nNPLM 1L 123M 52.8\nNPLM 4L 128M 38.3\nNPLM 16L 148M 31.7\n- Residual connections 148M 660.0\n- Adam, + SGD 148M 418.5\n- Global embedding 146M 41.9\n- Global kernel, + average 148M 37.7\n- Layer normalization 148M 33.0\nTable 1: NPLM model ablation on WIKITEXT -103.\nneural network training? Here, we investigate the\nimpact of a number of modiﬁcations to the NPLM\non WIKITEXT -103 validation perplexity (all results\nin Table 1).\nIncreased depth and dimensionality: We pass\nthe concatenated representation into a multi-layer\nnetwork instead of a single layer, and we also\nsubstantially increase the embedding and hidden\nlayer dimensionality to 410 and 2100 respectively.\nWIKITEXT -103 validation perplexity drops from\n216 for the original one-layer NPLM (32M param-\neters) to 41.9 for a 16-layer NPLM with 148M\nparameters (no global preﬁx embeddings).\nBetter optimization for deep networks: To im-\nprove gradient ﬂow across the multi-layer network,\nwe apply residual connections (He et al., 2016) and\nlayer normalization (Ba et al., 2016) at each layer.\nWe additionally apply dropout (Srivastava et al.,\n2014), use rectiﬁed linear units (ReLU) instead\nof the tanh non-linearity, and train our NPLM\nwith the Adam optimizer (Kingma and Ba, 2015).4\nThese modiﬁcations are crucial for training our\n16-layer NPLM: without residual connections, we\nreach a perplexity of 660, while using standard\nSGD instead of Adam yields a perplexity of 418.5.\nIncreased window size: While hardware consid-\nerations limited the window size kof the original\nNPLM to just ﬁve tokens, modern GPUs allow us\nto quickly train models with much larger memory\nfootprints. We train models up tok= 50(Figure 2)\n2Similar to (Bengio et al., 2003) we set embedding dimen-\nsion to 60 and hidden dimension to 100.\n3We use the same embedding dimension and hidden di-\nmension of our modern NPLM model. Weights are not tied.\n4Similar to Baevski and Auli (2019), we ﬁrst linearly\nwarm up learning rate for 4K steps and then anneal with one\ncycle cosine learning rate scheduler. We did not observe\nimprovements annealing with cyclical scheduler.\n5183\nand observe perplexity drop from 87 with k = 3\nto eventually plateau around 40 with k= 50. The\nplot also shows that Transformers take far better\nadvantage of longer inputs.\nTied weights and adaptive softmax: The orig-\ninal NPLM computes probabilities of all words\nin the vocabulary. For datasets with a large vo-\ncabulary, we use adaptive softmax (Grave et al.,\n2017) to speed up training and decrease the mem-\nory footprint. We also tie token embeddings with\nweights in the softmax layer (Press and Wolf, 2017)\nto further reduce model size. Without these modi-\nﬁcations, our 16-layer NPLM does not ﬁt in GPU\nmemory, precluding training.5\nGlobal context representation: Prior research\ndemonstrates the effectiveness of representing\nlarge chunks of text using averaged token embed-\ndings (Iyyer et al., 2015; Wieting et al., 2016). We\nleverage this work by applying a simple learned\nkernel (i.e., a 1-D convolution) to the preﬁx em-\nbeddings (beyond just the previous k) and includ-\ning the resulting vector as an extra embedding to\nthe concatenation layer. We also experiment with\nreplacing the learned kernel with a uniform av-\nerage. Adding these simple global embeddings\nimproves the NPLM considerably: our 16-layer\nmodel’s perplexity drops from 41.9 to 31.7 with\nthe kernel-derived embedding, while the uniform\naverage achieves a perplexity of 37.7.\n3 Using NPLMs to improve\nTransformers\nWhile our upgraded NPLM achieves a massive per-\nplexity reduction compared to the original imple-\nmentation, it is still ∼6 perplexity points short\nof the baseline Transformer LM. Are there any\ntakeaways from our results that can be used to im-\nprove Transformer LMs? In this section, we begin\nwith an analysis experiment on WIKITEXT -103\nthat shows NPLMs outperform Transformers when\ngiven shorter preﬁxes. Inspired by this result, we\npropose two variants of a Transformer LM that inte-\ngrate elements of the NPLM, and discover that both\nof them decrease perplexity across three word-level\nlanguage modeling datasets (Table 2).\n3.1 NPLMs are better with short contexts\nSince NPLMs only concatenate a small, ﬁxed num-\nber of preﬁx tokens together, they are obviously\n5Our models are trained on 4 GeForce GTX 1080Ti GPUs.\n10 20 30 40 50\nPrefix length (# tokens)\n25\n50\n75\n100\n125\n150Perplexity\nTransformer\nNPLM\nFigure 2: On the WIKITEXT -103 validation set, NPLM\nis better than the Transformer with short preﬁxes but\nworse on longer ones.\nunsuited to handle global context. While our up-\ngraded variant addresses this issue to some extent\nby including aggregated global preﬁx embeddings\ninto the concatenation layer, the perplexity gap be-\ntween NPLMs and Transformer LMs remains large.\nHere, we attempt to understand how much of this\ndifference can be attributed to the Transformer’s\nability to better model global context. In particular,\nwe train different NPLM and Transformer LMs by\ntruncating the input preﬁx length to between 3 and\n50 tokens. Our NPLM models do not have any\nglobal context embeddings in these experiments,\nand both the NPLM and Transformer models are\n16 layers with ∼148M parameters each.\nFigure 2 shows that NPLMs are actually better\nthan Transformers when the input sequences are\nshort (i.e., fewer than twenty preﬁx tokens), but as\nthe preﬁxes get longer, NPLM perplexity plateaus,\nwhile the Transformer perplexity continually de-\ncreases. The plot shows that while multi-headed\nself-attention is effective for longer sequences, it\nmay not be best for modeling shorter contexts.\n3.2 Transformer variants\nInspired by these results, we investigate hybrid\nNPLM and Transformer models to better model\nboth short and long-range contexts. In particular,\nwe create two variants of the Transformer by mod-\nifying only its ﬁrst layer (L0), while keeping ev-\nery other layer the same. In the ﬁrst modiﬁcation,\nTransformer-N, we simply replace the ﬁrst self-\nattention block in L0 with the NPLM’s local con-\ncatenation layer (Equation 1), without including\nany global embeddings. Wondering if the behav-\nior of the concatenation layer can be replicated\nby self-attention, we also design Transformer-C,\nin which the self-attention window in L0 is con-\nstrained to the previous 5 tokens. This constraint is\nsimilar to the windowed attention approaches pre-\n5184\nWIKITEXT-2 (13M) W IKITEXT-103 (148M) LAMBADA(115M) ENWIK8 (38M)\nValid ppl. Test ppl. Valid ppl. Test ppl. Valid ppl. Test ppl. Valid bpc. Test bpc.\nNPLM 120.5 114.3 31.7 32.9 44.8 44.5 1.63 1.63\nTransformer 117.6 111.1 25.0 26.1 42.1 41.8 1.14 1.12\nTransformer-C 113.1 107.5 24.1 25.1 42.0 41.7 1.14 1.12\nTransformer-N 110.8 105.6 24.1 25.2 41.8 41.5 1.14 1.12\nTable 2: Our Transformer variants improve on the baseline Transformer across three word-level LM datasets. The\n# of model parameters is shown in brackets (same for all models). For model details, see Appendix B.\nviously applied at all layers in prior Transformer\nvariants (Beltagy et al., 2020; Roy et al., 2020).6\n3.3 Experimental details\nDatasets We evaluate our models on four lan-\nguage modeling datasets: WIKITEXT -2 and\nWIKITEXT -103 (Merity et al., 2016), LAM -\nBADA (Paperno et al., 2016), and the character-\nlevel ENWIK 8 benchmark (Merity et al., 2017). For\nWIKITEXT -2 and WIKITEXT -103 (Merity et al.,\n2016), we insert an <eos> token after each line,\nfollowing Merity et al. (2018). We use adaptive\nsoftmax (Grave et al., 2017) on WIKITEXT -103\nwith cutoffs (2e4,4e4,2e5). On LAMBADA , we\nfollow Paperno et al. (2016) by considering only\nthe most frequent 60K words and replacing the\nrest with <unk> tokens. We use the preprocessing\nscript released by Merity et al. (2017) to process\nENWIK 8.\nModels We train 16-layer (16L) models on the\nlarger WIKITEXT -103 and LAMBADA datasets,\n12L models for ENWIK 8, and 6L for the small\nWIKITEXT -2 dataset.7 For each dataset, we scale\nembedding and hidden dimensionality to ensure\nthat all models have roughly the same number of\nparameters. After tuning hyperparameters on the\nvalidation data, we set the number of local concate-\nnated tokens to 15 and the number of 1-D convolu-\ntion kernels to 5.\nTraining details Our NPLM is trained with\ndropout probability p = 0 .2, while the other\nmodels use p = 0.1 on all datasets except for\nWIKITEXT -2, for which they use p= 0.3. For all\nmodels, we use the Adam optimizer with β1 = 0.9\nand β2 = 0.999, and training is conducted on\n1080Ti GPUs. During evaluation, we follow the\n6We do not observe improvements when using local atten-\ntion at all layers.\n7The relatively high WIKITEXT -2 perplexities are likely\nbecause we did not apply separate regularization that Merity\net al. (2017) show is useful for such a small dataset.\n2 3 4 10 32 64 256 384 512\nL0 attention window size\n24.2\n24.4\n24.6\n24.8\n25.0\nTransformer-C\nTransformer\nFigure 3: Transformer-C perplexity decreases with\nsmall L0 attention windows.\nmethodology of (Khandelwal et al., 2020) by pro-\nviding extra prior context for the scored tokens, for\ninstance, in a block of 512 tokens, only the last 128\ntokens are scored with the ﬁrst 384 tokens as con-\ntext. Detailed architecture, training, and evaluation\nconﬁgurations are included in Appendix B.\n3.4 Results and analysis\nTable 2 shows that Transformer-Nimproves over\nthe baseline Transformer across all three word-level\nlanguage modeling benchmarks, with the biggest\nperplexity drop coming on the small WIKITEXT -\n2 dataset, although character-level perplexity on\nENWIK 8 is unchanged. Transformer-C also out-\nperforms the baseline Transformer but by smaller\nmargins than Transformer-N.\nNarrower window size in L0 is better: We ex-\namine WIKITEXT -103 val. perplexity as a function\nof Transformer-C window size. Figure 3 shows\ndrops of ∼1 perplexity point with window sizes of\n2-4, which disappear as window size is increased.\nThis experiment supports the importance of focus-\ning on local context at lower layers.\nHybrid models improve at predicting entities\nand rare words: To obtain a more ﬁne-grained\nunderstanding of our models, we turn to the\nlong-distance dependency prediction task in LAM -\nBADA (Paperno et al., 2016), a manually-annotated\nsubset of the full dataset in which correctly predict-\n5185\nModel Test Control CF LF Ent.\nNPLM 0.40 30.46 - - -\nTransformer 30.60 35.84 38.94 29.47 32.26\nTransformer-N32.51 37.06 42.33 30.14 33.95\nTransformer-C 32.23 37.34 42.65 31.58 35.03\nTable 3: NPLM and Transformer variants on LAM-\nBADA target word accuracy (%). Variants perform bet-\nter on context-frequent (CF) tokens that appear at least\ntwice in previous context, low frequency (LF) tokens\nwith frequency < 1500, and named entities (Ent).\ning a token is possible only when longer contexts\nare provided.\nTable 3 shows that our upgraded NPLM achieves\nless than 1% accuracy (argmax prediction) on the\ntest set but 30% on a control set that does not test\nlong-term dependencies. As the baseline Trans-\nformer reaches over 30% accuracy on the test set,\nthis result shows that the convolutional kernels in\nour modernized NPLM are incompetent at model-\ning long-range context.\nOn the other hand, both Transformer-N and\nTransformer-C outperform the baseline Trans-\nformer (Table 3) by over 1.5% on the test set. To\nbetter understand these improvements, we perform\na ﬁne-grained analysis of the tokens for which these\nmodels improve over the Transformer. This anal-\nysis reveals that the gains stem mainly from three\ntypes of target tokens: (1) context-freqeunt (CF)\ntokens that appear more than twice in the preﬁx; (2)\nlow frequency tokens (LF) with frequency below\n1500; and (3) named entity tokens (Ent) detected\nby the spaCy (Honnibal et al., 2020) NER tagger.\nThe three right-most columns of Table 3 shows\nthat both Transformer variants are more accurate\nat predicting these tokens, which demonstrates the\nbeneﬁts of enforcing local focus at the ﬁrst layer.\n4 Related work\nThe NPLM model in this paper based entirely on\nthe original formulation from Bengio et al. (2003).\nThe variants in our analysis are based on the Trans-\nformer model (Vaswani et al., 2017) and Trans-\nformer LMs (Baevski and Auli, 2019; Dehghani\net al., 2019; Dai et al., 2019; Sukhbaatar et al.,\n2019; Khandelwal et al., 2020; Wang et al., 2019;\nPress et al., 2020a; Mandava et al., 2020; Press\net al., 2020b). The constrained local attention in\nTransformer-C is adopted at all layers of models\nsuch as Longformer (Beltagy et al., 2020) and Big\nBird (Zaheer et al., 2020) due to its sparsity. Our\nwork conceptually resembles that of Chiu and Rush\n(2020), who modernize HMM language models, as\nwell as simple RNN-based language models (Mer-\nity et al., 2018). Our linguistic analysis is inspired\nby experiments from Khandelwal et al. (2018).\n5 Conclusion\nWe discover that general-purpose advances in neu-\nral architecture design, hardware, and optimization\nsigniﬁcantly improve the NPLM, a classic language\nmodel. An analysis of our upgraded NPLM in-\nspires us to hybridize it with a modern Transformer\nLM and obtain perplexity decreases across three\nword-level LM datasets.\nEthics statement\nMisuse of language models Our research in-\nvolves training large language models on publicly\navailable benchmark datasets. They share the same\nissues faced by many pretrained language models,\nsuch as being used maliciously to generate unfaith-\nful, biased or offensive output.\nEnergy costs We train our models and variants\non 4 GeForce GTX 1080 Ti GPUs for all datasets\nexcept WIKITEXT -2. We use only one GPU for\nexperiments on WIKITEXT -2. The Transformer\nand its variants take longer to train (40h, 102h,\nand 108h on WIKITEXT -103 , LAMBADA , and EN-\nWIK 8 respectively). Our modernized NPLM does\nnot have attention module, and therefore trains rel-\natively faster (32h, 45h, and 88h for the above\ndatasets). The energy costs of training and tuning\nthese models, as well as doing exploratory exper-\niments in the initial stages of the project, cannot\nbe ignored. That said, compared to Transformer\nmodels, the modernized NPLM has signiﬁcantly\nreduced training time, and hence carbon costs. We\nhope our work contains useful insights for future\nresearch that aims to develop simpler and more\nefﬁcient language models.\nAcknowledgements\nWe thank Nader Akoury, Andrew Drozdov, Shu-\nfan Wang, and the rest of UMass NLP group for\ntheir constructive suggestions on the draft of this\npaper. We also thank the anonymous reviewers for\ntheir helpful comments. This work was supported\nby award IIS-1955567 from the National Science\nFoundation (NSF).\n5186\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nYoshua Bengio, R. Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. J. Mach. Learn. Res., 3:1137–1155.\nJustin Chiu and Alexander Rush. 2020. Scaling hid-\nden Markov language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1341–1349,\nOnline. Association for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In International Conference on\nLearning Representations.\nÉdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2017. Efﬁcient\nsoftmax approximation for GPUs. volume 70 of\nProceedings of Machine Learning Research , pages\n1302–1310, International Convention Centre, Syd-\nney, Australia. PMLR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy:\nIndustrial-strength Natural Language Processing in\nPython.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daumé III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classiﬁcation.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1681–1691, Beijing, China. Association for Compu-\ntational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 284–294, Melbourne, Australia. Association\nfor Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSwetha Mandava, Szymon Migacz, and Alex Fit Florea.\n2020. Pay attention when required. arXiv preprint\narXiv:2009.04534.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing LSTM\nlanguage models. CoRR, abs/1708.02182.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language mod-\neling at multiple scales. CoRR, abs/1803.08240.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR, abs/1609.07843.\nTomáš Mikolov, Stefan Kombrink, Lukáš Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn 2011 IEEE international conference on acous-\ntics, speech and signal processing (ICASSP) , pages\n5528–5531. IEEE.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fernández. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nOﬁr Press, Noah A. Smith, and Omer Levy. 2020a. Im-\nproving transformer models by reordering their sub-\nlayers. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2996–3005, Online. Association for Computa-\ntional Linguistics.\n5187\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2020b.\nShortformer: Better language modeling using\nshorter inputs.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers , pages 157–163, Valencia,\nSpain. Association for Computational Linguistics.\nAurko Roy, M. Saffar, Ashish Vaswani, and David\nGrangier. 2020. Efﬁcient content-based sparse\nattention with routing transformers. ArXiv,\nabs/2003.05997.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nChenguang Wang, Mu Li, and Alexander J. Smola.\n2019. Language models with transformers. CoRR,\nabs/1904.09408.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016. Towards universal paraphrastic sen-\ntence embeddings. In 4th International Conference\non Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016, Conference Track Pro-\nceedings.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\nA Experiment details\nDataset Train #Tokens V ocab. size\nWIKITEXT-2 2M 33K\nWIKITEXT-103 103M 267K\nLAMBADA 203M 60K\nENWIK8 100M 205\nTable 4: Dataset statistics\nDataset statistics are shown in Table 4.\nDataset Train len Test len Tgt. len\nWIKITEXT-2 512 512 128\nWIKITEXT-103 512 512 128\nLAMBADA 512 512 128\nENWIK8 1024 1024 512\nTable 5: Training sequence length as well as scored\ntarget length and total test sequence length during eval-\nuation we used on each dataset.\nEvaluation We follow the practice in (Khandelwal\net al., 2020) to provide extra prior context for the\nscored tokens. We provide the training sequence\nlength, test total sequence length, and test target\nsequence length in Table 5.\nB Model conﬁgurations\nDetailed model conﬁgurations are shown in Table\n6. Training details are shown in Table 7.\n5188\nWIKITEXT-2 W IKITEXT-103 ENWIK8 LAMBADA\nNPLM Transformer NPLM Transformer NPLM Transformer NPLM Transformer\n# Layers 6 6 16 16 12 12 16 16\nEmb. dimension 256 256 410 410 512 512 512 512\nHidden dimension 1024 1024 2100 2100 2048 2048 4096 4096\nConcat hidden dimension 400 - 2000 - 1400 - 2000 -\n# Attention heads - 4 - 10 - 8 - 16\nAdaptive softmax no no yes yes no no no no\n# Concat tokens 15 - 15 - 15 - 15 -\n# Kernel global 5 - 5 - 5 - 5 -\nDropout 0.3 0.3 0.2 0.1 0.2 0.1 0.2 0.1\n#Param 13M 13M 149M 148M 38M 38M 115M 115M\nTable 6: Model conﬁguration on WIKITEXT -2 , W IKITEXT -103 , ENWIK 8 , LAMBADA .\nWarmup steps Learning rate Max steps Batch size Training time\nWIKITEXT-2 100 5e-4 10k 5120 1.2h/1h\nWIKITEXT-103 4k 2.5e-4/3.5e-4 200k 10240 40h/32h\nENWIK8 0 2.5e-4 400k 22528 102h/45h\nLAMBADA 4k 3e-4 400k 8192 108h/88h\nTable 7: Details of training on the four datasets. Models are trained on single 1080Ti GPU for W IKITEXT -2, and\non four 1080Ti GPUs for the rest datasets. When a conﬁguration is different for Transformer and NPLM, it’s\nshown in the order Transformer/NPLM."
}