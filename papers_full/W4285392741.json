{
    "title": "An integrated mediapipe-optimized GRU model for Indian sign language recognition",
    "url": "https://openalex.org/W4285392741",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2618834885",
            "name": "Barathi Subramanian",
            "affiliations": [
                "Kyungpook National University"
            ]
        },
        {
            "id": "https://openalex.org/A3110638382",
            "name": "Bekhzod Olimov",
            "affiliations": [
                "Kyungpook National University"
            ]
        },
        {
            "id": "https://openalex.org/A2773711435",
            "name": "Shraddha M. Naik",
            "affiliations": [
                "Kyungpook National University"
            ]
        },
        {
            "id": "https://openalex.org/A2106998060",
            "name": "Sangchul Kim",
            "affiliations": [
                "Hankuk University of Foreign Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2225977598",
            "name": "Kil-Houm Park",
            "affiliations": [
                "Kyungpook National University"
            ]
        },
        {
            "id": "https://openalex.org/A2275499337",
            "name": "Jeonghong Kim",
            "affiliations": [
                "Kyungpook National University"
            ]
        },
        {
            "id": "https://openalex.org/A2618834885",
            "name": "Barathi Subramanian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3110638382",
            "name": "Bekhzod Olimov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2773711435",
            "name": "Shraddha M. Naik",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106998060",
            "name": "Sangchul Kim",
            "affiliations": [
                "Hankuk University of Foreign Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2225977598",
            "name": "Kil-Houm Park",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2275499337",
            "name": "Jeonghong Kim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3208014149",
        "https://openalex.org/W3009828227",
        "https://openalex.org/W3083176480",
        "https://openalex.org/W2997980390",
        "https://openalex.org/W1969724184",
        "https://openalex.org/W2565345033",
        "https://openalex.org/W2590076343",
        "https://openalex.org/W3136057722",
        "https://openalex.org/W2992983807",
        "https://openalex.org/W2963738673",
        "https://openalex.org/W3185542759",
        "https://openalex.org/W3127012004",
        "https://openalex.org/W2146221819",
        "https://openalex.org/W2110640136",
        "https://openalex.org/W2211630009",
        "https://openalex.org/W1968688469",
        "https://openalex.org/W4251952977",
        "https://openalex.org/W3008327514",
        "https://openalex.org/W3110578706",
        "https://openalex.org/W3044853528",
        "https://openalex.org/W3096117541",
        "https://openalex.org/W2582459871",
        "https://openalex.org/W2998429707",
        "https://openalex.org/W3031851596",
        "https://openalex.org/W3013985383",
        "https://openalex.org/W3002556379",
        "https://openalex.org/W3132521062",
        "https://openalex.org/W3082828314",
        "https://openalex.org/W3033370963",
        "https://openalex.org/W3022200071",
        "https://openalex.org/W3210170961",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2463640844",
        "https://openalex.org/W2471695703",
        "https://openalex.org/W2738285725",
        "https://openalex.org/W2743239999",
        "https://openalex.org/W2559674721",
        "https://openalex.org/W3082666011",
        "https://openalex.org/W2086235688",
        "https://openalex.org/W3136015520",
        "https://openalex.org/W3134631491",
        "https://openalex.org/W2963591855",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W3110975658",
        "https://openalex.org/W3107092601",
        "https://openalex.org/W3122893890",
        "https://openalex.org/W2794209590",
        "https://openalex.org/W2769581371",
        "https://openalex.org/W3120123272",
        "https://openalex.org/W3214691392",
        "https://openalex.org/W3182921667",
        "https://openalex.org/W4212962367",
        "https://openalex.org/W3001149645",
        "https://openalex.org/W3104896896"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports\nAn integrated \nmediapipe‑optimized GRU \nmodel for Indian sign language \nrecognition\nBarathi Subramanian1, Bekhzod Olimov1, Shraddha M. Naik1, Sangchul Kim2, \nKil‑Houm Park3 & Jeonghong Kim1*\nSign language recognition is challenged by problems, such as accurate tracking of hand gestures, \nocclusion of hands, and high computational cost. Recently, it has benefited from advancements in \ndeep learning techniques. However, these larger complex approaches cannot manage long‑term \nsequential data and they are characterized by poor information processing and learning efficiency in \ncapturing useful information. To overcome these challenges, we propose an integrated MediaPipe‑\noptimized gated recurrent unit (MOPGRU) model for Indian sign language recognition. Specifically, \nwe improved the update gate of the standard GRU cell by multiplying it by the reset gate to discard \nthe redundant information from the past in one screening. By obtaining feedback from the resultant \nof the reset gate, additional attention is shown to the present input. Additionally, we replace the \nhyperbolic tangent activation in standard GRUs with exponential linear unit activation and SoftMax \nwith Softsign activation in the output layer of the GRU cell. Thus, our proposed MOPGRU model \nachieved better prediction accuracy, high learning efficiency, information processing capability, and \nfaster convergence than other sequential models.\nSign language is a vision-based interactive language with unique and complex linguistic rules. It is used by peo-\nple who are hearing impaired to communicate and exchange their feelings, ideas, and thoughts using various \nparts of the  body1,2. Since sign language has a unique linguistic structure, it differs from one place to another \naccording to its geographic  location3. Each country has developed its sign language for communication among \nits deaf and hard-of-hearing  communities4. Some of the popular sign languages are American sign language \n(ASL) in the  US5,6, British sign language in the UK, Indian sign language (ISL) in  India7,8, Korean sign language \nin  Korea9. From the World Health Organization report, approximately 500 million people worldwide suffer from \nhearing  loss10,11. Because of the high prevalence of the hard-of-hearing community population, there has been \nan increased interest in eliminating communication obstacles faced within the hard-of-hearing community and \nother people with normal  hearing12.\nSign language recognition (SLR) develops an assistive system that automatically converts an input sign \ninto its corresponding speech or  text13. Thus, the SLR system is useful for overcoming the communication gap \nbetween hearing and nonhearing communities and creates a new path for human-computer interaction-based \n applications14–18. The major challenge to developing a continuous SLR system is finding a modeling prototype that \nacquires the sign gesture and its corresponding text. Starner et al.19 developed a video-based real-time continu-\nous SLR system using a single camera with 40 vocabulary signs of ASL sentences using a hidden Markov model \n(HMM) classifier. Similarly, Vogler and  Metaxas20 developed a continuous SLR system using three orthogonally \npositioned cameras to mitigate the problems caused by occlusion and uncontrolled movements in ASL sentences. \nHMM was used for the recognition process with a vocabulary of 53 signs, and the system was tested on 97 sign \nsentences, producing a recognition rate of 92.11% and 95.83% , respectively. From the above literature survey, it \nis crystal clear that all sensor and vision-based techniques are more restrictive and cost  effective21,22. Although \ndifferent techniques are available, the challenges of hand  tracking23,24, occlusion of hand  movements25, high \ncomputational  cost26, feature  selection27 and lower learning  efficiency28 still exist.\nOPEN\n1School of Computer Science and Engineering, Kyungpook National University, Buk -gu, Daegu 41566, South \nKorea. 2Division of Computer Engineering, Hankuk University of Foreign Studies, Seoul, South Korea. 3School \nof Electronics Engineering, Kyungpook National University, Buk -gu, Daegu 41566, South Korea.  *email: jhk@\nknu.ac.kr\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nTo address these drawbacks, we proposed a MOPGRU SLR system that diminishes the problem of hand occlu-\nsion and lower learning efficiency by adjusting the output of the update gate using the reset gate integrated with \nan open-source framework called the MediaPipe Holistic  pipeline29 . Furthermore, we changed the activation \nfunction in the output layer and candidate memory state of each GRU cell to achieve a faster, simpler, and cost \neffective SLR system. The main contributions of this study are summarized as follows:\n• We proposed a novel MOPGRU model that calibrate the resultant of the update gate by the reset gate, which \nenchances the learning process of the GRU gating unit, thereby accelerating the convergence rate, eliminating \ngradient depletion problem, and improving the learning efficiency.\n• We replaced the hyperbolic tangent (Tanh) activation function in the candidate memory state with an ELU \nactivation function to overcome the vanishing and exploding gradient problem, and further enhance the \nmodel. As a result, we obtained lower training time and good performance for the MOPGRU model than \nthose of other variants. Additionally, the activation function of the output layer of each GRU cell was replaced \nwith Softsign instead of SoftMax to reduce the computational complexity and hence, the training time of the \nmodel.\nThe rest of this paper is organized as follows: In “ Related work” section introduces existing SLR methods and \ntheir limitations. In “Proposed methodology ” section introduces and describes the proposed methodology. In \n“Experimental results and discussion ” section presents the experimental settings, results, comparison of our \nmethod with other methods, and analyzes the model performance. Finally, “ Conclusion” section presents our \ncontributions and outlines the future work.\nRelated work\nPrevious researchers have emphasized their work on the prediction of sign language gestures to support people \nwith hearing impairments using advanced technologies with artificial intelligence algorithms. Although much \nresearch has been conducted for SLR, there are still limitations and improvements that need to be addressed to \nimprove the hard-of-hearing  community30. This section presents a brief literature review of recent studies on \nSLR using sensor and vision-based deep learning techniques.\nSensor‑based deep learning techniques. To bridge the communication gap between the hard-of-\nhearing community and normal people, researchers have proposed a real-time ISL hand gesture recognition \nsystem that uses a Microsoft kinetic RGB-D camera for inputting images and applies deep learning techniques to \nachieve one-to-one mapping between the depth and RGB pixels on training over 45,000 RGB and depth images, \nwhile achieving a prediction accuracy of 98.81%31. Although the model resulted in good accuracy, it emphasized \nthe need for a large dataset with more images to train and a high-pixel RGB camera. Like the aforementioned \nmodel, an algorithm with a support vector machine and Microsoft kinetic Xbox 360 RGB images for translating \nIndian sign language gestures into English text and speech with 100 % prediction accuracy for the signs repre-\nsenting one numeric value and six ISL alphabets alone was  proposed32.\nUsing an expensive leap motion controller  [LMC]26,30, researchers have proposed a training method for ASL \nwith an long short-term memory (LSTM) recurrent neural network for handling a sequence of input and yields \nan average accuracy rate of 91.08% . This proposed model has several limitations due to the leap motion controller \nbecause the number of users affects the model accuracy. Furthermore, this method is limited to recognizing only \none hand gesture. Neethu et al.33 introduced a deep convolutional neural network (CNN) classification approach \nwith a connected component analysis algorithm to segment the fingertips from the hand image and classify only \neight different gestures into various classes with a 96.2% recognition rate. The performance was analyzed only in \nterms of sensitivity, accuracy, and recognition rate.\nGupta et al.28 proposed a sensor-based multilabel classification to categorize ISL isolated signs by processing \nsignals from sEMG and IMUs placed on both the forearms of signers in an integrated manner with some clas-\nsification and categorization errors. Similarly, Salem et al.34 proposed a real-time customize glove-based method \nwith five-flex and one accelerometer sensor to recognize Arabic sign language gestures and display correspond-\ning English text and audible sounds. Generally, motion gloves sign language prediction has high limitations in \nterms of hand tracking and is uncomfortable for users compared to vision-based methods. Like leap motion \ncontrollers, they are expensive, time-consuming, and may produce inaccurate calibrations due to wear and tear \nfrom the frequent usage of gloves.\nVision‑based deep learning techniques. Rastgoo et  al. 35 proposed a real-time isolated hand SLR \n(IHSLR) from an RGB  video36 by combining deep learning models, singular value decomposition (SVD), and \nSSD with LSTM with  ResNET5037 and further with SSD, 2DCNN,  3DCNN38, and LSTM to obtain features from \nthe 3D hand coordinators and achieved a high accuracy of 99%25. The proposed model is simple and fast; how-\never, the model is not able to recognize in case of high inter-class similarities, and in some cases there also exists \nsome misclassification because of the high occlusion of two hands in some signs, making it difficult to predict \nhand signs correctly. Similarly, Chen et  al. 39 proposed a three-tier network architecture with the short-term \ntraffic prediction model built by using LSTM to simplify network management and to reduce communication \noverheads. Hurroo et al.24 proposed a convolutional neural network (CNN) with the HSV color algorithm and \nvarious computer vision techniques for recognizing only 10 American Sign gesture alphabets and obtained an \naccuracy of 90% . Action recognition architectures constructed with 3D CNN models such as  I3D40 architecture \n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nis also used for SLR task  in41. Although this method uses low computing power, robustness is not achieved, and \nthe prediction accuracy is lower than that of other CNN models.\nOjha et al.42 implemented a fingerspelling sign language translator using a CNN to detect ASL and translate its \ncorresponding text and speech in real-time. The proposed model achieved an accuracy of 95% with certain limita-\ntions; for example, when running the project, the threshold must be monitored to avoid distorted grayscale in the \nframes if it does not lead to resetting the histogram or looking for appropriate lighting conditions. Additionally, \nseveral extensive literature on train CNNs for continuous SLR with weakly labeled data has been  reported43. \nHere, the CNN inside an iterative expectation-maximization algorithm was trained with over 1 million poorly \nlabeled hand gesture images representing the sign language. However, moderate prediction accuracy was achieved \ndespite using only the prerecorded pictures and videos as the input, which are unsuitable for real-time hand \ngesture recognition.For continuous SLR with deep  learning44, a heuristic approach for epenthesis detection to \nsupport continuous natural communication between the machine and user was proposed. Although they have \nreduced classification confusion, they showed good results only when tested individually with more resources \nneeded for implementing an integrated continuous SLR system.Likewise, several studies exist where both static \nand dynamic gesture recognition were performed using machine learning and deep learning  methods9,45–52.\nStandard GRU . Most computer vision problems require handling temporal dependencies among inputs \nand modeling short-term and long-term sequences. Recurrent neural networks (RNNs) are efficient in manag-\ning and processing such sequential data. Compared to traditional neural networks, RNNs focus on manipulating \nstate neurons to learn contextual relations in and between sequential  data53. Training RNNs is a difficult task \ndue to several limitations and the vanishing and exploding gradient problems. GRUs were applied to solving the \nvanishing and exploding gradients incorporated into conventional  RNNs54,55. Among the RNNs, the most fre-\nquently used are LSTM networks that have achieved state-of-the-art performance on various deep learning and \nmachine learning tasks. As a variant of LSTM, GRU performs equally as an LSTM and produces good results. \nIt enhances the configuration of the LSTM units and conjugates the three gating units to two gating units of \nthe LSTM as update gate and reset gate. Thus, the parameters of the GRU network model are considerably less, \nthereby sustaining information dependency and reducing the training time. Figure 1 shows the general structure \nof a standard GRU cell.\nFrom Fig. 1, at each time step t, a GRU cell takes the contents of previous hidden state H t−1 and present input \nX t , operates them through reset and update gates, and passes the computed current state H t to the next time step. \nThe general formulas of a standard GRU cell are as follows:\n(1)Rt = δ(W r · Xt+ Ur · Ht−1 + Br)\n(2)Zt =δ(W z · Xt+ Uz · Ht−1 + Bz)\n(3)Nt = tanh(W h · Xt+ Uh · (Ht−1 ⊙ Rt) + Bh)\n(4)H t =Zt ⊙ Nt + (1 − Zt) ⊙ H t−1\nFigure 1.  Structure of a standard GRU cell.\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nwhere R t and Z t denotes the resultant of the reset and update gates at time t in Eqs. (1) and (2), respectively. N t in \nEq. (3) denotes a current candidate memory value vector that is computed from a hyperbolic tangent activation \nfunction (tanh), represented in Eq. (7) at time t. H t in Eq. (4) indicates the resultant of the standard GRU unit \nat time t − 1 , and computed as a linear interpolation of previous states H t−1 and N t using the result from Z t in \nEq. (2). The Sigmoid activation ( δ ) in Eq. (6) is applied to both R t and Z t gates to scale the values within 0 and 1. \n⊙ denotes the Hadamard product which is nothing but element-wise multiplication. X t is the current input fed \ninto the network at time t. W r , W z and W h are trainable weights of feed-forward connections, whereas U r , U z and \nUh are weights of the recurrent connections. Br , Bz and Bh are bias vectors. Y t indicates the resultant of the GRU \nmodel at time t, which gives the detected result using the SoftMax activation function, and W o , Bo represents the \nweight and bias of H t . The error at each time step is calculated using the predicted output ˆY t at each time step \nand the actual output Y t at each time step and it is given by:\nAnd the total error is calculated by summing up the errors at all time steps, represented in Eq. (9 ). From the \nabove formulas, the GRU model accomplishes long-distance preservation of valuable particulars by reducing \nthe number of gating units, continuously disposing of unwanted particulars, and using the hidden state to store \ninformation dependencies. Although GRU maintains a long-term information dependency, it has a slow con -\nvergence rate and low learning efficiency. Therefore, we proposed an optimized GRU that uses a reset gate to \noptimize the learning structure of GRU and enhance the learning and prediction accuracy.\nMediaPipe. MediaPipe is an open-source framework with a hybrid platform that creates pipelines for pro-\ncessing perceptual data, such as images, videos, and audio. It is an extensive approach employed with ML for \nhand tracking and gesture recognition in real-time. It provides more hand and finger tracking solutions by \naccurately detecting the sign gestures. Specifically, we employed a MediaPipe Holistic pipeline to obtain the \nlandmarks from the face, hands, and body pose. Figure 2 clearly outlines the overall functionality.\nMediaPipe holistic pose landmarks. The MediaPipe Holistic body pose model infers approximately 33 3D land-\nmarks consisting of x, y, and z coordinates on the body from the input image or video using its BlazePose \ndetector and locates the person/pose regions of interest (ROI) within the frame. Using the ROI-cropped frame \nas input, the pose landmark and division masks within the ROI detect poses successively. Thus, it accurately \nlocalizes more key points and suitably fits SLR.\nMediaPipe holistic hand landmarks. MediaPipe Holistic hands infer approximately 21 3D hand landmarks con-\nsisting of x, y, and z coordinates in just a single frame and produce the desired output by combining two models: \nthe palm detection model and the hand keypoint localization model. Initially, the model was employed with a \nsingle-shot detector called Blaze Palm. This detector supports the MediaPipe to reduce the time complexity of \npalm detection given a large dataset of hand sizes in the input image. This model works on the entire image and \nreturns a focused bounding box that highlights the rigid parts, such as palm and fist, for palm detection rather \nthan concentrating on unnecessary objects. Then, the model uses the palm detection output to perform hand \nkeypoint localization. This produced three possible outputs as follows:\n• 21 hand knuckle points in a 2D or 3D space.\n• Hand flag showing the probability of hand presence in the input image.\n• Binary classification of left and right hand.\nMediaPipe holistic face landmarks. The MediaPipe face mesh is a face geometry solution that calculates 468 3D \nface landmarks in real-time with a single input camera and not a depth sensor. It works based on two deep neural \nnetwork models, a detector that computes and operates face locations on a full image and a 3D face landmark \nmodel that operates on the computed locations that predict approximate surface geometry using regression. \nWith accurate cropping of the face, data augmentation processes, such as rotation, scaling, and translation are \nreduced, allowing the network to focus more on coordinate prediction accuracy.\n(5)Yt =SoftMax((W o ∗ H t) + Bo)\n(6)δ(x) = 1\n1 + e−x\n(7)tanh(x) = ex − e−x\nex + e−x\n(8)E t =− Y tlog\n(\nˆY t\n)\n(9)E =\n∑\nt\nE t =⇒ E =\n∑\nt\n−Y tlog\n(\nˆY t\n)\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nProposed methodology\nTo accurately recognize the sign gestures and translate them into text, our proposed method comprises three \nstages: data preprocessing and feature extraction, data cleaning and labelling and gesture recognition. Data \npreprocessing and feature extraction are carried over by the MediaPipe framework. Here, features from the \nface, hands, and body are extracted as keypoints and landmarks using built-in data augmentation techniques \nfrom sequence of input frames taken from a web camera. In stage 2, the extracted keypoints from stage 1 are \nsaved in a file to identify and remove the null entries from the data, after which data labelling follows. In stage \n3, the cleaned and labelled gestures are trained and classified by our MOPGRU model for ISL recognition with \nthe translated sign gestures in the form of text on the screen. Figure 3 shows a general overview of the proposed \narchitecture for an SLR system. For further understanding, the three stages of the proposed methodology are \nelaborately discussed below.\nStage 1: data preprocessing and feature extraction. For data preprocessing and feature extraction \nfrom the image, we applied a multistage pipeline from MediaPipe, called MediaPipe Holistic. For each input \nframe from the web camera, the MediaPipe Holistic handled individual models for the hands, face, and pose \ncomponents using a region-appropriate image resolution. The workflow of stage 1 is briefly discussed below:\n• The human pose and subsequent landmark model were estimated using BlazePose’s pose detector. Then, \nthree ROI crops for the face and hands (2 × ) were derived from the inferred pose landmarks, and a recrop \nwas employed to improve the ROI.\n• Next, the corresponding landmarks were estimated. To achieve this, the full-resolution input coordinates \nwere cropped to the ROIs for task-specific hand and face models.\n• Finally, all landmarks were combined to yield the full 540+ landmarks.\nStage 2: data cleaning and labelling. After stage 1, the extracted features, that is, the landmark points \n( 21 ∗ 3 + 21 ∗ 3 + 33 ∗ 4 + 468 ∗ 3 = 1662 ) per frame are flattened, concatenated and stored in a file to check \nand remove any null entries from the data. Data cleaning is important since it prevents failed detection of \nFigure 2.  Overview of MediaPipe holistic.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\n features56–58, which occurs when a blurred image is sent to the detector and leads to a null entry into the dataset. \nThus, when training occurs with this noisy data, the prediction accuracy is reduced and bias may occur. To fit \nthe obtained data for the next stage of training, testing and validation, labels are created for each class and their \ncorresponding frame sequences are stored.\nStage 3: gesture recognition. Our proposed MOPGRU model. The cleaned and labelled data from stage \n2, are then passed to stage 3. The major alteration performed in the standard GRU cell is that its update gate is \nFigure 3.  General overview of our proposed architecture.\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nimproved, the candidate memory activation function (tanh) in Eq. (3) is substituted by the ELU activation func-\ntion, and SoftMax activation function in Eq. (5) by Softsign respectively.\nImproving the update gate of the standard GRU cell. Considering the problems, such as low learning efficiency, \nhigh computational cost, slow convergence rate and incapable of dropping out the unwanted information in one \nscreening with the complex state of time series data of the standard GRU model, we propose a MOPGRU neural \nnetwork model by improving the update gate; that is, modifying the original update gate input X t to X t multi-\nplied by R t . Thus, the update gate is adjusted by obtaining feedback from the output of the reset gate. The oppo-\nsite effects caused by unnecessary information are profoundly avoided by refining the present input information \nX t by the reset gate attains faster convergence and efficient in learning. Figure 4 shows the neuronal structure of \nour proposed GRU cell. The red dashed box represents the changes made in the update gate with the reset gate \nfrom the standard GRU; the blue dashed box shows the standard hyperbolic tangent activation replaced with \nELU activation and the black dashed box shows the SoftMax activation replaced with Softsign activation.\nThe formula remains unchanged as mentioned in the standard GRU except for Eqs. (2 ), (3) and (5 ). The \nmodified formula for the proposed MOPGRU model is as follows:\nHere, the symbols Z t and R t in Eq. (10) hold the same meaning as in the standard GRU cell unit, except in Z t \nunit structure. The reset gate, R t , is multiplied by the input vector X t and then, by the previous time step H t−1 in \nEq. (10) helped to conceal the state weight, so that the resultant of R t re-screens the present input X t by adjusting \nZ t to optimize the neuron structure. Our improved GRU cell will not cause any change in computing the deriva-\ntive of the loss functions as the weights are not changed. Thus, the proposed MOPGRU model makes more sense \nthan the standard GRU, reduces the hidden state and conceals the impoverished gradient to a limited extent. \nTherefore, our proposed model preserves the information dependency of a longer distance, while producing \nhigher learning efficiency and prediction accuracy.\nIncorporating ELU. The other change implemented in the standard GRU cell was replacing the candidate mem-\nory tanh activation function with the ELU function, as shown in Fig.  4. This is highlighted with a blue dashed \nbox. Thus, Eq. (3) in the standard GRU cell for calculating the candidate memory N t changes to the following \nform:\nThe use of the tanh  activation function in training feed-forward connections, especially in standard GRU, is \nineffective as shown in the performance decline when the network has deeper  connections59. Additionally, it is \ncomputationally expensive and has a vanishing gradient due to its exponential operation. Similarly, the rectified \nlinear unit (ReLU) activation has the dying ReLU problem as its derivative is 0 for negative inputs, meaning that \nweights are not updated during backpropagation, which leads to zero gradients and dead neurons. Additionally, \nusing it for long-range sequences leads to numerical instability because of its unbounded  nature60.\nThe ELU activation function given below in Eq. (12) was employed to determine the candidate memory state \nfor the following reasons:\n(10)Zt = δ((W z · Xt+ Uz · Ht−1) ∗ Rt+ Bz)\n(11)Nt = ELU(W h · Xt+ Uh(Ht−1 ⊙ Rt)) + Bh\nFigure 4.  Structure of our proposed GRU cell.\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\n• Using ELU in deep neural networks results in higher classification accuracy and speedy  learning61.\n• It does not suffer from vanishing and exploding gradient problems because of its non-saturating character-\nistics.\n• Since ELU is continuous and differentiable, the problem of dying neurons is solved.\n• Compared to other activation functions, ELU achieves higher accuracy and faster convergence in less training \ntime and computational complexity since the negative ELU value permits the mean unit to shift toward  061.\nMathematically, the ELU is defined as,\nand its corresponding derivative is defined as,\nwhere α is the ELU hyperparameter that controls the value of negative inputs. From Eq. (13), for all positive \ninput values x, the function simply returns the corresponding output Y. However, if the input x is negative, the \ncorresponding output Y will be exp(x) − 1 . The output of the derivative function moves closer to one. In order \nto understand the usage of ELU better, let us consider the gradient equation in the BPTT  algorithm62:\nwhere\nThus the total gradient is calculated using the Eqs. ( 14), (15) and (16) with the weight matrix W  contains dis-\ntinctive weights for current input and previous hidden state for each gate. Each Jacobian ∂Hk+1\n∂Hk\n is a product of \ntwo matrices is a item of two frameworks: the repetitive weight matrix and diagonal matrix composed of the \nsubordinate of non-linearity,ELU, related with the hidden units.In nonappearance of any input, i.e.,X t = 0 , and \nwith the choice of starting conditions, the two-norm of each Jacobian in Eq. ( 14) is indistinguishably one and \nthe error gradients don’t develop or decay exponential over time.\nSoftsign function for output prediction of the GRU cell. The output layers of the neural network use the SoftMax \nor Sigmoid activation functions for multivariate or binary classification problems, respectively. Softmax activa-\ntion is specifically used to normalize the outputs and convert the weighted sum values to probabilities that sum \nto one by exponentiating the features and scale with the sum of the exponents. Because of its exponential nature, \nit is computationally expensive and time-consuming to train the model. Hence, we incorporated the Softsign \nactivation function as mentioned in Eq. (17) in the output layer of each GRU neurons as shown in Eq. (18), to \nreduce the time as it finds the quadratic polynomials rather than exponentially. Additionally, as it is zero-cen-\ntered, the networks learn effectively and the saturation in the network does not occur easily. Like tanh activation \n, the Softsign ranges from −1 to 1, and is defined as\nwhere |x| is the absolute value of the input point of x. Thus, Eq. (5) in the standard GRU cell for output predic-\ntion Y t changes to the following form:\nWith changes made in Eqs. (10), (12) and (18) the MOPGRU model finally displays the recognized gesture \nresult on the screen with its corresponding English text translation, as shown in Fig. 3 from the output of stage 3.\nConsent to participate. Informed consent was obtained from the subject for publication of identifying \ninformation/images in an online open-access publication.\nExperimental results and discussion\nDataset. A real-time dataset of 30 videos for each sign gesture with 30 frames was created, each with a size of \n640 × 480, using a web camera in various directions and different lighting conditions. There are 13 sign gestures \nin the dataset, as described in Table  1. We separated the collected dataset in the ratio of 70:15:15 to form the \ncorresponding training, testing, and validation datasets. Thus, from the 900 images for each sign gesture, 780 \n(12)ELU (x)=\n{\nx, if x > 0\nα(exp(x)− 1), if x ⩽ 0\n}\n(13)ELU ′(x)=\n{\n1, if x > 0\nELU (x)+ α, if x ⩽ 0\n}\n(14)\n∂E\n∂W =\n∑\nk<=t\n∂Et\n∂W\n(15)=\n∑\nk<=t\n∂Et\n∂ ˆYt\n∂ ˆYt\n∂Ht\n∂Ht\n∂Hk\n∂+Hk\n∂W\n(16)\n∂H t\n∂H k\n= ∂H t\n∂H t−1\n∂H t−1\n∂H t−2\n··· ∂H k+1\n∂H k\n(17)Softsign(x ) = x\n|x |+ 1\n(18)Yt = Softsign((Wo ∗ H t) + Bo).\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nimages were used for the training set and the remaining 120 images were divided equally, providing 60 images \neach for testing and validation purposes. Additionally, we evaluated our model on two benchmark datasets: the \nWord Level American Sign Language (WLASL)  dataset63 and the LSA64  dataset41. The WLASL was created for \nteaching of sign language and hence the data was collected from multiple public resources that includes variety \nof signing styles and different video backgrounds.The LSA64 dataset contains 3200 videos of 64 isolated sign \ngestures from the Argentinian sign language which includes verbs and nouns, performed by 10 different people \nfor each word.\nExperimental setting. The simulation was conducted using Python 3.7 version on a desktop computer \nwith 32 GB RAM and an Intel Core i7 processor with a frequency of 3.60 GHz, running on Windows 10 Pro with \na 64-bit operating system. The input image was captured using a web camera with a resolution of 720pixels/30 \nfps of RGB images. A sequential model of nine layers were created. Of the nine layers, three are GRU layers, a \nbatch normalization layer, two dropout layers, and three dense layers. The first layer of GRU accepts a sequence \nof landmark keypoints (1662) extracted from the frames of each video with the time step of 30 as each video con-\ntains 30 frame sequences. For the hidden units at the first time-step, the input-hidden vectors and all elements of \nthe initial hidden state were set to 0.The number of hidden units per layer for the model was set to 128, 64 and 32 \nrespectively. The total number of parameters was approximately 450,445.The dropout ratio was fixed at 20% for \nthe hidden and fully connected layers. The training data were set to 100 epochs, with 13 gestures in each batch. \nTo optimize the network, the Adam  optimization64 method was used and was set to 10−4 with exponential decay \nrates of 0.9 and 0.999, respectively. The class scores were calculated using the SoftMax activation function with \nbatch normalized input from the fully connected layer. The number of hidden units per layer for the model was \nset to 128, 64 and 32 respectively. The total number of parameters was approximately 450,445.\nEvaluation metrics. To evaluate the performance of our MOPGRU model, we used the mean squared error \n(MSE), mean absolute error (MAE), and R-squared or coefficient of determination metrics, as listed in Table 2. \nMAE denotes the average of the absolute difference between the predicted and actual values in the dataset. It is \ngiven by the following formula:\nMSE is the average of the squared difference between the predicted and actual values in the dataset and is \ncalculated by the following formula:\n(19)MAE = 1\nN\nN∑\ni=1\n|yi−ˆy|\nTable 1.  13 sign gestures and its label.\nLabels Sign gestures (words)\n0 Fail\n1 Friend\n2 Good\n3 Hello\n4 I love you\n5 Like\n6 Location\n7 Meet\n8 Phone call\n9 Take care\n10 Thank you\n11 Think\n12 Yo u\nTable 2.  Comparison on MAE, MSE, and R2 for different models.\nNetwork model MAE MSE R2\nSimple RNN 4.10 28.90 − 1.38\nLSTM 0.75 4.95 0.59\nStandard GRU 0.44 1.38 0.83\nBiGRU 0.40 2.50 0.79\nBiLSTM 0.85 5.35 0.56\nMOPGRU 0.22 1.34 0.88\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nR2 score indicates how well the model fits the given dataset. It indicates how close the predicted value is to the \nactual data values. The value lies between 0.0 and 1.0, where 0.0 indicates the worst fit and 1.0 indicates the perfect \nfit. It was calculated using the following formula:\nwhere ˆy represents the predicted value of y, and ¯y denotes the mean value of y. Error calculations were performed \nfrom Eqs. (19), (20), and (21). From Table 2, we see that the MAE and MSE values are higher for simple RNN, \nLSTM, Standard GRU, bidirectional GRU (BiGRU), and deep bidirectional LSTM (BiLSTM), which means that \nthe average residual and variance of the residual are high. Previously BiLSTM showed good performance in \naction  recognition65, whereas training LSTM and BiLSTM models were not beneficial for our datasets because \nwe had limited data for sequence prediction. Furthermore, we attempted to produce comparatively low predic-\ntion errors using our proposed MOPGRU model.\nQuantitative analysis. The classification metrics were used to analyze the quality of prediction for each \nsign gesture, as shown in Tables 3, 4, and 5, respectively. These qualities include precision, recall, and F1-score66–68, \nwhich are calculated using four values: true positives (TP), false positives (FP), true negatives (TN), and false \nnegatives (FN). We conducted experiments using different models as mentioned in Table  2 by inserting each \nmodel in the place of our proposed MOPGRU layers in the architecture presented in the paper. The number of \ncorrectly predicted data points is called accuracy, and ideally, it must be close to one. The accuracy of the SLR \nsystem is calculated as follows:\n(20)MSE = 1\nN\nN∑\ni=1\n(yi−ˆy)2\n(21)R 2 = 1 −\n∑(yi −ˆy)2\n∑(yi −¯y)2\nTable 3.  Precision. Significant values are in [bold].\nClass label Simple RNN LSTM BiLSTM Standard GRU BiGRU MOPGRU*\n0 1 1 1 1 0.5 1\n1 – – – 1 0 1\n2 1 1 1 1 1 1\n3 1 1 1 1 1 0.75\n4 1 0 0 0 1 1\n5 0 0 0 1 1 1\n6 0.67 0.67 0.67 1 0 1\n7 1 0.75 0.75 – – 1\n8 1 1 1 1 1 1\n9 0.5 1 1 1 1 1\n10 1 1 1 – – 1\n11 0 0 0 0 1 1\n12 1 1 1 0.5 1 0.92\nTable 4.  Recall. Significant values are in [bold].\nClass label Simple RNN LSTM BiLSTM Standard GRU BiGRU MOPGRU*\n0 0.33 1 1 1 1 1\n1 – – – 0.5 0 1\n2 1 1 1 1 1 1\n3 0.5 1 1 1 1 1\n4 1 0 0 0 1 0.86\n5 0 0 0 1 1 1\n6 1 1 1 1 0 1\n7 1 1 1 – – 1\n8 1 1 1 1 1 1\n9 1 1 1 1 1 1\n10 1 1 1 – – 0.7\n11 0 0 0 0 0.5 1\n12 1 0.5 0.5 1 1 1\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nPrecision is the number of positive predictions divided by the total number of positive class values predicted, \nwhich is called the positive predicted value. High cost of false positive makes precision an important measure to \ndetermine. Like accuracy, this must also be close to one. The precision value is calculated as follows:\nA recall is the fraction of positive events predicted correctly by the model and high cost of false negative makes \nrecall an important measure to determine. It is calculated as follows:\nThe F1-score or the F-measure is the harmonic mean of precision, and recall means it conveys a balance \nbetween recall and precision. When there is perfect precision and recall, the F1 score reaches its best value at 1 \nand can be calculated using the following formula:\nBased on the calculations given in Eqs. ( 22), (23), (24) and (25), we obtain a classification report as shown \nin Tables 3, 4 and 5. From the classification metrics, the precision, recall, and F1-score of MOPGRU model are \nalmost 1 except for two values in both precision and recall, and four values in F1-score, which are still ideally \nclose to 1. This shows that our model effectively learned data on complete training. In Contrary, other models \nsuch as LSTM, Simple RNN, Standard GRU, BiLSTM and BiGRU resulted in good accuracy, but their learning \nefficiency is not good. As a result, these models did not perform well as they contain many zero metric scores \nfor precision, recall, and F1-score.\nConsidering the R2 values, our proposed MOPGRU outperforms the other three models with the highest score \nof 0.88, which is closer to 1, indicating that the model has a good fit. However, a negative score in the Simple \nRNN indicates that the model has the worst fit. Figure 5a and b show the training accuracy and loss of different \nmodels and Fig. 6a and b show the validation accuracy and loss of different models used to classify 13 individual \nsign gestures from the dataset. From Fig. 5, we can conclude that the training is unstable due to the fluctuation \nbetween the accuracy and epochs, as well as with loss and epochs. However, in the case of MOPGRU, the training \nis smooth and faster with efficient data learning. Therefore, our proposed model produces higher performance \nwith minimum loss compared with other models, such as RNN, LSTM, standard GRU, BiGRU, and BiLSTM, \nrespectively. By comparing the models with respect to both the test accuracy and loss, as shown in Figs. 7 and 8, \nwe conclude that our proposed MOPGRU model achieved the highest test accuracy of 95 % compared to other \nmodels such as LSTM with 85% , Simple RNN with 80% standard GRU with 85% accuracy, BiGRU with 90% , and \nBiLSTM with 85% and minimum loss of 0.21 compared to others.\nComparative analysis. To access the efficiency and performance of our proposed MOPGRU model, we \nconducted experiments on the two different benchmark datasets LSA64 and WLASL100 respectively and com-\npared our results with different existing state-of-the art models like I3D, Pose-based temporal graph convolu-\ntional network (Pose-TGCN) and Pose-based gated recurrent unit (Pose-GRU). We train the model with the \nsame parameters mentioned  in63 and achieved high recognition accuracy than the existing method. As com-\npared to other models used in the experiment, the learning efficiency and the convergence speed of our pro-\n(22)Accuracy= TP + TN\nTP + TN + FP + FN\n(23)Precision= TP\nTP + FP\n(24)Recall= TP\nTP + FN\n(25)F1 = 2TP\n2TP + FP + FN\nTable 5.  F1-Score. Significant values are in [bold].\nClass label Simple RNN LSTM BiLSTM Standard GRU BiGRU MOPGRU*\n0 0.5 1 1 1 0.67 1\n1 – – – 0.67 0 1\n2 1 1 1 1 1 1\n3 0.67 1 1 1 1 0.86\n4 1 0 0 0 1 0.92\n5 0 0 0 1 1 1\n6 0.8 0.8 0.8 1 0 1\n7 1 0.86 0.86 – – 1\n8 1 1 1 1 1 1\n9 0.67 1 1 1 1 1\n10 1 1 1 – – 0.82\n11 0 0 0 0 0.67 1\n12 1 0.67 0.67 0.67 1 0.96\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nFigure 5.  Learning graph with training accuracy and loss for 100 epochs of different models used.\nFigure 6.  Learning graph with validation accuracy and loss for 100 epochs of different models used.\nFigure 7.  Model comparison in terms of test accuracy in percent ( %).\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nposed MOPGRU was also high with the use of a smaller number of parameters and the activation function \nduring training. Datasets which are existing have their own properties to deal with the isolated word level sign \nrecognition task. However, they fail to capture the complexities of the task due to inadequate amount of instance \nand signers. Thus we evaluated our model on two benchmark datasets that have adequate amount of instance \nand signers. The recognition accuracy on both the datasets are presented in Table 6.\nConclusion\nIn this study, we proposed a MOPGRU model for ISL recognition. Specifically, we modified the update gate of \nthe standard GRU cell by multiplying its output by the reset gate. With our improved update gate mechanism, the \noutput of the reset gate re-screens the information and removes the unwanted information in the data, thereby \ngiving more attention to the important information. We cost-effectively implemented the model, which resulted \nin improved learning efficiency, prediction accuracy, and information processing capability of the standard GRU \nneural network. Furthermore, experimental results showed that compared to simple RNN, LSTM, standard GRU, \nBiGRU, and BiLSTM prediction models, MAE and MSE values of our proposed GRU neural network model \nwere very low with high R-squared values. Therefore, our proposed MOPGRU captured the full information \ndependency in time series data with a high prediction accuracy of an average of 95 % and a faster convergence \nspeed. Although our model performs well in terms of accuracy and learning efficiency, this study was conducted \nwith a limited dataset. Thus, for future work, we aim to improve our SLR system by expanding the dataset with \nmore vocabulary to predict continuous sign language sentences.\nData availibility\nThe datasets generated during and/or analysed during the current study are available from the corresponding \nauthor on reasonable request.\nReceived: 28 February 2022; Accepted: 4 July 2022\nReferences\n 1. Jain, R. K. & Rathi, S. K. A review paper on sign language recognition using machine learning techniques. In Emerging Trends in \nData Driven Computing and Communications (eds Mathur, R. et al.) (Springer, 2021).\n 2. Aloysius, N., Geetha, M. & Nedungadi, P . Incorporating relative position information in transformer-based sign language recogni-\ntion and translation. IEEE Access 9, 145929–145942 (2021).\n 3. Li, D., Rodríguez, C., Yu, X. & Li, H. Word-level deep sign language recognition from video: A new large-scale dataset and methods \ncomparison. arXiv: 1910. 11006 v2 (2019).\n 4. Kadhim, R. A. & Khamees, M. A real-time American sign language recognition system using convolutional neural network for \nreal datasets. TEM J. 9(3), 937–943 (2020).\nFigure 8.  Model comparison in terms of test loss.\nTable 6.  Recognition accuracy achieved by several models on the LSA64 and WLASL100 dataset. Significant \nvalues are in [bold].\nModel Dataset Accuracy (%)\nPose-TGCN WLASL100 55.43\nPose-GRU WLASL100 46.51\nMOPGRU WLASL100 63.18\nI3D LSA64 98.91\nMOPGRU LSA64 99.92\n14\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\n 5. Wadhawan, A. & Kumar, P . Deep learning-based sign language recognition system for static signs. Neural Comput. Appl. 32, \n7957–7968 (2020).\n 6. Zafrulla, Z., Brashear, H., Starner, T., Hamilton, H. & Presti, P . American sign language recognition with the kinect. In Proceedings \nof the 13th International Conference on Multimodal Interfaces (ICMI ’11) 279–286 (Association for Computing Machinery, 2011). \nhttps:// doi. org/ 10. 1145/ 20704 81. 20705 32.\n 7. Kumar, P ., Gauba, H., Roy, P . P . & Dogra, D. P . Coupled HMM-based multi-sensor data fusion for sign language recognition. Pat-\ntern Recognit. Lett. 86(C), 1–8 (2017).\n 8. Kumar, P ., Gauba, H., Roy, P . P . & Dogra, D. P . A multimodal framework for sensor based sign language recognition. Neurocomput-\ning 259, 21–38. https:// doi. org/ 10. 1016/j. neucom. 2016. 08. 132 (2017).\n 9. Elakkiya, R. & Selvamani, K. Subunit sign modeling framework for continuous sign language recognition. Comput. Electr. Eng.  \n74, 379–390. https:// doi. org/ 10. 1016/j. compe leceng (2019).\n 10. Gadekallu, T. R. et al. Hand gesture classification using a novel CNN-crow search algorithm. Complex Intell. Syst. 7, 1855–1868 \n(2021).\n 11. Ibrahim, N. B., Zayed, H. & Selim, M. Advances, challenges and opportunities in continuous sign language recognition. J. Eng. \nAppl. Sci. 15(5), 1205–1227 (2019).\n 12. Koller, O. Quantitative survey of the state of the art in sign language recognition. arXiv (2020).\n 13. Mittal, A., Kumar, P ., Roy, P . P ., Balasubramanian, R. & Chaudhuri, B. B. A modified LSTM model for continuous sign language \nrecognition using leap motion. IEEE Sens. J. 19, 7056–7063. https:// doi. org/ 10. 1109/ JSEN. 2019. 29098 37 (2019).\n 14. Kanisha, B. et al. Smart communication using tri-spectral sign recognition for hearing-impaired people. J. Supercomput.  78, \n2651–2664 (2022).\n 15. Sun, Z. A survey on dynamic sign language recognition. In Advances in Computer, Communication and Computational Sciences  \nVol. 1158 (eds Bhatia, S. K. et al.) (Springer, 2021).\n 16. Rakesh, S., Bharadhwaj, A. & Sree, H. E. Sign language recognition using convolutional neural network. In Innovative Data Com-\nmunication Technologies and Application Vol. 59 (eds Raj, J. S. et al.) (Springer, 2021).\n 17. Kiran, Kumar E., Kishore, P . V . V ., Sastry, A. S. C. S. & Anil, Kumar D. 3D motion capture for Indian sign language recognition \n(SLR). In Smart Computing and Informatics Vol. 78 (eds Satapathy, S. et al.) (Springer, 2018).\n 18. Itkarkar Rajeshri, R., Nandi, A. K. V . & Mungurwadi, V . B. Indian sign language recognition using combined feature extraction. \nIn Advances in Medical Physics and Healthcare Engineering (eds Mukherjee, M. et al.) (Springer, 2021).\n 19. Starner, T., Weaver, J. & Pentland, A. Real-time American sign language recognition using desk and wearable computer based \nvideo. IEEE Trans. Pattern Anal. Mach. Intell. 20, 1371–1375. https:// doi. org/ 10. 1109/ 34. 735811 (1999).\n 20. Vogler, C. & Metaxas, D. N. Adapting hidden Markov models for ASL recognition by using three-dimensional computer vision \nmethods. In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics Vol. 1 (1970). https:// doi. org/ 10. \n1109/ ICSMC. 1997. 625741.\n 21. Shukor, A. Z. et al. A new data glove approach for Malaysian sign language detection. Procedia Comput. Sci. 76, 60–67. https:// doi. \norg/ 10. 1016/j. procs. 2015. 12. 276 (2015).\n 22. Almeida, S., Guimaraes, F . G. & Ramirez, J. A. Feature extraction in Brazilian sign language recognition based on phonological \nstructure and using RGB-D sensors. Expert Syst. Appl. 41(16), 7259–7271. https:// doi. org/ 10. 1016/j. eswa. 2014. 05. 024 (2014).\n 23. Patil, A., Kulkarni, A., Y esane, H., Sadani, M. & Satav, P . Literature survey: Sign language recognition using gesture recognition \nand natural language processing. In Data Management, Analytics and Innovation Vol. 70 (eds Sharma, N. et al.) (Springer, 2021).\n 24. Hurroo, M. & Elham, M. Sign language recognition system using convolutional neural network and computer vision. Int. J. Eng. \nRes. Technol. (IJERT) 9(12), 59–64 (2020).\n 25. Rastgoo, R., Kiani, K. & Escalera, S. Hand sign language recognition using multi-view hand skeleton. Expert Syst. Appl. 150, 113336. \nhttps:// doi. org/ 10. 1016/j. eswa. 2020. 113336 (2020).\n 26. Lee, C. K. M. et al. American sign language recognition and training method with recurrent neural network. Expert Syst. Appl.  \n167(October), 114403. https:// doi. org/ 10. 1016/j. eswa. 2020. 114403 (2021).\n 27. Chen, R.-C., Dewi, C., Huang, S.-W . & Caraka, R. E. Selecting critical features for data classification based on machine learning \nmethods. J. Big Data 7, 52. https:// doi. org/ 10. 1186/ s40537- 020- 00327-4 (2020).\n 28. Gupta, R. & Kumar, A. Indian sign language recognition using wearable sensors and multi-label classification. Comput. Electr. Eng. \n90(December), 106898. https:// doi. org/ 10. 1016/j. compe leceng. 2020. 106898 (2020).\n 29. Grishchenko, I. & Bazarevsky, V . Mediapipe holistic. Retrieved from https://ai.googleblog.com/2020/2012 20/ (2020).\n 30. Naglot, D. & Kulkarni, M. Recognition using the leap motion controller. In International Conference on Inventive Computation \nTechnologies (ICICT) Vol. 2, 1–6 (2016). https:// doi. org/ 10. 1109/ INVEN TIVE. 2016. 78300 97.\n 31. Bhagat, N. K., Vishnusai, Y . & Rathna, G. N. Indian sign language gesture recognition using image processing and deep learning. \nIn 2019 Digital Image Computing: Techniques and Applications (DICTA) (2019). https:// doi. org/ 10. 1109/ DICTA 47822. 2019. 89458 \n50\n 32. Raghuveera Tripuraribhatla, R., Deepthi, R., Mangalashri, R. & Akshaya, R. A depth-based Indian Sign language recognition using \nMicrosoft Kinect. Sadhana Acad. Proc. Eng. Sci. 45(1), 1–13. https:// doi. org/ 10. 1007/ s12046- 019- 1250-6 (2020).\n 33. Neethu, P . S., Ramadass, S. & Sathish, D. An efficient method for human hand gesture detection and recognition using deep learn-\ning convolutional neural networks. Soft Comput. 24(20), 15239–15248. https:// doi. org/ 10. 1007/ s00500- 020- 04860-5 (2020).\n 34. Salem, N., Alharbi, S., Khezendar, R. & Alshami, H. Real-time glove and android application for visual and audible Arabic sign \nlanguage translation. Procedia Comput. Sci. 163, 450–459. https:// doi. org/ 10. 1016/j. procs. 2019. 12. 128 (2019).\n 35. Rastgoo, R., Kiani, K. & Escalera, S. Real-time isolated hand sign language recognition using deep networks and SVD. J. Ambient \nIntell. Humaniz. Comput.https:// doi. org/ 10. 1007/ s12652- 021- 02920-8 (2021).\n 36. Rastgoo, R., Kiani, K. & Escalera, S. Hand pose aware multimodal isolated sign language recognition. Multimed Tools Appl. 80, \n127–163. https:// doi. org/ 10. 1007/ s11042- 020- 09700-0 (2021).\n 37. Rastgoo, R., Kiani, K. & Escalera, S. Video-based isolated hand sign language recognition using a deep cascaded model. Multimedia \nTools Appl. 79(31–32), 22965–22987. https:// doi. org/ 10. 1007/ s11042- 020- 09048-5 (2020).\n 38. Al-Hammadi, M. et al. Hand gesture recognition for sign language using 3DCNN. IEEE Access 8, 79491–79509. https:// doi. org/ \n10. 1109/ ACCESS. 2020. 29904 34 (2020).\n 39. Chen, C., Liu, L., Wan, S., Hui, X. & Pei, Q. Data dissemination for industry 4.0 applications in internet of vehicles based on short-\nterm traffic prediction. ACM Trans. Internet Technol. 22, 1–18. https:// doi. org/ 10. 1145/ 34305 05 (2022).\n 40. Carreira, J. & Zisserman, A. (2017) Quo vadis action recognition? A new model and the kinetics dataset. In proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition 6299–6308 (2017).\n 41. Quiroga, F ., Ronchetti, F ., Estrebou, C. A., Lanzarini, L. C. & Rosete, A. Lsa64: An argentinian sign language dataset. In XXII \nCongreso Argentino de Ciencias de la Computación 794–803 (CACIC, 2016).\n 42. Ojha, A., Pandey, A., Maurya, S., Thakur, A. & Dayananda, P . Sign language to text and speech translation in real time using \nconvolutional neural network. Int. J. Eng. Res. Technol. (IJERT) 8(15), 191–196 (2020).\n 43. Koller, O., Ney, H. & Bowden, R. Deep hand: How to train a CNN on 1 million hand images when your data is continuous and \nweakly labelled. In IEEE International Conference on Computer Vision and Pattern Recognition Vol. 2016-Decem, 3793–3802 (2016). \nhttps:// doi. org/ 10. 1109/ CVPR. 2016. 412\n15\nVol.:(0123456789)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\n 44. Mocialov, B., Turner, G. H., Lohan, K. S. & Hastie, H. Towards continuous sign language recognition with deep learning. In Pro-\nceedings of the Workshop on the Creating Meaning With Robot Assistants: The Gap Left by Smart Devices (2017).\n 45. Molchanov, P ., Y ang, X., Gupta, S., Kim, K., Tyree, S. & Kautz, J. Online detection and classification of dynamic hand gestures with \nrecurrent 3D convolutional neural networks. In IEEE International Conference on Computer Vision and Pattern Recognition vol. \n2016-December, 4207–4215 (2016).\n 46. Elakkiya, R. & Selvamani, K. Enhanced dynamic programming approach for subunit modelling to handle segmentation and \nrecognition ambiguities in sign language. J. Parallel Distrib. Comput.  117, 246–255. https:// doi. org/ 10. 1016/j. jpdc. 2017. 07. 001 \n(2018).\n 47. Cheok, M. J., Omar, Z. & Jaward, M. H. A review of hand gesture and sign language recognition techniques. Int. J. Mach. Learn. \nCybern. 10(1), 131–153. https:// doi. org/ 10. 1007/ s13042- 017- 0705-5 (2019).\n 48. Nai, W ., Liu, Y ., Rempel, D. & Wang, Y . Fast hand posture classification using depth features extracted from random line segments. \nPattern Recognit. 65(November), 1–10. https:// doi. org/ 10. 1016/j. patcog. 2016. 11. 022 (2017).\n 49. Elakkiya, R. Machine learning based sign language recognition: a review and its research frontier. J. Ambient Intell. Humaniz. \nComput.https:// doi. org/ 10. 1007/ s12652- 020- 02396-y (2021).\n 50. Adithya, V ., Vinod, P . R. & Gopalakrishnan, U. Artificial neural network based method for Indian sign language recognition. In \n2013 IEEE Conference on Information and Communication Technologies (ICT) 1080–1085 (2013). https://  doi. org/ 10. 1109/ CICT. \n2013. 65582 59.\n 51. Meng, X. J., Qiu, S., Wan, S., Cheng, K. & Cui, L. A motor imagery EEG signal classification algorithm based on recurrence plot \nconvolution neural network. Pattern Recognit. Lett. 134146, 134–141. https:// doi. org/ 10. 1016/j. patrec. 2021. 03. 023 (2021). ISSN \n0167-8655.\n 52. Xiao, L., Fan, C., Ouyang, H., Abate, A. F . & Wan, S. Adaptive trapezoid region intercept histogram based Otsu method for brain \nMR image segmentation. J. Ambient Intell. Hum. Comput. 13, 2161–2176. https:// doi. org/ 10. 1007/ s12652- 021- 02976-6 (2022).\n 53. Lyu, Y . & Huang, X. Road segmentation using CNN with GRU. Comput. Vis. Pattern Recognit.arXiv: 1804. 05164 (2018).\n 54. Cho, K., Van Merriënboer, B., Bahdanau, D. & Bengio, Y . On the properties of neural machine translation: Encoder–decoder \napproaches. Comput. Lang.arxiv: 1409. 1259 (2014).\n 55. Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F ., Schwenk, H. & Bengio, Y . Learning phrase representations \nusing RNN encoder–decoder for statistical machine translation. Comput. Lang. Retrieved from arxiv: 1406. 1078 (2014).\n 56. Olimov, B. et al. Weight initialization based-rectified linear unit activation function to improve the performance of a convolutional \nneural network model. Pract. Exp. Concurr. Comput.https:// doi. org/ 10. 1002/ cpe. 6143 (2020).\n 57. Olimov, B., Kim, J. & Paul, A. Deep clean before training network: Training deep convolutional neural networks with extremely \nnoisy labels. IEEE Access 8, 220482–220495. https:// doi. org/ 10. 1109/ ACCESS. 2020. 30418 73 (2020).\n 58. Olimov, B., Kim, J. & Paul, A. REF-Net: Robust, efficient, and fast network for semantic segmentation applications using devices \nwith limited computational resources. IEEE Access 9, 15084–15098. https:// doi. org/ 10. 1109/ ACCESS. 2021. 30527 91 (2021).\n 59. Gulcehre, C., Moczulski, M., Denil, M. & Bengio, Y . Noisy activation functions. In Proceedings of the 33rd International Conference \non International Conference on Machine Learning, (ICML ’16) Vol. 48. JMLR.org, 3059–3068 (2016).\n 60. Ravanelli, M., Brakel, P ., Omologo, M. & Bengio, Y . Light gated recurrent units for speech recognition. IEEE Trans. Emerg. Top. \nComput. 2(2), 92–102. https:// doi. org/ 10. 1109/ TETCI. 2017. 27627 39 (2018).\n 61. Clevert, D.A., Unterthiner, T. & Hochreiter, S. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUS). \narXiv: 1511. 07289 v5 (2015).\n 62. Pascanu, R., Mikolov, T. & Bengio, Y . On the difficulty of training recurrent neural networks. In Proceedings of the 30th International \nConference on International Conference on Machine Learning, (ICML ’13) Vol. 28. JMLR.org, III-1310–III-1318 (2013).\n 63. Li, D., Rodriguez, C., Yu, X. & Li, H. Word-level deep sign language recognition from video: A new large-scale dataset and methods \ncomparison. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 1459–1469 (2020).\n 64. Kingma, D. P . & Ba, J. Adam: A method for stochastic optimization. arxiv: 1412. 6980 (2014).\n 65. Ullah, A., Ahmad, J., Muhammad, K., Sajjad, M. & Baik, S. W . Action recognition in video sequences using deep bi-directional \nLSTM with CNN features. IEEE Access 6, 1155–1166. https:// doi. org/ 10. 1109/ ACCESS. 2017. 27780 11 (2017).\n 66. Olimov, B. et al. FU-Net: Fast biomedical image segmentation model based on bottleneck convolution layers. Multimedia Syst. 27, \n1–14. https:// doi. org/ 10. 1007/ s00530- 020- 00726-w (2021).\n 67. Olimov, B., Koh, S.-J. & Kim, J. AEDCN-Net: Accurate and efficient deep convolutional neural network model for medical image \nsegmentation. IEEE Accesshttps:// doi. org/ 10. 1109/ ACCESS. 2021. 31286 07 (2021).\n 68. Olimov, B., Kim, J., Paul, A. & Subramanian, B. An efficient deep convolutional neural network for semantic segmentation. In 8th \nInternational Conference on Orange Technology (ICOT) 1–9 (2020). https:// doi. org/ 10. 1109/ ICOT5 1877. 2020. 94687 48.\nAcknowledgements\nThis research was supported by Basic Science Research Program through the National Research Foundation \nof Korea (NRF) funded by the Ministry of Education (2021R1I1A3043970). This study was also supported \nby the BK21 FOUR project (AI-driven Convergence Software Education Research Program) funded by the \nMinistry of Education, School of Computer Science and Engineering, Kyungpook National University, Korea \n(4199990214394).\nAuthor contributions\nB.S. conceived the idea and wrote the main manuscript. B.O. and S.M.N. contributed to the data analysis and \nedited the manuscript. S.K., K.-H.P . and J.K. supervised the work.\nCompeting interest \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n16\nVol:.(1234567890)Scientific Reports |        (2022) 12:11964  | https://doi.org/10.1038/s41598-022-15998-7\nwww.nature.com/scientificreports/\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}