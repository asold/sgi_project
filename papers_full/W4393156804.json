{
    "title": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering",
    "url": "https://openalex.org/W4393156804",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2101639624",
            "name": "Zhenyu Li",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A5113151725",
            "name": "Sunqi Fan",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2117761656",
            "name": "Yu Gu",
            "affiliations": [
                "The Ohio State University"
            ]
        },
        {
            "id": "https://openalex.org/A2615549568",
            "name": "Xiuxing Li",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Institute of Computing Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2951365664",
            "name": "Zhichao Duan",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2275601388",
            "name": "Bowen Dong",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2039430831",
            "name": "Ning Liu",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2101039141",
            "name": "Jianyong WANG",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2615549568",
            "name": "Xiuxing Li",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Institute of Computing Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2252136820",
        "https://openalex.org/W4285147034",
        "https://openalex.org/W4285276085",
        "https://openalex.org/W3175795662",
        "https://openalex.org/W3152740956",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3156366114",
        "https://openalex.org/W4224267275",
        "https://openalex.org/W3034273250",
        "https://openalex.org/W4285429094",
        "https://openalex.org/W4312091111",
        "https://openalex.org/W2604314403",
        "https://openalex.org/W4297161808",
        "https://openalex.org/W2566402689",
        "https://openalex.org/W6682291990",
        "https://openalex.org/W3199258975",
        "https://openalex.org/W4385573325",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4221149883",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4385571219",
        "https://openalex.org/W4297578259",
        "https://openalex.org/W4295683125",
        "https://openalex.org/W4303649020",
        "https://openalex.org/W4389520783",
        "https://openalex.org/W4385570040",
        "https://openalex.org/W4289870801",
        "https://openalex.org/W2511149293",
        "https://openalex.org/W2971136144",
        "https://openalex.org/W4385570025",
        "https://openalex.org/W4385281527",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4306311862",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4302010387",
        "https://openalex.org/W4206636317",
        "https://openalex.org/W4296604031",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W2151149636"
    ],
    "abstract": "Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. Code is available at https://github.com/leezythu/FlexKBQA.",
    "full_text": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base\nQuestion Answering\nZhenyu Li1*, Sunqi Fan1*, Yu Gu2, Xiuxing Li3, 4†, Zhichao Duan1,\nBowen Dong1, Ning Liu5, Jianyong Wang1†\n1Tsinghua University\n2The Ohio State University\n3University of Chinese Academy of Sciences\n4Key Laboratory of Intelligent Information Processing Institute of Computing Technology, CAS\n5 Shandong University\nAbstract\nKnowledge base question answering (KBQA) is a critical yet\nchallenging task due to the vast number of entities within\nknowledge bases and the diversity of natural language ques-\ntions posed by users. Unfortunately, the performance of most\nKBQA models tends to decline significantly in real-world\nscenarios where high-quality annotated data is insufficient.\nTo mitigate the burden associated with manual annotation, we\nintroduce FlexKBQA by utilizing Large Language Models\n(LLMs) as program translators for addressing the challenges\ninherent in the few-shot KBQA task. Specifically,FlexKBQA\nleverages automated algorithms to sample diverse programs,\nsuch as SPARQL queries, from the knowledge base, which\nare subsequently converted into natural language questions\nvia LLMs. This synthetic dataset facilitates training a special-\nized lightweight model for the KB. Additionally, to reduce the\nbarriers of distribution shift between synthetic data and real\nuser questions, FlexKBQA introduces an execution-guided\nself-training method to iterative leverage unlabeled user ques-\ntions. Furthermore, we explore harnessing the inherent rea-\nsoning capability of LLMs to enhance the entire framework.\nConsequently, FlexKBQA delivers substantial flexibility, en-\ncompassing data annotation, deployment, and being domain\nagnostic. Through extensive experiments on GrailQA, We-\nbQSP, and KQA Pro, we observe that under the few-shot even\nmore challenging zero-shot scenarios, FlexKBQA achieves\nimpressive results with a few annotations, surpassing all pre-\nvious baselines and even approaching the performance of su-\npervised models, achieving a remarkable 93% performance\nrelative to the fully-supervised models. We posit that FlexK-\nBQA represents a significant advancement towards exploring\nbetter integration of large and lightweight models. Code is\navailable at https://github.com/leezythu/FlexKBQA.\nIntroduction\nKnowledge base question answering (KBQA) plays a cru-\ncial role in leveraging the substantial knowledge stored in\nknowledge bases and making it accessible to users (Berant\net al. 2013; Yih et al. 2016; Gu et al. 2022). With the ever-\n*These authors contributed equally.\n†Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: An analytical experiment on 500 random samples\nof GrailQA dev set (oracle entity linking). FlexKBQA’s per-\nformance exhibits a consistently upward trend with the in-\ncreasing synthetic data size, surpassing all in-context learn-\ning models with limited window length. Since our synthetic\ndata are generated based on 25-shot real data, we also de-\npict the performance of our underlying model (RnG-KBQA)\ntrained by 25-shot real data as a baseline.\ngrowing size and complexity of knowledge bases (KBs), ef-\nfective KBQA systems are becoming increasingly impor-\ntant. Many current KBQA systems depend on supervised\ntraining using large sets of manually annotated data. This\napproach brings about significant labor challenges for two\nmain reasons: (1) The substantial size and comprehensive\nscope of the KB, coupled with its ever-evolving content, lead\nto a vast sample space (Berant et al. 2013; Gu et al. 2021).\nThis complicates the process of collecting a representative\nand exhaustive set of training data. (2) The heterogeneity in\nKB schema and design protocols, combined with differences\nin the query languages, such as SPARQL (Su et al. 2016), S-\nexpression (Gu et al. 2021), and KoPL (Cao et al. 2022a),\nrequires tailored training processes for each KB. This inten-\nsifies the labor involved in developing and adapting models\nfor each distinct KB. Considering these complexities, there\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18608\nemerges an imperative to forgea flexible framework that can\nefficiently build adaptable KBQA models for different KBs\nand query languages, utilizing only a limited set of anno-\ntated examples.\nThe emergence of large language models (LLMs) in re-\ncent times, such as Codex (Chen et al. 2021a) and GPT-\n4 (OpenAI 2023), suggests promising avenues for con-\nstructing such frameworks. LLMs have showcased versatil-\nity across diverse tasks and remarkable generalization with\nminimal demonstrations (Wei et al. 2022; Li et al. 2023a),\nas exemplified in Cheng et al. (2023), where Codex out-\nperforms previous fine-tuning methods with only a handful\nof examples. Such feats hint at LLMs’ potential in KBQA.\nHowever, research on LLMs’ utility for few-shot KBQA re-\nmains scant. Initial explorations by Li et al. (2023a) and Gu,\nDeng, and Su (2023) employ in-context learning, allowing\nLLMs to transform questions into programs using minimal\ndemonstrations. Nonetheless, this paradigm possess several\nlimitations. First, LLMs inherently scuffle with constraints\nsuch as limited context window and considerable inference\noverhead. Furthermore, LLMs, despite their generalization\ncapacities, face challenges in addressing the intricacies em-\nbedded within domain-specific KBs when not fine-tuned,\nleading to inaccuracies when generating target entities and\nrelations (Li et al. 2023a). Additionally, without fine-tuning,\nthe benefits of in-context learning diminish as more training\ndata is introduced (Gu, Deng, and Su 2023).\nIn light of these challenges, we present FlexKBQA (see\nFigure 2), a flexible KBQA framework harnesses the ad-\nvanced generative abilities of LLMs to produce synthetic\ndata, which in turn aids in the training of lightweight models.\nSpecifically, FlexKBQA first leverages collected templates\nof structured queries to sample a substantial number of pro-\ngrams (i.e., S-expressions) from the KB. These programs\nare then converted into coherent natural language questions\nusing LLMs, ensuring both the accuracy of the programs\nand the fluency of the generated questions. Upon gener-\nating these program-question pairs, they serve as valuable\nresources for the fine-tuning of lightweight models. How-\never, there could be a potential distribution shift between\nsynthetic data and real user queries. To bridge this gap, we\nfurther introduce the execution-guided self-training (EGST)\nmethod, which employs the fine-tuned lightweight model to\nproactively annotate real user queries over the KB. These an-\nnotated queries subsequently serve as valuable training data,\nenabling self-improvement. FlexKBQA addresses the inher-\nent constraints of the context window of LLMs, enabling\nthe KBQA model to exhibit consistent improvement as more\ntraining data is incorporated (Figure 1). Additionally, by del-\negating the inference process to lightweight models rather\nthan relying on the LLM directly, our approach ensures en-\nhanced efficiency in inference.\nWe conduct extensive experiments on three representa-\ntive datasets - GrailQA, WebQSP, and KQA Pro. FlexK-\nBQA outperforms existing models by a large margin in the\nfew-shot setting. Remarkably, with a mere 25 labeled ex-\namples, FlexKBQA surpasses the performance of all previ-\nous methods utilizing 100 shots on GrailQA. Furthermore,\nFlexKBQA’s results are comparable to several fully super-\nvised models on both GrailQA and WebQSP datasets.\nThe main contributions can be highlighted as follows:\n• We present an efficient and flexible KBQA framework,\ncapitalizing on the notable generative capabilities of\nLLMs.\n• We introduce the execution-guided self-training strategy\nto address the distribution shift challenge by facilitat-\ning interaction between heterogeneous information and\nleveraging the inherent reasoning ability of LLM.\n• Experimental results demonstrate that FlexKBQA signif-\nicantly outperforms all baselines on real-world datasets\nunder different settings.\n• To the best of our knowledge, our work represents the\ninaugural endeavor in exploring the zero-shot KBQA.\nRelated Work\nFew-Shot Language Understanding with LLMs\nResearchers have been dedicated to studying the language\nunderstanding capabilities in the zero-shot or few-shot set-\nting (Gao et al. 2019; Li et al. 2022, 2023b). In recent years,\nLLMs have demonstrated strong few-shot learning abilities\nacross various tasks, such as question answering (Cheng\net al. 2023), code generation (Ni et al. 2023), and embod-\nied agents (Singh et al. 2023). Recently, Pangu (Gu, Deng,\nand Su 2023) and KB-BINDER (Li et al. 2023a) have pio-\nneered the exploration of the few-shot setting on the KBQA\ntask. They conduct experiments under the in-context learn-\ning paradigm, providing a few question-program pairs and\nallowing the large language model to leverage its comple-\ntion capability to make new predictions. Despite the remark-\nable generalization ability of large language models, they are\nstill limited by the inherent window size of in-context learn-\ning, which constrains their performance. Additionally, large\nmodels come with drawbacks in terms of cost and security.\nTherefore, in this paper, we explore a hybrid approach that\ncombines large and lightweight models, aiming to achieve\nimproved performance while being more deployable.\nCombination of LLMs and Lightweight Models\nAlthough LLMs have demonstrated strong capabilities\nacross various tasks, they are computationally expensive,\nslow during inference, and difficult to deploy. Numerous re-\nsearch endeavors have been focused on exploring the inter-\naction and complementary aspects between large-scale mod-\nels and lightweight models, e.g., distilling the knowledge\nfrom large-scale models to lightweight models by matching\nthe output distribution (Hinton, Vinyals, and Dean 2015).\nDespite the traditional knowledge distillation, researchers\npropose a new paradigm called teaching via data (TvD), in\nwhich they use an LLM-based “teacher” model to generate\nsynthetic data for a specific task, then use the data to fine-\ntune a smaller “student” model (Ho, Schmid, and Yun 2023;\nSchick and Sch¨utze 2021; Rosenbaum et al. 2022a,b; Meng\net al. 2022; Ye et al. 2022a). The types of synthetic data gen-\nerated by LLM are diverse. SYMGEN (Ye et al. 2023) first\nproposes using LLM to generate symbolic language with\nexecution-based verification. Our work adopts the teaching\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18609\nvia data paradigm but we are the first to utilize LLM as pro-\ngram translator, thereby addressing the challenge of anno-\ntating KBQA training data.\nMethodology\nPreliminaries\nA knowledge base can be formally represented as K ∈\nE × R ×(E ∪ L ∪ C), where E represents the set of en-\ntities, R denotes the set of relations between entities, and\nC denotes the set of classes. The primary goal of KBQA is\nto answer user questions by leveraging the facts stored in\nthe knowledge base. For accuracy and interpretability, most\nexisting work adopt a semantic parsing framework, trans-\nforming natural language questions into programs, such as\nS-expression or SPARQL. These programs can be executed\non the knowledge base to retrieve the final answer.\nMost models utilize a framework comprising a ranking\nmodel, a generation model, or a combination of both. The\nranking model is usually optimized as follows:\nLranker = − es(q,p)\nes(q,p) + P\npi∈P∧pi̸=p es(q,pi) (1)\nwhere p is the target program, and s(q, pi) is the similar-\nity score between the question q and each program pi from\na candidate set P. The score is derived from a lightweight\nmodel like BERT (Devlin et al. 2019). On the other hand, a\ngeneration model employs advanced pre-trained generative\nmodels, such as T5, to generate the target program:\nLgen = −\nnX\nt=1\nlog(probability(pt|p<t; q; r)) (2)\nwhere n is the length of p, and r represents additional infor-\nmation like ranking results.\nAutomatic Program Sampling\nThe objective of program sampling is to generate valid (ex-\necutable on KB) programs. We break this process into two\ncrucial steps: template collection and step-wise grounding.\nTemplate collectionTaking a SPARQL query as an ex-\nample, if we replace the entities and relations within it\nwith variables (e.g., ent0, rel0, ent1), the resulting struc-\ntural form is referred to as a “template”. There are also\nprevious works tackling KBQA tasks using template-driven\nmethods directly (Unger et al. 2012; Zheng et al. 2018). By\nemploying automated algorithms, we can generate a diverse\nset of templates to cover various question types that users\nmay encounter in real-world scenarios. Compared to anno-\ntating programs for a large number of questions, the cost of\ncollecting a handful of templates is negligible.\nStep-wise groundingAfter obtaining a collection of di-\nverse program templates, the next step is to perform entity\nand relation grounding. Leveraging the querying mechanism\nof SPARQL, we can directly treat the variables as query ob-\njects. However, directly executing the query with multiple\nvariables can result in long execution times or errors, par-\nticularly when dealing with large-scale knowledge bases. To\naddress this challenge, we introduce a “step-wise ground-\ning” approach, where we iteratively determine the values of\nvariables. Through empirical assessment, we note that this\nstrategy efficiently narrows down the search space while\nmaintaining diversity, ultimately enabling the derivation of\na substantial number of programs.\nLow-Resource Program Translation\nAs illustrated in Figure 2, conventional approaches often uti-\nlized language models to directly convert questions into pro-\ngrams via in-context learning. However, given that LLMs\nare primarily trained on natural language and lack specific\ntraining on programs, we argue that translating programs\ninto natural language questions is a more efficient and in-\ntuitive approach, especially under the low-resource setting.\nIn this paper, we consider the LLM as a program trans-\nlator that converts a program ps\ni into its corresponding nat-\nural language question qs\ni . The prompt is composed of (1)\nan instruction Inst to guide the model in transforming pro-\ngrams into natural language questions. (2) a few seed pairs\nof golden programs and questions {(pf\n1 , qf\n1 ), ...,(pf\nN , qf\nN )}\nas demonstrations, where N denotes a N-shot setting, and\nN = 0represents the zero-shot setting. Note that when con-\nstructing seed pairs, we should prioritize selecting diverse\nprograms to cover a wide range of question types. Overall,\nthe translation process can be formulated as:\nqs\ni ← Translator (Inst; (pf\n1 , qf\n1 ), ...,(pf\nN , qf\nN ); ps\ni ) (3)\nExecution-Guided Self Training\nAfter generating a synthetic dataset using the aforemen-\ntioned steps for training lightweight models, a potential is-\nsue still remains – the distribution discrepancy between syn-\nthetic and real-world questions. This discrepancy primarily\narises from the limited overlap of entities and relations. Ex-\nisting literature emphasizes that distribution shifts can have a\nsubstantial impact on the model’s performance. For instance,\na model trained on the GrailQA dataset might achieve only\naround 65% performance compared to being trained directly\non the WebQSP (Gu et al. 2021). Therefore, in this paper, we\npropose using execution-guided self-training (EGST) to ad-\ndress this issue. Taking into account the ease of collecting\nuser questions in real-world scenarios, we apply the self-\ntraining method to harness the information from these unla-\nbeled user questions. Moreover, we introduce the utilization\nof execution results from structured queries as feedback in-\nformation, substantially enhancing the purity of the training\nsamples.\nSpecifically, the entire training process follows a teacher-\nstudent iterative training approach shown in Algorithm 1.\nFirstly, we train a teacher model using the synthetic data\ngenerated by the LLM. Subsequently, in each iteration, the\nteacher model generates pseudo programs for unlabeled real\nuser questions, which undergo an execution-guided filter-\ning mechanism to remove noisy data and obtain a cleaner\ntraining dataset. This filtered dataset, alongside the synthetic\ndata, is employed to train the student model, which serves as\nthe teacher model for the subsequent iteration. This iterative\nprocess continues until convergence is achieved.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18610\nFigure 2: A comparison between FlexKBQA and prior methods. (a): Prior approaches enable LLMs to directly ground the\nquestion to the knowledge base through in-context learning capabilities. (b): An illustration of FlexKBQA’s innovative design:\n(1) Automatic Program Sampling module generates diverse and executable programs. (2) Low-Resource Program Translation\nmodule synthesizes high-quality data pairs. (3) Execution-Guided Self-Training module addresses distribution shift. (4) Inherent\nReasoning module boosts the pipeline by leveraging inherent knowledge within LLMs.\nFor the execution-guided filtering mechanism, we employ\nthe following filtering rules during each iteration:\n• Error Filtering: we remove those data pairs where the\npredicted SPARQL queries either result in execution er-\nrors or fail to retrieve answers.\n• Semantic Filtering: we employ an off-the-shelf pre-\ntrained sentence-transformer (Reimers and Gurevych\n2019) to calculate the semantic textual similarity be-\ntween natural language questions and the relations in pre-\ndicted programs, and filter out those data pairs with low\nsimilarity.\n• Inherent Reasoning Filtering: We exclude samples whose\npseudo answers do not align with the outcomes of Inher-\nent Reasoning. Further details are provided in the next\nsection.\nInherent Reasoning Augmentation\nA widely accepted view is that LLMs encode knowledge in\ntheir parameters, allowing them to answer complex ques-\ntions and directly respond to user queries. We refer this\napproach as Inherent Reasoning (IR). However, compared\nto transforming user questions into executable programs on\nKBs, Inherent Reasoning lacks interpretability and could\nexhibit limited effectiveness when dealing with domain-\nspecific KBs. Thus, in this paper, we employ Inherent Rea-\nsoning as a data augmentation technique. It primarily oper-\nates in two stages. (1) In the execution-guided self-training\nstage, after annotating pseudo programs and obtaining an-\nswers for real user questions, we select samples where the\nanswers align with those generated through Inherent Rea-\nsoning. By combining Inherent Reasoning with execution-\nguided filtering, we can obtain a training dataset with re-\nduced noise. (2) Inherent Reasoning can serve as a comple-\nmentary approach to semantic parsing. When the semantic\nAlgorithm 1: Execution-Guided Self Training Procedure\nInput: Unlabeled user questions Du = {(qu\ni )} ;\nA few labeled question-program pairs Df = {(qf\ni , pf\ni )};\nSynthetic pairs from LLM Ds = {(qs\ni , ps\ni )};\nInitial model parameters θini\nOutput: Model parameterθfinal\n1: Fine-tune teacher model θtea on synthetic data and\nsmall labeled data Ds ∪ Df from θini\n2: while not converged do\n3: //Generate pseudo-label for user questions\npu\ni ← f(qu\ni ; θtea)\n4: //Execution-guided filtering\n{(quf\ni , puf\ni )} ←Filter (KB, {(qu\ni , pu\ni )})\n5: //Fine-tune student model θstu on all data\nD = {(qs\ni , ps\ni )} ∪ {(qf\ni , pf\ni )} ∪ {(quf\ni , puf\ni )}\n6: //Update the teacher model\nθtea ← θstu\n7: end while\n8: θfinal ← θteaparsing method fails to retrieve an answer (e.g. cannot enu-\nmerate ranking candidates or the predicted programs result\nin execution errors), we resort to using results from Inherent\nReasoning as the final answers for the initially unanswerable\nquestions. We believe that the integration of Inherent Rea-\nsoning and the semantic parsing approach holds significant\nvalue in the development of more accurate and interpretable\nKBQA systems.\nIn summary, with the aforementioned core components,\nFlexKBQA can achieve the following dimensions of flexi-\nbility: (1) Data Efficient: FlexKBQA requires only a small\nnumber of question-program pairs as prompts. (2) Domain-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18611\nDataset KB Questions Program Type\nGrailQA Freebase 64,331 S-expression\nWebQSP Freebase 4,737 SPARQL\nKQA Pro Wikidata 117,970 SPARQL\nTable 1: Dataset Statistics\nAgnostic: FlexKBQA is applicable to diverse knowledge\nbases, and alleviates common distribution shifting issues. (3)\nDeployable: Utilizing a lightweight model allows for cost-\neffective deployment compared to closed-source LLMs and\nenables seamless integration of domain-specific knowledge\nthrough fine-tuning.\nExperimental Setup\nDatasets\nGrailQA (Gu et al. 2021) dataset is a large-scale dataset\nfor knowledge base question answering that contains 64,331\nquestion-logical form pairs. It has a broad coverage and\nmany unique canonical logical forms. GrailQA introduces\nthree levels of generalization for KBQA: i.i.d., composi-\ntional and zero-shot. This dataset has attracted substantial\nresearch interest in recent years.\nWebQSP (Yih et al. 2016) is another widely recognized\nKBQA dataset. The questions in this dataset are extracted\nfrom Google query logs, making them reflective of user\npreferences in real-world scenarios. It also offers SPARQL\nannotations that can be executed directly on Freebase. The\nmain objective of this dataset is to evaluate the generaliza-\ntion capability in an i.i.d. setting, as the training and testing\ndata share common entities and relations.\nKQA Pro(Cao et al. 2022a) consists of around 120,000 di-\nverse natural language questions that require various reason-\ning capabilities, such as multi-hop inference, attribute com-\nparison, and set operations. KQA Pro is constructed based\non a sub-knowledge base from Wikidata and does not as-\nsume an entity linking stage. Therefore, it requires models\nto memorize entities and relations, making it a more strong\nand challenging i.i.d task.\nUnderlying Model and Baselines\nNote that FlexKBQA is model-agnostic. Considering per-\nformance and reproducibility, we choose a well performing\nRnG-KBQA (Ye et al. 2022b) as the underlying model for\nGrailQA and WebQSP. For KQA Pro, we select the BART-\nSPARQL (Cao et al. 2022a) model.\nFor baselines, we mainly evaluate FlexKBQA against\nPangu (Gu, Deng, and Su 2023) and KB-BINDER (Li et al.\n2023a). Both of them leverage the potent large language\nmodel Codex for in-context learning. Note that because\nthe few-shot KBQA is a relatively novel task, our baseline\nchoices are limited to these two methods. Since there are no\navailable experimental results for Pangu and KB-BINDER\non KQA Pro, we also re-implement an in-context learning\nmodel called LLM-ICL as an alternative for evaluation. We\nalso provide the supervised results of several representative\nmodels for comparison.\nImplementation Details\nIn experiments, we consider the original training dataset\nas the unlabeled real user questions. During the “step-wise\ngrounding” stage, we not only employ random sampling but\nalso gather the set of entities present in unlabeled user ques-\ntions. These entities are then used as “topic entities” for con-\nstructing SPARQL queries. After removing duplicates, we\nobtain 6,184 synthetic pairs on Freebase and 5,017 on Wiki-\ndata. For a fair comparison, we use the same off-the-shelf\nentity linkers as Pangu (Gu, Deng, and Su 2023). And the\nLLM we utilize for program translation is gpt-3.5-turbo1.\nResults\nMain Results\nThe results are presented in Table 2, 3 and 4. Compared to\nother baselines that also target the few-shot KBQA scenario,\nFlexKBQA demonstrates significant superiority. On the test\nset of GrailQA , it achieved an impressive Exact Match (EM)\nscore of 62.8 and an F1 score of 69.4 with only 25 anno-\ntated samples, outperforming the previous state-of-the-art\nmodel Pangu by a significant margin of 6.7 points in terms\nof F1 score, despite Pangu utilizing more shots. Surprisingly,\nFlexKBQA also surpasses several supervised models, such\nas ReTraCk, which demand training with tens of thousands\nof samples. Since we use RnG-KBQA model as the underly-\ning model of FlexKBQA, it’s interesting to note that FlexK-\nBQA attains a remarkable 93% performance relative to the\nfully-supervised RnG-KBQA model.\nOn WebQSP and KQA Pro datasets, FlexKBQA exhibits\na similar trend of remarkable superiority over in-context\nlearning methods. Specifically, when considering the F1\nscore, FlexKBQA achieves a significant 6.1-point improve-\nment over Pangu on WebQSP under the 100-shot setting. It\nis worth noting that FlexKBQA’s performance on the KQA\nPro dataset shows a gap compared to the best-performing\nmodel. This difference can be attributed to the absence of an\nentity linking stage in KQA Pro. As a result, if the relations\nor entities in the test set were not present during training,\nFlexKBQA is more prone to producing semantic correct but\nunexecutable programs.\nWe also conduct experiments under a more challenging\nzero-shot setting, assuming no available annotated samples\nat all. This scenario has been rarely explored in previous\nresearch. We observe that, although the performance is be-\nhind the level achieved in the few-shot scenario, results on\nGrailQA show significant potential of our method. And we\nbelieve that this direction is worth further exploration.\nEGST and IR\nAccording to Tables 2, 3, and 4, FlexKBQA trained using\nsynthetic data only demonstrates comparable performance\nwhen compared to previous in-context learning-based meth-\nods. The experimental results clearly illustrate the signif-\nicant contribution of EGST to the model’s performance.\n1https://platform.openai.com/docs/models/gpt-3-5\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18612\nOverall\nI.I.D. Compositional Zero-shot Dev Overall\nModel EM F1 EM F1 EM F1 EM F1 EM F1\nSupervised\nQGG (Lan\nand Jiang 2020) - 36.7 - 40.5 - 33.0 - 36.6 - -\nBERT+Ranking (Gu et al. 2021) 50.6 58.0 59.9 67.0 45.5 53.9 48.6 55.7 - -\nReTraCk (Chen et al. 2021b) 58.1 65.3 84.4 87.5 61.5 70.9 44.6 52.5 - -\nRnG-KBQA (Ye et al. 2022b) 68.8 74.4 86.2 89.0 63.8 71.2 63.0 69.2 71.4 76.8\nArcaneQA (Gu and Su 2022) 63.8 73.7 85.6 88.9 65.8 75.3 52.9 66.0 69.5 76.9\nDecAF (Yu et al. 2023) 68.4 78.7 84.8 89.9 73.4 81.8 58.6 72.3 - 81.4\nFew-Shot\n(100\nshots)\nKB-BINDER (Li et al. 2023a) 50.6 56.0 - - - - - - - -\nPangu (Gu, Deng, and Su 2023) 53.3 62.7 54.7 62.9 54.5 63.7 52.3 62.2 - -\nFew-Shot\n(25\nshots)\nFine-Tuning 16.6 21.3 19.0 24.8 17.3 21.8 15.2 19.4 16.4 21.6\nFlexKBQA 62.8 69.4 71.3 75.8 59.1 65.4 60.6 68.3 65.5 71.1\n-w/o IR 62.8 68.0 71.3 75.3 59.1 64.1 60.6 66.4 65.5 70.6\n-w/o EGST 52.4 57.7\n56.9 61.8 49.4 54.4 51.7 57.3 57.0 62.1\nZero-Shot\nFlexKBQA 61.9 68.9\n72.1 77.0 58.4 65.2 58.9 66.9 64.6 70.3\n-w/o IR 61.9 67.6 72.1 76.5 58.4 64.1 58.9 65.0 64.6 69.8\n-w/o EGST 51.9 57.5 56.1 60.5 49.6 53.7 52.9 57.8 56.4 61.2\nTable 2: Results on GrailQA\nModel F1\nSupervised\nQGG (Lan\nand Jiang 2020) 74.0\nReTraCk (Chen et al. 2021b) 71.0\nCBR (Das et al. 2021) 72.8\nProgram Transfer (Cao et al. 2022b) 76.5\nRnG-KBQA (Ye et al. 2022b) 75.6\nDecAF (Yu et al. 2023) 78.8\nFew-Shot\n(100\nshots)\nFine-Tuning 25.6\nKB-BINDER (Li et al. 2023a) 53.2\nPangu (Gu, Deng, and Su 2023) 54.5\nFlexKBQA 60.6\n-w/o IR 58.2\n-w/o EGST 51.1\nZero-Shot\nFlexKBQA 46.2\n-w/o IR 45.7\n-w/o EGST 33.4\nTable 3: Results on WebQSP\nIt resulted in a substantial increase of 10.3 and 7.1 in F1\nscore on GrailQA and WebQSP, respectively, and a notable\n10.2 increase in accuracy on KQA Pro. These findings pro-\nvide strong evidence of the effectiveness of EGST in miti-\ngating the impact of distribution shift. Inherent Reasoning\ncan be regarded as an effective and flexible enhancement\nmethod that leverages the inherent knowledge of large mod-\nels. On the GrailQA and WebQSP datasets, Inherent Rea-\nsoning leads to a F1 score improvement of 1.4 (EM score\nremains unchanged as it focuses solely on the consistency\nof structural queries) and 2.4, respectively. Remarkably, on\nthe KQA Pro dataset, Inherent Reasoning achieves a accu-\nracy increase of 13.5. This is because the absence of the en-\ntity linking stage results in numerous SPARQL execution\nerrors. However, LLM was able to provide accurate answers\ndirectly in such cases.\nModel Accuracy\nSupervised\nEmbedKGQA (Sax\nena et al. 2020) 28.36\nRGCN (Schlichtkrull et al. 2018) 35.07\nRNN SPARQL (Cao et al. 2022a) 41.98\nBART+SPARQL (Cao et al. 2022a) 89.68\nFew-Shot\n(100\nshots)\nFine-Tuning 22.45\nLLM-ICL 31.75\nFlexKBQA 46.83\n-w/o IR 33.32\n-w/o EGST 23.10\nZero-Shot\nFlexKBQA 33.28\n-w/o IR 11.11\n-w/o EGST 8.24\nTable 4: Results on KQA Pro\nIn conclusion, FlexKBQA achieves superior performance\ncompared to previous few-shot KBQA systems. Moreover,\non certain datasets, it demonstrates competitive results when\ncompared to supervised models. We claim that the superior-\nity two main reasons. Firstly, it benefits from a large amount\nof synthetic data compared to in-context learning, leading\nto better convergence with respect to data distribution. Sec-\nondly, the innovative EGST and IR approaches equip FlexK-\nBQA with the capability to harness unlabeled real user ques-\ntions and leverage the inherent prowess of LLMs. These\nunique features distinguish FlexKBQA from other methods.\nBeyond Few-Shot KBQA\nIn this section, we delve into its efficacy beyond the few-\nshot scenario. As demonstrated in Figure 3, with an increase\nin the number of samples, the model pretrained on our syn-\nthetic data consistently performs better. Even when there are\n1000 real samples available, it still maintains an 8-point ad-\nvantage over models trained solely on real samples. This ob-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18613\nQuestion I What\ntype of art leonardo da vinci do?\nPangu\n(JOIN (R visual art.visual artist.associated periods or mov\nements) m.04lg6) (%)\nFlexKBQA (JOIN (R visual art.visual artist.art forms)\nm.04lg6) (!)\nSynthetic Data\nPair What type of art did andy warhol create?\n(JOIN (R visual art.visual artist.art forms)\nm.0kc6)\nQuestion II What\nairport do you fly into to get to destin fl?\nPangu (JOIN\n(R travel.travel destination.tourist attractions)\nm.0rp8x) (%)\nFlexKBQA w/o EGST (JOIN (R travel.travel destination.how to get here)\nm.0rp8x) (%)\nFlexKBQA (JOIN (R location.location.nearby airports)\nm.0rp8x) (!)\nPseudo Labeled\nPair What airport is closer to downtown houston?\n(JOIN (R location.location.nearby airports)\nm.03l2n) (!)\nTable 5: Two typical questions from the test set of WebQSP that FlexKBQA succeeds while Pangu fails.\nFigure 3: Results beyond few-shot setting. FlexKBQA con-\nsistently performs better with more annotated data.\nservation underscores that our approach is not only applica-\nble to few-shot scenarios but can also serve as a valuable\ndata augmentation technique.\nAblation and Case Studies\nIn Figure 4, we delve into an ablation study to gauge the im-\npact of EGST. We can observe that with each iteration, the\nmodel’s performance steadily enhances, eventually converg-\ning around the 6th epoch. The underlying reason, illustrated\nby the green line, is the diminishing error rate of pseudo pro-\ngrams.\nTo better showcase the advantages of FlexKBQA, we con-\nduct a comparison between FlexKBQA and Pangu, a state-\nof-the-art model. In Table 5, Question I illustrates a non-\ni.i.d scenario, where the relation involved in the question\nis not present in in-context examples. Consequently, Pangu\nstruggles to answer correctly. However, due to FlexKBQA’s\ninclusion of the relevant relation in its synthetic data, it\nmanages to generate the correct answer. In an ideal situa-\ntion where the synthetic data coverage is extensive enough,\nnon-i.i.d scenarios like this could be eradicated. Question II\ndemonstrates the impact of EGST. By utilizing information\nFigure 4: Variation of model performances and the error rate\nof pseudo-labeled programs with EGST Iterations.\nfrom a correctly pseudo-labeled user question, the model can\nsuccessfully answer similar but ambiguous or more complex\nquestions in the test set.\nConclusion and Future Work\nIn conclusion, this paper presented FlexKBQA, a framework\nthat harnesses the power of LLMs as program translators\nand their inherent reasoning ability for KBQA tasks. Exten-\nsive experiments on diverse datasets showcase the effective-\nness of FlexKBQA under the few-shot setting, surpassing all\nbaseline methods and even approaching the performance of\nsupervised models. Furthermore, we are pioneers in explor-\ning zero-shot KBQA setting. FlexKBQA represents a signif-\nicant advancement in the integration of large and lightweight\nmodels, offering three-fold flexibilities: data annotation, de-\nployment, and being domain agnostic. Future research can\nexplore the broader applications of this framework in more\nnatural language understanding and reasoning tasks.\nAcknowledgments\nThis work was supported in part by National Key Re-\nsearch and Development Program of China under Grant No.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18614\n2020YFA0804503, National Natural Science Foundation of\nChina under Grant No. 62272264, and Beijing Academy of\nArtificial Intelligence (BAAI), and also supported by Inde-\npendent Research Project of Medical Engineering Labora-\ntory of Chinese PLA General Hospital (2022SYSZZKY23).\nReferences\nBerant, J.; Chou, A.; Frostig, R.; and Liang, P. 2013. Se-\nmantic parsing on freebase from question-answer pairs. In\nProceedings of the 2013 conference on empirical methods in\nnatural language processing, 1533–1544.\nCao, S.; Shi, J.; Pan, L.; Nie, L.; Xiang, Y .; Hou, L.; Li, J.;\nHe, B.; and Zhang, H. 2022a. KQA Pro: A Dataset with Ex-\nplicit Compositional Programs for Complex Question An-\nswering over Knowledge Base. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), 6101–6119.\nCao, S.; Shi, J.; Yao, Z.; Lv, X.; Yu, J.; Hou, L.; Li, J.; Liu,\nZ.; and Xiao, J. 2022b. Program transfer for answering com-\nplex questions over knowledge bases. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 8128–8140.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; de Oliveira Pinto,\nH. P.; Kaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brock-\nman, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf,\nH.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.;\nPavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.;\nTillet, P.; Such, F. P.; Cummings, D.; Plappert, M.; Chantzis,\nF.; Barnes, E.; Herbert-V oss, A.; Guss, W. H.; Nichol, A.;\nPaino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.;\nJain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.;\nAchiam, J.; Misra, V .; Morikawa, E.; Radford, A.; Knight,\nM.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; Mc-\nGrew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and\nZaremba, W. 2021a. Evaluating Large Language Models\nTrained on Code. arXiv:2107.03374.\nChen, S.; Liu, Q.; Yu, Z.; Lin, C.-Y .; Lou, J.-G.; and Jiang,\nF. 2021b. ReTraCk: A flexible and efficient framework for\nknowledge base question answering. In Proceedings of the\n59th annual meeting of the association for computational\nlinguistics and the 11th international joint conference on\nnatural language processing: system demonstrations, 325–\n336.\nCheng, Z.; Xie, T.; Shi, P.; Li, C.; Nadkarni, R.; Hu,\nY .; Xiong, C.; Radev, D.; Ostendorf, M.; Zettlemoyer, L.;\nSmith, N. A.; and Yu, T. 2023. Binding Language Models\nin Symbolic Languages. arXiv:2210.02875.\nDas, R.; Zaheer, M.; Thai, D.; Godbole, A.; Perez, E.; Lee,\nJ. Y .; Tan, L.; Polymenakos, L.; and McCallum, A. 2021.\nCase-based Reasoning for Natural Language Queries over\nKnowledge Bases. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 9594–9611.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nGao, T.; Han, X.; Zhu, H.; Liu, Z.; Li, P.; Sun, M.; and Zhou,\nJ. 2019. FewRel 2.0: Towards More Challenging Few-Shot\nRelation Classification. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), 6250–6255.\nGu, Y .; Deng, X.; and Su, Y . 2023. Don’t Generate, Dis-\ncriminate: A Proposal for Grounding Language Models to\nReal-World Environments. arXiv:2212.09736.\nGu, Y .; Kase, S.; Vanni, M.; Sadler, B.; Liang, P.; Yan, X.;\nand Su, Y . 2021. Beyond iid: three levels of generalization\nfor question answering on knowledge bases. In Proceedings\nof the Web Conference 2021, 3477–3488.\nGu, Y .; Pahuja, V .; Cheng, G.; and Su, Y . 2022. Knowledge\nBase Question Answering: A Semantic Parsing Perspective.\narXiv:2209.04994.\nGu, Y .; and Su, Y . 2022. ArcaneQA: Dynamic Program In-\nduction and Contextualized Encoding for Knowledge Base\nQuestion Answering. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics, 1718–\n1731.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. arXiv:1503.02531.\nHo, N.; Schmid, L.; and Yun, S.-Y . 2023. Large Language\nModels Are Reasoning Teachers. arXiv:2212.10071.\nLan, Y .; and Jiang, J. 2020. Query Graph Generation for\nAnswering Multi-hop Complex Questions from Knowledge\nBases. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 969–974.\nLi, T.; Ma, X.; Zhuang, A.; Gu, Y .; Su, Y .; and Chen, W.\n2023a. Few-shot In-context Learning for Knowledge Base\nQuestion Answering. arXiv:2305.01750.\nLi, X.; Li, Z.; Zhang, Z.; Liu, N.; Yuan, H.; Zhang, W.; Liu,\nZ.; and Wang, J. 2022. Effective few-shot named entity link-\ning by meta-learning. In 2022 IEEE 38th International Con-\nference on Data Engineering (ICDE), 178–191. IEEE.\nLi, Z.; Li, X.; Duan, Z.; Dong, B.; Liu, N.; and Wang,\nJ. 2023b. Toward a Unified Framework for Unsupervised\nComplex Tabular Reasoning. In 2023 IEEE 39th Inter-\nnational Conference on Data Engineering (ICDE), 1691–\n1704. IEEE.\nMeng, Y .; Huang, J.; Zhang, Y .; and Han, J. 2022. Gener-\nating Training Data with Language Models: Towards Zero-\nShot Language Understanding. arXiv:2202.04538.\nNi, A.; Iyer, S.; Radev, D.; Stoyanov, V .; Yih, W.-t.; Wang,\nS.; and Lin, X. V . 2023. Lever: Learning to verify language-\nto-code generation with execution. In International Confer-\nence on Machine Learning, 26106–26128. PMLR.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT:\nSentence Embeddings using Siamese BERT-Networks.\narXiv:1908.10084.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18615\nRosenbaum, A.; Soltan, S.; Hamza, W.; Saffari, A.; Da-\nmonte, M.; and Groves, I. 2022a. CLASP: Few-Shot\nCross-Lingual Data Augmentation for Semantic Parsing.\narXiv:2210.07074.\nRosenbaum, A.; Soltan, S.; Hamza, W.; Versley, Y .; and\nBoese, M. 2022b. LINGUIST: Language Model Instruction\nTuning to Generate Annotated Utterances for Intent Classi-\nfication and Slot Tagging. arXiv:2209.09900.\nSchick, T.; and Sch¨utze, H. 2021. Generating Datasets with\nPretrained Language Models. arXiv:2104.07540.\nSchlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg,\nR.; Titov, I.; and Welling, M. 2018. Modeling relational\ndata with graph convolutional networks. In The Semantic\nWeb: 15th International Conference, ESWC 2018, Herak-\nlion, Crete, Greece, June 3–7, 2018, Proceedings 15, 593–\n607. Springer.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\nProgprompt: Generating situated robot task plans using large\nlanguage models. In 2023 IEEE International Conference\non Robotics and Automation (ICRA), 11523–11530. IEEE.\nSu, Y .; Sun, H.; Sadler, B.; Srivatsa, M.; G ¨ur, I.; Yan, Z.;\nand Yan, X. 2016. On generating characteristic-rich question\nsets for qa evaluation. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 562–572.\nUnger, C.; B¨uhmann, L.; Lehmann, J.; Ngonga Ngomo, A.-\nC.; Gerber, D.; and Cimiano, P. 2012. Template-based ques-\ntion answering over RDF data. In Proceedings of the 21st\ninternational conference on World Wide Web, 639–648.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nYe, J.; Gao, J.; Li, Q.; Xu, H.; Feng, J.; Wu, Z.; Yu, T.; and\nKong, L. 2022a. ZeroGen: Efficient Zero-shot Learning via\nDataset Generation. arXiv:2202.07922.\nYe, J.; Li, C.; Kong, L.; and Yu, T. 2023. Generating\nData for Symbolic Language with Large Language Models.\narXiv:2305.13917.\nYe, X.; Yavuz, S.; Hashimoto, K.; Zhou, Y .; and Xiong,\nC. 2022b. RNG-KBQA: Generation Augmented Iterative\nRanking for Knowledge Base Question Answering. In Pro-\nceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 6032–\n6043.\nYih, W.-t.; Richardson, M.; Meek, C.; Chang, M.-W.; and\nSuh, J. 2016. The value of semantic parse labeling for\nknowledge base question answering. In Proceedings of the\n54th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), 201–206.\nYu, D.; Zhang, S.; Ng, P.; Zhu, H.; Li, A. H.; Wang, J.; Hu,\nY .; Wang, W.; Wang, Z.; and Xiang, B. 2023. DecAF: Joint\nDecoding of Answers and Logical Forms for Question An-\nswering over Knowledge Bases. arXiv:2210.00063.\nZheng, W.; Yu, J. X.; Zou, L.; and Cheng, H. 2018. Question\nanswering over knowledge graphs: question understanding\nvia template decomposition. Proceedings of the VLDB En-\ndowment, 11(11): 1373–1386.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18616"
}