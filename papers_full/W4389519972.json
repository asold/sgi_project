{
  "title": "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model",
  "url": "https://openalex.org/W4389519972",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2130067707",
      "name": "Jiabo Ye",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2973950256",
      "name": "Anwen Hu",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2103589644",
      "name": "Xu Haiyang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2108108901",
      "name": "Qinghao Ye",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2051706254",
      "name": "Ming Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113052374",
      "name": "Guohai Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105534490",
      "name": "Chenliang Li",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2130363364",
      "name": "Junfeng Tian",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2119459298",
      "name": "Qi Qian",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2109058337",
      "name": "Ji Zhang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A1996522893",
      "name": "Qin Jin",
      "affiliations": [
        "Renmin University of China",
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2105816867",
      "name": "Liang He",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2120503993",
      "name": "Xin Lin",
      "affiliations": [
        "East China Normal University",
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367367040",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4312233877",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W3205084812",
    "https://openalex.org/W4386141732",
    "https://openalex.org/W3121976951",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W3173585224",
    "https://openalex.org/W4287555689",
    "https://openalex.org/W3201693581",
    "https://openalex.org/W4386065837",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W2963420691",
    "https://openalex.org/W3120043490",
    "https://openalex.org/W2766732270",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W2988326850",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W4376653374",
    "https://openalex.org/W4320481960",
    "https://openalex.org/W3009518609",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W3004268082",
    "https://openalex.org/W3176851559",
    "https://openalex.org/W4382202658",
    "https://openalex.org/W3106859150",
    "https://openalex.org/W4304192731",
    "https://openalex.org/W4304013646",
    "https://openalex.org/W2963899988"
  ],
  "abstract": "Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, Fei Huang. Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2841–2858\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUReader: Universal OCR-free Visually-situated Language\nUnderstanding with Multimodal Large Language Model\nJiabo Ye1∗, Anwen Hu2∗, Haiyang Xu2†, Qinghao Ye2,\nMing Yan2†, Guohai Xu2, Chenliang Li2, Junfeng Tian2, Qi Qian2, Ji Zhang2,\nQin Jin3, Liang He1, Xin Lin1, Fei Huang2\n1East China Normal University\n2DAMO Academy, Alibaba Group 3Renmin University of China\njiabo.ye@stu.ecnu.edu.cn {huanwen.haw,ym119608,shuofeng.xhy}@alibaba-inc.com\nVisual Questioning:Who was ranked between denismenchovand stephane Goubert?Information Extraction:What is the value for thegross_amount? Text Reading:Recognize the textsIn the Image.\nA stop sign with a sticker that says eating animals.In 2020, the total value of beer imported into Canada was 716.55 million Canadian dollars.Samuel Sánchez (ESP)718,00.00For Clarly'sSake, Please Don't Say \"Licensed under GPL, 2……\nImage Captioning:Provide a brief description of the given image.\nNaturalImage\nKey Points Generation: Point out several critical features in this image.\nChart TableDocument\nWebpageUReader\nFigure 1: The OCR-free Visually-situated Language Understanding performance of UReader on various types of\nimages and tasks.\nAbstract\nText is ubiquitous in our visual world, convey-\ning crucial information, such as in documents,\nwebsites, and everyday photographs. In this\nwork, we propose UReader, a first exploration\nof universal OCR-free visually-situated\nlanguage understanding based on the Mul-\ntimodal Large Language Model (MLLM).\nBy leveraging the shallow text recognition\nability of the MLLM, we only finetuned 1.2%\nparameters and the training cost is much lower\nthan previous work following domain-specific\npretraining and finetuning paradigms. Con-\ncretely, UReader is jointly finetuned on a\nwide range of Visually-situated Language\nUnderstanding tasks via a unified instruction\nformat. To enhance the visual text and\nsemantic understanding, we further apply two\nauxiliary tasks with the same format, namely\ntext reading and key points generation tasks.\nWe design a shape-adaptive cropping module\nbefore the encoder-decoder architecture of\nMLLM to leverage the frozen low-resolution\nvision encoder for processing high-resolution\nimages. Without downstream finetuning, our\n∗Equal contribution\n††Corresponding authors\nsingle model achieves state-of-the-art ocr-free\nperformance in 8 out of 10 visually-situated\nlanguage understanding tasks, across 5\ndomains: documents, tables, charts, natural\nimages, and webpage screenshots. Codes\nand instruction-tuning datasets are released at\nhttps://github.com/LukeForeverYoung/UReader.\n1 Introduction\nLeveraging strong Large Language Models as the\nlanguage decoder, some recent works propose Mul-\ntimodal Large Language Models (MLLMs) (Zhu\net al., 2023; Liu et al., 2023a; Ye et al., 2023; Li\net al., 2023) and achieve promising vision-and-\nlanguage understanding performance. Surprisingly,\nwithout in-domain training, these MLLMs exhibit\nshallow zero-shot visual text recognition ability\nwhen fed a low-resolution image with salient text\ninformation (Ye et al., 2023; Liu et al., 2023b).\nHowever, due to the variety of image types and the\nwide range of image sizes, they are still far from\nuniversal visually-situated language understanding,\nsuch as extracting information from documents,\nreading texts from webpages, and visual question\nand answering on tables, as shown in Figure 1.\n2841\nExisting works for visually-situated language un-\nderstanding can be categorized into two-stage (Xu\net al., 2021; Huang et al., 2022; Yang et al., 2021)\nand end-to-end (Davis et al., 2022; Kim et al., 2022;\nLee et al., 2022) methods according to whether re-\nlying on an off-the-shelf OCR model or API. These\nworks all follow a domain-specific pretraining and\nfinetuning paradigm, thus leading to high training\ncosts, e.g. end-to-end model Donut (Kim et al.,\n2022) costs more than 192 A100 days.\nInspired by the shallow text recognition abil-\nity of existing MLLMs, in this work, we propose\nUReader for universal OCR-free visually-situated\nlanguage understanding, which leverages the multi-\nmodal Large Language Model via low-cost instruc-\ntion tuning (Dai et al., 2023). Different from previ-\nous works, we forgo pretraining tasks by leveraging\nthe existing MLLM and directly finetune MLLM by\ntaking full advantage of various Visually-situated\nLanguage Understanding datasets. To make the\nmost of the strong language understanding ability\nof MLLM, we convert all tasks into the vision-\nlanguage instruction tuning format. Besides, to en-\nhance text recognition and semantic understanding\nability across diverse domains, we design auxil-\niary text reading and key points generation tasks\nin the same instruction format. To utilize the low-\nresolution encoder of MLLM for processing high-\nresolution images and avoid blurry and distortion\nproblems due to resizing, we propose a shape-\nadaptive cropping module to cut a high-resolution\nimage into multiple local images. Each image is\nfirstly independently encoded with the frozen vi-\nsual encoder and a trainable visual abstractor and\nthen concatenated to feed into the language de-\ncoder. Moreover, we add learnable crop position\nencoding to help the model correlate local images\nand add a resized global image to alleviate salient\ninformation loss due to cropping.\nOur contributions in this work are four-fold:\n• We first propose instruction tuning with Multi-\nmodal Large Language Models for OCR-free\nVisually-situated Language Understanding.\n• We build an instruction-tuning dataset cover-\ning 5 domains of visually-situated language\nunderstanding: document, table, chart, natural\nimage, and webpage screenshot.\n• We design a shape-adaptive cropping module\nto utilize the frozen low-resolution vision en-\ncoder for processing high-resolution images.\n• UReader achieves state-of-the-art OCR-free\nperformance in 8 out of 10 tasks, across 5\ndomains.\n2 Related Work\nVisually-situated Language Understanding aims\nto comprehend images containing rich text infor-\nmation. The image types are quite diverse, cover-\ning document (Mathew et al., 2021, 2022; Stanis-\nlawek et al., 2021; Svetlichnaya, 2020; Zhang et al.,\n2023), table (Pasupat and Liang, 2015; Chen et al.,\n2020), chart (Masry et al., 2022; Methani et al.,\n2020; Kafle et al., 2018; Kahou et al., 2018), natu-\nral image (Singh et al., 2019; Mishra et al., 2019;\nBiten et al., 2019; Hu et al., 2021), webpage screen-\nshot (Tanaka et al., 2021; Chen et al., 2021), etc.\nTasks of Visually-situated Language Understand-\ning range from visual question answering, image\ncaptioning, information extraction to natural lan-\nguage inference.\nAccording to whether using off-the-shelf OCR\nmodels or APIs to recognize texts from images,\nexisting work can be divided into two-stage models\n(Xu et al., 2021; Huang et al., 2022; Tang et al.,\n2023; Yang et al., 2021) and end-to-end models\n(Kim et al., 2022; Davis et al., 2022; Lee et al.,\n2022). Two-stage work always designs pretrianing\ntasks to learn cross-modality alignment between\nvisual inputs and text inputs. For example, for doc-\nument understanding, UDOP (Tang et al., 2023)\ndesign a Joint Text-Layout Reconstruction task to\nrecover masked texts and layout information given\nthe visual inputs and retained text inputs. Lay-\noutLMv3 (Huang et al., 2022) applies a Masked\nImage Modeling task to recover masked image\ntokens with the context of their surrounding text\nand image tokens. Without the help of an off-the-\nshelf OCR model, end-to-end models need to learn\ntext recognition with a high-resolution image en-\ncoder during the pretraining stage. For example,\nPix2Struct (Lee et al., 2022) proposes a Screenshot\nParsing pretraining task, where the model needs to\ngenerate the complete HTML DOM tree with only\na masked webpage screenshot as the input. Donut\n(Kim et al., 2022) designs a pretraining task to gen-\nerate all texts in the document image. These work\nall follow a domain-specific pretraining and fine-\ntuning paradigm and therefore ask for high train-\ning costs, e.g. Donut is trained for more than 192\nA100 days. In this work, by leveraging the shal-\nlow text recognition ability of Multimodal Large\nLanguage Models, we propose to directly perform\n2842\ninstruction tuning across various types of images\nand greatly reduce the training cost for universal\nvisually-situated Language Understanding.\nMultimodal Large Language Model is developed\nto empower the Large Language Model with multi-\nmodality understanding ability, especially for vi-\nsion information. These work (Huang et al., 2023;\nZhu et al., 2023; Liu et al., 2023a; Ye et al., 2023;\nLi et al., 2023; Dai et al., 2023) mainly connect a\npre-trained vision encoder (usually CLIP VIT-L/14\n(Radford et al., 2021)) with a strong large language\nmodel, such as LLaMA (Touvron et al., 2023).\nThese MLLMs show some emergent abilities, in-\ncluding shallow zero-shot text recognition ability\n(Liu et al., 2023b). However, they are still far from\nuniversal visually-situated language understanding.\nFirstly, due to the pretraining data for the vision en-\ncoder being mostly natural images, MLLMs show\nbarely acceptable text understanding performance\non natural images but bad performance on other\ntypes, such as document (Liu et al., 2023b). Sec-\nondly, most images for visuall-situated language\nunderstanding are high-resolution. Rescaling them\nto low resolution to adapt to the vision encoder\ncan cause the texts blurry and distorted. In this\nwork, we propose to fully leverage the shallow text\nrecognition ability of MLLMs and perform instruc-\ntion tuning to enhance its universal understanding\nability across 5 domains. Besides, we design a\nshape-adaptive cropping module to alleviate the\ntext blur and distortion problem.\n3 UReader\nThe primary goal of UReader is to efficiently utilize\nexisting MLLMs for Visually-situated Language\nUnderstanding tasks. In this work, we utilize but\nare not limited to, the mPLUG-Owl (Ye et al., 2023)\nas our basic MLLM. Figure 2 presents an overall\narchitecture of UReader. The input image is firstly\npre-processed by a shape-adaptive cropping mod-\nule (in Section 3.1). The resulting sub-images are\nthen simultaneously passed through the visual en-\ncoder and visual abstractor. To enable the large\nlanguage model to correlate multiple cropped sub-\nimages, we apply a crop position encoding module\nto introduce spatial information across sub-images.\n(in Section 3.2).\n3.1 Shape-Adaptive Cropping Module\nImages with texts have various aspect ratios and a\ngreat range of resolutions. Simply resizing the im-\nLarge Language Model\nLoRA\nVisual Encoder\nVisual Abstractor\nShape-Adaptive  Cropping Module\nCrop Position Encoding\nTwitter\nWho has more active users, Pinterest or Twitter?\nFigure 2: The overall architecture of UReader.\nage to Hv, Wv (raw resolution of the MLLM) can\nresult in text being blurred, distorted, and unrecog-\nnizable. Thus we propose a shape-adaptive crop-\nping module. Specifically, as shown in Figure 3,\nwe pre-define grids {g = ( nh ×nw)|nh ·nw ≤\nNc, nh ∈N, nw ∈N}with various shapes, where\nnh and nw denote the number of rows and columns\nof the grid g and Nc denotes the maximum number\nof the cells (sub-images). To select a suitable grid\nfor an image I with shape H ×W, two rules should\nbe followed: (1) The grid should preserve the reso-\nlution of the image as much as possible, and (2) the\ngrid should fit the aspect ratio of the input image.\nTo measure the resolution coherence and shape sim-\nilarity between the image and each grid, we calcu-\nlate the resolution-related and resolution-agnostic\ninsection over union Srr and Sra as follows:\nSrr(I, g) = IoU ((H, W), (nhHv, nwWv))\nSra(I, g) = IoU\n(\n(nwH\nW , nw), (nh, nw)\n) (1)\nwhere IoU denotes the insection over the union\nbetween two rectangles centered and aligned with\neach other. The matched grid is selected by maxi-\nmizing the matching score:\ng∗= arg max\ng\nSra(I, g) + Srr(I, g) (2)\nwhere g∗is the selected grid. Then, we resize the\ninput image to (nhHv, nwWv) and crop it to nh ·\nnw local images. To maintain the global structure\ninformation of the image, we also resize the input\n2843\nPre-defined Grids\n...Grid Matching\nGlobal\nresize\ncrop\nLocal Images\nFigure 3: The Shape-Adaptive Cropping Module.\nimage to (Hv, Wv) as a global image. All images\nare then passed on to the visual encoder and visual\nabstractor in parallel.\nThe visual encoder extracts visual feature\nV ∈RN×(H′·W′)×dv from the input images I ∈\nRN×H×W×3, where N = (nh ·nw) + 1, H′·W′\nand dv denote the number and dimension of the\nextracted visual features, respectively. The visual\nabstractor further summarizes visual information\nand obtains higher semantic visual representations\nV l ∈RN×Nq×dl in language feature space by sev-\neral learnable queries, where dl denotes the dimen-\nsion of language feature space and Nq denotes the\nnumber of learnable queries.\n3.2 Cropped Images Modeling with LLM\nMLLMs are mostly trained with a single image as\nthe input. Due to the cropping module, we need\nto input visual features from multiple images into\nthe language model. The 1-dimensional position\nembeddings of LLM can not reflect the spatial po-\nsition of each sub-image, which is critical to cor-\nrelate local images. Therefore, we incorporate a\n2-dimensional crop position encoding to help the\nlanguage model to understand the spatial relation-\nship between cropped images. Specifically, we\nassign a location index (i, j) for each cell of the\nselected grid and obtain their row embedding and\ncolumn embedding by two auxiliary embedding\nlayers as follows:\nerow\ni,j = Embeddingrow(i)\necolumn\ni,j = Embeddingcolumn(j)\nei,j = erow\ni,j + ecolumn\ni,j\n(3)\nwhere ei,j ∈RDl denotes the crop position embed-\nding of the cell (ci, cj). We add the embedding to\nthe visual feature of each cell in the language space\nvia broadcasting along the dimension of learnable\nqueries: ¯V l\ni,j = V l\ni,j + ei,j. We then reshape the vi-\nsual features into ¯Vl ∈R(N·Nq)×dl . The resulting\nspatial-aware visual features and word embeddings\nof the input sentences are concatenated at sequence\ndimension and sent to the large language model.\nIn order to enhance the language model’s ability\nto effectively model multiple images while keeping\nlow training costs, we freeze the origin language\nmodel and adopt the low-rank adaptation approach\n(LoRA) (Hu et al., 2022).\n4 Instruction Tuning\nFor developing a universal visually-situated lan-\nguage understanding model that could process vari-\nous types of images and perform different com-\nprehension tasks, we conduct low-cost instruc-\ntion tuning with a Multimodal Large Language\nModel. Without introducing any large-scale pre-\ntraining datasets, we directly ensemble multiple\ndownstream datasets and perform joint training.\nDifferent downstream tasks are all reorganized to\nthe unified instruction format (Dai et al., 2023).\nBesides, we design auxiliary text reading and key\npoints generation tasks to enhance text recognition\nand semantic understanding ability.\n4.1 Tuning Tasks\nUnified downstream task. Downstream tasks of\nVisuall-situated Language Understanding cover Vi-\nsual Question Answering, Information Extraction,\nNatural Language Inference, and Image Caption-\ning. For developing a universal model, we reor-\nganize all tasks into the instruction tuning format\n(Dai et al., 2023). Concretely, for the Visual Ques-\ntion Answering task, the question is directly used\nas the instruction: \"Human: {question} AI: {an-\nswer}\". For the Information Extraction task, each\ncategory and value pair is expressed with a prompt\nas \"Human: What is the value for the {category}?\nAI: {value}\". If some categories don’t exist in the\nimage, the value is ‘None’. In the raw annotation\nof the Natural Language Inference task, ‘1’ means\n‘Entailed’ and ‘0’ means ‘Refuted’. We reorga-\nnize the NLI task by constructing the instruction\n\"Human: {statement}, Yes or No? AI: {answer}\",\nwhere ‘Yes’ means ‘Entailed’. For the Image cap-\ntioning task, we refer to 11 prompts from LLaVa\n2844\n(Liu et al., 2023a) to instruct the model to briefly\ndescribe the image and randomly choose 1 prompt\nfor each caption, such as \"Human: Provide a brief\ndescription of the given image. AI: {caption}\".\nText Reading task. Text Recognition is a basic\nability for OCR-free Visuall-situated Language Un-\nderstanding. Therefore, we apply an auxiliary Text\nReading task to strengthen text recognition ability\nacross different domains. With the text and position\ninformation in the image, we organize the texts in\nthe common reading order: from top to down, from\nleft to right. Directly utilizing all texts as targets\n(Kim et al., 2022) will result in the model focusing\non generating the starting texts and neglecting oth-\ners to reduce the loss. Instead, we randomly choose\na split position p from {0, L\n6 , 2L\n6 , ...,5L\n6 }, where L\nis the text sequence length. The left part is used\nas the input and the right one is the target. p = 0\nmeans to generate all texts while other cases ask\nthe model to continue reading following the input\ntexts. Such a design could enforce the model to\nread different parts of texts with the context. Start-\ning texts always convey key information about the\nimage, such as the chart title. Therefore, we apply a\nbigger sample rate (0.5) for the ‘0’ position and 0.1\nfor other positions. To distinguish reading from the\nbeginning and continuing reading, we design two\ngroups of prompts and randomly choose 1 prompt\nfor each sample. For example, an instruction of\nreading from the beginning can be \"Human: Rec-\nognize text in the image. AI: {all texts}\" and an\ninstruction of continuing reading can be \"Human:\nThe words on this picture are {left texts}. Continue\nreading the text. AI: {right texts}\".\nKey Points Generation task. Large Language\nModels learn strong understanding ability from\nthe tough language modeling task. Therefore, for\nstronger vision-and-language semantic comprehen-\nsion ability, we propose to design an auxiliary Key\nPoints Generation task, which requires the model to\ngive some key points about the image. To support\nthis task, we collect QA pairs of each image and\nconvert them to declarative sentences with Vicuna\n(Vicuna, 2023). These declarative sentences are\nfinally regarded as key points about the image. We\nalso build a set of templates to instruct this task,\nsuch as \"Human: Identify some key points in this\npicture. AI: {key points}\".\nAll templates for Text Reading and Key Points\nGeneration tasks can be found in Appendix D.\n4.2 Instruction Data Resources\nDocument. DocVQA (Mathew et al., 2021) com-\nprises 50k question and answer(QA) paris on\n12k document images from UCSF Industry Doc-\numents Library. InfographicsVQA (InfoVQA)\n(Mathew et al., 2022) collects 5k diverse info-\ngraphics from the internet and annotates 30k QA\npairs. DeepForm∗1 (Svetlichnaya, 2020) and Kleis-\nter Charity (KLC) (Stanislawek et al., 2021) are\ntwo Information Extraction datasets. DeepForm∗\ncontains 1.1k documents related to election spend-\ning. 2.7k documents of KLC come from published\nreports of charity organizations.\nTable. WikiTableQuestions (WTQ∗) (Pasupat and\nLiang, 2015) comprises 2.1k table images from\nWikipedia and is annotated with 23k question and\nanswer pairs demanding comparison and arithmetic\noperations. TabFact∗(Chen et al., 2020) is a Nat-\nural Language Inference dataset, which contains\n112k ‘entailed’ or ‘refuted’ statements about 16k\nWikipedia tables.\nChart. ChartQA (Masry et al., 2022) collects\nvarious topics and types of charts from four\nsources: Statista (statista.com), The Pew research\n(pewresearch.org), OWID (ourworldindata.org)\nand OECD (oecd.org). It totally contains 21k chart\nimages and 32k QA pairs.\nNatural Images. TextVQA (Singh et al., 2019)\nfilters 28k natural images with texts from Open\nImages V3 (Krasin et al., 2017) and annotates 45k\nQA pairs. To support image captioning with read-\ning comprehension, TextCaps (Sidorov et al., 2020)\nfurther collects 145k captions based on TextVQA.\nWebPage Screenshot. VisualMRC (Tanaka et al.,\n2021) collects 5k full screenshots of webpages\nfrom 35 websites. There are 30k annotated QA\npairs where answers are expressed in fluent sen-\ntences (avg. 9.53 words) and much longer than the\nones of QA datasets mentioned above.\n5 Experiments\n5.1 Implementation Details\nWe conduct experiments on a recently proposed\nMLLM named mPLUG-Owl (Ye et al., 2023) with-\nout modifying its hyperparameters. The number\nof learnable queries of visual abstractor is 65. The\ndimension of hidden states dv and dl are 1024. For\nthe shape-adaptive cropping module, we set the\n1Superscript ∗means the reformulated or modified version\nin DUE-benchmark (Borchmann et al., 2021)\n2845\nTable 1: Comparison with ocr-free methods on various types of visually-situated language understanding tasks.\n‘TSFT’ means task-spcific fine-tuning on the downstream dataset. ‘underline’ means achieving 80% SOTA perfor-\nmance.\nModel Train TS Doc Info Deep KLC WTQ TabFact ChartQA TextVQA TextCaps Visual\nParam FT VQA VQA Form MRC\nDessurt 127M ✓ 63.2 - - - - - - - - -\nDonut 176M ✓ 67.5 11.6 61.6 30.0 18.8 54.6 41.8 43.5 74.4 93.91\nPix2Structbase 282M ✓ 72.1 38.2 - - - - 56.0 - 88.0 -\nPix2Structlarge 1.3B ✓ 76.6 40.0 - - - - 58.6 - 95.5 -\nUReader 86M × 65.4 42.2 49.5 32.8 29.4 67.6 59.3 57.6 118.4 221.7\nmaximum number of cells Nc to 20 by default.\nDuring instruction tuning, the maximum sequence\nlength is limited to 2048, and Hv, Wv are set to\n224 to match the pretrained resolution of the vi-\nsual encoder. For LoRA, we set the rank r = 8.\nThe learning rate schedule uses a linear warmup\nof 36 steps to 1e−4, followed by cosine decay to\n0. The batch size is set to 256. For better conver-\ngence of each dataset, DocVQA is up-sampled 3\ntimes, InfoVQA, WTQ, DeepForm, and KLC are\nup-sampled 2 times. The instruction tuning pro-\ncess takes 16 A100 days for 20k training steps (10\nepochs).\n5.2 Evaluation\nWe use official training splits as tuning data and\nevaluate models on test splits. Following previous\nworks (Borchmann et al., 2021; Lee et al., 2022),\nDocVQA and InfoVQA are evaluated by ANLS\n(Biten et al., 2019), DeepForm and KLC are eval-\nuated by F1 score. WTQ, TabFact and TextVQA\nare evaluated by accuracy. ChartQA is evaluated\nwith the relaxed accuracy (Methani et al., 2020).\nTextCaps and VisualMRC are measured by CIDEr\n(Vedantam et al., 2015). Evaluation of TextVQA\nand TextCaps are performed with the official chal-\nlenge website.\n5.3 Main Results\nWe first compare UReader with state-of-the-art ocr-\nfree models on 10 datasets. For fair and consis-\ntent comparison across all datasets, we finetune the\nstrong and accessible baseline Dount on unreported\ndatasets. As shown in Table 1, UReader achieves\nstate-of-the-art performance in 8 tasks across 5\ndomains, covering Visual Question Answering, In-\nformation Extraction, Natural Language Inference\nand Image Captioning tasks. With much fewer\ntrainable parameters (86M vs 1.3B) and without\na specific finetuning stage, UReader outperforms\nthe strong pretriaining model Pix2Struct large in\nInfoVQA, ChartQA, and TextCaps. Considering\nthat Pix2Structlarge is trained more than 170k steps\nwith a batch size of 1024 on 128 TPUs, this val-\nidates that with the help of open-domain Multi-\nmodal Large Language Models, learning costs for\nuniversal visually-situated language understanding\ncan be greatly reduced. More detailed analysis can\nbe found in Appendix B.\n5.4 Ablation Study\nWe perform comprehensive ablation experiments\nto validate the contribution of two auxiliary tasks,\ntrainable architectures, cross-domain joint training\nand the design of shape-adaptive cropping module.\nAuxiliary Tasks. As shown in Table 2, dropping\nthe Key Points Generation task (r10 vs r2) causes a\nperformance decrease on all domains of datasets,\ndemonstrating that this task helps the model bet-\nter understand the vision-and-language semantic.\nFurther removing the Text Reading task (r2 vs r1)\ncauses more significant performance degradation,\nwhich validates the importance of enhancing text\nrecognition ability across different domains.\nTrainable Architectures. Both the visual ab-\nstractor and LoRA in LLM are finetuned in URe-\nader (r10). Freezing either the visual abstractor\n(r3) or LoRA (r4) causes performance decrease,\nwhich demonstrates that both the vision and lan-\nguage parts should be finetuned for adjusting to\nVisually-situated Language Understanding.\nCross-domain Joint Training. After removing\n4 document datasets from the training data, URe-\nader achieves worse performance (r10 vs r5) on\nthe table, natural image, and webpage domains,\nvalidating that images of different domains share\nsome common characteristics and cross-domain\njoint training improves the universal performance.\nBesides, although trained without document data,\n2846\nTable 2: Ablation study about auxiliary training tasks, trainable model architectures, cross-domain joint training\nand shape-adaptive cropping. ‘KPG’ and ‘TR’ refer to Key Points Generation and Text Reading tasks, respectively.\n‘Abs’ refers to the visual abstractor. ‘Doc Data’ means using 4 document datasets as training data or not. ‘Global’\nmeans using a resized global image as input. ‘Crops’ refers to Nc, the maximum number of local images after\ncropping. ‘CropPos’ refers to the crop position embedding.\nTasks Trainable Doc Shape-adaptive Cropping DocVQA WTQ ChartQA TextVQA Visual\nKPG TR Abs LoRA Data Global CropPos Crops MRC\nr1 ✓ ✓ ✓ ✓ ✓ 20 56.7 22.9 56.7 54.3 205.0\nr2 ✓ ✓ ✓ ✓ ✓ ✓ 20 64.3 28.1 58.6 56.0 213.5\nr3 ✓ ✓ ✓ ✓ ✓ ✓ 20 52.4 20.5 43.5 54.9 194.9\nr4 ✓ ✓ ✓ ✓ ✓ ✓ 20 59.5 23.5 58.5 53.3 177.0\nr5 ✓ ✓ ✓ ✓ ✓ ✓ 20 46.2 27.4 59.8 54.0 185.6\nr6 ✓ ✓ ✓ ✓ ✓ ✓ - 22.0 13.4 24.2 34.4 157.4\nr7 ✓ ✓ ✓ ✓ ✓ ✓ 9 58.0 24.7 58.9 55.5 215.3\nr8 ✓ ✓ ✓ ✓ ✓ ✓ 20 64.1 27.6 60.7 56.5 210.7\nr9 ✓ ✓ ✓ ✓ ✓ ✓ 20 62.8 26.7 58.7 55.4 181.1\nr10 ✓ ✓ ✓ ✓ ✓ ✓ ✓ 20 65.4 29.4 59.3 57.6 221.7\nFigure 4: Visualization of the frequency of selected\ngrid with shape-adaptive cropping module. The cell at\nrow i and column j denotes the selected frequency of\ngrid (nh = i, nw = j). Deeper colors represent higher\nselection frequencies.\nour model achieves a 46.2 score on the DocVQA\ndataset, showing the potential out-of-domain un-\nderstanding ability of our training paradigm.\nShape-adaptive Cropping. The r6 in Table 2\nrepresents directly tuning the mPLUG-Owl with-\nout any model revisions. With the shape-adaptive\ncropping, UReader achieves significantly better per-\nformance (r7 vs r6), showing that our cropping\nmodule is indispensable to leverage pretrained low-\nresolution vision encoder for universal visually-\nsituated language understanding. Besides, increas-\ning the cropping numbers (r8 vs r7) improves the\nmodel’s performance. Due to the resolution of\neach local image being constant (224x224), more\ncrops mean higher overall resolution and therefore\nachieves better performance. Furthermore, adding\na resized global image bring a slight improvement\nin most datasets (r10 vs r8), validating that a com-\nplete image could alleviate possible information\nloss due to image cropping. Finally, dropping crop\nposition encoding also hurts the model’s perfor-\nmance (r10 vs r9), proving the effectiveness of crop\nposition encoding for correlating local images.\nFor alleviating the distortion problem due to\nresizing, we propose to crop images according\nto their raw aspect ratio. Figure 4 shows the\nfrequency distribution of grids selected by our\nshape-adaptive cropping module on DocVQA, Vi-\nsualMRC and WikiTableQuestions (the distribution\non more datasets can be found in the Appendix\nA). For aesthetic purposes, we present the distribu-\ntion with Nc = 9. Apparently, different domains\nof images have different shape distributions. For\nmost document images in DocVQA, their height is\ngreater than the width, while table images are the\nopposite. As webpages are scrollable, their screen-\nshots are always in the form of a long rectangular\nshape. With the shape-adaptive cropping design,\nour model can easily adapt to various image shapes\nwithout domain-specific fine-tuning.\nText distortion may pose little influence on vi-\nsual question answering because they are always\nabout partial text information. But it is harmful\nfor reading texts in the image because every text\nmatters. For quantitative analysis of the influence\nof shape-adaptive design, we directly evaluate the\nperformance of reading all texts. We choose the\nBleu (Papineni et al., 2002) as the metric because\nit directly measures the n-gram overlap between\nthe ground-truth and predicted text sequence. The\nevaluation set is built by combining 100 randomly-\nselected test images from each dataset. As shown\nin Table 3, compared with cropping all images with\na fixed grid, UReader could better recognize texts\n2847\nHuman: On which date was the memorandamsend ? UReader: February 27, 1992 (√)\nHuman: How many pieces of content is shared each day by Facebook users?UReader: 2.5 BiLLION(√)Human: Who has more active users, Pinterest or Twitter? UReader: Twitter (√)\n(b)\nHuman: List a handful of essential elements in this visual.UReader: The country with the highest wine consumption per person in 1979 was Iceland. (√)The country with the lowest wine consumption per person in 1979 was Madagascar (x). \n(f)\n(a)\nHuman: What was the first movie that won the filmfareaward for best actress? Ureader: MogginaManasu(√)Human: What is the total number of films with the language of kannada listed?UReader: 15 (√)\n(c)\nHuman: As an intern who will you work closely with?UReader: You will work closely with the FSF staff. (√)\n(e)\nHuman: what does the image read? UReader:BIS SIBELIUS SYMPHONIES Nos 2&5 MINNESOTA ORCHESTRA VANSKA COSMO(√)(d)\nFigure 5: Qualitative results of UReader. Crucial regions are enlarged for clearer visualization.\nTable 3: The Text Reading performance of UReader un-\nder the condition of Nc = 9. ‘w/o adapt means remov-\ning the shape-adaptive design and cropping the image\nwith a fixed grid 3 ×3.\nModel Bleu1 Bleu2 Bleu3 Bleu4\nUReader w/o adapt 21.4 15.4 12.0 9.7\nUReader 24.9 18.1 14.3 11.7\nin the image due to our shape-adaptive design that\nalleviates the text distortion problem.\n5.5 Qualitative Results\nFigure 5 show some qualitative results produced\nby our UReader on different types of images. URe-\nader could not only extract information from the\ndocument (case a), but also understand different\ninstructions and provide corresponding answers by\nattending to different regions (case b). Table un-\nderstanding always involves layout comprehension\nand statistics. As shown in case c, given a table im-\nage, UReader could well relate different columns to\nanswer the ‘first movie’ and perform simple statis-\ntics about the ‘total number’. As for images with\nmultiple paragraphs of text, e.g. webpage screen-\nshot in case e, UReader could also locate the rele-\nvant paragraph, understand the texts and answer the\nquestion accurately. Case d shows the text reading\nperformance. With the help of the Text Reading\ntask, UReader is able to read texts from top left to\nbottom right. But, due to the language decoding\nmanner, when given an image with rich texts, such\nas a page of a book, the model often reads the be-\nginning texts and then continues writing without\nwatching the image. More qualitative results can\nbe found in Appendix C. Finally, as shown in case\nf, UReader is able to list some key points about the\nchart by combining the title and line information.\nListing key points in this work is just a superficial\nattempt at open-ended generation, and its perfor-\nmance is far from promising, e.g., UReader makes\na mistake about the lowest line. More effort is\nneeded towards a comprehensive understanding of\nimages with rich text.\n6 Conclusion\nWe first propose to leverage existing Multimodal\nLarge Language Models for universal ocr-free\nvisually-situated language understanding through\nlow-cost instruction tuning. All downstream tasks\nare reorganized into a unified instruction-tuning for-\nmat. Besides, we design the Text Reading task and\nKey Points Generation task to enhance text recogni-\ntion and vision-and-language semantic comprehen-\nsion abilities. To utilize the pre-trained vision en-\ncoder for processing high-resolution images, we de-\nsign a shape-adaptive cropping module, which cuts\nthe image into multiple local images considering its\nraw aspect ratio and resolution. UReader achieve\nstate-of-the-art ocr-free performance in 8 out of 10\ndatasets, ranging from documents, tables, charts,\nand natural images to webpage screenshots.\n2848\nLimitations\nOur experiments validate that UReader is able\nto correlate local images after cropping a high-\nresolution image. However, UReader struggles to\nunderstand multi-page documents (e.g. books and\npapers) due to lacking ability to correlate different\npages and the limited sequence length of the de-\ncoder. Besides, UReader feeds an equal number\nof features for each local image into the language\ndecoder. But, not all local images contain rich vi-\nsion or text information. In the future, we will\nexplore a more efficient way to encode different\ncrops. Furthermore, the open-ended generation\nabout Visually-situated Language understanding is\nfar from well studied. We try developing key points\ngeneration ability in this work but more difficult\ngeneration tasks are not currently considered, such\nas giving the chain-of-the-thought of the answer.\nHow to simulate such abilities through instruction\ntuning is a topic worth studying. Finally, the Text\nReading task helps the model recognize texts, but\nthe text reading performance with the LLM as the\ndecoder is far from satisfactory due to the halluci-\nnation problem. Instructing the LLM to read texts\nstrictly according to images is a challenging topic.\nEthics Statement\nOur UReader relies on multi-modal large language\nmodels that are trained on large-scale image and\ntext data from the web and therefore may be subject\nto issues such as toxic language and bias (Bender\net al., 2021). However, our model is further fine-\ntuned on publicly available datasets and is used\nspecifically in the domain of visually-situated lan-\nguage understanding, where these issues have min-\nimal impact.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAli Furkan Biten, Rubèn Tito, Andrés Mafla,\nLluís Gómez i Bigorda, Marçal Rusiñol, C. V . Jawa-\nhar, Ernest Valveny, and Dimosthenis Karatzas. 2019.\nScene text visual question answering. InICCV, pages\n4290–4300. IEEE.\nLukasz Borchmann, Michal Pietruszka, Tomasz Stanis-\nlawek, Dawid Jurkiewicz, Michal Turski, Karolina\nSzyndler, and Filip Gralinski. 2021. DUE: end-to-\nend document understanding benchmark. In NeurIPS\nDatasets and Benchmarks.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact : A large-scale\ndataset for table-based fact verification. In Inter-\nnational Conference on Learning Representations\n(ICLR), Addis Ababa, Ethiopia.\nXingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang\nZhang, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021.\nWebsrc: A dataset for web-based structural reading\ncomprehension. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4173–4185.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven C. H. Hoi.\n2023. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning. CoRR,\nabs/2305.06500.\nBrian L. Davis, Bryan S. Morse, Brian L. Price, Chris\nTensmeyer, Curtis Wigington, and Vlad I. Morariu.\n2022. End-to-end document recognition and under-\nstanding with dessurt. In ECCV Workshops (4), vol-\nume 13804 of Lecture Notes in Computer Science,\npages 280–296. Springer.\nHao Feng, Zijian Wang, Ji Tang, Jinghui Lu, Wen gang\nZhou, Houqiang Li, and Can Huang. 2023. Unidoc:\nA universal large multimodal model for simultaneous\ntext detection, recognition, spotting and understand-\ning. ArXiv, abs/2308.11592.\nAnwen Hu, Shizhe Chen, and Qin Jin. 2021. Question-\ncontrolled text-aware image captioning. In ACM\nMultimedia, pages 3097–3105. ACM.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Barun Patra, Qiang\nLiu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\nVishrav Chaudhary, Subhojit Som, Xia Song, and\nFuru Wei. 2023. Language is not all you need:\nAligning perception with language models. CoRR,\nabs/2302.14045.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\nFuru Wei. 2022. Layoutlmv3: Pre-training for doc-\nument AI with unified text and image masking. In\nACM Multimedia, pages 4083–4091. ACM.\nKushal Kafle, Brian L. Price, Scott Cohen, and Christo-\npher Kanan. 2018. DVQA: understanding data visu-\nalizations via question answering. In CVPR, pages\n5648–5656. Computer Vision Foundation / IEEE\nComputer Society.\n2849\nSamira Ebrahimi Kahou, Vincent Michalski, Adam\nAtkinson, Ákos Kádár, Adam Trischler, and Yoshua\nBengio. 2018. Figureqa: An annotated figure dataset\nfor visual reasoning. In ICLR (Workshop). OpenRe-\nview.net.\nGeewook Kim, Teakgyu Hong, Moonbin Yim,\nJeongYeon Nam, Jinyoung Park, Jinyeong Yim, Won-\nseok Hwang, Sangdoo Yun, Dongyoon Han, and Se-\nunghyun Park. 2022. Ocr-free document understand-\ning transformer. In ECCV (28), volume 13688 of\nLecture Notes in Computer Science, pages 498–517.\nSpringer.\nIvan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari,\nSami Abu-El-Haija, Alina Kuznetsova, Hassan Rom,\nJasper Uijlings, Stefan Popov, Shahab Kamali, Mat-\nteo Malloci, Jordi Pont-Tuset, Andreas Veit, Serge\nBelongie, Victor Gomes, Abhinav Gupta, Chen Sun,\nGal Chechik, David Cai, Zheyun Feng, Dhyanesh\nNarayanan, and Kevin Murphy. 2017. Openimages:\nA public dataset for large-scale multi-label and multi-\nclass image classification. Dataset available from\nhttps://storage.googleapis.com/openimages/web/index.html.\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,\nFangyu Liu, Julian Eisenschlos, Urvashi Khandel-\nwal, Peter Shaw, Ming-Wei Chang, and Kristina\nToutanova. 2022. Pix2struct: Screenshot parsing as\npretraining for visual language understanding. CoRR,\nabs/2210.03347.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. CoRR, abs/2301.12597.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023a. Visual instruction tuning. CoRR,\nabs/2304.08485.\nYuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu,\nMingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui\nChen, Chunyuan Li, Lianwen Jin, et al. 2023b. On\nthe hidden mystery of ocr in large multimodal models.\narXiv preprint arXiv:2305.07895.\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R.\nJoty, and Enamul Hoque. 2022. Chartqa: A bench-\nmark for question answering about charts with visual\nand logical reasoning. In ACL (Findings), pages\n2263–2279. Association for Computational Linguis-\ntics.\nMinesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis\nKaratzas, Ernest Valveny, and C. V . Jawahar. 2022.\nInfographicvqa. In WACV, pages 2582–2591. IEEE.\nMinesh Mathew, Dimosthenis Karatzas, and C. V . Jawa-\nhar. 2021. Docvqa: A dataset for VQA on document\nimages. In WACV, pages 2199–2208. IEEE.\nNitesh Methani, Pritha Ganguly, Mitesh M. Khapra,\nand Pratyush Kumar. 2020. Plotqa: Reasoning over\nscientific plots. In WACV, pages 1516–1525. IEEE.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh,\nand Anirban Chakraborty. 2019. OCR-VQA: visual\nquestion answering by reading text in images. In\nICDAR, pages 947–952. IEEE.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nACL (1), pages 1470–1480. The Association for Com-\nputer Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In ICML, volume 139 of Proceedings\nof Machine Learning Research, pages 8748–8763.\nPMLR.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. 2020. Textcaps: A dataset for\nimage captioning with reading comprehension. In\nECCV (2), volume 12347 of Lecture Notes in Com-\nputer Science, pages 742–758. Springer.\nAmanpreet Singh, Vivek Natarajan, Meet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. 2019. Towards VQA models\nthat can read. In CVPR, pages 8317–8326. Computer\nVision Foundation / IEEE.\nTomasz Stanislawek, Filip Gralinski, Anna Wróblewska,\nDawid Lipinski, Agnieszka Kaliska, Paulina Rosal-\nska, Bartosz Topolski, and Przemyslaw Biecek. 2021.\nKleister: Key information extraction datasets involv-\ning long documents with complex layouts. In ICDAR\n(1), volume 12821 of Lecture Notes in Computer\nScience, pages 564–579. Springer.\nS Svetlichnaya. 2020. Deepform: Understand struc-\ntured documents at scale.\nRyota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021.\nVisualmrc: Machine reading comprehension on doc-\nument images. In AAAI, pages 13878–13888. AAAI\nPress.\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,\nYang Liu, Chenguang Zhu, Michael Zeng, Cha\nZhang, and Mohit Bansal. 2023. Unifying vision,\ntext, and layout for universal document processing.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 19254–\n19264.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\n2850\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In CVPR, pages 4566–4575.\nIEEE Computer Society.\nVicuna. 2023. Vicuna: An open chatbot impress-\ning gpt-4. https://github.com/lm-sys/\nFastChat.\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\nWei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florên-\ncio, Cha Zhang, Wanxiang Che, Min Zhang, and\nLidong Zhou. 2021. Layoutlmv2: Multi-modal pre-\ntraining for visually-rich document understanding.\nIn ACL/IJCNLP (1), pages 2579–2591. Association\nfor Computational Linguistics.\nZhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin,\nDinei Florêncio, Lijuan Wang, Cha Zhang, Lei\nZhang, and Jiebo Luo. 2021. TAP: text-aware pre-\ntraining for text-vqa and text-caption. In CVPR,\npages 8751–8761. Computer Vision Foundation /\nIEEE.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming\nYan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong\nXu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang,\nand Fei Huang. 2023. mplug-owl: Modularization\nempowers large language models with multimodality.\nCoRR, abs/2304.14178.\nLiang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin\nJin. 2023. MPMQA: multimodal question answering\non product manuals. CoRR, abs/2304.09660.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models.\n2851\nA Grid Distribution on Downstream\nDatasets\nWe visualize the frequency distribution of grids\nselected by our shape-adaptive cropping module\non all ten datasets in Figure 6. The wide variety of\nimage shapes in downstream tasks highlights the\ncrucial role of the shape-adaptive cropping module.\nB Detailed Analysis on Performance\nB.1 Underperforms Ocr-Free Baselines on\nDocVQA and DeepForm\nIt can be seen that UReaderunderperforms ocr-free\nbaselines on DocVQA and DeepForm. There are\ntwo main factors: (1) Donut performs the pre-\ntraining on large-scale document dataset IIT-CDIP\n(11M document images), which is the same domain\nas DocVQA and DeepForm. But UReader does no\nhave a pretraining process and is just instruction\nfinetuned on ensembled datasets (less than 0.5M\nassorted images). Training with more document\nimages brings better performance. (2) The pretrain-\ning task of Pix2struct is to predict the HTML dom\ntree of a masked web screenshot, which requires\nthe model to fully understand the layout informa-\ntion of the image. But UReader is trained to read\ntexts from top to down, from left to right, which\nrequires a weaker layout understanding ability. The\npretraining on layout understanding also leads to\nimproved performance on DocVQA.\nThe conclusion can also be substantiated by the\nobservations on the other two datasets (i.e., In-\nfoVQA and KLC) included in the document do-\nmain as previous work (Tang et al., 2023). For the\nInfoVQA dataset, the image is poster style and the\nlayout is not as important as DocVQA and Deep-\nForm but the relationship between text and vision\nobjects matters more, like natural image and chart\nimage. As for the KLC dataset, ocr-free models\nare only fed with the first page (always the cover\nof a report) , where the layout is much simpler\nthan DocVQA and DeepForm. Therefore, URead-\nercan outperform baselines on these two document\ndatasets.\nIn summary, compared with ocr-free model\nDonut and Pix2Struct, due to the pretrianing of\nMLMM on open-domain datasets, UReaderis bet-\nter at understanding cross-modality relationships\nin the image but weaker at comprehending text\nlayout information without large-scale document\npretraining and specific layout understanding tasks.\nB.2 Compared with Pipeline Methods\nWe list the performance of state-of-the-art pipeline\nmodels in Table 4. We can summarize from\nthe results that there are two distinct aspects.\nFirstly, our model achieves comparable or slightly\nworse results compared to the pipeline methods on\nTextVQA, ChartQA, InfoVQA, TextCaps and Tab-\nFact. Secondly, there is a obvious gap between our\nmodel and pipeline methods on DocVQA, Deep-\nForm, KLC, WTQ and VisualMRC.\nFor the first aspect, there are two reasons for\nthe similarity performance: (1) Modeling the di-\nverse relationship between visual objects and text\npresents challenges for both pipeline-based meth-\nods and OCR-free methods. TextVQA, TextCaps\nand InfoVQA requires the relation understanding\nbetween text and visual objects (i.e. logos, icons\nand common objects). ChartQA asks for trend com-\nprehension of lines. Understanding such complex\ncross-modality relation is challenging for both ocr-\nfree and pipeline methods. (2) The simplicity of\ntask formats can reduces performance gaps. Tab-\nfact is a simply binary classification task resulting\nthe small performance gap.\nFor this second aspect, the main performance\ngap appears in three categories of datasets: docu-\nment, table, and webpage screenshot. The reasons\nare two folds: (1) The gap in terms of text recogni-\ntion and layout extraction. In document, table and\nwebsite, text is the dominant information source\nand the layout(e.g. row and column layout in table)\nis relatively uniformer than the chart and natural\nimages. Therefore, with pre-extracted texts and\nlayout information, it is more easy to understand\nthe image. But for OCR-Free models, such as our\nUReader and Donut, it’s still challenging to fully\nrecognize all texts. (2) The gap in terms of mod-\neling capacity on multi-page document input. for\nmultiple-page document datasets KLC (98% > 4\npages) and DeepForm (75% > 1 pages), OCR-Free\nmodels only input the first page and lose much\ninformation.\nB.3 Zero-shot Performance\nWe test the zero-shot performance of UReader on\nunseen dataset OCR-VQA. With the same evalua-\ntion metrics, UReader outperforms mPLUG-Owl\n(41.1 vs 28.6) and a recent work UniDoc (Feng\net al., 2023) (41.1 vs 34.5) with the training of lay-\nout prediction. The results show that the zero-shot\nperformance of our method on unseen domains is\n2852\nFigure 6: Visualization of the frequency of selected grid with the shape-adaptive cropping module on 10 downstream\ndatasets.\nDocVQA InfoVQA DeepForm KLC WTQ TabFact ChartQA TextVQA TextCaps VisualMRC\nOCR-Pipline 84.7(UDOP) 47.4(UDOP) 85.5(UDOP) 82.8(UDOP) 47.2(UDOP) 72.9(UDOP) 70.5(DePlot) 56.3(PreSTU) 139.1 (PreSTU) 364.2(LayoutT5)UReader 65.4 42.2 49.5 32.8 29.4 67.6 59.3 57.6 118.4 221.7\nTable 4: Performance comparison between UReaderand state-of-the-art pipeline methods.\nacceptable.\nC More Qualitative Results\nC.1 Downstream Results\nMore qualitative results on natural images, charts,\ntables, documents and webpage screenshots are\nshown in Figure 7-11.\nFigure 11 show a sample of Text Reading\nand Visual Question Answering about a webpage\nscreenshot from VisualMRC. As mentioned in Sec-\ntion 5.5, when given an instruction about reading\nall texts in the image, UReader can read the be-\nginning texts but sometimes is easy to continue to\ngenerate vision-irrelevant texts. With appropriate\ninstructions, UReader could indeed recognize texts\nin other regions, such as ‘exercise increases cellular\nrecycling’. Therefore, the hallucination problem\nduring text reading is not because UReader can-\nnot recognize texts, but the generating manner of\nLLM decoder. When beginning texts are read from\nthe image, the decoder may generate the following\ntexts according to the closer text context rather than\nthe image.\nC.2 Open-domain Results\nWe present open-domain examples in Figure 12.\nWe use randomly collected images and freely ask\nquestions to the model based on the content of\nthese images. The original mPLUG-Owl is used\nfor comparison.\nIn Figure 12 (a), UReader is able to accurately\nrecognize and answer questions about the small\ntext in natural images (\"Name of passenger\" and\n\"MORRIS/KARLA\"). In contrast, mPLUG-Owl\ndoes not respond with the name in the first round\nand gives an incorrect answer even with a prompt\nin the second round.\nIn Figure 12 (b), we raise a query consisting of\ntwo cascaded questions, which requires the model\nto simultaneously understand the spatial position\nof the non-textual objects referred to by the query\nand locate the corresponding fields. It can be seen\nthat the UReader completes this task well, while\nmPLUG-Owl answers incorrectly in both object\nrecognition and price extraction.\nIn Figure 12 (c), we conduct multi-turn conver-\nsions with UReader on a screenshot. The questions\nincluded references to the history of the conversa-\ntion. Some questions also require a certain amount\nof common sense. For example, the time of account\ncreation is equivalent to the time of joining Twitter,\nand the inactive state of the Follow button indicates\nthat the user has not followed Iron Musk. UReader\nanswers these questions well. mPLUG-Owl can\ncorrectly recognize the Iron Mush, but is it prone\nto generating content that is unrelated to the image,\n2853\nHuman: Recognizetext in the picture.  UReader: WILLIAM G. MATHER   Human: Is this ship used for a business?UReader: yesHuman: Who's name is on the ship?  UReader: williamg. matherHuman: Provide a brief description of the given image.UReader: A large ship named William G. Mathers is docked at a pier.\nHuman: What words are in the image? UReader: graphics World Day 2010 S#wgd2010Human: What day is it?UReader: world graphics dayHuman: What year is shown?  UReader: 2010Human: Render a clear and concise summary of the photo.UReader: A yellow and black pencil sits on a wooden table with the date 2010 on it.\n(a)\n(b)\nFigure 7: Text Reading, Visual Question Answering and Image Captioning performance of UReader on natural\nimages from TextVQA. Correct and wrong answers are colored green and red, respectively.\nHuman: List the words in the image.UReader: Global views of Trump's characteristics % who say they think of PresidentDonald Trump as.  Arrogant 75 Intolerant 65 Dangerous 62 A strong leader 55 Charismatic 39 Well-qualified to be president 26 Caring about ordinary people 23.Human: What percent who think of President Donald Trump as Dangerous?UReader: 62.Human: Is the sum of the percentsof Charismatic and Well-qualified to be president more than A strong leader?UReader: Yes.\nHuman: What dose the picture say?UReader:  Final to primary energy ratio (%), 1990 to 2010 Albania 80.0 60.0 Sudan 40.0 20.0 0.0 1990.0 1995.0 2000.0 2005.0 2010.0Human: Which country represents in red line?UReader: Benin.Human: Which country data below 80% for the last five years? UReader: Sudan.\n(a)\n(b)\nFigure 8: Text Reading and Visual Question Answering performance of UReader on charts from ChartQA. Correct\nand wrong answers are colored green and red, respectively.\nleading to some erroneous statements.\nIn Figure 12 (d), we ask the UReader about the\nprice and its components based on an image consist-\ning of multiple forms. Although UReader wrongly\nincludes the header in the answer and does not list\nthe prices for each component, we notice that it\nproactively filters out the components with a price\nof $0, making the answer more consistent with\nthe user’s intention. It indicates that UReader can\nfind the form related to the question and compre-\n2854\nHuman: Highlight a few significant elements in this photo. UReader: The constructor that had the most points is Ferrari. The constructor that had the most retirements was Williams-McLaren. The constructor that had the most laps completed was Ferrari. The constructor that had the most laps completed was Ferrari. The constructor that had the most laps completed was Ferrari. Human: How long did it take michaelschumacherto finish the race?UReader: 1:34:45.026\nHuman: What was edelweiss's biggest hit?UReader: “Bring Me Edelweiss”Human: how many years after “ican't get no... Edelweiss” did the single “bring me edelweiss” (remix) appear?UReader: 12\n(a)\n(b)\nFigure 9: Key Points Generation and Visual Question Answering performance of UReader on tables from Wik-\niTableQuestions. Correct, wrong and repetitive answers are colored green, red and gray, respectively.\nHuman: Identify some key points in this picture. UReader: The document is dated March 22, 1991. The document is from the United States.Human: What is the name of the Dealer ? UReader: A. C. MonkHuman: How many grass/straw pieces of matter is found in the core samples ? UReader: 2Human: How many lint/string pieces of matter is found in the core samples ? UReader: 2(a)\n(b)\nHuman: Parse text from imagery.UReader: A BRIEF HISTORY OF SOCIAL MEDIA PLATFORMS 1978 The first public Bulletin Board System went online. 1979 UseNet created by Duke University students Tom Truscott and Jim Ellis.2. 1982 Six Degrees, the first social media website, is launched.2002 LinkedIn is created by Reid Hoffman. 2003 MySpaceis launched by several eUniverseemployees. 2004 f Mark Zuckerberg officially launches Twitter. 2006 Twitter is launched by Jack Dorsey used by employeesat Oden. 20072008The photo-sharing app Instagram is launched. Tumblr is launched by David Karp. 2010 The blogging site Tumblr is launched by David Karp. 9. CREATED BY: Carly Racklin/ Ghost Tweeting ghostweetingHuman: Which social media site was launched in 2010?UReader: Instagram.\nFigure 10: Text Reading, Key Points Generation and Visual Question Answering performance of UReader on\ndocuments from DocVQA and InfoVQA. Correct and wrong answers are colored green and red, respectively.\nhensively understand the meaning of each field in\nthe form. In contrast, mPLUG-Owl generates re-\nsponses that are full of illusions due to the loss of\ntextual information in the image.\n2855\nHuman: Detect words in the photograph: UReader:  ARTICLES WITH THE TOPIC: AUTISM Could molecular handcuffs lower the protein that causes autism? By Timothy D. Rayne | May 20, 2012 Huntington’s diseaseis a neurodegenerative disorder that causes a progressive decline in cognitive unction and motor control. It is caused by a mutation in a gene that codesfor a protein called huntingtin……Human: What is suggested that molecular handcuffs could lower?UReader: The protein that causes Huntington's diseaseHuman: What does exercise increase?UReader: It increases cellular recycling.\nFigure 11: Text Reading and Visual Question Answering performance of UReader on a webpage screenshot from\nVisualMRC. Correct and wrong answers are colored green and red, respectively.\nThese results reveal that UReader maintains\nsome interactive ability of MLMM in the open\ndomain and shows stronger visually-situated lan-\nguage understanding ability.\nD Instruction Templates\nThe instruction templates of the auxiliary Text\nReading and Key Points Generation tasks are\nshown in Table 5.\n2856\nUReaderHuman: Who is he and how many people he is followingUReader: Elon Musk, 334Human: When did he create his twitter account?UReader: June 2009Human: Have I followed him?UReader: NomPLUG-OwlHuman:Who is he and how many people he is followingmPLUG-Owl: The man in the image is Elon Musk, a famous entrepreneur, engineer, and inventor. He is following 10 people on Twitter, including himself.Human: When did he create his twitter account?mPLUG-Owl: Elon Musk created his Twitter account in June 2006, making him one of the earliest adopters of the social media platform.Human: Have I followed him?mPLUG-Owl: Yes, you have followed Elon Musk on Twitter.\nUReaderHuman: Who is the passenger?UReader: morris/karlamPLUG-OwlHuman: Who is the passenger?mPLUG-Owl: The passenger in the image is a woman.Human: What’s her name?mPLUG-Owl: The name of the passenger is \"Mary.\"\nUReaderHuman: What is the fruit on the top left and what is the price of it?UReader: melon, $1.90mPLUG-OwlHuman: What is the fruit on the top left and what is the price of it?mPLUG-Owl: The fruit on the top left is a lemon, and the price of it is $1.\nUReaderHuman: How much should I pay?UReader: $136.38Human: What are the components of the payment?UReader: Service Name,Amazon CloudFront, AWS Data Transfer Service, Amazon Simple Storage Service, Amazon Simple DB, Amazon Elastic Compute CloudmPLUG-OwlHuman: How much should I pay?mPLUG-Owl: Based on the image, the total amount you should pay for the Amazon Web Services (AWS) bill is $1,000.Human: What are the components of the payment?mPLUG-Owl: The payment consists of a check and a credit card. The check is for $1,000, and the credit card is for $1,000.\n(a) Text understanding in natural image scenario(b) Spatial-aware comprehension involving non-textual objects.\n(c) Multi-turn information extraction\n(d) Multi-turn form understanding\nFigure 12: Comparsion with mPLUG-Owl on open-domain Visually-situated Language Understanding. The key\nwords in the answers and the key regions in the images are annotated with the same color. The incorrect response of\nUReader is colored red.\n2857\nTable 5: Instructuion templates used for text reading from the beginning, continue reading and key points generation\ntasks. The complete instruction for continuing reading is a random combination of a prompt from part A and another\none from part B.\nTask Part Instruction Template\ntext reading from the beginning -\n<Image>Human: what words are in the image? AI: {all texts}.\n<Image>Human: what texts are in the picture? AI: {all texts}.\n<Image>Human: what does the image read? AI: {all texts}.\n<Image>Human: what does the picture say? AI: {all texts}.\n<Image>Human: what is written in the image? AI: {all texts}.\n<Image>Human: list the words in the image. AI: {all texts}.\n<Image>Human: list the texts in the picture. AI: {all texts}.\n<Image>Human: Recognize text in the image. AI: {all texts}.\n<Image>Human: Identify text in the picture. AI: {all texts}.\n<Image>Human: Deciphering written content in the photo. AI: {all texts}.\n<Image>Human: Extract words from the graphic. AI: {all texts}.\n<Image>Human: Parse text from imagery. AI: {all texts}.\n<Image>Human: Read written language in the visuals. AI: {all texts}.\n<Image>Human: Decode text from the snapshot. AI: {all texts}.\n<Image>Human: Translate text in the picture. AI: {all texts}.\n<Image>Human: Retrieve written information from the image. AI: {all texts}.\n<Image>Human: Detect words in the photograph. AI: {all texts}.\ncontinue reading\nA\n<Image>Human: The picture reads {left texts}.\n<Image>Human: The image says {left texts}.\n<Image>Human: There are words {left texts} in the image.\n<Image>Human: Words {left texts} are in the picture.\n<Image>Human: The texts in this image read {left texts}.\n<Image>Human: The words on this picture are {left texts}.\n<Image>Human: The script depicted in this image reads {left texts}.\n<Image>Human: The writing on this visual representation states {left texts}.\n<Image>Human: The content presented in this diagram states {left texts}.\n<Image>Human: The language used in this photograph says {left texts}.\n<Image>Human: The inscription on this picture explain {left texts}.\nB\nContinue reading the text. AI: {right texts}.\nRead the following text. AI: {right texts}.\nRead the text behind. AI: {right texts}.\nWhat is the following text? AI: {right texts}.\nkey points generation -\n<Image>Human: Identify some key points in this picture. AI: {key points}.\n<Image>Human: Point out several critical features in this image. AI: {key points}.\n<Image>Human: Highlight a few significant elements in this photo. AI: {key points}.\n<Image>Human: Give some essential details in this illustration. AI: {key points}.\n<Image>Human: Draw attention to some important aspects in this diagram. AI: {key points}.\n<Image>Human: Mention a couple of crucial points in this snapshot. AI: {key points}.\n<Image>Human: Indicate a few pertinent items in this graphic. AI: {key points}.\n<Image>Human: Outline some significant characteristics in this image. AI: {key points}.\n<Image>Human: Specify some key components in this picture. AI: {key points}.\n<Image>Human: List a handful of essential elements in this visual. AI: {key points}.\n2858",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.6065653562545776
    },
    {
      "name": "Tian",
      "score": 0.6032904386520386
    },
    {
      "name": "Computer science",
      "score": 0.530195415019989
    },
    {
      "name": "Natural language processing",
      "score": 0.5206562876701355
    },
    {
      "name": "Linguistics",
      "score": 0.5194994211196899
    },
    {
      "name": "Situated",
      "score": 0.47355571389198303
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4298381209373474
    },
    {
      "name": "Philosophy",
      "score": 0.33085280656814575
    },
    {
      "name": "Art",
      "score": 0.15401345491409302
    },
    {
      "name": "History",
      "score": 0.1337297260761261
    },
    {
      "name": "Literature",
      "score": 0.10726004838943481
    },
    {
      "name": "China",
      "score": 0.09603679180145264
    },
    {
      "name": "Archaeology",
      "score": 0.05605557560920715
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ]
}