{
  "title": "Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training",
  "url": "https://openalex.org/W3199404008",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2100574101",
      "name": "Bo Zheng",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2612910427",
      "name": "Shaohan Huang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3043020393",
      "name": "Saksham Singhal",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096786427",
      "name": "Wanxiang Che",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098738246",
      "name": "Ting Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102676534",
      "name": "Xia Song",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3175327901",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W4300427991",
    "https://openalex.org/W3126822054",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W3175898847",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3105492289",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W3177035927",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3119175506",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W36903255"
  ],
  "abstract": "Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an algorithm VoCap to determine the desired vocabulary capacity of each language. However, increasing the vocabulary size significantly slows down the pre-training speed. In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VoCap benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github.com/bozheng-hit/VoCapXLM.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3203–3215\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3203\nAllocating Large Vocabulary Capacity for\nCross-lingual Language Model Pre-training\nBo Zheng†∗, Li Dong ‡, Shaohan Huang ‡,\nSaksham Singhal‡, Wanxiang Che †, Ting Liu †, Xia Song ‡, Furu Wei ‡\n†Harbin Institute of Technology\n‡Microsoft Corporation\n{bzheng,car,tliu}@ir.hit.edu.cn\n{lidong1,shaohanh,saksingh,xiaso,fuwei}@microsoft.com\nAbstract\nCompared to monolingual models, cross-\nlingual models usually require a more expres-\nsive vocabulary to represent all languages ad-\nequately. We ﬁnd that many languages are\nunder-represented in recent cross-lingual lan-\nguage models due to the limited vocabulary ca-\npacity. To this end, we propose an algorithm\nVOCAP to determine the desired vocabulary\ncapacity of each language. However, increas-\ning the vocabulary size signiﬁcantly slows\ndown the pre-training speed. In order to ad-\ndress the issues, we propose k-NN-based tar-\nget sampling to accelerate the expensive soft-\nmax. Our experiments show that the multi-\nlingual vocabulary learned with VOCAP bene-\nﬁts cross-lingual language model pre-training.\nMoreover, k-NN-based target sampling mit-\nigates the side-effects of increasing the vo-\ncabulary size while achieving comparable per-\nformance and faster pre-training speed. The\ncode and the pretrained multilingual vocab-\nularies are available at https://github.\ncom/bozheng-hit/VoCapXLM.\n1 Introduction\nPretrained cross-lingual language models (Con-\nneau and Lample, 2019; Conneau et al., 2020; Chi\net al., 2021b; Xue et al., 2020) have recently shown\ngreat success in improving cross-lingual transfer-\nability. These models encode texts from differ-\nent languages into universal representations with a\nshared multilingual vocabulary and a shared Trans-\nformer encoder (Vaswani et al., 2017). By pre-\ntraining cross-lingual language models on the large-\nscale multilingual corpus, the models achieve state-\nof-the-art performance on various downstream\ntasks, e.g., cross-lingual question answering and\ncross-lingual sentence classiﬁcation.\nAlthough the Transformer architecture used in\nmost pretrained monolingual and cross-lingual lan-\nguage models are almost identical, the vocabularies\n∗Contribution during internship at Microsoft Research.\nare quite different. The vocabulary sizes in exist-\ning pretrained monolingual language models typi-\ncally range from 30K to 60K subword units (Devlin\net al., 2019; Liu et al., 2019; Dong et al., 2019; Bao\net al., 2020). Meanwhile, state-of-the-art pretrained\ncross-lingual language models use the shared mul-\ntilingual vocabulary of 250K subword units to rep-\nresent more than 100 languages (Conneau et al.,\n2020; Chi et al., 2021b; Xue et al., 2020). Although\nsome subword units are shared across languages,\nno more than 2.5K language-speciﬁc subword units\non average are allocated for each language, which\nis still relatively small. Besides, the multilingual\nvocabulary is trained on the combined multilingual\ncorpus with subword segmentation algorithms like\nBPE (Sennrich et al., 2015) and unigram language\nmodel (Kudo, 2018). During vocabulary construc-\ntion, these algorithms tend to select more subword\nunits shared across languages with common scripts\nlike Latin and Cyrillic (Chung et al., 2020b), but\nhave a lower chance to select language-speciﬁc\nsubword units. It is hard to determine how much\nvocabulary capacity a particular language requires\nand whether the shared multilingual vocabulary has\nallocated enough vocabulary capacity to represent\nthe language.\nIn this paper, we propose VOCAP, an algorithm\nto allocate large vocabulary for cross-lingual lan-\nguage model by separately evaluating the required\nvocabulary capacity of each language. First, we\nuse the average log probability (ALP) to evaluate\nthe ability of a vocabulary to represent a particular\nlanguage. We ﬁnd that ALP is highly correlated to\nthe downstream task performance, and we use it\nas an indicator to allocate language-speciﬁc vocab-\nulary capacity. In addition, the language-speciﬁc\npre-training corpus size should also be considered\nsince the pretrained model can only learn limited\nknowledge from low-resource languages where the\npre-training data is scarce. Therefore, allocating\ntoo much vocabulary capacity for low-resource lan-\n3204\nguages is inefﬁcient. VOCAP leverages both ALP\nand pre-training corpus size to evaluate the required\nvocabulary capacity of each language. We ﬁnally\nallocate a multilingual vocabulary with 500K sub-\nword units with VOCAP and show it can signiﬁ-\ncantly improve the model performance.\nHowever, increasing the vocabulary size has two\npractical drawbacks: slow pre-training speed and\nheavy model size. To address the pre-training speed\nissue, we propose k-NN-based target sampling, an\napproximate algorithm to improve the computing\nefﬁciency in the expensive softmax caused by the\nlarge vocabulary. We pre-train the model with a\nsmall subset of the entire vocabulary constructed\nwith k nearest neighbors of the target words in\ncurrent mini-batch data, evaluated with the inner\nproduct of subword embeddings. As for the model\nsize, we halve the embedding dimension and draw\na different conclusion from Conneau et al. (2020)\nthat increasing vocabulary from 250K to 500K with\na ﬁxed capacity model can also improve the perfor-\nmance.\nOur contributions are summarized as follows:\n• We propose VOCAP, an algorithm to allocate\nappropriate vocabulary capacity for each lan-\nguage in the shared multilingual vocabulary\nof cross-lingual language models.\n• We propose k-NN-based target sampling, a\nsoftmax approximation algorithm to improve\nthe computing efﬁciency during cross-lingual\nlanguage model pre-training.\n• We evaluate our methods on the XTREME\nbenchmark (Hu et al., 2020), including three\ndifferent tasks on seven datasets. Experiments\nshow that VOCAP consistently outperforms\nprevious vocabulary construction methods.\nMeanwhile, our k-NN-based target sampling\nenables effective acceleration while achieving\ncomparable performance.\n2 V OCAP: Language-Speciﬁc\nVocabulary Capacity Allocation\nWe attribute the main factors that affect the per-\nformance of a particular language in a cross-\nlingual language model to language-speciﬁc pre-\ntraining corpus size and vocabulary capacity. While\nprevious work adjusts pre-training corpus size\nwith an exponentially smoothed sampling distri-\nbution (Conneau and Lample, 2019; Conneau et al.,\n2020), few existing works have explored the ef-\nfect of the language-speciﬁc vocabulary capacity\nin pretrained cross-lingual language models.\nIn this section, we ﬁrst investigate the correlation\nbetween the language-speciﬁc vocabulary capacity\nand downstream task performance through experi-\nments. Then we introduce our proposed multilin-\ngual vocabulary allocation algorithm VOCAP.\n2.1 Investigating Language-Speciﬁc\nVocabulary Capacity\nWe start by introducing average log probability\n(ALP) to quantify the language-speciﬁc vocabulary\ncapacity in the shared multilingual vocabulary for\na speciﬁc language.1 Given a monolingual corpus\ncomposed of sentences Di = {s1,...,s |Di|}from\nthe i-th language and tokenized with vocabulary V,\nthe average log probability is deﬁned as follows:\nALP(Di,V ) = 1\n|Di|\n|Di|∑\nj=1\n|sj|∑\nk=1\nlog puni(sk\nj) (1)\nwhere sk\nj is the k-th subword of the sentence sj,\nand puni(·) is the unigram distribution counted on\nthe monolingual corpus Di. It is difﬁcult to count\nthe language-speciﬁc subword units in multilingual\nvocabularies since the raw text contains a lot of\ncode-switched data. By contrast, ALP is a more\nconvenient indicator of language-speciﬁc vocabu-\nlary capacity and it is penalized by the subword\nunits with low-frequency.\nTo investigate the impact of language-speciﬁc\nvocabulary capacity, we ﬁrst learn monolingual\nvocabularies in different sizes to obtain vocabu-\nlaries with different ALP, i.e., language-speciﬁc\nvocabulary capacity. Then we conduct pre-training\nwith these monolingual vocabularies on their cor-\nresponding monolingual corpora. Finally, we eval-\nuate these monolingual models on downstream\ntasks and study the correlation between language-\nspeciﬁc vocabulary capacity and downstream task\nperformance.\n2.1.1 Setup\nTo alleviate the bias from the languages’ character-\nistics, we ﬁrst select four languages with different\npre-training corpus sizes from different language\nfamilies, which are Hindi (hi), Persian (fa), Italian\n(it), Russian (ru). We ﬁrst learn thirty monolingual\n1For brevity and consistency, we refer to the parameterized\ntokenizer also as vocabulary, e.g., SentencePiece model.\n3205\n0 5000 10000 15000 20000 25000 30000\nVocabulary Size\n−300\n−280\n−260\n−240\n−220\n−200\n−180\n−160\nALP\nhi\nfa\nit\nru\nFigure 1: ALP of different monolingual vocabularies\nwith different vocabulary sizes.\n−300 −280 −260 −240 −220 −200 −180 −160\nALP\n96.4\n96.8\n97.2\n97.6\nF1\nhi\nfa\nit\nru\nFigure 2: F1 score on POS task with different vocabu-\nlaries versus their ALP on the monolingual corpus.\n−300 −280 −260 −240 −220 −200 −180 −160\nALP\n86\n88\n90\n92\n94F1\nhi\nfa\nit\nru\nFigure 3: F1 score on NER task with different vocabu-\nlaries versus their ALP on the monolingual corpus.\nLow res. Mid res. High res.\n−240\n−230\n−220\n−210\n−200\n−190\n−180\n−170\n−160\n−150\nALP\nJoint250K\nJoint500K\nVoCap500K\nFigure 4: Comparison of vocabulary capacity of\ndifferent-resourced languages. Shorter bars indicate\nlarger vocabulary capacity.\nvocabularies for each language on the correspond-\ning monolingual corpus, with vocabulary size rang-\ning from 1K to 30K. Then we pretrain monolingual\nlanguage models with the corresponding monolin-\ngual vocabularies. We evaluate these pretrained\nmodels on two downstream tasks: NER (Pan et al.,\n2017) and POS (Zeman et al., 2019) from the\nXTREME benchmark since there is annotated task\ndata for a large number of languages. The vocab-\nularies are learned on the reconstructed Common-\nCrawl corpus (Chi et al., 2021b; Conneau et al.,\n2020) using SentencePiece (Kudo and Richardson,\n2018) with the unigram language model (Kudo,\n2018). The unigram distributions are also counted\non the CommonCrawl corpus. The Wikipedia cor-\npus is used for all pre-training experiments in this\npaper since it is easier to run experiments due to\nits smaller size. More details about the pre-training\ndata can be found in the appendix.\n2.1.2 Observations\nIncreasing vocabulary size affects ALP of dif-\nferent languages in varying degrees. In Fig-\nure 1, we show the correlation between vocabulary\nsize and ALP of four different languages. We ob-\nserve the ALP varies across different languages,\nmainly because ALP correlates with the lexicon\ngranularity of the language, i.e., the average num-\nber of tokens per sentence. Besides, when the vo-\ncabulary size is larger than 10,000, the gains of\nincreasing monolingual vocabulary size in hi and\nfa are less than it and ru. We attribute it to that hi\nand fa does not have extensive compoundings. An-\nother observation is that for each language, every\ntime we increase the vocabulary size by 1K, the\nincrement in ALP is monotonically decreasing.\nALP correlates positively with downstream\ntask performance. In Figure 2 and Figure 3, we\nillustrate downstream task performance of mod-\nels pretrained with monolingual vocabularies on\ncorresponding monolingual corpora. We observe\nthat ALP correlates positively with downstream\ntask performance, making language-speciﬁc ALP a\nvalid indicator to allocate multilingual vocabulary.\nAnother natural option to allocate multilingual vo-\n3206\nAlgorithm 1 Allocating Multilingual V ocabulary withVO-\nCAP\nInput: size of target multilingual vocabulary T; monolin-\ngual vocabularies of N languages {Vi\nti }N\ni=1; monolingual\ncorpus of N languages {Di}N\ni=1\nOutput: multilingual vocabulary V\n1: for i←1 to N do\n2: for j ←1 to 50 do\n3: ai,j×1000 ←ALP(Di,V i\nj×1000)\n4: ti ←0\n5: ai,0 ←−∞\n6: do\n7: j ←0\n8: δ←0\n9: for i←1 to N do\n10: if δ <ai,ti+1000 −ai,ti then\n11: δ←ai,ti+1000 −ai,ti\n12: j ←i\n13: tj ←tj + 1000\n14: V ←|⋃N\ni=1 Vi\nti |\n15: while |V|<T\n16: if |V|>T then\n17: Clip the size of V to T\ncabulary is directly using monolingual vocabulary\nsize to indicate language-speciﬁc vocabulary ca-\npacity. We compare ALP against vocabulary size\nand observe that ALP correlates better than vocab-\nulary size with the downstream task performance.\nBesides, ALP reﬂects the language-speciﬁc char-\nacteristics, while vocabulary size does not. The\ndetailed comparison is shown in the appendix.\n2.2 Allocating Multilingual Vocabulary with\nVOCAP\nBased on the observations in Section 2.1.2, we\nﬁrst give the implementation of our proposed vo-\ncabulary allocation algorithm VOCAP. Then we\ncompare the multilingual vocabulary learned with\nVOCAP and directly learned with SentencePiece\non the multilingual corpus.\n2.2.1 V OCAP Implementation\nWe formulate the vocabulary construction of VO-\nCAP as the problem of ﬁnding the optimal way\nto allocate language-speciﬁc vocabulary size to\neach language, such that the overall ALP of all\nlanguages is maximized. In addition to language-\nspeciﬁc vocabulary capacity measured with ALP\nfrom Equation (1), the language-speciﬁc pre-\ntraining corpus size also affects the downstream\ntask performance. Considering the two factors, the\nprocedure of VOCAP can be formulated as follows:\nargmax\nt1,...,tN\nN∑\ni=1\nqβ\ni ALP(Di,V i\nti) s.t. |\nN⋃\ni=1\nVi\nti|= T\n(2)\nwhere ti ∈{x×1000 |x ≤50,x ∈N+}is the\nnumber of subword units allocated to the i-th lan-\nguage,2 βis a rescaling factor,Vi\nti is the vocabulary\nof the i-th language with ti subword units, T is the\nsize of the target multilingual vocabulary, and qi\nis the probability of sampling training instances\nfrom i-th language during pre-training (Conneau\nand Lample, 2019; Conneau et al., 2020):\nqi = fα\ni∑N\nj=1 fα\nj\nwith fi = ni\n∑N\nk=1 nk\n(3)\nwhere ni is the number of instances in the i-th lan-\nguage, αis a rescaling factor used to alleviate the\nbias towards high-resource languages. Since the\nincrement in ALP when increasing the vocabulary\nsize by a certain number is monotonically decreas-\ning, Equation (2) can be solved with the greedy\nalgorithm in Algorithm 1.\n2.2.2 Intrinsic Analysis\nWe compare the multilingual vocabulary learned\nwith VOCAP and directly learned with Sentence-\nPiece on the multilingual corpus. The multilin-\ngual corpus to learn vocabularies in this paper is\nthe concatenation of sentences sampled randomly\nfrom the monolingual corpora. Sentences from the\ni-th language is sampled with probability qi from\nEquation (3) and use α= 0.7. We ﬁlter languages\nwith corpus size larger than 0.1 GB, resulting in 86\nlanguages.\nWe evaluate the multilingual vocabularies with\ntheir ALP on each language’s monolingual corpus,\nand show results of different-resourced languages\nin Figure 4. We refer to languages with less than\n1GB and more than 10GB pre-training corpus in\nthe reconstructed CommonCrawl as low-resource\nand high-resource languages, respectively, other-\nwise mid-resource languages. When directly learn-\ning vocabulary on the multilingual corpus using\nSentencePiece, the vocabulary with 500K subword\nunits (JOINT 500K) only has a negligible improve-\nment compared to the vocabulary with 250K sub-\nword units (JOINT 250K). Meanwhile, our method\n2Since the cost of learning monolingual vocabularies with\narbitrary sizes and getting the corresponding ALP is unafford-\nable, we learn monolingual vocabularies with vocabulary size\nrange from 1K to 50K at intervals of 1K.\n3207\n(VOCAP500K) consistently outperforms JOINT 500K\nin different-resourced languages, especially in mid\nand low-resource languages. The statistics of the\nallocated vocabulary size for each language in\nVOCAP500K are shown in the appendix.\n3 Accelerate Large-Vocabulary\nLanguage Model Pre-Training\nAlthough extending the multilingual vocabu-\nlary beneﬁts cross-lingual language models, pre-\ntraining with such large vocabularies brings two\npractical issues: slow pre-training speed and heavy\nmodel size. To tackle the issues, we ﬁrst introduce\nour k-NN-based target sampling in Section 3.1,\nwhich is a softmax approximation algorithm to im-\nprove computing efﬁciency. Then we describe how\nwe reallocate the model parameters to keep the\nmodel size ﬁxed in Section 3.2.\n3.1 k-NN-Based Target Sampling\nTo reduce the expensive computation cost of the\nsoftmax function, we propose k-NN-based target\nsampling to approximate the expensive softmax.\nThe original masked language modeling objective\nminimizes the cross-entropy loss for every masked\nsubword wi on the extensive multilingual vocabu-\nlary V. The proposed k-NN-based target sampling\ninstead uses a smaller vocabulary subset V′. The\napproximation of the masked language modeling\nloss for the masked subword wi is deﬁned as fol-\nlows:\nL(wi) =−log exp(hTvwi + bwi)∑\nwj∈V′exp(hTvwj + bwj ) (4)\nwhere his the corresponding output vector of the\npenultimate network layer, i.e., the output vector\nof the Transformer encoder, vwi is the embedding\nof the subword unit wi, and bwi is a bias term. We\nformulate the construction of the vocabulary subset\nV′as follows:\nV′=\n⋃\nwi∈W\nIk(wi) (5)\nIk(wi) = top-k({vT\nwivwj |wj ∈V}) (6)\nwhere Wdenotes the set of target masked subword\nunits in the current mini-batch, and Ik(wi) denotes\nthe k most similar subwords measured with the\ninner product of the subword embedding vwi and\nvwj .\nHowever, retrievingIk(wi) at every training step\nfor every subword unit wi ∈W requires as much\nAlgorithm 2 Pre-training with k-NN-based target sampling\nInput: multilingual corpus Dm; size kof k-NN-based target\nsampling; multilingual vocabulary V; learning rate τ\nOutput: model parameters θ\n1: while not converged do\n2: Sample nmini-batches {X(t),W(t)}n\nt=1 ∼Dm ⊿\nX(t) is a mini-batch of monolingual text, and W(t) is the\nset of masked subwords.\n3: Update Ik(wi) for every wi ∈V\n4: for t←1 to ndo ⊿Train the model for nsteps.\n5: V′←⋃\nwi∈W(t) Ik(wi)\n6: g←∑\nwi∈W(t) ∇θL(wi)\n7: θ←θ−τg\ncomputation cost as softmax, which is unaffordable.\nAs an alternative, we compute Ik(wi) for every\nsubword wi ∈V according to the current subword\nembeddings every ntraining steps and replace the\nprevious version of Ik(wi) with the new one. We\ndetermine the value of nsuch that |V|≪ n×|W|.\nWe illustrate the pre-training procedure withk-NN-\nbased target sampling in Algorithm 2.\nFrom a practical point of view under the cross-\nlingual setting, the previous sampling-based soft-\nmax approximation methods either sample sub-\nwords from recent mini-batches or samples sub-\nwords from unigram distribution, the task becomes\nsimpler since a considerable part of the subword\nsamples is from different languages. Meanwhile,\nour k-NN-based target sampling uses subwords\nwith similar representations like synonyms, which\nenforces the model focus on discriminating the\nground-truth subword from a set of noise samples\nthat are not easy to distinguish. When using an\napproximate algorithm, the key point is to remain\nthe difﬁcult part of the original masked language\nmodeling objective as much as possible.\n3.2 Reducing the Embedding Dimension\nIn order to keep the number of model parameters\nﬁxed while increasing the vocabulary size, we fol-\nlow (Lan et al., 2020) and (Chung et al., 2020a)\nto reduce both the input and output embedding di-\nmension and linearly project the embeddings to\nthe hidden dimension of the Transformer blocks.\nMore precisely, we halve the embedding dimension\nwhen the vocabulary size is doubled. This rebal-\nancing strategy only slightly degrades the model\nperformance but improves pre-training speed and\ndecreases the model size.\nConneau et al. (2020) also studied the relation\nbetween the size of the shared multilingual vocabu-\nlary and downstream task performance with multi-\n3208\nModel # Params Speed Pair Sentence Structure Prediction Question Answering\nXNLI PAWS-X POS NER XQuAD MLQA TyDiQA\nAcc. Acc. F1 F1 F1/EM F1/EM F1/EM Avg.\nXLM-R250K 265M 1.00x 68.7 82.6 72.1 60.6 63.4/47.4 57.2/39.6 45.2/29.6 60.7\nJOINT 250K 265M 1.00x 69.2 83.3 72.4 59.7 63.9/47.9 58.9/40.7 45.4/29.6 61.1\nJOINT 500K 448M 0.72x 69.4 82.2 72.1 60.5 64.7/48.0 58.2/40.3 48.0/32.6 61.4\nVOCAP250K 265M 1.00x 69.3 82.0 71.4 60.0 66.2/50.3 60.1/42.6 45.6/30.6 61.5\nVOCAP500K 448M 0.72x 70.5 83.0 72.9 62.7 66.8/50.6 60.9/42.9 50.0/34.5 63.1\n+ k-NN 448M 1.18x 70.8 82.6 72.5 61.8 67.1/49.8 61.4/42.5 56.3/39.3 63.7\n+ half emb 265M 0.94x 70.3 83.0 72.0 61.7 65.8/49.0 61.0/42.3 49.3/33.0 62.5\n+ k-NN & half emb 265M 1.35x 69.8 83.4 72.1 60.1 66.6/49.5 60.8/42.7 50.2/33.9 62.5\nTable 1: Evaluation results on the XTREME benchmark. “XLM-R250K” denotes using the XLM-R (Conneau et al.,\n2020) vocabulary with 250K subword units. “ k-NN” and “half emb” denote our k-NN-based target sampling\nmethod and using half embedding dimension, respectively.\nlingual models of the ﬁxed number of parameters.\nThey keep the overall number of parameters con-\nstant by adjusting the width (i.e., hidden size) of the\nTransformer. Notice that we only reduce the em-\nbedding dimension while keeping the Transformer\nblocks untouched.\n4 Experiment\n4.1 Setup\nFine-Tuning Datasets To validate the effective-\nness of our methods, we conduct experiments on\nthree types of cross-lingual understanding tasks\nfrom XTREME benchmark (Hu et al., 2020), in-\ncluding two classiﬁcation datasets: XNLI (Con-\nneau et al., 2018), PAWS-X (Yang et al., 2019),\nthree span extraction datasets: XQuAD (Artetxe\net al., 2020), MLQA (Lewis et al., 2020), TyDiQA-\nGoldP (Clark et al., 2020), and two sequence label-\ning datasets: NER (Pan et al., 2017), POS (Zeman\net al., 2019). The statistics of the datasets are shown\nin the appendix.\nImplementation Details We adapt the Trans-\nformer architecture from the base model setting\nin Conneau et al. (2020), i.e., 12 layers and 768\nhidden dimension size. We use masked language\nmodeling objective to train our models for 1 million\nupdates on eight 32GB Nvidia V100 GPUs with\na batch size of 256. We update the top-k indices\nfor every word in the multilingual vocabulary every\n1,000 training steps and use k= 50in k-NN-based\ntarget sampling. The learning rate is scheduled with\na polynomial decay with 10K warmup steps, where\nthe peak learning rate is set as 0.0001. We adapt\nother hyper-parameters in pre-training from Chi\net al. (2021b). All ﬁne-tuning results are averaged\nover ﬁve random seeds. The ﬁne-tuning pipeline\nis based on the code base of (Zheng et al., 2021).\nThe ﬁne-tuning implementation details are shown\nin the appendix.\n4.2 Results\nTable 1 shows XTREME ﬁne-tuning results with\nmodels pretrained using different vocabularies\nand acceleration strategies. Compared to vocab-\nulary directly learned on multilingual corpus with\nSentencePiece, i.e., XLM-R250K and JOINT 250K,\nour VOCAP250K improves on question answering\ndatasets but degrades on PAWS-X, POS and NER.\nThen increasing the vocabulary from VOCAP250K\nto VOCAP500K mitigates the gap and bring im-\nprovements on six datasets except for PAWS-X,\nwhich only includes seven high-resource languages.\nHowever, increasing the size of vocabulary di-\nrectly learned with Sentencepiece from JOINT 250K\nto JOINT 500K does not improve the performance\nas our VOCAP method does, showing the impor-\ntance of selecting language-speciﬁc subword units\nand leveraging how much vocabulary capacity each\nlanguage requires.\nSince increasing vocabulary size brings the is-\nsues of model size and pre-training speed, we study\nthe proposed method to accelerate pre-training:\nk-NN-based target sampling ( k-NN) and using\nhalf embedding dimension (half emb). Our k-\nNN method improves pre-training speed with a\n500K vocabulary so that the speed is 1.18 times\nthat vanilla pre-training with a 250K vocabulary.\nMeanwhile, pre-training with our k-NN method\ndoes not signiﬁcantly degrade the performance, it\neven brings improvement on XNLI, MLQA, and\nTyDiQA. Then we halve the embedding dimension\nof the models with 500K vocabulary and results in a\nsimilar number of parameters to models with 250K\n3209\nMethod XNLI POS MLQA Speed\nVOCAP500K 69.2 72.9 59.9/41.7 1.00x\n+ k-NN 69.3 72.1 59.6/40.3 1.64x\n+ target sampling 68.8 71.3 57.6/38.8 1.56x\n+ NCE 56.0 61.8 41.1/26.2 1.40x\n+ NEG 56.5 62.9 40.1/25.6 1.40x\nTable 2: Comparison between different sampling-based\nsoftmax approximation approaches with vocabulary\nVOCAP500K. Models are pretrained for 0.5M steps.\nMethod XNLI POS MLQA Speed\nVOCAP500K 69.2 71.8 59.9/ 41.7 1.00x\n+ k-NN (k=5) 68.5 71.3 58.6/40.0 1.76x\n+ k-NN (k=10) 69.3 71.4 58.9/39.6 1.74x\n+ k-NN (k=25) 69.2 71.7 59.8/40.9 1.69x\n+ k-NN (k=50) 69.3 72.1 59.6/40.3 1.64x\n+ k-NN (k=100) 69.5 72.1 60.0 /41.3 1.57x\nTable 3: Comparison between different k values in k-\nNN-based sampling method. Models are pretrained for\n0.5M steps.\nvocabulary. The overall performance degrades by\n0.6-points but still consistently improves over mod-\nels with 250K vocabularies while the speed is com-\nparable. Combining the two methods above, we\nachieve a 1.35-times speed-up and more than 1\npoint improvement with a similar model size com-\npared to models with 250K vocabularies.\n4.3 Analysis and Discussion\nWe conduct a thorough analysis to understand the\nimpact of our proposed methods on cross-lingual\nlanguage models. To reduce the computation load,\nwe only pre-train the cross-lingual language models\nfor 500K steps for some of our settings.\nk-NN-based target sampling outperforms previ-\nous sampling-based approaches. To verify the\neffectiveness of our proposed k-NN-based sam-\npling method, we compare it against previous\nsampling-based approaches used to approximate\nsoftmax, which are target sampling (Jean et al.,\n2015), noise contrastive estimation (Mnih and Teh\n(2012), NCE) and negative sampling (Mikolov et al.\n(2013), NEG). The results are shown in Table 2.\nTo make a fair comparison, since our k-NN-based\nsampling method using k = 50 samples vocabu-\nlary subset with less than 50,000 subword units\nper batch on average, we here sample 50,000 neg-\native subword units per batch for target sampling,\nNCE, and NEG. Among the four methods, NCE\nand NEG are signiﬁcantly worse than k-NN and\nMethod XNLI POS NER MLQA\nβ=0 66.9 71.8 61.5 58.6/41.0\nβ=0.3 69.0 71.7 61.6 59.2/40.1\nβ=0.7 69.2 71.8 61.5 59.9/41.7\nβ=1.0 69.5 71.8 60.9 58.4/40.3\nTable 4: Impact of adjusting high-resource versus low-\nresource vocabulary capacity trade-off with β. β = 0\nindicates the vocabulary is allocated without consider-\ning pre-training corpus size. Models are pretrained for\n0.5M steps.\ntarget sampling. We attribute it that NCE and NEG\nneed more training steps to converge (Mnih and\nTeh, 2012). Besides, the original NCE typically\nsample different negative samples for every target\nword, while we here use 50,000 negative samples\nfor all target word in current mini-batch, which is\nmore efﬁcient on GPUs.\nEffect of the value of k in k-NN-based target\nsampling. We illustrate the downstream task per-\nformance when using different values of kin our\nk-NN-based target sampling in Table 3. While a\nsmaller k indicates faster pre-training speed, we\nobserve even with a small value like 5, the result\ndoes not signiﬁcantly degrade compared to using\nthe original softmax. We attribute this to that by\nretrieving subword samples that are most similar to\nthe target subword, the model can focus on the difﬁ-\ncult part of the original masked language modeling\nobjective. More precisely, the model focus on dis-\ncriminating the ground-truth subword from a set of\nnoise samples that are not easy to distinguish. Con-\nsidering the overall performance, the pre-training\nspeed, and running memory to store k-NN indices,\nwe use k= 50in all our experiments.\nLanguage-speciﬁc pre-training corpus should\nalso be considered when allocating vocabulary\ncapacity. The pre-training corpus size varies\nacross different languages. It is inefﬁcient to al-\nlocate a large vocabulary capacity for low-resource\nlanguages with rare pre-training data since the pre-\ntrained model can only learn limited knowledge\nfrom these languages. Here we study the value of\nrescaling factor βfrom Equation (2) in multilingual\nvocabulary construction in Table 4. The rescaling\nfactor βcontrols the number of selected language-\nspeciﬁc subword units. Increasing the value of β\nimproves the performance of XNLI, where most\nlanguages are high-resource languages. However,\nit degrades the performance of NER, where more\n3210\n2.5 5.0 7.5 10.0 12.5\nPre-training cost (days)\n63\n64\n65\n66\n67\n68\n69\n70\n71Accuracy\n(a) XNLI\nJoint250K\nVoCap500K\nVoCap500K+KNN\nVoCap500K+half emb\nVoCap500K+KNN+half emb\n2.5 5.0 7.5 10.0 12.5\nPre-training cost (days)\n40\n42\n44\n46\n48\n50\n52(F1 + EM) / 2\n(b) MLQA\nJoint250K\nVoCap500K\nVoCap500K+KNN\nVoCap500K+half emb\nVoCap500K+KNN+half emb\nFigure 5: Performance on XNLI and MLQA versus the\ncross-lingual language models’ pre-training cost.\nlow-resources languages exist. When considering\noverall performance, we decide to use β = 0.7 in\nour experiments.\nThe proposed acceleration strategies signiﬁ-\ncantly improve the downstream task perfor-\nmance under the same pre-training cost. In-\ncreasing the vocabulary size slows the pre-training\nspeed, even though there is almost no difference\nin ﬁne-tuning speed. We study the relationship be-\ntween the downstream task performance and the\npre-training cost under different model settings in\nFigure 5. We observe VOCAP500K+k-NN achieves\nthe best performance. Models trained with 500K\nvocabulary consistently outperform 250K vocab-\nulary on XNLI. Besides, we observe the perfor-\nmance on MLQA with the model trained using\n250K vocabulary degrades as the training contin-\nues while models trained using 500K vocabulary\ndoes not, indicating the sufﬁcient vocabulary ca-\npacity is essential for question answering task.\nVOCAP gains more improvement on mid and\nlow-resource languages than high-resource lan-\nguages. In Figure 4 in Section 2, we show that\nthe vocabulary learned withVOCAP beneﬁts the vo-\ncabulary capacity of low-resource languages more\nthan high-resource languages, indicating the im-\nprovements should mainly come from low-resource\nlanguages. To verify this, we compare VOCAP\nagainst SentencePiece baseline on the performance\nof different-resourced languages on XNLI and\nNER in Figure 6. We observe that the vocabulary\nlearned with VOCAP signiﬁcantly outperforms the\nvocabularies directly learned with SentencePiece\non mid and low-resource languages. This obser-\nvation is also consistent with the ALP results in\nFigure 4.\nLow res. Mid res. High res.\n40\n45\n50\n55\n60\n65\n70\n75\n80Accuracy\n(a) XNLI\nJoint250K\nJoint500K\nVoCap500K\nLow res. Mid res. High res.\n40\n45\n50\n55\n60\n65\n70\n75\n80F1\n(b) NER\nJoint250K\nJoint500K\nVoCap500K\nFigure 6: Impact of V OCAP on the performance of\ndifferent-resourced languages on XNLI and NER.\n5 Related Work\nPretrained Cross-Lingual Language Models\nRecent work pre-trains Transformer mod-\nels (Vaswani et al., 2017) on the large-scale\nmultilingual corpus to obtain pretrained cross-\nlingual language models (Conneau and Lample,\n2019; Conneau et al., 2020; Chi et al., 2020,\n2021a,b,c,d; Chung et al., 2020a; Xue et al.,\n2020; Ma et al., 2020, 2021). These models are\ncapable of encoding texts from different languages\ninto universal representations and signiﬁcantly\nimproves cross-lingual transferability.\nMultilingual Vocabulary Construction Cross-\nlingual language models need large vocabularies\nto ensure all languages are adequately represented.\nRecent research work on constructing multilingual\nvocabulary for cross-lingual language models can\nbe categorized into two groups. mBERT (Devlin\net al., 2019), XLM (Conneau and Lample, 2019),\nand XLM-R (Conneau et al., 2020) learn vocab-\nularies on a combined multilingual corpus with\nWordPiece (Wu et al., 2016), BPE (Sennrich et al.,\n2015), and unigram language model (Kudo, 2018)\nfrom SentencePiece (Kudo and Richardson, 2018),\nrespectively. Chung et al. (2020b) propose to bal-\nance the trade-off between optimizing for cross-\nlingual subword sharing and the need for robust\nrepresentation of individual languages. They ﬁrst\ngroup languages into clusters and learn vocabular-\nies individually on each cluster, then combine all\ncluster-vocabularies to form a single uniﬁed mul-\ntilingual vocabulary. Compared to Chung et al.\n(2020b), our advantage is that we separately quan-\ntify the vocabulary capacity each language needs\nwith average log probability and balance the con-\nstruction procedure with pre-training corpus size.\n3211\nSoftmax Approximation Approximating the\nsoftmax was a core problem in training NLP tasks\nwith a large vocabulary, e.g., neural machine trans-\nlation, language modeling. With the rise of sub-\nword representations (Sennrich et al., 2015; Wu\net al., 2016; Kudo, 2018), the vocabulary size sig-\nniﬁcantly decreases, and the problem has been less\nstudied recently. Nevertheless, the need for train-\ning cross-lingual language models with a large\nmultilingual vocabulary has drawn our attention\nagain to the softmax approximation approaches.\nThe existing softmax approximation approaches\ncan be grouped into softmax-based and sampling-\nbased approaches. Softmax-based approaches in-\ncludes hierarchical softmax (Morin and Bengio,\n2005), differentiated softmax (Chen et al., 2016),\nand CNN-softmax (Kim et al., 2016). However,\nthese approaches improve the softmax efﬁciency\nby changing its architecture, which is unsuitable\nfor either training on GPUs or multilingual set-\ntings. Sampling-based approaches instead opti-\nmize some other easy-to-compute loss function to\napproximate the original softmax, including tar-\nget sampling (Jean et al., 2015), noise contrastive\nestimation (Mnih and Teh, 2012), negative sam-\npling (Mikolov et al., 2013). Our k-NN-based tar-\nget sampling is also a sampling-based approach.\n6 Conclusion\nIn this paper, we study pre-training cross-lingual\nlanguage models with large vocabulary capacity.\nFirst, we propose VOCAP to construct large multi-\nlingual vocabulary in cross-lingual language mod-\nels. We conduct a quantitative analysis to show\nthat average log probability is an valid indicator\nof vocabulary capacity for a particular language,\nwhich also correlates with downstream task perfor-\nmance on the language. VOCAP uses the language-\nspeciﬁc average log probability and pre-training\ncorpus size to allocate appropriate vocabulary ca-\npacity for each language in the multilingual vo-\ncabulary. Moreover, we propose k-NN-based tar-\nget sampling to accelerate pre-training with the\nallocated large multilingual vocabulary by approxi-\nmating the expensive softmax. We also show that\nreducing the embedding dimension is an effective\nway to keep the improvement brought by the large\nvocabulary without increasing the number of model\nparameters. The experiments demonstrate the effec-\ntiveness of the proposed vocabulary construction\nmethod as well as the acceleration methods.\nAcknowledgments\nWe would like to acknowledge Zewen Chi, Shum-\ning Ma for the helpful discussions. This work was\nsupported by the National Key R&D Program of\nChina via grant 2020AAA0106501 and the Na-\ntional Natural Science Foundation of China (NSFC)\nvia grant 61976072 and 61772153. Wanxiang Che\nis the corresponding author.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. In Proceedings\nof the 37th International Conference on Machine\nLearning, pages 7006–7016.\nWenlin Chen, David Grangier, and Michael Auli. 2016.\nStrategies for training large vocabulary neural lan-\nguage models. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nZewen Chi, Li Dong, Shuming Ma, Shaohan Huang,\nXian-Ling Mao, Heyan Huang, and Furu Wei.\n2021a. mT6: Multilingual pretrained text-to-text\ntransformer with translation pairs. arXiv preprint\narXiv:2104.08692.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7570–7577. AAAI Press.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. 2021b. In-\nfoXLM: An information-theoretic framework for\ncross-lingual language model pre-training. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3576–3588, Online. Association for Computational\nLinguistics.\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-\nLing Mao, Heyan Huang, and Furu Wei. 2021c.\n3212\nImproving pretrained cross-lingual language mod-\nels via self-labeled word alignment. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3418–3430,\nOnline. Association for Computational Linguistics.\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma,\nSaksham Singhal, Payal Bajaj, Xia Song, and\nFuru Wei. 2021d. XLM-E: Cross-lingual lan-\nguage model pre-training via ELECTRA. ArXiv,\nabs/2106.16138.\nHyung Won Chung, Thibault Févry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2020a. Re-\nthinking embedding coupling in pre-trained lan-\nguage models. CoRR, abs/2010.12821.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020b. Improving multilingual models\nwith language-clustered vocabularies. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020 , pages 4536–4546. As-\nsociation for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057–7067. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems , pages 13063–13075. Cur-\nran Associates, Inc.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. arXiv preprint arXiv:2003.11080.\nSébastien Jean, KyungHyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large tar-\nget vocabulary for neural machine translation. In\nProceedings of the 53rd Annual Meeting of the Asso-\nciation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing of the Asian Federation of Natural Lan-\nguage Processing, ACL 2015, July 26-31, 2015, Bei-\njing, China, Volume 1: Long Papers , pages 1–10.\nThe Association for Computer Linguistics.\nYoon Kim, Yacine Jernite, David A. Sontag, and\nAlexander M. Rush. 2016. Character-aware neural\nlanguage models. In Proceedings of the Thirtieth\nAAAI Conference on Artiﬁcial Intelligence, Febru-\nary 12-17, 2016, Phoenix, Arizona, USA , pages\n2741–2749. AAAI Press.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-\n20, 2018, Volume 1: Long Papers, pages 66–75. As-\nsociation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018 , pages 66–71. As-\nsociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\n3213\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShuming Ma, Li Dong, Shaohan Huang, Dong-\ndong Zhang, Alexandre Muzio, Saksham Sing-\nhal, Hany Hassan Awadalla, Xia Song, and Furu\nWei. 2021. DeltaLM: Encoder-decoder pre-training\nfor language generation and translation by aug-\nmenting pretrained multilingual encoders. ArXiv,\nabs/2106.13736.\nShuming Ma, Jian Yang, H. Huang, Zewen Chi,\nLi Dong, Dongdong Zhang, Hany Hassan Awadalla,\nAlexandre Muzio, Akiko Eriguchi, Saksham Sing-\nhal, Xia Song, Arul Menezes, and Furu Wei. 2020.\nXLM-T: Scaling up multilingual machine transla-\ntion with pretrained cross-lingual transformer en-\ncoders. ArXiv, abs/2012.15547.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and sim-\nple algorithm for training neural probabilistic lan-\nguage models. In Proceedings of the 29th Inter-\nnational Conference on Machine Learning, ICML\n2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012. icml.cc / Omnipress.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nProceedings of the Tenth International Workshop on\nArtiﬁcial Intelligence and Statistics, AISTATS 2005,\nBridgetown, Barbados, January 6-8, 2005 . Society\nfor Artiﬁcial Intelligence and Statistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008. Curran As-\nsociates, Inc.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual ad-\nversarial dataset for paraphrase identiﬁcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687–\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams,\nand et al. 2019. Universal dependencies 2.5.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntute of Formal and Applied Linguistics (ÚFAL), Fac-\nulty of Mathematics and Physics, Charles Univer-\nsity.\nBo Zheng, Li Dong, Shaohan Huang, Wenhui Wang,\nZewen Chi, Saksham Singhal, Wanxiang Che, Ting\nLiu, Xia Song, and Furu Wei. 2021. Consistency\nregularization for cross-lingual ﬁne-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 3403–\n3417, Online. Association for Computational Lin-\nguistics.\nA Correlation between\nLanguage-Speciﬁc Vocabulary\nCapacity and Task Performance\nWe compare the Pearson correlation coefﬁcients be-\ntween ALP and downstream task performance with\nthe coefﬁcients between vocabulary size and down-\nstream task performance in Table 5. The results\nshow that ALP correlates better than vocabulary\nsize with downstream task performance.\nB Statistics of XTREME Datasets\n.\n3214\nLanguage Task ρ(ALP,F1) ρ(|V|,F1)\nhi POS 0.922 0.787\nNER 0.879 0.890\nfa POS 0.905 0.700\nNER 0.912 0.872\nit POS 0.665 0.422\nNER 0.899 0.900\nru POS 0.423 0.327\nNER 0.872 0.833\nTable 5: Pearson correlation coefﬁcients between ALP\nand downstream task performance and between vocab-\nulary size and downstream task performance.\nTask Dataset |Train| |Lang|\nClassiﬁcation XNLI 392K 15\nPAWS-X 49.4K 7\nStructured POS 21K 33\nPrediction NER 20K 40\nQuestion\nAnswering\nXQuAD 87K 11\nMLQA 87K 7\nTyDiQA 3.7K 9\nTable 6: Statistics for the datasets in the XTREME\nbenchmark. we report the number of training examples\n(|Train|), and the number of languages (|Lang|).\nC Fine-tuning Settings\nImplementation Details For the POS dataset,\nwe use the average-pooling strategy on subwords to\nobtain word representation since part-of-speech is\nrelated to different parts of words, depending on the\nlanguage. We tune the hyper-parameter and select\nthe model with the best average results over all the\nlanguages’ development set. There are two datasets\nwithout development set in multi-languages. For\nXQuAD, we tune the hyper-parameters with the de-\nvelopment set of MLQA since they share the same\ntraining set and have a higher degree of overlap in\nlanguages. For TyDiQA-GoldP, we use the English\ntest set as the development set.\nHyper-Parameters For XNLI, PAWS-X, POS,\nand NER, we ﬁne-tune 10 epochs. For XQuAD\nand MLQA, we ﬁne-tune 4 epochs. For TyDiQA-\nGoldP, we ﬁne-tune 6 or 8 epochs and select the\nbest number of epochs with the English test set as\nthe development set. For learning rate, we select\nCode Size (GB) Code Size (GB) Code Size (GB)\naf 0.2 hu 9.5 pl 28.6\nam 0.4 hy 0.7 ps 0.4\nar 16.1 id 17.2 pt 39.4\nas 0.1 is 0.5 ro 11.0\naz 0.8 it 47.2 ru 253.3\nba 0.2 ja 86.8 sa 0.2\nbe 0.5 ka 1.0 sd 0.2\nbg 7.0 kk 0.6 si 1.3\nbn 5.5 km 0.2 sk 13.6\nca 3.0 kn 0.3 sl 6.2\ncs 14.9 ko 40.0 sq 3.0\ncy 0.4 ky 0.5 sr 7.2\nda 6.9 la 0.3 sv 60.4\nde 99.0 lo 0.2 sw 0.3\nel 13.1 lt 2.3 ta 7.9\nen 731.6 lv 1.3 te 2.3\neo 0.5 mk 0.6 tg 0.7\nes 85.6 ml 1.3 th 33.0\net 1.4 mn 0.4 tl 1.2\neu 1.0 mr 0.5 tr 56.4\nfa 19.0 ms 0.7 tt 0.6\nﬁ 5.9 mt 0.2 ug 0.2\nfr 89.9 my 0.4 uk 13.4\nga 0.2 ne 0.6 ur 3.0\ngl 1.5 nl 25.9 uz 0.1\ngu 0.3 nn 0.4 vi 74.5\nhe 4.4 no 5.5 yi 0.3\nhi 5.0 or 0.3 zh 96.8\nhr 1.4 pa 0.8\nTable 7: The statistics of the reconstructed Common-\nCrawl corpus for learning vocabularies.\nin [7e-6, 1e-5] for XNLI and PAWS-X, [1e-5, 2e-\n5] for POS and NER, [2e-5, 3e-5] for XQuAD,\nMLQA and TyDiQA-GoldP.\nD Pre-Training Data\nWe use the reconstruct CommonCrawl corpus in\nChi et al. (2021b) to learn vocabularies in our paper.\nBecause tokenizing the pre-training data is time-\nconsuming, we instead conduct our pre-training\non Wikipedia since it has a smaller size. We only\nconsider the languages that are shared by the re-\nconstructed CommonCrawl corpus and Wikipedia.\nThe statistics of the Wikipedia corpus and the re-\nconstructed CommonCrawl corpus are listed in Ta-\nble 8 and Table 7.\n3215\nCode Size (GB) Code Size (GB) Code Size (GB)\naf 0.12 hu 0.8 pl 1.55\nam 0.01 hy 0.6 ps 0.04\nar 1.29 id 0.52 pt 1.5\nas 0.04 is 0.05 ro 0.42\naz 0.24 it 2.69 ru 5.63\nba 0.13 ja 2.65 sa 0.04\nbe 0.31 ka 0.37 sd 0.02\nbg 0.62 kk 0.29 si 0.09\nbn 0.41 km 0.12 sk 0.21\nca 1.1 kn 0.25 sl 0.21\ncs 0.8 ko 0.56 sq 0.1\ncy 0.06 ky 0.1 sr 0.74\nda 0.33 la 0.05 sv 1.7\nde 5.43 lo 0.01 sw 0.03\nel 0.73 lt 0.19 ta 0.46\nen 12.58 lv 0.12 te 0.44\neo 0.25 mk 0.34 tg 0.04\nes 3.38 ml 0.28 th 0.52\net 0.23 mn 0.05 tl 0.04\neu 0.24 mr 0.1 tr 0.43\nfa 0.66 ms 0.2 tt 0.09\nﬁ 0.68 mt 0.01 ug 0.03\nfr 4.0 my 0.15 uk 2.43\nga 0.03 ne 0.06 ur 0.13\ngl 0.27 nl 1.38 uz 0.06\ngu 0.09 nn 0.13 vi 0.76\nhe 1.11 no 0.54 yi 0.02\nhi 0.38 or 0.04 zh 1.08\nhr 0.28 pa 0.1\nTable 8: The statistics of the Wikipedia corpus used for\npre-training.\nCode Size (K) Code Size (K) Code Size (K)\naf 2 hu 12 pl 20\nam 3 hy 5 ps 3\nar 15 id 13 pt 20\nas 2 is 3 ro 13\naz 5 it 22 ru 34\nba 2 ja 23 sa 1\nbe 3 ka 4 sd 2\nbg 9 kk 4 si 3\nbn 6 km 4 sk 11\nca 8 kn 2 sl 8\ncs 14 ko 17 sq 7\ncy 3 ky 3 sr 10\nda 9 la 3 sv 18\nde 24 lo 2 sw 3\nel 17 lt 7 ta 6\nen 23 lv 6 te 4\neo 4 mk 4 tg 5\nes 26 ml 3 th 14\net 5 mn 3 tl 4\neu 4 mr 3 tr 18\nfa 9 ms 4 tt 3\nﬁ 9 mt 3 ug 3\nfr 25 my 2 uk 12\nga 2 ne 3 ur 5\ngl 5 nl 14 uz 2\ngu 2 nn 3 vi 12\nhe 6 no 7 yi 2\nhi 6 or 2 zh 30\nhr 6 pa 3\nTable 9: The statistics of the allocated vocabulary size\nfor each language.",
  "topic": "Vocabulary",
  "concepts": [
    {
      "name": "Vocabulary",
      "score": 0.8864690065383911
    },
    {
      "name": "Computer science",
      "score": 0.8619265556335449
    },
    {
      "name": "Softmax function",
      "score": 0.7413256168365479
    },
    {
      "name": "Language model",
      "score": 0.6794807314872742
    },
    {
      "name": "Natural language processing",
      "score": 0.5932238698005676
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5909500122070312
    },
    {
      "name": "Code (set theory)",
      "score": 0.4886475205421448
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.4211021363735199
    },
    {
      "name": "Speech recognition",
      "score": 0.3975827097892761
    },
    {
      "name": "Linguistics",
      "score": 0.13328900933265686
    },
    {
      "name": "Artificial neural network",
      "score": 0.11186736822128296
    },
    {
      "name": "Programming language",
      "score": 0.09233739972114563
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.05541670322418213
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 13
}